{"title": "Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy", "authors": ["Cameron Allen", "Aaron Kirtland", "Ruo Yu Tao", "Sam Lobel", "Daniel Scott", "Nicholas Petrocelli", "Omer Gottesman", "Ronald Parr", "Michael L. Littman", "George Konidaris"], "abstract": "Reinforcement learning algorithms typically rely on the assumption that the environment dynamics and value function can be expressed in terms of a Markovian state representation. However, when state information is only partially observable, how can an agent learn such a state representation, and how can it detect when it has found one? We introduce a metric that can accomplish both objectives, without requiring access to or knowledge of an underlying, unobservable state space. Our metric, the \\(\\lambda\\)-discrepancy, is the difference between two distinct temporal difference (TD) value estimates, each computed using TD(\\$\\lambda\\$) with a different value of \\(\\lambda\\). Since TD(\\$\\lambda=0\\$) makes an implicit Markov assumption and TD(\\$\\lambda=1\\$) does not, a discrepancy between these estimates is a potential indicator of a non-Markovian state representation. Indeed, we prove that the \\(\\lambda\\)-discrepancy is exactly zero for all Markov decision processes and almost always non-zero for a broad class of partially observable environments. We also demonstrate empirically that, once detected, minimizing the \\(\\lambda\\)-discrepancy can help with learning a memory function to mitigate the corresponding partial observability. We then train a reinforcement learning agent that simultaneously constructs two recurrent value networks with different \\(\\lambda\\) parameters and minimizes the difference between them as an auxiliary loss. The approach scales to challenging partially observable domains, where the resulting agent frequently performs significantly better (and never performs worse) than a baseline recurrent agent with only a single value network.", "sections": [{"title": "1 Introduction", "content": "The dominant modeling frameworks for reinforcement learning [Sutton and Barto, 2018] define environments in terms of an underlying Markovian state representation. This modeling choice, called the Markov assumption, is nearly ubiquitous in reinforcement learning, because it allows environment dynamics, rewards, value functions, and policies all to be expressed as functions that are independent of the past given the most recent state. In principle, an environment can be modeled as either a Markov decision process (MDP) [Puterman, 1994], or its partially observable counterpart, a POMDP"}, {"title": "2 Background", "content": "We consider two frameworks for modeling sequential decision processes: MDPs and POMDPs. The MDP framework [Puterman, 1994] consists of a state space S, action space A, reward function \\(R : S \\times A \\rightarrow \\mathbb{R}\\), transition function \\(T : S \\times A \\rightarrow \\Delta S\\) mapping to a distribution over states, discount factor \\(\\gamma\\in [0, 1]\\), and initial state distribution \\(p_0 \\in \\Delta S\\). The agent's goal is to find a policy \\(\\pi : S \\rightarrow \\Delta A\\) that selects actions to maximize return, \\(g_t\\), the discounted sum of future rewards starting from timestep \\(t\\): \\(g_t = \\sum_{i=0}^{\\infty} \\gamma^i r_{t+i}\\), where \\(r_i\\) is the reward at timestep \\(i\\). We denote the expectation of these returns as value functions \\(V^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_t | S_t = s]\\) and \\(Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[g_t | S_t = s, a_t = a]\\).\nThe POMDP framework [Kaelbling et al., 1998] additionally includes a set of observations \\(\\Omega\\) and an observation function \\(\\Phi : S \\rightarrow \\Delta \\Omega\\) that describes the probability \\(\\Phi(w|s)\\) of seeing observation \\(w\\) in latent state \\(s\\). POMDPs are a more general model of the world, since they contain MDPs as a special case: namely, when observations have a one-to-one correspondence with states. Similarly, POMDPS where states correspond to disjoint sets of observations are called block MDPs [Du et al., 2019]. However, such examples are rare; in typical POMDPs, a single observation \\(w\\) does not contain enough information to fully resolve the state \\(s\\). While agents need not fully resolve the underlying state to behave optimally, they must retain at least enough information across timesteps that the optimal policy becomes clear.\nWe are interested in the learning setting, where the agent has no knowledge of the underlying state \\(s\\) nor even the set of possible states \\(S\\) (let alone the transition, reward, and observation functions). It receives an observation \\(w_t \\in \\Omega\\) at each timestep and must find a way to maximize expected return. One way to do this is to construct a state representation, perhaps using some form of memory, on which it can condition its behavior. A state representation \\(z \\in Z\\) is Markovian if at any timestep \\(t\\), the representation \\(z_t\\) and action \\(a_t\\) together are a sufficient statistic for predicting the reward \\(r_t\\) and next representation \\(z_{t+1}\\), instead of requiring the agent's whole history:\n\\begin{equation}\nPr(z_{t+1}, r_t | z_t, a_t) = Pr(z_{t+1}, r_t | z_t, a_t, ..., z_0, a_0).\n\\end{equation}\nStates and observations are equivalent in MDPs, and this property is satisfied for both by definition, but in POMDPs it typically only holds for the underlying, unobserved state \\(s\\)\u2014not the observations.\nMarkovian state representations have several desirable implications. First, if the Markov property holds then so does the Bellman equation: \\(V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a | s)(R(s, a) + \\gamma \\sum_{s' \\in S} T(s' | s, a) V^{\\pi}(s'))\\). The Bellman equation allows agents to estimate the value of a policy, from experiences, and without knowing \\(T\\) or \\(R\\), via a recurrence relation over one-step returns,\n\\begin{equation}\nV_{i+1}^{\\pi}(s) = \\mathbb{E}_{\\pi}[r_t + \\gamma V_i^{\\pi}(s_{t+1}) | S_t = s],\n\\end{equation}\nwhich converges to the unique fixed point \\(V^{\\pi}\\). A second implication is that the transition and reward functions, and consequently the value functions \\(V^{\\pi}\\) and \\(Q^{\\pi}\\), have fixed-sized inputs and are therefore easy to parameterize, learn, and reuse. Finally, it follows from the Markov property that the optimal policy \\(\\pi\\) can be expressed deterministically and does not require memory [Puterman, 1994].\nWe can unroll the Bellman equation over multiple timesteps to obtain a similar estimator that uses \\(n\\)-step returns: \\(V_n^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_{t:t+n} | S_t = s]\\), where \\(g_{t:t+n} := r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^n V^{\\pi}(s_{t+n})\\), with \\(g_{t:t+n} := g_t\\) if the episode terminates before \\(t + n\\) has been reached. The same process works for weighted combinations of such returns, including the exponential average:\n\\begin{equation}\nV^{\\pi}(s) = \\mathbb{E}_{\\pi}[(1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} g_{t:t+n} | s_t = s],\n\\end{equation}\nwith \\(V_1^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_t | S_t = s]\\) as a special case. Equation (3) defines the TD(\\$\\lambda\\$) value function as an expectation over the so-called \\(\\lambda\\)-return [Sutton, 1988]. Given an MDP and a fixed policy, the recurrence relations for all TD(\\$\\lambda\\$) value functions share the same fixed point for any \\(\\lambda \\in [0, 1]\\). If the Markov property does not hold, different \\(\\lambda\\) may have different TD(\\$\\lambda\\$) fixed points. In this work, we seek to characterize this phenomenon and leverage it for detecting and mitigating partial observability."}, {"title": "3 Detecting Partial Observability", "content": "Before we introduce our partial observability metric, let us first consider the T-maze example of Figure 1 under our two candidate decision-making frameworks: MDPs and POMDPs. In the T-maze"}, {"title": "3.1 Value Function Estimation under Partial Observability", "content": "The Bellman equation and its sample-based recurrence relations (2) and (3) are defined for Markovian states. If we apply them to the observations of a POMDP, we are actually working with the effective MDP model of that POMDP, instead of the POMDP itself. To see this, consider one-step TD (\\(\\lambda = 0\\)), where we use the same recurrence relation (2) but now our expectation is sampling from the POMDP:\n\\begin{equation}\nV^{\\pi, \\lambda=0}(w) = \\sum_{s \\in S} Pr(s | w) \\sum_{a \\in A} \\pi(a | w) (R_s(s, a) + \\gamma \\sum_{s' \\in S} \\sum_{w' \\in \\Omega} \\Phi(w' | s') T_s(s' | a, s) V^{\\pi, \\lambda=0}(w'))\n= \\sum_{a \\in A} \\pi(a | w) (R_{\\Omega}(w, a) + \\gamma \\sum_{w' \\in \\Omega} T_{\\Omega}(w' | a, w) V^{\\pi, \\lambda=0}(w'));\n\\end{equation}"}, {"title": "3.2 A-Discrepancy", "content": "We have shown that, under partial observability, there may be a difference between \\(Q\\) value functions for two different \\(\\lambda\\) parameters due to the implicit Markov assumption in TD(\\$\\lambda\\$). We call this difference the \\(\\lambda\\)-discrepancy, and we propose to use it as a measure of partial observability.\nDefinition 1 For a given POMDP model \\(P\\) and policy \\(\\pi\\), the \\(\\lambda\\)-discrepancy \\(\\Delta_{\\lambda_1, \\lambda_2}^{P, \\pi}\\) is the weighted norm of the difference between the Q-functions estimated by TD(\\$\\lambda\\$) for \\(\\lambda \\in {\\lambda_1, \\lambda_2}\\):\n\\begin{equation}\n\\Delta_{\\lambda_1, \\lambda_2}^{P, \\pi} := ||Q^{\\lambda_1} - Q^{\\lambda_2}|| = ||W (K^{\\lambda_1} - K^{\\lambda_2}): R^{SA}||.\n\\end{equation}\nThe choice of norm can be arbitrary, as can the norm weighting scheme, as long as it assigns positive weight to all reachable observation-action pairs. We discuss choices of weighted norm in Appendix E.2. For brevity, we suppress the \\(P\\) subscript when the POMDP model is clear from context.\nA useful property (that we will prove in Theorem 2) is that if the POMDP has Markovian observations, the \\(\\lambda\\)-discrepancy is exactly zero for all policies. However, for it to be a useful measure of parital observability, we must also show that the \\(\\lambda\\)-discrepancy is reliably non-zero when observations are non-Markovian. For this, we have the following theorem."}, {"title": "3.3 What conditions cause the \\(\\lambda\\)-discrepancy to be zero?", "content": "There are two cases in which the \\(\\lambda\\)-discrepancy is zero for all policies, which we can see by inspecting Definition 1. Because norms are positive definite, it suffices to consider the expression inside the norm. The only ways for this expression to equal zero are either when the difference term \\((K^{\\lambda_1} - K^{\\lambda_2})\\) is zero (which we will show implies Markovian observations), or it is non-zero but is projected away by the outer terms \\(W\\) and/or \\(R^{SA}\\). We first consider when the two inner terms\u2014which are the only terms that depend on \\(\\lambda\\)\u2014are equal, i.e. \\(K^{\\lambda_1} = K^{\\lambda_2}\\).\nTheorem 2 For any POMDP \\(P\\) and any \\(\\lambda_1 \\neq \\lambda_2\\), \\(K^{\\lambda_1} = K^{\\lambda_2}\\) if and only if \\(P\\) is a block MDP.\nNow let us consider the case where the difference between \\(K\\) is projected away by the outer terms \\(W\\) and \\(R^{SA}\\). In Appendix D, we expand Equation (6) as a power series and consider when \\(Q^{\\lambda=0} - Q^{\\lambda=1} = 0\\). It is instructive to let \\(\\lambda = 0\\) and \\(\\lambda' = 1\\), since this simplifies the math and allows us to group terms in the power series by their \\(\\gamma^n\\) coefficients. This leads to the following condition, which, if it holds for all time horizons \\(k\\), ensures \\(\\Delta^{\\lambda_1, \\lambda_2}_{P, \\pi} = 0\\):\n\\begin{equation}\nW (T \\Pi^S)^k: R^{SA} = W (T \\Phi W \\Pi)^k: R^{SA}.\n\\end{equation}\nThere are several ways to satisfy Equation (7). Let us start with the innermost terms and work our way outwards: when \\(\\Pi^S = \\Phi W \\Pi\\), there is no state aliasing and Theorem 2 applies. Assuming Theorem 2 does not apply, there may still be some uninteresting cases where the state transitions \\((T \\Pi^S)\\) and the projected state transitions \\((T \\Phi W \\Pi)\\) are identical (e.g. if the transition probabilities are the same for any states with aliased observations). If the transition probabilities are not identical, then they will also differ when rolled out for \\(k\\) steps. Equation (7) requires that when these \\(k\\)-step rollout dynamics differ, there are no differences in reward prediction at any time horizon, either because of how states are averaged together (\\(W\\)), or because rewards are constant for all states we might confuse. The following example illustrates one way this can occur.\nParity Check Environment. A helpful example for understanding the limitations of the \\(\\lambda\\)-discrepancy is the Parity Check environment of Figure 2. This POMDP has four equally likely starting states, each associated with a pair of colors that the agent will see during the first two timesteps. At the subsequent (white) junction state, the agent must decide based on these colors whether to go"}, {"title": "4 Memory Learning with the \\(\\lambda\\)-Discrepancy", "content": "We have established that the \\(\\lambda\\)-discrepancy can identify that we need memory, but it cannot yet tell us what to remember. For that, we must replace the POMDP \\(P\\) in Definition 1 with a memory-augmented version. In general, an agent's memory can be any mapping from a variable length history \\((w_0, a_0, ..., a_{t-1}, r_{t-1}, w_t)\\) to an internal memory state \\(m\\) within a set of possible memory states \\(M\\). For practical reasons, we restrict our attention to recurrent memory functions that update an internal representation incrementally from fixed-size inputs.\nWe define a memory function \\(\\mu: \\Omega \\times A \\times M \\rightarrow M\\) as a mapping from an observation, action, and memory state, \\((w, a, m)\\), to a next memory state \\(m' = \\mu(w, a, m)\\). Given a POMDP \\(P\\), a memory function \\(\\mu\\) induces a memory-augmented POMDP \\(P^{\\mu}\\), with extended states \\(S_M = S \\times M\\), actions \\(A_M = A \\times M\\), and observations \\(\\Omega_M = \\Omega \\times M\\). The augmented transition dynamics \\(T_M\\) preserve the original transition dynamics \\(T\\) for states \\(S\\) while allowing the agent full observability and control over memory states \\(M\\). Memory functions naturally lead to memory-augmented policies \\(\\pi_{\\mu} : \\Omega_M \\rightarrow \\Delta A_M\\) and value functions \\(V^{\\pi, \\mu} : \\Omega_M \\rightarrow \\mathbb{R}\\) and \\(Q^{\\pi, \\mu} : \\Omega_M \\times A_M \\rightarrow \\mathbb{R}\\) that reflect the expected return under such policies. For details, see Appendix E.1.\nThe \\(\\lambda\\)-discrepancy (Definition 1) applies equally well to memory-augmented POMDPs, and can thus be used to determine whether a particular memory function \\(\\mu\\) induces a POMDP \\(P^{\\mu}\\) with a Markovian observation space \\(\\Omega_M\\). We can also use it as a training objective for learning such a memory function. We conduct a proof-of-concept experiment on several classic POMDPs for which we can obtain closed-form gradients of the \\(\\lambda\\)-discrepancy with respect to a parametrized memory function. In each domain, we randomly generate a set of stochastic policies, select the one with maximal \\(\\lambda\\)-discrepancy \\(\\Lambda^1\\), and adjust the parameters of a memory function \\(\\mu\\) to minimize \\(\\Lambda\\) via gradient descent. Figure 3 shows the improvement in policy gradient performance due to the resulting memory function for increasing memory sizes.\nWe see that the \\(\\lambda\\)-discrepancy can help mitigate partial observability, provided that the agent can optimize it in closed-form. The question that remains is: can we somehow integrate \\(\\lambda\\)-discrepancy minimization into a sample-based learning algorithm and scale this up to more challenging problems?"}, {"title": "5 A Scalable, Online Learning Objective", "content": "So far, we have shown that the \\(\\lambda\\)-discrepancy can detect partial observability in theory and can mitigate it under certain idealized conditions. Now we demonstrate how to integrate our metric into sample-based deep reinforcement learning to solve problems requiring large, complex memory functions."}, {"title": "5.1 Combining the \\(\\lambda\\)-Discrepancy with PPO", "content": "To minimize the \\(\\lambda\\)-discrepancy, we augment a recurrent version of the proximal policy optimization (PPO) algorithm [Schulman et al., 2017] with an auxiliary loss. We use recurrent PPO as our base algorithm due to its strong performance in many POMDPs [Ni et al., 2022b], and since the \\(\\lambda\\)-discrepancy is a natural extension of generalized advantage estimation [Schulman et al., 2015], which is used in PPO. In this algorithm, a recurrent neural network [Amari, 1972] (specifically a gated recurrent unit, or GRU [Cho et al., 2014]) is used as the memory function \\(\\mu\\) that returns a latent state representation given previous latent state and an observation. This latent representation is used as input to an actor network to return a distribution over actions, as well as a critic. The critic is usually a value function network which learns a truncated TD(\\$\\lambda\\$) value estimate for its advantage estimate.\nTo estimate the \\(\\lambda\\)-discrepancy, we learn two TD(\\$\\lambda\\$) value function networks with different \\(\\lambda\\), parameterized by \\(\\theta_{v,1}\\) and \\(\\theta_{v,2}\\) respectively, and minimize their mean squared difference as an auxiliary loss:\n\\begin{equation}\nL_\\Lambda(\\theta) = \\mathbb{E}_{\\pi} [((V_{\\theta_{v,1}}^\\lambda(z_t) - V_{\\theta_{v,2}}^\\lambda(z_t))^2],\n\\end{equation}\nwhere \\(z_t = \\mu_{RNN}(w_t, z_{t-1})\\) is the latent state output by the GRU, and \\(\\theta\\) represents all parameters. We train this neural network end-to-end with the standard PPO actor-critic losses and backpropagation through time [Mozer, 1995]. Note that any algorithm that uses value estimates could potentially leverage the \\(\\lambda\\)-discrepancy. Full details of the algorithm are provided in Appendix G.1."}, {"title": "5.2 Large Partially Observable Environments", "content": "We evaluate our approach on a suite of four hard partially observable environments that require complex memory functions. Battleship [Silver and Veness, 2010] requires reasoning about unknown ship positions and remembering previous shots. Partially observable PacMan [Silver and Veness, 2010] requires localization within the map while tracking dot status and avoiding ghosts with only short-range sensors. RockSample (11, 11) and RockSample (15, 15) [Smith and Simmons, 2004] have stochastic sensors and require remembering which rocks have been sampled. While these environments were originally used to evaluate partially observable planning algorithms in large-scale POMDPS [Silver and Veness, 2010], we use them to test our sample-based learning algorithms due to the complexity of their required memory functions. See Appendix G.2 for more details."}, {"title": "5.3 Experiments", "content": "We show experimental results with regular PPO, recurrent PPO, and our \\(\\lambda\\)-discrepancy-augmented recurrent PPO in Figure 4. Experiments were conducted with a sweep over an extensive set of hyperparameters for both algorithms, and we report learning curves over 30 seeds for the best hyperparameters. Experimental details and hyperparameter sweeps are provided in Appendix G.3.\nThe \\(\\lambda\\)-discrepancy objective leads to significantly better final performance versus recurrent and memoryless PPO in all environments and improves the learning rate for all except one. In RockSample (15, 15), the baseline agents quickly learn to exit immediately (for +10 reward), but never improves on this. By contrast, the \\(\\lambda\\)-discrepancy objective ultimately leads to better performance, as the agent slowly learns the missing features that allow it to express better policies. We also run experiments on the classic POMDPs from Section 4, but due to the size of these problems, both baseline and our proposed approach performed almost optimally. We show these results in Appendix G.4.\nBesides the improvements to performance, the best selected hyperparameters from our sweep support the theory developed in Section 3. The sweep selected two \\(\\lambda\\)s with a large difference between \\(\\lambda_1\\) and \\(\\lambda_2\\), with either of the \\(\\lambda\\)s close to either 0 or 1. This reflects our theory that small or large \\(\\lambda\\)s control the weakness or strength of the model's Markov assumption, and that the difference of the two is a good indicator for partial observability. In addition to this, our hyperparameter sweep also includes a PPO variant with two different TD(\\$\\lambda\\$) value functions but without the \\(\\lambda\\)-discrepancy auxiliary loss described in Section 5.1. These agents were never selected in the sweep, and using the loss in Equation 8 seems to only help with performance in tested environments."}, {"title": "6 Related Work", "content": "There is an interesting connection between state abstraction [Li et al., 2006], which selectively removes information from state, and partial observability mitigation, which aims to recover state from incomplete observations. Allen et al. [2021] investigated the state abstraction perspective and characterized the properties under which abstract state representations of MDPs either do or do not preserve the Markov property. In that setting, the agent has access to the original state space while constructing its abstractions, whereas we assume the state space is unknown. Several other approaches characterize and measure partial observability and POMDPs. While POMDPs have been shown to be computationally intractable in general [Papadimitriou and Tsitsiklis, 1987], various works have studied complexity measures [Zhang et al., 2012] and defined subclasses with tractable solutions [Littman, 1993, Liu et al., 2022].\nThe most common strategies for mitigating partial observability are memory-based approaches that summarize history. Early approaches relied on discrete representations of history, including tree rep- resentations [McCallum, 1996] or finite-state controllers [Meuleau et al., 1999]. Modern approaches mostly use RNNs [Amari, 1972] trained via backpropagation through time (BPTT) [Mozer, 1995] to tackle non-Markovian decision processes [Schmidhuber, 1990]. Various approaches use recurrent function approximation to learn better state representations. One successful approach is learning a re- current value function [Lin and Mitchell, 1993, Bakker, 2001, Hausknecht and Stone, 2015] that uses TD error as a learning signal for memory. Policy gradient methods, including PPO (which we com- pare to), have also been used with recurrent learning to mitigate partial observability [Wierstra et al., 2007, Heess et al., 2015]. Model-based methods can learn a recurrent dynamics model [Hafner et al., 2020] to facilitate planning alongside reinforcement learning. These approaches learn their represen- tations implicitly to improve prediction error, rather than explicitly to mitigate partial observability."}, {"title": "7 Conclusion", "content": "We introduce the \\(\\lambda\\)-discrepancy: an observable and differentiable measure of non-Markovianity suitable for mitigating partial observability. The \\(\\lambda\\)-discrepancy is the difference between two distinct value functions estimated using TD(\\$\\lambda\\), for two different \\(\\lambda\\) values. We characterize the \\(\\lambda\\)-discrepancy and prove that it reliably distinguishes MDPs from POMDPs. We then use it as a memory learning objective and demonstrate that minimizing it in closed-form helps learn useful memory functions in small-scale POMDPs. Finally, we propose a deep reinforcement learning algorithm which leverages the \\(\\lambda\\)-discrepancy as an auxiliary loss, and show that it significantly improves the performance of a baseline recurrent PPO agent on a set of large and challenging partially observable tasks."}, {"title": "A TD(\\$\\lambda\\$) Fixed Point", "content": "Here we derive the fixed point of the TD(\\$\\lambda\\$) action-value update rule in a POMDP, following the Markov version by Sutton [1988]. First, define the expected return given initial observation \\(w_0\\) and initial action \\(a_0\\) as\n\\begin{eqnarray*}\n\\mathbb{E}^{\\pi} (g|w_0, a_0)\n&=& \\sum_{s_0} Pr(s_0|w_0) \\sum_{r_0} Pr(r_0|s_0, a_0)r_0\n+ \\gamma \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1), \\sum_{r_1} Pr(r_1|s_1, a_1)r_1\n+ \\gamma^2 \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} Pr(w_1|s_1) Pr(a_1|w_1) Pr(s_2|s_1, a_1)\n+...\n* \\sum_{w_2} \\sum_{a_2} Pr(w_2|s_2) Pr(a_2|w_2) Pr(r_2|s_2, a_2)r_2\n\\end{eqnarray*}\nWe can define the \\(n\\)-step bootstrapped update rule from this given a value matrix \\(Q\\) by replacing part of the term with coefficient \\(\\gamma^n\\) with a \\(Q^{\\pi}\\) value, e.g. for the \\(n = 2\\) case, we get\n\\begin{eqnarray*}\nQ^{\\pi}_{i+1} (w_0, a_0) &=& \\sum_{s_0} Pr(s_0|w_0) \\sum_{r_0} Pr(r_0|s_0, a_0)r_0\n+ \\gamma \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1)\n* \\sum_{r_1} Pr(r_1|s_1, a_1)r_1\n+ \\gamma^2 \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1)\n* \\sum_{s_2} Pr(s_2|s_1, a_1) \\sum_{w_2} \\sum_{a_2} Pr(w_2|s_2) Pr(a_2|w_2)Q^{(i)} (w_2, a_2).\n\\end{eqnarray*}\nTranslating these expressions into matrix notation, we have\n\\begin{eqnarray*}\n\\mathbb{E}^{\\pi} (g|w_0, a_0) &=& \\sum_{s_0} W_{w_0, s_0} R_{s_0, a_0}\n+ \\gamma \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} R_{s_1, a_1}\n+ \\gamma^2 \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} T_{s_1, a_1, s_2}\n+...,\n*\\sum_{w_2} \\sum_{a_2} \\Phi_{s_2, w_2} \\pi_{w_2, a_2} R_{s_2, a_2}\n\\end{eqnarray*}\nwhere the terms \\(W\\), \\(T\\), \\(R\\), and \\(I\\) are as in Equation 6, and \\(\\pi\\) is the \\(\\Omega \\rightarrow \\Delta A\\) policy written as an \\(\\Omega \\times A\\) tensor with entries in \\([0, 1]\\). In particular, \\(W_{w,s} = Pr(s|w)\\), which averages \\(Pr(s_t|w_t)\\) over all timesteps, weighted by visitation probability and discounted by \\(\\gamma\\). This is a well-defined stationary quantity, and it can be computed as follows. First solve the system \\(Cx = b\\) to find the discounted state occupancy counts \\(x = c(s)\\), where \\(b = p_0\\) is the initial state distribution over \\(S\\), and \\(C = (I - \\gamma (T^\\pi))\\) accounts for the policy-dependent state-to-state transition dynamics \\(T^\\pi\\) defined by \\(T^\\pi_{s,s'} = \\sum_w \\sum_a Pr(w|s) Pr(a|w) Pr(s'|s, a)\\). Then \\(Pr(s|w) \\propto c(s)*\\Phi(w|s)\\), so we can just multiply these terms together and renormalize. For the 2-step bootstrapped update rule above, we have:\n\\begin{eqnarray*}\nQ^{\\pi}_{i+1} (w_0, a_0) &=& \\sum_{s_0} W_{w_0, s_0} R_{s_0, a_0}\n+ \\gamma \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} R_{s_1, a_1}\n+ \\gamma^2 \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} T_{s_1, a_1, s_2}\n* \\sum_{w_2} \\sum_{a_2} \\Phi_{s_2, w_2} \\sum_{a_2} \\pi_{w_2, a_2} Q^{(i)} (w_2, a_2).\n\\end{eqnarray*}"}, {"title": "B Proof of Theorem 1 (Almost All)", "content": "In this section we prove Theorem 1", "follows": "n\\begin{eqnarray*"}, "n\\Delta_{\\lambda_1, \\lambda_2}^{P, \\pi} := ||Q^{\\lambda_1} - Q^{\\```json\n{\n  \"title\": \"Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy\",\n  \"authors\": [\n    \"Cameron Allen\",\n    \"Aaron Kirtland\",\n    \"Ruo Yu Tao\",\n    \"Sam Lobel\",\n    \"Daniel Scott\",\n    \"Nicholas Petrocelli\",\n    \"Omer Gottesman\",\n    \"Ronald Parr\",\n    \"Michael L. Littman\",\n    \"George Konidaris\"\n  ],\n  \"abstract\":", "Reinforcement learning algorithms typically rely on the assumption that the environment dynamics and value function can be expressed in terms of a Markovian state representation. However, when state information is only partially observable, how can an agent learn such a state representation, and how can it detect when it has found one? We introduce a metric that can accomplish both objectives, without requiring access to or knowledge of an underlying, unobservable state space. Our metric, the \\(\\lambda\\)-discrepancy, is the difference between two distinct temporal difference (TD) value estimates, each computed using TD(\\$\\lambda\\$) with a different value of \\(\\lambda\\). Since TD(\\$\\lambda=0\\$) makes an implicit Markov assumption and TD(\\$\\lambda=1\\$) does not, a discrepancy between these estimates is a potential indicator of a non-Markovian state representation. Indeed, we prove that the \\(\\lambda\\)-discrepancy is exactly zero for all Markov decision processes and almost always non-zero for a broad class of partially observable environments. We also demonstrate empirically that, once detected, minimizing the \\(\\lambda\\)-discrepancy can help with learning a memory function to mitigate the corresponding partial observability. We then train a reinforcement learning agent that simultaneously constructs two recurrent value networks with different \\(\\lambda\\) parameters and minimizes the difference between them as an auxiliary loss. The approach scales to challenging partially observable domains, where the resulting agent frequently performs significantly better (and never performs worse) than a baseline recurrent agent with only a single value network.", "n  \"sections\": [\n    {\n      \"title\": \"1 Introduction\",\n      \"content\":", "The dominant modeling frameworks for reinforcement learning [Sutton and Barto, 2018] define environments in terms of an underlying Markovian state representation. This modeling choice, called the Markov assumption, is nearly ubiquitous in reinforcement learning, because it allows environment dynamics, rewards, value functions, and policies all to be expressed as functions that are independent of the past given the most recent state. In principle, an environment can be modeled as either a Markov decision process (MDP) [Puterman, 1994], or its partially observable counterpart, a POMDP", "n    },\n    {\n      \"title\": \"2 Background\",\n      \"content\":", "We consider two frameworks for modeling sequential decision processes: MDPs and POMDPs. The MDP framework [Puterman, 1994] consists of a state space S, action space A, reward function \\(R : S \\times A \\rightarrow \\mathbb{R}\\), transition function \\(T : S \\times A \\rightarrow \\Delta S\\) mapping to a distribution over states, discount factor \\(\\gamma\\in [0, 1]\\), and initial state distribution \\(p_0 \\in \\Delta S\\). The agent's goal is to find a policy \\(\\pi : S \\rightarrow \\Delta A\\) that selects actions to maximize return, \\(g_t\\), the discounted sum of future rewards starting from timestep \\(t\\): \\(g_t = \\sum_{i=0}^{\\infty} \\gamma^i r_{t+i}\\), where \\(r_i\\) is the reward at timestep \\(i\\). We denote the expectation of these returns as value functions \\(V^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_t | S_t = s]\\) and \\(Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[g_t | S_t = s, a_t = a]\\).\nThe POMDP framework [Kaelbling et al., 1998] additionally includes a set of observations \\(\\Omega\\) and an observation function \\(\\Phi : S \\rightarrow \\Delta \\Omega\\) that describes the probability \\(\\Phi(w|s)\\) of seeing observation \\(w\\) in latent state \\(s\\). POMDPs are a more general model of the world, since they contain MDPs as a special case: namely, when observations have a one-to-one correspondence with states. Similarly, POMDPS where states correspond to disjoint sets of observations are called block MDPs [Du et al., 2019]. However, such examples are rare; in typical POMDPs, a single observation \\(w\\) does not contain enough information to fully resolve the state \\(s\\). While agents need not fully resolve the underlying state to behave optimally, they must retain at least enough information across timesteps that the optimal policy becomes clear.\\\nWe are interested in the learning setting, where the agent has no knowledge of the underlying state \\(s\\) nor even the set of possible states \\(S\\) (let alone the transition, reward, and observation functions). It receives an observation \\(w_t \\in \\Omega\\) at each timestep and must find a way to maximize expected return. One way to do this is to construct a state representation, perhaps using some form of memory, on which it can condition its behavior. A state representation \\(z \\in Z\\) is Markovian if at any timestep \\(t\\), the representation \\(z_t\\) and action \\(a_t\\) together are a sufficient statistic for predicting the reward \\(r_t\\) and next representation \\(z_{t+1}\\), instead of requiring the agent's whole history:\n\\begin{equation}\nPr(z_{t+1}, r_t | z_t, a_t) = Pr(z_{t+1}, r_t | z_t, a_t, ..., z_0, a_0).\n\\end{equation}\nStates and observations are equivalent in MDPs, and this property is satisfied for both by definition, but in POMDPs it typically only holds for the underlying, unobserved state \\(s\\)\u2014not the observations.\\\nMarkovian state representations have several desirable implications. First, if the Markov property holds then so does the Bellman equation: \\(V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a | s)(R(s, a) + \\gamma \\sum_{s' \\in S} T(s' | s, a) V^{\\pi}(s'))\\). The Bellman equation allows agents to estimate the value of a policy, from experiences, and without knowing \\(T\\) or \\(R\\), via a recurrence relation over one-step returns,\n\\begin{equation}\nV_{i+1}^{\\pi}(s) = \\mathbb{E}_{\\pi}[r_t + \\gamma V_i^{\\pi}(s_{t+1}) | S_t = s],\n\\end{equation}\nwhich converges to the unique fixed point \\(V^{\\pi}\\). A second implication is that the transition and reward functions, and consequently the value functions \\(V^{\\pi}\\) and \\(Q^{\\pi}\\), have fixed-sized inputs and are therefore easy to parameterize, learn, and reuse. Finally, it follows from the Markov property that the optimal policy \\(\\pi\\) can be expressed deterministically and does not require memory [Puterman, 1994].\nWe can unroll the Bellman equation over multiple timesteps to obtain a similar estimator that uses \\(n\\)-step returns: \\(V_n^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_{t:t+n} | S_t = s]\\), where \\(g_{t:t+n} := r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^n V^{\\pi}(s_{t+n})\\), with \\(g_{t:t+n} := g_t\\) if the episode terminates before \\(t + n\\) has been reached. The same process works for weighted combinations of such returns, including the exponential average:\n\\begin{equation}\nV^{\\pi}(s) = \\mathbb{E}_{\\pi}[(1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} g_{t:t+n} | s_t = s],\n\\end{equation}\nwith \\(V_1^{\\pi}(s) = \\mathbb{E}_{\\pi}[g_t | S_t = s]\\) as a special case. Equation (3) defines the TD(\\$\\lambda\\$) value function as an expectation over the so-called \\(\\lambda\\)-return [Sutton, 1988]. Given an MDP and a fixed policy, the recurrence relations for all TD(\\$\\lambda\\$) value functions share the same fixed point for any \\(\\lambda \\in [0, 1]\\). If the Markov property does not hold, different \\(\\lambda\\) may have different TD(\\$\\lambda\\$) fixed points. In this work, we seek to characterize this phenomenon and leverage it for detecting and mitigating partial observability.", "n    },\n    {\n      \"title\": \"3 Detecting Partial Observability\",\n      \"content\":", "Before we introduce our partial observability metric, let us first consider the T-maze example of Figure 1 under our two candidate decision-making frameworks: MDPs and POMDPs. In the T-maze", "n    },\n    {\n      \"title\": \"3.1 Value Function Estimation under Partial Observability\",\n      \"content\":", "The Bellman equation and its sample-based recurrence relations (2) and (3) are defined for Markovian states. If we apply them to the observations of a POMDP, we are actually working with the effective MDP model of that POMDP, instead of the POMDP itself. To see this, consider one-step TD (\\(\\lambda = 0\\)), where we use the same recurrence relation (2) but now our expectation is sampling from the POMDP:\n\\begin{equation}\nV^{\\pi, \\lambda=0}(w) = \\sum_{s \\in S} Pr(s | w) \\sum_{a \\in A} \\pi(a | w) (R_s(s, a) + \\gamma \\sum_{s' \\in S} \\sum_{w' \\in \\Omega} \\Phi(w' | s') T_s(s' | a, s) V^{\\pi, \\lambda=0}(w'))\n= \\sum_{a \\in A} \\pi(a | w) (R_{\\Omega}(w, a) + \\gamma \\sum_{w' \\in \\Omega} T_{\\Omega}(w' | a, w) V^{\\pi, \\lambda=0}(w'));\n\\end{equation}", "n    },\n    {\n      \"title\": \"3.2 A-Discrepancy\",\n      \"content\":", "We have shown that, under partial observability, there may be a difference between \\(Q\\) value functions for two different \\(\\lambda\\) parameters due to the implicit Markov assumption in TD(\\$\\lambda\\$). We call this difference the \\(\\lambda\\)-discrepancy, and we propose to use it as a measure of partial observability.\nDefinition 1 For a given POMDP model \\(P\\) and policy \\(\\pi\\), the \\(\\lambda\\)-discrepancy \\(\\Delta_{\\lambda_1, \\lambda_2}^{P, \\pi}\\) is the weighted norm of the difference between the Q-functions estimated by TD(\\$\\lambda\\$) for \\(\\lambda \\in {\\lambda_1, \\lambda_2}\\):\n\\begin{equation}\n\\Delta_{\\lambda_1, \\lambda_2}^{P, \\pi} := ||Q^{\\lambda_1} - Q^{\\lambda_2}|| = ||W (K^{\\lambda_1} - K^{\\lambda_2}): R^{SA}||.\n\\end{equation}\nThe choice of norm can be arbitrary, as can the norm weighting scheme, as long as it assigns positive weight to all reachable observation-action pairs. We discuss choices of weighted norm in Appendix E.2. For brevity, we suppress the \\(P\\) subscript when the POMDP model is clear from context.\nA useful property (that we will prove in Theorem 2) is that if the POMDP has Markovian observations, the \\(\\lambda\\)-discrepancy is exactly zero for all policies. However, for it to be a useful measure of parital observability, we must also show that the \\(\\lambda\\)-discrepancy is reliably non-zero when observations are non-Markovian. For this, we have the following theorem.", "n    },\n    {\n      \"title\": \"3.3 What conditions cause the \\(\\lambda\\)-discrepancy to be zero?\",\n      \"content\":", "There are two cases in which the \\(\\lambda\\)-discrepancy is zero for all policies, which we can see by inspecting Definition 1. Because norms are positive definite, it suffices to consider the expression inside the norm. The only ways for this expression to equal zero are either when the difference term \\((K^{\\lambda_1} - K^{\\lambda_2})\\) is zero (which we will show implies Markovian observations), or it is non-zero but is projected away by the outer terms \\(W\\) and/or \\(R^{SA}\\). We first consider when the two inner terms\u2014which are the only terms that depend on \\(\\lambda\\)\u2014are equal, i.e. \\(K^{\\lambda_1} = K^{\\lambda_2}\\).\nTheorem 2 For any POMDP \\(P\\) and any \\(\\lambda_1 \\neq \\lambda_2\\), \\(K^{\\lambda_1} = K^{\\lambda_2}\\) if and only if \\(P\\) is a block MDP.\nNow let us consider the case where the difference between \\(K\\) is projected away by the outer terms \\(W\\) and \\(R^{SA}\\). In Appendix D, we expand Equation (6) as a power series and consider when \\(Q^{\\lambda=0} - Q^{\\lambda=1} = 0\\). It is instructive to let \\(\\lambda = 0\\) and \\(\\lambda' = 1\\), since this simplifies the math and allows us to group terms in the power series by their \\(\\gamma^n\\) coefficients. This leads to the following condition, which, if it holds for all time horizons \\(k\\), ensures \\(\\Delta^{\\lambda_1, \\lambda_2}_{P, \\pi} = 0\\):\n\\begin{equation}\nW (T \\Pi^S)^k: R^{SA} = W (T \\Phi W \\Pi)^k: R^{SA}.\n\\end{equation}\nThere are several ways to satisfy Equation (7). Let us start with the innermost terms and work our way outwards: when \\(\\Pi^S = \\Phi W \\Pi\\), there is no state aliasing and Theorem 2 applies. Assuming Theorem 2 does not apply, there may still be some uninteresting cases where the state transitions \\((T \\Pi^S)\\) and the projected state transitions \\((T \\Phi W \\Pi)\\) are identical (e.g. if the transition probabilities are the same for any states with aliased observations). If the transition probabilities are not identical, then they will also differ when rolled out for \\(k\\) steps. Equation (7) requires that when these \\(k\\)-step rollout dynamics differ, there are no differences in reward prediction at any time horizon, either because of how states are averaged together (\\(W\\)), or because rewards are constant for all states we might confuse. The following example illustrates one way this can occur.\nParity Check Environment. A helpful example for understanding the limitations of the \\(\\lambda\\)-discrepancy is the Parity Check environment of Figure 2. This POMDP has four equally likely starting states, each associated with a pair of colors that the agent will see during the first two timesteps. At the subsequent (white) junction state, the agent must decide based on these colors whether to go", "n    },\n    {\n      \"title\": \"4 Memory Learning with the \\(\\lambda\\)-Discrepancy\",\n      \"content\":", "We have established that the \\(\\lambda\\)-discrepancy can identify that we need memory, but it cannot yet tell us what to remember. For that, we must replace the POMDP \\(P\\) in Definition 1 with a memory-augmented version. In general, an agent's memory can be any mapping from a variable length history \\((w_0, a_0, ..., a_{t-1}, r_{t-1}, w_t)\\) to an internal memory state \\(m\\) within a set of possible memory states \\(M\\). For practical reasons, we restrict our attention to recurrent memory functions that update an internal representation incrementally from fixed-size inputs.\nWe define a memory function \\(\\mu: \\Omega \\times A \\times M \\rightarrow M\\) as a mapping from an observation, action, and memory state, \\((w, a, m)\\), to a next memory state \\(m' = \\mu(w, a, m)\\). Given a POMDP \\(P\\), a memory function \\(\\mu\\) induces a memory-augmented POMDP \\(P^{\\mu}\\), with extended states \\(S_M = S \\times M\\), actions \\(A_M = A \\times M\\), and observations \\(\\Omega_M = \\Omega \\times M\\). The augmented transition dynamics \\(T_M\\) preserve the original transition dynamics \\(T\\) for states \\(S\\) while allowing the agent full observability and control over memory states \\(M\\). Memory functions naturally lead to memory-augmented policies \\(\\pi_{\\mu} : \\Omega_M \\rightarrow \\Delta A_M\\) and value functions \\(V^{\\pi, \\mu} : \\Omega_M \\rightarrow \\mathbb{R}\\) and \\(Q^{\\pi, \\mu} : \\Omega_M \\times A_M \\rightarrow \\mathbb{R}\\) that reflect the expected return under such policies. For details, see Appendix E.1.\\\nThe \\(\\lambda\\)-discrepancy (Definition 1) applies equally well to memory-augmented POMDPs, and can thus be used to determine whether a particular memory function \\(\\mu\\) induces a POMDP \\(P^{\\mu}\\) with a Markovian observation space \\(\\Omega_M\\). We can also use it as a training objective for learning such a memory function. We conduct a proof-of-concept experiment on several classic POMDPs for which we can obtain closed-form gradients of the \\(\\lambda\\)-discrepancy with respect to a parametrized memory function. In each domain, we randomly generate a set of stochastic policies, select the one with maximal \\(\\lambda\\)-discrepancy \\(\\Lambda^1\\), and adjust the parameters of a memory function \\(\\mu\\) to minimize \\(\\Lambda\\) via gradient descent. Figure 3 shows the improvement in policy gradient performance due to the resulting memory function for increasing memory sizes.\\\nWe see that the \\(\\lambda\\)-discrepancy can help mitigate partial observability, provided that the agent can optimize it in closed-form. The question that remains is: can we somehow integrate \\(\\lambda\\)-discrepancy minimization into a sample-based learning algorithm and scale this up to more challenging problems?", "n    },\n    {\n      \"title\":", 5, "A Scalable, Online Learning Objective", "n      \"content\":", "So far, we have shown that the \\(\\lambda\\)-discrepancy can detect partial observability in theory and can mitigate it under certain idealized conditions. Now we demonstrate how to integrate our metric into sample-based deep reinforcement learning to solve problems requiring large, complex memory functions.", "n    },\n    {\n      \"title\": \"5.1 Combining the \\(\\lambda\\)-Discrepancy with PPO\",\n      \"content\":", "To minimize the \\(\\lambda\\)-discrepancy, we augment a recurrent version of the proximal policy optimization (PPO) algorithm [Schulman et al., 2017] with an auxiliary loss. We use recurrent PPO as our base algorithm due to its strong performance in many POMDPs [Ni et al., 2022b], and since the \\(\\lambda\\)-discrepancy is a natural extension of generalized advantage estimation [Schulman et al., 2015], which is used in PPO. In this algorithm, a recurrent neural network [Amari, 1972] (specifically a gated recurrent unit, or GRU [Cho et al., 2014]) is used as the memory function \\(\\mu\\) that returns a latent state representation given previous latent state and an observation. This latent representation is used as input to an actor network to return a distribution over actions, as well as a critic. The critic is usually a value function network which learns a truncated TD(\\$\\lambda\\$) value estimate for its advantage estimate.\\\nTo estimate the \\(\\lambda\\)-discrepancy, we learn two TD(\\$\\lambda\\$) value function networks with different \\(\\lambda\\), parameterized by \\(\\theta_{v,1}\\) and \\(\\theta_{v,2}\\) respectively, and minimize their mean squared difference as an auxiliary loss:\n\\begin{equation}\nL_\\Lambda(\\theta) = \\mathbb{E}_{\\pi} [((V_{\\theta_{v,1}}^\\lambda(z_t) - V_{\\theta_{v,2}}^\\lambda(z_t))^2],\n\\end{equation}\nwhere \\(z_t = \\mu_{RNN}(w_t, z_{t-1})\\) is the latent state output by the GRU, and \\(\\theta\\) represents all parameters. We train this neural network end-to-end with the standard PPO actor-critic losses and backpropagation through time [Mozer, 1995]. Note that any algorithm that uses value estimates could potentially leverage the \\(\\lambda\\)-discrepancy. Full details of the algorithm are provided in Appendix G.1.", "n    },\n    {\n      \"title\": \"5.2 Large Partially Observable Environments\",\n      \"content\":", "We evaluate our approach on a suite of four hard partially observable environments that require complex memory functions. Battleship [Silver and Veness, 2010] requires reasoning about unknown ship positions and remembering previous shots. Partially observable PacMan [Silver and Veness, 2010] requires localization within the map while tracking dot status and avoiding ghosts with only short-range sensors. RockSample (11, 11) and RockSample (15, 15) [Smith and Simmons, 2004] have stochastic sensors and require remembering which rocks have been sampled. While these environments were originally used to evaluate partially observable planning algorithms in large-scale POMDPS [Silver and Veness, 2010], we use them to test our sample-based learning algorithms due to the complexity of their required memory functions. See Appendix G.2 for more details.", "n    },\n    {\n      \"title\": \"5.3 Experiments\",\n      \"content\":", "We show experimental results with regular PPO, recurrent PPO, and our \\(\\lambda\\)-discrepancy-augmented recurrent PPO in Figure 4. Experiments were conducted with a sweep over an extensive set of hyperparameters for both algorithms, and we report learning curves over 30 seeds for the best hyperparameters. Experimental details and hyperparameter sweeps are provided in Appendix G.3.\\\nThe \\(\\lambda\\)-discrepancy objective leads to significantly better final performance versus recurrent and memoryless PPO in all environments and improves the learning rate for all except one. In RockSample (15, 15), the baseline agents quickly learn to exit immediately (for +10 reward), but never improves on this. By contrast, the \\(\\lambda\\)-discrepancy objective ultimately leads to better performance, as the agent slowly learns the missing features that allow it to express better policies. We also run experiments on the classic POMDPs from Section 4, but due to the size of these problems, both baseline and our proposed approach performed almost optimally. We show these results in Appendix G.4.\\\nBesides the improvements to performance, the best selected hyperparameters from our sweep support the theory developed in Section 3. The sweep selected two \\(\\lambda\\)s with a large difference between \\(\\lambda_1\\) and \\(\\lambda_2\\), with either of the \\(\\lambda\\)s close to either 0 or 1. This reflects our theory that small or large \\(\\lambda\\)s control the weakness or strength of the model's Markov assumption, and that the difference of the two is a good indicator for partial observability. In addition to this, our hyperparameter sweep also includes a PPO variant with two different TD(\\$\\lambda\\$) value functions but without the \\(\\lambda\\)-discrepancy auxiliary loss described in Section 5.1. These agents were never selected in the sweep, and using the loss in Equation 8 seems to only help with performance in tested environments.", "n    },\n    {\n      \"title\": \"6 Related Work\",\n      \"content\":", "There is an interesting connection between state abstraction [Li et al., 2006], which selectively removes information from state, and partial observability mitigation, which aims to recover state from incomplete observations. Allen et al. [2021] investigated the state abstraction perspective and characterized the properties under which abstract state representations of MDPs either do or do not preserve the Markov property. In that setting, the agent has access to the original state space while constructing its abstractions, whereas we assume the state space is unknown. Several other approaches characterize and measure partial observability and POMDPs. While POMDPs have been shown to be computationally intractable in general [Papadimitriou and Tsitsiklis, 1987], various works have studied complexity measures [Zhang et al., 2012] and defined subclasses with tractable solutions [Littman, 1993, Liu et al., 2022].\nThe most common strategies for mitigating partial observability are memory-based approaches that summarize history. Early approaches relied on discrete representations of history, including tree rep- resentations [McCallum, 1996] or finite-state controllers [Meuleau et al., 1999]. Modern approaches mostly use RNNs [Amari, 1972] trained via backpropagation through time (BPTT) [Mozer, 1995] to tackle non-Markovian decision processes [Schmidhuber, 1990]. Various approaches use recurrent function approximation to learn better state representations. One successful approach is learning a re- current value function [Lin and Mitchell, 1993, Bakker, 2001, Hausknecht and Stone, 2015] that uses TD error as a learning signal for memory. Policy gradient methods, including PPO (which we com- pare to), have also been used with recurrent learning to mitigate partial observability [Wierstra et al., 2007, Heess et al., 2015]. Model-based methods can learn a recurrent dynamics model [Hafner et al., 2020] to facilitate planning alongside reinforcement learning. These approaches learn their represen- tations implicitly to improve prediction error, rather than explicitly to mitigate partial observability.", "n    },\n    {\n      \"title\": \"7 Conclusion\",\n      \"content\":", "We introduce the \\(\\lambda\\)-discrepancy: an observable and differentiable measure of non-Markovianity suitable for mitigating partial observability. The \\(\\lambda\\)-discrepancy is the difference between two distinct value functions estimated using TD(\\$\\lambda\\), for two different \\(\\lambda\\) values. We characterize the \\(\\lambda\\)-discrepancy and prove that it reliably distinguishes MDPs from POMDPs. We then use it as a memory learning objective and demonstrate that minimizing it in closed-form helps learn useful memory functions in small-scale POMDPs. Finally, we propose a deep reinforcement learning algorithm which leverages the \\(\\lambda\\)-discrepancy as an auxiliary loss, and show that it significantly improves the performance of a baseline recurrent PPO agent on a set of large and challenging partially observable tasks.", "n    },\n    {\n      \"title\": \"A TD(\\$\\lambda\\$) Fixed Point\",\n      \"content\":", "Here we derive the fixed point of the TD(\\$\\lambda\\$) action-value update rule in a POMDP, following the Markov version by Sutton [1988]. First, define the expected return given initial observation \\(w_0\\) and initial action \\(a_0\\) as\n\\begin{eqnarray*}\n\\mathbb{E}^{\\pi} (g|w_0, a_0)\n&=& \\sum_{s_0} Pr(s_0|w_0) \\sum_{r_0} Pr(r_0|s_0, a_0)r_0\n+ \\gamma \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1), \\sum_{r_1} Pr(r_1|s_1, a_1)r_1\n+ \\gamma^2 \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} Pr(w_1|s_1) Pr(a_1|w_1) Pr(s_2|s_1, a_1)\n+...\n* \\sum_{w_2} \\sum_{a_2} Pr(w_2|s_2) Pr(a_2|w_2) Pr(r_2|s_2, a_2)r_2\n\\end{eqnarray*}\nWe can define the \\(n\\)-step bootstrapped update rule from this given a value matrix \\(Q\\) by replacing part of the term with coefficient \\(\\gamma^n\\) with a \\(Q^{\\pi}\\) value, e.g. for the \\(n = 2\\) case, we get\n\\begin{eqnarray*}\nQ^{\\pi}_{i+1} (w_0, a_0) &=& \\sum_{s_0} Pr(s_0|w_0) \\sum_{r_0} Pr(r_0|s_0, a_0)r_0\n+ \\gamma \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1)\n* \\sum_{r_1} Pr(r_1|s_1, a_1)r_1\n+ \\gamma^2 \\sum_{s_0} Pr(s_0|w_0) \\sum_{s_1} Pr(s_1|s_0, a_0) \\sum_{w_1} \\sum_{a_1} Pr(w_1|s_1) Pr(a_1|w_1)\n* \\sum_{s_2} Pr(s_2|s_1, a_1) \\sum_{w_2} \\sum_{a_2} Pr(w_2|s_2) Pr(a_2|w_2)Q^{(i)} (w_2, a_2).\n\\end{eqnarray*}\nTranslating these expressions into matrix notation, we have\n\\begin{eqnarray*}\n\\mathbb{E}^{\\pi} (g|w_0, a_0) &=& \\sum_{s_0} W_{w_0, s_0} R_{s_0, a_0}\n+ \\gamma \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} R_{s_1, a_1}\n+ \\gamma^2 \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} T_{s_1, a_1, s_2}\n+...,\n*\\sum_{w_2} \\sum_{a_2} \\Phi_{s_2, w_2} \\pi_{w_2, a_2} R_{s_2, a_2}\n\\end{eqnarray*}\nwhere the terms \\(W\\), \\(T\\), \\(R\\), and \\(I\\) are as in Equation 6, and \\(\\pi\\) is the \\(\\Omega \\rightarrow \\Delta A\\) policy written as an \\(\\Omega \\times A\\) tensor with entries in \\([0, 1]\\). In particular, \\(W_{w,s} = Pr(s|w)\\), which averages \\(Pr(s_t|w_t)\\) over all timesteps, weighted by visitation probability and discounted by \\(\\gamma\\). This is a well-defined stationary quantity, and it can be computed as follows. First solve the system \\(Cx = b\\) to find the discounted state occupancy counts \\(x = c(s)\\), where \\(b = p_0\\) is the initial state distribution over \\(S\\), and \\(C = (I - \\gamma (T^\\pi))\\) accounts for the policy-dependent state-to-state transition dynamics \\(T^\\pi\\) defined by \\(T^\\pi_{s,s'} = \\sum_w \\sum_a Pr(w|s) Pr(a|w) Pr(s'|s, a)\\). Then \\(Pr(s|w) \\propto c(s)*\\Phi(w|s)\\), so we can just multiply these terms together and renormalize. For the 2-step bootstrapped update rule above, we have:\n\\begin{eqnarray*}\nQ^{\\pi}_{i+1} (w_0, a_0) &=& \\sum_{s_0} W_{w_0, s_0} R_{s_0, a_0}\n+ \\gamma \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} R_{s_1, a_1}\n+ \\gamma^2 \\sum_{s_0} W_{w_0, s_0} \\sum_{s_1} T_{s_0, a_0, s_1} \\sum_{w_1} \\sum_{a_1} \\sum_{s_2} \\Phi_{s_1, w_1} \\pi_{w_1, a_1} T_{s_1, a_1, s_2}\n* \\sum_{w_2} \\sum_{a_2} \\Phi_{s_2, w_2} \\sum_{a_2} \\pi_{w_2, a_2} Q^{(i)} (w_2, a_2).\n\\end{eqnarray*}", "n    },\n    {\n      \"title\": \"B Proof of Theorem 1 (Almost All)\",\n      \"content\":", "In this section we prove Theorem 1, that there is either a \\(\\lambda\\)-discrepancy for almost all policies or for no policies. Fix \\(\\lambda\\) and \\(\\lambda'\\). Recall that we define the \\(\\lambda\\)-dis"]}