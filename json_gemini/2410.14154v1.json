{"title": "RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training", "authors": ["Muhe Ding", "Yang Ma", "Pengda Qin", "Jianlong Wu", "Yuhong Li", "Liqiang Nie"], "abstract": "Multimodal Large Language Models (MLLMs) have recently received substantial interest, which shows their emerging potential as general-purpose models for various vision-language tasks. MLLMs involve significant external knowledge within their parameters; however, it is challenging to continually update these models with the latest knowledge, which involves huge computational costs and poor interpretability. Retrieval augmentation techniques have proven to be effective plugins for both LLMs and MLLMs. In this study, we propose multimodal adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training (RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering the redundant information within vision modality, we first leverage the question to instruct the extraction of visual information through interactions with one set of learnable queries, minimizing irrelevant interference during retrieval and generation. Besides, we introduce a pre-trained multimodal adaptive fusion module to achieve question text-to-multimodal retrieval and integration of multimodal knowledge by projecting visual and language modalities into a unified semantic space. Furthermore, we present an Adaptive Selection Knowledge Generation (ASKG) strategy to train the generator to autonomously discern the relevance of retrieved knowledge, which realizes excellent denoising performance. Extensive experiments on open multimodal question-answering datasets demonstrate that RA-BLIP achieves significant performance and surpasses the state-of-the-art retrieval-augmented models.", "sections": [{"title": "I. INTRODUCTION", "content": "HE birth of the Internet has triggered an unprecedented information revolution, catapulting humanity into the era of information explosion. It is a great challenge to efficiently find answers from a vast amount of information based on our questions. Open Multimodal Multihop Question Answer-ing (MMQA) [1]\u2013[7] can help alleviate this problem of in-formation overload by retrieving external knowledge based on questions and generating correct answers. In recent years, sev-eral advanced LLMs and MLLMs like FlanT5 [8], LLaMA [9],"}, {"title": "II. RELATED WORK", "content": "Vision-language pre-training (VLP) aims to train models on large-scale image-text datasets to capture the relationship between these two modalities. Broadly, VLP methodologies fall into two categories based on their training approach: 1) End-to-end Methods: This category includes methods [35]\u2013[38] that train models end-to-end, backpropagating learned sig-nals to achieve mutual learning between different modalities. 2) Modular Methods: In contrast, modular methods, as seen in works by [39]\u2013[43], involve keeping the parameters of specific pre-trained components (like image encoders or large language models) fixed while focusing on refining other aspects of the model. For instance, LiT [44] utilizes a pre-trained frozen image encoder from CLIP, while Flamingo [45] and BLIP-2 [10] freeze the language model to integrate LLMs into vision-language tasks better. Besides, instruction tuning is also an effective approach during VLP. InstructBLIP [30] represents a recent advancement in this area, achieving instruction-aware visual feature extraction and instruction-guided LLM genera-tion. The unique capability of InstructBLIP to extract features based on prompt instructions, combined with its utilization of frozen LLMs and image encoders, positions it as an ideal backbone for our proposed retrieval-augmented framework."}, {"title": "A. Vision-Language Pretraining", "content": "Vision-language pre-training (VLP) aims to train models on large-scale image-text datasets to capture the relationship between these two modalities. Broadly, VLP methodologies fall into two categories based on their training approach: 1) End-to-end Methods: This category includes methods [35]\u2013[38] that train models end-to-end, backpropagating learned sig-nals to achieve mutual learning between different modalities. 2) Modular Methods: In contrast, modular methods, as seen in works by [39]\u2013[43], involve keeping the parameters of specific pre-trained components (like image encoders or large language models) fixed while focusing on refining other aspects of the model. For instance, LiT [44] utilizes a pre-trained frozen image encoder from CLIP, while Flamingo [45] and BLIP-2 [10] freeze the language model to integrate LLMs into vision-language tasks better. Besides, instruction tuning is also an effective approach during VLP. InstructBLIP [30] represents a recent advancement in this area, achieving instruction-aware visual feature extraction and instruction-guided LLM genera-tion. The unique capability of InstructBLIP to extract features based on prompt instructions, combined with its utilization of frozen LLMs and image encoders, positions it as an ideal backbone for our proposed retrieval-augmented framework."}, {"title": "B. Text-modality Retrieval-Augmented Models", "content": "Retrieval-augmented techniques have proven to be ef-fective plugins for both LLMs and MLLMs in academia. These techniques extract pertinent world knowledge from"}, {"title": "C. Multimodal Retrieval-Augmented Models", "content": "To overcome the limitations of text-modality retrieval-augmented models, recent research [10], [15], [30], [46], [47] has made strides in integrating multimodal knowledge. No-table efforts, including AutoRouting [5] and MAE [6], involve training distinct models for each modality and using classifiers for task-specific routing, though this approach often hampers cross-modal reasoning. MuRAG [14] seeks to overcome this limitation by employing separate encoders for visual and textual modalities, followed by a joint encoder for multimodal fusion. However, this approach lacks integrated guidance for different modalities and cannot model the relations between knowledge sources during retrieval. SKURG [20] attempts to bridge this gap by using an entity-centered fusion encoder to align modalities, yet faces challenges in computational efficiency and limited interpretability. Besides, Solar [21] transforms multimodal inputs into a unified language format but falls short in handling complex tasks and generalizing visual information. REVAL [16] leverages large-scale knowl-edge graphs to assist visual language pre-training, but it brings a lot of calculations. In contrast, RA-BLIP distinguishes itself by seamlessly integrating visual and language modalities into a cohesive semantic space, enabling the autonomous selection of relevant knowledge for reasoning, thus addressing these limitations more effectively."}, {"title": "III. METHODOLOGY", "content": "In this section, we first formulate the research problem and subsequently elaborate on the model architecture of our retrieval-augmented framework. Then, we describe the learn-able query interaction approach, followed by multimodal adap-tive fusion module. Subsequently, we show how to train the retriever and rank the relevant knowledge. Lastly, we introduce the adaptive selection knowledge generation strategy."}, {"title": "A. Problem Formulation", "content": "This paper presents a multimodal adaptive retrieval-augmented framework called RA-BLIP for open multihop and multimodal QA, integrating retrieval and generation functions. For knowledge-intensive QA, we deconstruct the task into two stages: retrieval and generation, which are implemented by the retriever and generator respectively. The goal of our model training is to learn the distribution $P(y|x_q)$ to generate a textual output y conditioned on input question $x_q$ and multimodal knowledge base KB (KB = $k_1,...,k_n$). Firstly, the retriever encodes questions, images, and texts from the knowledge base KB. It identifies the most relevant retrieved knowledge, $K_{ret}$ C KB ($K_{ret}$ is the retrieved knowledge) for each question $x_q$, which is modeled as $p(K_{ret}|x_q)$. Secondly, the generator utilizes an LLM to generate answers y, condi-tioned on both the question and the retrieved knowledge, which is modeled as $p(y|x_q, K_{ret})$. We treat multimodal knowledge $K_{ret}$ as a latent variable from the external knowledge base and marginalize it to increase the overall likelihood of the answer y. The overall process is encapsulated in the equation:\n\n$p(y | x_q) = \\sum_{K_{ret} CKB} p (K_{ret} | x_q) p(y | x_q, K_{ret}).$\n\nThis dual-stage framework effectively addresses the complex-ities of open multimodal QA by balancing the retrieval of multimodal data and knowledge-based generation, and has been validated by extensive experiments and ablation studies."}, {"title": "B. Model Architecture", "content": "RA-BLIP is built on a simple backbone model that is pre-trained to encode image-text pairs so that they are suitable for both knowledge base retrieval and answer generation. The overall framework of RA-BLIP is shown in Fig. 2. The backbone model consists of a multimodal encoder $f_o()$ and decoder $g_e()$, which are used as components of the RA-BLIP model to implement retrieval and generation. The multimodal encoder $f_o()$ contains a frozen image encoder ViT [48], Q-Former architecture [30], and the pre-trained multimodal adaptive fusion module. The decoder $g_e()$ is composed of a LLM FlanT5 [8]. Querying Transformer (Q-Former) [10] is a lightweight Transformer consisting of two modules that share the same self-attention layer: one is an image transformer that interacts with the frozen image encoder ViT for visual feature extraction, and the other is a text transformer can act as both text encoder and text decoder for text feature. The visual encoder, composed of ViT and Q-Former image transformer, has instruction-aware visual feature extraction capabilities and can extract visual information based on question instructions. We input N learnable query embeddings into the Q-Former image transformer, which interacts with frozen image features through cross-attention layers to obtain visual representation $f_o(I) \\in R^{N\\times D}$, where D is the hidden dimension of the Q-Former. Additionally, we use the Q-Former text transformer to encode text, taking the [CLS] token as the text representation $f_o(T) \\in R^{1\\times D}$. To obtain multimodal features combining both image and text, we introduce a pre-trained multimodal adaptive fusion module $M_e()$ to obtain the multimodal representation"}, {"title": "C. Learnable Query Interaction for Multi-images", "content": "Questions and image captions are used as instructions to extract visual features to get learnable query embeddings and input them together with text knowledge to LLMs for generation. We employ a novel approach for extracting visual information to alleviate the burden of LLMs in distinguishing knowledge. In the original Q-Former in [30], multiple images are processed by employing multiple separate sets of queries for each, with each set of queries independently extracting visual features. This results in using multiple query features for generation, which can be computationally intensive and less efficient in capturing the interrelations among different images. In contrast, our method innovates by succinctly utilizing one set of learnable queries to directly interact with and extract features from multiple images in a unified manner. This pro-cess occurs during the Q-Former stage, enabling more efficient and integrated interaction among multiple visual references. By employing one set of query interaction approach, RA-BLIP not only simplifies the feature extraction process but also enhances the efficiency of information extraction. This unified interaction allows the model to understand better and represent the collective information presented in multiple im-ages, enabling more effective and cohesive feature utilization, especially when dealing with complex scenes or subjects across multiple images."}, {"title": "D. Multimodal Adaptive Fusion Module", "content": "The pre-trained multimodal adaptive fusion module $M_e()$ consists of a 3-layer BERT network [31], [32]. Since the Q-Former has aligned visual and text feature representation, we use Image-Text Matching loss and Image-grounded Text Generation loss for pre-training. As shown in Fig. 3, our approach is to fix the parameters of the image encoder and Q-Former, and solely fine-tune the parameters of the multi-modal adaptive fusion module. The module concatenates the visual embedding and text embedding with a dimension of $R^{(N+L)\\times D}$, where N is the learnable query embeddings and L is the length of text tokens. Image-text matching loss is used to fuse image and text representations, and the results of the fusion module are fed into a binary linear classifier for each output query to obtain the logits and take the average logits of all queries as the matching score. Given a pre-training dataset $X = {I_i, T_i}_{i=1}^n$, we randomly sample negative texts for each image and randomly sample negative images for each text, to generate negative training data. Therefore, we denote the ground truth label as y \u2208 {1,0} for each image-text pair $(I_i, T_i)$, indicating if the input image-text pair is relevant or not. We use the multimodal encoder $f_e(\\cdot)$ to encode image-text pairs and input it into the multimodal adaptive fusion module $M_o(\\cdot)$. The objective function is defined as follows:\n\n$L_{itm} = -\\frac{1}{n}\\sum_{I_i, T_i \\in X} y\\log (p (M_o (f_o(I_i); f_o (T_i)))),$\n\nwhere p(.) is the softmax function. Image-grounded Text Gen-eration loss trains the fusion module to generate texts, given input images as the condition [10], [30]. For the image-text pairs in the pre-training dataset, each image I corresponds to a text sentence $y_{1:T} = {y_1, \u2026\u2026\u2026, y_T}$ of length T. We employ a multimodal causal self-attention mask for multimodal encoder $f_o()$ and multimodal adaptive fusion module $M_e()$ to control the interaction between queries and text. The visual query functions as a prefix causal, ensuring that queries can attend to each other while excluding text tokens. Similarly, each text token y can attend to all visual queries and preceding text tokens. The loss function is defined as:\n\n$L_{itg} = - \\sum_{t=1}^T\\log M_o(f_o((y_t | y_{<t}, I)).$"}, {"title": "E. Retrieval-Augmented Retriever Training", "content": "During the retrieval stage, the retriever utilizes the question Xq to retrieve relevant knowledge from multimodal knowledge base KB. To achieve this, we apply the multimodal encoder fo(), which encodes the question xq along with all latent multimodal knowledge into an embedding space to identify the Top-K most relevant candidates, illustrated in Fig. 2 retrieval stage. We use contrastive learning to construct pos-itive and negative samples for training. The knowledge type consists primarily of three components: image k\u00b9, text k\u00b9, and image-text kIT. Thus, the 1-th example in in the dataset is represented as (x1, y1, {k, k, k{T}, {kj, k,k; }), where ki is the i-th positive (image, text, image-text) sample and kj represents j-th negative (image, text, image-text) sample. For a batch of knowledge examples, we gather all associ-ated positive and negative knowledge sources into a batch KB = {{k,,T}, {k,k,},...,,,T}B}. The multimodal encoder is responsible for encoding the mul-timodal feature representations and aligning the questions and knowledge within the unified semantic space. This alignment facilitates identifying the proximity between a question and its corresponding knowledge through contrastive learning. The objective function is defined as follows:\n\n$L_{con} = -log \\frac{exp(f_o(x_q)\\cdot f_o(k^I; k^T; k^{IT}))}{\\sum exp(f_o(x_q) f_o(k^I; k^T ; k^{IT}))},$\n\nTopK(Kret | xq) = TopK{fo(xq) \u00b7 fo(k\u00b9;kT;kIT)}.\n\nAlthough the retriever is more efficient for many retrieval tasks, its accuracy is lower on open multimodal question answering. There are instances where certain knowledge is confusing and bears token-wise similarity to the question"}, {"title": "F. Adaptive Selection Knowledge Generation", "content": "During the generation stage, the retrieved multimodal knowledge is combined with the question xq as an augmented input [k1, ..., k1, xq], which is fed to the multimodal encoder and LLMs [8] to produce multimodal representation encoding and generate answers. We observe that existing methods [14], [21] directly rely on retrieval results without distinguishing the correctness of the retrieved knowledge, potentially leading to the utilization of incorrect, confusing, or irrelevant infor-mation. To address this, we propose an adaptive selection knowledge generation (ASKG) strategy based on a question-and-answer formulation, shown in Fig. 2 generation stage. ASKG strategy enables the generator to go beyond mere word similarity between the question and the retrieved knowledge, allowing it to grasp the semantic information of the question and identify which piece of knowledge contains the answer. Specifically, we manually construct question-and-answer data to enable the model to discriminate the relevance of multi-modal knowledge, thereby utilizing the implicit capabilities of LLMs for knowledge filtering. Based on the original dataset, we select relevant knowledge as positive examples and irrel-evant knowledge as negative examples. We create an ASKG enhanced dataset and combine the knowledge according to templates, with the identifier of the positive examples serving as the answer. The template for the question Iq is: \"We would like to request your feedback on ranking the questions according to their relevance to the references below. Relevance refers to the degree to which the reference can answer the question. The input format is Question: [content], Reference [knowledge ID]: [content]. The output format is: Related content is [knowledge ID].\u201d, and the answer y is in the form of \u201cThe most relevant reference is Reference [knowledge ID].\u201d. We refer to the above enhanced dataset of questions and answers as I and \u1ef9 = {1, ...,\u04ef\u043c}, where M is the text"}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets: We evaluate our method on three QA datasets: WebQA [4], MultimodalQA [5], and MMCoQA [6]. The details of these datasets are showcased in Table I.\n\u2022 WebQA [4] is a large-scale dataset for multimodal and multihop QA where all questions are knowledge-seeking queries that require two or more knowledge sources. Evaluation metrics are retrieval F1 and QA for assessing answer generation quality, which is measured as both fluency (QA-FL) and accuracy (QA-ACC). We calculate fluency through BARTScore [50] and evaluate accuracy via F1 and recall. The fluency score and accuracy score are multiplied FL * Acc to calculate the overall score.\n\u2022 MultimodalQA [5] is a collection of multihop QA pairs that necessitate the fusion of knowledge from text, tables, and images. This dataset requires retrieval and reasoning"}, {"title": "A. Experimental Settings", "content": "1) Datasets: We evaluate our method on three QA datasets: WebQA [4], MultimodalQA [5], and MMCoQA [6]. The details of these datasets are showcased in Table I.\n\u2022 WebQA [4] is a large-scale dataset for multimodal and multihop QA where all questions are knowledge-seeking queries that require two or more knowledge sources. Evaluation metrics are retrieval F1 and QA for assessing answer generation quality, which is measured as both fluency (QA-FL) and accuracy (QA-ACC). We calculate fluency through BARTScore [50] and evaluate accuracy via F1 and recall. The fluency score and accuracy score are multiplied FL * Acc to calculate the overall score.\n\u2022 MultimodalQA [5] is a collection of multihop QA pairs that necessitate the fusion of knowledge from text, tables, and images. This dataset requires retrieval and reasoning"}, {"title": "B. Main Results", "content": "Results on WebQA. We show the WebQA results in Table III. We can see that RA-BLIP surpasses all baselines in terms of both QA and retrieval F1 scores. RA-BLIP (FlanT5xl) achieves 45.8% accuracy, which is +4.9% higher than the state-of-the-art Solar [21]. Especially the metric QA-Acc is +6.4% higher, proving the model's powerful generation abil-ity. Besides, RA-BLIP (FlanT5xxl) beats SOTA Solar by 7.6% on overall QA accuracy, which shows that RA-BLIP complies with scaling law [51] and can improve the generation accuracy by using more advanced LLM. In order to prove that it is our RA-BLIP framework rather than the advanced MLLM back-bone that improves the generative performance, we conducted experiments on InstructBLIP [30] on WebQA. We used RA-BLIP's optimal 0.89 search result for InstructBLIP generation and found that its accuracy was 5.9% lower than Solar, which further proves the effectiveness of RA-BLIP framework and ASKG. RA-BLIP (T5-base) and (T5-large) are not pre-trained to align with Q-Former, but they achieve 41.6% and 42.5% accuracy respectively based on 0.89 search results, surpassing Solar and proving it is the RA-BLIP framework rather than LLM that improves performance. Compared with Solar and SKURG which require additional model assistance, RA-BLIP does not use additional models, but it also achieves very good results in retrieval and is 14% higher than MuRAG, which similarly does not use additional models.\nResults on MultimodalQA. We demonstrate Multi-modalQA results in Table IV. MultimodalQA contains tables and has many multihop questions that require combining multimodal information. RA-BLIP also improved EM and F1 by 6.0% and 6.6%, respectively, compared to state-of-the-art Solar, which demonstrates the generative ability of our method in incorporating multihop knowledge. In addition,"}, {"title": "C. Ablation Study", "content": "To analyze the effectiveness of our proposed method, we conducted comprehensive ablations on the WebQA dataset in both retrieval and generation stages. As shown in Fig. 4 (a), RA-BLIP (FlanT5xl) and RA-BLIP (FlanT5xxl) with ASKG strategy improved by 1.9% and 2.7% respectively, which"}, {"title": "D. Sensitivity Analysis", "content": "The a is the trade-off hyperparameter of generation loss with ASKG strategy in Eq. (7). We set the range of a from 0.01 to 5. According to Fig. 5, we can see that even for such a large range, the difference between the best and the lowest results is less than 0.04%, indicating our method is robust and insensitive to this parameter."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel multimodal adaptive Retrieval-Augmented BLIP (RA-BLIP), a general retrieval-augmented framework for various classical MLLMs. RA-BLIP utilizes questions as instructions to extract visual features for less irrelevant interference. It incorporates a pre-trained multimodal adaptive fusion module to efficiently integrate information from both visual and textual modalities, thereby achieving question text-to-multimodal retrieval. Additionally, we introduce an adaptive selection knowledge generation strat-egy to make the generator autonomously discern the relevance of retrieved knowledge. Extensive experiments on multimodal multihop QA and ablation studies verify the effectiveness of RA-BLIP. In the future, we will explore image-multimodal retrieval and multimodal-multimodal retrieval to realize om-nipotent retrieval-augmented models."}]}