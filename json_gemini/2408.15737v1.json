{"title": "TCNFormer: Temporal Convolutional Network Former for Short-Term Wind Speed Forecasting", "authors": ["Abid Hasan Zim", "Aquib Iqbal", "Asad Malik", "Zhicheng Dong", "Hanzhou Wu"], "abstract": "Global environmental challenges and rising energy demands have led to extensive exploration of wind energy technologies. Accurate wind speed forecasting (WSF) is crucial for optimizing wind energy capture and ensuring system stability. However, predicting wind speed remains challenging due to its inherent randomness, fluctuation, and unpredictability. This study proposes the Temporal Convolutional Network Former (TCNFormer) for short-term (12-hour) wind speed forecasting. The TCNFormer integrates the Temporal Convolutional Network (TCN) and transformer encoder to capture the spatio-temporal features of wind speed. The transformer encoder consists of two distinct attention mechanisms: causal temporal multi-head self-attention (CT-MSA) and temporal external attention (TEA). CT-MSA ensures that the output of a step derives only from previous steps, i.e., causality. Locality is also introduced to improve efficiency. TEA explores potential relationships between different sample sequences in wind speed data. This study utilizes wind speed data from the NASA Prediction of Worldwide Energy Resources (NASA POWER) of Patenga Sea Beach, Chittagong, Bangladesh (latitude 22.2352\u00b0 N, longitude 91.7914\u00b0 E) over a year (six seasons). The findings indicate that the TCNFormer outperforms state-of-the-art models in prediction accuracy. The proposed TCNFormer presents a promising method for spatio-temporal WSF and may achieve desirable performance in real-world applications of wind power systems.", "sections": [{"title": "Introduction", "content": "The rising energy demand driven by economic growth and improved living standards, combined with finite fossil fuel reserves, poses risks of depletion and environmental harm. This highlights the urgent need for renewable energy, with wind energy emerging as a clean, abundant, and widely recognized alternative (de Mattos Neto et al. 2021; Wang et al. 2016). Global wind power capacity surged from 60 GW in 2000 to 460 GW in 2015, with annual growth rates in electricity generation fluctuating between 20% and 35% during this period (Afrasiabi et al. 2020).By 2020, global wind power capacity reached 743 GW, marking a 93 GW increase from 2019, according to the GWEC. The GWEC also forecasts that wind energy will contribute more than 20% of the total worldwide power output by 2030 (Wang et al. 2021; Lv and Wang 2022).\nWith a population of 166 million, expected to grow to 189 million by 2041, Bangladesh is focusing on industrialization to achieve developed nation status. This requires a reliable power supply, with electricity demand projected to rise from 19,034 MW in 2021 to 82,292 MW by 2041 (Babu, Nei, and Kowser 2022). In a bid to enhance energy security and sustainability, the Bangladeshi government has invested significantly in renewable energy sources. Bangladesh has significant wind energy potential, with speeds of 5.75 to 7.75 m/s over 20,000 square kilometers, capable of generating up to 30,000 MW of wind energy (Debnath et al. 2023). However, wind energy is inherently intermittent, posing significant challenges to the stability and safety of large-scale grid-integrated wind power systems due to its high variability and inconsistency. Furthermore, wind energy is susceptible to transmission and distribution losses. Consequently, integrating renewable energy sources such as solar and wind into grids of varying capacities requires meticulous scheduling, management, and optimization (Wang et al. 2018; Sabzehgar, Amirhosseini, and Rasouli 2020). The Texas Electricity Reliability Board has noted that biases in wind power might lead to operational challenges and significant economic losses. Therefore, accurate and precise prediction of wind speed is crucial for ensuring the stability of the power supply and addressing the challenges related to integrating wind energy (Zha et al. 2016; Lv and Wang 2022).\nConventional time series techniques depend on historical patterns, but machine learning frameworks make predictions by learning features of the underlying data trends (Zhang et al. 2022; Qiu et al. 2017). Deep learning has recently made significant advances in time series forecasting techniques, including recurrent neural networks (RNNs) (Schuster and Paliwal 1997), convolutional neural networks (CNNs) (Koprinska, Wu, and Wang 2018), and graph neural networks (GNNs) (Chen, Segovia, and Gel 2021). Deep learning models are very effective in handling time series data that is highly dimensional, irregular, and dynamic. Their capacity to autonomously extract properties enhances the precision of predictions (Salinas et al. 2020; Liu et al. 2024). Recently, transformer-based methods have demonstrated significant potential in time series forecasting (Liu et al. 2022; Cirstea et al. 2022; Nie et al. 2022). Nevertheless, these approaches mainly rely on positional encoding to preserve temporal information, which may lead to the loss of temporal patterns (Zeng et al. 2023). In this study, TCNFormer is proposed for short-term (12-hour) WSF. The proposed model integrates the strengths of TCN and transformer encoder. Key features of the TCN architecture include causal convolutions, which prevent information leakage from future to past samples, and the capability to process input sequences of any length while maintaining the same output sequence length. This ensures that the tensor shape is preserved throughout the convolution process (Bednarski et al. 2022; Lin et al. 2019). Additionally, the transformer encoder in the proposed model incorporates two types of attention mechanisms: CT-MSA and TEA, making it particularly suitable for wind speed forecasting. The main contributions of this study are outlined as follows:"}, {"title": "Related Works", "content": "In the domain of time series forecasting, RNNs based models (Sarp et al. 2022; Yu et al. 2018) such as Long Short-Term Memory (LSTM) (Yan et al. 2022), Bidirectional LSTM (BILSTM) (Joseph et al. 2023), Gated Recurrent Unit (GRU) (Li et al. 2019), and Bidirectional GRU (Bi-GRU) (Liu et al. 2021a) have been extensively employed in literature for wind power related works. Nevertheless, training RNNs presents specific challenges, such as the vanishing gradient issue (Huang et al. 2024). Additionally, various attention mechanisms have been integrated with RNN-based models to further enhance their performance (Zhang et al. 2023b; Suleman and Shridevi 2022; Niu et al. 2020). GNNS have been applied in the field of time series prediction (Verdone, Scardapane, and Panella 2022; Ma et al. 2023). However, these forecasting methods often inadequately leverage the spatio-temporal context inherent in wind conditions, potentially limiting their effectiveness in accurately predicting wind speed (Wu and Ling 2024).\nBuilding upon the concept of the attention mechanism, the transformer model represents an innovative approach to sequence transduction, specifically targeting applications in NLP. The transformer distinguishes itself from preceding models by eschewing the use of recurrence or convolution, thereby enhancing its capability to learn long-term dependencies (Zhang and Yan 2023; Zim et al. 2022a). Nevertheless, due to the quadratic scaling of complexity with respect to sequence length, several modifications to the original transformer architecture have been proposed to mitigate computational challenges and optimize the model for time series forecasting. These transformer-based methodologies typically involve the processing of input sequences through an embedding module, where positional encodings are employed to preserve data order by incorporating location or specific date information. Subsequently, various self-attention techniques are used to identify and investigate connections across the input sequence. The decoder subsequently produces the prediction sequence in one pass. Those models have shown promising outcomes in time series forecasting, but certain challenges associated with those models remain unresolved. Those models heavily rely on supplementary positional encodings to preserve the sequential order of attention scores. Nevertheless, the integration of positional encodings can sometimes hinder the effective capture of temporal patterns, resulting in suboptimal predictive performance. Furthermore, the autoregressive nature of the transformer's decoder, which generates sequences sequentially, can lead to slow inference speeds and the accumulation of errors, particularly in the context of WSF (Huang et al. 2024). To address the challenges of wind speed forecasting, we propose the TCNFormer model, which integrates TCN with a transformer encoder for enhanced forecasting performance. The Transformer encoder incorporates two distinct attention mechanisms: CT-MSA and TEA. Moreover, TCNFormer simplifies the decoder layers in the original design by replacing them with dense layers. This change aims to enhance the speed of inference and allow the development of sequences in a single step, so effectively mitigating the problem of accumulating errors."}, {"title": "Method", "content": "In this section, we present the different components of our proposed architecture. The TCNFormer integrates the TCN and transformer encoder to capture the spatio-temporal features."}, {"title": "Temporal Convolutional Network (TCN)", "content": "TCN represent a class of CNN specifically designed for sequence modeling tasks under causal constraints (Bai, Kolter, and Koltun 2018). A TCN is structured with several stacked residual blocks. Each residual block incorporates two layers of dilated causal convolution with the ReLU employed as the activation function. Additionally, batch normalization is applied to the convolutional filters, and dropout is used as a regularization method. Moreover, an optional 1 \u00d7 1 convolution can be incorporated to ensure that the tensors involved in the residual connections maintain the same shape.\nDilated causal convolutions expand the receptive field exponentially, enabling the incorporation of a more extensive range of historical information. This technique combines causal convolution with dilated convolution, leveraging their unique properties to capture a broader temporal context in sequential data. The specialized structure of dilated causal convolutions allows for an enhanced horizon of sequential features, effectively preserving historical information during data processing. The dilated convolution operation F applied to an element s in the 1-D sequence $x \\in R$, using a filter $f: \\{0,1,..., k - 1\\} \\rightarrow R$, can be expressed as follows:\n$F(s) = (x * df) (s) = \\sum_{i=1}^{k-1} f(i) \\cdot X_{s-d.i}$    (1)\nIn this context, k refers to the filter size, d signifies the dilation factor, and * represents the convolution operation. illustrates a dilated causal convolution with dilation factors of d = 1,2,4 and a filter size of k = 3 (Li et al. 2022)."}, {"title": "Causal temporal multi-head self-attention (CT-MSA)", "content": "Multi-head self-attention (MSA) serves as a fundamental mechanism in transformers, enabling each token within a sequence to align and effectively capture information from other tokens (Vaswani 2017). Given an input sequence $X \\in R^{N\\times C}$, where N represents the input sequence length and C denotes the feature dimension, the operation of a single attention head is defined as follows:\n$X_h = Softmax (\\frac{Q_hK_h^T}{\\alpha}) V_h$,   (2)\nHere, $X_h \\in R^{S \\times S}$ represents the output features, where $N_h$ is the number of heads; \u03b1 is introduced as a scaling factor. The queries, keys, and values are denoted as $Q_h = XW_q$, $K_h = XW_k$, and $V_h = XW_v$, respectively, with $W_q, W_k, and W_v \\in R^{C\\times {\\frac{S}{Nh}}}$ being the learnable parameters for linear projection. The computational complexity of Equation 2 scales quadratically with the sequence length S. In this research, we utilized CT-MSA as an effective and powerful alternative to traditional MSA for temporal modeling (Liang et al. 2023). The overall structure of CT-MSA closely resembles that of standard MSA, but it introduces two critical modifications to better capture temporal dynamics and leverage domain knowledge in time series modeling. First, local window to address the typically stronger correlations between adjacent time steps compared to distant ones, CT-MSA applies MSA within non-overlapping windows, thereby capturing local interactions among time steps. This approach results in a computational cost of O(TWC), which is T/W times lower than that of standard MSA, where W denotes the window size. To maintain the extensive receptive field characteristic of standard MSA, the window size is progressively increased at different stages. Second temporal causality, we ensure that the wind speed at any given time step is not influenced by future values. This is achieved using a causal approach similar to that used in WaveNet (Wu et al. 2019). Temporal causality is incorporated into the MSA mechanism by masking certain entries in the attention map, which preserves the temporal sequence of the input data."}, {"title": "Temporal external attention (TEA)", "content": "The architecture of the self-attention mechanism is designed to compute self-correlations within individual sequences, neglecting the potential interactions between different sequences. This limitation reduces the capacity and flexibility of self-attention, as it fails to capture correlations of different sequences that are essential for a more comprehensive understanding and modeling of the overall data structure (Huang and Liu 2024). To address this challenge, we integrated TEA into our transformer encoder (Huang et al. 2024).\nIn TEA, instead of defining the query matrix Q separately, it is directly equated to the input feature map F. To replace the traditional key (K) and value (V) matrices used in self-attention, external memory units $M \\in R^{S\\times L}$ are introduced. This attention mechanism computes the relationship between the input sequence and external memory units M as follows:\n$\\hat{a}_{i,j} = FM^T$,\n$A = \\alpha \\cdot \\hat{a}_{i,j}$,\n$a_{i,j} = \\frac{exp(\\hat{a}_{i,j})}{\\Sigma_k exp(\\hat{a}_{k,j})}$,    (3)\n$Attn = AM\u03c5$,\nIn this context, $M_r$ and $M_u$ are independent, trainable parameter matrices that are not directly associated with the input data. Instead, they function as a form of memory for the entire dataset. The notation $\\hat{a}_{i,j}$ represents the similarity measure between the i-th feature of the input F and the j-th column of M. Given a dataset with N training samples, starting with the first sample $S_1$ and ending with the last sample $S_N$, we begin by randomly initializing two external memory units, $M$ and $M^\\prime$. The process of updating the parameters of these external memory units can be described as follows:\n$M^\\prime_i = M^\\prime_{i-1} - \\eta \\frac{\\partial L(S_i)}{\\partial M^\\prime_{i-1}}$\n$M_i = M_{i-1} - \\eta \\frac{\\partial L(S_i)}{\\partial M_{i-1}}$,   (4)\nThe term $\\partial L(S_i)$ represents the training loss experienced by the model on the sample $S_i$. The parameters $M_i$ and $M^\\prime_i$ correspond to the external memory units that are updated as a result of the parameter adjustments based on the sample $S_i$. These memory units are designed to capture not only the internal feature relationships within the sample $S_i$ but also to identify and learn potential relationships among the set of samples $\\{S_1,..., S_i\\}$. In a similar vein, the final memory units $M$ and $M_N$ encapsulate both the intrinsic information of individual samples and the latent relationships that exist across different samples throughout the entire dataset."}, {"title": "Transformer Encoder", "content": "The encoder is composed of a series of N encoder layers, each containing two distinct sub-layers. In the first sub-layer, CT-MSA is applied, followed by dropout and layer normalization (LN). Subsequently, a residual connection is established by adding the original input, which is processed through a Conv1D layer. In the second sub-layer, a Conv1D layer with ReLU activation is employed. This is followed by the TEA mechanism, dropout, and LN. The output from the TEA mechanism undergoes an additional Conv1D layer and LN before being integrated with the original residual connection. Here, Figure 2(b) shows the structure of the transformer encoder.\n$X^{en}_1 = LN (CT-MSA (X^{en}_{l-1}) + X^{en}_{l-1}),$\n$X^{en}_2 = LN (TEA (X^{en}_1) + X^{en}_1)$.   (5)\nThe output of the first sub-layer of the encoder is denoted by $X^{en}_1$, while $X^{en}_2$ represents the output of the second sub-"}, {"title": "Experiments and analysis", "content": "In this study, hourly wind speed data (at 10 meters) was collected for short-term (12-hour) WSF at Patenga Sea Beach in Chittagong, Bangladesh (latitude 22.2352\u00b0 N and longitude 91.7914\u00b0 E). The dataset utilized in this research was sourced from the NASA POWER database (NASA Langley Research Center (LaRC) 2024). The data spans approximately one year (2021 -2022), covering six seasons: Summer (mid-April to mid-June), Rainy (mid-June to mid-August), Autumn (mid-August to mid-October), Late Autumn (mid-October to mid-December), Winter (mid-December to mid-February), and Spring (mid-February to mid-April). For forecasting purposes, data from an entire season (e.g., summer) was used for training, with the last 12 hours being predicted. An overview of the dataset is provided in Table 1. The location map is shown in Figure 6. At the Patenga sea beach, wind speed values ranged from a maximum of over 7.5 m/s to a minimum of approximately 2.8 m/s, with an average wind speed of about 5.15 m/s. The theoretical power output of the wind at this site, at a height of 1.62 m, is estimated to be around 298 W/m\u00b2 of wind area (Madlool et al. 2021). The dataset was scaled to fall within a predefined range of 0 to 1. In this study, the MinMaxS-caler was employed for this purpose (Zim et al. 2022b). MinMaxScaler standardization, as represented by:\n$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$   (6)\nWhere, $X_{min}$ = minimum value in X feature\n$X_{max}$ = maximum value in X feature"}, {"title": "Evaluation Metric", "content": "In this study, the performance of the models was evaluated using mean absolute error (MAE) and mean squared error (MSE), which are defined by the following equations:\n$MAE = \\frac{1}{n}\\sum_{i=0}^{n} |\\hat{y}_i - y_i|$   (7)\n$MSE = \\frac{1}{n}\\sum_{i=0}^{n} (\\hat{y}_i - y_i)^2$   (8)\nwhere n is the total number of samples, \u0177r the model predictions, and yi is the actual values."}, {"title": "Model Comparison", "content": "We conducted comparative experiments to evaluate our proposed TCNFormer model for short-term wind speed forecasting (12-hour horizon). TCNFormer was tested against benchmark models, including Autoformer (Wu et al. 2021), Pyraformer (Liu et al. 2021b), vanilla Transformer (Dosovitskiy et al. 2020), LSTM (Sutskever, Vinyals, and Le 2014), and TCN (Bai, Kolter, and Koltun 2018). As shown in Table 2 and Figure 5, TCNFormer consistently outperformed all other models across six seasons, demonstrating superior performance.\nFor the summer season, the proposed TCNFormer model demonstrated significant improvements in predictive accuracy across various benchmarks. Specifically, in terms of MAE, the TCNFormer outperformed the Autoformer, Pyraformer, Transformer, LSTM, and TCN by 128.29%, 156.88%, 140.93%, 58.12%, and 24.34% respectively.\nDuring the rainy season, the TCNFormer model continued to deliver exceptional performance, surpassing the Autoformer, Pyraformer, Transformer, LSTM, and TCN models by 191.42%, 193.28%, 186.02%, 117.46%, and 149.51% respectively in terms of MSE.\nSimilarly, during the autumn season, the TCNFormer model demonstrated superior efficacy, outperforming the Autoformer, Pyraformer, Transformer, LSTM, and TCN models by 158.04%, 137.72%, 138.14%, 77.55%, and 83.12% respectively in terms of MAE.\nIn the late autumn season, as evidenced by preceding analyses, the TCNFormer model consistently demonstrated superior performance, surpassing the Autoformer, Pyraformer, Transformer, LSTM, and TCN models by 175.26%, 191.89%, 180.65%, 117.24%, and 166.67% respectively in terms of MSE.\nA similar trend was observed during both the winter and full-season forecasts, where the TCNFormer model consistently outperformed the Autoformer, Pyraformer, Transformer, LSTM, and TCN models. Notably, the TCNFormer showed significant improvements in MAE, with respective increases of 127.36%, 126.85%, 111.73%, 63.20%, and 70.49%.\nA similar trend was observed during the spring season and in full-season forecasting, where the TCNFormer model consistently outperformed the Autoformer, Pyraformer, Transformer, LSTM, and TCN models. Specifically, TCNFormer achieved superior results with improvements of 178.22%, 178.64%, 174.71%, 90.00%, and 162.71%, respectively, in terms of MSE. These findings further affirm the TCNFormer's exceptional capability in delivering accurate forecasts across different seasonal contexts.\nThese findings indicate that the TCNFormer model is more accurate in predicting short-term (12-hour) WSF. TCN and LSTM models have exhibited superior efficacy in comparison to Transformer-based models, such as Autoformer, Pyraformer, and the vanilla Transformer, in the context of short-term (12-hour) WSF. Transformer-based models tend to be more suitable for long-term forecasting, particularly when handling extensive datasets, as they are more data-intensive due to their inherent inductive biases compared to conventional deep learning models (Lu et al. 2022). Furthermore, transformer models are dependent on positional encoding to preserve temporal features, which may result in a disruption of features (Huang and Liu 2024). And when it comes to RNN-based models they are susceptible to the vanishing gradient problem (Ribeiro et al. 2020). TCN models offer certain advantages. Notably, the convolutions in their architecture are causal, ensuring no information leakage from future to past samples. Moreover, TCN can process sequences of any length and map them to output sequences of the same length, maintaining consistent tensor shapes throughout the convolutional process (Zhu, Liao, and Wang 2020). Despite these benefits, TCN have limitations, such as their tendency to overlook interactions between internal states, their limited capacity to thoroughly investigate the inherent information and connections inside the data, as well as their insufficiency in resolving the correlation issue among data points. These constraints may impede the precision of forecast outcomes (Zhang et al. 2023a). The innovative structure of TCNFormer enables it to effectively capture spatio-temporal features in wind speed data. And also offers superior computational efficiency compared to traditional CNN, RNN, and transformer-based models. This suggests that utilizing TCNFormer for time series prediction holds significant potential for further enhancing wind power prediction accuracy."}, {"title": "Ablation Study", "content": "This section explores the impact of CT-MSA and TEA on our model's performance. To demonstrate the robustness of the proposed model, we substitute CT-MSA and TEA in the transformer encoder with standard MSA. The summer season dataset was used as an illustrative case. The results are illustrated in Figure 7."}, {"title": "Effects of CT-MSA", "content": "To assess the efficacy of CT-MSA, we remove CT-MSA from our TCNFormer and replace it with standard MSA, while keeping TEA unchanged, as depicted in the TCN-Former architecture. As demonstrated in Figure 7, the CT-MSA significantly outperforms the standard MSA. Notably, the incorporation of causality and local windows into the MSA consistently enhances performance across all future time steps."}, {"title": "Effects of TEA", "content": "To assess the effectiveness of TEA, the TEA was removed from our TCNFormer model and replaced with standard MSA. Notably, the CT-MSA component remained unchanged throughout the experiment, maintaining consistency with the original TCNFormer architecture. As illustrated in Figure 7, the model performs poorly without the TEA. One possible reason for this is the reliance on a uniform attention mechanism within the transformer encoder. Combining CT-MSA with the standard MSA diminishes the overall results. The CT-MSA builds upon the standard MSA. Notably, the enhanced structure of the proposed transformer encoder, incorporating CT-MSA and TEA, demonstrates significant potential for effectively capturing the spatio-temporal features of wind speed."}, {"title": "Conclusion", "content": "Wind energy is an essential renewable energy source because of its environmental friendliness and widespread accessibility. Accurate forecasting of wind speed is crucial but difficult due to its intermittent nature, which is influenced by external factors, volatility, and both linear and nonlinear patterns. This study introduces the TCNFormer for short-term (12-hour) WSF. The TCNFormer combines TCNs with transformer encoder to capture spatio-temporal features of wind speed data. The transformer encoder includes two attention mechanisms: CT-MSA, which integrates causality and locality into the standard MSA, and TEA, which employs two external memory units to investigate internal sequence relationships and possible inter-sequence relations. This research employed wind speed data collected over a year, encompassing six seasons, from NASA POWER for Patenga Sea Beach, Chittagong, Bangladesh. The findings indicate that the TCNFormer model outperforms existing models in predictive accuracy. Future research will investigate the influence of weather data and seasonal wind direction on the model's capabilities."}]}