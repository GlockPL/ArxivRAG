{"title": "QUEEN: Query Unlearning against Model Extraction", "authors": ["Huajie Chen", "Tianqing Zhu", "Lefeng Zhang", "Bo Liu", "Derui Wang", "Wanlei Zhou", "Minhui Xue"], "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://anonymous.4open.science/r/queen_implementation-5408/.", "sections": [{"title": "I. INTRODUCTION", "content": "Having achieved revolutionary breakthroughs in various domains, deep neural networks (DNNs) are currently being employed in diverse areas to solve sophisticated real-world problems. The cost of training a high-performance DNN is non-negligible due to the high cost of the large volume of dataset collection, long training time, hardware consumption, etc. For instance, Microsoft has spent 1 billion dollars on OpenAI to develop a large language deep neural network models [1]. Therefore, the deep learning models are considered valuable intellectual properties to be protected by the model owners (referred to as defenders in the following). Nowadays, Model Extraction Attack (MEA)[2] is considered one of the most critical threats to DNN properties. In MEA, an adversary can establish a piracy model that has the same functionality and comparable performance as that of the protectee model by sending queries to the original models. This type of attack can lead to serious consequences in terms of copyrights as the adversary can steal from real-world Machine Learning as a Service (MLaaS) models at a very low cost. Meanwhile, MEAs are often used as the initial step for other attacks such as membership inference or model inversion attacks, where the adversary trains shadow models in the manner of MEAs [3], [4]. Hence, we urgently need a series of defenses to counter MEAs, especially in the era of large model.\nTo mitigate the threat of MEAs, extensive efforts have been made to enhance defense strategies. The existing defense methods can be classified into two classes, passive and active defenses. Passive defenses mainly comprise model watermarking and fingerprinting, where the defender extracts the embedded watermark from the piracy model or uses the fingerprints to get pre-defined output as proof of copyright violation [5], [6], [7], [8], [9]. Despite the effectiveness of the passive defenses, the validation can only be conducted after the violation, and the adversary could have already benefited from the piracy model before the crime is discovered. Moreover, if the adversary uses the piracy model only as a proxy to launch further attacks, or the adversary keeps the piracy model for private usage, the defender can no longer protect the protectee model with the passive defenses.\nIn contrast, active defenses [10], [11], [12] proactively prevent MEAs from happening by jeopardizing the training process with deliberately falsified outputs or lengthening the query time. Active defenses decrease the performance of the piracy models by perturbing the outputs of the protectee model, resulting in a degraded performance of the protectee model. Additionally, the majority of active defenses perturb the outputs randomly, which degrades the protectee model's performance significantly. Alternatively, the defender can also extend the query time of the adversary. The current MLaaS provides most services in a black-box setting, where the user can only query the model and receive the query outputs. Hence, we aim to address two questions to counter the MEAs: 1) Can we detect threats from the queries? 2) Can we proactively eliminate the threat before the violation occurs?\nWe have explored the possibilities to answer above ques-tions. For the first question, we find that the threat queries show potential trend to cover all classes in the DNN model. In other words, the coverage of those threaten queries is much broader than normal queries series. This enlightens us to estimate the coverage of the query set to detect threats. For the second question, when we identify the threat, it is possible for defender to provide some perturbed query answers that can mislead the shadow models, so it is possible to eliminate the threat before it actually happens.\nDue to the above positive rationale, we propose QUEEN (QUEry unlEarNing), which proactively launches counterat-tacks on potential security risks accompanied by sensitivity measurement. As depicted in Figure 1, there are two main components in QUEEN, namely sensitivity measurement and output perturbation. Given an arbitrary query sequence, the defender estimates the risk of the query by computing the single query sensitivity (SQS) and iteratively updating the cumulative query sensitivity (CQS) class-wise. In output perturbation, if the CQS of the predicted class of the query does not exceed the pre-defined threshold, the defender will perturb the query in the feature space to obfuscate the softmax output. If the CQS exceeds the threshold, the defender launches gradient reverse to make the piracy model generate gradients that worsen the model during training.\nSeveral challenges are identified and resolved during our research: 1) How to effectively measure the sensitivity of single queries? For each class in the training dataset, the defender extracts the features of the training samples, and defines the cluster center of each class as the most sensitive point. The distance between the feature of the query and the cluster center is used to estimate the SQS. To estimate the CQS, each query feature is treated as a hypersphere. By computing the ratio of the query features' volume to the volume of the sensitive region, the defender can update the CQS by class query-wise. 2) How to guarantee that the piracy model produces reverse gradients when trained using the falsified confidence vector? To launch gradient reverse, the defender must estimate the output of the piracy model given an input. Thus, the defender trains a set of shadow models using subsets of the training dataset to represent the incomplete piracy model. To increase the randomness of the results, a fraction of the shadow models are randomly drawn from the set to produce the average confidence vector for each query. With the estimated piracy confidence vector and that of the protectee model, the defender is able to launch gradient reverse.\nOur contributions are summarized as follows.\n\u2022 We propose a novel counterattack against model extrac-tion named QUEEN that proactively sabotages the MEAs before the copyright violation happens. QUEEN can be efficiently integrated with generic classification networks without interfering with the training process.\n\u2022 We have designed novel sensitivity measurement and output perturbation algorithms that can efficiently and precisely prevent MEAs while maintaining the prediction accuracy of the protectee model. The output is selectively perturbed to worsen the piracy model.\n\u2022 We have conducted extensive experiments to validate the effectiveness of QUEEN, where QUEEN has outper-formed the SOTA defenses."}, {"title": "II. PRELIMINARIES AND RELATED WORK", "content": "The notations are listed in Table I.\nGiven the protectee model $f(.; \\theta) : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ parameterized by $\\theta$, the objective of MEAs is to establish a piracy model $h(\\cdot; \\tau) : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ parameterized by $\\tau$, where h mimics the functionality of f by optimizing\n$\\arg \\min_{\\tau} L(f(x; \\theta), h(x; \\tau)),$ (1)\nwhere $L$ and $x \\in \\mathbb{R}^m$ respectively denote the loss function and the query sample. The adversary has only black-box access to f, where the adversary has zero knowledge about the architecture, parameters or hyperparameters of f. Normally, the adversary is assumed to have a limited number of unla-belled data samples and computational resources, otherwise the adversary can train the model independently. The dataset owned by the adversary can either be in-distribution or out-of-distribution to the training dataset of f. The auxiliary dataset can be collected online.\nQuerying the protectee model with natural samples is the most direct attack. However, due to the limited query budget, the adversary tends to select the optimal samples for querying. For instance, semi-supervised learning [13], reinforcement learning [14], and active learning [15] are used for query sam-pling selection. Additionally, using synthetic data to query the protectee model can also achieve considerable attack accuracy. There are various ways to generate the synthetic samples, such as FGSM [16], C&W attack and feature adversary attack [17]. For example, FGSM uses the gradient to modify the sample x, such that the modified sample deviates from the predicted class given by h, which is defined as\n$x \\leftarrow x - \\eta \\nabla_x\\pounds(x, h(x; \\tau)),$ (2)\nwhere $\\eta$ controls the strength of the modification. Furthermore, even without an auxiliary dataset, the adversary can use generative models to launch data-free MEAs [18], [19], but this requires an enormous query budget."}, {"title": "B. Defenses against Model Extraction Attacks", "content": "Generally, the current defenses against MEAs can be di-vided into passive and active defenses.\nPassive defenses mainly include model watermarking and fingerprinting, where the defender can determine whether a suspect model is a piracy model or not by validation. Model watermarking [5], [6], [20], [21], [22] aims to embed watermarks into the protectee model during training, where a set of trigger samples can make the protectee model generate the pre-defined outputs. Model fingerprinting [8], [23], [7], in contrast, does not interfere with the training process, and it creates adversarial samples as fingerprints to make the piracy model and the protectee model share the same outputs when given the fingerprints. In summary, passive defenses allow the defender to claim the copyright of the piracy model by ownership verification. However, this does not prevent the violation from the root.\nActive defenses proactively prevent MEAs by perturbing the outputs of the protectee model or intentionally increasing the query time when the query number ascends. Orekondy et al. [24] perturb the output by maximizing the angle deviation between the original and the perturbed gradients. Lee et al. [11] attach an additional layer to the end of the protectee model to produce random softmax output while maintaining the argmax of the output unchanged. Juuti et al. [25] use the Shapiro-Wilk statistics test to distinguish the benign queries from the malicious queries, where the benign samples are believed to fit normal distributions and the malicious do not. Kariyappa et al. [12] train a out-of-distribution (OOD) sample detector to differentiate OOD query samples from normal query samples. The query results of OOD samples are perturbed by a misinformation function. Kariyappa et al. [26] train an ensemble of models to detect OOD query samples and perturb the corresponding outputs. Zhang et al. [27] perturb the query sample to the edge of the decision boundary while keeping the argmax of the output unchanged to produce obfuscated outputs. Dziedzic et al. [10] utilize private aggregation of teacher ensembles (PATE) to measure the privacy risk of each query, and increase the query time when the query number grows by forcing the adversary to solve the proof-of-work (PoW) puzzles."}, {"title": "III. PROBLEM DEFINITION", "content": "First, we formally define the protectee model and the piracy model. A problem domain is denoted by $D \\subset \\mathbb{R}^M$, where each element $x \\in D$ is labeled by one of N classes. We use $y \\in \\mathbb{R}^N$ to denote the one-hot encoded label vector. A deep learning model is a function $f(\\cdot; \\theta) : \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$, parameterized by $\\theta$.\nA protectee model is a deep learning model $f(\\cdot; \\theta) : \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$ with $\\theta$ as its parameters trained by a defender who owns a dataset $D \\subset D$. D comprises $\\{(x_1, y_1), ..., (x_n, y_n)\\}$. The defender trains the protectee model f by taking gradient steps to optimize $\\theta$ on\n$\\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_{fsm}(f(x_i; \\theta)), y_i),$ (3)\nwhere $\\mathcal{L}(\\cdot, \\cdot)$ denotes the loss function such as the cross entropy (CE) loss or Kullback-Leibler divergence (KLDiv) loss; $fsm(\\cdot)$ denotes the softmax function.\nA piracy model is another deep learning model $h(\\cdot;\\tau) : \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$ with $\\tau$ as its parameters established by the ad-versary by launching model extraction attack on the protectee model f. h shares the same or similar functionalities with f. The adversary collects an auxiliary dataset $D' \\subseteq D \\backslash D$. This means that $D' = \\{x_1', ..., x_m'\\}$ comes from the same problem domain D but does not overlap with D. Additionally, $x' \\in D'$ is not labeled by the adversary. The adversary feeds $x'$ into the protectee model f so as to get the softmax output, i.e., the confidence vector $f(x')$. Similarly, the adversary trains the piracy model h, taking gradient steps to optimize $\\tau$ on\n$\\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(fsm(h(x'; \\tau)), fsm(f(x'; \\theta))), \\forall x' \\in D'.$ (4)"}, {"title": "A. Threat Model", "content": "We consider the scenarios where not only the label of the prediction, but also the entire confidence vector is of the users' concern. For example, the confidence vector can be further utilized for downstream tasks such as OOD detection [28] or membership inference attack [29]. We define the threat model in terms of the capabilities and limitations of the two parties, namely the adversary and the defender.\nThe Adversary. We assume that the adversary is capable of:\n\u2022 Collecting public data. The adversary can access public datasets to construct an auxiliary dataset for model extrac-tion. The auxiliary dataset can be either in-distribution or out-of-distribution to the training dataset. For instance, given that the protectee model is trained on MNIST dataset, FEMNIST dataset is in-distribution, and CIFAR-10 dataset is out-of-distribution.\n\u2022 Accessing the protectee model. The adversary can query the protectee model with any input with black-box access. It means that the adversary is limited to sending queries to the protectee model and receiving the predictions from the protectee model only. The predictions are presented in the form of complete confidence vectors.\n\u2022 Customizing the training algorithm. The adversary is able to freely select the network architecture, optimization algorithms, loss functions, etc. However, we assume that the adversary is going to select either the CE or KLDiv loss function, because these two are the most commonly used loss functions in terms of training classifiers.\n\u2022 Knowing the specific defense method. The adversary knows which exact defense method is used, and therefore, the adversary can launch the most threatening attack on the protectee model.\nWe assume that the adversary is limited to:\n\u2022 Knowledge of the protectee model. The adversary does not know the model architecture, parameters, or hyper-parameters of the protectee model.\n\u2022 Access to the training data. The adversary cannot access the original training samples possessed by the defender.\nThe Defender. In contrast, the defender is capable of:\n\u2022 Completely accessing the protectee model. The defender has full knowledge about the protectee model including its model architecture, parameters and hyperparameters.\n\u2022 Completely accessing the original dataset. The defender can access and modify any samples in the original dataset.\n\u2022 Utilizing the queries. For any query sent by the user, the defender is allowed to perform any operation on the query, including storing and analyzing the query.\nMeanwhile, the defender is limited to:\n\u2022 Knowledge of the piracy model. The defender does not know the model architecture, parameters, or hyperparam-eters of the piracy model.\n\u2022 Knowledge of the specific attack. The attack method em-ployed by the attacker remains unknown to the defender.\nThe goal of the adversary is to launch model extraction attack on the protectee model so as to obtain a piracy model that performs comparably to the protectee model. Reversely, the defender aims to differentiate the adversaries from the benign users, and proactively jeopardize any possible threat.\nThe Adaptive Adversary. We further consider that the adaptive adversary knows about the defense mechanism of QUEEN. Given a perturbed confidence vector $\\tilde{y}$, The adaptive adversary aims to recover the clean prediction confidence vector $\\hat{y} = R(\\tilde{y})$ using a recovery function R(\u00b7). Thus, adaptive attacks such as D-DAE [2] and pBayes [30] that establishes R(\u00b7) pose the greatest threats to QUEEN. We thus test the performance of QUEEN against these adaptive attacks in the experiment."}, {"title": "B. Concepts of Defense Mechanism", "content": "Central Data vs. Peripheral Data. Before designing the method, we try to figure out how the classification works in f. Let $f_E$ and $f_C$ respectively denote the feature extraction block and the classification block in f. Then we have\n$f(x) = f_C(f_E(x)),$ (5)\nwhere $f_E$ maps x into a feature space $F \\subset \\mathbb{R}^O$, and $f_C$ maps $f_E(x)$ into $\\mathbb{R}^N$ so as to derive the confidence vector. Given the training dataset D MNIST [31], we use $f_E$ to extract the feature $f_E(x_i), \\forall x_i \\in D$. The features are then projected into a 2D space via t-SNE as depicted in Figure 8 in the Supplemental Materials.\nBased on our observation, $f_E(x_i)$s are separated into clearly distinguishable clusters by their label $y_i$, which are the black dots. We then process the test dataset with the same pro-cedure as above, where we use different colors to represent the classes. Two interesting phenomena are observed: 1) the projection of the test data highly overlaps with that of the training data; 2) the misclassified test data points fall in places that are distant from the center of the clusters. Thus, we assume that the center of each cluster in F represents the most sensitive region in the cluster. Thus, the queries that hit the center of the cluster are the most representative.\nTo justify our above assumption, we thus conduct a pre-experiment to check whether the central data is more repre-sentative than the peripheral data. Here, central data refers to the data whose feature is close to its cluster center, whereas peripheral data refers to the data whose feature is distant from its cluster center. In order to rank the data by their distances to their cluster center, we firstly need to define the cluster center.\nDefinition 1-Cluster Center: Given a feature cluster $\\{f_E(x_1), ..., f_E(x_n)\\}$ sharing the same label y, the cluster center is defined as\n$c_y = \\frac{1}{n} \\sum_{i=1}^n f_E(x_i),$ (6)\nwhere $c_y$ denotes the center of the feature cluster labeled by $y = arg \\max(y)$.\nThe experimental results show that the central data leads to higher performance of the trained classifier network compared to the peripheral data. This suggests that the query whose feature is closer to the cluster center is more sensitive. Section B in the Supplemental Materials shows the details.\nSingle Query Sensitivity. Given an arbitrary query, the defender aims to know how sensitive it is when the logit produced by the protectee network is honestly returned. Based on the pre-experiment, we know that the probability of a query being more representative than another gets higher, when its feature is closer to the cluster center than the other. Therefore, we define representative probability (RP) to measure the sensitivity of the query.\nDefinition 2-Single Query Sensitivity: Given an arbitrary query x labeled by $y = arg \\max y$, the single query sensi-tivity $sqs(x, y)$ is measured by the representative probability $P(x, y)$, which is computed by\n$sqs(x, y) = P(x, y)$\n$= \\frac{1}{2} erfc(\\frac{\\|f_E(x) - c_y\\| - d_y}{d_y \\sqrt{2}}).$ (7)\nwhere erfc is the complementary error function defined as\n$erfc(z) = \\frac{2}{\\sqrt{\\pi}} \\int_z^\\infty e^{-t^2} dt;$ (8)\n$d_y$ denotes the average distance between $c_y$ and the features of the training data labeled by y."}, {"title": "Cumulative Query Sensitivity", "content": "Given an arbitrary query sequence sharing the same predicted label, the defender aims to quantify the impact caused by honestly returning the con-fidence vector. As depicted in the right of Figure 2, for all data samples in the training dataset labeled by y, the defender uses $f_E$ to extract the training features. The training features together form a hypersphere dyed gray in the feature space F, which is called the sensitive space. The defender then extracts the query features from the queries and treats each query feature as a small hypersphere dyed red that represents itself and the queries whose features fall around the center of the hypersphere. Together, the query features forms the queried space.\nBy summing up the volume of the query features and dividing it by the volume of the hypersphere formed by the training data, the defender derives the ratio of the queried space to the sensitivity space. We thus define this ratio as the CQS.\nDefinition 3-Cumulative Query Sensitivity: For a query sequence $X = \\{x_1,...,x_m\\}$ labeled by $y = arg \\max y$, the cumulative sensitivity $cqs(X, y)$ is computed by\n$cqs(X,y) = \\frac{\\sum_{x \\in X} v(sqs(x, y), r)}{v(sqs(c_y, y), d_y)},$ (9)\nwhere $v(\\cdot, \\cdot)$ is the function that computes the volume of a hypersphere; r is a hyperparameter that defines the radius of the hyperspheres of the query features.\nIf $cqs(x, y)$ exceeds a pre-defined threshold t, this training class is considered to be threatened by the query sequence. The counterattack is launched to protect the protectee model.\nOutput Perturbation. If a query sequence categorized into one class is determined to be threatening that class, the defender stops offering the true confidence vector. Instead, the defender proactively jeopardizes the potential possibilities where the queries can support the training of a piracy model. The defender starts to offer falsified confidence vectors to the adversary. Theoretically, the falsified confidence vector is designed to make the piracy model to generate reverse gradient such that the piracy model gets worsened when the falsified confidence vector is used in the training process. We define this objective as follows."}, {"title": "Definition 4-Gradient Reverse", "content": "Given an arbitrary query x, the defender aims to create a falsified confidence vector $\\tilde{y}$ such that\n$\\nabla_{\\tau}\\mathcal{L}(fsm(h(x; \\tau)), \\tilde{y}) = -\\nabla_{\\tau}\\mathcal{L}(fsm(h(x; \\tau)), fsm(f(x; \\theta)))$ (10)\nmaking the gradient completely reverse to the correct direc-tion.\nThereby, the piracy model trained using $(x, \\tilde{y})$ is crippled, and therefore its performance will be far from comparable to the protectee model."}, {"title": "IV. QUEEN METHOD", "content": "There are two main components in our method, namely sensitivity measurement and output perturbation. As de-picted in Figure 3, given the query sequence sent by the user, the defender extracts the query feature with the protectee network. The query feature is then further mapped to a lower-dimensional space for sensitivity measurement. Based on the measurement, different strategies are employed to perturb the softmax output or to return the normal softmax output. The components are further specified as follows.\nIn sensitivity measurement, a query sensitivity estimation system that estimates the single query sensitivity (SQS) for each query is established. The cumulative query sensitivity (CQS) can thus be derived from the SQS of each query in a given query sequence.\nIn gradient reverse, given a user and a query sequence, the defender compute the CQS of the query sequence. Then, based on the CQS, the defender determines whether the query sequence is threatening the training dataset class-wise. If there is any potential that the queries of a class could support the training of a piracy model, the defender launches a counterattack by sending falsified confidence vectors. The falsified confidence vectors are designed to worsen the piracy model if they are used in the training process."}, {"title": "B. Sensitivity Analysis", "content": "Before making the pre-trained protectee model publicly available, the defender first performs a sensitivity analysis on every class of the training data, so that the defender is able to measure the CQS given any query sequence from the user. The process of sensitivity analysis is demonstrated in Alg. 1, where the objective is to obtain a trained mapping network g, the set of cluster centers c and the set of average distance d. The defender owns a protectee model f that is trained on the training dataset D. The defender uses the feature extraction layers $f_E$ of f to extract the training features of the training data, which is denoted as $U = \\{(u_1, y_1), ..., (u_n, y_n)\\} \\subseteq F$, where $u_i = f_E(x_i)$. However, it is not practical to compute the volume of the hypersphere in F, because volume compu-tation in high-dimensional space is complex. Additionally, the defender does not want to modify the parameters $\\theta$ in f to realize dimension reduction. Therefore, the defender needs a fixed mapping network g that maps the features from F to a low-dimensional space.\nIn our setting, we define g as a mapping network that maps the features to a two-dimensional (2D) space, denoted as g : $F \\rightarrow \\mathbb{R}^2$. g can have a very simple architecture, such as several fully-connected layers. Moreover, g is required to make the mapped features preserve the property as that of the features in F. That is to say, central features should be more representative than the peripheral features. To achieve this goal, we train g using the supervised contrastive loss[32] that takes the following form.\n$\\mathcal{L}_{sup} = \\sum_{i \\in I} -log ( \\frac{1}{|O(i)|} \\sum_{o \\in O(i)} \\frac{exp(<z_i, z_o>/\\gamma)}{\\sum_{a \\in A(i)} exp(<z_i, z_a>/\\gamma)} ),$ (11)\nHere, within a multi-view batch $U'$ randomly sampled from U, i \u2208 I = {1, ..., N} is the index of samples in the batch; z denotes the mapped 2D feature, namely $z_i = g(f_E(x_i))$; The operator denotes the inner product; A(i) = I\\{i} denotes the set of indices without i; O(i) = {o \u2208 A(i) : $y_o$ = $y_i$} is the set of indices of all other features sharing the same label as that of $z_i$; the operator |\u00b7| returns the number of elements in the set; \u03b3 \u2208 $R^+$ is a scalar temperature parameter. In short, by minimizing $\\mathcal{L}_{sup}$, the defender makes g maps the features in the same class closely, whereas those of different classes are mapped distantly. Thus, the relative position of feature $u_i \\in U$ is kept when it is mapped to $z_i$. Another feasible solution is to use the hierarchical contrastive loss proposed in [33].\nWith a trained mapping network g, the defender is able to map entire training feature set F to a 2D feature set denoted by $Z = \\{(z_1, y_1), ..., (z_n, y_n)\\}$. Let $Z_y \\subseteq Z$ denote the 2D training feature set of class $y = arg \\max(y)$. The defender now computes the cluster center $c_y^g$ in the 2D space for each $Z^y$ as defined in Eq. 6 by\n$c_y^g = \\frac{1}{|Z_y|} \\sum_{z_i \\in Z^y} z_i,$ (12)\nMeanwhile, the defender also computes the average distance $d_y$ between $c_y^g$ and $z_i \\in Z_y$ by\n$d_y = \\frac{1}{|Z_y|} \\sum_{z_i \\in Z^y} \\|c_y^g - z_i\\|.$ (13)"}, {"title": "C. Sensitivity Measurement", "content": "With the mapping network g, the set of cluster center $c = \\{..., c_y^g, ...\\}$ and the set of average distances $d = \\{..., d_y, ...\\}$, the defender can measure the sensitivity of any given query sequence. As illustrated in Alg. 2, given the auxiliary dataset $D' = \\{x_1,..., x_m\\}$, the defender gets the set of CQS $s = \\{..., s_y, ...\\}$, where $s_y$ denotes the CQS of class y.\nThe defender first gets the predicted label $\\hat{y}_i = arg \\max fsm(f(x_i)), \\forall x \\in D'$. Next, the defender splits D' by the predicted labels such that $D'_\\hat{y}$ denotes the subset of D' labeled by $\\hat{y}$. The sensitivity measurement is then conducted class-wise. For each class, the defender determines the sensitive region by setting its radius to $d_\\hat{y}$.\nTo compute the SQS, the defender gets the 2D feature $z_i = g(f(x_i)), \\forall x \\in D'_\\hat{y}$. The defender then needs to determine whether $z_i$ satisfies the following two conditions:\n1. $z_i$ is within the sensitive region, i.e., $\\|z_i - c_{\\hat{y}}^g\\| < d_{\\hat{y}}$;\n2. There is no previously recorded feature overlapping with $z_i$, i.e., $\\nexists z_j \\in Z_{\\hat{y}}$, s.t. $\\|z_i - z_j\\| < r, i \\ne j$. Here, r is a hyperparameter denoting the radius of each query circle representing the features of the query and those nearby it. If the conditions are satisfied, the defender proceeds to compute the SQS of the query by\n$sqs(z_i, \\hat{y}) = \\frac{1}{2} erfc(\\frac{(\\|z_i - c_{\\hat{y}}^g\\| - d_{\\hat{y}})}{d_{\\hat{y}} \\sqrt{2}}).$ (14)\nEventually, the CQS of the 2D features satisfying the conditions Z with the predicted label $\\hat{y}$ is computed by\n$cqs(Z, \\hat{y}) = \\frac{r^2}{(d_{\\hat{y}})^2} (sqs(z_i, y)^2.$ (15)\nIntuitively, $cqs(Z, \\hat{y})$ indicates the ratio of the queried area to the sensitive area in class $\\hat{y}$. If $cqs(Z, \\hat{y})$ exceeds a pre-defined threshold, the adversary is determined to be threatening class $\\hat{y}$. Thus, the defender ought to launch a counterattack to prevent further losses.\nEventually, there are four conditions after the sensitivity measurement as depicted in Figure 3:\n\u2022 Cond. A: the 2D feature is not within the sensitive region.\n\u2022 Cond. B: the 2D feature is within the sensitive region; the 2D feature overlaps with the records; the CQS exceeds the threshold.\n\u2022 Cond. C: the 2D feature is within the sensitive region; the 2D feature does not overlap with the records; the CQS is below the threshold.\n\u2022 Cond. D: the 2D feature is within the sensitive region; the 2D feature overlaps with the records.\nIn Cond. A/B, the defender launches feature perturba-tion/gradient reverse to perturb the softmax outputs, whereas the normal softmax outputs are returned in Cond. C/D.\nWe claim that the data points whose features are not in the sensitive region can also be used in training the piracy model. Our CQS measurement does not include those data points. But those data points are also perturbed by feature perturbation to reduce their contribution to the piracy model training."}, {"title": "D. Output Perturbation", "content": "As demonstrated in Alg. 3, the defender proactively de-fends any potential attacks by conditionally perturbing the confidence vector generated by the protectee model. Given a pre-defined threshold t, if $cqs(Z, \\hat{y}) > t$, the defender starts the gradient reverse counterattack by sending a falsified confidence vector to the adversary to achieve the objective described in Eq. 17. To achieve this objective, the defender needs two main components in gradient reverse, namely piracy model simulation and gradient reverse.\nFor the query whose feature is not within the sensitive region, the defender performs feature perturbation by mov-ing the feature towards the most distant cluster center. The perturbation stops before the predicted label of the perturbed feature changes, resulting in a confidence vector that worsens the piracy model trains with it.\nPiracy Model Simulation. Due to the fact that the defender has zero knowledge about the piracy model, the defender cannot launch gradient reverse because the output of the piracy model is not accessible. To address this problem, the defender needs piracy model simulation, i.e., to train a set of shadow models locally to simulate the behavior of the piracy model. The weights of different models typically converge to the initial stable points within the same optimization problem [34", "24": "."}]}