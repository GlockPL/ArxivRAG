{"title": "CompCap: Improving Multimodal Large Language Models with Composite Captions", "authors": ["Xiaohui Chen", "Satya Narayan Shukla", "Mahmoud Azab", "Aashu Singh", "Qifan Wang", "David Yang", "Sheng Yun Peng", "Hanchao Yu", "Shen Yan", "Xuewen Zhang", "Baosheng He"], "abstract": "How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.", "sections": [{"title": "1 Introduction", "content": "Recently, significant advancements have been made in Multimodal Large Language Models (MLLMs) (Alayrac et al., 2022; Li et al., 2023; McKinzie et al., 2024; Liu et al., 2023a). These models combine images with large language models (LLMs) (OpenAI, 2023b; Team et al., 2023; Dubey et al., 2024) to harness the powerful capabilities of LLMs, demonstrating exceptional powers in visual and language understanding and achieving remarkable conversational ability. However, despite these advances, a notable limitation remains: MLLMs often struggle with comprehensive understanding of composite images (CIs), extracting only partially accurate information. A composite image (CI) is a visual creation that combines various elements, such as photos, graphics, text, or other media, into a single cohesive image. It includes diverse types such as collages, posters, and charts. This raises an important question: Why do these limitations persist? Our hypothesis is that the observed shortcomings in MLLMs may stem from a lack of CI-caption pairs in the training data.\nIn essence, the training procedure for MLLMs generally involves two stages: first, pre-training (PT) on image-caption datasets to align the vision encoder with the LLM, and second, supervised fine-tuning (SFT) on instruction or visual question answering (VQA) datasets to enhance the MLLMs' instruction-following abilities (Li et al., 2023; Liu et al., 2023a; McKinzie et al., 2024). Research has shown that using high-quality image captions enhances the alignment between vision and language modalities, thereby improving MLLMs' image understanding (Chen et al., 2023; McKinzie et al., 2024). However, current training data primarily includes high-quality captions for natural images (NIs), while such captions for CIs are often missing. In this work, we find that MLLMs' captioning abilities are strongly correlated with their VQA performance, suggesting that instruction data is insufficient for MLLMs to fully comprehend CIs.\nWe introduce CompCap, a framework that automatically synthesizes high-quality CI-caption pairs, to bridge the data shortage in training MLLMs. CompCap functions as a flexible framework that utilizes various metadata to construct CIs along with their corresponding captions. This metadata could include a range of sources such as pre-existing image-caption pairs, layout information, text, or tabular data. For example, one implementation of CompCap could"}, {"title": "2 MLLMs Need Good CI Caption Data", "content": "In this section, we discuss the necessity of introducing high-quality CI captions for training MLLMs. We starts with giving the definition of composite image in \u00a7 2.1. \u00a7 2.2 illustrates the limitations of MLLMs in accurately understanding CI, which often leads to generating incorrect information during captioning. This observation motivates us to curate a CI-caption dataset."}, {"title": "3 CompCap", "content": "This section elaborates on the proposed method. \u00a7 3.1 explains the characteristics that define a high-quality CI caption. \u00a7 3.2 illustrates the CompCap framework. Lastly, \u00a7 3.3 provides a detailed implementation of CompCap to generate collage image-caption pairs."}, {"title": "3.1 What Makes a Good Cl Caption", "content": "Unlike NIs, CIs contain a diverse array of visual elements that convey complex information. Effective CI captions must be meticulously crafted to enhance MLLMs' ability to accurately interpret CIs. Specifically, we focus on two principles: accuracy and detailedness.\nAccuracy: An accurate caption faithfully represents the content of the image without introducing any false or misleading information. This is crucial for ensuring MLLMs generate reliable responses.\nDetailedness: A detailed caption provides specific insights into all visual elements and their relationships, going beyond a basic description. This is essential because CIs often have multiple layers of meaning, combining text, graphics, and data. For instance, a comprehensive caption for an infographic should not only describe the overall topic but also explain each section, data point, and visual cue. Including such details helps MLLMs align textual descriptions with all aspects of the visual content, thereby improving the model's ability to fully understand and interpret CIs."}, {"title": "3.2 The CompCap Framework", "content": "CompCap is a general framework that synthesizes CI-caption pairs for training MLLMs.  It leverages metadata to generate both a CI and its corresponding caption. Below, we explain what constitutes the metadata and how it is utilized in the creation of the composite image and caption generation.\nMetadata: Metadata comprises both raw data and configuration details necessary for generating a CI. The nature of the raw data varies depending on the CI class and could include a collection of image-caption pairs, tabular data, or textual data such as code or math expressions. Raw data serves as the fundamental content that the CI will represent. Configuration, or customization, on the other hand, is generated through a random process based on the raw data. It determines how this raw data is visually represented in the CI. For example, when generating a chart, the raw data might contain tabular information, while the configuration specifies details like the chart type (such as bar or line charts), the data columns to visualize, titles, and other visual parameters such as color and style."}, {"title": "3.3 An Instantiation of CompCap for Collage", "content": "This section outlines the CompCap framework applied to create a collage. We highlight the implemented pipeline in Figure 4. Detailed implementations for all CI pipelines are provided in the Appendix (Collage: A.1; Image-Text: A.2; Chart: A.3; Diagram: A.4; Code: A.5; Table: A.6).\nRaw data: The pipeline begins with retrieving a set of image-caption pairs from the database. To simulate diverse, real-world scenarios, we employ three retrieval methods:\nRandom retrieval: Sampling image-caption pairs uniformly from the datasets to create unrelated image.\nSimilarity-based retrieval: Sampling image-caption pairs with similar visual and textual features. We calculate the similarity between any two image-caption pairs by summing the cosine similarity of their image embeddings and that of their caption embeddings. We extract the image embeddings using Dino-v2 (Radford et al., 2021) and caption embedding using CLIP (Radford et al., 2021).\nEntity-based retrieval: Retrieving image-caption pairs that depict the same entity (e.g., a public figure or landmark). Beginning with entity randomly sampled from a predefined entity list, we filter pairs to include those whose captions contain the chosen keyword, and sample from the filtered group."}, {"title": "4 Training MLLMs with CompCap Data", "content": "We train MLLMs with CI-caption dataset to validate the effectiveness of the CompCap framework. We first describe the curated dataset in \u00a7 4.1, then the training details in \u00a7 4.2."}, {"title": "4.1 The CompCap-118K Dataset", "content": "The CompCap-118K dataset, generated via CompCap, is a synthetic collection of 117,879 image-caption pairs spanning six CI categories. Each category uses different types of metadata and simulation tools to generate the images, and the captions are created using various LLMs, depending on the complexity of the captioning task."}, {"title": "4.2 The CompCap-4B/7B/13B MLLMs", "content": "We develop the CompCap series MLLMs using two recently introduced MLLM architectures: LLaVA-NeXT (Liu et al., 2024) and xGen-MM (Xue et al., 2024). For LLaVA-NeXT, we use the 2024-01 release (7B and 13B Vicuna versions), while for xGen-MM, we use version 1.5 (4B model).\nThe MLLMs are trained in two stages: a PT stage and a SFT stage. We incorporate CompCap-118K dataset into the SFT stage. To ensure a fair comparison, we uniformly downsample the original SFT dataset and add CompCap-118K such that the total number of training samples remained equivalent. Since the SFT dataset for xGen-MM is not released, we curate a SFT dataset comprising 782K image-text pairs and 221K pure text samples, closely following the data recipe reported in Xue et al. (2024). We refer to the resulting MLLMs as CompCap-4B/7B/13B. We validate the effectiveness of the proposed framework through the experiments outlined in the following section."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Evaluation Benchmarks", "content": "We evaluate the MLLMs across multiple benchmarks, particularly with the focus on their ability to comprehend CIs. We adopt NI-focused benchmarks like SEEDBench (Li et al., 2024), TextVQA (Singh et al., 2019), MMBench (Liu et al., 2025), MME (Yin et al., 2023), and LLaVABench (Liu et al., 2023b) to test conversational, reasoning, perception, and text recognition abilities.\nWe also use CI-focused benchmarks, including ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), WebSRC (Chen et al., 2021), MathVista (Lu et al., 2023), and OCRBench (Liu et al., 2023c). Specifically, ChartQA, DocVQA, and InfoVQA measure the ability to interpret visually rich chart, document, or diagram images, while WebSRC focuses on web-based reading comprehension. MathVista and OCRBench contain both NIs and CIs, testing OCR abilities across various formats. For evaluation, we use VLMEvalKit (Duan et al., 2024) and LMMS-EVAL (Zhang et al., 2024)."}, {"title": "5.2 Main Results", "content": "We present the quantitative results in Table 2, comparing MLLMs of three different sizes against similarly scaled models (3B-4B, 7B-8B, and 13B).\nFrom Table 2a, we can see that CompCap-4B/7B/13B consistently outperform the other MLLMs (xGen-MM-inst.-4B* and LLaVA-NeXT-Vicuna-7B/13B) that share the same architectures and similar size of training data by 1.7%, 2.0%, and 2.9%, respectively. The performance gains are particularly noticeable on"}, {"title": "5.3 Ablations", "content": "Cl component ablation: We incrementally include the curated image-caption pairs from each CI type into the CompCap dataset to investigate the effectiveness of each type. We train LLaVA-NeXT-Vicuna-13B on the dataset variants and assess how the inclusions affect MLLMs' performance on NI-dominated benchmarks and CI-dominated benchmarks. Results are summarized in Table 3. With the introduction of each CI-caption component, the MLLM"}, {"title": "6 Related Works", "content": "MLLMs (OpenAI, 2023a; Liu et al., 2024; Dai et al., 2023; McKinzie et al., 2024) are designed to enhance LLMS (OpenAI, 2023b; Touvron et al., 2023; Dubey et al., 2024; Yang et al., 2024; Bai et al., 2023) with multimodal understanding, particularly for visual information. These models typically connect a pre-trained vision encoder (Radford et al., 2021; Zhai et al., 2023) to a powerful LLM, using a vision-language connector like MLPs (Liu et al., 2023b) or Q-former (Dai et al., 2023) to align the visual with text modalities. Recent advancement on MLLMs majorly focus on leveraging and curating extensive, diverse, and high-quality training datasets (Chen et al., 2023; Xue et al., 2024; Tong et al., 2024; McKinzie et al., 2024) to improve the MLLMs' abilities. Particularly, Chen et al. (2023); McKinzie et al. (2024) highlight the importance of high-quality caption data. While image-caption pairs being fundamental for aligning visual and textual representations, there is a lack of such dataset for CIs, which is an important gap our work aims to address.\nMultimodal synthetic datasets (Johnson et al., 2017; Kafle et al., 2018; Methani et al., 2020; Kim et al., 2022; Li et al., 2022; Chang et al., 2022; Lindstr\u00f6m and Abraham, 2022; Liu et al., 2023b) have emerged as a scalable and cost-effective solution for training MLLMs. These datasets are either generated by producing synthetic captions or instructions for real images using AI tools or by creating synthetic images paired with template-based instructional text. For instance, LLaVA (Liu et al., 2023b) prompts GPT to generate instruction data for COCO images (Lin et al., 2014), while BLIP (Li et al., 2022) employs CapFilt to generate more refined caption data. For complete image-text pair synthesis pipelines, DVQA (Kafle et al., 2018) and PlotQA (Methani et al., 2020) focus on synthetically generated charts, aiming to develop question-answering pairs that test the ability to interpret, retrieve data from, and reason about the information presented in these charts. Similarly, MapQA (Chang et al., 2022) emphasizes choropleth maps of the United States, where color variations depict data values across geographic regions, offering a range of map styles and question types to evaluate map interpretation and information extraction. Donut (Kim et al., 2022) presents SynthDoG, which generates synthetic document image from given text, targeting the ability of document understanding. In contrast to previous methods that generate synthetic images, our proposed CompCap framework covers a wider spectrum of image types (Collage, Image-Text, Chart, Diagram, Table, and Code). Moreover, CompCap targets at curating detailed caption for the generated images, which benefits more on the vision-language alignment rather than instruction-following ability."}, {"title": "7 Conclusion", "content": "In this work, we propose CompCap, a versatile framework designed to generate high-quality, detailed captions for composite images (CIs) such as charts, diagrams, and tables. The resulting dataset, CompCap-118K, comprises 118K captions across six CI categories, significantly enhancing MLLMs' capabilities in CI understanding. Experimental results demonstrate that incorporating CompCap-118K notably improves MLLMs' performance across eleven benchmarks, particularly in CI-specific tasks, emphasizing the critical role of caption data in achieving robust vision-language alignment. Additionally, by expanding CompCap with more CI pipeline implementation and raw data sources, we can further scale and enhance the generated dataset."}, {"title": "A CI Implementations of CompCap", "content": "This section elaborates the implementation of each CI type. We show the detailed pipeline implementation of collage in A.1; image-text in A.2; chart in A.3; diagram in A.4; Code in A.5; and table in A.6."}, {"title": "A.1 Collage Implementation", "content": "We summarize the workflow in Figure 4. In this section, we first show how the image-captions used for composing collage are retrieved, then we elaborate on the design of collage caption and LLM prompting."}, {"title": "A.1.1 Data Sources, Layout, and Retrieval Engines", "content": "We retrieve image-caption pairs from existing image-caption datasets. And we maintain a curated entity list of public figures, artworks, landmarks, and brands sourced from web data.\nPre-processing: We first process the datasets for better retrieval quality and efficiency:\nConstruct entity-sample lookup table. For each image, we identify entities in the original caption that match entries in our maintained entity list and create an entity-image lookup table specifically for entity-based retrieval.\nPre-compute embeddings. For each sample in both datasets, we pre-compute Dino-v2 image embeddings and CLIP caption embeddings.\nLayout: We define two collage layouts: grid collage and auto collage. In the grid collage layout, images are arranged in an $n \\times m$ grid, where $n,m \\in \\{1,2,3,4\\}$. To increase layout diversity, cells within the grid can merge to form larger cells. Since rows and columns in the grid layout are aligned, the layout will specify the width/height ratio for each image within a cell, posing constraints for the retrieval process. To further enhance diversity, we introduce the auto collage layout, where only rows or columns are aligned. This enable composing images of arbitrary width/height ratio into a collage image.\nSimilarity-based retrieval: We start by uniformly sampling an image-caption pair as the anchor data $x_a = (I_a, T_a)$ and then retrieve the top 20 most similar image-caption pairs from the database $D$. Let $I_{dino}$ and $T_{clip}$ represent the Dino-v2 image embedding and CLIP text embedding for the anchor data, while $I'_{dino}$ and $T'_{clip}$ are the embeddings of an data $x = (I,T) \\in D$. The similarity score between $x_a$ and $x$ is computed as follow:\n$\\text{sim}(x_a, x) = \\text{cos}(I_{dino}, I'_{dino}) + \\text{cos}(T_{clip}, T'_{clip})$.\nFrom the top 20 candidates, we randomly select samples to construct the collage. Where width/height ratios are specified for candidate images, a filter is applied to the database prior to calculating similarity.\nEntity-based retrieval: To optimize retrieval, we narrow down the entity list to include only entities that appear more than twice in the dataset. We randomly sample a keyword from this list and apply rule-based matching in the database to select related data. Since such data are sparse, we only use the auto layout to compile collages to avoid the width/height restrictions. In post-processing, we further de-duplicate collages to ensure variety.\nIn both similarity-based and entity-based retrieval, there may be cases where retrieved images are only loosely related to the anchor image. For instance, when retrieving images based on an anchor image of a cricket game, some results might instead depict baseball due to their visual similarities. However, as long as the corresponding caption accurately"}, {"title": "A.1.2 Caption and Prompt Design", "content": "We design the caption such that it provides a detailed walk-through on the images in the collage. Particularly, it either goes over the images by rows or by columns. For each row or column, we specify the demonstration order to be from left to right or from top to bottom, which comes first depends on the generated layout. For instance, for grid collage that comes with a multi-column cell, the caption is designed to go over images by row. And for auto collage whose columns are aligned, the caption is designed to go over images by column.\nIn order for LLMs to generate desired caption, different layout uses prompt that are slightly different in terms of the coordinate system notation and the in-context examples. We demonstrate the designed prompts and the example outputs from LLMs in Figures 8 and 9. We use Llama-3.1-70B (Dubey et al., 2024) for all caption generation.\nCaption post-processing: We find that some of the responses from LLMs do not completely follow the given instructions. For example, the response may start with \"#Recaption:\"or \u201cHere is the generated caption:\" before the actual image caption, or contain a follow-up question such as \"Let me know if you have further instruction\" after the caption. To address this, we perform a manual check on a batch of generated responses and summarize all possible patterns, and implement a post-processing pipeline to replace and delete undesired text automatically."}, {"title": "A.2 Image-Text Implementation", "content": "The curation pipeline for the Image-Text CI class is illustrated in Figure 10. This class is designed to assess MLLMs' capabilities in extracting text from images and understanding the relationship between text and visuals. We divide this into two subcategories: image-and-text and pure-text. The image-and-text category tests the model's ability to infer the relationship between text and the image, expecting MLLMs to interpret how text interacts with visual elements. The pure-text category focuses solely on text extraction. We present text in various styles against different backgrounds to strengthen the MLLMs' robustness in text recognition."}, {"title": "A.2.1 Image-and-text Pipeline", "content": "Similar to collage, we sample image-caption pair as the background. We only consider random retrieval as we only retrieve one image at a time. We then prompt an LLM to generate relevant text content to the image based on the caption. Note that we specifically instruct the LLM not to rephrase the caption. The prompt used for text content generation is shown in Figure 11. We first ask Llama-3.1-70B to infer the topic related to the image, then generate a sentence within the topic. To enhance visual diversity, we control two primary configurations:\nBox layout: We position the text within a bounding box, arranging the box alongside or overlaying the image. The box's size, color, and opacity are randomized to increase variety.\nText style: We customize the text's appearance by adjusting its size, color, font, and spacing, ensuring it contrasts well with the background for clear visibility."}, {"title": "A.2.2 Pure Text Pipeline", "content": "This pipeline synthesizes both digital and handwritten text images. The text corpus is sourced from Wikipedia, with digital text generated as in the image-and-text pipeline. Additional details for handwritten text and background generation are as follows:\nHandwritten text: We generate handwritten text images in SVG format following Graves (2013), offering 12 distinct writing styles. Similar to digital text, color, stroke width, line spacing, and alignment are customized to increase diversity.\nBackground: We choose two types of background image:\nNatural image. We sample images from COCO dataset and apply a blurring effect to highlight the foreground text.\nSynthetic paper image. We use Augraphy (Groleau et al., 2023) to render realistic document effect, which sequentially modifies the ink style and the background paper style to create an authentic appearance. We summarize the used pipelines in Table 4."}, {"title": "A.3 Chart Implementation", "content": "For chart visualizations, we consider bar charts, line plots, pie charts, and choropleth maps. In this section, we first explain how the bar, line and pie charts and their captions are generated generated as they share similar data sources. Then we illustrate the map visualization and caption design. Finally, we provide a comparison of our curate dataset against previous synthetic chart-text datasets. For all chart visualizations, we use Plotly (Inc., 2015). And for all caption generations, we use Llama-3.1-405B."}, {"title": "A.3.1 Bar, Line, and Pie Charts", "content": "Data source: The tabular data for visualization come from DVQA (Kafle et al., 2018) and UniChart (Masry et al., 2023). DVQA provides canonical tabular data suitable for bar and pie chart visualizations, while UniChart contains time-series tabular data for line charts.\nBar chart: The bar chart generation pipeline supports three bar types: single, grouped, and stacked bar charts. Single bar charts visualize one row of data, whereas grouped and stacked bar charts incorporate multiple rows. To enhance variety, each bar type includes the following customizations:\nBar pattern. Adjustments include bar texture, color, border, width, spacing, and the presence or absence of text on the bars.\nOrientation. Bars can be arranged horizontally or vertically.\nAxes. Customizations cover the range and tick intervals on both x-axis and y-axis.\nAnnotations and layouts. Variations include font styles, colors, and layout adjustments for titles, axis labels, and legends.\nLine chart: For line charts, we use both single-row and multiple-row time-series data, where each line corresponds to one line in a chart. Many customizations from bar charts apply here, including axes, annotations, and layouts. The primary distinction for line charts is in line pattern customization, such as line style, color, and marker pattern.\nPie chart: Pie charts use single-row data for visualization. Customizations include pie appearance adjustments, such as color, size, and display text placement. Text can be displayed inside or outside the pie; when segments are too small for text, pointers are used to indicate the designated region. Other customizations align with those used in bar and line charts.\nPrompt and caption design: For generating captions, the input to LLMs includes both data details and visualization parameters. Specifically, it incorporates axes details (type, range, and label) and additional elements like background patterns, titles, and style specifications. We instruct LLMs to create captions that summarize the chart's data, identify trends, and compare groups."}, {"title": "A.3.2 Choropleth Maps", "content": "Choropleth maps are created for four regions: European countries, global countries, the United States, and Chinese provinces.\nData source: For European and global countries, data is visualized at the country level, with each country assigned a data value. Data is sourced from Eurostat (Eurostat, 2024) and Gapminder (Bryan, 2023), or generated randomly. For Chinese provinces, we use randomly generated data, while for the United States, randomly generated state data from MapQA (Chang et al., 2022) is used.\nVisualization: Depending on the data type, choropleth maps can represent values using either a color bar for numerical data or a discrete color legend for categorical data. Each region is colored based on its value in the legend or color bar. Various visualization customizations include:\nColor pattern. Varying color schemes for regions, titles, and legends.\nProjection. Different projection methods for map rendering.\nValue dropout. Randomly omitting values for certain regions and marking these with a distinct color.\nLayouts. Randomized layout of titles, map entries, and legends.\nRegion annotation. Optional display of country/province names or acronyms within the map."}, {"title": "A.3.3 Post-processing", "content": "We filter and modify the generated responses from LLMs such that they mostly resembles caption of an image. We observe that the generated response describes the details without first identifying the type of the chart since it is provided in the context of the prompt. However, such information is not granted in real conversation. We rephrase the first sentence such that it always start with identifying the chart type presented in the image before actual captioning. For instance, \"The line chart titled \u201cxxxx\u201d visually represents...\" is rephrased into \"The image shows a line chart titled \u201cxxxx\u201d, which visually represents...\". We implement it by rule-based matching and replacement. Apart from first sentence rephrasing, we also reuse the processing strategies stated in \u00a7 A.1.2 to enhance caption quality."}, {"title": "A.3.4 Comparison with Existing Synthetic Datasets", "content": "Table 5 compares existing synthetic chart-QA datasets (FigureQA (Kahou et al., 2017), DVQA (Kafle et al., 2018), PlotQA (Methani et al., 2020), and MapQA (Chang et al., 2022)) with our chart-caption dataset. Unlike previous methods that generate templated question-answer pairs for charts, our pipeline emphasizes detailed captions.\nPrevious methods revoke a system to learn three abilities: structure understanding, data retrieval, and reasoning, often through carefully designed templates targeting a single ability. By providing precise instructions to the LLM, we enable it to generate captions that naturally integrate all three abilities. This approach not only eliminates the need for rigid templates but also encourage diversity in the generated captions."}, {"title": "A.4 Diagram Implementation", "content": "We employ Mermaid (Knsv, 2024), a JavaScript-based diagramming tool, to convert markdown text into diagram images. This tool allows us to transform text into visual representations seamlessly. Additionally, we prompt LLMs to analyze the markdown text, generating captions that not only describe each element in the diagram but also clarify relationships and provide potential insights. Figure 16 demonstrates the implementation of the pipeline."}, {"title": "A.4.1 Diagram Visualization", "content": "This section elaborates on the data source of the Mermaid codes, how they are rendered into diagram images, and customizations of the diagram style.\nData source: We acquire Mermaid code through two primary methods:\nGitHub Crawling. We collect text files containing \"mermaid\" as a keyword from licensed repositories on GitHub.\nLLM-Generated Code. We prompt LLMs to generate Mermaid code for specific diagram types such as class diagrams, ER diagrams, and flowcharts.\nWe filter the collected codes by running a rendering test, yielding approximately 3K valid diagram codes: 2K from GitHub and 1K generated by LLMs.\nRendering: Mermaid's advantage lies in its automatic optimization of spatial arrangements, ensuring diagrams display well in HTML. By simply declaring the required packages and placing the Mermaid code within the HTML body, the browser renders the diagram seamlessly. In our process, we generate an HTML file for each Mermaid code, open it in Chrome, and capture a screenshot of the rendered diagram. We use Selenium to automate this process of browsing and saving images.\nDiagram style customizations: Mermaid also offers styling parameters to customize the theme and visual appearance of rendered diagrams. These parameters can be included in the HTML header and thus separated from the main diagram content. We prompt LLMs to generate 53 styling specifications, creating a candidate set. For each HTML file, we randomly select one styling option from this set to increase visual diversity. In cases where a styling option is incompatible with a specific diagram type, the default styling is automatically applied. We retain all successfully rendered HTML files."}, {"title": "A.4.2 Prompt and Caption Design", "content": "Understanding diagrams is more challenging because they contain numerous objects and emphasize the relationships among them. Specifically, object relationships in diagrams are often more complex compared to other CI types, as they frequently use arrows or nesting to indicate directions or hierarchies. Therefore, our designed captions focus on extracting elements and relationships, placing less emphasis on the diagram's appearance details.\nTo generate captions that provide a detailed breakdown of the diagrams, we prompt Llama-3.1-405B to read the diagram code and translate it into natural language. To ensure the generated captions are as invariant as possible to the diagram's appearance, we include only the Mermaid code in the prompt, excluding any styling-related codes. We find that minimal instruction is sufficient for the LLM to accurately analyze the code.\nIn post-processing the generated captions, we first modify the opening sentence to include an identification of the diagram type, similar to our approach with chart captions. Some Mermaid code retrieved from GitHub contains style arguments like hex color codes or stroke widths for text boxes. Since the LLM interprets code, these styling details sometimes appear in their responses. For example, a box labeled \"Customer A\" might be described in the caption as \"Customer A (#a1320f, stroke width 2)\". This pattern also occurs when the diagram code assigns a shorter variable name (e.g., \"A\") to an object like \u201cCustomer A\u201d. To enhance caption quality, we refine the LLM-generated responses by removing parentheses that contain styling arguments or variable names."}, {"title": "A.5 Code Implementation", "content": "We use Carbon (Carbon", "source": "While numerous code generation datasets provide a variety of sources for code snippets", "languages": "C", "design": "Our goal is for MLLMs to first extract the code text from the screenshot, interpret it, and then provide an explanation of its functionality The code explanation can be obtained by"}]}