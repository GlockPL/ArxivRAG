{"title": "An MDP Model for Censoring in Harvesting Sensors: Optimal and Approximated Solutions", "authors": ["Jesus Fernandez-Bes", "Jes\u00fas Cid-Sueiro", "Antonio G. Marques"], "abstract": "In this paper we propose a novel censoring policy for energy-efficient transmissions in energy-harvesting sensors. The problem is formulated as an infinite-horizon Markov Decision Process (MDP). The objective to be optimized is the expected sum of the importance (utility) of all transmitted messages. Assuming that such importance can be evaluated at the transmitting node, we show that, under certain conditions on the battery model, the optimal censoring policy is a threshold function on the importance value. Specifically, messages are transmitted only if their importance is above a threshold whose value depends on the battery level. Exploiting this property, we propose a model-based stochastic scheme that approximates the optimal solution, with less computational complexity and faster convergence speed than a conventional Q-learning algorithm. Numerical experiments in single-hop and multi-hop networks confirm the analytical advantages of the proposed scheme.", "sections": [{"title": "I. INTRODUCTION", "content": "Efficient management of energy resources is essential to operate Wireless Sensors Networks (WSN) equipped with finite-size batteries and energy-harvesting devices [1]. Numerous works have designed energy-saving strategies that account for the limited and stochastic nature of the harvested energy. Many of them were aimed at solving general communication problems such as utility-based cross-layer design [2], power allocation [3], [4], or rate adaptation [5]. At the same time, in the field of WSN there has been a growing interest in strategies that take into account the importance of the information to be transmitted for the application at hand. The \u201cimportance value\u201d can be, for instance, the traffic priority of the routing protocol, the deviation from the mean in distributed estimation [6], or the likelihood ratio in decentralized detection [7]. Such strategies, sometimes referred to as selective communication [8] or sensor censoring strategies [7], assume that nodes can evaluate/quantify the importance of the current message and use it to decide whether transmitting or censoring it. To make the decision, additional parameters such as the cost of the communication, the confidence that the message will arrive to its destination, or the available energy resources, should also be taken into account. That is precisely the objective of this work: designing optimal censoring policies for an energy-harvesting scenario.\nSince the aforementioned parameters are correlated across time, decisions, which are made sequentially, should be designed to optimize the long-term behavior of the system (for example, by maximizing the aggregated importance of all messages transmitted by the WSN). From an algorithmic viewpoint, the design of such censoring policies is a Dynamic Programming (DP) problem that, under certain assumptions, can be modeled as a Markov Decision Process (MDP). The current \u201ctransmit or censor\u201d decision changes the amount of energy stored in the battery and, therefore, has an impact not only on the current battery state, but also on future ones. Therefore, efficient policies have to balance the benefits of an immediate reward with the (expected) impact of each decision on future costs/rewards. In general, the resulting DP problems are difficult to solve and approximate solutions are often required [9], [10].\nAssuming a \u201cpredictable\u201d setting where one knows not only the past, but also the future values of the state information (e.g., energy to be harvested in the future instants), optimal off-line decision policies can be applied [11], [12]. Although, such schemes serve as a benchmark, they do not cope well with many practical scenarios, where energy and packet-arrival processes are not known in advance. Consequently, many works in the literature are MDP based. In [13] a policy iteration algorithm [9] is used to estimate the decision policy maximizing the long-term average reward, for a dual recharge/replace battery harvesting model with unitary transmission costs. The scheme in [14] is also based on unitary costs and an average reward optimization. This work and its later extension in [15] show that (a) the optimal transmission policy applies a threshold over the importance values that is a decreasing function of the available energy, and (b) a balanced policy is close to optimal. The balanced policy is a simple scheme, also analyzed in this paper, that takes into account the long-term distribution of the energy harvested and ignores the instantaneous battery level [15], [16].\nThe main drawback of these approaches is that they need to know the distributions of the energy and packet arrival processes, which may not be available in practical scenarios. In such cases, nodes have to be able to learn whatever information is needed in real time. One approach to handle this problem, recently proposed in [17], is to apply Q-learning (a stochastic method widely used in the context of reinforcement learning [9], [10]) to solve the DP problem. But this approach has its own problems. Since Q-learning is a model-free algorithm that tries to estimate the value function in a non-parametric manner, it can result in a slow convergence and high computational complexity when the size of the state space grows (the problem is even more severe if the state space is continuous).\nIn this paper, we propose a model-based stochastic approximation algorithm to solve the previous difficulties. More specifically, we show that, under reasonable assumptions on the battery dynamics, the optimal censoring policy is a threshold function on the importance value. An ad-hoc stochastic approximation algorithm exploiting this property is proposed that, compared with a conventional Q-learning algorithm, is more efficient in terms of computational complexity, memory requirements and convergence speed.\nA similar approach for the case of censoring sensors with non-rechargeable batteries was proposed for single-hop communications [8], and for the optimization of multi-hop networks [16], [18]. Compared to our previous work in scenarios with non-rechargeable batteries [8], [16], the results in this paper reveal that the harvesting scenario is substantially different. While, in a non-rechargeable case, a censoring policy discarding messages whose importance is below a fixed threshold is quasi-optimal, censoring policies based on energy-dependent thresholds are significantly more efficient in harvesting sensors.\nOur experiments, based on a battery model that copes with more general stochastic transmission and reception costs, and also for stochastic harvesting patterns, show that energy-dependent transmission policies can be more efficient than balanced policies. Our use of a penalized discount with infinite horizon [9] is also more appropriate to cope with non-stationary environments. As in almost every approach in the literature, our MDP model optimizes the performance of each node separately, which is guaranteed to be globally optimal only for single-hop scenarios. However, by using a success index [16] to model the behavior of neighboring nodes, MDP models at different nodes get coupled and, as a result, a network of MDP-based sensors can significantly outperform networks where nodes implement balanced policies.\nThe remaining of the paper is organized as follows. Section II describes the WSN model. Section III states the main theoretical results. Section IV considers different simplifying assumptions to gain insights on the optimal solution. Section V develops low-complexity stochastic schemes that approximate the optimal solution and are robust to non-stationarities. Section VI evaluates the proposed schemes using numerical simulations. Conclusions in Section VII close the paper."}, {"title": "II. SYSTEM MODEL", "content": "In this section, we introduce notation, explain the mathematical model that describes the dynamics of our system under a censoring policy, and formulate the objective to be optimized.\nThe model is defined by four main components: a) a set of state variables, b) a set of possible actions, c) a probabilistic model of the state dynamics (that describes how future states depend on the current state and the actions taken), and d) a reward model (that describes the immediate reward obtained when some action is taken at a given state). As explained in the introduction, since we are interested in maximizing the long-term reward and current actions have an impact into the future states, our problem will fall into the DP framework. Moreover, because the state dynamics are assumed Markovian, the problem will be modeled as an MDP.\nConsider a node that receives a sequence of requests to transmit different messages. The messages can be received from another node or generated from local measurements. The state of the node will be characterized by two variables\n\u2022 ek: the battery level at step (slot) k. It reflects the \"internal state\" of the node,\n\u2022 xk: the importance of the message to be sent at step k.\nFollowing the typical terminology in MDP models, the state vector of the node is defined as sk = (ek, xk); i.e., the state vector contains all and only the information that is available at the node to make a decision at time k. The set of all possible states is denoted as S.\nTo facilitate exposition, k is considered an epoch or slot index, which starts when the node has to decide whether to censor or transmit a message (either received from one of its neighbors or generated from its sensing devices) and ends when the next message is received. Besides transmitting or censoring the message, during each epoch the node (eventually) collects some energy from the environment. This approach, which is used by many authors, implies that the actual duration of each slot k is stochastic. Clearly, the results in the paper also hold true if the system operates with a constant sampling period (the only modification required is to set xk = 0 for the time instants k where no message has been received).\nNote that, besides ek and xk, the node could use additional information to make decisions. This information can be also local (the packet length, the state of the communication channel) or pertain to other (neighboring) nodes. Additional local information can be easily incorporated into the formulation, provided that the state dynamics are similar to those of ek and xk; see, e.g., [16]. Incorporating information about the state or the eventual actions of neighboring nodes (e.g., battery levels, or information about their censoring policy) will lead to a better network operating point, although it raises issues such as the accuracy and the cost of acquiring the non-local information (exchange of information requires, for example, additional energy consumption). Since the paper investigates the design of separate (per-node) censoring policies, we will focus on local information. As we will see in Section III, the success index variable defined in Section II-D could be used as a means to couple the decisions across the network; see [16] for details."}, {"title": "B. Actions and policies", "content": "At each time epoch k, the sensor node must take an action (decision) ak about sending the current message (ak = 1), or censoring it (ak = 0). A forwarding policy \u03c0 = {a1, a2,...} at a given node is a sequence of decision rules, which are functions of the state vector; i.e.,\n$$a_k = \\pi_k(s_k) = \\pi_k(e_k, x_k).$$\n(1)"}, {"title": "C. State dynamics", "content": "Next, we describe the model for the stochastic processes ek and xk that form the state vector.\nThe energy consumed at each time epoch depends on the taken action. Let cost \u0109k denote the energy consumed by the node and bk be the amount of energy (if any) harvested by the node since the last action ak\u22121. Then, ek+1 can be written recursively as ek+1 = \u0424\u0432(\u0435\u043a \u2013 \u0109k + bk), where \u0444\u0432(\u0435) = max(0, min(e, B)) is a clipping (projection) function that guarantees that the energy stored in the battery is never negative, nor exceeds its maximum capacity B.\u00b9\nCost \u0109k may include the cost of data sensing (if the sensor is the source of the message), the cost of data reception (when data come from other nodes), the cost of idle periods, or whatever other costs incurred since the last action. When ak = 1, \u0109k includes the previous costs plus the cost of transmitting the message. This statistical model allows us to deal with a broad range of scenarios: stochastic packet arrivals, communications over fading channels, packet losses, automatic repeat request (ARQ) schemes, to name a few. In those networks, the energy consumption during node communications can vary depending on the amount of retransmissions required for a successful packet arrival. The range of values and statistical model for bk depend on both the type of harvesting device and the source of energy considered [19]. To simplify notation, we define Ck = \u0109k - bk, so that battery dynamics can be rewritten in a more compact form as\n$$e_{k+1} = \\Phi_B(e_k - C_k).$$\n(2)\nNote that high values of harvested energy can render Ck negative."}, {"title": "D. Rewards", "content": "The reward at time k is given by\n$$r_k = a_k w_k x_k,$$\n(3)\nwhere wk \u2208 {0,1} denotes the success index (a binary variable taking value 1 if the transmission is successful, and zero otherwise). Thus, the reward rk that each node receives is a positive value xk if and only if it decides to transmit the message (ak = 1) and the transmission is successful (wk = 1). Otherwise, the reward is zero.\nThe meaning of the success index wk depends on the application scenario. In general, a reasonable choice is to set wk = 1 if and only if the message is properly received at its final destination. However, in multi-hop networks, the information about the reception of messages at the sink node may not be available to all nodes along the route. In such a case, other (suboptimal) choices for wk are possible. For instance, in [16] it is shown that setting wk = 1 if the neighboring node forwards the transmitted message can be nearly as efficient as using the actual information from the"}, {"title": "E. Problem formulation", "content": "Our transmission policies will be designed so that the expected aggregate reward is maximized. Following a standard approach in DP, the \u201cso-called\" discount factor 0 < \u03b3 < 1 is considered [9] and, based on it, the expected aggregate reward is defined as\n$$V(s_0) = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k r_k \\mid s_0 = s \\right\\} = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k a_k w_k x_k \\mid s_0 = s \\right\\}.$$\n(4)\nThe optimal transmission policy is then\n$$\\pi^* = \\arg \\max_\\pi V.$$\n(5)\nNote that only messages successfully transmitted by the nodes are relevant in (4). Eq. (4) states a DP problem with infinite horizon (because all future time instants are considered) and discounted cost (due to the presence of y which penalizes future rewards exponentially) [9]. Indeed, V(s) is typically referred to as either value function or reward-to-go function. Mathematically, the presence of y eases the existence of a stationary policy that optimizes (4); see, e.g., [9]. Additional details will be given in the ensuing section."}, {"title": "III. OPTIMAL STATIONARY POLICY", "content": "This section is devoted to design stationary solutions that solve the DP formulated in Section II.E. Since the objective in (4) depends on the stochastic processes xk, Ck, and w\u03ba, assumptions on the stationarity of such processes are required. The relationships among the main variables in the MDP are represented in the graphical model in Fig. 1. Arrows in this model encode direct causal relationships between variables: the action is a function of the state, the success index depends on the state only (and also on ak, which is a deterministic function of the state), the energy consumed or harvested depends on the action taken, and the energy at the next state depend on the current battery level and the energy consumed or harvested at time k. The model assumptions underlying the graphical model representation and that will be used in our analysis, are the following:\nas1) the process Ik \u2265 0 is independent, identically distributed (i.i.d.) and independent of ek;\nas2) Ck is independent of xk, ek and all its previous history, given the action, ak, and p(ck|ak) does not depend on k;\nas3) wk is independent of all its previous history, given ek and xk, and p(wk|ek, xk) does not depend on k.\nSome independence assumptions may be oversimplifying for some applications: in particular, the independence of the importance values can be non realistic in scenarios where consecutive sensor measurements are correlated. Also, the harvested energy can be time-correlated when it depends on environmental variables that span over several epochs (on the other hand, the energy harvested by wind sensors is oftentimes modeled as i.i.d. [20]). Nonetheless, it is worth mentioning that: i) incorporating time-dependence into our model (while preserving Markovianity) does not state special difficulties, though it would imply some extra computational load and memory requirements, ii) the independence of ck with respect to xk or ek can be relaxed without entailing a big penalty in terms of complexity [16]; iii) due to the presence of the discount factor y, stationarity can be relaxed to short-term stationarity (more specific comments will be provided in this section after presenting the optimal solution); and iv) the stochastic schemes proposed in Section V will be able to handle non-stationarities.\nUnder the previous assumptions and due to the recursive definition of ek given by (2), the state dynamics are Markovian. Hence, the tuple (S, A, P,r), where S is the set of states, A = {0,1} is the finite set of possible decisions (actions), P is the transition probability measure that can be expressed as p(Sk+1|Sk,ak) = p(ek+1|ek, ak)p(xk+1), and r is the instantaneous reward function, constitutes an MDP. As a result the existence of a Markovian and stationary optimal policy \u03c0* is guaranteed [9], [21].\nClearly, the value function associated with this policy needs to satisfy the Bellman's optimality equation [9]\n$$V^* (s) = \\max_{a \\in \\{0,1\\}} \\mathbb{E} \\left\\{ r_k + \\gamma V^{\\pi^*}(S_{k+1}) \\mid a_k = a, s_k = s \\right\\},$$(6)\nwhich can be used to obtain the optimal decision rule. This is accomplished by Theorem 1. All expectations in the following are computed over xk, Ck and wk (which are the primary random variables in the model), unless otherwise stated through the conditional operators.\nTheorem 1: Under as1-as3, it holds that (4) is maximized by a stationary policy ak = \u03c0*(s) satisfying\n$$a_k = u\\left( \\mathbb{W}(e_k, x_k) x_k - \\mu(e_k) \\right),$$(7)\nwhere u is the Heaviside step function, W(e, x) = E{wk|ek = e, xk = x} is the success probability and the threshold function \u00b5 is defined recursively through the pair of equations\n$$\\mu(e) = \\gamma \\left( \\mathbb{E} \\left\\{ \\lambda(\\Phi_B(e - C_k)) \\mid a_k = 0 \\right\\} - \\mathbb{E} \\left\\{ V^{\\pi^*}(\\Phi_B(e - C_k)) \\mid a_k = 1 \\right\\} \\right),$$(8)\n$$\\lambda(e) = \\gamma \\mathbb{E} \\left\\{ \\lambda(\\Phi_B(e - C_k)) \\mid a_k = 0 \\right\\} + \\mathbb{E} \\left\\{ \\left( \\mathbb{W}(e, x_k) x_k - \\mu(e) \\right)^+ \\right\\},$$(9)\nwith (z)+ = max{z,0}, for any z."}, {"title": "IV. ANALYSIS OF THE OPTIMAL POLICY", "content": "In this section, we will analyze different aspects of the op-timal policies. In the first subsection, we will derive recursive expressions to compute those optimal policies. In the second subsection, we will obtain the steady-state energy distributions and assess their impact on the optimal policies. To facilitate the computation of the optimal schemes, we will consider three simplifying assumptions: AS1) Process wk does not depend on xk. AS2) The energy variables are discretized, so that ek, Ck and B are integer values. As a result, the energy space is approximated by a finite space, but the approximation error can be minimized by choosing the energy resolution \u03b5 small enough, though at the expense of increasing the memory requirements and the computational complexity. Discretization is a widely used approach to deal with continuous-state DPs. AS3) The success probability can be written as\n$$\\mathbb{W}(e) = P\\{ C_k \\leq e | a_k = 1 \\}$$(11)\nIn words, the transmission is successful if the node has energy enough to transmit the message. This is the case if, for example, the communications are error free. In the presence of path losses, (11) also holds if the message is retransmitted until a confirmation is received -the path-loss probability would modify the distribution of the energy cost ck, but not the formal expression in (11). Alternatively, if retransmissions are finite (or zero) path losses can be accommodated by just multiplying the right-hand side of (11) by the packet loss probability. This equation is specially suited to single-hop communications. In multi-hop networks, it is suboptimal because it does not consider whether the message is eventually forwarded through the network up to the sink. Nonetheless, the equation decouples variables across nodes, simplifying the derivation of analytical expressions that can be useful even for scenarios where it entails a loss of optimality.\nNote that these assumptions are only needed to have a simple scenario where the optimal policy can be computed, and consequently get some insights on its structure and performance. Once that goal is achieved, they will be relaxed in the following sections."}, {"title": "A. Optimal policies for particular cases", "content": "Under assumptions AS1 and AS2, the selective transmitter given by (7), (8) and (46) can be described by the following set of discrete equations\n$$a_k = u\\left( \\frac{\\mathbb{W}(e_k) x_k}{\\mu(e_k)} - 1 \\right)$$\n(12)\n$$\\mu(e) = \\gamma \\mathbb{E} \\left\\{ \\lambda(\\Phi_B(e - c)) | a = 0 \\right\\} - \\gamma \\mathbb{E} \\left\\{ V^*(\\Phi_B(e - c)) | a = 1 \\right\\}$$\n(13)\n$$\\lambda(e) = \\gamma \\mathbb{E} \\left\\{ \\lambda(\\Phi_B(e - c)) | a = 0 \\right\\} + \\mathbb{W}(e) h\\left( \\frac{\\mu(e)}{\\mathbb{W}(e)} \\right)$$\n(14)\nwhere\n$$h(a) = \\mathbb{E}\\left\\{ (x - a)^+ \\right\\}.$$\n(15)\nNote that since ck and ak are stationary, subindex k has been dropped to simplify notation.\nEven in this simplified scenario, (13) and (14) cannot be solved analytically, so that neither the \"reduced\" value function nor the transmission threshold can be found in closed form. However, the considered assumptions reduce the size of the state space and thus, facilitate the implementation of iterative methods, such as value iteration and policy iteration [9], [10], [21]. For example, using Value Iteration and with l denoting an iteration index, the optimal schemes can be found using the iterations\n$$\\mu_l(e) = \\gamma \\mathbb{E} \\left\\{ \\lambda_{l-1}(\\Phi_B(e - c)) | a = 0 \\right\\} - \\gamma \\mathbb{E} \\left\\{ \\lambda_{l-1}(\\Phi_B(e - c)) | a = 1 \\right\\}$$\n(16)\n$$\\lambda_l(e) = \\gamma \\mathbb{E} \\left\\{ \\lambda_{l-1}(\\Phi_B(e - c)) | a = 0 \\right\\} + \\mathbb{W}(e) h\\left( \\frac{\\mu_l(e)}{\\mathbb{W}(e)} \\right),$$(17)\nwhere \u00b50(e) and (e) are the arbitrary initial values.\nTo gain some insights, next we solve numerically and ana-lyze the optimal solution for different scenarios with stochastic energy costs. We consider the case when E{ca = 0} < 0, which implies that nodes can discard messages to recharge batteries during operation and, thus, the lifetime can be ex-tended indefinitely. To be more meaningful, we will assume a scenario where each decision epoch can be split into a variable number of fixed-duration time slots, and variable c can be decomposed as\n$$c = n_s \\cdot C_I + C_R - b + a \\cdot \\Delta,$$\n(18)\nwhere ns is the number of time slots since the last decision, CI is the (stand-by) energy consumed during each time slot, CR is the cost of receiving or sensing the current message, b is the amount of battery recharged since the last node decision, a is the action, and \u0394 is the incremental cost of deciding a = 1. We assume a lossy channel where retransmission trials are repeated until the message is successfully received at destination. Thus,\n$$a = n_T c_T,$$\n(19)\nwhere cT is the cost of each transmission trial and nT is the number of transmission trials.\nWe have simulated a scenario where CR = 2, CI = 1 and ns follow a geometric distribution with mean 2. We assumed a very poor channel, so that transmission trials fail with probability 0.4. The cost of each transmission trial is set to 4. This configuration tries to simulate WSN configurations where the energy cost of transmitting a message is substantially higher than that of sensing or receiving a message. The amount of battery recharged, b, is also stochastic. We assume that the amount of battery recharge can be decomposed as $$b = \\sum_i b_i$$ where bi are i.i.d. variables accounting for the battery recharged at each time slot i. During each time slot, the probability of a nonzero battery recharge is pb = 1/3, and, when bi > 0, bi is geometrically distributed with mean mb. Three different values of mb, namely, 5, 10 and 15, have been explored. For these values, the corresponding values of\n$$c_0 = \\mathbb{E}\\{ c | a = 0 \\}$$\n(20)\nare -0.1, -3.4 and -6.7, respectively. Finally, an i.i.d. expo-nential importance distribution with unit mean was assumed, and \u03b3 = 0.999."}, {"title": "B. Steady-state distributions", "content": "In this section, we will focus on asymptotic behavior, so that the effects of the initialization are disregarded.\nUsing elementary Markov Chain properties, it can be shown that, under some general conditions, the statistical distribution of the battery level converges, as k goes to infinity, to a distribution \u03a6 that is the solution of (I \u2013 P) = 0 subject to \u03a6i \u2265 0 and \u03a3i=0 \u03a6i = 1, where P be the transition probability matrix with entries\n$$P_{ij} = P\\{ e_k = j | e_{k-1} = i \\}, \\; i, j = 0, ..., B$$\n(22)\nand where, for notational convenience, we started the matrix indexing at 0.\nUsing the stationary distributions, the expected performance of a selective transmitter can be computed as\n$$V^* = \\lim_{k \\rightarrow \\infty} \\sum_{t=k}^{\\infty} \\gamma^{t-k} \\mathbb{E} \\{ a_t w_t x_t \\}.$$\n(23)\nLeveraging AS3, and using (7), (11) can be substituted into (23) to yield,\n$$V^* = \\frac{1}{1 - \\gamma} \\mathbb{E}\\{ a_t w_t x_t \\} = \\frac{1}{1 - \\gamma} \\sum_{e=0}^{B} \\mathbb{E}\\{ a_t w_t x_t | e_t = e \\} \\phi_e$$\n(24)\nTaking into account that w does not depend on xt, at and e, the expectation in the sum can be computed as\n$$\\mathbb{E}\\{ a_t w_t x_t | e_t = e \\} = \\mathbb{E}\\{ w_t x_t | a_t = 1, e_t = e \\} P\\{ a_t = 1 | e_t = e \\}$$\n$$= \\mathbb{E}\\{ w_t | a_t = 1, e_t = e \\} \\mathbb{E}\\{ x_t | a_t = 1, e_t = e \\} \\cdot P\\{ a_t = 1 | e_t = e \\}$$\n$$= \\mathbb{W}(e) \\mathbb{E}\\{ x_t | a_t = 1, e_t = e \\} P\\{ a_t = 1 | e_t = e \\}$$\n$$= \\mathbb{W}(e) \\mathbb{E}\\{ x_t u\\left( \\frac{\\mathbb{W}(e)}{\\mu(e)} x_t - 1 \\right) | e_t = e \\}$$\n(25)\nDefining function g as\n$$g(\\mu) = \\mathbb{E}\\{ u(x - \\mu) x \\} = h(\\mu) + \\mu(1 - F_x(\\mu))$$\n(26)\nand substituting (25) into (24) we arrive at\n$$V^* = \\frac{1}{1 - \\gamma} \\sum_{e=0}^{B} g\\left( \\frac{\\mu(e)}{\\mathbb{W}(e)} \\right) \\mathbb{W}(e) \\phi_e.$$\n(27)"}, {"title": "V. STOCHASTIC APPROXIMATE SCHEMES", "content": "The analysis in the previous sections provided insights on the behavior of optimal harvesting sensor nodes. However, the optimal policies presented so far are computationally very expensive, so that they can not be easily implemented in real time by sensors with limited computational capabilities. In this section, we present different ways to develop suboptimal adaptive stochastic schemes that reduce the computational complexity and, additionally, are able to deal with non-stationarities."}, {"title": "A. A stochastic approximation to the optimal policy", "content": "The threshold-based optimal policy presented in Section III stands on two main assumptions on the energy dynamics: (a) neither the energy consumption nor the recharge depend on the importance value, but only on the taken action, and (b) in the linear regime (i.e., with the exception of the battery saturation points) the variation of the energy stored in the battery does not depend on the current battery level.\nThe main difficulty to obtain the optimal solution using the value iteration method proposed in (16) and (17) is the computation of the expectations involved. But if we assume again that ek is discrete\u00b2, they can be stochastically approximated in a sample-based manner.\nIn order to do so, we will represent the policy as an instance of Robbins-Monro algorithm [23] and use stochastic approximation techniques to get our algorithm. First of all, we will decompose cost ck as\n$$c_k = c_{0,k} + a_k \\Delta_k,$$\n(30)\nthat is, co,k represents the energy consumption when a message is censored (\u03b1\u03ba = 0) and Ak represents the incremental cost of transmitting a message. In addition, it is useful to write (16) and (17) in matrix form. Let us define vectors \u03bb = (x(0), \u03bb(1),..., (B)), and w = (W (0), ...,W(B)), and the vector of success indices wc = (u(0 \u2013 \u0441), \u0438(1 - c),...,\u0438(\u0412-\u0441))T. We assume in the following that vectors are indexed from 0, in such a way that, for instance, Xe = x(e). Also, we define the transformation X' = Tex such that Xe = \u03bb\u03c6\u03b2(e-c) In Appendix C we derive the following adaptive rules as an instance of Robbins-Monro algorithm [23].\n$$w_{k+1} = (1 - \\eta_k) w_k + \\eta_k w_{c_{0,k} + \\Delta_k}$$\n(31)\n$$\\alpha_{k+1} = (1 - \\eta_k) \\alpha_k + \\eta_k T_{c_{0,k}}$$\n(32)\n$$\\beta_{k+1} = (1 - \\eta_k) \\beta_k + \\eta_k T_{c_{0,k}+ \\Delta_k}$$\n(33)\n$$\\Lambda_{k+1} = (1 - \\eta_k) \\Lambda_k + \\eta_k \\left(\\gamma \\alpha_k + \\left( \\omega_k x_k - \\gamma (\\alpha_k - \\beta_k) \\right)^+ \\right)$$\n(34)\nwhere \u03b7e stands for the learning stepsize, which can be set either to diminish with time (for instance in stationary scenarios where one wants \u03bc\u03b5 to converge to a fixed function) or to a small constant (for adaptation to changes in non-stationary scenarios). The stepsize must be chosen in order to balance a good convergence speed and a low steady-state error, but always satisfying the Robbins-Monro stepsize conditions for convergence [23].\nNote that, at each iteration, the above rules update all components of vector A, i.e., the whole estimate of A(e) is updated for all values of e. Thus, the computational load and memory requirements grow linearly with the number of discrete energy values. Neither the importance nor the energy distribution are required to be known to use (31), (32) and (33). Instead, they are run every time a sample of Co,k or Ak is observed. Regarding the observability of co,k and Ak some remarks are in order."}, {"title": "B. Q-learning", "content": "An alternative design is to use universal stochastic approx-imation methods that do not require any assumption on the state dynamics, like Q-learning [25]. They can be expected to outperform SAP in scenarios where the above assumptions are too unrealistic. However, there is a price to pay for this flexibility. The SAP algorithm leverages the structure of the optimal decision to reduce the search space and speed up con-vergence. On the other hand, Q-learning has to compute a Q value for each possible action and state (energy and importance value); as a consequence, the memory requirements may be too high. Furthermore, at each iteration, the Q-learning algorithm only updates the estimate of the value functions at the current state, and though convergence can be theoretically guaranteed, it requires to visit all possible states infinitely often [26]. In practice, for large state spaces, convergence to the optimal solution is difficult and learning time is much larger than that of model-based approaches as it will be shown in the numerical experiments in Section VI.\nOur Q-learning implementation is a minor variation of the algorithm proposed in [17, Eq. (8)] for a similar application. In order to be able to apply it to our setup, we quantized the importance value, which is a real number, into a number of levels (standard Q-learning needs a discrete state space), and apply the algorithm in [17]. This algorithm has two free parameters the learning rate ak, and e the exploration probability in the e-greedy action selection method."}, {"title": "C. Adaptive balanced transmitter", "content": "A further step to decrease computational complexity is to restrict"}]}