{"title": "HM3: Heterogeneous Multi-Class Model Merging", "authors": ["Stefan Hackmann"], "abstract": "Foundation language model deployments often include auxiliary \"guard-\nrail\" models to filter or classify text, detecting jailbreak attempts, biased\nor toxic output, or ensuring topic adherence. These additional models in-\ncrease the complexity and cost of model inference, especially since many\nare also large language models. To address this issue, we explore training-\nfree model merging techniques to consolidate these models into a single,\nmulti-functional model. We propose Heterogeneous Multi-Class Model\nMerging (HM3) as a simple technique for merging multi-class classifiers\nwith heterogeneous label spaces. Unlike parameter-efficient fine-tuning\ntechniques like LoRA [14], which require extensive training and add com-\nplexity during inference [12][14], recent advancements allow models to be\nmerged in a training-free manner [29][33][10]. We report promising re-\nsults for merging BERT-based guard models [7], some of which attain an\naverage F1-score higher than the source models while reducing the infer-\nence time by up to 44%. We introduce self-merging to assess the impact\nof reduced task-vector density, finding that the more poorly performing\nhate speech classifier benefits from self-merging while higher-performing\nclassifiers do not, which raises questions about using task vector reduction\nfor model tuning.", "sections": [{"title": "1 Introduction", "content": "Production machine learning systems are increasing in complexity. For exam-\nple, a state-of-the-art generative AI pipeline typically contains multiple sophis-\nticated models working together. A chatbot may have a foundation model like\nGPT-4 as the main component but be accompanied by a constellation of aux-\niliary \"guardrail\" models for detecting jailbreak attempts, and biased or toxic\noutput. Other auxiliary models may include application-specific models like sen-\ntiment detectors, task-completion detectors, etc. Similarly, mixed-reality action"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Model Merging", "content": "Model merging is a class of techniques to merge model capabilities from different\nmodels without any further training. For a broad overview of these techniques,\nwe refer to [18]. Model Soup [27] is a straightforward but powerful strategy,"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Ensembling", "content": "Model merging is distinct from model ensembling [3] [19] in that it produces a\nsingle model rather than combining the outputs of several models. Ensembling\ntext classifiers has been studied by various authors, for example Mohammed and\nKora [20] who propose an ensemble deep learning framework for text classifica-\ntion and compare the proposed ensemble with other methods. However, these\nmethods do not support heterogeneous text classification tasks with distinct\nclassification semantics or output label spaces. LLMs like GPT-4 [22], Claude 3\n[1] and many others can be prompted to act as classifiers and can be ensembled,\nbut they are slow and expensive to run."}, {"title": "3.2 Multi-Task Learning", "content": "Model merging is related to the wider and established field of multi-task learn-\ning (MTL) [4], where machine learning models are trained to complete multiple,\nrelated tasks in order to improve the model's performance across one or more\nof those tasks. Model merging has a comparable effect but is conducted af-\nter training. Zhang and Yang [34] provide a survey on MTL. Not surprisingly,\nMTL can also be regarded as multi-objective optimization [24]. Bhattacharjee"}, {"title": "3.3 LORA", "content": "An important alternative technique for extending model capabilities without full\nfine-tuning is low-rank adaption of LLMs (LoRA) [14]. Instead of fine-tuning\nthe weights of the base model, LoRA injects a reduced number of trainable\nweights into each layer of the extended LLM while keeping the original models'\nweights fixed. LoRA increases the number of parameters required for infer-\nence, but it has a negligible impact on latency because the additional weights\ncan be evaluated in parallel. Two popular systems for serving LoRA-adapted\nLLMs are vLLM, introduced by Kwon et al. [17], and S-LoRA, introduced by\nSheng et al. [25]. S-LoRA can handle a large collection of concurrent LORA\nadapters, significantly increasing throughput compared to vLLM, but does not\nenable multi-task inference. In this work, we focus on pushing the boundaries of\ntraining-free model merging methods, which do not require additional parame-\nters and enable multi-task inference."}, {"title": "4 Heterogeneous Multi-Class Model Merging", "content": "In our case-studies, we use Hugging Face as a resource for text classifiers. We\nmerge models that are fine-tuned versions of the same base model but adapted\nto heterogeneous label spaces. The merging strategies applied here require that\nthe merged models have the same architecture. Some merging strategies like\nTIES also require the common base model from which the fine-tuned models\nwere fine-tuned in order to compute task vectors. The base models, fine-tuned\nmodels, and merged models can all have different output spaces and semantics.\nIn order to be able to merge these models, we expand the final classifier layer\nof the models that we want to merge with zeros and adjust the base model\naccordingly, see Algorithm 1. Formally, we can describe the pre-processing step\nHM3 as follows.\nDefinition 1. Let $F = {f_1,..., f_n}$ be a collection of N text classifiers, where\n$f_i$ maps a text x to $K_i$ classes:\n$f_i :x \\rightarrow (P_{i,1},..., P_{i,K_i}),$\n(1)\nsuch that $\\sum_{j=1}^{K_i} P_{i,j} = 1$, for all i $\\in$ {1, ..., N}. Here, $p_{i,j}$ is the probability that\nthe input belongs to the j-th class of the i-th classifier. Each $f_i$ consists of a\ntokenization step $\\tau$ that maps the text x to tokens $(x_1,...,x_T)$, a forward pass\nby the model $m_i$ that converts the tokens to logits\n$m_i: (x_1,...,x_T) \\rightarrow (l_{i,1},..., l_{i,K_i})$\n(2)"}, {"title": "5 Case Studies", "content": "We conducted two case studies where we merge fine-tuned text classifiers with\nmergekit [9] that have been modified with HM3, see Section 4. We explore\nseveral merging strategies: Model Soup, TIES, and DARE-TIES, see Subsec-\ntion 2.1. When using HM3 with DARE-TIES, we sample the task vector density\nfrom a beta distribution with a = 1.2 and \u1e9e = 2, see Figure 2. The scatter plot\nin Figure 3 is one example that shows how test result scores can depend on the\ntask vector density. The elevated variance in the distribution of scores justifies\nthe implementation of a search process, see Algorithm 3."}, {"title": "5.1 LLM Moderation", "content": "Data engineers, machine learning en-\ngineers and prompt engineers go to\ngreat length to make LLMs safer and\nyet, we are still far from done. See,\nfor instance, Naveed et al. [21] for an\nintroduction to the field of LLMs in\ngeneral and Cui et al. [6] for related\nrisks. A starting point for risk mitiga-\ntion is the selection of training data.\nAnthropic puts an emphasis on safety\nand quality of training data [26], for\ninstance. Similarly, you could try to\nprevent harmful data from entering\nyour RAG-system. To make models\nless vulnerable to certain types of at-\ntacks and make them respond more\nlike humans would prefer, fine-tuning\n[21] and reinforcement learning from\nhuman feedback (RLHF) [5] are used\nto further improve pre-trained models\nand make them safer. Additional instructions in the model's context can also\nprovide an extra bit of safety [23] [16], for instance, you can reduce the overall\nrisk by instructing a model to stay on topic because many attacks will not be\naligned with the topic. All those efforts are important but still do not prevent\nLLMs from producing harmful content in certain situations. Here, we are par-\nticularly interested in another layer of protection that we aim to apply to make\nmodels safe enough to use them in a professional or educational environment:\nspecialized guard models are used to analyze the data that is entering a model\nor is produced by a model [8]. Those guard models are often text classifiers\nbut could also be image classifiers or LLMs themselves. Typical examples are\njailbreak, toxic speech and phishing classifiers.1"}, {"title": "5.4 Self-Merging", "content": "The best HM3-based models have been produced with DARE-TIES, using a low\ndensity. We were asking ourselves \"Can random resetting of task-vectors with-\nout merging with a different model already improve a fine-tuned model or is the"}, {"title": "6 Limitations and Future Directions", "content": "The evaluated BERT-models can process only 512 tokens at once. Longer se-\nquences should be split and evaluated separately in practice.\nIt would have been interesting to merge DeBERTa [13] models and try\nAdaMerging [30]. This is something we may add at a later point.\nWe discussed two interesting cases in detail, however, not every constellation\nof models can easily be merged into a well-performing single model. For instance,\nwhen we merged three fine-tuned versions of DistilBert base uncased, most\ntexts that test the emotion classifier were classified as \"anger\", see Figure 7.3"}, {"title": "7 Conclusion", "content": "We demonstrated that Heterogeneous Multi-Class Model Merging (HM3) can\nbe used to merge several text classifiers with different output labels into a sin-\ngle classifier such that the resulting classifier supports all labels. We found that\nusing one such merged model instead of several individual models requires signif-\nicantly less compute time. This has a positive impact on costs, energy consump-\ntion and the environment. We observe that the quality of the merged model is\noften comparable to the original models, and sometimes even better. We get the\nbest merging results with HM3 followed by DARE-TIES and a low task-vector\ndensity. Synergies between different merged models that have been fine-tuned\non different training data cannot be the only reason for the increased perfor-"}, {"title": "9 Appendix", "content": ""}, {"title": "9.1 Comparing Model Accuracy", "content": ""}, {"title": "9.2 Confusion Matrices", "content": ""}, {"title": "9.3 Self-Merging", "content": ""}, {"title": "9.4 Testing a Merged Text-Classifier", "content": "Figure 30: When testing a merged text-classifier, for instance, this merged\nguard model, we would like to evaluate our test datasets an all available output\nlabels. Where this is possible, we compare the performance of the merged model\nwith the performance of the corresponding original model. However, in some\nsituations, we do not know the expected label and the corresponding test field\nremains blank. For instance, we could find that the output label for phishing\nshows a high score/probability when giving a phishing example and use this\nto compare with the corresponding original model but we do not know how to\ninterpret the sentiment scores in this case."}]}