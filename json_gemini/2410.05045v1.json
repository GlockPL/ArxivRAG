{"title": "Can LLMs plan paths with extra hints from solvers?", "authors": ["Erik Wu", "Sayan Mitra"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, mathematical problem solving, and tasks related to program synthesis. However, their effectiveness in long-term planning and higher-order reasoning has been noted to be limited and fragile. This paper explores an approach for enhancing LLM performance in solving a classical robotic planning task by integrating solver-generated feedback. We explore four different strategies for providing feedback, including visual feedback, we utilize fine-tuning, and we evaluate the performance of three different LLMs across a 10 standard and 100 more randomly generated planning problems. Our results suggest that the solver-generated feedback improves the LLM's ability to solve the moderately difficult problems, but the harder problems still remain out of reach. The study provides detailed analysis of the effects of the different hinting strategies and the different planning tendencies of the evaluated LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Planning is one of the classical robotics problems that requires both step-by-step decision making and world knowledge [1]. Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, mathematical problem solving, and tasks related to program synthesis, using only implicit knowledge about the world in the form of large volumes of data. LLM's can be viewed as approximate knowledge retrieval engines trained on Internet-scale data, and there has been several works that claim that the embedded knowledge in the network can solve planning tasks directly or through appropriate prompting [2], [3], [4]. A less optimistic view is presented in [5] which reported that GPT-4 solves about 30% of Blocksworlds puzzles which requires the LLM to generate a sequence of pick and place actions to reach a target configuration of blocks from an initial disorganized state; Gemini Pro had a success rate of only 0.5%. A flurry of follow-on works have studied the capabilities of LLMs in Blocksworld and other common household planning benchmarks [4]. Chain-of-thought (COT) prompting, which supposedly \"unlocks the LLMs abilities to reason\" [6], has been applied to planning in [4] with some improvements. However, in [7] the authors reaffirm the findings of [5] showing that meaningful performance improvements from COT prompts appear only when those prompts are hyper-specific to the problem, and the improvements quickly deteriorate as the number of blocks grow. See Section IV, for a discussion of other related work on path planning and LLMs. In this paper, we investigate the capabilities of LLMs in solving the classical path planning problem: a robot or an"}, {"title": "II. PROBLEM AND METHOD", "content": "Path planning is a classical problem in robotics [1], [17]. A problem instance is specified by a initial position or configuration $I$, a goal position $G$, and a finite collection of obstacles $O_1,..., O_N$ or prohibited configurations. A sequence of positions (or configurations) in the robots workspace is called a path. A path $w_1,..., w_k$ is correct if it starts at $I$, ends at $G$, and for each $i < N$, the straight line joining $w_i$ and $W_{i+1}$ is disjoint from all obstacles (O's). The length of the path $\\sigma$ is the number of segments in the path or equivalently $k - 1$. Every set (e.g. $I$, $G$, and $0_1, ..., O_N$) is a convex polytope. An algorithm Alg solves path planning if, given any problem instance it can find a correct path, provided one exists, and can decide that none exists if that is the case. The agent's workspace $W$ is two or three-dimensional euclidean space for vehicles and can be higher-dimensional in general. In this paper, we empirically investigate whether the current generation of LLMs can solve path planning for two dimensional workspaces with quadrilateral obstacles (See Figure 1 for some examples). Since every path is described as a sequence of 2D coordinates in this setting, we refer to the positions $W_1,..., w_k$ as waypoints. The difficulty of a path planning problem depends factors such as the number of obstacles, the number of possible solutions, the robustness of solutions, etc. We use the path length as a proxy of difficulty of the problem as well as the the optimality of the found solution."}, {"title": "A. Closed Loop Prompting", "content": "Other researchers have noted that LLMs alone cannot reliably complete general planning and reasoning tasks [5], [18]. This matches our experience with the path planning problem. If we explain the objectives of the path planning and prompt the LLMs only once to solve a problem instance, it almost never generates a correct solution. With this observation, we explore whether LLMs can solve planning problems with a sequence of prompts and with additional fine-tuning. An initial prompt describes the generic planning problem and introduces the elements specifying the problem such as the initial set, the goal set, and the obstacles. Each set is a quadrilateral specified by its vertices. The initial prompt also contains instructions on the syntax of how the LLM should respond, that it should explain its thought process and one example problem with solution. Providing LLMs with a few examples, known as few-shot prompting, has proven to be effective [19], [20], [21]. After the LLM generates an initial candidate solution, which is typically incorrect, the Prompt generator produces a sequence of prompts with additional hints to aid the LLM in solving the problem. These hints are calculated exactly by solving a set of linear constraints. The different kinds of hints are discussed in the next section. This prompting procedure continues for a number of iterations at the end of which, either a correct path is successfully found or we register a failure for this problem instance."}, {"title": "B. Hinting Strategies", "content": "We experimented extensively with the following hinting strategies and we report only a summary of the findings in Section III-C. We note that all hinting strategies are based on local checks that can be implemented correctly and efficiently using external constraint solvers."}, {"title": "Collision Hints", "content": "One type of useful feedback an LLM can receive is a local explanation of how the proposed solution is incorrect. Specifically, given a proposed path $\\sigma$ (generated by the LLM) the collision hints will include the following types of information:\nwhether $\\sigma$ begins in the initial set $I$,\nwhether $\\sigma$ ends in the goal set $G$,\nwhich, if any, of the segments in $\\sigma$ intersects with obstacles.\nCollision hint information is generated using an external constraint solver. Since $I$, $G$, and $0_1, ..., O_N$ are all convex polytopes, we can utilize an SMT solver like Z3 [15] to verify all of the above conditions. To see the details of how such constraints can be encoded as SMT problems, we refer the reader to [14]."}, {"title": "Free-Space Hints", "content": "We observed that often LLM gen-erated paths explore the shortest way to the goal $G$, and fail to explore alternative routes. To mitigate this problem we provide it with a set of alternative waypoints, called Free-Space Hints. We split the workspace into multiple vertical slices and provide safe waypoints in each slide (see Figure 4). These free space hints define a tree of possibilities that the LLM could use to generate new paths."}, {"title": "Correct-Prefix Hints", "content": "To encourage the LLM to con-tinue exploring correct prefixes of candidate paths, our third hinting strategy provides the longest prefix of an LLM-generated path $\\sigma$ which starts in $I$ and does not violate the collision constraints. We observed that this helps steer the LLM toward more promising solutions without completely disregarding its previous answer."}, {"title": "Image Hints", "content": "With the advances in multi-modal mod-els, it is now possible to provide images as inputs to LLMS in addition to textual inputs. Multi-modal models have been successfully applied in robotics [22] and image classification [23], [24]. Our fourth strategy utilized this feature and provides an image rendering of the path planning problem (as in Figure 1) as an additional hint to the LLM. Picture hints involve providing the LLM with an image of the problem, alongside the last path it suggested."}, {"title": "C. Fine Tuning", "content": "Fine-tuning is a technique to improve the performance of pre-trained LLMs with application-specific data [25]. This process involves supervised traning on application-specific tailored dataset that is tailored to the desired application. Fine-tuning has been shown to make LLMs more effective and accurate, as demonstrated in fields like finance [26] and medicine [27]. For fine tuning LLMs for path plan-ning, we construct a dataset of problem-solution pairs by first randomly generating 200 planning problem instances following the same method outlined in Section III-A and generating correct solutions using the FACTEST tool [14]. These fine tuning instances are distinct from the ones used for evaluations in III-C."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "We will discuss the effectiveness of three different LLMs in solving path planning problems with different hinting strategies. Before presenting the results we discuss certain relevant details of the experiments."}, {"title": "A. The Problems, metrics, and evaluation", "content": "We evaluate the performance and impact of different hint strategies on two classes of path planning problems:\nHandcrafted Problems: These are natural problems obtained from existing literature on planning and control synthesis [14], [28]. They vary in the number of obstacles and the difficulty, i.e., the minimum solution length. All 10 handcrafted problems are shown in Figure 1 and are listed on the top part of Tables, roughly in the order of increasing difficulty. LLMs paths are randomly generated, and therefore, for each handcrafted problem, we tested each hinting strategy by performing 10 separate evaluations. During these evaluations, the number of feedback iterations was capped at a maximum of 20, and we report the average success rates and path lengths.\nRandom Problems: We have generated a large collec-tion of random problems for this work, For a given number of obstacles $k$, the workspace is divided into a grid of $n > K$ tiles. A hyperparameter controls the extent to which these tiles can overlap. Out of the $n$ tiles, $k$ are randomly selected, and four random points within the tile are generated, which form a convex obstacle. This approach can generate planning problems with different difficulty levels by varying the number, the locations, and the overlaps of the obstacles in the workspace. Several examples from this class of problems are shown in Figure 5 and are listed on the lower part of Tables. \nWe evaluate randomly generated problems with $n = 1,..., 5$ obstacles. For each obstacle count, we evaluated the different hinting strategies on 20 distinct randomly generated problem instances. This means, for example, that we tested on 20 different problems with 1 obstacle, 20 problems with 2 obstacles, and so on. For each problem, the maximum number of feedback iterations was limited to 5, which differs from the 20-iteration cap used in handcrafted problems. This is because the random problems tend to be simpler and our aim here was to study a range of problems rather than multiple attempts on the same problem.\nPerformance Metrics: For each experiment, we report three key metrics: Success rate (S%), the average number of iterations required to find a correct path (N), and the average length of the correct path (PL). The success rate measures the proportion of problems in which the hint strategy suc-cessfully guided the LLM to find a correct solution. The average number of iterations captures the efficiency of the hinting process. This metric is also a proxy for the running"}, {"title": "B. Implementation and the LLMs", "content": "Our iterative hint-generation framework is implemented in Python 3.10 using Z3 as the solver [15]. The code will be made publicly available.\nLLMs: We tested a variety of language models, includ-ing locally-hosted options like Llama 3 [29] and Mistral NeMo [30]. However, in what follows we present the detailed results from proprietary models owing to their superior performance. The models used in our experiments were Gemini-Pro-1.5-001 [31], ChatGPT-40-2024-08-06 [32], and Claude-3.5-Sonnet-20240620 [33]. In the following text, they will be referred to as Gemini, GPT-40, and Claude (Claude 3.5 Sonnet). These models were accessed via API, utilizing the default parameters provided by the respective platforms. The temperature parameter was left at its default value, rather than being set to 0 as commonly done in other studies, to encourage more diverse responses [34]. As mentioned, we perform multiple experiments for each setup to report average performance metrics.\nLocal Computations: The experiments and prompt gen-eration were executed on a local machine running Ubuntu 22.04 LTS. The machine was equipped with an NVIDIA GeForce GTX 1660 SUPER GPU (6 GB VRAM), an AMD Ryzen 5 3600 6-core processor, and 16 GB of RAM. Note that this machine was used for managing the experiments and handling auxiliary tasks, the LLM inferences ran on the servers of the respective API providers."}, {"title": "C. Main Results", "content": "We evaluated the effectiveness of the different hinting strategies listed in Section II-B and their combinations. Without any hints, we found that all the LLMs struggled to solve path planning problems beyond the simplest cases. This is consistent with the findings of [5], [18]. We proceed to report the results from three different levels of hints that cover the other combinations.\n1) LLMs with collision hints can solve moderate but not the hard planning problems: Collision hints provide local feedback, which makes them a baseline for the other hinting strategies. The introduction of collision hints improved the performance of all the LLMs, particularly for easy and mod-erate problems (see Table I). However, LLMs with collision hints rarely succeeded with the more difficult problems like Maze and Scots. Claude solved Scots once in 10 attempts. There is substantial differences across the LLMs. Gemini and Claude tend to generate axis-aligned paths, while GPT-40 frequently produces paths with diagonal segments (see Figure 2). GPT-40 consistently outperformed Gemini and Claude on the Canyon problem but was completely unable to solve Diagonal Wall. In addition, Gemini was the only LLM that never solved Box Boundary on the first try. These failures were primarily because the proposed solution did not reach the goal G. Overall, Claude demonstrated the strongest performance across the board.\nFor the random problems, no big surprises: with more obstacles, the problems became harder for the LLMs. Claude performed well, with over 90% success rate on problems with up to three obstacles. However, its performance dropped to only 25% success rate when faced with four or more.\n2) Collision, free space, and correct prefix hints improve performance for handcrafted problems, but shows mixed results for Spiral and random problems: Combining collision hints with free space and correct prefix hints gives the LLMs the most extensive amount of information that our text-based hinting strategies allow. The results are shown in Table II. With more extensive guidance, the LLMs demonstrated an improved performance across nearly all handcrafted prob-lems, with the exception for Spiral. Performance was equal or better than before for the majority of tasks. Gemini, which could not solve Canyon with only collision hints, improved to a 30% success rate; Claude's success rate increased from 40% to 90%; GPT-40 consistently solved Curve. For hard problems like Maze, the free space with correct prefix hints were critical. Correct prefix hints enabled the LLMs to refine previous candidate solutions rather than discarding them entirely. Without prefix hints, the iterative feedback can fall into random guessing.\nInterestingly, the additional hints resulted in a worse performance for Spiral. Gemini's success rate dropped from 60% to 30%, and Claude's performance declined even more sharply, from 80% to 20%. In the case of the Scots problem, Claude now encounters difficulties to solve the problem. It consistently finds a path through the suggested free space hint, but when compared to using collision hints only, the model now moves more directly, passing through obstacles to reach the free space hint, rather than taking the longer, but obstacle-free path.\nThe trend is unclear for the random problems. While Gemini and Claude achieve perfect success rates on one-obstacle problems, their success rate drop by at 35% and 25% for problems with two obstacles, respectively. There is no significant change in the success rate for GPT-40.\nGenerally, the number of feedback iterations needed to find a correct solution is higher with collision, free space, and prefix hints for Gemini compared to only using collision hints. For the Wall problem, Gemini needs more than four times and for Curve twice as many iterations on average. On the other hand, GPT-40 not only improved in solving Curve with a 100% success rate, it also needed half as many iterations. Claude demonstrated comparable improvements on the Canyon problem.\nThe mean length of correct paths increased in comparison to only using collision hints. This can be attributed to the inclusion of free space hints, which requires the path to incorporate at least one such point. Overall, while offering more guidance generally led to higher success rates, it did not consistently improve optimality (Table I).\n3) Image hints did not help LLMs solve planning problems: Two-dimensional planning problems and their solu-tions can be visualized clearly. However, image hints, did not appear to enhance the LLM's path-planning performance. The results of using all the textual hints of Section III-C.2 together with image hints are shown in Table III. This outcome is surprising, given that LLMs have demonstrated strong image comprehension in other contexts, including the ability to read text within images [35] and understand visual context [22], [23], [24]. In some cases, the LLM's performance even degraded with images.\n4) Fine tuning improves optimality of solutions: We used the fine-tuning capabilities of GPT-4o for this experi-ment [16], [32]; other models did not offer fine-tuning func-tionality at the time of writing. Fine-tuning (see Section II-C) led to a noticeable improvement in the length of the paths found (See Table IV). GPT-4o is able to solve Diagonal Wall more often and its performance on the random problems improved significantly. The paths that the LLM generated are generally shorter.\nThe dataset used for fine-tuning did not include prompts for explaining solutions and relied solely on solution arrays. We also experimented with fine-tuning the models on a synthetic dataset, where explanations for correct solutions were generated by another LLM. However, this approach proved less effective than using the dataset that contained only the solution arrays, indicating that direct task-specific training data may be more beneficial for path-planning than generated (and possibly wrong) explanations.\n5) Gemini and Claude tend to generate orthogonal paths: In our experiments, we observed differences in the path-planning tendencies and performance of the LLMs. Both"}, {"title": "IV. RELATED WORK", "content": "An alternative approach for integrating LLMs in the over-all planning stack is to use them as an interface between human users specifying the problem and the (classical) algo-rithmic planners. In this, the LLMs function as a translator from natural language to the formal or computer-readable specification of the task or motion planning problem and this has been explored in [36] and [37].\nA very different approach for combining LLMs with path planners has been explored in [38], [39]. Here the idea is to correct or improve the performance of classical planners, with LLMs. In [38] the LLM is used to diagnoses problems in a generated plan from detailed information about the scenario, type of algorithm used, objective functions, etc. In [39] a method is proposed for leveraging LLMs to guide search the classical A* algorithm.\nFinally, we mention that there has been research on closed-loop prompting with LLMs for travel planning [40], autonomous driving [41], [42], and task planning [43], which are less directly related to our work."}, {"title": "V. LIMITATIONS AND FUTURE WORK", "content": "For planned paths to be useful, the should be path dy-namically feasible. A correct path with sharp turns around an obstacle may not be safe for a vehicle with a large turning radius. Such dynamic or nonholonomic constraints are currently not included in our framework, although there are many algorithms for solving that problem. We did not investigate these dynamic constrains here because the LLMS seem to struggle with hard path planning problems even without dynamic constraints. As the LLMs improve, in the future it will be important to incorporate dynamic constraints and the constraints arising from agent's geometry into the planning task.\nAdditional prompting and fine-tuning strategies that are effective in other contexts should be explored for path planning as well. For example, presenting sequences with smaller reasoning steps, example solutions, identifying the symmetries and invariances that exist in the problem do-main, building-in linear constraint checks, etc. Our software framework can indeed be extended to support several of these hinting strategies.\nFinally, we conjecture that attaching the planning problems to real world data that is supposedly embedded in the LLM will improve their performance. To test this, we will have to present planning problems in the real world with names of places, street, zones (instead of abstract obstacles and free-space). To this end, it will be interesting to connect the planning problems with a geographical data and realistic maps."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose an approach to assist LLMs in complex planning tasks by incorporating solver feedback in a closed-loop system and evaluating their performance with different types of hints. We also introduce a benchmark of 10 handcrafted and 100 randomly generated problems. Our experiments show that even minimal collision hints significantly improve performance on moderately difficult problems, with LLMs consistently finding correct solutions once provided with the hints. However, LLMs still struggle with the most challenging tasks, even when provided with the full range of hints generated by our framework. Notably, image-based hints did not enhance performance, highlighting limitations in processing visual data. Fine-tuning, however, led to more consistent and efficient solutions. Finally, Gemini and Claude tend to generate orthogonal movements, in contrast to GPT-40. Future work will focus on incorporating dynamic constraints and agent geometry into path planning tasks, as well as exploring prompting strategies like smaller reasoning steps and real-world data integration to improve LLM performance."}]}