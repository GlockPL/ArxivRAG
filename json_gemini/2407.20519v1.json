{"title": "DuA: Dual Attentive Transformer in\nLong-Term Continuous EEG Emotion Analysis", "authors": ["Yue Pan", "Qile Liu", "Qing Liu", "Li Zhang", "Gan Huang", "Xin Chen", "Fali Li", "Peng Xu", "Zhen Liang"], "abstract": "Affective brain-computer interfaces (aBCIs) are increasingly recognized for their potential in monitoring and interpreting\nemotional states through electroencephalography (EEG) signals. Current EEG-based emotion recognition methods perform well with\nshort segments of EEG data (known as segment-based emotion analysis). However, these methods encounter significant challenges\nin real-life scenarios where emotional states evolve over extended periods. To address this issue, we propose a Dual Attentive (DuA)\ntransformer framework for long-term continuous EEG emotion analysis. Unlike segment-based approaches, the DuA transformer\nprocesses an entire EEG trial as a whole, identifying emotions at the trial level, referred to as trial-based emotion analysis. This\nframework is designed to adapt to varying signal lengths, providing a substantial advantage over traditional methods. The DuA\ntransformer incorporates three key modules: the spatial-spectral network module, the temporal network module, and the transfer\nlearning module. The spatial-spectral network module simultaneously captures spatial and spectral information from EEG signals,\nwhile the temporal network module detects temporal dependencies within long-term EEG data. The transfer learning module enhances\nthe model's adaptability across different subjects and conditions. We extensively evaluate the DuA transformer using a self-constructed\nlong-term EEG emotion database, along with two benchmark EEG emotion databases. On the basis of the trial-based\nleave-one-subject-out cross-subject cross-validation protocol, our experimental results demonstrate that the proposed DuA transformer\nsignificantly outperforms existing methods in long-term continuous EEG emotion analysis, with an average enhancement of 5.28%.\nThe DuA transformer's ability to adapt to varying signal lengths and its superior performance across diverse subjects and conditions\nhighlight its potential for real-world applications, enhancing the overall user experience and efficacy of aBCI systems.", "sections": [{"title": "1 INTRODUCTION", "content": "ELECTROENCEPHALOGRAPHY (EEG), a non-invasive stimuli might not effectively evoke vivid sadness, nor might\ntechnique for monitoring brain electrophysiological ac- they induce intense, profound sadness in subjects or bring\ntivity, records neuronal electrical activity signals through about significant changes in brain activity. This limitation\nscalp electrodes. Compared to other physiological signals, can lead to poor results in emotion recognition. Saarim\u00e4ki\nEEG more directly reflects changes in emotions, providing a et al. [3] found the lowest average classification accuracy\nneuroscientific interpretation of emotional states [1], [2]. For for sadness (18%) in their study. In Raz et al. 's work [4],\ncertain emotional states, a long-term continuous evolution it demonstrated that long and complex movie clips are\nmay be necessary. Take the sadness as an example. Short more suitable for inducing the dynamic changes associated\nwith sustained emotional experiences. Furthermore, Xu et\nal. [5] suggested that a 10-minute long stimulus could be\nmore beneficial for brain signal analysis, closely mirroring\nsustained emotional experiences. This finding confirms that\nemotional representations induced by long-duration stimuli\nare superior to those induced by short-duration stimuli.\nHowever, most existing EEG-based emotion studies uti-\nlize short-duration stimuli to evoke responses [6], [7]. The\nalgorithms used in these studies typically perform emo-\ntion analysis on segmented EEG signals (segment-based\nemotion analysis), rather than processing an entire trial\nas a whole (trial-based emotion analysis) [8], [9], [10]. A\nmore detailed introduction to these current algorithms is\nprovided in Section 2. This approach neglects the temporal\ninformation of the entire signal sequence, thus impacting\nthe recognition performance on the whole EEG signal.\nImplementing a long-duration, whole-segment EEG emo-\ntion recognition algorithm faces the challenge of handling\nvariable-length data inputs.\nThe transformer network excels in processing variable-\nlength data and has been increasingly applied in the field of"}, {"title": "2 RELATED WORK", "content": "EEG for emotion analysis [10], [11], [12]. For example, Liu\net al. [13] proposed four variant transformer frameworks\n(spatial attention, temporal attention, sequential spatial-\ntemporal attention and simultaneous spatial-temporal at-\ntention) for EEG-based emotion recognition, exploring the\nrelationship between emotion properties and EEG features.\nSimilarly, Sun et al. [12] introduced a transformer-based dy-\nnamic graph convolutional neural network (CNN) designed\nfor feature fusion, where the extracted graph features are\nfurther updated by the transformer. Additionally, Wei et al.\n[14] proposed a transformer capsule network that integrates\nan EEG transformer module to extract EEG features and\nan emotion capsule module to refine these features for eas-\nier classification. However, existing transformer-based EEG\nalgorithms have several notable limitations, such as inade-\nquate incorporation of spatial and spectral information, in-\nsufficient handling of long-term dependencies, and a lack of\nresearch on long-term EEG modeling methods. To tackle the\nexisting limitations, we propose a novel DuA transformer\nframework for long-term continuous EEG emotion analysis.\nThe main contributions of this study are summarized as\nbelow.\n\u2022\n\u2022 We propose a novel Dual Attentive (DuA) transformer\nframework for long-term continuous EEG analysis. This\nnovel framework enhances traditional attention mecha-\nnisms by improving the performance of emotion decod-\ning for long sequential signals, reducing GPU memory\nusage, and effectively utilizing the spatial-spectral and\ntemporal information of EEG signals.\n\u2022 We approach emotion analysis with a trial-based inter-\npretation. Unlike traditional methods that segment EEG\ntrials into short, fixed-length segments (e.g., 1 second)\nand assign the same label to all segments within a\nsingle trial, our proposed framework processes entire\nEEG trials with variable lengths. This allows for a\nmore flexible and comprehensive analysis of long-term\ncontinuous EEG data.\nExtensive experiments are conducted on self-\nconstructed long-term EEG database and two\nbenchmark databases, covering various data lengths\nand emotional states. The model's reliability and\neffectiveness are validated using a strict trial-based\nleave-one-subject-out cross-subject cross-validation\nprotocol, resulting in an average performance\nenhancement of 5.28%."}, {"title": "2.1 Short Segments based EEG Analysis", "content": "In recent years, a growing number of deep learning algo-\nrithms have been developed for EEG-based emotion recog-\nnition. For example, Liang et al. [15] introduced EEGFuseNet\nto dynamically capture both shallow and deep EEG features\nwithout requiring label information. Fernandez et al. [16]\nleveraged prior knowledge to train a CNN using differential\nentropy (DE) features, improving the network's ability to\nrecognize emotions. To incorporate structural information,\nZhong et al. [9] developed a restricted graph network with\nNodeDAT and EmotionDL, achieving an impressive accu-\nracy of 85.30% in 3-class cross-subject emotion recognition."}, {"title": "2.2 Transfer Learning in aBCI", "content": "However, most existing EEG-based emotion recognition al-\ngorithms rely on segmenting EEG trials into fixed-length\nsegments (e.g., 1 second), with each segment assigned the\nsame emotional label [7], [17], [18]. This approach assumes\nthat the emotional state remains constant throughout each\nsegment in a single trial.\nIn practical emotion experiments, labels are typically\nprovided for entire trials rather than individual segments.\nEmotional states are dynamic and can fluctuate significantly\nover longer periods, making it unrealistic to assume a static\nemotional label for each short segment. Moreover, research\nhas shown that emotion elicitation is an accumulative pro-\ncess, evolving rather than remaining fixed [19]. Given these\nconsiderations, emotion analysis using EEG signals should\nbe conducted in a trial-based manner rather than a segment-\nbased manner, especially in supervised learning contexts\nwhere accurate label information is crucial. This trial-based\napproach acknowledges the dynamic nature of emotional\nstates and allows for more accurate and realistic emotion\nrecognition in continuous EEG analysis.\nEmotion recognition algorithms face significant challenges\ndue to the variability and complexity of EEG signals and the\nindividual differences in these signals. To address these is-\nsues, transfer learning has emerged as a valuable technique,\nhelping to mitigate the discrepancies in feature distribution\nextracted from various subjects and enhancing model stabil-\nity in cross-subject EEG-based emotion recognition tasks.\nFor non-deep transfer learning methods, Zheng et al. [20]\nemployed transfer component analysis (TCA) and transduc-\ntive parameter transfer (TPT) to achieve an average accuracy\nof 76.31%, compared to 56.73% without transfer learning.\nConsidering the marginal and conditional distributions in\nthe feature alignment process, Luo et al. [21] introduced a\nmanifold-based domain adaptation method. This method\nadaptively and dynamically adjusted the importance of\nmarginal and conditional distributions during the feature\nalignment process based on Grassmann manifold space,\nleading to a further average improvement of 3.54%.\nDeep transfer learning algorithms have also shown\npromise in this field. Jin et al. [22] introduced an EEG-based\nemotion recognition model using domain adversarial neu-\nral networks (DANN) The results demonstrated that mod-\nels using deep transfer learning frameworks outperformed\nthose using non-deep transfer methods, with average accu-\nracy increasing from 76.31% to 79.19%. To further enhance\nfeature alignment, a series of improved DANN frameworks\nwere proposed [23], [24]. Considering the interaction feature\nrepresentation between sample features and prototype fea-\ntures, Zhou et al. [8] introduced a prototype-representation\npairwise learning-based transfer learning framework. This\napproach not only addressed individual differences but also\ntackled the issue of noisy labeling, achieving state-of-the-art (SOTA) results in cross-subject emotion recognition. The\nabove studies demonstrate that transfer learning, both non-deep learning and deep learning, plays a crucial role in advancing aBCI systems by improving the generalization and\nrobustness of emotion recognition models across different\nindividuals. Future research should continue to explore op-"}, {"title": "2.3 Transformer Network with Attention Mechanism", "content": "Transformer networks, introduced by Vaswani et al. [25],\nhave become integral in both computer vision and natural\nlanguage processing. In computer vision, models such as\nvision transformers (ViT) and swin transformers have set\nnew standards. In natural language processing, models like\nBERT and GPT exemplify their success. The strength of\ntransformer networks lies in their extensive use of attention\nmechanisms to process sequential data. This mechanism\nenables the model to identify and focus on the most relevant\nparts of the input. For each input element, the model assigns\na weight indicating its importance, and through learning,\nit adaptively adjusts these weights. This process mimics\nhuman attention to specific details, significantly enhancing\nthe model's performance.\nIn the field of continuous EEG analysis, a number of\nstudies have integrated attention mechanisms and trans-\nformer networks to enhance performance. For example, Jia\net al. [11] proposed a 3D fully-connected network based\non attention mechanisms for DE feature projection. Tao et\nal. [10] incorporated self-attention mechanisms into CNNs\nand long short-term memory (LSTM) networks for better\nfeature representation. Additionally, Si et al. [26] introduced\na temporal aware mixed attention-based convolution and\ntransformer network, which combines CNN and attention\nmechanisms to jointly extract local and global emotional\nfeatures of EEG segments. Furthermore, some researchers\nattempts to directly apply transformer networks. Wang et\nal. [27] proposed a transformer-based model to robustly\ncapture temporal dynamics and spatial correlations of EEG\nsignals. Sun et al. [12] extracted power spectral density (PSD)\nand DE features and then utilized graph and transformer\nnetworks to fuse and extract features representing emotions.\nSimilarly, Cui et al. [28] introduced a multi-view graph\ntransformer based on spatial relations that integrates infor-\nmation from the temporal frequency and spatial domains\nto enhance the expressive power of the model comprehen-\nsively. However, most existing methods emphasise learning\nshort-term temporal patterns, neglecting significant long-\nterm contextual and information related to emotional cog-\nnitive processes [29]. In addition, these methods inadequate\nincorporate spatial and spectral information, further leading\nto poor model performance in long-term continuous EEG\nanalysis tasks.\nTo address the challenge of processing long-term con-\ntinuous EEG signals for emotion recognition, we propose a\nDual Attention (DuA) transformer framework, as illustrated\nin Fig. 1. Unlike typical one-dimensional time series, long-\nterm EEG signals encompass rich temporal and spatial in-\nformation. Directly applying transformer networks to these\ntime series might overlook the spatial nuances, thereby\ndimishing the network's ability to accurately represent\nfeatures and impacting the effectiveness of emotion recog-\nnition. On the other hand, traditional transformer networks,\nconsisting of encoders and decoders, are typically employed"}, {"title": "3 METHODOLOGY", "content": "for diverse tasks. Encoders are generally used for classi-\nfication, while decoders are suited for generation tasks.\nGiven that EEG-based emotion recognition is fundamentally\na classification task, our approach focuses on utilizing the\nencoder architecture of the transformer network.\nOur proposed framework comprises three main mod-\nules: the spatial-spectral network, the temporal network,\nand a transfer learning module. This model extracts spa-\ntiotemporal features from EEG signals, fully exploiting both\nthe temporal and spatial dimensions, overcoming the lim-\nitations inherent in traditional transformer networks. Ad-\nditionally, to address the variability in EEG signals across\ndifferent individuals, we incorporate a transfer learning\nstrategy, enhancing the network's accuracy in cross-subject\nlong-term EEG emotion recognition tasks. This combination\nof modules ensures a comprehensive analysis of trial-based\nEEG signals, leading to more accurate and reliable continu-\nous EEG emotion analysis."}, {"title": "3.1 Transformer Basics", "content": "To address the limitations of traditional transformer net-\nworks in effectively utilizing the diverse aspects of long-\nterm EEG signals, we propose a DuA transformer network\nspecifically designed for continuous EEG analysis. DuA\ntransformer includes three key modules. Spatial-Spectral\nNetwork Module. This module targets the spatial domain\nby analyzing the spatial distribution of EEG signals. It\ncaptures intricate spatial relationships between different\nEEG channels in terms of spectral patterns, enabling the\nmodel to understand complex spatial dependencies. Tempo-\nral Network Module. This module focuses on the temporal\ndomain, effectively modeling the dynamic changes in EEG\nsignals over time. By doing so, it captures the temporal\ndependencies and variations inherent in EEG data, allowing\nfor a more nuanced understanding of the temporal dynam-\nics. Transfer Learning Module. This module adapts the\nmodel to new individuals by leveraging knowledge from\npreviously seen data, ensuring better generalization across\ndifferent individuals. It enhances the model's ability to gen-\neralize and perform well on unseen subjects by transferring\nlearned features and patterns from prior data. The details of\neach module will be illustrated below.\nIn this section, we will introduce the encoder part of\nthe transformer network and its various configurations.\nThe input sequence is processed through the encoder to\nextract useful information and project the sequence into\nlow-dimensional features for subsequent downstream tasks,\nsuch as classification. The encoder consists of several\nstacked encoder layers. The input and output of the first\nlayer are sequences, and each subsequent layer's input is the\noutput from the previous layer. Each encoder layer is com-\nposed of a multi-head self-attention (MHSA) mechanism\nand a feed-forward network (FFN), connected by residual\nconnections for deeper network construction, followed by\nlayer normalization (LN).\nThe MHSA mechanism consists of several self-attention\nunits. Self-attention operates on a \"matching\" mechanism,\nmapping each element of the sequence into Key, Query,\nand Value vectors. It calculates the dot product between\nthe Query and Key vectors, transforming the result into\nattention scores ranging from 0 to 1 using the Softmax\nfunction. These attention scores are then multiplied with the\nValue vectors as follows:\n$\\begin{cases}\nq = W_q \\times X, q \\in \\mathbb{R}^{n \\times d_k},\nk = W_k \\times X, k \\in \\mathbb{R}^{n \\times d_k},\nv = W_v \\times X, v \\in \\mathbb{R}^{n \\times d_k},\n\\end{cases}$$\n(1)\n$A (q, k, v) = Softmax(\\frac{qk^T}{\\sqrt{d_k}}). v,$ (2)\nwhere $X = \\{X_1, X_2, ..., X_n\\}$ is the input sequence, n is\nthe length of the sequence, and $d_k$ is the dimension of\nthe vectors, which also serves as the normalization factor\nto maintain the numerical stability of the attention scores.\nMHSA consists of multiple self-attentions, meaning the\noutputs from several self-attentions are integrated after\nmapping as:\n$MHSA(X) = W^o \\times (head_1, head_2, ..., head_i),$ (3)\nwhere $head_i$ is given as:\n$head_i = A(q_i, k_i, v_i).$ (4)\nFollowing MHSA, there is a FFN layer, composed of\nfully connected networks. It performs a nonlinear mapping\nindividually and identically on the output of MHSA at\neach position in the sequence. First, the features from the\nMHSA output undergo an upscaling projection to obtain\nhigh-dimensional feature representations. Then, these high-\ndimensional features are projected down to the original\ndimension size for seamless integration into the subsequent\nlayer. The entire feed-forward network is as follows:\n$FFN(X) = GeLU(XW_1+b_1)W_2 + b_2,$ (5)\nAmong these, $W_1 \\in \\mathbb{R}^{D_m \\times D_f}, b_1 \\in \\mathbb{R}^{D_f},W_2 \\in \\mathbb{R}^{D_f \\times D_m}$,\nand $b_2 \\in \\mathbb{R}^{D_m}$ are all trainable parameters. Typically, $D_f$\nis larger than $D_m$, and generally $D_f$ is four times $D_m$.\nLN is applied after the residual connection, normalizing\nthe output of the forward propagation to ensure that the\noutput and input maintain a similar distribution. This helps\nto reduce the phenomenon of covariance shift and accelerate\nthe convergence speed of network training.\n$LN(X) = \\frac{X - E(X)}{\\sqrt{var[X] + \\epsilon}},$ (6)\nIn summary, the entire encoder network layer is defined\nas:\n$H' = LN(MHSA(LN(X) + LN(X))),$ (7)\n$H = LN(FFN(H') + H').$\nTransformers can process entire sequences using a non-\nautoregressive approach. However, the self-attention mech-\nanism within transformer networks is permutation invari-\nant; for any given sequence, the attention scores remain\nthe same regardless of the order of elements. However,\nthe position within a sequence is often critical information.\nTherefore, in transformer networks, sinusoidal and cosine\nformulas of different frequencies are used across different"}, {"title": "3.2 Spatial-Spectral Network Module", "content": "dimensions to generate high-dimensional positional encod-\nings that are added to the inputs. This method of incorpo-\nrating positional information is called Positional Encoding,\ngiven as:\n$\\begin{cases}\nPE(pos, 2i) = sim(pos/10000^{2i/d_{model}}),\n\\PE(pos, 2i + 1) = cos(pos/10000^{2i/d_{model}}),\n\\end{cases}$$\n(8)\nwhere $i$ refers to the specific dimension within the data,\n$d_{model}$ represents the total dimensionality of the input data,\nand 10000 signifies that the model can accommodate se-\nquences with a maximum length of 10000.\nTo enhance the representational power of features in EEG\nsignal processing, we propose a novel spatial-spectral net-\nwork. This network is designed to simultaneously extract\nsignificant spatial and frequency domain information from\nEEG signals at each time point, rather than processing these\ntypes of information separately.\nTo realize this purpose, we optimize the multi-head at-\ntention component of the spatial-spectral network to handle\nboth types of information simultaneously. Given an input\nsignal $X \\in \\mathbb{R}^{c \\times f}$, where $c$ is the number of channels and $f$\nis the number of frequency bands, the optimized multi-head\nself-attention mechanism maps the input signal X to Query,\nKey, and Value vectors. During the multi-head self-attention\ncomputation, even-indexed heads process the channels as\nsequences for spatial feature extraction, while odd-indexed\nheads process the frequency bands as sequences for spectral\nfeature extraction, as;\n$\\begin{cases}\n\\ headi(qi, ki, vi) = Softmax(\\frac{qi k_i^T}{\\sqrt{d_k}}). Vi,\\\nheadj (qj, kj, vj) = Softmax(\\frac{qj k_j^T}{\\sqrt{d_k}}), \\\n\\end{cases}$$\n(9)\nHere, $i$ represents odd indices and $j$ represents even indices.\nThe transformation of the Query, Key, and Value vectors in\nthe transformer network is a linear mapping. Therefore, in\nthe proposed spatial-spectral network, it is only necessary to\ntranspose the Query, Key, and Value vectors of some heads\nto extract features from both spatial and spectral informa-\ntion, without the need to reconstruct the input sequences.\nNote that the inputs for spatial and spectral feature extrac-\ntion are the sequences formed by the different channels of\nthe input EEG signals at each time point and the extracted\nDE features at different frequency bands, respectively.\nThis proposed network simplifies the encoder process,\nreducing computational complexity while effectively cap-\nturing both spatial and spectral features. By transposing\nthe Query, Key, and Value vectors for specific heads, the\nnetwork could efficiently handle the different types of infor-"}, {"title": "3.3 Temporal Network Module", "content": "mation.\nThe temporal network module receives input from the\nspatial-spectral network, which provides characterized\nspatial-spectral EEG information at each second. This in-\nformation is represented as $L \\in \\mathbb{R}^{t \\times h}$, where $t$ denotes the\ntime length and $h = c \\times f$ represents the concatenation of\nchannels and frequency bands. The resulting dimensionality\nof $h$, formed by combining channels and frequency bands,\nis typically high. To address this high dimensionality and\ncreate a more manageable feature space, we incorporate\nan embedding layer for dimensionality reduction before\nfeeding the data into the next transformer. This embedding\nlayer consists of a fully connected network that performs\nnonlinear mapping on the features at each second within\nthe time series.\nTo facilitate sequence classification, we introduce a train-\nable parameter, CLS, which is randomly initialized and ap-\npended to the first position of the sequence. The CLS param-\neter plays a crucial role in extracting temporal information\nfrom the entire long-term EEG signal. Once the sequence,\nincluding the CLS parameter, is processed through the\ntransformer's encoder, the final output CLS can effectively\nrepresent the entire long-term EEG signal. This approach\nleverages the power of the transformer architecture to cap-\nture intricate temporal patterns and dependencies within\nthe EEG data, enhancing the model's ability to perform\naccurate and robust sequence classification.\nIn addition to the dimensionality reduction and se-\nquence classification capabilities, the temporal network is\ndesigned to handle varying lengths of EEG signals by\nemploying positional encoding. This encoding helps the\nmodel maintain the temporal order of the data, ensuring\nthat the sequence information is preserved throughout the\nprocessing stages."}, {"title": "3.4 Transfer Learning Module", "content": "One of the fundamental assumptions of deep learning is that\ntraining and testing data are drawn from the same distribu-\ntion and are ideally independent and identically distributed\n(i.i.d.). However, EEG signals exhibit significant individual\nvariability, making it challenging to satisfy this assumption\nin the task of emotion recognition from EEG signals. To\naddress this issue, a transfer learning module is incorpo-\nrated, which includes a feature extractor (Generator) and a\ndomain discriminator (Discriminator). During training, the\nfeature extractor and domain discriminator are trained in\nopposition to each other to learn domain-invariant features,\neffectively mitigating the impact of individual variability in\nEEG signals.\nIn the implementation, the subject data of the training\nset is defined as the source domain (Source), and the subject\ndata of the test set is defined as the target domain (Target).\nThe feature extractor is denoted as $G_f(\\theta_f)$, the domain\ndiscriminator as $G_d(\\theta_d)$, and the classifier as $G_y (\\theta_y)$. Here,\nz represents the extracted features at the output of the\ntemporal network, y represents the label of the training\nset, and domain distribution alignment is achieved through\nadversarial loss:\n$\u0395(\\theta_f, \\theta_y, \\theta_d) = \\sum_{z_i\\in D_s} L_y(G_y(G_f(z_i)), y_i)-\n\\lambda \\sum_{z_i\\in D_s\\cup D_t} L_d(G_d(G_f(z_i)), d_i),$ (10)\nwhere $L_y$ is the classification loss, $L_d$ is the discriminator\nloss, and $d_i$ is the label for data domain. $d_i = 0$ indicates that\nthe data is from the source domain, while $d_i = 1$ denotes\nthat it is from the target domain. To facilitate implementa-\ntion, a Gradient Reversal Layer (GRL) is incorporated into"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "the network's backpropagation process. During forward\npropagation, GRL acts as an identity mapping:\n$R_x(z) = z.$ (11)\nDuring backpropagation, GRL reverses the gradients:\n$\\frac{dR}{dz} = -\\lambda I,$ (12)\nwhere $\\lambda$ is a dynamic balancing hyperparameter used to en-\nsure the stability of the domain adversarial training process,\ngiven as:\n$\\lambda = \\frac{2}{1+ exp(-p)} - 1.$ (13)\nEEG signals from different subjects not only exhibit\ndistributional differences but also individual temporal vari-\nations. To address this, our algorithm treats the spatial and\ntemporal feature extraction networks as a unified feature\nextractor, rather than relying solely on the spatial feature\nextraction network. The loss function of the network is\ndefined as follows:\n$L_{overall} = L_c(c(f(X_s))) + \\lambda L_{adv} (d(f(X_s, X_T))),$ (14)\nwhere $L_c$ denotes the cross-entropy loss function for clas-\nsification, $L_{adv}$ corresponds to the binary cross-entropy\nloss function for domain discrimination. $f(.)$ is the feature\nextractor, $c(.)$ is the classifier, and $d(\\cdot)$ is the domain discrim-\ninator. $X_s$ and $X_T$ indicate the input data from the source\nand target domains, respectively. An overall algorithm of\nthe proposed DuA transformer is illustrated in Algorithm 1.\nThe study involved 50 subjects (25 males and 25 females,\naverage age: 18.72 \u00b1 1.23 years). A total of 12 long-term\nvideos (6 positive and 6 negative) was selected as stimuli,\neach with an average duration of 11.11 \u00b1 1.64 minutes.\nThere was no content overlap among the selected videos.\nAfter watching each video, the subjects were asked to recall\ntheir emotions and rate their valence levels on a scale of [-\n10, -5, 0, 5, 10]. Here, -10 indicates extremely negative, 10\nindicates extremely positive, and 0 refers to neutral.\nEEG signals were recorded in real-time using a Brain\nProducts device with 63 channels configured according to\nthe 10-20 system, as each subject watched the 12 long-term\nvideos. To manage fatigue, the experiment was conducted\nin two sessions, with each session featuring videos that\ninduced a single type of emotion (either positive or nega-\ntive). These sessions were spaced at least one week apart. In\ntotal, the database comprises 600 long-term continuous EEG\nsignal recordings.\nFor the recorded EEG signals, we extracted DE features\nfrom each channel every second across the following ten\nfrequency bands: Theta (4-6 Hz), Alpha1 (6-8 Hz), Alpha2\n(8-10 Hz), Alpha3 (10-12 Hz), Beta1 (12-16 Hz), Beta2 (16-20\nHz), Beta3 (20-28 Hz), Gamma1 (28-34 Hz), Gamma2 (34-39\nHz), and Gamma3 (39-45 Hz). For each trial of the long-term\ncontinuous EEG signal recordings, the EEG signals were\nthen transformed into a three-dimensional matrix by time,\nchannel, and frequency band.\nFor model validation, we adopt a trial-based leave-one-\nsubject-out cross-validation protocol. Using our database of\n50 subjects, we iteratively use the data from 49 subjects as\nthe training set and the data from the remaining one subject\nas the test set. This process is repeated until each subject\nhas served as the test set exactly once. The final validation\nresults are obtained by calculating the average and standard\ndeviation of these 50 test iterations. Additionally, leveraging\nthe collected subjective scores ([-10, -5, 0, 5, 10]), we define\nthree types of emotion classification tasks (2-class, 3-class,\nand 5-class), as illustrated in Fig. 2. This approach ensures a\nrobust assessment of the model's ability to generalize across\ndifferent subjects and emotional states."}, {"title": "4.2 Implementation Details", "content": "Algorithm 1 The algorithm flow of DuA transformer.\nRequire:\nmax iteration 7, batch size \u00a7, length t, channel c, the\nsize of feature h, source data {(X, Y)}, target data\n{Xt}.\n- feature extractor f(\u00b7) contains spatial-spectral network\nf1 (\u00b7) and temporal network f2(\u00b7), classifier c(\u00b7), discrim-\ninator d(.).\nEnsure: f1(), f2(\u00b7), c(\u00b7), d(\u00b7).\n1: Random initialization of f\u2081(\u00b7), f2(\u00b7), c(\u00b7) and d(\u00b7);\n2: for 1 to t do\n3: Reshape X = {Xs, Xt} (\u00a7, t, c, h) \u2192 (\u00a7 \u00d7 t, c, h);\n4: Generate second's feature L = f\u2081(X);\n5: Reshape L = {Ls, Lt} (\u00a7 \u00d7 t, c, h) \u2192 (\u00a7, t, c \u00d7 h);\n6: Generate sequential feature CLS = f2(L);\n7: Calculate classification probability qc = c(CLS\u300f);\n8: Calculate discrimination probability qa = d(CLS);\n9: Calculate overall loss Loverall;\n10: Gradient back-propagation;\n11: Update network parameters;\n12: end for\nIn our experiment, we utilize the proposed DuA trans-\nformer, which integrates a spatial-spectral network and a\ntemporal network as the feature extractor, and an MLP with\na ReLU activation function as the domain discriminator.\nThe parameters of the transformer network are randomly\ninitialized from a uniform distribution.\nSpecifically, the spatial-spectral network is configured as\nbelow. The input is X \u2208 Rcxf, where c is the number of"}, {"title": "4.3 Emotion Recognition Results", "content": "4.1 EEG Data and Experimental Protocol\nGiven the limited availability of EEG emotional databases\nspecifically designed for long-term EEG signals, we con-\nducted a study to fill this gap by undertaking EEG emo-\ntional experiments under prolonged emotional stimulation.\nchannels (63), f is the number of frequency bands (10).\nThe network uses 6 heads, with QKV vector mapping\ndimensions of 60 (each head having a QKV dimension of\n10). The FFN layer is configured with a 10-40-10 mapping.\nThe spatial-spectral network consists of one encoder layer.\nSubsequently, the channel and frequency band dimensions\nare concatenated and mapped from 630 to 128 before being\nfed into the temporal network. For the temporal network, it\nuses 3 heads, with QKV vector mapping dimensions of 384\n(each head having a QKV dimension of 128). The FFN layer\nis configured with a 128-512-128 mapping. Similar as the\nspatial-spectral network, the temporal network also consists\nof one encoder layer.\nDuring training, the number of epochs is set to 300. We\nuse a random mini-batch gradient optimization algorithm to\nupdate the model parameters. The batch size for the training\nprocess is 12, the optimizer is Adam with a momentum\nparameter of 0.9, and the learning rate is 1 \u00d7 10-3. The\nmodel is trained on an NVIDIA GeForce RTX 3090 GPU,\nusing CUDA 10.0 drivers, and the training is conducted\nwith the Pytorch API.\nWe conduct an extensive evaluation of existing machine\nlearning and deep learning methods on long-term contin-\nuous EEG signals using trial-based leave-one-subject-out\ncross-subject cross-validation. As shown in Table 1, our\nproposed DuA transformer model demonstrates substan-\ntial performance improvements over traditional machine\nlearning methods, achieving 16.83% higher accuracy for\n2-class classification, 36.89% for 3-class classification, and\n31.73% for 5-class classification. When compared to ad-\nvanced deep learning techniques, our DuA transformer also\nshows significant performance enhancements. Specifically,\nit outperforms the best transfer learning method by"}]}