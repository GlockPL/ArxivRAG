{"title": "DuA: Dual Attentive Transformer in\nLong-Term Continuous EEG Emotion Analysis", "authors": ["Yue Pan", "Qile Liu", "Qing Liu", "Li Zhang", "Gan Huang", "Xin Chen", "Fali Li", "Peng Xu", "Zhen Liang"], "abstract": "Affective brain-computer interfaces (aBCIs) are increasingly recognized for their potential in monitoring and interpreting\nemotional states through electroencephalography (EEG) signals. Current EEG-based emotion recognition methods perform well with\nshort segments of EEG data (known as segment-based emotion analysis). However, these methods encounter significant challenges\nin real-life scenarios where emotional states evolve over extended periods. To address this issue, we propose a Dual Attentive (DuA)\ntransformer framework for long-term continuous EEG emotion analysis. Unlike segment-based approaches, the DuA transformer\nprocesses an entire EEG trial as a whole, identifying emotions at the trial level, referred to as trial-based emotion analysis. This\nframework is designed to adapt to varying signal lengths, providing a substantial advantage over traditional methods. The DuA\ntransformer incorporates three key modules: the spatial-spectral network module, the temporal network module, and the transfer\nlearning module. The spatial-spectral network module simultaneously captures spatial and spectral information from EEG signals,\nwhile the temporal network module detects temporal dependencies within long-term EEG data. The transfer learning module enhances\nthe model's adaptability across different subjects and conditions. We extensively evaluate the DuA transformer using a self-constructed\nlong-term EEG emotion database, along with two benchmark EEG emotion databases. On the basis of the trial-based\nleave-one-subject-out cross-subject cross-validation protocol, our experimental results demonstrate that the proposed DuA transformer\nsignificantly outperforms existing methods in long-term continuous EEG emotion analysis, with an average enhancement of 5.28%.\nThe DuA transformer's ability to adapt to varying signal lengths and its superior performance across diverse subjects and conditions\nhighlight its potential for real-world applications, enhancing the overall user experience and efficacy of aBCI systems.", "sections": [{"title": "1 INTRODUCTION", "content": "ELECTROENCEPHALOGRAPHY (EEG), a non-invasive\ntechnique for monitoring brain electrophysiological ac-\ntivity, records neuronal electrical activity signals through\nscalp electrodes. Compared to other physiological signals,\nEEG more directly reflects changes in emotions, providing a\nneuroscientific interpretation of emotional states [1], [2]. For\ncertain emotional states, a long-term continuous evolution\nmay be necessary. Take the sadness as an example. Short\nstimuli might not effectively evoke vivid sadness, nor might\nthey induce intense, profound sadness in subjects or bring\nabout significant changes in brain activity. This limitation\ncan lead to poor results in emotion recognition. Saarim\u00e4ki\net al. [3] found the lowest average classification accuracy\nfor sadness (18%) in their study. In Raz et al. 's work [4],\nit demonstrated that long and complex movie clips are\nmore suitable for inducing the dynamic changes associated\nwith sustained emotional experiences. Furthermore, Xu et\nal. [5] suggested that a 10-minute long stimulus could be\nmore beneficial for brain signal analysis, closely mirroring\nsustained emotional experiences. This finding confirms that\nemotional representations induced by long-duration stimuli\nare superior to those induced by short-duration stimuli.\nHowever, most existing EEG-based emotion studies uti-\nlize short-duration stimuli to evoke responses [6], [7]. The\nalgorithms used in these studies typically perform emo-\ntion analysis on segmented EEG signals (segment-based\nemotion analysis), rather than processing an entire trial\nas a whole (trial-based emotion analysis) [8], [9], [10]. A\nmore detailed introduction to these current algorithms is\nprovided in Section 2. This approach neglects the temporal\ninformation of the entire signal sequence, thus impacting\nthe recognition performance on the whole EEG signal.\nImplementing a long-duration, whole-segment EEG emo-\ntion recognition algorithm faces the challenge of handling\nvariable-length data inputs.\nThe transformer network excels in processing variable-\nlength data and has been increasingly applied in the field of"}, {"title": "2 RELATED WORK", "content": "In recent years, a growing number of deep learning algo-\nrithms have been developed for EEG-based emotion recog-\nnition. For example, Liang et al. [15] introduced EEGFuseNet\nto dynamically capture both shallow and deep EEG features\nwithout requiring label information. Fernandez et al. [16]\nleveraged prior knowledge to train a CNN using differential\nentropy (DE) features, improving the network's ability to\nrecognize emotions. To incorporate structural information,\nZhong et al. [9] developed a restricted graph network with\nNodeDAT and EmotionDL, achieving an impressive accu-\nracy of 85.30% in 3-class cross-subject emotion recognition."}, {"title": "2.1 Short Segments based EEG Analysis", "content": "EEG for emotion analysis [10], [11], [12]. For example, Liu\net al. [13] proposed four variant transformer frameworks\n(spatial attention, temporal attention, sequential spatial-\ntemporal attention and simultaneous spatial-temporal at-\ntention) for EEG-based emotion recognition, exploring the\nrelationship between emotion properties and EEG features.\nSimilarly, Sun et al. [12] introduced a transformer-based dy-\nnamic graph convolutional neural network (CNN) designed\nfor feature fusion, where the extracted graph features are\nfurther updated by the transformer. Additionally, Wei et al.\n[14] proposed a transformer capsule network that integrates\nan EEG transformer module to extract EEG features and\nan emotion capsule module to refine these features for eas-\nier classification. However, existing transformer-based EEG\nalgorithms have several notable limitations, such as inade-\nquate incorporation of spatial and spectral information, in-\nsufficient handling of long-term dependencies, and a lack of\nresearch on long-term EEG modeling methods. To tackle the\nexisting limitations, we propose a novel DuA transformer\nframework for long-term continuous EEG emotion analysis.\nThe main contributions of this study are summarized as\nbelow.\n\u2022 We propose a novel Dual Attentive (DuA) transformer\nframework for long-term continuous EEG analysis. This\nnovel framework enhances traditional attention mecha-\nnisms by improving the performance of emotion decod-\ning for long sequential signals, reducing GPU memory\nusage, and effectively utilizing the spatial-spectral and\ntemporal information of EEG signals.\n\u2022 We approach emotion analysis with a trial-based inter-\npretation. Unlike traditional methods that segment EEG\ntrials into short, fixed-length segments (e.g., 1 second)\nand assign the same label to all segments within a\nsingle trial, our proposed framework processes entire\nEEG trials with variable lengths. This allows for a\nmore flexible and comprehensive analysis of long-term\ncontinuous EEG data.\n\u2022 Extensive experiments are conducted on our self-\nconstructed long-term EEG database and two\nbenchmark databases, covering various data lengths\nand emotional states. The model's reliability and\neffectiveness are validated using a strict trial-based\nleave-one-subject-out cross-subject cross-validation\nprotocol, resulting in an average performance\nenhancement of 5.28%.\nMost existing EEG-based emotion recognition al-\ngorithms rely on segmenting EEG trials into fixed-length\nsegments (e.g., 1 second), with each segment assigned the\nsame emotional label [7], [17], [18]. This approach assumes\nthat the emotional state remains constant throughout each\nsegment in a single trial.\nIn practical emotion experiments, labels are typically\nprovided for entire trials rather than individual segments.\nEmotional states are dynamic and can fluctuate significantly\nover longer periods, making it unrealistic to assume a static\nemotional label for each short segment. Moreover, research\nhas shown that emotion elicitation is an accumulative pro-\ncess, evolving rather than remaining fixed [19]. Given these\nconsiderations, emotion analysis using EEG signals should\nbe conducted in a trial-based manner rather than a segment-\nbased manner, especially in supervised learning contexts\nwhere accurate label information is crucial. This trial-based\napproach acknowledges the dynamic nature of emotional\nstates and allows for more accurate and realistic emotion\nrecognition in continuous EEG analysis."}, {"title": "2.2 Transfer Learning in aBCI", "content": "Emotion recognition algorithms face significant challenges\ndue to the variability and complexity of EEG signals and the\nindividual differences in these signals. To address these is-\nsues, transfer learning has emerged as a valuable technique,\nhelping to mitigate the discrepancies in feature distribution\nextracted from various subjects and enhancing model stabil-\nity in cross-subject EEG-based emotion recognition tasks.\nFor non-deep transfer learning methods, Zheng et al. [20]\nemployed transfer component analysis (TCA) and transduc-\ntive parameter transfer (TPT) to achieve an average accuracy\nof 76.31%, compared to 56.73% without transfer learning.\nConsidering the marginal and conditional distributions in\nthe feature alignment process, Luo et al. [21] introduced a\nmanifold-based domain adaptation method. This method\nadaptively and dynamically adjusted the importance of\nmarginal and conditional distributions during the feature\nalignment process based on Grassmann manifold space,\nleading to a further average improvement of 3.54%.\nDeep transfer learning algorithms have also shown\npromise in this field. Jin et al. [22] introduced an EEG-based\nemotion recognition model using domain adversarial neu-\nral networks (DANN) The results demonstrated that mod-\nels using deep transfer learning frameworks outperformed\nthose using non-deep transfer methods, with average accu-\nracy increasing from 76.31% to 79.19%. To further enhance\nfeature alignment, a series of improved DANN frameworks\nwere proposed [23], [24]. Considering the interaction feature\nrepresentation between sample features and prototype fea-\ntures, Zhou et al. [8] introduced a prototype-representation\npairwise learning-based transfer learning framework. This\napproach not only addressed individual differences but also\ntackled the issue of noisy labeling, achieving state-of-the-\nart (SOTA) results in cross-subject emotion recognition. The\nabove studies demonstrate that transfer learning, both non-\ndeep learning and deep learning, plays a crucial role in ad-\nvancing aBCI systems by improving the generalization and\nrobustness of emotion recognition models across different\nindividuals. Future research should continue to explore op-\ntimized transfer learning methods, aiming to further bridge"}, {"title": "2.3 Transformer Network with Attention Mechanism", "content": "Transformer networks, introduced by Vaswani et al. [25],\nhave become integral in both computer vision and natural\nlanguage processing. In computer vision, models such as\nvision transformers (ViT) and swin transformers have set\nnew standards. In natural language processing, models like\nBERT and GPT exemplify their success. The strength of\ntransformer networks lies in their extensive use of attention\nmechanisms to process sequential data. This mechanism\nenables the model to identify and focus on the most relevant\nparts of the input. For each input element, the model assigns\na weight indicating its importance, and through learning,\nit adaptively adjusts these weights. This process mimics\nhuman attention to specific details, significantly enhancing\nthe model's performance.\nIn the field of continuous EEG analysis, a number of\nstudies have integrated attention mechanisms and trans-\nformer networks to enhance performance. For example, Jia\net al. [11] proposed a 3D fully-connected network based\non attention mechanisms for DE feature projection. Tao et\nal. [10] incorporated self-attention mechanisms into CNNs\nand long short-term memory (LSTM) networks for better\nfeature representation. Additionally, Si et al. [26] introduced\na temporal aware mixed attention-based convolution and\ntransformer network, which combines CNN and attention\nmechanisms to jointly extract local and global emotional"}, {"title": "3 METHODOLOGY", "content": "To address the limitations of traditional transformer net-\nworks in effectively utilizing the diverse aspects of long-\nterm EEG signals, we propose a DuA transformer network\nspecifically designed for continuous EEG analysis. DuA\ntransformer includes three key modules. Spatial-Spectral\nNetwork Module. This module targets the spatial domain\nby analyzing the spatial distribution of EEG signals. It\ncaptures intricate spatial relationships between different\nEEG channels in terms of spectral patterns, enabling the\nmodel to understand complex spatial dependencies. Tempo-\nral Network Module. This module focuses on the temporal\ndomain, effectively modeling the dynamic changes in EEG\nsignals over time. By doing so, it captures the temporal\ndependencies and variations inherent in EEG data, allowing\nfor a more nuanced understanding of the temporal dynam-\nics. Transfer Learning Module. This module adapts the\nmodel to new individuals by leveraging knowledge from\npreviously seen data, ensuring better generalization across\ndifferent individuals. It enhances the model's ability to gen-\neralize and perform well on unseen subjects by transferring\nlearned features and patterns from prior data. The details of\neach module will be illustrated below."}, {"title": "3.1 Transformer Basics", "content": "In this section, we will introduce the encoder part of\nthe transformer network and its various configurations.\nThe input sequence is processed through the encoder to\nextract useful information and project the sequence into\nlow-dimensional features for subsequent downstream tasks,\nsuch as classification. The encoder consists of several\nstacked encoder layers. The input and output of the first\nlayer are sequences, and each subsequent layer's input is the\noutput from the previous layer. Each encoder layer is com-\nposed of a multi-head self-attention (MHSA) mechanism\nand a feed-forward network (FFN), connected by residual\nconnections for deeper network construction, followed by\nlayer normalization (LN).\nThe MHSA mechanism consists of several self-attention\nunits. Self-attention operates on a \"matching\" mechanism,\nmapping each element of the sequence into Key, Query,\nand Value vectors. It calculates the dot product between\nthe Query and Key vectors, transforming the result into\nattention scores ranging from 0 to 1 using the Softmax\nfunction. These attention scores are then multiplied with the\nValue vectors as follows:\n\\begin{equation}\n\\begin{cases}\nq = Wq \u00d7 X, q \u2208 R^{n\u00d7dk},\nk = Wk \u00d7 X, k \u2208 R^{n\u00d7dk},\nv = W \u00d7 X, v \u2208 R^{n\u00d7dk},\n\\end{cases}\n\\tag{1}\n\\end{equation}\n\\begin{equation}\nA (q, k, v) = Softmax(\\frac{q k^T}{\\sqrt{dk}}) . v,\n\\tag{2}\n\\end{equation}\nwhere X = {X1, X2, ..., Xn} is the input sequence, n is\nthe length of the sequence, and dk is the dimension of\nthe vectors, which also serves as the normalization factor\nto maintain the numerical stability of the attention scores.\nMHSA consists of multiple self-attentions, meaning the\noutputs from several self-attentions are integrated after\nmapping as:\n\\begin{equation}\nMHSA(X) = W^o \u00d7 (head\u2081, head2, ..., headi),\n\\tag{3}\n\\end{equation}\nwhere head; is given as:\n\\begin{equation}\nheadi = A(qi, ki, Vi).\n\\tag{4}\n\\end{equation}\nFollowing MHSA, there is a FFN layer, composed of\nfully connected networks. It performs a nonlinear mapping\nindividually and identically on the output of MHSA at\neach position in the sequence. First, the features from the\nMHSA output undergo an upscaling projection to obtain\nhigh-dimensional feature representations. Then, these high-\ndimensional features are projected down to the original\ndimension size for seamless integration into the subsequent\nlayer. The entire feed-forward network is as follows:\n\\begin{equation}\nFFN(X) = GeLU(XW1+b1)W2 + b2,\n\\tag{5}\n\\end{equation}\nAmong these, W1 \u2208 RDm\u00d7Df, b\u2081 \u2208 RDf.W2 \u2208 RDfXDm,\nand b2 \u2208 RDm are all trainable parameters. Typically, Df\nis larger than Dm, and generally Df is four times Dm.\nLN is applied after the residual connection, normalizing\nthe output of the forward propagation to ensure that the\noutput and input maintain a similar distribution. This helps\nto reduce the phenomenon of covariance shift and accelerate\nthe convergence speed of network training.\n\\begin{equation}\nLN(X) = \\frac{X - E(X)}{\\sqrt{var[X] + \\epsilon}},\n\\tag{6}\n\\end{equation}\nIn summary, the entire encoder network layer is defined\nas:\n\\begin{equation}\nH' = LN(MHSA(LN(X) + LN(X))),\n\\tag{7}\n\\end{equation}\n\\begin{equation}\nH = LN(FFN(H') + H\u0384).\n\\end{equation}\nTransformers can process entire sequences using a non-\nautoregressive approach. However, the self-attention mech-\nanism within transformer networks is permutation invari-\nant; for any given sequence, the attention scores remain\nthe same regardless of the order of elements. However,\nthe position within a sequence is often critical information.\nTherefore, in transformer networks, sinusoidal and cosine\nformulas of different frequencies are used across different"}, {"title": "3.2 Spatial-Spectral Network Module", "content": "To enhance the representational power of features in EEG\nsignal processing, we propose a novel spatial-spectral net-\nwork. This network is designed to simultaneously extract\nsignificant spatial and frequency domain information from\nEEG signals at each time point, rather than processing these\ntypes of information separately.\nTo realize this purpose, we optimize the multi-head at-\ntention component of the spatial-spectral network to handle\nboth types of information simultaneously. Given an input\nsignal X \u2208 Rexf, where c is the number of channels and f\nis the number of frequency bands, the optimized multi-head\nself-attention mechanism maps the input signal X to Query,\nKey, and Value vectors. During the multi-head self-attention\ncomputation, even-indexed heads process the channels as\nsequences for spatial feature extraction, while odd-indexed\nheads process the frequency bands as sequences for spectral\nfeature extraction, as;\n\\begin{equation}\n\\begin{cases}\nheadi(qi, ki, vi) = Softmax(\\frac{qikiT}{\\sqrt{dk}}) . Vi,\nheadj (qj, kj, vj) = Softmax(\\frac{qjkjT}{\\sqrt{dk}}) . Vj,\n\\end{cases}\n\\tag{9}\n\\end{equation}\nHere, i represents odd indices and j represents even indices.\nThe transformation of the Query, Key, and Value vectors in\nthe transformer network is a linear mapping. Therefore, in\nthe proposed spatial-spectral network, it is only necessary to\ntranspose the Query, Key, and Value vectors of some heads\nto extract features from both spatial and spectral informa-\ntion, without the need to reconstruct the input sequences.\nNote that the inputs for spatial and spectral feature extrac-\ntion are the sequences formed by the different channels of\nthe input EEG signals at each time point and the extracted\nDE features at different frequency bands, respectively.\nThis proposed network simplifies the encoder process,\nreducing computational complexity while effectively cap-\nturing both spatial and spectral features. By transposing\nthe Query, Key, and Value vectors for specific heads, the\nnetwork could efficiently handle the different types of infor-\nmation."}, {"title": "3.3 Temporal Network Module", "content": "The temporal network module receives input from the\nspatial-spectral network, which provides characterized\nspatial-spectral EEG information at each second. This in-\nformation is represented as L \u2208 Rt\u00d7h, where t denotes the\ntime length and h = c \u00d7 f represents the concatenation of\nchannels and frequency bands. The resulting dimensionality\nof h, formed by combining channels and frequency bands,"}, {"title": "3.4 Transfer Learning Module", "content": "One of the fundamental assumptions of deep learning is that\ntraining and testing data are drawn from the same distribu-\ntion and are ideally independent and identically distributed\n(i.i.d.). However, EEG signals exhibit significant individual\nvariability, making it challenging to satisfy this assumption\nin the task of emotion recognition from EEG signals. To\naddress this issue, a transfer learning module is incorpo-\nrated, which includes a feature extractor (Generator) and a\ndomain discriminator (Discriminator). During training, the\nfeature extractor and domain discriminator are trained in\nopposition to each other to learn domain-invariant features,\neffectively mitigating the impact of individual variability in\nEEG signals.\nIn the implementation, the subject data of the training\nset is defined as the source domain (Source), and the subject\ndata of the test set is defined as the target domain (Target).\nThe feature extractor is denoted as Gf(0f), the domain\ndiscriminator as Ga(a), and the classifier as Gy (0y). Here,\nz represents the extracted features at the output of the\ntemporal network, y represents the label of the training\nset, and domain distribution alignment is achieved through\nadversarial loss:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{E}(\\theta_f, \\theta_d, \\theta_y) = &\\sum_{z_i \\in D_s} L_y(G_y(G_f(z_i)), y_i) - \\\\\n& \\lambda \\sum_{z_i \\in D_s \\cup D_t} L_d(G_d(G_f(z_i)), d_i),\n\\end{aligned}\n\\tag{10}\n\\end{equation}\nwhere Ly is the classification loss, La is the discriminator\nloss, and d\u2081 is the label for data domain. d\u2081 = 0 indicates that\nthe data is from the source domain, while d\u2081 = 1 denotes\nthat it is from the target domain. To facilitate implementa-\ntion, a Gradient Reversal Layer (GRL) is incorporated into"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "Given the limited availability of EEG emotional databases\nspecifically designed for long-term EEG signals, we con-\nducted a study to fill this gap by undertaking EEG emo-\ntional experiments under prolonged emotional stimulation."}, {"content": "The study involved 50 subjects (25 males and 25 females, average age: 18.72 \u00b1 1.23 years). A total of 12 long-term videos (6 positive and 6 negative) was selected as stimuli, each with an average duration of 11.11 \u00b1 1.64 minutes. There was no content overlap among the selected videos. After watching each video, the subjects were asked to recall their emotions and rate their valence levels on a scale of [- 10, -5, 0, 5, 10]. Here, -10 indicates extremely negative, 10 indicates extremely positive, and 0 refers to neutral.\nEEG signals were recorded in real-time using a Brain Products device with 63 channels configured according to the 10-20 system, as each subject watched the 12 long-term videos. To manage fatigue, the experiment was conducted in two sessions, with each session featuring videos that induced a single type of emotion (either positive or nega- tive). These sessions were spaced at least one week apart. In total, the database comprises 600 long-term continuous EEG signal recordings.\nFor the recorded EEG signals, we extracted DE features from each channel every second across the following ten frequency bands: Theta (4-6 Hz), Alpha1 (6-8 Hz), Alpha2 (8-10 Hz), Alpha3 (10-12 Hz), Beta1 (12-16 Hz), Beta2 (16-20 Hz), Beta3 (20-28 Hz), Gamma1 (28-34 Hz), Gamma2 (34-39 Hz), and Gamma3 (39-45 Hz). For each trial of the long-term continuous EEG signal recordings, the EEG signals were then transformed into a three-dimensional matrix by time, channel, and frequency band.\nFor model validation, we adopt a trial-based leave-one- subject-out cross-validation protocol. Using our database of 50 subjects, we iteratively use the data from 49 subjects as the training set and the data from the remaining one subject as the test set. This process is repeated until each subject has served as the test set exactly once. The final validation results are obtained by calculating the average and standard deviation of these 50 test iterations. Additionally, leveraging the collected subjective scores ([-10, -5, 0, 5, 10]), we define three types of emotion classification tasks (2-class, 3-class, and 5-class). This approach ensures a robust assessment of the model's ability to generalize across different subjects and emotional states."}, {"title": "4.1 EEG Data and Experimental Protocol", "content": "4.2 Implementation Details\nIn our experiment, we utilize the proposed DuA trans- former, which integrates a spatial-spectral network and a temporal network as the feature extractor, and an MLP with a ReLU activation function as the domain discriminator. The parameters of the transformer network are randomly initialized from a uniform distribution.\nSpecifically, the spatial-spectral network is configured as below. The input is X \u2208 Rcxf, where c is the number of"}, {"title": "4.3 Emotion Recognition Results", "content": "We conduct an extensive evaluation of existing machine\nlearning and deep learning methods on long-term contin-\nuous EEG signals using trial-based leave-one-subject-out\ncross-subject cross-validation. As shown in Table 1, our\nproposed DuA transformer model demonstrates substan-\ntial performance improvements over traditional machine\nlearning methods, achieving 16.83% higher accuracy for\n2-class classification, 36.89% for 3-class classification, and\n31.73% for 5-class classification. When compared to ad-\nvanced deep learning techniques, our DuA transformer also\nshows significant performance enhancements. Specifically,\nit outperforms the best transfer learning method by 4.16%\nin 2-class classification, 10.50% in 3-class classification, and"}, {"title": "5 DISCUSSION CONCLUSION", "content": "We conduct ablation experiments to investigate the contri-\nbutions of various components of our model to its overall\nperformance. As shown in Table 4, we evaluate the role\nof the spatial-spectral network module under three condi-\ntions. (1) Complete Removal of the Spatial-Spectral Network\n(Temp-only). When the spatial-spectral network is entirely\nremoved, there is a significant drop in accuracy: 8.83% for 2-\nclass classification, 7.66% for 3-class classification, and 8.51%\nfor 5-class classification. This highlights the importance of\nthe spatial-spectral network in achieving higher accuracy\nacross different classification tasks. (2) Spatial Information\nOnly (Spat-Temp). Retaining the spatial-spectral network\nbut utilizing only spatial information results in a notice-\nable decline in performance across all classification tasks.\nThis indicates that spatial information alone is insufficient\nfor optimal model performance. (3) Spectral Information\nOnly (Spec-Temp). Similarly, when we retain the spatial-\nspectral network but focus solely on spectral information,\nthe results demonstrate that considering spatial information\namong EEG channels significantly benefits model perfor-\nmance. Balancing model performance and computational\ncost, our findings suggest that the proposed method strikes\nan effective balance. It not only reduces computational com-\nplexity but also preserves critical emotion representations,\nthereby maintaining robust performance across various clas-\nsification tasks. This balanced approach ensures that the\nmodel remains both efficient and effective in recognizing\nand classifying emotions."}, {"title": "5.1 Ablation Study", "content": "Spec-Temp w/o PE). In this condition, we maintain the temporal network but remove the positional encoding from the transformer network. The results reveal that positional encoding is crucial for capturing the temporal dynamics of the signals. The absence of positional encoding results in a 14.50% decrease in 2-class classification, a 11.51% decrease in 3-class classification, and a 9.17% decrease in 5-class classifi- cation. This decline highlights the importance of preserving the temporal order of signals in long-term EEG data, as it plays a vital role in the accurate classification of emotional states. Overall, these experiments illustrate the indispens- able nature of temporal features and positional encoding in enhancing the performance of emotion recognition models utilizing long-term EEG signals. The findings emphasize that both the ability to capture temporal dependencies and the maintenance of signal order are essential for achieving robust and accurate emotion classification."}, {"title": "5.2 Temporal Analysis Effect", "content": "To further evaluate the effect of temporal analysis on long- term continuous EEG signals, we replace the temporal net- work module with an LSTM network. For a fair comparison, we only modify the temporal network in the proposed framework, maintaining the spatial feature extraction net- work unchanged to eliminate any experimental interference. As shown in Table 6, our method significantly outperforms the LSTM method, improving recognition accuracy by 7.00% in 2-class classification, 15.00% in 3-class classification, and 12.84% in 5-class classification."}, {"title": "5.3 Hyperparameter Analysis", "content": "We conduct multiple sets of comparative experiments by adjusting the hyperparameters of our algorithmic frame- work. Given the constraints of computational power, we systematically analyze the model's sensitivity to various parameters. Specifically, we examine the number of heads in the spatial-spectral network (2, 4, 6, 8), the number of layers in the spatial-spectral network (1, 2, 3), and the hidden size in the temporal network (32, 64, 128, 256).\nTo ensure a robust analysis, we vary one set of hyperpa- rameters while keeping the others constant. This method allows us to isolate the effects of each parameter on the model's performance. The experimental results, presented in Fig. 3, demonstrate that our model exhibits low sensitivity to changes in these hyperparameters. This stability indi- cates that the model is robust across a range of parameter values. Moreover, the results reveal that achieving optimal performance does not require extreme parameter values. Instead, moderate values are sufficient to attain the best outcomes, suggesting a balance between model complexity and computational efficiency. This finding is particularly important for practical applications where computational resources are limited."}, {"title": "5.4 Conclusion", "content": "In this paper, we present a novel Dual Attentive (DuA) transformer framework designed to address the challenges associated with processing long-term continuous EEG sig- nals. Unlike existing segment-based processing pipelines, our approach treats a single trial as a comprehensive entity, analyzing dependencies across the entire trial by leveraging spatial-spectral and temporal information. The proposed DuA transformer outperforms various baseline methods, demonstrating the effectiveness of this optimized algorithm framework. By capturing the full scope of emotional expe- riences, our DuA transformer framework paves the way for the development of more accurate, reliable, and prac- tical emotion recognition systems. Furthermore, the DuA transformer framework's ability to integrate and process comprehensive EEG data can lead to breakthroughs in understanding the neural underpinnings of emotions. This advancement holds significant potential for applications in mental health, human-computer interaction, and affective computing, offering improved tools for diagnosing and monitoring emotional states, enhancing user experience in interactive systems, and advancing research in affective sciences."}, {"title": "6 ACKNOWLEDGMENTS", "content": "This work was supported in part by the National Natural Science Foundation of China under Grant 62276169, in part"}]}