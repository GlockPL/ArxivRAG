{"title": "ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance", "authors": ["Renyang Liu", "Ziyu Lyu", "Wei Zhou", "See-Kiong Ng"], "abstract": "In the rapidly evolving field of Artificial Intelligence Generated Content (AIGC), one of the key challenges is distinguishing AI-synthesized images from natural images. Despite the remarkable capabilities of advanced AI generative models in producing visually compelling images, significant discrepancies remain when these images are compared to natural ones. To systematically investigate and quantify these discrepancies, we introduce an AI-Natural Image Discrepancy Evaluation benchmark aimed at addressing the critical question: how far are AI-generated images (AIGIs) from truly realistic images? We have constructed a large-scale multimodal dataset, the Distinguishing Natural and AI-generated Images (DNAI) dataset, which includes over 440,000 AIGI samples generated by 8 representative models using both unimodal and multimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text vs. Image-to-Image (TI2I). Our fine-grained assessment framework provides a comprehensive evaluation of the DNAI dataset across five key dimensions: naive visual feature quality, semantic alignment in multimodal generation, aesthetic appeal, downstream task applicability, and coordinated human validation. Extensive evaluation results highlight significant discrepancies across these dimensions, underscoring the necessity of aligning quantitative metrics with human judgment to achieve a holistic understanding of AI-generated image quality.", "sections": [{"title": "Introduction", "content": "With the rapid advancement of deep learning techniques, the proliferation of Artificial Intelligence Generated Content (AIGC) has garnered significant attention across various domains, such as e-commerce, gaming, medicine, animation, and autonomous driving (Li et al. 2024; Qian et al. 2024). AI-generated Images (AIGI) have been one of the mainstream forms of AIGC, and a variety of AI image generative models have been proposed to make the generated synthetic images as realistic as natural images, ranging from the earlier generative adversarial networks (GANs) (Tao et al. Despite the proliferation of AI image generative models, AI-generated images are not qualified for real-world applications, and the discrepancies between generated synthetic images and realistic natural images still exist (Li et al. 2023a). Therefore, many AI-generated image quality assessment and evaluation methods have been proposed (Wang et al. For example, Wang et al. (Wang et al. 2023) established an AIGC Image Quality Assessment Database called AIGCIQA2023 from 6 text-to-image generative models, and devised a subjective evaluation framework to assess human visual preferences for each AI-generated image from three aspects, e.g., quality, authenticity, and text-to-image correspondence. PKU-I2IQA (Yuan et al. 2023) constructed a human perception-based image-to-image database named PKU-I2IQA and conducted a subject analysis based on both no-reference and full-reference methods. Different from prior studies focusing on establishing the AIGI dataset and conducting a subjective evaluation with human feedback, another group of studies (e.g. Pick-a-Pic (Kirstain et al. 2023), HPS v2 (Wu et al. 2023), TIFA (Hu et al. 2023), and ImageReward (Xu et al. 2023a)) focused on training a unified score model as the automatic evaluation metrics for AIGC image quality assessment (AIGIQA) measures, in contrast with traditional image quality assessment for natural images measures like Inception Score (Salimans et al. 2016) and FID (Heusel et al. However, some issues and problems have not yet been fully answered and solved. First, prior studies only considered unimodal promopted generation, e.g. Text-to-Image (T2I) or Image-to-Image (I2I), ignoring multimodal prompted content like Text vs.Image-to-Image (TI2I). The data size of the established T2I or I2I dataset is relatively small (around thousands of images), and might be insufficient for comprehensive analysis and evaluation. Second, prior studies typically focused on the perceptual quality of AIGC images with traditional image quality assessment metrics, the text-to-image correspondence, and subjective human preference. No studies have fully answered the important questions about what discrepancies exist between generated synthetic images and realistic natural images, and they lacked a systematic and compressive assessment and evaluation to investigate and interpret the discrepancies between AI-generated synthetic images and realistic natural images. The investigation and understanding of the discrepancies are key ways to realize the practical application of AIGC images in real-world scenarios and make breakthroughs in AIGC.\nTo address the above challenges, we proposed an AI-Natural Image Discrepancy Evaluation Benchmark to investigate and interpret what discrepancies still remained between AI-generated images and natural images, finally answering the important question \u201cHow far are AI generative models with respect to the visual forms of images?\u201d. Especially, we have two important contributions with reference to the key problems. Large Multimodal Evaluation Dataset construction: we have curated a large multimodal evaluation dataset called DNAI (Distinguishing Natural and AI-generated) dataset, with about 440,000 AIGI generated from 8 representative generative models based on both unimodal and mutimodal guidance including Text-to-Image (T2I), Image-to-Image (I2I), and Text vs. Image-to-Image (TI2I). The data size of our DNAI dataset scales to 100x that of prior datasets. Fine-grained Evaluation Framework, we propose a fine-grained evaluation framework to conduct the systematic and comprehensive assessment and evaluation of DNAI, covering 5 diverse and important aspects, including naive visual feature quality, semantic alignment among multimodal generation, aesthetic appeal, downstream applicability, and the coordinated human validation.\nThrough our DNAI dataset and fine-grained evaluation framework, we conduct extensive benchmark analysis and evaluation and conclude key insights to answer the discrepancy questions:\n\u2022 Significant discrepancies in key Areas: Al-generated images exhibit substantial discrepancies from natural images in terms of quantitative measures from all aspects, with about 10% to 30%.\n\u2022 Multimodal Alignment: Different prompted generation might have different semantic alignment scores. Generated images prompted with texts including both T2I and TI2I demonstrated remarkable semantic alignments.\n\u2022 Downstream Task Applicability: Significant differences exist in the usability of AI-generated images vs. natural ones in downstream tasks, highlighting the need for further improvements of generative models to ensure practical applicability in real-world scenarios.\n\u2022 Human Evaluation vs. Quantitative Metrics: Human evaluation results reveal larger discrepancies compared with quantitative metrics. It validates the necessity of incorporating human evaluation for coordination."}, {"title": "Related Work", "content": "Natural Image Evaluation: Over the past decades, numerous image quality assessment methods have been developed to evaluate traditional natural images (N. et al. These methods have focused on various visual features and properties, such as perceptual appearance (Zhang et al. 2018), naturalness (Ma et al. 2018), and aesthetics (Esfandarani and Milanfar 2018). For instance, BRISQUE evaluates natural image quality by analyzing spatial domain features, while PIQE assesses perceptual quality through block-based image segmentation, both serving as effective no-reference metrics. Other notable measures include FID and Inception Scores, which assess the quality and diversity of generated images, and SSIM (Wang et al. 2004) and PSNR, which quantify structural similarity and pixel-level accuracy. Beyond quantitative measures, human evaluations have also been leveraged to estimate natural image quality.\nAI-generated Image Evaluation: Recently, AI-generated content (AIGC) has seen significant advancements with the rise of generative models. Several methods and benchmarks have been introduced to evaluate AI-generated images (Zhang et al. 2023; Hu et al. 2023). These methods primarily focus on three aspects: perceptual quality, text-image correspondence, and aesthetics (Xu et al. Often training unified models to assess the overall quality of AI-generated images. For example, ImageReward (Xu et al. 2023a) presents a framework for aligning image generation with human preferences, AGIQA-3k (Li et al. 2023a) offers a comprehensive benchmark for assessing AI-generated image quality, and QBench (Wu et al. 2024) evaluates content quality across multiple dimensions.\nDespite advancements, little research has delved into the fine-grained differences between AI-generated and natural images, and existing datasets are often too small for comprehensive evaluation. To address these gaps, we propose an AI-Natural Image Difference Evaluation Benchmark, featuring a large Distinguishing AI-Natural Image Dataset and a systematic, fine-grained evaluation framework to thoroughly explore these differences."}, {"title": "AI-Natural Image Discrepancy Evaluation Benchmark", "content": "To tackle the challenges of current AI-generated image evaluation, we construct an AI-Natural Image Discrepancy evaluation benchmark (ANID) to evaluate the potential discrepancies between AI-generated images and natural images.\nOur ANID benchmark contributes to two core components: (1) A Distinguishing AI-Natural Image (DANI) Dataset is constructed, in which we collect AI-generated images from diverse generative models for real natural images in MS COCO dataset (Lin et al. 2014) based on three types of guidance prompts e.g. text-only, image-only and text-image prompts; (2) We devise a systematic and comprehensive evaluation framework to measure and evaluate the potential differences between AI-generated images and real natural images from five aspects including naive image quality, semantic alignment, aesthetic appeal, downstream applicability, and the coordinated human assessment. In the following parts, we introduce the DANI Dataset and the evaluation framework in detail."}, {"title": "Distinguishing AI-Natural Image Dataset", "content": "We construct the Distinguished Natural and AI-generated Image Dataset based on the classical natural image dataset MS COCO dataset. We select 5,000 images covering diverse captions from the MS COCO dataset and collect a total of 25,000 text-image pairs by pairing each image with five different pieces of text descriptions. The 25,000 text-image pairs are seen as the referenced natural image set.\nFor each text-image sample in the referenced natural image set, we collect AI-generated images from 8 representative generative models, guided by three types of prompts, including text-only prompts, image-only prompts, and text-image prompts. The 8 representative generative models are from diverse generative models ranging from the earlier generative adversarial networks (GANs) (e.g. GALIP (Tao et al. 2023), DF-GAN (Tao et al. 2022)) to the recent benchmarking diffusion models (DMs) like Stable Diffusion v1.4, v1.5, v2.1, XL (Rombach et al. 2022), Versatile-Diffusion (Xu et al. 2023b) and OpenAI DALLE 2 (Ramesh et al. 2022).\nThe referenced natural images and the generated images comprise our distinguishing AI-Natural image dataset. More details about data construction and collection are presented in Section Appendix A.\nOur DANI Dataset has convincing contributions:\n\u2022 DANI Dataset is the most extensive dataset for AI-generated image evaluation, and the data size scales to 100x.\n\u2022 DANI Dataset is guided by both unimodal prompts and multi-modal prompts (text-image). Prior datasets barely considered text prompts.\n\u2022 Diverse generative models are exploited, ranging from the earlier GAN models, the recent popular diffusion models, and the influential large commercial generative model DALL-E 2 issued by OpenAI.\nOur large and comprehensive dataset works as the foundation of the evaluation benchmark and enables thorough evaluation and analysis of the differences between AI-generated images and realistic natural images."}, {"title": "Fine-Grained Evaluation Framework", "content": "On top of the DANI dataset, we construct a fine-grained evaluation framework to systematically measure and interpret the differences between AI-generated images and realistic natural images from five different aspects, including naive image quality, semantic alignment, aesthetic appeal, downstream applicability, and coordinated human assessment.\n1. Naive Image Quality: We leverage traditional image quality assessment methods for natural images to measure the AI-Natural image differences. In order to perform fine-grained analysis and exploration, we estimate the AI-Natural image differences from three levels by considering the inherent properties of image-style content, ranging from the low-level visual features, frame-level visual features to the holistic content distribution.\n\u2022 First, we use the pixel-level similarity measures like SSIM, LPIPS, and DISTS to compute the visual similarities between the paired AI-generated image and natural image and use the inverse similarity values to quantify the AI-Natural image differences from the low-level visual view.\n\u2022 Second, we use frame-level visual features to qualify the perceptional quality of images and compare the global-level visual quality differences between AI-generated images and natural images from the high-level visual view. The exploited frame-level visual features include PIQE (N. et al. 2015), IL-NIQE (Zhang, Zhang, and Bovik 2015), MUSIQ (Ke et al. 2021), DBCNN (Zhang et al. 2020), LIQE (Ma et al. 2018), Inception Score (Salimans et al. 2016), CLIPIQA (Radford et al. 2021a), TRES (Gu et al. 2015), HyperIQA (Su et al. 2020), UNIQUE (Zhang et al. 2021), BRISQUE, NIQE (Mittal, Soundararajan, and Bovik 2013), NRQM (Ma et al. 2018), etc. These metrics provide insights into global-level visual quality differences.\n\u2022 Third, we investigate the visual content distribution to capture the holistic differences between AI-generated images and natural images by computing the FID and Inception Score of the whole AI-generated image dataset and the referenced natural image set.\nBased on the different levels of image quality measures, our framework can evaluate the fine-grained AI-Natural image differences, covering both low-level and high-level. More details about the utilized traditional image quality assessment measures are described in Appendix Sec. C.\n2. Semantic Alignment: Generative models require the referenced prompts as semantic guidance to generate images. Therefore, semantic alignment can be an important quality indicator of the AIGIs. We use the widely used CLIP model (Radford et al. 2021b) to measure semantic alignment and compare the different CLIP Scores from natural text, image, and the paired AI-generated image-text prompt. The studied AI-generated images were originally generated from the 8 Representative generative models based on three types of prompts as described in Appendix Sec. B; the semantic alignment evaluation can provide a fine-grained analysis of the potential differences in semantic alignment with respect to both unimodal and multimodal guidance.\n3. Aesthetic Appeal: Aesthetic Appeal evaluation is to estimate the visual appeal and artistic quality of images, reflecting the visual attractiveness and artistic quality of images. We utilize the classical aesthetic measures NIMA (Esfandarani and Milanfar 2018) and LAION-AES (Schuhmann et al. 2022) as the quantitative metrics and compare the aesthetic scores for the paired AI-generated and natural images.\n4. Downstream Applicability: This aspect is devised to investigate the practical utility of AI-generated images in downstream tasks and evaluate whether AI-generated images can have different practical utilities with realistic natural images in downstream application tasks. We mainly focus on two classical downstream tasks, e.g., image recognition and object segmentation. For the image recognition task, we evaluate the Classification Mismatch Rate (MR) rates between AI-generated images and natural images when using a pre-trained image recognizer like ResNet-152 (He et al. 2016) model. For the object segmentation task, we use the Intersection over Union (IoU) metric (Everingham et al. 2010) as quantitative measures and investigate the segmentation results for AI-generated images with the segmentation model U2NET (Qin et al. 2020).\n5. Human Assessment: We involve human assessments to coordinate the above evaluation aspects. We develop the human assessment interface to demonstrate AI-generated images with the referenced natural images and text descriptions and collect human ratings (on a scale from 1 to 5) alongside the following three aspects: naive image quality, semantic alignment, and Aesthetic Appeal as human assessments. Human participants are provided with example images before the evaluation to understand high and low scores without being given specific numerical values, ensuring unbiased and informed ratings. Detailed information about human assessment procedures is provided in Appendix Sec. D.\nThrough the integration of diverse and comprehensive evaluation dimensions, our fine-grained evaluation framework offers a systematic assessment solution to investigate and interpret the differences that still remain between AI-generated images and natural images."}, {"title": "Evaluation", "content": "With respect to the 5 different aspects, we conducted benchmark experiments and performed experimental evaluation alongside the following research questions.\nRQ1: What are the fine-grained discrepancies between AIGIs and natural images?\n\u2022 RQ1.a: What discrepancies between I2I-guided AIGIS and their natural counterparts can be interpreted via the pixel-level quality measures?\n\u2022 RQ1.b: What discrepancies between multimodal-guided AIGIs and their natural counterparts can be revealed via the frame-level metrics?\n\u2022 RQ1.c: How do the structural visual content distributions differ between AIGIs and natural images?\nRQ2: How significant are the discrepancies in semantic alignment between AIGIs and natural images in different types of guided prompts?\nRQ3: What are the discrepancies in aesthetic appeal between AIGIs and natural images?\nRQ4: How do AIGIs differ from natural images in downstream task applicability?\nRQ5: Are human assessment results consistent with quantitative measures? What are the discrepancies revealed from human evaluation?\nExperimental Setting\nWe use the described quantitative evaluation metrics, and details of all metrics are presented in Appendix Sec. B.\nFull-reference metrics: For full-reference metrics like pixel-level image quality, we directly report the calculated value cause it already can present the discrepancy.\nNo-reference metrics We calculate Difference Rate (DR) for each quantitative metric in non-reference scenarios, by subtracting the value of each AI-generated image from its corresponding natural reference image and then averaging these difference rates, defined as follows:\n\nDifference Rate = $\\frac{1}{n}\\sum_{i=1}^{n}\\frac{|m(X_i) \u2013 m(N_i)|}{m(N_i)}$\n\nwhere n is the number of images, m represents the metric, X denotes the generated images, and N refers to the referenced natural images."}, {"title": "RQ1: Naive Quality Results", "content": "To evaluate the image quality of AI-generated images versus natural images, we conducted a comprehensive analysis using a variety of metrics that assess visual feature quality, naturalness, and similarity of images. Our findings reveal substantial discrepancies between AIGIs and natural images across these dimensions.\nRQ1.a Pixel-level We evaluated the visual similarity between AI-generated and natural images using structural metrics like SSIM, LPIPS, DISTS, and PSNR. The results, as shown in Table 2, indicate that Al-generated images exhibit significantly lower similarity to natural counterparts. AI-generated images show a 20% to 50% reduction in SSIM, highlighting a major loss in structural fidelity. LPIPS and DISTS further reveal notable perceptual dissimilarities, with LPIPS scores ranging from 0.19 to 0.60. Additionally, PSNR values, between 12.42 and 23.25, suggest that AI-generated images have higher noise levels, reducing their overall visual similarity.\nRQ1.b Frame-level We assessed the frame-level quality of AI-generated and natural images using a suite of metrics, including MUSIQ, DBCNN, HyperIQA, LIQE, BRISQUE, NRQM and NIQE.  uses radar figures to visualize the results from the six metrics, and indicates that there are significant quality differences between Al-generated and natural images. These deviations typically range from 10% to 20%, irrespective of the guidance method used, with some instances showing quality differences as high as 30%. This underscores the considerable gaps between AIGIs and natural images in terms of global image quality. Moreover, the results of NRQM and NIQE reveal that Al-generated images significantly differ from natural images in terms of naturalness. Specifically, AI-generated images show notable deviations from natural images in visual realism and adherence to natural scene statistics. The NRQM difference rates range from 5% to 20%, while the NIQE difference rates range from 10% to 50%, highlighting the challenges AI-generated images face in replicating the inherent naturalness of real-world scenes. More detailed results with more metrics are presented in Appendix Sec. C.\nRQ1.c Visual Content Distribution We present the FID and Inception Score results, presented suggesting that AI-generated images often exhibit relatively higher FID scores and lower Inception Scores. This finding indicates that current generative models, particularly GAN-based models and earlier versions of Stable Diffusion, struggle to consistently produce high-quality images. Notably, these models tend to generate better images when a reference image is provided (TI2I), as it has more detailed guidance.\nIn conclusion, our evaluation of image visual feature quality, naturalness, and similarity reveals significant disparities between AI-generated and natural images. These differences highlight the current limitations of AIGC technologies in matching the visual quality and realism of natural images."}, {"title": "RQ2: Alignment Results", "content": "In our evaluation of alignment results, we assessed how well AI-generated images correspond to their respective reference prompts across different guidance types: Text-to-Image (T2I), Image-to-Image (I2I), and Text-and-Image-to-Image (TI2I). And we find that Al-generated images often struggle to maintain high alignment with their prompts, especially the image-only reference provided.  indicate that AI-generated images typically achieve lower CLIP Score compared to natural images, with alignment discrepancies ranging from 4.02% to 36.69%. We also find that even if the AIGIs can get a higher CLIP score, especially with text and image-guided, they are looking stranger to humans. The CLIP score, which measures the alignment between text descriptions and generated images by embedding both into a shared space and calculating cosine similarity, revealed a noticeable gap in how accurately AIGC images reflect the input prompts. This suggests that current AIGC models often struggle to maintain high fidelity to the provided references, leading to variations that may impact the intended alignment. Our findings underscore the challenges that these models face in consistently generating content to accurately reflect the specified input conditions, highlighting both their strengths and limitations in real-world applications."}, {"title": "RQ3: Aesthetic Appeal", "content": "We assessed the aesthetic appeal of AI-generated and natural images using NIMA and LAION-AES metrics and found that Al-generated images generally fall short of natural images in terms of aesthetic quality.  presents the results, which reveal that AI-generated images receive lower scores for aesthetic appeal, with discrepancies ranging from 3.34% to 23.85%. These findings indicate that while AI models can produce visually appealing images, they often lack the nuanced artistic quality and emotional impact that are characteristic of natural images."}, {"title": "RQ4: Applicability Results", "content": "When evaluating the applicability of AI-generated images, we focused on two crucial downstream tasks: image recognition and semantic segmentation. Our findings indicate significant discrepancies between the performance of AIGC images and natural images in these tasks.\nImage Recognition: For image recognition, we measured the classification mismatch rate, which quantifies the differences in predicted labels between AIGC images and their natural counterparts. The results in Table 4 showed a substantial gap, with the mismatch rate ranging from 29.89% to 94.44%. This indicates that AI-generated images often fail to achieve the same level of accuracy as natural images, highlighting the limitations of current AIGC models in producing reliable content for image recognition tasks."}, {"title": "Human Validation", "content": "To rigorously assess the differences between AI-Natural images and to evaluate the effectiveness of existing metrics in measuring AIGC image quality, we conducted human evaluations focusing on three key aspects: Image Quality, Alignment, and Aesthetic Appeal. We excluded results for Applicability since this dimension inherently involves human-generated ground truth labels. Participants rated images on a scale from 1 (poor) to 5 (excellent). Across all three dimensions, AI-generated images generally received lower scores, often between 2 and 3, compared to natural images.\nImage Quality: Participants assessed the overall quality of the images, considering factors such as clarity, detail, and the presence of artifacts. The results revealed a noticeable gap between AIGC images and their natural counterparts, with the generated images often lacking the same level of detail and clarity.\nAlignment: This aspect measured how well the generated images matched the given prompts, revealing a notable discrepancy. Generated images frequently deviated from the descriptions, indicating issues with model adherence to specified conditions.\nAesthetic: The aesthetic appeal of the images was rated based on visual attractiveness, composition, and overall artistic quality. While some generated images displayed impressive artistic qualities, the overall consistency and aesthetic appeal were generally considered inferior to that of natural images.\nOverall, our human evaluation results  reveal significant discrepancies between AI-generated images and natural images across all three assessed aspects: quality, alignment, and aesthetic appeal. AIGC images consistently received lower scores, emphasizing the need for further advancements to narrow these gaps. These findings underscore the importance of continued research and development in AI image generation to bring AIGC images closer to the standards of natural images.\nMoreover, the results from human validation indicate that traditional quantitative metrics often fail to align with human preferences, suggesting that they are not fully compatible with assessing AIGC images. This highlights the necessity for developing new measures tailored specifically to the unique characteristics of AI-generated content."}, {"title": "More Evaluations", "content": "To analyze the differences between AI-generated and natural images across various categories, we evaluated the performance of generated data from eight categories ('person', 'animal', 'indoor', 'outdoor', 'vehicle', 'food', 'sports', 'accessory') across three key aspects: (1) Naive Quality, including pixel-level similarity (SSIM), frame-level quality (CLIPIQA), and content distribution (FID); (2) Alignment, measured by CLIP Score; and (3) Aesthetic Quality, assessed by LAION-AES. Partial results are presented  we observe substantial variability in the differential rates for categories such as \u201cfood\u201d and \u201csports\u201d. Compared to \"sports\u201d images, the \u201cfood\u201d demonstrate significantly higher discrepancies in image quality (CLIPIQA), revealing AI models struggle to generate natural food images. , the FID and SSIM metrics highlight distinct trends across categories. The \u201caccessory\u201d category exhibits the highest FID values, indicating poorer content distribution, while the \"animal\" category consistently achieves higher SSIM scores, reflecting better pixel-level similarity. These results suggest that certain categories, such as \u201caccessory\u201d and \u201cindoor\u201d, remain particularly challenging for AIGC models to reproduce accurately, especially in maintaining semantic consistency and achieving balanced content generation."}, {"title": "Conclusion and Discussion", "content": "In this paper, we develop a systematic and comprehensive framework to evaluate the discrepancies between AI-Natural images using the formulated DNAI dataset. Our findings reveal significant disparities in naive image quality, semantic alignment, aesthetics, and downstream task applicability. While AIGC models have achieved impressive innovations, AIGIS still lag behind natural images in these critical areas. Besides, human evaluations also reveal lower scores for AIGC in terms of visual quality, semantic alignment, and aesthetics, emphasizing the gaps between AIGIs and natural images. Finally, the poor utility of AIGIs in downstream tasks indicates practical application challenges for AIGC.\nThis study also has limitations, including reliance on specific quantitative metrics and models available at the time. Future studies should incorporate more generative models, larger datasets, and sophisticated metrics to capture further discrepancies. Addressing these limitations and improving models will help reduce the perceptual gaps between AIGC images and natural images, facilitating better integration into real-world applications."}, {"title": "Appendix Overview", "content": "The appendix provides supplementary details and additional experimental results that were not included in the main paper due to space limitations. It is organized as follows:\n\u2022 Section A: In-depth Description of the DNAI Dataset.\n\u2022 Section B: Detailed Overview of the Evaluated Generative Models.\n\u2022 Section C: Comprehensive Summary of Evaluation Metrics.\n\u2022 Section D: Extended Information on Human Perceptual Evaluations.\n\u2022 Section E: Discussion on the Safety Mechanisms in Existing Diffusion Models."}, {"title": "A Details of the DNAI Dataset", "content": "Overview\nThe DNAI dataset is a carefully curated collection of both natural and AI-generated images, designed to support comprehensive evaluations of image generation models. The natural images are sourced from the COCO validation set. AI-generated images were produced using eight different models. These images were generated under three distinct guidance modes: Text-to-Image (T2I), Image-to-Image (I2I), and Text vs. Image-to-Image (TI2I). Representative samples from the DNAI dataset are shown in Figure 8. For a detailed breakdown of the DNAI dataset composition, please refer to Table 8.\nNatural Images\nThe DNAI dataset features a carefully curated selection of natural images sourced from the COCO validation set. This subset comprises 5,000 images, each paired with at least five unique captions, totaling 25,014 captions. These rich and varied textual descriptions enhance the dataset\u2019s utility across a wide range of image-to-text and text-to-image tasks, making it an invaluable resource for evaluating and training image generation and understanding models. In this study, we use these images and their accompanying (five) captions to guide the generative models in creating new images.\nGenerated Images\nThe DNAI dataset includes AI-generated images created using several advanced generative models across three distinct guidance modes: Text-to-Image (T2I), Image-to-Image (I2I), and Text-and-Image-to-Image (TI2I). These modes collectively contributed a significant volume of images to the dataset. The models utilized in this process include DFGAN, GALIP, various versions of Stable Diffusion (v1.4, v1.5, v2.1, XL), Versatile Diffusion, and DALLE 2. Each model, except DALL\u00b7E 2, generated 25,000 images per guidance mode. DALLE 2 produced 5,000 images per mode, resulting in a total of 440,000 generated images. It is important to note that DF-GAN and GALIP were used only for T2I, while DALL\u00b7E 2 was employed for both T2I and I2I modes. For detailed information, please refer to Table 8.\nImage Resolution\nThe resolution of the generated images varies depending on the model used. Images generated by GALIP are 224x224 pixels, while those produced by DF-GAN are 256x256 pixels. The other models, including various versions of Stable Diffusion and DALL\u00b7E 2, generate images at a resolution of 512x512 pixels.\nDataset Composition\nThe DNAI dataset offers a robust foundation for evaluating the performance and capabilities of various AIGC models under different conditions and guidance modes. By encompassing a broad spectrum of images and resolutions, the dataset ensures comprehensive coverage of the potential use cases and challenges associated with AI-generated content. The combination of natural and generated images, alongside diverse textual descriptions, facilitates a thorough assessment of image generation models, supporting the development of more accurate and reliable AI systems."}, {"title": "B Evaluated Generative Models", "content": "GALIP (Generative Adversarial Latent Image Processing): (Tao et al. 2023) is a generative model that produces high-quality images by leveraging adversarial techniques. It utilizes latent image processing to enhance both the quality and fidelity of generated content. GALIP's architecture involves a generator and discriminator, where the generator creates images from latent vectors, and the discriminator evaluates their authenticity. Through iterative training, GALIP achieves highly detailed and realistic image synthesis.\nDFGAN (Deep Fusion Generative Adversarial Network): (Tao et al. 2022) focuses on generating images from text descriptions using deep fusion techniques. The model integrates textual information into the image generation process through multiple layers of fusion, ensuring alignment with input descriptions. DFGAN operates in two stages: the first generates a coarse image based on the text, and the second refines the image, enhancing detail and coherence.\nStable Diffusion v1.4 (SD_V14): (Rombach et al. 2022) is part of the Stable Diffusion family, designed for image synthesis through iterative diffusion processes. Starting with pure noise, the model refines it into coherent images through a series of steps, guided by a learned model that predicts noise distribution. SD_V14 is recognized for producing high-resolution images with fine textures and complex structures.\nStable Diffusion v1.5 (SD_V15): (Rombach et al. 2022) builds on SD_V14, introducing enhancements in architecture and training techniques to improve image quality, consistency, and the handling of complex scenes. While continuing to use the diffusion process, SD_V15 incorporates better optimization strategies and larger datasets, yielding superior results.\nVersatile Diffusion: (Xu et al. 2023b) is a diffusion-based model designed for a variety of image generation tasks, including style transfer, image inpainting, and super-resolution. Its versatility lies in its ability to adapt the diffusion process to the specific requirements of each task, making it suitable for diverse applications.\nStable Diffusion v2.1 (SD_V21): (Rombach et al. 2022) is an advanced iteration of the Stable Diffusion series, offering significant improvements in image fidelity and generation speed. With optimizations in the diffusion process, better noise handling, and enhanced training algorithms, SD_V21 produces more realistic and high-quality images, making it highly effective for a range of creative and practical uses.\nStable Diffusion XL (SD_XL): (Rombach et al. 2022) represents the most advanced model in the Stable Diffusion series, capable of generating extremely high-resolution images with intricate details. SD_XL uses an extended number of diffusion steps and a larger network architecture to manage increased complexity, making it ideal for applications requiring ultra-high resolution and precision, such as detailed artworks and large-scale prints.\nDALL-E 2: (Ramesh et al. 2022) is a generative model developed by OpenAI that excels in creating images from textual descriptions. As an advancement over the original DALL\u00b7E, it offers improved capabilities in generating high-quality, diverse, and coherent images. DALL\u00b7E 2 combines CLIP (Contrastive Language-Image Pre-training) with a transformer-based generative model to translate complex and abstract textual inputs into visual content, making it renowned for its creative and realistic outputs.\nThese generative models represent significant advancements in AI-driven image synthesis, each with unique strengths tailored to specific applications. From text-to-image generation and high-resolution synthesis to versatile image processing, these models push the boundaries of AI-generated content."}, {"title": "C Evaluation Metrics", "content": "Overview\nIn this study", "sub-aspects": "pixel-level similarity", "Pixel-level": "Metrics such as PSNR", "Frame-level": "Metrics including PIQE, IL-NIQE, MUSIQ, DBCNN, CNNIQA, CLIPIQA, BRISQUE, TRES, HyperIQA, LIQE, and UNIQUE provide no-reference assessments of image quality. They evaluate attributes such as distortion levels, natural scene statistics, and learned features to predict the overall quality of the images. Additionally, naturalness metrics like NIQE and NRQM help gauge how closely AI-generated"}]}