{"title": "Examining False Positives under Inference Scaling for Mathematical Reasoning", "authors": ["Yu Wang", "Nan Yang", "Liang Wang", "Furu Wei"], "abstract": "Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem-solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how \"false positives\u201d influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to \u201cfalse positives\", suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of \"false positives\" and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.", "sections": [{"title": "1 Introduction", "content": "Recent developments in language models, including improvements in inference time scaling and self-improvement techniques, have significantly enhanced performance in mathematical reasoning tasks (Snell et al., 2024; Shao et al., 2024). Both closed-source and open-source models have demonstrated impressive capabilities in solving mathematical problems across major benchmarks"}, {"title": "2 Related Work", "content": "The \"false positive\" problem we investigate arises primarily due to the evaluation methods employed for assessing large language model (LLM) performance on mathematical tasks. Many existing approaches focus solely on comparing the final generated answers to the ground truth annotations. In these methods, the final answer is extracted either through predefined rules or using LLMs themselves, followed by normalization and comparison with the gold-standard solution. This evaluation strategy is efficient, inexpensive, and fully automated; however, it fails to account for the intermediate reasoning steps involved in generating the solution. Moreover, it is not applicable to tasks such as mathematical proofs, which do not have a single final answer. To overcome these limitations, some studies leverage powerful LLMs to compare the reasoning steps in generated solutions with reference answers or directly identify step errors within the reasoning path, in an attempt to evaluate the validity of mathematical reasoning (He et al., 2023; Tyen et al., 2023; Hao et al., 2024). The effectiveness of this approach is heavily reliant on the capabilities of the LLM used, and it remains uncertain how reliably LLMs can detect reasoning flaws produced by strong LLMs themselves. Alternatively, other research has explored the use of formal proof systems for mathematical reasoning. Benchmarks such as MiniF2F (Zheng et al., 2021) and ProofNet (Azerbayev et al., 2023) utilize formal languages like Lean (Moura and Ullrich, 2021) to specify math problems, and LLMs are tasked with generating formal proofs, which can be rigorously checked by the formal system. While formal proofs inherently avoid the \"false positive\" issue present in natural language solutions, the translation of informal mathematical statements into formal systems remains a significant challenge, limiting the broader applicability of this approach. Previous studies, such as Hao et al. (2024) and Zheng et al. (2024), have also highlighted the presence of \u201cfalse positives\u201d in LLM-generated mathematical solutions. A significant line of research focuses on improving the accuracy of reasoning steps through process supervision (Lightman et al., 2023; Setlur et al., 2024; Luo et al., 2024). For instance, Lightman et al. (2023) demonstrated that training on explicitly annotated flaws in reasoning paths could enhance the performance of reward models, leading to improved accuracy on mathematical benchmarks. Works such as Luong et al. (2024) and Shao et al. (2024), employ reinforcement learning techniques to generate higher-quality reasoning paths derived from final answer annotations, which are then utilized to train better policy models in a self-improving way. In addition, studies like Golovneva et al. (2022), Prasad et al. (2023) and Xia et al. (2024) have proposed filtering and rescoring strategies, as well as novel metrics, to identify erroneous reasoning steps and mitigate the \"false positive\u201d problem. While Snell et al. (2024) investigated the inference time scaling of LLMs on mathematical problems, their work did not consider the impact of \"false positives\". Moreover, Stroebl et al. (2024) studied how the \u201cfalse positive\" problem affects inference time scaling in the coding domain, showing that flawed verifiers lead to a decrease in true accuracy as more computational resources are allocated, due to the growing rate of \"false positives\u201d in the generated solutions."}, {"title": "3 Evaluation Methodology", "content": "In mathematical evaluations, two primary assessment methods are commonly employed: automatic evaluation and manual evaluation. Automatic evaluation includes rule-based assessment and the use of powerful large language models (LLMs) for evaluation. Currently, most benchmarks for mathematical models typically rely on rule-based automatic evaluation (Yang et al., 2024; Shao et al., 2024). This approach utilizes predefined heuristic rules to evaluate the correctness of a model's output by comparing its final answer to the ground truth. While this method is straightforward and easy to implement, it has notable limitations. Specifically, it fails to effectively assess the correctness and logical coherence of intermediate reasoning steps, leading to the phenomenon of \u201cfalse positives\u201d. A \"false positive\" arises when the final answer is correct, but the solution process contains errors or lacks logical validity. The detection of such \"false positives\" can be conducted either through model-based methods or human evaluation, as discussed in the following sections."}, {"title": "3.1 Model Detection of False Positives", "content": "To assess the ability of current models to detect errors in intermediate reasoning steps, we can utilize M(True or False | P, x, y), where M denotes the model used for error detection, x and y represent the question and the model-generated response, respectively, and P refers to the prompt utilized (see Appendix C.1 for details). Although the costs of employing the model for error detection are relatively low, its effectiveness remains limited. We present a comprehensive analysis and discussion in Section 5.2."}, {"title": "3.2 Human Detection of False Positives", "content": "Due to the limited capability of existing models to identify errors in reasoning steps, we introduce manual evaluation as a complementary approach to better understand the occurrence of \"false positives\". Human evaluation involves a meticulous, step-by-step review of the model's responses by human annotators, ensuring not only the correctness of the final answer but also the logical coherence and mathematical validity of the intermediate solution steps. While more resource-intensive, this method significantly improves the accuracy and comprehensiveness of the evaluation, providing deeper insights into the model's reasoning processes.\nIn human evaluation, annotators classify a model's response as a \u201cfalse positive\u201d if it exhibits any of the following errors, despite the final answer being correct:\n1. Jump in Reasoning: This occurs when essential logical steps or intermediate calculations are omitted, resulting in a direct leap to the final answer without adequate justification. Such omissions undermine the validity of the solution, even if the answer itself is correct.\n2. Logical Error: This category encompasses errors such as the misapplication of theorems or rules, reliance on unjustified assumptions, contradictory reasoning, and the incorporation of conditions absent from the problem statement.\n3. Calculation Error: Mistakes in arithmetic or algebraic computations, while potentially offset by other errors, still reflect a lack of precision in the solution process.\n4. Conceptual Error: Misinterpretation of mathematical theorems, concepts, or the problem itself.\nAdditionally, human annotators may disregard minor errors in the reasoning path that do not affect the final answer. This approach ensures a balanced evaluation by focusing on substantive flaws while avoiding penalization for inconsequential mistakes, thereby providing a clearer assessment of the model's reasoning capabilities and the prevalence of \"false positives\"."}, {"title": "4 Inference Scaling Methods in the Presence of False Positives", "content": "Recent studies have demonstrated that allocating additional computational resources to the inference phase can significantly enhance model performance in mathematical tasks (Snell et al., 2024; Wu et al., 2024). However, these studies typically rely on rule-based evaluation methods, which may result in \"false positives\u201d, as discussed in Section 3. To investigate whether \u201cfalse positives\u201d also manifest in inference scaling, this section offers a comprehensive overview of the inference scaling methods utilized in our study.\nWe classify current inference scaling methods into two categories: solution-level inference scal-"}, {"title": "4.1 Solution-Level Inference Scaling", "content": "Currently, the most widely used solution-level inference scaling methods are Best-of-N (Charniak and Johnson, 2005; Pauls and Klein, 2009), Self-Consistency (Wang et al., 2022), Weighted Self-Consistency (Li et al., 2023). These approaches have proven their effectiveness in reasoning tasks (Cobbe et al., 2021; Lightman et al., 2023). \u03a4\u03bf clarify the methods, we first introduce the notation employed throughout this section. Let Y signify the output space of large language models, A correspond to the answer space, where answers are extracted from the model outputs, and \\( v : Y \\rightarrow \\mathbb{R} \\) represent the score function.\n1. Best-of-N: Best-of-N is a simple yet effective reranking algorithm (Welleck et al., 2024). It begins by generating N candidate solutions, and subsequently selects the one with the highest score assigned by the score function. A common practice in reasoning tasks is to train a reward model to predict the correctness of a solution, which serves as the score function. Best-of-N can be defined as:\n\\( y^* = \\underset{y_i \\in Y_i, i \\in \\{1,...,N\\}}{\\arg \\max} v(y_i) \\)\n2. Self-Consistency: Self-Consistency is a transformation algorithm (Welleck et al., 2024), leveraging the idea that correct reasoning processes, though diverse, often converge on the same answer. This method first samples N candidate reasoning paths and then determines the final answer by selecting the one that appears most frequently. Self Consistency is formally expressed as:\n\\( a^* = \\underset{a \\in A}{\\arg \\max} \\sum_{i=1}^N \\mathbb{1}(a = a_i) \\)\n3. Weighted Self-Consistency: Weighted Self-Consistency extends Self-Consistency by incorporating the scores provided by the reward model to weigh candidate solutions. The optimal answer is chosen using the following formula:\n\\( a^* = \\underset{a \\in A}{\\arg \\max} \\sum_{i=1}^N v(y_i) \\mathbb{1}(a = a_i) \\)\nIn subsequent experiments, we employ Best-of-N and Weighted Self-Consistency instead of Self-Consistency. While both Weighted Self-Consistency and Self-Consistency are based on the principle of consistency, Weighted Self-Consistency exhibits superior performance (Snell et al., 2024). Within the Weighted Self-Consistency algorithm, we first use it to select the candidate final answer. To further detect \u201cfalse positives\u201d, the solution with the highest reward among these candidates is selected as the final target for evaluation."}, {"title": "4.2 Step-Level Inference Scaling", "content": "4.2.1 Diverse Verifier Tree Search\nDiverse Verifier Tree Search (DVTS, (Beeching et al., 2024)) is an extension of step-level beam"}, {"title": "4.2.2 Monte Carlo Tree Search", "content": "Monte Carlo Tree Search (MCTS, (Browne et al., 2012)) is a tree search algorithm designed to balance exploration and exploitation effectively. It has achieved remarkable success in game domains such as chess, shogi, and Go (Silver et al., 2016, 2018). Recent research has highlighted the potential of MCTS in large language model (LLM) reasoning (Hao et al., 2023; Zhang et al., 2024).\nIn this work, We utilize the Vanilla MCTS implementation from Wang et al. (2024a), which comprises four main steps: selection, expansion, evaluation, and backpropagation. During the selection stage, Vanilla MCTS employs a variant of the PUCT algorithm (Silver et al., 2016) to choose child nodes. In the evaluation stage, it leverages Process Reward Model (PRM) to compute state values. Each iteration of the algorithm continues until a complete reasoning path is obtained.\nFor further details on Vanilla MCTS, see Algorithm 2."}, {"title": "5 Experiments", "content": "Benchmarks. To validate the proposed phenomenon, we conduct experiments on three mathematical benchmarks: MATH (Hendrycks et al., 2021), AIME (Numina), Omni-MATH (Gao et al., 2024). MATH benchmark comprises problems collected from high school math competitions. Following Lightman et al. (2023), we use MATH500 as our test set. AIME benchmark includes questions from AIME22, AIME23, and AIME24, totaling 90 problems. Omni-MATH is a highly challenging benchmark designed for Olympiad-level mathematical reasoning, and we utilize Omni-MATH-Rule (Gao et al., 2024), a subset suitable for rule-based evaluation. For further convenience of manual evaluation, We randomly select 100 problems from MATH500 and 100 problems from Omni-MATH-Rule, which we refer to as MATH100 and Omni-\n5.1 Experimental Setup"}, {"title": "5.2 Model and Human Detection of False Positives", "content": "Prior to investigating the \"false positive\" phenomenon, we first analyze the differences between the capabilities of models and humans in detecting \"false positives\". To this end, we construct a comprehensive false positive detection benchmark that encompasses multiple models, diverse mathematical benchmarks, and various inference scaling"}, {"title": "5.3 Findings and Analysis", "content": "In this section, we investigate the accuracy of the inference scaling curve using solution-level and step-level inference scaling methods. Specifically, We analyze the relationship between automatic accuracy and manual accuracy across varying values of N in different approaches. Additionally, we explore the impact of model types and benchmark difficulty on the false positive rate."}, {"title": "5.3.1 False Positives in Inference Scaling", "content": "In this section, we investigate the accuracy of the inference scaling curve using solution-level and step-level inference scaling methods. Specifically, We analyze the relationship between automatic accuracy and manual accuracy across varying values of N in different approaches. Additionally, we explore the impact of model types and benchmark difficulty on the false positive rate.\nFalse positives occur in both inference scaling methods. As shown in Figure 2, both automatic accuracy and manual accuracy generally increase with N across all methods. However, the gap between automatic accuracy and manual accuracy persists when N takes different values, indicating a consistent presence of \"false positives\". This observation reveals that inference scaling curves are not as reliable as they might initially appear and that inference scaling does not effectively mitigate \"false positives\".\nGeneral models exhibit higher false positive rates than mathematical models. Figure 4 depicts the relationship between the false positive rate and different model types. General models, which are not specialized in solving mathematical problems, exhibit significantly higher false positive rates compared to mathematical models when evaluated on the relatively challenging AIME benchmark. This suggests that the correct answers generated by in-\n5.3.2 Is Automatic Pass@N the Upper Bound\nof Inference Scaling?\nTo further investigate the potential and possible upper limit of inference scaling, we examine the"}]}