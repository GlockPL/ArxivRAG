{"title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy", "authors": ["Yuhui Chen", "Shuai Tian", "Shugao Liu", "Yingting Zhou", "Haoran Li", "Dongbin Zhao"], "abstract": "Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144 % improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications. Videos and code are available at our project website https://cccedric.github.io/conrft/.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in training generalist robotic policies using Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in understanding and executing various manipulation tasks. These successes are primarily attributed to large-scale imitation-style pre-training and grounding with robot actions [1, 2, 3, 4, 5, 6]. While pre-trained policies capture powerful representations, they often fall short when handling the complexities of real-world scenarios [7]. Fine-tuning with domain-specific data is essential to optimize model performance for downstream tasks [8, 6]. While Supervised Fine-Tuning (SFT) of the VLA model using human teleoperation data remains the predominant adaptation approach, this process faces significant challenges: the model's performance heavily relies on the quality and quantity of task-specific data. However, these human-collected teleoperation datasets may not consistently provide optimal trajectories due to inherent issues such as sub-optimal demonstration and inconsistent action [9].\nSignificant progress in Large-Language-Models (LLMs) and Vision-Language-Models (VLMs) have highlighted the value of reinforcement learning as a powerful tool for bridging the gap between policy capabilities and human preference [10, 11, 12, 13] or improving model reasoning [14]. In addition, deploying reinforcement learning (RL) with task-specific reward functions to learn from online interaction data is also a promising direction [15, 16, 17]. However, extending these insights to VLA models presents unique challenges because, unlike LLMs, VLA models necessitate direct physical interaction in real-world robotic manipulation tasks. The safety and cost constraints of collecting data in contact-rich environments demand high sample efficiency and risk-aware exploration, making a straightforward implementation of RL infeasible. Recent work has attempted to leverage RL to address the challenges faced in SFT [9, 18]; these methods primarily focus on utilizing RL for data augmentation or quality improvement rather than directly optimizing VLA models through RL objectives. This reliance limits the policy's ability to explore states out of the demonstration dataset, thus undermining the potential benefits of RL-based fine-tuning in real-world settings.\nTo leverage the benefits of RL-based techniques for efficiently fine-tuning VLA models with online interaction data, we propose a reinforced fine-tuning (RFT) approach consisting of offline and online stages with a unified consistency-based training objective. While this design is similar to offline-to-online methods [19, 20, 21], we found that expert demonstrations' scarcity constrains their offline training performance. Motivated by insights from CPQL [22], we propose a unified training objective that integrates supervised learning with Q-learning in the offline stage and further fine-tunes the VLA model via consistency policy through online RL. During offline training, our approach leverages prior demonstrations and handles out-of-distribution (OOD) states, effectively extracting the policy and value function before interacting with real-world environments. In the subsequent online stage, we solve two challenges of sample efficiency and real-world safety requirements by exploiting task-specific rewards with CPQL [22] under human interventions through Human-in-the-Loop (HIL) learning [23, 24].\nOur contributions are summarized as follows:\n1) We present a Consistency-based Reinforced Fine-Tuning (ConRFT) method, a unified pipeline with the same training objective both for offline and online fine-tuning.\n2) By integrating offline RL with a consistency-based behavior cloning (BC) loss, we propose Cal-ConRFT,"}, {"title": "II. RELATED WORK", "content": "RL has been widely adopted for fine-tuning LLMs and VLMs. Early works have primarily focused on RL incorporating human feedback [10, 25, 11, 12, 13] by learning from human preferences or by integrating task-specific rewards without explicit human preference [15, 16, 17, 26]. While many of these approaches employ on-policy algorithms (e.g., PPO [27]) to fine-tune pre-trained policies [28, 16, 29], they typically demand large amounts of interaction data to achieve desirable performance [30]. While RL has demonstrated success in many domains, it typically learns within self-generated synthetic environments rather than real-world environments. This gap prevents direct transfer for VLA models, which require real-world interaction. Our work addresses this discrepancy by developing RL frameworks tailored for efficient real-world VLA fine-tuning.\nReal-world robotic RL systems require algorithms that are both sample-efficient in handling high-dimensional inputs and flexible enough to accommodate practical considerations like reward specification and environment resets [24]. Several previous methods have effectively demonstrated policy learning directly in physical environments [31, 32, 33, 24], using both off-policy [34, 35, 36, 37], on-policy [38, 39] methods, or posing \"RL as supervised learning\" [40, 18]. Despite this progress, many real-world RL systems still demand prolonged training sessions or require large amounts of interaction data [41], which can be impractical and risk-prone in contact-rich tasks. In contrast to previous methods that train from scratch, our work focuses on utilizing pre-trained VLA models to provide high-quality policy initialization. This approach effectively mitigates unnecessary exploratory behaviors in early RL phases, thereby optimizing both policy learning efficiency and operational safety in the training process."}, {"title": "III. PROBLEM SETUP AND PRELIMINARIES", "content": "We focus on fine-tuning a pre-trained VLA model for downstream tasks. Specifically, we assume access to a pre-trained VLA model pre, which encodes high-level representations from both visual inputs (e.g., RGB images) and language instructions. In supervised fine-tuning (SFT), we aim to adapt pre to on the target task using a small set of labeled demonstrations while preserving the model's general feature-extraction capability. Formally, let \\(\\tau = (s_0, a_0,...,s_H)\\) be a trajectory for the target task, then the VLA model fine-tuning aims to solve min L( \\( \\tau \\) , \\(\\phi\\)) where L may be a negative log-likelihood (NLL) or a mean-squared error (MSE) measuring the discrepancy between the predicted actions and those in the demonstration. This procedure allows us to effectively leverage compressed knowledge in robotic tasks while steering the VLA model to the downstream environment.\nSince demonstrations are often limited, inconsistent, and sub-optimal, preventing the policy from covering diverse states, SFT struggles in real-world, contact-rich robotic tasks. To address these issues, we formulate each robotic task as a Markov Decision Process (MDP), where the goal of RL is to find the optimal policy in the MDP, \\(M = (S, A, P, r, \\rho, \\gamma)\\), where \\(s \\in S\\) denotes the state space and \\(a \\in A\\) denotes the action space. \\(P(s'|s, a)\\) is the environmental transition probabilities that depend on the system dynamics, and \\(\\rho(s)\\) denotes the initial state distribution. \\(r(s, a)\\) and \\(\\gamma \\in (0,1)\\) are the reward functions and the reward discount factor. The policy \\(\\pi\\) is estimated by maximizing the cumulative expected value of the reward, denoted as \\(V^{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^H \\gamma^t r(s_t, a_t)|s_0 = s, a_t \\sim \\pi(s_t), s_{t+1} \\sim p(\\cdot|s_t, a_t)]\\). The Q-function of a given policy \\(\\pi\\) is denoted as \\(Q^{\\pi}(s, a) = E_{\\pi}[\\sum_{t=0}^H \\gamma^t r(s_t, a_t)|s_0 = s, a_0 = a, a_{t+1} \\sim \\pi(s_{t+1}), s_{t+1} \\sim p(\\cdot|s_t, a_t)]\\). By coupling the VLA policy with the learned Q-function, RFT allows the VLA model to refine its behavior based on trial-and-error interactions and task-specific feedback."}, {"title": "IV. METHOD", "content": "The proposed unified pipline ConRFT consists of two stages: offline fine-tuning followed by online fine-tuning to optimize robotic policies. The overview of our method is shown in Fig. 1. In the following sections, we provide a detailed description of the two stages. And we provide the whole pipeline illustration in Appendix A.\nSince pre-trained VLA models often lack zero-shot generalizability to novel robotic configurations, in the offline stage, we focus on training the policy using a small, pre-collected offline dataset (20-30 demonstrations) before transitioning to online reinforcement learning. We initialize the policy with the pre-trained VLA model for reinforcement learning, reducing both the exploration burden and the overall online training time. Considering the ability to utilize offline data effectively, we choose the Calibrated Q-Learning (Cal-QL) [20] as our base offline RL method since we want the Q-function to be robust to out-of-distribution (OOD) actions. Specifically, Cal-QL trains the Q-function on a pre-collected dataset by reducing temporal difference (TD) error and an additional regularizer. This regularizer penalizes Q-values for OOD actions when they exceed the value of the reference policy V(s), while compensating for this penalization on actions observed within the offline dataset. The Cal-QL training objective for the critic is given by:\n\n\\(L_{offline} (\\theta) = \\alpha (E_{s \\sim D, a \\sim \\pi(\\cdot|s)} [max(Q_{\\theta}(s, a), V^{\\phi}(s))] - E_{s,a \\sim D}[Q_{\\theta}(s, a)]) + \\frac{1}{2}E_{(s,a,s') \\sim D}[(Q_{\\theta}(s, a) \u2013 B^{\\pi}Q_{\\theta}(s, a))^2]\\)\n\nwhere \\(Q_{\\theta}\\) is the learned Q-function parameterized by \\(\\theta\\), \\(Q_{\\overline{\\theta}}\\) is the delayed target Q-function parameterized by \\(\\overline{\\theta}\\). \\(B^{\\pi}Q(s,a) = r(s, a) + \\gamma E_{a' \\sim \\pi(\\cdot|s')}(Q(s', a'))\\) is the backup operator. \\(\\alpha\\) is a hyper-parameter to control the conservative penalty. And D is the demo buffer that stores demonstrations.\nHowever, while Cal-QL is generally efficient at leveraging offline datasets, it struggles to train an effective policy when only small set of demonstrations (e.g., 20-30) are available. In such cases, limited state coverage leads to poor value estimates, making it difficult for the policy to generalize to unseen states. By contrast, typical offline RL datasets are often collected from multiple behavior policies, providing a broad range of state coverage to reduce the distribution shift. Lacking this breadth, the Cal-QL loss alone may not adequately guide the learning process, resulting in poor performance.\nTo address this issue, we propose augmenting the offline training process by incorporating a BC loss. The BC loss directly minimizes the difference between the actions generated by the policy and those from the demonstrations. By incorporating BC loss, we encourage the model to imitate the behaviors from the demonstrations, providing additional supervisory signals during the offline stage. This helps the VLA model to learn a more effective policy and initialize a stable Q function with few demonstrations, especially in the case of contact-rich manipulation tasks where precision is critical.\nMotivated by combining the BC loss with Q guidance under a consistency-based objective [22], we introduce Cal-ConRFT in the offline stage. This approach employs a consistency policy as the action head for fine-tuning the VLA model, addressing two key concerns: 1) it helps leverage inconsistent and sub-optimal demonstrations that often arise in pre-collected data, and 2) compared to diffusion-based action head, the consistency-based action head remains computationally lightweight for efficient inference [22, 47]. The consistency policy is a diffusion-model-based policy that learns to map random actions sampled from the unit Gaussian to generate actions drawn from the expert action distribution conditioned on the current state. For the consistency policy, we discretize the diffusion horizon [\\(\\epsilon\\), K] into M sub-intervals with boundaries \\(k_1 = \\epsilon < k_2 \\le : \\le k_M = K\\) and \\(\\epsilon\\) = 0.002. Specifically, the VLA model with a consistency policy as the action head is given by:\n\n\\(\\pi_{\\psi}(a|s) = f_{\\psi}(a_{k}|E_{\\phi}(s))\\)\n\nwhere f denotes the consistency policy parameterized with \\(\\psi\\), subscripts k denoted the diffusion step, \\(a_k \\sim N(0,kI)\\) and \\(E_{\\phi}(s)\\) denotes the encoded state of the pre-trained VLA model parameterized with \\(\\phi\\). The consistency-based training objective for VLA model fine-tuning is given by:\n\n\\(C_{offline} (\\psi) = -\\eta E_{s \\sim D, a \\sim \\pi_{\\psi}} [Q(s, a)] + \\beta E_{(s,a) \\sim D, m \\sim U[1,M-1]}[d(f_{\\psi}(a + k_m z,k_m|E(s)),a)]\\)\n\nwhere \\(z \\sim N(0,I)\\), d stands for the Euclidean distance \\(d(x,y) = ||x \u2212 y||_2\\), \\(\\beta\\) and \\(\\eta\\) are two hyper-parameters to balance the BC loss and Q loss. This combination enables efficient policy learning and stable value estimation, even with a small set of demonstrations, by aligning value estimates with expert actions and improving policy performance during offline training. Moreover, it provides a reliable initialization for the online stage, facilitating safe and effective exploration.\nWhile the offline stage provides an initial policy from a small set of demonstration data, its performance is limited by the scope and quality of the pre-collected demonstrations. Therefore, we have the online stage with HIL-ConRFT, where the VLA model is further fine-tuned online via the consistency policy through interacting with the real-world environment. During online training process, the demo buffer D for offline stage is remained. Furthermore, we have a replay buffer R to store online data, then implement symmetric sampling [30], whereby for each batch, we sample equally between these two buffers to form each training batch. Since the VLA model continuously gathers new transitions based on its current policy, the data distribution naturally evolves with the policy. This ongoing interaction reduces the distribution-shift problem that the offline stage faces. As a result, we use a standard Q loss for online critic updating:\n\n\\(L_{online} (\\theta) = E_{(s,a,s') \\sim (D \\cup R)} [(Q_{\\theta}(s, a) \u2013 BQ(s, a))^2]\\)\n\nThe consistency-based training objective for VLA model fine-tuning is given by:\n\n\\(L_{online} (\\psi) = -\\eta E_{s \\sim (D \\cup R), a \\sim \\pi_{\\psi}} [Q(s, a)] + \\beta E_{(s,a) \\sim (D \\cup R), m \\sim U[1,M-1]}[d(f_{\\psi}(a + k_m z, k_m|E(s)), a)]\\)\n\nNote that this objective closely mirrors Equation 3 from the offline stage, enabling a quick adaption to online fine-tuning. Typically, we decrease the BC loss weight \\(\\beta\\) while increasing the Q loss weight \\(\\eta\\) during the online stage, yet we keep the BC loss for two main reasons. 1) Firstly, it ensures the policy continues to align with the demonstration data, preventing drastic deviations that could lead to performance collapse. This is important for maintaining the quality of actions in contact-rich manipulation tasks, where sudden changes in the policy can result in unsafe or inefficient behaviors. 2) Secondly, since reinforcement learning inherently involves exploration, it can become unstable in high-dimensional state-action spaces. By providing a stabilizing effect on exploration [48], the BC loss prevents the policy from deviating too far from its offline baseline, thereby reducing the risk of inefficient or unsafe behaviors. This aspect is important in real-world robotic training, especially in physical environments where unsafe actions can lead to damage or other hazards.\nAlso, we integrate human interventions into the online stage through Human-in-the-Loop learning. Specifically, HIL learning allows for timely interventions by a human operator who can provide corrective actions during the exploration process, which will then take over the control of the robot from the VLA model. These human corrections are added to the demo buffer D, offering high-level guidance that steers exploration in a safer and more efficient direction [49]. Human interventions are essential when the robot engages in destructive behaviors, such as colliding with obstacles, applying excessive force, or damaging the environment. In addition to ensuring safe exploration, human interventions accelerate policy convergence. In scenarios where the policy leads the robot into an unrecoverable or undesirable state or when the robot becomes stuck in a local optimum that would otherwise require significant time and steps to overcome without external assistance, the human operator can step in to correct the robot's actions and guide it towards safer and more effective behavior. This results in a stable learning process, where the VLA model is fine-tuned quicker and more safely than it would through autonomous exploration alone."}, {"title": "V. EXPERIMENT AND RESULTS", "content": "In this section, we validate the proposed fine-tuning framework through real-world experiments. We first present the experimental setup and the results for various baselines and then discuss these results and their implications.\nOur experiments aim to evaluate our approach's effectiveness and efficiency for fine-tuning VLA models in real-world scenarios. To this end, we perform real-world experiments across eight diverse manipulation tasks, as illustrated in Figure 2. These tasks are designed to reflect a variety of manipulation challenges, including object placement tasks (e.g., placing bread into a toaster and putting bread on a white plate), precise and contact-rich manipulation (e.g., aligning and inserting a wheel into the chair base), and dynamic object handling (e.g., hanging a Chinese Knot). To validate our fine-tuning approach, we select the Octo-small model [4] for its balance of performance and inference efficiency, and employ a consistency policy [47] as the action head on a 7-DoF Franka Emika robot arm.\nFor all tasks, the state observation includes two RGB images captured from a wrist-mounted camera (128 \u00d7 128) and a side camera (256 \u00d7 256), in combination with the robot's proprioceptive state of the robot arm, including end-effector poses, twists, forces/torques, and gripper status. The action space is defined as either a 6-dimensional end-effector delta pose for the downstream impedance controller or a 7-dimensional target that includes 1-dimensional binary gripper action, additionally for tasks that involve grasping. Data collection and policies command actions at 10Hz. Before training,"}, {"title": "VI. LIMITATIONS", "content": "Although our approach demonstrates strong performance and sample efficiency for fine-tuning VLA models in real-world manipulation tasks, several limitations remain:\nIn this work, we implement a task-specific binary classifier to calculate the reward for RL. However, the inherent distributional shift between the classifier's training data and the state-action distributions generated during RL exploration creates a critical vulnerability,as it can lead the learned policy to engage in reward hacking, exploiting unintended behaviors where the classifier provides inaccurate rewards. For instance, the robot might position its end-effector at a specific location that triggers a false positive, causing the policy to converge to an incorrect behavior. Moreover, because these reward classifiers typically provide only sparse feedback, the policy may learn slowly, even with the help of human interventions. Introducing dense reward signals could enhance sample efficiency and accelerate policy convergence, but it would also require more sophisticated reward engineering for real-world environments.\nOur current implementation runs the interaction and policy learning processes in separate threads, fine-tuning only the action head network with consistent policy while keeping the visual encoders and transformer backbone frozen. While this"}]}