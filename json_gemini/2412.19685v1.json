{"title": "A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization", "authors": ["Jingchun Lian", "Lingyu Liu", "Yaxiong Wang", "Yujiao Wu", "Li Zhu", "Zhedong Zheng"], "abstract": "Image forgery localization, which centers on identifying tampered pixels within an image, has seen significant advancements. Traditional approaches often model this challenge as a variant of image segmentation, treating the binary segmentation of forged areas as the end product. We argue that the basic binary forgery mask is inadequate for explaining model predictions. It doesn't clarify why the model pinpoints certain areas and treats all forged pixels the same, making it hard to spot the most fake-looking parts. In this study, we mitigate the aforementioned limitations by generating salient region-focused interpretation for the forgery images. To support this, we craft a Multi-Modal Tramper Tracing (MMTT) dataset, comprising facial images manipulated using deepfake techniques and paired with manual, interpretable textual annotations. To harvest high-quality annotation, annotators are instructed to meticulously observe the manipulated images and articulate the typical characteristics of the forgery regions. Subsequently, we collect a dataset of 128,303 image-text pairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture designed for concurrent forgery localization and interpretation. ForgeryTalker first trains a forgery prompter network to identify the pivotal clues within the explanatory text. Subsequently, the region prompter is incorporated into", "sections": [{"title": "1. Introduction", "content": "The emergence of advanced generative models, particularly diffusion models [9, 34], has significantly enhanced the sophistication and realism of image generation techniques, making them increasingly difficult to detect. While these techniques have demonstrated immense potential in creative fields such as digital art and film production [5], they have also raised profound concerns about their misuse in malicious contexts, including misinformation campaigns and privacy violations [24, 29], especially the manipulation of facial images. Given these threats, DeepFake detection techniques have garnered significant attention and have rapidly evolved in recent years. Recent studies are shifting from simple real-fake detection to fine-grained forgery region localization to address the growing complexity of modern forgery techniques [32, 36, 37, 39].\nUnlike binary classification methods, which merely determine whether an image is fake or real, forgery localization segments the exact areas that have been tampered with [36], aiming to explain the reason behind a forgery determination. Despite the recent significant strides in forgery localization, current methods still lack the ability to provide clear, interpretable justifications for their detections.\nBinary masks, which merely highlight tampered pixels, provide limited insights into the rationale behind the model's predictions [32]. Furthermore, these masks fail to differentiate between subtle and more significant alterations, treating all manipulated pixels equally, which often obscures the most critical areas that warrant closer scrutiny. Meanwhile, modern forgeries are often visually indistinguishable from real images. This makes it challenging for even human reviewers to identify tampered regions. For example, slight modifications in facial features, such as subtle distortions of the eyes or lips, are often overlooked in existing works, providing human observers with insufficient information to recognize the most anomalous regions. Given this, current models require additional outputs beyond binary masks to better reveal forged areas.\nBased on the considerations outlined above, this work aims to develop an interpretable image forgery localization framework, including two abilities of segmenting the forgery pixels and generating interpretations for the tampered pixels. To enable the construction of such a framework, we first create a large-scale Multi-Modal Tampering Tracing (MMTT) dataset, as shown in Figure 1, comprising image-text pairs of forgery images and the corresponding textual annotations. In specific, the MMTT dataset, focusing on face images and consisting of 128,303 forged facial samples, contains manipulated images that pose more threats to public information and privacy. Each image undergoes various manipulations, and the pixel-level forgery mask is automatically generated from the manipulation processes. To annotate the textual descriptions, we adopt a human-in-the-loop approach. Annotators first observe each forged image alongside its original version and are asked to pinpoint specific altered regions and describe the changes in detail. For each forged area, the type of manipulation (e.g., blurring, unnatural texture, or geometry distortion) is documented to ensure precise interpretability. The descriptions are iteratively refined through repeated inspection and feedback, ensuring that each annotation accurately captures even subtle alterations in the tampered regions, thereby enhancing the interpretability and quality of the dataset. This structured annotation procedure provides high-quality textual interpretations for the manipulations, offering a distinct advantage over existing datasets that typically lack such detailed contextual information.\nWith the MMTT dataset established, we design a framework, dubbed ForgeryTalker, to simultaneously perform forgery localization and generate interpretive reports for the manipulated regions. The overall architecture includes three primary components: the Forgery Prompter Network, a Mask Decoder, and a Multimodal Large Language Model (MLLM) as the backbone. The Forgery Prompter Network analyzes the manipulated features within the image and produces a concise yet informative prompt, capturing the core artificial characteristics of the forgeries. This prompt provides crucial priors for subsequent reasoning and makes the generation of a coherent explanation significantly easier. The Mask Decoder refines the pixel-level predictions, ensuring that only the most prominent manipulated regions are emphasized. Finally, the Language-based Explanation Module utilizes the generated prompt to articulate a coherent report that accurately captures the rationale behind the predicted forgery mask, addressing the inherent limitations of traditional binary segmentation approaches. Through the integration of these three components, our model not only achieves precise forgery localization but also provides contextually rich, human-understandable interpretation reports of the detected manipulations. Our contributions include:\n\u2022 We make an early study for an unexplored problem, i.e., interpretable forgery localization. A Multi-Modal Trampering Tracing (MMTT) dataset is collected to support the exploration of this problem, consisting of 128,303 forged facial image-text pairs. Each image is annotated with interpretable textual reasons, and paired with a corresponding forgery mask."}, {"title": "2. Related Work", "content": "2.1. Interpretation Annotation.\nFacial Manipulation Localization. Detecting manipulated facial regions, especially deepfakes, has garnered attention. CNN-based methods [33] utilize temporal inconsistencies for videos, while GAN-based approaches, such as GANprintR [26] and MaskGAN [23], address synthetic artifacts. Hybrid models like HCiT [12] combine CNNs and ViTs to enhance generalization, and multi-modal methods [14, 35] leverage spatial-temporal inconsistencies. However, these models lack interpretability and fine-grained mask generation, which our work addresses by providing both localization masks and textual explanations.\nMulti-label Classification for Facial Localization. Multi-label classification captures independent alterations in facial regions but struggles with dependencies across features. CNNs [18] face limitations in fine-grained tasks, while hybrid models [12] improve detection by combining local and global features. Weighted loss functions [28] and parallel branches [30] address class imbalance and refine detection. Yet, few works integrate multi-label classification with localization. Our ViT-based classifier bridges this gap by capturing complex dependencies with parallel branches and weighted loss functions.\nSegmentation Techniques. Segmentation is crucial for identifying localized manipulations. Models like U-Net and DeepLab [31] focus on spatial features, while Transformer models [2] capture global context. Recent methods like SAM [16] use a Two-Way Transformer for high-quality masks but lack manipulation-specific context. By integrating SAM with InstructBLIP, we create context-aware forgery masks, unifying segmentation and manipulation detection for enhanced localization."}, {"title": "3. Multi-Modal Tramper Tracing dataset", "content": "Although many existing datasets provide annotations for forgery localization, they lack detailed, descriptive explanations for the detected manipulations. To bridge this gap, we introduce the Multi-Modal Tramper Tracing (MMTT) dataset, which uniquely combines pixel-level forgery masks with comprehensive textual descriptions. Unlike conventional datasets that focus solely on binary classification [6, 22] or mask-based localization [11, 32], MMTT emphasizes interpretability by integrating annotations that explain how and why the manipulated regions appear forged. This emphasis on human-generated interpretations allows for a richer understanding of the manipulations.\n3.1. Source Image Collection\nWe develope our MMTT dataset based on the CelebAMask-HQ (CelebA-HQ) [42] and Flickr-Faces-HQ (FFHQ) [13] datasets. Both datasets offer high-quality, high-resolution facial images, CelebAMask-HQ containing 30,000 images and FFHQ providing 70, 000 images, totaling 100, 000 samples. All images are resized to 512 \u00d7 512 pixels for uniformity. The selected 100,000 images serve as the primary dataset for our subsequent forgery manipulations.\n3.2. Forgery Generation\nGeneration and editing are two main threats for the face image protection, We incorporate both techniques for forgery image generation to construct a more challenging dataset. To keep pace with the latest techniques, we employ three manipulation methods: face swapping [1], along with image inpainting techniques, which include both transformer-based [21] and diffusion-based methods [27], to produce a comprehensive forgery dataset.\nFace Swapping. For the face swapping task, we employ E4S [1], a GAN-based model designed specifically for high-quality face swapping. Given a target image $I_t$ and a source image $I_s$, E4S generates a swapped face image $I_f$ by replacing the entire face region in $I_t$ with the facial features from $I_s$. For the CelebA-HQ dataset, target and source images are randomly paired from the entire dataset, while for FFHQ, the source image is chosen from a separate subfolder to maintain visual diversity.\nDuring the swapping process, E4S automatically generates a binary mask $M$, which covers the entire face region of the target image $I_t$. This dynamically generated mask is used to blend facial features from $I_s$ into $I_t$, ensuring the swapped image $I_f$ only alters the facial region and preserves non-facial elements like hair and background from the target image.\nThe generated binary mask $M$ is stored as the ground-truth annotation for the altered regions, representing the full face replacement for both CelebA-HQ and FFHQ datasets. As a result, the final outputs include both the forged images $I_f$ and their corresponding binary masks $M$, providing a consistent representation of the modified regions for subsequent training and evaluation tasks.\nImage Inpainting. For generating localized facial manipulations, we utilize MAT [21] (transformer-based) and SDXL [27] (diffusion-based).\nFor each image $I$, the process commences by defining a binary mask $M$ that indicates the regions to be inpainted. Depending on the dataset, the process of mask generation varies. For the CelebAMask-HQ dataset, which contains predefined masks for 21 facial components (e.g., eyes, nose,"}, {"title": "4. ForgeryTalker", "content": "4.1. Architecture\nOur framework, ForgeryTalker, extends the InstructBlip [4] model by introducing a Forgery Prompter Network (FPN) and a Mask Decoder. The system accepts a tampered image $I$ and encodes it into patch embeddings following Vision Transformer [7]. These embeddings are first processed by the Q-former, and the resulting features undergo cross at-"}, {"title": "4.2. Forgery Prompter Network", "content": "Motivation. Accurately identifying the most salient manipulated regions in forged images is difficult due to the high visual fidelity of modern manipulation techniques. Even human reviewers often need to inspect the image closely to spot inconsistencies. Thus, we propose the Forgery Prompter Network to provide an initial set of salient region keywords, guiding the downstream reasoning and facilitating the coherent generation of explanations.\nGroundTruth Extraction. We extract region labels from our interpretation annotations. The label space consists of 21 face semantics, with each image's label as a 21-dimensional vector Y, where the i-th position is 1 if the corresponding face part is in the interpretation, 0 otherwise.\nFPN takes the vision transformers as the main architecture. Considering the crucial role of fine-grained local context in identifying subtle flaws, we introduce a convolution branch at the early m layers to complement the global contexts captured by the vision transformer. As shown in Figure 4, the forgery image I concurrently traverses self-attention blocks and convolution blocks in parallel, producing global-aware features $F_g = \\{F^1_g, F^2_g, ..., F^{m-1}_g\\}$ and local-aware features $F_l = \\{F^1_l, F^2_l, ..., F^{m-1}_l\\}$. At each encoding level, the corresponding features are element-wise summed and fed into next attention block:\n$F^i_l = MHA^{i-1}(F^{i-1}_g), F^i_l = Conv^{i-1}(F^{i-1}_g), (1)$\n$F^i = MHA^i(F^i_g + F^i_l), i = 1,\u2026, m$  (2)\nwhere \"MHA\" and \"Conv\" mean the multi-head attention and convolution, respectively. Furthermore, we note that the positioning of facial regions in a natural image follows a rigid and predictable structure, with the eyes typically positioned laterally relative to the nose and the eyebrows aligned above the eyes. Leveraging this regularity, we integrate coordinate convolution [25] in the initial convolutional layer to detect anomalies in the arrangement of facial features, i.e., $Conv_0 = CoorConv$.\nThe resultant feature $F^m_g$ contains both global and local contexts and is then fed into the subsequent multi-head attention blocks and a classification head to produce the probability Y across regions, as well as being used in cross attention with Q-former features for enhanced forgery localization. Finally, the forgery prompter network is optimized by a combined loss, incorporating both Binary Cross-Entropy"}, {"title": "4.3. Interpretation Generation", "content": "Subsequently, we fix the trained FPN network and take the region predictions from FPN as prior clues to aid both the interpretation generation and the cross attention process for improved forgery localization. Assume the set of regions from FPN is $R = \\{r_1,r_2, ...\\}$, we next design a particular template to include R to form a interpretation-friendly instruction T:\nThese facial areas may be manipulated by AI: [R]. Please describe the specific issues in these areas.\nThe structured prompt serves as the guiding context for the language model, thereby ensuring that the final output accurately reflects the manipulations detected by the FPN. This integration enhances the interpretability and coherence of the generated explanations, offering a comprehensive understanding of the tampered regions. Subsequently, the instruction and the image embeddings into the Q-former and the resultant feature are fed into the large-language model to generate the interpretation text T, which is then supervised by language modeling loss:\n$L_t = -E_{(I,T)~D} [ \\sum_{k=1}^{K} log P(t_k|(I, T), t_0, \u2026, t_{k-1})], (6)$\nwhere $t_k$ is k-th predicted words, P is the word probability distribution from LLM."}, {"title": "4.4. Mask Decoder", "content": "We employ SAM's Two-way Transformer [16] as the mask decoder. The image encoder of InstructBLIP encodes the forgery image. The resulting features from the Q-former are then enhanced through cross attention with FPN's regional prompts. These enriched features are subsequently fed into the Two-way Transformer to predict the forgery mask $M$. The cross entropy loss is performed: $L_m = -\\frac{1}{HW} \\sum_{i,j} M_{ij} log \\hat{M_{ij}} log \\hat{M_{ij}}$, where H, W is the height and width of image.\nOverall, the full loss in the second stage for interpretation and forgery localization is formulated as:\n$L = L_t + L_m$. (7)"}, {"title": "5. Experiment", "content": "5.1. Experimental Setup\nImplementation Details. We implement our ForgeryTalker framework using PyTorch and train it on four NVIDIA A100 GPUs. The Forgery Prompter Network is fine-tuned for 125,000 steps with a batch size of 16, an initial learning rate of 7.5e-3, using a cosine decay strategy and warmup steps of 125. The convolution branch in FPN includes one 3\u00d73 Coordinate Convolution (CoordConv) layer and one 5\u00d75 Convolution layer. The discount factor in Eq. 3 is set as w = 0.2 to balance the unmodified regions. Next, we fix FPN and tune the Q-former and the mask decoder by 60 epochs, starting with a learning rate of 4e-6. The training setup includes a batch size of 16 and a gradient accumulation strategy with an accumulation step of 1, with mixed-precision training (fp16) enabled for faster convergence and reduced memory usage. The Multi-Modal Tampering Tracing (MMTT) dataset is divided into training, validation, and test sets with a ratio of 8:1:1.\nWe use a range of captioning and segmentation metrics for performance evaluation, including CIDEr, BLEU, METEOR, and IoU. We use Positive Label Matching (PLM) to evaluate the effectiveness of FPN. PLM calculates the ratio of correctly predicted positive labels over the union of predicted and ground-truth positive labels:\n$PLM = \\frac{|\\text{Predicted Positive Labels} \\cap \\text{Ground Truth Positive Labels}|}{|\\text{Predicted Positive Labels} \\cup \\text{Ground Truth Positive Labels}|}$  (8)\nUnlike IoU, PLM focuses on detecting manipulated regions without being influenced by a large number of correctly predicted negative labels, making it ideal for tasks with sparse modifications.\n5.2. Quatitative Results\nAs shown in Table 2, we compare our ForgeryTalker framework against several baseline models: SCA [10], LISA-7B [17], Osprey [40], and InstructBLIP [4].\nIn text generation, ForgeryTalker achieves the highest CIDEr score of 21.5, outperforming SCA's 10.6, Osprey's 9.2, and InstructBLIP's 20.9. Additionally, ForgeryTalker surpasses InstructBLIP in BLEU-1 (31.1 vs. 30.6), BLEU-2 (16.9 vs. 16.8), and BLEU-4 (5.9 vs. 5.6), showcasing"}, {"title": "5.3. Ablation Study", "content": "We performed ablation experiments to analyze the effects of key components, focusing on text generation performance (CIDEr). As shown in Table 3, we study several variants:\nw/ FPN-GT. Uses ground-truth labels instead of the predicted labels from the Forgery Prompter Network, achieving the best CIDEr score (48.1), indicating the value of precise label guidance.\nw/o FPN. Removes Forgery Prompter Network, leading to a"}, {"title": "6. Conclusion", "content": "This paper addresses the limitations of traditional image forgery localization by developing an advanced framework that generates comprehensive interpretive reports for forged images. Existing binary forgery masks often lack the detail needed to convey model predictions and effectively highlight key forgery areas. To address this, we created the MMTT dataset with deepfake-manipulated images and corresponding textual annotations. Our proposed ForgeryTalker framework combines forgery localization with interpretive text generation to enhance both accuracy and transparency. Experiments on the MMTT dataset validate the model's distinct advantages in forgery localization and interpretation."}]}