{"title": "Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training", "authors": ["Ting Wang", "Zhixin Zhou", "Rui Luo"], "abstract": "Graph Neural Networks (GNNs) has been widely used in a variety of fields because of their great potential in representing graph-structured data. However, lacking of rigorous uncertainty estimations limits their application in high-stakes. Conformal Prediction (CP) can produce statistically guaranteed uncertainty estimates by using the classifier's probability estimates to obtain prediction sets, which contains the true class with a user-specified probability. In this paper, we propose a Rank-based CP during training framework to GNNS (RCP-GNN) for reliable uncertainty estimates to enhance the trustworthiness of GNNs in the node classification scenario. By exploiting rank information of the classifier's outcome, prediction sets with desired coverage rate can be efficiently constructed. The strategy of CP during training with differentiable rank-based conformity loss function is further explored to adapt prediction sets according to network topology information. In this way, the composition of prediction sets can be guided by the goal of jointly reducing inefficiency and probability estimation errors. Extensive experiments on several real-world datasets show that our model achieves any predefined target marginal coverage while significantly reducing the inefficiency compared with state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) has been widely used in many applications, such as weather forecasting (Lam et al. 2023), drug discovery (Li, Huang, and Zitnik 2022) and recommendation systems (Wu et al. 2022). However, predictions made by GNNs are inevitably present uncertainty. Though to understand the uncertainty in the predictions they produce can help to enhance the trustworthiness of GNNs (Huang et al. 2023), most existing uncertainty quantification (UQ) methods can not be easily adopted to graph-structured data (Stadler et al. 2021). Among various UQ methods, Conformal Prediction (CP) is an effective approach for achieving trustworthy GNNs (Wang et al. 2024). It relaxes the assumption of existing UQ methods, making it suitable for graph-structured data.\nInstead of relying solely on uncalibrated predicted distribution \u03bc(yx), CP constructs a prediction set that informs a plausible range of estimates aligned with the true outcome distribution p(y|x). The post-training calibration step make the output prediction sets provably include the true outcome with a user-specified coverage of 1 - a. A conformity score function is the key component for CP to quantify the agreement between the input and the candidate label.\nSome studies have draw attention to the application of CP to graph-structured data (Clarkson 2023; H. Zargarbashi, Antonelli, and Bojchevski 2023; Lunde 2023; Lunde, Levina, and Zhu 2023; Marandon 2024). However, CP usually suffer inefficiency when there are no well-calibrated probabilities, with the intuition that larger prediction sets covers higher uncertainty. How to achieve desirable efficiency beyond validity is still a noteworthy challenge. Existing studies (Sadinle, Lei, and Wasserman 2019; Romano, Sesia, and Candes 2020) are typically changing the definition of the conformity scores for inefficiency reduction. The challenge is that CP is always used as a post-training calibration step, thus hindering its ability of underlying model to adapting to the prediction sets.\nRecently, (Stutz et al. 2022; Bellotti 2021) try to using CP as a training step to make model parameter @ dependent with the calibration step, so as to modifying prediction sets towards reducing inefficiency. However, the integration of conformal training and GNNs still remain largely unexplored. The very resent work (Huang et al. 2023) proposed a conformal graph neural network which develops a topology-aware calibration step during training. Differently, we focus this problem with two lines. One is that a suitable conformity score is applied for GNNs who often struggle with miscalibration predictions (Wang et al. 2021). Another is that a conformal training framework based on the differentiable variant of this conformity score is designed to adjust the prediction sets along with model parameters' optimization.\nIn conclusion, our contributions are two-fold. First, we propose a novel rank-based conformity scores that emphasizes the rank of prediction probabilities which is more robust to GNNs. Second, we develops a calibration step during training for adjusting the prediction sets along with the model parameters. We demonstrate that the rank-based conformal prediction method we introduce is performance-"}, {"title": "Preliminaries", "content": "Let G = (V,E, X) be a graph, where V is a set of nodes,\nE is a set of edges, and X = {xv}vey is the attributes.\nWe denote y as the discrete set of possible label classes.\nLet {(xv, Yv)}v\u2208D be the random variables from the training data, where x \u2208 Rd is the d-dimensional vector for\nnode v and yv \u2208 Y is its corresponding class. The training data D is randomly split into Dtr/Dval/Dcalib as training/validation/calibration set. Note that the subset Dcalib is\nwithhold as calibration data for conformal prediction. Let {(xv)}v\u2208Dte be the random variables from the test data\nwhose true labels {(Yv)}v\u2208Dte is unknown for model. The\ngoal of node classification tasks is to obtain a classifier\n\u03bc: X \u2192 Y, which can approximate the posterior dis-\ntribution over classes yv \u2208 Y. During the training step,\n{(Xv, Yv)}v\u2208DtrUDvalid, {(xv)}v\u2208DteUDcalib and the graph\nstructure (V, E) are available to GNN model for node repre-\nsentations."}, {"title": "Graph Neural Networks (GNNS)", "content": "In this paper, we focus on GNNs in the node classification\nscenario. GNNs is the most common encoder to learn com-\npact node representations, which is generated by a series of\npropagation layers. For each layer l, each node representa-\ntion ha is updated by its previous representations ha-1),\nand aggregated features ml) obtained through passing mes-\nsage from its neighbours N(u):\n$h^{(l)}_u = F_{upd}(h_u^{(l-1)}, m_u^{(l)})$\n(1)\n$m_u^{(l)} = F_{agg} (m_{uv}, |v \\in N(u))$\n(2)\n$m_{uv} = F_{msg}(h_u^{(l-1)}, h_v^{(l-1)})$ (3)\nwhere Fupd(\u00b7) is a non-linear function to update node repre-\nsentations. Fagg() is the aggregation function while Fmsg(\u00b7)\nis the message passing function. We use node representa-\ntions in the last layer as the input of a classifier to obtain a\nprediction \u03bc\u03bf(x)."}, {"title": "Conformal Prediction", "content": "For CP on GNNs, a valid coverage guarantee requires the\nexchangeability of the calibration and test data. Since our\nmodel is transduction node classification, the calibration ex-\namples are drawn exchangeability from the test distribution\nfollowing (Huang et al. 2023).\nFor a new test point Xn+1, the goal of CP is to construct\na reasonably small prediction set C(xn+1), which contains\ncorresponding true label yn+1 \u2208 Y with pre-defined cover-\nage rate 1 \u03b1:\nP(yn+1 \u2208 C(Xn+1)) \u2265 1 \u2212 a\n(4)\nwhere a \u2208 [0, 1] is the user-specific miscoverage rate. The\nstandard CP is usually conduct at test time after the clas-\nsification model \u03bc\u03b5 is trained, which is achieved in two\nsteps: 1) In the calibration step, a cut-off threshold \u1f22 is cal-\nculated by the quantile function of the conformity scores\nV:X\u00d7Y \u2192 R on the hold-out calibration set Dealib. Dur-\ning calibration, the true classes yi are used for computing\nthe threshold to ensure coverage 1 a. 2) In the prediction\nstep, the prediction sets C(x) depending on the threshold \u1f22\nand the model parameters @ are constructed. The conformity\nscore function is designed to measure the predicted probabil-\nity of a class, and it is typically changed for various objec-\ntives. Two popular conformity scores are described in details\nbelow.\nThreshold Prediction Set (THR) The threshold \u1f22 for\nTHR (Sadinle, Lei, and Wasserman 2019) is calculated by\nthe a quantile of the conformity scores:\n$ \\hat{\\eta} = Q({V(x_i, y_j)|i \\in D_{calib}}, \\alpha(1+\\frac{1}{|D_{calib}|}))$ (5)\nwhere Q() is the quantile function. The prediction sets in-\ncluding labels with sufficiently large prediction values are\nconstructed by thresholding probabilities:\nC(x) = {k \u2208 Y : V(x, k) \u2265 \u1f22} (6)\nV(x,k) = \u03bc\u03ba(x) (7)\nAdaptive Prediction Set (APS) APS (Romano, Sesia, and\nCandes 2020) takes the cumulative sum of ordered probabil-\nities \u03bc\u03c0(1)(x) > \u03bc\u03c0(2)(x) > \u00b7\u00b7\u00b7 \u03bc\u03c0(|y|)(x) for prediction\nset construction:\nC(x) = {k \u2208 Y : V(x, k) \u2264 \u1f22} (8)\n$V(x, k) = \\sum_{j=1}^{k} \\mu_{\\pi(j)}(x)$ (9)\nwhere is a permutation of Y, and the (1 \u2212 a)(1 +\n1/|Dcalib|)-quantile is also required for calibration to ensure\nmarginal coverage."}, {"title": "RANK: Rank-based Conformal Prediction", "content": "Following our previous work (Luo and Zhou 2024), we advancing CP to GNNs through rank-based conformity scores, named RANK, to directly reduce the inefficiency. Assuming that a higher value of \u00b5k (xi) indicates a greater likelihood of xi belonging to class k. Consequently, if class k is included in the prediction set, and \u03bc\u03ba\u03b9 (xi) > \u03bc\u03ba(xi) satisfied, then k' must be in the prediction set. According to this assumption, the size of the prediction set including the true label can be evaluated in the calibration step. The smallest prediction set that includes the true label yi will be constructed by the rank of \u03bcy; (xi) within the sequence {\u03bc\u2081(x),\u00b7\u00b7\u00b7\u03bc\u03b3(x)}.\nRanked Threshold Prediction Sets For each i \u2208 Dcalib, the following rank is defined to establish a rule to choose labels:\n$T_i = \\text{rank of }\\mu_{y_i}(x_i) \\text{ in } {\\mu_k(x_i) : k \\in Y}$ (10)"}, {"title": "RCP-GNN: Rank-Based Conformal Prediction on Graph Neural Networks", "content": "so that the order statistics can be find: r(1) \u2265 r(2) \u2026\u2026 \u2265 (n).\nLetra = r([(n+1)a]), either top-(ra \u2013 1) or top-(r) classes\nwill be included in the prediction set. The top-(r) classes\nrefers to the classes corresponding to the (r*)-th largest pre-\ndiction values. To achieve the target coverage, the u* is de-\nfined to determine when the (r*)-th class should be included\nin the prediction sets:\n$ \\mu^* = [n p]-\\text{th largest value in } {\\hat{\\mu}_{r^*}(x_i) : i \\in D_{calib}}$ (11)\nwhere n is the size of Dcalib, p is the proportion of instances\nwe should included in the r-th label, and \u017f\u00fbk (xi) denotes the\nk-th order statistics in (\u03bc\u2081(xi), ..., \u03bc\u03b7(xi)). The prediction\nset is defined as follows:\n$C(x) =\n\\begin{cases}\n{k\\in Y : \\mu_k(x) \\geq \\mu_{r^*}(x)}, & \\text{if } \\mu_{r^*}(x) \\geq \\mu^*;\\\\\n{k\\in Y : \\mu_k(x) \\geq \\mu_{r^{*}-1}(x)}, & \\text{otherwise};\\\\\n\\end{cases}$ (12)\nAccording to above analysis, the rank-based conformity\nscores calculated on the calibration set can be defined fol-\nlowing:\n$V(x_i, y_i) = \\\n[\\text{rank of }\\mu_{y_i}(x_i) \\text{ in } {\\mu_1(x_i),\\cdots, \\mu_{|Y|}(x_i)}] - 1 \\\\\n+ \\frac{1}{n}[\\text{rank of }\\mu_{y_i}(x_i) \\text{ in } {\\mu_{y_i}(x_1), \\cdots, \\mu_{y_i}(x_n)}]$\n(13)\nand the quantile Q as the [(n+1)a\u300d-th largest value among\nthe conformity scores, defining the prediction set is equiva-\nlent to selecting the calibration samples that satisfy the con-\ndition V(xi, Yi) < Q, is employed to construct the predic-\ntion set with 1 a coverage.\nWe propose RCP-GNN into two-stage: model training stage\nand conformal training stage, as Figure 1 shows. In model\ntraining stage, the base model GNNbase is trained only by\nprediction loss (i.e., cross-entropy loss), and \u03bc(X) is the es-\ntimator of base model.\nIn conformal training stage, the correct model GNNcor\nis trained by both prediction loss and conformity loss. We\nset \u03bc(X) = GNNcor(\u03bc(X), G) as the estimator of cor-\nrect model. The prediction set will be optimized when CP\nperforming on each mini-batch. For the reason that train-\ning with original rank-based CP may cause limited gradient\nflow, a differentiable implementation for RANK is designed\nto enable smooth sorting and ranking, and is further used to\nconstruct the conformity loss function, which sharpens the\nprediction set."}, {"title": "Conformal Training", "content": "We try to train our model end-to-end with the conformal\nwrapper in order to allow fine-grained control over the pre-\ndiction sets C(x). Following the split CP approach (Lei, Ri-\nnaldo, and Wasserman 2013), we randomly split the test data\nset Dte into folds with 50%/50% as Dcalib/Dte for calibra-\ntion and constructing prediction sets. Before splitting the test\ndata, a fraction of test data is withhold for further standard\nrank-based conformal prediction stage."}, {"title": "Differentiable Prediction and Calibration Steps", "content": "A dif-\nferentiable CP method which involves differentiable predic-\ntion and calibration step is defined for the training process:\n1) In the prediction step, the prediction sets C(x) w.r.t. the\nthreshold \u1f22 and the predictions po(x) is set to be differen-\ntiable. 2) In the calibration step, the conformity scores w.r.t.\nthe predictions \u03bce(x) as well as quantile function is set to\nbe differentiable. Notably, the predictions \u03bce(x) are always\ndifferentiable throughout calibration and prediction steps.\nTherefore, The key component of differentiating through CP\nis the differentiable conformity scores and the differentiable\nquantile computation.\nGiven the prediction probabilities (X), the smooth sort-\ning designed by a sigmoid(x) = $\\frac{1}{1+e^{-x}}$ function and a tem-\nperature hyper-parameter \u03c4\u2208 [0,1] is utilized to replace\n\"hard\" rank manipulation for the smoothed rank-based con-\nformity scores:\n$\\tilde{V}(x_i, k) = \\sum_{j=1}^{|Y|} \\text{sigmoid}(\\frac{\\mu_j(x_i) - \\mu_k(x_i)}{\\tau})$ (14)\nAfter that, a differentiable quantile computation is employed\nfor smoothed thresholding under smooth sorting.\n$\\hat{\\eta} = Q({\\tilde{V}(x_i, y_i)|i \\in D_{calib}},\\alpha(1+\\frac{1}{|D_{calib}|}))$ (15)"}, {"title": "Loss Function", "content": "where Q(\u00b7) is the smooth quantile function that are\nwell-established in (Blondel et al. 2020; Chernozhukov,\nFern\u00e1ndez-Val, and Galichon 2007).\nThe conformal training stage performs dif-\nferentiable CP on data batch during stochastic gradient de-\nscent (SGD) training. As mentioned above, the \u1f22 is cal-\nibrated by a(1 + 1/|Dcalib|)-quantile of the conformity\nscores in a differentiable way. Under the constraint of hyper-\nparameter 7, we empirically make coverage close to 1 \u03b1\nby approximating \"hard\" sorting. Then we propose a con-\nformity loss function to further optimize the inefficiency\nthrough training. Given the estimator \u016bj(xi) for the condi-\ntional probability of Y being class k \u2208 Y at X = xi and\nthe true label yi. Similar with Eq.14, the smooth conformity\nscores on test data is defined as:\n$\\tilde{V}(x_i, y_i) = \\sum_{k=1}^{|Y|} \\text{sigmoid}(\\frac{\\mu_k(x_i) - \\mu_{y_i}(x_i)}{\\tau})$ (16)\nGiven i \u2208 Dcalib, a soft assignment (Stutz et al. 2022; Huang\net al. 2023) of each class k to the prediction set is defined\nsmoothly as follows:\n$c_i = \\text{max}(0, \\sum_{k \\in Y} \\text{sigmoid}(\\frac{\\tilde{V}(x_i, k) - \\hat{\\eta}}{\\tau}))$ (17)\nThen the conformity loss function is defined by:\n$L_{cp} = \\frac{1}{|D_{te}|} \\frac{1}{|Y|} \\sum_{i \\in D_{te}}(c_i - 1)^2$ (18)\nThus, the loss function optimized in conformal training stage\nis defined as follows:\n$L = L_{pred} + \\lambda * L_{cp}$ (19)"}, {"title": "Experiment", "content": "We conduct experiments to demonstrate the advantages of our model over existing methods in achieving empirical marginal coverage for graph data, as well as the efficiency improvement. We also conduct systematic ablation and parameter analysis to show the robustness of our model."}, {"title": "Experiment Setup", "content": "Dataset We choose eight popular graph-structured datasets, i.e., Cora, DBLP, CiteSeer and PubMed (Yang, Cohen, and Salakhudinov 2016), Amazon-Computers and Amazon-Photo, Coauthor-CS and Coauthor-Physics (Shchur et al. 2019) for evaluation. We randomly split them with 20%/10%/70% as training/validation/testing set"}, {"title": "Ablation Study", "content": "We conduct ablations in Table 5 to test main components in RCP-GNN. 1) RCP-THR. It is a variant of RCP-GNN that using THR to compute the conformity scores. Since the THR conformity scores is naturally differentiable w.r.t. the model parameters @ according to Eq. 7, we only need to ensure the quantile function differentiable in the conformal training stage. 2) RCP-APS. Similar to RCP-THR, this variant leverage APS to compute the conformity scores. The differentiable implementation closely follows the one for RANK outlined in Eq. 14:\n$\\tilde{V}(x_i, y_i) = \\sum_{k=1}^{|Y|} \\text{sigmoid}(\\frac{\\mu_{y_i}(x_i) - \\mu_k(x_i)}{\\tau})\\mu_k(x_i)$ (23)\n3) w/o Conf.Tr. In order to figure out the power of conformal training step, we remove the conformity loss and replace it with standard prediction loss. Compared with RCP-THR and RCP-APS, our model can achieve pre-defined marginal coverage with satisfactory inefficiency reduction, which demonstrates that the rank-based conformal prediction component is performance-critical to ensure valid coverage guarantees while simultaneously enhancing efficiency. Compared with w/o Conf.Tr., our model achieves consistent"}, {"title": "Conformal Training", "content": "RCP-GNN constructs and adjusts prediction sets based on ranking and probability of the labels, while CF-GNN only rely on assumptions about the model's probabilities. Therefore, CF-GNN may not fully capture the model's uncertainty, which hinders its perfor-"}, {"title": "Hyper-Parameter Sensitivity", "content": "the robustness of RCP-GNN. In concrete, the hyper-parameter temperature is changed from 0.01 to 10 and the results show that our model is not sensitivity to the temperature. And we select the median value 1 for its relatively better performance. The hyper-parameter A in Eq. 19 is used to balance the prediction loss and the conformity loss. We report the converge and inefficiency results as A changes from 0.01 to 10 and we can observe that a proper weight of conformity loss can help to inefficiency reduction."}, {"title": "Related Works", "content": "Uncertainty Quantification in Deep Learning. It is important for trustworthy modern deep learning models to mitigate overconfidence (Wang et al. 2020; Slossberg et al. 2022; Jiang et al. 2018). Uncertainty quantification (UQ), which aims to construct model-agnostic uncertain estimates, have great potential in many high-stakes applications (Abdar et al. 2021; Gupta et al. 2021; Guo et al. 2017; Kull et al. 2019; Zhang, Kailkhura, and Han 2020). Most of existing UQ methods rely on the i.i.d assumption. Thus make them be not easily adopt to inter-dependency graph-structure data. Some network principle-based UQ methods (Wang et al. 2021; Hsu et al. 2022) designing for GNNs have been proposed in recent years. However, these UQ methods fail to achieve valid coverage guarantee."}, {"title": "Standard Conformal Prediction.", "content": "Conformal prediction (CP) is early proposed on (Vovk, Gammerman, and Shafer 2005). Compared with other CP framework, e.g., cross-validation (Vovk 2015) or jackknife (Barber et al. 2021), most approaches follow a split CP method (Lei, Rinaldo, and Wasserman 2013), where a held-out calibration set is necessary. For the reason that it defines faster and more scalable CP algorithms. However, it sacrifices statistical efficiency.\nDifferent variants of CP are struggled to the balance between statistical and computational efficiency. Some contributions made in conformity score function have been explored (Sadinle, Lei, and Wasserman 2019; Angelopoulos et al. 2021; Romano, Sesia, and Candes 2020) for inefficiency reduction. Other studies (Bates et al. 2021; Yang and Kuchibhotla 2024) have explored in the context of ensembles to obtain smaller confidence sets while avoiding to sacrifice the obtained empirical coverage. But these methods do not solve the major limitation of CP methods: the model is independent, leaving CP little to no control over the prediction sets (Guzm\u00e1n-rivera, Batra, and Kohli 2012). Recently, the work of (Bellotti 2021) and (Stutz et al. 2022) try to better integrated CP into deep learning models by simulating CP during training to make full use of CP benefits. For GNNs, how to define a trainable calibration step still remains an open space for exploration."}, {"title": "Conclusion", "content": "In this work, we extend CP to GNNs by proposing a trainable rank-based CP framework for marginal coverage guaranteed and inefficiency reduction. In future work we will focus on more tasks like link prediction, and extensions to graph-based applications such as molecular prediction and", "keyWords": ["Conformal Prediction", "ranking", "conformity scores", "graph neural networks", "GNNs", "training", "parameters", "optimization"]}]}