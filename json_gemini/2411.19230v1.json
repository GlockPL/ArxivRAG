{"title": "PRE-TRAINING GRAPH CONTRASTIVE MASKED AUTOENCODERS ARE STRONG DISTILLERS FOR EEG", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Nancy B. Carlisle", "Hua Xie", "Yu Zhang"], "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this by framing it as a graph transfer learning and knowledge distillation problem. We propose a Unified Pre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE, to bridge the gap between unlabeled/labeled and high/low-density EEG data. To fully leverage the abundant unlabeled EEG data, we introduce a novel unified graph self-supervised pre-training paradigm, which seamlessly integrates Graph Contrastive Pre-training and Graph Masked Autoencoder Pre-training. This approach synergistically combines contrastive and generative pre-training techniques by reconstructing contrastive samples and contrasting the reconstructions. For knowledge distillation from high-density to low-density EEG data, we propose a Graph Topology Distillation loss function, allowing a lightweight student model trained on low-density data to learn from a teacher model trained on high-density data, effectively handling missing electrodes through contrastive distillation. To integrate transfer learning and distillation, we jointly pre-train the teacher and student models by contrasting their queries and keys during pre-training, enabling robust distillers for downstream tasks. We demonstrate the effectiveness of our method on four classification tasks across two clinical EEG datasets with abundant unlabeled data and limited labeled data. The experimental results show that our approach significantly outperforms contemporary methods in both efficiency and accuracy.", "sections": [{"title": "INTRODUCTION", "content": "Electroencephalography (EEG) is a pivotal tool for elucidating neural dysfunctions, making it indispensable for the clinical diagnosis of brain disorders (Sanei & Chambers, 2013). Manual analysis of resting-state EEG (rs-EEG) signals often suffers from low accuracy due to their inherent complexity. In contrast, computer-aided diagnostic methods offer substantial improvements in diagnostic performance. Traditional methods typically involve extracting temporal and spatial features from EEG signals and applying machine learning techniques to develop effective classifiers (Trivedi et al., 2016). Recent advances have seen deep graph learning revolutionize EEG signal analysis. Instead of treating EEG data as conventional numerical inputs, researchers now represent it as non-Euclidean graph data. Graph Neural Networks (GNNs) (Kipf & Welling, 2016) are employed to capture the intricate features and topological structures inherent in these graphs. This innovative approach has markedly enhanced the accuracy and reliability of EEG-based diagnostics, showcasing the potential of GNNs in advancing applications (Song et al., 2018).\nDespite these advancements, several critical issues remain unresolved. Firstly, acquiring a substantial amount of accurately labeled clinical rs-EEG data for supervised training on a specific task is challenging due to the complexities involved in data collection (Siuly et al., 2016). Models trained on these limited labeled datasets often exhibit poor accuracy and generalization (Lashgari et al.,"}, {"title": "RELATED WORKS", "content": ""}, {"title": "GRAPH NEURAL NETWORKS FOR EEG MODELING", "content": "Recent advancements in Graph Neural Networks (GNNs) have demonstrated their potential in enhancing the modeling and interpretation of EEG data. Notably, the Dynamical Graph Convolutional Neural Network (DGCNN) (Song et al., 2018) was introduced to improve emotion recognition by dynamically learning the interrelationships among EEG channels. Similarly, the Regularized Graph Neural Network (RGNN) (Zhong et al., 2020) applied a regularization strategy to advance emotion recognition from EEG data. Liu et al. (Liu et al., 2023) tackled a similar problem by developing a novel method for emotion recognition from few-channel EEG signals, integrating deep feature aggregation with transfer learning. For medical EEG field, Tang et al. (Tang et al., 2021) employed self-supervised GNNs to advance seizure detection and classification, achieving significant improvements in identifying rare seizure types."}, {"title": "SELF-SUPERVISED GRAPH PRE-TRAINING", "content": "Self-supervised learning (SSL) pre-training (Zhang et al., 2022a) has proven effective in harnessing extensive unlabeled datasets. Two predominant SSL methods are contrastive learning-based (CL-PT) pre-training, originating from computer vision, and generative-based masked autoencoders (MAE-PT) pre-training, adapted from natural language processing (NLP). These pre-training techniques have been extended to graph models. For instance, GCC (Qiu et al., 2020) and GraphCL (You et al., 2020) were among the pioneers in applying contrastive learning to graphs by leveraging graph augmentation to generate sample pairs and construct contrastive losses. Concurrently, GMAE (Hou et al., 2022) and GPT-GNN (Hu et al., 2020) adapted the generative masked pre-training ap-"}, {"title": "GRAP\u03a1\u0397 KNOWLEDGE DISTILLATION", "content": "Graph Knowledge Distillation focuses on transferring knowledge from a complex, large-scale model (teacher) to a more streamlined and efficient model (student), thus preserving performance while reducing computational demands. G-CRD (Joshi et al., 2022) introduced a distillation loss function for GNN-to-GNN transfer, employing a contrastive learning strategy to enhance similarity among nodes of the same class and increase separation between different classes. MSKD (Zhang et al., 2022b) proposed a multi-teacher distillation approach, integrating various teacher GNN models of different scales into a single student GNN model. Approaches such as Graph-MLP (Hu et al., 2021), and VQGraph (Yang et al., 2024) focused on transferring knowledge from structure-aware teacher GNNs to structure-agnostic student MLPs."}, {"title": "METHODOLOGY", "content": ""}, {"title": "EEG GRAPH CONSTRUCTION", "content": "An EEG graph can be formally represented as $G = (V, A)$, where $V = {V_1, . . ., V_n}$ denotes the set of nodes in the graph G. The matrix $X \\in R^{n \\times d}$ represents the node features, with n indicating the number of nodes (or electrodes) and d specifying the dimensionality of the feature vector associated with each node. The adjacency matrix of the EEG graph G is denoted by $A \\in R^{n \\times n}$. The EEG graph G is derived from the original EEG time series signals $S = {s_1,...,s_n} \\in R^{n \\times t}$ recorded by EEG caps, where n represents the number of channels or electrodes, and t denotes the length of the time series for each channel.\n$X = PSD(Filter(S))$\n$A = Corr(X)$  (1)\nTo convert resting-state EEG (rs-EEG) time series into graph representations, we first apply band-pass filtering $Filter(\\cdot)$ to extract EEG signals within the following frequency bands: $\\theta$ (4-8 Hz), $\\alpha$ (8-14 Hz), $\\beta$ (14-30 Hz), and $\\gamma$ (30-50 Hz). Subsequently, we compute the power spectral density (PSD) features $PSD(\\cdot)$ for each band, selecting the a band for this study. These PSD features are utilized as node features for the EEG graph. The Pearson correlation $Corr(\\cdot)$ is then computed between nodes to construct the adjacency matrix A, which represents the edge connectivity."}, {"title": "UNIFIED GRAPH PRE-TRAINING FOR DISTILLATION", "content": "To fully leverage the extensive amount of unlabeled EEG data, we propose a graph self-supervised pre-training approach to pre-train EEG models from a graph-based perspective. Our motivation stems from the observation that prior research has predominantly focused on either contrastive-based or generative-based pre-training methods for EEG time series, with limited studies addressing these techniques within the context of EEG graph models. To address this gap, we introduce a unified graph self-supervised pre-training paradigm, termed GCMAE-PT, based on the following assumptions:\nAssumption 1: (Combining GCL and GMAE for Enhanced Distillation) Hybridizing contrastive-based and generative-based pre-training by simultaneously reconstructing contrastive pairs and contrasting the reconstructed samples provides a more robust distiller, rather than applying these methods separately or in sequence.\nAssumption 2: (Joint Pre-Training of Teacher and Student Models) Both the teacher and student models benefit from joint pre-training through the contrasting of each other's positive and negative pairs, leading to improved distillation performance.\nConsider two types of EEG graph inputs: the high-density EEG graph $G^h = (V^h, A^h) \\in R^{m \\times d}$ with nodes $V^h = {vr, ..., vm}$ and the low-density EEG graph $G^l = (V^l, A^l) \\in R^{n \\times d}$ with nodes $\u03bd\u03b9 = {v1, ..., v_n}$, where m and n represent the number of nodes (or electrodes) in $G^h$ and $G^l$, respectively, and m > n. Note that $G^l$ can be regarded as a subgraph of $G^h$. Additionally, two graph encoders are employed: a teacher graph encoder $M_T^Q$ (where Q denotes queries) with extensive parameters and robust feature extraction capabilities, and a lightweight student graph encoder $M_S^Q$ with fewer parameters and comparatively lower learning capacity. It is noteworthy that $A^h$ and $A^l$ can be dynamically learned and adjusted throughout the training process. The teacher and student models are adaptable to different types of GNNs, such as transductive spectral-based traditional GCNs (like DGCNN(Song et al., 2018)) or spatial-based graph transformers (Yun et al., 2019).\nSince $V^l$ is derived from $V^h$, that is $V^l \\subseteq V^h$, we partition the complete node set $V^h$ (the Com-plete/HD Set) in $G^h$ into the Deleted Set $V^d = {v_q, ., v_{m-n}}$ and the Remaining/LD Set $V^l$. The set $V^d$ comprises (m \u2013 n) nodes present in $G^h$ but absent from $G^l$, representing the removed electrodes/nodes. Conversely, $V^l$ includes the n nodes retained in $G^l$. The relationships among these sets can be expressed as $V^l = V^h \u2013 V^d$, where $V^l \\subseteq V^h$, $V^d \\subseteq V^h$, $V^d \\cap VI = \\O$, and $V^d\\cup V^l = V^h$. Thus, the complete set $V^h$ is composed of the deleted set $V^d$ and the remaining LD set $V^l$.\nAs illustrated in Fig. 1, to construct the contrastive-based pre-training paradigm, graph augmentation techniques $Aug(.)$ (You et al., 2020) are initially applied to $G^h$ and $G^l$ by randomly dropping nodes (i.e., count = c) and removing edges. This process yields Query graphs $(Q^h = {q_h,...,q_{m-c}} \\in IR^{(m-c) \\times d}, Q^l = {q_1,...,q_{n-c}} \\in R^{(n-c) \\times d})$ and Key graphs $(K^h = {k_h,..., k_{m-c}} \\in R^{(m-c) \\times d}, K^l= {k_1,...,k_{n_c}} \\in R^{(n-c) \\times d})$ for both $G^h$ and $G^l$. This can be formulated as $(Q^h, K^h) = Aug(G^h)$ and $(Q^l,K^l) = Aug(G^l)$. Consequently, the total augmented graphs for $G^h$ and $G^l$ are denoted as $\\hat{G}^h$ and $\\hat{G}^l$. Note that $\\hat{G}^h = Mix(Q^h, K^h) = Q^h, K^h$ and $\\hat{G}^l = Mix(Q^l, K^l) = Q^l, K^l$, where the function Mix(.) represents the integration of two graph sets.\nTo achieve the goal of reconstructing the contrastive pairs as outlined in Assumption 1, the Masked Graphs $G_m^h$ and $G_m^l$ for GMAE-PT are constructed from the mixed contrastive augmented samples $\\hat{G}^h$ and $\\hat{G}^l$ by substituting the dropped nodes $v_d$ from $V^d$ with learnable embeddings $e_i \\in R^{1 \\times d}$. Subsequently, both the teacher and student encoders are employed to encode the masked graphs $G_m^h$ and $G_m^l$ into the graph embeddings. To accomplish GMAE-PT, graph decoders $D^Q_T$ and $D^Q_S$ for both teacher and student encoders are utilized to reconstruct the masked graph embeddings into the original inputs $G^h$ and $G^l$ by applying the MSE Loss as the reconstruction loss $L_{Rec}$ on both the reconstructed node features $\\hat{X}$ and graph structures $\\bar{A} = X \\cdot X^{tr}$ (Yang et al., 2024).\n$L_{Rec} = ||X - \\hat{X}||_2^2 + ||A - \\bar{A}||_2^2$   (2)\nwhere $X^{tr}$ means the transpose of X. Then the reconstructed HD and LD query $(\\tilde{Q}^h, Q^l)$ and key $(\\tilde{K}^h, K^l)$ graphs are split out from the reconstructed $\\hat{G}^h$ and $\\hat{G}^l$."}, {"title": "GRAPH TOPOLOGY DISTILLATION FOR HD-LD EEG", "content": "In the downstream stage, the pre-trained models $M_T^Q$ and $M_S^Q$ are fine-tuned for specific classification tasks using limited labeled EEG data from $G^h$ and $G^l$. We employ the Cross-Entropy loss $L_{CE}$ for classification. To transfer logit-based knowledge, we adopt the classic logit distillation loss $L_{logits}$ (Hinton et al., 2015), using KL divergence $KL(.)$ to align the predicted logit distributions, allowing $M_S^Q$ to mimic the logits of $M_T^Q$. Moreover, since $G^h$ contains more nodes than $G^l$, the topological information $A^h$ learned by $M_T^Q$ from the high-density graph $G^h$ is more precise and discriminative than $A^l$, learned by $M_S^Q$ from the low-density graph $G^l$. These topological features capture the spatial connectivity of EEG electrodes, which is crucial for task performance. Thus, distilling the topological knowledge from $M_T^Q$ into $M_S^Q$ is essential to boost the performance of $M_S^Q$. To address this, we propose the Graph Topology Distillation loss $L_{GTD}^{Dis}$.\nTo quantify the similarity between node features $X_i$ of node $v_i$ and $X_j$ of node $v_j$ in the graph, we employ a similarity kernel function (Joshi et al., 2022). This function computes the similarity $Z_{ij}$ for both $G^h$ and $G^l$. Specifically, we adopt the Linear Kernel as the node similarity function $F(.)$, defined as follows:\n$Z_{ij}^h = F(X_i^h, X_j^h) = X_i^h \\cdot X_j^h$\n$Z_{ij}^l = F(X_i^l, X_j^l) = X_i^l \\cdot X_j^l$  (8)"}, {"title": "SPECIAL CASE FOR THE PROPOSED GTD LOSS", "content": "The GTD loss is primarily designed to distill topological knowledge from $G^h$ to $G^l$. However, there is a special case known as H2H distillation, where $G^l$ and $G^h$ have the same number of nodes, meaning $V^l = V^h$ and $V^d = \\O$. In this scenario, no nodes are removed, and only the connections in $A^l$ and $A^h$ may differ. With slight modifications, our loss function can also be applied to this special case. The modified GTD loss for the H2H distillation scenario is given as follows:\n$P_{ij}^+ = I(A_{ij}^l > 0)$\n$P_{ij}^- = I (A_{ij}^l > 0 \\and A_{ij}^h = 0)$  (14)\nIn this special case, GTD loss does not consider $V^d$. The learning objective becomes utilizing the learned $A^h$ learned from $M_T^Q$ to correct incorrectly edges in $A^h$ learned from $M_S^Q$, thereby making $A^l$ as close to $A^h$ as possible."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EEG DATASETS AND DOWNSTREAM TASKS", "content": "We evaluated our EEG-DisGCMAE framework on two clinical datasets with rs-EEG time series: the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) (Trivedi et al., 2016) and the Healthy Brain Network (HBN) (Alexander et al., 2017). The EMBARC dataset comprises EEG data from 308 eye-open and 308 eye-closed samples, while the HBN dataset includes 1,594 eye-open and 1,745 eye-closed samples. Detailed dataset preprocessing information is provided in the appendices. For EMBARC, we performed binary classification tasks: sex classification in Major Depressive Disorder (MDD) patients (Male vs Female) and depression severity classification based on the Hamilton Depression Rating Scale (HAM D17) (Williams, 1988) (Mild vs Severe Depression) (Boessen et al., 2013). For HBN, we conducted binary classifications for MDD (Healthy vs MDD) and Autism Spectrum Disorder (ASD) (Healthy vs ASD). Additional details can be found in the appendices. We tested three EEG electrode density levels: high-density (HD), medium-density (MD), and low-density (LD). In EMBARC, these densities correspond to the 10-20 EEG system electrode distributions of 64 (HD), 32 (MD), and 16 (LD) electrodes, respectively. For HBN, the densities correspond to 128 (HD), 64 (MD), and 32 (LD) electrodes."}, {"title": "COMPARATIVE EXPERIMENT ANALYSIS", "content": "We compared the proposed EEG-DisGCMAE against five categories of methods: Traditional Machine Learning Methods (SVM, MLP, LSTM), GNN-based Models (GCN, GFormer, Hyper-GCN (Feng et al., 2019)), EEG-specific Models (EEGNet (Lawhern et al., 2018), DGCNN, EEG-Conformer (Song et al., 2022), RGNN), Graph Contrastive Pre-training Models (GCC, GraphCL, GRACE, AutoGCL (Yin et al., 2022)), and Graph Generative Pre-training Models (GraphMAE, GPT-GNN, GraphMAE2, S2GAE (Tan et al., 2023)). As demonstrated in Table 1, our model outperforms all other state-of-the-art methods. Notably, pre-training-based models, including those based on GCL-PT (GCC, GraphCL, GRACE, AutoGCL (Yin et al., 2022)) and GMAE-PT (GraphMAE, GPT-GNN, GraphMAE2, S2GAE (Tan et al., 2023)), utilize large Graph Transformers as their backbone in this study. In contrast, our method can be suitable to both spatial-based Graph Transofrmer and spectral-based vanilla GCNs (DGCNN) as the backbone. We evaluated both tiny and large model sizes. As illustrated in Fig. 2 (a), our tiny model, with only 1.3M parameters, performs comparably to pre-training-based methods with larger models (5.7M parameters). Moreover, our large-tiny model, despite having a similar parameter size to others, significantly outperforms them by about 5% in both AUROC and accuracy. This demonstrates that our approach achieves a superior balance between performance and efficiency, delivering high performance with a more compact parameter set. As illustrated in Fig. 2 (b), we investigated the relationship between model parameters and performance across three factors: model size, model type, and varying input EEG densities. It is evident that when the model type and input EEG density are fixed, the large-size model outperforms the tiny-size model. For a given model, reducing the input density (i.e., using LD data) leads to a decline in performance compared to using HD data. However, after pre-training and distillation, the performance of the initially less effective tiny-size model improves significantly, reaching a level comparable to that of the large-size teacher model using HD data without pre-training. This demonstrates that our proposed GCMAE-PT and GTD loss can enhance model performance while maintaining a lightweight parameter set without compromising efficiency."}, {"title": "ABLATION STUDY ANALYSIS", "content": ""}, {"title": "ELECTRODE DENSITY AND MODEL SIZE", "content": "Table 2 presents ablation experiments examining EEG graphs with varying densities (HD/MD/LD) and model types (teacher/student) with different sizes (tiny/large). The results reveal that as electrode density decreases, performance on EEG recognition tasks deteriorates. The decline is more pronounced when reducing density from MD to LD than from HD to MD. This is because, while the reduction from HD to MD removes redundant electrodes, MD still retains essential information, preserving performance. However, reducing from MD to LD results in the loss of critical electrodes, leading to a significant performance drop. Additionally, ablation experiments comparing different model sizes, including tiny and large versions of the spatial-based graph transformer and spectral-"}, {"title": "ANALYSIS OF DIFFERENT PRE-TRAINING METHODS", "content": "As detailed in Table 3, we compared our GCMAE-PT with three other pre-training approaches: graph contrastive pre-training (GCL-PT) (You et al., 2020), graph masked autoencoder pre-training (GMAE-PT) (Hou et al., 2022), and a sequential combination of GCL-PT and GMAE-PT (Seq. Comb.). Following pre-training, we evaluated the models on downstream classification tasks. The results indicate that our framework surpasses GCL-PT, GMAE-PT, and their sequential combination. This underscores that sequentially combining contrastive and generative pre-training methods does"}, {"title": "ANALYSIS OF EEG PATTERNS FOR DIFFERENT MASKING AND RECONSTRUCTION", "content": "To illustrate the effectiveness of our proposed pre-training method, we visualized EEG data patterns across various densities, masking ratios, and reconstruction methods, as depicted in Fig. 3. Figure 3(a) shows clear and well-connected activated regions with no masking. As we increased the masking ratio in Figures 3(b), 3(c), and 3(d), the activated regions diminish and connectivity deteriorates, reflecting increased information loss. Figure 3(e) demonstrates the effectiveness of our reconstruction method with 50% masking, revealing a pattern that closely resembles the unmasked data in Fig. 3(a), with improved activation and high reconstruction accuracy."}, {"title": "ANALYSIS OF (PRE-)TRAINING AND DISTILLATION", "content": "As shown in Figure 4, we visualized the optimization process of the loss curves, including contrastive loss, reconstruction loss, and GTD loss, during both the pre-training and fine-tuning stages. Figure 4(a) shows that during pre-training, we jointly optimized the contrastive loss and reconstruction loss for both the teacher and student models. All four losses converge effectively during"}, {"title": "CONCLUSION", "content": "In this paper, we present an innovative framework for EEG pre-training and distillation, which effectively integrates contrastive-based and generative-based graph pre-training paradigms. Furthermore, our framework incorporates a specifically designed EEG graph topology distillation loss function, tailored for the distillation process from high-density to low-density EEG data. Our method demonstrates substantial and efficient improvements over contemporary approaches, significantly enhancing the accuracy of EEG-based disease diagnosis while facilitating seamless deployment across diverse medical devices. Moreover, our method is readily extendable to a range of EEG application scenarios, including emotion recognition, brain-computer interfacing, and epilepsy detection."}, {"title": "IMPLEMENTATION DETAILS", "content": "All our training was conducted on an NVIDIA GeForce RTX 4090 GPU. During pre-training, we used a batch size of 128, trained for 200 epochs. For downstream fine-tuning, we used a batch size of 32, trained for 400 epochs. Both pre-training and fine-tuning were optimized using the Adam optimizer."}, {"title": "PRELIMINARIES OF DYNAMIC GNNS", "content": "In traditional GNNs, the adjacency matrix A is static. However, in this paper, we adopt dynamic GNNs, where the adjacency matrix can be dynamically adjusted during training to suit the specific task better. This approach allows the model to adapt the graph structure based on the input data and learning objectives. In such models, the edge weights aij between nodes (i, j) are learned during training. The edge weights can be computed as:\n$\\alpha_{\u03caj} = \u03c3 (f (X_i, X_j))$ (15)\nwhere f() is a function for calculating edge weights, and o is an activation function (e.g., Sig-moid). The dynamic adjacency matrix A is then updated based on these weights, typically using a thresholding mechanism:\n$A_{ij} = \\begin{cases}\n1, & \\text{if } a_{ij} > \u03b8 \\\\\n0, & \\text{otherwise}\n\\end{cases}$ (16)\nwhere @ is a threshold. During message passing, the dynamic adjacency matrix influences how messages are aggregated from neighboring nodes:\n$m_i = \\sum_{j \\in N(i)} \u03b1_{ij}W x_j$ (17)\nHere, aij represents the dynamically computed edge weight used to weight the messages from neighbors. Node features are updated as follows:\n$x_i^{(1+1)} = 0 (W^lx_i^{(l)}+b^{(l)} + m_i)$  (18)\nBy dynamically adjusting the adjacency matrix, dynamic GNNs can capture more complex and evolving relationships within the graph, thereby enhancing flexibility and overall performance."}, {"title": "DETAILS OF MOTIVATION AND PROBLEM", "content": ""}, {"title": "GTL FOR UNLABELED/LABELED EEG", "content": "Many existing methods primarily focus on training models with limited labeled EEG data, overlooking the potential of abundant unlabeled data. These methods emphasize novel GNN architectures but fail to fully leverage the available data. Additionally, they do not exploit high-density (HD) EEG data to improve models for low-density (LD) scenarios. This underscores the need for strategies that integrate both labeled and unlabeled data, and use HD data to enhance performance in LD contexts.\nMoreover, most pre-training methods are directly applied to EEG time series, with very few addressing the issue from the perspective of large-scale graph pre-training. In contrast, our approach proposes pre-training EEG graph models using a graph-based pre-training perspective. This not only aims to transfer knowledge from unlabeled EEG data to tasks on labeled EEG data but also benefits HD-to-LD distillation. This is based on the following observation:\nObservation: An LD EEG graph can be viewed as an HD EEG graph with specific nodes removed. In graph contrastive self-supervised pre-training, contrastive views are obtained by graph augmentation, such as removing nodes and edges. Another graph pre-training method, graph masked autoencoders pre-training, operates by masking node features and then reconstructing them. The relationships between these methods are formulated as follows:\nDensity Decrease \u2192 Node Dropping \u21d4 Node Masking\nElectrodes Loss   GCL Augmentation  GMAE Masking (19)"}, {"title": "GKD FOR HIGH/LOW-DENSITY EEG", "content": "As previously mentioned, an LD EEG graph can be viewed as an HD EEG graph with specific nodes removed. Consequently, HD EEG contains many features that LD EEG lacks. We naturally formulate this as a graph knowledge distillation (GKD) task, focusing on how to transfer information from HD EEG data to LD EEG applications, which is a data-level distillation process. Additionally, if a more complex teacher model with a larger number of parameters is used to extract features from HD EEG data, and a simpler student model with fewer parameters is used for LD EEG data, this involves model-level distillation. The aim is to deploy the lightweight student model while ensuring that its performance approaches, or even surpasses, that of the more cumbersome teacher model.\nTherefore, the GKD process can be represented by the following formula:\n$\\begin{aligned}\n&\\text { Teacher Model }\\\\\n&\\text { HD EEG Data }\n\\end{aligned} \\stackrel{\\text { Compress (Model-level) }}{\\text { Distill (Data-level) }} \\begin{aligned}\n&\\text { Student Model }\\\\\n&\\text { LD EEG Data }\n\\end{aligned}$ (20)"}, {"title": "DATA COLLECTION AND PRE-PROCCESSING", "content": ""}, {"title": "EEG DATA QUANTITY STATISTICS", "content": "As illustrated in Fig. 5, the EMBARC dataset consists of EEG signals collected from 308 sub-jects in both eye-open and eye-closed states. The EEG time series were sampled at 250Hz, with each trial lasting approximately 200 sec-onds. Similarly, in the HBN dataset, EEG sig-nals were collected in both eye-open and eye-closed states, with 1,594 subjects for the eye-open condition and 1,764 subjects for the eye-closed condition. The duration of the record-ings is also around 200 seconds, with the same sampling frequency of 250Hz. Both EMBARC and HBN datasets use the 10-20 EEG stan-dard system, with EMBARC employing a 64-electrode cap and HBN using a 128-electrode cap."}, {"title": "EXPLANATION OF UNLABELED DATA", "content": "Collecting EEG recordings, each patient diagnosed with a particular mental disorder can be classified as a labeled subject. Patients with EEG diagnosed as other disorders or healthy controls, are categorized as unlabeled data. In clinical, the amount of labeled data diagnosed as certain disorders was limited. Therefore, models trained exclusively on such sparse labeled data are prone to un-derfitting, undermining their predictive performance. However, by broadening the scope to include aggregated data from a range of disorders to form a comprehensive unlabeled or mixed-labeled dataset, pre-training models on this enriched dataset can mitigate the constraints imposed by data scarcity. This approach enhances the model's generalizability and improves performance, even in the face of limited labeled examples."}, {"title": "CONSTRUCTION AND AUGMENTATION OF PRE-TRAINING GRAPH DATASETS", "content": "To construct the pre-training dataset, we combined the data from both the eye-open and eye-closed states from these two datasets. For EEG data augmentation, we applied a sliding window sampling"}, {"title": "(PRE-)TRAINING AND EVALUATION SETTINGS", "content": "For pre-training on the EMBARC dataset, we addressed the issue of dataset size disparity between EMBARC and the HBN dataset, which both originate from the same EEG system. Specifically, we downsampled the 128 channels of the HBN data to 64, 32, and 16 channels, maintaining the same arrangement. These downsampled data were then combined with the corresponding density datasets from EMBARC to create a unified pre-training dataset. Note that, as the EMBARC dataset does not include 128 channels, the 128-channel HD pre-training dataset does not incorporate data from EMBARC (HBN only).\nFor downstream task fine-tuning, due to the limited amount of labeled data, we employed 10-fold cross-validation with 10 runs for all model training. The Adam optimizer (Kingma, 2014) was used to optimize the training process. Pre-training was performed over 200 epochs, while downstream fine-tuning was carried out for 400 epochs."}, {"title": "CONSTRUCTION OF DOWNSTREAM DATASETS", "content": "Table 5 provides the quantity of labeled data for four downstream classification tasks across the EMBARC and HBN datasets.\nIn the EMBARC dataset, the number of subjects is consistent across eye-open and eye-closed conditions. For the MDD sex classification task, there are 296 subjects with varying levels of depression (all diagnosed with depression) and 12 normal subjects. Among the depressed individuals, there are 194 males and 102 females. For the depression severity classification task, 166 subjects are diagnosed with severe depression (HAMD17 score > 17) Boessen et al. (2013), and 130 subjects are diagnosed with mild depression (HAMD17 score < 17).\nThe HBN dataset, which includes a range of diseases, has significantly fewer labeled samples compared to the total data volume due to the high number of samples without explicit MDD and ASD diagnostic labels. Additionally, the number of labeled subjects differs between eye-open and eye-closed conditions. In the eye-open data, there are 178 healthy controls, 109 MDD patients, and 234 ASD patients. In the eye-closed data, there are 187 healthy controls, 120 MDD patients, and 245 ASD patients.\nTo ensure a large-scale pre-training dataset, we utilized slicing operations to expand the dataset size. However, for constructing labeled datasets for downstream tasks, slicing was not employed. Instead, we calculated the PSD features for the entire 200-second EEG time series."}, {"title": "COMPARISON BETWEEN THE PRE-TRAINING DATASET AND DOWNSTREAM DATASETS", "content": "For the pre-training dataset, which includes both labeled and unlabeled data, we applied slicing operations to significantly increase the dataset size. In contrast, for the downstream dataset, particularly for the HBN data, the labeled data constitutes only a small fraction of the total dataset, and no"}, {"title": "DIFFERENT CONFIGURATIONS OF TEACHER AND STUDENT MODELS", "content": "Table 6: Comparison of Model Configurations. Note that DGCNN is a spectral-based vanilla GCNs model (DGCNN), while GFormer means the spatial-based Graph Transformer model. We considered both types of graph models to demonstrate the versatility of our pipeline."}, {"title": "ABLATION STUDY ON DIFFERENT EEG BANDS", "content": "As shown in Table 7"}]}