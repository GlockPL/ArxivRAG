{"title": "ARE FRONTIER LARGE LANGUAGE MODELS SUITABLE FOR\nQ&A IN SCIENCE CENTRES?", "authors": ["Jacob Watson", "Fabr\u00edcio G\u00f3es", "Marco Volpe", "Talles Medeiros"], "abstract": "This paper investigates the suitability of frontier Large Language Models (LLMs) for Q&A inter-\nactions in science centres, with the aim of boosting visitor engagement while maintaining factual\naccuracy. Using a dataset of questions collected from the National Space Centre in Leicester (UK),\nwe evaluated responses generated by three leading models: OpenAI's GPT-4, Claude 3.5 Sonnet,\nand Google Gemini 1.5. Each model was prompted for both standard and creative responses tailored\nto an 8-year-old audience, and these responses were assessed by space science experts based on\naccuracy, engagement, clarity, novelty, and deviation from expected answers. The results revealed\na trade-off between creativity and accuracy, with Claude outperforming GPT and Gemini in both\nmaintaining clarity and engaging young audiences, even when asked to generate more creative\nresponses. Nonetheless, experts observed that higher novelty was generally associated with reduced\nfactual reliability across all models. This study highlights the potential of LLMs in educational\nsettings, emphasizing the need for careful prompt engineering to balance engagement with scientific\nrigor.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) such as GPT (Generative Pre-Trained Transformer), Claude and Gemini have advanced\nconsiderably in recent years, now being capable of maintaining conversations and providing complex and creative\nanswers to prompts. These frontier models provide opportunities to science centres and museums to capture audiences\nand enhance the experience of visitors through emergent and procedural experiences, educating them about the\ninstitution's chosen thematic field while ensuring the delivery that is entertaining, engaging and creative [1, 2, 3].\nHowever, LLMs must be used with caution to mitigate well-known risks such as hallucinations, bias and misinformation\n[4, 5].\nIn educational contexts like science centres, LLMs have shown promise in both enhancing accuracy and public\nengagement through interactive experiences. For instance, approaches such as TUTOREVAL and TUTORCHAT have\nbolstered models' accuracy in science tutoring [2], while prompt engineering alone has yielded high performance\non scientific benchmarks like SciQA [4]. Other applications include personalized tours in virtual environments [5]\nand interactive Q&A systems providing contextual cultural knowledge [1]. Although these systems achieved reliable\nresponses, balancing creativity with scientific accuracy remains a key challenge, particularly in engaging young\naudiences with dynamic and accurate interactions.\nThe main goal of this research is to identify whether recent developments in frontier LLMs have allowed them to\nbecome suitable for Q&A interactions with visitors in science centres. In order to do it, we collected a set of questions\nfrom visitors from a science centre, the National Space Centre in Leicester (UK), and prompted LLM models to provide\nresponses. These responses were then assessed by science experts according to a set of criteria of scientific rigor and\ncreativity. The main contributions of this paper are:"}, {"title": "Related Work", "content": "The rapid advancements in LLMs have opened new avenues for educational and cultural applications, particularly\nin science centres. These environments demand a careful balance between creativity, factual accuracy, and user\nengagement. This section reviews existing literature, moving from foundational research on balancing creativity and\nfactuality in LLMs to applications in specialized contexts such as science tutoring and visitor attraction experiences."}, {"title": "Balancing Creativity and Factual Accuracy in LLMS", "content": "A critical aspect of applying LLMs is the trade-off between creative generation and factual accuracy, especially in\nsettings that prioritize reliable information alongside engaging content. The approach in [6] addressed this balance\nthrough a mathematical framework using a combined loss function, enabling fine-tuning that adjusts responses based\non specific application needs. This adaptable approach provides a theoretical foundation for managing creativity and\nfactuality in LLM outputs, setting a basis for applications in contexts such as interactive educational platforms and\ndigital learning environments.\nFurther investigation into the effects of model alignment on creativity was conducted by [7], who examined how\nReinforcement Learning from Human Feedback (RLHF) impacts LLMs. The study revealed that RLHF reduces biases\nand improves response safety but can restrict creativity by forming \"attractor states\", where responses become uniform.\nThis phenomenon has implications for models used in engaging applications like Q&A in science centres, where\noriginality and accuracy must coexist to effectively captivate and inform visitors.\nAn additional approach to managing creativity within factual constraints was explored by [8], who analyzed hallucina-\ntions in LLMs from a creative perspective. The authors suggested that, when controlled, hallucinations can enhance\ncreativity by introducing innovative responses in scenarios that do not require strict factual adherence. This approach is\nrelevant to settings such as science centres, where creative responses can foster visitor interest, provided that content\nremains broadly grounded in reality."}, {"title": "Educational Applications of LLMs", "content": "LLMs have demonstrated significant potential as educational tools, particularly in science and technology fields where\naccuracy and interactive learning are crucial. In [9] the authors developed TUTOREVAL and TUTORCHAT, two\nbenchmarks designed to evaluate and enhance the ability of LLMs to engage in structured science tutoring. These\nbenchmarks show how LLMs can support accurate, interactive tutoring in STEM disciplines, establishing a framework\nfor deploying LLMs in science centres, where educational rigor and engagement are paramount.\nBeyond tutoring, recent research has employed LLMs to improve educational experiences through Question Answering\n(Q&A) systems, particularly in science centres and cultural institutions. Notably, in [10], the authors examined GPT-3's\nperformance in Visual Question Answering (VQA) for cultural heritage applications, where the model generated\ncontextual descriptions of artworks. However, it struggled with detailed visual interpretations, indicating that while\nLLMs excel in contextual understanding, supplementary techniques may be necessary for fine-grained visual analysis\nin science centre settings."}, {"title": "LLMs for Visitor Engagement and Personalization in Museums", "content": "Recent advancements by [11] have enabled LLMs to create more engaging, personalized experiences for museum\nvisitors. The authors developed a LLM-based recommendation system tailored to individual visitor interests, providing\ndynamic and immersive museum tours. This approach aligns closely with the needs of science centres, where unique,\ncustomized interactions enhance visitor experiences by responding to specific user interests and inquiries."}, {"title": "Techniques to Improve Factual Consistency and Creativity", "content": "To improve factual accuracy in Q&A applications, as described in [14], researchers proposed a method that integrates\nontologies for real-time verification of SPARQL queries in knowledge graph systems. This approach dynamically\ndetects and corrects factual inaccuracies, emphasizing the role of semantic verification in contexts where accuracy is\ncritical, such as in enterprise Q&A and science centres.\nFor creativity enhancement, based on findings of [15], a role-playing framework was introduced, employing LLM-agent\ndialogues with diverse perspectives to promote original thinking. This collaborative approach fosters creativity in\nmodel responses and is relevant to science centres, where engaging visitors with novel perspectives is valuable. In [16],\nLLM creativity was further assessed through adapted Torrance Tests, revealing that while LLMs display fluency and\nflexibility, they often lack originality. These insights are significant for science centres aiming to balance factuality with\nengaging responses that capture visitor interest.\nAccording to [12], the exploration of the use of ChatGPT as a museum guide emphasized the importance of human\noversight to ensure factual accuracy. This study demonstrated the model's ability to adapt tone and content based\non visitor demographics, a concept resonating with the goal of creating adaptable educational experiences in science\ncentres."}, {"title": "Contributions and Positioning of Current Research", "content": "Collectively, these studies illustrate the progress in balancing creativity, factual accuracy, and personalized engagement\nin LLMs, spanning applications from science education to cultural heritage. Building on this foundation, our study\nevaluates frontier models like GPT, Gemini, and Claude in the Q&A context of science centres. We focus on\ndetermining their suitability for engaging young audiences while maintaining high standards of factual accuracy,\nspecifically addressing the challenge of balancing educational rigor with engaging and creative responses tailored for\nvisitor interactions."}, {"title": "Experimental Setup", "content": "In this section, we provide a detailed account of how the experiments were set up to assess the suitability of frontier\nLLMs to answer visitors' questions at science centres. The National Space Centre (NSC) served as a case study,\nwhere we collected commonly asked questions from visitors. Responses were then generated by the chosen LLMs and\nassessed by science experts from the NSC."}, {"title": "The National Space Centre", "content": "The National Space Centre is the UK's largest space-themed visitor attraction, educating visitors in the excess of\n300,000 per year. It contains a range of exhibits informing visitors on space science topics including the history\nof human space exploration, the science of celestial bodies and potential future developments in space travel. This\ninformation is currently conveyed through a wide range of interactive activities, displays and planetarium shows in the\nUK's largest planetarium."}, {"title": "Questions Dataset", "content": "Initially, an internal review at the National Space Centre aimed to identify frequently asked questions, and three potential\nsources were recognized: i) questions from visitor polling, ii) recorded frequently asked questions that the Space Centre\nstaff were trained to answer, and iii) questions submitted during 'Ask-the-Experts' events geared towards adult space\nenthusiasts. This approach ensured diversity in the dataset, catering to various visitor groups with different levels\nof specificity and prior knowledge. From the initial dataset of 141 questions (see Table 10 in the Appendix for all"}, {"title": "LLMs Responses Generation", "content": "We selected the three most advanced frontier models to generate responses for the visitors' questions: OpenAI's GPT-4,\nClaude 3.5 Sonnet and Google Gemini 1.5. Each LLM was prompted to generate two responses for each question.\nIn Table 2, the two types of prompts are presented: standard and creative. The standard prompt demands short and\nconcise answers suitable for an eight-year-old audience, a common age group among National Space Centre visitors\ndue to the inclusion of astronomic topics in the British science syllabus for this age. Although the questions were\nposed from a diverse range of ages and sources, the science centre would require that a model be capable of fielding\nquestions from adults and answering a manner that includes younger visitors therefore analysing against a target age\nfor the models response allows for a review of the models suitability with the target market in mind. The creative\nprompt maintains this requirement but also encourages the model to produce responses that are both surprising and\nunconventional. This tests the LLM's ability to uphold high standards of accuracy and clarity while aligning with the\nscience centre's secondary objective of engaging its audience."}, {"title": "Science Experts Assessment", "content": "The final stage of the experiments was to send the generated AI responses to the education teams at the National\nSpace Centre, specifically targeting those identified as space experts. The review board of five experts had their status\nvalidated through a range of academic and professional qualifications including relevant PhDs and degrees in space\nscience related fields, years of experience within the education sector and significant careers as communicators within\nthe National Space Centre and at educational events across the world. Given their roles at the NSC, these experts\nare specifically focused on a high level of accuracy and clarity when educating visitors on interesting and engaging\ninformation about space and space science. A subset of responses, chosen for their varied outputs across models, was\nrandomized and presented to the experts for evaluation using a five-point Likert scale. Responses were assessed in a\nblind testing environment, with original prompts included but without indicating which LLM generated each response,\nso to ensure unbiased assessment. Evaluation criteria included accuracy, clarity, engagement, deviation from typical\nexpert answers, and level of surprise. Namely, for each response, each science expert was required to answer the 5\nquestions listed in Table 3."}, {"title": "Results", "content": "In this section, we present the main results from our experiments. Figure 1 presents a comparison of the average scores\nfor the three selected LLMs (OpenAI's GPT-4, Claude 3.5 Sonnet, and Gemini 1.5), while Figure 2 shows the average\nscores for each question type from combined LLM responses."}, {"title": "Metrics Used", "content": "Experts from the science centre rated the models based on the evaluation questions listed in Table 3, with scores ranging\nfrom 1 (lowest) to 5 (highest) for each LLM response. Each metric corresponds to a specific evaluation question. Error\nbars indicate the standard deviation of scores, illustrating the variability in each model's performance. The metrics can\nbe described as follows:\n\u2022 (a) Accuracy: This measures how well the models' responses align with correct answers. Higher scores\nindicate greater accuracy.\n\u2022 (b) Clarity: This evaluates the clarity of communication, assessing how understandable the response is for a\nyoung child. Higher scores indicate simpler, more accessible language.\n\u2022 (c) Engagement Factor: This reflects how engaging each model's response is for a young audience. Higher\nscores suggest that the response is likely to capture the interest of an 8-year-old child.\n\u2022 (d) Difference to Expected Answer: This measures the deviation from what an expert might typically explain.\nLower scores indicate responses that are closer to an expert's expected answer, while higher scores reflect\ngreater deviation.\n\u2022 (e) Surprise Factor: This represents the level of novelty or unexpectedness in the responses, indicating\ncreativity or novelty. Higher scores suggest a more surprising answer."}, {"title": "Analysis per LLM", "content": "Figure 1(a) shows that Claude consistently presented higher accuracy than the other models for both standard and\ncreative responses. Claude also presented higher scores for clarity and engagement (Figures 1(b) and 1(c)) when\nprompted for creativity, with answer including 'NASA studies the ocean because Earth is a water world; oceans cover\n71% of our planet's surface! By studying the vast oceans from space, NASA can uncover secrets about Earth's climate,\nweather, and even the origins of life itself. Exploring the deep blue is key to understanding our amazing home planet.'\nin response to the prompt 'Answer this question in 50 words or less. You are aiming to educate an 8 year old. Try to\nbe surprising and unexpected. Why does NASA study the ocean?\". Answers like this outperformed GPT and Gemini,\nwhich both exhibited declines in both accuracy and engagement with the creative prompt. Claude's average accuracy\ndecreased by only 0.4, whereas both GPT and Gemini experienced significant reductions in their reviewed scores.\nA similar trend is observed in the expert ratings for clarity. Claude's average clarity score remained unchanged when the\ncreative prompt was used, while the scores for Gemini and GPT both decreased. Gemini providing responses including\n'Astronauts might live on Mars someday, like explorers in a giant bug suit! But first, we gotta build a pizza place - gotta\nhave fuel for all that exploring, right?' to the prompt 'Answer this question in 50 words or less. You are aiming to\neducate an 8 year old. Try to be surprising and unexpected. Do you think we will ever live on Mars?' which confused\nthe experts and was reviewed with particularly low clarity. This suggests that Claude's LLM is more resilient to quality"}, {"title": "Analysis per Question Type", "content": "Figure 2 presents the results based on the same metrics as above, but organized by question types (closed, open,\ndivergent, wildcard). This graph offers additional insights into the trade-offs between factual alignment and engagement\nwith respect to the diverse types of queries visitors might pose and to how LLMs are prompted to generate responses.\nFigures 2(a) and 2(b) demonstrate that responses generated using standard prompts consistently achieve higher scores in\naccuracy and clarity, particularly for the open-question and divergent-question categories. This confirms, as noted in the\nprevious section, that concise and straightforward prompts result in precise and easily comprehensible answers suitable\nfor an 8-year-old audience. As expected, prompts aimed at fostering creativity exhibit a certain decline in accuracy\nand clarity, indicating that an emphasis on creativity may inadvertently introduce factual inaccuracies or complicate\nlanguage. Conversely, we notice increased scores in Figures 2(d) and 2(e) when creative prompts are used; this might\nsuggest that a high level of unexpectedness and surprise can sometimes simply arise from a factually incorrect answer.\nA notable exception is observed in the wildcard category, where shifting from standard to creative prompts leads to a\nslight increase in accuracy and clarity, accompanied by a similar rise in the surprise factor. This can be explained by the\ninherent nature of wildcard questions, which lack an objectively valid answer.\nOverall, our experimental findings suggest that standard prompts are more effective in achieving the primary goals\nof accuracy and accessibility in educational contexts. Conversely, creative prompts excel in metrics such as deviation\nfrom expected answers and the surprise factor. This emphasizes the importance of tailoring prompts to the specific\neducational or communicative goals in science outreach, and possibly to the type of questions being answered. For\ninstance, while standard prompts seem to perform better for closed questions, where accuracy is crucial, our study\nsuggests that prompts designed to induce creative answers can produce more appropriate responses for questions that\nencourage imaginative thinking, such as those in our wildcard category."}, {"title": "Conclusion", "content": "This study investigated the potential of frontier Large Language Models (LLMs) to generate responses that engage\nand educate young audiences in science centres, focusing specifically on 8-year-old children. Our findings indicate\nthat Claude consistently outperformed GPT-4 and Gemini in maintaining educational quality, even when prompted to\nproduce creative responses. This superior performance demonstrates that LLMs like Claude have significant potential\nto enhance educational experiences by effectively balancing engagement with factual accuracy. Our study highlights\nthe necessity for careful prompt engineering and potential fine-tuning to mitigate losses in accuracy when aiming for\nincreased creativity. The varying effectiveness of different prompt types across question categories underscores the\nimportance of adaptive strategies in LLM deployment.\nA limitation of this research is the reliance on expert assessments without direct feedback from the target young\naudience. Addressing this limitation, future work should involve refining prompt structures to better balance creativity\nand clarity across different models and expanding the assessment framework to include feedback from actual young\naudiences. Additionally, experimenting with adaptive prompt strategies based on the nature of the question, such as\ndistinguishing between factual and conceptual queries, could further enhance the educational effectiveness of LLMs in\nscience centres.\nIn conclusion, while LLMs hold great promise for educational applications, particularly in engaging young audiences,\nachieving the optimal balance between creativity and accuracy remains a challenge. By addressing this balance through\ntargeted prompt engineering and model refinement, we can successfully integrate LLMs into educational settings like\nscience centres, ultimately enriching the learning experience for young visitors and fostering a lifelong interest in\nscience."}, {"title": "Ethics", "content": "This research has been approved by the University of Leicester Ethics Committee under the title Creativity Evaluation\nOf Generative AI In Museum Exhibitions and Project ID 0696 and authorized by the National Space Centre."}]}