{"title": "FairREAD: Re-fusing Demographic Attributes after Disentanglement for Fair\nMedical Image Classification", "authors": ["Yicheng Gao", "Jinkui Hao", "Bo Zhou"], "abstract": "Recent advancements in deep learning have shown transformative potential in medical imaging, yet concerns about\nfairness persist due to performance disparities across demographic subgroups. Existing methods aim to address these\nbiases by mitigating sensitive attributes in image data; however, these attributes often carry clinically relevant in-\nformation, and their removal can compromise model performance\u2014a highly undesirable outcome. To address this\nchallenge, we propose Fair Re-fusion After Disentanglement (FairREAD), a novel, simple, and efficient framework\nthat mitigates unfairness by re-integrating sensitive demographic attributes into fair image representations. FairREAD\nemploys orthogonality constraints and adversarial training to disentangle demographic information while using a con-\ntrolled re-fusion mechanism to preserve clinically relevant details. Additionally, subgroup-specific threshold adjust-\nments ensure equitable performance across demographic groups. Comprehensive evaluations on a large-scale clinical\nX-ray dataset demonstrate that FairREAD significantly reduces unfairness metrics while maintaining diagnostic accu-\nracy, establishing a new benchmark for fairness and performance in medical image classification.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of deep learning-based\ncomputer vision and medical imaging techniques, artifi-\ncial intelligence (AI) models are receiving increasingly\nwidespread attention for their enormous potential in diag-\nnosis and assisting medical decisions. As of August 2024,\nthe U.S. Food and Drug Administration (FDA) has ap-\nproved 950 AI/ML-enabled medical devices, with the list\nexpanding month by month (FDA, 2024). However, con-\ncerns about the fairness of these models have surfaced, as\nstudies (Gichoya et al., 2022; Glocker et al., 2023; Yang\net al., 2024a) reveal that deep learning models can inher-\nently encode sensitive information such as race, gender,\nand age, which can lead to significant performance dis-\nparities across various demographic subgroups. This phe-\nnomenon, referred to as \u201cunfairness,\u201d poses challenges to\nequitable healthcare delivery.\nIn the context of deep learning, unfairness is formally\ndefined as \u201cthe phenomenon where the effectiveness of\ndeep learning models notably favors or opposes one sub-\ngroup over another\" (Xu et al., 2024). Unfairness has\nbeen widely observed in medical imaging models. For\ninstance, Seyyed-Kalantari et al. (2021) and Zhang et al.\n(2022) examined chest X-ray classifiers and observed sig-\nnificant performance gaps across sex, age, and ethnicity\nsubgroups, measured by metrics such as True Positive\nRate (TPR). Similar disparities have been found in tasks\nsuch as brain MRI reconstruction (Du et al., 2023), car-\ndiac MRI segmentation (Puyol-Ant\u00f3n et al., 2021), and\nthoracic CT classification (Ashames et al., 2024).\nThe underlying causes of unfairness in medical imag-\ning models remain an open question. Some studies sug-"}, {"title": "2. Methods", "content": "The overall architecture of our proposed model, Fair-\nREAD, is illustrated in Figure 1. There are three key fea-\ntures included in our model: fair image encoder (2.1), re-\nfusion mechanism (2.2), and subgroup-specific classifica-\ntion threshold (2.3). The aim of our model is to make use\nof information from both the input image and the demo-\ngraphic attributes, and at the same time mitigate unfair-\nness of the classification output.\nIn this section, we also explain the detailed implemen-\ntation of FairREAD (2.4), and the dataset (2.5), baseline\nmethods (2.6) and metrics (2.7) we use in our experi-\nments."}, {"title": "2.1. Fair image encoder construction", "content": "The input image is first passed into a fair image en-\ncoder, from which we aim to obtain a fair representa-\ntion of the image that contains no information related to\nthe demographic attributes. To construct the fair image\nencoder, we design a novel training scheme for the en-\ncoder such that our target representation is independent\nof the sensitive representation where combined orthogo-\nnality and adversarial training are leveraged.\nConsider a binary image classification problem with in-\nput images x \u2208 X, and ground truth labels y \u2208 {0,1}.\nSuppose each input sample has da demographic attributes\ngiven as a column vector a \u2208 {0, 1}$^{da}$. Our goal is to train\nan image-to-target classifier $f_{T}$ : X \u2192 {0, 1} with n train-\ning samples. Suppose that $f_{T}$ is composed of two parts:\nan image encoder $\\phi_{T}$ : X \u2192 $R^{d_{Z}}$ and a classification head\n$h_{T}$ : $R^{d_{Z}}$ \u2192 {0, 1}, where the output of $\\phi_{T}$, defined as latent\nvector $z_{T}$ \u2208 $R^{d_{Z}}$, is the input to $h_{T}$. In order to mitigate\nunfairness in $f_{T}$, we also train two auxiliary models: an\nimage-to-demographic-attribute classifier $f_{A}$ : X \u2192 $R^{d_{A}}$,\nand an adversarial latent-vector-to-demographic-attribute\nclassifier $f_{adv}$ : $R^{d_{Z}}$ \u2192 $R^{d_{A}}$. We denote the image encoder\npart of $f_{A}$ as $\\phi_{A}$ : X \u2192 $R^{d_{Z}}$, and the classification head\nof $f_{A}$ as $h_{A}$ : $R^{d_{Z}}$ \u2192 $R^{d_{A}}$. Note that the dimension of the\nlantent space vector in $f_{A}$ is the same as $f_{T}$. Let $Z_{T}$ \u2208 $R^{n \\times d_{Z}}$\nbe a matrix of latent space vectors given by $\\phi_{T}$ with all\ntraining samples, where each row $Z_{T}^{i}$ \u2208 $R^{d_{Z}}$ corresponds\nto one training sample, and each column $Z_{T}^{j}$ \u2208 $R^{n}$ cor-\nresponds to one latent feature. Similarly, let $Z_{A}$ \u2208 $R^{n \\times d_{Z}}$\nbe a latent space matrix given by $\\phi_{A}$, where each row is\ndenoted as $Z_{A}^{i}$ \u2208 $R^{d_{Z}}$ and each column is $Z_{A}^{j}$ \u2208 $R^{n}$. With\nthe definitions above, we construct three loss functions,\ndescribed in the following.\nColumn space orthogonality loss:. The goal of the col-\numn space orthogonality loss is to ensure that the target\nlantent vector $z_{T}$ has minimal projection onto the sensi-\ntive space, meaning that the information about the target\nis maximally independent of the demographic attributes."}, {"title": "2.2. Re-fusion mechanism", "content": "In order to make use of information from the demo-\ngraphic attributes, we propose a novel re-fusion mecha-\nnism, which aims to fuse the fair image representation and\nthe demographic attributes.\nTo begin with, the demographic attributes are encoded\nusing a Multi-Layer Perceptron (MLP), which produces\ntwo outputs, \u00b5 and \u03c3\u00b2. Next, in the re-fusion block, the\nfair image representation is projected to a lower dimen-\nsion, rescaled using \u00b5 and \u03c3\u00b2, and projected back. Then,\nthe rescaled representation and the fair representation are\nfused via element-wise multiplication. Concretely, given\nan input image x and a sensitive attribute vector a, the\nfused representation $Z_{fused}$ is derived as follows."}, {"title": "2.3. Subgroup-specific classification threshold", "content": "After a predicted logit \u0177 has been generated as de-\nscribed above, we apply subgroup-specific classification\nthreshold to further improve fairness across each demo-\ngraphic subgroup. Specifically, for each subgroup g, we\ndetermine a unique classification threshold $\u03b8_{g}$ by mini-\nmizing the absolute difference between the True Positive\nRate (TPR) and the True Negative Rate (TNR) within that\ngroup:"}, {"title": "2.4. Implementation details", "content": "The training process of FairREAD is taken out in two\nstages. In the first stage, we train the fair image en-\ncoder using column and row orthogonality loss described\nin Section 2.1. Concretely, we first train the image-\nto-demographic-attribute classifier $f_{A}$ using the cross-\nentropy loss function:\nwhere a\u1d62 and \u00e2\u1d62 represents the ground-truth and predicted\ndemographic attributes, respectively. Next, we train the\nimage-to-target classifier $f_{T}$ using the following loss func-\ntion:\nwhere $\u03bb_{C}$ and $\u03bb_{r}$ are hyperparameters controlling the\nweight of column- and row-orthogonality loss. After the\nfirst stage training is complete, we use the encoder part\n($\\phi_{T}$) of $f_{T}$ as the fair image encoder and plug it into the\nFairREAD architecture shown in Figure 1."}, {"title": "2.5. Dataset", "content": "We conduct our experiments on CheXpert (Irvin et al.,\n2019), a large public dataset containing 224,316 chest X-\nray images of 65,240 patients. We focus on four diseases\nin CheXpert: Cardiomegaly, Pleural Effusion, Pneumo-\nnia, and Fracture. We consider three demographic at-\ntributes: gender, age, and primary race. For simplicity,\nwe discretize each demographic attribute into binary val-\nues. For age, we classify all patients as less than 60 years\nand 60 years or older; For race, we classify all patients\nas White and Non-white. For gender, since the dataset\nonly contains two gender values (Male and Female), we\nuse these labels directly. In order to better manifest the\neffectiveness of our method in unfairness mitigation, we\nmagnify subgroup disparity of the dataset by subsampling\nCheXpert to create larger disparities in positive rate and\nnumber of samples across demographic subgroups. Con-\ncretely, for Cardiomegaly and Pleural Effusion, we sub-\nsample the training set to create a positive rate disparity\nof around 16%. For Pneumonia and Fracture, we only use\na positive rate disparity of around 10% since a larger value\nwould result in the number of samples within some sub-\ngroups being too small. Details of subgroup distribution\nin the dataset is shown in Table 1 and Figure 2.\nIn order to compare the performance of our model and\nthe baseline models, we hold out 10% of the data as our\ntest set, and conduct a five-fold cross-validation on the\nother 90% of the data. The data split is fixed between the\ntwo stages of training to avoid data contamination."}, {"title": "2.6. Baseline methods", "content": "In order to manifest the effectiveness of our method, we\ncompare our model to the following baselines:\nEmpirical Risk Minimization (ERM). ERM stands\nfor the vanilla training process that only optimizes against\nthe classification cross-entropy loss $L_{CE}$, and without any\nmeasures to improve fairness.\nAdversarial Learning (AL) (Wadsworth et al., 2018).\nAL trains a separate adversarial model trained to recog-\nnize the demographic attributes given the output logit of\nthe original classification model. The classification model\nis trained with a combination of minimizing the classi-\nfication loss and maximizing the loss of the adversarial\nmodel.\nLearning Not To Learn (LNTL) (Kim et al., 2019;\nBevan and Atapour-Abarghouei, 2022b). LNTL trains a\nseparate adversarial model trained to recognize the demo-\ngraphic attributes given the feature map calculated with an\nencoder. Besides, a classification head is trained to pre-\ndict the target label given the feature map. The encoder is\ntrained with a combination of minimizing the loss of the\nclassification head and maximizing the loss of the adver-\nsarial model.\nAdaptive Batch Normalization (FairAdaBN) (Xu\net al., 2023). FairAdaBN substitutes all the batch normal-\nization layers within the model with a subgroup-specific\nbatch normalization, where each subgroup is assigned to\na separate set of batch normalization parameters.\nLearning Orthogonal Representations (LOR) (Deng\net al., 2023). LOR first trains a demographic attribute"}, {"title": "2.7. Evaluation metrics", "content": "We evaluate all the methods above for both perfor-\nmance and fairness. In terms of performance, we measure\nclassification accuracy and AUC. In terms of fairness, fol-\nlowing previous works (Deng et al., 2023; Xu et al., 2023;\nZhang et al., 2022), we use subgroup disparity with re-\nspect to Equal Odds ($\u2206_{EO}$), and AUC (\u2206$_{AUC}$). Given pre-\ndicted labels \u0176, ground truth labels Y, and corresponding\nsubgroup of each sample A, the fairness metrics are de-\nfined as follows:\nwhere A is the set of all subgroups, AUC\u2090 is the AUC of\nthe model on all samples belonging to subgroup a.\nAdditionally, we use $FATE_{EO}$ and $FATE_{AUC}$ (Xu\net al., 2023) as comprehensive measures of both perfor-\nmance and fairness. These metrics measure the trade-off\nquality between performance and fairness, defined as fol-\nlows:"}, {"title": "3. Experimental results", "content": "The results of our experiments are listed in Table 2. We\ncan observe the following:\nOn average, FairREAD outperforms all baseline\nmethods at all measured metrics except accuracy,\nwhere FCRO performs the best. In terms of perfor-\nmance, FairREAD reaches the highest AUC in classify-\ning all the diseases examined. In terms of fairness, Fair-\nREAD is consistently among the top 2 in all the examined\nmethods. Notably, FairREAD outperforms other methods\nsignificantly at $FATE_{EO}$ and $FATE_{AUC}$, which demon-\nstrates that our method achieves the best comprehensive\nresult considering both performance and fairness.\nFairREAD demonstrates stability in performance\nacross all examined diseases. It consistently ranks in the\ntop two methods for $FATE_{EO}$ and $FATE_{AUC}$ across all\ndiseases. Furthermore, it is the only method with positive\n$FATE_{EO}$ and $FATE_{AUC}$ values for every disease, high-\nlighting its robustness in enhancing the trade-off between\nperformance and fairness."}, {"title": "3.2. Ablation studies", "content": "Ablation on threshold selection method. We fit\nthe classification threshold for FairREAD using different\nthreshold selection methods, including Min-gap (ours),\nYouden's J, G-Means, and default threshold (where we\nuse a classification threshold of 0.5). The results aver-\naged over all four examined diseases are shown in Table\n3. Note that threshold selection is independent to the pa-\nrameters of the model, therefore here we fix the trained\nmodel and only re-fit the threshold using the training set,\nin order to prevent the randomness in the training process\nto affect comparison of threshold selection methods. Fur-\nthermore, since the threshold selection process does not\naffect threshold-agnostic metrics including AUC, \u2206AUC,\nand FATE$_{AUC}$, these metrics are not shown in Table 3.\nWe can observe that Min-gap achieves the best perfor-\nmance in terms of \u2206EO and FATE$_{EO}$. Also, the accuracy\nof Min-gap is comparable to Youden's J and G-Means.\nThe high accuracy of the default threshold is achieved\ntrivially by taking advantage of the class imbalance in the\ndataset. Specifically, a default threshold value of 0.5 is of-\nten too high, leading FairREAD to classify almost all the\nimages as negative. Since positive images only takes up a\nsmall proportion (as shown in Table 1b), the accuracy is\nsuperficially high.\nAblation on adversarial learning. In the training pro-\ncess of FairREAD, $\u03b1_{adv}$ is a crucial hyperparameter that\ncontrols the weight of the adversarial loss in the second\nstage of training. Figure 3 shows the performance of\nFairREAD on Fracture classification with different $\u03b1_{adv}$\nvalues ranging from 0.01 to 3. We can observe that in\ngeneral, the performance metrics (Accuracy, AUC) de-\ncrease as $\u03b1_{adv}$ increases. This is because a larger $\u03b1_{adv}$"}, {"title": "3.3. Embedding space visualization", "content": "To illuminate on how FairREAD balances fairness\nand classification performance, we employ t-distributed\nStochastic Neighbor Embedding (t-SNE) (van der Maaten\nand Hinton, 2008) to visualize the embedding spaces at\nvarious stages of the model. Figure 4 presents these visu-\nalizations for the Cardiomegaly classification task, high-\nlighting the progression from the fair image encoder to\nthe final fused representations. For comparison, we also\ninclude the embedding space of a standard Empirical Risk\nMinimization (ERM) model.\nIn the embedding space of $z_{T}$, the output of the fair\nimage encoder, we can observe that samples from differ-\nent demographic attributes do not form distinct clusters,\nindicating that $z_{T}$ is largely invariant to these sensitive at-\ntributes. This demonstrates the effectiveness of the dis-\nentanglement process, where the encoder successfully re-\nmoves demographic information from the representation.\nHowever, when colored by target labels in Figure 4b, the\npositive and negative samples also become less clearly"}, {"title": "4. Discussion", "content": "In this paper, we propose a novel re-fusion after dis-\nentanglement (FairREAD) framework to mitigate unfair-\nness in medical image classification without sacrificing\nperformance of the model. To achieve this, we first train\na fair image encoder that encodes the input image while\ndiscarding information with respect to demographic at-\ntributes. Next, we pass the fair image representation into\na re-fusion module, where the encoded demographic at-\ntributes are fused with the disentangled image representa-\ntion. Finally, we apply subgroup-specific decision thresh-\nold to the output logit to derive the classification output.\nSpecifically, we propose the Min-gap strategy to decide\nthe decision threshold for each subgroup."}, {"title": "4.1. Experimental summaries", "content": "Our experiments show that FairREAD consistently out-"}]}