{"title": "Ehrenfeucht-Haussler Rank and Chain of Thought", "authors": ["Pablo Barcel\u00f3", "Alexander Kozachinskiy", "Tomasz Steifer"], "abstract": "The notion of rank of a Boolean function has been a cornerstone in the theory of PAC learning, enabling quasipolynomial-time learning algorithms for polynomial-size decision trees. We present a novel characterization of rank, grounded in the well-known Transformer architecture. We show that the rank of a function f corresponds to the minimum number of Chain of Thought (CoT) steps required by a single-layer transformer decoder with hard attention to compute f. Based on this characterization we establish tight bounds on the number of CoT steps required for specific problems, showing that l-fold function composition necessitates exactly l CoT steps. Furthermore, we analyze the problem of identifying the position of the k-th occurrence of 1 in a Boolean sequence, proving that it requires k CoT steps.", "sections": [{"title": "Introduction", "content": "Ehrenfeucht and Haussler [6] introduced the notion of the rank of a Boolean function and showed that, for any constant r, the class of Boolean functions with rank at most r is properly PAC-learnable in polynomial time. As a corollary, they derived their renowned quasipolynomial-time PAC-learning algorithm for polynomial-size decision trees. Pudl\u00e1k and Impagliazzo [19] further characterized the rank\u2014not only for Boolean functions but also for Boolean relations\u2014through Prover-Delayer games. Since its introduction, this concept has played a significant role in proof complexity [12, 7].\nIn this paper, we present a new characterization of the notion of rank. Surprisingly, this characterization is grounded in the Transformer architecture [20], which has recently revolutionized the field of NLP and facilitated the development of LLMs. In essence, we show that the rank of a function f corresponds to the minimum number of Chain of Thought (CoT) steps required by a single-layer Transformer to compute f. The Transformers used in our characterization are based on the hard attention mechanism\u2014a theoretical abstraction of the soft attention mechanism employed in practice. Hard attention has been widely used in theoretical studies [8, 10, 2, 23] due to its amenability to formal analysis, while still effectively capturing the essence of practical models [4, 21].\nThe Transformer architecture is built upon attention layers and a decoder. An attention layer performs attention on the input sequence, mapping a sequence of input vectors to another sequence of vectors of the same length. Attention layers are used to generate vector representations of sentences in natural language. However, a more common application of Transformers is sequence generation, where the input sequence is mapped to an unbounded sequence of output vectors, generated iteratively, one at a time. This task is carried out by the decoder. In the first iteration, the decoder processes the input sequence through the attention layers and outputs the vector in the last position. This output is then appended to the input sequence. During subsequent iterations, the decoder applies its attention layers to the extended sequence, computes the next output, and appends it to the sequence. These are the CoT steps mentioned earlier [16, 14].\nBelow we summarize our main results:\n\u2022 We show that the rank of a function f, denoted by rk(f), is the minimal number of iterations of a single-layer decoder with one hard-attention head that computes f. We establish our result not only for Boolean functions, generalizing the notion of the rank to the non-Boolean case (as far as we know, for the first time).\n\u2022 In practice, Transformers are equipped with multiple attention heads, which enhance their computational capabilities. We show that the ability of such Transformers to compute functions can also be characterized using the notion of rank. Specifically, we define the H-head rank of a function f, denoted as rk(H) (f), for H > 1. We prove that rk(H) (f) equals the minimum number of iterations required by a single-layer decoder with H hard-attention heads to compute f.\n\u2022 We then explore methods for obtaining tight bounds on the multi-head rank. We begin by observ-ing that rk(H) (f) is at most a factor of H smaller than rk(f). While computing rk(f) is typically straightforward, it does not always provide an accurate bound for rk(H) (f). To address this limitation, we propose a general communication complexity lower bound for rk(H) (f). Using this technique, we derive a tight bound on the H-head rank for the t-fold iterated composition, a function whose com-plexity has been previously studied for single-layer decoders with soft attention [17]. The function t-Comp takes as input a sequence of n integers from {1, ..., n}, interpreted as the values of a function \u03c6: {1,..., n} \u2192 {1,...,n}. The output of t-Comp is the value of $, composed with itself t times, evaluated at 1.\nIt is easy to see that rk(t-Comp) \u2264 t for any input length n. A decoder, establishing this upper bound works by computing \u03c6(1) in the first iteration, then \u03c6(\u03c6(1)) in the second iteration, and so on. We prove that this is optimal even if we increase the number of attention heads. Namely, for any H, we show that rk(H) (t-Comp) = t for all large enough input lengths.\n\u2022 Finally, we study the k-thOne function. This function takes as input a Boolean sequence of length n, and it returns the position of the k-th one in it. It is easy to see that rk(k-thOne) \u2264 k for any input length. In terms of decoders, in the first iteration we can compute the position of the first one, then of the second one in the second iteration, and so on. We prove that for any H and for large enough n, we have rk(H) (k-thOne) = k, showing that even increasing the number of attention heads we cannot improve upon the trivial solution for large enough input lengths. Interestingly, this result cannot be obtained via the communication complexity techniques used for iterated composition. Instead, our proof relies on a purely combinatorial argument."}, {"title": "Decision Trees and Rank", "content": "We use a notation [n] = {1, . . . , n} for n \u2208 N.\nConsider n + 1 finite sets \u03a31, . . . , \u03a3n, O, for n > 0. We are interested in decision trees that compute functions:\nf : \u03a31 \u00d7 \u03a32 \u00d7 . . . \u00d7 \u03a3n \u2192 O.\nTo do this, we consider decision trees over arbitrary families of queries, where a query is a function q whose domain is \u03a31 \u00d7 . . . \u00d7 \u03a3n. We write Im(q) for the image of query Q. If F is a set of queries, a decision tree over F is a rooted tree T such that:\n\u2022 Every non-leaf node v is labeled by some query qv \u2208 F and has exactly |Im(qv)| out-going edges, each one of them labeled by a different element from Im(qv).\n\u2022 Every leaf \u2113 is labeled by some element o\u2113 \u2208 O.\nGiven an input \u00afw = (\u03c31, . . . , \u03c3n) \u2208 \u03a31 \u00d7 . . . \u00d7 \u03a3n, the output of decision tree T on \u00afw is computed by descending from the root to one of the leaves. At each intermediate non-leaf node v, the tree computes the value qv( \u00afw) \u2208 Im(qv) and descends to the unique child of v that is linked to v through an edge labeled q( \u00afw). In this way, we reach some leaf \u2113, where T outputs the element o\u2113 as its result on \u00afw. We denote this output as T ( \u00afw).\nThe function f : \u03a31 \u00d7 . . . \u00d7 \u03a3n \u2192 O is computed by T , if T ( \u00afw) = f( \u00afw) for every input \u00afw \u2208 \u03a31 \u00d7 . . . \u00d7 \u03a3n.\nBoolean case. Decision trees are often defined for Boolean functions, i.e., functions of the form f : {0, 1}n \u2192 {0, 1}. In our notation, this corresponds to the case \u03a31 = . . . = \u03a3n = O = {0, 1}. Boolean decision trees are decision trees over a family {p1, . . . , pn} of queries, where for i = 1, . . . , n the function pi : {0, 1}n \u2192 {0, 1} is defined as follows on input (b1, . . . , bn) \u2208 {0, 1}n:\npi(b1, . . . , bn) = bi.\nThat is, at every node, a Boolean decision tree queries the value of some coordinate of the input.\nEhrenfeucht and Haussler [6] defined the rank of a Boolean decision tree T by inductively defining the rank of its nodes as follows:\n\u2022 the rank of a leaf is 0, and\n\u2022 the rank of a non-leaf v, whose two children have ranks r0, r1, is r = max{min{r0, r1}+1, max{r0, r1}}.\nThe rank of T is then the rank of its root, and the rank of a Boolean function f : {0, 1}n \u2192 {0, 1} is the minimum rank of a Boolean decision tree that computes f.\nRank in the non-boolean case and a-queries. We extend the notion of rank to the non-Boolean case through decision trees over assignment queries. We start by introducing some terminology. Pairs of the form (i, \u03c3), where i \u2208 [n] and \u03c3 \u2208 \u03a3i, are called assignments. We denote by\nA = {1} \u00d7 \u03a31 \u222a \u00b7 \u00b7 \u00b7 \u222a {n} \u00d7 \u03a3n\nthe set of assignments. An assignment (i, \u03c3) is consistent with an input \u00afw = (\u03c31, . . . , \u03c3n) \u2208 \u03a31 \u00d7 . . . \u03a3n if and only if \u03c3i = \u03c3. By a permutation of a finite set B we mean a bijection \u03c4 : {1, . . . , |B|} \u2192 B."}, {"title": "Attention Layers and Decoders", "content": "Attention layer. We consider layers with unique hard attention, and possibly multiple attention heads, where the output of the layer is computed in the last token. By unique hard attention we refer to the mechanism in which each position attends to the element with the highest attention score (breaking ties arbitrarily).\nFormally, a unique hard-attention layer (or, simply, attention layer) with H heads and embedding dimension d is a function L: (Rd)* \u2192 Rd, which is defined by\n\u2022 H query matrices $Q^{(h)} \\in \\mathbb{R}^{d \\times d}$ and H key matrices $K^{(h)} \\in \\mathbb{R}^{d \\times d}$, for h = 1,..., H,\n\u2022 two matrices $W_1, W_2 \\in \\mathbb{R}^{d \\times d}$, and\n\u2022 a matrix $W_0 \\in \\mathbb{R}^{d \\times (dH)}$.\nConsider an input sequence of vectors $(x_1,...,x_m) \\in (\\mathbb{R}^{d})^m$. The output of L on $(x_1,...,x_m)$ is computed as follows. For every h = 1, . . ., H, we compute the value of the h-th head on $(x_1,..., x_m)$, which is a vector from Rd denoted by headh \u2208 Rd. Namely, we start by computing \"attention scores\"\n$a_{i,m}^{(h)} = \\langle K^{(h)}x_i, Q^{(h)}x_m \\rangle$,\ndefining, for every i = 1,..., m, the attention from the last token to the i-th token with respect to the h-th head. The vector $K^{(h)}x_i$ is called the key of the i-th token, and the vector $Q^{(h)}x_m$ is called the query of the mth token.\nFor every h = 1,..., H, we let ih \u2208 {1,...,m} to be the index maximizing (3). If there are multiple indices achieving the maximum, we let ih be the leftmost one. We then set $head_h = x_{i_h}$, for h = 1,..., H, and define:\n$\\text{multihead} = W_0\\begin{pmatrix} head_1\\\\\\vdots\\\\head_H\\end{pmatrix} \\in \\mathbb{R}^d$\nFinally, we define:\n$L(x_1,...,x_m) = W_2 \\cdot ReLU (W_1 (\\text{multihead} + x_m)) \\in \\mathbb{R}^d$.\nRecall that ReLU(x) = max {0,x}, for every x \u2208 R, and if x \u2208 Rd then ReLU(x) is obtained by applying ReLU to each one of its components.\nDecoders. A decoder, defined by the d-dimensional attention layer L, is a function that takes on input a sequence of vectors $(x_1,...,x_m) \\in (\\mathbb{R}^{d})^m$ and in the output produces an infinite sequence of vectors $\\{y_t \\in \\mathbb{R}^{d}\\}_{t=1}^\\infty$, defined by:\n$y_1 = L(x_1,..., x_m)$,\n$y_t = L(x_1,..., x_m, y_1,\\dots, y_{t-1})$, t > 2.\nThat is, the decoder works in iterations: first, it computes the output of L, adds it to the end of the input sequence, computes the output of L on the new sequence, adds this output to the end, and so on. We refer to yt as the output of the decoder after t iterations (sometimes these iterations are called \"chain of thought steps\").\nComputation of functions by decoders. Fix n and n + 1 finite sets \u03a31,..., \u03a3\u03b7, O. We want to define how a decoder computes functions of the form:\nf : \u03a31 \u00d7 ... \u00d7 \u03a3\u03b7 \u2192 O.\nInputs to f are interpreted as words with n letters, with the i-th letter coming from the alphabet \u03a3i, for i = 1,...,n (alphabets are possibly different at different positions). We put this word as an input to a decoder using n + 1 tokens, one per letter plus a special token at the end for the \"end of line\" symbol. Input tokens can use arbitrary encodings of letters by d-dimensional vectors, potentially different at different positions of the input word, utilizing in this form a positional information. We then run the decoder on the resulting input for some number t of iterations. The output of f is computed by applying an output function to the decoder's output yt from the final iteration.\nDefinition 2 (Computation of functions by decoders). Let n be a natural number and \u03a31,..., \u03a3n, O be n+1 finite sets. A function f : 21 \u00d7 ... \u00d7 \u03a3n \u2192 O can be computed by t iterations of a decoder with H heads, if there exist:\n\u2022 d\u2208 N and an attention layer L of embedding dimension d with H heads,"}, {"title": "One-Head Decoder Depth vs Tree Rank", "content": "In this section, we show that the rank of a function is equivalent to its decoder depth in the single-head setting.\nTheorem 1. For any function f: \u03a3\u2081 \u00d7 ... \u03a3n \u2192 O, we have rk(f) = dd(1) (f).\nAs a corollary to Theorem 1 and Proposition 2, we obtain that for suitable n the decoder depth with one head of the iterated composition function t-Comp is precisely t:\nCorollary 1. For each t and for all n > 2t, we have dd(1)(t-Compn) = t.\nWe now prove our main theorem.\nProof of Theorem 1. We first show the inequality rk(f) \u2264 dd(1) (f). Assume that f can be computed by a decoder with one head in r iterations, for somer \u2208 N. We deduce that rk(f) \u2264 r. For that, we show that at the cost of ta-queries one can compute the outputs of the decoder in the first t iterations on a given input. Hence, in ra-queries, we can compute the rth output of the decoder, which uniquely determines the value of f, implying that rk(f) <r.\nConsider any input w = (\u03c31,...,\u03c3\u03b7) \u2208 \u03a31 \u00d7 ... \u00d7 \u03a3\u03b7. Define then:\nx1 = p(1,\u03c3\u2081), ..., xn = p(\u03b7,\u03c3\u03b7), yo = p(EoL) \u2208 Rd,\nwhere d is the dimension of our decoder and p is its positional encoding function. Let $\\{y_t \\in \\mathbb{R}^d\\}_{t=1}^\\infty$ be the sequence of the outputs of our decoder on input (x1,...,xn, Yo). Assume that we have already computed Y1,..., yt for some t > 0 (if t = 0, we just know yo = p(EoL)). We explain how to compute Yt+1 using one a-query. By definition,\nYt+1 = L(x1,..., Xn, YO, Y1, \u00b7 \u00b7 ., Yt),\nwhere L is the attention layer defining our decoder. It is enough to compute $s \\in \\{x_1,..., x_n, y_0, y_1, ..., y_t\\}$ with the maximal value of $\\langle Ks, Qy_t\\rangle$ for the key and query matrices K, Q \u2208 Rd\u00d7d of our attention layer. If there are multiple vectors $s \\in \\{X_1, ..., X_n, Y_0, ..., y_t\\}$ with the maximal value of this scalar product, we need to compute the leftmost one among them. Since we already have computed yo, Y1,..., yt, it suffices to find this maximal s over $\\{x_1,...,X_n\\} = \\{\\rho(1,\\sigma_1),..., \\rho(\\eta, \\sigma_\\eta)\\}$.\nConsider the following linear order of the set A of assignments. Given two different assignments a =\n(\u03af, \u03c3), \u03b1' = (i', \u03c3'), we say that a is larger than a' if either $\\langle K\\rho(a), Qy_t\\rangle > \\langle K\\rho(a'), Qy_t\\rangle$ or $\\langle K\\rho(a), Qy_t\\rangle = \\langle K\\rho(a'), Qy_t\\rangle$ and i < i'. We arbitrarily order assignments with $\\langle K\\rho(a), Qy_t\\rangle = \\langle K\\rho(a'), Qy_t\\rangle$ and i = i'.\nOur task is to find the maximal assignment from $\\{p(1,\\sigma_1), ..., p(n, on)\\}$ in this order. For that, we ask the a-query qr for a permutation \u03c4, where the first assignment is the maximal in our linear order, the second one is the second maximal, and so on.\nWe now show the inequality dd(1) (f) \u2264 rk(f). Assume that T is an r-depth decision tree over a-queries that computes f. We transform into a decoder with one head that computes f in r iterations. We assume that T is a complete r-depth |A|-ary tree, where A is the set of assignments.\nThe embedding dimension of our decoder will be:\nd = 1 + |A| + ... + |A|r\u22121\n+1+A+... + |A|\n+ A\nThe coordinates will be split into 4 groups:\n+1.\n\u2022 the first 1+ |A|+ . . . + |A|r\u22121 coordinates are called positional coordinates and are indexed by non-leaf nodes of T;\n\u2022 the second 1+ |A|+ . . . + |A|r coordinates are called output coordinates and are indexed by nodes of T;\n\u2022 the third |A| coordinates are called assignment coordinates and are indexed by assignments;\n\u2022 the last coordinate will be called special."}, {"title": "Multihead Rank", "content": "In order to generalize Theorem 1 to decoders with many heads, we define the notion of H-head rank for a function f: 21 \u00d7 ... \u00d7 \u03a3\u03b7 \u2192 O. For that we require a notion of the product of two functions with the same domain. Namely, by the product of g: A \u2192 Bandh: A \u2192 C, we mean a function (f \u2297 g): A \u2192 B \u00d7 C, defined by:\n(fg)(a) = (f(a), g(a)).\nAn H-degree a-query is a product of H a-queries.\nDefinition 4. The H-head rank of a function f : \u03a3\u2081 \u00d7 ...\u00d7 \u2211n \u2192 O, denoted rk(H)(f), is the minimal depth of a decision tree over H-degree a-queries that computes f.\nA simple generalization of the construction of Theorem 1 allows us to obtain the following result.\nTheorem 2. For any H \u2208 N and for any function f: \u03a3\u2081 \u00d7 ... \u00d7 \u03a3\u03b7 \u2192 O, we have rk(H) (f) = dd(H) (f).\nProof. We first show that rk(H) (f) \u2264 dd(H) (f). The proof for the case H = 1 works almost verbatim for the general case. As we have shown in the proof of Theorem 1, for a given decoder with 1 head, knowing the first t outputs on an input w\u2208 \u03a3\u2081 \u00d7... \u00d7 \u03a3\u2211n, we can compute the value of the head (which would give us the (t+1)-st output), asking one a-query about w. For H-head decoders, we simply compose H a-queries for each of H heads into a single H-degree a-query.\nWe now establish the inequality dd(H) (f) \u2264 rk(H) (f). Assume that T is an r-depth decision tree over H-degree a-queries, computing f. In the construction of Theorem 1, we need to multiply the number of positional and assignment coordinates by H. Positional coordinates are now indexed by pairs (v, i), where v is a non-leaf node of T and i \u2208 [H] (with i referring to one of the H a-queries, asked at v). Likewise, assignment coordinates are now indexed by pairs (a, i), where a is an assignment and i \u2208 [H].\nThe positional encoding of the assignment a is modified as follows. As before, output coordinates of p(a) are 0 and the special coordinate of p(a) is 1. Next, p(a) has 1 in the assignment coordinate, indexed by (a, 1), and 0 in the remaining assignment coordinates. Finally, for a non-leaf node v of T and i \u2208 [H], and for the corresponding positional coordinate p(a)v,i, we set $p(a)_{v,i} = 1/\\tau_{v,i}^{-1}(a)$, where Tv,i is the permutation"}, {"title": "Multihead decoder depth of iterated composition", "content": "In this section, we show a method for lower bounding the multihead rank of a function based on commu-nication complexity [13]. Let X, Y, Z be finite sets and f : X \u00d7 Y \u2192 Z be a function. Imagine that there are two players, Alice and Bob. Alice is given x \u2208 X and Bob is given y \u2208 Y. Their goal is to cooperatively compute f(x, y). For that, they can send each other messages that are binary words. They want to minimize the number of messages and their total length in bits.\nFormally, a k-round Alice-first communication protocol \u03a0 is given by:\n\u2022 k positive integer numbers l1,..., lk (messages lengths);\n\u2022 a function $M_i: {0,1}^{l_1+...+l_{i-1}} \\times X \\rightarrow {0,1}^{l_i}$ for every odd i \u2208 {1, ..., k};\n\u2022 a function $M_i: {0,1}^{l_1+...+l_{i-1}} \\times Y \\rightarrow {0,1}^{l_i}$ for every even i \u2208 {1, ..., k}; and\n\u2022 the output function out: {0,1}l1+...+lk \u2192 Z.\nThe communication complexity of \u03a0 is the sum l\u2081 + ... + lk.\nOn input (x, y) \u2208 X \u00d7 Y, the output of \u03a0 on (x, y) is computed as follows. We inductively define a sequence of binary words m\u2081 \u2208 {0, 1}l1, ..., mk \u2208 {0,1}lk by setting\nmi = Mi(m\u2081... mi\u22121,x) for odd i \u2208 {1, . . ., k},\nmi = Mi(m\u2081 ... mi\u22121, y) for even i \u2208 {1, . . ., k}.\nIntuitively, m\u2081 = M\u2081(\u03b5,x) is the first message of Alice that she sends to Bob in the protocol on input x. Upon receiving m\u2081, Bob replies with the second message m2 = M2(m1, y) that depends on his input and the first of Alice's messages. Then Alice sends the third message m3 = M3(m1m2,x), and so on. The output of the protocol is defined as out(m\u2081 ... mk) \u2208 Z.\nBy $C'_{k,A}(f)$ we mean the minimal communication complexity of a k-round Alice-first protocol that com-putes f. By reversing the roles of Alice and Bob, we define k-round Bob-first protocols, and $C'_{k,B}(f)$, the minimal communication complexity of a k-round Bob-first protocol for a function f.\nAssume we have a function f: \u03a3\u2081 \u00d7 ... \u00d7 \u03a3n \u2192 O and a subset S\u2286 [n]. Suppose that positions of an input word w\u2208 \u03a3\u2081\u00d7... \u00d7 \u03a3n are split between Alice and Bob like this: Alice knows letters of wat positions i \u2208 S, and Bob knows letter of wat positions i \u2208 [n] \\ S. Their goal is to find out f(w). This defines a function:\n$\\ f_S: (\\prod_{i \\in S} \\Sigma_i) \\times (\\prod_{i \\in [n] \\setminus S} \\Sigma_i) \\rightarrow O$\nwhere the two inputs correspond to the parts of w that Alice and Bob knows, respectively, and the output of is f(w).\nAssuming that the H-head rank of f is r, we construct low-communication (r + 1)-round Alice-first and Bob-first protocols for fs, for any S \u2286 [n]. This gives a method for lower bounding the multihead rank of f: by showing that either Cr+1,A(f) and Cr+1,B is large enough, we conclude that the H-head rank of f is larger than r.\nLemma 1. For every f: 21 \u00d7 ... \u00d7 \u03a3n \u2192 {0,1}, for every S \u2286 [n], and for every H > 1, denoting r = rk(H) (f) and |A| the number of assignments for f, we have:\n$C_{r+1,A}^\\mathcal{A}(f_S) \\leq 2Hr \\cdot \\lceil log_2 |A| \\rceil \\text{ and } C_{r+1,B}^\\mathcal{B}(f_S) \\leq 2Hr \\cdot \\lceil log_2 |A| \\rceil$."}, {"title": "Multihead decoder depth of the k-th one", "content": "In this section, we establish a tight lower bound on the multi-head rank of k-thOne.\nTheorem 3. For any k, H \u2208 N, for all but finitely many n \u2208 N, we have rk(H) (k-thOnen) = k.\nWe observe that our communication complexity tool is not applicable in this case, as for any partition of the input positions between Alice and Bob, there exists a 2-round protocol with logarithmic communication that computes the position of the k-th one: Alice sends the positions of the first k ones in her part of the input, and Bob does the same.\nProposition 6. For any k,n and S\u2286 [n]:\n$C_{2,\\mathcal{A}}^\\mathcal{A}(k\\text{-thOne}) = C_{2,\\mathcal{B}}^\\mathcal{B}(k\\text{-thOne}) = O(k\\log n)$."}]}