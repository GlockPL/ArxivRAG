{"title": "UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater Images", "authors": ["Tatiana Ta\u00eds Schein", "Gustavo Pereira de Almeira", "Stephanie Loi Bri\u00e3o", "Rodrigo Andrade de Bem", "Felipe Gomes de Oliveira", "Paulo L. J. Drews-Jr"], "abstract": "Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques. A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment. Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement. This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model. Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images. The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images. The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks. Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process. The source code is available here.", "sections": [{"title": "I. INTRODUCTION", "content": "The underwater environment is influenced by several factors that negatively impact visibility and consequently affect the quality of underwater images and videos. The presence of suspended sediments in the water, such as sand, plankton, algae, and other organic matter blocks the passage of light, making the environment darker and hazier. They also reflect incoming light, creating an uneven haze that distorts the colors of submerged objects. Another critical factor is the density of water, which is approximately 800 times greater than that of air. This density significantly attenuates visible light as depth increases, making visual clarity even more difficult. Thus, these combined factors make the visual exploration of the underwater environment a considerable challenge [1].\nUnderwater low light conditions distort colors, resulting in bright spots in the center of images and darker edges. Artificial lighting, which is susceptible to this distortion, involves heavy and costly equipment that requires constant power, either from batteries or surface connections. Enhancing underwater images is a deeply explored field, with various methods investigated to achieve this goal. However, there is still limited research on low-light underwater images. Especially due to limited underwater datasets for research, making studies even more challenging when paired images are not available [2].\nTo tackle this problem, different algorithms and methods have been proposed to mitigate the impacts of low light scattering and absorption. In this scenario diffusion models arise as a potential solution to attenuate the negative effects caused by limited lighting conditions. Diffusion models are deep generative models composed of two main stages: i) the direct diffusion, including gradual Gaussian noise into the raw image; and ii) the reverse diffusion, which learns to remove the noise and to generate a clear output image [3].\nIn this paper, we propose a novel unsupervised learning method called UDBE, based on the Denoising Diffusion Probabilistic Model (DDPM) [3], for enhancing underwater images. Our approach enhances the visibility and quality of subaquatic images, supporting visual exploration of the underwater environment, even without using reference images in the learning process (paired images). To achieve this, we use the U-Net network [4], trained with conditional and unconditional data. The conditional data are provided by a brightness module incorporated in every U-Net layer. Thereby, by combining low-lighting images, SNR maps, and raw images, the illumination conditions are compensated, providing visible and good-quality underwater images. The network was validated using PSNR, SSIM, UIQM, and UISM metrics.\nThe two main contributions are summarized as follows:\n\u2022\nWe propose an unsupervised learning method for lighting and turbidity enhancement in underwater images, over-"}, {"title": "II. RELATED WORK", "content": "A wide range of applications for underwater image enhance-ment can be found, including navigation, offshore engineering, and monitoring [5]. Related techniques can be divided into three categories: physics-based, traditional, and deep learning-based methods. The following subsections introduce the most related works found in the literature.\nA. Physics-based Underwater Image Enhancement\nRecently, researchers have proposed different estimation methods for recovering degraded underwater images. The essence of physical model-based methods is to establish an underwater image formation model with prior knowledge, such as Dark Channel Prior (DCP) [6] that removes haze. However, the concentration and dispersion of light in the underwater environment block some adaptations of the method. Peng et al. [7] developed an improved DCP method that uses the degree of image blur and the difference in light absorption to estimate the transmission map of underwater scenes. Peng et al. [8] created a Generalized Dark Channel Prior (GDCP) for image restoration, which incorporates adaptive kernel correction into an image formation model.\nAlthough physics-based models can be effective in certain underwater environments, their limitations are evident. Inac-curacy in estimating the degradation process, especially in dynamic underwater scenarios, and lack of consideration for the human visual system are significant challenges.\nB. Traditional Underwater Image Enhancement\nDifferent traditional approaches have tackled the underwater image enhancement problem through intensity transformation techniques. Huang et al. [9] proposed a Relative Global Histogram Stretching (RGHS) algorithm, mainly based on histogram equalization and stretching. Hassan et al. [10] have employed the Retinex theory, a classic method for image restoration. These authors enhanced the underwater images using CLAHE, and then the Retinex theory is applied to restore the distorted colors. Zhuang et al. [11] developed a Bayesian Retinex algorithm to enhance single underwater images with prior multi-order gradients of reflectance and illumination. Chen et al. [12] present a hybrid underwater enhancement method that solves an inverse problem with the new Retinex transmission map estimation and adaptive color correction. Martinho et al. [13] proposed an approach combining intensity transforming techniques. Color, gamma, contrast, and brightness intensity corrections were combined to compensate for the underwater image degradation.\nTraditional underwater image enhancement methods apply individual transformation techniques to improve the quality of underwater images, or even combine different transformation techniques. However, the aforementioned approaches are lim-ited to fixed settings and parameters, making them ineffective for different and dynamic underwater environments.\nC. Deep Learning-based Underwater Image Enhancement\nDeep learning-based techniques have approached the en-hancement of underwater images through a wide range of strategies. Wang et al. [14] presents the UIEC^2-Net network using two color spaces for performing subaquatic image enhancement. This method introduces a CNN architecture to learn visual features from distinct color spaces, intending to be more sensitive to luminance and saturation issues. Martinho et al. [5] introduces an adaptive learning model that estimates optimal (or near-optimal) parameters for intensity transforma-tion techniques. The method applies a CNN architecture for the parameter estimation.\nAnother learning-based field considers the employment of attention mechanisms. Li et al. [15] proposed Ucolor, which gathers features from three color spaces to enrich represen-tations. Using an attention module, Ucolor integrates and highlights distinct features across a network of encoders. Addi-tionally, it employs a network of transmission guide decoders, using the reversed-medium transmission (RMT) map as input, to improve response in degraded areas. These methods also consider the depth map. Wang et al. [16] used class condition attention, in which an underwater image is classified first and then the class label guides the generation of the enhanced images. Fu et al. [17] uses residual dual attention to extract and enhance features with non-local and channel attention. Due to variable lighting and the sea depth, these approaches fall short in image restoration.\nSaleh et al. [18] performed unsupervised underwater image enhancement. The model, called UDnet, combines a U-Net as a feature extractor with PAdaIN to encode uncertainty, eliminating the need for manual annotations. Mello et al. [19] proposed a self-supervised underwater image enhancement method that estimates image degradation and uses an autoen-coder for reconstruction. The network learns to compensate for degradation by incorporating it into the loss function. An attention module mitigates high-intensity areas caused by color imbalances.\nRecently, diffusion models have emerged for enhancing underwater images. Chen et al. [20] proposed WF-Diff, which uses frequency domain information features and diffusion models. This framework includes WFI2-net, which enhances the frequency information in the wavelet space, and FRDAM, which refines the high and low-frequency information of the enhanced images. Dazhao et al. [21] presented UIEDP for underwater image enhancement, a network that samples the posterior distribution of sharp images conditioned on degraded"}, {"title": "III. METHODOLOGY", "content": "We introduce the UDBE, an unsupervised diffusion-based approach for improving underwater images. Our approach consists of two primary steps: i) Image Pre-Processing, which extracts and represents crucial information for the learning process in the Diffusion model; and ii) Diffusion-based Learn-ing Process, which through the diffusion model learns to compensate for brightness variation in underwater images.\nA. Problem Statement\n[Brightness Enhancement in Underwater Images] Let I = {11, 12, ..., in} be a set of raw underwater images. Let H be a set of high-brightness underwater images and C be a set of low-brightness underwater images, computed from I. Also, let C be a series of color maps and N be a series of noise representation maps, computed from L. Let D be an unsupervised Diffusion Model, trained from H, L and F, the fusion between C and N. Our main goal is to predict brightness-enhanced underwater images B, using the unsupervised Diffusion Model D, from unseen raw underwater images U.\nB. Image Pre-Processing\nIn this step, a series of raw underwater images (I) in Red, Green, and Blue (RGB) color space is processed to extract and represent important features for the unsupervised learning process. To achieve this, different operations are carried out, such as Brightness Adjustment, Color Map Generation, Noise Representation Map Generation, and Combining Color and Noise Representations, whose details will be presented as follows.\n1) Brightness Adjustment: To teach the diffusion model (D) to compensate for brightness degradation in images acquired in underwater environments, high and low-brightness underwater images are generated. Thereby, the instances of different brightness conditions provide important features regarding the brightness variation improving the learning stage.\nFor this, the high-brightness underwater images (H) are obtained through the computation of random numbers rep-resenting bright values to be added to the raw images. The random numbers for the high-brightness images are comprised of the range [50, 100]. For the low-brightness underwater images (C), random numbers representing bright values are also computed to be subtracted from the raw images. The random numbers for the low-brightness images are comprised of the range [-50, -100].\n2) Color Map Generation: In addition to brightness, another important feature related to the quality of underwater images is color. In this step, the color features are computed from Limages and depicted as color maps (C). The color maps reduce color distortion by normalizing the range of the three channels in the input images [24]. Equation (1) is given as follows:\n$Color(l) = \\frac{1}{I_{max}} \\begin{bmatrix} I_r\\\\ I_g\\\\ I_b \\end{bmatrix} = \\begin{bmatrix} \\frac{l_r}{l^{max}}\\\\ \\frac{l_g}{l^{max}} \\\\ \\frac{l_b}{l^{max}} \\end{bmatrix}$ (1)\nwhere the low-brightness underwater image (l) is decomposed into three channels, red ($l_r$), green ($l_g$), and blue ($l_b$). Mean-while, $I_{max}$ represents the maximum value in the l image.\n3) Noise Representation Generation: A relevant aspect, also related to the quality of underwater images, is noise. In this stage the noise feature is represented as Signal-to-Noise Ratio (SNR) maps (N), computed from L images. The SNR maps are used to visually highlight the areas where the signal-to-noise ratio is lowest. It is calculated according to the equation (2) as follows,\n$Noise(l) = \\frac{G(1)}{|l \u2013 G(l) + \\epsilon|}$ (2)\nWe use the parameter \u03f5 to guarantee numerical stability and the filter G as a Gaussian blur. We define the high-frequency component of the image as noise and directly calculate the relationship between the original image and this noise.\n4) Combining Color and Noise Representations: Finally, after the processing of brightness, color, and noise features in the previous steps, the color maps (C) and SNR maps (N) are combined through a concatenation operation, as defined in the equation below:\nF = Concatenate[C,N]. (3)\nOur motivation for the color and noise maps concatenation is due to the loss of features during the diffusion model learn-ing, thereby, the concatenation provides meaningful features to be highlighted in the learning process. In this sense, the inputs for the diffusion model are the high-brightness underwater images (H), the low-brightness underwater images (L), and the concatenation of color maps and SNR maps (F)."}, {"title": "C. Diffusion-based Learning Process", "content": "In this step, a CLE Diffusion-based model is used to enhance the brightness in degraded underwater images. It is important to highlight that the CLE Diffusion model [24] is not employed in the underwater domain, thereby the proposed diffusion-based learning model addresses the enhancement of lighting in subaquatic images. Additionally, we designed domain-specific loss functions tailored to our needs. It is worth mentioning that the proposed unsupervised learning process is based on the lack of reference images in the dataset, for the learning stage. For this, the low-brightness underwater images created in the Image Pre-Processing step are used as input data and the high-brightness underwater images are used as reference data.\n1) Diffusion Model: A Diffusion model is a probabilistic generative model that uses an iterative approach to create data. This model consists of a forward diffusion process that introduces noise to clean images and a reverse diffusion process that restores the clean images.\nIn this stage, the diffusion process is a Markov chain, which adds noise to the input data $x_0$ at each time interval t. Equation (4) describes the forward diffusion process using the DDPM model [3].\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 \u2013 B_t}x_{t-1}, B_tI),$ (4)\nwhere N represents the Gaussian distribution, which is defined by the mean $\\sqrt{1 - B_t}x_{t-1}$ and the variance $B_tI$. The sequence $B_1,..., B_T$ is a fixed noise variance schedule, which must converge to pure random Gaussian noise N(0, 1). Defining $a_t = 1 \u2212 B_t$ and $\\bar{a}_t = \\prod_{1=1}a_j$, we can sample $x_t$ at any time step, as described in equation (5):\n$x_t = \\sqrt{a_t}x_0 + \\sqrt{1 \u2013 \\bar{a}_t}\\epsilon,$ (5)\nwhere \u03f5 represents Gaussian noise. The reverse diffusion pro-cess using DDPM model can be defined by the equation (6):\n$P_0(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),$ (6)\nwhere $P_0(x_t,t)$ denotes the mean and $\\Sigma_\\theta(x_t, t)$ the variance of the distribution. It is worth mentioning that in the DDPM model, only the mean is learned and the variance is fixed [3]. Equation (7) defines the mean $\\mu_\\theta(x_t,t)$ as:\n$\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{a_t}} \\bigg( x_t - \\frac{B_t\\epsilon_\\theta(x,t)}{\\sqrt{1-\\bar{a}_t}} \\bigg) ,$ (7)\nwhere \u03f5 is obtained with a U-Net model [4] to estimate the noise component of an image. In addition to the diffusion model, a brightness control module assumes the input of a brightness level (\u03bb), that improves the quality of features extracted by the U-Net model. A FiLM layer [25], is added to learn an affine transformation incorporating the light fea-tures to be applied during the unsupervised diffusion learning process employed in this work.\nDuring inference, the noise is gradually reduced until a clear image is obtained. Additionally, to speed up the sampling process, the DDIM (Denoising Diffusion Implicit Models) sampler is used, following [26] and defined as in equation (8):\n$x_{t-1} = \\sqrt{a_{t-1}} \\bigg( \\frac{x_t}{\\sqrt{a_t}} - (\\sqrt{1 \u2013 \\bar{a}_t})\\epsilon_\\theta(x_t,t) \\bigg) + (1 - \\bar{a}_{t-1})\\epsilon_\\theta(x_t,t).$ (8)\nModel training takes place through the optimization of the negative log-likelihood loss function [3], given by equation (9) in a simplified form:\n$L_{simple}(\\theta) = E_{x_0,t,\\epsilon} [||\\epsilon \u2013 \\epsilon_\\theta(\\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 \u2013 \\bar{a}_t}\\epsilon, t)||^2].$ (9)"}, {"title": "2) Loss Functions:", "content": "A combination of different loss functions to address the brightness enhancement in underwater im-ages is proposed for the unsupervised diffusion-based learning stage, as shown in equation (10).\n$L = \\gamma_1L_{LPIPS}+\\gamma_2L_{SSIM}+\\gamma_3L_{MSE}+\\gamma_4L_{brightness} + \\gamma_5L_{color}$ (10)\nwhere for i = 1,2,3,4,5 the weights $\\gamma_i$ are values $\\gamma_1 = 30, \\gamma_2 = 2.83, \\gamma_3 = 1, \\gamma_4 = 20$ and $\\gamma_5 = 100$. The aforementioned weights were empirically defined in order to improve the brightness enhancement of underwater images, regarding the contribution of every component for the combined loss function.\nThe applied loss functions tackle distinct features, improving the overall learning. The employed loss functions are presented below:\nPerceptual Loss: The LPIPS [27] perceptual loss function was used, which extracts high-level features using a pre-trained AlexNet net-work, trained on the ImageNet dataset. Additionally, it is calculated the distance between the enhanced features and the original features. This can be computed using the equation below:\n$L_{LPIPS} = \\sum_{i=1}^k \\frac{1}{H_kW_k} || Alex^k(B) \u2013 Alex^k(I)||^2$ (11)\nwhere $Alex^k$ represents the feature maps extracted from the k - th layer of the AlexNet model, and $H_k, W_k$ are the height and width of the feature map in layer k, respectively. The LPIPS loss function is used due to its ability to enhance the model for restoring high-frequency information [27].\nSSIM Loss: It calculates the structural similarity between the real image and the resulting image, providing statistics on overall contrast and luminance consistency [28]. It can be computed using the equation (12):\n$L_{SSIM} = \\frac{(2\\mu_I \\mu_B + C_1) (2\\sigma_{IB} + c_2)}{(\\mu_I^2 + \\mu_B^2 + C_1)(\\sigma^2_I + \\sigma^2_B+C_2)},$ (12)\nwhere $\\mu_I$ and $\\mu_B$ are the mean pixel values, $\\sigma^2_I$ and $\\sigma^2_B$ are the variances, $\\sigma_{IB}$ is the covariance, and $C_1$ and $C_2$ are constants for numerical stability.\nAngular Color Loss: It ensures that the colors of the enhanced images B match the ground truth I [29]. The color loss can be expressed as the equation below:\n$L_{color} = \\sum_d (B_d, I_d),$ (13)\nwhere d indicates the pixel position and < (,) denotes the calcu-lation of the angular difference between two vectors in the three-dimensional RGB space.\nBrightness Loss: Its functionality is based on the $L_1$ loss function, but in this approach, it operates on grayscale images to ensure brightness consistency between the input image and the resulting image. Supervision is performed using the average pixel intensities, as presented in equation (14) [24].\n$L_{brightness} = Ig|(B) \u2013 Ig (I)|_1,$ (14)\nwhere $I_g(.)$ represents the version in gray-scale of a RGB image.\nMSE Loss: The Mean Squared Error (MSE) function measures how well a model performs by calculating the square of the distance (i.e., the error) between the predicted and actual values. In other words, the closer the predicted value is to the actual value, the lower the mean squared error between the two. Its equation is as follows:\n$L_{MSE} = \\frac{1}{N}\\sum_{d=1}^N (B_i \u2013 I_i)^2.$ (15)\nwhere N is the number of samples, I represents the actual value of the dth sample, B represents the predicted value of the d - th sample."}, {"title": "IV. EXPERIMENTS", "content": "This section presents a qualitative and a quantitative evaluation of the proposed approach, benchmarking it against traditional and state-of-the-art techniques in deep learning.\nA. Experimental Datasets\nIn the experiments, the challenging UIEB, SUIM and RUIE underwater image datasets are used. The mentioned datasets are composed of underwater images from the sea and are widely used in the literature [30] [31] [32]. The UIEB underwater image dataset is composed of 890 images of natural environments, the SUIM underwater image dataset is composed of 1635 images of natural environments, while the RUIE underwater image dataset is composed of 4230 images of natural environments.\nB. Comparison and Metrics\nOur brightness enhancement method is compared with well-established techniques, including RUIDL [5], UESAM [19] and UDNet [18]. To evaluate and compare the results we use the metrics PSNR [33], SSIM [28], UIQM [34], and UISM [34]. We aim to com-prehensively assess the effectiveness of our method in comparison to these literature techniques.\nC. Implementation Details\nOur approach uses OpenCV and PyTorch frameworks on a Work-station, with an Intel\u00ae Core\u2122 i7-10700K CPU @ 3.80GHz, 64 GB DDR4-3000 main memory and an NVIDIA\u00ae Titan\u00ae RTX 24 GB GDDR6. For the training stage, we defined batch size equal to 8, 1000 epochs and learning rate equal to 5 \u00d7 10-5. The AdamW optimization algorithm is used for training UDBE, with weight decay equal to 1 \u00d7 10-4, regarding dataset split of 90% for train and 10% for test.\nFor the learning process, the LPIPS, SSIM, and MSE loss functions were first used in training, addressing more general and structural features in underwater images. After the first 20 training epochs, the color and brightness loss functions were included in the training to refine the previous knowledge by considering more specific and relevant features for brightness enhancement of underwater images.\nD. Qualitative Evaluation\nIn the qualitative analysis, we compare the proposed underwater image enhancement technique with the literature methods. We assess the visual quality and perceptual enhancements across scenes from all datasets. Through the comparison of our technique's results with those from other methods, we intend to highlight differences in image brightness, visibility, and overall visual quality. This qualitative evaluation offers valuable insights into the strengths and limitations of our approach. Additionally, this evaluation method serves as a fundamental step in validating the quality of our proposed technique for brightness enhancement in underwater images."}, {"title": "V. CONCLUSION", "content": "In this paper we presented an approach for lighting enhancement in underwater images. For this, we proposed an unsupervised diffusion-based learning approach for mitigating the lighting degradation in subaquatic images. From the obtained results we verify that the proposed method is accurate and presents clearer underwater images. Additionally, the experiments demonstrate the robustness of our approach, regarding different and challenging datasets and present quality results in distinct underwater scenarios, brightness levels and water colors. Finally, qualitative and quantitative assessment experiments demonstrated the effectiveness of our approach, which achieves reasonable performance in terms of brightness quality, even regarding different image quality metrics. As future work, we intend to investigate other strategies to employ state-of-the-art diffusion models for the enhancement of underwater images, aiming to improve their quality and visibility. Additionally, we plan to evaluate the computational complexity of UDBE. We also intend to explore the relationship and impacts of loss functions in the underwater domain for image enhancement, providing solutions for subaquatic applications."}]}