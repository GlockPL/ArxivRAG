{"title": "MONTESSORI-INSTRUCT: GENERATE INFLUENTIAL TRAINING DATA TAILORED FOR STUDENT LEARNING", "authors": ["Xiaochuan Li", "Zichun Yu", "Chenyan Xiong"], "abstract": "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose MONTESSORI-INSTRUCT, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35% and 46.24% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-40. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.", "sections": [{"title": "1 INTRODUCTION", "content": "Synthetic training data is highly effective in various applications of large language models (LLMs) (Lu et al., 2023), spanning from general pretraining (Allal et al., 2024; Zhou et al., 2024), instruction-tuning (Tong et al., 2024) to domain-specific scenarios such as mathematics (Yu et al., 2023) and coding (Jiang et al., 2024). The advantages of synthetic data include its low cost, convenience, and flexibility, making them an appealing choice for scaling up training data (Yue et al., 2024), mitigating the shortage of human labels (Chang et al., 2024), and improving data diversity (Sun et al., 2023).\nTypical data synthesis methods (Wang et al., 2023) employ an instruction-tuned teacher model and prompt it with seed data to generate synthetic training data for a student model. It is widely observed that the teacher-generated data can be noisy and non-informative (Bauer et al., 2024), their simple and uniform format may lead to pattern overfitting (Chen et al., 2024), and their biased and ungrounded content can introduce ambiguity in AI alignment (Liu et al., 2024). These are fundamental challenges of synthetic data as they can mislead students and sometimes even result in model collapse (Shumailov et al., 2023; Seddik et al., 2024).\nIn this paper, we propose MONTESSORI-INSTRUCT, a novel data synthesis framework designed to generate more tailored and informative data by directly optimizing the synthesis ability of the teacher toward the student's learning preferences. We first leverage influence functions (Koh & Liang, 2017; Yu et al., 2024b) to precisely measure the utility of synthetic data-its ability to effectively train the students. Then, we optimize the parameters of the teacher model according to the student's preferences through Direct Preference Optimization (DPO) (Rafailov et al., 2024). The preference-optimized teacher then synthesizes influential training data for the students. As shown in Figure 1,"}, {"title": "2 RELATED WORK", "content": "Synthetic data has been shown highly effective in various applications of large language models (Lu et al., 2023), including pretraining (Allal et al., 2024; Zhou et al., 2024), instruction-tuning (Tong et al., 2024; Yue et al., 2024), mathematics (Yu et al., 2023) and coding (Jiang et al., 2024). Typical approaches like Self-Instruct (Wang et al., 2023) leverages an instruction-tuned teacher to generate instruction-response pairs given a small amount of seed data. Following the similar pipeline, Self-Guide (Zhao et al., 2024) and Self-Alignment (Sun et al., 2023; Guo et al., 2024) further enhance data quality for specific tasks, such as safety, truthfulness, and instruction-following, by carefully curating task-relevant seeds. In parallel, Instruction Backtranslation (Li et al., 2023) and Bonito (Nayak et al., 2024) collect massive texts from the internet as responses, prompt LLMs to synthesize instructions reversely, and select high-quality candidates.\nDespite its promising potential, synthetic data primarily rely on the teacher's free-form generations, thus is inevitably often biased, non-informative, and misleading (Bauer et al., 2024; Liu et al., 2024)."}, {"title": "3 MONTESSORI-INSTRUCT", "content": "This section first introduces the overall framework of MONTESSORI-INSTRUCT (\u00a7 3.1) and then elaborates its two main components: local data influence collection (\u00a7 3.2) and student-preference-guided teacher optimization (\u00a7 3.3)."}, {"title": "3.1 OVERALL FRAMEWORK", "content": "Standard data synthesis methods (Wang et al., 2023; Yuan et al., 2024; Lee et al., 2024) begin with a teacher model M and a seed prompt p formed using a few-shot sample of example data. The teacher model processes the seed p to generate a set of N new instructions, {xi | 1 < i < N}, that follow a similar format to the seed but with a variety of contents. Each generated instruction xi is then used to prompt the teacher to synthesize the corresponding response yi. This yields a set of instruction-response pairs {(xi, Yi) | 1 \u2264 i \u2264 N} that are then used to train the student model m.\nMontessori-Instruct upgrades this standard data synthesis pipeline with the optimization of the teacher model toward the student's learning preferences. The student-preference-guided teacher optimization starts with prompting the teacher to generate a probing dataset Dprobing using Self-Instruct and then collecting these data points' local data influence Im on the student model (\u00a7 3.2). The collected data preferences form the preference dataset Dpreference, and Montessori-Instruct uses it to update the teacher model via Direct Preference Optimization (DPO) (Rafailov et al., 2024) (\u00a7 3.3). The optimized teacher then generates the actual training dataset to train the student model m. The process can be iterated multiple rounds to continually refine the teacher according to the student's updated preferences. This process is illustrated in Figure 2 and discussed in detail in the next two sections."}, {"title": "3.2 LOCAL DATA INFLUENCE COLLECTION", "content": "A key component of our framework is to precisely measure the utility of synthetic data, i.e., how good they are at improving the student's learning outcomes. This question is often approached using influence functions (Weisberg & Cook, 1982; Koh & Liang, 2017), which was designed to quantify changes in reference loss when a data point (xi, Yi) is upweighted in the training sets Park et al. (2023), thus reflecting the utility of this data point to the student's learning.\nIn order to efficiently calculate the data influence, we follow Yu et al. (2024b) and approximate influence functions locally, using the change of the model's reference loss before and after training on a single data point (xi, Yi):\n$I_m(x_i; D_{ref}) \\approx -\\mathcal{L}(D_{ref} | A(y_i | x_i; m)) + \\mathcal{L}(D_{ref} | m),$\nwhere $\\mathcal{L}(D_{ref} | m) = E_{(x,y)~D_{ref}} l(y | x; m)$,\nwhere $D_{ref}$ denotes the reference data that measure the student's capability, and $l(y|x; m)$ is the loss of student m on an input-output pair (x, y). A(yi | xi; m) refers to the optimization operation of student m on data (xi, Yi), e.g., one-step training with Adam (Kingma & Ba, 2015) on (xi, Yi).\nThe local data influence, Im (xi; Dref), represents how the instruction-response pair (xi, Yi) impacts the student's learning outcome as measured on the reference data. A positive Im indicates that the data benefits the student's reference performance, while a negative Im shows the opposite. A complete theoretical derivation of local data influence is provided in Appendix B."}, {"title": "3.3 STUDENT-PREFERENCE-GUIDED TEACHER OPTIMIZATION", "content": "After calculating local data influence for each instruction in the probing dataset Dprobing, we pair every two instructions with positive and negative influence, along with their corresponding seed prompt p, to construct the preference dataset:\n$D_{preference} = {(p, x+,x\u00ae) | I_m(x\u00ae;D_{ref}) <0 < I_m(x+;D_{ref})}.$\nWe then apply DPO to optimize the teacher model M toward the student's learning preferences:\n$\\mathcal{L}_{DPO}(\\mathcal{M}^*; \\mathcal{M}) = -E_{(p,x+,x\u00af)~D_{preference}} [logo (Blog:\\frac{\\sigma(\\mathcal{M}^*(x+ |p))}{\\mathcal{M}(x+|p)})-\\beta log \\frac{\\sigma(\\mathcal{M}^*(x- |p))}{\\mathcal{M}(x-|p)})]$,\nwhere \u03b2 is a parameter that controls the deviation from the initial teacher M and \u03c3 is the logistic function. The updated teacher, M*, after one or multiple iterations, is then used to synthesize the training data for the student model m."}, {"title": "4 EXPERIMENTAL METHODOLOGIES", "content": "This section details our main experimental setups, including a thorough configuration of the data synthesis process, the chosen baselines, and the evaluation methods."}, {"title": "5 EVALUATION RESULTS", "content": "This section evaluates the effectiveness of Montessori-Instruct (\u00a7 5.1), illustrates the correlation between the teacher's learning and the student's performance (\u00a7 5.2), conducts comprehensive"}]}