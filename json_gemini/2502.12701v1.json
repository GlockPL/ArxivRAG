{"title": "Translate Smart, not Hard:\nCascaded Translation Systems with Quality-Aware Deferral", "authors": ["Ant\u00f3nio Farinhas", "Nuno M. Guerreiro", "Sweta Agrawal", "Ricardo Rei", "Andr\u00e9 F.T. Martins"], "abstract": "Larger models often outperform smaller ones\nbut come with high computational costs. Cas-\ncading offers a potential solution. By default,\nit uses smaller models and defers only some in-\nstances to larger, more powerful models. How-\never, designing effective deferral rules remains\na challenge. In this paper, we propose a simple\nyet effective approach for machine translation,\nusing existing quality estimation (QE) metrics\nas deferral rules. We show that QE-based de-\nferral allows a cascaded system to match the\nperformance of a larger model while invoking\nit for a small fraction (30% to 50%) of the ex-\namples, significantly reducing computational\ncosts. We validate this approach through both\nautomatic and human evaluation.", "sections": [{"title": "1 Introduction", "content": "Larger models consistently outperform smaller\nones in NLP tasks, but the trade-off is the increased\ncomputational cost. This raises the question:\nHow can we maintain high performance\nwhile reducing computational load?\nA promising solution is model cascading, where\nsmaller models handle examples by default, and\nonly a subset of hard instances is deferred to a\nlarger model. However, this approach requires a ro-\nbust deferral system that reliably determines when\nto defer. Common approaches often involve de-\nsigning and training specialized deferral models,\nwhich determine when a large model is needed-\ne.g., based on reliability or uncertainty estimates\n(Chen et al., 2023; Gupta et al., 2024). But do we\nreally need to train new models for every task, or\ncan existing resources speed up this process?\nFor machine translation (MT), extensive re-\nsearch on reference-free automatic evaluation of-\nfers an appealing alternative (Zerva et al., 2022,\n2024; Blain et al., 2023). In this paper, we leverage\nrecent quality estimation (QE) metrics to create\nstraightforward and relatively lightweight deferral\nrules. This approach draws inspiration from pro-\nfessional translation workflows, where QE metrics\nhelp identify translations that should be deferred\nto expert post-editing (Castilho and O'Brien, 2017;\nB\u00e9chara et al., 2021). Our main contributions are:\n\u2022 We introduce a cascaded translation system\nthat uses pretrained QE metrics to determine\nwhether to defer examples from a smaller\nmodel to a larger one, balancing efficiency\nand quality (\u00a73). See Fig. 1 for an illustration.\n\u2022 We confirm that the benefits of QE-based\nmodel cascading hold across different com-\nbinations of translation and QE models (\u00a74).\n\u2022 We perform human evaluation, further val-\nidating our approach on two language pairs\n(en-es and en-ja) in the WMT24 test set (\u00a75).\n\u2022 We release our code, all generated translations,\nand human quality assessments."}, {"title": "2 Adaptive Inference in NLP", "content": "Adaptive inference techniques are increasingly be-\ning adopted in natural language processing tasks\n(Mamou et al., 2022; Varshney and Baral, 2022;\nChen et al., 2023; Ong et al., 2024). These methods\ntypically use models of different sizes and predic-\ntive power (often two, though most frameworks can\neasily accommodate more), with the primary goal\nof reducing the computational load by using the\nlarger, more computationally expensive model only\nwhen necessary (e.g., for more difficult examples\nor when a model is highly uncertain about its pre-\ndiction). Current strategies include routing, where\na decision rule determines which model to use, en-\nsuring only one model is used to handle each input,\nand cascading, which starts with a smaller model\nand may invoke a larger one afterward based on\nthe small model's output and a deferral rule. In this\npaper, we focus on the second approach.\nThe computational efficiency of model cascad-\ning comes at the cost of designing a robust de-\nferral system that can reliably identify when to\ndefer to the larger model. This is often handled\nusing simple decision rules, such as nonparametric\nmethods or other approaches based on uncertainty\nmeasures (Ram\u00edrez et al., 2024; Gupta et al., 2024).\nA recent alternative involves training external mod-\nels specifically to predict when deferral is needed \u2013\nfor a given example, these models can be trained,\ne.g., to assess if a given candidate is correct (Chen\net al., 2023). Here, we propose a simple and ef-\nfective deferral rule for MT that is conceptually\nsimilar to this approach while offering a particu-\nlarly straightforward solution for this task."}, {"title": "3 Quality-Aware Deferral for MT", "content": "Although human evaluations and reference-based\nmetrics remain the standard for evaluating machine\ntranslations, reference-free/quality estimation (QE)\nmetrics have shown strong correlations with human\njudgments (Zerva et al., 2024), holding promise in\ndistinguishing between the quality of translations\nfor the same source (Agrawal et al., 2024). Since\nQE models are typically much smaller than current\ntranslation models (Kocmi et al., 2024a), we pro-\npose to leverage them for an efficient deferral rule.\nRather than training new bespoke decision models\n(\u00a72), existing QE models can evaluate translations\nfrom a lightweight model and determine when to\naccept them or defer to a larger one.\nHow to choose which examples to defer? Set-\nting a fixed threshold on QE scores is challenging-\ntoo high a threshold wastes computational re-\nsources, while too low a threshold risks compro-\nmising quality. Throughout this paper, we use a\nbudget-constrained computation approach: we\nfirst translate all examples in a batch with the\nsmaller model, then rank them based on QE scores,\ndeferring only the lowest-scoring subset accord-\ning to a predefined compute budget (the fraction\nof examples deferred to the larger model). This\nassumes parallel processing of entire batches rather\nthan processing individual instances sequentially.\nWe leave alternatives such as dynamic thresholding\n(Ram\u00edrez et al., 2024) for future work. See Fig. 1\nfor an illustration with 50% of deferral.\nComputational efficiency. The standard approx-\nimation for the number of floating point operations\n(FLOPs) required for inference with a transformer\nmodel is $2ND$, where N represents the number\nof model parameters and D is the number of to-\nkens generated at inference time (Sardana et al.,\n2024; Snell et al., 2024). For a cascaded approach\nwith superscripts S and L denoting the smaller and\nlarger models, respectively, this becomes:\n$2BDs(Ns + NQE) + 2\u03b7BDLNL$,\nwhere B is the batch size and \u03b7 is the proportion\nof instances the larger model processes. Assuming\nDs \u2248 DL, this approach achieves computational\nparity with the larger model (i.e., 2BDN\u2081) when:\n$\u03b7^* = 1 - \\frac{NS + NQE}{NL}$\nThis expression provides a simple rule of thumb: to\nmaintain computational efficiency, the larger model\nshould handle at most n* of the examples. For\ninstance, if it is 10\u00d7 larger than the smaller model\nand the QE model is negligible (NQE < Ns),\nthen n* \u2248 0.9. This means the cascading is more\nefficient than always using the larger model as long\nas fewer than 90% of the examples are deferred."}, {"title": "4 Experiments and Analysis", "content": "We consider Tower-v2 models (Rei et al., 2024)\nof different size and predictive power: Tower-v2\n70B, an improved iteration of Tower (Alves et al,"}, {"title": "5 Human Evaluation", "content": "Since using QE metrics during inference can bias\nautomatic evaluations, we conduct a human study\nto validate our approach. We randomly sample\n500 source instances and ask human annotators to\nrate translations from Tower-v2 7B and Tower-v2\n70B on a continuous scale from 1 (no overlap in\nmeaning) to 100 (perfect translation). This is done\nfor en-es and en-ja. Further details are in App. \u0421.\nFig. 4 shows the performance of cascaded sys-"}, {"title": "6 Conclusions and Future Work", "content": "We propose a simple yet effective approach to\nmodel cascading for MT using QE metrics for de-\nferral. Our method matches the quality of larger\nmodels while requiring them to handle only a sub-\nset of examples, significantly reducing computa-\ntional costs. This is shown through automatic and\nhuman evaluations. The effectiveness of our frame-\nwork depends on the quality of existing QE models,\nand improving them can further strengthen our ap-"}, {"title": "7 Limitations", "content": "We highlight three main limitations of our work.\nFirst, we focus on a two-stage cascade, where ex-\namples are handled by a small model or deferred to\na larger one. Extending this to a multistage setup\nwith more than two models could further improve\nefficiency but also add complexity. Second, our\nstudy is limited to machine translation. QE-based\ndeferral works particularly well in MT due to the\navailability of high-quality human-labeled data for\ntraining QE models. Extending this approach to\nother tasks where such data is scarce is not straight-\nforward. Finally, our method assumes the smaller\nmodel is reasonably competitive with the larger\none, which is a fair assumption for MT, as shown\nin our experiments. If the gap in win rates is too\nlarge, cascading offers little benefit, as most exam-\nples would require deferral."}, {"title": "A Experimental Details", "content": "Through the paper, we experiment with the follow-\ning generation models:\n\u2022 Tower-v2 70B (Rei et al., 2024): An im-\nproved iteration of Tower (Alves et al., 2024),\nobtained by continued pertaining Llama-3\n(AI@Meta, 2024) on a multilingual dataset\nwith billions of tokens, followed by super-\nvised finentuning for translation-related tasks.\nIt has 70B parameters. Compared to the first\niteration of Tower, this model is better at para-\ngraph and document-level translation and sup-\nports more language (15, instead of 10), in-\ncluding all the languages in the WMT24 test\nsets. Combined with quality-aware decoding\n(Fernandes et al., 2022), this is the winning\nsubmission of the WMT24 general translation\nshared task (Kocmi et al., 2024a).\n\u2022 Tower-v2 7B (Rei et al., 2024): A smaller\nversion of Tower-v2 70B based on Mistral\n(Jiang et al., 2023).\n\u2022 Tower-v2 7B (Llama-3): We follow the\nrecipe described above to train a smaller ver-\nsion of Tower-v2 70B based on LLama-3.\nThis model slightly underperforms its Mistral\ncounterpart.\n\u2022 EuroLLM Instruct (9B and 1.7B) (Martins\net al., 2024): EuroLLM models are open-\nweight multilingual models trained on 4 tril-\nlion tokens covering all European Union and\nmany other relevant languages across several\ndata sources: web data, parallel data (en-xx\nand xx-en), and high-quality datasets. The\ninstruction-tuned models are obtained after\nfinetuning the base models on the EuroBlocks\ndataset, which includes general instruction-\nfollowing and machine translation tasks.\nWe generate all translations with greedy decod-\ning using vLLM (Kwon et al., 2023) for faster infer-\nence. Table 2 shows the performance of these mod-\nels on the WMT24 test sets (Kocmi et al., 2024a), according to METRICX and COMET (results are\naveraged across all language pairs), along with\ntheir win rates against Tower-v2 70B. Our use\nof datasets and models aligns with their intended\npurposes as defined by the licenses."}, {"title": "B Quality-Aware Decoding", "content": "There is a large body of work on reranking for\nlanguage generation, where we start by generat-\ning multiple hypotheses with a language model,\nand then use a reranker to select the best one\n(Farinhas et al., 2024). For machine translation,\nan example is quality-aware decoding (Fernan-\ndes et al., 2022; Freitag et al., 2022). The sim-\nplest/cheapest approach is QE reranking, where\nwe first generate multiple translation hypotheses\nand then rerank them using a quality estimation\nmodel. This strategy is often used to reduce the\npropensity of language models to hallucinate or\ngenerate critical errors (Guerreiro et al., 2023; Far-\ninhas et al., 2023). While our approach is con-\nceptually different-designed with efficiency in\nmind, whereas QE reranking is often computation-\nally expensive\u2014it is nonetheless valuable to com-\npare its performance against QE reranking based\non hypotheses generated by the small model.\nComputational efficiency. Following the discus-\nsion in \u00a73, the number of FLOPS required for infer-\nence with a large model on a batch of B examples\nis given by:\n$2BDNL$,\nwhere NL represents the number of model parame-\nters and D is the number of generated tokens. In\nthis section, we assume that our goal is to reduce\nthe computational cost by $(1 \u2013 X) \\%$, meaning\nthat we operate under a computational budget of:\n$X2BDNL$.\nThe number of FLOPs required to run inference\nwith our cascaded approach is given by:\n$2BD(Ns + NQE + \u03b7NL)$"}]}