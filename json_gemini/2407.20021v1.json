{"title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity", "authors": ["Kanghyun Choi", "Hye Yoon Lee", "Dain Kwon", "SunJong Park", "Kyuyeun Kim", "Noseong Park", "Jinho Lee"], "abstract": "Data-free quantization (DFQ) is a technique that creates a lightweight network from its full-precision counterpart without the original training data, often through a synthetic dataset. Although several DFQ methods have been proposed for vision transformer (ViT) architectures, they fail to achieve efficacy in low-bit settings. Examining the existing methods, we identify that their synthetic data produce misaligned attention maps, while those of the real samples are highly aligned. From the observation of aligned attention, we find that aligning attention maps of synthetic data helps to improve the overall performance of quantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method designed for ViTs that focuses on inter-head attention similarity. First, we generate synthetic data by aligning head-wise attention responses in relation to spatial query patches. Then, we apply head-wise structural attention distillation to align the attention maps of the quantized network to those of the full-precision teacher. The experimental results show that the proposed method significantly outperforms baselines, setting a new state-of-the-art performance for data-free ViT quantization.", "sections": [{"title": "1 Introduction", "content": "Over the past few years, the Vision Transformer (ViT) [14] has gained increasing interest among researchers, due to its remarkable performance on many computer vision tasks. However, ViT has high computational costs compared to conventional CNN networks, making it difficult to adopt in many resource-constrained devices (e.g., embedded systems). This has led to various works to focus on reducing the computational complexity of ViT architectures [21, 23, 30, 33, 48]. One popular approach is network quantization [2,20,28,36], which converts full-precision floating-point parameters and features to low-bit integers. However,"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Vision Transformers and Multi-Head Attention", "content": "Vision Transformers (ViT) [14,32,43] are adaptations of Transformer architectures from NLP tasks [3, 13] to vision. Each Transformer block comprises two consecutive layers: a multi-head self-attention (MSA) layer and a feed-forward (FF) layer. The MSA layer performs attention using multiple heads for the length $N_a$ input sequence with $d$-dimension, $X\\in\\mathbb{N}^{a \\times d}$, to obtain diverse representations from each head as follows:\n$$MSA(X) = [H_1(X),\\ldots, H_\\mathbb{N}(X)]W^O,$$\nwhere N is the number of attention heads. The outputs of each head are concatenated ([]) and merged by multiplication with projection matrix $W^O$. Each attention head has separated weights $(W_Q^h, W_K^h, W_V^h)$ for computing query, key, and value vectors. For input sequence X, the output of h-th head is as follows:\n$$(Q_h, K_h, V_h) = (XW_Q^h, XW_K^h, XW_V^h)$$\n$$H_h(X) = softmax(\\frac{Q_h K_h^T}{\\sqrt{d}})V_h.$$"}, {"title": "2.2 Data-Free Quantization", "content": "Quantization reduces network complexity by converting floating-point to integer operations, decreasing storage needs, and improving computational efficiency [5, 10, 15, 20, 34-36, 40, 53]. We employ uniform quantization which uses a"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Vision Transformer Quantization", "content": "After the success of ViTs, many efforts have been followed to reduce its expensive computational and memory cost through quantization. One of the pioneering efforts is PTQ-ViT [33], which performed quantization to preserve the functionality of the attention mechanism. Then followed FQ-ViT [30], which proposed to fully quantize ViT including LayerNorm and Softmax. PTQ4ViT [49] applied"}, {"title": "3.2 Data-Free Quantization of Vision Transformers", "content": "After DFQ [35]'s first proposal for data-free quantization problem, many efforts specialized for CNNs have followed, including ZeroQ [4], DSG [50], and intraQ [51]. Notably, GDFQ [46] proposed to jointly train generators to synthesize samples, which laid foundation for variants using better generator [54], boundary samples [8], smooth loss surface [9], and sample adaptability [39]. This stream of methods owe a large portion of its success to the technique that utilizes the statistics stored in the BN layers (Eq. (7)) [4].\nUnfortunately, BN layer is absent in ViT architectures, making existing CNN-targeted techniques suffer from inaccurate sample distribution when adopted to ViTs. To the best of our knowledge, DFQ for ViT is still in an early stage of development, with only two prior works in existent. PSAQ-ViT [26] was the first to present DFQ for ViTs, proposing inter-patch discrepancy of foreground and background patches to generate realistic samples. The following work PSAQ-ViT V2 [24] improved this using an adaptive method by applying the concept of adversarial training. Although they have shown successful results in 8-bit quantization, their method suffers from drastic accuracy drop in low-bit quantization."}, {"title": "4 Motivational Study", "content": "As previously discussed with Fig. 1, all the baseline DFQ methods experience huge accuracy drops in low-bit settings compared to the real-data QAT using"}, {"title": "5 Proposed Method", "content": "Inspired by the observation from Sec. 4, we propose a simple but effective method named MimiQ, a data-free quantization method for the ViTs that extracts knowledge from MSA layers of a pretrained full-precision network. The overall process is depicted in Fig. 4. We devise a synthetic data generation scheme designed for ViT architectures that directly minimizes inter-head attention discrepancy of MSA layers (Sec. 5.1). Furthermore, we propose a fine-tuning scheme for a quantized network in Sec. 5.2, which employs fine-grained head-level attention transfer between the full-precision teacher and the quantized student."}, {"title": "5.1 Sample Synthesis with Inter-Head Attention Similarity", "content": "We propose synthetic sample generation enhanced with inter-head attention similarity by aligning attention maps sharing the same spatial query patch index. The left side of Fig. 4 depicts attention head alignment. First, we collect attention maps from each head in relation to spatial query patch index q:\n$$A_q = [Q_{1,q}K^T Q_{2,q}K^T \\ldots Q_N,qK^T],$$"}, {"title": "5.2 Structural Attention Head Distillation", "content": "On top of our inter-head aligned samples, we propose to use head-level structural distillation from a full-precision teacher to its quantized counterpart. In addition to the output matching loss (i.e., LKL) commonly adopted for fine-tuning the quantized network, we further reduce the distance $g_{dist}$ between each attention output pair by optimizing the following objective:\n$$L_{HAD} = \\frac{1}{LN}\\sum_l^L \\sum_i^N g_{dist} (H_i^T, H_i^S),$$\nwhere $L_{HAD}$ is head-wise attention distillation loss. $H_i^T$ and $H_i^S$ are i-th attention head output from l-th layer of teacher and student, respectively.\nWe compare four candidate metrics for $g_{dist}$: Mean-squared error (MSE), L1 distance, KL-divergence, and structural dissimilarity (DSSIM), which is defined as the negative of absolute SSIM. Unlike $f_{dist}$ where we have concrete required characteristic that it should be invariant of color inversion, $g_{dist}$ is meant to"}, {"title": "6 Performance Evaluation", "content": ""}, {"title": "6.1 Experimental Settings", "content": "We evaluate MimiQ using three popular ViT architectures: ViT [14], DeiT [43], and Swin Transformer [32]. We report the quantization accuracy of tiny, small, and base versions of each architecture. MimiQ is compared with baseline methods on image classification, object detection, and semantic segmentation tasks. We employed the Timm library [1] for network implementation and its pretrained weights to benchmark on ImageNet [22] classification tasks. We conducted further comparisons on COCO [29] and ADE20K [52] datasets. For COCO dataset, we use Mask R-CNN [18] decoder with Swin Transformer backbone from mmdetection [6] library. For ADE20K dataset, we choose the UPerNet decoder with DeiT and Swin Transformer backbones from mmsegmentation [11] library. Pre-trained weights and network implementations were adopted from these libraries.\nWe used the Adam optimizer for the synthetic image generation with lr=0.1 and $\u03b2_1$=0.9, $\u03b2_2$=0.999. We used a batch size of 32, where each batch of synthetic samples was updated for 2K steps with an \u03b1=1.0 and \u03b2=2.5e-5 following [47]."}, {"title": "6.2 Comparison on Image Classification Task", "content": "Tab. 1 shows the comparison of MimiQ against baseline DFQ methods. For comparison, we first provide \"Real-Data FT\" accuracies, which are from QAT with the original training dataset. Then, for the DFQ devised for CNN, we utilize all components that are applicable to ViT architecture. Lastly, for the DFQ methods for ViT, we utilize the official code from the authors."}, {"title": "6.3 Object Detection and Semantic Segmentation", "content": "We conduct further evaluations on object detection and semantic segmentation with various combinations of Transformer backbone and decoder architecture."}, {"title": "6.4 Sensitivity and Ablation Study", "content": "We conduct an ablation study of the individual effect of loss functions, shown in Tab. 4. In the first three rows, we show an ablation study regarding each component of $L_G$ ($L_{IHC}$, $L_{CL}$, and $L_{TV}$) (Sec. 5.1). The first row shows the performance of the base synthesis method, which does not include the proposed loss functions. We see accuracy drop when $L_{CL}$ is excluded, due to lack of crucial class information from pretrained classifier. As $L_{tv}$ only regularize steep changes across nearby pixels, it is the least significant generation loss affecting quantization accuracy. However, the overall trend shows that all generation losses cooperate to bring performance gain, showing the best accuracy when all losses are applied. On top of that, the proposed distillation $L_{HAD}$ (Sec. 5.2) further boosts the quantization accuracy, providing up to 23.24%p additional gain. Notably, the attention distillation method is particularly effective on larger models"}, {"title": "6.5 Inter-head Attention Similarity Metrics", "content": "We find that DSSIM has the highest correlation with quantized accuracy in Sec. 5.2. Here, we compare the quantization results from various choices of attention coherency metric $g_{dist}$ in Tab. 5. The results show that attention head distillation improves performance regardless of the coherency metric. The trend is prominent in ViT-Base network, where +DSSIM achieves 14.29%p accuracy gain and (+MSE, +L1-Dist. +KL-Div.) get (5.23%p, 4.89%p, 1.29%p) improvements, which agree with the rank correlation coefficients order in Fig. 5. This indicates the clear advantage of DSSIM over other choices, which are mainly used for capturing distributional similarity or general magnitude difference."}, {"title": "7 Discussion", "content": ""}, {"title": "7.1 Grad-CAM Analysis", "content": "As part of our analysis of how the MimiQ-generated images are viewed from the network's perspective, we utilize Grad-CAM [41] to visualize the attention map from the last layer. From Fig. 6 which compares Grad-CAM of MimiQ against other data-free methods, it can be observed that MimiQ-generated images have most of its object information clear and well-clustered, similar to the real images. On the other hand, images from other methods have much of its object information scattered, which could harm the accuracy."}, {"title": "7.2 Societal Concern", "content": "MimiQ may be considered as related to input reconstruction attacks [37, 44] which generate samples resembling the original dataset. To examine its potential risk, we used LPIPS to compare synthetic samples with the training dataset, as shown in Fig. 7. The upper images are synthetic samples from MimiQ, where the corresponding image below is a real sample with the lowest LPIPS value, which indicates that they form the most similar pair among the whole dataset. By observation, they capture general features of the class but do not replicate specific training images, demonstrating that MimiQ is far from having a serious threat of privacy leaks.\nFollowing [38], we conducted two additional experiments using ResNet-18, depicted in Tab. 6. First, we trained the network to distinguish between real and synthetic samples. The average train and test accuracies are 99.97% and 99.99%, respectively, which means each are distinguishable from a perspective of the neural networks. This shows that MimiQ is hard to be considered as susceptible to identity attacks. We also evaluated a synthetic-trained network from scratch on a real dataset (ImageNet), which showed average train and test accuracies are 49.69% of 0.16%, respectively. This indicates such networks fail to learn significant information related to real datasets, reducing the risk of model inversion attacks."}, {"title": "8 Conclusion", "content": "In this paper, we propose MimiQ, a data-free quantization method for vision transformers inspired by attention similarity. MimiQ utilizes inter-head attention similarity to better leverage the knowledge instilled in the attention architecture, synthesizing training data that better aligns inter-head attention responses. In addition, MimiQ utilizes fine-grained head-wise attention map distillation from full-precision teacher to quantized counterpart. As a result, MimiQ brings significant performance gain, setting a new state-of-the-art DFQ method for Vision Transformer architectures."}]}