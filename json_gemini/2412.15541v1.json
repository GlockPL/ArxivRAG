{"title": "ChangeDiff: A Multi-Temporal Change Detection Data Generator with Flexible Text Prompts via Diffusion Model", "authors": ["Qi Zang", "Jiayi Yang", "Shuang Wang", "Dong Zhao", "Wenjun Yi", "Zhun Zhong"], "abstract": "Data-driven deep learning models have enabled tremendous progress in change detection (CD) with the support of pixel-level annotations. However, collecting diverse data and manually annotating them is costly, laborious, and knowledge-intensive. Existing generative methods for CD data synthesis show competitive potential in addressing this issue but still face the following limitations: 1) difficulty in flexibly controlling change events, 2) dependence on additional data to train the data generators, 3) focus on specific change detection tasks. To this end, this paper focuses on the semantic CD (SCD) task and develops a multi-temporal SCD data generator ChangeDiff by exploring powerful diffusion models. ChangeDiff innovatively generates change data in two steps: first, it uses text prompts and a text-to-layout (T2L) model to create continuous layouts, and then it employs layout-to-image (L2I) to convert these layouts into images. Specifically, we propose multi-class distribution-guided text prompts (MCDG-TP), allowing for layouts to be generated flexibly through controllable classes and their corresponding ratios. Subsequently, to generalize the T2L model to the proposed MCDG-TP, a class distribution refinement loss is further designed as training supervision. Our generated data shows significant progress in temporal continuity, spatial diversity, and quality realism, empowering change detectors with accuracy and transferability. The code is available at https://github.com/DZhaoXd/ChangeDiff.", "sections": [{"title": "Introduction", "content": "Change detection (CD), a key Earth observation task, employs bitemporal remote sensing data to gain a dynamic understanding of the Earth's surface, producing pixel-wise change maps for ground objects (Feranec et al. 2007; Chen et al. 2013; Kadhim, Mourshed, and Bray 2016). In recent years, data-driven deep learning models have provided promising tools for CD and achieved remarkable progress (Lei et al. 2019; Arabi, Karoui, and Djerriri 2018; Dong et al. 2018). These advancements rely on large-scale, high-quality pixel-level annotations. However, building such a dataset poses a significant challenge because collecting diverse data and manually annotating them is costly, labor-intensive, and requires expert intervention. As a result, these challenges unsurprisingly restrict the size of existing public CD datasets, compared to general-purpose vision datasets such as ImageNet (Deng et al. 2009).\nTo alleviate high demand for data annotation, data synthesis has emerged as an alternative solution with promising application potential. Currently, a few synthesis techniques for binary CD (e.g., building variations) have been studied, categorized into two mainstreams: data augmentation-based and data generation-based methods. In the former, IAug (Zheng et al. 2021) and Self-Pair (Seo et al. 2023) use copy-paste and image inpainting techniques, pasting instances or patches from other regions onto target images to simulate building changes. However, the inconsistency between pasted areas and backgrounds makes it challenging to create realistic scene changes. In the latter, Changen (Zheng et al. 2023) introduces a generic probabilistic graphical model to generate continuous change pairs, improving the realism of synthetic images. However, Changen still relies on copy-paste operation of the image mask (semantic layout) to create changes, making it difficult to flexibly control change events. Additionally, the mask-based copy-paste is not easily applicable to semantic CD (SCD) task due to the lack of complete masks. Moreover, it requires additional segmentation data to train the probabilistic model, limiting transferability to specific target data.\nRecently, driven by latent diffusion models (Rombach et al. 2022), generative models have reached a new milestone(Khanna et al. 2023). Stable Diffusion (Podell et al. 2023) and DALL-E2 (Ramesh et al. 2022) introduce large-scale pretrained text-to-image (T2I) diffusion models that can generate high-quality images matching textual descriptions. Furthermore, advanced work, e.g., ControlNet (Zhang 2023), has shown that by incorporating fine-grained con-"}, {"title": "Related Work", "content": "Binary & Semantic Change Detection & Text-guided Diffusion Model. Please refer to the Appendix A.\nData Synthesis in Change Detection. Currently, there are several advanced data synthesis methods for the change detection task. ChangeStar (Zheng et al. 2021) and Self-Pair (Seo et al. 2023) employ simple copy-paste operations, pasting patches from other regions onto the target image to simulate changes. However, artifacts introduced by the paste operation and the inconsistency between foreground and background make it challenging to create realistic scene changes. IAug (Chen, Li, and Shi 2021) uses a generative model (GAN) to synthesize changed objects, but its building-specific modeling approach limits its generalization to diverse scenes. Changen (Zheng et al. 2023) proposes a generic probabilistic graphical model to represent continuous change events, enhancing the realism of synthetic images. Its latest version, Changen2 (Zheng et al. 2024), introduces a diffusion transformer model, further improving generation quality. However, it relies on additional segmentation data to train the probabilistic model, making it unsuitable for direct data augmentation in change detection tasks.\nOur ChangeDiff does not rely on additional segmentation data, simplifying its integration into existing workflows. Besides, ChangeDiff supports text control, enabling users to specify the generated changes precisely. Furthermore, ChangeDiff can synthesize diverse and continuous layouts, which is crucial for improving the transferability of synthetic data."}, {"title": "Method", "content": "Given a single-temporal image $x \\in \\mathbb{R}^{H\\times W\\times 3}$ and its sparsely-labeled (only the change area) semantic layout $y \\in \\mathbb{R}^{H\\times W}$ in the semantic change detection (SCD) dataset, our SCD data generator ChangeDiff aims to simulate temporal changes via diffusion models conditioned on diverse completed semantic layouts. The pipeline of ChangeDiff is shown in Fig. 2. Overall, ChangeDiff consists of the text-to-layout (T2L) diffusion model for changing layout synthesis and the layout-to-image (L2I) diffusion model for changing image synthesis. In the next part, we first introduce the detailed design of T2L and L2I diffusion models. Then, we discuss how to flexibly encode semantic layouts into the text prompt. Lastly, we introduce how to synthesize complete and diverse layouts via text prompts."}, {"title": "Preliminary", "content": "Both T2L and L21 models are built on the latent diffusion model (LDM) (Rombach et al. 2022), widely used in conditional generation tasks for its powerful capabilities. LDM learns the data distribution by constructing a T-step denoising process in the latent space for the normally distributed variables added to the image $x \\in \\mathbb{R}^{H\\times W\\times 3}$. Given a noisy image $x_t$ at time step $t \\in \\{1,...,T\\}$, the denoising function $\\epsilon_\\theta$ parameterized by a U-Net (Ronneberger, Fischer,\nand Brox 2015) learns to predict the noise $\\epsilon$ to recover x.\n$L_{LDM} = \\mathbb{E}_{z_0, t, \\epsilon \\sim \\mathcal{N}(0, 1)} ||\\epsilon - \\epsilon_\\theta(z_t, t, T_\\theta(T))||_2^2$,\nwhere $z = \\delta_\\theta(x)$ is the encoding of the image in latent space. $T_\\theta$ is a pre-trained text encoder (Radford et al. 2021) that enables cross-attention between the text T and $z_t$ to steer the diffusion process."}, {"title": "T2L Model", "content": "Given a semantic layout y where pixel values are category IDs, we encode y as a three-channel color map $C_y$ in RGB style, and each color indicates a category. This enables leveraging the pre-trained LDM model to generate semantic layouts from input text prompts T."}, {"title": "L2I Model", "content": "Recent works like ControlNet (Zhang 2023) propose conditioning on both semantic layout y and text prompt T to synthesize images aligned well with specific semantic layouts. Following this, we adopt the ControlNet structure, which adds a trainable side network to the LDM model for encoding semantic layout y."}, {"title": "Flexible Text Prompt as Condition", "content": "We explore encoding semantic layouts via text prompts to utilize the T2L model for completing sparse layouts and generating diverse layouts."}, {"title": "Semantic Layout Translation", "content": "A semantic layout y can be decomposed into two components: category names $\\{n_j\\}$ and their corresponding connected areas $\\{a_j\\}$. The names can be naturally encoded by filling in the corresponding textual interpretation into T, but connected areas can not since the pixels within them are arranged consecutively. To discretize, we partition each pixel into cells at distinct coordinate points. Cells with the same category ID can be count-aggregated into a unique class ratio, quantitatively characterizing the corresponding $\\{a_j\\}$ for insertion into the encoding vocabulary of T. Formally, given a semantic layout y, the connected areas $a_j$ of the j-th class are represented by the class ratio $R_j$ as,\n$R_j = \\sum_{h=1}^{H} \\sum_{w=1}^{W} [1(y_{hw} = j)]/HW$.\n$1(.)$ is an indicator function, which is 1 if the category ID is j, otherwise it is 0. Then, a certain category j in the semantic layout y is encoded as a phrase (nj, Rj) with two tokens."}, {"title": "Text Prompt Construction", "content": "With the phrases encoding multiple categories, we serialize them into a single sequence to generate text prompts. All phrases will be sorted randomly. Specifically, we adopt a template to construct the text prompt, \"A remote sensing photo with {class distributions}\", where class distributions = \u201c... $(n_{j-1}, R_{j-1})(n_j, R_j)(n_{j+1}, R_{j+1})$ . . . \". We term this as multi-class distribution-guided text prompt (MCDG-TP)."}, {"title": "Class Distribution Refinement Loss", "content": "The loss function $L_{LDM}$ used to pre-train the LDM model only optimizes the denoising function $\\epsilon$ (See Eq. (1)), enabling it to predict noise and thus recover the clean image. There is no explicit constraint between the embeddings"}, {"title": "Changing Layout Synthesis", "content": "To synthesize the changed images, reasonable and diverse layout synthesis in the temporal dimension is required as a semantic guide."}, {"title": "T2L Model Training", "content": "Given the target SCD dataset, we use the [text prompt $T_y$, color map $C_y$] training pairs to fine-tune the T2L model, where $T_y$ is the corresponding text generated via our MCDG-TP for the color map. With the proposed loss $L_{CDR}$ and the original loss $L_{LDM}$, the T2L model is supervised during fine-tuning as follows,\n$L_{ChangeDiff} = L_{LDM} + \\sum_{m \\in M} L_{DR}$"}, {"title": "Sparse Layout Completion", "content": "Since generating a changed image requires a complete layout, it is necessary to complete the sparse layout to serve as a reference for synthesizing the changed layout. To obtain the completed layout, we input text prompt $T_c$ with amplified class ratios and random noise $z_c$ sampled from $z \\sim \\mathcal{N}(0, 1)$ into the fine-tuned T2L model. By varying $T_c$ and $z_c$, various reference color maps $C_i$ with different object compositions can be obtained."}, {"title": "Time-varying Event Simulation", "content": "With an arbitrary reference layout and its corresponding $[T_c, z_c]$, we input noise sampled following $z_c$ and varied text into the fine-tuned T2L model to simulate real-world time-varying events. For the"}, {"title": "Changing Image Synthesis", "content": "Conditioned on the synthesized color maps $\\{C_i, ..., C_{i+n}\\}$ and texts $\\{T_i,..., T_{i+n}\\}$, we use the fine-tuned L2I model to synthesize images $\\{x_i, ..., x_{i+n}\\}$ aligned with the given layouts. During synthesis, we randomly sample a noise $z_i$ from $z \\sim \\mathcal{N}(0, 1)$ to obtain a reference image $x_i$. Starting from $x_i$, the semantic content of images over time should remain within a certain similarity range. To this end, the input noises $\\{z_{i+1},..., z_{i+n}\\}$ for the changed images is sampled via a stitching mechanism, which is formulated by,\n$z_{i+1} = \\alpha z_i + (1 - \\alpha) z_{r(r \\neq i)}.$"}, {"title": "Experiments", "content": "Setup. We evaluate the effectiveness of ChangeDiff on semantic change detection tasks using the following two settings: 1) Data Augmentation: This setup aims to verify if synthetic data from ChangeDiff can enhance the model's discrimination capability on in-domain samples. We use three commonly used semantic change detection datasets, including SECOND (Yang et al. 2021), Landsat-SCD (Yuan et al. 2022), and HRSCD (Daudt et al. 2019). We train our ChangeDiff on these datasets, respectively. 2) Pre-training Transfer: This setup aims to verify if ChangeDiff can leverage publicly available semantic segmentation data to synthesize extensive data for pre-training, benefiting downstream change detection tasks. We use additional semantic segmentation data from LoveDA (Wang et al. 2021) as the training source for ChangeDiff and perform pre-training with the synthetic data. We then validate its effectiveness on the SECOND and HRSCD target datasets using two transfer ways, including \"zero-shot transfer\" and \"fine-tuning transfer\".\nWe validate the effectiveness of ChangeDiff as data augmentation on three datasets: SECOND, Landsat-SCD, and HRSCD. SECOND and Landsat-SCD datasets contain incomplete (sparse) semantic annotations, while HRSCD has complete semantic annotations. We integrate ChangeDiff with various semantic change detection methods, including CNN-based approaches like SSCDL, BiSRNet, TED, the lightweight A2Net, and the Transformer-based SCanNet. Experimental results clearly demonstrate the effectiveness of our method, particularly in addressing semantic imbalance (measured by the Sek metric) and improving binary change detection performance (measured by the F1-score).\nShort on SECOND. In contrast, our approach surpasses the baseline by 2.1% SeK points on SECOND and 2.2% SeK points on HRSCD, demonstrating superior effectiveness in enhancing detection performance across diverse datasets."}, {"title": "Pre-training Transfer", "content": "In this section, we aim to validate the benefit of using ChangeDiff for data synthesis with the out-of-domain semantic segmentation dataset, LoveDA, in the context of the SCD task. We conduct experiments to validate this from two perspectives: zero-shot transfer and fine-tuning transfer.\nZero-shot Transfer. The results highlight that the ChangeDiff outperforms the others across all metrics. Copy-Paste shows the weakest performance, with a mIoU5 of 39.9%, F1-score of 42.7%, and SeK5 of 4.7%. ControlNet + Copy-Paste improves performance, achieving an mIoU5 of 55.1%, F1-score of 49.4%, and SeK5 of 10.7%. Changen also performs reasonably well, with a mIoU5 of 53.1%, F1-score of 47.2%, and SeK5 of 7.9%. ChangeDiff (Ours) demonstrates the best results, with a mIoU5 of 60.1%, F1-score of 55.5%, and SeK5 of 13.6%. These results suggest that our ChangeDiff generates higher-quality synthetic data with better transferability to downstream tasks."}, {"title": "Ablation Studies", "content": "We ablate our core module MCDG-TP on three experiments: augmentation for SECOND and Landsat-SCD, and fine-tuning transfer on SECOND. We replace MCDG-TP with two strong variants: Copy-Paste and original T2I, and present the performance results in Table. 6. The ablation studies highlight the significant effectiveness of our MCDG-TP module. When compared to the baseline, MCDG-TP improves SeK by 2.3% and F1 by 1.3% in the Augment SECOND scenario. In the Augment Landsat-SCD scenario, shows enhancements of 2.4% in SeK and 3.0% in F1. For Fine-tuning Transfer, MCDG-TP achieves improvements of 2.7% in SeK and 1.3% in F1. These results demonstrate that MCDG-TP consistently delivers substantial performance gains across different experimental setups, outperforming the alternative methods."}, {"title": "Qualitative Analysis", "content": "Comparison of Synthesis Quality. As shown in Fig. 4, we compare semantic change detection data synthesized using different methods: b) Self-Pair (Seo et al. 2023): Objects are pasted from other patches, creating mismatches in the foreground and background (highlighted in red). c) Changen (Sparse Layout) (Zheng et al. 2023): Images are synthesized from a sparse layout, with semantically unclear regions (highlighted in red) due to unknown semantics in the white areas. d) Changen (Random Filled Layout): Images synthesized with random filling show visible artifacts due to poor semantic consistency (highlighted in red). e) Ours: Our method generates high-quality images with improved consistency and clarity, thanks to a well-designed layout and change events."}, {"title": "Conclusion", "content": "Change detection (CD) benefits from deep learning, but data collection and annotation remain costly. Existing generative methods for CD face issues with realism, scalability, and generalization. We introduce ChangeDiff, a new multi-temporal semantic CD data generator using diffusion models. ChangeDiff generates realistic images and simulates continuous changes without needing paired images or external datasets. It uses a text prompt for layout generation and a refinement loss to improve generalization. Future work could extend this approach to other CD tasks and enhance model scalability."}]}