{"title": "Pre-training with Synthetic Patterns for Audio", "authors": ["Yuchi Ishikawa", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "abstract": "In this paper, we propose to pre-train audio encoders using synthetic patterns instead of real audio data. Our proposed framework consists of two key elements. The first one is Masked Autoencoder (MAE), a self-supervised learning framework that learns from reconstructing data from randomly masked counterparts. MAEs tend to focus on low-level information such as visual patterns and regularities within data. Therefore, it is unimportant what is portrayed in the input, whether it be images, audio mel-spectrograms, or even synthetic patterns. This leads to the second key element, which is synthetic data. Synthetic data, unlike real audio, is free from privacy and licensing infringement issues. By combining MAEs and synthetic patterns, our framework enables the model to learn generalized feature representations without real data, while addressing the issues related to real audio. To evaluate the efficacy of our framework, we conduct extensive experiments across a total of 13 audio tasks and 17 synthetic datasets. The experiments provide insights into which types of synthetic patterns are effective for audio. Our results demonstrate that our framework achieves performance comparable to models pre-trained on AudioSet-2M and partially outperforms image-based pre-training methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Large-scale models have demonstrated high performance in the audio processing field. Among these, Transformers [1] have played an important role in advancing this field, although they have the drawback of requiring large amounts of labeled data for training. Moreover, it is challenging to collect high-quality labeled audio data in the real world, which impedes effective training. We identify two approaches in existing works aiming to leverage Transformers to solve downstream tasks: (i) transferring Vision Transformers [2] (ViTs) pre-trained on ImageNet [3] to audio tasks [4], (ii) learning feature representations through self-supervised learning [5]-[7] from large amounts of audio data (e.g., AudioSet [8]) and VGGSound [9]). However, these methods harbor problems related to real data, as described below:\nPrivacy issues: ImageNet and AudioSet consist of images that portray people and human voices, respectively. Therefore, using these datasets may potentially infringe on privacy.\nLicense infringement issues: Most of the data included in large-scale datasets are collected from the web, such as YouTube and search engines. Some of these data may have licenses that prohibit their use for model training. Using such data or models trained on such data is legally sensitive.\nOne approach to solving these issues is to synthesize realistic data. In the audio domain, while some works have proposed utilizing text-to-speech systems [10]-[12], others have suggested training models with sounds generated by synthesizers [13], [14]. However, many of these approaches still rely on real data and have subpar performance when using synthetic data alone primarily due to a lack of diversity. As an alternative approach in computer vision, existing works have proposed pre-training models with synthetic visual patterns [15]-[18]. These methods have been reported to demonstrate high performance"}, {"title": "II. PROPOSED FRAMEWORK", "content": "To address privacy and licensing concerns in pre-training audio encoders, we propose pre-training an MAE with synthetic patterns and then transferring it to audio tasks (Fig. 1). In this section, We first provide an overview of MAEs (Sec. II-A), and then describe the synthetic patterns used in our experiments (Sec. II-B). Finally, we explain how to transfer MAEs to audio data (Sec. II-C).\n\nA. Masked Autoencoder\nMasked Autoencoder (MAE) [21] is a self-supervised learning framework, where the transformer-based autoencoder aims to reconstruct an input from its masked version. While MAE was originally developed for image recognition, it has been successfully applied to audio [5], [6].\nThe MAE pre-training process (illustrated in the upper half of Fig. 1) begins by splitting the input the input $X \\in R^{C \\times H \\times W1}$ into non-overlapping $P \\times P$ patches, which are then linearly embedded to form $X_p \\in R^{N \\times D}$, where $N = \\frac{HW}{P^2}$ is the number of patches and $D$ is the embedding dimension. Subsequently, a high proportion of patches (e.g., 75%) are randomly masked out using a binary mask, producing visible embeddings. These embeddings are then fed into a transformer-based autoencoder, which is trained to reconstruct the original pixel values using mean squared error as the loss function. After pre-training, the encoder part of the MAE, which is equivalent to a Vision Transformer (ViT), will be fine-tuned for downstream tasks (in the lower part of Fig. 1).\nWhile existing MAE approaches typically use real-world data, [19] has demonstrated that VideoMAE [20] can learn spatiotemporal feature representations for action recognition, from synthetic videos which do not contain humans or objects. This suggests that MAEs focus on low-level information (e.g. visual patterns and regularities) and it is unimportant what is portrayed in the input, whether it be images, audio mel-spectrograms, or even synthetic patterns.\nInspired by this finding, we propose pre-training MAEs with synthetic patterns, and then transfering them to audio tasks. This approach can eliminate the need for real audio data during pre-training, therefore alleviating issues related to real data like privacy and license infringement. In the following section, we will elaborate on the synthetic patterns that we use in this study.\nB. Synthetic Patterns\nWhile various types of synthetic patterns can be used for training MAEs, it is challenging to explore effective synthetic patterns for MAEs without any constraints. Therefore, in this study, we focus on synthetic images as one form of synthetic patterns. Specifically, we utilize 17 synthetic image datasets proposed in computer vision, as illustrated in Fig. 2.\nDatasets labeled (a-n) contain 100k images, respectively, and are generated from structured noise [17]. We use these datasets to investigate which types of synthetic patterns are beneficial for MAE training. We also use large-scale synthetic image datasets labeled (o-q) for comparison with existing pre-training methods. FractalDB1k [15] and VisualAtom1k [16] are generated from mathematical formulas, while Shaders1k [18] is synthesized with OpenGL fragment shaders. Each of these large-scale datasets includes over 1M images. Pre-training on these datasets achieves performance on image classification tasks comparable to that achieved by pre-training on large-scale real image datasets such as ImageNet.\n$^{1}C$ represents the number of channels and $H$ and $W$ each represent the height and width of the input."}, {"title": "C. Transferring MAEs to Audio Tasks", "content": "When we pre-train MAEs with synthetic images and transfer them to audio tasks, we need to modify its encoder part (i.e., Vision Transformer; ViT) because the input shape is different between ViTs and Audio Spectrogram Transformers (ASTs) [4]. We make the following modifications:\nPatch Embedding: The input dimension C differs between ViTs (3 channels) and ASTs (1 channel). To enable RGB-image-based ViTs to handle 1-channel inputs, we sum the weights of the patch embedding along the channel dimension.\nPositional encoding: The input size of ViTs is typically different from that of ASTs, while the patch size is the same. To address this, we directly modify the positional encoding. Both ViTs and ASTs use sinusoidal positional encoding, so we simply replace the positional encoding of ViTs with that of ASTs."}, {"title": "III. EXPERIMENTS", "content": "To evaluate the efficacy of our framework, we pre-train MAEs using synthetic images and fine-tune them on various audio tasks. We provide the details of the experimental setting as follows.\nDatasets: For pre-training, we use 17 synthetic image datasets described in Sec. II-B. For downstream tasks, we use a total of 12 datasets. Following [6], we evaluate our framework on AudioSet-20k/2M (AS-20k/2M) [8], ESC50 [22], DCASE2019 task1A dataset [23], OpenMIC2018 dataset [24], and Speech Command V2 (SCV2) [25]. Additionally, following [14], we conduct experiments on selected tasks from HEAR benchmark [26] and ARCH benchmark [27]: UrbanSound 8K (US8K) [28], Variably Intense Vocalizations of Affect and Emotion dataset (VIVAE) [29], NSynth Pitch 5h dataset (NSynth) [13], CREMA-D (C-D) [30], FSD50k [31], Vocal Imitations dataset (VI) [32], and LibriCount dataset (LCount) [33]. We report mean average precision (mAP) for AS-20k/2M, OpenMIC2018, FSD50k, and VI, while we use accuracy as an evaluation metric for other datasets. For datasets with multi-fold splits, we conduct cross-validation and report the average metric.\nImplementation Details: We conducted pre-training MAEs fol-lowing the setting of [21], with a mask ratio of 0.75 and the number of epochs set to 800. For fine-tuning, our experiments are mainly based on MaskSpec [6] due to high reproducibility. For the model architecture, We adopted a vanilla ViT [2] with non-overlapped patches as the backbone, especially the ViT-Base variant.\n\nA. Properties of synthetic images\nFirst, we used small-scale synthetic image datasets (labeled a-n in Fig. 2) to investigate which types of synthetic images are effective for our framework. We examined the Pearson correlation coefficient r between the following four properties of the datasets and their performance on ESC-50 fold5 (Fig. 3).\nColor Entropy: To measure the diversity of colors in images, we calculated color histograms for each image and computed the average entropy of these histograms. Fig. 3a shows that color diversity has little correlation with performance.\nBrightness Entropy: We converted images to grayscale, calculated brightness histograms, and computed their average entropy. We found there is a weak correlation with performance (Fig. 3b).\nTotal Variation: Total Variation (TV) [34] is typically used as regularization in image restoration and denoising. Here, we adopt TV as a metric to evaluate how much noise is in the images We calculated the sum of TV for each image and reported the average value. We observed a negative correlation with performance ($r = -0.4$). This"}, {"title": "B. Comparison with image-based pre-training", "content": "To demonstrate the transferability of MAEs, we compare MAEs pre-trained on synthetic images with other image-based pre-training methods. For this comparison, we use ViT pre-trained with supervised learning on ImageNetlk and Formula-Driven Supervised Learning"}, {"title": "D. Evaluation on HEAR and ARCH benchmark", "content": "Table III presents the results from eight datasets selected from the HEAR and ARCH benchmarks, following the setting of [14]. Although our model performs well when fully fine-tuned, it exhibits a significant limitation in the linear probing setting, where the encoder weights remain fixed and only the linear layer is trained. However, this limitation is not unique to our framework; it is also observed in MaskSpec, which is a self-supervised learning method using MAE and real audio. Therefore, we consider this limitation results from the characteristics of MAE which focuses on low-level features, rather than high-level features. To learn more audio-specific feature representations and improve performance in the linear probing setting, a more sophisticated approach beyond masked visual modeling is necessary, which remains a challenge for future work."}, {"title": "IV. CONCLUSION", "content": "In this work, we propose pre-training audio encoders utilizing synthetic patterns, addressing challenges associated with real data, such as privacy concerns and licensing issues. Our framework demonstrates robust performance across diverse audio tasks, even without audio data during pre-training. Through extensive experiments, we have revealed what types of synthetic patterns are effective for audio tasks. Specifically, we have found that smoother images with fewer Total Variations contribute significantly to MAE pre-training. In comparison with existing methods, our framework achieves comparable performance to self-supervised pre-training methods with real audio and partially outperforms image-based pre-training methods. We posit that our framework offers a viable solution to mitigate the costs of audio data collection and alleviate concerns regarding privacy and license infringements during audio pre-training."}]}