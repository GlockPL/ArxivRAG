{"title": "Data-Constrained Synthesis of Training Data for De-Identification", "authors": ["Thomas Vakili", "Aron Henriksson", "Hercules Dalianis"], "abstract": "Many sensitive domains \u2013 such as the clinical domain - lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study - using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.", "sections": [{"title": "1 Introduction", "content": "Many useful applications in NLP involve domains where data are sensitive. These privacy risks, and the accompanying limits to sharing data, have traditionally been solved by de-identification. This process involves finding parts of the text that can be used to identify an individual. Such information is typically referred to as personally identifiable information (PII). After locating PII, the passages need to be processed to remove or obscure the PII. Traditionally, this time-consuming work has been done manually. Automatic de-identification (Meystre et al., 2010) is a machine-driven approach that typically relies on named entity recognition (NER) to detect PII that needs to be removed.\nUnfortunately, the PII datasets that exist to assist in privacy preservation are themselves sensitive, and can typically not be shared. This circularity, together with the increasing generative capabilities of large language models (LLMs), has led to a growing interest in overcoming data limitations by eschewing the use of real data altogether. Instead, one can use generated synthetic corpora.\nPrevious studies have mainly been concerned with evaluating the privacy of the synthetic text (Yue et al., 2023; Miranda et al., 2024) or with creating the strongest-performing model possible using synthetic data (Libbi et al., 2021; Hiebel et al., 2023; Liu et al., 2025). Our study instead examines how synthetic data can be produced under constrained resources. This understudied problem is common in clinical institutions that lack resources both in terms of data and computational hardware.\nWe carry out a systematic evaluation of key factors impacting the utility of LLM-generated synthetic data as training data for downstream tasks. Specifically, we study synthetic NER data for PII detection - an important task in the privacy-sensitive healthcare domain. Synthetic clinical data are generated using domain-adapted LLMs and machine-annotated using fine-tuned NER models. The evaluations focus primarily on the utility of synthetic data for training NER models.\nThrough extensive experimentation, we investigate the impact on utility of (i) the amount of data used for domain adaptation of the synthesizing LLM, (ii) the quality of the machine annotator, (iii) the amount of synthetic data generated, and (iv) model size. We also quantify the diversity and privacy of the generated data, and carry out experiments across two languages \u2013 Swedish and Spanish.\nOur main contributions are:\n1. Demonstrating that moderately-sized LLMs can be adapted to the clinical domain to produce high-utility text with relatively small amounts of in-domain data.\n2. Showing that using synthetic machine-annotated data allows for training NER models that perform only slightly worse compared to using real, sensitive data, while reducing the risk of exposing sensitive information in the original data.\n3. Finding that, for the task of detecting PII, using larger generative LLMs for synthesis does not yield clear improvements in terms of utility. Rather, downstream performance relies on having a high-quality gold standard NER model for providing machine annotations."}, {"title": "2 Related Research", "content": "There have been several approaches to generating synthetic clinical data for a number of languages and for different purposes. Broadly speaking, most prior works have either focused on maximizing the utility of the synthetic data, or on studying the privacy characteristics of synthetic corpora.\nWhile most papers studying data synthesis contain some form of privacy analysis, other papers have this as their main focus. Several papers study how differentially private learning impacts the utility (Yue et al., 2023; Igamberdiev et al., 2024) and the privacy of the data (Miranda et al., 2024). While privacy is an important justification for synthesizing data, it is not the main focus of our paper.\nThe second main current in the literature explores how to optimize synthesis to create the best possible synthetic corpora. These papers synthesize data using locally domain-adapted LLMs (Ive et al., 2020; Hiebel et al., 2023), or using instruction-tuned models (Kiefer, 2024; Liu et al., 2025). They show that synthesis of high-utility data is possible. However, fewer papers systematically examine the conditions required for success.\nIn our literature review, two studies stand out as particularly relevant to this study. Libbi et al. (2021) synthesize a Dutch corpus for PII detection using a GPT-2 model (Radford et al., 2019) domain-adapted using 1 million documents and add rule-based machine annotations. Our study follows the same overall process for synthesis, but uses much less data and more modern NLP techniques. Xu et al. (2023) similarly create synthetic corpora and experiment with constraining the total amount of data used, but do so for the relation extraction task. In this paper, we focus on a different task \u2013 NER for PII detection. Furthermore, in contrast to both studies, we systematically evaluate the impact of constraining data alternately for both domain adaptation and machine annotation, try two different model sizes, synthesize corpora of different sizes, and validate our results across two languages."}, {"title": "3 Data and Methods", "content": "In this study, we investigate the impact of various factors related to generating synthetic data for fine-tuning encoder language models on downstream tasks in the healthcare domain. Specifically, we study the possibility of generating synthetic clinical text for training NER models for detecting PII. The synthetic text is created by a domain-adapted generative LLM and then machine-annotated for PII using a fine-tuned encoder model. This process follows the structure of previous works (Libbi et al., 2021) and is illustrated in Figure 1.\n3.1 Generative Models\nThe aim of this study is to examine the feasibility of generating training data for NER models detecting PII. The foundation of the training data are synthetic texts, generated using autoregressive LLMs. Two model families are used as a base for domain adaptation to the clinical domain.\nGPT-SW3 For Swedish, we use the GPT-SW3 model (Ekgren et al., 2024). This autoregressive language model was trained using approximately 320 billion tokens. The data were mainly composed of Scandinavian texts and 35.3% of the data is Swedish.\nFLOR The autoregressive model used to generate Spanish data is the FLOR model (Da Dalt et al., 2024). The model was initialized with the weights of the multi-lingual BLOOM model (Scao et al., 2023) and trained with continued pre-training. The data used spanned 140 billion tokens and was composed of equal parts English, Spanish, and Catalan data.\nBoth models are used autoregressively, without instruction tuning. The hypothetical \u2013 but often occurring - scenario motivating the study design is where researchers have access to a small and sensitive NER dataset that cannot be shared outside of their organization. Zero-shot synthesis is an alternative strategy, but we leave to future research to evaluate if this approach can yield clinical texts that are sufficiently similar to the real data. Instead, we perform different degrees of domain-adaptive fine-tuning to train the LLMs to produce such texts.\nThe primary experiments in Section 4.1 used the versions of FLOR and GPT-SW3 with 6.3"}, {"title": "3.2 NER Datasets", "content": "This study focuses on a particular type of NER dataset - NER for detecting PII. Such datasets exist for several languages but are, as discussed in the introduction, very difficult to share and access. This is particularly true for datasets targeting the clinical domain. In this study, two datasets for detecting PII in clinical data are used.\nSEPR PHI The Stockholm EPR PHI Pseudo Corpus (SEPR PHI) is a Swedish dataset from five different healthcare units consisting of 100 patient records split into 21,553 sentences. The corpus spans 282,766 tokens where 6,755 are manually annotated for nine different PII classes (Velupillai et al., 2009). The dataset has then been pseudonymized, meaning that all the annotated entities have been replaced with realistic pseudonyms (Dalianis, 2019).\nMEDDOCAN The second dataset is MEDDOCAN a Spanish dataset consisting of 1,000 medical texts. These are based on clinical cases augmented with PII from auxiliary sources (Marimon et al., 2019). The texts were then manually annotated for 19 different PII. Out of 504,569 tokens, 41,859 are tagged as PII.\nEach dataset was split into three subsets: one for training, another for validation, and a third held-out subset for testing. The training sets are used to domain-adapt the generative models and to train the machine-annotating NER models. The purpose of the validation subsets is two-fold. First, they are used to monitor the training processes when fine-tuning the models for synthesis and NER. Once the models for synthesis have been trained, the validation data also serve as starting points when creating the synthetic corpora. Finally, the quality of the NER models trained using the synthetic corpora is evaluated using the held-out test sets, as these are not in any way part of the training or synthesis processes."}, {"title": "3.3 Encoder Models", "content": "This study trains NER models by fine-tuning pre-trained encoder models. Two different models are used, one for each language.\nSweDeClin-BERT We use SweDeClin-BERT (Vakili et al., 2022) for Swedish data, as it has previously shown strong performance on the SEPR PHI corpus. This BERT-style model is based on the general-domain KB-BERT model (Devlin et al., 2019; Malmsten et al., 2020) and adapted to the clinical domain through continued pre-training on the Swedish Health Bank corpus (Dalianis et al., 2015). It consists of 125 million parameters.\nroberta-base-bne For Spanish data, we use the roberta-base-bne model trained by Guti\u00e9rrez-Fandi\u00f1o et al. (2022). This RoBERTa-based model (Liu et al., 2019) consists of 125 million parameters. It was trained using a large Spanish corpus collected by the National Library of Spain and performs strongly on the MEDDOCAN task."}, {"title": "3.4 Synthesizing and Evaluating the Corpora", "content": "The generative models were domain-adapted by fine-tuning them for causal language modeling using QLORA (Dettmers et al., 2023) as implemented in the Axolotl framework\u00b9 with r = 8 and \u03b1 = 32. A comprehensive specification of all hyperparameters is available in Appendix A. GPT-SW3 was domain adapted using the Swedish SEPR PHI corpus and FLOR using the Spanish MEDDOCAN corpus. Domain adaptation was carried out with varying amounts of data to determine the impact on the utility of the synthetic data.\nAs mentioned in Section 3.2, the validation sets used for monitoring the fine-tuning process were also used for data synthesis. The 5% validation subsets were used to create starting points for generating text, as suggested by Libbi et al. (2021). The starting points were created by taking the first three words of each document in the validation sets.\nSynthetic corpora have an intriguing advantage over real corpora: they can be arbitrarily large. Taking this feature into account, each three-word starting point was used to create 80 new samples. Consequently, the synthetic corpora are four times larger than the gold-standard corpora. In Section 4.5, the benefits of exploiting this feature are examined through experiments that use smaller amounts of synthetic data.\nSynthesis was done using the vLLM library (Kwon et al., 2023). We use nucleus sampling (Holtzman et al., 2020) with p = 0.95 and the minimum token length is set to 10. The maximum token length is set to the length of the longest document in the validation set, or at least 50. The temperature was set to t = 1.0 after preliminary experiments showed that varying 0.8 \u2264 t \u2264 1.2 had very little impact on the results.\nFinally, the synthetic texts were machine-"}, {"title": "4 Experiments", "content": "The main contribution of this paper is a systematic investigation into how the different steps of synthetic corpora creation respond to data constraints. This section describes these experiments and their results. All experiments, except for those in Section 4.5, take advantage of the unbounded nature of synthetic corpora and allow them to be four times larger than the gold standard datasets. The performance of the NER models is summarized using token-level F\u2081 score. These evaluations rely on each fold's held-out gold-standard test data.\n4.1 Constraining the Total Amount of Data\nThe first experiment of this study investigated how much data is required to produce a well-performing NER model for detecting PII. The amount of data used for domain-adapting the generative model and fine-tuning the gold NER model is varied. This models the common situation where there is limited access to data, and demonstrates what performance can be expected for different data sizes.\nWithin each fold, this is scaled to between 5% and 95% of the training data in the fold. Five different amounts are used: 5%, 10%, 25%, 50%, and 95%. These subsets correspond to the Sensitive NER corpus in Figure 1. The final 5% are used for validation and for creating prompts for synthesis.\nSynthetic corpora have an advantage over real corpora: their size is only constrained by the amount of compute available for generation. As explained in Section 3.4, this feature is incorporated by letting the synthetic data be four times larger"}, {"title": "4.2 Scaling the Data for Domain Adaptation", "content": "The experiments in the previous section show that it is indeed feasible to create well-performing NER models trained on synthetic data using our method. On the other hand, the results depend on the amount of data used for domain-adapting the synthesizing generative LLM and for fine-tuning the machine annotating encoder model. In the previous experiment, the data were fixed for both purposes.\nThis section describes an ablation study that measures the impact of varying the amounts of data used for domain adaptation. As before, the synthetic corpora are allowed to be four times larger than the original corpora. In these experiments, the amount of data used for fine-tuning the machine annotator is kept constant at 95%, while the amount used for domain adaptation is varied between 5% and 95%. Additionally, we also use synthetic corpora generated without domain adaptation.\nThe average F1 scores of the models resulting from these experiments are listed in Table 2. Unsurprisingly, the worst-performing models are those that were trained using corpora synthesized without domain adaptation. These results show that domain adaptation does matter. However, there are clearly"}, {"title": "4.3 Varying the Data for Machine Annotation", "content": "The experiments in Section 4.2 indicated that the synthetic corpora improve when more data are available to domain adapt the model generating the text. However, the values in Table 2 and the values from the original experiments in Table 1 differ greatly. The results indicate that using a strong machine annotator as in Section 4.2 \u2013 explains more of the performance. Another set of experiments was conducted to examine this effect.\nIn contrast to the previous experiments, these experiments use 95% of the data for domain adaptation of the generative model that produces the synthetic text. This corpus is still allowed to be four times larger than the original training corpus. The data used to create the machine annotating NER model is varied between 5% and 95%.\nModels were trained and evaluated using 5-fold cross-validation and the resulting F1 scores are"}, {"title": "4.4 Using Smaller Generative Models", "content": "The generative LLMs used for domain adaptation in this study - GPT-SW3 and FLOR are available in different sizes. The previous experiments have used the 6.3 billion and 6.7 billion versions of the models. Although these models are not very large from a research perspective, domain-adapting them still requires expensive hardware. In this experiment, we try synthesizing data using the smaller versions of these LLMs.\nBoth smaller versions consist of approximately 1.3 billion parameters. Table 4 lists the F\u2081 scores obtained when using 95% of the data for domain adaptation and for creating the machine annotator. Despite being around five times smaller than their larger counterparts, the smaller models yielded very similar results to their larger counterparts. This suggests that smaller models are a viable alternative, at least for synthesizing data for PII identification."}, {"title": "4.5 How Much Synthesis is Enough?", "content": "In all previous experiments, we have exploited the fact that synthetic corpora can be generated indefinitely. This has been represented by letting the corpora be four times larger than the training data. In this experiment, we examine the effect of removing this advantage. In addition to training on the four times larger corpora, we also trained models using corpora of the same size as the training corpora. Finally, we trained models using just 5% of the synthetic corpora. The data used for domain adaptation and fine-tuning the machine annotator was kept at 95%.\nTable 5 shows that, for these datasets, generating extra data has a small impact on the results. Generating a synthetic corpus that is the same size as the original corpus yields downstream results that are within one standard deviation of the results from generating a four times larger corpus. This is true both for MEDDOCAN and for SEPR PHI."}, {"title": "4.6 Diversity of the Generated Data", "content": "Three different metrics were used to quantify the data themselves. These were lexical diversity, the length of the documents, and the number of entities in the documents. The metrics were calculated both for the generated corpora and for the gold"}, {"title": "4.7 Estimating Privacy", "content": "Creating a synthetic variant of a sensitive dataset only protects the original data if the synthetic and sensitive datasets are sufficiently different. A common proxy for measuring these risks is to study the n-grams of the original and synthetic corpora (Ive et al., 2020; Hiebel et al., 2023). We calculated the n-gram recall of each generated dataset and the training data from which it is derived. This metric measures the proportion of unique n-grams in a reference document that is shared with a candidate document\u00b2. In this experiment, the reference documents are the real sensitive corpora used for domain adaptation and the synthetic corpora are the candidates. In other words, given a real corpus with a set of n-grams R and a synthetic corpus with n-grams S:\n\n        n-gram recall = \\frac{|R \u2229 S|}{|R|} \\qquad(1)\n      \n\nSince the data in this study are tagged for PII, a PII-sensitive n-gram recall is also used to estimate the degree of leakage of potentially sensitive information. Instead of considering all n-grams R in the reference document, as in Equation 1, this metric only considers the n-grams R* \u2286 R that overlap with sensitive entities in the gold standard corpus.\n\nngram recall = \\frac{|R \u2229 S|}{|R|} \\qquad(1)\n\n\nn-gram recall values were calculated for n = {3, 5, 10}. This was done for each of the 5 folds in the previous experiments. The n-grams were created by concatenating n tokens from the tokenizers of the domain-adapted generative models. The values for 5-grams are summarized in Table 7 and the other values in Tables 10 and 11 in Appendix B.\nAll three configurations produced similar patterns. The bottom row of Table 7 shows the average 5-gram recall scores of the synthetic corpora that were generated without any domain adaptation. These values were obtained by comparing these corpora to the 95% training corpora. These values serve as a useful baseline since any shared n-grams in these cases are purely incidental.\nMore interestingly, the n-gram recall values decrease as more data are used for domain-adapting"}, {"title": "5 Discussion", "content": "The results in Table 2 show that domain adaptation to a point - was needed to generate synthetic data of sufficient quality. From a privacy perspective, there is, of course, a risk that domain adaptation causes sensitive data to be memorized and reproduced during data synthesis. The results in Table 7 indicate that a lower proportion of sensitive n-grams were reproduced when more data were used for domain adaptation. On the other hand, using fewer data minimizes the attack surface of the models. If the data requirements are lower, this may make it feasible to, for example, manually de-identify the data, or to audit an automatic de-identification of them.\nAn example of an application of our results is cross-institutional validation of NLP systems. In sensitive domains such as the clinical domain, researchers are often barred from sharing their data and trained models due to privacy concerns. A common is where research group (A) has created a system that works very well on their in-house data. Due to privacy regulations, another group (B) cannot share their data with (A), and this makes it difficult for (A) and (B) to validate if the system generalizes. An example of an attempt to work under these constraints is described by Bridal et al. (2022). They were able to make limited claims of generalization but where limited due to the restrictions on sharing data. The method for synthesis explored in our paper would allow these research groups to share synthetic versions of their datasets or models, as these are much less sensitive than artifacts based on real data."}, {"title": "6 Conclusions", "content": "Data synthesis is an attractive tool for dealing with data scarcity and privacy risks. However, synthesis itself can be challenging when access to data is constrained. Through extensive ablation studies validated on models and data in two different languages \u2013 our experiments show that not all parts of the synthesis process are equally sensitive to these resource constraints. Domain adapting the models to create high-utility clinical text did not require using all of the data. The experiments show that using between 25% and 50% of the data can be enough for domain adaptation, at least for these datasets."}, {"title": "7 Limitations", "content": "The results demonstrated the importance of adapting the synthesizing LLM to the clinical domain in order to generate high-utility training data. For synthesizing corpora for PII detection, this was possible to do with very small amounts of data. However, it may be the case that PII detection is a task where the domain-specific details of the data are less important. Indeed, Libbi et al. (2021) argue that NER data, in general, retain their utility for machine learning even if, qualitatively, their contents lack coherence. In future work, it would be interesting to investigate if the same holds for clinical tasks that are more challenging and domain-specific tasks. Nevertheless, PII detection in the clinical domain is an important task in itself and any advances in this area will help to combat the issue of data scarcity.\nOur process for synthesis also uses autoregressive language modeling without instruction-tuning. We opted for this design to make the task as simple as possible. As previously mentioned, this may not necessarily be a good design for other types of tasks where the document-level semantics matter more. For example, Kiefer (2024) synthesized data for the task of assigning diagnosis codes to discharge summaries by instruction-tuning models to create documents with different characteristics.\nAnother limitation of the study is the reliance on n-gram-based metrics for estimating privacy risks. This is a common practice (Ive et al., 2020; Hiebel et al., 2023) and can detect when data are being reproduced verbatim in the synthetic corpora. On the other hand, n-grams vary greatly in how sensitive they are. We try to address this with our n-gram recall metric that takes PII into account. However, we make limited claims about the privacy of the data and instead direct our focus towards their utility.\nIn Section 4.4, we find that smaller versions of the generative models could generate data of near-equal utility as their larger counterparts. This was especially clear when generating MEDDOCAN data. An interesting continuation would have been to fine-tune an even smaller LLM using MEDDOCAN data. Unfortunately, there is no smaller FLOR model than the 1.3 billion version that we use. GPT-SW3 is available in smaller versions, but proceeding to a monolingual analysis would lower the validity of the results and fell outside the scope of this study. Future work could try similar experiments with languages for which smaller models exist."}, {"title": "8 Ethical statement", "content": "This work was conducted under ethical permission no. 2019-05679 granted by Swedish Ethical Review Authority. MEDDOCAN is a publicly available dataset, where the PII in the documents are unrelated to the original patients. SEPR PHI is available on request and is a manually pseudonymized corpus where all identified PII have been replaced with surrogate values. Because of this, the privacy risks of the experiments in this paper are very small. Regardless, the experiments have been carried out in a computational environment in which only the authors and system administrators have had access to the data. Our experiments are also in accordance with the intended uses of the datasets.\nThe experiments conducted in this paper required considerable amounts of computational resources. We estimate that creating and evaluating the data and models for our experiments took approximately 130 GPU hours\u00b3. Luckily, the computational infrastructure on which the experiments ran is located in a country where virtually all energy comes from sustainable sources. Nevertheless, the electricity expended when conducting these experiments could have been directed toward alternative purposes.\nOn the other hand, our results indicate that high-utility synthetic corpora can be created using small-scale data and without relying on the very largest"}, {"title": "LLMs. These results can be particularly helpful for researchers working in resource-constrained environments. This includes not only researchers in, e.g., the clinical domain, but also those working with low-resource languages. These parts of the NLP community are often under-served as increasing focus is placed on terabyte-scale datasets and LLMs with unwieldy parameter sizes.\nThere are other strong ethical benefits of conducting this study. Training models using synthetic corpora is safer than using real data, but is no panacea. On the other hand, no currently existing technique for privacy preservation is sufficient when used in isolation. For example, NER-based automatic de-identification covers only a subset of PII (Pil\u00e1n et al., 2022), and differentially private learning for NLP is difficult to implement properly (Brown et al., 2022; Miranda et al., 2024) and is often inefficient when done so (Igamberdiev et al., 2024). Synthetic data generation will likely be an important ingredient in many domains to overcome privacy issues.\nThere is a risk that other researchers over-interpret our results and use them to justify irresponsible uses of synthetic data. Our focus on constraining the amounts of data used hopefully mediates some of these potential risks. Furthermore, we have been clear in the limitations of our results and clearly defined the scope of our experiments.", "content": null}]}