{"title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning", "authors": ["Heewoong Choi", "Sangwon Jung", "Hongjoon Ahn", "Taesup Moon"], "abstract": "In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-art baselines even with modest feedback budgets and enjoying robustness with respect to the number of feedbacks and feedback noise. Our code is available at https://github.com/chwoong/LiRE", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has demonstrated considerable success in various domains such as robotics (Haarnoja et al., 2018; Kalashnikov et al., 2018), game (Silver et al., 2017; Mnih et al., 2013; Vinyals et al., 2019), autonomous driving (Kiran et al., 2021), and real-world tasks (Tan et al., 2018; Chebotar et al., 2019). An essential component of RL is to define suitable and precise reward functions so that an RL agent can be trained successfully (Wirth et al., 2017).\nHowever, designing the reward function is time-consuming, especially if we want to align it with human intent (Hejna & Sadigh, 2024).\nThis shortcoming has led to research on learning the reward model from human feedback without explicitly designing the reward function. While expert demonstration is one type of human feedback (Abbeel & Ng, 2004), recent papers use preference feedback on which of a pair of trajectory segments is preferred since it is a significantly easier type of feedback to collect (Kaufmann et al., 2023; Casper et al., 2023). More specifically, the common approach for the Preference-based RL (PbRL) consists of two steps: (1) learn a reward model using preference feedback from trajectory segment pairs, then (2) apply ordinary RL algorithms with the learned reward model. After successfully training a robot agent with PbRL (Christiano et al., 2017), it was shown that novel behaviors aligned with human intent, e.g., backflips, can also be learned (Lee et al., 2021b), while learning such behavior would be extremely hard from explicitly hand-coded rewards. The PbRL framework has gained popularity in both online (Park et al., 2021; Liang et al., 2021) and offline (Kim et al., 2022; Shin et al., 2022; An et al., 2023; Hejna & Sadigh, 2024) settings, in which the former allows the agents to interact with their environments, while the latter does not.\nIn this paper, we focus on the offline PbRL setting, in which the goal is to find an optimal policy solely from the previously collected preference feedbacks on the pairs of trajectories obtained from some past, fixed policy. This setting is challenging since the preference feedback cannot be actively collected on the trajectories generated by the current, updated policy. Hence, developing effective methods for collecting maximally informative preference feedback data from the past policy as well as devising efficient reward learning schemes is indispensable.\nThe current norm is to collect ternary preference feedback (i.e., more/less/equally preferred) for independently sampled pairs of trajectories, and then employ the standard Bradley-Terry (BT) model (Bradley & Terry, 1952) on the collected data to learn the reward function. While the above approach was shown to be effective to some extent, a critical limitation also exists. Namely, due to the independent sampling of the"}, {"title": "2. Related Works", "content": "2.1. Offline Preference-based RL\nDue to the difficulty of defining rewards in reinforcement learning (Sutton & Barto, 2018; McKinney et al., 2023), PbRL uses comparison information between trajectories to learn a reward function (Christiano et al., 2017; F\u00fcrnkranz et al., 2012; Wilson et al., 2012; Akrour et al., 2012; Ouyang et al., 2022; Stiennon et al., 2020). However, the human preference feedback required for PbRL is expensive to obtain. Thus, several PbRL approaches have been devised to reduce the number of expensive human feedbacks, such as using additional expert demonstrations (Ibarz et al., 2018), meta-learning (Hejna III & Sadigh, 2023), semi-supervised learning or data augmentation (Park et al., 2021), unsupervised pre-training (Lee et al., 2021b), exploration based on reward uncertainty (Liang et al., 2021), and using sequen-\ntial preference ranking (Hwang et al., 2023). Offline PbRL assumes a more challenging problem setting where agents cannot interact with the environment, unlike online PbRL where preference feedback can be obtained while interacting with the environment.\nIn offline PbRL, the two kinds of data are provided: offline data obtained from an unknown policy and preference feedbacks on the pairs of trajectories. Also, traditional offline PbRL methods have two phases; they train a reward model using the preference feedback and then perform RL with the trained reward model without interacting with the environment. On the other hand, recent works propose performing offline PbRL without the reward model by directly optimizing policies (An et al., 2023; Kang et al., 2023), or learning state-action value function or regret from preference labels (Hejna & Sadigh, 2024; Hejna et al., 2023). However, due to the constraint of no interaction with the environment, obtaining the most informative preference feedback from the offline dataset is as important as developing a new training method without the reward model or designing the structure of the reward model well (e.g., non-Markovian reward modeling (Kim et al., 2022)). An active query selection method has been proposed to obtain informative preference pairs (Shin et al., 2022), but their method did not attempt to obtain second-order preference.\nMost offline PbRL papers have validated their algorithms on the D4RL dataset (Fu et al., 2020). However, it has been shown that typical offline RL algorithms can produce good policies on D4RL even with a completely wrong reward (e.g., zero, random, negative reward) due to the pessimism and survival instinct of offline RL algorithms (Shin et al., 2022; Li et al., 2023). Hence, to properly evaluate how well offline PbRL algorithms learn the reward model, we need to validate them on a new dataset, on which the policy cannot be easily learned due to survival instincts."}, {"title": "2.2. Second-order Preference Feedback", "content": "While typical approaches in PbRL only focus on the first-order preference (i.e., ternary labels including bad, equal, and good), several approaches in the NLP and RL domains have recently been proposed to utilize second-order preference about the relative difference between preferences. One approach is to directly obtain a relative rating for each trajectory pair (e.g., significantly better or slightly better) or an absolute rating for each trajectory (e.g., very good or good) (Touvron et al., 2023; Cao et al., 2021; White et al., 2023). However, the more granular the preferences are, the more expensive they are than just ternary labels.\nThere is a rich Learning-to-Rank literature that learns the ranking given second-order preference feedback in the form of absolute ratings (Burges et al., 2005; Xia et al., 2008; Xu & Li, 2007; Swezey et al., 2021), but they do not address how to obtain second-order preference only with ternary labels. Another approach is to obtain the second-order preference between samples from a fully-ranked list for multiple trajectories (Chen et al., 2022; Palan et al., 2019; Zhu et al., 2023; Song et al., 2024; Myers et al., 2022; B\u0131y\u0131k et al., 2019; Brown et al., 2019). However, they do not address how to efficiently obtain the fully-ranked list in terms of the number of feedbacks. Since naively constructing a fully-ranked list would require a large number of feedbacks that increase quadratically with the number of trajectories, developing a more efficient list construction method is crucial.\nAccordingly, some recent studies have developed how to obtain partially-ranked lists that only know the rankings among a few trajectories (Zhao et al., 2023; Hwang et al., 2023). Perhaps, one of the closest research to ours is Sequential Preference Ranking (SeqRank) (Hwang et al., 2023), which sequentially collects the preference feedback between a newly observed segment and a previously collected segment. However, since their method builds partially-ranked lists rather than fully-ranked lists, the short length of the lists limits the ability to fully utilize second-order information."}, {"title": "3. Preliminaries", "content": "An RL algorithm considers a Markov decision process (MDP) and aims to find the optimum policy that maximizes the cumulative discounted rewards. MDP is defined by a tuple (S, A, P, r, \u03b3) where S, A are state, action space, P = P(s'|s, a) is the environment transition dynamics, r = r(s,a) is reward function, and \u03b3 is discount factor. In offline PbRL, we assume that we do not know the true reward r, but we have a pre-collected dataset that is a set of tuples, Ds := {(s,a,s')|(s,a) ~ \u03bc,s' ~ P(\u00b7|s,a)}. In general, the policy \u03bc from which the data was collected is unknown. We are allowed to ask for preference feedbacks to obtain preference labels for two distinct trajectory segments sampled from Ds := {\u03c3|\u03c3 = (s0, a0, s1, a1, ..., sT\u22121, aT\u22121), (st, at, st+1) \u2208 D\uff61}. Annotators assign a ternary label l given a pair of segments \u03c31, \u03c32 \u2208 Ds; l = 0 indicates that \u03c3\u2081 is preferred over \u03c32 (i.e., \u03c3\u2081 > \u03c32), l = 1 indicates the opposite preference (i.e., \u03c31 < \u03c32), and l = 0.5 indicates that \u03c31 and \u03c32 are equally preferred (i.e., \u03c3\u2081 = \u03c32).\nThe goal of acquiring preference labels is to learn the unknown reward function. Conventional offline PbRL methods use a preference model that defines the probability that one segment is better than the other as\n\\(P_\\theta(\\sigma_1 > \\sigma_2) = \\frac{\\varphi(r_\\theta(\\sigma_1))}{\\varphi(r_\\theta(\\sigma_1)) + \\varphi(r_\\theta(\\sigma_2))}\\)   (1)\nin which \\(r_\\theta(\\sigma_i) = \\sum_{(s_t, a_t) \\in \\sigma_i} r_\\theta(s_t, a_t)\\) and \u03c6 is the parameter of the reward model. The score function \u03c6(x) = exp(x) is commonly used in the BT model (Bradley & Terry,"}, {"title": "4. LiRE: Listwise Reward Estimation", "content": "As mentioned in Section 1, the conventional offline PbRL approaches cannot utilize the second-order information of the preference feedback. In order to describe our method, we begin by stating the mild assumptions we make.\nAssumption 4.1. (Completeness) For any two segments \u03c3i, \u03c3j, the human feedbacks are provided in the following three ways, \u03c3i > \u03c3j or \u03c3i < \u03c3j or \u03c3i = \u03c3j.\n(Transitivity) For any three segments \u03c3i,\u03c3j, and \u03c3\u03ba, if \u03c3i = \u03c3j and \u03c3j = \u03c3\u03ba, then \u03c3i = \u03c3\u03ba. Also, if \u03c3i > \u03c3j and \u03c3j > \u03c3\u03ba, then \u03c3i > \u03c3\u03ba.\nRemarks: These assumptions are a generalization of SeqRank (Hwang et al., 2023) to include equal labels. While the transitivity assumption may not always hold in practice, we demonstrate that our method is robust both in the presence of feedback noise (Section 5.4.3) and in real human experiments (Section 5.5), even when the transitivity assumption may not hold."}, {"title": "4.1. Constructing a Ranked List of Trajectories (RLT)", "content": "Our goal is to obtain an RLT in which the segments \u03c3 are ordered by their level of preference. We represent RLT, L, in the following form:\nL = [g1 > g2 >\u2026\u2026> gs],\nin which gi = {\u03c3i\u2081,\u00b7\u00b7\u00b7, \u03c3ik} is a group of segments with the same preference level and s is the number of groups in the list. Namely, if m > n, we note any segment \u03c3i \u2208 gm is preferred over any segment \u03c3j \u2208 gn.\nSince we assume to have exactly the same type of ternary feedback defined in Section 3, we cannot build RLT by obtaining the listwise feedback at once. Hence, we construct by sequentially obtaining the labels as we describe below.\nWe start with an initial list [{\u03c31}] by selecting a random segment \u03c31 from Ds. We then repeat the process of sequentially sampling the new segment \u03c32, \u03c33,\u00b7\u00b7 \u2208 Ds and placing it in the appropriate position in the list until the feedback budget limit is reached. To place a newly sampled \u03c3i in the RLT, we compare it with a segment \u03c3\u03ba \u2208 gm for some group gm in the list and obtain the ternary preference feedback. Depending on the feedback, we proceed as follows:\n\u2022 If \u03c3i = \u03c3\u03ba, add \u03c3i to the group gm.\n\u2022 If \u03c3i < \u03c3\u03ba, find the position within g1,...,gm\u22121.\n\u2022 If \u03c3i > \u03c3\u03ba, find the position within gm+1,\u2026\u2026,gs.\nFor the latter two cases, we use a binary search so that we can recursively find the correct group for each segment. Namely, the RLT construction algorithm is based on a binary insertion sort and the pseudocode is summarized in Algorithm 1 (Appendix). We note that while we can also adopt merge sort or quick sort to construct an RLT after collecting multiple segments, if we already have a partially constructed RLT, binary insertion sort would be more feedback-efficient.\nFeedback efficiency and sample diversity Note that by design, we need to obtain multiple preference feedbacks for each new segment \u03c3i. Therefore, for a fixed feedback budget, our method samples fewer segments. However, from the constructed RLT, we can generate many preference pairs by exploiting the second-order information encoded in the list; namely, \u03c3i is preferred to all the segments in the groups that rank lower than the group that \u03c3i belongs to.\nTo that end, we analyze the feedback efficiency and sample diversity of RLT. Feedback efficiency is defined in SeqRank (Hwang et al., 2023) as the ratio of the number of total preference pairs generated to the number of preference feedbacks obtained. We also define sample diversity as the ratio of the total number of sampled segments to the number of preference feedbacks obtained. Suppose we obtain preference feedbacks until we collect a total of M segments in the preference dataset. Constructing an RLT with M segments requires O(M log M) feedbacks because we use an efficient sorting method based on binary search. In this case, the number of all possible preference pairs (including ties) that can be generated from the RLT is \\(O(M^2)\\). Table 1 summarizes the feedback efficiency and sample diversity of independent pairwise sampling, SeqRank, and RLT. Note our method has a faster rate of increase in the feedback efficiency even with diminishing sample diversity as the number of segments M in RLT increases.\nConstructing multiple RLTs Algorithm 1 places all the segments in a single ranked list. Instead of constructing one long list, we devise a variant that generates multiple lists by setting a limit (Q) on the feedback budget for each list. The reason for generating multiple lists is that as the length of the list increases, the number of preference feedbacks required by the binary search process increases. Hence, we increase the sample diversity within the total feedback budget by generating multiple RLTs."}, {"title": "4.2. Listwise Reward Estimation from RLT", "content": "Once the RLT is constructed, we construct the preference dataset, D\u2081 = {(\u03c3i1, \u03c3i2, li)}{1 with all the pairs we can obtain from the RLT. Specifically, when \u03c3i1 \u2208 gm and \u03c3i2 \u2208 gn, the preference label li is as follows: li = 0.5 if m = n, li = 0 if m > n, and li = 1 if m < n. The key difference from traditional pairwise PbRL methods is that, instead of independently sampling segment pairs, we derive preference pairs from the RLT. To compare with the independent sampling, suppose that the RLT has segments with the relationship, \u03c3a < \u03c3b < \u03c3c. If we sample all pairs from the RLT, then (\u03c3\u03b1, \u03c3b, 1), (\u03c3b, \u03c3c, 1), (\u03c3\u03b1, \u03c3c, 1) \u2208 D\u03b9. From these preference pairs, it can be inferred that the degree to which \u03c3c is preferred over \u03c3\u03b1 is stronger than the degree to which \u03c3c is preferred over \u03c3b. Consequently, the reward model trained with pairwise loss in (2) can learn second-order preference between each pair of segments. In contrast, the reward model learned from independent sampling cannot learn second-order preference because each segment is not compared to multiple other segments.\nWe use pairwise loss in our main experiments, but we can also train the reward model with listwise loss since the segments are ranked in the RLT. To train the reward model with listwise loss, we assume the segments follow a Plackett-Luce model (Plackett, 1975) which defines the probability distribution of objects in a ranked list. We discuss listwise loss more in detail in Appendix A.3 but, our experimental results show that training with pairwise loss performs better than listwise loss in most cases.\nOur proposed LiRE trains the reward model with linear score function \u03c6(x) = x in (1). The choice of linear score function has the same effect as setting the reward to be the exponent of the optimal reward value obtained through training with an exponential score function \u03c6(x) = exp(x). Therefore, the linear score function amplifies the difference in reward values, particularly in regions with high reward values, compared to the exponential score function.\nBounding reward model If \u03c6(x) = exp(x), then adding a constant to the reward function re does not affect the resulting probability distribution. To align the scaling of the learned re in ensemble reward models, a common choice for the reward model is using the Tanh activation, i.e., \u1fd2\u03bf(\u03c3) = \u2211tro(st,at) = \u2211ttanh(fo(st, at)) (Lee et al., 2021b; Hejna & Sadigh, 2024), to bound the output of the reward model.\nIn the case of \u03c6(x) = x, scaling the reward function by a constant does not affect the probability distribution. Similarly, we use the same Tanh activation function for \u03c6(x) = x to bound the output of the reward model. Specifically, we set \u0390\u03bf(\u03c3) = \u2211t(1 + tanh(fo(st, at))) > 0 to ensure that the probability defined in (1) is positive."}, {"title": "5. Experimental Results", "content": "5.1. Settings\nDataset Previous offline PbRL papers are evaluated mainly on D4RL (Fu et al., 2020), but D4RL has the problem that RL performance can be high even when wrong rewards are used (Li et al., 2023; Shin et al., 2022). To that end, we newly collect the offline PbRL dataset with Meta-World (Yu et al., 2020) and DeepMind Control Suite (DMControl) (Tassa et al., 2018) following the protocols of previous work: medium-replay dataset, e.g., (Yu et al., 2021a; Mazoure et al., 2023; Gulcehre et al., 2020) and medium-expert dataset, e.g., (Yu et al., 2021b; Sinha et al., 2022; Hejna & Sadigh, 2024; Li et al., 2023).\nThe medium-replay dataset collects data from replay buffers used in online RL algorithms, such as the SAC (Haarnoja et al., 2018), and the medium-expert dataset collects trajectories generated by the noisy perturbed expert policy. We experiment on both datasets while our main analyses are done on medium-replay; see Appendix C.2 for complete details on constructing them. The prior works (Shin et al., 2022; Zhang, 2023) have created datasets that consider survival instinct. However, their dataset was evaluated with only 100 or fewer preference feedbacks, whereas we use 500, 1000, or more feedbacks.\nBaselines In our experiments, we consider five baselines: Markovian Reward (MR), Preference Transformer (PT) (Kim et al., 2022), Offline Preference-based Reward Learning (OPRL) (Shin et al., 2022), Inverse Preference Learning (IPL) (Hejna & Sadigh, 2024), and Direct Preference-based Policy Optimization (DPPO) (An et al., 2023). MR refers to the method trained with the MLP layer with the Markovian reward assumption, which is the baseline model used in PT. OPRL learns multiple reward models to select the query actively with the highest preference disagreement. Lastly, IPL and DPPO are algorithms that learn policies without the reward model.\nAll the above five baselines belong to pairwise PbRL because they all train based on the BT model given first-order preference feedbacks sampled as independent pairs. In addition to pairwise PbRL, we also compare with the sequential pairwise comparison method proposed by SeqRank (Hwang et al., 2023).\nImplementation details For LiRE, we use the linear score function and set Q = 100 as the default feedback budget for each list. Therefore, if the total number of feedbacks is 500, then five RLTs will be constructed. All baseline methods, including ours, can be applied to any offline RL algorithm, but, as in previous works, we use IQL (Kostrikov et al., 2021). The hyperparameters for each algorithm and the criteria for the equally preferred label threshold of scripted teacher can be found in the Appendix C.4."}, {"title": "5.3. Ablation Studies of LIRE", "content": "5.3.1. Factors of performance improvement\nWe conduct an ablation study to verify if the performance improvement of LiRE is due to two factors: the linear score function and the RLT construction. Table 3 demonstrates that using the linear score function (bottom two rows) clearly"}, {"title": "5.4. Additional Analyses of LiRE", "content": "5.4.1. Varying the number of feedbacks\nWe evaluate how the performances of the offline PbRL algorithms are affected by the number of feedbacks. Namely, we measure the average success rate of the sweep-into, box-close, and button-press-topdown-wall tasks of the medium-replay dataset while varying the number of the preference feedbacks from 50 to 2000. We note that most previous works (Kim et al., 2022; Hejna & Sadigh, 2024; An et al., 2023) using D4RL only use up to 500 preference feedbacks. As shown in Figure 3, we observe that the typical baseline, MR with exponential score function (denoted as MR w/ exp), cannot achieve a success rate higher than 50% for all three tasks even with 2000 preference feedbacks. When we instead use the linear score function, we observe that MR w/ linear performs much better than MR w/ exp, but the success rates sometimes still remain to be low (e.g., box-close with 500 feedbacks and button-press-topdown-wall for most of the times). In contrast, it is evident that LiRE mostly surpasses the two baselines with large margins, even with fewer number of preference feedbacks. Specifically, for the button-press-topdown-wall task, LiRE with only 100 feedbacks outperforms not only the baselines with 2000 feedbacks but also the policy learned using the GT reward. Again, we can confirm that the high feedback efficiency enabled by RLT makes LiRE very effective even with a smaller number of feedbacks."}, {"title": "5.4.2. Varying Q budget", "content": "In Section 4.1, we described that multiple RLTs can be constructed by putting the budget limit (Q) in order to increase the sample diversity. In this subsection, we show the effect of Q. Table 4 shows the performance change of LiRE"}, {"title": "5.4.3. Robustness to Feedback Noise", "content": "If the preference feedback used in PbRL models human preference labeling, it would be reasonable to assume that the preference feedback may be noisy. To that end, we experiment to assess the robustness of the offline PbRL performance of LiRE with respect to the preference feedback noise. We assume that the preference feedback can be noisy with probability p (i.e., if l\u2081 = 0 or 1, the label is flipped to 1 \u2212 li with probability p, and for tie labels, we flip to li = 0 or 1 with probability p/2, respectively). We varied the noise probability p from 0 to 0.3, and Figure 4 compares the success rates of MR w/ linear and LiRE. From the figure, we confirm that the performance of LiRE does not drop as severely as MR w/ linear when p increases. In particular, for lever-pull task, we observe that LiRE with feedback noise of p = 0.3 even results in a higher success rate than MR w/ linear with no noise, highlighting the robustness of LiRE with respect to feedback noise."}, {"title": "5.4.4. Impact of Feedback Granularity", "content": "In Figure 5, we compare the performance of LiRE based on the threshold that determines the tie between the segments. Specifically, we adjust the threshold value for the reward difference that indicates whether two segments are equally preferred. Namely, a higher threshold value means that more segment pairs are labeled as equally preferred, result-"}, {"title": "5.4.5. Comparison with SeqRank", "content": "Here, we compare LiRE with SeqRank, which also utilizes partially-ranked lists. We also employed the linear score function for SeqRank since it gave better results than using the exponential function and led to a fair comparison with LiRE. We evaluate the average success rates of SeqRank and LiRE on the Meta-World medium-replay dataset.\nThe experimental results in Table 5 show that LiRE clearly achieves higher performance than SeqRank. We argue that SeqRank does not fully utilize the second-order information because SeqRank does not construct a fully-ranked list. Indeed, the third column of Table 5 shows that the number of groups in the ranked lists averages less than 3 with the SeqRank, whereas it increases to about 9 on average with LiRE. The last two columns of Table 5 compare feedback efficiency and sample diversity. LiRE achieves a sample diversity of approximately 0.47 through the use of binary search, and the feedback efficiency increases significantly to 11.33 by constructing RLT. Additionally, Table 6 shows the superiority of LiRE over SeqRank on the DM-Control medium-replay dataset. We note SeqRank also performs similarly to MR w/ linear on walker-walk and humanoid-walk tasks, while LiRE achieves much higher performance gains on all three tasks. Thus, we confirm that"}, {"title": "5.4.6. Compatibility with other methods", "content": "To check the compatibility of LiRE with other methods, we tested the performance of LiRE when combined with OPRL and PT, respectively. First, to apply OPRL and LiRE simultaneously, we trained a reward model each time an RLT was newly constructed, and then actively sampled based on the disagreement of the reward models (following the method of OPRL) when constructing the next RLT. In Figure 6(a), we observe that LiRE+OPRL outperforms LiRE in sweep-into and lever-pull tasks but performs worse in button-press-topdown task. This discrepancy suggests that while the OPRL method enhances the consistency of the reward model, it may lead to oversampling similar segments that are challenging to distinguish depending on the task. Second, as shown in Figure 6(b), LiRE does not necessarily gain improvements when combined with PT. That is, since PT was originally designed to capture temporal dependencies of segments in reward modeling, it seems to struggle in accurately capturing the second-order preference information from RLT possibly due to overfitting to the sequence of past segments."}, {"title": "5.5. Human Experiments", "content": "Table 7 presents the results with real human preference feedback on the new button-press-topdown offline RL dataset, which is distinct from the dataset used in Table 2. Namely, we collected 200 preference feedbacks from one of the authors for each of the three feedback collection methods: MR, SeqRank, and LiRE. For LiRE, we used the feedback budget of Q = 100, resulting in two RLTs. The results again indicate that LiRE dominates other baselines and gets stronger when the linear score function is used. We believe this result shows the potential of LiRE that it can be very effective in practical scenarios with real human preference feedback, as in LLM alignment."}, {"title": "6. Limitation", "content": "We believe there are two limitations of LiRE. First, LiRE lacks the ability to parallelize the construction of RLT since there are dependencies between the order in which feedbacks are obtained to construct a fully-ranked list. Therefore, in scenarios where parallel feedback collection is feasible, constructing an RLT could be more time-consuming compared to collecting preference feedbacks independently in pairs. Nevertheless, the results presented in Appendix A.2 show that LiRE with only 200 feedbacks outperforms the independent pairwise sampling method using 1000 feedbacks, suggesting the importance of constructing RLT. Second, LiRE relies on the transitivity assumption outlined in Assumption 4.1. Although our experiments with feedback noise indicate LiRE's robustness to noise that violates this assumption, transitivity violations can occur even with noiseless labels in real-world applications. This issue is not unique to LiRE but affects other preference-based RL methods as well. Addressing transitivity violation remains a challenge for scalar reward models, so future research could explore solutions by using multi-dimensional preference feedback to construct RLTs for each dimension."}, {"title": "7. Concluding Remarks", "content": "In this paper, we propose a novel Listwise Reward Estimation (LiRE) method for offline preference-based RL. While obtaining second-order preference from a traditional framework is challenging, we demonstrate that LiRE efficiently exploits second-order preference by constructing an RLT using ordinary, simple ternary feedback. Our experiments demonstrate the significant performance gains achieved by LiRE on our new offline PbRL dataset, specifically designed to objectively reflect the effect of estimated rewards. Notably, the reward model trained with LiRE outperforms traditional pairwise feedback methods, even with fewer preference feedbacks, highlighting the importance of second-order preference information. Moreover, our findings suggest that constructing ranked lists can be straightforward without complex second-order preference feedback, indicating the broad applicability of LiRE to more challenging tasks and real-world applications."}, {"title": "Impact Statement", "content": "We believe our LiRE can be potentially applied to aligning the RL agent with more fine-grained human intent and preference. Such applications can bring significant societal consequences by enhancing the precision and effectiveness of AI systems in various fields such as health care and education. By ensuring that AI systems more closely reflect and respond to the detailed intentions of their users, LiRE has the potential to foster trust and acceptance of AI technologies, ultimately contributing to their more widespread and ethical adoption."}]}