{"title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning", "authors": ["Heewoong Choi", "Sangwon Jung", "Hongjoon Ahn", "Taesup Moon"], "abstract": "In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-art baselines even with modest feedback budgets and enjoying robustness with respect to the number of feedbacks and feedback noise. Our code is available at https://github.com/chwoong/LiRE", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has demonstrated considerable success in various domains such as robotics (Haarnoja et al., 2018; Kalashnikov et al., 2018), game (Silver et al., 2017; Mnih et al., 2013; Vinyals et al., 2019), autonomous driving (Kiran et al., 2021), and real-world tasks (Tan et al., 2018; Chebotar et al., 2019). An essential component of RL is to define suitable and precise reward functions so that an RL agent can be trained successfully (Wirth et al., 2017).\nHowever, designing the reward function is time-consuming, especially if we want to align it with human intent (Hejna &\nSadigh, 2024).\nThis shortcoming has led to research on learning the reward model from human feedback without explicitly designing\nthe reward function. While expert demonstration is one type of human feedback (Abbeel & Ng, 2004), recent papers use preference feedback on which of a pair of trajectory segments is preferred since it is a significantly easier type of feedback to collect (Kaufmann et al., 2023; Casper et al., 2023). More specifically, the common approach for the Preference-based RL (PbRL) consists of two steps: (1) learn a reward model using preference feedback from trajectory segment pairs, then (2) apply ordinary RL algorithms with the learned reward model. After successfully training a robot agent with PbRL (Christiano et al., 2017), it was shown that novel behaviors aligned with human intent, e.g., backflips, can also be learned (Lee et al., 2021b), while learning such behavior would be extremely hard from explicitly hand-coded rewards. The PbRL framework has gained popularity in both online (Park et al., 2021; Liang et al., 2021) and offline (Kim et al., 2022; Shin et al., 2022; An et al., 2023; Hejna & Sadigh, 2024) settings, in which the former allows the agents to interact with their environments, while the latter does not.\nIn this paper, we focus on the offline PbRL setting, in which the goal is to find an optimal policy solely from the previously collected preference feedbacks on the pairs of trajectories obtained from some past, fixed policy. This setting is challenging since the preference feedback cannot be actively collected on the trajectories generated by the current, updated policy. Hence, developing effective methods for collecting maximally informative preference feedback data from the past policy as well as devising efficient reward learning schemes is indispensable.\nThe current norm is to collect ternary preference feedback (i.e., more/less/equally preferred) for independently sampled pairs of trajectories, and then employ the standard Bradley-Terry (BT) model (Bradley & Terry, 1952) on the collected data to learn the reward function. While the above approach was shown to be effective to some extent, a critical limitation also exists. Namely, due to the independent sampling of the"}, {"title": "2. Related Works", "content": "2.1. Offline Preference-based RL\nDue to the difficulty of defining rewards in reinforcement learning (Sutton & Barto, 2018; McKinney et al., 2023),\nPbRL uses comparison information between trajectories to learn a reward function (Christiano et al., 2017; F\u00fcrnkranz et al., 2012; Wilson et al., 2012; Akrour et al., 2012; Ouyang et al., 2022; Stiennon et al., 2020). However, the human preference feedback required for PbRL is expensive to obtain. Thus, several PbRL approaches have been devised to reduce the number of expensive human feedbacks, such as using additional expert demonstrations (Ibarz et al., 2018), meta-learning (Hejna III & Sadigh, 2023), semi-supervised learning or data augmentation (Park et al., 2021), unsupervised pre-training (Lee et al., 2021b), exploration based on reward uncertainty (Liang et al., 2021), and using sequen-\ntial preference ranking (Hwang et al., 2023). Offline PbRL assumes a more challenging problem setting where agents cannot interact with the environment, unlike online PbRL where preference feedback can be obtained while interacting with the environment.\nIn offline PbRL, the two kinds of data are provided: offline data obtained from an unknown policy and preference feedbacks on the pairs of trajectories. Also, traditional offline PbRL methods have two phases; they train a reward model using the preference feedback and then perform RL with the trained reward model without interacting with the environment. On the other hand, recent works propose performing offline PbRL without the reward model by directly optimizing policies (An et al., 2023; Kang et al., 2023), or learning state-action value function or regret from preference labels (Hejna & Sadigh, 2024; Hejna et al., 2023). However, due to the constraint of no interaction with the environment, obtaining the most informative preference feedback from the offline dataset is as important as developing a new training method without the reward model or designing the structure of the reward model well (e.g., non-Markovian reward modeling (Kim et al., 2022)). An active query selection method has been proposed to obtain informative preference pairs (Shin et al., 2022), but their method did not attempt to obtain second-order preference.\nMost offline PbRL papers have validated their algorithms on the D4RL dataset (Fu et al., 2020). However, it has been shown that typical offline RL algorithms can produce good policies on D4RL even with a completely wrong reward (e.g., zero, random, negative reward) due to the pessimism and survival instinct of offline RL algorithms (Shin et al., 2022; Li et al., 2023). Hence, to properly evaluate how well offline PbRL algorithms learn the reward model, we need to validate them on a new dataset, on which the policy cannot be easily learned due to survival instincts.", "2": null}, {"title": "2.2. Second-order Preference Feedback", "content": "While typical approaches in PbRL only focus on the first-order preference (i.e., ternary labels including bad, equal, and good), several approaches in the NLP and RL domains have recently been proposed to utilize second-order preference about the relative difference between preferences. One approach is to directly obtain a relative rating for each trajectory pair (e.g., significantly better or slightly better) or an absolute rating for each trajectory (e.g., very good or good) (Touvron et al., 2023; Cao et al., 2021; White et al., 2023). However, the more granular the preferences are, the more expensive they are than just ternary labels.\nThere is a rich Learning-to-Rank literature that learns the ranking given second-order preference feedback in the form of absolute ratings (Burges et al., 2005; Xia et al., 2008; Xu & Li, 2007; Swezey et al., 2021), but they do not address"}, {"title": "3. Preliminaries", "content": "An RL algorithm considers a Markov decision process (MDP) and aims to find the optimum policy that maximizes the cumulative discounted rewards. MDP is defined by a tuple $(S, A, P, r, \\gamma)$ where S, A are state, action space, $P = P(\\cdot | s, a)$ is the environment transition dynamics, $r = r(s,a)$ is reward function, and \\gamma is discount factor. In offline PbRL, we assume that we do not know the true reward r, but we have a pre-collected dataset that is a set of tuples, $D_{\\circ} := \\{(s,a,s')|(s,a) \\sim \\mu, s' \\sim P(\\cdot | s, a)\\}$. In general, the policy \\mu from which the data was collected is unknown. We are allowed to ask for preference feedbacks to obtain preference labels for two distinct trajectory segments sampled from $D_s := \\{\\sigma | \\sigma = (s_0, a_0, s_1, a_1, \\ldots, s_{T-1}, a_{T-1}), (s_t, a_t, s_{t+1}) \\in D_{\\circ}\\}$. Annotators assign a ternary label l given a pair of segments $\\sigma_1, \\sigma_2 \\in D_s$; $l = 0$ indicates that $\\sigma_1$ is preferred over $\\sigma_2$ (i.e., $\\sigma_1 \\succ \\sigma_2$), $l = 1$ indicates the opposite preference (i.e., $\\sigma_1 \\prec \\sigma_2$), and $l = 0.5$ indicates that $\\sigma_1$ and $\\sigma_2$ are equally preferred (i.e., $\\sigma_1 = \\sigma_2$).\nThe goal of acquiring preference labels is to learn the unknown reward function. Conventional offline PbRL methods use a preference model that defines the probability that one segment is better than the other as\n$P_{\\theta}(\\sigma_1 \\succ \\sigma_2) = \\frac{\\varphi(r_{\\theta}(\\sigma_1))}{\\varphi(r_{\\theta}(\\sigma_1)) + \\varphi(r_{\\theta}(\\sigma_2))}$ (1)\nin which $r_{\\theta}(\\sigma_i) = \\sum_{(s_t, a_t) \\in \\sigma_i} r_{\\theta}(s_t, a_t)$ and $\\theta$ is the parameter of the reward model. The score function $\\varphi(x) = \\exp(x)$ is commonly used in the BT model (Bradley & Terry,"}, {"title": "4. LiRE: Listwise Reward Estimation", "content": "As mentioned in Section 1, the conventional offline PbRL approaches cannot utilize the second-order information of the preference feedback. In order to describe our method, we begin by stating the mild assumptions we make.\nAssumption 4.1. (Completeness) For any two segments $\\sigma_i, \\sigma_j$, the human feedbacks are provided in the following three ways, $\\sigma_i \\succ \\sigma_j$ or $\\sigma_i \\prec \\sigma_j$ or $\\sigma_i = \\sigma_j$.\n(Transitivity) For any three segments $\\sigma_i, \\sigma_j$, and $\\sigma_k$, if $\\sigma_i = \\sigma_j$ and $\\sigma_j = \\sigma_k$, then $\\sigma_i = \\sigma_k$. Also, if $\\sigma_i \\succ \\sigma_j$ and $\\sigma_j \\succ \\sigma_k$, then $\\sigma_i \\succ \\sigma_k$.\nRemarks: These assumptions are a generalization of SeqRank (Hwang et al., 2023) to include equal labels. While the transitivity assumption may not always hold in practice, we demonstrate that our method is robust both in the presence of feedback noise (Section 5.4.3) and in real human experiments (Section 5.5), even when the transitivity assumption may not hold.", "4": null}, {"title": "4.1. Constructing a Ranked List of Trajectories (RLT)", "content": "Our goal is to obtain an RLT in which the segments \\sigma are ordered by their level of preference. We represent RLT, L, in the following form:\n$L = [g_1 \\prec g_2 \\prec \\ldots \\prec g_s]$,\nin which $g_i = {\\sigma_{i_1}, \\cdots, \\sigma_{i_{k_i}}}$ is a group of segments with the same preference level and s is the number of groups in the list. Namely, if m > n, we note any segment $\\sigma_i \\in g_m$ is preferred over any segment $\\sigma_j \\in g_n$.\nSince we assume to have exactly the same type of ternary feedback defined in Section 3, we cannot build RLT by obtaining the listwise feedback at once. Hence, we construct by sequentially obtaining the labels as we describe below.\nWe start with an initial list $[{\\sigma_1}]$ by selecting a random segment $\\sigma_1$ from $D_s$. We then repeat the process of sequentially sampling the new segment $\\sigma_2, \\sigma_3, \\cdots \\in D_s$ and placing it in the appropriate position in the list until the feedback budget limit is reached. To place a newly sampled $\\sigma_i$ in the RLT, we compare it with a segment $\\sigma_k \\in g_m$ for some group $g_m$ in the list and obtain the ternary preference feedback. Depending on the feedback, we proceed as follows:"}, {"title": "4.2. Listwise Reward Estimation from RLT", "content": "Once the RLT is constructed, we construct the preference dataset, $D_l = \\{(\\sigma_{i_1}, \\sigma_{i_2}, l_i)\\}_{i=1}^{K}$ with all the pairs we can obtain from the RLT. Specifically, when $\\sigma_{i_1} \\in g_m$ and $\\sigma_{i_2} \\in g_n$, the preference label $l_i$ is as follows: $l_i = 0.5$ if $m = n$, $l_i = 0$ if $m > n$, and $l_i = 1$ if $m < n$. The key difference from traditional pairwise PbRL methods is that, instead of independently sampling segment pairs, we derive preference pairs from the RLT. To compare with the independent sampling, suppose that the RLT has segments with the relationship, $\\sigma_a < \\sigma_b < \\sigma_c$. If we sample all pairs from the RLT, then $(\\sigma_a, \\sigma_b, 1), (\\sigma_b, \\sigma_c, 1), (\\sigma_a, \\sigma_c, 1) \\in D_l$. From these preference pairs, it can be inferred that the degree to which $\\sigma_c$ is preferred over $\\sigma_a$ is stronger than the degree to which $\\sigma_c$ is preferred over $\\sigma_b$. Consequently, the reward model trained with pairwise loss in (2) can learn second-order preference between each pair of segments. In contrast, the reward model learned from independent sampling cannot learn second-order preference because each segment is not compared to multiple other segments.\nWe use pairwise loss in our main experiments, but we can also train the reward model with listwise loss since the segments are ranked in the RLT. To train the reward model with listwise loss, we assume the segments follow a Plackett-Luce model (Plackett, 1975) which defines the probability distribution of objects in a ranked list. We discuss listwise loss more in detail in Appendix A.3 but, our experimental results show that training with pairwise loss performs better than listwise loss in most cases.\nOur proposed LiRE trains the reward model with linear score function $\\varphi(x) = x$ in (1). The choice of linear score function has the same effect as setting the reward to be the exponent of the optimal reward value obtained through training with an exponential score function $\\varphi(x) = \\exp(x)$. Therefore, the linear score function amplifies the difference in reward values, particularly in regions with high reward values, compared to the exponential score function.\nBounding reward model If $\\varphi(x) = \\exp(x)$, then adding a constant to the reward function $r_{\\theta}$ does not affect the resulting probability distribution. To align the scaling of the learned $r_{\\theta}$ in ensemble reward models, a common choice for the reward model is using the Tanh activation, i.e., $\\hat{r}_\\theta(\\sigma) = \\sum_t r_{\\theta}(s_t, a_t) = \\sum_t \\tanh(f_{\\theta}(s_t, a_t))$ (Lee et al., 2021b; Hejna & Sadigh, 2024), to bound the output of the reward model.\nIn the case of $\\varphi(x) = x$, scaling the reward function by a constant does not affect the probability distribution. Similarly, we use the same Tanh activation function for $\\varphi(x) = x$ to bound the output of the reward model. Specifically, we set $\\hat{r}_\\theta(\\sigma) = \\sum_t(1 + \\tanh(f_{\\theta}(s_t, a_t))) > 0$ to ensure that the probability defined in (1) is positive."}, {"title": "5. Experimental Results", "content": "5.1. Settings\nDataset Previous offline PbRL papers are evaluated mainly on D4RL (Fu et al., 2020), but D4RL has the problem that RL performance can be high even when wrong rewards are used (Li et al., 2023; Shin et al., 2022). To that end, we newly collect the offline PbRL dataset with Meta-World (Yu et al., 2020) and DeepMind Control Suite (DMControl) (Tassa et al., 2018) following the protocols of previous work: medium-replay dataset, e.g., (Yu et al., 2021a; Mazoure et al., 2023; Gulcehre et al., 2020) and medium-expert dataset, e.g., (Yu et al., 2021b; Sinha et al., 2022; Hejna & Sadigh, 2024; Li et al., 2023).\nThe medium-replay dataset collects data from replay buffers used in online RL algorithms, such as the SAC (Haarnoja et al., 2018), and the medium-expert dataset collects trajectories generated by the noisy perturbed expert policy. We experiment on both datasets while our main analyses are done on medium-replay; see Appendix C.2 for complete details on constructing them. The prior works (Shin et al., 2022; Zhang, 2023) have created datasets that consider survival instinct. However, their dataset was evaluated with only 100 or fewer preference feedbacks, whereas we use 500, 1000, or more feedbacks.\nBaselines In our experiments, we consider five baselines: Markovian Reward (MR), Preference Transformer (PT) (Kim et al., 2022), Offline Preference-based Reward Learning (OPRL) (Shin et al., 2022), Inverse Preference Learning (IPL) (Hejna & Sadigh, 2024), and Direct Preference-based Policy Optimization (DPPO) (An et al., 2023). MR refers to the method trained with the MLP layer with the Markovian reward assumption, which is the baseline model used in PT. OPRL learns multiple reward models to select the query actively with the highest preference disagreement. Lastly, IPL and DPPO are algorithms that learn policies without the reward model.\nAll the above five baselines belong to pairwise PbRL because they all train based on the BT model given first-order preference feedbacks sampled as independent pairs. In addition to pairwise PbRL, we also compare with the sequential pairwise comparison method proposed by SeqRank (Hwang et al., 2023).\nImplementation details For LiRE, we use the linear score function and set Q = 100 as the default feedback budget for each list. Therefore, if the total number of feedbacks is 500, then five RLTs will be constructed. All baseline methods, including ours, can be applied to any offline RL algorithm, but, as in previous works, we use IQL (Kostrikov et al., 2021). The hyperparameters for each algorithm and the criteria for the equally preferred label threshold of scripted teacher can be found in the Appendix C.4."}, {"title": "5.2. Evaluation on the Offline PbRL Benchmark", "content": "We compare LiRE with the baselines mainly on the Meta-World medium-replay dataset. Table 2 summarizes the results of offline RL performance using ground-truth (GT) rewards and preference feedbacks respectively. For many tasks, such as button-press-topdown and box-close, MR performs poorly compared to training with GT rewards, even with 1000 preference feedbacks. The problem of poor performance remains even if we replace the reward model with a more complex transformer architecture, PT. PT improves performance in dial-turn and lever-pull tasks, but for other tasks, the performance worsens. OPRL generally performs better than MR due to the increased consistency of the reward models, but the performance improvement is small. Lastly, DPPO and IPL perform better than MR on only a few tasks. We note that existing offline PbRL methods are rarely better than MR when validated on our new dataset.\nIn contrast, LiRE shows a significant performance improvement over MR except for the sweep task. We demonstrate the importance of RLT and the linear score function by achieving high performance even when compared to SeqRank, which is also not an independent pairwise method. In addition, policies trained with preference data outperform policies trained with GT rewards on the button-press-topdown-wall task, suggesting that reward models trained with preference data may be more effective, as"}, {"title": "5.3. Ablation Studies of LIRE", "content": "5.3.1. Factors of Performance Improvement\nWe conduct an ablation study to verify if the performance improvement of LiRE is due to two factors: the linear score function and the RLT construction. Table 3 demonstrates that using the linear score function (bottom two rows) clearly"}, {"title": "5.3.2. Effect of RLT and Score Function on Reward Estimation", "content": "We examine the estimated reward values of the learned reward models. Figure 2 scatter plots the estimated rewards (y-axis), learned with 500 preference feedbacks, of the segments in box-close task against the GT rewards (x-axis). Note our LiRE uses fewer segments to train the reward model, so Figure 2(b) contains fewer dots than Figure 2(a). Each segment has a length of 25 and both GT and the estimated rewards are normalized to values between [0, 25].\nFrom the figure, we clearly observe that the estimated rewards in Figure 2(b) are more highly correlated than those in Figure 2(a). Namely, by constructing the RLT, LiRE exploits the second-order preference, and the high and low reward segments are more clearly distinguished by the reward estimates than vanilla MR. Additionally, when training the reward model with the linear score function, there is a larger gap in the estimated rewards within the reward region for higher GT rewards, as shown in Figure 2(d). We speculate that using the linear score function and RLT makes the estimated reward discern the optimal and suboptimal segments (with respect to the GT rewards) more clearly, hence, the policy learned with the estimated reward turns out to perform much better."}, {"title": "5.4. Additional Analyses of LiRE", "content": "5.4.1. Varying the Number of Feedbacks\nWe evaluate how the performances of the offline PbRL algorithms are affected by the number of feedbacks. Namely, we measure the average success rate of the sweep-into, box-close, and button-press-topdown-wall tasks of the medium-replay dataset while varying the number of the preference feedbacks from 50 to 2000. We note that most previous works (Kim et al., 2022; Hejna & Sadigh, 2024; An et al., 2023) using D4RL only use up to 500 preference feedbacks. As shown in Figure 3, we observe that the typical baseline, MR with exponential score function (denoted as MR w/ exp), cannot achieve a success rate higher than 50% for all three tasks even with 2000 preference feedbacks. When we instead use the linear score function, we observe that MR w/ linear performs much better than MR w/ exp, but the success rates sometimes still remain to be low (e.g., box-close with 500 feedbacks and button-press-topdown-wall for most of the times). In contrast, it is evident that LiRE mostly surpasses the two baselines with large margins, even with fewer number of preference feedbacks. Specifically, for the button-press-topdown-wall task, LiRE with only 100 feedbacks outperforms not only the baselines with 2000 feedbacks but also the policy learned using the GT reward. Again, we can confirm that the high feedback efficiency enabled by RLT makes LiRE very effective even with a smaller number of feedbacks.", "5": null}, {"title": "5.4.2. Varying Q Budget", "content": "In Section 4.1, we described that multiple RLTs can be constructed by putting the budget limit (Q) in order to increase the sample diversity. In this subsection, we show the effect of Q. Table 4 shows the performance change of LiRE"}, {"title": "5.4.3. Robustness to Feedback Noise", "content": "If the preference feedback used in PbRL models human preference labeling, it would be reasonable to assume that the preference feedback may be noisy. To that end, we experiment to assess the robustness of the offline PbRL performance of LiRE with respect to the preference feedback noise. We assume that the preference feedback can be noisy with probability p (i.e., if $l_i = 0$ or 1, the label is flipped to $1 - l_i$ with probability p, and for tie labels, we flip to $l_i = 0$ or 1 with probability p/2, respectively). We varied the noise probability p from 0 to 0.3, and Figure 4 compares the success rates of MR w/ linear and LiRE. From the figure, we confirm that the performance of LiRE does not drop as severely as MR w/ linear when p increases. In particular, for lever-pull task, we observe that LiRE with feedback noise of p = 0.3 even results in a higher success rate than MR w/ linear with no noise, highlighting the robustness of LiRE with respect to feedback noise."}, {"title": "5.4.4. Impact of Feedback Granularity", "content": "In Figure 5, we compare the performance of LiRE based on the threshold that determines the tie between the segments. Specifically, we adjust the threshold value for the reward difference that indicates whether two segments are equally preferred. Namely, a higher threshold value means that more segment pairs are labeled as equally preferred, result-"}, {"title": "5.4.5. Comparison with SeqRank", "content": "Here, we compare LiRE with SeqRank, which also utilizes partially-ranked lists. We also employed the linear score function for SeqRank since it gave better results than using the exponential function and led to a fair comparison with LiRE. We evaluate the average success rates of SeqRank and LiRE on the Meta-World medium-replay dataset.\nThe experimental results in Table 5 show that LiRE clearly achieves higher performance than SeqRank. We argue that SeqRank does not fully utilize the second-order information because SeqRank does not construct a fully-ranked list. Indeed, the third column of Table 5 shows that the number of groups in the ranked lists averages less than 3 with the SeqRank, whereas it increases to about 9 on average with LiRE. The last two columns of Table 5 compare feedback efficiency and sample diversity. LiRE achieves a sample diversity of approximately 0.47 through the use of binary search, and the feedback efficiency increases significantly to 11.33 by constructing RLT. Additionally, Table 6 shows the superiority of LiRE over SeqRank on the DM-Control medium-replay dataset. We note SeqRank also performs similarly to MR w/ linear on walker-walk and humanoid-walk tasks, while LiRE achieves much higher performance gains on all three tasks. Thus, we confirm that"}, {"title": "5.4.6. Compatibility with other methods", "content": "To check the compatibility of LiRE with other methods, we tested the performance of LiRE when combined with OPRL and PT, respectively. First, to apply OPRL and LiRE simultaneously, we trained a reward model each time an RLT was newly constructed, and then actively sampled based on the disagreement of the reward models (following the method of OPRL) when constructing the next RLT. In Figure 6(a), we observe that LiRE+OPRL outperforms LiRE in sweep-into and lever-pull tasks but performs worse in button-press-topdown task. This discrepancy suggests that while the OPRL method enhances the consistency of the reward model, it may lead to oversampling similar segments that are challenging to distinguish depending on the task.\nSecond, as shown in Figure 6(b), LiRE does not necessarily gain improvements when combined with PT. That is, since PT was originally designed to capture temporal dependencies of segments in reward modeling, it seems to struggle in accurately capturing the second-order preference information from RLT possibly due to overfitting to the sequence of past segments."}, {"title": "5.5. Human Experiments", "content": "Table 7 presents the results with real human preference feedback on the new button-press-topdown offline RL dataset, which is distinct from the dataset used in Table 2. Namely, we collected 200 preference feedbacks from one of the authors for each of the three feedback collection methods: MR, SeqRank, and LiRE. For LiRE, we used the feedback budget of Q = 100, resulting in two RLTs. The results again indicate that LiRE dominates other baselines and gets stronger when the linear score function is used. We believe"}, {"title": "6. Limitation", "content": "We believe there are two limitations of LiRE. First, LiRE lacks the ability to parallelize the construction of RLT since there are dependencies between the order in which feedbacks are obtained to construct a fully-ranked list. Therefore, in scenarios where parallel feedback collection is feasible, constructing an RLT could be more time-consuming compared to collecting preference feedbacks independently in pairs. Nevertheless, the results presented in Appendix A.2 show that LiRE with only 200 feedbacks outperforms the independent pairwise sampling method using 1000 feedbacks, suggesting the importance of constructing RLT. Second, LiRE relies on the transitivity assumption outlined in Assumption 4.1. Although our experiments with feedback noise indicate LiRE's robustness to noise that violates this assumption, transitivity violations can occur even with noiseless labels in real-world applications. This issue is not unique to LiRE but affects other preference-based RL methods as well. Addressing transitivity violation remains a challenge for scalar reward models, so future research could explore solutions by using multi-dimensional preference feedback to construct RLTs for each dimension."}, {"title": "7. Concluding Remarks", "content": "In this paper, we propose a novel Listwise Reward Estimation (LiRE) method for offline preference-based RL. While obtaining second-order preference from a traditional framework is challenging, we demonstrate that LiRE efficiently exploits second-order preference by constructing an RLT using ordinary, simple ternary feedback. Our experiments demonstrate the significant performance gains achieved by LiRE on our new offline PbRL dataset, specifically designed to objectively reflect the effect of estimated rewards. Notably, the reward model trained with LiRE outperforms traditional pairwise feedback methods, even with fewer preference feedbacks, highlighting the importance of second-order preference information. Moreover, our findings suggest that constructing ranked lists can be straightforward without complex second-order preference feedback, indicating the broad applicability of LiRE to more challenging tasks and real-world applications."}, {"title": "Impact Statement", "content": "We believe our LiRE can be potentially applied to aligning the RL agent with more fine-grained human intent and preference. Such applications can bring significant societal consequences by enhancing the precision and effectiveness of AI systems in various fields such as health care and education. By ensuring that AI systems more closely reflect and respond to the detailed intentions of their users, LiRE has the potential to foster trust and acceptance of AI technologies, ultimately contributing to their more widespread and ethical adoption."}]}