{"title": "DLBACKTRACE: A MODEL AGNOSTIC EXPLAINABILITY FOR\nANY DEEP LEARNING MODELS", "authors": ["Vinay Kumar Sankarapu", "Chintan Chitroda", "Yashwardhan Rathore", "Neeraj Kumar Singh", "Pratinav Seth"], "abstract": "The rapid advancement of artificial intelligence has led to increasingly sophisticated deep learning\nmodels, which frequently operate as opaque \"black boxes\u201d with limited transparency in their decision-\nmaking processes. This lack of interpretability presents considerable challenges, especially in\nhigh-stakes applications where understanding the rationale behind a model's outputs is as essential\nas the outputs themselves. This study addresses the pressing need for interpretability in AI systems,\nemphasizing its role in fostering trust, ensuring accountability, and promoting responsible deployment\nin mission-critical fields. To address the interpretability challenge in deep learning, we introduce\nDLBacktrace, an innovative technique developed by the AryaXAI team to illuminate model decisions\nacross a wide array of domains, including simple Multi Layer Perceptron (MLPs), Convolutional\nNeural Networks (CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\nWe provide a comprehensive overview of the DLBacktrace algorithm and present benchmarking\nresults, comparing its performance against established interpretability methods, such as SHAP, LIME,\nGradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based\nmetrics. The proposed DLBacktrace technique is compatible with various model architectures built\nin PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as\nBERT and LSTMs, computer vision models like ResNet and U-Net, as well as custom deep neural\nnetwork (DNN) models for tabular data. This flexibility underscores DLBacktrace's adaptability and\neffectiveness in enhancing model transparency across a broad spectrum of applications. The library is\nopen-sourced and available at https://github.com/AryaXAI/DLBacktrace.", "sections": [{"title": "1 Introduction", "content": "Despite remarkable advancements in artificial intelligence, particularly with the evolution of deep learning architectures,\neven the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal\nprocesses that remain opaque and difficult to interpret. These models produce highly accurate predictions, yet provide\nlimited insights into how and why they make specific decisions. This opacity raises significant concerns, especially\nin high-stakes applications like healthcare, finance, and law enforcement, where understanding the rationale behind\nmodel outputs is critical. For example, in the healthcare sector, AI-driven diagnostics must be interpretable to ensure\nthat medical professionals can trust and act on recommendations for patient treatment. Similarly, in finance, regulations\nsuch as the European Union's GDPR mandate a \"right to explanation\" for automated decisions affecting individuals,\nmaking explainability not only an ethical imperative but also a regulatory requirement.\nThe growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus\non maximizing raw performance. This trend is especially evident with the increasing use of models like OpenAI's"}, {"title": "2 Relevant Literature", "content": ""}, {"title": "2.1 Importance of eXplainable AI (XAI)", "content": ""}, {"title": "2.1.1 XAI for Responsible and Trustworthy AI", "content": "Responsible AI is essential for deploying systems that align with ethical standards and societal values, especially in\ncritical sectors like healthcare, finance, and law enforcement, where AI decisions can profoundly impact individuals\nand communities. Responsible AI emphasizes fairness, transparency, accountability, privacy, and ethical alignment"}, {"title": "2.1.2 XAI for Safe A\u0399", "content": "AI safety aims to ensure Al systems are predictable, controllable, and aligned with human values, especially in\ncritical areas like healthcare, autonomous vehicles, and infrastructure. A key aspect of AI safety is explainability, as\ntransparent models enable developers, users, and regulators to understand system behavior, detect risks, and implement\nsafeguards. Core pillars of AI safety include robustness, reliability, alignment with human intentions, and the use of\nexplainability to clarify decision processes. Explainability is essential for risk mitigation in dynamic environments,\nwhere understanding AI behavior allows for safe intervention. For example, [12] discusses challenges like reward\nhacking and exploration hazards in reinforcement learning. Explainability techniques help analyze reward structures\nand behavior patterns, enabling developers to detect and correct unintended actions. Catastrophic forgetting, where\nmodels lose prior knowledge when learning new tasks, poses risks in sequential learning [13]. Explainable AI (XAI)\nmethods can identify model areas vulnerable to forgetting, supporting memory mechanisms that preserve safety-critical\ninformation. [14] introduces reward modeling to align AI with human preferences through feedback. Explainability tools\nprovide insights into reward structures' impact on agent behavior, aiding iterative refinement to align model goals with\nhuman values. Explainability also supports adversarial robustness by revealing patterns in model vulnerability, guiding\ndefenses that enhance safety and reliability. These examples underscore explainability's role in AI safety, enhancing\ntransparency, accountability, and risk mitigation. Embedding explainable practices within AI safety frameworks helps\ndevelopers control AI behavior, reducing risks in diverse operational contexts."}, {"title": "2.1.3 XAI for Regulatory AI", "content": "Explainable AI (XAI) is crucial for regulatory compliance, promoting transparency, fairness, and accountability in AI-\ndriven decisions in sectors like finance, healthcare, and law. Regulatory frameworks increasingly mandate interpretable\nmodels to ensure oversight, protect user rights, and uphold ethical standards. Key elements of XAI in regulatory\ncontexts include transparent decision processes, model auditability, and mechanisms to mitigate bias. In finance, XAI\nhelps institutions clarify decisions on credit scoring, loan approvals, and fraud detection, reducing regulatory risks and\nfostering public trust. For instance, [15] systematically reviews XAI applications in finance, illustrating transparency's\nrole in regulatory compliance. As large language models (LLMs) become ubiquitous, interpretability in NLP has gained\nimportance. [16] examines alignment of interpretability methods with stakeholder needs, categorizing techniques\nand identifying differences between developer and non-developer requirements. Stakeholder-centered frameworks\nhelp ensure more responsible AI deployment. In healthcare, XAI is vital for patient safety and ethical standards.\nExplainable models enable healthcare providers to interpret AI-driven diagnoses, treatments, and risk assessments,\naligning these with medical regulations. [17] addresses the challenge of conflicting post hoc explanations, often resolved"}, {"title": "2.2 Explainability Methods", "content": ""}, {"title": "2.2.1 Tabular Data", "content": "In high-stakes applications such as finance and healthcare, machine learning models like regression and probabilistic\nalgorithms (e.g., decision trees and their variants) are often preferred. This is because deep learning models are\nfrequently described as \"black boxes,\" making it challenging to interpret how they arrive at their conclusions. This lack\nof transparency can be particularly problematic in critical contexts where accountability and trust are essential.\nTo enhance interpretability, explainable algorithms like LIME [3] and SHAP [4] are increasingly used. LIME (Local\nInterpretable Model-Agnostic Explanations) builds simple, interpretable models around specific data points to highlight\nthe most influential features for each prediction. SHAP (SHapley Additive exPlanations) assigns importance scores\nto each feature, providing both global explanations (by ranking features based on overall importance) and local\nexplanations (by illustrating how individual features contribute to specific predictions).\nHowever, these methods have limitations. LIME's reliance on random sampling can lead to inconsistent explanations\nfor the same data point, and its effectiveness can be sensitive to the choice of perturbation method. SHAP, while\ncomprehensive, can be computationally expensive for large datasets and complex models. Additionally, SHAP's\nmodel-agnostic nature may result in less accurate explanations for highly intricate models, like deep neural networks.\nAs a result, both LIME and SHAP may face challenges in providing precise, interpretable explanations for complex\ndeep learning models."}, {"title": "2.2.2 Image Data", "content": "In explainable AI (XAI) for image modality-based tasks, gradient-based methods such as GradCAM [20], Vanilla\nGradient [21], SmoothGrad [22], and Integrated Gradients [5] are widely used for interpreting model predictions.\nGradCAM generates heatmaps by calculating the gradient of the target class with respect to convolutional layer\nactivations, but may miss fine details and is sensitive to input noise. Vanilla Gradient directly computes gradients\non the input, though it faces the \"saturation problem,\" where gradients become too small for clear interpretation.\nSmoothGrad improves clarity by averaging gradients with added noise, albeit at a computational cost, while Integrated\nGradients addresses saturation by calculating an integral of gradients from a baseline to the input, though it also\ndemands significant computation.\nIn recent advances, Vision Transformers (ViTs) require specific interpretability approaches due to their reliance on\nattention mechanisms. In the paper [23], authors introduces TokenTM, a method designed for ViTs that considers both\ntoken transformations (changes in token length and direction) and attention weights across layers. By aggregating these\nfactors, TokenTM provides more focused and reliable explanations, addressing unique interpretability challenges in\ntransformer models. These developments reflect a shift in XAI, where interpretability techniques are tailored to the\nunique demands of model architectures, like CNNs and transformers, enhancing transparency and reliability across\ndifferent models."}, {"title": "2.2.3 Textual Data", "content": "In the text modality, quite a few explanation methods are employed to enhance the interpretability of machine learning\nmodels. Among these, LIME [3] and SHAP [4] are baseline for interpreting text classification models. Gradient-based\nmethods, such as GradCAM [20], Integrated Gradients [5], and Attention Rollout [24], also play a significant role\nacross diverse model architectures.\nFor text generation tasks, 17 challenges were identified by [25], such as tokenization effects and randomness, and\nadvocates for probabilistic explanations and perturbed benchmarks to address these issues. Furthermore, LACOAT\nintroduced by [26], which clusters word representations to produce context-aware explanations. It maps test features\nto latent clusters and translates these into natural language summaries, improving interpretability for complex NLP"}, {"title": "2.2.4 Metrics for Benchmarking Explainability", "content": "Recent advancements in Explainable AI (XAI) emphasize the need for robust evaluation frameworks, benchmarks,\nand specialized toolkits to enhance transparency and trust in machine learning systems. Quantus [32], provides a\nmodular toolkit with metrics such as faithfulness, robustness, and completeness, promoting reproducible and responsible\nevaluation of neural network explanations. BEExAI [33] addresses the underexplored domain of tabular data by\nbenchmarking explanation methods using tailored datasets and metrics like local fidelity and human interpretability,\nenabling systematic comparisons in real-world scenarios. In a survey over 30 XAI toolkits by [34], including Quantus,\nhighlighting challenges such as inconsistent metrics, lack of fairness assessments, and insufficient focus on human-\ncentered evaluation, calling for standardized benchmarks and unified platforms. PyXAI [35] complements these efforts\nby focusing on tree-based models, introducing efficient tools and unique metrics such as path importance and rule-level\ninterpretability for domains like healthcare and finance. Despite these advancements, key gaps remain, including\nstandardization across tools, evaluation for diverse data types, human-centered usability, and fairness and robustness\nassessments, underscoring the need for continued research to achieve actionable and responsible XAI."}, {"title": "3 Backtrace", "content": ""}, {"title": "3.1 Introduction", "content": "Backtrace is a technique for analyzing neural networks that involves tracing the relevance of each component from the\noutput back to the input.\nThis approach clarifies how each element contributes to the final prediction. By distributing relevance scores across\nvarious layers, Backtrace provides insights into feature importance, information flow, and potential biases, which\nfacilitates improved model interpretation and validation without relying on external dependencies.\nBacktrace has the following advantages over other available tools:\n\u2022 No dependence on a sample selection algorithm :\nThe relevance is calculated using just the sample in focus. This avoids deviations in importance due to varying\ntrends in sample datasets.\n\u2022 No dependence on a secondary white-box algorithm :\nThe relevance is calculated directly from the network itself. This prevents any variation in importance due to\ntype, hyperparameters and assumptions of secondary algorithms.\n\u2022 Deterministic in nature\nThe relevance scores won't change on repeated calculations on the same sample. Hence, can be used in live\nenvironments or training workflows as a result of its independence from external factors."}, {"title": "3.2 Methodology", "content": "Backtrace operates in two modes: Default Mode and Contrast Mode.\nFirst we describe the Basic Methodology of Backtrace in Default Mode as follows :"}, {"title": "3.2.1 Basic Methodology", "content": "Every neural network consists of multiple layers. Each layer has a variation of the following basic operation:\n$y = \\Phi(Wx + b)$\nwhere,\n\u2022 \u03a6 = activation function\n\u2022 W = weight matrix of the layer\n\u2022 b = bias\n\u2022 x = input\n\u2022 y = output\nThis can be further organized as:\n$y = \\Phi(X_p + X_n + b)$\nwhere,"}, {"title": "3.2.2 Relevance Propagation", "content": "The aforementioned modes represent the basic operations at each source layer for propagating relevance to the\ndestination layer. The procedure for relevance calculation is as follows:\n1. Construct a graph from the model weights and architecture with output nodes as root and input nodes as leaves.\n2. Propagate relevance in a breadth-first manner, starting at the root.\n3. The propagation completes when all leaves (input nodes) have been assigned relevance.\nNote: Any loss of relevance during propagation is due to network bias.\nThe relevance of a single sample represents local importance. For global importance, the relevance of each feature can\nbe aggregated after normalization at the sample level."}, {"title": "3.3 Algorithm", "content": "The algorithm has two modes of operation:\n\u2022 Default Mode\n\u2022 Contrastive Mode"}, {"title": "3.3.1 Default Mode", "content": "In this mode, a single relevance is associated with each unit. The relevance is propagated by proportionately distributing\nit between positive and negative components. If the relevance associated with y is ry and with x is rx, then for the jth\nunit in y, we compute:\n$T_j = X_{pj} + |X_{nj}| + |b_j|$\n$R_{pj} = \\frac{X_{pj}}{T_j}r_y, R_{nj} = \\frac{X_{nj}}{T_j}r_y, R_{bj} = \\frac{b_j}{T_j}r_y$\n$R_{pj}$ and $R_{nj}$ are distributed among $x$ in the following manner:\n$r_{x_{ij}} = \\begin{cases}\n\\frac{W_{ij}x_{ij}}{X_{pj}} R_{pj} & \\text{if } W_{ij} x_{ij} > 0 \\\\\n0 & \\text{if } W_{ij}x_{ij} > 0 \\text{ and } \\Theta \\text{ is saturated on negative end} \\\\\n-\\frac{W_{ij}x_{ij}}{X_{nj}} R_{nj} & \\text{if } W_{ij}x_{ij} < 0 \\\\\n0 & \\text{if } W_{ij}x_{ij} < 0 \\text{ and } \\Theta \\text{ is saturated on positive end} \\\\\n0 & \\text{if } W_{ij} x_{ij} = 0\n\\end{cases}$\nThe total relevance at layer $x$ is:\n$r_x = \\sum_i r_{x_i}$"}, {"title": "3.3.2 Contrastive Mode:", "content": "In this mode, each unit is assigned dual relevance, distributed between positive and negative components. This\napproach facilitates separate analyses of supporting and detracting influences. Unlike single-mode propagation, which\ncombines relevance into aggregated scores, dual-mode propagation provides clarity by isolating favorable and adverse\ncontributions. This separation enhances interpretability, enabling deeper insights into features that negatively impact\npredictions a capability essential for identifying counterfactuals or assessing model biases in high-stakes scenarios.\nIf the relevance associated with y are $r_{yp}, r_{yn}$ and with x are $r_{xp}, r_{xn}$, then for the jth unit in y, we compute:\n$T_j = X_{pj} + X_{nj} + b_j$\nWe then calculate Determine $R_{pj}, R_{nj}$, and Relevance Polarity as described in Algorithm 1."}, {"title": "3.4 Relevance for Attention Layers:", "content": "Currently, the majority of AI models across various applications are primarily based on the attention mechanism [36].\nAccordingly, we have extended our Backtrace algorithm to provide support for this attention model [37].\nAttention mechanism allows the model to focus on specific parts of the input sequence, dynamically weighting the\nimportance of different elements when making predictions. The attention function employs the equation:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})$"}, {"title": "3.4.1 Relevance Propagation for Attention Layers", "content": "Suppose the input to the attention layer is x and the output is y. The relevance associated with y is ry. To compute the\nrelevance using the Backtrace, we use the steps as indicated in Algorithm 3 below:"}, {"title": "4 Benchmarking", "content": "In this section, we present a comparative study to benchmark our proposed Backtrace algorithm against various existing\nexplainability methods. The goal of this evaluation is to assess the effectiveness, robustness, and interpretability of\nBacktrace in providing meaningful insights into model predictions across different data modalities, including tabular,"}, {"title": "4.1 Setup", "content": "The experimental setup consists of three distinct data modalities: tabular, image, and text. Each modality is associated\nwith specific tasks, datasets, and model architectures tailored to effectively evaluate the explainability methods."}, {"title": "4.1.1 Tabular Modality", "content": "For the tabular data modality, we focus on a binary classification task utilizing the Lending Club dataset. This dataset\nis representative of financial applications, containing features that capture various attributes of borrower profiles. We\nemploy a four-layer Multi-Layer Perceptron (MLP) neural network, which is well-suited for learning from structured\ndata and provides a foundation for assessing the performance of explainability techniques."}, {"title": "4.1.2 Image Modality", "content": "In the image data modality, we conduct a multi-class classification task using the CIFAR-10 dataset. This benchmark\ndataset consists of images across 10 different classes, making it ideal for evaluating image classification algorithms.\nFor this experiment, we utilize a fine-tuned ResNet-34 model, known for its deep residual learning capabilities, which\nenhances the model's ability to learn intricate patterns and features within the images."}, {"title": "4.1.3 Text Modality", "content": "The text data modality involves a binary classification task using the SST-2 dataset, which is focused on sentiment\nalysis. The dataset consists of movie reviews labeled as positive or negative, allowing for a nuanced evaluation of\nsentiment classification models. We employ a pre-trained BERT model, which leverages transformer-based architectures\nto capture contextual relationships in text. This approach facilitates the generation of high-quality explanations for\nthe model's predictions, enabling a thorough assessment of explainability methods in the realm of natural language\nprocessing."}, {"title": "4.2 Metrics", "content": "To assess the effectiveness of explanation methods across various modalities, we utilize different metrics tailored to\nspecific use cases. Further details are provided below:"}, {"title": "4.2.1 Tabular Modality", "content": "\u2022 Maximal Perturbation Robustness Test (MPRT): This metric assesses the extent of perturbation that can be\napplied to an input before there are significant changes in the model's explanation of its decision. It evaluates\nthe stability and robustness of the model's explanations rather than solely its predictions.\n\u2022 Complexity Metric: This metric quantifies the level of detail in a model's explanation by analyzing the\ndistribution of feature contributions."}, {"title": "4.2.2 Image Modality", "content": "\u2022 Faithfulness Correlation: Faithfulness Correlation [38] metric evaluates the degree to which an explanation\naligns with the model's behavior by calculating the correlation between feature importance and changes in\nmodel output resulting from perturbations of key features.\n\u2022 Max Sensitivity: Max-Sensitivity [39], is a robustness metric for explainability methods that evaluates\nhow sensitive explanations are to small perturbations in the input. Using a Monte Carlo sampling-based\napproximation, it measures the maximum change in the explanation when slight random modifications are\napplied to the input. Formally, it computes the maximum distance (e.g., using L1, L2, or L\u221e norms) between\nthe original explanation and those derived from perturbed inputs.\n\u2022 Pixel Flipping: Pixel Flipping method involves perturbing significant pixels and measuring the degradation in\nthe model's prediction, thereby testing the robustness of the generated explanation."}, {"title": "4.2.3 Text Modality", "content": "To evaluate the textual modality, we use the Token Perturbation for Explanation Quality (ToPEQ) metric, which assesses\nthe robustness of model explanations by analyzing the impact of token perturbations. We employ the Least Relevant\nFirst AUC (LeRF AUC) and Most Relevant First AUC (MoRF AUC) to measure sensitivity to the least and most\nimportant tokens, respectively. Additionally, we calculate Delta AUC, the difference between LeRF AUC and MORF\nAUC, to further indicate the model's ability to distinguish between important and unimportant features.\n\u2022 LeRF AUC (Least Relevant First AUC): This metric evaluates how gradually perturbing the least important\nfeatures (tokens) affects the model's confidence. The AUC measures the model's response as the least relevant\nfeatures are replaced with a baseline (e.g., [UNK]), indicating the degree to which the model relies on these\nfeatures.\n\u2022 MORF AUC (Most Relevant First AUC): This metric measures how quickly the model's performance\ndeteriorates when the most important features are perturbed first. The AUC represents the decrease in the\nmodel's confidence as the most relevant tokens are removed, revealing the impact of these key features on the\nprediction.\n\u2022 Delta AUC: This metric represents the difference between LeRF AUC and MoRF AUC. It reflects the model's\nsensitivity to the removal of important features (MoRF) compared to less important ones (LeRF). A larger delta\nsuggests that the explanation method effectively distinguishes between important and unimportant features."}, {"title": "4.3 Experiments", "content": ""}, {"title": "4.3.1 Tabular Modality", "content": "We evaluated 1,024 samples from the test set of the Lending Club dataset, using a fine-tuned MLP checkpoint that\nattained an accuracy of 0.89 and a weighted average F1 score of 0.87. We assessed Backtrace against widely used\nmetrics for tabular data, specifically LIME and SHAP [4], and employed MPRT for comparison, along with Complexity\nto examine the simplicity of the model explanations as illustrated in Appendix A.1.1."}, {"title": "4.3.2 Image Modality", "content": "We conducted an evaluation on 500 samples from the CIFAR-10 test set using a supervised, fine-tuned ResNet-34\nmodel, which achieved a test accuracy of 75.85%. We compared Backtrace against several methods as illustrated in\nAppendix A.1.2, including Grad-CAM, vanilla gradient, smooth gradient, and integrated gradient. The comparison\nutilized metrics such as Faithfulness Correlation, Max Sensitivity, and Pixel Flipping."}, {"title": "4.3.3 Text Modality", "content": "For the text modality, evaluation was performed on the evaluation set of the SST-2 dataset using a fine-tuned BERT\nmodel\u00b9, achieving an F1 score of 0.926. Since explainable AI (XAI) for BERT and other transformer-based models is\nrelatively new, we employed metrics based on token perturbation for explanation quality, specifically LeRF, MORF, and\nDelta AUC, as introduced in [37]. We used methods like IG, SmoothGrad, AttnRoll, GradCAM and Input Grad as\nillustrated in Appendix A.1.5."}, {"title": "4.4 Observations", "content": "The quality of explanations is influenced by both the input data and the model weights. The impact of model performance\nis significant; low model performance tends to result in unstable explanations characterized by high entropy, while good\nmodel performance is associated with stable explanations that are more sparse. Additionally, the inference time for a\nsample is proportional to both the size of the model and the computational infrastructure used."}, {"title": "5 Discussion", "content": "Additional illustrations of Backtrace for various use cases are provided in Appendix A."}, {"title": "5.1 Advantages of Backtrace", "content": ""}, {"title": "5.1.1 Network Analysis", "content": "\u2022 Existing solutions involve distribution graphs and heatmaps for any network node based on node activation.\n\u2022 These are accurate for that specific node but don't represent the impact of that node on the final prediction.\n\u2022 Existing solutions are also unable to differentiate between the impact of input sources versus the internal\nnetwork biases."}, {"title": "5.1.2 Feature Importance", "content": "\u2022 With each input source being assigned a fraction of the overall weightage, we can now quantify the dependence\nof the final prediction on each input source.\n\u2022 We can also evaluate the dependence within the input source as the weight assignment happens on a per unit\nbasis.\n\u2022 Integrated Gradients and Shapley values are other methods available for calculating feature importance from\nDeep Learning Models. Both come with caveats and give approximate values:\nIntegrated Gradients depends on a baseline sample which needs to be constructed for the dataset and\naltered as the dataset shifts. This is extremely difficult for high-dimensional datasets.\nShapley Values are calculated on a sample set selected from the complete dataset. This makes those\nvalues highly dependent on the selection of data."}, {"title": "5.1.3 Uncertainty", "content": "\u2022 Instead of just relying on the final prediction score for decision-making, the validity of the decision can now\nbe determined based on the weight distribution of any particular node with respect to the prior distribution of\ncorrect and incorrect predictions."}, {"title": "5.2 Applicability", "content": "The Backtrace framework is applicable in the following use-cases:"}, {"title": "5.2.1 Interpreting the model outcomes using the local and global importance of each feature", "content": "The local importance is directly inferred from the relevance associated with input data layers. For inferring global\nimportance, the local importance of each sample is normalized with respect to the model outcome of that sample. The\nnormalized local importance from all samples is then averaged to provide global importance. The averaging can be\nfurther graded based on the various outcomes and binning of the model outcome."}, {"title": "5.2.2 Network analysis based on the relevance attributed to each layer in the network", "content": "The two modes together provide a lot of information for each layer, such as:\n\u2022 Bias to input ratio\n\u2022 Activation Saturation\n\u2022 Positive and negative relevance (unit-wise and complete layer)\nUsing this information, layers can be modified to increase or decrease variability and reduce network bias. Major\nchanges to the network architecture via complete shutdown of nodes or pathways are also possible based on the total\ncontribution of that component."}, {"title": "5.2.3 Fairness and bias analysis using the feature-wise importance", "content": "This is in continuation of the global importance of features. Based on the global importance of sensitive features (e.g.\ngender, age, etc.) and their alignment with the data, it can be inferred whether the model or data have undue bias\ntowards any feature value."}, {"title": "5.2.4 Process Compliance based on the ranking of features on local and global levels", "content": "Using the local and global importance of features and ranking them accordingly, it can be determined whether the\nmodel is considering the features in the same manner as in the business process it is emulating. This also helps in\nevaluating the solution's alignment with various business and regulatory requirements."}, {"title": "5.2.5 Validating the model outcome", "content": "Every model is analyzed based on certain performance metrics which are calculated over a compiled validation dataset.\nThis doesn't represent the live deployment scenario.\nDuring deployment, validation of outcomes is extremely important for complete autonomous systems. The layer-wise\nrelevance can be used for accomplishing this. The relevance for each layer is mapped in the vector space of the same\ndimension as the layer outcome, yet it is linearly related to the model outcome.\nSince the information changes as it passes through the network, the relevance from lower layers, even input layers, can\nbe used to get different outcomes. These outcomes can be used to validate the model outcome. The layers are generally\nmulti-dimensional, for which either proximity-based methods or white-box regression algorithms can be used to derive\noutcomes."}, {"title": "6 Conclusion", "content": "In this paper, we introduced the DLBacktrace, a new method that significantly improves model interpretability for\ndeep learning. DLBacktrace traces relevance from output back to input, giving clear and consistent insights into which\nfeatures are important and how information flows through the model. Unlike existing methods, which often rely on\nchanging inputs or using other algorithms, DLBacktrace is stable and reliable, making it especially useful in fields that\nneed high transparency, like finance, healthcare, and regulatory compliance. Our benchmarking results demonstrate\nthat DLBacktrace performs better in terms of robustness and accuracy across various model types, proving it can\nprovide practical insights. Overall, DLBacktrace contributes to the growing field of explainable AI by enhancing model\ntransparency and trustworthiness, promoting responsible AI deployment in critical applications."}, {"title": "7 Future Works", "content": "Future research on DLBacktrace will aim to broaden its use and improve how it scores relevance. Key areas for\ndevelopment include adapting DLBacktrace for complex and evolving model architectures, like advanced transformers\nand multimodal models to ensure it remains effective across different AI applications. Additionally, we aim to reduce\nthe inference time making DLBacktrace more suitable for real-time applications in production environments such as\nautonomous systems and dynamic decision-making scenarios.\nFuture development will also explore the use of DLBacktrace for specific model improvements, including diagnosis\nand targeted editing. For example, DLBacktrace could assist in model pruning by identifying less critical components,\ntehreby optimizing model performance and efficiency. In addition, DLBacktrace's potential for targeted model\nimprovements will be explored. It can assist in model pruning, especially for Mixtures of Experts (MoEs), by\nidentifying underutilized components or redundant experts to optimize performance and efficiency. It can also help\nin facilitate model merging, providing insights for seamless integration of multiple models, and layer swapping,\nenabling selective replacement of layers to enhance adaptability or performance. We also plan to apply DLBacktrace to\nout-of-distribution (OOD) detection, where it can help distinguish instances that fall outside the model's training data,\nenhancing the robustness and reliability of AI systems.\nFurthermore, extending DLBacktrace's support for model-agnostic explainability will allow it to be seamlessly applied\nacross various architectures, making it a versatile tool in explainable AI. These improvements will make DLBacktrace\nmore useful and establish it as an important tool for understanding and improving models across a wide range of AI\napplications and tasks."}]}