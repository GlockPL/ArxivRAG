{"title": "HASHEVICT: A PRE-ATTENTION KV CACHE EVICTION STRATEGY USING LOCALITY-SENSITIVE HASHING", "authors": ["Minghui Liu", "Tony O'Halloran", "Mary-Anne Hartley", "Cornelia Ferm\u00fcller", "Tahseen Rabbani", "Brian Gravelle", "Ananth Sankaralingam", "Furong Huang", "Yiannis Aloimonos"], "abstract": "The key-value (KV) cache in large language models (LLMs) markedly accelerates attention and inference. However, since it requires storing the key and value embeddings of past tokens it can quickly consume a significant amount of GPU memory. In this work, we introduce HASHEVICT, a KV cache compression strategy that uses locality-sensitive hashing (LSH) to efficiently estimate attention and evict unimportant tokens. At every decoding step, the key and value of the current token replace the embeddings of a token estimated to produce the lowest attention score. We project key and query embeddings to dimensionally-reduced binary hash codes and locate low-attention tokens via a lightweight Hamming distance calculation, all of which is efficiently computable on the GPU. Unlike existing compression strategies that compute attention to determine token retention, HASHEVICT makes these decisions pre-attention, thereby reducing computational costs. We demonstrate that HASHEVICT can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks. Our method is fast: we achieve 1.5-2x prefill speed and competitive decoding speed against baseline methods such as H2O, Scissorhands, and 17\u00d7 prefill/2x decoding speed against FastGen.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has enabled sharp improvements over innumerable downstream natural language processing (NLP) tasks, such as summarization and dialogue generation [Zhao et al., 2023, Wei et al., 2022]. The hallmark feature of LLMs, the attention module [Bahdanau, 2014, Luong, 2015, Vaswani, 2017], enables contextual processing over sequences of tokens. To avoid repeated dot products over key and value embeddings of tokens, a key-value (KV) cache is maintained in VRAM to maintain these calculations. This technique is particularly popular with decoder LLMs.\nHowever, the size of the KV cache scales quadratically with sequence length n and linearly with the number of attention layers and heads. Assuming the size of the KV cache is n tokens, for each new decoded token, n attention scores need to be added which requires a total of O(dn\u00b2) computation, where d is the projection dimension, and O(n\u00b2) storage. For example, maintaining the KV cache for a sequence of 4K tokens in half-precision (FP16) can require approximately ~16GB of memory for most models within the Llama 3 family [Dubey et al., 2024]. These memory costs are exacerbated with batched inference and result in high decoding latency [Fu, 2024]. Consequently, there is significant interest in compressing the size of the KV cache to enable longer context windows and low-resource, on-device deployment.\nAn emerging strategy for reducing the size of the KV cache is token eviction. This approach drops the key and value embeddings for past tokens in the cache, skipping future attention calculations involving these tokens.\nVarious token eviction/retention policies have been explored in recent literature, including\nthe profiling of token type preferences [Ge et al., 2023], retention of heavy-hitter tokens [Zhang et al., 2024a,b], and dropping tokens based on the high L2 norms of their key embeddings [Devoto et al., 2024].\nThe latter approach [Devoto et al., 2024] is intriguing as eviction decisions are performed pre-attention. However, this L2 dropout strategy in inclined towards long-context retrieval tasks. It developed based on an empirical observation that smaller norm of key embedding correlates with higher attention score. For long-context retrieval tasks, high-attention score tokens are the most important tokens since the question's text will overlap with the piece of context that needs to be retrieved. Thus, it is specialized to retain only those tokens with the highest attention, which we find unsuitable for free response reasoning tasks. Existing literature suggests that retaining tokens with a diverse spectrum of attention scores (skewing high) is necessary [Guo et al., 2024, Zhang et al., 2024a, Long et al., 2023].\nIs there a non-attentive KV cache compression strategy that is performant over a wide variety of tasks, including multiple-choice, summarization, long-context retrieval, and free response question-answering? This work answers this question positively by introducing a novel strategy, HASHEVICT, that dynamically determines token eviction pre-attention via locality-sensitive hashing (LSH) [Goemans and Williamson, 1995, Charikar, 2002]. HASHEVICT evicts a past token from the cache whose key embedding is highly cosine dissimilar to the current query token embedding. The intuition behind this strategy is that high cosine dissimilarity indicates a low dot-product attention score. To efficiently scan for cosine (dis)similar tokens without performing attention, HASHEVICT leverages the SimHash [Charikar, 2002, Goemans and Williamson, 1995] to instead compare Hamming distances between c-length binary hashes of cached key embeddings and the current query embedding. We depict a high-level visualization of this strategy in Figure 1.\nHASHEVICT requires minimal overhead: for a total sequence length of l tokens with embedding dimension d, HASHEVICT maintains a constant-size, low-cost binary array in GPU memory of size c \u00d7 k bytes, where c < d is the hash dimension and k < l. Cached tokens with key embeddings that register low Hamming similarity measurements to decoded query embeddings are gradually replaced.\nOur contributions are as follows:\n\u2022 Novel Attention-Free Token Eviction We introduce a novel attention-free token eviction strategy, HASHEVICT, that leverages locality-sensitive hashing (LSH) to quickly locate which token in the cache is the least relevant to the current query. This ranking procedure consists entirely of cheap Hamming distance calculations. The associated binary array for computing these similarities requires minimal memory overhead. For a Llama 3 model, HASHEVICT can compress the KV cache by 30%-70% with minimal performance drop\n\u2022 State-of-the-Art Performance HASHEVICT demonstrates high performance on reasoning tasks (GSM8K Cobbe et al. [2021], MedQA Cobbe et al. [2021]), multiple-choice (GSM8K MC, MedQA MC), long-context retrieval (Needle-in-a-Haystack, Common Word [Hsieh et al., 2024]), and long-text summarization (MultiNews, GovReport Bai et al. [2023]). To the best of our knowledge, HASHEVICT achieves state-of-the-art performance for attention-free eviction, outperforming the similar attention-free L2 method. Additionally, HASHEVICT outperforms attention-accumulation-based methods on long text summarization tasks and achieves 1.5x speedup in the prefilling stage and comparable speed in the decoding stage withoug low-level optimiations.\n\u2022 Open-Source Implementation Upon public release of our manuscript, we will release an open-source implementation of HASHEVICT through a fork of the popular cold-compress library (https://github.com/AnswerDotAI/ cold-compress)."}, {"title": "2 Preliminaries", "content": "We aim to capture tokens whose query embeddings will form a large sum of dot products (i.e., attention scores) with other key embeddings, but without explicitly calculating attention. We will leverage locality-sensitive hashing (LSH) to quickly determine cosine similarities since the angle is equivalent to the dot product (for unit vectors). In this section, we review technical concepts crucial to attention and locality-sensitive hashing. We assume some base level of similarity with transformers, but we refer the reader to precise formalism [Phuong and Hutter, 2022].\nScaled Dot-Product Attention Consider a sequence of n tokens with e-dimensional real-valued representations X\u2081, X\u2082,..., X\u2099. Let Q = [q\u2081 q\u2082 ... q\u2099] \u2208 \u211d\u207f\u02e3\u1d48, K = [k\u2081 k\u2082 ... k\u2099] \u2208 \u211d\u1d48\u02e3\u207f where q\u1d62 = Wq x\u1d62, k\u1d62 = Wk x\u1d62 and Wq, Wk \u2208 \u211d\u1d48\u02e3\u1d49. The query and key projectors W\u2081 and Wk are pre-trained weight matrices. We also define a value matrix V = [v\u2081 v\u2082 ... v\u2099] \u2208 \u211d\u1d48\u1d52\u1d58\u1d57\u02e3\u207f with v\u1d62 = Wu x\u1d62 with trainable V \u2208 \u211d\u1d48\u1d52\u1d58\u1d57\u02e3\u1d48, the scaled dot product attention mechanism is given as\nAttention(Q, K, V) = V softmax(Q\u1d40K / \u221ad)   (1)\nTypically, attention layers contain multiple heads {h\u1d62}\u1d62\u208c\u2081 each with distinct query, key, and value projectors {W(hi), W(hi), W(hi)}\u1d62\u208c\u2081. In a multi-head setup, attention is computed in parallel across all heads, and the outputs are concatenated together and then passed through a linear layer for processing by the next transformer block.\nAs Q, K, V are updated with each new incoming token, to avoid significant re-computation, the current state of Q\u1d40K, Q, and K are maintained in the KV cache. Our goal is to bypass attention computation and caching for select tokens, i.e., sparsify the attention matrix Q\u1d40K, K, and V.\nLocality-Sensitive Hashing We will now describe a family of locality-sensitive hashing (LSH) functions able to efficiently approximate nearest neighbors (per cosine similarity) of key/query vectors in high-dimensional \u211d\u1d48 through comparison in a reduced c-dimensional space (per Hamming distance) with c \u226a d. Here, \"locality-sensitive\" means points that are close together according to a distance function dista(\u00b7, \u00b7) in the ambient space remain close per another distance function distc(\u00b7, \u00b7) in the lower-dimensional space with high-probability. For a rigorous treatment of LSH functions, see [Andoni et al., 2018, Charikar, 2002].\nFormally for our setup, dista(x, y) \u2261 cos x,y = x\u1d40y / ||x|| ||y|| and distc(p, q) \u2261 d(p, q) which denotes the Hamming distance. We will project each vector from \u211d\u1d48 into \u2124\u2082, the space of c-bit binary strings (which is often referred to as a"}, {"title": "2.1 Related Works", "content": "KV Cache Compression Many popular compression strategies adopt an eviction approach, which removes embed-dings from the KV cache. H2O [Zhang et al., 2024a] and Scissorhands [Liu et al., 2024a] calculate token importance by their accumulated attention scores and keep the \u201cheavy hitters\" in the cache. FastGen [Ge et al., 2023] performs a profiling pass before the generation stage that assigns to each head, according to the head's attention patterns, a pruning policy which only retains categories of tokens (punctuation, special, etc.) favored by the head. These eviction strategies depend on the computation of attention scores for their policy. An attention-free L2 dropout method [Devoto et al., 2024], which we compare ourselves to in this work, uses the observation that high-attention tokens tend to have low L2 key norms to approximately keep important tokens in cache. Other methods seek to merge KV caches across heads, such as grouped query attention (GQA) [Ainslie et al., 2023, Dubey et al., 2024]. KVMerger [Wang et al., 2024] and MiniCache [Liu et al., 2024b], which searches for similarity between tokens in consecutive attention layers and subsequently merges KV cache entries across these layers. While these consolidation approaches prevent memory complexity associated with KV caches from scaling with depth or multi-head attention, the size of any singular cache still tends to scale with sequence length.\nLSH Based Attention Similar to our work, Reformer [Kitaev et al., 2020] employs LSH to find similar tokens, but as a way to replace the softmax attention as opposed to token eviction. It creates hash buckets of tokens that form local attention groups and only attends to tokens in the same and neighboring buckets. However, this makes Reformer vulnerable to missing important tokens due to hash collision or boundary issues, and therefore, it must use multiple hash tables to mitigate this issue. In a similar vein, KDEFormer [Zandieh et al., 2023], HyperAttention [Han et al., 2023], and Zandieh et al. [2024a], use LSH to stably approximate and compressing the attention module thus accelerating the computation, but without token eviction. SubGen [Zandieh et al., 2024b] uses LSH to cluster key embeddings and samples representatives from each cluster to reduce the size of the KV Cache and consequently speed up attention, though it must initially view all queries and keys to perform this clustering which could result in VRAM blowup, which our method avoids.\nMemory Efficient Transformers Multi-Query Attention [Shazeer, 2019] and Grouped Query Attention [Ainslie et al., 2023] reduce the number of key-value matrices by sharing them across multiple query heads to save KV cache memory usage. However, they require retraining or up-training the LLM. Cache quantization methods [Hooper et al., 2024, Sheng et al., 2023] reduce the KV cache size by compressing the hidden dimension instead of along the sequence dimension but can result in information loss. Linear Transformer [Katharopoulos et al., 2020] reduces memory usage by replacing softmax attention with linear kernels and, therefore, achieves a constant memory requirement."}, {"title": "3 HASHEVICT: A Locality-Sensitive Eviction Strategy", "content": "We now formalize our eviction method reflected in Algorithm 1. We assume that the KV cache has a limited and fixed budget and conceptually divide the KV cache management during LLM inference into two stages: the initial Prompt Encoding Stage and then a Decoding Stage (i.e., generation).\nLet C be a constant and fixed cache budget, K be the key cache, and V be the V cache in a K-V attention head. We define our eviction policy as a function\nK\u209c, V\u209c, H\u209c \u2190 P(q, K\u209c\u208b\u2081, V\u209c\u208b\u2081, H\u209c\u208b\u2081)  (4)\nwhere H\u209c \u2208 {0,1}\u1d9c\u02e3\u1d9c is a hash table that contains hash codes of keys in K. We then define a function Fscore to assign a score for each key inside the K cache. Fscore outputs an array which contains the negative of hamming distances dh between the hash code of a query vector q and columns of H, which are hash codes of all non-evicted keys.\nFscore(q, K) = -d\u2095(h(q), H)  (5)\nThe eviction index et at any step t is selected as the index with the lowest score:\ne\u209c \u2190 arg min Fscore(q\u209c\u208b\u2081, H\u209c\u208b\u2081)  (6)\nwhich points to the key that is most distant from the query vector at time step t. Entries at index et from the K and V are evicted and H is updated (step 3-6 of Algorithm 1).\nPrompt Encoding Stage During the prompt encoding stage, the model processes the prompt, X\u209a\u1d63\u2092\u2098\u209a\u209c = [X\u2081, \u2026\u2026, X\u2099] \u2208 \u211d\u1d3a\u02e3\u1d48. The KV cache and the hash table are first filled to full by the first C tokens. K\u2080 = {k\u2081, ..., kc}, V\u2080 = {v\u2081, ..., vc}, H\u2080 = h(K\u2080) = \u222a\u1d62\u2091[\u2081,c] h(k\u1d62). We then set t \u2190 C + 1, and begin Algorithm 1.\nDecoding Stage Let Xdecoding = [z\u2081, \u2026\u2026z\u209c] \u2208 \u211d\u1d40\u02e3\u1d48 be the generated tokens during auto-regressive decoding. In the decoding stage, we continue Algorithm 1 by setting t < -N + 1. The generation completes at time step N + T.\nComplexity Our strategy assumes a fixed memory budget, and therefore, uses constant memory. The computation overhead per time step is also constant, because Fscore is calculated for a constant C number of key vectors in the cache. The extra memory overhead that HASHEVICT introduces to each attention head is the hash table H, which only uses C * b bits of space and is independent of the sequence length. The hash table is stored on GPU memory and does not introduce any latency bottlenecks associated with CPU-to-GPU streaming [Strati et al., 2024]."}, {"title": "4 Experiments", "content": "Tasks We evaluated HASHEVICT across various tasks to demonstrate its effectiveness in reducing the memory cost of the KV cache while preserving the language quality of the generated text. Our experiments are split into four main categories: free response question answering, multiple choice, long-context retrieval and long-context summarization. Our long context retrieval tasks include the multi-key needle-in-a-haystack task and the common words task from [Hsieh et al., 2024]. Question answering tasks include GSM8K [Cobbe et al., 2021] and MedQA [Jin et al., 2021]. Summarizaiton tasks include GovReport and MultiNews from Bai et al. [2023].\nMetrics The question-answering tasks were evaluated using BERTScore (which includes precision, recall, and F1 scores), ROUGE (ROUGE-1, ROUGE-2 and ROUGE-L and ROUGE-Lsum), and GPT4-Judge. GPT-4 was prompted to look at both the model prediction and the ground truth answer, then provide a score from 1 - 5 on the coherence,"}, {"title": "4.1 Free Response Question Answering", "content": "We tested our strategy against tasks that require generating accurate answers using multi-step reasoning. Specifically, we used the GSM8K and MedQA datasets to assess language quality for each strategy, given a constrained KV cache budget. Both tasks are used to test the potential side effects of compression on the LLM's reasoning ability.\nGSM8K GSM8K consists of grade-school-level math problems that typically require multiple reasoning steps. As shown in Figure 2, HASHEVICT strategy consistently outperforms the L2 norm-based method across various cache sizes. Notably, even when the KV cache budget is set to 50% of the full capacity, the HASHEVICT eviction strategy maintains a high answer quality, with minimal degradation in BERTScore F1, ROUGE-L, and GPT4-Judge scores. Additionally, HASHEVICT performs on par with H2O and Scissorhands without accumulating attention scores.\nMedQA MedQA is a free response multiple choice question answering dataset collected from professional medical board exams. We randomly sampled 100 questions from this dataset. Each question has 5 choices and only one correct answer, along with ground truth explanations and reasoning steps. Figure 3 illustrates that HASHEVICT performs better than all baseline methods for all cache budgets tested. For both datasets, HASHEVICT produced more coherent and helpful answers across all cache budgets than the baselines per Table 9.\nFor detailed experiment results of both question anwering tasks, and for comparison with Fastgen at various attention recovery ratios, please refer to Appendix A."}, {"title": "4.2 Multiple Choice Question Answering", "content": "We evaluated our method on multiple-choice versions of GSM8K and MedQA. Multiple choice is a more difficult test of a model's reasoning capability under the constraint of cache compression, as it takes away the ability to use intermediate results in the generated text. The model has to keep useful tokens during prompt compression in order to pick the correct answer choice.\nGSM8K Multiple Choice For the GSM8K multiple choice experiments, HASHEVICT significantly outperforms L2 for cache budgets of 30% and 50%. As shown in Figure 4a, the L2 method's accuracy drops significantly at smaller cache sizes, while the performance of HASHEVICT does not significantly drop until the cache budget is set at 10%.\nMedQA Multiple Choice Per Figure 4b, the MedQA multiple choice experiment, HASHEVICT offers better per-formance than L2 eviction for all tested cache budgets except for 50%. Performance between both methods is highly similar at lower budgets."}, {"title": "4.3 Long-Context Retrieval", "content": "To evaluate HASHEVICT's ability to retain and retrieve important pieces of information from long contexts, we used the Needle-in-a-Haystack and Common Words tasks from Hsieh et al. [2024] with 4K context length. These tests benchmark the ability of a compression strategy to retain important tokens inside the KV cache within a large, complex stream of context."}, {"title": "4.4 Long Text Summarization", "content": "To evaluate HASHEVICT's ability to handle exceptionally long context lengths, we incorporated the MultiNews and GovReport summarizations tasks from LongBench Bai et al. [2023]. We tested both tasks using the Llama3.1-8B-Instruct model and used context size of 16K tokens.\nMultiNews The MultiNews dataset contains clusters of 2-10 news articles discussing the same event or topic. The model is asked to provide a one-page summary of the articles. HASHEVICT outperforms all baselines in the MultiNews summarization task at 30-70% cache budget. At 90% cache budget, HASHEVICT still outperforms H2O and Scissorhands while being slighly lower thant L2.\nGovReport The GovReport dataset contains reports spanning a wide variety of national policy issues from the U.S. Government. The model is asked to produce a one-page summary of the reports. HASHEVICT performs on par with and sometimes slightly better than L2 at 30-70% cache budget, while not as well as H2O or Scissorhands."}, {"title": "4.5 Throughput", "content": "To evaluate the speed of HASHEVICT and baseline methods, we measured the decoding and prefilling speed during the MultiNews evaluation. Because the length of answers generated by each eviction strategy generates can be different, we report decoding and prefilling speed in tokens per second instead of elapsed time."}, {"title": "4.6 Memory Usage", "content": "Table 2 compares the memory usage of the KV cache and relevant data structures of L2 and HASHEVICT on the GSM8K and MedQA question answering experiments. HASHEVICT maintains H, a binary hash matrix of the attention keys in memory and, therefore, has slightly higher memory usage than L2 eviction. Our implementation uses 8 bits for binary values instead of 1 bit. Using 1-bit binary numbers would reduce the memory overhead of HASHEVICT by a factor of 8 and narrow the difference in memory usage between HASHEVICT and L2."}, {"title": "4.7 Ablation on Hash Dimension", "content": "To determine the effect of the hash dimension, we conducted an ablation study using the GSM8K free response dataset. Fixing the cache budget to 50%, we tested hash dimensions of 4, 8, 16, 32 and 64 bits. The choice of hash dimension does not significantly impact performance. In fact, 8 bits performed the best, but not noticeably better than higher dimensions. This demonstrates that HASHEVICT does not require a high hashing dimension and can be executed with minimal storage overhead. When using 8 bits, the storage overhead is 1 byte \u00d7 cache size. For example, in a Llama3 70B-Instruct deployment with 80 layers, 8 KV-heads, sequence length of 8192, batch size of 8 and 50% cache budget, hash dimension of 8-bits, we have that 16-bits and 32-bits only use an extra 20MB, 40MB, and 80MB respectively, which are significantly smaller than the KV cache size of 640GB. Detailed results can be found in Table 10 of Appendix B."}, {"title": "4.8 Attention Loss Ratio", "content": "Table 3: Attention Loss Ratio measured at 50% cache budget on the GSM8K question answering dataset. HASHEVICT achieves lower attention loss ratio compared to L2 and even the attention based Scissorhands.\nTo empirically verify that HASHEVICT selects and evicts tokens of low relevance, we measured the average attention loss of the attention heads for HASHEVICT and compared with L2, Scissorhands and H2O. Attention loss is defined as the sum of the attention probabilities for the evicted tokens. Or equivalently, 1 - the sum of the attention probabilities for the tokens in the compressed cache. The attention loss ratios were measured at 50% cache budget using prompts from the GSM8K question answering dataset. As per table 3, by accumulating attention score of tokens H2O has the lowest attention loss ratio as expected. HASHEVICT has slightly lower attention loss compared to L2 and even the attention-based Scissorhands, proving HASHEVICT's ability of discarding irrelevant tokens."}, {"title": "5 Discussion & Conclusion", "content": "In this paper, we introduce HASHEVICT, a novel attention-free eviction strategy for KV cache compression in transformer-based LLMs. By leveraging locality-sensitive hashing (LSH) to approximate cosine similarity, HASHEVICT dynamically determines which tokens to evict from the cache without performing costly attention calculations. Our experiments demonstrate that HASHEVICT can achieve 30-70% compression of the KV cache while maintaining strong performance across various tasks, including free-response Q&A, multiple-choice Q&A, and long-context retrieval.\nThe key advantage of HASHEVICT lies in its ability to efficiently compress the KV cache pre-attention, enabling significant memory savings and faster inference times. Compared to traditional strategies like L2 norm-based eviction [Devoto et al., 2024], HASHEVICT excels particularly in reasoning and multiple-choice tasks, where maintaining a diverse set of tokens in the cache is crucial for generating accurate and coherent responses.\nThere are several potential areas for future work. Investigating hybrid approaches that combine LSH-based eviction with attention-based mechanisms such as [Zhang et al., 2024a, Ge et al., 2023] could offer a middle ground between computational efficiency and retention of high-importance tokens. Further, reducing the overhead associated with maintaining binary hash codes (e.g., by optimizing bit precision) could further enhance the applicability of HASHEVICT to memory-constrained environments."}]}