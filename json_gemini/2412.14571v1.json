{"title": "SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar\nObject Detection", "authors": ["Ruoyu Xu", "Zhiyu Xiang", "Chenwei Zhang", "Hanzhi Zhong", "Xijun Zhao", "Ruina Dang", "Peng\nXu", "Tianyu Pu", "Eryun Liu"], "abstract": "3D object detection is one of the fundamental perception\ntasks for autonomous vehicles. Fulfilling such a task with a\n4D millimeter-wave radar is very attractive since the sensor is\nable to acquire 3D point clouds similar to Lidar while main-taining robust measurements under adverse weather. How-ever, due to the high sparsity and noise associated with the\nradar point clouds, the performance of the existing meth-ods is still much lower than expected. In this paper, we pro-pose a novel Semi-supervised Cross-modality Knowledge\nDistillation (SCKD) method for 4D radar-based 3D object\ndetection. It characterizes the capability of learning the fea-ture from a Lidar-radar-fused teacher network with semi-supervised distillation. We first propose an adaptive fusion\nmodule in the teacher network to boost its performance.\nThen, two feature distillation modules are designed to facil-itate the cross-modality knowledge transfer. Finally, a semi-supervised output distillation is proposed to increase the ef-fectiveness and flexibility of the distillation framework. With\nthe same network structure, our radar-only student trained\nby SCKD boosts the mAP by 10.38% over the baseline and\noutperforms the state-of-the-art works on the VoD dataset.\nThe experiment on ZJUODset also shows 5.12% mAP im-provements on the moderate difficulty level over the baseline\nwhen extra unlabeled data are available. Code is available at\nhttps://github.com/Ruoyu-Xu/SCKD.", "sections": [{"title": "Introduction", "content": "3D object detection is an essential perception task for au-tonomous vehicles operating in real traffic scenes. Thanks to\nthe dense point clouds acquired and the development of deep\nlearning technology, Lidar-based 3D object detection meth-ods(Yang et al. 2020; Yan, Mao, and Li 2018; Lang et al.\n2019; Yin, Zhou, and Krahenbuhl 2021) have made remark-able progress in recent years. However, constrained by its\nshort wavelength, Lidar performs poorly in adverse weather\nconditions such as rain and fog(Sheeny et al. 2021). In con-trast, millimeter-wave radar has gained widespread attention\ndue to its resistance to adverse weather and long measure-ment distance. Traditional 3D millimeter-wave radar could\nmerely provide two-dimensional point clouds and Doppler\nvelocity, which makes it difficult for 3D object detection.\nIn recent years, with the development of high-resolution 4D\nmillimeter-wave radar, exploring the 4D radar point clouds\nfor scene understanding task becomes very attractive.\nDespite promising prospects, point clouds of 4D radar still\nsuffer from the issues of high sparsity and flickering noise.\nThe point density of existing 4D radar is only less than one-\ntenth of that of Lidar, and the \"ghost point\" caused by the\nmulti-path effect also largely degrades its range measure-ments. Existing approaches based on pure radar(Xu et al.\n2021; Yan and Wang 2023; Zheng et al. 2023; Liu et al.\n2023) typically employ classical backbones that are origi-nally designed for Lidar. Without special consideration of\nthe nature of sensors, the performance of these approaches\nstill leaves much to be desired. Multi-modality fusion based\nmethods(Nabati and Qi 2021; Kim et al. 2023a,b; Xiong\net al. 2023; Lin et al. 2024; Wang et al. 2022b) usually be-have better, but the introduction of more modalities such as\ncamera and Lidar increases the cost of the system and de-creases real-time performance.\nIn this paper we propose SCKD, a Semi-supervised\nCross-modality Knowledge Distillation method to aggregate\nthe merits of multi-modal fusion and real-time of radar-only based methods. The main differences between our\nmethod and the existing mainstream cross-modality distilla-tion frameworks(Chen et al. 2022c; Chong et al. 2022; Bang\net al. 2024; Wang et al. 2023; Zhou et al. 2023) are shown\nin Figure 1. As shown in Figure 1(a) and (b), most of the ex-isting cross-modality distillation methods equip the teacher\nwith another modality different from the student's, empha-sizing the idea of the knowledge transfer from a strong-input\nteacher to a weak-input student. However, the different char-acteristics of the input and the de facto feature gap between\nthe teacher and the student are largely ignored, leading to\nlow effectiveness in knowledge distillation. Meanwhile, they\nall keep the ground truth as a necessary supervision for the\nstudent. In contrast, as shown in Figure 1(c), our method em-ploys a multi-modality fusion based teacher which contains\nthe same modality as the student. Besides improving the per-formance of the teacher, this manner also narrows the differ-ences of the feature spaces, making the knowledge transfer\neasier. Moreover, our method no longer needs the ground\ntruth supervision for the student, which converts the distilla-tion into a semi-supervision manner and opens the potential\nfor utilizing large quantities of unlabeled data.\nSpecifically, we design an adaptive fusion module in the\nteacher network to effectively fuse the feature of the Li-dar and radar. Two different ways of feature distillation,\nnamely, Lidar to Radar Feature Distillation(LRFD) and Fu-sion to Radar Feature Distillation(FRFD), are then pro-posed. Together with the Semi-Supervised Output Distilla-tion(SSOD), the pipeline effectively fulfills the knowledge\ntransfer between the teacher and the student. Extensive ex-perimental results show that our SCKD outperforms the\nstate-of-the-art methods, especially when large unlabeled\ndata are available.\nIn summary, our main contributions are as follows:\n\u2022 We propose a novel semi-supervised cross-modality dis-tillation framework for radar-based 3D object detection.\nLearning the knowledge from the teacher, simple student\nnetwork can boost its performance while maintaining the\nreal-time efficiency;\n\u2022 A Lidar and radar bi-modality teacher network embed-ded with adaptive fusion module is proposed to boost the\nperformance of the teacher and reduce the difficulty of\nknowledge transfer;\n\u2022 LRFD and FRFD module are designed to facilitate and\nenhance the feature distillation;\n\u2022 Semi-supervised output distillation is proposed, which\nimproves the performance and flexibility of the method;\n\u2022 Extensive experiments on the VoD and ZJUODset\ndatasets are carried out for evaluation. The results show\nthat our radar-only student network is able to boost the\nperformance of the baseline method by a large margin\nand outperforms the state-of-the-art methods."}, {"title": "Related Work", "content": "Lidar-based 3D object detection\nGiven 3D Lidar point clouds, Lidar-based 3D object de-tection can roughly be divided into point-based, pillar\nor voxel-based, and multi-view based methods. Approaches\nlike PointNet(Qi et al. 2017), PointRCNN(Shi, Wang, and Li\n2019), and 3D-SSD(Yang et al. 2020) directly extract feature\nfrom point clouds, while methods such as VoxelNet(Zhou\nand Tuzel 2018), Second(Yan, Mao, and Li 2018), PointPil-lars(Lang et al. 2019), and Centerpoint(Yin, Zhou, and Kra-henbuhl 2021) divide irregular point clouds into pillars or\nvoxels for feature extraction. The multi-view based methods\nfuse the features from different representations to achieve\nbetter performance. To mitigate the shortage of semantic in-formation in Lidar point clouds, Lidar-image fusion based\nmethods, e.g., PointPainting(Vora et al. 2020), PointAug-menting(Wang et al. 2021), MSFDFusion(Jiao et al. 2023),\nand LogoNet(Li et al. 2023) fuse the spatial and semantic\nfeatures at different scales, resulting in improved 3D object\ndetection performance.\nRadar-based 3D object detection\nDue to the absence of height information, 3D object de-tection is seldom carried out on traditional 3D radar. Most\nof the radar-only based detection works take the 4D radar\npoint clouds as input. RPFA-Net(Xu et al. 2021) utilizes a\nself-attention mechanism to enhance radar feature extrac-tion. MVFAN(Yan and Wang 2023) exploits valuable infor-mation of RCS and Doppler velocity and constructs a multi-view feature assisted detection network. Based on Point-Pillars, RadarPillarNet(Zheng et al. 2023) designs modules\nspecifically tailored to radar characteristics and improves the\nperformance. SMURF(Liu et al. 2023) introduces a novel\nnetwork branch for kernel density estimation to better fuse\nradar feature at different levels. Due to the high sparsity\nof radar point clouds, more studies have focused on fusing\nradar with other sensors. CenterFusion(Nabati and Qi 2021),\nCRAFT(Kim et al. 2023a), and CRN(Kim et al. 2023b) pri-marily focus on exploring the fusion of 3D radar and images.\nRecently, many studies have begun to investigate the fu-sion of 4D radar with other sensors. RCFusion(Zheng et al.\n2023), LXL(Xiong et al. 2023), and RCBEVDet(Lin et al.\n2024) probe the fusion of 4D radar and images, while In-terFusion(Wang et al. 2022b) and M2-Fusion(Wang et al.\n2022a) have ventured into the fusion of 4D radar and Lidar.\nAlthough these fusion-based methods can obtain better de-tection performance, they still suffer from higher sensor and\ncomputing cost, resulting in a much lower real-time perfor-mance than the radar-only methods.\nKnowledge Distillation for object detection\nKnowledge distillation is popularly known as a model com-pression method, which is first applied in image classifica-tion and 2D object detection tasks(Yang et al. 2022c; Chen\net al. 2022b; Yang et al. 2022b; Zheng et al. 2022b; Chen\net al. 2022a). Recently, many knowledge distillation works\nhave been proposed for 3D object detection. Depending\non whether the same sensor modality is employed for the"}, {"title": "SCKD: Semi-Supervised Cross-modality\nKnowledge Distillation", "content": "In this chapter, we elaborate our proposed SCKD framework\nin detail. As shown in Figure 2, the teacher is a Lidar-Radar\nbi-modality fusion network, while the student is a radar-only network. By the effective knowledge distillation of the\nteacher, the student can learn to extract sophisticated feature\nfrom the radar input and boost its detection performance.\nWe first introduce the design of the teacher network. Then,\nthe distillation methods along with the loss function of the\nnetwork are described.\nTeacher Network\nThe teacher shares similar backbone with the SEC-OND(Yan, Mao, and Li 2018), except for the bi-modality\ninput and an adaptive fusion module. The structure of\nthe teacher is shown in Figure 2. Compared to RadarDis-till(Bang et al. 2024), our teacher network integrates features\nfrom two modalities, containing much richer semantic infor-mation that is helpful for knowledge distillation. Lidar point\nclouds PL and 4D radar point clouds PR are first encoded\nby voxelization and sparse 3D convolution to acquire corre-sponding features FI and FR, respectively. The process is\nas follows:\n$F_{L}^{T}$ = Spconv3D(Voxelization(PL)) (1)\n$F_{R}^{T}$ = Spconv3D(Voxelization(PR)) (2)\nConsidering the different nature of the Lidar and radar\npoints, it is unwise to directly concatenate them and feed\nthem to the consequent 2D multi-scale CNN block of the\nSECOND backbone. To better explore the feature of the two\nmodalities, we propose an Adaptive Fusion (AF) module to\nadaptively weight and aggregate the two feature maps. As"}, {"title": "Feature Distillation", "content": "We choose SECOND as our student model. The goal of dis-tillation is to transfer the feature extraction capability to the\nstudent as much as possible. We propose two types of distil-lation, termed LRFD and FRFD, to accomplish this task.\nLRFD: Lidar to Radar Feature Distillation The feature\nmap from Lidar contains abundant object information which\ncan provide many hints to the extraction of student's radar\nfeature. Given current student feature FR, the LRFD takes\nthe teacher's Lidar feature FE as supervision for learning.\nHowever, directly enforcing similarities between the two\ntypes of features via a loss function tends to adversely af-fect the overall network performance(Chen et al. 2022b).\nTherefore, we feed the student's radar feature through an\nAdapter\u2517, which is a simple convolution layer, to simulate\nthe Lidar feature before computing the MSE loss as:\n$L_{LRFD}$ = MSE($F_{R\u2192L}$, $F_{L}^{T}$) (8)\nwhere\n$F_{R\u2192L}$ = Adapter\u2517($F_{R}^{S}$) (9)\nFRFD: Fusion to Radar Feature Distillation The fusion\nfeature in the teacher network contains weighted Lidar and\nRadar feature, which is more effective for the object detec-tion task. Comparing with the LRFD, distillation from the\nfused feature map has two advantages. Firstly, it contains\nmore valuable information than the pure Lidar feature for\nthe detection task. Secondly, the learning difficulty is lower\nsince the fused feature itself also contains radar informa-tion. One problem is that the channel number between the\nfused and the radar is different and has to be aligned be-fore distillation. Existing works(Chen et al. 2022b,c; Wang\net al. 2023; Bang et al. 2024) fulfill the channel alignment\nthrough a simple convolutional upscaling operation. How-ever, this operation will change the original fused feature and\ndamage the effect of distillation. We propose two separate\nadapters to accomplish this task. As shown in Eq. (10) (11),\ntwo convolution-layer-based adapters, i.e., AdapterL' and\nAdapter R' are responsible to separately map the student's\nradar feature space to the teacher's weighted Lidar and radar\nfeature space, as:\n$F_{R\u2192L}^{S}$ = Adapter\u2517'($F_{R}^{S}$) (10)\n$F_{R\u2192R}^{S}$ = Adapter R'($F_{R}^{S}$) (11)\nAfter that, we employ the MSE loss to calculate FRFD loss\n$L_{FRFD}$ as:\n$L_{FRFD}$ = MSE(Concat[$F_{R\u2192L}^{S}$, $F_{R\u2192R}^{S}$, $F_{fusion}^{T}$]) (12)"}, {"title": "SSOD: Semi-Supervised Output Distillation", "content": "Most of existing distillation-based detection methods, e.g.,\nRadarDistill(Bang et al. 2024), rely on ground truth labels\nas the main supervision. However, ground truth labels are\ngenerally obtained through manual annotation, which can be\nexpensive. Moreover, the existence of some difficult samples\nsuch as largely occluded objects means that directly using\nthe ground truth labels as supervision may not bring nec-essary benefits to the training. To mitigate these problems,\nwe propose to use the predictions of the teacher network as\nsupervisions for the student network. This semi-supervised\ntraining method has two advantages. Firstly, trained by the\nsmall quantity of the labeled data, the predictions (including\nsome false positive targets) of the teacher network are likely\nto provide more valuable information for the student net-work. Secondly, we can train the student network with much\nmore extra unlabeled data, which can possibly improve the\ntask performance at low costs.\nSpecifically, we select the detection targets DT of the\nteacher network based on a confidence threshold. The tar-gets with confidence above this threshold will be regarded\nas pseudo-labels for the student network, as:\n$D^{T}$ = 1(conf($D^{T}$) > \u03c3) * $D^{T}$ (13)"}, {"title": "Overall Distillation Loss", "content": "Considering that we have eliminated the supervision from\nthe ground truth, the student network is trained entirely by\nthe supervision of the teacher network. The overall distilla-tion loss is as follows:\n$L_{total}$ = \u03b1$L_{LRFD}$ + \u03b2$L_{FRFD}$ + $L_{SSOD}$ (15)\nwhere \u03b1 and \u03b2 are hyper-parameters to balance the losses."}, {"title": "Experiments", "content": "Dataset and evaluation Metrics\nWe conduct experiments on the popular VoD and ZJUOD-set datasets with accessible 4D radar and Lidar data.\nNuScenes (Caesar et al. 2020) and TJ4DRadset(Zheng et al.\n2022a) datasets are not chosen because the former only con-tains 3D radar, and the latter's LiDAR data are not available\nnow.\nVoD Dataset(Palffy et al. 2022) The VoD dataset is cur-rently the most popular 4D radar object detection dataset\nwhich includes Lidar, Radar, and Camera data. Following\nthe official partitioning, we divide the training and valida-tion set into 5139 and 1296 frames, respectively. In addition\nto evaluating in the entire annotated area, the dataset also re-quires evaluation on the driving corridor, which is a narrow\nregion that is more likely to impact driving. To keep with\nprevious works, we employ the AP11 evaluation metrics,\nand set the IOU thresholds for car, pedestrian, and cyclist\nto 0.5, 0.25, and 0.25, respectively.\nZJUODset(Xu et al. 2023) The ZJUODset is a dataset\nfor long-distance 3D object detection, with the farthest de-tection distance reaching up to 150 meters. It also contains\nthe data of 4D radar, Lidar and camera. Within the labeled\ndata, we allocate 2660 frames for training and the subse-quent 1140 frames for validation. We also use the rest 10640\nunlabeled raw frames for semi-supervised distillation. Due\nto the limited samples of 'pedestrian', we only evaluate on\nthe 'car' and 'cyclist' categories in this dataset. The metrics\nutilized for evaluation are AP40, with the IOU thresholds for\ncar and cyclist set to 0.5 and 0.25, respectively.\nImplementation Details\nFor the VoD dataset, the detecting range of the network is set\nto [0, 51.2m] on the x-axis, [-25.6m, 25.6m] on the y-axis,\nand [-3m, 2m] on the z-axis. The voxel size is set to 0.05m\n\u00d7 0.05m x 0.1m. For the ZJUODset, the detection region\nis defined as [0, 158.4m], [-39.6m, 39.6m] and [-5m, 3m]\non the x, y and z axis, respectively. A voxel size of 0.075m\n\u00d7 0.075m x 0.2m is employed for both the teacher and the\nstudent network.\nWe implement our SCKD based on OpenPCDet(Team\n2020) and mmdetection3d(Contributors 2020) framework.\nFor data augmentation, we use the random flipping along\nthe x-axis and random global scaling with the scaling fac-tor within 0.95 and 1.05. We employ AdamW optimizer for\nparameter update with an initial learning rate 0.001 and a\nweight decay factor 0.01. The learning rate is updated with\na cyclical decay method, with maximum 0.01 and minimum\n10-7. The retention threshold \u03c3 for the output distillation\nis set at 0.1, and the hyper-parameters \u03b1 and \u03b2 for the loss\nfunction are both set to 3 * 10-4. Two NVIDIA RTX 4090\nGPUs are employed during the training and distillation, with\nthe batch size set to 8."}, {"title": "Main Results", "content": "Results on the VoD dataset. The experimental results on\nthe VoD dataset are shown in Table 1. In addition to radar-only methods, we also list the methods based on the fusion\nof Radar and Camera for reference.\nCompared with the existing radar-only methods(Yan,\nMao, and Li 2018; Yan and Wang 2023; Zheng et al. 2023;\nLiu et al. 2023; Deng et al. 2023), our approach achieves\nstate-of-the-art performance. In contrast to the baseline\nmethod SECOND, which owns the same network structure\nas ours but is trained with ground truth labels instead of dis-tillation, our approach greatly improves the mAP by 10.38%\nand 6.21% in the entire annotated area and the driving cor-ridor, respectively. In comparison with the previously top-performing radar-based method SMURF, our SCKD also\nachieves an increase of 1.11% and 2.08% in mAP over the\nentire annotated area and driving corridor, respectively. The\nqualitative results are shown in Figure 5.\nThe radar-image fusion based methods usually perform\nbetter than the radar-only ones. However, they have to sac-rifice the real-time performance due to the large computing\ncosts of the fusion of image feature. It is worth mention-ing that apart from the currently best-performing method\nLXL(Xiong et al. 2023), our radar-only based method sur-passes the rest of those radar-image fusion methods(Zheng\net al. 2023; Lin et al. 2024; Chen et al. 2023; Liang et al.\n2022). Compared with LXL, we have a significant advan-tage in real-time performance, with more than 6 times faster\nthan LXL in inference speed.\nResults on the ZJUODset. We also conduct experiments\non the ZJUODSet dataset. As shown in Table 2, our SCKD\noutperforms its competitors at all difficulty levels. Since our\nmodel is semi-supervised, we further train our model with 4\ntimes more unlabeled raw data provided in the dataset. As\nexpected, the performance can further be improved by up to\n4.04% mAP on the moderate level, which shows the great\npotential of our semi-supervised distillation mechanism."}, {"title": "Ablation Study", "content": "The ablation results are carried out on the VoD dataset, and\nthe results are shown in Table 3 and Table 4.\nEffects of SSOD. Comparing (b) with (a) in Table 3, it can\nbe seen that the SSOD is more effective than GT in super-vising the student model. Integrating both GT and SSOD can"}, {"title": "More Discussion", "content": "The role of the bi-modality-fusion based teacher. The\nexperimental results of SCKD distilled from different con-figurations of teacher are shown in Table 5. Compared to the\nLidar-only teacher, the final Lidar-Radar-fusion teacher can\nimprove mAP of student network by 3.84% and 3.01% in the"}, {"title": "Conclusion", "content": "In this paper, we propose a semi-supervised cross-modality\ndistillation method for 3D object detection based on 4D\nradar-only. We design a bi-modality fusion based teacher\nnetwork, which is strengthened with adaptive fusion and ran-dom dropout upon the backbone. We then design three dis-tillation components, namely LRFD, FRFD, and SSOD, at\nthe feature and output levels of distillation. Without intro-ducing any computational overhead in the inference phase,\nour distilled student model significantly improves the object\ndetection performance over the baseline method, surpassing\nall state-of-the-art radar-based methods and even most of the\nradar-camera fused methods. Experimental results on both\nthe VoD and ZJUODset datasets demonstrate the effective-ness of our method."}]}