{"title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping", "authors": ["Amirreza Fateh", "Mohammad Reza Mohammadi", "Mohammad Reza Jahed Motlagh"], "abstract": "Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as PASCAL-52 and COCO-202 in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation is a key task in computer vision, where each pixel of an image is labeled as part of a specific category. This is important in many areas like OCR, autonomous driving, medical imaging, and scene understanding [1]\u2013[3]. To perform this task well, models need to learn detailed object boundaries. In recent years, deep Convolutional Neural Networks (CNNs) have made big improvements in this area [4]. However, these high-performing models usually need large datasets with lots of labeled examples [5], [6], which takes a lot of time and effort to create. In real-world scenarios, like in medical imaging or other fields where labeled data is limited, this becomes a big problem [7], [8]. To solve this, Few-shot Semantic Segmentation (FSS) has become a useful approach.\nFSS tries to segment new object classes in images using only a few labeled examples, called support images, that show the target class [9]. This method helps reduce the need for large datasets, making it more practical for real-world use [10]. Addressing the challenges of FSS requires handling differences in texture or appearance between the target object in the query image and similar objects depicted in the support examples. Effectively using the relationship between the query image and the support examples is essential in tackling FSS.\nFSS can be widely categorized into two groups: Prototype-based approaches and Pixel-wise methods. As shown in Figure 1-(a), prototype-based approaches involve abstracting semantic features of the target class from support images through a shared backbone network [11]. This process results in feature vectors called class-wise prototypes, which are obtained using techniques such as class-wise average pooling or clustering. These prototypes are then combined with query features through operations like element-wise summation or channel-wise concatenation. The combined features are refined by a decoder module to classify each pixel as either the target class or background [12]. In contrast, as shown in Figure 1-(b), pixel-wise methods take a different approach by focusing directly on pixel-level information rather than compressing it into prototypes. These methods aim to predict the target class for each pixel in the query image by comparing it directly with corresponding pixels in the support images. To achieve this, they establish pixel-to-pixel correlations between the support and query features, which allows the model to find precise matches even when the object's appearance varies [13]. This process is often enhanced by attention mechanisms, like those found in Transformer models, which help the model focus on important relationships between pixels. By avoiding the need for prototypes, Pixel-wise methods aim to preserve more detailed information, allowing for finer-grained segmentation [14], [15].\nWhile both groups have demonstrated efficacy, they also have certain limitations. Prototype-based methods may inadvertently discard complex local semantic features specific to the target class in support images. This can lead to coarse segmentation of the target class in query images, especially for objects with complex appearances. On the other hand, while pixel-wise methods have notably improved performance compared to prototype-based approaches, they grapple with computational complexity due to dot-product attention calculations across all pixels of support features and query features. Moreover, a large amount of pixel-wise support information can lead to confusion in attention mechanisms [13]. Also, a shared limitation across both approaches is the lack of use of encoder middle features in the decoder section. Many methods in both categories employ straightforward decoders that fail to incorporate encoder middle features. However, in few-shot scenarios where data samples are limited, leveraging the global features captured by the encoder in the decoder phase can prove to be highly beneficial.\nInspired by recent developments, we aim to develop a straightforward and effective framework to address limitations in FSS methods. A notable approach gaining traction is the Query-based\u00b9 Transformer architecture, which has demonstrated versatility across various computer vision tasks, including few-shot learning scenarios [16], [17]. This architecture utilizes learnable Query embeddings derived from support prototypes, enabling nuanced analysis of their relationships within the query feature map.\nInspired by previous works, as shown in Figure 1-(c), we have designed a novel Transformer-based module, known as the Spatial Transformer Decoder (STD), to enhance the relational understanding between support images and the query image. This module operates concurrently with the multi-scale decoder. Within the STD module, we introduce a common strategy: Using the prototype of support images as a Query, while utilizing the features extracted from the query image as both Value and Key embeddings inputted into the Transformer decoder. This formulation allows the Query to effectively focus on the semantic features of the target class within the query image. Furthermore, to reduce the impact of information loss resulting from the abstraction of support images into a feature vector named the 'support prototype,' we integrate global features from the intermediate stages of the encoder, which are fed with the support images, into our decoder. Incorporating these features allows us to leverage features from different stages of the encoder, thereby enriching the decoder's contextual understanding. Additionally, we introduce the Contextual Mask Generation Module (CMGM) to further augment the model's relational understanding, operating alongside the STD and enhancing the model's capacity to capture relevant contextual information."}, {"title": "II. RELATED WORKS", "content": "A. Semantic Segmentation\nSemantic segmentation, a crucial task in computer vision, involves labeling each pixel in an image with a corresponding class [18], [19]. CNNs significantly advanced semantic segmentation by replacing fully connected layers with convolutional layers, enabling the processing of images of various sizes [20]. Since then, subsequent advancements have focused on enhancing the receptive field and aggregating long-range context in feature maps. Techniques such as dilated convolutions [21], spatial pyramid pooling [22], and non-local blocks [23] have been employed to capture contextual information at multiple scales. More recently, Transformer-based backbones, including SegFormer [24], Segmenter [25], and SETR [26], have been introduced to better capture long-range context in semantic segmentation tasks. Further enhancing this approach, hierarchical architectures like the Swin Transformer [27] have achieved state-of-the-art performance by using shifted windows in their general-purpose backbones. In parallel, self-supervised pretraining strategies, such as the masked image modeling used in BEiT [28], have also shown strong results, fine-tuning directly on the semantic segmentation task and pushing the boundaries of model performance.\nSemantic segmentation tasks typically involve per-pixel classification. as demonstrated by approaches like Mask-Former [29] and Mask2Former [30], which predict binary masks corresponding to individual class labels. Older architectures, such as UNet [31], PSPNet [32], and Deeplab [33], [34], have also significantly contributed to the field by incorporating features like global and local context aggregation and dilated convolutions to increase the receptive field without reducing resolution. Building upon these foundational approaches, more recent studies, including CRGNet [35] and SAM [36], have focused on further improving model performance, exploring new techniques to enhance accuracy in segmentation tasks. Despite the progress made in per-pixel classification, addressing the challenge of segmenting unseen classes remains an open area for future research\nB. Few-Shot Semantic Segmentation\nFSS is a challenging task in computer vision, wherein the objective is to segment images with limited annotated examples, known as support images. Approaches to FSS can be categorized into various groups based on their primary aims and methodologies employed [37]. One significant challenge in FSS is addressing the imbalance in details between support and query images. Methods like PGNet [38] and PANet [39] aim to eliminate inconsistent regions between support and query images by associating each query pixel with relevant parts of the support image or by regularizing the network to ensure its success regardless of the roles of support and query. But methods like ASGNet [37], on the other hand, focuses on finding an adaptive quantity of prototypes and their spatial expanses determined by image content, utilizing a boundary-conscious superpixel algorithm.\nAnother critical aspect of FSS is bridging the inter-class gap between base and novel datasets. Approaches like RePRI [40] and CWT [41] address this gap by fine-tuning over support images or episodically training self-attention blocks to adapt classifier weights during both training and testing phases. Additionally, architectures designed for supervised learning often trouble recognizing objects at different scales in few-shot scenarios. To address this issue, new methods have been developed to allow information exchange between different resolutions [42].\nMoreover, ensuring the reliability of correlations between support and query images is essential in FSS. Methods like HSNet [43] and CyCTR [44] utilize attention mechanisms to filter out erroneous support features and focus on beneficial information. VAT [45], meanwhile, employs a cost aggregation network to aggregate information between query and support features, leveraging a high-dimensional Swin Transformer to impart local context to all pixels.\nOverall, the field of FSS is advancing rapidly with innovative methods aimed at enhancing model performance and overcoming challenges in adapting segmentation models to novel classes with limited annotated data. These efforts are driven by the ongoing need to improve the effectiveness and versatility of segmentation models in real-world applications."}, {"title": "III. PROPOSED METHOD", "content": "A. Problem Definition\nIn FSS, the task involves segmenting images belonging to novel classes with limited annotated data. We operate with two datasets, $D_{train}$ and $D_{test}$, each associated with class sets $C_{train}$ and $C_{test}$, respectively. Notably, these class sets are disjoint ($C_{train} \\cap C_{test} = \\{\\O\\}$), ensuring that there is no overlap between the classes in the training and test datasets. Each training episode consists of a support set $S$ and a query set $Q$, where $S$ includes a set of $k$ support images along with their corresponding binary segmentation masks, while $Q$ contains a single query image. The model is trained to predict the segmentation mask for the query image based on the support set.\nBoth $D_{train}$ and $D_{test}$ consist of a series of randomly sampled episodes (an episode is defined as a set comprising support images and a query image. During each epoch, we can have many episodes (e.g., 1000 episodes), each containing its own set of support and query images). During training, the model learns to predict the segmentation mask for the query image based on the support set. Similarly, during testing, the model's performance is evaluated on the $D_{test}$ dataset, where it predicts the segmentation mask for query images from the test dataset using the knowledge learned during training.\nOverall, the goal of FSS is to develop a model that can accurately segment images from novel classes with only a few annotated samples, demonstrating robust generalization capabilities across different datasets and unseen classes.\nB. Overview\nGiven a support set $S = \\{I_s, M_s\\}$ and a query image $I_q$, the objective is to generate the binary segmentation mask for $I_q$, identifying the same class as the support examples. To address this task, we introduce a straightforward yet robust framework, outlined in Figure 2. For simplicity, we illustrate a 1-shot setting within the framework, but this can be easily generalized to a 5-shot setting as well. The proposed method comprises several key components, including a shared pretrained backbone, support prototype, CMGM, a multi-scale decoder, and STD. These elements collectively contribute to the model's ability to accurately segment objects of interest in the query image based on contextual information provided by the support set. In the following, we'll take a closer look at each component, explaining its role and how it interacts within our framework.\n1) Backbone: In our proposed framework, we adopt a modified ResNet architecture, initially pre-trained on the ImageNet dataset, to serve as the backbone for feature extraction from raw input images, ensuring that the size of the output of each block does not reduce below a specified dimension. For instance, like [46], we define that the output sizes from conv2_x to conv5_x are maintained at 60 \u00d7 60 pixels. Specifically, we utilize a ResNet with shared weights between support and query images. This type of ResNet maintains the spatial resolution of feature maps at 60 \u00d7 60 pixels from the conv2_x stage forward, preserving finer details crucial for accurate segmentation. We extract high-level features (conv5_x), as well as mid-level features (conv3_x and conv4_x) from both support and query images using the backbone.\nThe mid-level features of the support image are denoted as $X_{conv3}^s$ and $X_{conv4}^s$, while the high-level features are denoted as $X_{conv5}^s$. Similarly, for the query image, the mid-level features are represented as $X_{conv3}^q$ and $X_{conv4}^q$, and the high-level features as $X_{conv5}^q$. To integrate mid-level features across different stages, we concatenate the mid-level feature maps from conv3_x and conv4_x stages and apply a 1\u00d71 convolution layer to yield a merged mid-level feature map, denoted as $X_{merged}^s$. This merging process ensures that the resultant feature map retains essential information from both mid-level stages, enhancing the model's ability to capture diverse contextual information (Equation 1, Equation 2).\n$X_{merged}^s = C_{1\\times1}(Cat(X_{conv3}^s, X_{conv4}^s))$ (1)\n$X_{merged}^q = C_{1\\times1}(Cat(X_{conv3}^q, X_{conv4}^q))$ (2)\nWhere $Cat$ denotes concatenation along the channel dimension, and $C_{1\\times1}$ denotes the $1 \\times 1$ convolution operation. These equations illustrate the process of merging mid-level features from different stages of the backbone network, resulting in a combined mid-level feature map that retains crucial information from both stages.\nThe decision to employ this modified ResNet architecture is grounded in its ability to balance computational efficiency with feature representation. By maintaining the feature map size at 60 \u00d7 60 pixels, the backbone effectively captures detailed spatial information while avoiding excessive computational overhead. This approach strikes a pragmatic balance between model complexity and segmentation performance, making it well-suited for our few-shot segmentation task, where computational efficiency is paramount.\n2) Support Prototype: In our proposed framework, the Support Prototype serves as a condensed representation of the mid-level features extracted from the support example ($X_{merged}^s$). The Support Prototype is obtained by applying a Masked Average Pooling (MAP) operation, which selectively aggregates information based on the support mask. Mathematically, the Support Prototype $P_s$ is defined in Equation 3.\n$P_s = F_{pool}(X_{merged}^s \\odot M_s)$ (3)\nWhere $F_{pool}$ represents the average pooling operation, and $\\odot$ signifies element-wise multiplication (Hadamard product) with the support mask $M_s$. The MAP operation involves computing the average pooling of the masked feature map, focusing solely on regions of interest specified by the support mask. This results in the generation of the Support Prototype, which encapsulates essential semantic information from the support example, facilitating effective few-shot segmentation.\n3) Contextual Mask Generation Module (CMGM): The CMGM is a novel component introduced by our framework, designed to enhance the contextual understanding between support and query images in FSS tasks. As shown in Figure 3, CMGM leverages the feature representations extracted from both the support and query images to generate a contextual mask that encapsulates pixel-wise relations indicative of the target object. This process involves computing the cosine similarity between the query feature vector and the support feature vector. Mathematically, cosine similarity $cos(q,s)$ is calculated as the dot product of the normalized query and support feature vectors. In a five-shot scenario, where there are five support examples, five cosine similarities are computed and subsequently averaged, yielding a novel cosine similarity measure representative of the collective support set.\n4) Multi Scale Decoder: The multi scale decoder in our proposed method is a critical component designed to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. The decoder consists of three stages, each comprising two residual layers. Input feature map undergoes a sequence of convolutional operations within residual layers to gradually upsample the mask image.\nAs shown in Figure 2, in the first stage of the decoder, the input feature map has a size of 60 \u00d7 60 pixels. This stage begins with two residual layers applied to the input feature map. Each residual layer receives input from combination of the previous layer's output and $X_{conv5}^q$. Following these layers, a convolutional operation is employed to upsample the mask image to a resolution of 120 \u00d7 120 pixels.\nSecond stage of the decoder, which operates on a feature map size of 120 \u00d7 120 pixels, has two residual layer like the first stage. Each residual layer takes input from combination of the previous layer's output and the merged mid-level features ($X_{merged}^q$) obtained from the support image's encoder. Since the size of $X_{merged}^q$ remains at 60 \u00d7 60 pixels, it is upsampled to 120 x 120 pixel resolution using a convolutional layer. This upsampled feature map, denoted as $X_{merged(120 \\times 120)}^q$.\nFinally, in the third stage of the decoder, which operates on a feature map size of 240 x 240 pixels, the input to each residual layer comprises the output from the combination of preceding layer and the upsampled $X_{merged}^q$ feature map. in this stage $X_{merged(120 \\times 120)}^q$, upsamples to 240 \u00d7 240 pixel resolution, denoted as $X_{merged(240 \\times 240)}^q$. This upsampled feature map is then integrated with the output from the preceding layer to form the input for subsequent processing.\nNotably, one of the distinctive aspects of our multi-scale decoder is the incorporation of mid-level and high-level features from the encoder, like U-Net architecture. Specifically, in each stage of the decoder, the input to the residual layers combines the output from the previous layer with either the $conv5\\_x$ features (the output of the last block of the encoder) or the merged mid-level features ($X_{merged}^q$) extracted from the support image's encoder. This fusion of features from different levels of abstraction enhances the decoder's ability to capture both detailed and contextual information essential for accurate segmentation.\n5) Spatial Transformer Decoder (STD): In parallel with the multi-scale decoder module, STD plays a pivotal role in refining the final segmentation mask. As illustrated in Figure 4, the STD module operates by leveraging multi-head cross-attention, focusing on target objects within the query features to generate semantic-aware dynamic kernels. This process begins by treating the support features as the Query embeddings, while the query features are utilized as the Key and Value embeddings within the STD. Through this strategic integration, the STD module adeptly captures intricate relationships between target objects present in the query features and their corresponding representations in the support features.\nThe architecture of the STD module employs multi-head cross-attention, rather than a traditional Transformer decoder paradigm. The prototype vector, representing the support features, is integrated as a Query, enriched with learnable positional encodings for heightened spatial context awareness. The query feature map serves as Key and Value embeddings for multi-head cross-attention, enabling comprehensive exploration of their interplay with the support features. Through this multi-head cross-attention process, the STD dynamically generates semantic-aware dynamic kernels crucial for fine-tuning segmentation predictions. The output of the STD module represents a segmentation mask embedding that captures the semantic information of the target objects within the query features. This embedding is crucial for refining the segmentation results. To integrate this information into the final segmentation output, the segmentation mask embedding is combined with the feature map of the output from the multi-scale decoder using a dot-product operation. This operation efficiently merges the information from both modules, enhancing the overall segmentation accuracy."}, {"title": "C. Loss function", "content": "In our method, we employ the Dice loss function to train our model. This loss function measures the dissimilarity between the predicted segmentation mask $M$ and the corresponding ground truth query mask $M_q$. The Dice loss is formulated in 4.\n$Dice Loss = 1 - \\frac{2 \\times |M \\cap M_q|}{|M|+|M_q|}$ (4)\nWhere $|M \\cap M_q|$ represents the intersection between the predicted and ground truth masks, and $|M|$ and $|M_q|$ denote the cardinality of the predicted and ground truth masks, respectively. Minimizing the Dice loss encourages the model to generate segmentation masks that closely match the ground truth masks, leading to more accurate segmentation results during training."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "A. Datasets\nWe evaluated our proposed method on two widely used datasets commonly employed in few-shot segmentation tasks: PASCAL - 5\u00b2 [57] and COCO \u2013 20\u00b2 [53].\nPASCAL-5\u00b2 Dataset. The PASCAL \u2013 5 dataset, introduced by Shaban et al. [57], is derived from the PASCAL VOC dataset [58], and augmented with the SDS [59]. The original PASCAL VOC dataset comprises 20 object categories. For PASCAL \u2013 52, these 20 categories are evenly divided into 4 subsets, each denoted as PASCAL \u2013 5\u00b2. Consequently, each subset consists of 5 distinct object categories.\nCOCO-20 Dataset. The COCO \u2013 20 dataset, introduced by Nguyen et al. [53], is derived from MSCOCO dataset [60]. The COCO - 20 dataset includes a total of 80 object categories. Similar to PASCAL 52, these 80 categories are divided into 4 subsets, with each subset denoted as COCO \u2013 20\u00b2. Each subset contains 20 distinct object categories. Notably, COCO \u2013 20\u00b2 presents a greater challenge due to its larger number of categories and images compared to PASCAL \u2014 5\u00b2.\nCross-Validation Training. To ensure robust evaluation, we adopted a cross-validation training strategy commonly employed in few-shot segmentation literature. Specifically, we divided each dataset into four subsets. Three subsets were utilized as training sets, while the remaining subset served as the test set for model evaluation. During testing, we randomly selected 1000 support-query pairs from the test set for evaluation.\nB. Experimental Setting\nWe implemented our proposed method using PyTorch version 1.8.1. For feature extraction, we employed pretrained ResNet-50 and ResNet-101 backbones, which were originally trained on the ImageNet dataset. During training, the parameters of these pretrained models were frozen, and only the newly added modules were trainable. For training on the COCO-20 dataset, we conducted training for each fold over 30 epochs. Conversely, for the PASCAL-5 dataset, training was extended to 60 epochs to ensure optimal convergence."}, {"title": "C. Loss function", "content": "We employ the following evaluation metrics to assess the performance of our proposed method:\nMean Intersection over Union (mIoU). mIoU is a widely used metric for evaluating segmentation performance. It calculates the average intersection over union (IoU) across all classes in the target dataset (Equation 5).\n$mIoU = \\frac{1}{C} \\sum_{i=1}^{C} IoU_i$ (5)"}, {"title": "D. Comparison with SOTA", "content": "In this subsection, we compare our proposed method with several SOTA methods on both the PASCAL 52 and COCO - 20 datasets. We present the results in Table I and Table II, respectively, where we report the mIoU and FB-IoU scores under both 1-shot and 5-shot settings, along with the final FB-IoU value. The results of other methods are obtained from their respective original papers.\nResults on PASCAL-5 Dataset. As shown in Table I, our proposed method, utilizing ResNet50 and ResNet101 backbones, consistently surpasses SOTA methods in both 1-shot and 5-shot scenarios across all four folds of the PASCAL-5 dataset. Notably, our method achieves the one of the highest performance across all folds.\nResults on COCO-20 Dataset. Similarly, Table II presents the results on the COCO 20 dataset, where our proposed method demonstrates superior performance under both ResNet50 and ResNet101 backbones in both 1-shot and 5-shot settings. Our method consistently outperforms all competing approaches across all four folds of the COCO \u2013 20\u00b2 dataset, consistently achieving either first or second rank. We obtained the highest mIoU scores in several folds and secured the second rank in others. Notably, our method exhibits superior performance in terms of mean and FB-IoU scores, further emphasizing its effectiveness and robustness.\nIt is important to highlight that our proposed method maintains a remarkably low number of learnable parameters, with only 1.5 million parameters. This stands in stark contrast to some SOTA methods, which possess significantly higher parameter counts, exceeding 40 million parameters in certain cases. This demonstrates the efficiency and effectiveness of our approach in achieving superior segmentation performance while maintaining a compact model architecture."}, {"title": "E. Cross-dataset task", "content": "In this study, we investigate the cross-domain generalization capabilities of our proposed few-shot segmentation method through rigorous domain shift testing. Specifically, we trained our model on the COCO \u2013 20 dataset and conducted testing on the PASCAL-5 dataset to evaluate its adaptability across different datasets and domain settings."}, {"title": "F. Ablation Study", "content": "For the ablation study, we conduct experiments on the COCO-20 dataset using the ResNet50 backbone in 1-shot scenario. Our first aim is to investigate the individual impact of various components on the segmentation performance. Table IV show the impact of each component of the proposed method.\nThe first row of Table IV represents the performance of the baseline model, consisting solely of the backbone architecture and support prototype mechanism. Subsequent rows introduce additional components incrementally, including the CMGM, STD, and multi-scale decoder.\nAs shown in Table IV, each component contributes to an improvement in segmentation performance, with the Multi Scale Decoder showcasing the most substantial impact. The progressive integration of these components results in a notable enhancement in mIoU scores across all folds, underscoring their significance in refining segmentation masks and capturing contextual information effectively.\nFurthermore, in Figure 5, we present the qualitative results obtained by incorporating each component into the baseline model on the COCO \u2013 20 dataset. As illustrated in Figure 5, the addition of each component leads to noticeable improvements in the segmentation results. Particularly, the integration of the multi-scale decoder component demonstrates significant enhancement in segmentation accuracy.\nTo further explore the influence of the architecture within the multi-scale decoder, we conducted an ablation study varying the number of residual blocks in each stage. Figure 6 provides an overview of the Multi-Scale Decoder with different numbers of residual blocks in each stage. The experiment involved evaluating the segmentation performance on the COCO \u2013 20 dataset using the ResNet50 backbone in a 1-shot scenario. As depicted in Table V, we examined configurations ranging from one to four residual blocks per stage. Interestingly, the results revealed that the optimal segmentation performance was achieved with three residual blocks in each stage. This finding suggests that an appropriate balance in the depth of the decoder architecture plays a crucial role in enhancing segmentation accuracy. Too few blocks may limit the model's capacity to capture intricate features, while an excessive number of blocks could lead to overfitting or computational inefficiency. Therefore, our results underscore the importance of carefully tuning the architecture parameters to achieve optimal performance in few-shot segmentation tasks."}, {"title": "V. CONCLUSION", "content": "In conclusion, our proposed few-shot segmentation framework, leveraging a combination of components including a shared pretrained backbone, support prototype mechanism, CMGM, STD, and multi-scale decoder, has demonstrated remarkable efficacy in achieving SOTA performance on both PASCAL-5\u00b2 and COCO \u2013 20\u00b2 datasets. Through extensive experimentation and ablation studies, we have highlighted the critical contributions of each component, particularly emphasizing the significant impact of the multi-scale decoder in enhancing segmentation accuracy while maintaining computational efficiency. Looking ahead, further investigation into the dynamic adaptation of prototype representations and the exploration of additional attention mechanisms could offer avenues for improving the adaptability and robustness of our method across diverse datasets and scenarios. Additionally, exploring semi-supervised learning paradigms could enhance the generalization capability of our framework, enabling effective segmentation in scenarios with limited labeled data. These avenues for future work hold promise for advancing the effectiveness and applicability of few-shot segmentation methods in real-world scenarios."}]}