{"title": "Complex Ontology Matching with Large Language Model Embeddings", "authors": ["Guilherme Santos Sousa", "Rinaldo Lima", "Cassia Trojahn"], "abstract": "Ontology, and more broadly, Knowledge Graph Matching is a challenging task in which expressiveness has not been fully addressed. Despite the increasing use of embeddings and language models for this task, approaches for generating expressive correspondences still do not take full advantage of these models, in particular, large language models (LLMs). This paper proposes to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The generation of correspondences is performed by matching similar surroundings of instance sub-graphs. The integration of LLMs results in different architectural modifications, including label similarity, sub-graph matching, and entity matching. The performance word embeddings, sentence embeddings, and LLM-based embeddings, was compared. The results demonstrate that integrating LLMs surpasses all other models, enhancing the baseline version of the approach with a 45% increase in F-measure.", "sections": [{"title": "1 Introduction", "content": "Ontology matching (and more broadly, knowledge graph matching) aims at enabling interoperability between knowledge expressed in different schemes. This task is at the core of knowledge graph-oriented applications. While the ontology matching field has reached some maturity, most of the matching approaches still focus on generating simple correspondences (i.e., those linking one single entity of a source ontology to one single entity of a target ontology, as Authors = Writer). However, this type of correspondence is not expressive enough to fully cover the different kinds of heterogeneities (lexical, semantic, conceptual, granularity) from different schemes. The need for complex correspondences (i.e., those involving logical constructors or transformation functions, as e.g., Accepted_Paper = Paper \u2203hasDecision.Acceptance) has been recognized across various fields, such as cultural heritage [16], agronomic [23], or still biomedical [12].\nWith the rise of language models, recent matching approaches rely on such models [21]. This increased adoption is due to their capability of modeling the textual and structural information present in ontologies and knowledge graphs."}, {"title": "2 Proposed Approach", "content": "Baseline approach CANARD takes as input a set of SPARQL SELECT queries over the source ontology, which express the user needs in terms of alignment. The reader can refer to [25] for details. According to query arity, three types of queries are considered: a unary question expects a set of instances, e.g., \"Which are the accepted papers?\" (paper1), (paper2); a binary question expects a set of instances or value pairs, e.g., \"What is the decision on a paper?\" (paper1, accept), (paper2, reject); and an-ary question expects a tuple of size \u2265 3, e.g., \"What is the decision associated with the review of a given paper?\" (paper1, review1, weak accept), (paper1, review2, reject). Queries for the approach are limited to unary and binary questions, of select type, and no modifier\u00b3. \u0421\u0410-NARD requires that the source and target ontologies have an Abox with at least one common instance for each SPARQL query. The overall approach is articulated in 9 steps (Figure 1). These steps and the subsequent modifications are introduced in the rest of the paper. Overall, the matching is performed by finding the surroundings of the target instances which are lexically similar to the CQA. The hypothesis behind the approach is to rely on a few examples (answers) to find a generic rule that describes more instances.\nRevised approach In the baseline approach, two core steps require the lexical comparison of entity labels: (1) finding common instances between source and target KG (Step 4 in Figure 1); and (2) computing the similarity between SPARQL query labels and the target subgraphs retrieved from the common instances found in the linking step (Step 7 in Figure 1). To enhance the effectiveness of similarity methods in these steps, the proposal is to incorporate embedding similarity. LLMs are employed to encode textual information and generate embeddings for each node in the KG. The hypothesis is that leveraging LLMs can"}, {"title": "Embedding Generation", "content": "The embeddings are prepared before the matching process starts. The ontology is loaded, and all labels from the ontology entities are processed to generate the embeddings, as illustrated in Figure 2. First, each label is tokenized into individual tokens using the model default tokenizer. Then the tokens are fed into the LLM that produces embeddings as output. The last hidden layer of this output is averaged to produce the final embedding of the label. Specifically, the output from the last hidden layer is a tensor of dimensions (B, S, N), where B is the batch size, S is the number of tokens in the label, and N is the embedding dimension. Second, to generate a single embedding vector for each label, the embeddings are averaged over the sentence dimension (S). This results in a final embedding of dimensions (B, 1, N), producing a fixed-size embedding for each label. This process is uniform across all models, regardless of their architecture (encoder-only, decoder-only, or encoder-decoder), as all models provide a last hidden layer output that can be averaged."}, {"title": "Matching Step", "content": "The proposed pipeline is presented in Figure 1 where one of the major differences from the baseline approach concerns the three variations of how the embeddings are combined in the subgraph similarity step (step 7 in Figure 1). The overall approach leverages a SPARQL query to guide the alignment of entities within the query context, effectively reducing the matching space. This process involves using the query to retrieve instances that serve as anchors for identifying common subgraphs between the source and target datasets. These subgraphs, which can consist of triples or paths (complex entities), vary depending on the query type (unary or binary). Unary queries result in triple subgraphs, while binary queries yield path subgraphs. Consequently, different aggregation methods are applied to each subgraph type (Figure 4). Subsequently, the identified subgraphs are compared based on their similarity to the entities mentioned in the query text. For instance, a query regarding accepted papers"}, {"title": "2.1 Instance embeddings (IE)", "content": "The Instance Embeddings (IE) setting involves integrating embeddings into the process of identifying common instances between the different KGs. In the baseline approach, the SPARQL query is consulted in the source KG and the resulting instances are used to find the corresponding instances in the target KG. These instances will serve as anchor links between the source and target knowledge graphs and used to retrieve the subgraphs in target KG. For this search, the baseline architecture seeks for common instances associated by predicates such as rdf-schema#seeAlso, owl#sameAs, skos/core#closeMatch, or skos/core#exactMatch. If these predicates are not found, an exact string matching is performed between the source and target KG (step 4 in Figure 1).\nGiven the unreliability of exact string matching in finding similar data within this context, incorporating embeddings can enhance the process of identifying similar instances between the knowledge bases. In the proposed approach, embeddings for the resulting entities are retrieved and stacked. Then to find similar instances, the labels of source query instances are embedded, and a cross-cosine similarity is computed between the instance embeddings in the source and the stacked embeddings from the target KG, resulting in a similarity matrix. Then, the instance with the highest similarity score in this matrix is selected, and, if it surpasses the link similarity threshold, it is returned as a link between the instance in source KG and the corresponding instance found in the target KG."}, {"title": "2.2 Similarity step settings", "content": "In this step, the Levenshtein similarity metric is replaced with embedding similarity (step 7 in Figure 1), which can be configured in three different settings as depicted in Figure 3. These settings alter the level of aggregation of the embeddings before comparison and are progressively applied while keeping each modification from the previous ones. A threshold filter is applied to determine which parts of the subgraphs contribute to the final similarity value, allowing for similarity values greater than 1 in the final value."}, {"title": "Label embedding similarity (LES)", "content": "Previously, in this step, all labels retrieved from the entities present in the SPARQL query source SELECT distinct ?s WHERE {?s a <:AcceptedPaper>. } (e.g., \"accepted paper\", \"paper accepted in a conference\") are compared with the labels of the entities in the subgraphs (Step 7 in Figure 1) retrieved after the linking step (Step 4 in Figure 1) using the Levenshtein similarity. This resulted in NX M computations, where N denotes the labels associated with the SPARQL query, and M represents the labels from the entities in the target subgraphs. The final similarity is calculated as the sum of all similarity values between the labels in the Cartesian product N\u00d7M, with similarities below a certain threshold being filtered out. The Levenshtein metric is known to introduce false positive correspondences that are lexically similar but not equivalent (e.g., \"Review\" and \"Reviewer\"). By replacing this metric with similarity derived from embedding representations, a better comparison of semantic information in labels becomes feasible, as false positives can be filtered out by assessing the semantic meaning present in the embeddings. In the new architecture, embeddings are employed to compute the similarity between the labels associated with the SPARQL queries and the labels in target subgraphs. An embedding is retrieved for each label before comparison and then the simi-"}, {"title": "Embeddings of SPARQL query (ESQ)", "content": "Another improvement in the architecture builds upon the previous one and focuses on the aggregation of SPARQL query embeddings in the similarity comparison in Step 7 (Figure 1). In the previous configuration, the embeddings from the labels are individually compared in a cross-product manner, and the final similarity is computed as the summation of each similarity after filtering. While the initial modification enhanced semantic comparison by employing individual embeddings for each label, aggregating the SPARQL query embeddings before comparison can offer a more contextual representation of the SPARQL query and enhance the quality of the correspondences identified. The proposed modification involves aggregating embeddings associated with each label extracted from the SPARQL query by averaging them once before all similarity comparisons with labels retrieved from target subgraphs. Once the aggregated embedding for the SPARQL query is obtained, it is compared with the embeddings of labels retrieved from target subgraphs using cosine similarity, similar to the previous modification. However, instead of comparing individual embeddings, the aggregated embedding is compared directly with the embeddings of labels in the subgraphs, and the threshold filter is also applied. This process is illustrated in Figure 3 (B). It offers advantages over the previous one since aggregating embeddings can capture a richer semantic context of the SPARQL query. Comparing the embeddings individually may lack essential information to identify some correspondences, which can be overcome by aggregating the embeddings beforehand."}, {"title": "Subgraph embeddings (SE)", "content": "Inspired by graph neural network embeddings [27] that aggregate node embeddings to represent graphs and subgraphs, this modification aims to capture the collective semantic information embedded within subgraphs. It entails aggregating embeddings associated with labels extracted from subgraphs, considering both unary and binary queries. In the baseline approach, unary and binary queries generate different types of subgraphs, and the embeddings of these subgraphs vary accordingly. For unary SPARQL queries, the subgraph consists of triples composed of subject, predicate, and object. In addition, subjects and objects are associated with subjectType and objectType, respectively, which correspond to the most similar type of the corresponding entity determined by employing embedding similarity in the entity labels."}, {"title": "3 Experiments", "content": "Dataset The dataset used in this experiment consists of the populated version of the OAEI Conference benchmark5. It comprises 5 ontologies and 100 manually generated SPARQL queries. The evaluation is conducted in pairs, yielding a total of 20 evaluation pairs. For each pair, the SPARQL queries were taken from the source ontology, and thus, the order of the ontologies in the pair counts. This dataset has been chosen as it is equipped by CQAs and is already used for running several systems, which allows for comparisons.\nModels The LLMs were selected from the Massive Text Embedding Bench-mark (MTEB) [15], which serves as a benchmark for evaluating various lan-guage models across diverse text embedding similarity tasks such as clustering, retrieval, and classification. The proposed approach is tested with the 11 best-performing models on average. Not all models were selected strictly based on their leaderboard order. Some models were excluded due to requiring a paid API subscription where the free subscription proved insufficient for our experi-mental needs, while others were omitted because they exceeded the capacity of the GPUs available to us at the time. The leaderboard of models was accessed through https://huggingface.co/spaces/mteb/leaderboard on 13/03/2024. These models were loaded using the Transformers library [26] from Hugging Face. In addition to the selected LLMs, smaller embedding models are included for comparison purposes: BERT-INT [22] in the entity link step; and word em-beddings such as GloVe [18], Word2Vec [14], and FastText [5] in other steps, resulting in a total of 15 models. The selected models are presented in Table 1."}, {"title": "Metrics", "content": "The evaluation metrics used here are the ones adopted in the OAEI campaigns for the Populated Conference dataset. These metrics are based on the comparison of instance sets. The generated alignment by the systems is used to rewrite a set of reference source SPARQL queries whose results (set of in-stances) are compared to the ones returned by the corresponding target reference SPARQL query. This comparison shows the overall coverage of the generated alignment concerning the knowledge needs and the best-rewritten query6. A bal-ancing strategy calculates the intrinsic alignment precision based on common instances. Given an alignment Aeval to be evaluated, a set of SPARQL query reference pairs querypairs (composed of source querys and target queryt), kbs the source knowledge base, kbt a target knowledge base, and f an instance set (I) comparison function:\n$cov (Aeval, querypairs,kbs, kbt, f) = average_{(querys, queryt) \\in querypairs} f(I_{querys}^{kbs}, bestqt)$\nDifferent functions f can be used to compare the similarity of instance sets (overlap, precision-oriented, recall-oriented, etc.). Here, coverage is based on the queryFmeasure (also used for selecting the best-rewritten query). This is moti-vated by the fact that it better balances precision and recall. Given a reference instance set $I_{ref}$ and an evaluated instance set $I_{eval}$:\n$QP = \\frac{|I_{eval} \\cap I_{ref}|}{|I_{eval}|}$\n$QR = \\frac{|I_{eval} \\cap I_{ref}|}{|I_{ref}|}$\n$queryFmeasure(I_{ref}, I_{eval}) = 2 \\times \\frac{QR \\times QP}{QR+QP}$\n$bestqt = argmax_{qt \\in rewrite(querys, Aeval, kbs)} queryFmeasure(I_{querys}^{kbs}, I_{qt}^{kbt})$\nBalancing coverage, precision is based on classical (i.e., scoring 1 for same instance sets or 0 otherwise) or non-disjoint functions f:\n$precision(Aeval, kbs, kbt, f) = average_{(es,et) \\in Aeval} f(I_{es}^{kbs}, I_{et}^{kbt})$\nParameters The proposed approach takes several inputs, including the source ontology path, target ontology path, output folder, link for embedding files, similarity threshold, architecture setting, and link similarity (if embeddings are used in the link step). There are 20 possible pairs for both source and target ontologies. In addition, 6 possible architecture settings can be combined with 2 possible link types. With the selected models, there are 15 choices, leading"}, {"title": "4 Results and Discussion", "content": "The analysis of all models in all settings is presented in Figure 5. The highest results are observed from LES and ESQ. In both settings, increasing the size of the models also enhances the system's performance. The setting with the poorest results is SE. In this setting, the embeddings with some LLMs exhibit reduced performance, while GloVe embeddings demonstrate less degradation. Additionally, the setting IE improves the precision of the matcher in all tested models while reducing the query-based metrics in some models.\nThe results of the best model in all architecture settings are presented in Table 2. The setting achieving the highest query-oriented classical and query-oriented overlap is LES with embeddings from the LLM GritLM-7B, reaching 0.37 compared to the baseline's 0.35 and 0.36 compared to the baseline's 0.35, respectively. In query-oriented recall, the baseline still achieves the best results with 0.36 compared to the second-place ESQ (ignore case) with SFR-Mistral reaching 0.32. For query-oriented precision and query-oriented F-measure, the ESQ setting with the LLM GritLM-7B reached the best results with 0.68 in precision and 0.68 in F-measure compared to the baseline's 0.47 in both metrics. In precision-oriented evaluation, the setting in the IE with Stella-Base achieved the highest results in all metrics.\nAs depicted in Table 2, LES and ESQ exhibit the highest results when LLMs are applied, both individually and on average with the other models. These set-tings involve fewer embedding aggregations. It is also evident that as the number of aggregations increases, as in the SE configurations, the results of all models de-teriorate. One of the reasons for this degradation is that combining embeddings"}, {"title": "Impact of each setting", "content": "Another experiment conducted is the impact of each setting when using the same LLM to see how the aggregation of embeddings and also how the use of IE impacts the matcher performance. In this experiment, the LLM GritLM was considered and evaluated with all. The results of this evaluation are in Table 4. The values from the LES and Embeddings of the SPARQL query are quite similar. The SE reduces the results in both precision and query-oriented. In the IE the results of the precision-oriented increase while the query-oriented is reduced compared to the best approaches. Also, the values in the query-oriented and precision-oriented IE settings follow an inverse relation with the change of threshold, when the threshold increases the query-oriented values increase while the precision-oriented values decrease. The last analysis is regarding the runtime of each setting. The experiments were conducted using the same LLM embeddings in the same ontology pair between cmt and conference varying only in the architecture setting. All the experiments were run on CPU without GPU acceleration. The fastest setting is the baseline approach with 29 seconds. The ESQ, SE, and IE take around 2 minutes. Among these, the ones with more aggregations have reduced runtime. The setting with the highest runtime is the IE with 2 hours and 11 minutes. This high runtime is due to the need to get the similarity of all instances in the target knowledge graph to get the equivalent instances in the link step."}, {"title": "5 Related Work", "content": "Previous work in ontology matching has explored the use of embeddings and language models. The review in [21] categorizes ontology matching approaches"}, {"title": "6 Conclusions and Future Work", "content": "This paper has proposed to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The proposed approach has achieved superior results in nearly every metric compared to the baseline without embeddings, as well as improvement over other state-of-the-art systems in this task. The approach does not require any training or reference alignments. Also, the models used to generate the em-beddings are not fine-tuned improving the approach capacity of generalization. The approach can be extended in several directions. The guidance provided by user needs is both a strength and a limitation of the approach (it facilitates generalization across a limited number of instances but requires users' ability to express their needs as SPARQL queries). The first direction for extension involves devising a purely T-Box strategy. Second, the problem could be sub-divided into sub-tasks through ontology partitioning, given the inherently vast search space of the task. Third, exploring improved aggregation techniques for subgraphs may yield superior results. Finally, fine-tuning LLMs and delving deeper into the prompts guiding the creation of entity embeddings can also be addressed."}]}