{"title": "Towards Within-Class Variation in Alzheimer's Disease Detection from Spontaneous Speech", "authors": ["Jiawen Kang", "Dongrui Han", "Lingwei Meng", "Jingyan Zhou", "Jinchao Li", "Xixin Wu", "Helen Meng"], "abstract": "Alzheimer's Disease (AD) detection has emerged as a promising research area that employs machine learning classification models to distinguish between individuals with AD and those without. Unlike conventional classification tasks, we identify within-class variation as a critical challenge in AD detection: individuals with AD exhibit a spectrum of cognitive impairments. Given that many AD detection tasks lack fine-grained labels, simplistic binary classification may overlook two crucial aspects: within-class differences and instance-level imbalance. The former compels the model to map AD samples with varying degrees of impairment to a single diagnostic label, disregarding certain changes in cognitive function. While the latter biases the model towards over-represented severity levels. This work presents early efforts to address these challenges. We propose two novel methods: Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two problems respectively. Experiments on the ADRESS and ADRESSo datasets demonstrate that the proposed methods significantly improve detection accuracy. Further analysis reveals that SoTD effectively harnesses the strengths of multiple component models, while InRe substantially alleviates model over-fitting. These findings provide insights for developing more robust and reliable AD detection models.", "sections": [{"title": "I. INTRODUCTION", "content": "Alzheimer's Disease (AD) poses a significant and escalating challenge within the aging population, characterized by progressive cognitive decline across multiple domains including memory, attention, and executive function [1]. Early screening for AD is crucial for timely intervention and management. Conventional screening protocols rely predominantly on in-person clinical assessments, often leveraging neurocognitive tasks such as picture description to identify potential AD cases [2]-[4]. Recent advances in machine learning have enabled speech-based automatic AD detection as a promising, cost-efficient screening approach. Typically, AD detection is framed as a binary classification task, i.e., models trained on audio recordings or transcripts from assessment tasks classifing participants as AD or non-AD [5]-[7]. This modeling largely inherits the paradigm of standard machine learning classification, focusing on extracting discriminative features or patterns for AD, followed by downstream classification algorithms. In recent years, progress in AD detection has been largely driven by the exploration of effective features and representation learning. Early works leveraged handcrafted acoustic and linguistic features. For example, Alhanai et al. [8] identified 12 acoustic features, including decreasing jitter, strongly associated with cognitive impairment. Winer and Frankenberg et al. [9], [10] demonstrated the relevance of linguistic features such as parts-of-speech (POS) and word categories to AD. More recently, the development of pre-trained models has mitigated the challenge in data-scarce tasks, leading to extensive research on deep embeddings for AD detection, encompassing speech-based [11]\u2013[14], text-based [15]-[18], and multi-modal approaches [11], [19].\nBeyond the challenge of feature extraction and representation learning, we propose that AD detection faces unique difficulties inherent to the nature of Alzheimer's Disease. Medical literature [20]-[22] establishes AD as a degenerative disorder characterized by a continuum of pathophysiological changes, resulting in gradual cognitive and functional decline. This spectrum of cognitive performance among AD patients, is illustrated in Fig. 1(a). Consequently, under standard classification modeling, samples with the same diagnostic label may exhibit varying degrees of cognitive impairment, leading to significant variability in features of interest. This distinguishes AD detection from conventional classification tasks such as image classification, which typically involve objects with consistent features within clearly defined categories. An ideal solution would be to model AD detection by regression or multi-way classification to capture this continuum. However, granular labels are rarely available for most cognitive assessment tasks, especially for open-source datasets. While some studies utilized continuous scores from specific cognitive tests (e.g., Mini-Mental State Examination) to label data collected from different tasks [23], [24], our work focuses on scenarios where data and labels originate from the same task. This eliminates potential cross-task validity concerns, as different tasks may target specific cognitive domains such as memory or attention.\nGiven the within-class variation (WCV) issue in AD detection, the conventional binary classification paradigm overlooked two critical aspects: a) within-class difference (WCD), where samples with varying AD severity are assigned to the same class, potentially reducing model's sensitivity to certain changes in cognitive function; and b) instance-level imbalance (ILI), where the distribution of varying-degree samples is skewed in instance-level even with balanced class"}, {"title": "II. ADDRESSING WITHIN-CLASS VARIATION IN ALZHEIMER'S DISEASE DETECTION", "content": "We first revisit AD detection as a binary classification task. Given an input feature z derived from sample x, the posterior probability $p_+$ and $p_-$ of x being positive (AD) of negative (Non-AD) are estimated by a neural network classifier $f(z; \\theta)$, optimized using cross-entropy (CE) loss. PCA visualization of BERT features from the ADRESS corpus [23] (Fig 1(b)) reveals that while class sizes are balanced, positive samples exhibit significantly larger variance compared to negative samples, indicating substantial within-class variation (WCV). Furthermore, the histogram of classifier output logits (Fig 1(c)) shows negative samples following a relatively standard normal distribution, while positive samples are more dispersed. These observations align with the continuous nature of cognitive decline in AD, where individuals diagnosed with AD could present varying levels of cognitive impairment [20], [21].\nConsider a generalizable case with only two subgroups of pos-itive samples: mild and major AD, with ground-truth posteriors $p \\in {p_m, p_M}$, $0 < p_m < p_M < 1$. Ideally, misclassifying major AD samples as negative should incur larger losses. However, under CE loss $L_{BCD} = \\Sigma(H(y_-, p_-) + H(y_+,p_+))$, misclassifying mild samples will always yields larger losses since $-H(y_+, p_M) > -H(y_+,p_m)$. As a result, we argue that ignoring within-class differ-ences leads to biased optimization. On the other hand, the imbalance in sample severity (evidenced by the skewed distribution in Fig 1(c)) leads to over-representation of frequent groups, resulting in biased focus on specific degrees of impairment. A similar issue is long-tailed recognition [25], [26], while it differs in a crucial aspect: long-tailed recognition typically addresses explicit between-class imbalance, whereas our scenario is characterized by implicit instance-level imbalance and not necessarily tied to class membership."}, {"title": "B. Sample score estimator", "content": "To address WCV, a straightforward solution is to treat the task in a regression-like manner, where each sample has confidence label(s) and the model could be aware of subtle differences between samples. In this work, we achieve this by a preliminary module called Sample Score Estimator (SSE). Specifically, we hypothesized that classification models is able to implicitly measure and rank samples with pattern similarities and prediction confidence [27], and thus can be used to generate sample-wise posterior probabilities as soft targets. However, individual models may produce uncertain predictions due to factors such as limited data and randomness during training. To ensure robustness, we introduce two methods for more accurate soft targets: ensemble estimation (EE) and refinery estimation (RE), drawing inspiration from knowledge distillation [28], [29]. As illustrated in Fig 2(a), EE method averages the prediction of a series of component models, while RE method progressively refines outputs. The initial model in RE uses standard cross-entropy loss, while subsequent models learn from predecessors by soft cross-entropy, which calculate cross-entropy using previous model's output probabilities, instead of one-hot labels, as ground-trues (illustrated in Fig 2(b))."}, {"title": "C. Soft target distillation", "content": "Based on sample score estimation, we propose to train a follow-up classifier with soft targets and soft cross-entropy, termed soft target distillation (Fig 2(b)). Here the softmax values were used as a target to replace one-hot labels. Compared to standard CE loss, the use of soft targets introduced the consideration of sample confidence, thus addressingly releasing the over-focusing of mild cases as we mentioned earlier. This multi-stage approach has proven effective in addressing long-tailed recognition problems [30]. Besides, an alternative approach is to combine a soft target with standard CE loss for distillation, however our previous experiments and prior work [29] suggest this will not benefit the performance."}, {"title": "D. Instance-level re-balancing", "content": "Class re-balancing is a type of method to tackle class imbalance, typically emphasizing and de-emphasizing specific classes by re-sampling or loss re-weighting. We adapt this concept to the instance level to tackle within-class imbalance. The key challenge of instance-level re-balancing is to cluster samples according to AD severity to obtain frequency distribution. We achieve this by two steps: a) Measuring scalared AD confidence with log-probability ratio (LPR) $log(p_+/p_-)$ for each sample. In case posterior probabilities p are modeled by neural networks with softmax functions, LPR is equiv-alent to logits difference: $log(p_+/p_-) = log(e^{o_+}/e^{o_-}) = o_+ - o_-$, where o represents pre-softmax logits in neural network classifiers. b) Calculating LPRs for all training samples and applying Gaussian kernel density estimation to obtain probability densities representing sample frequencies. With these estimated sample probability densities, instance-level re-balancing can be implemented analogously to common re-balancing strategies in long-tailed recognition problems. We used re-weighting in our implementation, where inverse probabil-ity densities serve as sample weights multiplied with sample losses."}, {"title": "III. EXPERIMENTAL SETUP", "content": "Our study leveraged standardized datasets from the ADRESS [23] and ADRESSO [24] challenges, both derived from the Pitt Corpus in the DementiaBank database [31]. The ADRESS dataset comprises 108 samples in the training set and 48 samples in the test set, with an equal distribution of positive and negative cases. The ADRESSo dataset consists of 166 training samples (87 positive and 79 negative) and 71 test samples (35 positive and 36 negative). Notably, ADRESS provides both speech recordings and corresponding manual transcriptions, whereas ADRESSo only includes speech recordings. Therefore, we employed automatic speech recognition to transcribe the ADRESSo dataset, details are elucidated in the subsequent sections."}, {"title": "B. Automatic speech recognition", "content": "To transcribe speech into transcription for the ADRESSo dataset, we employed a fine-tuned Whisper-large-v3 model [32] utilizing the Low-Rank Adaptation (LoRA) strategy [33]. The fine-tuning process was conducted for 2 epochs using the AdamW optimizer with a batch size of 8. Specifically, we applied a weight decay of 0.01 and implemented a learning rate schedule with a peak rate of 1e-5, incorporating 50 warmup steps. For the LORA configuration, we set the rank to 32, alpha to 64, and employed a dropout rate of 0.05. It is noteworthy that the fine-tuning procedure was performed on the ADReSo training set. During the inference phase, we implemented a repetition penalty of 1.1 to enhance the model's performance."}, {"title": "C. AD detection model", "content": "Pre-trained language models have shown superior performance in AD detection [6], [34], [35]. In this study, we used uncased BERT-base [36] and RoBERTa-base [37] models to extract linguistic features from transcribed speech data. Following feature extraction, we aggregated the embedding sequences by computing their average across the sequence length, yielding compact feature vectors. As for classifiers used in this work, we implemented a 3-layer multi-layer perceptron model with hidden dimensions of (32,16,16). The classifier used the Adam optimizer with a learning rate of 5e-3, batch size of 16, and a maximum of 10 epochs. Cross-entropy was employed as the loss function. For comparison, we also implement an additional baseline system using a standard model ensemble approach. In contrast to our proposed methods that use ensemble models to generate soft scores for new classifiers, this approach directly averages the softmax posteriors of 10 models to make the final decision."}, {"title": "D. Sample score estimator", "content": "The component classifiers employed for sample score estimator are identical to the aforementioned AD detection classifier. As a default setting, we utilized an ensemble of 10 classifiers in ensemble estimation and 2 classifiers in refinery estimation. The second classifier in refinery estimation was optimized using soft cross-entropy to refine the first classifier."}, {"title": "IV. RESULTS AND DISCUSSIONS", "content": "Table I and II summarize our experimental results (test/cross-validation) using BERT and RoBERTa features respectively. For each evaluation metric, the best-performing results are underlined. We pri-marily focus on AUC and accuracy as the key performance indicators, with cross-validation results given higher priority over test set results. The proposed SoTD method demonstrates consistent improvement over both the baseline and ensemble systems across features and datasets, validating its effectiveness. Specifically, we denote the best results of SoTD methods in blue. It shows that SoTD with EE estimator outperforms that with RE estimator in most experiments. For instance, on the ADRESS dataset using BERT features, SoTD (EE) achieves an AUC of 86.25/85.89 compared to 85.42/84.24 for SOTD (RE). We further conducted EE+RE experiments, wherein EE is initially employed to average a preliminary soft target, followed by RE for refinement. Notably, experiments reveal that EE+RE does not necessarily outperform standalone estimations, suggesting a potential convergence in soft score estimation that may limit further refinement. As for the InRe method, we denote the best results in orange. Similarly, InRe systems exhibit overall superiority to the baseline when using BERT features. For example, InRe (EE) achieves an accuracy of 87.71/85.06 on the ADRESS dataset, surpassing the baseline's 85.00/83.33. With ROBERTa features, InRe shows more pronounced improvements in accuracy. Analogous to SoTD, InRe with EE consistently outperforms that with RE, while the EE+RE combination does not provide performance gains. These results collectively demonstrate the efficacy of both proposed methods in addressing within-class variation in AD detection tasks. Furthermore, we observe that the proposed methods generally demonstrate more improvements on the ADRESS dataset compared to ADRESSo. Con-sidering ADReSSo utilizes automatically generated transcriptions, a plausible explanation is that the soft targets estimated by SSE may be less accurate due to potential errors in the transcriptions.\nFurther experiments were conducted for insightful analysis. Fig 3 illustrates the intermediate results of ensemble and refinery estimation under the SoTD framework. Subfigures (a) and (b) demonstrate that the AUC performance of 10 component classifiers in the EE method exhibits fluctuations > 5%. The SoTD method's final results (red line) consistently outperform the means (blue dash-line), while the basic ensemble method (green line) demonstrates inconsistent superiority over the average results. Subfigures (c) and (d) indicate that refinement in RE enhances the initial model, but the optimal refinery iteration is uncertain: BERT features achieved optimal per-formance at the second model, whereas ROBERTa features peaked at the fourth model. This variability may explain why the RE method, despite its validated effectiveness in certain computer vision studies [29], underperformed in our primary experiments given that we consistently employed the second model for estimation.\nFig 4 presents a comparison of validation loss between the baseline classifier (blue) and InRe methods (orange), based on a validation set with 1/5 of training data. We repeated 10 experiments with different random seeds for each method, illustrating the mean and variations of validation losses of compared systems. The results clearly show significant variations in validation loss for the baseline, with severe over-fitting observed in some runs. In contrast, while InRe does not necessarily achieve lower-bound results, it demonstrates remarkable stability. Comparing these outcomes, we posit that the biased focusing induced by data imbalance likely leads to over-fitting, whereas InRe largely mitigates this problem."}, {"title": "V. CONCLUSIONS", "content": "In this work, we discussed within-class variation (WCV) in Alzheimer's Disease (AD) detection. We propose that WCV may cause models to overlook two crucial aspects: within-class differences and instance-level imbalance. We respectively propose two methods: soft target distillation (SoTD) and instance-level re-balancing (InRe), along with two approaches for estimating sample confidence scores - ensemble estimation (EE) and refinery estimation (RE). Experimental results demonstrate that both proposed methods yield significant performance improvements, particularly when using EE to estimate individual sample confidence scores. The RE method could be further enhanced through dynamic adjustment of refinery iterations. We also observe that baseline classifiers produced unstable training results due to over-fitting, while InRe significantly improves training stability. These efforts represent an initial investigation of the WCV problem, demonstrating that guiding the models with informative soft targets can be an effective solution. Future work will explore the combination of SoTD and InRe methods, and further investigate WCV in diverse languages and AD tasks."}]}