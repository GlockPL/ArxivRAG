{"title": "Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy Morphology Analysis", "authors": ["Ruoqi Wang", "Haitao Wang", "Qiong Luo"], "abstract": "Galaxy morphology analysis involves classifying galaxies by their shapes and structures. For this task, directly training domain-specific models on large, annotated astronomical datasets is effective but costly. In contrast, fine-tuning vision foundation models on a smaller set of astronomical images is more resource-efficient but generally results in lower accuracy. To harness the benefits of both approaches and address their shortcomings, we propose GalaxAlign, a novel method that fine-tunes pre-trained foundation models to achieve high accuracy on astronomical tasks. Specifically, our method extends a contrastive learning architecture to align three types of data in fine-tuning: (1) a set of schematic symbols representing galaxy shapes and structures, (2) textual labels of these symbols, and (3) galaxy images. This way, GalaxAlign not only eliminates the need for expensive pretraining but also enhances the effectiveness of fine-tuning. Extensive experiments on galaxy classification and similarity search demonstrate that our method effectively fine-tunes general pre-trained models for astronomical tasks by incorporating domain-specific multi-modal knowledge.", "sections": [{"title": "1. Introduction", "content": "Galaxy morphology analysis involves studying galaxies based on their shapes and structures. This information is crucial for understanding galaxy formation and evolution, and it can be conveyed through natural language and schematic diagrams of galaxy images. As shown in Figure 1, schematic symbols paired with textual labels effectively capture the distinct characteristics of individual galaxies[3, 4]. These textual descriptions and schematic diagrams have proven useful in guiding amateur volunteers in galaxy image annotation[29]. In this paper, we explore how this multi-modal information can enhance foundation models for galaxy morphology analysis.\nFoundation models pre-trained on large-scale natural image datasets, such as ImageNet [21], perform well in various multimedia applications. However, researchers commonly believe that these pre-trained models are insufficient for astronomical images [24, 27, 31]. This concern arises because the domain-specific scientific data differ significantly from the general natural image datasets used for foundation models, leading to distribution shifts that reduce the effectiveness of these models when applied directly to galaxy analysis [27].\nAs a result, astronomical foundation models have typically relied on pretraining with large-scale astronomical datasets [7, 15, 16, 34]. These models are trained from scratch using extensive domain-specific datasets and subsequently fine-tuned for downstream tasks, overlooking the potential benefits of adapting publicly available vision foundation models that are pre-trained on natural datasets."}, {"title": "2. Background and Related Work", "content": "Moreover, annotating galaxy images heavily depends on the efforts of experts and volunteers, requiring significant human resources and time [14, 15, 29]. Therefore, there is a pressing need for a method that reduces reliance on large domain-specific datasets and directly utilizes readily available pre-trained models trained on natural image data.\nBased on the above observations, this work aims to investigate whether a galaxy model extended from pre-trained foundation models on natural image data can reduce the reliance on large astronomical datasets. To address this need, we introduce GalaxAlign, a tri-modal fine-tuning framework that adapts pre-trained CLIP models [20] for galaxy morphology analysis by integrating schematic symbols, textual descriptions, and galaxy images. Since human amateur volunteers can label astronomical images based on their knowledge learned from textual descriptions and schematic diagrams, we believe that these modalities can also assist models pre-trained on general datasets in performing classification tasks on astronomical images.\nSpecifically, GalaxAlign employs a two-stage fine-tuning approach: In the first stage, galaxy images and schematic symbols are input into a shared image encoder, while textual descriptions are processed by a separate text encoder. This stage enables the image encoder to learn a shared representation of galaxy features from both symbolic and photographic images. In the second stage, GalaxAlign transfers the parameters from the shared encoder in Stage 1 to initialize a separate symbol encoder, enabling each encoder to specialize in a single modality\u2014images, symbols, or text. This parameter transfer leverages the shared encoder's foundational understanding of galaxy morphology, learned from both images and schematic symbols in Stage 1, as a strong starting point for Stage 2. The second stage fine-tuning aligns each modality for more precise feature embedding by focusing each encoder on its unique input.\nThis tri-modal alignment enhances the model's ability to distinguish detailed structural features, improving classification and similarity search accuracy without costly large-scale pretraining. Extensive experiments show that GalaxAlign effectively fine-tunes pre-trained models, achieving high accuracy in astronomical tasks by incorporating domain-specific multi-modal knowledge.\nOur contributions are as follows:\n\u2022 Our method effectively extends models pre-trained on natural images for astronomical tasks, eliminating the need for large-scale astronomical datasets.\n\u2022 We reduce reliance on manually labeled data, providing a novel solution for galaxy morphology tasks.\n\u2022 We present a multi-modal learning architecture that incorporates text and symbols to adapt models pre-trained on general image datasets for use with galaxy images."}, {"title": "2.1. Foundation Models in Astrophysics", "content": "Unlike natural images, astrophysics images tend to have the following properties [27]:\n\u2022 Sparseness: Objects occupy only a small fraction of each image.\n\u2022 Noise: Systematic noise is present in the images.\n\u2022 High Dynamic Range: Object brightness spans several orders of magnitude.\n\u2022 Artifacts: Instrumental effects or reconstruction residuals introduce unintended structures of various scales.\nDue to these differences, astronomers regard general vision models trained on natural images as inadequate for astronomical tasks. As such, instead of fine-tuning existing vision foundation models to fit astronomical data, Walmsley et al. proposed a galaxy morphology foundation model pre-trained on large-scale galaxy morphology datasets [31]. However, constructing such datasets is time-consuming and labor-intensive. For instance, the GZD-5 project, which classified 262,000 galaxies, spanned over three years from March 2017 to October 2020 [30]. Similarly, in the recent Galaxy Zoo campaign, 38,949 volunteers annotated a total of 105,000 galaxies over nearly two years (November 2020 to October 2022) [33].\nTo reduce the amount of time and labor required for creating large, labeled astronomical datasets, an alternative approach is to adapt existing foundation models pre-trained on natural images to astronomical tasks. Some representative pre-trained general vision foundation models including DINOv2 [19], MAE [10], MSN [1], supervised ResNet [9], and supervised ViT [8], have been fine-tuned to adapt to galaxy morphology tasks. However, their performance is generally poor [27]. These results suggest that adapting vision foundation models for astrophysics applications requires further considerations of data characteristics and alignment with domain-specific knowledge. Our work contributes to this effort."}, {"title": "2.2. Citizen Science and Multi-Modal Data", "content": "Citizen science projects are the primary method for large-scale galaxy morphology annotation. In the Galaxy Zoo projects [16, 30, 34], volunteers annotate astronomical images based on specific instructions. This process heavily relies on providing amateur volunteers with simplified schematic symbols and natural language descriptions to guide them through the classification process.\nFigure 2 illustrates an example of the annotation guidance available to volunteers on the Galaxy Zoo online platform [35]. Volunteers are presented with symbols that depict different morphological features, such as spiral arms, bars, or mergers, accompanied by concise textual descriptions of these features. In this annotation process, the combination of visual symbols and natural language descriptions helps non-experts connect their common sense with astronomical knowledge, enabling them to effectively complete labeling tasks.\nInspired by this approach, our method integrates schematic symbols and natural language descriptions as key components in galaxy morphology tasks, aiming to bridge the gap between pre-trained general models and astronomical data."}, {"title": "2.3. Multi-Modal Contrastive Learning Models", "content": "Vision-language models, such as CLIP [20], utilize contrastive learning to align visual and textual embeddings. These models obtain strong generalization capabilities by training on large-scale datasets that pair images with corresponding textual descriptions. Bowles et al. [3, 4] proposed a method to connect radio galaxy images with human-generated natural language descriptions to derive semantic morphology classes for classification, demonstrating that textual descriptions can align with distinct identifying features in the images [18].\nMoreover, multi-modal contrastive learning has been widely applied in various scientific domains [13, 15, 17, 18, 22, 26], learning semantic representations effectively across diverse modalities. In this paper, we present the first application of associating astronomical data with three modalities\u2014images, text, and schematic symbols\u2014demonstrating that contrastive learning can effectively align these modalities."}, {"title": "3. Method", "content": "We introduce GalaxAlign, a tri-modal learning framework based on the CLIP architecture, for galaxy morphology tasks by incorporating and aligning three modalities: textual descriptions, astronomical images, and schematic symbols. Our approach contains three encoders: a text encoder for textual labels, an image encoder for galaxy images, and a symbol encoder for schematic symbols. Our finetuning is conducted in two stages: (1) Symbol-Image with Text Training and (2) Tri-modal Joint Training.\nOur framework contains three encoders:\n\u2022 Text Encoder: It processes textual descriptions related to galaxy morphology.\n\u2022 Image Encoder: Initially it encodes both galaxy images and schematic symbols. In the second stage, the image encoder encodes galaxy images only.\n\u2022 Symbol Encoder: It is derived from the Image Encoder after the first stage to encode schematic symbols in the second stage."}, {"title": "3.1. Textual Description", "content": "Our input text follows the common format A picture of a/an {class name}. Since the class name clearly and concisely describes the structure of the galaxies, it provides an efficient and identifying textual label. For example, sentences such as \u201cA picture of an unbarred-spiral galaxy\u201d or \u201cA picture of a cigar round smooth galaxy\" effectively capture the structural information of the galaxies."}, {"title": "3.2. Stage 1: Symbol-Image with Text Training", "content": "In the first stage, the Image Encoder takes both galaxy images and symbols as its input, whereas the Text Encoder processes corresponding text descriptions. This stage aligns the representations of images and symbols with text descriptions, setting up a shared multi-modal embedding space. The shared Image Encoder extracts patterns in galaxy shapes and structures that are consistent across symbols and images. Both encoders are initialized with pre-trained weights of the CLIP model [12] on natural images.\nFor each input galaxy image Ximg, schematic symbol Xsym, and text description xtxt, we obtain embeddings:\nzimg = Eimg(Ximg), zsym = Eimg(xsym), ztxt = Etxt(Xtxt)\nTo align image/symbol and text embeddings, we use a contrastive loss function that maximizes the cosine similarity between positive (matching) pairs and minimizes it for non-matching pairs. For batch size N, the loss function for image-text and symbol-text pairs is:\n$L_{total} = -\\frac{1}{2N} \\sum_{i=1}^{N} \\log \\frac{exp(sim(z_{img/sym}^i, z_{txt}^i)/\\tau)}{\\sum_{j=1}^{N} exp(sim(z_{img/sym}^i, z_{txt}^j)/\\tau)}$\nwhere sim(za, zb) denotes the cosine similarity and \\tau is a learnable temperature parameter.\nIn our method, Stage 1 serves as a warm-up phase rather than full- convergence training. Empirically, we found that training for just over 10 epochs in our experiments was sufficient to achieve optimal results, eliminating the need for full convergence at this stage. This approach allows the model to establish a solid foundation for galaxy morphology analysis without excessive computation in the initial stage."}, {"title": "3.3. Stage 2: Tri-Modal Joint Training", "content": "In Stage 2, we transition from a shared encoder to separate, modality-specific encoders, allowing the encoders to refine and specialize in the unique features of images, symbols, and text individually. To facilitate this specialization, we copy the parameters from the shared image encoder Eimg from Stage 1 to initialize the symbol encoder Esym:\nEsym \u2190 Eimg\nWith the symbol encoder starting from the same representation as the image encoder, both encoders are more likely to produce embeddings that are well-aligned in the feature space. Moreover, rather than learning from scratch, the symbol encoder refines and specializes existing representations from Stage 1, allowing itself to capture modality-specific details more quickly.\nAll three encoders are then fine-tuned together, optimizing the alignment across text, image, and symbol representations. The resulting embeddings are:\nzimg = Eimg(Ximg), zsym = Esym(Xsym), ztxt = Etxt(Xtxt)\nThe Stage 2 loss function includes three contrastive components to ensure effective alignment across all three modalities. We define the modality pairs as:\n(a, b) \u2208 {(img, txt), (img, sym), (sym, txt)}\nFor each pair, the contrastive loss is computed as:\n$L_{total} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{(a,b)} \\log \\frac{exp(sim(z_a^i, z_b^i)/\\tau)}{\\sum_{j=1}^{N} exp(sim(z_a^i, z_b^j)/\\tau)}$\nThis loss function facilitates a balanced alignment across the three modalities, enabling the model to jointly learn textual descriptions, visual images, and schematic symbols for galaxy morphology tasks."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": ""}, {"title": "4.1.1 Platform", "content": "We conduct all experiments on a server with two AMD EPYC 7543 CPUs, 512GB main memory, and four NVIDIA RTX A6000 GPUs each with 48GB device memory. The operating system is Ubuntu 22.04. Our model is implemented in PyTorch 2.1.0."}, {"title": "4.1.2 Datasets", "content": "In our experiments, we evaluate our method on two representative public galaxy datasets: (1) Galaxy10 DECALS [11] and (2) GalaxyMNIST [28]. Galaxy10 contains 17,736 colored galaxy images divided in 10 classes, with each image of a size 256 \u00d7 256 pixels. This dataset is from the DESI Legacy Imaging Surveys [6], which merges data from the Beijing-Arizona Sky Survey (BASS) [36], the DECam Legacy Survey (DECaLS) [2], and the Mayall z-band Legacy Survey [23]. In comparison, GalaxyMNIST [28], derived from Galaxy Zoo DECALS [30], contains 10,000 galaxy images (64 \u00d7 64) of four morphological classes."}, {"title": "4.1.3 Evaluation Metrics", "content": "In this study, we evaluate the performance of GalaxAlign on classification tasks using Accuracy and F1 Score (macro). For the similarity search task, we use mean Average Precision (mAP), which reflects the ranking quality by calculating the average precision at various recall levels for each query and then averaging across all queries."}, {"title": "4.1.4 Methods under Comparison", "content": "We evaluate GalaxAlign with the state-of-the-art foundation models, including mainstream general vision models and specialized astronomical foundation models trained on large-scale domain-specific datasets. The baseline models include:\n\u2022 MAE [10], DINOv2 [19], MSN [1]: Vision foundation models using self-supervised pretraining techniques. Following Lastufka et al. [27], we fine-tune the models using galaxy datasets of variant sizes in downstream tasks."}, {"title": "4.2. Feature Projections", "content": "To demonstrate GalaxAlign's strong performance without relying on extensive domain-specific datasets, We present the embedding visualization comparing our method with Zoobot, which has been pretrained on large astronomical datasets and then fine-tuned on smaller datasets. Figure 4 provides a t-SNE visualization [25] of galaxy data embeddings learned by different models. Feature embeddings of other baseline methods are presented in the appendix."}, {"title": "4.3. Galaxy Morphology Classification", "content": "Table 2 provides a comparative analysis of classification performance across different methods on the GalaxyMNIST and Galaxy10 datasets.\nThe self-supervised models (MAE [10], DINOv2 [19], and MSN [1]), when fine-tuned following Lastufka et al. [27], show promising performance on GalaxyMNIST but are generally outperformed by models pretrained with supervised learning [8, 9], particularly on Galaxy10. This result suggests that while self-supervised pretraining captures general features, supervised models pretrained on natural images, tend to transfer more effectively to the morphological distinctions in astronomical images.\nThe Zoobot models [32] (using MaxViT and ConvNeXT as the backbone network, respectively), specifically pretrained on large-scale galaxy data, demonstrate strong performance on both datasets. This result underscores the value of domain-specific pretraining for high-performance astronomical classification.\nIn contrast, our proposed models\u2014Ours (ViT-16) and Ours (ConvNext)\u2014pretrained on natural image datasets and fine-tuned using a multi-modal architecture that aligns textual descriptions, schematic symbols, and astronomical images, achieve comparable performance to Zoobot and outperform all other methods. These results highlight the effectiveness of adapting general pretrained models through multi-modal fine-tuning, providing an alternative to large-scale astronomical pretraining for galaxy morphology tasks.\nFigure 6 compares the F1 scores of various models pretrained on natural data across different data sizes for the fine-tuning datasets, GalaxyMNIST and Galaxy10. Our models achieve the highest performance on both datasets, with all data sizes, showing efficient generalization and effective adaptation to astronomical tasks. Self-supervised models (e.g., DINOv2 [19], MAE [10], MSN [1]) and standard supervised models (e.g., ResNet-18, ResNet-50 [9] and ViT [8]) achieve moderate results, indicating that pretraining with natural image alone may not fully capture the domain-specific details required for astronomical tasks. These results illustrate the effectiveness of our multi-modal adaptation strategy in bridging general pretrained models and domain-specific applications in astronomy."}, {"title": "4.4. Similarity Search", "content": "In astronomy, automatically identifying the similarity between two galaxies is a challenging but essential task [31]. Effective searches for similar galaxies can help us find counterparts of rare galaxies, making the leap from a one-off discovery to a new class of phenomena [31]. Latent representations encoded by neural networks provide a new opportunity for measuring morphological similarity. In our similarity search evaluation, we measure the morphological similarity of galaxies by comparing feature embeddings through a similarity matrix. Specifically, we calculate pairwise similarities using dot products between feature vectors, creating a matrix where each entry indicates the similarity value between two galaxy images. For each image, we retrieve the most similar images (excluding the image itself) and use these to evaluate retrieval performance.\nThe comparison results of GalaxAlign and other methods evaluated in mAP@5 and mAP (mAP@all) are shown in Table 3. Our models demonstrate superior performance over baseline methods on both GalaxyMNIST and Galaxy10 datasets. While other fine-tuned general-purpose models show limited effectiveness, our approach, incorporating multi-modal alignment, outperforms traditional architectures, achieving better results in identifying galaxy morphological similarities. This result underscores our model's capacity to adapt well to astronomical data.\nThe results in Figure 7 demonstrate that our model effectively retrieves galaxies with similar morphological characteristics to the query. Across various galaxy types, the top-ranked matches closely resemble the query galaxy's shape and structure, indicating strong model performance in identifying morphological similarities."}, {"title": "4.5. Ablation Studies", "content": "We conduct an ablation study to evaluate the contribution of each alternative in our approach. First, we compare how our model works with ViT and ConvNeXT. Then we examine the effect of our tri-modality input in comparison with the text-image bimodal CLIP model without the schematic symbol input. Finally, we compare our models across different stages of fine-tuning: Ours_v1 is the warm-up model trained through the first stage, omitting the second training stage; Ours_v2 skips the first stage and directly goes into the second stage of training until convergence; Ours_v3 omits the second stage, training through the first stage to convergence; Ours_Scratch goes through both stages, but is trained from scratch on the Galaxy10 and GalaxyMNIST datasets, without pre-training on large-scale natural datasets.\nThe results in Table 4 show the impact of each alternative in our approach. The full models, Ours(ViT-16) and Ours(ConvNeXT), achieve the highest accuracy and F1 scores on both datasets. Removing the symbol modality (CLIP models) or omitting training stages (e.g., Ours_v1, Ours_v2, and Ours_v3) reduces performance. The Ours_Scratch variant, trained without pretraining, shows notable drops, indicating the impact of pretraining with large-scale general datasets on astronomy tasks."}, {"title": "5. Conclusion and Future Work", "content": "In this paper, we introduced GalaxAlign, a tri-modal framework designed to finetune vision foundation models for galaxy morphology analysis, effectively utilizing schematic symbols, text descriptions, and galaxy images. By integrating domain-specific knowledge in a multi-modal approach, GalaxAlign effectively reuses the general foundation models pre-trained on natural datasets and reduces the need for manually labeled data for training foundation models from scratch.\nWhile GalaxAlign is designed for galaxy data, the multi-modal framework\u2014integrating images, textual descriptions, and schematic symbols\u2014is well-suited to other natural sciences such as biology for cellular and species classification, or geology for analyzing mineral structures, where both structure and descriptive information are essential and available."}]}