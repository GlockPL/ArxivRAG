{"title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models", "authors": ["Siqi Wang", "Zhengyu Chen", "Bei Li", "Keqing He", "Min Zhang", "Jingang Wang"], "abstract": "The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size and learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.", "sections": [{"title": "1 Introduction", "content": "The advent and scaling of large language models (LLMs), such as GPT (Brown et al., 2020; Achiam et al., 2023), Llama (Touvron et al., 2023a,b), Gemini (Team et al., 2023), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), and Mistral (Jiang et al., 2023), have marked a transformative era in artificial intelligence and natural language processing. Characterized by their vast parameter counts and extensive training datasets, these models have significantly advanced capabilities across various domains, including machine translation (Brown et al., 2020; Hendy et al., 2023; Garcia and Firat, 2022), logical reasoning (Huang and Chang, 2022; Wei et al., 2022; Chen et al., 2021a, 2024a), and medical applications (Thirunavukarasu et al., 2023; Xiao et al., 2022). However, their increasing complexity and parameter scale posits an urgent need for innovative scaling strategies that optimize computational efficiency without compromising performance.\nHistorically, Dense Transformer Models have dominated due to their simplicity and scalability. And the scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) for dense models have been thoroughly investigated across different circumstances, such as over-training (Gadre et al., 2024) and data-limiting (Muennighoff et al., 2024; Chen et al., 2022). Despite their efficacy, the huge computational demands of these models necessitate exploration of alternative architectures like Mixture of Experts (MoE) (Yuksel et al., 2012; Shazeer et al., 2017; Du et al., 2022; Shen et al., 2024; Chen et al., 2021b; Xiao et al., 2021), which offer a promising reduction in computational load through sparse activations and dynamic expert routing.\nThis paper delves into the analysis of scaling laws for Dense and MOE Models within the context of LLMs. We extend foundational research on hyperparameters, such as compute budget, batch size, and learning rate (Kaplan et al., 2020; Hoffmann et al., 2022; McCandlish et al., 2018; Li et al., 2024; Chen et al., 2024b), to explore their transferability and applicability across these architectures. Our experiments involve models up to 7 billion parameters and datasets exceeding 100 billion tokens, aiming to uncover universal scaling behaviors potentially applicable to both model types.\nOur results verify the hypothesis that certain scaling laws, particularly those related to loss and hyperparameters, may indeed be universal, bridging architectural gaps between Dense and MoE Models. This universality suggests a simplification in hyperparameter tuning across different scales and architectures, which could significantly streamline the training processes for various LLMs. Furthermore, we provide detailed analyses of the dif-"}, {"title": "2 Related Work", "content": "Large Language Models Large language models (LLMs) such as GPT (Brown et al., 2020), Llama (Touvron et al., 2023a,b), Chinchilla (Hoffmann et al., 2022), Gopher (Rae et al., 2021), Mixtral 8x7B (Jiang et al., 2024), Switch Transformer (Fedus et al., 2022), GLaM (Du et al., 2022), and DeepSpeed-MoE (Rajbhandari et al., 2022) have advanced significantly, categorized into Dense Models and Mixture of Experts (MoE) Models. Dense Models activate all parameters per forward pass, while MoE Models activate only a subset, allowing for larger model scales without proportional increases in computational costs. Despite their complexity, MoE Models have shown potential for superior performance and efficiency.\nScaling Laws for LLMs Due to the significant costs associated with training process, understanding the scaling laws of large language models (LLMs) is crucial. Studies (Bahri et al., 2021; Kaplan et al., 2020; Bi et al., 2024) have established a power-law relationship between model loss and factors like training tokens and compute budget. Recent work Yun et al. (2024) has explored these relationships further in MoE Models, indicating cost-effective scaling benefits but also highlighting challenges such as expert selection and load balancing. However, a systematic investigation into the scaling laws of MoE Models' hyperparameters and the transferability of scaling laws between Dense Models and MoE Models remains lacking, which is the focus of our work.\nHyperparameters Estimation As model sizes increase, precise optimal hyperparameter estimation becomes critical (Chen and Wang, 2021). Research McCandlish et al. (2018) has focused on optimizing batch size and learning rates to balance training speed and efficiency. Novel approaches (Yang et al., 2022, 2023) like Maximal Update Parametrization suggest that optimal hyperparameters for smaller models might scale to larger models effectively. Our study extends these insights to explore hyperparameter transferability between Dense and MoE Models, focusing on resource allocation, learning rate, batch size, and their transfer rules for Dense Models and MoE Models."}, {"title": "3 Preliminary", "content": "The scaling law of the training loss for Dense Models with respect to the number of training tokens and model size has been extensively studied (Kaplan et al., 2020; Hoffmann et al., 2022). Previous work (Hoffmann et al., 2022) has proposed the following scaling law (shown in Equation 1). To introduce the concept of model scale (the FLOPs divided by the number of training tokens) as N in our work, we denote model size (number of parameters) as P to avoid confusion.\n$\\\u00ce(P, D) = \\frac{A}{P^\u03b1} + \\frac{B}{D^\u03b2} + \u03c3$\ns.t. $FLOPs(P, D) = C$\nwhere \u00ceL is the training loss, D is the number of training tokens, P is the model size (number of parameters), and o represents the minimum achievable training loss due to the dataset's inherent noise.\nFor MoE (Mixture of Experts) Models, previous research (Clark et al., 2022) has proposed a separable scaling law (Equation 2) for loss between model size and the number of experts.\n$\\\u00ce(P,E) = a log(P) + b log(E) + d$\nwhere P is the number of model parameters, E is the number of experts, and a, b, d are coefficients. This equals to Equation 3.\n$\\\u00ce(P, E) = \\frac{10^d}{P^a E^b}$$\nWhen using Equation 2 to fit the training loss curve of MoE Models, Clark et al. (2022) claim that a decrease in performance has been observed given by expert scaling. Specifically, the value of b increases with model size (in Equation 2) when model size is large. This suggests that as the model size increases, the benefit from increasing the number of experts E will finally decrease. To enhance the fitting ability of the scaling laws for MoE Models, a quadratic interaction term is added, resulting in Equation 4.\n$\\\u00ce(P, E) = a log(P) + b log(E) + c log(P) log(E) + d = \\frac{10^d}{P^a E^{b+c \\log(P)}} = \\frac{10^d}{E^{b} P^{a+c\\log(E)}}$"}, {"title": "4 Estimating Resources Allocation Strategy Scaling", "content": "4.1 Scaling Laws for Training Loss\nKaplan et al. (2020) and Hoffmann et al. (2022) originally proposed a scaling law for Dense Models, while Clark et al. (2022) extended this law to scenarios involving multiple experts (MoE Models). Upon closer examination of Equation 3 and Equation 4, we observed that when the number of experts (E) remains fixed, these equations can be simplified to the first term in Equation 1. Motivated by these insights, we introduce a unified scaling law for both Dense Models and MoE Models, represented by Equation 5. Specifically, since the decrease in performance is observed only when the number of experts (E) is large, we adopt the simplified one (Equation 2) for small E value (below 100). Besides, previous work (Bi et al., 2024) suggests replacing the model size P (number of parameters) with model scale N, which is the result of the FLOPs divided by the number of tokens in order to fit Equation 1 more accurately. Finally, we get Equation 5:\n$\\\u00ce(N, D, E) = \\frac{A}{N^\u03b1 E^\u03b3} + \\frac{B}{D^\u03b2} + \u03c3$\ns.t. $FLOPs(N, D) = C$\nwhere L is the training loss, D is the number of training tokens, and N is the model scale, which is the non-embedding FLOPs (C) divided by D. E is the number of experts and we suggest E is smaller than 100. \u03c3 roughly estimates the natural noise of the dataset, representing the minimum achievable training loss. \u0391, \u0392, \u03b1, \u03b2 and y are coefficients.\nIn order to validate Equation 5, in our experiment, we fitted the training data for both 200M and 700M MoE Models (both with Eight Experts) to a curve. We then used this curve to predict the training loss scaling behavior of a 1.5B MoE model. The results, shown in Figure 1, demonstrate that the formula we proposed based on previous work is applicable to MoE Models when the number of experts is not large. It could be observed that the reduction in benefits from increasing E is minimal and the scaling equation stands within our experiment scope. Therefore, we adopted the simplified version. This formula suggests that the Equation 5 could equal to Equation 1, given a fixed number of experts, for MoE Models. This consistency could help us to compare the computing resource allo-"}, {"title": "4.2 Estimating Optimal Resource Allocation Strategy Scaling", "content": "After fitting the training loss (L) as a function of the number of tokens (D), the model scale (N), and the number of experts (E), we proceed to derive the optimal computing resource allocation strategy for model scale and the number of training tokens given a fixed compute budget.\nThe objective can be defined as follows: given a fixed number of compute budget, how to estimate the optimal resource allocation strategy for model scale and the number of training tokens to minimize the training loss.\n$D_{opt}(C), N_{opt}(C) = \\arg \\min_{D,Ns.t. C=ND} L(N, D, E)$\nWe take the differentiation of Equation 5 with respect to C = DN, we derive the optimal values of D and N for a given compute budget:\n$D_{opt}(C) = K_D . C^{\u03b1_D}$\nwhere $k_D = (\\frac{B}{A} \\frac{\u03b2}{\u03b1} )^{\\frac{1}{\u03b1+\u03b2}}$ and $\u03b1_D = \\frac{\u03b1}{\u03b1+\u03b2}$\n$\u00d1_{opt}(C) = K_N . C^{\u03b1_N}$\nwhere $k_N = (\\frac{A}{B} \\frac{\u03b1}{\u03b2} )^{\\frac{1}{\u03b1+\u03b2}}$ and $\u03b1_N = \\frac{\u03b2}{\u03b1+\u03b2}$. Equation 7 and 8 indicate how the model scale (compute budget per token N) and the optimal number of training tokens D scale with the overall compute budget C. The most crucial part of this process is to find the scaling exponent of the model scale and tokens number with reference to the compute budget. Then we use the empirical fitting results obtained previously to calculate these two exponents."}, {"title": "5 Optimal Hyperparameters Scaling", "content": "5.1 Estimating Optimal Batch Size\nBatch size is a crucial hyperparameter for the training process. Previous works (McCandlish et al., 2018; Kaplan et al., 2020) have investigated the scaling law between optimal batch size and loss for Dense Models. In this experiment, we conduct the analysis of both models.\nTo start, according to previous work (McCandlish et al., 2018), let \u2206Lopt(B) denote the optimal improvement in the loss function when using a batch size B with the optimal step size opt(B). It takes into account the noise introduced by the gradient estimation process. Then \u2206Lopt(B) could be shown in Equation 9.\n$\u2206L_{opt}(B) = \u2206L_{max}(1+ \\frac{B_{noise}}{B})^{-1}$\nwhere ALmax is the maximum possible improvement in the loss function when the true gradient is used without noise. Here, the noise scale Bnoise measures the scale of the noise in the gradient estimates relative to the true gradient. It helps quantify how much the noise affects the gradient estimation process. The noise scale could be defined as:\n$B_{noise} = \\frac{tr(H\u03a3)}{G^T HG}$\nwhere G denotes the true gradient and H the true Hessian at parameter values \u03b8. \u03a3 represents covariance matrix, and tr(\u00b7) represents the trace. A higher noise scale (Bnoise) implies a larger optimal batch size for reducing the variance in the gradient estimate.\nFor training efficiency, the relationship between training steps S and training examples E derived from the SGD optimizer could be expressed as:\n$\\frac{S}{S_{min}} (\\frac{E}{E_{min}} - 1) = 1$\nwhere Smin and Emin are the minimum training steps and training examples needed to achieve a specific performance. Finally, from the empirical and theoretical verification (McCandlish et al., 2018; Kaplan et al., 2020; Hu et al., 2024), the optimal batch size at a specific training loss could be approximated using Bopt \u2248 Bnoise, then we get Equation 12.\n$B_{noise} \u2248 B_{opt} = \\frac{E_{min}}{S_{min}} \\frac{\u03bb_\u03b2}{L^{a_\u03b2}}$\nwhere AB and a\u00df are both coefficients. Bopt is the optimal batch size given a noisy gradient and L is the loss value.\nEquation 12 indicates that Bopt serves as the balance point between training speed and data efficiency. It represents the optimal trade-off between training speed and data efficiency. Furthermore, it indicates that as training progresses and the loss decreases, Bopt gradually becomes larger, indicating that larger batch size is required to maintain the balance between training speed and data efficiency as the model converges."}, {"title": "5.2 Estimating Optimal Learning Rate", "content": "In previous work (McCandlish et al., 2018), the scaling relationship between optimal learning rate and optimal batch size when using SGD optimizer are illustrated as Equation 13.\n$\u20ac_{opt}(B) = \u20ac_{max}(1+ \\frac{B_{noise}}{B})^{-1}$\nwhere Copt(B) represents the optimal step size that minimizes the expected loss from the noisy gradient. And Emax represents the optimal step size that minimizes the loss function when using the noiseless true gradient G to update the parameters. It is defined as emax = CT\u09a6\u09a8. This relationship (Equation 13) shows that when B is relatively small, this Equation could be reduced as a nearly linear scaling (Granziol et al., 2022; Goyal et al., 2017). When the batch size is fixed, with optimal learning rate decreases with the increasing of noise scale.\n$\u20ac_{opt}(B) = \u20ac_{max}\\sqrt{B}{Bnoise}$\nRecent research by Li et al. (2024) and Granziol et al. (2022) reveals an interesting trend in the optimal learning rate for the Adam Optimizer concerning the optimal batch size Bopt or noise scale Bnoise. It shows a non-monotonic behavior, indicating that as the noise scale Bnoise increases, the optimal learning rate initially follows a nearly square root scaling pattern, dominated by the second term, before decreasing as the first term gains dominance. The transition point occurs when these two terms reach a balance, a critical juncture determined by the specific values of Bnoise and B.\n$\u20ac_{opt}(B) = \\frac{2\u20ac_{max}}{\\sqrt{\\frac{B_{noise}}{B}} + \\sqrt{\\frac{B}{B_{noise}}}}$"}, {"title": "6 Generalization of Scaling Law", "content": "We predict that MoE Models could have better generalization ability compared to their Dense counterparts. Firstly, MoE Models consist of multiple expert sub-networks that specialize in different aspects of the data, which enables the model to capture a broader range of features and patterns and then leads to better generalization across various tasks and datasets. Additionally, the ensemble nature of MoE Models, where each expert contributes to the final prediction, reduces overfitting and improves robustness by combining predictions from multiple models. Finally, the gating mechanisms in MoE Models control the contribution of each expert to the final prediction, acting as a form of regularization. This regularization helps prevent overfitting by focusing on the most relevant experts for each input, resulting in better generalization.\nTo investigate the generalization performance of the scaling law for both MoE Models (Eight Experts) and Dense Models, we explore and compare the relationship of training loss with testing loss and compute budget for both Dense Models and MoE Models (shown in Figure 6). We highlight the stable power-law relationship interval across different model sizes, illustrating the correlation between testing loss and compute budget. Notably, MoE Models consistently exhibit a smaller testing loss for a given compute budget, indicating their superior generalization performance.\nWe also explore the performance of the Dense and MoE Models on different testing sets in Table 2. It clearly shows that MoE Models could outperform a Dense Model with comparative model size. The consistent trend in performance across different datasets underscores the transferability and reliability of the scaling law observed in our study. This finding suggests that the performance improvements achieved by MoE Models are not limited to specific datasets or conditions but hold true across diverse testing sets, indicating robustness and generalizability in real-world applications."}, {"title": "7 Conclusion", "content": "In this paper, we investigate the transfer of traditional scaling laws from Dense Models to Mixture of Experts (MoE) Models. Our investigation confirms that the power-law relationship extends to MoE Models regarding the consistency and transferability of scaling strategies, including resource allocation strategy, optimal batch size / learning rate scaling, and so on. This observation indicates that the fundamental principles of training dynamics and the behavior of scaling rules are similar for both Models. This means existing knowledge and practices for optimizing Dense Models can be easily adapted to MoE Models, potentially reducing the experimental burden of finding the best hyperparameters. Besides, we find that MoE Models demonstrate approximately a 16.37% improvement in data utilization efficiency compared to Dense Models with a fixed compute budget. Thus we suggest prioritizing increasing model scale over other factors when training MoE Models, highlighting their greater data efficiency. Finally, we use both theoretical and empirical analysis to reveal that during the training process, MoE Models exhibit a lower gradient noise scale when using the Adam Optimizer. At the same training loss value, MoE Models can achieve stable training with smaller batch sizes and larger learning rates, potentially speeding up the training process and improving convergence. It shows that MoE Models can make more efficient use of training data and computational resources, extracting more information per training token, leading to faster training times and better utilization of available data. These results offer valuable insights for refining training and deployment strategies for MoE Models."}, {"title": "Limitation", "content": "The experiments were constrained by the available computational resources. Firstly, we have not yet explored scenarios with more than 100 experts in our experiments. It has been observed that as the model size increases, the marginal benefit from increasing the number of experts tends to decrease. Moreover, although we tested our model on several benchmark datasets, they don't cover all domains. To ensure robust evaluation, we plan to validate the model on more tasks and domains and investigate the scaling relationships between different metrics. Additionally, as the number of training tokens exceeded 100 billion, we observed signs of overtraining for both models, indicating diminishing returns in performance improvements with additional training data. These areas remain open for future research."}, {"title": "C Details of Differentiation", "content": "Given the loss function:\n$\\\u00ce(N, D, E) = \\frac{A}{N^\u03b1 E^\u03b3} + \\frac{B}{D^\u03b2} + \u03c3$\nand the constraint:\n$C = ND$\nwhere N, D, C and E are the model scale (non-embedding FLOPs per token) and the number of"}]}