{"title": "3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography", "authors": ["Weicheng Zhu", "Haoxu Huang", "Huanze Tang", "Rushabh Musthyala", "Boyang Yu", "Long Chen", "Emilio Vega", "Thomas O'Donnell", "Seena Dehkharghani", "Jennifer A. Frontera", "Arjun V. Masurkar", "Kara Melmed", "Narges Razavian"], "abstract": "Head computed tomography (CT) imaging is a widely-used imaging modality with multitudes of medical indications, particularly\nin assessing pathology of the brain, skull, and cerebrovascular system. It is commonly the first-line imaging in neurologic\nemergencies given its rapidity of image acquisition, safety, cost, and ubiquity. Deep learning models may facilitate detection\nof a wide range of diseases. However, the scarcity of high-quality labels and annotations, particularly among less common\nconditions, significantly hinders the development of powerful models. To address this challenge, we introduce FM-CT: a\nFoundation Model for Head CT for generalizable disease detection, trained using self-supervised learning. Our approach\npre-trains a deep learning model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans without the need for\nmanual annotations, enabling the model to learn robust, generalizable features. To investigate the potential of self-supervised\nlearning in head CT, we employed both discrimination with self-distillation and masked image modeling, and we construct our\nmodel in 3D rather than at the slice level (2D) to exploit the structure of head CT scans more comprehensively and efficiently.\nThe pre-training phase is followed by fine-tuning on smaller, annotated downstream datasets, thereby optimizing the model\nfor specific diagnostic tasks, such as detecting hemorrhages, tumors, and other abnormalities. The model's downstream\nclassification performance is evaluated using internal and three external datasets, encompassing both in-distribution (ID)\nand out-of-distribution (OOD) data. Our results demonstrate that the self-supervised foundation model significantly improves\nperformance on downstream diagnostic tasks compared to models trained from scratch and previous 3D CT foundation models\non scarce annotated datasets. Furthermore, the model maintains strong generalization across different datasets, indicating its\npotential for broad clinical applicability. This work highlights the effectiveness of self-supervised learning in medical imaging\nand sets a new benchmark for head CT image analysis in 3D, enabling broader use of artificial intelligence for head CT-based\ndiagnosis.", "sections": [{"title": "Introduction", "content": "Head computed tomography (CT) is often the first step in diagnosing a wide range of neurological disorders, including head\ntrauma, hemorrhages, hydrocephalus, and malignancies. Head CT scans are faster, more accessible, and generally less expensive\nthan magnetic resonance imaging (MRIs), making them ideal for emergencies like traumatic brain injury (TBI) or suspected\nstroke. They are also effective in detecting bone fractures, or neurovascular pathologies such as arterial venous malformations.\nDespite its widespread use, CT lacks the contrast resolution and hence the sensitivity for many disorders dependent upon\ndiagnosis by MRI, thus MRI is the imaging modality of choice for many neurologic diseases. MRI, however, is more costly,\nrisks potential heating or displacement of indwelling implants, and suffers generally slower acquisition times, increasing patient\ndiscomfort and risking non-diagnostic examinations due to its greater sensitivity to motion-related artifacts. It is also more\nexpensive than CT and is contraindicated in specific patients. Access to MRI is a major challenge in resource-limited countries.\nThe timely and arduous determination of certain pathologies can delay appropriate medical and surgical treatment for patients.\nThere is significant potential to harness artificial intelligence (AI) algorithms to enhance the diagnostic and early detection"}, {"title": "Results", "content": "Foundation model for disease detection with 3D head CT scans\nThe key aim of the foundation model is to develop a single model that improves performance on a wide range of downstream\ntasks of detecting recognizable abnormalities from head CT scans. To evaluate the capability of the foundation model, we\ntrain classification models for multiple disease detection tasks by fine-tuning the foundation model separately per disease,\nand assessing the fine-tuned model's performance on held-out validation and external data sets. The selected downstream\ntasks include detecting various types of hemorrhages (intraparenchymal hemorrhage (IPH), intraventricular hemorrhage (IVH),\nsubdural hemorrhage (SDH), epidural hemorrhage (EDH), subarachnoid hemorrhage (SAH), and intracranial hemorrhage\n(ICH)), brain tumors, hydrocephalus (HCP), edema, and Alzheimer's diseases and related dementia (ADRD). Fig. 1a,b,c show\nthe overview of our pre-training framework and included data, EHR-matching, and datasets used in pre-training, in-domain\nfine-tuning, and external validation. Overall N=361,663 scans were used during pre-training, and four distinct datasets from\ndifferent sources were used for various forms of validation (NYU Langone N=26,487; NYU Long Island N=2,202; RSNA\nN=1,058; and CQ500 N=236). NYU Langone is a hospital system comprised of multiple geographically distinct hospitals\nincluding two Level 1 Trauma Centers and three Comprehensive Stroke Centers. NYU Long Island, a Level 1 Trauma\nCenter/Comprehensive Stroke Center, is treated as an external dataset for the purposes of this study.\nThe first two rows of Fig. le report the task-specific AUCs for Vision Transformer (ViT) classifiers trained from scratch\nwith random initialization, namely scratch, versus those fine-tuned from the foundation model, namely fine-tuned on NYU\nLangone data. The fine-tuned models consistently outperform the scratch model across all 10 disease detection tasks, achieving\na macro-AUC of 0.852 a 16.07% increase over the scratch model's 0.734 (P < 0.001). Additionally, in Fig. 1f and\nSupplementary Fig. 5 we compared the foundation model with two other foundation model for 3D CT scans Merlin 20\nand Google's CT Foundation38 model. Merlin outperforms the scratch model with a macro-AUC of 5.67% while falling\nshort compared to our foundation model with 7.51% lower macro-AUC (P < 0.001, illustrated in Supplementary Fig. 5).\nAlthough Merlin is not directly comparable to our foundation model as it was pre-trained on abdominal CT, it still provides a\nvaluable baseline. We compare our model to Google CT Foundation model with linear probing, because trainable weights for\nend-to-end fine-tuning are not provided for this model. We consistently observe improved model performance across the board\n(in Fig. 1f and Supplementary Fig. 7). These findings demonstrate that despite the progress in general domain multimodal\nmodels, specialized foundation model pre-trained on head CT data still significantly enhance the understanding of brain CTs.\nTo assess our foundation model's generalization to out-of-distribution data, we compiled three external datasets from\nmultiple institutions and sources: NYU Long Island, RSNA\u00b9, and CQ5002, as shown in Fig. 1c (NYU Langone and NYU Long\nIsland are geographically separate and distinct institutions within the broader health system). The data in these external datasets\nhas a different distribution than the data used for pre-training. We evaluate the generalization on external datasets via two\ncommon practices to utilize the foundation model: (1) in-domain fine-tuning on separated datasets and tasks, and (2) fully\nexternal validation of the disease detection models without any site-specific fine-tuning.\nFor in-domain fine-tuning, the foundation model is fine-tuned on each external dataset's training set and validated on\nheld-out sets from the same source. The bottom four rows in Fig. le report the tasks-level performances on NYU Long Island\nand RSNA datasets. The fine-tuned model yields a macro-AUC of 0.904 across the 10 tasks on NYU Long Island dataset\nand a macro-AUC of 0.923 for five types of hemorrhages on the RSNA dataset. In comparison, the scratch model results in\nmacro-AUC scores of 0.748 and 0.824, respectively. Moreover, the foundation model also significantly outperforms Merlin, as\nshown in Supplementary Fig. 5. The superior performances on external datasets indicate the generalizability of the foundation\nmodel. Note that the limited data size of CQ500 forbids training a effective deep learning model from scratch, reinforcing the\nimportance of the foundation model in label efficiency, which is further studied in Section \"Label efficiency\". Interestingly,\nwhen comparing performances across different datasets, Fig. le demonstrates that the AUCs of the in-domain fine-tuned\nmodel on the external dataset even exceed the AUCs achieved on the internal dataset. For instance, the fine-tuned models\nconsistently obtained AUCs greater than 0.90 in all the hemorrhage detection tasks on RSNA dataset, surpassing the AUCs on\nNYU Langone data. This may be attributed to the higher label quality in radiologist-reviewed datasets, for which label noise\nmay be better controlled by comparison to EHR-derived labels.\nIn the full external validation without any site-specific fine-tuning (illustrated in Fig. 1c), we evaluated classification models\nfine-tuned on the NYU Langone training set, as-is, on the held-out validation sets from each external dataset. Fig. 1f compares\nperformance between external validation and in-domain fine-tuning. Results show that, for the NYU Long Island and RSNA\ndatasets -where the training set used for fine-tuning includes a sufficient number of high-quality labeled samples- in-domain\nfine-tuning does enhance the model performance. However, on the CQ500 dataset, with only 1,120 training samples, the\nin-domain fine-tuned model performs worse than the model transferred from NYU Langone, especially for EDH and SDH,\nwhich have a greater class imbalance. These comparisons highlight two typical use cases for foundation models depending on\nthe availability of labeled data for fine-tuning. Additionally, comparing the first row of Fig. 1e and external validation in Fig. 1f,\nthe fine-tuned model on NYU Langone achieves similar AUC values on both internal and external datasets, indicating robust"}, {"title": "Label efficiency of few-shot classification performance", "content": "Another key advantage of the foundation model is its ability to facilitate transfer learning and fine-tuning tasks with minimal\nlabeled data. For example, as shown in Fig. 1c, the CQ500 dataset contains only 1,585 scans. Despite the small dataset size,\nfine-tuning our foundation model on CQ500 achieves promising results, with an AUC of 0.863.\nTo systematically evaluate the label efficiency of our foundation model, we also assess the generalization capabilities of\nmodels on new tasks given a limited number of examples within the paradigm of few-shot learning, where only K positive\nand negative samples each are used for training in each task. Since the quality of few-shot learning is largely determined\nby the sampled K-shots training data, we re-sampled and re-trained the model 5 times for calculating means and confidence\nintervals. As expected, Fig. 2 shows that performance improves as more data is used for training, with narrower confidence\nintervals. Surprisingly, even with a small number of examples (e.g., 512 total, with K = 256), the model achieves performance\ncomparable to training with the full dataset, which contains over at least 16 times more training examples in the RSNA. Notably,\nfor tasks like detecting IVH in the RSNA dataset, the 8-shots model achieves an AUC above 0.90, a result that rivals full-data\ntraining. These findings suggest that our foundation model has learned diverse and expressive features/representations during\nSSL pre-training, making it highly effective for new tasks even when trained on small labeled datasets."}, {"title": "Scaling up pre-training data", "content": "Scaling laws have proven effective in enhancing the performance of foundation models by increasing the size of the training\ndataset39. This phenomenon is not only observed in natural language and image domains40,41, but also extends to medical\nimaging14,42. As shown in Fig. 3, scaling up the foundation model by incorporating more data during self-supervised pre-\ntraining significantly improves downstream tasks performances. We compared models pre-trained with varying proportions of\nthe available data 10%, 30%, and 100% (full dataset), observing that larger pre-training datasets consistently led to better\ndownstream task performance. These findings highlight the potential of leveraging more data to achieve superior results, further\nsuggesting the value of multi-institutional collaboration and federated approaches to aggregating larger datasets to enhance\nmodel quality. Noticeably, the performance for CQ500 does not change a lot from 10% to 30%, but 100% gives a sudden\nperformance improvement, this indicates that for smaller datasets like CQ500, scaling up the data size is crucial for learning\nmeaningful representations."}, {"title": "Visual Interpretation", "content": "To gain insight into the features learned through self-supervised pre-training and supervised fine-tuning of the foundation\nmodel, we visualize the attention maps within the Vision Transformer (ViT), as shown in Fig. 4. These heatmaps highlight"}, {"title": "Discussion", "content": "Despite advances in disease detection using 3D head CT scans, current solutions are limited by the availability of annotated data\nand the complex, task-specific design requirements of network architectures. These constraints hinder the broader application of\nmachine learning in clinical disease detection. To address this, we developed a foundation model, trained on a large unlabeled\ndataset, to enable fine-tuning for multiple tasks with minimal labeled data under a unified network architecture.\nHighly accurate detection of intracerebral hemorrhages without delay is a critical clinical issue for the diagnostic decision\nmaking and treatment in an emergency room43,44. Our results indicate that 3D Head CT scans can also be used to help identify\nhemorrhage subtypes and, more interestingly, etiology. High performances and generalizability observed by our model in\ndetecting intracerebral hemorrhage have a potential to greatly assist in pre-hospital and early hospital management of blood\npressure. This is particularly important given that early blood pressure control is a key factor in preventing hematoma expansion\nand improving patient outcomes45,46.\nThis approach is also particularly valuable for extending detection capabilities to new diseases in CT imaging. For example,\nearly detection of ADRD with deep learning has traditionally relied on MRI scans47\u201349. However, access to MRI machines is"}, {"title": "Methods", "content": "Datasets\nDataset for pre-training foundation model\nWe utilized a large-scale head CT scan dataset from NYU Langone, consisting of 499,084 scans across 203,665 patients,\ncollected between 2009 and 2023. These scans were acquired using Siemens and Toshiba machines. We included all the\nnon-contrast head CT scans with ranging from 0.5mm to 5mm, kVp values between 70 and 150, and convolution kernels\nHr/Qr/J with sharpness levels of 35-45. We filtered out corrupted scan series with missing DICOM files and those containing\nless than 10 slices, resulting in 451,298 scans. We partitioned these scans by the patient IDs into training, validation, and\nheld-out validation sets in an 8:1:1 ratio to avoid the leakage of scans from the sample patient. As illustrated in Fig. 1a, this led\nto training, validation and held-out validation set with 361,663, 44,886 and 44,749 scans, respectively. The scans in the training\nset were used to train the foundation model.\nDatasets for downstream tasks\nWe evaluated our model using four datasets: one in-domain (ID) dataset from NYU Langone and three out-of-domain\n(OOD) datasets from NYU Long Island, the RSNA Challenge, and the public CQ500 dataset. Each dataset includes multiple\nhead CT disease detection classes, with some classes abbreviated as follows: Hydrocephalus (HCP), Dementia (ADRD),\nIntraparenchymal Hemorrhage (IPH), Intraventricular Hemorrhage (IVH), Subdural Hemorrhage (SDH), Epidural Hemorrhage\n(EDH), Subarachnoid Hemorrhage (SAH), and Intracerebral Hemorrhage (ICH). These classes can have co-occur in the same"}, {"title": "Label acquisition from electronic health records", "content": "As illustrated in Fig. 1b, we labeled head CT scans from NYU Langone and Long Island Hospital using electronic health\nrecords (EHR). For each head CT, we retrieved an EHR snippet for the corresponding patient based on their Medical Record\nNumber (MRN), starting from the time of the scan and covering a 90-day period. We then checked for the presence of any\ndiagnosis codes (ICD-10 codes) and medication records, within this EHR snippet that matched the predefined definitions for\neach disease, allowing us to create binary labels for each condition. The complete list of ICD-10 codes and the medications\nused for disease definitions is provided in Supplementary Table 1."}, {"title": "Data preprocessing", "content": "For the NYU Langone and Long Island datasets, we converted the DICOM files into NIfTI format using MRIcroGL dcm2nii55,\nstandardizing the file format with those from the RSNA and CQ500 datasets. Given the variability in scan protocols, which\ncan result in differing orientation, resolutions and slice thicknesses, we applied spatial normalization to transform the volume\norientation to right-anterior-superior (RAS) angle and resample with bicubic interpolation to the isotropic resolution ratio of\n(1.0, 1.0, 1.0) in the world coordinate system. This ensures uniform pixel spacing across all scans and axes.\nHead CT scans use Hounsfield Units (HU) to represent various tissue types, which span a broad range of values. To better\ncapture tissue characteristics, we applied three windowing ranges, each emphasizing specific tissue types: (40, 80) for soft\ntissue, (80, 200) for contrast-enhanced tissues and blood vessels, and (600, 2800) for bone. We then stacked the values from\neach window, producing a 3-channel 3D volume that enhances the representation of these key tissues. Similar strategy has been\napplied in Chilamkurthy et al.2.\nTo ensure compatibility with model input requirements, we transformed each volume into the desired size. We first padded\nor cropped each volume to a size of (224, 224, 224), preserving the whole brain across all axes. Then for training, we applied\ndata augmentations detailed in Supplementary Section \"Data Augmentation details\"; for evaluation, we center-cropped the\nvolumes to (192, 192, 192). Finally, we resized each volume to (96, 96, 96) as the input size for the model."}, {"title": "Model architecture", "content": "Numerous studies have demonstrated that ViT can effectively learn high-quality representations for 2D medical images at\nscale12\u201314,36,56. Our study extends this by exploring whether representations of 3D medical images (specifically head CT\nscans) can also be effectively learned at scale through the direct compression of 3D patches as model input. We employ\nthe Vision Transformer (ViT)37 as the volume encoder for our foundation model, as well as for baseline comparisons in all\nexperiments. Our model uses a ViT-Base architecture with an embedding dimension of 768, 12 self-attention layers, 12 heads,"}, {"title": "Self-supervised pretaining", "content": "Self-Supervised Learning recently has been widely adopted as learning framework for building medical foundation mod-\nels12\u201314,31,32. While previous works mainly focus on directly applying existing self-supervised learning algorithms on 2D\nmedical images, we explore how to effectively leverage these algorithms with 3D medical images. Specifically, we explore\ntwo main branches of self-supervised learning framework for building our 3D foundation model discriminative with\nself-distillation (DINO) and masked image modeling (MAE).\nSelf-Distillation Modelling (DINO) DINO7,24 is a self-supervised learning method shown promising and robust downstream\nevaluation performance in previous studies on different areas12,13. DINO uses a student-teacher framework for learning\nmeaningful representations. Both student and teacher networks share the same model architecture, while the teacher's\nparameters are updated using an exponential moving average of the student's parameters. Each input image is augmented\nmultiple times to create different views as student and teacher networks input. Specifically, we applied random global and local\ncrops, random flips, shifts in intensity and contrasts, and Gaussian blurs for augmented views. Then the student's output is\ntrained to match the teacher's output using a distillation loss, ensuring similar representations for different views of the same\nimage. We pre-trained the ViT in the DINO framework for 1000 epochs with batch size at 64 per GPU and an AdamW58\noptimizer ($\\beta_1 = 0.9, \\beta_2 = 0.95$, 0.05 weight decay). A base learning rate 3 $\\times 10^{-4}$ was applied combined with cosine scheduling\nand a linear warmup on the first 5 epochs. During pre-training, two global augmentations and three local augmentations were\napplied to enable ViT to learn both global and local features of the head CT. Because small region of brain is likely to be\ndissimilar, we observed cropping too small brain regions would cause unstable model training by making the learning task to\nbe too challenging. Therefore, we first resample the input images to 224 $\\times$ 224 $\\times$ 224. Subsequently, we perform multi-scale\ncropping by extracting both global and local crops regions, ranging from 112 $\\times$ 112 $\\times$ 112 to 224 $\\times$ 224 $\\times$ 224 for global crops\nand from 64 $\\times$ 64 $\\times$ 64 to 112 $\\times$ 112 $\\times$ 112 for local crops. After the cropping, all cropped regions are resampled to 96 $\\times$ 96 $\\times$ 96.\nFor training on 100% data, convergence on the performance for downstream tasks is observed at around 300 epochs, which\ntook around one week on four 80GB NVIDIA A100 GPUs.\nMasked Image Modeling (MAE) MAE26 is another self-supervised learning method for vision tasks, inspired by masked\nlanguage modeling in Natural Language Processing (NLP). MAE is trained to reconstruct randomly-masked patches via\nan encoder-decoder architecture, where the encoder processes visible patches of an image, while the decoder reconstructs\nthe image from encoded patches and mask tokens. Specifically, we randomly masked the patches from each volume with a\nprobability of 0.75. Mean squared error (MSE) loss is optimized to minimize the difference between the reconstructed volume\nand the original volume. We pre-trained the ViT in MAE framework for 400 epochs with batch size at 64 per GPU and an\nAdamW58 optimizer ($\\beta_1 = 0.9, \\beta_2 = 0.95$, 0.05 weight decay). A base learning rate 1.5 $\\times 10^{-3}$ was applied combined with\ncosine scheduling and a linear warmup on the first 5% steps, For training on 100% data, convergence is observed at around 250\nepochs, which took around 4 days on four 80GB NVIDIA A100 GPUs for MAE. Similar to DINO, MAE has shown success in\nlearning robust representations in many previous works35,59\u201364, including the studies on both 2D and 3D data.\nWe compared the performance on downstream tasks between two versions of foundation models pre-trained using DINO\nand MAE, as shown in Fig. 3 and Supplementary Figs. 6, 8 and 9. The results indicate that DINO consistently outperforms\nMAE across all datasets. Based on this finding, we selected the DINO-pre-trained model as our final foundation model."}, {"title": "Evaluation setting", "content": "Baseline comparisons Since no prior foundation models have been specifically trained on 3D Head CT for direct comparison,\nwe benchmark our model against Merlin20 and Google CT Foundation model17 to highlight the advantages of our domain-\nspecific foundation model. Merlin is a 3D Abdomen CT foundation model pre-trained on vision-language pairs with contrastive\nlearning and ICD code prediction task, where 6+ million images from 15,331 CTs, 1.8+ million diagnostic ICD codes from"}, {"title": "Fine-tuning and Probing classification evaluation", "content": "We assessed pre-trained model performance through full fine-tuning\n(updating all weights) and various probing methods (updating only the classification layers). For both approaches, images\nwere normalized to isotropic spacing, transformed to three HU interval channels, and reshaped to 3 $\\times$ 96 $\\times$ 96 $\\times$ 96. The entire\ntransformed 3D image was then input into the ViT model for feature extraction, followed by an additional classification layer\nfor downstream tasks. Probing utilized two strategies: linear probing, which adds a linear layer atop the ViT backbone, and\nattentive probing, which incorporates an attention layer. Attentive probing is chosen since MAE does not use [CLS] token as\nthe learning objective. Linear probing only relies on [CLS] token to perform classification and attentive probing explores\nthe interaction among all tokens67. Given the imbalances of downstream task labels, we randomly sampled a balanced subset\nfrom the training set per epoch, consisting of 5,000 samples (when fine-tuning on the NYU Langone, NYU Long Island, and\nRSNA datasets), and 500 samples when fine-tuning on CQ500. We trained all methods using the AdamW58 optimizer with a\ncosine learning rate scheduler, a learning rate of 1 $\\times 10^{-5}$ for backbone and 1 $\\times 10^{-3}$ for classification layers, cross-entropy\nloss, and a maximum of 10 epochs. The main evaluation result with linear probing is shown in Fig. 1 with fine-tuning and\nprobing comparison shown in Supplementary Fig. 6 for average performance across all diseases and Supplementary Fig. 9 for\nper disease performance. The result indicates that probing achieves performance levels close to full fine-tuning, underscoring\nthe high quality of learned representations in our model.\nFor fine-tuning model from scratch, as we observe more unstable model performance from different hyper-parameters\nacross different datasets, we perform hyper-parameters sweep across different setting and report the best performance model.\nThe sweeping hyper-parameters are lr={1e-3, 1e-4, 1e-5}, weight decay={0.01, 0.05, 0.0001, 0.00001}, epochs={10, 15, 30,\n50}, optimizer={SGD, Adam, AdamW}."}, {"title": "Few-shots classification evaluation", "content": "In order to evaluate the effectiveness of our model under scare label conditions, we\napplied few-shots learning where each class is only sampled K-times. Specifically, we chose K = 8, 16, 32, 64, 128, 256, where\nthe data is sampled such that positive and negative samples equal to K for each disease. Few-shot training was performed\nusing full fine-tuning with the same hyper-parameter settings. While we also attempted some other commonly used few-shots\nclassification methods such as k-nearest neighbors (KNN), Simple Shots68 and Prototypical Networks, we did not observe\nperformance improvement on our datasets over full fine-tuning. The main evaluation for few-shots classification is present in\nFig. 2, where we observed our model can already reach performance close to full fine-tuning with only K = 256 samples. This\ndemonstrates the effectiveness of our model under scare data training regime."}, {"title": "Visual Interpretation", "content": "Self-attention enables the Vision Transformer (ViT) to integrate information across the entire volume, even in its lowest layers.\nTo analyze the relationships among different patches within the CT volumes, we calculate the average spatial distance over\nwhich information is integrated, using the attention weights.\nLet $A^{(l,h)} \\in R^{N \\times N}$ represent the attention weight matrix for the hth attention head in the lth layer of ViT and N is the\nnumber of patches in a CT volume. d(i, j) denotes the spatial distance between patch i and patch j within the 3D volume. The\nattention distance for each patch i is computed as a weighted average distance to other patches, based on the attention weights:\n$D_i^{(l,h)} = \\sum_{j=1}^{N} A_{ij}^{(l,h)} d(i, j)$"}, {"title": "Statistical analysis", "content": "In each experiment, we report the mean and confidence interval, calculated by bootstrapping the held-out validation set 100\ntimes. For few-shot learning, where model variance is also influenced by the specific training data samples, we repeated the\ntraining and evaluation process five times with randomly sampled training data, reporting the mean and confidence interval of\nthe resulting metrics. For all statistical significance (p-values) reported in this study, we used a two-sided paired permutation\ntest with 1,000 permutations to assess the performance difference of two compared models."}, {"title": "Computing Hardware Software", "content": "All experiments are performed under Python (v3.8.11), PyTorch (v2.4.1), CUDA (12.1) and MONAI (v1.2.0). We extend ViT,\nMAE, DINO implementation from original their corresponding repositories to match our need for 3D CT image encoding."}, {"title": "Data availability", "content": "The internal clinical data involved in the study is unavailable due to privacy concerns and institutional policy. Public dataset\nRSNA is available from https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection.\nPublic dataset CQ500 is available from https://www.kaggle.com/datasets/crawford/qureai-headct. The\noriginal data is provided as DICOM files. We converted each scan from DICOM to NIfTI files and removed the scans with\nmissing slices for creating 3D imaging datasets in our evaluation. We use all slice thickness scan protocols in each scan (e.g.\nthin, plain thin, and plain scan) for CQ500, hence providing a more exhaustive evaluation on our model adaptability on different\nslice thickness for scan."}, {"title": "Code availability", "content": "The code for pre-training, fine-tuning and evaluation of the foundation model is available on https://github.com/\nNYUMedML/headCT_foundation. Due to the possibility of inferring patient face from headCT data, the model weights are\nonly available upon request after signing institutional agreement. Requests for model weight should be sent to the corresponding\nauthor and the NYU Langone Data Sharing Strategy Board (DSSB) Committee (DataSharing@nyulangone.org)."}]}