{"title": "Bridging vision language model (VLM) evaluation gaps with a framework for scalable and cost-effective benchmark generation", "authors": ["Tim R\u00e4dsch", "Leon Mayer", "Simon Pavicic", "A. Emre Kavur", "Marcel Knopp", "Bar\u0131\u015f \u00d6zt\u00fcrk", "Klaus Maier-Hein", "Paul F. Jaeger", "Fabian Isensee", "Annika Reinke", "Lena Maier-Hein"], "abstract": "Reliable evaluation of AI models is critical for scientific progress and practical application. While existing VLM benchmarks provide general insights into model capabilities, their heterogeneous designs and limited focus on a few imaging domains pose significant challenges for both cross-domain performance comparison and targeted domain-specific evaluation. To address this, we propose three key contributions: (1) a framework for the resource-efficient creation of domain-specific VLM benchmarks enabled by task augmentation for creating multiple diverse tasks from a single existing task, (2) the release of new VLM benchmarks for seven domains, created according to the same homogeneous protocol and including 162,946 thoroughly human-validated answers, and (3) an extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks, revealing performance variances across domains and tasks, thereby supporting the need for tailored VLM benchmarks. Adoption of our methodology will pave the way for the resource-efficient domain-specific selection of models and guide future research efforts toward addressing core open questions.", "sections": [{"title": "1. Introduction", "content": "The reliable and objective performance assessment, i.e., validation of Al models is crucial for both the measurement of scientific progress and translation into practice. Benchmarking for traditional narrow, task-specific AI already comes with numerous challenges (Myllyaho et al., 2021), but validation has proven to be even more complex and error-prone in the emerging field of generalist multimodal foundation models (Schaeffer et al., 2024). In the context of Vision-Language Models (VLMs), one issue that has received limited attention is the heterogeneous and often non-targeted nature of model validation (Tong et al., 2024a;b). Widely used VLM benchmarks span diverse domains and encompass a variety of tasks, providing a broad view of model capabilities across different contexts (Fu et al., 2024b; Liu et al., 2024; Ying et al., 2024; Al-Tahan et al., 2024; Yue et al., 2024).\nWe identify three key trends that highlight the critical need for personalized benchmarking approaches:\nDomain-specific benchmark demand: Numerous datasets and benchmarks are continually being released in the general computer vision field. According to our analyses, ~400 out of the 2,700 CVPR 2024 publications propose a new or modified dataset as detailed in Appendix A.1. These benchmarks cover a wide range of domains, from autonomous driving to wildlife monitoring, underscoring the need for domain-specific benchmarks.\nPopular arena platforms do not scale from an individual user's perspective: Arena-style platforms such as Chatbot Arena\u00b9 or WildVision Arena\u00b2 allow users to submit single tasks and rate the outputs of different (anonymized) models. The aggregated user ratings, in turn, can be used for the objective and comparative assessment of models. While this allows for personalized and domain-relevant evaluation, large-scale assessment from a single user perspective would be cumbersome due to the required annotation effort.\nHomogeneous evaluation: Most existing VLM benchmarks (Fu et al., 2024b; Yue et al., 2024; Wang et al., 2024; Zhang et al., 2024c), generally evaluate models using a single question per image. While this can suffice when large datasets are available-allowing for a broad range of tasks-domain experts with smaller, curated datasets face a more significant limitation. From a resource standpoint, image acquisition may also be expensive, and few tasks emerge if there is only one question per image. Furthermore, such an approach provides little insight into whether a VLM truly comprehends broad aspects of an image's semantic content."}, {"title": "2. Related Work", "content": "Taking these three trends together we conclude that there is a lack of guidance on how to set up a framework that enables personalized, domain-specific benchmarking in a resource-efficient manner. Such a framework must address the scarcity of labeled data, leverage task diversity by systematically generating multiple questions per image, and maintain resource efficiency to ensure accessibility for researchers working in specialized fields, such as wildlife monitoring, or autonomous driving.\nIn this work, we propose a resource-efficient framework for creating domain-specific VLM benchmarks via task augmentation. Our approach transforms a single type of annotation-instance segmentation into a diverse set of tasks that test a broad range of perception abilities, such as object counting, occlusion detection, brightness comparison, and more. Specifically, we focus on 2D natural images that either (1) already include instance segmentations or (2) can be annotated using recent advances in semi-automatic labeling tools (e.g., SAM (Ravi et al., 2024)). This approach allows even domains with limited labeled data to efficiently generate custom evaluation tasks. Our main contribution, summarized in Figure 1, is a resource-efficient framework for creating domain-specific VLM benchmarks via task augmentation, transforming a single type of annotation (instance segmentation) into a diverse set of tasks. We apply this framework to create seven new domain-specific VLM benchmarks and comprehensively evaluate 22 open and closed VLMs on over 37,000 tasks (for the full model list see Appendix C.1). To establish strong reference points for model evaluation, we collected an additional 162,946 human baseline answers corresponding to 37,171 questions across 1,704 images."}, {"title": "2.1. Vision-Language Benchmarks", "content": "Recent studies propose a range of evaluation benchmarks for VLMs, varying in size, number, and type of VL capabilities. Examples include Blink (Fu et al., 2024b) and MMBench (Liu et al., 2024) (>3,000 multiple-choice questions each), and MME (Fu et al., 2024a) (Yes/No questions on perception and cognition). The largest benchmarks include MMT-Bench (Ying et al., 2024) (>31,000 questions), MME-RealWorld (Zhang et al., 2024c) (>29,000 image-question pairs), and MMMU (Yue et al., 2024) (>11,500 questions). While these benchmarks cover multiple VL capabilities and domains, they require extensive labeling efforts. For example, MME-RealWorld involved 25 annotators and seven VLM experts, MMMU relied on 50 college students, while MMT-Bench lacks details on annotator numbers. Other benchmarks focus on much smaller question sets (Chen et al., 2024; Yu et al., 2024), integrating multiple existing benchmarks (Jiang et al., 2024; Al-Tahan et al., 2024), or collecting individual human preferences (Lu et al., 2024; Xu et al., 2023). Tong et al. (2024a) present a critical examination of multimodal LLM benchmarks.\nDespite the variety of datasets and tasks, a resource-efficient and generalizable approach that enables extensive evaluation of VLMs across multiple (domain-specific) tasks is still lacking. Our framework addresses this gap by empowering users to create domain-specific VLM perception benchmarks from just a few images."}, {"title": "2.2. Task augmentation and metadata", "content": "Task augmentation refers to generating multiple diverse tasks from a single existing task (Muennighoff et al., 2023). While task augmentation has been addressed from various directions (Johnson et al., 2017; Zhang et al., 2024a; Zamir et al., 2018a; Wang et al., 2023; 2024; Kuznetsova et al., 2020; Krishna et al., 2017), an easy to use framework for evaluating VLMs by domain users on their own images is still missing. The closest works to ours are Zhang et al. (2024a) and Zhang et al. (2024b), which programmatically generate benchmarks using a library of visual assets and task templates. A comprehensive comparison to other task augmentations works and their applicability is provided in Appendix A.5."}, {"title": "2.3. Resource-efficient VLM benchmarking", "content": "Most existing benchmarks often focus on performance metrics without considering the human and computational resources required to generate a benchmark (see, e.g., (Fu et al., 2024b; Liu et al., 2024)). The work that has been done on efficient benchmarking has been focused in the realm of unimodal language models (Polo et al., 2024; Perlitz et al., 2023). An exception has been Ging et al. (2024), who investigated the automatic creation of VLM benchmarks from classification datasets. Nevertheless, the increasing prominence of VLMs in research and industry (Li et al., 2024; Yang et al., 2023) is not yet reflected in efforts to increase efficiency during benchmark creation."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Framework for resource-efficient in-domain benchmarking", "content": "The framework for resource-efficient in-domain benchmarking is depicted in Figure 2. Starting with domain images that include instance segmentations (existing or created with semi-automatic labeling tools, such as SAM (Ravi et al., 2024)), metadata for each image is acquired from multiple sources (humans, pre-defined heuristics, and models) to transform the single task into a collection of perception tasks.\nFor our seven new datasets, we use existing instance segmentation as the core perceptual task to generate the diverse set of VLM benchmark tasks depicted in Appendix A.4 (examples in Figure 5 and more detailed in Appendix B.2).\nThe metadata enrichment is derived from three sources:\n1) Human annotators were used to generate information that"}, {"title": "3.2. Seven new datasets from diverse domains", "content": "We applied our proposed framework to images from seven different domains. Overall, the input images and instance segmentations for our framework were extracted from KITTI (Geiger et al., 2012), COCO (Lin et al., 2014), and COCONut (Deng et al., 2024). In summary, we added 300,000 metadata annotations to a total of 1,704 images across seven domains. This includes 15 annotations per object (e.g. occlusion, relative_size, seg_mask_touches_segmask, or average_depth). For truncation, occlusion, and direction, we obtained up to five annotations per object from human annotators (UI example is displayed in Appendix A.2). Early stopping was applied when four annotators reached a consensus. The complete list is provided in Appendix Table 4."}, {"title": "Accuracy$\\%%_{m}(t)$", "content": "The metadata were then used to define a set of 25 different VLM tasks (see Figure 3), including six tasks concerning the entire image, 13 related to individual objects, and six focused on object pairs.\nSetup for automatic task processing after metadata extraction: To create a concrete list of vision-language tasks for each image we employed a systematic process. We began by prioritizing images in the datasets that featured a higher number of classes and objects to maximize task diversity and complexity. Next, specific criteria for each task were evaluated to ensure appropriate task generation for each image. For instance, in tasks requiring the comparison of two objects, it was essential that both objects were present in the image and belonged to the relevant classes. Furthermore, we established minimum thresholds for various measures, such as requiring a substantial depth difference between objects, to ensure the correct answers for the task could be reliably determined. Overall, our objective was to generate as many of the 25 different tasks as possible for each image. No LLMs or VLMs were used for task generation, as these methods are prone to injecting hallucinations (Wang et al., 2023; 2024). We prioritized quality and reliability instead.\nHuman ambiguity baseline: To rate the difficulty and ambiguity for each of the 37,171 tasks, we further acquired annotations from six human raters per image. We implemented early stopping if four raters reached agreement on a task. Overall, this resulted in 162,946 human reference annotations. An overview of the resulting datasets is provided in Table land exemplary images for all generated datasets are included in Appendix B."}, {"title": "3.3. Benchmarking strategy", "content": "VLM benchmarking results can vary substantially with various factors, such as the images used, the domain, and the applied prompts. This often renders comparison of results across papers infeasible. For example, Accuracy is a prevalence-dependent metric, meaning that results should not be compared across datasets. To address this bottleneck, we fully homogenized our benchmarking framework using the proposed framework.\nModel selection: We selected 22 frontier and open VLMs of various sizes and from various providers and sources, as illustrated in Appendix C.1. The oldest model was released in January 2024, while the most recent one included was released at the end of September 2024.\nBenchmarking workflow: To ensure fair and consistent evaluation of all selected VLMs, we developed a standardized benchmarking workflow applied uniformly across all models. We assessed them in a zero-shot setting without any additional fine-tuning or domain-specific training. We strictly followed the configurations and setups recommended by each model's authors, using the exact settings provided in their official repositories (e.g., on Hugging Face) to ensure that each model was evaluated under conditions intended by its creators. Each model was provided with a carefully crafted text prompt alongside the corresponding image. To eliminate potential ambiguities in the questions, we conducted iterative testing of these prompts among human evaluators in our department. Through four rounds of refinement, we adjusted the prompts until all four human evaluators consistently agreed on their interpretation. Furthermore, we evaluated the sensitivity of the VLMs to variations in image markers, as many questions involved marked objects. Altering the box colors used to highlight objects-from green and red to other colors resulted in slight performance fluctuations in both directions across different VLMs. To maintain consistency, we used the commonly recognized colors red and green, assigning them to objects at random."}, {"title": "VLM tasks", "content": "We evaluated the models on a comprehensive set of 25 tasks derived from our task augmentation framework (overview in Figure 3, full list in Appendix A.4 and examples per dataset in Appendix B). Each task was associated with specific evaluation criteria and standardized prompts. For instance, when dealing with multiple-choice questions or tasks involving object selection, we established clear guidelines on how options were presented and how objects were chosen within images. This attention to detail ensured that the evaluation was both rigorous and reproducible."}, {"title": "Metrics and rankings", "content": "Choosing an adequate strategy for performance assessment is far from trivial and a research topic of its own (Maier-Hein et al., 2024; Reinke et al., 2024). In this work, we were specifically interested in relative performance differences rather than in the specific ability of VLMs to serve a specific task. To obtain aggregated performance values across images, we define the Accuracy % (t) metric with a threshold $t \\in [0, 1]$. For each image i in a dataset D, let $Q_i$ denote the set of questions associated with that image. Let $C_{i,q,m} \\in \\{0,1\\}$ indicate whether model m correctly answered question q for image i (1 for correct, 0 otherwise). The model m is considered to meet the threshold t on image i if the fraction of questions q in $Q_i$ answered correctly by the model is at least t. Formally, we define:\n$\\text{Accuracy}\\%_m(t) = \\frac{1}{|D|} \\sum_{i \\in D} I(\\frac{1}{|Q_i|} \\sum_{q \\in Q_i} C_{i,q,m} \\geq t) \\times 100$\nHere, $I(\\cdot)$ is an indicator function defined as:\n$I(x \\geq t) = \\begin{cases} 1, & \\text{if } x \\geq t, \\\\ 0, & \\text{otherwise.} \\end{cases}$\nExplanation:\n* $\\sum_{q\\in Q_i} C_{i,q,m}$: Total number of correctly answered questions for image i.\n* $\\frac{1}{|Q_i|}\\sum_{q\\in Q_i} C_{i,q,m}$: Fraction of questions answered correctly for image i.\n* t \u2208 [0, 1]: Desired minimum accuracy level assessed for each $Q_i$."}, {"title": "4. Experiments and Results", "content": "The primary purpose of our experiments was to showcase the benefit of our task augmentation approach (sec. 4.1). To assess the value of each task for VLM benchmarking, we related it to average model performance, resources needed to create the task, and corresponding human ambiguity (sec. 4.2). Finally, we leveraged our concept and data to explore the capabilities of the most recent open and closed VLMs (sec. 4.3.)."}, {"title": "4.1. Benefit of the proposed framework", "content": "Figure 2 shows aggregated performance values for all models, separated by imaging domain. As the tasks and prompts were homogenized, the results clearly indicate that performance varies substantially across domains, supporting the hypothesis that in-domain validation is crucial for real-world translation. Note that this holds true despite the fact that we purposely chose domains that are relatively common (presumably captured in the model training) and closely related to one another.\nFurthermore, as shown in Figure 3a, the performance of models varies substantially across VLM tasks, suggesting that the tasks generated by our framework are diverse. The hardest tasks on average across domains are (1) T7.2 \u201cJigsaw Puzzle Completion\", (2), T1.2 \u201cObject Counting\", (3), T7.1 \u201cRotated Jigsaw Puzzle Completion\", (4), T2.1 \u201cObject Occlusion Detection\", and (5) T5.2 \u201cSecond Brightest Image Selection\". The easiest task on average was T1.3 \"Additional Object Presence Detection\" (see Figure 24)."}, {"title": "4.2. Human Ambiguity", "content": "As demonstrated in Appendix C.3, there is a high discrepancy in task rankings between humans and models. While the \"Jigsaw Puzzle Completion\" tasks ranked amongst the most challenging for the models, humans found \"Object Occlusion Detection\" and \"Object Touching Detection\" to be the most difficult.\nFrom a resource perspective, tasks should be (1) hard to solve for models and (2) require as little human annotation as possible. This potential trade-off is captured in Appendix subsection C.6. It can be seen that many hard tasks, including the top four, can already be extracted from instance segmentations alone."}, {"title": "4.3. Insights on current models", "content": "Figure 4 summarizes the performance of a model selection and reference baselines. Further detailed analysis, including all tested models, examples, and errors for each generated dataset are provided in the Appendix. The following insights can be extracted:\nConfirming common findings from the community: Our analysis reinforces several established patterns in the field. Closed models continue to demonstrate superior performance across tasks and domains, although open models have significantly narrowed this performance gap. In particular, Qwen2 72B stands out as the strongest performer among open models. The superiority of human evaluation remains evident, with human raters achieving near-perfect performance on most tasks, though they notably struggle with specific challenges such as counting, occlusion, and direction-related tasks-counting being particularly problematic. Regarding model scaling, larger variants typically show better performance, with some notable exceptions such as Molmo 7B outperforming Pixtral 12B.\nInteresting new findings: The need for specific in-domain evaluation is highlighted by the high performance variability across imaging domains for the same perception tasks, see Table 2 and Figure 2. The overall best model, Gemini 1.5 Pro, varies between domains from 22% (Kitchen dataset) to 72% (Kitti dataset). Qwen2 72B slightly surpasses Gemini 1.5 Pro on the kitchen and animals datasets but ranks only fifth on the person dataset. Additional insights emerge from model comparisons, with Qwen2 7B consistently outperforming Molmo 7B across most datasets, and Gemini Flash 1.5 showing superior Point Depth Comparison capabilities over Gemini Pro. These results indicate that our newly introduced metric, Accuracy%(t), can effectively capture model performance in a single value."}, {"title": "5. Discussion", "content": "This paper contributes to the advancement of VLM benchmarking in three ways:\n1) Framework for resource-efficient and domain-specific benchmarking: We showed that task augmentation, using instance segmentation as the root task, enables the generation of a diverse set of VLM tasks and could thus evolve as a core method for resource-efficient domain-specific VLM benchmarking. The insights gained on the varying difficulty of presented VLM tasks will further guide the design of future benchmarks. The framework can be easily applied to other domains, even with a small number of images. The computational and monetary costs for each generated dataset are minimal and displayed in Appendix A.6.\n2) Seven new openly available datasets: Our seven new datasets will help assess generalist capabilities of future VLMs. Furthermore, we release the six human annotations per task (totaling 162,946 annotations) to assist researchers working on human annotations.\n3) New insights: The insights on current capabilities of closed and open VLMs highlight the narrowing gap between closed and open models. Most importantly, we showcased the need for domain-specific validation.\nCore strengths of our contribution include the broad applicability of our concept, the open dataset and benchmark contribution, and the wide range of state-of-the-art closed and open models investigated here.\nAs an implicit contribution, we introduced the new metric Accuracy%(t), which offers several key strengths. First, it captures model performance in a single very intuitive value. The metric is extendable with additional tasks, allowing for gradually increasing difficulty, and can be adapted to evaluate domain-specific tasks effectively. It is worth mentioning, however, that the specific properties of the metric require further analyses (Reinke et al., 2024). For example, some questions require specific image conditions, such as the presence of multiple objects for comparison. This can result in a varying number of questions per image, which, in turn, has an influence on the metric. Furthermore, tasks are treated equally without any weighting, which may overlook differences in task difficulty or importance. Users can, however, easily modify the weighting scheme to better reflect their specific evaluation priorities.\nA limitation of our work is model family dependence, as many models come from closely related families, which may hinder statistical analysis. For closed-source models, specific information about training and data is often unavailable, creating transparency issues. We provide further statistical analysis, such as ranking variability in Appendix C. Model performance showed small variations with prompt phrasing, which we mitigated through iterative testing for consistency. Additionally, our human annotations were performed by professional annotators, which may introduce ambiguity since annotators aim to complete tasks quickly.\nFuture work should focus on expanding the number of tasks generated, further enhancing the diversity and comprehensiveness of VLM benchmarks. Additionally, our method can be adapted to different domains with domain-specific questions or scaled up to support continuous extension, providing a versatile approach for evaluating models across diverse applications."}, {"title": "CODE / DATASETS / HUMAN ANNOTATIONS", "content": "Code, datasets, and annotations will be made available."}, {"title": "Impact Statement", "content": "This paper advances Machine Learning by enabling researchers to benchmark with their own data on a minimal budget. All human annotations were sourced from a reputable company following ethical guidelines."}]}