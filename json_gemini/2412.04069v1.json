{"title": "ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description", "authors": ["Xiao-Yu Guo", "Yi-Fan Li", "Yuan Liu", "Xiaoyong Pan", "Hong-Bin Shen"], "abstract": "Protein design has become a critical method in advancing significant potential for various applications such as drug development and enzyme engineering. However, protein design methods utilizing large language models with solely pretraining and fine-tuning struggle to capture relationships in multi-modal protein data. To address this, we propose ProtDAT, a de novo fine-grained framework capable of designing proteins from any descriptive protein text input. ProtDAT builds upon the inherent characteristics of protein data to unify sequences and text as a cohesive whole rather than separate entities. It leverages an innovative multi-modal cross-attention, integrating protein sequences and textual information for a foundational level and seamless integration. Experimental results demonstrate that ProtDAT achieves the state-of-the-art performance in protein sequence generation, excelling in rationality, functionality, structural similarity, and validity. On 20,000 text-sequence pairs from Swiss-Prot, it improves pLDDT by 6%, TM-score by 0.26, and reduces RMSD by 1.2 \u00c5, highlighting its potential to advance protein design.", "sections": [{"title": "Introduction:", "content": "Extensive research in the protein field has led to the accumulation of a large amount of multimodal data, supporting advancements in protein design. In recent years, models for protein structure prediction based on protein sequences have been continuously emerging, such as ESMFold \u00b9, RoseTTAFold 2, and AlphaFold 3,4. They bridge the gap between protein sequence and three-dimensional protein structure information, enabling the rapid acquisition of one modality (e.g., protein structure) from the other (e.g., protein sequence) 5. The unimodal learning tends to constrain the performance of language models 6 since unimodal information can only partially capture the complexity of protein-related data, leading to an incomplete representation.\nProtein design involves predicting or generating the sequences 7, structures 8, and functions of proteins to address specific scientific and engineering challenges. Thus, the integration of multimodal data represents a significant trend in the development of protein design models. While the connection between protein sequence and structures are well established, the relationship between protein textual information (e.g., protein function) and protein sequences has become a focal point. Consequently, a variety of Protein Language Models (PLMs) are emerging, offering new perspectives and methods that enrich the protein design process.\nThe existing protein design methods based on PLMs can be broadly classified into 6 categories.\n(1) Only sequence modality. ProGen2 10 and ProtGPT2 11 utilize millions of protein sequences for model pretraining. By leveraging the Transformer decoder 12 architecture, these models learn autoregressive sequence generation patterns, which can partially guide protein sequence generation.\n(2) Word prompts. ProGen 13 and NetGo3.0 14 perform large-scale pretraining by incorporating protein sequences along with text information. In ProGen, the word prompts are tokenized to regulate the generation of protein sequences.\n(3) Search models. ProtDT 15 and ProtST 16 enhance the pretraining of protein sequences and text-based information through improvements in contrastive learning 17 and attention mechanisms. These models construct a multimodal interaction framework between protein description texts and protein sequences. While contrastive model is not capable of directly generating protein sequences, it can facilitate search functionalities. However, they are inherently limited and unable to handle entirely novel protein sequences 18.\n(4) Knowledge graphs. OntoProtein 19 and PANNZER 20 integrate different modalities of data through knowledge graphs, designing networks to guide multimodal protein design.\n(5) Fine-tuning models. Models such as ProLLaMA 21 and HelixProtX 22 incorporate multimodal protein information, including protein sequences, protein structures and descriptive annotations, into pretrained models. These models achieve modality interaction by designing abstractors between different modalities, enabling the precise generation of proteins.\n(6) Various types of data. BioTranslator 23 and BioT5 24 integrate manually annotated text with biological data, including drug and pathway information. This multimodal integration aids in protein design and supports various protein-related tasks.\nHowever, there are several restrictions of existing PLMs. The emerging protein generation methods either focus solely on information from a single modality, such as protein sequences, or rely only on simple word prompts, lacking the ability to integrate other modalities to guide the protein generation process. Multimodal pretraining models for proteins present significant challenges, as they require high compatibility across various modalities, are susceptible to overfitting on training data, and encounter difficulties with modality alignment and expansion."}, {"title": "Previous Methods (Sequence+Text)", "content": "Recently, generative models have been widely applied in protein design, achieving impressive results 25. Previous work 26 has demonstrated that generating protein sequences from texts is effective and has been validated to some extent. In Fig 1, the training paradigm of previous generative models captures the correlation between protein sequences and their descriptive texts, allows for the expansion of knowledge, thereby enhancing the model's compatibility. However, these methods are only built on original LLMs by pretraining or fine-tuning for different modalities, enabling only coarse-grained interactions among outputs from different modality-specific models. Inspired by the large language models, such as the GPT 27 and LLaMA 28 series models, we developed ProtDAT (Protein Design from Any Text Descriptions), a protein design framework that generates protein sequences based on any specified training dataset of textual descriptions. Our method designs Multi-modal Cross-attention Mechanism (MCM) for fine-grained interactions of different modality information at the modality fusion decoder layer. By altering the framework structure, it directly integrates embeddings internally, improving the efficiency and accuracy of text-guided protein generation.\nTo the best of our knowledge, ProtDAT is the first de novo fine-grained trained framework that addresses the significant issue of inadequate guidance from protein description texts for protein sequence generation in previous PLMs. Additionally, ProtDAT removes the limitations of prompt-based input, enabling training on any standardized protein dataset composed of protein sequences and descriptions. It pushes forward for the boundary of the long-standing challenge in biological data by building effectively linking to vast multimodal data, enabling organized natural language to be converted into new protein molecules with reasonable and novel sequence and structure.\nWe first construct protein text-to-sequence datasets in a specific format and employ pretraining to develop a protein design model. ProtDAT surpasses the demand for fine-tuning across multiple pretrained models, introduces improvements to the cross-attention mechanism within Transformer models based on an autoregressive generation framework. These enhancements broaden the model's understanding of protein data and overcome the restrictions imposed by multimodal pretrained models in previous protein design frameworks.\nProtDAT is pretrained on the constructed protein text-to-sequence dataset (ProtDAT-Dataset) without explicit protein structure information. It employs a Transformer decoder-only architecture to facilitate interactions between the two modalities. A novel cross-attention mechanism is proposed in ProtDAT, i.e. MCM, which is specifically designed for handling comprehensive information, providing an intuitive, human-logical pathway to protein design. Unlike other PLMs that handle interactions between individual modal models, ProtDAT integrates the interaction of the two modalities within a single model, establishing a strong connection between them from the ground up. During this unsupervised training process, ProtDAT acquires the relationship between protein sequences and descriptions, analogous to how natural language models learn semantic and syntactic rules. This framework combines information from both modalities, enabling multimodal knowledge to interact throughout the entire training process, thereby allowing the model to generate protein sequences that satisfy specific goals.\nIn summary, the contributions of our research are as follows:\n(1) We introduce ProtDAT, the first de novo fine-grained trained framework capable of being trained on any specialized protein text-sequence dataset. By addressing the barrier of inadequate guidance from protein descriptive texts in previous methods, ProtDAT enables the generation of entirely new proteins with broad applicability and flexibility.\n(2) A novel cross-attention mechanism, MCM, is designed for the interaction between protein sequences and textual modalities, pushing the boundaries of protein design. Unlike previous approaches treating sequence and text as separate entities, MCM integrates both modalities at a foundational level, and builds effective linking to the transformation for multimodal data. It enables a unified approach that significantly enhances the model's ability to generate proteins that align more closely with the given textual descriptions.\n(3) Experiments demonstrate that in the protein generation process, textual information plays a dominant role with MCM module. ProtDAT achieves the state-of-the-art results across various evaluation metrics for both sequence and structure on the Swiss-Prot 29 dataset."}, {"title": "Results:", "content": ""}, {"title": "ProtDAT Training Framework and Data Preparation", "content": "The ProtDAT framework in Fig 2a consists of three primary input components: protein description text, protein sequence, and cross-modality tensor. These multimodal inputs are first processed by a preprocessing module that vectorizes the information. The vectorized embeddings are then passed through decoder layers with the MCM module, which enables the model to generate modality-specific output vectors. Finally, these vectors are utilized in downstream protein generation tasks, facilitating the synthesis of novel protein sequences based on the integrated textual and sequence information."}, {"title": "MCM's Role in ProtDAT Sequence Generation", "content": "In Fig 2, MCM proficiently integrates PSM, PTM, and CIM, enabling ProtDAT to generate protein sequences guided by description texts. To further illustrate the role of different attention mechanisms in facilitating the sequence generation within MCM, we visualize the attention weights across different modules during the protein sequence generation process. we applied to the 20,000 generated sequences using PM1 prompts of ProtDAT as described in Table 1.\nAssume $D_t, D_c, D_s$ represent the lengths of the text, cross-modality tensor, and sequence, respectively, the shape of attention weights in Fig 4a and Fig 4b are $D_t \\times D_t, D_c \\times D_t$. Additionally, to highlight the impact of cross-modality tensor in the CCA mechanism, its attention weights in Fig 4d is merged. Thus, the attention weights originally have a dimension of $D_s \\times (D_c + D_s)$ to illustrate the influence of CIM and PSM in guiding protein sequence generation. Therefore, with $D_c$ is condensed into one dimension, the shape of attention weights in Fig 4c is effectively represented as $D_s \\times (1 + D_s)$. It provides a magnified view of the first 20 tokens from Fig 4d, allowing for a clearer representation of the weights of CIM in guiding sequence generation. Also, Fig 4e is a zoomed-in view of the first $D_c$ columns of attention weights in Fig 4d. To facilitate visualization analysis, at most 100 dimensions of each modality can be displayed.\nFig 4 visualizes the average attention weights of the three attention mechanism modules in MCM during the generation process. The self-attention mechanism in Fig 4a is similar to the training process of the BERT 37 based model, appropriately integrating single modality data. The attention weights of the protein description text tokens gradually decrease as the text length increase, indicating that textual tokens in the beginning positions have more substantial influence in guiding the generation method. In Fig 4b, the weight differences for each token in PTM are not significant, indicating that the cross-attention mechanism effectively integrates the textual information into the cross embedding (with $D_c$ is 50), laying the foundation for subsequent fusion with protein sequence information.\nIn Fig 4d, the CCA module is performed under a causal mask shaped $D_s \\times D_s$ only apply to the last $D_s$ columns of CCA attention weights, where it can be observed that the selection of each amino acid is closely related to the weights of CIM and the preceding few amino acid tokens of PSM. As illustrated in Fig 4c, the compact attention weights of CIM play a significant role in protein sequence generation, holding a substantial weight and serving as a critical part of the procedure. Furthermore, the weights of few amino acid tokens just before the current generation time step are relatively high. The results indicate that error accumulation is a persistent phenomenon when using autoregressive models for designing proteins. The inclusion of CIM in generation procedure addresses this issue to some extent, stabilizing the whole sequence generation process.\nFig 4e is another detailed view of Fig 4d with shape as $D_s \\times D_c$. The attention weights decrease to some extent as the dimension of $D_s$ (i.e., the number of rows) increases, indicating that the guidance of the protein description text on sequence generation diminishes as the protein sequence lengthens, while still maintaining a certain level of influence.\nThus, Fig 4f provides a more intuitive display of average attention weights in Fig 4e in facilitating the generation process as sequence length (maximum to 500) grows. Assuming using $D_c$ amino acid tokens to assist ProtDAT in generation, a function is derived as $D_c * 1/(D_c + m)$ to demonstrate the weights of the prompt tokens, where $m$ denotes the length of already generated sequence tokens in the procedure. This function is presented as a curve named \u2018Reference Value' in Fig 4f. It is observed that as the protein sequence length rises, the contribution of the protein sequence portion used as a prompt to subsequent sequence generation diminishes. However, in our method, when the sequence length reaches 100, the contribution of the protein descriptive text to the generation stabilizes at around 20%, regardless of the sequence length. This further demonstrates that the textual description ensures the accurate instruction of essential amino acid tokens during the early stages of generation, and continues to provide directional support as the protein sequence grows. This effectively addresses the issue of instability in generation as the protein sequence lengthens.\nIn conclusion, MCM plays a critical role in guiding the protein generation. As a bridge connecting protein description texts and protein sequences, CIM significantly contributes to both the integration of information from protein texts and the guidance of sequence generation. MCM tackles the problem of insufficient guidance from protein descriptive texts in existing methods, facilitating robust multimodal integration and offering a cohesive framework to handle both modalities. By leveraging MCM, ProtDAT greatly enhances the precision of generated protein sequences, ensuring better alignment to the provided textual descriptions."}, {"title": "ProtDAT Designs Remote Homologs Protein Sequences", "content": "To further investigate whether the protein sequences generated by ProtDAT possess the characteristics described in the text, we compared 40,000 sequences generated from ProtDAT with different prompt methods in Generated-Dataset with the test set of ProtDAT-Dataset. ESM1b has extensively learned the representation patterns of protein sequences, it produces protein representation vectors that encode diverse structural and functional properties. These vectors can be further analyzed and compared in the vector space, enabling a deeper understanding of the relationships among proteins through the dimensionality reduction technique t-SNE 38 and clustering algorithm K-means 39,. This approach facilitates not only protein classification but also the prediction of novel functional insights by exploring the vector space for hidden patterns.\nIn Fig 5a, it is evident that the distribution of the generated and reference protein sequences in the vector space exhibits a significant similarity, indicating that the generated sequences share similar underlying properties with the test set. Notably, the clustering of sequences is predominantly concentrated at the bottom of Fig 5a, suggesting a high density of points in this region. This pattern of distribution further emphasizes the model's ability to capture and replicate the spatial characteristics of sequences in the embedding space, reinforcing its effectiveness in generating biologically relevant protein representations.\nTo further assess the level of similarities between the generated sequences and their corresponding descriptive texts, we selected six protein sequences from different regions in the mapped space with UniProt 40 IDs. These generated protein sequences exhibit an average structural similarity of over 90%, while maintaining a sequence similarity less than 25%. Such characteristics align with the definition of remote homologs, which are proteins with low global sequence similarity but high structural resemblance. This result highlights the capability of the generation process to explore and replicate the principles of remote homolog evolution, producing novel sequences that deviate at the sequence level while significantly preserving structural conservation.\nThe protein generation and evaluation process of ProtDAT is outlined as follows, with Fig 5b illustrating one case. First, ProtDAT is informed of the generation task through the input of different prompt methods, followed by the selection of generation parameters to produce protein sequences. Subsequently, the generated sequences are subjected to a similarity analysis against the references to obtain global sequence similarity. Additionally, the natural and generated sequences can be input into protein structure models such as ESMFold to derive their structures. These structures are then compared using TM-align to assess TM-score, pLDDT, and RMSD, which measure structural quality and similarity."}, {"title": "ProtDAT generate sequences with natural protein characteristics", "content": "In the ProtDAT generation module, whether the generated protein sequences closely resemble the natural arrangement of amino acids is a crucial evaluation criterion. Therefore, designing novel sequences is of paramount importance.\nTo determine the values of the generation parameters, Optuna 41 hyperparameter optimization framework is deployed, randomly selecting 3,000 protein sequences of the ProtDAT-Dataset test set. The similarities between generated sequences and natural sequences are calculated by KL divergence 42, thereby determining the range of generation parameters, as shown in Fig 6a. Subsequently, the model generates sequences via protein description texts under different Top-p and temperature coefficient parameters, while Top-p filters out tokens with low probabilities and temperature coefficient controls the diversity in the sequence generation process. Since the global sequence identity 43 cannot directly determine whether protein sequences have similar functions, therefore TM-score is implemented to evaluate the generation results from the 3D structures' perspective. Experiments are conducted with the repetition penalty set to 1.2 according to our local tests, it prevents excessive repetition of tokens, ensuring sequence variability.\nIn Fig 6a, the optimal settings were identified as Top-p=0.95 and T=1.0. Therefore, we selected Top-p values ranging from 0.55 to 1.0 with a step of 0.15 and temperature coefficients from 0.4 to 1.4 with a step of 0.2 for further generation experiments. The TM-Vec model 44 was employed to rapidly assess structural similarity from the generated protein sequences. In Table 3, the TM-score reaches the highest value when Top-p is 0.85 and the temperature coefficients are 1.0 and 1.2. To introduce randomness in sequence generation, the temperature coefficient in the subsequent model generation processes is set to 1.0."}, {"title": "Conclusion and discussion", "content": "Proteins can be described through various modalities, including natural language text, sequences, and structures, making the integration of different modalities crucial for a comprehensive understanding. ProtDAT stands as the first de novo fine-grained trained framework that can be trained on any specialized protein text-sequence dataset. By overcoming the persistent challenge of insufficient guidance from protein descriptive texts in previous methods, ProtDAT enables the generation of entirely new proteins with wide-ranging applicability and flexibility. We introduce MCM, a novel cross-attention mechanism specifically designed to facilitate interactions between protein sequences and textual data. Unlike existing methods that handle sequence and text independently, MCM integrates both modalities from the ground up, establishing robust connections for multimodal data transformation. Experimental results demonstrate that the protein sequences generated by ProtDAT effectively incorporate text information, achieving promising performance in terms of rationality, functionality and structural similarity with an average 6% improvement in pLDDT, a 0.26 increase in TM-score, and a 1.2 \u00c5 reduction in RMSD. Relevant resources can be found at https://github.com/GXY0116/ProtDAT.\nIn the future, we plan to pretrain ProtDAT by expanding its linguistic capacity with a wider range of annotated protein datasets for more nuanced text-guided protein generation. Additionally, we aim to broaden the MCM by incorporating structural attention mechanisms, extending current protocol to cover more modality, creating a more powerful tool for the field of protein design. Furthermore, we intend to extend ProtDAT beyond protein sequences to other biological languages, such as RNA, drug design and single-cell data, by training the framework on datasets from different domains with advanced mechanisms."}, {"title": "Methods", "content": ""}, {"title": "Construction and Preprocessing of ProtDAT-Dataset.", "content": "To train the ProtDAT, we obtained a non-redundant set of 469,395 protein entries from Swiss-Prot 29 database, including all entries available before 2023-02-22. We extracted protein text-based information and protein sequence from Swiss-Prot. Each entry in the dataset is a pair consisting of a protein description and its corresponding protein sequence. The protein description texts consist of three components: protein functions, subcellular localization, and protein families. These characteristics are described using expressive biomedical texts 45. To identify the correlations between sequence fragments and description sentences, the model preprocesses the two modalities using different tokenizers.\nThe texts are processed using pre-trained PubMedBERT 31 to obtain description embeddings. The tokenizer of PubMedBERT has a vocabulary size of 30,522, which covers most biomedical terms, enabling effective text embedding with a dimension of 768. For protein sequences, the ESM1b 32 tokenizer is applied for tokenization, as the ESM series models are proficient in capturing the relationships between sequences. Protein sequences are made up of combinations of amino acids, which makes it difficult to identify whether specific groups of amino acids have particular biological functions. To address this challenge, each amino acid is tokenized individually, which maximizes the model's ability to learn the relationship between the entire protein sequence and the description text. This approach enhances the likelihood of identifying protein sequence fragments with specific functions during the protein generation. To ensure uniformity in batch processing, the length of the text is limited to 512 tokens, and the length of the protein sequence is limited to 1024 tokens. The masking pretraining process for the model is outlined in MCM module.\nAdditionally, ProtDAT does not impose a strict format requirement on the dataset, as long as it is constructed based on specific description content tailored to different tasks. Here, we provide a method for extracting protein texts from Swiss-Prot to construct the dataset. During the training of ProtDAT, the format of the input ProtDAT-Dataset in Fig 2b is shown in Table A1 of Appendix A. The protein description texts primarily include protein function, subcellular localization, and protein family information, while the protein sequence is represented by its amino acid tokens. Each sequence has at least one of the aforementioned textual annotations. The specific distribution of all data pairs is shown in Table A2 of Appendix A, which indicates that these three types of description texts have a relatively high proportion in the overall dataset."}, {"title": "Decoder-Only Masking Pretraining Framework of ProtDAT", "content": "The ProtDAT pretraining framework is based on the decoder-only architecture and trained using the autoregressive generation loss function 46 (denoted as $\\mathcal{L}$). It is constructed through 12 decoder layers in Fig 2c, where each layer utilizes different attention mechanisms of different types of data. The aim is to embed multimodal information to guide the generation of protein sequences.\n(1) Input embeddings of ProtDAT. In Fig 2b, assume $s = [s_1, s_2 ..., s_n]$ represents the protein sequence tokens obtained after the ESM1b tokenization, and $t = [t_1, t_2..., t_m]$ represents the corresponding protein text annotation tokens obtained from PubMedBERT. Since the framework is designed to generate protein sequences with the aid of text, the output of the final layer of the PubMedBERT model $E_{t\\_input} = [E_{t_1}, E_{t_2}..\u2026, E_{t_m}]$ is the protein text embedding. Due to the use of a causal mask during sequence training, ProtDAT does not have access to the full information of the protein sequence. Therefore, it is not suitable to directly input the entire sequence into the ESM1b model to obtain embedding vectors. In ProtDAT, after mapping through an embedding layer, the sequence embedding is denoted as $E_{s\\_input} = [E_{s_1}, E_{s_2}..., E_{s_n}]$. Additionally, at the training step of each batch, a cross-modality tensor $c = [c_1, c_2 ..., c_{c\\_size}]$ is initialized (where $c\\_size$ represents the length of $c$ in ProtDAT).\nUpon transformation via the shared embedding layer, this vector is denoted as $E_{c\\_input} = [E_{c_1}, E_{c_2} ..., FC_{c_2}..., E_{c_{c\\_size}}]$. The above three components serve as the protein sequence embedding inputs for the subsequent training process.\n(2) Output embeddings of each decoder layer. In Fig 2c, input embeddings are first passed through layer normalization before being fed into the attention module. Compared to absolute position encoding, the relationship between the sequence and the text is more closely relevant to relative position and neighborhood information. Therefore, during the attention matrix multiplication process, ROPE 30 is applied. After the embeddings pass through the multi-modal cross-attention mechanism (MCM) module, they are then passed through a feedforward network followed by another layer normalization. The output of the i-th decoder layer is given by.\n$E^i_t, E^i_c, E^i_s = [DL(E_{s\\_input}, E_{c\\_input}, E_{t\\_input})]\\xi$ (1)\nwhere DL represents the decoder layer, $E^i_t, E^i_c, E^i_s$ are also the input of the (i+1)-th layer. The final output of the 12-layer decoder consists of the vectors denoted as: $E_{s\\_output}, E_{t\\_output}, E_{c\\_output}$, which respectively represent the embeddings of the protein sequences, the protein annotations, and the modality cross vectors.\n(3) Loss function design. The loss function is the commonly used next-token prediction loss in autoregressive generative models, which is well-suited for sequential data generation tasks. Since ProtDAT is designed specifically for text-guided protein sequence generation, this loss function ensures that each token prediction aligns with the context provided by preceding tokens, enhancing sequence coherence. Therefore, the loss function is computed only for $E_{s\\_output}$.\n$\\mathcal{L}(x) = -\\sum_{i=1}^{t}log P(X_i|x_{i_{t-1}})$ (2)\n$Loss = \\mathcal{L}(E_{s\\_output}, S)$ (3)\nwhere $\\mathcal{L}$ represents the loss for generating the sequence at time stept based on the sequence from time steps 0 to t \u2212 1, where it is implemented using the cross-entropy function.\nUnder this pretraining paradigm, the loss function does not directly involve $E_{t\\_output}$ and $E_{c\\_output}$. The MCM module of each layer enables the fusion of the two modalities during training, ensuring that the sequence embedding incorporates information from its corresponding protein description text.\nTo handle variable-length sequences, ProtDAT imposes length limits using padding"}, {"title": "ProtDAT Multi-Modal Cross-Attention Mechanism", "content": "To integrate information of diverse modalities, ProtDAT incorporates a multi-modal cross-attention mechanism (MCM) specifically designed for protein sequences and their corresponding descriptive texts, as illustrated in Fig 2d. MCM builds effective links for the transformation of multimodal data. Unlike other cross-attention mechanisms 12,16, which directly utilize the query from one modality to interact with the key and value from another modality, MCM implements a different strategy. In protein sequences, each token represents an amino acid, and simply aligning medical text vocabulary with individual amino acid token makes it challenging to learn the relationships between them. Therefore, merely crossing the two modalities is unlikely to yield optimal results, especially in protein generation tasks where error accumulation during the generation process may cause serious consequences. To address this issue, we introduced the Cross-Modality Interaction Module (CIM) in MCM module to facilitate the nuanced transfer of information from natural language into the protein sequences.\nMCM is built upon Multi-Head Attention (MHA) and incorporates the CIM. Additionally, we modified the attention masking mechanism specifically for the CIM, embedding it into the framework to ensure that the interaction between the sequence and textual information. This approach addresses the challenge faced by previous models 15,16, which requires pretraining textual annotations and sequences as separate modules. The MCM module includes three components: Protein Sequence Module (PSM), Protein Text Module (PTM), and the CIM. During training, the presence of CIM necessitates masking both the descriptive annotations and the protein sequences. For PSM, the token at time stept should only be able to attend to sequences from time steps 0 to t \u2212 1, so a causal mask should be deployed. For PTM and CIM, since they utilize self-attention and basic cross-attention mechanism, only a padding mask is required.\nIn Fig 2d, $E^{i-1}_s$, $E^{i-1}_t$ and $E^{i-1}_c$ are the inputs to the i-th decoder layer, while $E_{s\\_input}, E_{t\\_input}, E_{c\\_input} = E^i_s, E^i_t, E^i_c$. After applying layer normalization and RoPE, linear transformation is used to obtain the Query $Q^i_t$, Key $K^i_t$, and Value $V^i_t$ for PTM, and the Query $Q^i_c$ for CIM.\n$E^i_t = softmax(\\frac{Q^i_t.(K^i_t)^T}{\\sqrt{d_k}})V^i_t  E^i_c = softmax(\\frac{Q^i_c.(K^i_t)^T}{\\sqrt{d_k}})V^i_t$ (4)\nwhere $E^i_t$ represents the output of PTM from the self-attention mechanism at the current layer. $E^i_c$ is the output of CIM from cross-attention mechanism at the i-th decoder layer, which contains the compressed and unified text information, preparing for subsequent interaction with the protein sequence modality. Then MCM combines $E^i_c$ to obtain $K^i_s$ and $V^i_s$, the updated representations of the sequence and annotation for the next layer are:\n$K^i_s = Linear_k(E^i_s), V^i_s = Linear_v(E^i_s)$ (5)\nwhere $Linear_k$ and $Linear_v$ represent the linear layers that projected from $E^i_c$ to the key and value for CIM.\nSubsequently, the MCM module combines the text and sequence information, where $Q^i_s, K^i_s$ and $V^i_s$ are the query, key, and value for PSM:\n$E^i_s = softmax(\\frac{Q^i_s.concat((K^i_s, K^i_s)^T}{\\sqrt{d_k}})concat(V^i_s, V^i_s)$ (6)\nwhere $E^i_s$ serves as the output of PSM within the CCA mechanism. It appropriately integrates the data of different modalities, making it a crucial factor for calculating the loss function and designing new protein sequences. Finally, $E^i_s, E^i_t$ and $E^i_c$ are passed through residual connections, layer normalization, and a feedforward layer, formed as the inputs for the (i+1)-th decoder layer.\nThe MCM module is built upon the traditional attention mechanism, centering around CIM to link PSM and PTM. This design successfully incorporates multimodal information, enabling the model to identify associations between biomedical annotations and protein sequences. It also addresses the challenges faced by traditional cross-attention mechanisms, which result in a lack of comprehensive guidance from protein annotations throughout the generation process, thereby limiting the model's ability to accurately capture functional and structural nuances. Notably, to prevent CIM from becoming overly complex and negatively impacting the training process, vector needs to be reinitialized for each batch during the training process."}, {"title": "Protein Sequence Generation Module of ProtDAT", "content": "To assess the ability to generate protein sequences and design novel proteins, the generation module is specifically constructed for an autoregressive approach: it generates protein sequences one token at one time step, from left to right, with each subsequent token being conditioned on all previously generated tokens. Assume $[x_1, x_2 ... x_{t\u22121}]$ is the tokenized sequence at time step t\u22121 and text information is $\\mathcal{T}$, the calculation process of the probability of generating the protein sequence token at time step t = 1 is:\n$p(x_t) = \\prod_{i=1}^{t}P(x_i|(x_1, x_2 ... x_{i-1},\\mathcal{T}))$ (7)\nDue to the relatively small vocabulary size of protein sequences, Top-k parameter is unnecessary for decoding. Instead, a temperature coefficient is applied to control the diversity of the sequence generation process. Additionally, a Top-p decoding strategy is employed to eliminate tokens with very low probabilities, thereby reducing error accumulation. Finally, a repetition penalty parameter is applied to prevent the model from generating a large number of consecutive repetitive tokens during the generation process.\nIn the case of greedy search 47, the generated sequences tend to have higher repetition and lower similarity to natural sequences. Beam search 48, to some extent, can lead to error accumulation, as the autoregressive generation mechanism relies on previously generated sequences to guide subsequent token selection. Therefore, the model opts for a generation method based on probabilistic sampling 33, where multiple samples are drawn at the start of the procedure to further improve the diversity and orderliness of the generated sequences. In summary, ProtDAT employs a generation strategy 49 that combines Top-p, temperature coefficient, and repetition penalty.\nProtDAT is able to design protein sequences either individually or in batches, the required input prompt can be provided in the following two modes.\nMode I. Text-Only Input. ProtDAT only receives the protein descriptive text as input, which can be organized by selecting one or more aspects in protein function, subcellular localization, and protein family information. ProtDAT will generate a completely new protein sequence from scratch with the characteristics described in the given input text.\nMode II. Text and Sequence Input. In addition to the input described in Mode I, if there are specific requirements for the generated protein sequence, a prompt fragment can be added for a protein sequence. ProtDAT will then generate the sequence based on the provided protein description text and the input protein sequence fragment. Examples are provided in Table 4, while pseudocodes are in Appendix D."}]}