{"title": "WiFi CSI Based Temporal Activity Detection Via Dual Pyramid Network", "authors": ["Zhendong Liu", "Le Zhang", "Bing Li", "Yingjie Zhou", "Zhenghua Chen", "Ce Zhu"], "abstract": "We address the challenge of WiFi-based temporal activity detection and propose an efficient Dual Pyramid Network that integrates Temporal Signal Semantic Encoders and Local Sensitive Response Encoders. The Temporal Signal Semantic Encoder splits feature learning into high and low-frequency components, using a novel Signed Mask-Attention mechanism to emphasize important areas and downplay unimportant ones, with the features fused using Contra-Norm. The Local Sensitive Response Encoder captures fluctuations without learning. These feature pyramids are then combined using a new cross-attention fusion mechanism. We also introduce a dataset with over 2,114 activity segments across 553 WiFi CSI samples, each lasting around 85 seconds. Extensive experiments show our method outperforms challenging baselines. Code and dataset are available at https://github.com/AVC2-UESTC/WiFiTAD.", "sections": [{"title": "Introduction", "content": "Using IoT devices to recognize human activities like walking, falling, and lying down has numerous applications (Kong and Fu 2022). Recently, there has been growing interest in not just short-term activity analysis but also long-term daily behavior monitoring, which is vital for real-world applications like health monitoring and medical statistics (Gu et al. 2019).\nFor long-term monitoring, researchers are increasingly focusing on processing extended data streams, specifically through Temporal Activity Detection (TAD), which aims to automatically identify activities and their timing within prolonged monitoring data (Shou, Wang, and Chang 2016). However, most of these efforts rely on camera sensors, which require a direct line of sight (LOS) to function effectively. This limitation restricts their use in low-light conditions and raises significant privacy concerns, making them less suitable for sensitive environments where confidentiality is important (Sun et al. 2022).\nAs a privacy-preserving alternative, researchers have turned to non-invasive technologies like WiFi Channel State Information (CSI) to sense and recognize human activities (Chen et al. 2018). Unlike cameras, WiFi CSI doesn't capture visual images, eliminating privacy issues. By detecting the multi-path effects caused by human movements, CSI allows for Non-Line-of-Sight (NLOS) monitoring along fixed transmitter-receiver paths. The widespread availability of WiFi makes this approach not only cost-effective but also highly accurate for activity recognition, even in challenging conditions (Zeng et al. 2018).\nDespite significant progress in using CSI for human activity recognition (HAR), most existing methods assume that input signals are pre-segmented into distinct activities, focusing on correctly identifying activities from a predefined set of classes (Chen et al. 2018; Meng et al. 2023; Li et al. 2021a; Chavarriaga et al. 2013; Xiao et al. 2020; Yousefi et al. 2017). While effective for well-defined segments, these methods struggle in more complex scenarios where activity boundaries are not predefined, and the signal is continuous and untrimmed, as shown in Fig. 1.\nTemporal activity detection is well-developed in computer vision using visual inputs (Wang et al. 2023a), but applying these methods directly to WiFi CSI-based activity detection is challenging due to differences in data modality and characteristics. Unlike visual data, which provides rich spatial features and clear temporal sequences, WiFi CSI data is primarily temporal, noisy, and lacks intuitive spatial cues. Environmental factors introduce noise in WiFi signals, which is different from the noise typically encountered in visual data. Additionally, WiFi CSI suffers from a scarcity of annotated data, unlike the large labeled datasets available in computer vision. Moreover, WiFi CSI data, generated by inexpensive sensors, requires more efficient models than the computationally intensive approaches used in computer vision. These differences highlight the need for tailored methods for WiFi CSI, making it unsuitable to directly apply computer vision-based approaches.\nTo address these challenges, we explore wireless Temporal Activity Detection (TAD) and introduce DPWiT, an end-to-end learning model. DPWiT uses a multi-scale dual pyramid structure that combines frequency-aware feature learning with fluctuation information to accurately identify activities and their precise locations within untrimmed, long-term signals. The core component, the Dual Pyramid Temporal Context Modeling (DPTCM), generates multi-scale features through Temporal Signal Semantic Encoders (TSSE) and Local Sensitive Response Encoders (LSRE), which are fused using Cross-attention Pyramid Fusion modules. We also collected and annotated a comprehensive untrimmed WiFi CSI dataset covering seven daily activities: walk, run, jump, wave, fall, sit, and stand. This dataset includes 553 untrimmed samples with 2,114 activity instances, each annotated with start time, end time, and category. To summarize, we contribute in:\n\u2022 We systematically study the WiFi based Temporal Activity Detection (TAD) task and introduce a comprehensive solution. This includes developing a novel method, creating a real-world dataset of long-term, untrimmed multi-activity wireless signals, and establishing a new benchmark for future research.\n\u2022 We explore the classification and localization sub-tasks of TAD, finding that high-frequency information is crucial for localization, while low-frequency information is better for identifying activity categories.\n\u2022 We propose DPWiT, a model that combines frequency-aware learning with dual pyramid fusion, achieving state-of-the-art results on real-world datasets. For low-frequency learning, we introduce Signed Mask-Attention, which better highlights important areas and downplays unimportant ones, enhancing the model's focus on critical regions."}, {"title": "Related Work", "content": "Recently, deep learning-based strategies have been increasingly applied to CSI-based human activity recognition, inspired by the success of deep neural networks in various fields (Krizhevsky, Sutskever, and Hinton 2012; Li et al. 2021a). Models like ABLSTM (Chen et al. 2018) and Transformers (Li et al. 2021b) have demonstrated the advantages of temporal context modeling in improving recognition accuracy. Some studies have extended WiFi signal use to long-term monitoring, achieving effective respiration monitoring in real home environments (Tian et al. 2018; Liu et al. 2021; Wang et al. 2023b). Although they analyze successive activity recognition from the temporal angle, these methods are generally limited to single, well-defined activity segments and regular patterns. The challenge of accurately detecting and recognizing various activities from untrimmed, unrestricted WiFi CSI data remains largely unresolved."}, {"title": "Temporal Activity Detection", "content": "Temporal activity detection is well-developed in computer vision using visual inputs (Wang et al. 2023a). Existing vision-based TAD methods are divided into one-stage and two-stage approaches. Two-stage methods (Chen et al. 2022; Xia et al. 2022) first generate potential instance proposals and then classify and refine them using independently trained detectors. In contrast, one-stage methods (Shi et al. 2023; Yang et al. 2020) use an end-to-end pipeline to simultaneously localize and recognize actions. Although temporal action detection (TAD) is well-studied in the vision community, WiFi-based TAD remains in its early stages. It requires precise modeling of signal temporal boundaries and an in-depth understanding of multiple actions. However, as demonstrated in our experiments, directly applying vision-based methods to WiFi CSI signals often yields suboptimal results due to the unique characteristics of wireless signals."}, {"title": "Method", "content": ""}, {"title": "Problem Definition", "content": "Given a dataset of long-term signals $D = \\{X_i\\}_{i=1}^N$, where each signal instance $X_i$ contains $M_i$ action segments $Y_i = \\{(s_m, e_m, c_m)\\}_{m=1}^{M_i}$, with $s_m$ representing the start time, $e_m$ the end time, and $c_m$ the corresponding action category, our task is to detect all action segments in $Y_i$ based on the input signal $X_i$."}, {"title": "Some preliminary Results", "content": "Before delving into the detailed design of our proposed methods, we present some preliminary results that motivate our solution.\nWe empirically observed that CSI signals are highly complex, as illustrated in Fig. 3, primarily due to the inherently noisy characteristics caused by multi-path effects (Yang, Zhou, and Liu 2013). Despite this complexity, our task involves identifying the precise start and end times of each potential activity. We hypothesize that these temporal boundaries are largely driven by rapid changes in the signal, which can be effectively captured by high-frequency information. Additionally, identifying the specific activity category requires a comprehensive understanding of the semantic information across an entire input segment, which can be modeled by low-frequency information. Low-frequency components are more effective for distinguishing between different types of activities, as they capture the broader, more stable patterns in the signal that are crucial for accurate classification within the detected temporal boundaries. Similar approaches to signal analysis from a frequency perspective has also been validated in the vision community (Wang et al. 2022; Li et al. 2023).\nExisting work (Wang et al. 2022; Li et al. 2023) has shed light on network design from a frequency perspective. Self-attention, by focusing on all parts of the input sequence, tends to aggregate information across the entire sequence, smoothing out variations and emphasizing the global structure. This behavior is akin to a low-pass filter in signal processing, making self-attention particularly effective at extracting low-frequency features that capture broader, more stable patterns-ideal for tasks requiring a global context or semantic understanding. Convolutional layers, on the other hand, especially with small kernel sizes, are designed to capture local patterns by applying filters over small receptive fields. These operations are sensitive to sharp transitions and fine details, characteristic of high-frequency components. When combined with pooling, which down-samples the input, convolutional layers further amplify high-frequency features, making them effective at detecting edges, textures, and other fine details.\nTo validate our hypothesis, we conducted preliminary studies from a frequency perspective. In our first case study, we designed two networks: one featuring a transformer backbone and the other a convolutional network backbone. Both networks were configured with comparable numbers of parameters and employed the same classification and localization heads. We assessed these networks using two metrics: localization mean Intersection over Union (mIoU), classification precision. The mIoU measures the average IoU between the predicted and ground truth (GT) boundaries, regardless of the activity labels, evaluating the network's capability to detect potential activities within the input signal. Precision is defined as the ratio of correctly predicted activity labels those whose predicted boundaries overlap with the GT boundaries within a predefined tIoU range of [0.3:0.7:0.1]-to the total number of predictions. Precision evaluate the network's ability to accurately understand the context of the input signal, assuming that rough segmentation has already been achieved. The results, summarized in Table 1, indicate that the transformer-based solution, which focuses on learning global dependencies and capturing low-frequency semantic information, excels in classifying activity categories. Conversely, the convolutional network-based solution, adept at capturing local patterns and high-frequency features, performs better in identifying activity boundaries. To further validate this phenomenon, we conducted additional experiments. Initially, we transformed the input signal into the frequency domain using the Fast Fourier Transform (FFT). We then identified the cutoff frequency at the point where the power spectrum decreased by 6 dB and divided the frequency spectrum into two parts: low-frequency and high-frequency, based on this cutoff point. Subsequently, we obtained the low-frequency and high-frequency signals by transforming the respective frequency bands back to the time domain using the Inverse FFT (IFFT). We then input these transformed signals into our proposed model, details of which will be elaborated in the following section, and evaluated their results. The outcomes are consistent with our previous study, further confirming the role of different frequency components in the task of Temporal Activity Detection. More results could be found in the supplementary material at https://github.com/AVC2-UESTC/WiFiTAD."}, {"title": "Model Overview", "content": "Given the distinct roles of frequency components in Temporal Activity Detection (TAD), we propose a frequency-aware learning framework. As shown in Figure 2, the input signal is first processed through three CGR (Conv+GroupNorm+ReLU) layers. The resulting features are then passed to a dual pyramid temporal context modeling module, which includes $L\\times$ Temporal Signal Semantic Encoders (TSSE) and Local Sensitive Response Encoders (LSRE), followed by a cross-attention pyramid fusion module. The TSSE consists of a transformer branch and a Conv-Pool branch, designed to learn low and high frequencies, respectively. These features are integrated via a Contra-Norm module. The LSRE captures signal fluctuations in a learning-free manner, and the features from both encoders are aligned through a cross-attention pyramid fusion mechanism. Finally, a prediction head outputs the detection results for training and inference."}, {"title": "Dual Pyramid Temporal Context Modeling", "content": "The feature encoders starts from the projected feature $f \\in \\mathbb{R}^{T \\times D}$, where $T$ represents the signal timestamp points and $D$ represents the channels. Through the local sensitive response encoder and temporal signal semantic encoders, two multi-scale feature pyramids are created."}, {"title": "Temporal Signal Semantic Encoder", "content": "We designed two distinct network branches, each featuring core modules of self-attention and convolutional-pooling operations, tailored to preferentially learn low and high frequencies, respectively.\nMore specifically, given a feature $f \\in \\mathbb{R}^{T \\times D}$, we employ two branches to process the feature. The first branch is transformer-based, where we introduce a novel Signed Mask-Attention (SMA) mechanism to enhance the extraction of low-frequency features in the signal. These low-frequency features serve as crucial cues for achieving a comprehensive understanding of the semantic information across the entire segment of the input. For the input feature $f$, we divide it into multi-heads. For the $i$-th head $f_i \\in \\mathbb{R}^{T \\times d_k}$, we compute the queries, keys, and values as follows:\n$Q_i = f_iW^Q, K_i = f_iW^K, V_i = f_iW^V$ (1)\nwhere $W^Q \\in \\mathbb{R}^{T \\times d_k}$, $W^K \\in \\mathbb{R}^{T \\times d_k}$, and $W^V \\in \\mathbb{R}^{T \\times d_k}$ are projection matrices, with $d_k$ representing the projection dimension, typically defined as $d_k = \\frac{D}{M}$, where $M$ denotes the number of attention heads. The Signed Mask-Attention matrix can be expressed as:\n$A = \\sigma_e(||Q + K||_1W_\\O) \\odot \\sigma_s(Q K^T)$ (2)\nwhere $\\sigma_e$ denotes the tanh activation function, which outputs values in the range [-1, 1], and $\\sigma_s$ denotes the sigmoid activation function, which outputs values in the range [0, 1]. Additionally, $W_\\O$ is a learnable matrix with the same dimensions as $Q$. These activation functions introduce non-linearity into the learning process while also constraining the matrix values to prevent them from becoming excessively large. Our newly designed attention matrix leverages the information in $Q$ and $K$ more effectively, adjusting the magnitude of the original attention mechanism. This approach emphasizes important areas while downplaying unimportant ones, enhancing the model's focus on critical regions.\n$SMA(Q_i, K_i, V_i) = Softmax (A_i/\\sqrt{d_k}) \\times V_i$ (3)\nSubsequently, this result is added to the original features, followed by the application of a feedforward network. The output is then added again to obtain the final vector. Therefore, the SMA branch can be computed as:\n$f_{sma} = FFN(LN(SMA(DS(f)) + DS(f)))$ (4)\nwhere LN is the LayerNorm, DS is the down-sampling layer, FFN is the FeedForward Network.\nAdditionally, we designed a convolutional network-based branch to more effectively extract the high-frequency information from the input signal. This is accomplished through the use of convolution and max-pooling operations. Specifically, we have:\n$f_{pool} = Conv(\\sigma_s(Maxpool(f)) \\odot DS(f))$, (5)\nFinally, we employ a mixed module that combines channel-wise concatenation with ContraNorm (Guo et al. 2023) to aggregate the features from the two distinct branches. The ContraNorm operation has been demonstrated to effectively disentangle representations in the embedding space, thereby enhancing generalization performance.\n$f_c = Conv_{1x1}([f_{sma}, f_{pool}])$\n$f_{TSSE} = f_c - \\tau \\cdot softmax(f_c \\times f_c)f_c$ (6)\nwhere $Conv_{1x1}$ denotes a convolutional layer with a kernel size of 1 and a stride of 1, the channel is set as D. The channel-wise concatenation module is used to fuse $f_{ssa}$ and $f_{pool}$, $\\tau$ is the parameter.\nOur TSSE design allows our network to leverage the unique strengths of each frequency band, thus enhancing the precision and robustness of our temporal activity detection system. By fusing the separately learned high and low-frequency features, we create a more comprehensive representation of the signal. This fused representation effectively combines the detailed temporal boundaries derived from the high-frequency features with the contextual insights from the low-frequency features. As a result, our model excels in accurately identifying both the timing and nature of activities, even amidst complex and noisy environments, as evidence by the numerical results in the experimental section."}, {"title": "Local Sensitive Response Encoder", "content": "We further design another encoder to capture the fluctuation information to enhance the localization. As depicted in Fig. 2, the LSRE employs a channel-wise window to slide on temporal axis to extract regional information. The scale of window is determined by the LSRE order $l$. To capture the regional fluctuations, we compute the maximum and minimum values within the window to obtain the aggregated feature. This process can be formalized as:\n$f_w = Concat\\{max(f_{in}[t : t+2^l]) - min(f_{in}[t: t+2^l])\\}_{l=0}^{L}$ (7)\nwhere $f_w$ represents the aggregated feature, and the $t$ denotes the window's position, Concat means to concatenate the features in the temporal dimension. The sliding window effectively captures the local fluctuation by computing the difference between the maximum and minimum values within each window, thereby enhancing the regional saliency of the input signal. Most importantly, this operation is achieved in a learning-free manner. In this way, it reduces complexity and computational overhead, enabling faster processing and lower memory usage. It's also less prone to overfitting, making it an efficient and generalizable method for capturing essential signal characteristics. For the aggregated features, a MLP layer is utilized to transform the raw regional difference into a more robust and discriminative feature space\n$f_{LSRE} = MLP(f_w)$ (8)\nwhere the output feature $f_{LSRE}$ encapsulates crucial regional saliency information, serving as a robust and informative input for subsequent processing."}, {"title": "Cross-attention Pyramid Fusion", "content": "By obtaining the outputs from each encoder, we construct the feature pyramids. One pyramid contains the multi-scale output of TSSE, denoted as $Set_T = \\{f_0^{TSSE}, f_1^{TSSE}, ..., f_L^{TSSE}\\}$, and the other is the LSRE pyramid, denoted as $Set_L = \\{f_0^{LSRE}, f_1^{LSRE}, ..., f_L^{LSRE}\\}$. The scale of these pyramids is determined by a stride of 2. To efficiently and comprehensively fuse these two pyramid sets, we designed the Cross-Attention Pyramid Fusion, which aims to align and integrate the feature information from both pyramids. Specifically, for the features at the $l^{th}$ level, this process can be formulated as:\n$\\begin{aligned} f_{c1} &\\leftarrow CrossAttention(f_l^{LSRE}, f_l^{TSSE}), \\\\  f_{c2} &\\leftarrow CrossAttention(f_l^{TSSE}, f_l^{LSRE}) \\end{aligned}$ (9)\nwhere $f_l^{LSRE}$ represents the feature at the $l^{th}$ level of the LSRE pyramid, and $f_l^{TSSE}$ represents the corresponding feature at the $l^{th}$ level of the TSSE pyramid. The CrossAttention operation between these features enables the model to capture and exchange complementary information from both pyramids. This bi-directional attention mechanism ensures that the fused features $f_{c1}$ and $f_{c2}$ incorporate both local and global context, enhancing the overall representation capability of the model.\nAfter that, we have post-processed the two features and add additional $f_l^{TSSE}$ through layernorm for facilitating training\n$f_{det} = LN(FFN(f_{c1}+f_{c2}) + LN(f_l^{TSSE})$ (10)\nwhere $f_{det}$ represents the final fused feature used for detection at the $l^{th}$ level of the pyramid."}, {"title": "Prediction Head", "content": "We build the prediction head to process the pyramid features across multi levels. It can be divided into two symmetric classification branch and localization branch which are both realized by 3 CGR layers with a single convolution layer. The difference between the two branches is the classification branch conv projects the feature $f_{det} \\in \\mathbb{R}^{T_l \\times D}$ into the class score predictions $f_{cls} \\in \\mathbb{R}^{T_l \\times cls}$ and localization branch conv projects the feature $f_{reg} \\in \\mathbb{R}^{T_l \\times 2}$ into the boundary locations, respectively at different time stamps.\nFor an instant $t_l$ in the $l^{th}$ level, the prediction head estimates the boundary distance $d_{start}$ and $d_{end}$, the class scores $\\hat{c}$ of all categories with the background is also obtained. Then the candidate activity segments $\\hat{o} = (a, \\hat{s_t}, \\hat{e_t})$ can be decoded by\n$\\hat{a} = arg \\max(\\hat{c}), \\hat{s_t}=t-d_{start}, \\hat{e_t} = t+d_{end}$ (11)"}, {"title": "Training and Inference", "content": "Training In the training stage, the network outputs predicted candidates $\\hat{o}$, we optimize the model by aligning the predicted results with the ground truth annotations. The objective function of the proposed DPWiT optimization follows the design in (Zhang, Wu, and Li 2022), which has two sub-functions, the first sub-function $L_{Cls}$ is a focal loss(Lin et al. 2017) for classification, the second sub-function $L_{Loc}$ is a DIoU loss(Zheng et al. 2020) for distance regression. The objective function is defined as\n$L = \\sum_t(L_{Cls} + \\lambda L_{Loc})/T^+$ (12)\nwhere $T^+$ is the number of positive predictions and $\\lambda$ is an hyper-parameter to modulate the ration of $L_{Cls}$ and $L_{Loc}$.\nInference At inference stage, the predicted candidates $\\hat{o}$ with classification scores higher than threshold $\\beta$ and their corresponding instances are kept. We then assemble all predictions and process them with Soft-NMS(Bodla et al. 2017) to duplicate overlapped instances."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "We collected CSI samples to create the dataset used in our experiments. The CSI data was gathered in an empty office room measuring 7m x 12m \u00d7 2.5m. Our test bed consists of two laptops equipped with commercial Intel 5300 NICs, functioning as the transmitter (TX) and receiver (RX), respectively. Both the TX and RX each have one antenna, with thirty sub-channels available. Three student volunteers participated in the experiment, where they were asked to randomly perform a set of predefined daily activities between the TX and RX. The device's sampling frequency is 100Hz. The signal recordings cover seven daily activities-walking, running, jumping, waving, falling, sitting, and standing and include 553 untrimmed samples with a total of 2,114 activity instances. Each instance is meticulously annotated with its start time, end time, and activity category. The whole dataset is split with a 7:3 ratio as the training and testing subsets."}, {"title": "Experimental Setups", "content": "We evaluate our model's performance using Mean Average Precision (mAP) at several temporal Intersection over Union (tIoU) thresholds. tIoU is defined as the ratio of the intersection to the union of two temporal windows, determining localization accuracy. If tIoU exceeds a threshold, the window is validated for correct action classification. mAP is then obtained by averaging the Average Precision (AP) across all categories. Following Thumos14 (Idrees et al. 2017), tIoU thresholds range from 0.3 to 0.7 in steps of 0.1.\nThe baseline comparison methods are primarily adapted from the vision community and include the following: AFSD (Lin et al. 2021), which learns salient boundary features without anchors; BREM (Hu et al. 2022), which predicts multi-scale boundary quality to improve proposal scores; ActionFormer (Zhang, Wu, and Li 2022), which combines local self-attention with multiscale features for long-range temporal context; TADTR (Liu et al. 2022), which uses temporal deformable attention to focus on key snippets; Tridet (Shi et al. 2023), which models action boundaries with a relative probability distribution; TemporalMaxer (Tang, Kim, and Sohn 2023), which emphasizes feature extraction while minimizing long-term context modeling; and DyFADet (Yang et al. 2024), which introduces Dynamic Feature Aggregation to adapt kernel weights and receptive fields over time. To ensure fair comparison, we modified their pipelines to fit signal data and replaced their backbones with the same encoder used in our network. Additionally, we implemented two sliding-window baseline methods: one based on a Convolutional Network and the other on a Transformer. The input is pre-segmented according to ground truth annotations, with a window size matching the maximum ground truth boundary. These networks are trained to classify activity categories and background, and during inference, a sliding window approach is used to identify activities within each segment.\nImplementation Details The model is implemented in PyTorch, using Adam as the optimizer with an initial learning rate of 4e-5 and a weight decay coefficient of 1e-3. Training was conducted on a workstation equipped with an Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz and two Nvidia 3090 GPUs, with a batch size of 2. Training our dataset required 40 epochs, taking approximately 4 hours to complete. During the inference stage, the model outputs were processed by Soft-NMS, with a sigma value of 0.95 and a confidence threshold of 0.01. For both training and inference, single signal samples were divided into clips, each with a length of 4096 time stamps (approximately 41 seconds, covering over 2 activities), with a stride of 0.5. We utilized 8 TSSE and LSRE backbones as feature encoders, and the output features from the last 4 layers were used for detection. Regarding the hyperparameters, the coefficient \\lambda in the objective function was set to 10, the scale \\tau in Contra-Norm was set to 0.1, and the confidence threshold in focal loss \\beta was set to 0.9."}, {"title": "Main Results", "content": "We report our main results in Table 3, which demonstrate that our model outperforms all baselines and achieves state-of-the-art performance on the dataset. Notably, DyFADet achieves close to 60% accuracy, while TemporalMaxer, using MaxPooling for dimensionality reduction, loses semantic information, resulting in a lower mAP of 56.4%. Single-stage models like AFSD and BREM achieve 39.5% and 41.4% mAP, respectively, while TadTR, adapted from DETR, scores 54.1%. These results highlight the critical role of model design in temporal activity detection using WiFi-based data. Cross-person evaluation is provided in the supplementary material at https://github.com/AVC2-UESTC/WiFiTAD."}, {"title": "Ablation Study", "content": "To further verify the efficacy of our contributions, we conduct extensive ablation studies on Dataset for our method in Table 4."}, {"title": "Analyze", "content": "We present a qualitative visualization of our dataset in Fig. 3. The figure shows the ground truth activities in the CSI data alongside the best-predicted proposals from four different models. As observed, our method accurately identifies candidate actions and provides reliable temporal locations. In contrast, DyFADet correctly identifies the location but misclassifies the action, TADTR misses the location, and AFSD produces scattered predictions that fail to form a coherent result."}, {"title": "False Positive Analyze", "content": "To further diagnose the errors predicted by our proposed method, we follow the process outlined in (Alwassel et al. 2018) to conduct a False Positive (FP) Analysis. Specifically, this framework involves five error metrics, which are defined in Table 5. The results are illustrated in Fig. 4. This analysis allows us to evaluate the error profile of the top-10G predictions, where G represents the number of ground truth instances. We select the top predictions in a per-class manner, meaning we choose the top-10G; predictions from class j, where $G_j$ is the number of instances in class j. Additionally, to observe the trend of each error type, we divide the top-10G predictions into ten equal splits and examine the breakdown of the five FP error types in each split. Compared to the leading competing method, DyFADet, our method generates more informative predictions that improve positive localization and reduces confusion errors, thereby minimizing false positive judgments."}, {"title": "Conclusion", "content": "This work tackles the challenging problem of wireless temporal activity detection. We propose a Dual Pyramid Network that integrates high- and low-frequency features via a Temporal Signal Semantic Encoder and refines them using a Local Sensitive Response Encoder and cross-attention pyramid fusion. To support this task, we introduce a dataset with 2,114 activity segments from 553 WiFi CSI samples. Extensive experiments show our method significantly outperforms existing baselines, advancing temporal activity detection."}]}