{"title": "Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation", "authors": ["Quang Vinh Nguyen", "Van Thong Huynh", "Soo-Hyung Kim"], "abstract": "Colonoscopy is a common and practical method for detecting and treating polyps. Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress. Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context. This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage. The architecture consists of a complementary trilateral decoder to produce an early global map. A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map. The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task. Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones.", "sections": [{"title": "1 Introduction", "content": "Image segmentation is a major and significant topic in computer vision. This task classifies each pixel of the incoming image into predetermined classifications. Medical image segmentation is one of the highlight applications of segmentation techniques such as polyp segmentation, brain tumor segmentation, skin lesion segmentation, or lung segmentation.\nDue to its complexity, polyp segmentation has recently attracted a lot of interest. Polyps are abnormal tissue growth from the surface of internal organs and can be found in the colon, rectum, stomach, or even throat. In most cases, polyps are benign, which means that they do not indicate illness or maliciousness. However, since polyps are capable of developing into cancer, a long-term diagnostic is necessary to determine whether or not they have become malignant. Therefore, identifying polyps in the colonoscopy image is helpful in facilitating the early detection of polyp-related diseases. Colonoscopy is the primary method to locate and remove polyps, but this procedure requires a significant amount of time. In addition, there are still some challenges in a practical setting: common polyps can differ in size, color, and shape. Besides, polyps can develop haphazardly or densely in numerous sites, and may be challenging to distinguish them from surrounding tissues. This requires a reliable endoscopic polyps segmentation approach with high segmentation efficiency in difficult contexts."}, {"title": "2 Related Work", "content": "Salient Object Detection. SOD is a crucial preprocessing method for many computer vision applications including semantic segmentation, visual tracking, and image retrieval. Traditional SOD solutions are mainly based on heuristic priors (such as color, texture, and contrast) to construct saliency maps. With the advancement of Deep Neural Networks (DNNs), salient object detection (SOD) [3] [23] has made significant strides. First, several methods pay attention to improve the accuracy, Edge Guidance Network (EGNet) [29] combines each path from the top-down stream for the salient object branch and the edge detection branch.\nPolyp Segmentation. With the development of deep learning techniques, the polyp segmentation task is explored strongly. Brandao et al. [4] proposed a fully convolutional neural network to gain great polyp segmentation performance. The first U-shape architecture Unet [17] with two paths: an encoder to extract features and a decoder to educate the final map proved excellent performance in biomedical image segmentation. Motivated by the U-shape structure, several advanced versions were introduced. ResUnet++ [11] utilized the advantages of residual link, channel attention, and Atrous Spatial Pyramidal Pooling. Unet++ [32] kept rich information by dense concatenation via multi-stages. Several other variants use multi U-shape like DoubleUnet [12], and CUNet [21] to achieve better feature representation. Newer methods focus on the relationship between area and boundary, SFANet [9] considered area and border constraints. PraNet [8] applied Reverse Attention Module to correct the boundary area to boost the segmentation performance. Inspired by PraNet [8], UACANet [14] was designed with a new attention: parallel axial attention and augmenting uncertain area to model border information. SSFormer [22] used a pyramid Transformer encoder to improve the generalization ability of models and propose a new decoder to emphasize local features. Another transformer-based architecture, Polyp-PVT [7] proposed a similarity aggregation module to extract local pixels and global semantic cues from the polyp area, effectively suppressing noise in features and significantly improving their expressive capabilities."}, {"title": "3 Methodology", "content": "The overall proposed architecture is visualized in Figure 2. The network extracts four levels of spatial features ${f_i : i = 1,...,4}$. The complementary trilateral decoder [30] constructs an early global map from encoder features. The continuous attention produces two distinct semantic masks from weak and strong regions before concatenating with the early global map to give a superior final mask. Each component will be explained in further detail in the following subsections."}, {"title": "3.1 Encoder Backbone", "content": "In computer vision tasks in general, the encoder component extracts features that bring important semantics for analyzing objects. Recent works often used CNN-based, Transformer-based encoders or combined both in several particular situations. CNN-based approaches do well in extracting local features by using the local kernel, Transformer-based architectures are effective in grasping global relationships, improving generalization ability, and multi-scale feature processing ability, but they need more data or a strong pre-trained. In a different approach without transformer, our work uses a CNN-based encoder to extract features and exploit the features obtained by analyzing separate semantics. This also brings many advances in generalization ability and multi-scale feature processing ability. In particular, we use Efficientnet-V2S [20] as an encoder backbone that produces multiple levels of spatial features ${f_i \\in \\mathbb{R}^{\\frac{241 \\times 241}{W} \\times C_i}}$ where $C_i \\in {256, 512, 1024, 2048}$ and $i \\in {1,2,3,4}$."}, {"title": "3.2 Complementary Trilateral Decoder", "content": "Deep features offer important information for locating objects, while shallow features have high resolution and do not store many valuable textures. Therefore, we focus on utilizing high-level features with no regard for low-level features. In this work, we introduce a Complementary Trilateral Decoder [30] - a new SOTA decoder for silent object detection to address the loss of spatial structure, lack of boundary detail, and diluted semantic context. The early global map is obtained by $M = CTD(f_1, f_2, f_3, f_4)$. By estimating and using a decision parameter, we divide the early global map into two weak and strong regions to analyze separated semantics. The first component is strong (S) areas that hold clear object structure, and the second one is weak (W) areas that are determined by the decision parameter and refer to uncertain areas that could be reconstructed."}, {"title": "3.3 Continuous Attention for Background Semantic and Object Semantic", "content": "The multi-scale representation can assist in perceiving multi-scale objects for SOD. Particularly, polyps frequently come in a variety of sizes, hence we suggest a unique structure called Progress Atrous Spatial Pyramidal Pooling (PASPP) [27] to progressively capture multi-scale high-level features ${f_2, f_3, f_4}$. Lower-level features ${f_1}$ often hold rich detail information in appearance, we adapt Camouflage Identification Module (CIM) [7] to represent better texture and edge information. The CIM [7] is operated by the following two consecutive attention mechanisms:\n$Att_c = \\sigma (H_1(G_{max}(x))+H_2(G_{avg}(x))) \\otimes x$\n$Att_s = \\sigma (Conv3x3(Cat(R_m(x), R_a(x)))) \\otimes x$\nWhere x is the input feature. $G_{max}, G_{avg}$ are global max pooling and global average pooling functions, respectively. $H_1, H_2$ are two convolutional layers to reduce the dimension and recover the original dimension. $R_m, R_a$ are max pooling and average pooling following channel dimension. $Cat$ is concatenate operation, while $\\sigma$ stands for Sigmoid function.\nBackground Semantic. At the early stage, the beginning mask rough prediction objects without clear structural details. To boost the segmentation performance, the details around the objects need to be detected and classified more precisely. Several previous works used reverse attention[5] to highlight the outside area of objects to process the boundary constraint without correcting inaccurate details through texture information at low-level features. While low-level features often hold rich detail information, such as texture, color, edges, and polyps frequently resemble the background in appearance. Inspired by that motivation, we utilize $f_1$ to guide high-level features, modify inaccurate details around polyp objects and highlight strong components (S) to give a background semantic mask (BS).\n$BS = Conv3x3(Conv3x3(Cat(S \\otimes AG_i), i = 2,3,4))$\nObject Semantic. Recent architectures can not segment sufficiently recognized objects by creating a final mask only with an encoder and a decoder. Following our findings, several areas the model can detect, but they are so weak and easy to vanish through decoder progress because of the highlight of strong details, or detected areas but very noisy with the surrounding space. In addition, polyp colonoscopy images are one of the most difficult objects to segment because of the similarity between objects and backgrounds. Discovering weak features or challenging objects will have the potential to give outstanding performance. Therefore, instead of only using the strong (S) component as in the background semantic, we combine weak (W) and strong (S) components to explore and recover uncertain areas.\n$OS = Conv3x3(Conv3x3(Cat((S,W) \\otimes AG_i), i = 2,3,4))$\nTo implement the two ideas proposed above, we introduce Continuous Attention as depicted in Figure 3 including gate attention mechanisms that are executed consecutively to modify the semantics of input high-level features ${f_2, f_3, f_4}$ before concatenating them to provide OS and BS as described in Figure 2."}, {"title": "3.4 Loss Function", "content": "When analyzing polyp images, the consideration of boundary and area contributes greatly to segmentation performance, so we propose using a loss function described below. Whereas ACE loss [6] offers advantages in terms of geometrical limitations, compact curvature, length, and area of active contours with region similarity, whereas BCE loss [26] represents a pixel restriction. We define the loss function as follows:\n$Loss(y, \\bar{y}) = ACE (y, \\bar{y}) + BCE (y, \\bar{y})$\nWhere y, \\bar{y} denote the predicted mask and ground truth, respectively.\nACE Loss\n$ACE(y,\\bar{y}) =(\\alpha +\\beta K^2)|\\nabla \\bar{y}| + \\lambda y(c_1 - y)^2$ \n$+ \\lambda (1 - y) (c_2 - y)^2$\nWhere $\\alpha$ and $\\beta$ execute the trade-off of length and curvature. $\\lambda$ is used to balance two clauses. K is the curvature of y, $c_1$ and $c_2$ are mean intensities of the interior and outer regions.\nBCE Loss\n$BCE (y,\\bar{y}) = \\sum_{i=1}^{n}y_ilog(\\bar{y_i}) + (1-y_i)log(1-\\bar{y_i})$"}, {"title": "4 Experiments", "content": "This section describes the dataset, evaluation metrics, experimental results, and further insights into the output. From experimental results, we evaluate and compare ADSNet with current cutting-edge techniques for the polyp segmentation challenge."}, {"title": "4.1 Results", "content": "We set up the same training and testing with other methods which are compared in the next section: 1450 (900 images from Kvasir-Seg [13] and 550 images from CVC-ClinicDB [2]) for training. ETIS [1], CVC-ColonDB [18] for generalization ability testing, and remaining images in Kvasir-Seg [13] and CVC-ClinicDB [2] for learning ability evaluation."}, {"title": "Learning ability.", "content": "In this experiment, the domain of the test and train set is similar. We compare ADSNet with recent SOTA methods including: PraNet [8], DCRNet [28], SANet [24], and Polyp-PVT [7]. For a fair comparison, all results of the models mentioned above are referenced from the original papers. The detailed results shown in Table 1 indicate that ADSNet improves recent approaches. 0.920 Dice score and 0.871 IoU score on Kvasir-Seg [13] and 0.938, 0.890 on CVC-ClinicDB [2], respectively. Besides, our model also is better on $F_{\\beta}$ and MAE indexes. For qualitative results, Figure 4 shows obtained segmentation performance by all models on Kvasir-Seg [13] and CVC-ClinicDB [2]. The segmentation performance of competitors is referenced completely from public sources in Polyp-PVT [7]. The great agreement between predicted samples by ADSNet and ground truths is illustrated. This proves the efficient and accurate segmentation ability of the proposed architecture in difficult and challenging situations with uncertain areas that previous approaches have not dealt with yet."}, {"title": "Generalization ability.", "content": "The compared results are shown in Table 2. On ETIS [1] dataset, the performance obtain 0.798 Dice score and 0.715 IoU score improved by 1.39% and 1.27% over Polyp-PVT [7] respectively. ADSNet also shows great generalizability in CVC ColonDB [18] when surpassing all other SOTA methods with the highest scores, 0.815 Dice and 0.730 IoU score. Although on CVC-ColonDB [18] the $F^{w}_{\\beta}$ reduces when compared with SANet [24], our proposal still improves on the remaining. Several noteworthy and difficult samples are shown in Figure 5. It is clear that ADSNet is capable of partitioning complex and minute details in obscure situations in addition to segmenting well in challenging cases with uncertain areas."}, {"title": "4.2 Further insights", "content": "This part proves the effectiveness of the combination of OS, BS, and M in the final mask. When the background semantic (BS) modifies wrong details around and highlights the primary structure of objects. The object semantic (OS) explores ambiguous and challenging areas. We display several segmentation performances in Figure 6. It can be seen that OS and BS assist the model capture context information in both object and background semantics. Meanwhile, without OS or BS, the architecture has difficulty in detecting and categorizing uncertain details, causing misclassification as in the last column. We also visualize the output feature map of the early, background semantic, and object semantic in Figure 7 to verify the effectiveness of analyzing two distinct semantics, ADSNet offers significantly more sufficient features."}, {"title": "5 Conclusion", "content": "In this paper, we have proposed a new novel architecture called ADSNet for exploring uncertain areas to improve polyp segmentation performance. We have introduced a new SOTA decoder that utilizes high-level features from the CNN-based encoder to produce an early global map. Finally, we have analyzed two separate semantics of the early global map: background semantic and object semantic using continuous attention to give an excellent map. The obtained results demonstrate the superiority of the proposed model in dealing with challenging cases with uncertain areas, from that, obtain better Dice, IoU, $f_{\\beta}$, and MAE scores over recent state-of-the-art methods."}]}