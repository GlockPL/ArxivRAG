{"title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection", "authors": ["Kangqi Ma", "Hao Dong", "Yadong Mu"], "abstract": "This paper addresses the challenge of robotic grasping of general objects. Similar to prior research, the task reads a single-view 3D observation (i.e., point clouds) captured by a depth camera as input. Crucially, the success of object grasping highly demands a comprehensive understanding of the shape of objects within the scene. However, single-view observations often suffer from occlusions (including both self and inter-object occlusions), which lead to gaps in the point clouds, especially in complex cluttered scenes. This renders incomplete perception of the object shape and frequently causes failures or inaccurate pose estimation during object grasping. In this paper, we tackle this issue with an effective albeit simple solution, namely completing grasping-related scene regions through local occupancy prediction. Following prior practice, the proposed model first runs by proposing a number of most likely grasp points in the scene. Around each grasp point, a module is designed to infer any voxel in its neighborhood to be either void or occupied by some object. Importantly, the occupancy map is inferred by fusing both local and global cues. We implement a multi-group tri-plane scheme for efficiently aggregating long-distance contextual information. The model further estimates 6-DoF grasp poses utilizing the local occupancy-enhanced object shape information and returns the top-ranked grasp proposal. Comprehensive experiments on both the large-scale GraspNet-1Billion benchmark and real robotic arm demonstrate that the proposed method can effectively complete the unobserved parts in cluttered and occluded scenes. Benefiting from the occupancy-enhanced feature, our model clearly outstrips other competing methods under various performance metrics such as grasping average precision.", "sections": [{"title": "1 Introduction", "content": "General object grasping [8,21, 42, 45] plays a critical role in a variety of robotic applications, such as manipulation, assembling and picking. Its success lies in the ability to generate accurate grasp poses from visual observations, without requiring prior knowledge of the scene's exact structure. In recent years, substantial advancements have occurred in this domain, leading to the widespread adoption of object grasping in both industrial and service sectors. Most of modern object grasping methods rely on either point clouds [8], RGB-D multi-modal images [25] or voxels [3] as inputs for predicting the best grasp point / pose. To attain high success rate of grasping operations, it is crucial to have a full perception of the object shapes locally around each proposed grasp point. Nevertheless, since most methods only exploit a single snapshot of the target scene, the desired shape information is often incomplete owing to self-occlusion under specific camera viewpoint or mutual occlusion across adjacent objects. This causes lots of crucial volumtric clue unavailable when conducting object grasping, and thus leads to various failing cases. Fig. 1 shows an illustrative case where in a failure the gripper collides with a target object (the red hair dryer) due to incompletely-estimated object shapes under the single-view setting.\nThere are multiple potential solutions to resolve the aforementioned issue, including the adoption of multi-view scene snapshots or multi-modal learning (e.g., the joint optimization over color image and point clouds). This paper tackles this challenge by adhering to a depth-based scene representation in a single view, recognizing the practical limitations often associated with employing multi-view or multi-modal data. For example, in many cases the robotic arm is required to conduct swift grasp estimation or physically constrained to observe the scene from a restricted set of viewing angles. To restore the occluded or unobserved portions of the objects, it is a natural idea to resort to some point cloud completion works [10,14,44,46]. However, most of them focused on the fine-grained object-level completion and are compute-intensive. We instead adopt a voxel-based, relatively coarse-grained scene representation, and formulate the problem of object shape completion as inference over occupancy maps.\nIn this work we develop a local occupancy-enhanced object grasping method utilizing multi-group tri-planes. There are a vast literature on occupancy estimation neural networks [32, 39, 49] that recover the voxel-level occupancy of the scene from a single-view RGB / RGB-D image or point cloud. We argue that directly deploying these models are not computationally optimal for the task of object grasping, since most of them operate over the full scene and are not scalable to large scenes. There are two key considerations in expediting and improving the occupancy estimation. First, this work follows previous practice in object grasping that first generates a sparse set of most confident 3-D grasp points for a scene. To achieve the optimal gripper pose, the geometry surrounding the current grasp point is of central importance. The compute-demanding occupancy estimation is restricted to be within some local neighborhood of the grasp point, striking an accuracy / efficacy balance. Secondly, holistic scene context plays a pivotal role for precisely inferring the state of each voxel. However, learning over 3-D volumes is neither computationally feasible (the large number of voxels is not amenable to intensive convolutions or attention-based operations) nor necessary (most voxels are void and should not been involved in the computation). To effectively aggregate multi-scale information, we propose an idea of multi-group triplanar projection to extract shape context from point clouds. Each tri-plane constructs three feature planes by projecting the scene along its three coordinates, providing a compact representation of the scene. Features on each plane are obtained by aggregating the information from partial observation along an axis. Since such projection is information lossy, we utilize multi-group tri-planes that are uniformly drawn from SO(3) and collectively preserve the major scene structures via diverse snapshots. When predicting the occupancy of an arbitrary voxel, a feature querying scheme fusing global and local context is proposed. Both the tri-plane based occupancy estimation and grasp pose estimation are jointly learned through an end-to-end differentiable optimization."}, {"title": "2 Related Work", "content": "Object grasping. Most of the object grasping methods [15,17,20,25,27,28,45] leverage RGB-D image or point cloud to extract shape features for estimating grasp poses in cluttered scenes. Among them, [2, 15, 17] take RGB-D images as input to estimate grasp poses with rotated boxes. [20, 29, 31, 35] generate dense grasp poses and evaluate them to elect the best one. [8] propose an end-to-end 6-DoF pose-estimating model from point clouds, and [42] improve grasp point sampling via grasp-oriented affordance segmentation. Though several works (e.g., [22,23,41]) have explored object completion to facilitate object grasping, these completions focus on single object and are not jointly optimized with grasping. The separately optimized models cost more resources to train, and are incapable of discovering grasp-related regions to complete, preventing them from promoting grasp quality further and being applied to complex scenes.\nOccupancy network. Several relevant networks [4,13,26,40] have emerged recently to complete the scene from incomplete observations. [39] proposes a 3-D convolutional network in voxel form. Some works [6,18,48] devise lighter and more expressive 3D convolutions for scene completion. [19,49] propose to predict occupancy from RGB images through attentional modules. [13] reduces the attention memory cost by tri-plane representation. Nevertheless, these methods need to predict occupancy densely for the whole scene, which is unnecessary and overburdened for grasping tasks.\nTri-plane representation. A few works [1,5, 9, 30, 37, 38] have utilized tri-plane as compact 3-D scene representation. [5] and [32] bring up the idea of representing a scene as three orthogonal feature planes for 3D rendering and occupancy prediction. [13] uses the tri-plane representation for scene completion in outdoor autonomous driving tasks. [43] improves the tri-plane performance by adding positional encoding to it. [12] attempts to enrich the 3D information by lifting tri-plane to tri-volume. All of these methods only considered to aggregate context to a single group tri-plane. It will suffer from information loss in complex and occluded grasping scenes with limited views."}, {"title": "3 The Proposed Method", "content": "This section elaborates on the proposed local occupancy-enhanced object grasping model in details. The model is fed with single-view point clouds and returns multiple optimal 6-DoF grasp poses. In specific, we use the grasp pose formulation in [8], defining a 6-DoF grasp pose via grasp point, grasp direction, in-plane rotation, grasp depth and grasp width. Fig. 2 illustrates the framework of our model."}, {"title": "3.1 Local Occupancy Regions", "content": "To begin with, we encode the input point cloud $P\\in \\mathbb{R}^{3\\times N}$ with a 3-D UNet backbone network and obtain the point cloud embedding $F_P \\in \\mathbb{R}^{C_P \\times N}$, where $N$ and $C_P$ are the count of points and the number of feature channels respectively. Assume $G$ to be the collection of candidate grasp points. To construct $G$, we employ a grasp affordance segmentation procedure proposed in [42] on observed point clouds, and sample a fixed number of possible grasp points (1,024 in our implementation) from the affordance area into $G$. For the grasp direction, we regress the view-wise affordance (the average quality of grasp poses along each direction) of each grasp point and choose the view with the maximum affordance as its grasp direction. Through these, areas with a large probability of having high quality grasp poses are selected to predict local occupancy and facilitate grasp pose estimation.\nFor a 2-fingered gripper, knowing its radius $r$ and grasp depth interval $[d_{min}, d_{max}]$, we define a local grasp region $S$ in the coordinate frame centered at the grasp point to be a cylinder reachable to the gripper $S = \\{(x, y, z) | x^2 + y^2 < r^2, d_{min} \\leq z \\leq d_{max}\\}$. Let $R_g \\in \\mathbb{R}^{3\\times 3}$ be the rotation matrix derived by the grasp direction of grasp point $p_g \\in \\mathbb{R}^3$, the total possible grasp region $\\tilde{S}$ in the camera frame is formulated as:\n$S = \\{R_gx + p_g | x \\in S, p_g \\in G\\}.\\qquad(1)$\nWe voxelize the possible grasp region $\\tilde{S}$ with a fixed voxel size $v$ and get voxel set $S$, which is treated as the local occupancy prediction region."}, {"title": "3.2 Multi-group Tri-plane", "content": "Computation over the entire 3-D scene volume is computationally forbidden for large scenes. To avoid it, we devise a scheme of multi-group triplanar projection for holistic/ local scene context extraction in cluttered scenes. Each group of tri-plane is composed of three feature planes that pool the spatial features projected onto three orthogonal coordinates in some frame. Specifically, we implement the feature on each plane as the aggregation of both point cloud embeddings and the point density along an axis. Importantly, the above process of triplanar projection is lossy, thus we further propose to use multiple groups of tri-planes that differ in 3-D rotations and share the same origin, thereby more key information can be preserved via diverse aggregations.\nTo ensure the diversity across different tri-planes, we conduct a spherical linear interpolation of quaternion [36] to draw multiple tri-plane coordinate rotations uniformly in the rotation group SO(3). Given the start and the end quaternions $q_1, q_2 \\in \\mathbb{R}^4$ with $||q_1|| = ||q_2|| = 1$, and the number of tri-plane groups $K$, the interpolated coordinate frame rotations are:\n$q_i = \\frac{\\sin[(1-\\frac{i}{K})\\phi]q_1 + \\sin(\\frac{i}{K}\\phi)q_2}{\\sin\\phi},\\quad i=0,1,...,K-1,\\qquad(2)$\nwhere $\\phi = arccos(q_1^T q_2)$. Then the quaterion $q_i = (x_i, y_i, z_i, w_i)$ can be transformed to a rotation matrix $R_i \\in \\mathbb{R}^{3\\times 3}$ by:\n$R_i =\\begin{bmatrix}\n1-2z_i^2 - 2w_i^2 & 2y_iz_i + 2x_iw_i & 2y_iw_i - 2x_iz_i\\\\\n2y_iz_i - 2x_iw_i & 1 - 2y_i^2 - 2w_i^2 & 2z_iw_i + 2x_iy_i\\\\\n2y_iw_i + 2x_iz_i & 2z_iw_i - 2x_iy_i & 1 - 2y_i^2 - 2z_i^2\n\\end{bmatrix}.\n\\qquad(3)$\nIn practice we set $q_1$ as the identity rotation and set $q_2 = (0,\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}})$ satisfying $q_1^T q_2 = 0$ to maximize the distance of different rotations.\nNext, all tri-planes aggregate the point cloud embeddings and the point density along each axis separately. Let $F_{T_{ij}} \\in \\mathbb{R}^{C_P\\times H\\times W}$ and $D_{ij} \\in \\mathbb{N}^{1\\times H\\times W}$ be the aggregated point cloud embeddings and the point density on the i-th ($i \\in \\{0,1,2\\}$) plane of the j-th ($j \\in \\{0, ..., K - 1\\}$) group respectively, where $H$ and $W$ define the resolution of the plane. The aggregated point cloud embedding and the density located at $(x, y)$ on tri-planes are calculated as:\n$F_{T_{ij}} (x,y) = A(\\{1_{proj_i}(R_j p, x, y) \\cdot f_p | p \\in P\\}),\\qquad(4)$\n$D_{ij} (x,y) = \\sum_{p\\in P} 1_{proj_i} (R_j p, x, y),\\qquad(5)$\nwhere $f_p \\in \\mathbb{R}^{C_P}$ is the $p$'s corresponding embedding in $F_P$, $1_{proj_i}(\u00b7, x, y)$ is the indicative function showing whether a point's normalized projected point along the i-th axis locates at $(x, y)$, and $A(\u00b7)$ could be any kind of aggregation function (we choose max-pooling). Afterwards, $D_{ij}$ is normalized via soft-max and obtain density $\\tilde{D}_{ij} \\in [0,1]^{1\\times H\\times W}$. Ultimately, three 2-D plane-oriented encoders $\\mathcal{E}_i$ ($i \\in \\{0,1,2\\}$) shared by all groups fuse the aggregated embedding and density into multi-group tri-plane context $F_{T_{ij}} \\in \\mathbb{R}^{C_T\\times H\\times W}$ by:\n$F_{T_{ij}} = \\mathcal{E}_i(F_{T_{ij}} \\oplus \\tilde{D}_{ij}),\\qquad(6)$\nwhere $\\oplus$ refers to concatenation and $C_T$ is the number of feature channels of each plane.\nThe utilization of multi-group tri-plane approximately captures global scene context in a concise way. On the one hand, more aggregation groups improve the possibility of restoring features for the occluded parts and enriches the 3-D shape clues during projection. On the other hand, it significantly reduces the data size during calculation and avoids the direct operation on dense 3D volume features. The spatial resolution of tri-planes can thus be set to be larger for better representing delicate shape information."}, {"title": "3.3 Local Occupancy Query", "content": "We further propose a feature query scheme for efficiently fusing the global and local context of the scene, for the sake of occupancy estimation. The target points to be queried $P_q \\in \\mathbb{R}^{3\\times M}$ are the centers of the voxels in local occupancy region $S$, where $M$ is the number of the voxels in $S$. For each queried point $p_q \\in P_q$, its global context $f_G$ is the fusion of the bi-linear interpolated features on the projection points of different planes. Specifically, an encoder $\\mathcal{E}_1$ shared by all tri-plane groups will first fuse the three interpolated features from j-th group into $f_{ij}$, and an another encoder $\\mathcal{E}_2$ will then fuse the features from different groups into $f_G$:\n$f_{ij} = BI(F_{T_{ij}}, proj_i(R_j p_q)),\\qquad(7)$\n$f_{T_j} = \\mathcal{E}_1(\\oplus_i f_{ij}),\\quad f_G = \\mathcal{E}_2(\\oplus_j f_{T_j}),\\qquad(8)$\nwhere $proj_i()$ is the function which calculates the normalized projected point along the i-th axis, $BI(\u00b7, \u00b7)$ is the bi-linear interpolation function and $\\oplus$ denotes the concatenation. While global context $f_G$ contains the long-distance context related to the querying point, such as the scene's structure and object occlusion relationships, it still needs delicate local shape context to predict occupancy. For this reason, the local context $f_L$ draws the information from observed point clouds and the position embeddings of the relative translation to the nearest grasp point. We first find $p_q$'s nearest neighbour $p'$ in $G$ and the corresponding point cloud embedding $f_{p'}$, then the local context $f_L$ is calculated as:\n$f_L = f_{p'} \\oplus \\mathcal{E}_{PE}(p_q, p', p_q - p'),\\qquad(9)$\nwhere $\\mathcal{E}_{PE}$ is an MLP to generate position embedding. At last, the queried feature $f_{p_q} \\in \\mathbb{R}^{C_Q}$ is obtained by $f_{p_q} = f_G \\oplus f_L$ where $C_Q$ is the number of feature channels, and an MLP based decoder predicts the occupancy probability of $p_q$ according to $f_{p_q}$."}, {"title": "3.4 Grasp Pose Estimation With Completed Shape", "content": "Having obtained the completed shape around grasp-related regions, the local occupancy-enhanced grasp pose estimation is done in three sequential steps: extracting occupancy-enhanced local shape feature, refining grasp direction and decoding shape feature into grasp poses.\nOccupancy-enhanced local shape feature extraction. With the queried features and the occupancy probability of a grasp region, we can extract local occupancy-enhanced feature from completed shape information in local regions. For each grasp point $p_g \\in G$, we conduct the cylinder crop as in [8] and get the predicted occupied voxels in its local grasp region. Assume the center points of occupied voxels in one local grasp region are $P_o \\in \\mathbb{R}^{3\\times N_o}$ and their corresponding queried features are $F_o \\in \\mathbb{R}^{C_Q\\times N_o}$, where $N_o$ is the number of occupied voxels in local grasp region. Next, as $P_o$ is an explicit form of local shape, a shape encoder composed of 4 point set abstraction layers proposed in Pointnet++ [33] extracts the delicate shape feature from $P_o$. In addition, some important implicit shape information may have been embedded in $F_o$, therefore we randomly sample a few key points from $P_o$. Their corresponding queried features in $F_o$ are processed with max-pooling as the holistic feature of the local region. Finally, these two kinds of features are concatenated as the local occupancy-enhanced shape feature.\nGrasp direction refinement. From the experiments, we observe that compared with the other grasp pose parameters, grasp directions have a greater impact on the quality of the grasp poses. However, the grasp directions predicted for local grasp regions previously only consider the information from the incomplete single view point cloud. Therefore, due to the lack of complete scene information, the previously predicted directions may result in bad grasp poses and increases the risk of collision, especially in occluded areas. To address this, we propose to refine the grasp direction based on complete shape information. To this end, after local occupancy prediction, we extract the local shape feature of the grasp region with the method mentioned above, and then use it to re-estimate the grasp direction. Thus, a refined grasp direction can be inferred. Then, we extract local shape feature again in the new grasp region for grasp pose estimation. It should be noted that after refining the grasp direction, there is no need of querying occupancy again in the new grasp region. This is because we find the densely sampled grasp points make the grasp region $S$ cover almost every occupied voxels around the grasp affordance areas, which means the refined local region rarely contains undiscovered occupied voxels.\nEstimating grasp poses. Up to now the grasp point and direction have been determined. For the rest parameters of a grasp pose, including in-plane rotation, grasp depth and width, we use a grasp pose decoder with the same structure in [42] to decode the occupancy-enhanced local shape feature. It regresses the grasp widths and the grasp scores for several discrete combinations of in-plane rotations and grasp depths. Finally, the parameter combination with the maximum grasp score is regarded as the grasp pose estimation result of each grasp region."}, {"title": "3.5 Loss Function", "content": "Our model is optimized in an end-to-end fashion and supervised by the ground-truth local occupancy labels and grasp pose labels. The loss function is a multi-task loss consisting of local occupancy loss and grasp pose loss. Assume the occupancy prediction in local grasp regions is $o$, the corresponding occupancy ground truth $o_{gt}$ is generated by cropping the total scene occupancy with the predicted local region, then the occupancy loss is a binary cross-entropy loss $L_o(o, o_{gt})$. The grasp pose loss consists of the affordance segmentation loss $L_a$, view-affordance loss $L_v$, grasp width loss $L_w$ and grasp score loss $L_s$. Following common practice, $L_a$ is a binary cross-entropy loss and the rested losses are smooth-L1 losses. Putting all together, assume $g_t$ to be the ground-truth, the total loss is written as:\n$L = L_o(o, o_{gt}) + \\lambda_1 L_a(a, a_{gt}) + \\lambda_2 L_v(v, v_{gt}) + \\lambda_3 (L_w(w, w_{gt}) + L_s(s, s_{gt})),\\qquad(10)$\nwhere $\\lambda_1, \\lambda_2, \\lambda_3$ are loss weights and $a, v, w, s$ denote predicted affordance segmentation, view-affordance, grasp width and grasp score respectively."}, {"title": "4 Experiments", "content": "Dataset. We evaluate the proposed grasping model on GraspNet-1Billion benchmark [8]. It is a large-scale real-world grasping dataset containing 190 cluttered grasping scenes and 97,280 RGB-D images captured by 2 kinds of RGB-D cameras from 256 different views. 88 objects with dense grasp pose annotations are provided. The test set is divided into 3 levels (seen / similar / novel) according to the familiarity of objects. For the occupancy label, we utilize the Signed Distance Function (SDF) of each object and the object pose annotations to generate scene level occupancy.\nBaselines. We run several state-of-the-art competing methods on GraspNet-1Billion [8] benchmark, including [7, 8, 11, 20, 24, 27, 31, 34, 42, 47]. For fair comparison, all baseline models only read single-view point clouds as ours.\nMetrics. For grasp pose estimation, we report the grasp AP [8] of the top-50 grasp poses after grasp pose-NMS. The grasp AP is the average of $AP_\\mu$, where $\\mu$ is the friction coefficient ranging from 0.2 to 1.2 and $AP_\\mu$ is the average of Precision@k for k ranges from 1 to 50. For the local occupancy prediction, we report the F1-Score and the volumetric IOU in the predicted local occupancy region. F1-Score is the harmonic average of the precision and recall and volumetric IOU is the intersection volume over the union volume for occupancy prediction and ground truth.\nImplementation details. As the number of voxels varies across different scenes, for the convenience of mini-batch training, we randomly sample 15,000 voxels for occupancy prediction during training. For the inference period, there is no such restriction about the number of voxels. In addition, the view-wise affordance loss is the averaged of the view-wise affordance before and after grasp direction refinement. The width loss is only calculated when the corresponding grasp score is positive. As for the occupancy ground truth label, it is generated from the object models in [8]. We first generate object-level occupancy label by the SDF of each object. We sample points with a fixed voxel size in the self coordinate frame of the object, and for each point $x \\in \\mathbb{R}^3$, $SDF(x) \\leq 0$ means $x$ is an inner point. These inner points are regarded as the centers of occupied voxels. Then both of the object and table plane voxels are transformed to the camera frame to generate the scene-level occupancy labels. The object poses in camera frame is annotated by the dataset providers.\nThe default number of tri-plane groups $K$ is set to be 3 and the default tri-plane size $H \\times W$ is set as 64 \u00d7 64. The 3D UNet backbone has 14 layers with the output dimension $C_P = 256$. The tri-plane encoders {$\\mathcal{E}_i$} are 6-layer ResNets with the output dimension $C_T = 128$. For the local grasp regions, the gripper radius $r$ is 0.05m, $d_{min}$ and $d_{max}$ are -0.01m and 0.04m, and the voxel size $v$ is 0.01m. The number of the queried features' channels is $C_Q = 512$. For each grasp point, we predict grasp scores and widths for 12 possible in-plane rotations combined with 4 possible depths. We train our model on 4 Nvidia 1080Ti GPUs for 12 epochs with the Adam optimizer [16], and the training procedure takes about 30 hours. The learning rate is 0.001 and the batch size is 4. The loss weights are $\\lambda_1 = 10, \\lambda_2 = 100, \\lambda_3 = 10$ without further finetuning.\nExperimental results. The grasping performance on RealSense and Kinect RGB-D cameras are reported in Tab. 1. Note that [24] uses different grasp points sampling strategies so that it performs well in some cases (e.g., $AP_{0.4}$ of similar objects). Under the metric of AP, our method achieves scores of 70.88 / 63.41 / 27.31 on seen / similar / novel scenes using the sensor of RealSense respectively. This outperforms previous state-of-the-art method by large margins of 5.18 / 9.66/3.33. Similar observation holds for the Kinect-captured data. Notice that the promotion of the similar scenes is the most distinct among three test levels. We give this credit to that the grasp pose estimation with only incompletely observed shape has a weakness of generalizing to other objects due to the lack of shape information. With the local occupancy enhancement, grasping module can associate grasp poses with the completed shape and thus improves the ability to be generalized to the objects with similar shapes. Moreover, as local occupancy prediction can reconstruct the scene with an explicit voxel representation, utilizing the predicted occupancy also improves the effectiveness of collision detection. With an additional post processing to filter out the grasp poses collided with predicted occupancy, the results are improved by 1.69 AP on average.\nFor the results of local occupancy prediction, we report the performance of occupancy prediction in the grasp regions in Tab. 2. It turns out that the local occupancy prediction is capable to complete the shape of objects in grasp regions and can be generalized to different scenes.\nOccupancy prediction strategies. To demonstrate the effectiveness of local occupancy prediction and multi-group tri-planes, comparisons of different occupancy prediction strategies (on RealSense set) are shown in Tab. 3. The comparisons include (1) whether to use tri-planes, (2) whether to predict occupancy locally and (3) whether to fuse global and local context during occupancy query. The grasp AP, volumetric IOU, inference time and number of calculated voxels are reported. We compare our method with four kinds of strategies. The first one is the baseline without occupancy enhancement. The second type 'Global 3D Conv' stands for densely estimating the occupancy of the whole scene with a scene-completion-oriented 3D convolution proposed in [18] in a predefined region with a resolution of 603. The third kind 'Global Tri-plane' denotes to use multi-group tri-plane to capture scene context but to predict occupancy of the whole scene. The forth kind 'Ball Query' predicts occupancy in local occupancy region, but only uses the features from nearby point cloud embedding by conducting ball query proposed in [33] to get local context. Note that the 'Global 3D Conv' and 'Global Tri-plane' models cost too large GPU memory for end-to-end training, thus we first train occupancy prediction modules and then freeze them during training grasp pose estimators. The result shows that predicting occupancy within the local grasp regions instead of the whole scene and aggregating scene context with multi-group tri-plane are important for reducing the computational cost without losing the performance. Theoretically, the complexity of multi-group tri-plane aggregation is O(KHW) and the complexity of local occupancy query is O(KM), while the complexity of learning 3D feature volume is O(HWD), where D is the depth length. Since the area of grasp regions is far less than the whole scene (statistically 15 times in Tab. 3), K(HW + M) is smaller than HWD by a non-trivial factor. Therefore the overhead of occupancy prediction is greatly lightened and the additional inference time is acceptable for real-time practical application. Besides, demonstrated by the poor performance of 'Ball Query', capturing long-distance scene context is crucial for occupancy prediction in cluttered scenes. These results prove the effectiveness as well as the efficiency of our method.\nAblation study. We explore the effect of each design through ablation studies. We compare the default setting with different modules combinations in Tab. 4. From the first three rows, aggregating point cloud embedding and point density are both helpful for occupancy prediction and grasp pose estima-"}, {"title": "5 Conclusion", "content": "In this paper, we propose a local occupancy-enhanced grasp pose estimation method. By completing the missing shape information in the candidate grasp regions from a single view observation, our method boosts the performance of object grasping with the enhanced local shape feature. Besides, to infer local occupancy efficiently and effectively, the multi-group tri-plane is presented to capture long-distance scene context as well as preserving 3D information from diverse aggregation views. Comprehensive experiments on benchmarks and real robotic arm demonstrate that completed shape context is essential to grasp pose estimation in cluttered scenes, and our local occupancy prediction is of significance for promoting the performance of object grasping."}]}