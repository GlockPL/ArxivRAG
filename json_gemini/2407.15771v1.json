{"title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection", "authors": ["Kangqi Ma", "Hao Dong", "Yadong Mu"], "abstract": "This paper addresses the challenge of robotic grasping of general objects. Similar to prior research, the task reads a single-view 3D observation (i.e., point clouds) captured by a depth camera as input. Crucially, the success of object grasping highly demands a comprehensive understanding of the shape of objects within the scene. However, single-view observations often suffer from occlusions (including both self and inter-object occlusions), which lead to gaps in the point clouds, especially in complex cluttered scenes. This renders incomplete perception of the object shape and frequently causes failures or inaccurate pose estimation during object grasping. In this paper, we tackle this issue with an effective albeit simple solution, namely completing grasping-related scene regions through local occupancy prediction. Following prior practice, the proposed model first runs by proposing a number of most likely grasp points in the scene. Around each grasp point, a module is designed to infer any voxel in its neighborhood to be either void or occupied by some object. Importantly, the occupancy map is inferred by fusing both local and global cues. We implement a multi-group tri-plane scheme for efficiently aggregating long-distance contextual information. The model further estimates 6-DoF grasp poses utilizing the local occupancy-enhanced object shape information and returns the top-ranked grasp proposal. Comprehensive experiments on both the large-scale GraspNet-1Billion benchmark and real robotic arm demonstrate that the proposed method can effectively complete the unobserved parts in cluttered and occluded scenes. Benefiting from the occupancy-enhanced feature, our model clearly outstrips other competing methods under various performance metrics such as grasping average precision.", "sections": [{"title": "1 Introduction", "content": "General object grasping [8,21, 42, 45] plays a critical role in a variety of robotic applications, such as manipulation, assembling and picking. Its success lies in the ability to generate accurate grasp poses from visual observations, without requiring prior knowledge of the scene's exact structure. In recent years, substantial advancements have occurred in this domain, leading to the widespread"}, {"title": "2 Related Work", "content": "Object grasping. Most of the object grasping methods [15,17,20,25,27,28,45] leverage RGB-D image or point cloud to extract shape features for estimat-ing grasp poses in cluttered scenes. Among them, [2, 15, 17] take RGB-D im-ages as input to estimate grasp poses with rotated boxes. [20, 29, 31, 35] gener-ate dense grasp poses and evaluate them to elect the best one. [8] propose an end-to-end 6-DoF pose-estimating model from point clouds, and [42] improve grasp point sampling via grasp-oriented affordance segmentation. Though sev-eral works (e.g., [22,23,41]) have explored object completion to facilitate object grasping, these completions focus on single object and are not jointly optimized with grasping. The separately optimized models cost more resources to train, and are incapable of discovering grasp-related regions to complete, preventing them from promoting grasp quality further and being applied to complex scenes.\nOccupancy network. Several relevant networks [4,13,26,40] have emerged recently to complete the scene from incomplete observations. [39] proposes a 3-D convolutional network in voxel form. Some works [6,18,48] devise lighter and more expressive 3D convolutions for scene completion. [19,49] propose to pre-"}, {"title": "3 The Proposed Method", "content": "This section elaborates on the proposed local occupancy-enhanced object grasp-ing model in details. The model is fed with single-view point clouds and returns multiple optimal 6-DoF grasp poses. In specific, we use the grasp pose formula-tion in [8], defining a 6-DoF grasp pose via grasp point, grasp direction, in-plane rotation, grasp depth and grasp width."}, {"title": "3.1 Local Occupancy Regions", "content": "To begin with, we encode the input point cloud $P\\in R^{3\\times N}$ with a 3-D UNet backbone network and obtain the point cloud embedding $F_p \\in R^{C_p \\times N}$, where N and $C_p$ are the count of points and the number of feature channels respectively. Assume G to be the collection of candidate grasp points. To construct G, we employ a grasp affordance segmentation procedure proposed in [42] on observed point clouds, and sample a fixed number of possible grasp points (1,024 in our implementation) from the affordance area into G. For the grasp direction, we regress the view-wise affordance (the average quality of grasp poses along each direction) of each grasp point and choose the view with the maximum affordance as its grasp direction. Through these, areas with a large probability of having high quality grasp poses are selected to predict local occupancy and facilitate grasp pose estimation.\nFor a 2-fingered gripper, knowing its radius r and grasp depth interval [dmin, dmax], we define a local grasp region S in the coordinate frame centered at the grasp point to be a cylinder reachable to the gripper $S = \\{(x, y, z) | x^2 + y^2 < r^2, d_{min} \\le z \\le d_{max}\\}$. Let $R_g \\in R^{3\\times 3}$ be the rotation matrix derived by the grasp direction of grasp point $p_g \\in R^3$, the total possible grasp region S in the camera frame is formulated as:\n$S = \\{R_gx + p_g | x \\in S, p_g \\in G\\}.$"}, {"title": "3.2 Multi-group Tri-plane", "content": "Computation over the entire 3-D scene volume is computationally forbidden for large scenes. To avoid it, we devise a scheme of multi-group triplanar projection for holistic/ local scene context extraction in cluttered scenes. Each group of tri-plane is composed of three feature planes that pool the spatial features projected onto three orthogonal coordinates in some frame. Specifically, we implement the feature on each plane as the aggregation of both point cloud embeddings and the point density along an axis. Importantly, the above process of triplanar pro-jection is lossy, thus we further propose to use multiple groups of tri-planes that differ in 3-D rotations and share the same origin, thereby more key information can be preserved via diverse aggregations.\nTo ensure the diversity across different tri-planes, we conduct a spherical linear interpolation of quaternion [36] to draw multiple tri-plane coordinate ro-tations uniformly in the rotation group SO(3). Given the start and the end quaternions $q_1,q_2 \\in R^4$ with $||q_1|| = ||q_2|| = 1$, and the number of tri-plane groups K, the interpolated coordinate frame rotations are:\n$q_i = \\frac{sin[(1 - \\frac{i}{K-1})\\phi]q_1 + sin(\\frac{i}{K-1}\\phi)q_2}{sin\\phi}, i = 0, 1, ..., K - 1,$"}, {"title": "3.3 Local Occupancy Query", "content": "We further propose a feature query scheme for efficiently fusing the global and local context of the scene, for the sake of occupancy estimation. The target points to be queried $P_q \\in R^{3\\times M}$ are the centers of the voxels in local occupancy region S, where M is the number of the voxels in S. For each queried point"}, {"title": "3.4 Grasp Pose Estimation With Completed Shape", "content": "Having obtained the completed shape around grasp-related regions, the local occupancy-enhanced grasp pose estimation is done in three sequential steps: extracting occupancy-enhanced local shape feature, refining grasp direction and decoding shape feature into grasp poses."}, {"title": "3.5 Loss Function", "content": "Our model is optimized in an end-to-end fashion and supervised by the ground-truth local occupancy labels and grasp pose labels. The loss function is a multi-task loss consisting of local occupancy loss and grasp pose loss. Assume the occupancy prediction in local grasp regions is o, the corresponding occupancy ground truth ogt is generated by cropping the total scene occupancy with the predicted local region, then the occupancy loss is a binary cross-entropy loss $L_o(o, o_{gt})$. The grasp pose loss consists of the affordance segmentation loss $L_a$, view-affordance loss $L_v$, grasp width loss $L_w$ and grasp score loss $L_s$. Following common practice, $L_a$ is a binary cross-entropy loss and the rested losses are smooth-L1 losses. Putting all together, assume gt to be the ground-truth, the total loss is written as:\n$L = L_o(o, o_{gt}) + \\lambda_1L_a(a, a_{gt}) + \\lambda_2L_v(v, v_{gt}) + \\lambda_3(L_w(w, w_{gt}) + L_s(s, s_{gt})),$"}, {"title": "4 Experiments", "content": "Dataset. We evaluate the proposed grasping model on GraspNet-1Billion bench-mark [8]. It is a large-scale real-world grasping dataset containing 190 cluttered grasping scenes and 97,280 RGB-D images captured by 2 kinds of RGB-D cam-eras from 256 different views. 88 objects with dense grasp pose annotations are provided. The test set is divided into 3 levels (seen / similar / novel) according to the familiarity of objects. For the occupancy label, we utilize the Signed Dis-tance Function (SDF) of each object and the object pose annotations to generate scene level occupancy.\nBaselines. We run several state-of-the-art competing methods on GraspNet-1Billion [8] benchmark, including [7, 8, 11, 20, 24, 27, 31, 34, 42, 47]. For fair com-parison, all baseline models only read single-view point clouds as ours.\nMetrics. For grasp pose estimation, we report the grasp AP [8] of the top-50 grasp poses after grasp pose-NMS. The grasp AP is the average of AP\u00b5, where \u00b5 is the friction coefficient ranging from 0.2 to 1.2 and APu is the average of Precision@k for k ranges from 1 to 50. For the local occupancy prediction, we report the F1-Score and the volumetric IOU in the predicted local occupancy re-gion. F1-Score is the harmonic average of the precision and recall and volumetric IOU is the intersection volume over the union volume for occupancy prediction and ground truth.\nImplementation details. As the number of voxels varies across different scenes, for the convenience of mini-batch training, we randomly sample 15,000 voxels for occupancy prediction during training. For the inference period, there is no such restriction about the number of voxels. In addition, the view-wise affordance loss is the averaged of the view-wise affordance before and after grasp direction refinement. The width loss is only calculated when the corresponding grasp score is positive. As for the occupancy ground truth label, it is generated"}, {"title": "5 Conclusion", "content": "In this paper, we propose a local occupancy-enhanced grasp pose estimation method. By completing the missing shape information in the candidate grasp regions from a single view observation, our method boosts the performance of object grasping with the enhanced local shape feature. Besides, to infer local occupancy efficiently and effectively, the multi-group tri-plane is presented to capture long-distance scene context as well as preserving 3D information from diverse aggregation views. Comprehensive experiments on benchmarks and real robotic arm demonstrate that completed shape context is essential to grasp pose estimation in cluttered scenes, and our local occupancy prediction is of significance for promoting the performance of object grasping."}]}