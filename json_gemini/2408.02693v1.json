{"title": "DIFF-PIC: Revolutionizing Particle-In-Cell Simulation for Advancing Nuclear Fusion with Diffusion Models", "authors": ["Chuan Liu", "Chunshu Wu", "Mingkai Chen", "Ang Li", "Chuang Ren", "Ying Nian Wu", "James Chenhao Liang", "Michael Huang", "Dongfang Liu", "Tong Geng"], "abstract": "Sustainable energy is a crucial global challenge, and recent breakthroughs in nuclear fusion ignition underscore the potential of harnessing energy extracted from nuclear fusion in everyday life, thereby drawing significant attention to fusion ignition research, especially Laser-Plasma Interaction (LPI). Unfortunately, the complexity of LPI at ignition scale renders theory-based analysis nearly impossible; instead, it has to rely heavily on Particle-in-Cell (PIC) simulations, which is extremely computationally intensive, making it a major bottleneck in advancing fusion ignition. In response, this work introduces DIFF-PIC, a novel paradigm that leverages conditional diffusion models as a computationally efficient alternative to PIC simulations for generating high-fidelity scientific data. Specifically, we design a distillation paradigm to distill the physical patterns captured by PIC simulations into diffusion models, demonstrating both theoretical and practical feasibility. Moreover, to ensure practical effectiveness, we provide solutions for two critical challenges: \u25cf We develop a physically-informed conditional diffusion model that can learn and generate meaningful embeddings for mathematically continuous physical conditions. This model offers algorithmic generalization and adaptable transferability, effectively capturing the complex relationships between physical conditions and simulation outcomes; and We employ the rectified flow technique to make our model a one-step conditional diffusion model, enhancing its efficiency further while maintaining high fidelity and physical validity. DIFF-PIC establishes a new paradigm for using diffusion models to overcome the", "sections": [{"title": "1 Introduction", "content": "Sustainable energy stands as one of the paramount challenges of our era. The recent successful demonstration of fusion ignition [2] underscores the transformative potential of fusion as a sustainable energy source. In 2023 and 2024, the National Ignition Facility (NIF) achieved groundbreaking milestones, generating 3.4 MJ and 5.2 MJ of fusion energy from input energies of 2.2 MJ, respectively. These substantial energy gains emphasize the necessity for a deeper understanding of the fundamental sciences to enhance ignition efficiency. However, the complex and nonlinear nature of Laser-Plasma Interaction (LPI) in physics poses significant challenges for theory-based analysis, necessitating a heavy reliance on Particle-in-Cell (PIC) simulations [64, 35, 61, 5, 39]. Despite being the preeminent standard for modeling the physics of LPI, PIC simulations are exceedingly computationally intensive, often requiring tens of millions of CPU hours and consuming millions of dollars for high-resolution outputs [22, 16, 7]. The computational overhead of PIC simulations has become a bottleneck in fusion research, hindering progress toward the development of practical fusion power plants. Moreover, it is paradoxical that these extensive simulations, which aim to address environmental energy challenges, exacerbate the carbon footprint. Consequently, there is an urgent need for the development of innovative methodologies capable of producing high-quality data with substantially reduced computational burden.\nOver the years, numerous CPU-GPU optimizations have been developed to maximize computational efficiency. While these efforts are invaluable, they do not sufficiently address the inherent compu-tational overhead. Recent advancements in generative AI, particularly diffusion models, present a novel approach to these challenges. Diffusion models [56, 24, 59] have demonstrated exceptional capabilities in Computer Vision (CV) [58, 49, 25, 50, 43, 52], synthesizing highly complex and high-dimensional data distributions that match real data with high fidelity. This has sparked significant interest in their potential for generating scientific synthetic data, as recent applications of diffusion models in molecular dynamics simulations have demonstrated their promise in this domain [69, 46].\nTheoretically, by distilling the intricate physical patterns captured by PIC simulations into score functions for diffusion models, these models could potentially synthesize complex and high-resolution PIC simulations. In practice, however, the effectiveness of this approach to alleviate the current bottleneck in fusion research remains uncertain. Achieving this goal demands, first of all, that the synthesized data maintain physical soundness, and secondly, that there is an acceleration in simulation speed compared to conventional PIC methods. Consequently, two critical challenges naturally arise:\n\u2022 PIC simulations utilize physical conditions as inputs, which differ significantly from the typical inputs in CV tasks, such as text prompts or specific classes. These physical conditions require continuity, meaning the synthesized results must vary smoothly with changes in input conditions.\nDiffusion models generally demand multiple denoising steps to produce high-fidelity data. In this context, it is essential to consider the speedup that diffusion models can provide over traditional PIC simulations while preserving high fidelity.\nIn light of this perspective, this paper aims to offer the community an alternative to PIC simulations for generating synthetic LPI data and to tackle the two challenges mentioned above. Specifically, we propose an effective and efficient distillation paradigm, titled DIFF-PIC (see Fig. 1), that leverages the entire denoising process of diffusion models to generate any arbitrary snapshot produced by PIC simulations. This paradigm treats each snapshot of the system's evolution as a unique data distribution, thereby enhancing systemic efficiency. To address the first challenge 1, we develop a conditional diffusion model with a physically-informed condition encoder. This encoder allows the conditional diffusion model to learn the relationship between continuous input conditions and generated data, making the physical phenomenon captured by PIC simulation easily distilled to DIFF-PIC. For the second challenge, we employ the rectified flow technique to further optimize the runtime efficiency of the proposed conditional diffusion model, demonstrating orders-of-magnitude speedup compared to PIC simulations.\nIn summary, our work represents the first known effort to tackle the imperative challenges associ-ated with generating high-quality PIC simulation data for LPI using diffusion models. The core contributions of our work include:"}, {"title": "2 Background", "content": "Inertial Confinement Fusion (ICF) is a method of achieving nuclear fusion by using intense energy pulses to compress and heat small fuel pellets [28, 9], typically containing isotopes of hydrogen such as deuterium and tritium. This process unfolds the nuclear fusion reaction as delineated below:\n$2H + 3H \\rightarrow ^4He + ^1n + Energy$.\nGiven the ubiquity of these hydrogen isotopes in the ocean, there is immense potential to harness \"near-infinite\" energy through the study of nuclear fusion. In particular, the goal of the study is to achieve the conditions of temperature and pressure that are sufficient to initiate fusion reactions, and attain a positive net energy gain (i.e., output energy surpassing the input). To optimize the sophisticated initiation of fusion, an advanced understanding of processes is imperative, especially for LPI. For such purposes, PIC is considered a crucial tool to provide theoretical insights into LPI, due to its capability of predicting and interpreting physical phenomena.\nParticle-in-Cell Simulations. The PIC method is a computational technique widely used in the study of plasma physics and fusion energy research [64, 47, 11, 27, 16, 53, 48, 68]. Developed in the mid-20th century, the PIC method has become a cornerstone in the simulation of complex plasma behaviors, enabling researchers to delve into the intricate dynamics of particles and electromagnetic fields [36, 37]. To highlight, PIC is especially useful in LPI studies [29, 5, 60, 32, 4]: LPI involves complex dynamics of electrons and ions. PIC simulations track the trajectories and interactions of these charged particles under the influence of electromagnetic fields, providing insights into both shock wave formation, and heating mechanisms that are essential for ICF.\nIn essence, PIC is an iterative finite element method applied to atomic particles such as electrons and ions. Within each iteration, particles are systematically arranged into discrete cells according to their spatial distribution, with their positions and velocities being updated over infinitesimally small time steps, typically on the scale of femtoseconds (i.e., $10^{-15}$ seconds). In contrast to molecular"}, {"title": "3 DIFF-PIC", "content": "This section introduces DIFF-PIC, a novel physically-informed conditional diffusion model tailored for generating high-fidelity synthetic data for fusion ignition research. As illustrated in Fig. 2, this paradigm conceptualizes each arbitrary snapshot in PIC simulations as a sample from a separate distribution, utilizing the denoising mechanism of diffusion models to predict an arbitrary snapshot $X(t_{as})$ within the system's evolution. Subsequently, we delve into the Physically-Informed Condition Encoder, crafted to tackle the physic condition encoding challenge in this context. Lastly, the Rectified Flow Acceleration technique is employed, significantly accelerating the denoising process and thereby enhancing the efficacy of the proposed conditional diffusion model."}, {"title": "3.1 The Overall Distillation Paradigm", "content": "By extending the power of diffusion models to PIC simulations, we aim to distill the physical phenomena captured by PIC simulations into diffusion models. This distilling process enables the diffusion model to learn and replicate the complex behaviors and patterns observed in PIC simulations, providing a powerful tool for generating high-fidelity synthetic data in nuclear fusion research.\nConcretely, we propose a distillation paradigm as follows: under a specified condition set $\\theta$, using the entire denoising process of diffusion models to predict an arbitrary snapshot $X(t_{as})$ of the system's evolution, each representing a unique data distribution. $t_{as}$ specifies the simulation time step, different from the denoising step $t$. Therefore, our goal is to learn:\n$X(t_{as}) = U(\\epsilon | t_{as}, \\theta).$"}, {"title": "3.2 Physically-Informed Condition Encoder", "content": "PIC simulations employ diverse physical conditions as inputs, which necessitate a seamless and continuous transition in the resulting synthesized data as these input conditions vary. Therefore, the condition encoder plays a crucial role, responsible for transforming domain-specific inputs into embeddings comprehensible by the model. These inputs comprise the simulation conditions $\\theta$ and the target simulation time step $t_{as}$ that the conditional diffusion model aims to generate.\nGiven the extensive potential range of simulation conditions and the limited data available during training, an optimal encoder must excel in both interpolation and extrapolation critical measures of the model's generalizability. Recall that interpolation capability refers to the encoder's proficiency in generating suitable embeddings for new conditions that, although not encountered during training, lie between two observed conditions. Extrapolation capability, conversely, pertains to generating embeddings for conditions that fall outside the range of those observed during training. Both capabilities are indispensable for addressing the LPI problem.\nTo meet the two essentials, we introduce a Physically-Informed Condition Encoder (PICE) to encode the simulation conditions $\\theta$, incorporating two distinct types of encoders. For interpolation, we employ Positional Encoding [65] to learn a continuous representation of input conditions, enabling smooth transitions between observed conditions. To augment extrapolation capability, we enhance the encoder with a polynomial architecture, utilizing transformation functions constructed as a linear combination of polynomial basis functions $f_i(\\theta)$ of varying degrees:\n$P(\\theta) = \\sum_{i=0}^n f_i(\\theta),$\nwhere $n$ denotes the maximum degree, and $P(\\theta)$ can be chosen as Chebyshev polynomials, or Legen-dre polynomials, based on the characteristics of the condition space. This polynomial enhancement"}, {"title": "3.3 Rectified Flow-Based Acceleration", "content": "To enable rapid generation of high-fidelity synthetic data, we employ the Rectified Flow Acceleration (RFA) technique in model optimization. RFA leverages the principles of rectified flow [40, 18, 41] to convert the complex data trajectory from initial noise $\\epsilon$ to the target snapshot $X(t_{as})$ into a single streamlined denoising step, specifically aiming to distill the typically winding diffusion path into a direct and straight path the shortest route between two distributions. During training, RFA minimizes the following objective function with straight ordinary differential equations:\n$\\underset{\\zeta}{\\text{arg min }} \\mathbb{E} \\bigg[\\int_{0}^{1} ||(X(t_{as}) - \\epsilon) - \\zeta (X_t, t | t_{as}, \\theta) ||^2 dt \\bigg]$\nwhere $X_t = tX(t_{as}) + (1 - t)\\epsilon$ denotes the linear interpolation between $X(t_{as})$ and $\\epsilon$ across the diffusion timeline, with $t$ ranging from 0 to 1. The score-based model $\\zeta$, approximated using the U-Net, defines the learned trajectory. By minimizing the expectation of the squared deviations between the straight path $X(t_{as}) - \\epsilon$ and the learned trajectory $\\zeta (X_t, t | t_{as}, \\theta)$, RFA promotes the adoption of the shortest and most direct path in the transformation process, thus significantly reducing the denoising time. Once this time-dependent score-based model $\\zeta$ is trained, we further straighten the learned trajectories through an interactive reflow procedure [41]. In summary, the RFA module provides additional benefits for our paradigm:\n\u2022 Streamlined Denoising Process. RFA significantly accelerates the denoising process (see Table 4) by converting the complex data trajectory from initial noise to the target snapshot into a direct denoising step. By distilling the typically winding diffusion path into the shortest route between two distributions, RFA greatly reduces the time required for generating high-fidelity synthetic data.\n\u2022 Robust Optimization. RFA leverages the principles of rectified flow to minimize deviations between the winding data trajectory and the optimal, shortest path. This direct path approach reduces the possibility of error accumulation that can occur with more winding, iterative methods."}, {"title": "4 Evaluation", "content": "Datasets. We provide a novel dataset comprising 6,615 simulations across varied physical conditions, each containing 80 snapshots of electric fields along two orthogonal directions denoted E1 and E2. The data were generated by OSIRIS [19], a well-established PIC simulation software suite. The dataset covers diverse conditions, including Electron Temperature (Te), Ion Temperature (Ti), and Laser Intensity (Li), all of which are critical parameters influencing particle dynamics and the resultant electric fields. To foster further advancements in fusion research, we will release the dataset publicly upon acceptance."}, {"title": "4.3 Discussion", "content": "To provide further insights and highlight the value of this work, it is worth noting that our approach exhibits outstanding scalability compared to traditional PIC simulations. In PIC, for N particles, the computing complexity can reach a formidable $N^2$, as each particle interacts with every other particle. On the contrary, DIFF-PIC focuses on the macroscopic data (e.g., electric field) in the form of distribution, therefore it is not sensitive to the number of particles in space. For larger particle spaces, the speedup achieved by DIFF-PIC can be easily improved by extra orders of magnitude, further accelerating the research of fusion, or other research areas involving large-scale PIC simulations."}, {"title": "5 Related Work", "content": "Particle-in-Cell Simulations have long been fundamental to modeling physical processes in fusion research [63, 20]. However, the computational intensity of PIC simulations presents significant chal-lenges [66]. To mitigate these computational constraints, various methods have explored GPU and hybrid CPU-GPU acceleration technologies. Studies such as [1, 12, 15, 33, 62] have utilized parallel computing, high memory bandwidth, and multiple processors to expedite simulations. Architecturally, the simulator optimized for the Kepler GPU architecture, as discussed in [54], underscores the poten-tial of specific GPU architectures to enhance simulation efficiency. For more intricate simulations, research efforts like [70, 13] have developed hybrid CPU-GPU implementations, [67] introduced a hybrid approach for multi-core and multi-GPU systems, highlighting the continuous integration and evolution of these technologies in advancing PIC simulations. Despite these advancements, these approaches remain reliant on the fundamental PIC framework, which may not completely address the computational burden due to the inherent algorithmic complexity of the PIC method. In recent years, rapid advancements in deep learning have opened new pathways for accelerating scientific"}, {"title": "6 Conclusions", "content": "This paper presents DIFF-PIC, a pioneering approach that leverages the capabilities of diffusion models to generate high-fidelity synthetic data for LPI, offering a computationally efficient alternative to conventional PIC simulations in fusion ignition research. By integrating a Physically-Informed Condition Encoder and applying the Rectified Flow Acceleration, DIFF-PIC significantly augments the diffusion model's capacity to manage diverse experimental conditions, thereby expediting the generation of high-fidelity synthetic data. These advancements facilitate rapid, resource-efficient exploration of the design space, markedly diminishing the computational demands associated with PIC simulations. Our research not only catalyzes accelerated scientific discoveries within the realm of fusion research but also sets a novel precedent for the application of generative AI models in scientific simulations. Future investigations may focus on optimizing the distillation paradigm, harmonizing the simulation time $t_{as}$ with the diffusion time $t$, and refining the condition encoder to encompass a broader spectrum of physical conditions."}, {"title": "S1 Implementation Details", "content": "Experimental Configurations. The comprehensive schematic of DIFF-PIC is depicted in Fig.2. The architectural foundation of our model is predicated on the U-Net framework [51], incorporating a series of three down-sampling blocks followed by three up-sampling blocks. The training regimen of our model encompasses an extensive 600 epochs, employing a batch size of 64, a configuration empirically validated to secure model convergence. Furthermore, the training protocol adheres to a fixed learning rate of 0.0005, optimized via the Adam optimizer[30]. The optimization objective is formulated as delineated in Eq. 7.\nPhysically-Informed Condition Encoder. We employ a quartet of distinct condition encoders, each tailored to a specific type of condition. Apart from the encoder addressing the designated snapshot $t_{as}$, the remaining three encoders share an identical architectural framework, albeit undergoing separate training processes. Specifically, the positional encoding mechanism yields a 16-dimensional embedding from a single input. The polynomial encoder operates with a maximum polynomial degree of 4. Subsequently, a single-layer MLP module processes the concatenation of outputs from both the positional and polynomial encoders to further refine the learned embedding, preserving its dimensionality. Notably, the architecture for the snapshot encoder mirrors this design, excluding the polynomial encoder component.\nRectified Flow-based Acceleration. We adhere to the theoretical framework proposed in [40] to refine the denoising trajectory (see Eq. 7) of DIFF-PIC. Additionally, we further streamline the learned trajectories by applying the reflow technique, thereby transforming our model into a one-step conditional diffusion model."}, {"title": "S2 Ablation Study", "content": "This section provides an in-depth analysis of the systemic design of DIFF-PIC through an ablation study. By systematically removing or altering components of the model, we aim to isolate and understand the contributions of each part to the overall performance.\nPhysically-Informed Condition Encoder. This study provides a comprehensive evaluation of the efficacy of our Physically-Informed Condition Encoder by juxtaposing it with a baseline model. The baseline model employs a straightforward encoder consisting of a naive MLP layer, augmented by Rectified Flow-based Acceleration. The performance metrics for this baseline model are presented in Table Sla, showcasing FID, SWD, and MMD scores of 1.87, 46.8, and 7.21e-4, respectively. To examine the impact of the encoder architecture on model performance, we first replaced the naive"}, {"title": "S3 Social Impacts & Limitation", "content": "The advent of DIFF-PIC heralds a substantial breakthrough in the synthesis of PIC simulations with diffusion model paradigms, significantly augmenting predictive capabilities in the realm of ICF. This pioneering approach has demonstrated commendable performance metrics. From a societal vantage point, the ramifications of DIFF-PIC are profoundly advantageous, as our approach furnishes a pivotal instrument for advancing our comprehension and proficiency in exploiting fusion energy an endeavor that holds promise as a cornerstone for long-term sustainable energy solutions.\nNonetheless, it is crucial to recognize and meticulously evaluate the potential negative impact and limitations inherent in this technology. Analogous to other generative models, DIFF-PIC encounters challenges when addressing corner cases, which may not be adequately represented in the training data. This limitation accentuates the necessity for continuous research and refinement, particularly in its application to real-world ICF scenarios where unforeseen behaviors may manifest. Therefore, despite"}, {"title": "S4 Ethical Safeguards", "content": "In our study, which introduces a novel dataset, we will implement comprehensive ethical safeguards to mitigate potential misuse and ensure responsible utilization, as detailed in the protocols included in the final release of models and datasets. These protocols encompass strict usage guidelines, access restrictions, the incorporation of safety filters, and monitoring mechanisms. We perform thorough risk assessments to identify potential misuse scenarios and develop tailored mitigation strategies, such as robust data governance frameworks. While not all research may necessitate stringent safeguards, we adhere to best practices, promoting ethical awareness and encouraging researchers to consider the broader impacts of their work. Additionally, we maintain detailed documentation for transparency and accountability for the data we release. These efforts underscore our commitment to upholding the highest standards of conduct in scientific inquiry, aiming to protect the interests of involved parties."}, {"title": "S5 Reproducibility", "content": "DIFF-PIC is implemented using PyTorch [45]. PIC simulations are executed on the Perlmutter supercomputer, located at the National Energy Research Scientific Computing (NERSC) facility, utilizing AMD EPYC 7763 CPUs. The experimental procedures for DIFF-PIC are conducted on an Nvidia RTX 4090 GPU in conjunction with an Intel 13th Gen i9-13900KF CPU. To ensure reproducibility, the complete implementation will be made publicly available upon acceptance."}, {"title": "S6 Licenses for existing assets", "content": "Most of the methods utilized for comparison in our study are reproduced by ourselves. The PIC Simulation is implemented based on the publicly available version at OSIRIS, and is distributed under the AGPL-3.0 license."}]}