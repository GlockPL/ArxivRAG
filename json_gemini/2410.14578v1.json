{"title": "Large Language Models Are Overparameterized Text Encoders", "authors": ["Thennal D K", "Tim Fischer", "Chris Biemann"], "abstract": "Large language models (LLMs) demonstrate strong performance as text embedding models when finetuned with supervised contrastive training. However, their large size balloons inference time and memory requirements. In this paper, we show that by pruning the last p% layers of an LLM before supervised training for only 1000 steps, we can achieve a proportional reduction in memory and inference time. We evaluate four different state-of-the-art LLMs on text embedding tasks and find that our method can prune up to 30% of layers with negligible impact on performance and up to 80% with only a modest drop. With only three lines of code, our method is easily implemented in any pipeline for transforming LLMs to text encoders. We also propose L\u00b3Prune, a novel layer-pruning strategy based on the model's initial loss that provides two optimal pruning configurations: a large variant with negligible performance loss and a small variant for resource-constrained settings. On average, the large variant prunes 21% of the parameters with a -0.3 performance drop, and the small variant only suffers from a -5.1 decrease while pruning 74% of the model. We consider these results strong evidence that LLMs are overparameterized for text embedding tasks, and can be easily pruned.", "sections": [{"title": "1 Introduction", "content": "In the past few years, the field of natural language processing (NLP) has seen a significant shift towards large-scale language models (LLMs). These models, due to a combination of their large size, extensive pre-training, and instruction-following ability, have achieved state-of-the-art performance on a wide range of NLP tasks, such as language modeling, text generation, and text understanding.\nDespite their strong generative capabilities, decoder-only LLMs have seen comparatively little adoption for text embedding tasks until recently. Text embedding, which involves mapping a text sequence of varying length to a fixed-dimensional vector representation, is a fundamental task in NLP and is used as a building block for a wide range of downstream tasks, such as semantic textual similarity, information retrieval, and text classification. Further, it is a fundamental step required for retrieval-augmented generation, a well-known paradigm for improving the performance of LLMs in knowledge-intensive tasks. Traditionally, text embedding models have been based on masked language models (MLMs) and bidirectional encoders, such as BERT and T5, typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly- and fully-supervised contrastive training.\nDecoder-only LLMs, however, offer several advantages over their encoder-only counterparts. They are more sample-efficient during pre-training, leverage instruction-following capabilities for task generalization, and benefit from a rich and evolving research ecosystem. Further, the availability of high-performing public pre-trained LLMs and their continual development make it appealing to explore their use for text embedding tasks. To this end, several studies have experimented with various pipelines, training methods, and architectural modifications, effectively converting LLMs into state-of-the-art text embedding models with small amounts of supervised contrastive training.\nOn the other hand, the increasingly large size of LLMs, with parameters ranging up to 540B , stands in stark contrast to traditional small bidirectional encoders of sizes almost universally less than 1B parameters. Even the smallest LLMs in use typically have 3-8B parameters. Consequently, inference with LLM-based text encoders is far more demanding in terms of compute and memory requirements in comparison to traditional methods.\nTherefore, there are a variety of post-training techniques for reducing the cost of LLMs, such as pruning, quantization, and distillation. In particular, the recent work of Gromov et al. (2024) has shown that LLMs can be pruned to up to half their size with minimal impact on downstream performance (i.e. question answering) by dropping the last half of the model's layers, with the exception of the final layer, and applying a small amount of parameter-efficient finetuning. Layer-dropping as a pruning strategy has particular benefits: it is straightforward to implement, with memory and inference time decreasing linearly with the number of layers dropped, and it can be combined with other efficiency methods such as quantization.\nIn this work, we build on these findings and apply them in the context of text embedding, resulting in a easy-to-use and efficient approach to transform any pre-trained decoder-only LLM into a much smaller text embedding model. By simply pruning the last n% layers of a model before supervised contrastive training, we reduce the final model size with a proportional decrease in memory and inference time. We experiment with four different decoder-only LLMs ranging from 3.8B to 8B parameters with a variety of pruning percentages and show that up to 30% of a model's layers may be pruned with almost no impact in performance, and may even increase it. Even intensive pruning on the order of 80% still provide reasonably effective text embedding models, with a drop in performance on the downstream task from 64.9 to 59.8 for our highest-performing model.\nFurther, we propose L\u00b3Prune, a simple and novel method that pinpoints particular layers to prune to based on the initial without requiring significant testing or experimentation. With no input, our method produces both a) a lightly-pruned model, 69-89% of the original size with minimal performance loss of -0.2 on average and even a performance improvement in one model, and b) a heavily pruned model, 16-36% of their original size with a modest performance drop of -4.4 to -6.9.\nWe note that our pruning technique is both orthogonal to other commonly used pruning and quantization methods and to the specifics of the methodology used to finetune the LLM for text embedding. As such, it can be seamlessly integrated with other efficiency techniques and finetuning methods to further optimize model size and performance.\nOur contributions can be summarized as follows:\n\u2022 We are the first to apply pruning in a text embedding setting, formulating a simple procedure that can be easily applied to pipelines converting an LLM to a text encoder.\n\u2022 We demonstrate that LLMs can be pruned by up to 30% with negligible impact on the quality of representations and up to 80% with a modest performance drop.\n\u2022 We propose and evaluate L\u00b3Prune, a novel method that identifies layers to prune by leveraging the model's initial loss, thus minimizing the need for trial-and-error for effective pruning.\nOverall, our work demonstrates that decoder-only LLMs are generally overparameterized for text embedding tasks, and that significant reductions in model size can be achieved with minimal impact on performance. We release the full code for L\u00b3Prune."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Encoder-only Text Embedding Models", "content": "BERT-based models have largely dominated the field of text representation in the past, relying on supervised training with natural language inference or sentence similarity to produce high-quality sentence embeddings. Recent methods have further improved these representations through large-scale contrastive pretraining followed by multi-task finetuning. These methods generally require a complex multi-stage training pipeline that demands substantial engineering effort, along with large-scale compute-intensive pretraining."}, {"title": "2.2 Decoder-only Text Embedding Models", "content": "A variety of recent works have explored leveraging LLMs and their capabilities to generate high-quality text representations. Generally, a combination of (a) a pooling method, (b) architectural modifications, and (c) supervised or unsupervised fine-tuning is used to effectively convert LLMs to text embedding models.\nThe majority of prior work consider two straightforward pooling strategies to extract embeddings for a sequence of tokens: mean pooling and last-token pooling.\nMean pooling is more effective with bidirectional embedding models while last-token pooling is generally preferred when working with causal attention. Muennighoff (2022) introduces weighted mean pooling, assigning a higher weight to later tokens to offset the autoregressive nature of decoder-only LLMs, with significant success. Lee et al. (2024) utilizes a trainable latent attention layer as a pooling technique and obtains consistent improvement.\nSeveral studies identify the causal attention mechanism of decoder-only LLMs as a obstacle in obtaining performant representations, and suggest modifications to the architecture to compensate. Li and Li (2024) and replace the causal attention mechanism with bidirectional attention. Muennighoff et al. (2024) utilizes a hybrid objective with both bidirectional representation learning and causal generative training. Lee et al. (2024) finds that simply removing the causal attention mask works compellingly well.\nFinally, both supervised and unsupervised finetuning have been extensively explored to significantly improve the performance of decoder-only LLMs in representational tasks, with supervised training consistently producing the best results. Several modifications to the training pipeline have been proposed, such as an additional masked token prediction training step , or a two-stage instruction-tuning setup. The zero-shot setting has also been studied with limited success by Springer et al. (2024) and Jiang et al. (2023b)."}, {"title": "2.3 LLM Pruning", "content": "Pruning as a method of size reduction has a long history in the field of deep learning. Classic pruning techniques sparsify networks by removing individual parameters based on various criteria. While these models were smaller, these techniques generally lead to irregular sparsification patterns that require specialized hardware or libraries to fully utilize. Structured pruning techniques were developed to remove irrelevant groups of parameters together, such as particular channels or filters in convolutional neural networks.\nRecent work has focused on applying structure pruning methods to transformers. Almost every possible component of the model architecture is studied as candidates for removal, most prominently methods that drop attention heads and layers. Prior literature on layer pruning generally consider BERT-like models , with recent studies shifting focus to decoder-only LLMs. \nSajjad et al. (2023) finds that for BERT-like models, dropping the last layers is the best layer pruning strategy. Gromov et al. (2024) extends this research to decoder-only LLMs, and presents a layer pruning strategy, pruning a block of layers based on angular distance between layer representations. Their results indicate that the last layer in particular is essential for maintaining performance. Informed by this finding, they propose a simpler strategy: dropping the last n layers with the exception of the final layer. They conclude that simply dropping the last layers works effectively to prune the model, with a caveat: after dropping the layers, it is required to \"heal\" the model via finetuning with QLORA for 1000 steps.\nWhile these results suggest that the last layer in particular is essential when pruning LLMs for text generation, this is not necessarily the case when utilizing the LLM for other tasks. To this end, Fan et al. (2024) finds that for \"simpler\" tasks such as sentiment analysis, early stopping\u2014stopping the inference after a certain number of layers\u2014is an effective strategy to significantly reduce inference time with minimal impact on performance. The"}, {"title": "3 Pruning", "content": "We borrow the intuition from Gromov et al. (2024), that the representations in a transformer can be thought of as a slowly changing function of the layer index. Specifically, the representation can be formulated as the following iterative residual equation:\n$x(l+1) = x(l) + f(x(l), \u03b8(l)),$ (1)\nwhere $x(l)$, $\u03b8(l)$, respectively, are the multi-dimensional input and parameter vectors for layer l, and f(x, \u03b8) describes the transformation of one multi-head self-attention and MLP layer block.\nThe authors assert that these representations converge to a slowly changing function:\n$x(l) \u2248 x(l-1) + \u03b5$ (2)\nwith e < xl as an approximation. They verify this hypothesis experimentally by calculating the distance between layer representations and using them for a pruning algorithm. Their findings indicate that the earlier layers have a significantly larger impact on the representation compared to the later layers, with a particular caveat: the final layer also modifies the representation significantly. They thus propose and verify a simpler pruning strategy, where the last n layers of the model, excluding the final layer, are dropped. This method non-optionally requires a \"healing\" step, recovering the downstream performance with a small number of QLORA finetuning steps.\nOur hypothesis extends theirs, and posits that for the task of generating text embeddings, the final layer is also not necessary. Our pruning experiments are conducted with the percentage pruned p, between 0% (all layers intact) and 100% (all layers removed). Given a pruning percentage and a total number of layers n, the new number of layers $n^*$ is calculated as\n$n^* = [n \u00d7 (1-p)]$\nGiven a model and its configuration, this straightforward procedure can be integrated with modern LLM implementations with just three lines of code:\nWe then conduct supervised contrastive training, as with prior work on converting LLMs to text encoders. In lieu of an explicit healing step, we hypothesize that the aforementioned training acts as such. Thus, no additional or separate training is necessary to execute our method."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 General Setup", "content": "For our experiments, we chose four instruct-tuned decoder-only LLMs across different families ranging from 3.7B to 7.5B:\nThese model families were chosen due to their widespread use in open-source communities and LLM literature. As we are chiefly concerned with the effects of pruning, we opt for no modification to the LLM architecture itself. We use weighted mean pooling to generate embeddings from the outputs of the LLM as it is straightforward to implement and outperforms other pooling measures when paired with causal attention. \nWe also conduct supervised contrastive finetuning, known to outperform unsupervised finetuning and the zero-shot setting, and considered to be an integral part of effectively utilizing LLMs as embedding models. We use the replication of the public portion of the E5 dataset, curated by Springer et al. (2024), as the training dataset. Consisting of approximately 1.5 million samples, it is a multilingual compilation of various retrieval datasets, meant for supervised contrastive training of embedding models. In accordance, we use contrastive loss with hard negatives and in-batch negatives.\nFurther details on the dataset and training are provided in Appendix A.\nAll experiments were conducted on a single A100 GPU, reinforcing the accessible nature of our proposed procedure."}, {"title": "4.2 Zero-shot Loss Evolution Over Layers", "content": "As a preliminary test of our hypothesis\u2014that an LLM can form performant text representations even before reaching the final layer-we first calculate how well the output of each layer of the model performs as an embedding. We note that this is equivalent to a zero-shot setting. As we are interested in a comparative measure between layers intra-model, the loss as a metric is sufficient. We take a random sampling of 1280 tuples from the training dataset and calculate the embeddings via weighted mean pooling of the outputs of each layer. Then, the loss is calculated and averaged per layer. We find that the loss values converge fairly quickly, and so 1280 samples are sufficient for our purposes.\nThe loss for all four models follow a similar curve: an initial drop to around layer 5-10, a subsequent rise around layer 15, and then a slower drop up to layer 22-25, where it rises again by the end with layer 28-32. While the specifics of the ways in which LLM representations evolve are not well-understood, these results suggest that the early layers of the model are generally focused on representation, while the final layers transform the representation into the specific probability distribution for the next token. Regardless of the underlying dynamics, the drop-rise-drop curve is consistent across model sizes and families in our experiments. We expect that training will transform the shape of this layerwise evolution considerably. We also have little reason to expect that the final downstream performance of layer-dropped models will be accurately modeled by the effectiveness of these initial representations. However, we posit that"}, {"title": "4.3 Supervised Training", "content": "In order to verify the general efficacy of our hypothesis-that LLMs can form effective text representations even before reaching their deeper layers-we conduct training on pruned LLMs to convert them into effective text encoders. We keep the training procedure fairly straightforward: supervised contrastive learning for 1000 steps with LoRA modules. Other hyperparameters are detailed in Appendix A.2. We first test a range of pruning percentages from 10% to 90% at 10% intervals. Figure 2 shows the training loss for all models and pruning percentages. We note that the training loss curves all generally follow the same shape, indicating stability in training even with the modified architecture.\nFigure 3 shows the final loss in relation to the pruned model parameters, with each marked point representing a model pruned by an additional 10%. The final loss values for each model follow a straightforward trajectory with increasing pruning percentage: minimal increases up to 30-40%, with larger increases as the pruning percentage hits 90%. Notably, we find that the final loss of different models correlates more with the final parameter count after pruning than with the percentage of layers retained. This suggests that the parameter count is a more significant factor in determining the effectiveness of a pruned model than simply the proportion of layers kept.\nIf we presume that training loss correlates well with downstream accuracy for text embedding, we can make a series of predictions from an analysis of the plots:\n\u2022 Performance always degrades sharply as the parameter count approaches and goes below 1 billion.\n\u2022 In contrast, performance degrades little even with 30-50% pruning. LLaMA-3-8B degrades minimally up to 40-50%, Mistral-7B up to 30-40%, and Phi3-4B up to 20-30%. Qwen2-7B"}, {"title": "4.4 Simple Pruning Evaluation", "content": "To validate the predictions made from the training loss, we evaluate the models at various pruning percentages on downstream text embedding tasks. Specifically, to speed up evaluation, we opt for the 15-task subset of the Massive Text Embedding Benchmark (MTEB, Muennighoff et al., 2023) collected and used by BehnamGhader et al. (2024). The subset, which we term MTEB-15 for clarity, covers representative tasks from the full 56 tasks in MTEB, including tasks from each category with almost the same proportion to prevent bias. Further details are provided in Appendix B.1.\nIn accordance with previous work , we evaluate with task-specific instructions. We use the same instructions as Wang et al. (2024). The instructions can be found in Appendix Table 4. Following BehnamGhader et al. (2024), for symmetric tasks, the same instruction is used for the query and the document. Instruction tokens are excluded from the final pooling.\nFigure 5 shows the impact of pruning on MTEB-15 results across a range of pruning percentages. We plot with respect to the number of parameters as opposed to relative pruning percentages because parameter count correlates better with the score.\nWe can see that the training loss and MTEB-15 score also roughly correlate. This confirms that our predictions in Section 4.3, based on the supervised training loss, are fairly accurate.\nLLama at 50% pruning (3.77B) is only degraded by -1.89, still providing a strong performance of 63.10. Even at 80% pruning (1.41B), it performs at a reasonable 59.69. Mistral's performance decrease is an almost negligible -0.08 up to 30% (4.91B). Qwen's performance increases by +0.32 with a pruning of 10%. It drops distinctly at 30% pruning. However, it stabilizes at a reasonable 61.51 up till 60% (2.79B). Phi degrades negligibly up to 20% pruning (2.91B) with -0.03, and -0.53 at 30% (2.56B). Higher pruning percentages degrade it significantly, as the model parameter count decreases below the 2 billion mark.\nOur results correspond roughly with those of Gromov et al. (2024): sharp transitions in performance around 45%-55% for models in the Llama family, 35% for Mistral, 25% for Phi, and 20% for Qwen. However, instead of a sharp transition to near-random performance, we observe a steady but reasonable decline even at higher pruning percentages. In general, we only observe a significant decline in performance as model size goes below roughly 2 billion parameters. These results also correlate roughly with previous findings by Jiang et al. (2023b), who investigated LLM-based sentence embedding models between 125M to 66B parameters and found diminishing returns at parameter counts over 2B.\nWe can derive some general insights from these experiments. For one, the resilience of a model to pruning is not entirely consistent across families and sizes. Thus, model-specific experimentation may be required. However, in general, models can be pruned 10-30% with minimal drop in downstream performance. Further, higher pruning percentages up to 80% still yield reasonably effective embedding models.\nWe note that LLaMa-3-8B at 50% pruning, with 3.77B parameters, outperforms an unpruned Phi3-4B at 3.73B parameters. In conjunction with our other results, we suggest that given a compute/memory budget, simply dropping layers of a high-performing LLM may be a superior and significantly simpler strategy than training a smaller LM that fits the budget."}, {"title": "4.5 L\u00b3Prune Evaluation", "content": "As mentioned in Section 4.2, we hypothesize that the minima in the layer-loss curve before and after the midpoint are particularly effective points for pruning. We prune to those particular layers and conduct the same training and evaluation as described in Sections 4.3 and 4.4. Table 1 aggregates the results across base models for the two resulting prune configurations, termed small and large. It also shows the particular layer numbers and parameter counts.\nThe results are consistent with our previous findings. The small models generally perform worse than the full-sized models, with performance drops ranging between -4.4 and -6.9. However, at 16%-36% of their original size (84%-64% pruning), the models are proportionally compute and memory-efficient in exchange for the dropped performance. The large models, on the other hand, perform almost as well as the unpruned models, with only a slight drop in performance, while pruned to 69%-89% (31%-11% pruning). As we have seen before, Qwen2-7B's performance increases slightly with pruning, and both Mistral-7B and Phi3-4B's performance drops are negligible. LLaMA-3-8B's performance drops by -1.4 points but still remains a fairly strong 63.5.\nCombined with the results from Section 4.4, we can see that the layers picked by L\u00b3Prune are generally optimal. For instance, Mistral-7B, Qwen2-7B, and Phi3-4B show strong performances up to 30%, 10%, and 20% pruning respectively, and the layers corresponding to those pruning percentages are exactly the layers pinpointed by L\u00b3Prune for the large variant. As LLaMa-3-8B's performance decrease remains fairly consistent when pruning below 50%, we infer that there is no particularly optimal point for pruning. Similarly, the small variants are pruned up to the point before each model's performance drops drastically roughly 85% for LLAMA-3-8B, 75% for Mistral-7B, 65% for Qwen2-7B, and 75% for Phi3-4B.\nBased on these results, we can conclude that the layerwise loss evolution of a model can be used to effectively pick optimal points for pruning. The resulting variants can be used to provide a range of models with different performance and efficiency trade-offs. The large models are particularly effective, with a negligible drop (or even an increase) in performance for a significant reduction in size. The small models can be used for resource-constrained settings, with reasonable performance.\nWe further note that the training of the small variants required only 23.6 GB of VRAM at maximum, and the layerwise loss curves can be calculated with less than 17 GB of VRAM. The training is only conducted for 1000 steps and takes less than an hour on average using an A100 GPU. Thus, small variant models can be trained on consumer-grade GPUs, making it accessible to open-source and practitioner communities. Further details on training times are given in Appendix A.3."}, {"title": "5 Conclusion", "content": "In this work, we presented a simple and effective pruning approach to convert LLMs into lightweight, performant text embedding models. By dropping the last p% layers of the model, we achieved significant reductions in model size and inference time, with minimal impact on text embedding tasks. Our procedure is straightforward to implement in pipelines converting LLMs to text encoders, and requires no additional training, providing smaller models at no cost. Based on the initial model loss, we also proposed L\u00b3Prune, a method to pinpoint optimal layers to prune to, providing an efficient strategy for pruning without extensive experimentation. We demonstrated that significant pruning-up to 31%-can be conducted with a negligible performance loss, and substantial pruning-up to 84%-can still produce effective models. Overall, our results show that LLMs are significantly overparameterized for text embedding tasks, and can be pruned with minimal performance loss."}, {"title": "6 Limitations", "content": "Our work only takes into consideration the supervised finetuning setting for utilizing LLMs as text encoders, as this is the most common and generally effective. Further, our results may not hold with extensive modifications to the architecture or training process. Lastly, even with extensive pruning, our smallest models are still generally larger than traditionally trained encoder-only models. However, the inexpensive finetuning required and the performance advantages offered by LLM-based text encoders motivates pruning as a viable alternative in resource-constrained settings."}, {"title": "7 Ethical Considerations", "content": "Our work provides an effective and efficient method to produce optimized text embedding models from LLMs. As we mentioned in Section 4.5, our method is memory and compute-efficient, and can be conducted on consumer-grade GPUs, making it accessible for a wider audience of practitioners and academics. However, this also enhances potential issues with misuse, lowering the bar for malicious actors to train and host embedding models. Regardless, embedding models in general have significantly fewer avenues for malicious behaviour in comparison to e.g. generative LLMs."}, {"title": "A Training", "content": ""}, {"title": "A.1 Dataset", "content": "The dataset we use consists of ELI5, HotpotQA, FEVER, MIRACL"}, {"title": "A.2 Hyperparameters", "content": "All models are trained with LORA rank r = 16 and use brain floating point (bfloat16) precision, gradient checkpointing, and FlashAttention-2 to optimize GPU memory consumption. Training is conducted with a batch size of 64 for 1000 steps, with gradient accumulation over 1 step, and a maximum sequence length of 512 tokens. The Adam optimizer is employed with a learning rate of 2 \u00d7 10-4 and a linear warm-up over the first 300 steps."}, {"title": "A.3 Training Time", "content": ""}, {"title": "B Massive Text Embeddings Benchmark (MTEB)", "content": ""}, {"title": "B.1 MTEB subset details", "content": "MTEB encompasses a diverse array of embedding tasks varying in size, making a full evaluation quite time-consuming\u2014it takes over 160 hours for a full-sized 7B model, such as Qwen2-7B, on an A100 GPU. To expedite our analysis, we use a representative subset of 15 tasks from MTEB, selected and used by BehnamGhader et al. (2024), detailed in Table 3. This subset includes tasks from each category in proportions closely matching those of the full MTEB."}, {"title": "B.2 MTEB instructions", "content": "For evaluation on MTEB-15, we use the instructions from Wang et al. (2024), also used by BehnamGhader et al. (2024). The list of instructions for each task is listed in Table 4."}, {"title": "C Licenses", "content": "All four models we used are available for research purposes-LLaMA-3-8B is under its own permissive license, Mistral-7B and Qwen2-7B are under Apache License 2.0, and Phi3-4B is under MIT License. MTEB and the tasks it includes are provided under the Apache License 2.0. We overview the licenses of all datasets used in training below:\n\u2022 ELI5: Provided under no specified license, available for research purposes.\n\u2022 HotpotQA: Provided under CC BY-SA 4.0.\n\u2022 FEVER: Provided under CC BY-SA 3.0.\n\u2022 MIRACL: Provided under Apache License 2.0.\n\u2022 MS-MARCO: Provided under no specific license, available for non-commercial research purposes.\n\u2022 Natural Questions (NQ): Provided under CC BY 4.0.\n\u2022 Stanford Natural Language Inference (SNLI): Provided under CC BY-SA 4.0.\n\u2022 Multi Natural Language Inference (MNLI): Provided under a combination of permissive licenses, elaborated by Williams et al. (2018).\n\u2022 SQUAD: Provided under CC BY-NC 4.0.\n\u2022 TriviaQA: Provided under Apache License 2.0.\n\u2022 Quora Duplicate Questions: Provided under no specified license, available for non-commercial purposes.\n\u2022 Mr. TyDi: Provided under Apache License 2.0\n\u2022 DuReader: Provided under Apache License 2.0\n\u2022 T2Ranking: Provided under Apache License 2.0"}]}