{"title": "On a measure of intelligence", "authors": ["Yuri Gurevich"], "abstract": "The Fall 2024 Logic in Computer Science column of the Bulletin of EATCS is a little discussion on intelligence, measuring intelligence, and related issues, provoked by a fascinating must-read article \"On the measure of intelligence\" by Fran\u00e7ois Chollet. The discussion includes a modicum of critique of the article.", "sections": [{"title": "Cybernetics vs. AI, and podcasts vs. reading", "content": "Quisani2 (walking in): What are you reading?\nAuthor: An article \u201cOn the measure of intelligence\" by Fran\u00e7ois Chollet [3].\nQ: Is it about psychology?\nA: It is mostly about AI. Chollet is a prominent figure in AI.\nQ: We spoke about AI last spring. But you didn't seem to be interested in AI before that.\nA: This is largely correct, though I read Norbert Wiener's \u201cCybernetics\u201d [18], when it was translated to Russian in 1968, and was taken with it. For a while I tried to follow cybernetics developments, at least in the USSR.\nQ: What's cybernetics?\nA: Wiener gives a concise definition in the subtitle of that book of his: \"Control and communication in the animal and the machine.\"\nQ: How is this different from AI?\nA: This is a good question. Here is an explanation by Michael Jordan, UC Berkeley Professor, not the basketball player:\nIt was John McCarthy (while a professor at Dartmouth, and soon to take a position at MIT) who coined the term AI, apparently to distinguish his budding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined \u201ccybernetics\u201d to refer to his own vision of intelligent systems a vision that was closely tied to operations research, statistics, pattern recognition, information theory, and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener's intellectual agenda that has come to dominate in the current era, under the banner of McCarthy's terminology [8].\nIn his conversation with Lex Fridman, Jordan tells this story in more colorful terms [9, 18:06].\nIn the USSR of my time (I left \u201cthe land of victorious socialism\u201d in 1973) and for a long time after, the term cybernetics was used for the field.\nQ: Do you spend much time reading stuff outside your immediate research interests?\nA: I do, though these days I spend more time listening to podcasts.\nQ: Why?\nA: You can walk and listen to a podcast; you can't walk and read.\nQ: I can't but some do, even as they cross the road. But I digress.\nA: A podcast often provokes reading. Something attracts your attention, and you want to know more about it. That is exactly what happened in this case. First I heard Fran\u00e7ois Chollet on a podcast [4]."}, {"title": "The g-factor", "content": "Q: Is Chollet's article about IQ scores? I have never heard of any other measure of intelligence.\nA: Chollet mentions IQ scores and the g-factor, but his 64-page article goes far beyond that.\nQ: What's the g-factor?\nA: British psychologist Charles Spearman noticed positive correlations when a person performs different tests. He suggested that general intelligence (g-factor) is a single underlying ability influencing (but not determining) performance across different cognitive tasks [13].\nQ: I know people who perform well on one kind of problems, say complex word problems, but not so well on another kind of problems, say mathematical problems.\nA: Richard Haier, the editor-in-chief of Intelligence, says that, typically, the correlation is still positive [6, 04:32].\nQ: How well has Spearman's idea held up?\nA: There is an ongoing debate about its validity and limitations, especially in modern psychology, but it seems to be accepted in prinicple. \u201cI think I can say without fear of being empirically contradicted that it is the most replicated finding in all of psychology\" [6, 06:05].\nQ: Wow! And IQ scores are supposed to measure the g-factor?\nA: Modern IQ tests are designed to do just that by assessing various cognitive domains.\nQ: It is hard to believe that intelligence, that is general intelligence, can be summarized by one number.\nA: I agree with you. Intelligence is too complex and multifaceted to be reduced to one number. But the proponents of IQ scores point out its empirical support, practicality, and predictive utility. This issue is too involved and important to be discussed on one foot, as they say in Hebrew. We would need to give it the time and attention it deserves.\nQ: I understand."}, {"title": "Intelligence as skill-acquisition efficiency", "content": "Q: I guess Chollet also wants to measure the g-factor. What is special about his approach?\nA: To make progress towards the promise of AI, one needs a workable definition of intelligence and a quantitative measure of intelligence - in particular human-like general intelligence. Chollet makes a good step in this direction.\nWe... articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems [3, Abstract].\nA task-specific performance, say at chess or Go, measures a particular skill; the performance can be inflated through training data and prior knowledge.\nWhether you are a human or an AI system, general intelligence is not about what you know or can do. According to Chollet, it is about how efficiently you can acquire a new skill or adapt to a new environment that you did not anticipate. \"There's a big distinction to be drawn between intelligence, which is a process, and the output of that process which is skill\" [2, 28:52].\nQ: I always thought about intelligence as a capacity, rather than a process. Maybe Chollet means \u201cskill-acquisition\u201d which is a process. In any case, I like very much the definition of intelligence as skill-acquisition efficiency. What about you?\nA: I doubt that the sprawling intuitive notion of intelligence reduces to skill-acquisition efficiency but it is hard to argue with the thesis that skill-acquisition efficiency is a manifestation of intelligence.\nQ: What does efficiency mean in Chollet's definition? Humans can often grasp a new concept with just a few examples. Is that the kind of efficiency he is talking about?\nA: Yes, an efficient system can generalize from limited examples to solve a broader range of related problems. This is an important aspect of skill-acquisition efficiency. But there are other aspects. In particular, an efficient system can quickly adapt to moderate changes in its environment without extensive retraining, can apply knowledge from previously learned tasks to a new one, and can use less compute."}, {"title": "Algorithmic Information Theory (AIT)", "content": "Q: It is surprising that skill-acquisition efficiency has a formal definition. Does Chollet prove interesting theorems about it?\nA: Chollet expresses various notions involved in his definition of intelligence in the AIT formalism. He proves no theorems. To me, the formalization exercise is not convincing. It is not even sound in a sense.\nQ: What do you mean?\nA: Let me introduce Chollet's notation and, at the same time, recall algorithmic complexity, also known as Kolmogorov complexity [11,12]. Below, strings are binary strings.\nThe Algorithmic Complexity (noted H(s)) of a string s is ... the length of the shortest program that outputs the string when running on a fixed universal Turing machine. Since any universal Turing machine can emulate any other universal Turing machine, H(s) is machine-independent to a constant. [D]efine the information content that a string s2 possesses about a string s\u2081 (called \u201cRelative Algorithmic Complexity\" and noted H($1|82)) as the length of the shortest program that, taking s2 as input, produces s\u2081 [3, p. 34].\nMachine-independence follows from the fact that a given universal programming language L can emulate any universal programming language L' (as well as any non-universal programming language) up to an additive constant. As a result, HL(S) \u2264 HL(S) + c and H\u2081(81|82) \u2264 HL($1|82) + c, where cis essentially the length of an L'-to-L translator. If machine independence is a must, then Chollet has a problem. For example, he formalizes the generalization difficulty GDTC of a task T given an experience curriculum Cas\n\n(GD)\nwhere Sol is the shortest solution of T of threshold during evaluation, and TrainSol is the shortest optimal training-time solution of T given C, so that 0 < GDC \u2264 1. (Chollet describes T, C, 0, Solf, and TrainSolo more precisely, but this is not important for our purposes.) The allegedly innocent additive constant makes GD highly dependent on the fixed universal language, not just up to an additive constant.\nQ: Explain.\nA: Notice that the additive constant c may be arbitrarily large. Indeed, flip a fair coin N times, thus producing a random strings of length N. Then, with overwhelming probability, H(s) \u2248 N for the fixed language L [12, p. 8]. If L' is the extension of L with a single short command that prints s then, for L', H(s) is a small constant, and so c\u2248 N.\nLet A and B be the numerator and denominator in (GD). Consider a situation where, for our fixed language L, GDC is small, say A = 1 and B = 100, so that GDC = 1/100. Let L' be another language whose Kolmogorov complexities exceed those of L by 1000000. Then, for L', we have A = 1000001 and B = 1000100, so that GDI, \u2248 1, so that GD moves from one extreme (\u2248 0) to the other (\u2248 1).\nQ: What does curriculum mean in Chollet's approach?\nA: A curriculum is a (carefully crafted) sequence of tasks. It is an important concept in Chollet's approach. Curricula aim to cover a wide range of task-solving skills and are designed to minimize the reliance on pre-existing knowledge.\nQ: Back to your critique, while machine independence is desirable, one still can work with a fixed universal programming language, carefully chosen to minimize built-in information.\nA: This is true, at least in principle. But let me notice that the functions H(s) and H($182) are uncomputable. Moreover, they are not even approximable by computable functions. Indeed suppose toward a contradiction that |H(s) \u2212 f(s)| \u2264 N for some computable f(s). Then f(s) \u2013 N is a computable lower bound for H(s). By Theorem 6 in [12], f(s) \u2013 N is bounded. Then f(s) is bounded and therefore H(s) is bounded, which is impossible.\nQ: Still, there may be something useful in algorithmic complexity. An expression like (GD) is more succinct than a textual description of it.\nA: Yes, symbolic expressions may be succinct, and this is an advantage.\nQ: How important is the AIT foundation for Chollet's approach?\nA: I don't think it is important. Chollet's approach isn't really founded in AIT. In the literature, AIT is typically used as motivation and inspiration rather than literally, and it can play such a role in Chollet's approach as well.\""}, {"title": "Measuring intelligence", "content": "Q: What else is there in Chollet's article?\nA: He covers a lot of ground. There is a general theme of defining intelligence, that we discussed above, and measuring intelligence, that I intend to bring up. But many sections are educational essays in their own right, at least for non-experts. For example, there is a section \u00a7I.3.2 on generalization theory. It addresses defining, measuring, and maximizing generalization.\nQ: Advanced mathematics is all about generalizations. But I have never heard of measuring generalization. Maximizing generalization would probably result in some trivial scenario, with no interesting theorems.\nA: Chollet speaks about different notion of generalization, \"originally developed to characterize how well a statistical model performs on inputs that were not part of its training data\" [3, p. 9].\nMeasuring intelligence is super important to Chollet: \u201cwe need to be able to define and evaluate intelligence in a way that enables comparisons between two [AI] systems, as well as comparisons with humans\u201d [3, Abstract]. To this end, he reifies his definition of intelligence as \u201cskill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty\" [3, p. 27].\nQ: I have encountered the noun prior before, but only in Bayesian statistics, where it means prior distribution.\nA: In AI the noun prior, often in the form priors, is used to mean previously acquired knowledge that the agent, an AI system or a human, brings to the table. Chollet proposes a new benchmark for general intelligence that he calls Abstraction and Reasoning Corpus (ARC).\nQ: How does ARC differ from previous benchmarks?\nA: Traditionally, AI benchmarks focused on tasks solvable through pattern recognition and statistical learning. ARC emphasizes generalization, abstraction, and few-shot learning.\nQ: I haven't heard of few-shot learning but the name seems to suggest learning new tasks from a small number of examples.\nA: Exactly.\nQ: ARC seems ambitious.\nA: It is very ambitious, a paradigm shift in how we assess intelligence. ARC aims to provide a standard measure for generalization and abstraction abilities of different intelligent systems, whether AI systems or humans.\nQ: Is ARC already available? Are there some tests that I can try?\nA: Yes, see [1]. There are quite a number of tests.\nQ: Great. I intend to try some of those tests, at least on myself."}, {"title": "Reaction to Chollet's paper", "content": "Q: You will need a GitHub account and a bit of programming prowess to make those tests available in convenient visual form.\nQ: ARC looks super attractive but also super challenging, especially if one wants to compare AI systems with humans.\nA: I agree. The project is grandiose, and challenges are abundant. In particular, ARC involves new algorithms to assess abstract reasoning and few-shot learning.\nQ: Does Chollet propose ARC as an adequate measure of intelligence?\nA: Not at this point. ARC is a project in progress. The current implementation presupposes only human core knowledge and even that only in part. One has to start somewhere.\nQ: What's core knowledge?\nA: The knowledge that we are born with or hardwired to acquire quickly after birth.\nQ: To compare AI systems with humans, Chollet needs to make human core knowledge explicit. Is this even possible?\nA: There has been impressive progress on understanding the human core knowledge [14]. The core knowledge comprises a few distinct systems, i.e. distinct priors. One prior allows us to see the world split into distinct objects. Another prior is that some of these objects are goal-pursuing agents. Two additional priors are related to primitive geometry and numbering respectively. \"Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners\" [14, Abstract].\nQ: Has Chollet's definition of intelligence been accepted?\nA: It generated significant debate in the AI research community. It influenced discussions on AI evaluation metrics and benchmarks. It sparked renewed interest in developing more comprehensive intelligence tests for AI systems. But it's not universally accepted. Some question whether Chollet's definition of intelligence is comprehensive. Does it capture creativity? Does it capture emotional intelligence?"}, {"title": "Logic", "content": "Q: I understand creativity doubts. As far as emotional intelligence is concerned, my impression is that it is irrelevant here. That is unless advanced intelligence is impossible without emotional intelligence.\nA: Recent advances in neuroscience shed light on this question. Jeff Hawkins, neuroscientist and businessman, addresses the issue head-on in his beautiful book The Thousand Brains Theory of Intelligence [7].\nThe parts of the brain responsible for intelligence and emotion are distinct, albeit connected.\nThe newest part of our brain is the neocortex The neocortex is the organ of intelligence. Fears and emotions are created by neurons in the old brain [7, \u00a71].\nEmotionless artificial intelligence is not only possible; it is more straightforward.\nIntelligent machines need to have a model of the world and the flexibility of behavior that comes from that model, but they don't need to have human-like instincts for survival and procreation [7, \u00a710].. . . We, the designers of intelligent machines, have to go out of our way to design in motivations [7, \u00a711].\nQ: Should we design in emotions?\nA: Personally, I don't think so. Can you switch off a computer that implores you not to do that? But the issue is deep.\nQ: I am becoming a serial digresser Let's return to Chollet. Since he has the ambition to compare AI with humans, what do neuroscientists say? What, if anything, do computer scientists say?\nA: I don't have a representative sample of opinions. My impression is that, by and large, neuroscientists thought and continue to think that intelligence is poorly defined. Recall Michael Jordan whom I quoted earlier. He says: \u201cWe don't know what intelligence is. We don't know much about abstraction and reasoning at the level of humans; we don't have a clue\" [9, 16:28].\nComputer scientist Leslie Valiant, who received the Turing Award in 2010, has a similar opinion, with a twist. In his conversation with Sean Carroll [16] he says:\nI think the main downside of intelligence is that no one can define it. That is, of course, people have complained that we give importance to intelligence. We test people for intelligence, and this has consequences. And we don't even know what we're testing for, where the questions come from. So I think it's very unfortunate that the notion of intelligence has become so important, because it's not explicitly defined. So I've explicitly defined educability; intelligence has no such definition [14, 45:51].\nQ: I see what you mean by the twist. Tell me about educability.\nA: I will, though not now. Valiant's approach [13] deserves a deep dive, a separate conversation.\nQ: OK, I understand. I wonder whether Michael Jordan and Leslie Valiant heard of Chollet's definition of intelligence and what they think about it.\nA: I asked Valiant about that. Here's his reply:\nI define a model (educability) and describe what real world phenomenon it is intended to correspond to (extra cognitive capabilities of humans). Chollet does indeed also have a model. But my question is: what real world phenomenon does it correspond to? Intelligence as generally used is a broad sprawling concept but some aspects, such as what IQ tests measure, seem inseparable from it. I think it is difficult to argue that any model of intelligence corresponds well to the intuitive concept as widely used. I think the challenge for any model is to articulate what important phenomenon it captures [15].\nQ: We can't finish without touching on logic. I am looking at the official site of Abstraction and Reasoning Corpus for Artificial General Intelligence [1]. The terms abstraction and reasoning should make the logician in you happy.\nA: You bet. But such abstractions and reasoning are a challenge to the logic community.\nQ: Yes, we spoke about this last spring [5].\nA: Let me add a few words. Many mathematicians and mathematical logicians look down on philosophers. There is a joke about the chairman of math department trying to squeeze out a new position from the dean:\nContrary to physicists and chemists, mathematicians don't need labs, just a pen, paper, and a waste basket.\nYou think you are cheap. The chairman of the philosophy department tells me that philosophers need only a pen and paper, no waste basket.\nQ: That's cruel. Also, philosophical departments don't have a monopoly on philosophy.\nA: I agree on both counts. In any case, mathematical logicians should not forget that it was essentially philosophical work of formalization that prepared the ground for mathematical logic. Think of Boole, Cantor, Frege, Russell & Whitehead, Zermelo. The logics of today are not sufficient to satisfy the needs of rapidly developing AI.\nQ: Yes, in our conversation last spring, you mentioned fast thinking in the sense of Kahneman [10].\nA: Right. We need new foundational investigations, new formalizations which would enable us to develop logics appropriate for new applications.\nQ: Who will do the work? Mathematical logicians or philosophers? Or maybe computer scientists?\nA: The work requires (at least) intimate knowledge of AI, mathematical maturity, and a philosophical/foundational attitude. An appropriate person can start his career in AI, computer science, logic, mathematics, philosophy, or some different field altogether.\nQ: It could be a team.\nA: Yes, it could be a team. Russell and Whitehead worked as a team, with a clear division of labor. Daniel Kahneman and Amos Tversky did their foundational work, albeit on psychology rather than logic, as a closely knit team [10]"}]}