{"title": "Tiny Helen's First Curriculum: Training and Evaluating Tiny Language Models in a Simpler Language Environment", "authors": ["Ke Yang", "Volodymyr Kindratenko", "ChengXiang Zhai"], "abstract": "Training language models (LMs) and their appli- cation agents has become increasingly expensive due to the massive size of both datasets and mod- els, making the cost of each test failure difficult to bear. Constructing a simplified language en- vironment acts as a primordial training and test- ing ground for LMs. Such environments do not exclude commonsense knowledge or communica- tion skill foundations, but rather present them in a plainer, more easily comprehensible form, poten- tially enhancing the learning efficiency of LMS, and thus reducing the required model size and data volume for effective training and evaluation. In these simplified language environments, workable strategies for small models, datasets, and agents may be adaptable to larger models, datasets, and agents in complex language environments.\nTo create such a simple language environment, we propose to consider the following aspects: i) min- imizing language dataset noise and complexity, and ii) preserving the essential characteristics of the text distribution. Prior methods have struggled with either one or both of these aspects. In con- trast, we introduce a pipeline that revises text data to eliminate noise, reduce vocabulary size, and maintain genre-specific linguistic patterns (e.g., for books, conversation, code, etc.). Implement- ing this pipeline with large LMs, we have cre- ated a leaner suite of LM training and evalua- tion datasets: a 71-million token (71M) LEANER- Pretrain, a 7M LEANER-Instruct for instruction- tuning, a LEANER-GLUE for assessing LMs' ba- sic linguistic proficiency, and a LEANER-Eval for evaluating the instruction-following capability.\nOur experiments demonstrate that the leaner pre- training enhances the learning efficiency of LMs. Furthermore, we show that tiny LMs trained on the leaner datasets outperform those trained on the", "sections": [{"title": "1 Introduction", "content": "\"Yet at the fountainhead of rivers, the waters flow ever so slender, limpid beyond compare.\"\n\u2013 Hippolyte Taine, The Philosophy of Art\nIn documenting civilization, human language systems have evolved into unprecedented complexity, seemingly over- looking how early humans used simple vocabularies to build comprehensive knowledge systems. This complex- ity presents challenges in machine learning, especially in training large language models (LLMs) and the downstream agents in complex linguistic environments. Such training requires vast datasets and computational resources that are often unaffordable for most institutions. Additionally, many research questions, particularly those concerning model ar- chitectures and training processes, demand extensive exper- imentation with LLMs. The high costs of these experiments and the prohibitive loss of failures hinder progress in these critical areas. Unlike the current methods for training LLMs, humans learn languages more efficiently by first being ex- posed to simplified language environments that gradually increase in complexity. This allows children, with their relatively small neural networks, to effectively grasp lan- guages. This prompts us to explore whether we can apply this approach to LLMs by beginning with a smaller model and training it using simpler language data, thus emulating a child\u2019s language acquisition process."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-based Self-evolving Agents with Curriculum Learning", "content": "The existing framework for developing self-evolving agents typically involves four iterative stages: experience acquisi- tion, experience refinement, updating, and evaluation (Tao et al., 2024). In each cycle, agents engage with tasks to gain experience, refine it (e.g., by filtering out unsuccessful tra- jectories), and use the refined data for subsequent training, as showcased in work such as Huang et al. (2022), Gulcehre et al. (2023), Wang et al. (2023), and Tu et al. (2024).\nHowever, this bootstrap approach misses the opportunity for agents to seek external insights through knowledge reflec- tion and information requests. These agents can enhance their existing skills but struggle to improve on tasks they haven't previously encountered or successfully fulfilled.\nAs a complement to existing work, we propose self-evolving agents that actively seek knowledge for continual improve- ments, starting with tiny LMs for resource-efficient research. We introduce a no-noise, low-complexity training dataset in- spired by curriculum learning (Soviany et al., 2022), initial- izing LMs\u2019 training with easier linguistic patterns to smooth the loss function and improve their learning efficiency."}, {"title": "2.2 Tiny Datasets for Tiny Language Models", "content": "Recent studies have focused on developing or compiling tiny-scale datasets to explore language modeling within a dataset size similar to children's mind development and sup- port research in low-resource contexts. These studies also train tiny-scale models to validate their datasets, detailed in Table 1. Specifically, Eldan & Li (2023) create a dataset of children\u2019s stories using only words understandable by chil-"}, {"title": "2.3 Model Architecture Comparison and Curriculum Learning Strategies for Pre-training", "content": "Several studies compare transformers and state space mod- els. Patro & Agneeswaran (2024) examines transformer and state space model variants on computer vision (CV) and natural language processing (NLP) benchmarks but doesn't control for model size or pre-training data. Waleffe et al. (2024) controls for these variables (8B parameters, 3.5T tokens) when comparing transformers and MAMBA models on tasks like commonsense reasoning and long-text tasks, finding MAMBA models can match or exceed transformers.\nIn contrast, our study uses a simpler language environment with smaller models (14M parameters) and a smaller dataset (100M tokens), focusing on evaluating general linguistic ability, which leads to different conclusions.\nRegarding curriculum learning for causal language mod- eling pre-training, Campos (2021) uses sample-difficulty- measuring metrics like sentence length and entropy, ap- plying competence-based sampling but finds no significant training improvements. Li et al. (2022) and Nagatsuka et al. (2023) use sentence length, concluding that curricu- lum learning accelerates convergence, reducing token use and training time with a step-wise linear pacing function and the Square scheduler, respectively. Fan & Jaggi (2023) employs a learnability score, i.e., the loss gap from a proxy model at early and late stages, prioritizing samples with high scores, achieving better validation perplexity and accuracy on knowledge benchmarks. Our study, conducted in a sim- pler linguistic setting, uses metrics like sentence length, LM self-paced loss, and proxy LM loss. Our pacing function iteratively updates the training dataset to be sampled. We evaluate performance on general linguistic tasks."}, {"title": "3 Dataset Curation and Statistics", "content": "Our goal is to refine the original (naturally available) training data of large LMs, such as web text, code, and instruction-answer pairs, by reducing noise and simplifying the text for easier learning. As demonstrated in Appendix B, the complexity of a language dataset is lower-bounded by the information entropy of its text distribution. Thus, our dataset revision process focuses on reducing vocabulary size and rephrasing content in simpler terms to minimize entropy. The entire pipeline is outlined in Algorithm 1. Additionally, we revise the GLUE (Wang et al., 2019) bench- mark for testing LMs' linguistic abilities, and create an instructing-following benchmark adhering to the same prin- ciple, presented in Algorithm 2 and 3, respectively. The key algorithm components are elaborated on in the following sec- tions. We name the initial datasets LEANER-Pretrain-71M, LEANER-Instruct-7M, LEANER-GLUE, and LEANER-Eval- 1K, omitting the token counts for brevity in later references."}, {"title": "3.1 LEANER-Pretrain and LEANER-Instruct", "content": "Algorithm 1 LEANER-Training Dataset Collection\nInput:\nPre-training size: 100M tokens, distributed as:\nNatural language (NL): {web: 65%, book: 5%, wiki: 2.5%, textbook: 5%, conversation: 2.5%};\nFormal language (FL): code: 15%, math: 5%}.\nInstruction-tuning (Instruct) size: 10M tokens.\nProcedure:\n1. Identify sources for each subset.\n2. for subset in NL do\nRandomly sample from source until target token count is met.\nend for\n3. for subset in FL do\nRandomly sample from source partitions, ordered by increasing difficulty, until target token count is met.\nend for\n4. for subset in NLU FLU Instruct do\nDefine revision principles.\nAn LLM revises text following principles.\nend for"}, {"title": "3.1.1 DATA SOURCES", "content": "We follow a pretrainer's guide (Longpre et al., 2024) when deciding the pre-training corpus for simplification, including standard domains like web, books, wikis, and conversations for natural language, and code and math for formal language. We exclude specialized areas like biomed and academic texts due to their atypical distribution. Given the proven benefits of high-quality textbooks (Gunasekar et al., 2023), we also include a textbook segment in the natural language portion. We randomly sample instances from the following"}, {"title": "3.1.2 LANGUAGE SIMPLIFICATION", "content": "To manage time and cost constraints, we rely on LLMs rather than human annotators to revise datasets. Specifically, the LLM is prompted to rephrase text, ensuring simple gram- mar and a vocabulary limited to the 2,000 most common words, and to simplify complex concepts. Furthermore, for noisy sources like web data, we instruct the LLMs to exclude irrelevant content such as formatting and adver- tisements. Detailed prompts are in Appendix G, with the heuristics behind provided in the later paragraphs.\nThe main challenge in simplifying natural language and instruction splits lies in overly detailed descriptions, which often include numerous references to specific entities such as individuals, places, organizations, times, artifacts, and links. Retaining these references would overextend the vocabulary and overwhelm a tiny model with trivial and detailed relationships, illustrated by samples such as \"The Alexander ALX400 (later known as the TransBus ALX400 and the Alexander Dennis ALX400) is a 2-axle double- decker low-floor bus body. It was built from 1997 to 2006 by Alexander and then TransBus International and then Alexander Dennis respectively.\" Therefore, we simplify the world setting, assuming a limited number of people from a small set of places or organizations, at general times, engaged in simple activities. The guidelines to simplify the world setting are summarized below.\nWorld Setting Simplification Guidelines\ni. Replace any mentioned name with a name from the provided list that shares the same initial letter.\nii. To rephrase an entity like a place, use its first two uppercase letters followed by its name.\niii. Only keep specific times if essential for understanding the context; otherwise, use vague references.\niv. Use simple, common language to describe an artifact."}, {"title": "3.2 LEANER-GLUE", "content": "Algorithm 2 LEANER-GLUE Benchmark Creation\nInput:\nMinimum coverage ratio $r = 70\\%$.\nProcedure:\nDefine revision principles.\nfor task in GLUE do\nLoad the original sample list ori.\nInitialize the leaner sample list leaner = [].\nwhile len(leaner) <r\u00d7len(ori) do\nKeep a revised sample buffer revised = [].\nfor sample in ori\\leaner(pre \u2013 revision) do\nAn LLM revises sample following principles and outputs revised_sample.\nrevised + = [revised_sample].\nend for\nfor sample in revised do\nGiven the input $x$ and label $y_{true}$ in sample, let an LLM predict $x$'s label $y_{pred}$.\nif $y_{pred} == y_{true}$ then\nleaner + = [sample].\nend if\nend for\nend while\ndocument leaner.\nend for\nWe intend to evaluate the performance of tiny models across a diverse range of tasks, with the GLUE benchmark (Wang et al., 2019) serving as a promising candidate. GLUE is a suite of tasks designed to assess a model's general linguistic capabilities, covering areas such as linguistic acceptability (COLA (Warstadt et al., 2019)), sentiment analysis (SST2 (Socher et al., 2013)), sentence similarity (MRPC (Dolan & Brockett, 2005), QQP (quora dataset release, 2012), and STS-B (Cer et al., 2017)), and natural language inference"}, {"title": "3.3 LEANER-Eval", "content": "Algorithm 3 LEANER-Eval Benchmark Creation\nInput:\nGeneration iteration $I = 50$, example number $n_e = 10$, output question number $n_q = 50$, similarity threshold $s = 0.876$.\nProcedure:\nInitialize the raw leaner question list $raw\\_Qs = []$.\nfor $i$ in $I$ do\nInstantiate the prompt with $n_e$ randomly sampled examples from LEANER-Instruct, and specify $n_q$.\nAn LLM generates the question list $q_s$.\n$raw\\_Qs + = q_s$.\nend for\nDeduplicate $raw\\_Qs$ to $Q_s$ by question similarity $> s$.\nfor $q$ in $Q_s$ do\nCompute $q$'s similarity with instances in LEANER- and Ori-Instruct, and record them.\nend for\ndocument $Q_s$.\nLLMs with extensive vocabularies and knowledge bases. To address this, we propose a set of problems and evalua- tion metrics named LEANER-Eval, specifically for assessing the instruction-following abilities of tiny LMs with limited vocabulary and knowledge scope. During our dataset cu- ration, we aim to generate questions that test the linguistic proficiency and commonsense knowledge of LMs, without delving into advanced knowledge-based queries such as the mechanism behind the hexagonal symmetry of snowflakes. To ensure a diverse and relevant set of questions, we sample examples from a question set that features a broad range of topics in simple language. These examples serve as tem- plates for question structure and content, and are incorpo- rated into the prompts to guide the generation of new ques- tions. Therefore, our problem generation strategy involves randomly sampling 10 questions from the LEANER-Instruct dataset as examples, then using LLMs (GPT-4-0125 and"}, {"title": "4 Experiments", "content": "The main goal of our experiments is to study the following three new research questions around the optimization of efficiency and effectiveness of LLM training: i) Can training with clean datasets that have lower linguistic complexity (e.g., LEANER-Training) enhance the learning efficiency of LMs? ii) Do LMs pre-trained and instruction-tuned with these low-complexity datasets tend to develop instruction- following abilities earlier? and iii) Would the LEANER datasets, similar in composition to traditional LLM training sets, enable more efficient development of LM architectures and training techniques on a resource-efficient scale?\nTo answer these questions, we design three experiments. Ex- periment 1 (EXP1) compares downsized versions of varied LM architectures on language modeling and downstream tasks, ensuring consistent pre-training data and model size. Experiment 2 (EXP2) assesses the instruction-following abilities of tiny LMs trained with different pre-training and instruction-tuning data combinations. Experiment 3 (EXP3) explores curriculum learning strategies for LM pre-training."}, {"title": "4.1 EXP1: Comparing Model Architectures with the LEANER Dataset Suite", "content": ""}, {"title": "4.1.1 MODELS", "content": "In this experiment, we control for model size, training data, and setup to compare the learning efficiency of different LM architectures. Specifically, we select LM architectures in- cluding BERT (transformer (Vaswani et al., 2023) encoder) (Devlin et al., 2019), LLAMA (transformer decoder) (Tou- vron et al., 2023), XLNET (which uses permutation-based language modeling) (Yang et al., 2020), and MAMBA (state space model) (Gu & Dao, 2024a).\u00b9 We configure each model to have approximately 14M parameters, with detailed parameters provided in Appendix H and model sizes listed in Table 4. To ensure comparable tokenization, we limit the vocabulary size to 2K tokens and retrain new tokenizers using the LEANER-Training corpus. All tokenizers are ini- tialized with the OPENAI-COMMUNITY/GPT-2 (Radford et al., 2019) tokenizer. For LLAMA and MAMBA, both de- coders, the retrained tokenizer could be applied directly. For BERT and XLNET, we replace less common words in the retrained tokenizer with model-specific special tokens (5 for BERT and 9 for XLNET).\nThe pre-training datasets included the natural language sec- tions of LEANER-Pretrain and Original-Pretrain. BERT, LLAMA, and XLNET are trained using model code from HuggingFace (Wolf et al., 2020), while Mamba is trained us- ing the GitHub STATE-SPACES/MAMBA (Gu & Dao, 2024b). Detailed machine and training settings are in Appendix H. We perform a grid search of leanrning rate to identify the setup with the lowest evaluation loss for each architecture. The model hyperparameter selection is also provided in Appendix H."}, {"title": "4.1.2 BENCHMARK", "content": "Beyond language modeling, we assess the performance of these pre-trained tiny LMs against their non-pretrained counterparts on general linguistic downstream tasks using the LEANER-GLUE benchmark (introduced in Section 3.2). LEANER-GLUE is a simplified vocabulary version of the GLUE benchmark (Wang et al., 2019), maintaining the orig- inal test objectives, which include:\nLinguistic acceptability: LEANER-COLA;\nSentiment analysis: LEANER-SST2;\nSentence similarity: LEANER-MRPC, LEANER-QQP, and LEANER-STS-B;\nNatural language inference: LEANER-MNLI, LEANER-"}, {"title": "4.1.3 RESULTS", "content": "The results are presented in Table 4. Firstly, it is notable that providing a simpler language environment as the learn- ing context enhances the learning efficiency of LMs. This is evidenced by the fact that all LMs pre-trained with the LEANER-Pretrain dataset outperform those pre-trained with the Original-Pretrain dataset in terms of average scores and most sub-task scores, despite LEANER-Pretrain is 41% smaller (71M tokens compared to 100M tokens). Secondly, pre-training offers LMs a significant advantage on down- stream tasks, as pre-trained models consistently outperform those fine-tuned from scratch on each sub-task. Finally, the learning efficiency varies among different LM architectures. Under both pre-training scenarios, the order is XLNET > BERT > LLAMA > MAMBA, while for models fine-tuned on sub-task from scratch, the order is LLAMA > XLNET > MAMBA > BERT. It is important to note that LM Loss is not comparable across LM architectures, except LLAMA and MAMBA, due to differing learning objectives. Additionally, because the epoch-wise learning objective remains constant for decoders (i.e., LLAMA and MAMBA), they are more prone to overfitting, resulting in fewer pre-training steps for the final tested models compared to other architectures."}, {"title": "4.2 EXP2: Evaluating the Instruction-following Ability of Tiny LMs Trained with Different Data Recipes", "content": ""}, {"title": "4.2.1 MODELS AND THE BENCHMARK", "content": "We employ two pre-training datasets and two instruction- tuning datasets, each in both leaner and original versions, creating four data recipes for training instruction-following models. We train the LLAMA model with 14M parame- ters using consistent settings, with details on model con- figuration, hardware, training settings, and hyperparame- ters provided in Appendix I. During pre-training, we select the model with the lowest evaluation loss from both the LEANER-Pretrain and Original-Pretrain datasets as the start- ing point for instruction-tuning. In the instruction-tuning"}, {"title": "4.2.2 RESULTS", "content": "The evaluation results of the model's responses are listed in Table 5, with examples of instruction-following genera- tion provided in Appendix J. First, we observe that a 14M LM pre-trained on approximately 70M tokens struggles to consistently produce high-quality responses. This can be inferred from the optimal model's average metric-wise score of around 1, indicating a bad level according to the bench- mark. Notably, the 100M token dataset is comparable to the total volume of text accessible to a human adolescent (Gilkerson et al., 2017), which guides our training dataset size decision. However, we acknowledge that during early human development, individuals are exposed to multimodal data, and the functioning of the human brain differs from computational models. Consequently, a 100M token scale LEANER-Pretrain dataset is insufficient for enabling a 10M scale model (larger models would easily overfit on 100M tokens) to acquire instruction-following capabilities.\nFurthermore, when analyzing the overall performance by holding constant the instruction-tuning corpus, we find that pre-training on the LEANER dataset enhances the grammar, coherence, and specificity of instruction-following gener- ations, with grammar improvements being the most pro- nounced. Additionally, when comparing model performance across different similarity partitions, we notice a significant decline in response quality for questions without similar instances in the instruction-tuning training set.\nFinally, the LEANER-Ori model outperforms the LEANER- LEANER model. We attribute it to the LEANER-ori model's tendency to produce general responses, such as \u201cAs an AI, I"}, {"title": "4.3 EXP3: Investigate Curriculum Learning Strategies for Pre-training with Tiny Proxy Models", "content": ""}, {"title": "4.3.1 CURRICULUM LEARNING STRATEGIES", "content": "Curriculum learning, first training a model with simple ex- amples before gradually introducing more complex ones, has been theoretically and empirically shown to be effective in various CV, NLP and robotics tasks (Abbe et al., 2023; Soviany et al., 2022). Pre-training in NLP is a crucial task in machine learning, but conventional pre-training often re- quires massive datasets and large models, which can be pro- hibitively expensive for many institutions. We hypothesize that using smaller datasets and LMs in simplified language environments can replicate the trends observed with larger datasets and LMs in more complex environments. This al- lows us to conduct a preliminary exploration of curriculum learning strategies for LM pre-training using the 71M token LEANER-Pretrain dataset and a 1M parameter LM.\nDesigning a curriculum learning strategy typically involves two key considerations: i) defining the difficulty metric of training samples, and ii) adjusting the training pace to progressively introduce samples from easy to difficult. For"}, {"title": "4.3.2 MODELS AND THE BENCHMARK", "content": "We use the same model, specifically the 1M LLAMA, to evaluate all pre-training strategies at intervals of 500 training steps. In the ITER. LM-LOSS strategy, we calculate the LM loss using the 10M LLAMA model, which has been pre-trained on the LEANER-Pretrain dataset, as described in Section 4.1.1. Details about the model configuration and training setups are provided in Appendix K. Our benchmark is LEANER-GLUE, where we maintain the testing settings as outlined in Section 4.1.2, with the exception that the learning rate is set consistent."}, {"title": "4.3.3 RESULTS", "content": "The complete results are presented in Table 9 and illus- trated in Figure 2. Our findings indicate that reordering training samples by appropriately defined sample difficulty and gradually introducing new samples when the model has learned all the generalizable knowledge from the previous pre-training dataset can save pre-training steps and data. Specifically, as shown in the left Figure 2, in a total of 2,500"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 TinyHelen's First Curriculum", "content": "This study could be interpreted as optimizing language datasets to enhance the model's learning efficiency, aim- ing to identify the minimal dataset volume necessary for an LM to grasp commonsense knowledge and basic com- munication abilities. The ultimate goal is to establish a cost-effective testbed for crafting learning strategies for self- evolving text-based agents. Analogously to nurturing young Helen Keller, we expect that with the right foundation in human-like learning skills, these agents can independently and progressively accumulate and expand knowledge.\nEquipping artificial intelligence agents with human-like learning abilities allows them to autonomously grow their understanding by actively identifying knowledge gaps and"}, {"title": "5.2 Other \"No Noise Low Complexity\" Datasets", "content": "We suggest that some remarkably comprehensible datasets might have been benefited from being generated by LLMs, potentially including a cleaning step to reduce noise and complexity. LLM-generated content often shows consistent and predictable linguistic patterns reinforced by its human- aligned training data, resulting in lower information entropy and better learnability. For instance, Gunasekar et al. (2023) demonstrate how filtering The Stack and StackOverflow data for educational value, and using LLMs to generate textbook-quality code, can produce high quality text that re- duces dataset size and the computational resources required for training. However, we acknowledge that while LLM rewriting reduces complexity, it introduces biases, such as those related to preferences of those who annotate the align- ment data, which we will discuss later."}, {"title": "6 Conclusion", "content": "In this work, we construct a simplified language environ- ment where we demonstrated that both language under- standing and generation can be learned more efficiently. This suggests that for LMs and their most widespread ap- plication, agents, the strategies tested effective for small models, datasets, and agents in simpler language environ- ments can potentially be adapted to large models, datasets,"}, {"title": "7 Limitations", "content": ""}, {"title": "7.1 Limited Instruction-following Expertise Exhibited in Tiny LMs Trained with 71M LEANER-Pretrain", "content": "The experiment shows that a 71M token LEANER-Pretrain is insufficient for training an LM that effectively exhibits instruction-following capabilities. One potential solution is to create larger datasets following similar pipelines, but the overhead cost of processing such data, whether using human resources or more advanced LLMs, is prohibitively high. We anticipate that future work can utilize the methods indicated in Section 4.3.3, leveraging the sample difficulty suggested by language modeling loss, along with quality considerations, to filter large-scale web datasets."}, {"title": "8 Ethics", "content": ""}, {"title": "8.1 Bias from the LLMS", "content": "The text in the LEANER-Training, LEANER-GLUE and LEANER-Eval is rewritten using LLMs, which inevitably in- troduces biases accustomed by the LLM, such as knowledge cutoffs and social biases. We anticipate that using more diverse prompts, akin to seeking input from multiple human annotators, can help mitigate these biases."}, {"title": "A Quotation Explanation", "content": "Despite their era's limited knowledge and a small vocabulary, ancient Greek philosophers could still formulate coherent worldviews and convey intricate concepts through language. Just as a river begins from a narrower yet cleaner origin before expanding and deepening, the early stages of human language, though leaner than today's complex linguistic systems, were sufficiently comprehensive for humans to express ideas and develop knowledge."}, {"title": "B Exploring the Relationship between Complexity of Language Dataset and Its Distribution Properties", "content": "We define the complexity of a language dataset to be the total number of unique token combination patterns present within the dataset. All variable notations utilized in this section are explained in Table 6. Here, we would like to prove that a dataset's naive complexity is $\u03a9(e^{N\u00b7H(P)})$ and $O(e^{N ln N})$, and training techniques, such as setting the context window length, would affect the complexity."}, {"title": "B.1 Analyzing Dataset Complexity Without Considering Language Model Training Techniques", "content": "The complexity of the dataset, denoted as $C(D)$, is mathematically defined as $\\frac{N!}{\\prod_{\\nu=1}^{V} (C_{\\nu}!)}$ which quantifies the unique pattern count in the corpus. We aim to show that $e^{C+N\u00b7H(P)}<C(D)< e^{C+N ln (N)}$, where $C = ln \\frac{N!}{N^{N}}$.\nProof. Initially, we apply the logarithm to the complexity:\n$ln (C(D)) = In \\frac{N!}{\\prod_{\\nu=1}^{V} (C_{\\nu}!)} = ln \\frac{N!}{N^{N}} - ln  \\prod_{\\nu=1}^{V} (C_{\\nu}!) = ln \\frac{N!}{N^{N}} + \\sum_{\\nu=1}^{V} ln(\\frac{C_{\\nu}!}{N^{N}})$.\nWith $ln \\frac{N!}{N^{N}}$ (denoted as $C$) as a constant term when the dataset size is fixed, we mainly focus on the latter part $\\sum_{\\nu=1}^{V}  (ln(\\frac{C_{\\nu}!}{N^{N}}))$. For each individual term in the summation, we have:\n$-ln(\\frac{C_{\\nu}!}{N^{C_{\\nu}}}) \\geq -Cln(\\frac{C_{\\nu}}{N})$\nwhich leads to:\n$\\sum_{\\nu=1}^{V} (-ln(\\frac{C_{\\nu}!}{N^{C_{\\nu}}})) = -(\\sum_{\\nu=1}^{V} Cln(\\frac{C_{\\nu}}{N})) = N\u00b7H(P)$"}, {"title": "B.2 Incorporating the Effect of Language Model Training Techniques on Dataset Complexity", "content": "During the pre-training phase of the language model, the introduction of the context window length significantly diminishes the number of pattern combinations within a dataset that need to be learned. We now redefine the empirical language dataset complexity as the number of token combination patterns in the partitioned dataset. For training step j, the complexity of the corresponding dataset partition is $\\frac{L!}{\\prod_{\\nu=1}^{V} (C_{\\nu}!)}$\nIt is important to note, however, that the complexities of two dataset divisions, for example steps m and n, would be consolidated into a single term if, for any word at the ith position, $c_{i}^{m} = c_{i}^{n}$ is consistently satisfied. Essentially, the two divisions are duplicated. Hence, the complexity of the language dataset during training does not exceed the sum of the complexities for each step, i.e., $C(D^{training})) \\leq \\sum_{s=1}^{S} (\\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})$, with equality holding when there is no duplication among the dataset divisions.\nWe use $\\sum_{s=1}^{S} (\\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})$ to denote the pseudo dataset complexity. We further establish that the partition technique used during training alters the best case of this pseudo dataset complexity, which is associated not just with the dataset size but also with the context window length.\nProof. As done previously, we compute the logarithm of the complexity term:\n$In (\\sum_{s=1}^{S} (\\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})) \\geq In (S) + \\frac{1}{S}\\sum_{s=1}^{S} (ln \\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})$\nwhere each $ln(\\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})$ can be considered the complexity of a dataset with a total token count of $L$. Therefore, we obtain:\n$In (\\sum_{s=1}^{S} (\\frac{L!}{\\prod_{\\nu=1}^{V} (c_{\\nu}!)})) \\geq In (S) + \\frac{1}{S}\\sum_{s=1}^{S} (ln \\frac{L!}{L^{L}} + L\u00b7H(P_{s}))$\n$= ln ( \\frac{S\u00b7L!}{L^{L}} ) +  L\\cdot  (H(H(P_{s})))$\nwhich gives us:\n$C(D^{training}) \\geq e^{ln(\\frac{SL!}{L^{L}}) + LH(H(P_{s}))}$"}, {"title": "C Establishing the Relationship Between Training Loss and the Capability of the Model to Learn and Forget", "content": "We would like to establish that the expected accuracy of a language model in predicting the subsequent token in a text segment is bounded below by the exponential of negative training loss. Considering a text segment denoted by t, we use $p_i$ to represent the likelihood of accurately predicting the ith token $t_i$ given the preceding text $t_{:i-1}$. This translates to $p_i=p(t_i|t_{:i-1})$ for $i>1$ and $p_1=p(t_1)$. Training loss is defined as $L = -\\frac{1}{L}\\sum_{i=1}^{L} ln (p_i)$, where $L$ denotes the length of the context window.\nProof.\n$L = -ln (\\prod_{i=1}^{L} (p_i)) = -ln (\\prod_{i=1}^{L} (E(p_i))) > -ln(\\frac{1}{L}  \\sum_{i=1}^{L} (p_i)) = -ln(E(p_i))$\n$E(p_i) \\geq e^{-L}$"}, {"title": "D Rank the Formal Language Subsets by Difficulty", "content": "We rank the code subset using the target_audience label from the original dataset. For the math subset, given that the original MATH AMPS contains various question categories, each with multiple questions, we randomly select three questions from each category. We then prompt GPT-4-0125 to classify these questions into difficulty levels (e.g., preschool, elementary school, etc.), with the prompt provided below. We assign exponentially increasing scores to each difficulty level, from the easiest to the hardest, and rank the categories based"}]}