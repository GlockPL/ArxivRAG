{"title": "Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling", "authors": ["Haicheng Liao", "Yongkang Li", "Chengyue Wang", "Songning Lai", "Zhenning Li", "Zilin Bian", "Jaeyoung Lee", "Zhiyong Cui", "Guohui Zhang", "Chengzhong Xu"], "abstract": "The primary goal of traffic accident anticipation is to foresee potential accidents in real time using dashcam videos, a task that is pivotal for enhancing the safety and reliability of autonomous driving technologies. In this study, we introduce an innovative framework, AccNet, which significantly advances the prediction capabilities beyond the current state-of-the-art 2D- based methods by incorporating monocular depth cues for sophisticated 3D scene modeling. Addressing the prevalent challenge of skewed data distribution in traffic accident datasets, we propose the Binary Adaptive Loss for Early Anticipation (BA-LEA). This novel loss function, together with a multi-task learning strategy, shifts the focus of the predictive model towards the critical moments preceding an accident. We rigorously evaluate the performance of our framework on three benchmark datasets-Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), and AnAn Accident Detection (A3D), and DADA-2000 Dataset-demonstrating its superior predictive accuracy through key metrics such as Average Precision (AP) and mean Time-To-Accident (mTTA).", "sections": [{"title": "1. Introduction", "content": "The surge in global traffic accidents has escalated traffic safety into a paramount public health issue [1]. The World Health Organization (WHO) highlights an alarming statistic: annually, 1.45 million lives are claimed by traffic accidents, with injuries surpassing 50 million [2]. By 2030, traffic accidents are projected to become the fifth leading cause of death globally [3]. The financial impact of these accidents is tremendous, costing more than $180 billion annually in the United States alone. This economic burden, coupled with the loss of life, underscores the urgency of enhancing traffic safety measures [4]. This has become one of the most important and challenging tasks for traffic management authorities [5-7].\nIn response to this challenge, accident anticipation, the predictive analysis of vehicular collisions before they occur is a pivotal development for enhancing the safety protocols of autonomous driving [8, 9]. This field has garnered significant interest due to its potential to elevate the safety measures of intelligent vehicle systems dramatically [10-12]. Effective anticipation, even seconds before a potential accident, can empower autonomous systems to initiate critical safety maneuvers, potentially averting collisions [13-15].\nYet, mastering accident anticipation poses substantial challenges, primarily due to the complexity and the often limited and noisy visual data in an observed dashcam video [16]. Traffic scenes, viewed from the perspective of the onsite camera, are cluttered with various elements, such as cars, pedestrians, and motorcyclists. Within this busy visual landscape, cues critical to predicting accidents can be lost amidst irrelevant data, making it difficult for systems to detect impending collisions, especially in complex intersections. Despite these challenges, advancements in modeling, particularly those focusing on uncertainty, hold promise in distinguishing between pertinent and extraneous visual information, identifying potential risks based on variables like irregular vehicle movements [17, 18].\nFurthermore, the issue of data imbalance in traffic datasets poses a significant challenge for deep learning models, which rely on balanced data distributions for effective pattern recognition. Since videos containing accidents often include numerous non-accident clips, this imbalance favors non-accident scenarios, substantially diminishing the models' exposure to, and recognition of, the rare yet crucial instances preceding an accident. Additionally, the introduction of depth information may enhance the existing data bias. These factors lead to another critical question: How can we refine predictive models to highlight the ephemeral, yet pivotal moments that precede an accident? The limited availability of accident data further impacts the models' ability to generalize, increasing the risk of overlooking actual accidents.\nIn response to these challenges, we present a novel framework designed to enhance the anticipation of traffic accidents in complex traffic environments. Central to our approach is the development of a 3D Collision Module [22, 23], which leverages monocular depth information to derive precise three-dimensional coordinates of critical entities like vehicles and pedestrians. This module ensures a more accurate depiction of spatial relationships and dynamics captured in dashcam footage, fostering a deeper understanding of potential collision scenarios. Concurrently, we address the issue of imbalanced data distribution by introducing the Binary Adaptive Loss for Early Anticipation (BA-LEA). This innovative loss function refocuses the model on the decisive moments leading up to an accident, augmented by a multi-task learning approach that fine-tunes the balance between different loss functions to optimize overall model performance. Additionally, the implementation of a Smooth Module promotes training stability, smoothing out fluctuations and bolstering the model's capacity to assimilate complex data sets effectively.\nTo sum up, our contributions are threefold:"}, {"title": "2. Related Work", "content": "The problem of accident anticipation in autonomous driving systems presents a formidable challenge, requiring the detection of collisions from dashboard camera footage and the accurate prediction of their occurrence. Introduced by Chan et al. [24] in 2016, this domain extends beyond traditional accident prediction by necessitating a deep understanding of the complex and unpredictable nature of traffic accidents. The rarity of crash events, coupled with the dynamic and variable nature of traffic scenes, significantly complicates this task.\nIn response to these challenges, a spectrum of studies [25-29] have employed Convolutional Neural Networks (CNNs) in tandem with sequential networks like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) units, and Gated Recurrent Units (GRUs). For example, Yao et al. [30] combine CNN and RNN to predict the temporal features of scenes leading up to accidents, thereby identifying clues in videos that point to future incidents. Takimoto et al. [31] apply GRU and RNN networks to predict accidents by considering deviations in the future positions of traffic agents. Furthermore, Basso et al. [32] propose a new image-inspired architecture that consists of CNNs to capture the microscopic scene of vehicle behavior. Thakare et al. [33] propose a convolutional autoencoder designed for detection and classification, providing an efficient means of extracting key features while minimizing computational demands. Yet, this approach encounters limitations in identifying long-distance patterns, especially in scenarios involving sparse objects. This combination facilitates the processing of time-series visual data and the extraction of complex motion features, representing a significant stride in capturing the temporal dynamics of traffic scenes. Further advancements in this field have seen the incorporation of attention mechanisms [21, 34-36], which refine the model's ability to distill complex temporal interactions. Furthermore, in light of the burgeoning success of Transformers in computer vision, numerous researchers have explored transformer-based models in this field. These models leverage self-attention mechanisms to capture interrelationships within time-series trajectory data, which minimizes information loss and enables the network to learn and extract long-range dependencies. For instance, UniFormerv2 [37], VideoSwin [38], CAVG [39], and MVITv2 [40] all employ the transformer-based model to delineate the relative positioning of traffic agents such as vehicles and pedestrians, underscoring the potential of Transformers in enhancing model understanding of dynamic traffic scenes.\nDespite their proficiency in handling temporal data, these frameworks often fail to accurately capture spatial relationships, a critical aspect when interpreting visually dense traffic scenes. To bridge this gap, researchers have turned to Graph Convolutional Networks (GCNs) [41-44], leveraging the spatial positioning of traffic agents to construct graph-based representations. Notable work such as Song et al. [45] present a dynamic attention-augmented graph network for accident anticipation and Wang et al. [46] introduce a spatio-temporal graph convolutional network in this domain. These approaches, utilizing bounding boxes to outline agents, facilitate a more nuanced understanding of spatio-temporal dynamics, exemplified by the uncertainty-based GCN proposed by Bao et al. [41] and the innovative GCRNN framework by Wang et al. [46], which crafts 2D coordinates to represent missing agents in the visual field. Notably, Liao et al. [47] enhance accident anticipation by incorporating accident localization through the use of LLMs for in-depth scene analysis, allowing for accurate warnings regarding the nature, timing, and location of possible incidents.\nDespite these advancements, a predominant reliance on GCRNN-based frameworks, which process video frames sequentially, has been observed. This methodological approach not only impedes the efficiency of model training and inference but also approximates the distance between objects using two-dimensional pixel distances, neglecting the critical depth aspect. Such limitations underscore a pressing need for models that can accurately capture the three-dimensional spatial dynamics of traffic scenes."}, {"title": "3. Methodology", "content": "Inspired by the burgeoning field of monocular depth estimation [22, 23, 48], our work introduces a novel model, AccNet, built upon the foundation of ZoeDepth [49]. This model innovatively combines monocular depth information with pixel distances, transcending the conventional 2D analysis limitations. By accurately extracting the absolute 3D coordinates of objects such as vehicles, pedestrians, and buildings from images, AccNet offers a precise representation of spatial dynamics in traffic accident scenes. This capability not only enhances the reconstruction of 3D crash scenarios but also marks a significant leap forward in the domain of crash anticipation, setting a new benchmark for future research."}, {"title": "3.1. Problem Formulation", "content": "The advancement of automated systems in identifying potential traffic accidents through dashcam footage is pivotal in enhancing road safety. Our model, leveraging dashcam videos, aims to predict traffic accidents by analyzing sequential frames, thereby providing early warnings that could be crucial in preventing misshapes.\nGiven a video sequence $V = {V_0, V_1, ..., V_T}$ consisting of T frames, our task is to assign a probability score $s_t$ to each frame, indicating the likelihood of an accident occurring at or after that frame. If an accident is anticipated to occur at frame $t$, we define the Time-to-Accident (TTA) as $\u2206t = t \u2212 t^\u00ba$, where $t^\u00ba$ is the earliest frame at which the probability score $s_t$ exceeds a predefined threshold $s^\u00ba$. Thus, a video is classified as containing an accident (positive) if $s_t > s^\u00ba$ for any $t > t^\u00ba$. Otherwise, it is considered accident-free (negative), setting $\u03c4 = 0$. Our goal is to enhance the accuracy of predicting such incidents and maximize the TTA, providing more lead time for preventive actions."}, {"title": "3.2. Model Architecture", "content": "As illustrated in Fig. 2, our proposed model architecture, termed AccNet, integrates feature extraction, attention mechanisms, and a novel 3D collision detection module to analyze dashcam video data effectively. At its core, AccNet aims to discern intricate patterns that precede accidents, enabling early prediction and extending the TTA window."}, {"title": "3.2.1. Feature Extraction and Depth Enhancement", "content": "The initial phase involves processing the video frames through a series of pre-trained models for feature extraction. Utilizing MMDetection [50], we identify traffic agents (vehicles, pedestrians) in each frame $V_t$, focusing on the top N objects based on confidence scores. We then crop images {$V_{t,1}$, $V_{t,2}$,..., $V_{t,N}$} from the bounding boxes of these identified objects, employing VGG-16 [51] to extract comprehensive context H from $V_t$ and visual features 0 = {$0_1,0_2,\u2026,0_N$} from {$V_{t,1}$, $V_{t,2}$,..., $V_{t,N}$}. To capture the spatial depth, we leverage the ZoeDepth model [49], an advanced tool for depth estimation, to construct depth matrices $D_t$ for each frame t. This multi-faceted approach ensures a rich feature set, ready for subsequent analysis."}, {"title": "3.2.2. Context Attention and Object Attention", "content": "Context Attention and Object Attention are designed to prioritize context H and object O features from video frames. The Context Attention employs a multi-head attention framework, translating the context features into query $Q_t$, key $K_t$, and value $V_t$ components through Multilayer Perceptrons (MLP). These components are then segmented into h distinct heads $Q_t^g$, $K_t^g$, and $V_t^g$, and $g \\in [1, h]$, producing the context vector $I_c$. Mathematically,\n$I_c = \\sum_{g=1}^{h} head_g = \\sum_{g=1}^{h} softmax(\\frac{Q_t^g K_t^{gT}}{\\sqrt{d_k}})V_t^g$         (1)\nHere, $softmax$ denotes the softmax activation function. $d_k = \\frac{d}{h}$ and d represent the dimension size of $Q_t^g$ divided across the heads. In parallel, the Object Attention dynamically allocates attention across the detected objects using a series of learnable weight matrices $W_c$, $W_\u03b2$, and $W_b$, which yields a weighted representation $W_o$ and then results in the object vector $I_o$, thus enhancing the model's focus on pertinent objects. Formally,\n$I_o = \\phi_{sum}[W_o O] = \\phi_{sum}[softmax(W_c^T \\phi_{tanh}(W_o O + W_b))O]$\n   (2)\nwhere $\\phi_{tanh}$ is the tanh activation function, and $\\phi_{sum}$ represents the sum of the vectors of the detected objects."}, {"title": "3.2.3. 3D Collision Module", "content": "Existing work [43, 46] typically constructs graph weights using the pixel distance between target objects to learn the latent spatial relationships between agents. However, these approaches often overlook the significant discrepancy between pixel distances and actual distances. To address this issue, we introduce an innovative 3D Collision Module that transcends traditional pixel distance metrics.\nSpecifically, the pre-extracted depth information $D_t$ is fed to construct a GCN under 3D settings. For the n-th object in the t-th frame, the position expression $p_n^t$ of the object detection bounding box can be represented as follows:\n$p_n^t = [p_{1,n}^t, p_{2,n}^t] = [(x_{1,n}, y_{1,n}), (x_{2,n}, y_{2,n})]$     (3)\nwhere $p_{1,n}^t$ and $p_{2,n}^t$ are the pixel coordinates of the top-left and bottom-right coordinates of the bounding box, respectively. Furthermore, given the center coordinates of the bounding box $c_{x,n}^t = \\frac{(x_{1,n}+x_{2,n})}{2}$ and $c_{y,n}^t = \\frac{(y_{1,n}+y_{2,n})}{2}$, the depth at the target object's center point can be applied as its depth information $D^t(c_{x,n}^t, c_{y,n}^t)$.\nHence, the constructed three-dimensional coordinates of the target can be represented as $P_n^t = (c_{x,n}^t, c_{y,n}^t, D^t(c_{x,n}^t, c_{y,n}^t))$. For target objects i, j \u2208 n, we define the distance between target objects at time t as:\n$d_{{i,j}}^t = ||P_i^t \u2212 P_j^t||_2$. In real-world driving scenarios, collisions are often related to the approach and retreat movements of vehicles. To assess this, it is necessary to measure the velocity of two vehicles in adjacent frames. In this study, for target object n, the velocity between two consecutive frames is represented as follows:\n$\\overrightarrow{P_n^t} = (c_{x,n}^t - c_{x,n}^{t-1}, c_{y,n}^t - c_{y,n}^{t-1}, D^t(c_{x,n}^t, c_{y,n}^t) - D^{t-1}(c_{x,n}^{t-1}, c_{y,n}^{t-1}))$\n       (4)"}, {"title": "3.2.4. Temporal Attention", "content": "The Temporal Attention is primarily responsible for extracting the temporal features from the context C, object O, and spatial G vectors. The objective of this task is to calculate the likelihood of an accident happening in each frame of a video sequence. To accomplish this, we combine these vectors and apply a hyperbolic tangent function. This is followed by multiplication with a weight matrix Wm and subsequent normalization with a softmax activation function, culminating in the generation of an attention matrix $W_a$ over the feature dimensions. The attention matrix $W_a$ plays a critical role in balancing the interrelationships between the context C, object O, and spatial G vectors, generating a focused feature fusion $W_f$. This process is expressed by the following equations:\n$W_a = softmax(tanh(I_c \\oplus O_o \\oplus G) \\odot W_m), W_f = (I_c \\oplus O_o \\oplus G) \\odot W_a$   (8)\nwhere $\\odot$ is the element-wise product operator. Following the attention mechanism, we feed $W_f$ into a Gated Recurrent Unit (GRU) for temporal processing. To mitigate overfitting during training, we incorporate a two-layer \u201cdropout-linear\u201d configuration, with dropout rates set to 0.5 and 0.1, respectively."}, {"title": "3.2.5. Smooth Module", "content": "It is critical to distinguish between genuinely hazardous situations and those that, while potentially dangerous, do not result in accidents. Notably, certain maneuvers by vehicles or pedestrians, although aggressive or risky, may not lead to accidents but still cause significant spikes in the model's predicted probability scores $s_t$, crossing predefined thresholds $s^\u00ba$ and erroneously flagging false positives.\nAs previously stated, it is important for this task to differentiate between hazardous situations and those that are potentially dangerous but do not result in accidents. Certain maneuvers by vehicles or pedestrians, although aggressive or risky, may not lead to accidents but can still cause significant spikes in the model's predicted probability scores $s_t$. This can result in false positives if the predefined thresholds $s^\u00ba$ are crossed. These instances, occurring within a brief time window (\u03b4 = $t_2$ - $t_1$), highlight the need for a refined analytical approach. To mitigate this, we introduce a sophisticated Smooth Module, as shown in Fig. 2, employing convolutional and deconvolutional operations across variable temporal scales. This technique allows for the downsampling of video data to unearth latent features F& within these critical intervals, subsequently upsampled to their original temporal resolution Fo. The application of this method over scales of 2, 5, and 10 seconds achieves comprehensive smoothing of feature volatility, effectively distinguishing between transient high-risk behaviors and actual precursors of accidents. The temporally smoothed features are then harmonized with the original input via a residual connection, at a mixing ratio of 1:0.15, further enhancing the model's ability to accurately interpret dynamic road scenarios without overfitting to sporadic high-risk maneuvers."}, {"title": "3.2.6. Accident Module", "content": "The Smooth Module refines the model's sensitivity to false triggers, while the Accident Module uses insights from the Temporal Attention Module to determine the occurrence of accidents in analyzed video sequences. This module employs a novel statistical attention mechanism, which differs from traditional queries by utilizing statistical pooling layers. These layers are adept at extracting key probabilistic features\u2014mean $S_h$, variance \u03c3($S_h$), maximum value max($S_h$), and range \u2206 = $S_h$ \u2013 $S_{h_{min}}$\u2014from the output probability scores of each video. By integrating statistical dimensions using a specialized attention framework, the module synthesizes a statistical weight matrix that reflects the nuanced interplay of predictive scores over time. This matrix informs a GRU and subsequent linear layers, resulting in a refined model capable of discerning the likelihood of accidents with greater precision and reliability."}, {"title": "3.3. Training", "content": "Our training loss function is divided into two main components: the probability score loss function $L_s$ and the prediction loss function $L_p$, which aim to optimize the model's output probability scores S and improve the accuracy of traffic accident detection, respectively.\nTraditionally, the loss in probability score is computed using a cross-entropy formula that aligns the output probability scores of positive and negative videos with their expected outcomes. These outcomes are close to 1 for accidents and close to 0 for non-accidents. However, this binary classification does not fully account for temporal variability and the gradual evolution of risk across video frames. To tackle the issue of temporal inconsistency in accident prediction, we present the Binary Adaptive Loss for Early Anticipation (BA-LEA) method, which is an extension of the AdaLEA framework [52]. The BA-LEA method adjusts the loss calculation by considering the temporal proximity to potential accidents, thereby enhancing the model's sensitivity to the unfolding dynamics within video sequences. It employs two distinct temporal coefficients, \u03bb\u2081 and \u03bb\u2082, for positive and negative instances, respectively, which are meticulously calibrated to enhance prediction accuracy by adjusting the penalty based on the temporal distance from the observed incident: For positive instances (indicative of accidents), $\u03bb_1 = e^{\\frac{-t}{max(T)}* f_1}$ is designed, where t represents the actual occurrence time of an accident, and $f_1$ serves as a decay factor. This coefficient increases the penalty for deviation from the expected probability score as the frame approaches the accident time, ensuring heightened model responsiveness during critical moments preceding an accident. Conversely, for negative instances (where no accident occurs), $\u03bb_2 = \\frac{t}{T}*f_2$ progressively amplifies the penalty for scores straying towards accident prediction as time advances, addressing the challenge of overestimating the likelihood of accidents in benign scenarios. This dual-coefficient strategy ensures a dynamic recalibration of the model's focus, prioritizing temporal segments closer to accident events in positive videos and enhancing vigilance over time in negative videos to avoid false alarms. This nuanced approach is encapsulated in the revised loss function, which can be defined as follows:\n$L_s = \\frac{1}{V} \\sum_{v=1}^{V} [ \\sum_{t=1}^{T} ( l_v * (- \u03bb_1 e^{-\\frac{t}{max(T)} f_1} log(s_t) ) - (1 - l_v) * ( \u03bb_2 \\frac{t}{T} f_2log(1 - s_t)) ]$      (9)\nwhere V represents the total number of videos, and $l_v$ denotes the label of the video, with $l_v = 1$ for positive videos and $l_v = 0$ for negative ones.\nComplementing $L_s$, the prediction loss function $L_p$ directly influences the model's accuracy in distinguishing between accident and non-accident scenarios. This is achieved by penalizing deviations from the true accident occurrence within the training dataset, encouraging the model to refine its predictive capabilities. Formally,\n$L_p = \\frac{1}{V} \\sum_{v=1}^{V} [ l_v log(l_p) - (1 - l_v) log(1 - l_p) ]$     (10)\nwhere $l_v$ is the prediction output from the Accident Module. In traditional approaches, the sum of the loss functions $L_s$ and $L_p$ is scaled by a coefficient to adjust their relative importance.\nThe fusion of $L_s$ and $L_p$ into a cohesive loss function poses a challenge, given the necessity to balance their contributions effectively. Inspired by multitask learning principles and the work of Kendall et al. [53], we adopt a multitask loss modulation strategy. This approach introduces uncertainty coefficients $\u03c3_1$ and $\u03c3_2$, allowing for the dynamic adjustment of each loss component's influence during the training process. The operations are formalized as:"}, {"title": "4. Experiment", "content": "where $\u03b3$ is a predetermined coefficients, with $\u03c3_1$ and $\u03c3_2$ initially set to 1. Both $\u03c3_1$ and $\u03c3_2$ could dynamically adjust the contribution of each task's loss during training. This loss function construction ensures accurate model predictions on a video level while also enabling early accident prediction on a frame level."}, {"title": "4.1. Experiment Setup", "content": "The study utilized Pytorch to implement the proposed method and conducted training and testing on an A40 48G GPU. Pre-trained models with features extracted by VGG-16 with a dimension of 4096 were used. Depth information from videos was extracted using the \u201cZoeD-M12-NK\u201d version of the ZoeDepth model. Regarding training parameters, the learning rate of the model is set to $1 \u00d7 10^{\u22124}$ with a batch size of 16. We use ReduceLROnPlateau as the learning rate scheduler to ensure that each model undergoes at least 50 training epochs. For the loss function parameters, the decay coefficients are set as $f1 = 20, f2 = 150$, and the proportional coefficient of the loss function $\u03b3$ is $1 \u00d7 10^{\u22123}$. Further specifics regarding the implementation and essential parameter settings of our model are provided as follows:"}, {"title": "Feature Extraction and Depth Enhancement", "content": "These two modules receive video V as input, with dimensions of (B, T, X, Y), where X and Y correspond to the width and height of the video in pixels. The feature extraction process generates a feature matrix with dimensions of (B, T, No, D), where B represents the batch size, T represents the total number of frames in the video, No is the sum of the number of target detection objects N and the overall image feature, thus $N_o = N + 1$, and D signifies the feature hidden dimension after feature extraction by VGG-16. The output of the depth enhancement process has the same dimensions (B, T, X, Y) as the original input video. In this implementation, N is set to 19, which is the maximum number of detected objects."}, {"title": "Context Attention and Object Attention", "content": "The matrix F undergoes a linear transformation that reduces the hidden dimension D to $D_h$. The input matrix is divided into object vector $H \u2208 (B,T,N, D)$ and context vector $O \u2208 (B, T, 1, D)$. Furthermore, H and O are then separately processed by Context Attention and Object Attention to obtain outputs $H_c$ and $O_c$ with dimensions (B,T, 1, D) and (B, T, 1, $D_o$), respectively. The input matrix is divided into object features H with dimensions (B, T, N, D) and context features O with dimensions (B, T, 1, D). Then, H and O are separately processed by Context Attention and Object Attention to obtain outputs $H_c$ and $O_c$ with dimensions (B, T, 1, D) and (B, T, 1, $D_o$), respectively. Additionally, a multi-head attention mechanism with 8 heads is employed in these components."}, {"title": "3D Collision Module", "content": "The input depth features with dimensions (B, T, X, Y) along with context features O are fed into the 3D Collision Module, resulting in an output $F_d$ with dimensions (B, T, 1, $D_d$). The $\u03bb_1$ and $\u03bb_2$ are set to 0.6 and 0.4, respectively."}, {"title": "Temporal Attention", "content": "The outputs from Context Attention, Object Attention, and the 3D Collision Module are concatenated into a mixed feature $F_m$ with dimensions (B, T, 1, $H_c + O_o + F_d$). In our implementation, H, O, and $F_d$ are set to 512, 256, and 256, respectively. $F_m$ is subsequently passed into the Temporal Attention module to produce an output with dimensions (B, T, 2). Within the Temporal Attention, $F_m$ is first fed into a GRU with a hidden dimension of HT = 512, yielding a hidden feature tensor of dimensions (B,T, $H_T$), which is then passed to a linear layer for dimension transformation. This output is then forwarded to both the Smooth Module and the Accident Module for further processing."}, {"title": "Accident Module", "content": "Similar to the Temporal Attention, the Accident Module takes $F_m$ as input and computes four statistical measures of the feature dimensions, concatenating them into a tensor of dimensions (B, T, 4). This tensor undergoes dimension transformation via linear layers, then is input into a GRU with hidden dimensions $H_A = 32$, and finally through a linear transformation to produce an output of dimensions (B, 2)."}, {"title": "4.2. Comparison to State-of-the-art (SOTA) Baselines", "content": "Our experiments on the DAD, CCD, and A3D datasets demonstrate the exceptional performance of our model on all three datasets, as shown in Table 1. These evaluation results indicate an inverse relationship between AP and mTTA, suggesting that a higher AP typically results in a shorter mTTA and vice versa, a trend evident from Fig. 3. The presented results demonstrate a balance between the two metrics, indicating that our model achieved the highest AP values across the datasets, outperforming the second-best by 2.6% on the DAD dataset. AccNet also surpasses every other model in terms of mTTA. Notably, UString is significantly outperformed by our model in AP by 7.1% on DAD and 1.9% on A3D, demonstrating superior overall performance.\nTo demonstrate the superiority of our model, we compared the best AP among the models. Since there were only small differences between the models on the CCD and A3D datasets, we primarily focused on the DAD dataset. Table 2 shows that AccNet achieved the highest AP value, along with the corresponding highest mTTA. Moreover, our model achieved the highest AUC, indicating that it not only effectively distinguishes accident videos but also ensures the highest accuracy. However, it is important to note that our TTA@80 and TTA@50 performance are slightly lower than that of DSTA, indicating that our model takes a more cautious approach in identifying the majority of positive samples. Notably, as comparing the best mTTA without considering AP does not provide meaningful insights, such a comparison is not included in our analysis."}, {"title": "4.3. Ablation Studies", "content": "To further validate the contribution of each component within our model, we conducted module ablation studies on the DAD dataset, the results of which are presented in Table 3. The results of these experiments underscore that the"}, {"title": "4.3.2. Ablation Studies of the 3D Collision Module", "content": "To determine if the depth information in the 3D Collision Module (3D-CM) significantly contributes to the model's prediction, we developed a comparative 2D Collision Module (2D-CM). The construction method for the 2D-CM is the same as the 3D-CM, except that it creates a two-dimensional graph representation. Both models were trained and evaluated using the first 30 epochs, with testing performed every half epoch, resulting in 60 test results. The rest of the model architecture and training hyperparameters were kept constant. To make a clearer comparison of the two models, scatter plots of AP vs. mTTA were used, due to the considerable variability during the training phase. Figure 3 demonstrates that the overall Average Precision (AP) for the 2D-CM is lower than that for the 3D-CM, with a significantly higher variance, indicating increased uncertainty throughout the training process. Experimental results suggest that, when balancing AP and mean Time to Accident (mTTA), the AP and mTTA for the 2D-CM are 54.3% and 3.46s, respectively, whereas those for the 3D-CM are 60.8% and 3.58s, marking improvements of 10.7% and 3.4%, respectively. Similarly, when comparing the highest AP values, the method using the 3D model also shows enhanced performance, with the highest AP for the 2D-CM at 67.8%, compared to 70.1% for the 3D-CM. The 2D-CM struggles to accurately reflect collision position information from the images due to visual parallax issues, leading to challenges in achieving model convergence during training and resulting in greater instability with the 2D-CM approach."}, {"title": "4.3.3. Ablation Studies of the Smooth Module", "content": "Multitask adaptive loss function. The Smooth Module in our study consists of three convolution-deconvolution (Conv-Deconv) pairs, each tasked with smoothing operations over different receptive fields. This design allows for"}, {"title": "4.4. Case Studies of the BA-LEA Loss and Qualitative Results", "content": "This study extends the classical loss function optimization from AlaLEA to BA-LEA. The BE-LEA loss depends on two critical components: the \u03bb1, \u03bb2 coefficients and the implementation of the multitask adaptive loss function."}, {"title": "5. Conclusion", "content": "In this study, we have introduced AccNet, an innovative traffic accident prediction model leveraging Monocular Depth-Enhanced 3D Modeling to precisely extract 3D coordinates of objects from images. This approach has enabled us to construct a novel topological structure for Graph Neural Networks, substantially enhancing the spatial dynamic understanding essential for analyzing dashboard camera footage. The introduction of a new Binary Adaptive Loss for Early Anticipation refines the model's focus on crucial moments, significantly enhancing its predictive accuracy. Furthermore, by employing a multi-task learning strategy, our model adeptly balances the weight between different loss functions, ensuring an optimized gradient descent path and stable training. Evaluated across three benchmark datasets\u2014DAD, CCD, and A3D\u2014AccNet has demonstrated superior performance, outpacing existing models in terms of both Average Precision and mean Time-To-Accident, affirming its exceptional capability in anticipating traffic accidents.\nThe implications of AccNet's performance are profound, particularly for the development of advanced driver-assistance systems (ADAS) and the future of autonomous driving technologies. By accurately predicting accidents before they occur, AccNet offers the potential to significantly reduce the number of traffic accidents, thereby saving lives and reducing injury severity. Its ability to understand spatial dynamics in real-world scenarios also opens up new avenues for research in visual perception and decision-making processes in autonomous systems.\nIn the future, the integration of AccNet with real-time vehicular systems presents an exciting avenue for enhancing road safety. Future research could explore the adaptation of AccNet to different environmental conditions and its performance in diverse scenarios, including varying weather conditions and urban vs. rural driving environments. Additionally, further refinement of the model to reduce computational requirements without compromising accuracy could facilitate its deployment in real-world applications. Exploring the incorporation of additional data sources, such as lidar and radar, may also enhance the model's robustness and reliability, paving the way for a new era of traffic safety solutions."}]}