{"title": "On Mitigating Code LLM Hallucinations with API Documentation", "authors": ["Nihal Jain", "Robert Kwiatkowski", "Baishakhi Ray", "Murali Krishna Ramanathan", "Varun Kumar"], "abstract": "In this study, we address the issue of API hallucinations in various software engineering contexts. We introduce \\textsc{CloudAPIBench}, a new benchmark designed to measure API hallucination occurrences. \\textsc{CloudAPIBench} also provides annotations for frequencies of API occurrences in the public domain, allowing us to study API hallucinations at various frequency levels. Our findings reveal that Code LLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58% valid low frequency API invocations. We demonstrate that Documentation Augmented Generation (DAG) significantly improves performance for low frequency APIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs when using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this, we propose to intelligently trigger DAG where we check against an API index or leverage Code LLMs' confidence scores to retrieve only when needed. We demonstrate that our proposed methods enhance the balance between low and high frequency API performance, resulting in more reliable API invocations (8.20% absolute improvement on \\textsc{CloudAPIBench} for GPT-4o).", "sections": [{"title": "Introduction", "content": "Programmers frequently utilize third-party Application Programming Interfaces (APIs) as foundational elements for new software development, particularly in domains like cloud services, web and mobile development, e-commerce, FinTech, and data analytics. These APIs offer essential functionalities, enabling developers to create robust and feature-rich applications efficiently.\nLarge Language Models for code generation (Code LLMs) are being increasingly used by programmers (Peng et al., 2023; Chen et al., 2021; Dakhel et al., 2023). However, these models can generate incorrect API-related code, known as API hallucinations (Liu et al., 2024), especially when under-trained on certain under-represented APIs \u2013 referred to as low frequency APIs (see Figure 1 (left)). This problem is exacerbated by the constant evolution of APIs, including frequent updates and deprecation of existing APIs (McDonnell et al., 2013). Consequently, new, updated, or infrequently used APIs are more prone to hallucinations. To systematically measure the prevalence of such hallucinations, we introduce \\textsc{CloudAPIBench}, a benchmark specifically designed to evaluate API hallucinations, focusing on APIs from major cloud service providers like Amazon Web Services (AWS) and Microsoft Azure.\nNext, we explore mitigation strategies for API hallucinations. When uncertain about API usage, human developers frequently rely on API documentation. Likewise, we hypothesize that Code LLMs should consult these resources under uncertainty. Hence, to address API hallucinations, we adopt retrieval augmented generation with documentation, i.e., Documentation Augmented Generation (DAG), which has shown early promise (Zhou et al., 2023; Patil et al., 2023).\nHowever, DAG may be unnecessary when APIs are stable or well-covered in the model's training data (i.e., high frequency APIs)\u2014we find that DAG with suboptimal retrievers indeed degrades performance for high frequency APIs, supporting the observation that LLMs are sensitive to irrelevant information (Shi et al., 2023a; Yoran et al., 2023). As such, we also present two simple yet effective strategies that can be easily adapted with DAG to address such pitfalls.\nFigure 1 (right) demonstrates how the frequency of an API's occurrence in the public domain affects Code LLMs. We analyze the perplexity of StarCoder2-15B (base model) (Lozhkov et al., 2024) on API tokens across two frequency groups: low (< 10 occurrences in training data: The Stack v2) and high (\u2265 100 occurrences), with detailed fre-\nquency descriptions in Section 2.2. The base model excels with high frequency APIs but struggles with low frequency ones. While DAG enhances performance for low frequency APIs, it compromises high frequency API performance due to occasional irrelevant augmentations from suboptimal retrievers: these distract the Code LLM's reliance on its internal knowledge, which is sufficient for high frequency APIs. To address DAG's limitations, we explore methods such as inspecting model confidence scores (Jiang et al., 2023; Varshney et al., 2023) and validating model generations against an API index before retrieval. These strategies effectively mitigate DAG's drawbacks, enhancing the reliability of Code LLMs.\nWe outline our key contributions and the structure of the paper as follows:\n\u2022 We introduce \\textsc{CloudAPIBench} to systematically study real-world API hallucinations; this benchmark evaluates API hallucinations across major cloud SDK APIs, i.e., those by AWS and Azure (Section 2).\n\u2022 We present a thorough study of DAG to enhance \\textsc{CloudAPIBench} performance, identifying the parts of documentation that reduce hallucinations and quantifying the impact of other retrieval components on model efficacy (Section 3).\n\u2022 We characterize scenarios where DAG may degrade performance and discuss selective retrieval methods to improve Code LLMs' utilization of documentation (Section 4).\nWe believe this is the first work to measure and characterize real-world API hallucinations for vari-"}, {"title": "API Hallucinations & CloudAPIBench", "content": "We first comment on the impact of API hallucinations in Section 2.1 and introduce \\textsc{CloudAPIBench} to measure these in Section 2.2."}, {"title": "Impact of API Hallucinations", "content": "Liu et al. (2024) identify that API hallucinations constitute up to 15% of all hallucinations in state-of-the-art Code LLMs, influencing cloud software engineering where code is API-intensive. These hallucinations can propagate errors, creating a snowball effect (Zhang et al., 2023b). For e.g., a hallucinated API call can lead to hallucinated handling of its response in subsequent code segments, compounding the problem (Ding et al., 2023a). Such incorrect API usage can also introduce security vulnerabilities, like improper data handling, which may lead to attacks or data breaches (Pearce et al., 2021). As adoption of Code LLMs grows, the cognitive burden on developers increases (Barke et al., 2022), as they must trace and rectify both the initial hallucination and all affected code segments. Given this severe impact of API hallucinations, it is critical to explore effective methods for their detection and mitigation, as we study in this work."}, {"title": "CloudAPIBench", "content": "Current benchmarks evaluate various programming skills such as problem solving (Chen et al., 2021; Austin et al., 2021), repository-level coding (Ding et al., 2023b; Liu et al., 2023), and tool-"}, {"title": "Documentation Augmented Generation (DAG)", "content": "In this section, we see how DAG enhances performance on \\textsc{CloudAPIBench}. We first outline the key components of DAG: augmentation design, retrieval index and retriever, in Section 3.1. Subsequently, we discuss how different design choices affect downstream performance in Section 3.2."}, {"title": "Setup", "content": "Overview. Following Zhang et al. (2023a); Jiang et al. (2023), we implement an iterative pipeline for DAG. Starting with a prompt, the Code LLM generates a hypothetical API invocation. This invocation forms a query to retrieve documentation for similar APIs. The retrieved documentation is processed"}, {"title": "Experiments & Results", "content": "In this section, we perform ablations on various DAG components to analyze their impact on API hallucinations.\nExperimental Setup. We present results from abla-"}, {"title": "Improving DAG: When to Retrieve?", "content": "Given that suboptimal retrievers can increase hallucination rates with DAG, we investigate strategies to address this issue. By triggering DAG selectively primarily when the Code LLM lacks knowledge of the target API \u2013 we can mitigate the negative impact of suboptimal retrievals, and allow the model to invoke APIs correctly using its internal knowledge. We discuss two strategies towards this here."}, {"title": "Index Lookup", "content": "Method. This simple technique verifies if the API name invoked during the first iteration generation exists in the API index. If not, the Code LLM is trying to call a non-existing API, and DAG provides the necessary references. Thus, DAG is not triggered for existing APIs; since this is likely to happen for high-frequency APIs, we expect fewer imprecise DAG triggers with this method.\nExperimental Setup. As before, we perform ablations with StarCoder2-3B. We use a 50% precision retriever to retrieve one documentation.\nResults. Table 2 presents the results. The index lookup method significantly reduces the regressions introduced by DAG for high-frequency APIs, even showing slight improvements over the base model. However, this gain comes at the expense of"}, {"title": "API Invocation Confidence", "content": "Background: LLM Calibration. Prior work has highlighted that LLM probabilities are well-calibrated, allowing for uncertainty estimation for various tasks (Kadavath et al., 2022; Si et al., 2023; Jiang et al., 2023; Li et al., 2023). As such, leveraging a Code LLM's probabilities to predict potential hallucinations, we could selectively trigger DAG for those scenarios.\nTo quantify a Code LLM's uncertainty during API invocation, we define API invocation confidence as the minimum probability among all predicted API name (sub-)tokens (see Figure 8). This minimum captures minor uncertainties in API prediction better than other aggregators like the mean (Varshney et al., 2023; Jiang et al., 2023).\nThe focus remains on the API name, not the entire invocation, as Code LLMs may show low confidence in tokens in the face of multiple alternatives (e.g., constants in API arguments, etc.; this represents aleatoric uncertainty (Yadkori et al., 2024)).\nEvidence from various Code LLMs, shown in Figure 9a, confirms their calibration for API invocation on \\textsc{CloudAPIBench}. A strong positive correlation is observed between API invocation confidence and correct API usage, indicating that confidence levels can preemptively identify likely hallucinations (i.e., they capture epistemic uncertainty (Yadkori et al., 2024)).\nMethod. We measure the API invocation confidence of the first iteration of generation, and if this is below a certain fixed threshold, indicating the model's lack of knowledge about the target API, we trigger DAG to assist the model.\nExperimental Setup. Towards finding an optimal"}, {"title": "DAG++ & Discussion", "content": "Having seen the benefits of the above approaches, we now discuss how DAG can be effectively improved by combining these, i.e., DAG++. In this"}, {"title": "Related Work", "content": "Program Synthesis & API Invocations. Code LLMs are actively being used for automatic program synthesis (Rozi\u00e8re et al., 2023; Guo et al., 2024). Relevant to our study is API invocation generation (Qin et al., 2023; Patil et al., 2023), often done on tool-usage benchmarks that do not account for the distribution of APIs in the public domain. We develop \\textsc{CloudAPIBench}, a benchmark targeting cloud-based software engineering scenarios, that includes API frequency annotations, allowing for nuanced failure analyses and targeted improvements through DAG. Works such as Zhou et al. (2023); Patil et al. (2023); Eghbali and Pradel (2024); Zan et al. (2023) also use documentation to improve API generation, but their evaluations do not capture the granularities discussed here.\nLLM Hallucinations. LLMs may generate factually incorrect statements about concepts, diminishing their utility (Mishra et al., 2024a; Kang et al., 2023; Lee et al., 2023). As such, several works have emerged to deal with this issue. Some works focus on hallucination detection by exploiting the well-calibrated nature of LLMs (Kadavath et al., 2022; Si et al., 2023; Li et al., 2023) and using model confidence scores (Jiang et al., 2023; Varshney et al., 2023). Closest to our work, Liu et al. (2024), give a taxonomy of hallucinations for code generation. While they focus on identifying hallucinations with Code LLMs, we focus on mitigating API hallucinations using documentation.\nRetrieval Augmented Generation (RAG). RAG supplements language models by retrieving from external data-stores (Asai et al., 2023a). Some studies use fixed algorithms for retrieval (Wang et al., 2023b; Shi et al., 2023b; Patil et al., 2023), while others adopt adaptive retrieval through special tokens (Asai et al., 2023b) or model confidence scores (Jiang et al., 2023). In this work, we establish how to use selective retrieval effectively to mitigate API hallucinations with documentation."}, {"title": "Conclusion & Future Work", "content": "In this work, we thoroughly investigate API hallucinations and demonstrate mitigation strategies for various Code LLMs. We introduce \\textsc{CloudAPIBench}, a benchmark to measure API hallucinations for diverse AWS and Azure APIs, including API frequencies to categorize low, medium, and high frequency APIs. We adapt RAG with documentation (DAG) to inform Code LLMs about the correct syntax during inference. We discuss which parts of documentation are important and how various retrieval components affect hallucinations. While DAG significantly enhances low-frequency API performance, it can degrade high-frequency API performance with irrelevant retrievals. We tackle this issue by selectively triggering retrievals through index lookup and API invocation confidence thresholding, and combine these methods in DAG++ leading to top performance on \\textsc{CloudAPIBench} across Code LLMs. Future research could extend \\textsc{CloudAPIBench} for long-context evaluations, explore DAG beyond iterative generation, and improve DAG by enhancing Code LLMs' robustness to irrelevant augmentations."}, {"title": "Limitations", "content": "Scope of \\textsc{CloudAPIBench}. \\textsc{CloudAPIBench} is a Python only benchmark containing short synthetic prompts to evaluate API hallucinations. While these represent various software-engineering scenarios, these might not represent all real-world cloud API invocations across different programming languages and contexts.\nConstruction of \\textsc{CloudAPIBench}. We create \\textsc{CloudAPIBench} using a multi-step process as discussed in Section 2.2 and Appendix A.2. Some of these steps are based on carefully crafted heuristics such as a customized logic to estimate API frequencies. Given that our findings are consistent with literature and also match our expectations, the impact of the approximations employed, if any, should be limited.\nIterative generations for DAG. In this work, we have adopted an iterative approach to DAG where we generate, retrieve and re-generate. Due to the overhead introduced by this iterative process, it may not be suitable for scenarios where latency is crucial."}, {"title": "Ethics Statement", "content": "Use of Generative AI. Code generation models are subject to ethical risks as these models can generate harmful content or content similar to their pre-training data. For real world applications, the generated content should ideally be reviewed by human developers and should be executed in sandbox environments. For the scope of experiments in this work, these risks are relatively low.\nCompute. Use of deep learning models is computationally expensive and raises environmental concerns. We have not trained any models as part of this work, so, the computational footprint is relatively low. All experiments for this paper were done using 4 NVIDIA A100 machines."}, {"title": "Supplementary Material: Appendices", "content": "1. Download The Stack v2. We download The Stack v2 from its official repository on HuggingFace and SoftwareHeritage (Lozhkov et al., 2024).\n2. Locate Documentation & Syntax. We use boto3 1.34.108 for AWS and the Python package azure-sdk-for-python for Azure. For AWS, we use the mypy_boto3_builder tool (YouType, 2024) to create API stubs for all AWS APIs; this helps us obtain the list of APIs. We obtain the official documentation for each of these by scraping the boto3 webpage for the respective APIs. For Azure, the complete docstring in the source code for an API's defintion is its documentation.\n3. Extract source specific code. We identify source specific code samples in The Stack v2 so that we restrict the count of API frequencies to only these. For AWS, source specific files are those that import one of {boto3, botocore} or contain one of {aws, boto, amazon} in the filepath. Similarly, Azure specific samples are those that import azure or contain azure in the filepath.\n4. Extract API specification. For Azure, the complete documentation is available as a docstring in the respective function definitions for that API. Using tree-sitter, we parse the code files to obtain the list of APIs, their correct usages and complete docstrings for Azure, for as many APIs as possible. For AWS, we parse API stubs obtained using mypy_boto3_builder to curate the API specifications. This also serves as the index of APIs that we use in our experiments.\n5. Measure API frequencies. Given the list of APIs for a source, we count the number of times functions with the name as an API are invoked or defined within the source specific code samples identified above. We use several heuristics to avoid edge cases and maintain reliability. Nevertheless, some noise may creep in and we acknowledge that this process is far from perfect. However, the findings based off of these API frequencies align with our expectations, indicating their reliability."}]}