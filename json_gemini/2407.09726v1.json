{"title": "On Mitigating Code LLM Hallucinations with API Documentation", "authors": ["Nihal Jain", "Robert Kwiatkowski", "Baishakhi Ray", "Murali Krishna Ramanathan", "Varun Kumar"], "abstract": "In this study, we address the issue of API hallucinations in various software engineering contexts. We introduce CloudAPIBench, a new benchmark designed to measure API hallucination occurrences. CloudAPIBench also provides annotations for frequencies of API occurrences in the public domain, allowing us to study API hallucinations at various frequency levels. Our findings reveal that Code LLMs struggle with low frequency APIs: for e.g., GPT-40 achieves only 38.58% valid low frequency API invocations. We demonstrate that Documentation Augmented Generation (DAG) significantly improves performance for low frequency APIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs when using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this, we propose to intelligently trigger DAG where we check against an API index or leverage Code LLMs' confidence scores to retrieve only when needed. We demonstrate that our proposed methods enhance the balance between low and high frequency API performance, resulting in more reliable API invocations (8.20% absolute improvement on CloudAPIBench for GPT-40).", "sections": [{"title": "1 Introduction", "content": "Programmers frequently utilize third-party Application Programming Interfaces (APIs) as foundational elements for new software development, particularly in domains like cloud services, web and mobile development, e-commerce, FinTech, and data analytics. These APIs offer essential functionalities, enabling developers to create robust and feature-rich applications efficiently.\nLarge Language Models for code generation (Code LLMs) are being increasingly used by programmers (Peng et al., 2023; Chen et al., 2021; Dakhel et al., 2023). However, these models can generate incorrect API-related code, known as API hallucinations (Liu et al., 2024), especially when under-trained on certain under-represented APIs \u2013 referred to as low frequency APIs (see Figure 1 (left)). This problem is exacerbated by the constant evolution of APIs, including frequent updates and deprecation of existing APIs (McDonnell et al., 2013). Consequently, new, updated, or infrequently used APIs are more prone to hallucinations. To systematically measure the prevalence of such hallucinations, we introduce CloudAPIBench, a benchmark specifically designed to evaluate API hallucinations, focusing on APIs from major cloud service providers like Amazon Web Services (AWS) and Microsoft Azure.\nNext, we explore mitigation strategies for API hallucinations. When uncertain about API usage, human developers frequently rely on API documentation. Likewise, we hypothesize that Code LLMs should consult these resources under uncertainty. Hence, to address API hallucinations, we adopt retrieval augmented generation with documentation, i.e., Documentation Augmented Generation (DAG), which has shown early promise (Zhou et al., 2023; Patil et al., 2023).\nHowever, DAG may be unnecessary when APIs are stable or well-covered in the model's training data (i.e., high frequency APIs)\u2014we find that DAG with suboptimal retrievers indeed degrades performance for high frequency APIs, supporting the observation that LLMs are sensitive to irrelevant information (Shi et al., 2023a; Yoran et al., 2023). As such, we also present two simple yet effective strategies that can be easily adapted with DAG to address such pitfalls."}, {"title": "2 API Hallucinations & CloudAPIBench", "content": "We first comment on the impact of API hallucinations in Section 2.1 and introduce CloudAPIBench to measure these in Section 2.2."}, {"title": "2.1 Impact of API Hallucinations", "content": "Liu et al. (2024) identify that API hallucinations constitute up to 15% of all hallucinations in state-of-the-art Code LLMs, influencing cloud software engineering where code is API-intensive. These hallucinations can propagate errors, creating a snowball effect (Zhang et al., 2023b). For e.g., a hallucinated API call can lead to hallucinated handling of its response in subsequent code segments, compounding the problem (Ding et al., 2023a). Such incorrect API usage can also introduce security vulnerabilities, like improper data handling, which may lead to attacks or data breaches (Pearce et al., 2021). As adoption of Code LLMs grows, the cognitive burden on developers increases (Barke et al., 2022), as they must trace and rectify both the initial hallucination and all affected code segments. Given this severe impact of API hallucinations, it is critical to explore effective methods for their detection and mitigation, as we study in this work."}, {"title": "2.2 CloudAPIBench", "content": "Current benchmarks evaluate various programming skills such as problem solving (Chen et al., 2021; Austin et al., 2021), repository-level coding (Ding et al., 2023b; Liu et al., 2023), and tool usage (Patil et al., 2023; Basu et al., 2024). However, a comprehensive benchmark for assessing real-world API hallucinations in software engineering remains absent. To address this, we introduce CloudAPIBench, a benchmark designed to evaluate Code LLMs' abilities to invoke cloud APIs.\nComposition. CloudAPIBench is a Python benchmark comprising 622 synthetic tasks to prevent data leakage with Code LLMs. Each task requires invoking a specific cloud API from providers like AWS and Azure, reflecting practical software engineering scenarios. Task prompts include imports, variable declarations, and a developer-style comment describing the API's purpose, stopping just before the API invocation. Figure 1 (left) illustrates a sample task and a model response (demonstrating an API hallucination). Figure 2 presents a detailed task distribution, showing that CloudAPIBench captures diverse API invocation scenarios to evaluate Code LLMs comprehensively.\nAPI Frequency Annotations. CloudAPIBench also contains the API frequency for APIs, i.e., how often they occur in The Stack v2 (Lozhkov et al., 2024). As The Stack v2 is one of the largest open code pre-training datasets, we assume that our API frequencies approximates the distribution of APIs in public sources. Hence, this can be used to explore the relationship between hallucination rates and API frequencies for various Code LLMs.\nTo enhance interpretability, we classify API frequencies into three categories: Low (0 \u2013 10 occurrences), Medium (11 \u2013 100), and High (\u2265 101). Since this treats APIs within the same class as identical, we minimize confounding factors (such as invocation complexity) by selecting diverse APIs within each class. This approach parallels the categorization of concepts based on popularity or pre-training frequencies (Razeghi et al., 2022; Mallen et al., 2022). To our knowledge, this is the first granular analysis of a Code LLM's pre-training corpus. Detailed API frequency distributions in CloudAPIBench are provided in Appendix A.1.\nConstruction. We construct CloudAPIBench with the goal of scaling coverage to multiple APIs from various providers. First, we source API specifications from official documentation to index their correct usage. Next, we determine each API's frequency in The Stack v2 by counting function definitions and calls with the same names as the APIs in relevant files. We select APIs for CloudAPIBench while ensuring diversity of invocation complexity and frequency. Using Claude 3 Sonnet (Anthropic, 2024), we convert API descriptions into developer-style comments, and create prompts with necessary imports, declarations, and a descriptive comment before the API call. We provide elaborate details of this process in Appendix A.2, and present more samples from CloudAPIBench in Appendix A.4.\nEvaluation Metric. We introduce the valid \u0391\u03a1\u0399 invocation metric, which verifies if an API is invoked according to its syntax. We obtain this syntax by tracking the API's arguments and whether they are required. The metric is computed as follows: we create a dummy function mirroring the API's signature (i.e., API stub (Zhu et al., 2023)). A candidate invocation is tested against this stub, and only successful bindings indicate validity. This evaluation method bypasses the intricacies of static analysis (Patil et al., 2023) and is more robust than string matching (Ding et al., 2023b), ensuring reliable and scalable evaluations. Figure 3 illustrates this process. See Appendix A.3 for more details."}, {"title": "2.3 Evaluation & Results", "content": "Models. We evaluate the following recent Code LLMs (and sizes) on CloudAPIBench: StarCoder2-{3B, 7B, 15B} (Lozhkov et al., 2024), DeepSeekCoder-{1.3B, 6.7B, 33B} (Guo et al., 2024), Google CodeGemma-{2B, 7B} (Team et al., 2024), IBM Granite-Code-{3B, 8B, 20B} (Mishra et al., 2024b) and GPT-40 (gpt-4o-2024-05-13) (OpenAI, 2024).\nInference. We use greedy decoding, generating one sequence per task up to 256 tokens, and post-process until the first function call; this is evaluated for validity as detailed in Section 2.2. This strategy is used throughout this work consistently. For instruction-tuned models, we specify a system prompt indicating the model to generate only the API invocation (see Appendix A.3).\nResults. Table 1 presents the performance of all models on CloudAPIBench and HumanEval (for a reference of generic performance). Key observations include:\n- API Hallucinations. All Code LLMs exhibit API hallucinations to a certain degree. These primarily occur due to (1) usage of non-existing APIs, (2) incorrect usage of the target API or, (3) usage of incorrect existing APIs. We illustrate these failure cases in Appendix A.5.\n- API Frequency Trends. A strong correlation exists between API frequency and valid API invocations: high frequency APIs yield fewer hallucinations, while low frequency APIs result in more. While this is expected, this trend verifies the applicability of our API frequency annotations.\n\u2013 Low Frequency APIs. Despite strong performance on high frequency APIs and generic benchmarks, all models exhibit high hallucination rates for low frequency APIs. This disparity highlights the value of CloudAPIBench in pinpointing scenarios where Code LLMs are prone to hallucinate. We dive deeper into various low frequency API failure cases for all models in Appendix A.6.\nGiven the poor performance on low frequency APIs, we now explore the use of documentation to enhance performance on CloudAPIBench."}, {"title": "3 Documentation Augmented Generation (DAG)", "content": "In this section, we see how DAG enhances performance on CloudAPIBench. We first outline the key components of DAG: augmentation design, retrieval index and retriever, in Section 3.1. Subsequently, we discuss how different design choices affect downstream performance in Section 3.2."}, {"title": "3.1 Setup", "content": "Overview. Following Zhang et al. (2023a); Jiang et al. (2023), we implement an iterative pipeline for DAG. Starting with a prompt, the Code LLM generates a hypothetical API invocation. This invocation forms a query to retrieve documentation for similar APIs. The retrieved documentation is processed and appended to the original prompt, after which inference is re-triggered. This process is illustrated in Figure 4.\nQuery Formulation & Retrieval Index. Given a CloudAPIBench task, the Code LLM generates a candidate API invocation, which we process as the query. This query focuses solely on API-relevant keywords, excluding any distractor prompt content (Jiang et al., 2023; Zhang et al., 2023a; Eghbali and Pradel, 2024). Our retrieval index includes all collected AWS and Azure APIs, identified using keys prepared similarly as the queries.\nRetriever. We develop a retriever with configurable precision to study the effect of retrieval accuracy on CloudAPIBench. For an $x\\%$ precision@k setting, we return k documents via BM25, ensuring that the target API's documentation is included $x\\%$ of the time. This approach allows us to examine the impact of varying retrieval precision (x). We chose BM25 for its simplicity (Patil et al., 2023; Cheng et al., 2023), though our results are likely robust to different retrievers.\nAugmentation Design. We prepend the retrieved documentation to the original prompt as a Python docstring after postprocessing. We test various augmentation strategies, each capturing different levels of API information and token count efficiencies: (1) API Name Only, (2) API Description, (3) \u0391\u03a1\u0399 Specification, (4) API Description + Specification, and (5) Full Documentation. Figure 5 shows \u201cAPI Specification\" while additional details and examples are in Appendix B.1."}, {"title": "3.2 Experiments & Results", "content": "In this section, we perform ablations on various DAG components to analyze their impact on API hallucinations.\nExperimental Setup. We present results from ablations on StarCoder2-3B. When testing a component (e.g., retriever precision), other components (e.g., number of retrievals) are held constant to isolate the effect. We also report the average valid API invocations across all tasks in CloudAPIBench for a concise performance measure, wherever indicated.\nAugmentation Design. Our objective is to determine the most useful and efficient information to retain from an API's documentation for augmentation. So, we use an Oracle retriever to fetch only one documentation; this guarantees that the relevant information is present somewhere in the documentation. Results are presented in Figure 6, showing valid API invocation rates and the number of tokens introduced per augmentation across all APIs. \u201cAPI Name Only\" and \"API Description\" do not significantly reduce hallucination rates, as they lack detailed API syntax. However, adding \u201cAPI Specification\u201d dramatically improves model performance (41.80% \u2192 86.82% on average), indicating that detailed API specifications are crucial. While \"Full Documentation\" optimizes performance, it is highly token-inefficient (685.24 tokens per augmentation). \u201cAPI Description + Specification\u201d strikes an optimal balance between token efficiency and performance, so, we adopt this design for all subsequent experiments.\nPrecision of Retriever. Results are displayed in Figure 7a. Here, we retrieve one document and vary the retriever's precision. As anticipated, the API hallucination rate decreases as retriever precision increases. Low frequency APIs show improvement over the base model even with low precision retrievers, while high frequency APIs require precision > 80% to match the base model's performance. Thus, at most precision levels, DAG induces higher hallucination rates for high frequency APIs compared to the base model (e.g., 84.39% \u2192 67.32% valid API invocations at 50% precision), underscoring Code LLMs' sensitivity to irrelevant augmentations (Shi et al., 2023a).\nNumber of Retrievals. Here we maintain the retriever precision at 50%. Figure 7b illustrates our findings. For low-frequency APIs, one or more retrievals consistently enhance performance. Conversely, high-frequency APIs show a sharp decline with one retrieval, partially recovered with two or more. This indicates that irrelevant augmentations can lead to unexpected behavior in Code LLMs, especially when a single augmentation conflicts with the model's internal knowledge.\nDiscussion. Our experiments above show that DAG significantly reduces hallucinations for low frequency APIs. However, high frequency APIs may suffer performance drops with DAG due to irrelevant retrievals. This issue can potentially be resolved by allowing the Code LLM to use its internal knowledge for high frequency APIs, bypassing DAG; this forms the core of the next section."}, {"title": "4 Improving DAG: When to Retrieve?", "content": "Given that suboptimal retrievers can increase hallucination rates with DAG, we investigate strategies to address this issue. By triggering DAG selectively primarily when the Code LLM lacks knowledge of the target API \u2013 we can mitigate the negative impact of suboptimal retrievals, and allow the model to invoke APIs correctly using its internal knowledge. We discuss two strategies towards this here."}, {"title": "4.1 Index Lookup", "content": "Method. This simple technique verifies if the API name invoked during the first iteration generation exists in the API index. If not, the Code LLM is trying to call a non-existing API, and DAG provides the necessary references. Thus, DAG is not triggered for existing APIs; since this is likely to happen for high-frequency APIs, we expect fewer imprecise DAG triggers with this method.\nExperimental Setup. As before, we perform ablations with StarCoder2-3B. We use a 50% precision retriever to retrieve one documentation.\nResults. Table 2 presents the results. The index lookup method significantly reduces the regressions introduced by DAG for high-frequency APIs, even showing slight improvements over the base model. However, this gain comes at the expense of reduced retrievals for low-frequency APIs: sometimes, the model invokes an existing incorrect API or incorrectly invokes the target API, leading to more hallucinations compared to DAG. Overall, this method shows promise for enhancing DAG."}, {"title": "4.2 API Invocation Confidence", "content": "Background: LLM Calibration. Prior work has highlighted that LLM probabilities are well-calibrated, allowing for uncertainty estimation for various tasks (Kadavath et al., 2022; Si et al., 2023; Jiang et al., 2023; Li et al., 2023). As such, leveraging a Code LLM's probabilities to predict potential hallucinations, we could selectively trigger DAG for those scenarios.\nTo quantify a Code LLM's uncertainty during API invocation, we define API invocation confidence as the minimum probability among all predicted API name (sub-)tokens (see Figure 8). This minimum captures minor uncertainties in API prediction better than other aggregators like the mean (Varshney et al., 2023; Jiang et al., 2023). The focus remains on the API name, not the entire invocation, as Code LLMs may show low confidence in tokens in the face of multiple alternatives (e.g., constants in API arguments, etc.; this represents aleatoric uncertainty (Yadkori et al., 2024)).\nEvidence from various Code LLMs, shown in Figure 9a, confirms their calibration for API invocation on CloudAPIBench. A strong positive correlation is observed between API invocation confidence and correct API usage, indicating that confidence levels can preemptively identify likely hallucinations (i.e., they capture epistemic uncertainty (Yadkori et al., 2024)).\nMethod. We measure the API invocation confidence of the first iteration of generation, and if this is below a certain fixed threshold, indicating the model's lack of knowledge about the target API, we trigger DAG to assist the model.\nExperimental Setup. Towards finding an optimal configuration, we vary the threshold of API invocation confidence below which to trigger DAG, and measure the API hallucination rate for StarCoder2-3B. As before, we use a 50% precision retriever with one retrieved document.\nResults. Figure 9b shows the relation between the confidence threshold and valid API invocations. As we raise the threshold, DAG is triggered more often, leading to a consistent reduction in hallucinations for low frequency APIs. Conversely, high frequency APIs remain largely unaffected until a certain point, beyond which irrelevant augmentations start causing hallucinations. The optimal threshold balances improved performance for low frequency APIs without significant regressions for high frequency APIs; for StarCoder2-3B, this optimal range is approximately 0.7 \u2013 0.8."}, {"title": "4.3 DAG++ & Discussion", "content": "Having seen the benefits of the above approaches, we now discuss how DAG can be effectively improved by combining these, i.e., DAG++. In this method, we trigger DAG iff the API in the first iteration of generation does not exist in the index OR is being invoked with an API invocation confidence below a fixed threshold. We anticipate that this would help combine the benefits of both the discussed approaches.\nExperimental Setup. We use a 50% precision retriever with one retrieval, consistent with previous experiments. Further, we fix the confidence threshold to be 0.8. Finally, to investigate the generalizability of our findings, we evaluate the largest models of all model families from Table 1.\nResults. The results are shown in Table 3. For each model, show how often retrieval is triggered and the resulting performance on CloudAPIBench. We make the following key observations:\nTrigger of Retrievals. We first examine how often retrieval is triggered with each method. The base model never triggers retrieval, DAG always does, and DAG++ selectively retrieves documentation. DAG++ exhibits a strong negative correlation between retrieval trigger frequency and API frequency: it triggers retrieval more often for low frequency APIs and less for high frequency APIs, aligning with the principle of retrieval only when necessary. For e.g., with GPT-40, DAG++ retrieves only 3.41% of the time for high frequency APIs indicating minimal need for documentation; conversely, retrieval is triggered 50.56% of the time for low frequency APIs, supplementing the model's limited knowledge with relevant documentation.\nDAG v/s DAG++. Table 3 also shows the performances (and absolute improvements over the base model in subscript) of various models on CloudAPIBench. As noted in Section 3, while DAG significantly boosts low frequency API performance, it degrades high frequency API performance. For instance, GPT-4o experiences a 39.02% drop in performance for high frequency APIs with DAG, highlighting the the model's sensitivity to irrelevant augmentations. DAG++ successfully mitigates this issue for high frequency APIs while maintaining or improving gains on low frequency APIs. Overall, DAG++ outperforms DAG indicating that selective retrieval of API documentation, that respects API frequency, aids performance on CloudAPIBench.\nGeneralizability. All model families demonstrate similar enhancement trends with DAG++, despite architectural and training differences. This underscores the generalizability of the importance of selectively retrieving API documentation when Code LLMs lack API specific knowledge. Additionally, scaling trends with model sizes (Kaplan et al., 2020; Wang et al., 2023a) are evident: average performance monotonically improves with model size in Table 3. Finally, DAG++ reveals that larger models require fewer retrievals for optimal performance, suggesting that they are more efficient at memorizing API syntax, even for low frequency APIs."}, {"title": "5 Related Work", "content": "Program Synthesis & API Invocations. Code LLMs are actively being used for automatic program synthesis (Rozi\u00e8re et al., 2023; Guo et al., 2024). Relevant to our study is API invocation generation (Qin et al., 2023; Patil et al., 2023), often done on tool-usage benchmarks that do not account for the distribution of APIs in the public domain. We develop CloudAPIBench, a benchmark targeting cloud-based software engineering scenarios, that includes API frequency annotations, allowing for nuanced failure analyses and targeted improvements through DAG. Works such as Zhou et al. (2023); Patil et al. (2023); Eghbali and Pradel (2024); Zan et al. (2023) also use documentation to improve API generation, but their evaluations do not capture the granularities discussed here.\nLLM Hallucinations. LLMs may generate factually incorrect statements about concepts, diminishing their utility (Mishra et al., 2024a; Kang et al., 2023; Lee et al., 2023). As such, several works have emerged to deal with this issue. Some works focus on hallucination detection by exploiting the well-calibrated nature of LLMs (Kadavath et al., 2022; Si et al., 2023; Li et al., 2023) and using model confidence scores (Jiang et al., 2023; Varshney et al., 2023). Closest to our work, Liu et al. (2024), give a taxonomy of hallucinations for code generation. While they focus on identifying hallucinations with Code LLMs, we focus on mitigating API hallucinations using documentation.\nRetrieval Augmented Generation (RAG). RAG supplements language models by retrieving from external data-stores (Asai et al., 2023a). Some studies use fixed algorithms for retrieval (Wang et al., 2023b; Shi et al., 2023b; Patil et al., 2023), while others adopt adaptive retrieval through special tokens (Asai et al., 2023b) or model confidence scores (Jiang et al., 2023). In this work, we establish how to use selective retrieval effectively to mitigate API hallucinations with documentation."}, {"title": "6 Conclusion & Future Work", "content": "In this work, we thoroughly investigate API hallucinations and demonstrate mitigation strategies for various Code LLMs. We introduce CloudAPIBench, a benchmark to measure API hallucinations for diverse AWS and Azure APIs, including API frequencies to categorize low, medium, and high frequency APIs. We adapt RAG with documentation (DAG) to inform Code LLMs about the correct syntax during inference. We discuss which parts of documentation are important and how various retrieval components affect hallucinations. While DAG significantly enhances low-frequency API performance, it can degrade high-frequency API performance with irrelevant retrievals. We tackle this issue by selectively triggering retrievals through index lookup and API invocation confidence thresholding, and combine these methods in DAG++ leading to top performance on CloudAPIBench across Code LLMs. Future research could extend CloudAPIBench for long-context evaluations, explore DAG beyond iterative generation, and improve DAG by enhancing Code LLMs' robustness to irrelevant augmentations."}, {"title": "7 Limitations", "content": "Scope of CloudAPIBench. CloudAPIBench is a Python only benchmark containing short synthetic prompts to evaluate API hallucinations. While these represent various software-engineering scenarios, these might not represent all real-world cloud API invocations across different programming languages and contexts.\nConstruction of CloudAPIBench. We create CloudAPIBench using a multi-step process as discussed in Section 2.2 and Appendix A.2. Some of these steps are based on carefully crafted heuristics such as a customized logic to estimate API frequencies. Given that our findings are consistent with literature and also match our expectations, the impact of the approximations employed, if any, should be limited.\nIterative generations for DAG. In this work, we have adopted an iterative approach to DAG where we generate, retrieve and re-generate. Due to the overhead introduced by this iterative process, it may not be suitable for scenarios where latency is crucial."}, {"title": "8 Ethics Statement", "content": "Use of Generative AI. Code generation models are subject to ethical risks as these models can generate harmful content or content similar to their pre-training data. For real world applications, the generated content should ideally be reviewed by human developers and should be executed in sandbox environments. For the scope of experiments in this work, these risks are relatively low.\nCompute. Use of deep learning models is computationally expensive and raises environmental concerns. We have not trained any models as part of this work, so, the computational footprint is relatively low. All experiments for this paper were done using 4 NVIDIA A100 machines."}, {"title": "Supplementary Material: Appendices", "content": "We give more details about the composition of CloudAPIBench here. CloudAPIBench comprises several AWS and Azure APIs, each annotated with an API frequency proportional to the API's representation in public sources. Figure 10 shows the API frequency distribution across three bins: low, medium and high, for both AWS and Azure."}, {"title": "A.2 Construction", "content": "We follow a multi-step procedure to obtain synthetic evaluation tasks in Python for AWS and Azure APIs to include in CloudAPIBench. A summary of the process is shown in Figure 11. We describe each step in detail here.\n1. Download The Stack v2. We download The Stack v2 from its official repository on HuggingFace and SoftwareHeritage (Lozhkov et al., 2024).\n2. Locate Documentation & Syntax. We use boto3 1.34.108 for AWS and the Python package azure-sdk-for-python for Azure. For AWS, we use the mypy_boto3_builder tool (YouType, 2024) to create API stubs for all AWS APIs; this helps us obtain the list of APIs. We obtain the official documentation for each of these by scraping the boto3 webpage for the respective APIs. For Azure, the complete docstring in the source code for an API's defintion is its documentation.\n3. Extract source specific code. We identify source specific code samples in The Stack v2 so that we restrict the count of API frequencies to only these. For AWS, source specific files are those that import one of {boto3, botocore} or contain one of {aws, boto, amazon} in the filepath. Similarly, Azure specific samples are those that import azure or contain azure in the filepath.\n4. Extract API specification. For Azure, the complete documentation is available as a docstring in the respective function definitions for that API. Using tree-sitter, we parse the code files to obtain the list of APIs, their correct usages and complete docstrings for Azure, for as many APIs as possible. For AWS, we parse API stubs obtained using mypy_boto3_builder to curate the API specifications. This also serves as the index of APIs that we use in our experiments.\n5. Measure API frequencies. Given the list of APIs for a source, we count the number of times functions with the name as an API are invoked or defined within the source specific code samples identified above. We use several heuristics to avoid edge cases and maintain reliability. Nevertheless, some noise may creep in and we acknowledge that this process is far from perfect. However, the findings based off of these API frequencies align with our expectations, indicating their reliability."}, {"title": "A.3 Evaluation", "content": "Metric Calculation. When more than one target API is identified, as described in Appendix A.2, we consider the candidate to be valid as long as it satisfies the syntax of any one of the target APIs.\nInference. While base models can be directly evaluated on CloudAPIBench, instruction-tuned models need to be instructed to generate an API invocation. We use a system prompt to achieve this; this is shown in Listing 1."}, {"title": "A.4 CloudAPIBench Samples", "content": "We show more samples from CloudAPIBench for illustration purposes here. Azure samples are shown in Figure 12 and AWS samples are shown in Figure 13. Each sample also shows the target API and the frequency classification of the API."}, {"title": "A.5 Hallucination Categorization & Illustration", "content": "We classify API hallucinations into three broad categories:\n1. Usage of incorrect existing API. This occurs when the Code LLM attempts to invoke an API that exists but does not fulfill the task (see Figure 14a).\n2. Invalid usage of target API. This occurs when the Code LLM attempts to invoke the correct API but does so incorrectly due to an invalid configuration of arguments; here the model may either pass arguments that the API does not accept or not pass a correct combination of required and optional arguments (see Figure 14b).\n3. Usage of non-existing API. This occurs when the Code LLM attempts to invoke an API that does not exist (see Figure 14c)."}, {"title": "A.6 Analyzing Low Frequency API Failures", "content": "We look closer into the various modes of failure for low frequency APIs in Table 4. We present this analysis for the largest model in each model family. As shown, most failures arise when the models try to invoke a non-existing API or use the target API incorrectly. This goes to show the lack of knowledge about low frequency APIs, and the propensity to hallucinate under these scenarios in Code LLMs."}, {"title": "B.1 Augmentation Designs", "content": "We define and illustrate various augmentation designs in this section.\n\u2022 API Name Only. We include only the name of the retrieved APIs as augmentation. This can test if the Code LLM can invoke the correct API just by referencing its name during inference.\n\u2022 API Description. We include the name and a short description of the API. For Azure the short description is the first sentence from the API's docstring, whereas for AWS the short description is the first 5 sentences from the API's documentation on the boto3 webpage. We choose 5 here as we found, in several cases, the first 2 3 sentences to be irrelevant to the API's description.\n\u2022 API Specification. This is a concise summary of the syntax of the API. It includes the name of the API and the list of required and optional arguments without specifying any descriptions of the arguments.\n\u2022 API Description + API Specification. This includes the description as defined above along with the specification as discussed above.\n\u2022 Full Docstring. This uses the entire collected documentation as augmentation. Since this can be arbitrarily large, especially for AWS documentation, we right-truncate the documentation up to 5000 characters before augmenting. This assumes that the necessary information to invoke the API is within the first 5000 characters."}]}