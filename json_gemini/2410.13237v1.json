{"title": "Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis", "authors": ["Yiyi Chen", "QiongXiu Li", "Russa Biswas", "Johannes Bjerva"], "abstract": "Language Confusion is a phenomenon where Large Language Models (LLMs) generate text that is neither in the desired language, nor in a contextually appropriate language. This phenomenon presents a critical challenge in text generation by LLMs, often appearing as erratic and unpredictable behavior. We hypothesize that there are linguistic regularities to this inherent vulnerability in LLMs and shed light on patterns of language confusion across LLMs. We introduce a novel metric, Language Confusion Entropy, designed to directly measure and quantify this confusion, based on language distributions informed by linguistic typology and lexical variation. Comprehensive comparisons with the Language Confusion Benchmark (Marchisio et al., 2024) confirm the effectiveness of our metric, revealing patterns of language confusion across LLMs. We further link language confusion to LLM security, and find patterns in the case of multilingual embedding inversion attacks. Our analysis demonstrates that linguistic typology offers theoretically grounded interpretation, and valuable insights into leveraging language similarities as a prior for LLM alignment and security.", "sections": [{"title": "Introduction", "content": "Multilingual Large Language Models (LLMs) revolutionized Natural Language Processing (NLP), offering crosslinguality in various applications, including translation (Zhu et al., 2024), text generation (Chen et al., 2022), and information retrieval (Guo et al., 2024). Besides the challenges faced by LLMs such as bias and fairness (Talat et al., 2022), hallucinations (Augenstein et al., 2024), multilingual LLMs are more vulnerable to adversarial and inversion attacks than monolingual LLMs (Song et al., 2024; Chen et al., 2024a,b).\nMultilingual LLMs are trained on a diverse range of linguistic data to represent the intricacies of multiple languages within a single model. However, this often results in inconsistencies in comprehension and response, leading to language confusion instances where LLMs generate text that is neither in the desired language nor in a contextually appropriate one. For example, when an LLM is queried/prompted in Arabic, it may respond in text that is either partially or entirely in languages other than Arabic, e.g., English.\nMarchisio et al. (2024) propose metrics to measure the percentage of model responses containing no undesired languages at both line and word levels but fail to capture nuances within language distributions. It is observed that language confusion tends to occur when the model's distribution over the next tokens is flat. We hypothesize that language confusion in LLMs is not merely a performance limitation but an inherent vulnerability, partly due to imbalanced multilingual data sources, which are amplified with increasing numbers of languages and can be analyzed through linguistic typology.\nTo thoroughly investigate language confusion as a phenomenon and its role within LLMs, we introduce the following research questions:\nRQ1: What measurable patterns characterize language confusion in LLMs, and how can these patterns be quantified effectively?\nRQ2: How do linguistic similarities influence language confusion, and how can this knowledge be applied to enhance LLM alignment and security?\nTo this end, we propose a novel metric called Language Confusion Entropy, which provides a quantifiable measure of uncertainty and facilitates the detection of when an LLM is confused. Building on observations by Marchisio et al. (2024) that uniformity of the distribution indicates higher uncertainty, Language Confusion Entropy re-weights language distributions by emphasizing long-tail distributions, effectively capturing language confusion"}, {"title": "Related Works", "content": "Language Confusion This phenomenon observed in NLP, is often described as \u201coff-target translation\" (Chen et al., 2023a; Sennrich et al., 2024) or \"accidental translation\" (Zhang et al., 2020; Xue, 2020), or as \u201csource language hallucinations\" in zero-shot transfer scenarios (Vu et al., 2022; Li and Murray, 2023; Pfeiffer et al., 2023; Chirkova and Nikoulina, 2024). Language confusion, a term coined by Marchisio et al. (2024) occurs when the LLMs' outputs are generated erroneously in languages different from the desired (target) languages and identified as 'surprising limitation' diminishing LLM utility for non-English languages, indicating the unpredictable nature. The phenomenon of language confusion has not only been pervasive in LLMs, but also in tasks pertinent to LLM security, such as multilingual inversion attacks (Chen et al., 2024b,a). Furthermore, Chen et al. (2024a) observes language confusion across 20 languages from diverse scripts and language families in multilingual embedding inversion. They analyzed the pattern of confusion using basic typological features between train and eval languages, however, the proposed Language Confusion Entropy provides more interpretable analysis.\nMultilingual LLM Safety and Security Yong et al. (2024) exposes vulnerabilities of AI safety mechanism by jailbreaking GPT-4's safeguard through translating unsafe English inputs into low-resource languages. Deng et al. (2024) impose unintentional and intentional jailbreak on multilingual LLMs, using multilingual prompts. It is observed that low-resource languages are more vulnerable, making them the weakest links in AI security.\nBackdoor attacks on multilingual machine translation pose significant threats, as injecting very less poisoned data into low-resource language pairs can achieve a high attack success rate (ASR) in high-resource language pairs (Wang et al., 2024). Poisoning instruction-tuning data for one or two languages can affect other languages, surpassing 99% ASR in the cross-lingual settings in prominent LLMs resisting current defenses (He et al., 2024). Multilingual textual embedding inversion attacks pose additional risks, as any encoder can be attacked to reconstruct original texts. Traditional defenses for monolingual LLMs are ineffective for multilingual LLMs (Chen et al., 2024b,a). Moreover, Song et al. (2024) generates language blending for adversarial attacks, necessitating systematic analysis of language similarity and language confusion for targeted defenses.\nLinguistic Typology and Language Similarities Previous research on multilingual effects on linguistic level uses three approaches: (i) phylogenetic variation, (ii) linguistic typological variation, and (iii) embedded and data-driven language variation. While genealogical relations are intuitive, the correlations between language similarity and genealogical relations are often spurious (Rama and Kolachina, 2012). Ploeger et al. (2024) challenge this approach highlighting its negative impact on downstream NLP tasks.\nLinguistic typology offers a theoretically grounded approach to measuring similarity between languages (Kashyap, 2019). Languages can be cat-"}, {"title": "Explainable Language Confusion", "content": "To investigate the phenomenon of language confusion, we use the datasets i) Language Confusion Benchmark (LCB) (Marchisio et al., 2024) and ii) Multilingual Textual Embedding Inversion (MTEI) (Chen et al., 2024a) (see Appendix for task details and Table 8 for processed datasets sample).\nGeneration Settings Consider a LLM trained or prompted with a set of $n \\in \\mathbb{N}$ languages $L_s = \\{l_1,..., l_n\\}$, and $l_t$ is the target language. We probe language confusion for both LLM instruction and textual embedding inversion attacks"}, {"title": "Quantifying Language Confusion", "content": "The phenomenon of language confusion is particularly prominent in crosslingual generation settings. For instance in Fig. 1 1, when an LLM is prompted in English and expected to generate a response in German ($X_1$), the output may unexpectedly have a mix of other languages, such as Spanish and French ($X_2$). Ideally, the LLM should focus on the expected languages; thus, the model exhibits greater confusion if its output distribution assigns high probabilities to unexpected languages. To quantify this, we propose Language Confusion Entropy ($H_c$), defined as follows:\n$H_c(X) = - \\sum_{x \\in X_1} (1 - p(x)) log(p(x)) - \\sum_{x \\in X_2} p (x) log p(x),$ (1)\nwhere $X_1$ denotes the expected language set and $X_2$ the unexpected language set, $X_1 \\cup X_2 = X$, $X_1 \\cap X_2 = 0$, $p(x)$ denotes the probability and $\\sum_{x \\in X} p(x) = 1$. Ideally, an unconfused model would satisfy $\\sum_{x \\in X_1} p(x) = 1$ and $\\sum_{x \\in X_2} p(x) = 0$."}, {"title": "The Role of Language Similarity in Language Confusion", "content": "We compare the language graphs with language confusion matrices to assess how well language confusion aligns with language similarities and to identify specific aspects where they match. To quantify the divergence between language confusion (denoted as $P$) and language similarity (denoted as $Q$), we employ Kullback-Leibler Divergence (Kullback and Leibler, 1951), expressed as $KL(P||Q)$. $P(x)$ represents the distribution of language confusion entropy of language $x$ relative to other languages, while $Q(x)$ represents the distribution of language similarity of $x$ relative to other languages. $KL(P||Q)$ is computed as follows:\n$KL(P||Q) = \\sum_{X} P(x)log(\\frac{P(x)}{Q(x)})$ (2)\nwhere a lower $KL(P||Q)$ indicates a stronger correspondence between language confusion patterns and underlying language similarities."}, {"title": "Analysis and Results", "content": "Language Confusion in LLM Prompting"}, {"title": "Language Confusion Entropy vs. Pass Rates", "content": "The binary metrics, Pass Rates at line-level LPR and word-level WPR are used to evaluate whether the LLM output contains no error, following Marchisio et al. (2024) (see details in Appendix A). We apply language confusion entropy to LCB, calculating it at both the line-level $H_c[L]$ and word-level $H_c[W]$ across generation settings.\nCompared to Marchisio et al. (2024), our approach detects language confusion across all languages, including at the word level, by using language-specific tokenizers and more accurate language identification (LID) tool. We reproduce LPR and WPR (ref. Table 10, 11) and compute $H_c[L]$ and $H_c[W]$ (ref. Table 9) in both crosslingual and monolingual settings, for 14 languages and 8 LLMs, following (Marchisio et al., 2024).\nTo evaluate the efficacy of language confusion entropy compared to pass rate metrics, we calculate the Spearman correlation coefficients between these metrics across levels and generation settings. Overall, $H_c[L]$ shows a strong negative correla-"}, {"title": "Language Confusion Entropy Across LLMS", "content": "As shown in Table 9 and Fig. 3, language confusion is more likely to occur in crosslingual compared to monolingual, with each LLM presenting significant variance. Word-level language confusion presents more variance per LLM and higher severity than line-level. Overall, it is consistent that Command and GPT LLMs have relatively lower language confusion than Mistral and LLaMA LLMs, projecting similar findings from Marchisio et al. (2024).\nThere is a clear consistency in language confusion across different LLMs, particularly at the line level and in crosslingual settings, as shown in Fig. 3. LLaMA 3 70B-I consistently exhibits the highest confusion across nearly all languages, while GPT-4 Turbo demonstrates the lowest confusion, especially"}, {"title": "Language Confusion Entropy Across Data Sources", "content": "The data for monolingual and crosslingual tasks in LCB consists of 2,600 and 4,500 prompts, respectively, sourced from 7 datasets (details in Table 4). As shown in Fig. 4, language confusion is more pronounced in crosslingual settings and at the word level. Monolingually, language confusion tends to align with the median word length (W) of prompts in each dataset. For example, Aya, Dolly, Okapi, and Native prompts have median lengths of 9, 10, 13, and 19 words, respectively, and their language confusion follows this order. Crosslingually, the Complex Prompts dataset has the highest median word length (159, compared to 18 for ShareGPT and 15 for Okapi), and it also exhibits the highest language confusion.\nObserving language confusion across datasets at the line level for crosslingual settings (Fig. 44), a clear pattern emerges. Complex Prompts has the highest confusion across all languages, while ShareGPT shows the lowest confusion for most languages, except for French and Spanish. Consistent with previous findings, languages written in non-Latin scripts show higher confusion in datasets like Okapi and ShareGPT. However, in Complex Prompts, non-Latin-script languages such as Chinese, Korean, Arabic, and Japanese demonstrate lower confusion than Latin-script languages."}, {"title": "Language Confusion in Multilingual Textual Embedding Inversion Security", "content": "Language Confusion Entropy for Eval Languages When embeddings are in languages that are more likely to be confused, they are more prone to being inverted into text in \"incorrect\" languages, reducing the inversion performance, especially with word-matching metrics like BLEU (Post, 2018). Also, the languages generated by the inversion model are often skewed by the pre-training data of the LLM, such as mT5 in \u039c\u03a4\u0395\u0399."}, {"title": "Language Confusion Entropy for Train Languages", "content": "In MTEI, the inversion models are trained in three different settings - monolingual, in-family and in-script, and cross-family and cross-script. As shown in Fig. 6 (bottom) and Table 13, monolingual training renders lower language confusion for each train language while pairing training languages, in-script/in-family training renders higher language confusion compared to cross-script/cross-family training. These findings substantiate the intuition that similar languages are more prone to confusion.\nOur study reveals that inversion performance significantly improves when trained in in-script/in-family settings(ref. Table 13 in Appendix). Crosslingual inversion performances are comparable to in-script/in-family training when trained in Kazakh (Latin-script) combined with Gujarati and Punjabi, respectively, and language confusion is notably lower. This suggests that while similar languages tend to increase confusion, certain crosslingual combinations can achieve strong performance without the added confusion seen in in-family/in-script training. Overall, these findings highlight the trade-off between inversion performance and language confusion indicating further optimization is needed to strike the ideal balance between them."}, {"title": "Language Confusion and Linguistic Typology", "content": "Table 3 shows the best results from KL divergence between language confusion and language similarities based on different language graphs for both LCB and MTEI, using Algorithm 1, the whole results are presented in 15 in Appendix.\nOur findings reveal strong correlations between language confusion and language similarities based on various typological sources. For instance, the similarity measures based on semantic typology correlate the most strongly, followed closely by more general lexical similarity measures. Language similarities based on typological feature databases like Grambank and WALS show stronger correlations than those based on parallel Bible texts (\u00d6stling and Tiedemann, 2017). Interestingly, and echoing previous findings on typological variation, we find genetic variation is a poor proxy for this analysis(Bjerva et al., 2019b; Ploeger et al., 2024), indicating the need for theoretically grounded approaches to linguistic interpretation."}, {"title": "Discussion", "content": "In response to RQ1, we proposed an effective metric Language Confusion Entropy, through which we identified several patterns contributing to language confusion. These include prompt complexity, imbalanced distributions of training sources, and similarities within language families all play a significant role in language confusion. Furthermore, our findings indicate that these factors strongly correlate with inversion performance and the pretraining languages in LLMs.\nIn response to RQ2, our findings confirm the hypothesis that language confusion is an inherent vulnerability of LLMs, and can be influenced and explained by linguistic typological variation. Thus, there is a potential to leverage language similarities as a prior for LLM alignment and security. For instance, our findings suggest that language confusion in LLMs could be reduced by incorporating languages with distinct semantic typological features. As observed in Table 3, ensuring that the model is exposed to languages with greater differences in colexification patterns may enhance its ability to distinguish between them. This strategy could promote more resilient LLMs, as we have shown that models are less likely to confuse typologically dissimilar languages. Hence, exploring typology-aware design strategies could provide both offensive and defensive insights in LLM security."}, {"title": "Conclusion and Future Work", "content": "Addressing the challenge of language confusion, we introduce Language Confusion Entropy, a novel metric that quantifies language confusion by re-weighting language distributions and emphasizing long-tail patterns. This metric captures language confusion in multilingual LLM tasks, revealing patterns of uncertainty in both training and evaluation phases. Our findings show strong correlations between language confusion and semantic similarities among languages, with less confusion observed in low-resource languages and when training incorporates diverse scripts and language families. These insights confirm that language confusion fundamentally impacts LLMs and suggest linguistic typology as a potential tool for enhancing model security. In future work, we aim to apply these findings to practical applications, such as developing typology-aware defense mechanisms and attack strategies to"}, {"title": "Limitations", "content": "A core limitation of this work is that our analysis and downstream implications can only be carried out on languages that are represented in typological databases. While these databases typically have high coverage, this still leaves some languages in the dark. When this work inspires downstream solutions in terms of defense mechanisms, undocumented languages may not benefit from these advances."}, {"title": "Ethics Statement", "content": "This work adheres to the ACL ethics guidelines. We investigate language confusion and link findings to security vulnerabilities of low-resource languages, including those using non-Latin scripts and with diverse typologies. Our work highlights how these factors can be used to potentially improve the security of low-resource language technology. We encourage the community to incorporate a broader range of languages in NLP security research, to ensure that low-resource languages are also covered by defense mechanisms developed in the future."}, {"title": "Datasets for Language Confusion", "content": "Langauge Confusion Benchmark Marchisio et al. (2024) create and release a language confusion benchmark covering 15 languages, sourcing prompts from publicly available multilingual instruction datasets, and also creating new data with more complex promts, see detailed data sources in Table 4.\nAdditionally, the binary metrics such as Line Pass Rate (LPR) and Word Pass Rate (WPR) are defined in Marchisio et al. (2024) to measure whether a response contains any instance of a) a line in an incorrect language and b) an isolated English word/phrase for languages using non-Latin scripts.\nLPR calculates the percentage of model responses that pass the line-level language confusion detector without error. A response is \u201ccorrect\" if all lines match the user's desired language.\n$LPR = \\frac{|R \\setminus E_L|}{|R|}$ (3)\nwhere R is the set of all responses and $E_L$ is the set of responses that contain line-level errors.\nWPR measures the percentage of responses where all words are in the desired language.\n$WPR = \\frac{|(R \\setminus E_L) \\setminus E_W|}{|R \\setminus E_L|}$ (4)\nwhere R is the set of all responses, $E_L$ is the set of responses with line-level errors, and $E_W$ the set of responses with word-level errors.\nWe reproduce the LPR and WPR on LCB for both crosslingual and monolingual settings (as shown in Table 10 and 11). The detailed results applying language confusion entropy to LCB are presented in Table 9, for comparison."}, {"title": "Language Graphs for Language Similarities", "content": "We curate language graphs from a diverse range of sources, as shown in Table 14. The language vectors from Grambank and WALS consist of multi-valued features, while those derived from colexification patterns in CLICS\u00b3 and WordNet (WN) are binarized. For these, we employ the Jaccard index to"}]}