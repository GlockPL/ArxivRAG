{"title": "Large-Scale 3D Medical Image Pre-training with Geometric Context Priors", "authors": ["Linshan Wu", "Jiaxin Zhuang", "Hao Chen"], "abstract": "The scarcity of annotations poses a significant challenge in medical image analysis, which demands extensive efforts from radiologists, especially for high-dimension 3D medical images. Large-scale pre-training has emerged as a promising label-efficient solution, owing to the utilization of large-scale data, large models, and advanced pre-training techniques. However, its development in medical images remains underexplored. The primary challenge lies in harnessing large-scale unlabeled data and learning high-level semantics without annotations. We observe that 3D medical images exhibit consistent geometric context, i.e., consistent geometric relations between different organs, which leads to a promising way for learning consistent representations. Motivated by this, we introduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage geometric context priors for self-supervision. Given an input volume, we extract base crops from different regions to construct positive and negative pairs for contrastive learning. Then we predict the contextual position of a random crop by contrasting its similarity to the base crops. In this way, VoCo implicitly encodes the inherent geometric context into model representations, facilitating high-level semantic learning without annotations. To assess effectiveness, we (1) introduce PreCT-160K, the largest medical image pre-training dataset to date, which comprises 160K Computed Tomography (CT) volumes covering diverse anatomic structures; (2) investigate scaling laws and propose guidelines for tailoring different model sizes to various medical tasks; (3) build a comprehensive benchmark encompassing 48 medical tasks, including segmentation, classification, registration, and vision-language. Extensive experiments highlight the superiority of VoCo, showcasing promising transferability to unseen modalities and datasets. VoCo notably enhances performance on datasets with limited labeled cases and significantly expedites fine-tuning convergence. Codes, datasets, and models are available at https://github.com/Luffy03/Large-Scale-Medical.", "sections": [{"title": "1 INTRODUCTION", "content": "AI-driven medical image analysis has witnessed emerg- ing development in recent years [2], [3], [4], [5], [6], [7], yet is heavily hampered by the high costs of the required expert annotations, especially for large-scale 3D medical im- ages that with volumetric information [8], [9], [10], [11]. To address this dilemma, Self-Supervised Learning (SSL) [12], [13], [14], [15], [16] for pre-training foundation models have demonstrated the potential to learn feature representations without the guidance from annotations, offering a promis- ing solution in addressing the annotation bottleneck in 3D medical image analysis [8], [9], [17], [18], [19].\nRecent advances [12], [20], [21], [22], [23], [24] have highlighted the critical elements contributing to the success of vision foundation models, i.e., large-scale data, large models, and advanced pre-training techniques. However, how well these solutions transfer to 3D medical image pre-training has not been thoroughly investigated. As shown in Fig. 1, (1) Data: previous methods [8], [9], [17], [18], [25], [26], [27], [28] are limited by the data scale (at most 10K vol- umes are used). Specifically, UniMiss [9], [27] innovatively proposed to boost chest CT pre-training by integrating 2D chest X-rays. However, the extendability to other anatomic regions remains under-explored. (2) Models: models trained in previous methods [8], [9], [17], [18], [25], [26], [27], [28] are still small-scale, with parameters only in the tens of millions. The scaling law of model capacity in medical image pre- training has not been well-explored. (3) Pre-training tech- niques: SuPreM [26] focused on supervised pre-training and annotated an abdomen segmentation dataset [29] for this purpose. Although showcasing state-of-the-art performance compared to previous methods, SuPreM [26] is still con- strained by the scale of labeled data and fails to incorporate large-scale unlabeled data from diverse anatomical regions. In SSL, the majority of existing approaches [8], [9], [10], [17], [19], [25], [30], [31] relied on low-level information reconstructions to learn augment-invariant representations, which typically employ data augmentation to the images and then reconstruct the raw information. However, the lack of high-level semantics in pre-training still impedes the performance of various downstream tasks.\nThe primary challenge is to incorporate high-level se- mantics for pre-training large-scale unlabeled data. We high- light that the geometric context priors of 3D medical images can be exploited. As illustrated in Fig. 2, we observe that in 3D medical images, different organs (semantic regions) exhibit relatively consistent geometric relations with simi- lar anatomic characteristics. Thus, the consistent geometric context between different organs offers a promising avenue for us to learn consistent semantic representations without the guidance of annotations in pre-training.\nIn this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework, aiming to leverage the geomet- ric context priors for contrastive learning. VoCo introduces"}, {"title": "2 RELATED WORK", "content": "Vision pre-training opens up immense opportunities for harnessing large-scale vision data, playing a pivotal role in"}, {"title": "2.1 Large-scale Vision Pre-training", "content": "Vision pre-training opens up immense opportunities for harnessing large-scale vision data, playing a pivotal role in"}, {"title": "2.2 Large-scale Medical Image Pre-training", "content": "Medical image pre-training has been proven as an effec- tive way to mitigate the scarcity of annotation in medical tasks [8], [9], [18], [26], [52], [53]. Early attempts [25], [54], [55] conducted pre-training on 2D X-ray images [56], [57], demonstrating improvements on chest pathology identifi- cation and pneumothorax segmentation. In comparison, 3D medical images, e.g., CT and Magnetic Resonance Imaging (MRI), offer richer volumetric information for clinical diag- nosis, which has received increasing attention in medical image analysis [10], [11], [16], [26], [48], [58]. Nonethe- less, the complexity inherent in 3D medical images intro- duces significant challenges to pre-training. Although recent works [8], [9], [17], [18], [19], [26], [28] have demonstrated the effectiveness of 3D medical image pre-training, signifi- cant challenges still persist, particularly in the realms of data scale, model capacity, and pre-training methods."}, {"title": "2.2.1 Large-scale Data", "content": "Compared with 2D X-ray, collecting 3D medical images like CTs is more difficult, stemming from factors such as slower imaging speeds, heightened radiation exposure, and increased costs [59], [60]. As shown in Fig. 1(b), most exist- ing methods [8], [17], [18], [19], [25], [26] leveraged limited scale of 3D data for pre-training. FreeTumor [61] first in- vestigated the data-scaling law in tumor segmentation with 11K CTs. Wang et al. [51] built a dataset of 100K CTs for pre- training but it is not publicly available for research. To col- lect large-scale 3D data for pre-training, the necessity arises to aggregate datasets from diverse sources, encompassing various hospitals across different regions and countries [29], [62]. This will lead to diverse image characteristics and inconsistent imaging quality in the dataset, introducing new challenges to pre-training.\nMoreover, previous methods mainly collected data from specific body parts for pre-training, e.g., PCRL [8], [25] and Unimiss [9], [27] on chest region, Alice [18] and SuPreM [26] on abdomen region, GVSL [28] on cardiac region. However, given the distinct characteristics present in various anatom- ical regions, the transferability of models pre-trained on one region to another may be constrained [1], [16], [49]. In this paper, we build a large-scale dataset PreCT-160K that encompasses diverse anatomic structures. However, data sourced from different anatomical regions exhibit vary- ing imaging parameters, i.e., different sizes, spacing, and intensities, posing new challenges for learning consistent representations in pre-training."}, {"title": "2.2.2 Large Model", "content": "Early works [25], [54], [55] in 3D medical image pre- training were constrained in model capacity, typically com- prising only tens of millions of parameters. Recent ad- vances [20], [21], [23], [44], [63] have demonstrated the astonishing effectiveness of scaling law, where large models trained on large-scale data exhibit remarkable intelligence. In this paper, we collect a large-scale 3D medical image dataset, which comprises diverse image characteristics from various sources. The availability of such extensive data unlocks new opportunities for us to train large models.\nGiven the diversity of various medical tasks, it is im- perative to evaluate large models on comprehensive bench- marks. Previous methods [8], [17], [18], [26], [27], [54], [55] primarily evaluated the pre-trained models on only a few downstream tasks, typically focusing on segmentation or classification tasks. STU-Net [64] was the first to evaluate large models yet is limited in segmentation tasks. In this pa- per, we delve deeper into the scaling law in various medical tasks, providing insights into tailoring diverse model sizes to accommodate varying medical tasks effectively."}, {"title": "2.2.3 Advanced Pre-training Techniques", "content": "SSL for 3D medical images. Existing methods [8], [10], [19], [30], [31], [65] are mostly based on information recon- structions to learn augment-invariant representations of 3D medical images, which first employ strong data augmenta- tion to the images and then reconstruct the raw informa- tion. Rotate-and-reconstruct [10], [17], [30], [66] proposed to randomly rotate the 3D volumetric images and learn to"}, {"title": "3 METHOD", "content": "The pivotal procedure is to generate position labels for self-supervision. We propose to leverage the inherent geo- metric context priors in 3D medical images. As illustrated in Fig. 3, given an input volume V, we first randomly crop a sub-volume k, with the objective of constructing positive and negative pairs with k for contrastive learning. Specifically, we propose to employ position encoding to generate n non-overlap base crops $q_i, i \\in n$. For example, n = 4\u00d74 base crops are generated in Fig. 3, where each base crop $q_i$ represents a distinct region of the input volume.\nWithin human body anatomy, various organs are situ- ated in distinct regions, leading to a potential way for us to form positive and negative pairs. As shown in Fig. 3, the random crop k and the positive base crops $q_{pos}$ exhibit overlap areas, whereas the negative base crops $q_{neg}$, lacking such overlaps, more likely encompass different organs (not absolutely). For example, in Fig. 3, k and $q_{pos}$ both contain stomach, pancreas, vein, aorta, and cava, while k and $q_{neg}$ exhibit different organ information. Thus, we can employ the position encoding to construct positive and negative pairs for contrastive learning.\nPrevious contrastive learning methods [14], [15], [38], [43] mainly employ InfoNCE loss [78] to maximize the mutual information of positive pairs. In this paper, we propose to generate labels with specific values to supervise the correlation extent of positive pairs, i.e., with labels to reflect how similar between k and $q_{pos}$. It can be observed that the correlation between k and $q_{pos}$ is associated with their overlap proportions. Intuitively, if a positive base crop $q_{pos}$ shares more overlap areas with k, this $q_{pos}$ would be more similar with k. Thus, as shown in Fig. 3, we propose to assign the overlap proportions as the values of position labels y, enabling us to measure the similarity"}, {"title": "3.1 Generate Position Labels for Supervision", "content": "The pivotal procedure is to generate position labels for self-supervision. We propose to leverage the inherent geo- metric context priors in 3D medical images. As illustrated in Fig. 3, given an input volume V, we first randomly crop a sub-volume k, with the objective of constructing positive and negative pairs with k for contrastive learning. Specifically, we propose to employ position encoding to generate n non-overlap base crops $q_i, i \\in n$. For example, n = 4\u00d74 base crops are generated in Fig. 3, where each base crop $q_i$ represents a distinct region of the input volume.\nWithin human body anatomy, various organs are situ- ated in distinct regions, leading to a potential way for us to form positive and negative pairs. As shown in Fig. 3, the random crop k and the positive base crops $q_{pos}$ exhibit overlap areas, whereas the negative base crops $q_{neg}$, lacking such overlaps, more likely encompass different organs (not absolutely). For example, in Fig. 3, k and $q_{pos}$ both contain stomach, pancreas, vein, aorta, and cava, while k and $q_{neg}$ exhibit different organ information. Thus, we can employ the position encoding to construct positive and negative pairs for contrastive learning.\nPrevious contrastive learning methods [14], [15], [38], [43] mainly employ InfoNCE loss [78] to maximize the mutual information of positive pairs. In this paper, we propose to generate labels with specific values to supervise the correlation extent of positive pairs, i.e., with labels to reflect how similar between k and $q_{pos}$. It can be observed that the correlation between k and $q_{pos}$ is associated with their overlap proportions. Intuitively, if a positive base crop $q_{pos}$ shares more overlap areas with k, this $q_{pos}$ would be more similar with k. Thus, as shown in Fig. 3, we propose to assign the overlap proportions as the values of position labels y, enabling us to measure the similarity"}, {"title": "3.2 Volume Contrast for Contextual Position Prediction", "content": "The overall framework of VoCo is present in Fig. 4. Specifically, we propose a novel pretext task, i.e., contextual position prediction, which employs volume contrast to pre- dict the contextual positions of a random crop k. This pretext task includes: (1) intra-volume contrast among k, $q_{pos}$, and $q_{neg}$, where k, $q_{pos}$, and $q_{neg}$ are from the same volume; (2) inter-volume contrast between different volumes $V_A$ and $V_B$, which is established by consistency regularization with a typical student-teacher module [14], [15], [38], [43].\nContextual position prediction. As shown in Fig. 4(a), given an input volume, we first extract a random crop k and a group of base crops q, where the corresponding position labels Yi for $q_i$ are generated as Sec. 3.1 and Fig. 3. Then we feed k and q into the model to extract high-dimension features. After extracting the features, we employ a typi- cal momentum-based student-teacher module [15], [38] to project k and q separately. Specifically, the teacher projector Pt is frozen during training, where its parameters $\\theta_t$ are updated from the parameters $\\theta_s$ of the student projector ps by Exponential Moving Average (EMA):\n$\\theta_t = \\rho \\theta_t + (1 - \\rho) \\theta_s,$\\n(1)\nwhere \u03c1 is the momentum factor and is empirically set to 0.9. The momentum-based student-teacher module is effective in contrastive learning [15], [38], which enables stable training and avoids feature collapse [14], [34], [43].\nWith features extracted from the projectors, we conduct 3D adaptive average pooling to resize k and q as one dimension features, i.e., $k \\in \\mathbb{R}^{1 \\times C}$ and $q \\in \\mathbb{R}^{1 \\times C}$, where C is"}, {"title": "3.3 Towards Omni-supervised Pre-training", "content": "As shown in Fig. 5, both fully- and self-supervised learning have specific merits and drawbacks. (a) Fully- supervised learning can learn discriminative decision boundaries with the guidance of labels yet it is constrained by the lack of labeled data. (b) SSL can leverage large-scale unlabeled data. However, lacking annotations for supervi- sion, it struggles with learning clear decision boundaries between distinct classes. To this end, we propose omni- supervised pre-training to effectively leverage both labeled and unlabeled data, as described in Algorithm 1. Our omni- supervised learning amalgamates the strengths of both fully- and self-supervised learning and effectively unleashes the potential of labeled and unlabeled data.\nCurating labeled segmentation dataset XL, YL. The PreCT-160K dataset includes extensive labeled segmenta- tion datasets. However, many of these datasets have in-complete labels [2], [6], [29], such as one dataset containing"}, {"title": "4 EXPERIMENTS", "content": "Pre-training dataset1. In this paper, we curate the exist- ing largest dataset medical image pre-training PreCT-160K, as shown in Table 1. PreCT-160K is collected from diverse sources and underwent thorough pre-processing to ensure a consistent data format for training. Specifically, to address variations in sizes, spacing, and intensity across volumes from different anatomical regions, we have devised tailored pre-processing protocols. Since in PreCT-160K, data from chest regions cover larger proportions, we simply balance the sampling during pre-training. For VoComni dataset2, we use model ensembling to generate pseudo labels, where we discard the volumes with low prediction confidence. Conse- quently, we have created a segmentation dataset comprising 20K pseudo-labeled volumes (encompassing 20 organ and tumor classes) for our omni-supervised pre-training.\nEvaluation benchmark. We build a large-scale evalua- tion benchmark as shown in Table 2, which includes 48 downstream datasets for various tasks. It can seen in Table 3 that our evaluation benchmark is more comprehensive than that of previous works [8], [9], [17], [18], [19], [26], [28]. A number of datasets [2], [84], [90], [95] are evaluated on the public leaderboards. If the test sets and public leaderboards are not available, we report the offline val sets results with the same data splits for fair comparisons.\nExperiment settings. We first conduct pre-training on PreCT-160K then finetune the pre-trained models on 48 downstream datasets (Table 2) for evaluation. We adopt both SwinUNETR [129] and nnUNet [48] as the backbones"}, {"title": "4.1 Dataset and Implementation Details", "content": "Pre-training dataset1. In this paper, we curate the exist- ing largest dataset medical image pre-training PreCT-160K, as shown in Table 1. PreCT-160K is collected from diverse sources and underwent thorough pre-processing to ensure a consistent data format for training. Specifically, to address variations in sizes, spacing, and intensity across volumes from different anatomical regions, we have devised tailored pre-processing protocols. Since in PreCT-160K, data from chest regions cover larger proportions, we simply balance the sampling during pre-training. For VoComni dataset2, we use model ensembling to generate pseudo labels, where we discard the volumes with low prediction confidence. Conse- quently, we have created a segmentation dataset comprising 20K pseudo-labeled volumes (encompassing 20 organ and tumor classes) for our omni-supervised pre-training.\nEvaluation benchmark. We build a large-scale evalua- tion benchmark as shown in Table 2, which includes 48 downstream datasets for various tasks. It can seen in Table 3 that our evaluation benchmark is more comprehensive than that of previous works [8], [9], [17], [18], [19], [26], [28]. A number of datasets [2], [84], [90], [95] are evaluated on the public leaderboards. If the test sets and public leaderboards are not available, we report the offline val sets results with the same data splits for fair comparisons.\nExperiment settings. We first conduct pre-training on PreCT-160K then finetune the pre-trained models on 48 downstream datasets (Table 2) for evaluation. We adopt both SwinUNETR [129] and nnUNet [48] as the backbones"}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "We perform in-depth comparisons with previous meth- ods [8], [9], [17], [26], [27], [48], [54], [66], [68], [69] that have released their codes and checkpoints. Note that in instances where certain datasets necessitate extensive com- putational resources or involve limited cases, we exclusively report the results of methods [17], [26], [48] that with better performances. Our evaluations span across segmentation, classification, registration, and vision-language tasks. In fol- lowing discussions, the term baseline denotes adopting the same backbones but without pre-training (from scratch)."}, {"title": "4.2.1 Medical Image Segmentation", "content": "Seven widely-used segmentation datasets. As shown in Table 4, on seven widely-used segmentation datasets, VoCo demonstrates leading performances and surpass previous methods [9], [17], [26], [27], [48], [54], [68], [69] by a clear margin. It can be seen that the general method MoCo- v3 [15], [38] did not perform well on medical tasks. Since MoCo v3 [15], [38] heavily relies on a large batch size to acquire adequate negative samples, which is not feasible in 3D medical images. Moreover, the negative relation between different images used in MoCo v3 [15], [38] is not appropri- ate in medical images.\nNotably, VoCo outperforms the baseline by average 3.12% DSC. SuPreM [26] achieved the best results among the previous pre-training methods since it used an ab- domen dataset [29] for supervised pre-training and the datasets in Table 4 are almost abdomen datasets. VoCo sur- passes SuPreM [26] and achieves new state-of-the-art per- formances. Specifically, for the challenging ToTalSegmen- tor [88] dataset, VoCo (Swin-H) outperforms SuPreM [26] by 3.22% DSC. The overall results in Table 4 vividly under- score the effectiveness of our method.\n24 organ/tumor segmentation tasks. As shown in Ta- ble 5, we report the results on 24 organ and tumor segmen- tation datasets, across different modalities and anatomical regions as shown in Table 2. Notably, models with VoCo pre- training outperform those without pre-training by average 4.42% DSC. It is worth noting that a majority of these datasets contain fewer than 50 annotated cases for fine- tuning, highlighting the effectiveness of pre-training as a label-efficient solution. The overall improvements observed across these 24 datasets serve as compelling evidence for the efficacy of our proposed large-scale pre-training method.\nMSD Challenge. Table 6 reports the results on the MSD 10-Task [89] dataset. We adopt the settings of nnUNet [48] for fair comparisons. Notably, with VoCo pre-training, the segmentation DSC is improved by average 2.98%."}, {"title": "4.2.2 Medical Image Classification", "content": "The medical image classification results on CC- CCII [121] and LUNA16 [83] are shown in Table 7. Given the near-optimal accuracy of lung nodule detection on LUNA16 [83], the benefits of pre-training are not obvious. For Covid classification on CC-CCII [121], VoCo outper- forms the baseline by 2.76% and SuPreM [26] by 1.97%. Notably, SuPrem [26] conducted supervised segmentation pre-training on only abdomen datasets, potentially limiting its transferability to chest classification tasks."}, {"title": "4.2.3 Medical Image Registration", "content": "The medical image registration results on IXI [126] and OASIS [127] datasets are shown in Table 8. We adopt TransMorph [134] as the baseline. Note that in this paper we focus on evaluating the effectiveness of pre-training, thus we did not propose new registration algorithms. Thus, our registration analyses emphasize backbone comparisons (scratch versus pre-trained). We find that previous pre- training methods [17], [26] did not perform well on registra- tion. While on brain MRI registration dataset OASIS [127], VoCo brings 2.6% DSC improvements, which is a non- trivial improvement in registration."}, {"title": "4.2.4 Vision-Language Analysis", "content": "As shown in Table 3, this study pioneers the assessment of medical image pre-training efficacy in Vision-Language (VL) tasks. Specifically, we evaluate the report generation task on CTRG-Chest [142] and extend the evaluation to vocabulary classification and report-volume retrieval on the CT-RATE [105] dataset. The results are shown in Tables 9 and 10. Note that in this paper we focus on medical image pre-training, thus we verify the effectiveness via replacing the vision encoders. For the language models, we maintain the original settings from M2KT [142] and CT-CLIP [105] for CTRG-Chest and CT-RATE, respectively.\nVoCo attains superior performances compared to pre- vious medical image pre-training methods [8], [17], [26], [55]. Specifically, for report generation in Table 9, VoCo (Swin-H) achieves the highest score BLEU-4 (37.91%). In Ta- ble 10, VoCo (Swin-H) achieves 73.69% AUC in vocabulary classification and 24.12% recall in report-volume retrieval. Although SuPreM [26] performs well in abdomen segmenta- tion datasets, it falls short in enhancing chest VL tasks. The results from VoCo underscore the significance of a robust vision encoder in VL tasks, which can provide more precise visual information for language models."}, {"title": "4.2.5 Discussion", "content": "Overall improvements. As shown in Fig. 6, with the same backbone, VoCo outperforms the baseline (from scratch) by a clear margin. SuPreM [26] emerged as the top performer among previous pre-training methods [8], [9], [17], [27], [48], [54], [55], [66], [68], [69]. Specifically, VoCo surpasses SuPreM [26] by an average of 2.93%, 3.72%, 2.57%, 2.18%, 3.52%, and 2.72% on 24 organ segmentation datasets, 14 tumor segmentation datasets, 15 chest analysis datasets, 28 unseen datasets, 13 cross-modal datasets, and"}, {"title": "4.3 Scaling Law in Medical Image Analysis", "content": "Are larger models always better? In medical tasks, the answer appears to be no. It can be observed from Fig. 8 that for some specific tasks, models with smaller sizes can achieve better performances. In this paper, we delves into factors affecting the model capacity scaling law, including: number of finetuning cases, data diversity, and task difficulties.\nAs shown in Fig. 8, (a) TotalSegmentator [88] is a chal- lenging dataset, containing 1.2K cases and 104 classes for segmentation. In this case, the largest model VoCo-H yields the best results. (b) BTCV [81] is with only 24 cases for finetuning, potentially leading larger models to overfit on limited data, thus hindering validation performance. (c) Al- though CC-CCII [121] encompasses 4.2K cases for training, it is a simple binary classification task (over 90% accuracy), suggesting that excessively large models may not be neces- sary. (d) OASIS [127] is brain MRI datasets with only 0.4K cases for registration and it also lacks significant structural diversity. In this case, the smallest VoCo-B delivers the best results. (e) CT-Rate [105] is with 50K cases for 18 classes vocabulary classification. Given large-scale data for training, larger models demonstrate higher performances.\nTailor different model sizes to various medical tasks. Drawing from experimental insights discussed above, we"}, {"title": "4.4 Ablation Studies", "content": "Our preliminary investigation VoCo-v1 [1] has provided fundamental ablation studies, focusing on exploring various loss functions and hyperparameter configurations. Com- pared with VoCo-v1 [1], we further evaluate the effective- ness of volume contrast, omni-supervised learning, and data scaling from 10K to 160K. We use Swin-B [129] as the backbone and present the results on diverse datasets, including TotalSegmentator [88], BTCV [81], CC-CCII [121], OASIS [127], and CTRG [142], across segmentation, classifi- cation, registration, and vision-language tasks.\nVolume contrast. As shown in Table 11, inter-volume contrast consistently enhances performance across five datasets. The combination of intra- and inter-volume con- trast can yield higher improvements compared with the randomly initialized backbone [129].\nOmni-supervised pre-training. As shown in Table 11, semi-supervised learning can effectively improve the per- formances. Specifically, for TotalSegmentator [88], it leads to substantial DSC improvements from 82.07% to 84.84%, which is a non-trivial boost in this challenging segmentation dataset. It is worth noting that the pure semi-supervised pre- training can achieve competitive results on segmentation tasks [81], [88], but it does not improve significantly in classification, registration, and VL tasks. Combined with self- and semi-supervised learning, the omni-supervised pre-training can achieve the best performances.\nData scaling law in medical image pre-training. We scale up the pre-training data (Table 1) from 10K to 160K and present the findings on the TotalSegmentator [88] dataset in Fig. 9, showcasing the impact of expanding the pre-training dataset. Notably, the enhancements from 10K to 160K ap- pear marginal. This phenomenon could be attributed to factors like data quality and diversity, network scalability, or nearing the upper limit of improvement."}, {"title": "4.5 Qualitative Visualization Results", "content": "Contextual Position Prediction. As shown in Fig. 10, we present some visualization results of contextual position prediction. The loss for contextual position prediction con- verges steadily during pre-training. The position predictions generated by VoCo closely align with the ground truth, underscoring the efficacy of our proposed pretext task.\nQualitative segmentation results. We present some vi- sualization results in Fig. 11, which covers different anatom- ical regions. The visualization results demonstrate that our method can broadly apply to various downstream tasks."}, {"title": "5 LIMITATIONS AND FUTURE DIRECTIONS", "content": "Although our pre-training method has demonstrated promising results across various medical tasks, there are still several limitations that can be further explored in the future:\nData engines for improving data quality. The im- provements become marginal when scaling the data from 10K to 160K. Although we have curated and pre-processed the pre-training dataset, the PreCT-160K still"}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed a simple-yet-effective Volume Contrast (VoCo) framework for large-scale 3D medical im- age pre-training. Inspired by the consistent geometric re- lation between different organs, we propose to leverage the geometric context priors to learn consistent semantic representations for SSL. VoCo can also be seamlessly inte- grated into a semi-supervised learning framework for omni- supervised pre-training. To facilitate the study of large- scale 3D medical image pre-training, we curated the exist- ing largest medical image pre-training dataset PreCT-160K, which encompasses 160K CT volumes (42M slices) covering diverse anatomical structures. We further delve into the scaling law of model capacity and propose the guidelines for tailoring different model sizes to various medical tasks. To evaluate the effectiveness of pre-training, we establish a comprehensive evaluation benchmark encompassing 48 downstream datasets across various tasks. Extensive ex- periments highlighted the superior performances of VoCo compared with previous methods."}]}