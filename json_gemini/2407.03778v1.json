{"title": "From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI", "authors": ["Stefanie Krause", "Frieder Stolzenburg"], "abstract": "Commonsense reasoning is a difficult task for a computer, but a critical skill for an artificial intelligence (AI). It can enhance the explainability of AI models by enabling them to provide intuitive and human-like explanations for their decisions. This is necessary in many areas especially in question answering (QA), which is one of the most important tasks of natural language processing (NLP). Over time, a multitude of methods have emerged for solving commonsense reasoning problems such as knowledge-based approaches using formal logic or linguistic analysis.\nIn this paper, we investigate the effectiveness of large language models (LLMs) on different QA tasks with a focus on their abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma and Llama 3. We further evaluate the LLM results by means of a questionnaire. We demonstrate the ability of LLMs to reason with commonsense as the models outperform humans on different datasets. While GPT-3.5's accuracy ranges from 56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets with an average 21% higher accuracy over ten datasets.\nFurthermore, we can appraise that, in the sense of explainable artificial intelligence (\u03a7\u0391\u0399), GPT-3.5 provides good explanations for its decisions. Our questionnaire revealed that 66% of participants rated GPT-3.5's explanations as either \"good\" or \"excellent\u201d. Taken together, these findings enrich our understanding of current LLMs and pave the way for future investigations of reasoning and explainability.", "sections": [{"title": "1. Introduction", "content": "Commonsense knowledge encompasses everyday assumptions about the world [53]. It is typically acquired through personal experience but can also be inferred by generalizing from common knowledge [44]. Humans employ a diverse range of knowledge and reasoning to comprehend the meanings of language. [45]. Commonsense reasoning enables us to link various pieces of knowledge to draw new conclusions [45]. For instance, consider the sentence from the commonsense reasoning dataset CODAH [5], \"Sally thought the test was a piece of cake.\" It is easy for us to infer that Sally found the test easy. While this kind of knowledge and reasoning is instinctive for human readers, it remains challenging for machines [45].\nLLMs play a crucial role in developing adaptable, general language systems [4]. Lately, a media hype was triggered by the LLM ChatGPT.\u00b9 This model from OpenAI provides a user-friendly interface and excels across various tasks, e.g., businesses can rapidly and accurately address customer inquiries, thereby freeing up resources and delivering a more personalized customer experience [10]. In the education sector, students use this new technology for assignment writing and exam preparation among other tasks [23]. In healthcare, LLMs can be used, e.g., for effective medical documentation [18], in legal departments as tax attorney [34] or in combination with robotics [50, 54]. \u03a4o facilitate the application of AI across diverse domains, the need for explainability in these systems is becoming more crucial. According to [3], it is essential to consider the target audience when seeking explainability. They define \u03a7\u0391\u0399 as follows: \u201cGiven an audience, an XAI is one that produces details or reasons to make its functioning clear or easy to understand.\u201d Explainability refers to presenting a model's outcome in a human-readable format, such as through explanatory text. In our work, we focus on explainability of AI models in the above sense with the goal of XAI to provide human-readable explanations to make users understand the automated decision-making of large language models a posteriori.\nThere is a strong link between XAI and commonsense reasoning, as both focus on enhancing the explainability of AI models. Commonsense reasoning can enhance the explainability of AI models by enabling them to provide intuitive and human-like explanations for their decisions. According to [9], starting with a better understanding of human cognition is a solid foundation. Humans use cognitive reasoning to draw meaningful conclusions despite incomplete and inconsistent knowledge [15]. For us, cognitive reasoning is particularly useful when we encounter new situations that are not covered by formal rules or guidelines. In these situations, we rely on our commonsense to make judgments and decisions that are appropriate and effective. Furthermore, commonsense reasoning is essential in interpersonal interactions and communication, as it allows us to understand the perspectives of others and to navigate social situations effectively. Commonsense reasoning can help AI models to be more robust in the context of novel situations. A model that can reason based on common-sense principles is better equipped to handle situations that it has not explicitly encountered before, as it can draw on its general understanding of the world to make informed decisions. So far, commonsense reasoning is intuitive for humans but has been a long-term challenge for AI models.\nWe assume that an LLM can reason similar to humans without the need of logical formulas or explicit ontology knowledge. Recent advances in LLMs (e.g. [28]) have pushed machines closer to human-like understanding capabilities. We believe that language com-"}, {"title": "2. Related Work", "content": "2.1. Commonsense Reasoning Approaches\nCommonsense reasoning is a difficult task for a computer to handle [41]. To address this problem, various approaches have been followed in the past. McCarthy [30] was the first who outlined the basic approach of representing commonsense knowledge with predicate logic. Symbolic logic approaches were the main representation type, see e.g. [13, 25]. While still in use today [7] for this extremely complex task to work well it requires a large amount of additional logical scaffolding to precisely define the terms used in the statement and their interrelationships [27].\nThere is a big gap between the logical approach with deductive reasoning and human reasoning, which is largely inductive, associative, and empirical, i.e., based on former experience. Human reasoning, in contrast to formal logical reasoning, does not strictly follow the rules of classical logic. There have been efforts to utilize an approach which uses an automatic theorem prover (that allows to derive new knowledge in an explainable way), large existing ontologies with background knowledge, and recurrent networks with long short-term memory (LSTM) [20] but still did not stand out much from the baseline [41].\nRecent efforts to acquire and represent commonsense knowledge resulted in large knowledge graphs, acquired through extractive methods [43] or crowdsourcing [39]. Some approaches use supervised training on a particular knowledge base, e.g., ConceptNet for commonsense knowledge. ConceptNet is a crowd-sourced database that represents commonsense knowledge as a graph of concepts connected by relations [43].\nInterestingly, LLMs (cf. Section 2.2) do not contain any explicit semantic knowledge or grammatical let alone logical rules that would allow an explicit reasoning process, not even the large ontologies from the logical knowledge representation like Cyc [26] or Adimen-SUMO [1]. A way out might be to have neural networks learn reasoning explicitly, possibly by focusing on certain sentence forms as in syllogistic reasoning maybe implemented with neural-symbolic cognitive reasoning by specifically structured neural networks [16, 17, 55]. In contrast to simple deep learning, information from different places and/or documents must be merged here in any case. It does not suffice to investigate any local text properties, e.g., determining the text form.\nThere are different types of commonsense reasoning, e.g., causal, temporal, physical, social etc. (see Section 3.1), each with its unique characteristics and applications. Understanding how these different reasoning tasks can be solved with LLMs is essential."}, {"title": "2.2. LLMs", "content": "In the past, most deep learning methods used supervised learning and therefore required substantial amounts of manually labelled data. Recent research has shown that learning good representations in an unsupervised fashion can provide a significant performance boost. The capacity of LLMs is essential to the success of zero-shot task transfer [37]. An example of a premier LLM that can handle a wide range of natural language processing tasks is OpenAI's GPT-3 [4]. GPT-3 (Generative Pre-trained Transformer) is a third-generation, autoregressive language model that uses unsupervised learning to produce human-like text. In our further investigation, we will focus on version GPT-3.5 with 175B parameters, as this chatbot based model is regarded as one of the most groundbreaking LLMs.\nWe further utilise the open-source model Meta-Llama-3-70B-Instruct by Meta. This is a instruction-tuned version with 70 billion parameters [48, 32]. Nevertheless, we use the very lightweight model with only 7 billion parameters \"Gemma-1.1-7b-it\" by Google. This model is open-source and built for responsible AI development based on the same research and technology used to create Gemini models [47]. We carefully chose these 3 LLMs, as they are state-of-the-art, differ in size and are mostly open-source."}, {"title": "2.3. Combining LLMs and Reasoning", "content": "Reasoning is one of the most actively discussed and debated capabilities of LLMs. However, it is important to note that proficiency in language does not necessarily equate to strong reasoning abilities in models [29]. Experts continue to debate the ability of LLMs to reason effectively in zero-shot scenarios [12]. While some researchers argue that LLMs demonstrate satisfactory zero-shot reasoning capabilities [22], others contend that the models struggle with planning and reasoning tasks [49].\nChatGPT, for example, is often seen as an unreliable reasoner due to issues such as hallucinations, a problem common to many LLMs [2]. Specifically, ChatGPT-3.5 faces significant challenges in certain areas, such as mathematics [12]. The mathematical performance of both GPT-3.5 and GPT-4 falls well below the level of a graduate student [14]. Attempts to combine elementary mathematics with commonsense reasoning have shown that no existing AI systems can reliably solve these problems [8]. Thus, we investigate the reasoning ability of different current LLMs in a fine-grained manner, which includes causal, temporal, comparative, physical, social, numerical reasoning, etc., via question-answering tasks."}, {"title": "3. Evaluating LLMs on QA Tasks", "content": "We assess the LLMs twofold: First, we evaluate the accuracy of GPT-3.5, Gemma and Llama 3 on QA benchmarks with multiple-choice questions. Please note that the analysis of GPT-3.5 has been taken from our previous work in 2023 [24]. The evaluation of Gemma and Llama 3 was conducted in May 2024. In the commonsense reasoning benchmarks we considered, the correct answer is indicated, although it is not always clear whether this answer really is objectively the best one. Second, we use part of the questions from the QA benchmarks for a questionnaire to evaluate the quality of all three LLM responses compared to human accuracy. Furthermore, we evaluate the explanation quality of GPT-3.5 with the help of our questionnaire.\n3.1. Benchmark Datasets\nWe utilize 11 publicly available benchmark datasets carefully designed to be difficult to solve without commonsense knowledge. From each dataset, we select 30 random examples, covering different QA tasks like text completion, understanding of cause and effect or temporal relationships. In addition, different fields like medicine, physics, and everyday life situations are covered. We evaluate the performance of the LLMs with the QA benchmarks presented in Table 2.\n3.2. LLM Results on Different Datasets\nUsing 30 randomly selected examples from each dataset we tested the respective QA tasks with GPT-3.5, Gemma and Llama 3. We analysed the accuracy and further differentiate between incorrect and invalid answers. Invalid means that the LLM does not respond which answer option is correct and instead asks for further context information, see Fig. 1 for an example. However, invalid answers have only been an issue with the COPA dataset and the models GPT-3.5 and Llama 3. GPT-3.5 (33.33% invalid responses) has a lot more troubles when short context is provided than Llama 3 (3.33% invalid responses)."}, {"title": "3.3. Analysis of LLM Results", "content": "In our error analysis, we identified seven types of challenges where some LLMs still face problems:\n1. missing context: When GPT-3.5 lacks sufficient context, it may occasionally be unable to provide an answer to the QA task. This has happened 10 times in total and solely with examples of the COPA dataset. This could be due to the very short premise texts in the COPA dataset, see Fig. 1. In this dataset, the premise texts consist of only five to nine words (on average six words) in the cases where GPT-3.5 complained about not having enough information to answer the question. In some of those cases, GPT-3.5 reveals which context information is missing: \u201cThe actual outcome would depend on a variety of factors, such as the political climate, the credibility of the politician, and the specific details of the argument in question. Without this information it is impossible to determine which alternative is more likely.\" (COPA example 619).\n2. comparative reasoning: Both GPT-3.5 and Gemma have problems when more than one answer option is plausible. This is the case in comparative scenarios in the COM2SENSE and Social IQa dataset. In such cases, the commonsense reasoner must explicitly investigate the likelihood of different answer candidates. For the Social IQa example 26823 \u201cSasha was throwing a party in her new condo which they bought a month ago. What does Sasha need to do before this?", "Turn music on\" which is likely but the correct and even more likely answer is \u201cneeded to buy food for the party\".\n3. subjective reasoning: Some answers depend on the personality of the reasoner, e.g. Social IQa example 18571: \u201cAlex's powers were not as strong since he was just starting out. Alex used Bailey's powers since hers were stronger. How would Bailey feel as a result?\" the correct answer according to the benchmark is \u201cgood": "ut instead GPT-3.5 answers", "Bailey may feel that her powers are being taken advantage of ...\u201d which we think is more a personalized subjective inference instead of a commonsense answer.\n4. slang, unofficial abbreviations, and youth language: GPT-3.5 and Gemma have difficulties to understand slang, unofficial abbreviations and youth language like \"subs\u201d for \u201csubscribers\u201d or \u201cyrs\u201d for \u201cyears\u201d. This could be observed, e.g., in Cosmos QA examples 6599 and 5748.\n5. social situations: We identified a particular difficulty in correctly understanding social situations in the Social IQa dataset. For instance, when asked the question \u201cKai was visiting from out of state and brought gifts for Quinn's family. What will Kai want to do next?": "PT-3.5 choose the answer", "hometown": "nstead of the correct answer option \u201cwatch the opening of gifts", "domain": "The analysis of MedMCQA showed that GPT-3.5 and Gemma is lacking a deep domain knowledge in the medical field. The answers of GPT-3.5 were always plausible and explained with a lot of details (on average 43 words per explanation) but 40% were incorrect. This is assumed to be because of many medical technical terms that are not common knowledge, e.g., \u201cStyloglossus muscle\u201d or", "relations": "All LLMs had their worst performance on the CommonsenseQA dataset with five answer options and usage of multiple target concepts that have the same semantic relation to a single source concept. This indicates that semantic relations are difficult to understand for the LLMs.\nPlease be aware that for certain questions to be answered correctly, one must possess in-depth knowledge rather than commonsense reasoning ability, e.g., you have to know that \"Prison Break\" is a television show, not a movie in a theatre to tell that \u201cThe couple went to the movie theatre to watch Prison Break"}, {"title": "4. Questionnaire", "content": "4.1. Design of the Questionnaire\nTo further evaluate the quality of LLM responses on different benchmark datasets and to make a comparison to human performance, we created a questionnaire in our previous work [24]. Since the number of participants was quite low, we conducted the questionnaire again intending to reach more participants. For a detailed description of the questionnaire we refer to our previous work [24]. We used two randomly selected examples for each of the above mentioned datasets \u2013 except for MedMCQA, as we believe these questions are too challenging for individuals without a medical background to answer.\nWe developed a publicly open online questionnaire. Participation was voluntary; participants could not be identified from the material presented and no plausible harm to participating individuals could arise from the study. Survey content validity was reviewed in a pretest by one professor, one academic staff and one non-academic volunteer (business consultant) who did not participate in developing the survey. The questionnaire was structured in three parts, the first containing demographic and personal information (gender, age, nationality, English level). The main part then consists of the QA tasks of the different datasets as well an evaluation of GPT-3.5 explanations. Note that the survey participants did not know that all explanations have been generated by GPT-3.5. An example of the main questionnaire section is provided in Fig. 2. We used this structure for 10 datasets and randomly selected two examples from every dataset. Therefore we considered 2\u00b710 = 20 QA tasks in our questionnaire.\n4.2. Questionnaire Participants\nIn total, 152 people participated in the questionnaire, but because of missing data, we only used the responses of 81 participants. The time to fully answer the whole questionnaire was about 25 minutes, which is probably why several participants did not complete the questionnaire until the last question. The participant's English level was mainly advanced or excellent so there was no language barrier in understanding the QA tasks. Among the completed questionnaires, the average age was rather young with 26 years, with a minimum of 19 years and maximum of 51 years. Most of the participants were from India or Germany and the rest was from Bangladesh, Pakistan, Finland, Russia, Turkey, Canada, Iran, Lebanon and Switzerland.\n4.3. Questionnaire Responses\nWe found that the participants answered with a mean accuracy of 69.00% on a subset of QA tasks compared to GPT-3.5 74.33%, Gemma 72.00% and Llama 3 89.00% (mean accuracy over all datasets, except MedMCQA). Note that 20 in detail analyzed QA tasks of the questionnaire are not as representative as the over 300 QA tasks from Section 3.2, even though we selected the 20 QA tasks randomly.\nA comparison of the performance of three different LLMs to the questionnaire participants on the different datasets is presented in Fig. 3. One can recognize very quickly that"}, {"title": "5. Discussion and Future Directions", "content": "Over the past, research often focused on logical approaches and large knowledge graphs to deal with commonsense reasoning. Given that we are currently in the era of LLMs which have shown substantial performance improvements across various tasks, we hypothesized that LLMs are capable of handling commonsense reasoning in QA tasks with almost human-level performance (Hypothesis 1). As LLMs are trained on a large number of data and produces human-like text, we assume that it can perform commonsense reasoning without explicit semantic knowledge or logical rules. To show that we evaluated three LLMs on eleven different QA benchmark datasets which are difficult to solve without commonsense reasoning.\nMoreover, we evaluated explanations generated with GPT-3.5 by means on an online questionnaire to investigate how sufficient explanations are to users. Our Hypothesis 2 is stating that an LLM like GPT-3.5 is able to provide good explanations to users without the need of explicit formalized knowledge representation. Most participants are content with GPT-3.5's explanations. Thereby apparently the problem of explainability of AI decisions can be overcome easily.\n5.1. Main Findings\nThis study shows that different LLMs reached an overall accuracy of 70.91% - 89.70% on eleven challenging QA datasets that are difficult to handle without commonsense reasoning. The new Llama 3 model performed a lot better than Gemma und GPT-3.5 on all eleven commonsense reasoning datasets. While there are still challenges such as too little context information, semantic relations or the sciences and medical domain , GPT-3.5 still outperforms our questionnaire participants in six out of ten datasets (not considering the medical dataset MedMCQA). The results of our questionnaire show that participants answered 69% of the 20 QA tasks correctly and are outperformed by Llama 3 on every single dataset. We found that Llama 3 is outperforming humans impressively by an average 21% higher accuracy over ten datasets. Although we only compared the performance of humans versus LLMs on a small amount of examples, we believe that the outcome indicates that our Hypothesis 1 is true and LLMs can handle commonsense reasoning in QA tasks with near-human-level performance, even outperforming human accuracy.\nThis research focused as well on assessing the explainability of LLMs, recognizing the significant importance of addressing the black-box problem. This is particularly relevant as users need to understand AI decisions. By means of a web-based questionnaire, we evaluated GPT-3.5's explanations for 20 QA tasks. We found a mean linear positive correlation of 0.64 between the comprehensibility of the QA tasks and that of GPT-3.5's explanations. This observation is relevant for the way GPT-3.5's users describe their tasks as it has an impact on the quality of the explanation they receive. In our questionnaire, GPT-3.5's explanations were mostly rated \u201cgood\u201d or \u201cexcellent\u201d with 66% and fair or better with 86%. Our Hypothesis 2 that LLMs can generate good explanations could be confirmed. However, to improve explanations, it is recommended to not only focus on explaining why one option is correct but also why other answering options are false or less likely.\n5.2. Impact on the Field\nThe development of XAI is facing both scientific and social demands [52], and scientists aim to achieve this without sacrificing performance. So far, this grand challenge is mainly dealt with by explicit knowledge, such as knowledge graphs. However, we found that implicit knowledge in the form of probabilistic models can generate good explanations. LLMs made significant advancements in NLP tasks in recent years. Due to the chat function of GPT-3.5, users can easily ask for explanations to understand the response of the AI system. This can tackle the lack of explainability and is a quite simple and yet effective way. Using a questionnaire, we could measure the quality of GPT-3.5's explanations.\nMoreover, commonsense reasoning is very important for various NLP tasks. It assesses the relative plausibility of different scenarios and recognizes causality. Until now, research has focused on mathematical logic and the formalization of commonsense reasoning knowledge. However, some philosophers, e.g., Wittgenstein, already claimed that commonsense reasoning knowledge is unformalizable or mathematical logic is inappropriate [31]. As seen in our evaluation, LLMs have a good zero-shot performance on different QA tasks that require commonsense reasoning. Nevertheless, we detected seven problems where some LLMs still have difficulties and further research is necessary, e.g., little context information, comparative reasoning, knowledge in the medical and science domain or semantic relations.\nEvaluation of LLMs brings AI closer to making a practical impact in the area of XAI and commonsense reasoning. There are still rich opportunities for novel AI research to further measure the quality of explanations as well as opportunities in tackling difficult commonsense reasoning tasks like CommonsenseQA. In future research, one can also investigate certain domains in more detail e.g. focus on the medical domain to foster the way for medical LLM based applications. We believe LLMs have great potential for various applications in combination with robotics, customer service and even legal. However, we have to make sure to keep the concerns like incorrect answers and possible negative impacts on human skills in mind. Furthermore, while the zero-shot performance of the LLMs was already quite good, the accuracy and explanation quality might improve using few-shot learning and chain-of-thought prompting [51, 19].\n5.3. Limitations\nOur study has limitations that need to be acknowledged. The number of survey participants we included was rather small, which limits generalization of our results. The average age was 26 years and primarily the participants were university students. In general, more participants with diverse ages and nationalities would help to strengthen the results. Furthermore the key challenge for explainability is to determine what constitutes a \"good\" explanation since this is subjective and depends on context. We evaluated explanations using a five-level Likert scale from \"very poor\" to \"excellent\u201d. However, we only analyzed 20 explanations of GPT-3.5 and argue that our Hypothesis 2 (that LLMs can generate good explanations) can be confirmed. Nevertheless, explainability is very important in the medical field, but we did not consider the MedMCQA dataset in our questionnaire due to a supposed lack of participant's knowledge in medicine."}, {"title": "6. Conclusion", "content": "The field of AI has made considerable progress towards large-scale models, especially for NLP tasks. Although the field requires more testing, we argue that LLMs can be used for commonsense reasoning tasks and as well generate helpful explanations for users to understand AI decisions. The use of LLMs is a promising area of research that offers many opportunities to enhance explainability. However, to unleash their full potential for XAI, it is crucial to approach the use of these models with caution and to critically evaluate users concerns. The explainability of LLMs is crucial for many applications and was an important part of this research. We have shown important future directions research involving XAI and commonsense reasoning. Several state-of-the-art LLMs have proven capable of outperforming human performance on a large number of different QA datasets which require commonsense reasoning.\nDespite the potential of the field of LLMs, important questions remain for a comprehensive evaluation of LLM explanations. As these key issues are systematically addressed, the potential real world applications will grow. In particular, the stochastic aspects of LLMs, where repeated queries may lead to different answers, should be considered in future work. This would also allow for a better assessment of the error in the LLM performance estimates."}]}