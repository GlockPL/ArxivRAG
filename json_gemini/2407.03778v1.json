{"title": "From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI", "authors": ["Stefanie Krause", "Frieder Stolzenburg"], "abstract": "Commonsense reasoning is a difficult task for a computer, but a critical skill for an artificial\nintelligence (AI). It can enhance the explainability of AI models by enabling them to provide\nintuitive and human-like explanations for their decisions. This is necessary in many areas\nespecially in question answering (QA), which is one of the most important tasks of natural\nlanguage processing (NLP). Over time, a multitude of methods have emerged for solving\ncommonsense reasoning problems such as knowledge-based approaches using formal logic or\nlinguistic analysis.\nIn this paper, we investigate the effectiveness of large language models (LLMs) on differ-\nent QA tasks with a focus on their abilities in reasoning and explainability. We study three\nLLMs: GPT-3.5, Gemma and Llama 3. We further evaluate the LLM results by means of\na questionnaire. We demonstrate the ability of LLMs to reason with commonsense as the\nmodels outperform humans on different datasets. While GPT-3.5's accuracy ranges from\n56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on all\neleven datasets. Thereby Llama 3 is outperforming humans on all datasets with an average\n21% higher accuracy over ten datasets.\nFurthermore, we can appraise that, in the sense of explainable artificial intelligence\n(\u03a7\u0391\u0399), GPT-3.5 provides good explanations for its decisions. Our questionnaire revealed\nthat 66% of participants rated GPT-3.5's explanations as either \"good\" or \"excellent\u201d.\nTaken together, these findings enrich our understanding of current LLMs and pave the way\nfor future investigations of reasoning and explainability.", "sections": [{"title": "1. Introduction", "content": "Commonsense knowledge encompasses everyday assumptions about the world [53]. It\nis typically acquired through personal experience but can also be inferred by generalizing"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Commonsense Reasoning Approaches", "content": "Commonsense reasoning is a difficult task for a computer to handle [41]. To address this\nproblem, various approaches have been followed in the past. McCarthy [30] was the first who\noutlined the basic approach of representing commonsense knowledge with predicate logic.\nSymbolic logic approaches were the main representation type, see e.g. [13, 25]. While still\nin use today [7] for this extremely complex task to work well it requires a large amount of\nadditional logical scaffolding to precisely define the terms used in the statement and their\ninterrelationships [27].\nThere is a big gap between the logical approach with deductive reasoning and human\nreasoning, which is largely inductive, associative, and empirical, i.e., based on former ex-\nperience. Human reasoning, in contrast to formal logical reasoning, does not strictly follow\nthe rules of classical logic. There have been efforts to utilize an approach which uses an au-\ntomatic theorem prover (that allows to derive new knowledge in an explainable way), large\nexisting ontologies with background knowledge, and recurrent networks with long short-term\nmemory (LSTM) [20] but still did not stand out much from the baseline [41]."}, {"title": "2.2. LLMs", "content": "In the past, most deep learning methods used supervised learning and therefore required\nsubstantial amounts of manually labelled data. Recent research has shown that learning good\nrepresentations in an unsupervised fashion can provide a significant performance boost. The\ncapacity of LLMs is essential to the success of zero-shot task transfer [37]. An example\nof a premier LLM that can handle a wide range of natural language processing tasks is\nOpenAI's GPT-3 [4]. GPT-3 (Generative Pre-trained Transformer) is a third-generation,\nautoregressive language model that uses unsupervised learning to produce human-like text.\nIn our further investigation, we will focus on version GPT-3.5 with 175B parameters, as this\nchatbot based model is regarded as one of the most groundbreaking LLMs.\nWe further utilise the open-source model Meta-Llama-3-70B-Instruct by Meta. This is a\ninstruction-tuned version with 70 billion parameters [48, 32]. Nevertheless, we use the very\nlightweight model with only 7 billion parameters \"Gemma-1.1-7b-it\" by Google. This model\nis open-source and built for responsible AI development based on the same research and\ntechnology used to create Gemini models [47]. We carefully chose these 3 LLMs, as they are\nstate-of-the-art, differ in size (see Table 1) and are mostly open-source."}, {"title": "2.3. Combining LLMs and Reasoning", "content": "Reasoning is one of the most actively discussed and debated capabilities of LLMs. How-\never, it is important to note that proficiency in language does not necessarily equate to strong\nreasoning abilities in models [29]. Experts continue to debate the ability of LLMs to reason\neffectively in zero-shot scenarios [12]. While some researchers argue that LLMs demonstrate\nsatisfactory zero-shot reasoning capabilities [22], others contend that the models struggle\nwith planning and reasoning tasks [49].\nChatGPT, for example, is often seen as an unreliable reasoner due to issues such as hallu-\ncinations, a problem common to many LLMs [2]. Specifically, ChatGPT-3.5 faces significant\nchallenges in certain areas, such as mathematics [12]. The mathematical performance of\nboth GPT-3.5 and GPT-4 falls well below the level of a graduate student [14]. Attempts\nto combine elementary mathematics with commonsense reasoning have shown that no ex-\nisting AI systems can reliably solve these problems [8]. Thus, we investigate the reasoning\nability of different current LLMs in a fine-grained manner, which includes causal, temporal,\ncomparative, physical, social, numerical reasoning, etc., via question-answering tasks."}, {"title": "3. Evaluating LLMs on QA Tasks", "content": "We assess the LLMs twofold: First, we evaluate the accuracy of GPT-3.5, Gemma and\nLlama 3 on QA benchmarks with multiple-choice questions. Please note that the analysis\nof GPT-3.5 has been taken from our previous work in 2023 [24]. The evaluation of Gemma\nand Llama 3 was conducted in May 2024. In the commonsense reasoning benchmarks we\nconsidered, the correct answer is indicated, although it is not always clear whether this\nanswer really is objectively the best one. Second, we use part of the questions from the QA\nbenchmarks for a questionnaire to evaluate the quality of all three LLM responses compared\nto human accuracy. Furthermore, we evaluate the explanation quality of GPT-3.5 with the\nhelp of our questionnaire."}, {"title": "3.1. Benchmark Datasets", "content": "We utilize 11 publicly available benchmark datasets carefully designed to be difficult\nto solve without commonsense knowledge. From each dataset, we select 30 random exam-\nples, covering different QA tasks like text completion, understanding of cause and effect or\ntemporal relationships. In addition, different fields like medicine, physics, and everyday life\nsituations are covered. We evaluate the performance of the LLMs with the QA benchmarks\npresented in Table 2."}, {"title": "3.2. LLM Results on Different Datasets", "content": "Using 30 randomly selected examples from each dataset we tested the respective QA tasks\nwith GPT-3.5, Gemma and Llama 3. We analysed the accuracy and further differentiate\nbetween incorrect and invalid answers. Invalid means that the LLM does not respond which\nanswer option is correct and instead asks for further context information, see Fig. 1 for an\nexample. However, invalid answers have only been an issue with the COPA dataset and the"}, {"title": "3.3. Analysis of LLM Results", "content": "In our error analysis, we identified seven types of challenges where some LLMs still face\nproblems:\n1. missing context: When GPT-3.5 lacks sufficient context, it may occasionally be\nunable to provide an answer to the QA task. This has happened 10 times in total and\nsolely with examples of the COPA dataset. This could be due to the very short premise\ntexts in the COPA dataset, see Fig. 1. In this dataset, the premise texts consist of only\nfive to nine words (on average six words) in the cases where GPT-3.5 complained about\nnot having enough information to answer the question. In some of those cases, GPT-\n3.5 reveals which context information is missing: \u201cThe actual outcome would depend\non a variety of factors, such as the political climate, the credibility of the politician,\nand the specific details of the argument in question. Without this information it is\nimpossible to determine which alternative is more likely.\" (COPA example 619).\n2. comparative reasoning: Both GPT-3.5 and Gemma have problems when more\nthan one answer option is plausible. This is the case in comparative scenarios in the\nCOM2SENSE and Social IQa dataset. In such cases, the commonsense reasoner must\nexplicitly investigate the likelihood of different answer candidates. For the Social IQa\nexample 26823 \u201cSasha was throwing a party in her new condo which they bought a\nmonth ago. What does Sasha need to do before this?", "Turn music\non\" which is likely but the correct and even more likely answer is \u201cneeded to buy food\nfor the party\".\n3. subjective reasoning: Some answers depend on the personality of the reasoner, e.g.\nSocial IQa example 18571: \u201cAlex's powers were not as strong since he was just starting\nout. Alex used Bailey's powers since hers were stronger. How would Bailey feel as a\nresult?\" the correct answer according to the benchmark is \u201cgood": "ut instead GPT-\n3.5 answers", "Bailey may feel that her powers are being taken\nadvantage of ...\u201d which we think is more a personalized subjective inference instead\nof a commonsense answer.\n4. slang, unofficial abbreviations, and youth language: GPT-3.5 and Gemma have\ndifficulties to understand slang, unofficial abbreviations and youth language like ": "ubs\u201d\nfor \u201csubscribers\u201d or \u201cyrs\u201d for \u201cyears\u201d. This could be observed, e.g., in Cosmos QA\nexamples 6599 and 5748.\n5. social situations: We identified a particular difficulty in correctly understanding\nsocial situations in the Social IQa dataset. For instance, when asked the question \u201cKai\nwas visiting from out of state and brought gifts for Quinn's family. What will Kai want\nto do next?", "needed to leave his hometown": "nstead of\nthe correct answer option \u201cwatch the opening of gifts", "domain": "The analysis of MedMCQA showed that GPT-3.5\nand Gemma is lacking a deep domain knowledge in the medical field. The answers\nof GPT-3.5 were always plausible and explained with a lot of details (on average 43\nwords per explanation) but 40% were incorrect. This is assumed to be because of many\nmedical technical terms that are not common knowledge, e.g., \u201cStyloglossus muscle\u201d\nor", "relations": "All LLMs had their worst performance on the CommonsenseQA\ndataset with five answer options and usage of multiple target concepts that have the\nsame semantic relation to a single source concept. This indicates that semantic relations\nare difficult to understand for the LLMs.\nPlease be aware that for certain questions to be answered correctly, one must possess\nin-depth knowledge rather than commonsense reasoning ability, e.g., you have to know that", "Break": "s a television show, not a movie in a theatre to tell that \u201cThe couple went\nto the movie theatre to watch Prison Break"}, {"title": "4. Questionnaire", "content": ""}, {"title": "4.1. Design of the Questionnaire", "content": "To further evaluate the quality of LLM responses on different benchmark datasets and\nto make a comparison to human performance, we created a questionnaire in our previous\nwork [24]. Since the number of participants was quite low, we conducted the questionnaire\nagain intending to reach more participants. For a detailed description of the questionnaire\nwe refer to our previous work [24]. We used two randomly selected examples for each of the\nabove mentioned datasets \u2013 except for MedMCQA, as we believe these questions are too\nchallenging for individuals without a medical background to answer.\nWe developed a publicly open online questionnaire. Participation was voluntary; par-\nticipants could not be identified from the material presented and no plausible harm to\nparticipating individuals could arise from the study. Survey content validity was reviewed\nin a pretest by one professor, one academic staff and one non-academic volunteer (business\nconsultant) who did not participate in developing the survey. The questionnaire was struc-\ntured in three parts, the first containing demographic and personal information (gender,\nage, nationality, English level). The main part then consists of the QA tasks of the different\ndatasets as well an evaluation of GPT-3.5 explanations. Note that the survey participants\ndid not know that all explanations have been generated by GPT-3.5. An example of the\nmain questionnaire section is provided in Fig. 2. We used this structure for 10 datasets and\nrandomly selected two examples from every dataset. Therefore we considered 2\u00b710 = 20 QA\ntasks in our questionnaire."}, {"title": "4.2. Questionnaire Participants", "content": "In total, 152 people participated in the questionnaire, but because of missing data, we\nonly used the responses of 81 participants. The time to fully answer the whole questionnaire\nwas about 25 minutes, which is probably why several participants did not complete the\nquestionnaire until the last question. The participant's English level was mainly advanced\nor excellent so there was no language barrier in understanding the QA tasks. Among the\ncompleted questionnaires, the average age was rather young with 26 years, with a minimum\nof 19 years and maximum of 51 years. Most of the participants were from India or Germany\nand the rest was from Bangladesh, Pakistan, Finland, Russia, Turkey, Canada, Iran, Lebanon\nand Switzerland."}, {"title": "4.3. Questionnaire Responses", "content": "We found that the participants answered with a mean accuracy of 69.00% on a subset\nof QA tasks compared to GPT-3.5 74.33%, Gemma 72.00% and Llama 3 89.00% (mean\naccuracy over all datasets, except MedMCQA). Note that 20 in detail analyzed QA tasks of\nthe questionnaire are not as representative as the over 300 QA tasks from Section 3.2, even\nthough we selected the 20 QA tasks randomly.\nA comparison of the performance of three different LLMs to the questionnaire partici-\npants on the different datasets is presented in Fig. 3. One can recognize very quickly that\nsurvey participants by 45% more accuracy. When comparing human and GPT-3.5 perfor-\nmance the greatest difference is on the Story Cloze Test and Cosmos QA datasets. In these\ntwo datasets, GPT-3.5 outperforms humans with over 20% difference in accuracy. It is quite\ninteresting that all LLMs perform better on Cosmos QA than the survey participants as\ncontextual commonsense reasoning is needed for this dataset. It focuses on reading between\nthe lines over a diverse collection of people's everyday narratives. In contrast, humans per-\nform a lot better than GPT-3.5 and Gemma on ARC where an understanding of science\nquestions is needed.\nWe want to make sure that we recognize any correlations regarding the tasks compre-\nhensibility influencing the accuracy of answers or the explanation quality. We found that\n82% of the questions used in our questionnaire were rated \"excellent\", \"good\" or \"fair\"\ncomprehensible. The SocialIAQ examples have been rated worst by our human participants.\nThe human accuracy on this QA task is only 55%, however GPT-3.5 reached 67% accuracy.\nWe aimed to explore the relationship between the comprehensibility of tasks and GPT-3.5's\nexplanations. It is worth noting that most questions of the different QA tasks are compre-\nhensible according to the participants. We observed that there is a mean linear positive\ncorrelation of 0.64 between the comprehensibility of the QA tasks and that of GPT-3.5's\nexplanations. This means that the way the users comprehend the QA tasks has an impact\non the estimated quality of the explanation from GPT-3.5.\nThe quality of GPT-3,5's explanations is visualised in Fig. 4. In general, explanations\nwere mostly rated \u201cgood\u201d or \u201cexcellent\" with 66% and only 42 times very poor. Explanations\""}, {"title": "5. Discussion and Future Directions", "content": "Over the past, research often focused on logical approaches and large knowledge graphs\nto deal with commonsense reasoning. Given that we are currently in the era of LLMs which\nhave shown substantial performance improvements across various tasks, we hypothesized\nthat LLMs are capable of handling commonsense reasoning in QA tasks with almost human-\nlevel performance (Hypothesis 1). As LLMs are trained on a large number of data and\nproduces human-like text, we assume that it can perform commonsense reasoning without"}, {"title": "5.1. Main Findings", "content": "This study shows that different LLMs reached an overall accuracy of 70.91% - 89.70% on\neleven challenging QA datasets that are difficult to handle without commonsense reasoning.\nThe new Llama 3 model performed a lot better than Gemma und GPT-3.5 on all eleven\ncommonsense reasoning datasets. While there are still challenges such as too little context\ninformation, semantic relations or the sciences and medical domain (cf. Section 3.3), GPT-\n3.5 still outperforms our questionnaire participants in six out of ten datasets (not considering\nthe medical dataset MedMCQA). The results of our questionnaire show that participants\nanswered 69% of the 20 QA tasks correctly and are outperformed by Llama 3 on every single\ndataset. We found that Llama 3 is outperforming humans impressively by an average 21%\nhigher accuracy over ten datasets. Although we only compared the performance of humans\nversus LLMs on a small amount of examples, we believe that the outcome indicates that\nour Hypothesis 1 is true and LLMs can handle commonsense reasoning in QA tasks with\nnear-human-level performance, even outperforming human accuracy.\nThis research focused as well on assessing the explainability of LLMs, recognizing the\nsignificant importance of addressing the black-box problem. This is particularly relevant as\nusers need to understand AI decisions. By means of a web-based questionnaire, we evaluated\nGPT-3.5's explanations for 20 QA tasks. We found a mean linear positive correlation of 0.64\nbetween the comprehensibility of the QA tasks and that of GPT-3.5's explanations. This\nobservation is relevant for the way GPT-3.5's users describe their tasks as it has an impact\non the quality of the explanation they receive. In our questionnaire, GPT-3.5's explanations\nwere mostly rated \u201cgood\u201d or \u201cexcellent\u201d with 66% and fair or better with 86%. Our Hypoth-\nesis 2 that LLMs can generate good explanations could be confirmed. However, to improve\nexplanations, it is recommended to not only focus on explaining why one option is correct\nbut also why other answering options are false or less likely."}, {"title": "5.2. Impact on the Field", "content": "The development of XAI is facing both scientific and social demands [52], and scientists\naim to achieve this without sacrificing performance. So far, this grand challenge is mainly\ndealt with by explicit knowledge, such as knowledge graphs. However, we found that implicit\nknowledge in the form of probabilistic models can generate good explanations. LLMs made\nsignificant advancements in NLP tasks in recent years. Due to the chat function of GPT-3.5,\nusers can easily ask for explanations to understand the response of the AI system. This"}, {"title": "5.3. Limitations", "content": "Our study has limitations that need to be acknowledged. The number of survey partici-\npants we included was rather small, which limits generalization of our results. The average\nage was 26 years and primarily the participants were university students. In general, more\nparticipants with diverse ages and nationalities would help to strengthen the results. Fur-\nthermore the key challenge for explainability is to determine what constitutes a \"good\"\nexplanation since this is subjective and depends on context. We evaluated explanations us-\ning a five-level Likert scale from \"very poor\" to \"excellent\u201d. However, we only analyzed 20\nexplanations of GPT-3.5 and argue that our Hypothesis 2 (that LLMs can generate good\nexplanations) can be confirmed. Nevertheless, explainability is very important in the medical\nfield, but we did not consider the MedMCQA dataset in our questionnaire due to a supposed\nlack of participant's knowledge in medicine."}, {"title": "6. Conclusion", "content": "The field of AI has made considerable progress towards large-scale models, especially\nfor NLP tasks. Although the field requires more testing, we argue that LLMs can be used\nfor commonsense reasoning tasks and as well generate helpful explanations for users to\nunderstand AI decisions. The use of LLMs is a promising area of research that offers many"}]}