{"title": "MATRYOSHKA: LEARNING TO DRIVE BLACK-BOX LLMS WITH LLMS", "authors": ["Changhao Li", "Yuchen Zhuang", "Rushi Qiang", "Haotian Sun", "Hanjun Dai", "Chao Zhang", "Bo Dai"], "abstract": "Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation or in-context learning, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka, a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with Matryoshka serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. Matryoshka is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on three diverse tasks demonstrate that Matryoshka effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks, including reasoning, planning, and personalization. By leveraging this pioneering controller-generator framework to mitigate dependence on model parameters, Matryoshka provides a transparent and practical solution for improving black-box LLMs through controllable multi-turn generation using white-box LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Most of the commercial large language models (LLMs) (Radford et al., 2019; Brown, 2020; Achiam et al., 2023; Chowdhery et al., 2023; Team et al., 2023; Reid et al., 2024) are black-box models (Sun et al., 2024b; Zhuang et al., 2024b), where the model structure, parameters, or even output logits are not accessible. Although these black-box LLMs have exhibited remarkable efficacy across a diverse array of applications, revolutionizing natural language processing tasks such as text completion (Radford et al., 2019; Brown, 2020), translation (Zhu et al., 2023), question-answering (Hendrycks et al., 2020), etc, the applications of black-box LLMs continue to face significant challenges when faced with tasks that require more advanced cognitive capabilities, particularly in the realms of reasoning (Hendrycks et al., 2021; Wang et al., 2024b), planning (Valmeekam et al., 2022; Zhuang et al., 2023; Jimenez et al., 2023; Mialon et al., 2023; Zhuang et al., 2024a; Shi et al., 2024b), and personalization problems (Salemi et al., 2023; Tan et al., 2024a). Enhancing such capabilities within black-box LLMs presents unique challenges, primarily due to the lack of direct access to internal model parameters (Huang et al., 2023; Sun et al., 2024b; Zhuang et al., 2024b). This opacity introduces substantial complexity in efforts to refine and augment these advanced cognitive functions within the framework of black-box architectures.\nExisting research efforts for improving black-box LLM performance can be largely categorized into two main methodological paradigms (Figure 1): (1) In-context learning (ICL)-based methods (Sun et al., 2024a; Tan et al., 2024b; Zhuang et al., 2024b) that are designed to guide LLM in exhibiting specific capabilities or adhering to particular directives. However, these frameworks necessitate"}, {"title": "PROBLEM FORMULATION", "content": "Our objective is to enhance the capability of a black-box LLM in solving complex, long-horizon problems by calibrating its output generation to better align with specific tasks. To achieve this, we conceptualize both the original outputs and the optimal solutions as distributions within a joint space, \\(Y \\sim Y_{org} \\times \\mathcal{y}_{sol}\\), where \\(y_{org}\\) and \\(Y_{sol}\\) represent the original text generations and target solutions, respectively. Specifically, given a set of task descriptions \\(\\mathcal{D} = \\{X_i\\}_{i=1}^N\\), our goal is to adjust the outputs \\(\\hat{y} \\in y_{org}\\) of the black-box LLM toward the hidden target solutions \\(y_i \\in Y_{sol}\\) that successfully solve the problems. This involves driving the black-box LLM to generate outputs more closely aligned with the desired solutions without requiring access to its internal parameters.\nWe propose utilizing a lightweight white-box language model as a controller to enhance the capabilities of black-box LLMs in solving various tasks. The process begins by feeding a text-grounded task description x from the task space X into a smaller language model \u03b8, which acts as the controller. This smaller model generates \\(f_\\theta(x)\\), an automatic prompt designed to augment the performance of black-box LLMs on the specific task. These prompts can facilitate various functions, such as chain-of-thoughts for reasoning, task decomposition for planning, and user profile summarization from historical records for personalization. The generated intermediate prompt \\(f_\\theta(x)\\) is then combined with the original problem description x and input \\((x, f_\\theta(x))\\) into the black-box LLM. The capability enhancement will be measured by evaluating the performance improvements achieved through this controller-generator framework.\nWe emphasize that unlike works focusing on token-level LLM policy (Wang et al., 2024a; Rafailov et al., 2024a), our action space consists of entire intermediate guidance generations to solve the task.\nWe recognize the black-box LLM as an environment to be controlled by the white-box LLM policy. After inputing the prompts \\((x, f_\\theta(x))\\), the black-box LLM produces a final solution \\(\\hat{y} = g_{LLM}(x, f_\\theta(x))\\) for the task. We utilize the final correctness of the black-box LLM's output to evaluate the quality \\(u(x, f_\\theta(x))\\) as the reward of the intermediate guidance produced by the white-box LLM controller (Figure 2):\n\\[u(x, f_\\theta(x)) := eval(x, g_{LLM}(x, f_\\theta(x)), \\]\nwhere eval(\u00b7) denotes the oracle evaluation function of the final answer. For example, in question-answering tasks with ground-truth final answer y, the evaluation function measures accuracy by comparing the prediction with ground truth as eval(x, \\(g_{LLM}(x, f_\\theta(x))\\) = \\(1(g_{LLM}(x, f_\\theta(x)) = y)\\), where 1(\u00b7) is the indicator function. For planning tasks without a ground-truth solution, the evaluation function assesses the success rate after executing the final solution as eval(x, \\(G_{LLM}(x, f_\\theta(x))\\) = 1 succ \\((g_{LLM}(x, f_\\theta(x))\\).\nThe above interaction between the white-box LLM controller and the black-box environment can be repeated for multi-turns for long-horizon tasks.\nFor initialization, a prompt x is sampled from task space X and serves as the initial state \\(s_0 = x\\). At each subsequent step t \u2208 [T], the controller generates prompts \\(a_t\\) based on the current \\(s_{t\u22121}\\). In response to the controller's action, the environment returns an observation \\(o_t\\) based on the history \\(s_{t\u22121}\\) and the current action \\(a_t\\). The state then transitions to include the new action and observation:\n\\[s_t = (s_{t\u22121}, a_t, o_t) = (x, a_1, o_1, s_1,\u22ef, a_t, o_t), \\]\nand the next step begins. This process repeats for T rounds, resulting in a trajectory:\n\\[\\tau = (x, a_1, o_1, s_1,\u2026\u2026,o_T, s_T),\\]"}, {"title": "MATRYOSHKA", "content": "In this section, we specialize the white-box LLM controller that generates intermediate guidance to assist in task understanding and problem-solving in Section 3.1 and discuss the data collection procedure by interacting with black-box LLM in Section 3.2, which will be used for Matryoshka training to align the outputs of the black-box LLM with preferences in Section 3.3.\nWe instantiate the white-box LLM as a controller to generate additional guidance that assists the black-box LLM in understanding and solving a diverse range of problems. Given the varying complexity and distinct characteristics of different tasks, the controller should be capable of generating guidance in various formats. We provide examples corresponding to reasoning, planning, and personalization tasks (Figure 3):\nFor reasoning tasks, generating a sequence of reasoning steps is essential to solve the problem effectively. Existing works (Zhou et al., 2023) have observed that models often perform poorly on tasks that require solving problems more complex than the exemplars provided in the prompts. To enable the model to develop better reasoning and overcome the easy-to-hard generalization issue, one strategy is to decompose complex problems into a series of simpler sub-problems and solve them sequentially. Therefore, for reasoning tasks, the white-box LLM controller outputs decomposed sub-tasks to assist the subsequent black-box LLM generator in enhancing its reasoning capabilities."}, {"title": "DATA COLLECTION BY INTERACTING WITH BLACK-BOX LLM ENVIRONMENT", "content": "Optimizing the intermediate guidance generated by the controller presents significant challenges for two main reasons: (1) Lack of ground-truth guidance: There are no ground-truth intermediate generations available to serve as supervision signals for the controller's outputs. (2) Uncertainty in performance improvement: It is difficult to determine which guidance will reliably enhance the downstream performance of the black-box LLM. To address these challenges, we formulate the black-box LLM as an environment system and employ multi-turn interactions with environmental feedback during data sampling.\nIn the MDP formulation, we consider the action space as the set of possible guidance that can enhance the capabilities of black-box LLMs. The observation space is determined by the oracle evaluation function for each task, defined as eval(\u00b7), where the sampled supervision signal is denoted as z, with z = 1 indicating that \\(f_\\theta(x)\\) is positive guidance while z = 0 indicating \\(f_\\theta(x)\\) negative guidance. During the multi-turn interactions, if the observation \\(o_t\\) at the t-th step returns a negative signal, the next action step \\(a_{t+1}\\) involves modifying the intermediate guidance based on the feedback. The interactions continue until a positive signal is observed or the maximum number of interaction turns T is reached.\nFor each input \\(x_i\\), we perform T-step multi-turn interactions with the black-box LLM-based environment to obtain the trajectories \\((a_{i,1}, o_{i,1}, a_{i,2}, o_{i,2}, \\ldots, a_{i,T}, o_{i,T})\\). To increase the diversity of intermediate generations, we introduce randomness into the policy and repeat the entire interaction process K times. This results in K trajectories, yielding intermediate generations \\(\\{g_{i,1}, g_{i,2}, \\ldots, g_{i,K \\times T}\\} \\) along with their corresponding observations \\(\\{o_{i,1}, o_{i,2}, \\ldots, o_{i,K \\times T}\\} \\), which serve as sampling signals. We then sample the positive guidance \\(g_i^+\\) from the set of guidance with positive observations, \\(g^+ \\sim \\{g_{i,j} | o_{i,j} = 1\\}\\) and the negative guidance from the remaining generations, \\(g_i^- \\sim \\{g_{i,j} | o_{i,j} = 0\\}\\)."}, {"title": "ITERATIVE GUIDANCE OPTIMIZATION", "content": "As white-box LLMs like LLaMA are pre- and post-trained for general purposes, they may struggle to fulfill the specific tasks required by the controller. Additionally, there may be discrepancies between what the controller considers \u201cgood\u201d guidance and what the generator interprets as \u201cgood\u201d guidance. To this end, the guidance generated by the white-box LLM controller needs further optimization to enhance the performance of the black-box LLM generator."}, {"title": "EXPERIMENTS", "content": "In this section, we present comprehensive experiments on a diverse set of complex, long-horizon tasks to demonstrate the enhanced capabilities of black-box LLMs using Matryoshka."}, {"title": "EXPERIMENTAL SETUP", "content": "We consider three types of tasks in experiments, each targeting a distinct capability of black-box LLMs: (1) LaMP (Salemi et al., 2023) for personalization capabilities, (2) GSM8K (Cobbe et al., 2021) for reasoning capabilities, and (3) ALFWorld (Shridhar et al., 2020) for planning capabilities. Dataset details are available in Appendix C.\nWe consider the following baselines: (1) Baselines in personalization, we consider both one-stage and two-stage personalization models, including Profile-Augmented Generation (PAG) (Richardson et al., 2023) and Retrieval-Augmented Generation (RAG) (Salemi et al., 2023). (2) Baselines in reasoning, we include Chain-of-Thoughts (CoT) (Wei et al., 2022), Least-to-Most (Zhou et al., 2023), Program-Aided Language Models (PAL) (Gao et al., 2023), and PALSelf-Debug (Chen et al., 2023). (3) Baselines in planning, we mainly compare Matryoshka with BUTLER (Shridhar et al., 2020), ReAct (Yao et al., 2023), Reflextion (Shinn et al., 2023), and AdaPlanner (Sun et al., 2024a). Baseline details can be found in Appendix D.\nFor the personalization tasks, consistent with the evaluation metrics specified in LaMP (Salemi et al., 2023), we use accuracy (Acc) and F1 score (F1) for the classification tasks in LaMP-2N and LaMP-2M. For the ordinal multi-class classification task in LaMP-3, we employ mean absolute error (MAE) and root mean squared error (RMSE). To comprehensively evaluate the personalized text generation tasks in LaMP-4 and LaMP-5, we report ROUGE-1 (R-1), ROUGE-L (R-L), and BLEU scores. For the math reasoning task, we assess the models based on the accuracy of obtaining the final correct answer. For the planning task, consistent with previous works (Sun et al., 2024a), we evaluate performance using the success rate (%). The success rate is calculated as the number of successful episodes divided by the total number of episodes. In ALFWorld, an episode is considered a failure if the task remains unsolved after executing 50 actions, which is the maximum allowed number of actions per episode.\nFor the white-box LLM controller, we utilize LLAMA-3-8B-Instruct as the backbone language model. In the black-box LLM environment, our experiments employ gpt-40-mini for personalization tasks in LaMP, and gpt-3.5-turbo for reasoning and planning tasks in GSM8K and ALFWorld, respectively. All experiments with GPTs are conducted using the Microsoft Azure OpenAI service. Please refer to Appendix E for implementation details."}, {"title": "PERSONALIZATION: LAMP", "content": "Table 1 summarizes the primary experimental results on the LaMP dataset. Our proposed method, Matryoshka, consistently outperforms or matches other state-of-the-art baselines, highlighting its efficacy of advancing black-box LLMs in personalization. For classification tasks, Matryoshka achieves an accuracy of 0.832 on LaMP-2N and 0.535 on LaMP-2M, surpassing other baselines by a significant margin. For generation tasks, Matryoshka also attains over a 25% improvement in BLEU score on LaMP-4. These results demonstrate the effectiveness of Matryoshka in both classification and generative personalization tasks. Furthermore, Matryoshka has the potential to be enhanced with RAG, combining intermediate generations with the retrieved user history data to improve performance further.\nIn our ablation studies, we compare our proposed method, Matryoshka, with a baseline lacking Intermediate Guidance Optimization (IGO) in Table 1. Using the same black-box model (gpt-40-mini), our optimized white-box controller consistently and significantly"}, {"title": "REASONING: GSM8K", "content": "Table 3 presents the main results on the GSM8K dataset. We employ a three-shot prompt design across all baselines, including ours. PALSelf-Debug refers to the addition of close-loop refinement to PAL during the inference stage. Our method consistently outperforms all baselines across the dataset, surpassing the strongest baseline, PALSelf-Debug, by a margin of 4.2% when using the base LLM. This improvement stems from the optimized intermediate guidance generated by Matryoshka. Conditioned on this guidance, Matryoshka enables the black-box LLM to generate long-horizon solutions to solve the tasks. Similar to LaMP, Matryoshka trained with gpt-3.5-turbo can be seamlessly applied to other black-box models for solving mathematical problems on GSM8K without additional training costs. Notably, Matryoshka learns high-level planning abilities without focusing on specific details, which broadens its applicability."}, {"title": "PLANNING: ALFWORLD", "content": "Matryoshka consistently outperforms existing baselines, achieving state-of-the-art performance with an overall success rate of 95.52% on ALFWorld tasks (Table 4). This superior performance indicates that Matryoshka effectively generates plans to guide the task execution of the black-box model, enhancing its ability to interact with the environment. Furthermore, we observe that Matryoshka exhibits superior performance compared to both the untuned white-box model (w/o Guidance Optimization) and the white-box models trained with fewer rounds of Intermediate Guidance Optimization (w/o 1st/2nd-round IGO). As the number of IGO training rounds increases, Matryoshka's performance on ALFWorld correspondingly improves, ultimately raising the success rate from 81.34% to 95.52%. These results underscore the efficacy of the IGO training employed in Matryoshka."}, {"title": "RELATED WORKS", "content": "Existing approaches aiming to enhance the generation capabilities of black-box LLMs can be broadly categorized into two groups: (1) ICL- and (2) adapter-based methods. ICL-based methods (Sun et al., 2024a; Tan et al., 2024a; Zhuang et al., 2024b) are designed to augment the original query with carefully crafted instructions or meticulously constructed few-shot demonstrations to guide the model. While this enables the black-box LLM to exhibit specific capabilities or adhere to particular directives, these methods require significant human effort in prompt engineering and result in prompts that are rigid and static. Adapter-based methods (Sun et al., 2024b; Shi et al., 2024a; Zhuang et al., 2024b) follow a best-of-N selection evaluation paradigm (Lightman et al., 2023). Given a problem, adapter-based methods generate N candidate solutions from the generator and subsequently evaluate them using a lightweight adapter to identify the highest-scoring solution as the final answer. However, such methods are heavily dependent on the generative capabilities of the black-box LLM, which may result in selecting a suboptimal candidate as the best of a bad bunch.\nSuperICL (Xu et al., 2023) incorporates outputs from smaller language models (LMs) as complementary information for input queries, integrating them into the context provided to black-box LLMs. However, these smaller LMs are fixed and can only support classification tasks that rely on label predictions with associated confidence scores. HYDRA (Zhuang et al., 2024b) is a retrieval-augmented generation framework that trains a BERT-sized reranker to reorder retrieved passages to better cater to user-specific requirements. Nevertheless, these methods apply only discrete optimization on the prompt through reranking and selection of few-shot demonstrations, which limits the potential improvements achievable via prompt engineering.\nAs LLMs scale, new capabilities emerge, enabling models to learn tasks efficiently through a few in-context demonstrations. To harness these capabilities, several approaches have been proposed to leverage reinforcement learning for improved prompt generation, enhancing LLM performance. RLPrompt (Deng et al., 2022) introduces an RL-based framework for generating optimal prompts via black-box optimization. Similarly, TEMPERA (Zhang et al., 2023) formulates prompt optimization as test-time prompt editing, using RL to efficiently explore the editing space. BDPL (Diao et al., 2023) further advances this by proposing a variance-reduced policy gradient algorithm to estimate gradients of parameters in the categorical distribution of each discrete prompt. However, these methods primarily focus on classification tasks, where gradient estimation is straightforward, limiting their applicability to more complex generation tasks requiring long-horizon solutions."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We introduced Matryoshka, a lightweight white-box LLM controller designed to augment the capabilities of large-scale black-box LLMs across a wide range of complex tasks, including reasoning, planning, and personalization. By leveraging a controller-generator framework with environmental feedback, Matryoshka effectively decomposes complex tasks and guides black-box LLMs through intermediate guidance. Through policy gradient optimization, Matryoshka exhibits a self-improving nature that continually enhances LLM capabilities via multi-turn guidance optimization. Extensive experiments on three diverse datasets demonstrate its effectiveness in steering black-box LLMs for long-horizon tasks without requiring access to model parameters or output probabilities. Compared to the best-performing state-of-the-art baselines, Matryoshka achieves average improvements of 3.19% in reasoning tasks, 7.46% in planning tasks, and 5.82% in personalization tasks. These results underscore the potential Matryoshka as a transparent and scalable solution, enabling white-box LLMs to drive black-box LLMs in complex problem-solving. Future work could extend Matryoshka to tackle more complex applications requiring long-horizon generation and reasoning, such as solving software engineering problems and proving mathematical theorems. Additionally, the controller component of Matryoshka could be developed into a self-enhancing mechanism or a universal controller applicable to a wide range of real-world applications."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "The datasets utilized in this study are all publicly available, including LaMP for the personalization task, GSM8K and GSM-Hard for the reasoning task, and ALFWorld for the planning task. Detailed descriptions of these datasets and their corresponding tasks are provided in Appendix C. In Appendix D, we outline the baselines used for comparison and describe the experimental setup. Appendix E offers an in-depth explanation of the main experiments, including hardware and software configurations, hyperparameter settings, and step-by-step procedures for the three tasks. Additionally, Appendix F presents case studies for each task, demonstrating the superior performance of our method compared to the baselines in a more intuitive manner. Appendix G details the prompts used for each task. The implementation of Matryoshka is provided in the supplementary materials and will be released publicly available on GitHub upon acceptance."}, {"title": "ETHICS STATEMENT", "content": "We strictly followed the data usage guidelines for interactions with Microsoft Azure's OpenAI API and Gemini API service. Although our research relied solely on publicly available datasets, we took extra precautions to minimize any potential risk of information leakage. Specifically, we opted out of the human review process by completing and submitting the Azure OpenAI Additional Use Case Form\u00b2. This proactive measure highlights our commitment to maintaining the highest data privacy standards and ethical research practices, especially concerning personalization tasks."}, {"title": "LIMITATIONS AND BROADER IMPACTS", "content": "In this study, we propose a modular framework, Matryoshka, that leverages a lightweight white-box LLM controller to enhance the capabilities of black-box LLMs. Despite its effectiveness, we have identified several potential limitations of Matryoshka:\nSince Matryoshka employs a white-box LLM controller to augment black-box LLMs, there are notable risks to consider. Malicious actors could exploit this approach to engineer harmful capabilities or generate toxic content for training purposes. While black-box LLMs are designed to resist producing such content, our controller could be misused to manipulate these models into generating undesirable outputs. Furthermore, there is a risk that the intermediate guidance produced by our controller could be exploited to extract sensitive information from black-box LLMs, potentially facilitating jailbreaking or other targeted attacks.\nMatryoshka preserves the confidentiality of training data by avoiding third-party API sharing, thereby safeguarding the integrity of training samples during the enhancement process of black-box LLMs. However, when applied to personalization tasks, it is important to recognize that retrieved historical records or the queries themselves may inadvertently contain sensitive information, potentially risking unintended disclosure of private data."}, {"title": "BROADER IMPACTS", "content": "The proposed Matryoshka framework addresses a critical challenge in consistently enhancing the capabilities of black-box LLMs for long-horizon tasks with broad scopes. By improving reasoning, planning, and personalization, Matryoshka can deliver significant benefits across various domains. For instance, it can provide insights into complex theorems, advance industrial automation, and offer more personalized interactions for end users. Overall, Matryoshka has the potential to facilitate more useful, relevant, and satisfying interactions, thereby improving productivity, decision-making, and quality of life. Moreover, Matryoshka operates without requiring access to the model weights of black-box LLMs, making the technology accessible to a wide range of off-the-shelf LLM APIs and enabling seamless integration into diverse use cases. By leveraging existing LLMs, Matryoshka can be readily adopted by researchers, developers, and organizations, accelerating the development and deployment of advanced language models in real-world applications.\nEnhancing black-box LLMs through a small-scale white-box LLM introduces potential risks. One significant concern is the possibility of using the white-box model to jailbreak black-box LLMs, injecting malicious instructions or producing harmful content. This could lead to the spread of misinformation, hate speech, or other offensive materials, with severe consequences for individuals and society. Additionally, this approach poses a threat to user data privacy. Training the white-box model requires collecting and storing interaction data between the black-box LLM and the environment, which could be improperly handled or misused, potentially compromising sensitive information."}, {"title": "ADDITIONAL RELATED WORKS", "content": "Proximal policy optimization (PPO) (Schulman et al., 2017) is the predominant deep reinforcement learning method used in RLHF, leading to significant successes in models like InstructGPT (Ouyang et al., 2022), ChatGPT (Achiam et al., 2023), and Gemini (Reid et al., 2024). However, applying PPO requires extensive effort and resources (Choshen et al., 2019; Engstrom et al., 2020; Tang et al., 2024), often beyond the scope of open-source capabilities. To simplify implementation and streamline the training process, recent works (Azar et al., 2024; Ethayarajh et al., 2024) have proposed direct preference learning algorithms following the DPO framework (Rafailov et al., 2024b). These algorithms bypass the reward modeling step and directly optimize carefully designed loss objectives on the preference dataset, hence the term direct preference learning.\nRecent advances in self-improvement methods for language models fall broadly into two categories: (1) online fine-tuning approaches and (2) bootstrapping methods. Fine-tuning approaches aim to enhance models by adjusting their parameters based on additional data or objectives. Notable methods include Rejection Fine-Tuning (RFT) (Yuan et al., 2023), which augments the training set with correct completions; Alignment Fine-Tuning (AFT) (Wang et al., 2023), which introduces an alignment loss to increase the probabilities of correct chain-of-thoughts; Reinforced Fine-Tuning (ReFT) (Luong et al., 2024), which applies reinforcement learning to token prediction; and self-play (Chen et al., 2024), which iteratively refines the model using its own previous outputs. Bootstrapping methods, on the other hand, leverage the model's own generations to create new training data. Notable examples include Self-Taught Reasoner (STaR) (Wu et al., 2024), which iteratively samples high-quality data; Reinforcement and Self-Training (ReST) (Gulcehre et al., 2023) and its simplified version ReSTEM (Singh et al., 2023), which alternate between data generation and reward-based optimization; and Verified Self-Taught Reasoner (V-STaR) (Hosseini et al., 2024), which combines self-training with outcome-based verification. Collectively, these approaches offer diverse strategies for enhancing model performance through targeted training and iterative refinement, highlighting the potential for self-improvement in language models."}, {"title": "DATASET AND TASK DETAILS", "content": "We employ the Language Model Personalization (LaMP) benchmark (Salemi et al., 2023), an open-source benchmark specifically designed to train and evaluate the capability of language models in generating personalized content. LaMP encompasses a diverse set of tasks (with LaMP-2 comprising two tasks, LaMP-2N, and LaMP-2M), covering both personalized text classification and generation tasks. The dataset statistics are presented in Table 5 for a clear overview of its structure. Below are detailed descriptions of each task:\nA binary text classification task aimed at citation recommendation. The task assesses the language model's ability to identify a user's citation preferences. Given a user and their authored paper, the model predicts which of two candidate papers the user is more likely to cite. The user's profile contains titles and abstracts of their authored papers.\nA categorical text classification task that involves categorizing news articles into one of 15 categories based on a journalist's profile. Given an article written by a user, the model predicts its category using the user's history of articles and their categories.\nAn ordinal text classification task focused on predicting one of 15 tags for a movie based on a user's tagging history. The task evaluates the model's ability to assign tags to a movie description using historical user-specific movie-tag pairs.\nA text classification task that involves predicting product ratings, framed as a five-class problem. The model must predict a rating between one and five for a product review, using the user's past review and rating history. This task tests the model's ability to capture user-specific rating patterns.\nA text generation task in which the model generates personalized news headlines for articles based on the author's past article-title pairs. The task assesses the model's ability to replicate the author's stylistic preferences when creating headlines.\nLaMP-6 has been excluded because the dataset is not publicly available. Furthermore, Tasks 1, 2, and 3 above cover personalization classification tasks, Task 4 covers personalization rating tasks, and Task 5 covers personalization generation tasks. Therefore, the tasks we selected encompass all categories of tasks in the LaMP benchmark.\nGSM8K (Cobbe et al., 2021) is a dataset focused on high school-level mathematical reasoning. The numerical reasoning tasks within this dataset typically consist of a descriptive scenario followed by a culminating question. Answering these questions requires performing multi-step mathematical calculations based on the context provided in the description."}, {"title": "PLANNING: ALFWORLD", "content": "AlfWorld (Shridhar et al., 2020) is a comprehensive suite of synthetic, text-based environments set within a virtual household, featuring six distinct task types: Pick, Clean, Heat, Cool, Examine, and Pick Two. Each task presents a unique high-level objective (e.g., \"put a vase in the safe\") that requires the agent to navigate and interact with various objects or receptacles (e.g., go to shelf 6, clean apple). To accomplish the assigned task, the agent must execute a series of actions to achieve the specified goal. However, the challenge lies in the object's potential location - it could be in any of over 50 possible places within a given task instance - necessitating sequential exploration of each location by the agent. Consequently, the complete action sequence may encompass more than 50 discrete actions, posing a considerable challenge to the agent's capabilities and efficiency."}, {"title": "BASELINE DETAILS", "content": "We compare our proposed Matryoshka with several competitive baselines", "baseline": "nMatryoshka w/o IGO utilizes the controller model Llama-3-8B-Instruct to first generate a summary of the user's retrieved history data. It then combines this summary with the input question as prompts for the environment model gpt-40-mini to generate the final answer.\nFor all baselines, we employ gpt-3.5-turbo as the black-box model to facilitate the description of their processes with 3-shot prompt template. The ablated baselines primarily focus on problem decomposition, including Matryoshka w/o IGO. The remaining baselines for mathematical reasoning consist of CoT (Wei et al., 2022), Least-to-Most (Zhou et al., 2023), PaL (Gao et al., 2023), and PALSelf-Debug (Chen et al., 2023).\nMatryoshka w/o IGO first utilizes a vanilla LLAMA3-8B-Instruct to break down the problem into sub-questions, and then gpt-3.5-turbo provide solutions based on both the main problem and the decomposed sub-questions.\nCoT uses gpt-3.5-turbo to break the problem down into a series of intermediate reasoning steps that ultimately lead to the final answer.\nPaL utilizes gpt-3.5-turbo to interpret natural language problems and generate programs as intermediate reasoning steps, delegating the solution process to a runtime environment like a Python interpreter.\nPALSelf-Debug builds upon PaL by introducing a close-loop refinement during the inference phase. Specifically, if the code generated by PaL encounters issues during execution, gpt-3.5-turbo is instructed to reflect on"}]}