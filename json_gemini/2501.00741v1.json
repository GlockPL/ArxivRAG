{"title": "Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors", "authors": ["Chuanzhi Xu", "Langyi Chen", "Vincent Qu", "Haodong Chen", "Vera Chung"], "abstract": "Neuromorphic cameras, also known as event cameras, are asynchronous brightness-change sensors that can capture extremely fast motion without suffering from motion blur, making them particularly promising for 3D reconstruction in extreme environments. However, existing research on 3D reconstruction using monocular neuromorphic cameras is limited, and most of the methods rely on estimating physical priors and employ complex multi-step pipelines. In this work, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our method incorporates a novel event representation to enhance edge features, enabling the proposed feature-enhancement model to learn more effectively. Additionally, we introduced Optimal Binarization Threshold Selection Principle as a guideline for future related work, using the optimal reconstruction results achieved with threshold optimization as the benchmark. Our method achieves a 54.6% improvement in reconstruction accuracy compared to the baseline method.", "sections": [{"title": "I. INTRODUCTION", "content": "3D reconstruction in VR/AR applications enables realistic restoration of scenes and objects, serving as a 3D form of information present that provides users with a more immersive experience [1], [2]. Many devices can be used to collect data for 3D reconstruction, including traditional RGB cameras, RGB-D cameras, LiDAR, structured light systems, etc. However, they have different limitations, including limited dynamic range, motion blur, high power consumption, etc. [3].\nNeuromorphic cameras, also known as event cameras, are bio-inspired sensors responding to local brightness changes [3]. Each pixel in a neuromorphic camera operates independently and asynchronously, which differs from traditional (frame-based) RGB cameras that capture pictures with a shutter and have a fixed interval in recording video [3]\u2013[5]. Neuromorphic cameras report changes in brightness only when a threshold is reached. When there is a greater change in brightness in the scene or object, such as when the object is moving faster, more event data will be generated. The neuromorphic camera produces a continuous stream of events, which includes the coordinates, precise timestamp, and the polarity of brightness change. Neuromorphic cameras in 3D reconstruction tasks can be divided into stereo and monocular types. Stereo neuromorphic cameras involve multiple rigidly connected neuromorphic cameras, providing information from multiple viewpoints. These methods typically perform scene scanning and produce real-time semi-dense reconstruction results. They often follow the classical two-step stereo solution: matching disparities at the same timestamps and then triangulating 3D points to calculate the depth information of every point in the scene [3], [6]. However, performing 3D reconstruction with a monocular neuromorphic camera, which lacks disparity information, often requires more complex computations. Such methods must be combined with the physical prior knowledge (e.g., camera trajectories) to achieve similar results as parallax in the stereo task. The physical prior is always required, and it can either be obtained by simultaneous mapping with other devices or be predicted by a Visual Odometer (VO) or SLAM algorithm targeting the event stream [6]. Moreover, these methods all require a complex event-to-3D pipeline [7]\u2013[10]. The pipeline is a full workflow that optimally processes event data and reconstructs it into a 3D model, including steps such as event representation, prior estimation, computation of parallax, triangulation, depth estimation, and 3D model reconstruction. Event processing pipelines are a common research direction in these tasks. A recent study introduced a neural network, E2V [1], capable of directly taking the represented event stream as input and outputting voxel results. This approach made progress in simplifying the event processing pipeline. It is also the first method to perform voxel-based 3D reconstruction using a monocular neuromorphic camera. However, it leaves substantial room for improvement in reconstruction accuracy.\nIn this research, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our contributions can be summarized as:\n\u2022 We propose a novel event representation method, Sobel Event Frame, which enhances edge features and restrains redundant data in the event stream, enabling effective learning of 3D features.\n\u2022 We propose a 3D reconstruction model designed to enhance the learning of edge features in event streams with Efficient Channel Attention, building on the approach of event-based 3D reconstruction without the need for physical priors or pipelines.\n\u2022 We propose Optimal Binarization Threshold Selection Principle and suggest it as a guideline for future research about event-based 3D reconstruction with deep learning."}, {"title": "II. RELATED WORK", "content": "Most methods for performing spatial scanning and instantaneous 3D reconstruction using a monocular neuromorphic camera are based on physical and geometric computations."}, {"title": "B. Deep Learning-based Methods", "content": "Recent methods using monocular neuromorphic cameras for 3D reconstruction rely on existing synthetic or real event datasets to train deep learning models for dense 3D reconstruction results. However, pipelines and the estimation of physical priors remain indispensable.\nBaudron et al. proposed the E3D method, including a pipeline with a neural network [7], E2S, to convert event frames into contours while using an additional neural branch for camera pose regression, ultimately generating multi-view mesh reconstruction results. Xiao et al. utilized the E2VID [4] deep learning method to process continuous event streams and output normalized intensity image sequences [8]. They used Structure-from-Motion (SfM) to estimate sparse intrinsic, extrinsic point clouds, followed by Multi-View Stereo (MVS) techniques to complete dense reconstruction. Wang et al. proposed a method that includes a physical prior extraction branch and a NeRF [14] rendering branch [9]. By incorporating an event warping field and a deterministic event generation model, they integrated physical priors into the NeRF pipeline. They also introduced a novel probabilistic chunk sampling strategy based on spatial event density, which helps the model learn local geometric features more robustly and efficiently.\nThe study done by Chen et al. in 2023 serves as the baseline for this research [1]. It is the first study to use the voxel grid to represent event-based 3D reconstruction results. The study primarily introduces a deep learning model, E2V, which consists of a 3D event frame encoder to transform the feature representation of the data and a 3D voxel decoder to convert these features into a 3D voxel grid. Additionally, this research proposed a dataset, SynthEVox3D, for event-based 3D reconstruction. This method opened up a new direction for event-based 3D reconstruction, while its reconstruction accuracy on mIoU was only 0.346, which shows room for further improvement."}, {"title": "III. METHOD", "content": "We propose an end-to-end method for dense voxel 3D reconstruction using monocular neuromorphic camera that eliminates the need to estimate physical priors."}, {"title": "A. Novel Event Representation: Sobel Event Frame", "content": "Event representation refers to preprocessing the data captured by the neuromorphic camera, which encodes the information of the event's coordinate, timestamp, and polarity.\nEvent Frame is the most commonly used event representation method, referring to all approaches that can frame event streams. Event Frame divides event data into multiple fixed-length time windows, either based on timestamp or event count. Each time window generates an image-like frame where each pixel represents whether an event occurred within that time window. Pixels where no event occurred during the time window are filled with 0.\nEach event frame $F_t(x, y)$ is a pixel value generated in the time window t, where $(x, y)$ represents the pixel coordinates. $E_i(x, y)$ represent the i-th event in the event stream within time window t. $p_i$ denote the polarity of event $E_i$ (with +1 for positive polarity and -1 for negative polarity). n is the total number of events in the time window t."}, {"title": "We summarize all five design modes of Event Frame as follows:", "content": "a) Last Positive Event (Pos): If the last event at each pixel within the time window has a positive polarity, it is marked as 1, otherwise 0.\n$F_t(x, y) =\\begin{cases} 1, & \\text{if } p_n = +1, \\\\ 0, & \\text{if } p_n \\neq +1. \\end{cases}$\nb) Last Negative Event (Neg): If the last event at each pixel within the time window has a negative polarity, it is marked as 1, otherwise 0.\n$F_t(x,y) =\\begin{cases} 1, & \\text{if } p_n = -1, \\\\ 0, & \\text{if } p_n \\neq -1. \\end{cases}$\nc) Last Event Polarity (Last): If the last event at each pixel within the time window has a positive polarity, it is marked as 1, and if negative, it is marked as -1.\n$F_t(x, y) = p_n$.\nd) Any Event (Any): If any event occurs at a pixel within the time window, regardless of polarity, it is marked as 1.\n$F_t(x,y) =\\begin{cases} 1, & \\text{if } n > 0, \\\\ 0, & \\text{if } n = 0. \\end{cases}$\ne) Separate Frames for Polarity (Sep): For each time window, a positive event frame $F_t^+(x,y)$ is generated for positive events in a time window, and a negative event frame $F_t^-(x, y)$ is generated for negative events in a time window.\n$F_t^+(x,y) =\\begin{cases} 1, & \\text{if any } p_i = +1, \\\\ 0, & \\text{otherwise}. \\end{cases}$\n$F_t^-(x, y) =\\begin{cases} 1, & \\text{if any } p_i = -1, \\\\ 0, & \\text{otherwise}. \\end{cases}$\nThe Sobel operator is an image processing algorithm primarily used for edge detection. It computes the gradient of pixel intensity in both the horizontal (x-axis) and vertical (y-axis) directions using convolution, aiming to highlight the edges in the image [15]."}, {"title": "B. Event-to-Voxel 3D ResNet with ECA", "content": "We propose a model to process event data to dense voxel 3D reconstruction, effectively incorporating the event data represented by Sobel Event Frame and further enhancing the learning on edge feature information.\nThe E2V model uses a deeper model architecture, ResNet-152, as the encoder to extract complex 3D features from the represented event stream [1]. However, due to massive information in 3D data, the extracted features may include irrelevant or redundant information (e.g., the object is primarily located in the center of the event frame, while no events occur around the edges), which limits the performance of model.\nInspired by Wang et al. [18], we introduce the Efficient Channel Attention (ECA) mechanism into the encoder with deeper convolutional layers. This mechanism aims to efficiently capture inter-channel interactions by employing a 1D"}, {"title": "C. Optimal Binarization Threshold Selection Principle", "content": "After generating the model, we use it to perform inference (prediction) on the testing data subset. The output of the model consists of continuous logits, which will be converted into continuous probability values $o(x)$ by Sigmoid function. By setting a binarization threshold $p$, we can determine whether a reconstructed point is present or absent as follows:\nOutput Binary Value =$\\begin{cases} 1, & \\text{if } o(x_{\\text{out}}) > p, \\\\ 0, & \\text{if } o(x_{\\text{out}}) \\leq p. \\end{cases}$\n,where $o(x_{\\text{out}}) = \\frac{1}{1+e^{-x_{\\text{out}}}}$\nPrevious studies used a fixed binarization threshold such as 20% [1] or 50% [7]. However, we found that the optimal threshold varies depending on the event representation method."}, {"title": "IV. EXPERIMENTS", "content": "Dataset: Synthetic Event Camera Voxel 3D Reconstruction Dataset (SynthEVox3D) is the only available dataset for 3D reconstruction with voxel label based on event data. SynthEVox3D consists of 39,739 event data samples in 13 categories sourced from ShapeNet [19], including Airplane, Bench, Cabinet, Car, Chair, Displayer, Lamp, Speaker, Rifle, Sofa, Table, Telephone, and Watercraft. Each category contains objects of different shapes, ensuring data differentiation. The event data are generated by all-angle scanning within 0.5s using a neuromorphic camera, producing event streams with a resolution of 512 \u00d7 512 pixels. SynthEVox3D-Tiny is a subset of SynthEVox3D, which randomly selects 80 samples from each category, making a total of 1,040 samples. These samples include 832 training data (80%), 104 validation data (10%), and 104 testing data (10%).\nEvent Representation: We use a fixed time window of 5.0\u00d710-3 to segment the event data into event frames, forming 100 frames (total duration of 0.5s). The karnel size of Sobel Event Frame is set to 3 \u00d7 3. After event representation, the event data is normalized to [0, 1] and rescaled to a shape of (n, 256, 256) for fitting the model input. To further prevent overfitting, we randomly apply operations such as flipping, rotation, and inversion of the event stream.\nDeep Learning: We set the batch size to 5 and use the Adam optimizer with a learning rate of 1.0 \u00d7 10-6. The optimizer is adopted with B = {0.9, 0.999}. The total number"}, {"title": "B. Experimental Processes & Results", "content": "First, we conducted training on the SynthEVox3D-Tiny dataset. We reproduced the E2V method using Event Frame (Pos) and evaluated it on the testing data subset as the baseline. At the optimal threshold of 0.22, the E2V method achieved mIoU of 0.358 and F-Score of 0.507.\nSubsequently, we tested E2V on the other four modes of Event Frame and evaluated all five modes of Sobel Event Frame using our method. The (Pos) and (Any) modes, which do not contain negative data, performed better under the current experimental settings. Therefore, in Tables I and II, we only present the results of them.\nUsing Sobel Event Frame (Pos) with a binarization threshold of 0.34, our method achieved mIoU of 0.523 and F-Score of 0.682. This represents an improvement of 0.165 (46.1%) in mIoU and 0.175 (34.5%) in F-Score compared to the E2V method with Event Frame (Pos)."}, {"title": "C. Ablation Study", "content": "Our method can be divided into three components: event representation, model, and binarization threshold selection, for conducting ablation studies to evaluate the contribution of each component to the reconstruction results.\nEvent Representation and Model: We conducted ablation studies on event representation and the model using the SynthEVox3D-Tiny dataset, referring to Table IV for the results. By using Event Frame (Pos) to process event data as input to our proposed model, we obtained a mIoU of 0.421 at a threshold of 0.21, which means our model brought about a 17.6% improvement. Using the E2V model, we also trained and tested with Sobel Event Frame (Pos), achieving a mIoU of 0.471 at a threshold of 0.33. In other words, relying solely on Sobel Event Frame resulted in a 31.6% improvement. The combination of both led to a further enhancement, reaching a mIoU of 0.523, a 46.1% improvement over the baseline.\nBinarization Threshold: To demonstrate that the binarization threshold should be optimally selected rather than fixed, we listed the mIoU and F-Score results using Sobel Event Frame (Pos) at thresholds ranging from 0.16 to 0.42 (with a step size of 0.02), as shown in Table V. The best results were achieved at a threshold of 0.34, reaching a mIoU of 0.523"}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "In this study, we proposed a novel event representation method, Sobel Event Frame, and an ECA-enhanced deep learning method for end-to-end 3D reconstruction using the monocular neuromorphic camera without relying on physical priors and pipelines. Additionally, we introduced a Optimal Binarization Threshold Selection Principle and advocated for it as a guideline for future event-based 3D reconstruction. Under our approach, we achieved a significant improvement in reconstruction accuracy, further bridging the gap with results from traditional camera-based 3D reconstruction.\nLooking ahead, we plan to expand this project further. Regarding event representation, our aim is to explore more potential representations and extend Sobel Event Frame to other computer vision tasks. Although we introduced Optimal Binarization Threshold Selection Principle, it is still necessary to investigate the factors that influence threshold selection. Most importantly, we need to test the performance of traditional methods and event-based methods under extreme conditions, such as very fast motion, low light, or high brightness scenarios, which will demonstrate the advantages of neuromorphic cameras in the field of 3D reconstruction."}]}