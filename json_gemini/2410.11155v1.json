{"title": "LATENT-PREDICTIVE EMPOWERMENT: MEASURING EMPOWERMENT WITHOUT A SIMULATOR", "authors": ["Andrew Levy", "Alessandro Allievi", "George Konidaris"], "abstract": "Empowerment has the potential to help agents learn large skillsets, but is not yet a scalable solution for training general-purpose agents. Recent empowerment methods learn diverse skillsets by maximizing the mutual information between skills and states; however, these approaches require a model of the transition dynamics, which can be challenging to learn in realistic settings with high-dimensional and stochastic observations. We present Latent-Predictive Empowerment (LPE), an algorithm that can compute empowerment in a more practical manner. LPE learns large skillsets by maximizing an objective that is a principled replacement for the mutual information between skills and states and that only requires a simpler latent-predictive model rather than a full simulator of the environment. We show empirically in a variety of settings- including ones with high-dimensional observations and highly stochastic transition dynamics-that our empowerment objective (i) learns similar-sized skillsets as the leading empowerment algorithm that assumes access to a model of the transition dynamics and (ii) outperforms other model-based approaches to empowerment.", "sections": [{"title": "1 INTRODUCTION", "content": "Empowerment offers an intuitive approach for training agents to have large skillsets. In an empowerment-based approach, the empowerment for a variety of states is first computed, in which the empowerment of a state measures the size of the largest skillset in that state (Klyubin et al., 2005; Salge et al., 2013a). The resulting state empowerment values are then used as a reward in a Rein-forcement Learning (Sutton & Barto, 1998) setting, encouraging agents to take actions that grow the size of their skillsets (Klyubin et al., 2008; Jung et al., 2012; Mohamed & Jimenez Rezende, 2015).\nA major obstacle to implementing the empowerment-based approach for training generalist agents is that there is not yet a scalable way to compute the empowerment of a state. Recent approaches try to measure empowerment of a state by searching for a skillset (e.g., a skill-conditioned policy) that maximizes a particular lower bound on the mutual information between skills and states (Gregor et al., 2016; Eysenbach et al., 2019; Achiam et al., 2018; Lee et al., 2019; Choi et al., 2021; Strouse et al., 2021; Levy et al., 2024a), which measures skillset diversity by capturing how distinct the skills are from one another in terms of the states they target. The problem with this approach is that it can require an infeasible amount of interaction with the environment prior to each update to the skillset. To estimate the mutual information lower bound for a single skillset in a single state, many skills need to be executed in the environment from the state under consideration to obtain the needed tuples of skills and skill-terminating states. But because empowerment seeks to find the most diverse skillset for a distribution of states, tuples of skills and states need to be collected for many skillsets (e.g., skill-conditioned policies with small differences from the current skill-conditioned policy) starting from many states. Because this amount of interaction prior to each update to the skillset is intractable, recent empowerment approaches assume the agent has access to a model of the transition dynamics (i.e., a simulator of the environment) (Eysenbach et al., 2019; Gu et al., 2021; Levy et al., 2024b;a). But this is not a scalable assumption because a model of the transition dynamics is typically not available and can be difficult to learn in settings with high-dimensional and stochastic observations."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 SKILLSET MODEL AND EMPOWERMENT", "content": "We model an agent's skillset in a state using a probabilistic graphical model defined by the tuple (S, A, Z, T, \u03c6, \u03c0). S is the space of states; A is the space of actions; Z is the space of skills; T is the transition dynamics distribution T(St+1|st, at) that provides the probability of a state given the prior state and action. The transition dynamics are assumed to be conditionally in-dependent of the history of states and actions (i.e., T(st+1|St,at) = T(st+1|80,ao, ..., St, at)).\nThe remaining distributions \u03c6 and \u03c0 are the learnable distributions in a skillset. \u03c6 repre-sents the distribution over skills (z|so) given a skill start state so. \u03c0represents the skill-conditioned policy \u03c0(at|st, z) that provides the distribution over primitive actions given a state st and skill z. Assuming each skill consists of n primitive actions, the full joint distribution of a skill and a trajectory of actions and states (z, a0, S1,..., an-1,Sn) conditioned on a particu-lar start state so and skillset defined by \u03c6 and \u03c0 is given by p(z, ao, S1,..., an\u2081, Sn|50, \u03a6, \u03c0) = \u03c6(z|80)\u03c0(\u03b1\u03bf|8o, z)p(81|80, \u03b1\u03bf) ...\u03c0(an\u22121|Sn\u22121,z)p(Sn|Sn-1, an-1). Note that this definition is for closed loop skills. Skillsets can also use open loop skills, in which the skill-conditioned pol-icy would be defined defined by the distribution \u03c0(\u03b1|80, z). The output of the open loop skill-conditioned policy a is a concatenation of n primitive actions (i.e., a = [ao, . . ., an\u22121]). The joint distribution for a skillset containing open loop skills is the same as for closed loop skills except there is only one sample taken from the skill-conditioned policy, which includes all n primitive actions.\nIn this paper, we measure the diversity of a skillset defined by \u03c6 and \u03c0 using the mutual infor-mation between the skill random variable Z and the skill-terminating state random variable Sn, I(Z; Sn|50, \u03a6, \u03c0). This mutual information measures the number of distinct skills in a skillset, in which a skill is distinct if it targets a set of states not targeted by other skills in the skillset. I(Z; Sn|50, \u03c6, \u03c0) is defined\n$I(Z; S_n|s_0, \\phi, \\pi) = H(Z|s_0, \\phi, \\pi) - H(Z|s_0, \\phi,\\pi, S_n)$\n= $E_{z\\sim \\phi(z|s_0),s_n\\sim p(s_n|s_0,\\pi,z)} [log p(z|s_0, \\phi, \\pi, s_n) - log p(z|s_0, \\phi)]$"}, {"title": "2.2 SKILLSET EMPOWERMENT", "content": "A leading algorithm for computing empowerment is Skillset Empowerment (Levy et al., 2024a), which measures a variational lower bound on empowerment, \u0190(50), defined as follows:\n$E(s_0) = max_{\\phi,\\pi} \\hat{I}(Z; S_n|s_0, \\phi, \\pi)$,\n$\\hat{I}(Z; S_n|s_0, \\phi, \\pi) = E_{z\\sim \\phi(z|s_0),S_n\\sim p(s_n|s_0,\\pi,z)} [log q_{\\psi^*}(z|s_0, \\phi, \\pi, s_n) - log \\phi(z|s_0)]$,\n$\\psi^* = arg min_{\\psi} D_{KL} (p(z|s_0, \\phi, \\pi, S_n)||q_{\\psi}(z|s_0, \\phi, \\pi, s_n))$.\nSkillset Empowerment measures a tighter lower bound on empowerment than prior work (Gregor et al., 2016; Eysenbach et al., 2019; Achiam et al., 2018; Lee et al., 2019; Choi et al., 2021; Strouse et al., 2021) because, for any candidate (\u03c6, \u03c0) skillset, it learns a tighter variational lower bound \u00ce(Z; Sn|50, \u03c6, \u03c0) on the true mutual information I(Z; Sn|so, \u03c6, \u03c0) as a result of (i) conditioning the variational posterior, qy(z|50, \u03a6, \u03c0, sn), on the (\u03c6, \u03c0) skillset distributions and then (ii) training the variational posterior qy(z|50, \u03c6, \u03c0, sn) for a candidate (\u03c6, \u03c0) skillset to match the true posterior p(z|50, \u03a6, \u03c0, 5n) of the candidate skillset. As a result of this tighter lower bound on empowerment, Skillset Empowerment was the first unsupervised skill learning algorithm to learn large skillsets in domains with stochastic and high-dimensional observations. Skillset Empowerment maximizes the variational mutual information \u00ce(Z; Sn|so, \u03c6, \u03c0) with respect to the skillset distributions \u03c6 and \u03c0 using a particular actor-critic architecture. We will be using the same actor-critic architecture in our approach, so we review this architecture in section A of the Appendix.\nThe problem with Skillset Empowerment is that it is not a scalable approach for measuring the empowerment of a state because it assumes a model of the transition dynamics, p(st+1|St, at), is either provided or learned. But this is not a practical assumption in real world settings where a simulator of the environment is typically not available and too difficult to learn because it is hard to predict high-dimensional and stochastic observations. Skillset Empowerment requires a model of the transition dynamics because of the large number of skills that need to be executed in the environment to obtain the (skill z, skill-terminating state sn) tuples needed to optimize the objec-tive. In order to estimate the variational mutual information I(Z; Sn|80, \u03a6, \u03c0) for a single candidate skillset (\u03c6, \u03c0), Skillset Empowerment requires many (z, sn) tuples generated by this (\u03c6, \u03c0) skillset to learn the parameters * for the variational posterior. This is because in practice the KL di-vergence minimization objective provided in equation 5 is implemented as a maximum likelihood objective: Ez~$(z|80),sn~p(sn|80,\u03c0,z)[log qy (z|80, \u03c6, \u03c0, 8n)], which requires (z, sn) samples to find the best fitting variational posterior q\u2084(z|so, \u03c6, \u03c0, \u03c2\u03b7). But in order to learn the empowerment of a state, or the maximum mutual information with respect to (\u03c6, \u03c0), the variational mutual lower bound needs to be estimated for a large number of combinations of skill distributions (z|80) and skill-conditioned policies \u03c0(at|st, z). In Skillset Empowerment specifically, the variational lower bound on mutual information needs to be computed for potentially thousands of skill-conditioned policies \u03c0, in which each policy contains a small change to one of the parameters of the current skill-conditioned policy neural network (see section A for a review of how Skillset Empowerment optimizes I(Z; Sn|50,\u03a6, \u03c0) with respect to \u03c0). Obtaining the required (z, sn) tuples for a large number of (\u03c6, \u03c0) skillsets in an online fashion is not practical, which is why Skillset Empowerment requires access to a model of the transition dynamics p(st+1|st, at)."}, {"title": "3 LATENT-PREDICTIVE EMPOWERMENT", "content": "We introduce a new algorithm, Latent-Predictive Empowerment (LPE), that can measure the em-powerment of a state in a more scalable manner. The key component of our algorithm is our objec-tive for learning diverse skill-conditioned policies. Instead of maximizing the mutual information between skills and states with respect to the skill-conditioned policy, we maximize an alternative objective that has the same optimal skillset under certain conditions, but is also more tractable to maximize because it only requires learning a latent-predictive model rather than a full simulator of an environment. We maximize skillset diversity using the same actor-critic structures as used by Skillset Empowerment, which is reviewed in Appendix section A."}, {"title": "3.1 TRAINING OBJECTIVE FOR SKILL-CONDITIONED POLICY ACTOR", "content": "In Latent-Predictive Empowerment, the objective used to train the skill-conditioned policy actor so that it outputs diverse skill-conditioned policies \u03c0 given a skill start state so and skill distribution \u03c6 is:\n$E_{LPE,\\pi}(s_0, \\phi) = max_{\\pi} J(s_0, \\phi, \\pi)$,\n$J(s_0, \\phi, \\pi) = \\hat{I}(Z; Z_n|s_0, \\phi, \\pi) - E_{(a,s_n)\\sim p(a,s_n|s_0,\\pi)}[D_{KL}(p_{\\xi}(z_n|s_0, \\phi, \\pi, a)||p_{\\eta}(z_n|s_0, \\phi, \\pi, s_n))]$\n$\\hat{I}(Z; Z_n|s_0, \\phi, \\pi) = E_{z\\sim \\phi(z|s_0),a\\sim \\pi(a|s_0,z),z_n\\sim p_{\\xi}(z_n|s_0,\\phi,\\pi,a)} [log q_{\\zeta}(z|s_0, \\phi, \\pi, z_n) - log \\phi(z|s_0)]$.\nThat is, for a given skill start state so and skill distribution size \u03c6, Latent-Predictive Empowerment seeks to find the most diverse skill-conditioned policy \u03c0, in which skillset diversity is measured by J(80, \u03c6,\u03c0). J(50, \u03c6, \u03c0) consists of the difference of two terms and provides an intuitive way to measure skillset diversity, which we describe next."}, {"title": "3.1.1 INTUITIVE SKILLSET DIVERSITY OBJECTIVE", "content": "The first term, I(Z; Zn|80, \u03c6, \u03c0), in LPE's objective for measuring skillset diversity is the variational lower bound on the mutual information between skills and latent state representations, in which the latent state is generated by the latent-predictive model pg(zn|50, \u03a6, \u03c0,\u03b1), which maps open loop action sequences a to the latent vector zn for the given skill start state so and skillset distributions \u03c6 and \u03c0. This is a variational lower bound on mutual information because the variational posterior q\u03c8(2/80, \u03a6, \u03c0, \u0396n) replaces the intractable true posterior p(z|so, \u03c6, \u03c0, \u0396\u03b7) (Barber & Agakov, 2003).\nGiven that \u00ce(Z; Zn|so, \u03c6, \u03c0) is a lower bound on the mutual information between skills and actions \u0399(\u0396; \u0391\u038480, \u03c6, \u03c0) via the data processing inequality (Cover & Thomas, 2006), the contribution that the I(Z; Zn|80, \u03a6\u03c6, \u03c0) term makes to measuring skillset diversity is that it measures how many dif-ferent actions the (\u03c6, \u03c0) skillset executes in state 80. The more unique open loop action sequences executed by the (\u03c6, \u03c0), regardless of the states they target, the higher the I(Z; Zn|so, \u03c6, \u03c0) can be.\nNote that when trained to maximize \u00ce(Z; Zn|so, \u03a6, \u03c0), the latent-predictive model p\u025b (zn|80, \u03a6, \u03c0, \u03b1) is encouraged to output unique latent vectors zn for each open loop action sequence a.\nThe second term in the skillset diversity objective is an average KL divergence between the latent-predictive model p\u03b5 (zn|50, \u03c6, \u03c0, \u03b1) and the state encoding distribution p\u03b7(zn|50, \u03a6, \u03c0, sn), which en-codes skill-terminating states sn to latent states zn for a given skill start state so and (\u03c6, \u03c0) skillset. The KL divergence is averaged over the different (open loop action sequence a, skill-terminating state sn) generated by the (\u03c6, \u03c0) skillset under consideration. The contribution of this term to mea-suring skillset diversity is to penalize the skillset for any skills that the \u00ce(Z; Zn|80, \u03a6, \u03c0) term had counted as unique because they output different actions but actually target the same terminating state. For instance, if there are two distant skills z that execute different actions a that target the same state sn, and the latent-predictive model p\u025b assigns two distant latent states zn for the different actions, then the KL divergence will lower the diversity score because the state encoding distribu-tion pn will need to take on a more entropic distribution to cover the different latent states output by the latent-predictive model p\u025b in order to minimize the KL divergence. Note that when the latent-predicted model p\u025b and the state encoding distribution p\u03b7 are jointly trained to minimize this KL divergence, they are encouraged to output similar distributions.\nThe two terms together provide an intuitive way to measure skillset diversity, in which skillsets that execute more distinct actions that target distinct groupings of states are assigned higher diversity scores. Regarding the forms the latent-predictive p\u0119, state encoding p\u03b7, and variational posterior q distributions take when they are are jointly trained to maximize the diversity score for a particular (\u03a6, \u03c0) skillset, the latent-predictive model is encouraged to output latent states that both (a) match the output of the state encoding distribution (decreasing the KL divergence) and (b) are unique so that they can be decoded back to the original skill via the variational posterior q\u00a2(z|80, \u03a6, \u03c0, \u0396\u03b7) (increasing I(Z; Zn)). For diverse skillsets in which different skills target different states, the dis-tributions can take this form, as illustrated in Figure 5 of the Appendix."}, {"title": "3.1.2 TRACTABLE DATA REQUIREMENTS", "content": "Next we discuss the data required to maximize LPE skillset diversity objective with respect to the skill-conditioned policy \u03c0. Because we will use the same actor-critic optimization architecture as Skillset Empowerment in which a critic is trained for each parameter of the skill-conditioned policy \u03c0, we will need to measure the diversity of a large number of skillsets that contain some changes to each of the \u3160 parameters of the skill-conditioned policy. To measure the diversity of a single (\u03c6, \u03c0) skillset (i.e., optimize the J(80, \u03a6, \u03c0) objective with respect to the latent-predictive model, state encoding distribution, and variational posterior), (i) tuples of (skills z, open loop action sequences a, and skill-terminating latent states zn) are needed to optimize the variational mutual information \u00ce(\u0396; Zn|80, \u03c6, \u03c0) and (ii) transition tuples of (skill start state 80, action sequence a, skill-terminating state sn) generated by the (\u03c6, \u03c0) are needed to optimize the KL divergence between the latent-predictive model and the state encoding distribution.\nObtaining this data for a large number of skillsets is significantly more tractable then acquiring the data needed to maximize the variational lower bound on the mutual information between skills and states I(Z; Sn|80, \u03c6, \u03c0) as is done by Skillset Empowerment. Maximizing I(Z; Sn|80, \u03a6, \u03c0) requires a simulator of the environment to generate the needed (z, sn) tuples. On the other hand, the (z, a, zn) tuples needed to optimize the I(Z; Zn|80, \u03a6, \u03c0) term only requires a latent-predictive model, which is more feasible to train because it predicts lower dimensional latent states and the latent-predictive model can take the form of simple distribution like a diagonal gaussian. In addition, the needed transition data (so, a, sn) can be mostly sampled from a replay buffer of online transition data. In the LPE algorithm, we will assume the agent, in between updates to its skillset, interacts with the environment by sampling skills z ~ \u00a2(z|80) from its skillset, greedily executing its skill-conditioned policy \u03c0(\u03b1|so, z), and then storing the (so, a, sn) transitions that occur. In section B of the Appendix we discuss how the LPE objective responds to (\u03c6, \u03c0) skillset candidates that execute new actions that are not in the replay buffer and why this helps LPE explore new skillsets despite only using a nearly deterministic skill-conditioned policy in the environment."}, {"title": "3.1.3 PRINCIPLED REPLACEMENT FOR I(Z; Sn|50, \u03a6, \u03c0)", "content": "The skillset diversity objective used in equation 6 is a principled replacement for the mutual infor-mation between skills and states I(Z; Sn so, \u03a6, \u03c0) because under certain reasonable assumptions both objectives have the same maximum with respect to the skill-conditioned policy \u03c0 (see section D of the Appendix for a proof and additional commentary on the assumptions). The assumptions include (i) there exists some finite maximum posterior for the relevant true and variational posteri-ors and that (ii) there exists a (\u03c6, \u03c0) skillset such that produces maximum variational posteriors q\u03c8(z|so, \u03c6, pi, zn). In practice, the second assumption is more realistic for smaller skill distribu-tions & because for larger distributions there may not be enough states that can be targeted to pro-duce only tight posteriors q\u2084(z|so, d, pi, zn). The proof makes use of the following connection between the skillset diversity objective J(so, \u03c6, \u03c0) and the mutual information between skills and states, I(Z; Sn 50, \u03a6, \u03c0). In the first step of this connection, we note that the LPE skillset diversity objective J(so, \u03c6, \u03c0) is a lower bound of the following objective (see Appendix section C for proof)\n$I.J(s_0, \\phi,\\pi) = H(Z|s_0, \\phi) + log(E_{z\\sim \\phi(z|s_0),z_n\\sim P_{\\eta}(z_n|s_0,\\phi,\\pi,z)} [P(z|s_0, \\phi, \\pi, z_n)])$,\nin which the distribution pn (zn|50, \u03c6, \u03c0, z) is the marginal of the joint distribution \u03a1\u03b7(\u03b1, Sn, zn|50, \u03a6, \u03c0,z) = \u03c0(\u03b1|8o, z)p(sn|50, a)Pn(zn|50,\u03a6, \u03c0, Sn) where p(sn|so, a) is the open loop transition dynamics and pn(zn|80, \u03a6, \u03c0, sn) is the state-encoding distribution. Thus, by max-imizing the skillset diversity objective J(80, \u03c6, \u03c0) with respect to \u03c0 (and p\u03b5, \u03a1\u03b7, and, q$), LPE is learning (\u03c6, \u03c0) skillsets with larger true posterior distributions p(z|so, \u03c6, \u03c0, \u0396\u03b7), meaning that agents are learning skillsets with more distinct skills \u201cpacked\u201d inside them. Next, we note that the I.J(50, \u03c6, \u03c0) objective is an upper bound of the the mutual information between skills and latent rep-resentations I(Z; Zn 80, \u03a6, \u03c0), in which the latent representation zn is the sampled from the same distribution pn (zn|50, \u03a6, \u03c0, z) used in IJ. The inequality is due to Jensen's Inequality as I has an log of an expectation over posteriors term while I(Z; Zn|80, \u03c6, \u03c0) has an expectation of the log of the posteriors. We complete the connection by noting that I(Z; Zn|50, \u03c6, \u03c0) is a lower bound to I(Z; Sn 50, \u03c6, \u03c0) using the data processing inequality. In the proof, we show that for certain (\u03c6, \u03c0) skillsets, these inequalities become equalities and the same can maximize both J(50, \u03c6.\u03c0) and I(Z; Sn|80, \u03a6, \u03c0)."}, {"title": "3.1.4 PRACTICAL IMPLEMENTATION OF SKILL-CONDITIONED POLICY ACTOR-CRITIC", "content": "LPE learns diverse skill-conditioned policies \u03c0 for a variety of skill start state so and skill distribution combinations using a similar actor-critic architecture to the one used by Skillset Empowerment, which we review in section A of the Appendix. The actor fx will take as input a (so, $) tuple and output a skill-conditioned policy parameter vector \u03c0. The parameter-specific critic Qw; for i = 0, ..., |\u03c0| \u2013 1 will measure the J(50, \u03a6, \u03c0) diversity of skillsets defined by (80, \u03a6, \u03c0\u2081) tuples, in which \u03c0\u2081 = fx (80, 4) except for the i-th parameter which can take on noisy values. We detail the objectives using for training the actor and critics in section E of the Appendix."}, {"title": "3.2 TRAINING OBJECTIVE FOR SKILL DISTRIBUTION ACTOR", "content": "We train the skill distribution actor fu actor, which outputs a distribution over skills $ for a given 80, to maximize the variational mutual information objective \u00ce(Z; Zn|50,\u03c6,\u03c0 = fx(50,4)). In this mutual information term, the skill-conditioned policy used is the greedy output of the policy fx(50, $) and zn ~ \u03c1\u03b5(zn|So, \u03c6,\u03c0 = fx(so, \u03c6), a) is sampled from a latent-predictive model that has been trained to match the state encoding distribution p\u03b7(zn|50,\u03c6,\u03c0 = fx(so, $), sn), which is trained during the update to the skill-conditioned policy actor-critics. As discussed previously, \u00ce(\u0396; Zn|50, \u03c6, \u03c0 = fx(so, $)) when p\u03b5 \u2248 p\u03b7 offers a principled substitute for the mutual informa-tion between skills and states I(Z; Sn|50, \u03c6, \u03c0), particularly for relatively small 4. Note that for the update to the & actor, we do not use the same latent-predictive model that was trained during the skill-conditioned policy actor-critic update, but instead train a new latent-predictive model to minimize the KL divergence between the state-encoding distribution and the new latent-predictive model. We train a new model because for relatively larger values of $, there could be a scenario in which a \u03c0 is learned during the skill-conditioned policy update step that trades off artificially high \u00ce(Z; Zn|80, \u03c6, \u03c0) (i.e., redundant skills are treated as unique skills) for lower DKL(PE||P\u03b7), which would mean the learned latent-predictive model is not accurate. Although the latent-predictive mod-els learned in the \u03c0 actor-critic were diagonal gaussian, we implement the latent-predictive model in the & update using the more expressive Variational Autoencoder (VAE) (Kingma & Welling, 2014). The objective for training the VAE is provided in section F of the Appendix.\nThe objective functions used to train the & actor and critic are provided in section G of the Appendix. The full LPE procedure is provided in Algorithm 1."}, {"title": "3.3 LIMITATIONS", "content": "The main limitation of Latent-Predictive Empowerment is that it can be limited to measuring only short term empowerment because of the use of open loop skills. The inability to adjust a policy makes it difficult to target specific states over longer time horizons, particularly in domains with high levels of randomness. As a result, LPE can be a poor way to measure longer term empowerment.\nFuture work can investigate how a longer term empowerment can be computed from the short term empowerment measured by LPE."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 ENVIRONMENTS", "content": "We test LPE and a group of baselines on the same five domains that were used in Skillset Empower-ment. In terms of stochasticity and the observation dimensionality, these environments are complex"}, {"title": "4.2 BASELINES", "content": "We compare LPE to three versions of Skillset Empowerment. The first version is regular Skillset Empowerment, in which the agent is given access to the model of transition dynamics. Levy et al. (2024a) showed that Skillset Empowerment is able to learn large skillsets in all domains while both Variational Intrinsic Control (Gregor et al., 2016), an empowerment-based skill learning algorithm similar to Diversity Is All You Need (Eysenbach et al., 2019), and Goal-Conditioned RL were unable to learn meaningful skillsets. In the second version, the Skillset Empowerment agent learns a model of the transition dynamics p(st+1|St, at) using a VAE (Kingma & Welling, 2014) generative model. We expect this agent to struggle in the stochastic settings because it is challenging to learn simulators in these domains, which in turn means the agent may struggle to accurately measure the diversity of a skillset. Learning a simulator in stochastic four rooms is difficult because the agent's next location occurs in the same offset location in any of the four rooms and it is difficult for VAE's to learn disjoint distributions. Further, learning a perfect simulator in the RGB QR code domains in which the agent needs to predict the next QR code is not feasible due to the number of RGB-colored QR code combinations.\nIn the third version of Skillset Empowerment, the agent learns a latent-predictive model using a BYOL-Explore objective (Guo et al., 2022), which is a leading method for learning latent-predictive models. Similar to other latent-predictive model learning methods (Grill et al., 2020; As-sran et al., 2023; Bardes et al., 2024), BYOL-Explore trains a latent-predictive model p\u025b(zn|50, a) to match a state encoding distribution p\u03b7(zn|Sn), in which the parameters of the state encoding distribution are updated as an exponential moving average of the latent-predictive model param-eters: \u03b7 \u2190 \u03b1\u03b7 + (1 \u2212 \u03b1)\u03be. We also expect this approach to struggle because it is susceptible to only maximizing a loose lower bound on the mutual information between skills and states. By training the latent-predictive model to match the state encoding distribution (i.e., minimize DKL(P\u03b7(zn|50, \u03b1)||pg(zn|50, a))), this approach will be measuring the diversity of skillsets using the mutual information I(Z; Zn|80, \u03c6, \u03c0), in which zn is generated by the state encoding distribution P\u03b7(2|80,5n). This mutual information is a lower bound on the mutual information between skills and states I(Z; Sn|80, \u03c6, \u03c0) due to the data processing inequality, and the tightness of this bound depends on the state encoding distribution pn(zn|50, Sn). If pn maps states different sn to different latent states zn, then this bound can be tight, but otherwise this bound can be loose. The problem with BYOL is that it does not directly train the state-encoding distribution p\u03b7 to output unique zn for different sn. Instead, as a result of the exponential moving average update strategy, the out-put of the state-encoding distribution depends significantly on the initial parameter settings of n. If the initial setting of \u03b7 does not map different sn to different zn, then I(Z; Zn|80, \u03a6, \u03c0) may be a loose bound on I(Z; Sn|50, \u03a6, \u03c0), meaning the agent is not able to accurately measure the diversity of a skillset. In contrast, LPE does not have this issue because the state-encoding distribution is trained to match the latent-predictive model, which is also trained to maximize the mutual informa-tion I(Z; Zn 50, \u03a6, \u03c0), encouraging the latent-predictive model and the state encoding distribution to output unique zn for different inputs."}, {"title": "4.3 RESULTS", "content": "Table 1 shows the size of the skillsets learned by all algorithms in all domains except for the 8-dim underlying state domain where the agents learned an average skillset size of 15.9\u00b10.6 nats. Skillset size is measured with the variational mutual information I(Z; Sn). Note that mutual information in measured on a logarithmic scale (in this case, nats) so the 8.6 nats of skills learned by LPE in the pick-and-place version of the Stochastic Four Rooms domain means that LPE learned e8.6 \u2248 5,400"}, {"title": "5 RELATED WORK", "content": "There have been many prior works that have used empowerment to try to learn large skillsets. Early empowerment methods showed how mutual information between actions and states could be opti-mized in small settings with discrete state and/or action spaces (Klyubin et al., 2008; Salge et al., 2013a; Jung et al., 2012). Several later works integrated variational inference techniques that en-abled empowerment-based skill learning to be applied to larger continuous domains (Mohamed & Jimenez Rezende, 2015; Karl et al., 2017; Gregor et al., 2016; Eysenbach et al., 2019; Sharma et al., 2020; Li et al., 2019; Hansen et al., 2020; Zhang et al., 2021). However, these methods were limited in the size of skillsets they were able to learn as they only maximize a loose lower bound on mutual information, making it difficult to accurately measure the diversity of a skillset (Levy et al., 2024a).\nRelated to empowerment-based skill learning is unsupervised goal-conditioned reinforcement learn-ing (GCRL) that learn goal-conditioned skills using some automated curriculum that expands the distribution over goal states over time (Ecoffet et al., 2019; Mendonca et al., 2021; Nair et al., 2018; Pong et al., 2020; Campos et al., 2020; Pitis et al., 2020; Florensa et al., 2018; McClinton et al.,"}, {"title": "6 CONCLUSION", "content": "Empowerment has the potential to help agents become general purpose agents with large skillsets, but this potential may never be realized as long as measuring the empowerment of a state requires a simulator of the environment. In this work, we takes a step toward a more scalable way to compute empowerment by presenting a method that can measure empowerment using only latent-predictive models. We show empirically in a variety of settings that our approach can learn equally-sized skillsets as the leading empowerment algorithm that requires access to a simulator of the environ-ment. Future work should investigate how longer term empowerment can be computed using LPE's short term empowerment measurements."}, {"title": "A SKILLSET EMPOWERMENT ACTOR-CRITIC ARCHITECTURE", "content": "This section reviews how Skillset Empowerment maximizes the variational mutual information ob-jective with respect to the skillset distributions & and \u03c0 as we will use a similar optimization architec-ture in our approach. In order to optimize the variational mutual information \u00ce(Z; Sn|so, \u03c6, \u03c0) with respect to & and using deep learning, Skillset Empowerment first vectorizes these distributions. Skillset Empowerment represents the distribution over skills & with a scalar representing the side length of a uniform distribution in the shape of a d-dimensional cube. For instance, if the skill space"}, {"title": "B How LPE EXPLORES THE SPACE OF SKILL-CONDITIONED POLICIES", "content": "Even though LPE agents only interact with the environment by greedily following its nearly deter-ministic skill-conditioned policy, equation 6 has built-in mechanisms that encourage agents to try different skillsets that execute new actions"}]}