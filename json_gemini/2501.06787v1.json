{"title": "Improving Pain Classification using Spatio-Temporal Deep Learning Approaches with Facial Expressions", "authors": ["Aafaf Ridouan", "Amine Bohi", "Youssef Mourchid"], "abstract": "Pain management and severity detection are crucial for effective treatment, yet traditional self-reporting methods are\nsubjective and may be unsuitable for non-verbal individuals (people with limited speaking skills). To address this\nlimitation, we explore automated pain detection using facial expressions. Our study leverages deep learning techniques\nto improve pain assessment by analyzing facial images from the Pain Emotion Faces Database (PEMF). We propose two\nnovel approaches\u00b9: (1) a hybrid ConvNeXt model combined with Long Short-Term Memory (LSTM) blocks to analyze\nvideo frames and predict pain presence, and (2) a Spatio-Temporal Graph Convolution Network (STGCN) integrated\nwith LSTM to process landmarks from facial images for pain detection. Our work represents the first use of the PEMF\ndataset for binary pain classification and demonstrates the effectiveness of these models through extensive\nexperimentation. The results highlight the potential of combining spatial and temporal features for enhanced pain\ndetection, offering a promising advancement in objective pain assessment methodologies.", "sections": [{"title": "1. INTRODUCTION", "content": "According to the International Association for the Study of Pain (IASP), pain is defined as an unpleasant sensory and\nemotional experience associated with, or resembling that associated with, actual or potential tissue damage. Managing\npain and detecting its severity play essential roles in treating diseases. Traditionally, this is done through self-reporting,\nwhere patients rate their pain level on a scale. Various pain measurement scales have been designed to describe a\npatient's intensity of pain, including the Visual Analogue Scale (VAS), Verbal Rating Scale (VRS), Faces Pain Scale-\nRevised (FPSR), and the Numerical Rating Scale (NRS) [1] [2]. However, these approaches depend on the patient's self-\nrating of pain, making them less objective, as the conceptualization of pain varies from patient to patient and between\nclinicians and patients [3]. Thus, relying solely on self-reporting can lead to unsuitable and inadequate pain management.\nIn the literature, wide ranges of approaches based on facial expressions have been used for pain detection. It has been\nfound that pain generates spontaneous facial expressions, which can be used to automate pain assessment by detecting it\nor estimating its level. Moreover, most publicly available pain databases include facial video frames of patients,\nproviding valuable data for analysis. As a result, some approaches have been employed in this context, ranging from\ntraditional methods to machine learning and deep learning approaches. Deep neural networks have shown remarkable\neffectiveness in automatic recognition tasks [31], including pain assessment. With the use of multiple hidden layers,\nthese models' capacity to perform complex and highly nonlinear predictive modeling tasks has increased. Convolutional\nNeural Network (CNN) models achieve higher performance in image classification and feature extraction [4]. Adapting\nthem to facial feature extraction has improved the task of pain detection based on facial image datasets. On the other\nhand, Recurrent Neural Networks (RNNs) are known for their ability to manage sequential data by capturing its temporal\naspects [32]. By combining both CNN and RNN, we enhance the extraction of spatio-temporal features, leading to\ngreater efficiency in classification tasks.\nIn this work, we propose two networks for processing two different types of data. The first is a hybrid ConvNeXt\nmodel combined with Long Short Term Memory (LSTM) [5] blocks, designed to process video frames from the Pain\nEmotion Faces Database (PEMF) [6] to predict the presence of pain in these videos. The second is a Spatio-Temporal\nGraph Convolution Network STGCN-LSTM network that uses landmarks extracted from the same dataset to predict\npain. The contributions of this work are as follows:"}, {"title": "2. RELATED WORK", "content": "Pain assessment is a critical component in medical diagnosis and treatment, particularly for conditions where pain is\na primary symptom. Over the years, various methods have been developed to assess pain, ranging from handcrafted\nmethods to advanced machine learning (ML) and deep learning (DL) techniques. Pikulkaew et al. [7] propose deep\nlearning system using CNN ResNet-34 developed on the UNBC-McMaster Shoulder Pain Archive database [8]. This\ndatabase consists of facial images of patients with shoulder pain to evaluate pain severity, classifying it into three levels:\nnot painful, becoming painful, and painful. Similarly, Gholami et al. [9] performed binary pain classification on infants\nusing a relevant vector machine (RVM) trained on the COPE dataset [10]. In the context of multimodal-based\napproaches, Semwal et al. [11] proposed a spatio-temporal behavioral multiparametric pain assessment system that\ncombines three different behavioral pain parameters: facial expression, body movement, and pain sound. Three\nclassifiers were introduced for each modality: CNNTL-BiLSTM for facial expressions, VGGish-SVM for audio, and\nSFCN-BiLSTM for body movements. A decision-level fusion approach was used to combine the outputs of these\nclassifiers to estimate pain intensity as No pain, Mild pain, and Severe pain. They also introduced the Behavioral\nMultiparametric Pain dataset (BMP dataset) to support this approach. Bargshady et al. [12] proposed a hybrid learning-\nbased algorithm using CNN and BiLSTM networks trained on facial images to estimate pain intensities. 3D\nconvolutional networks have also shown promising results; in [13], a deep 3D convolutional network was used for pain\ndetection from facial images, although these models remain computationally expensive. Y. Li et al. [14] used to the\nEmoPain body movement dataset to predict pain recognition and protective behavior. Their lightweight LSTM-DNN\nmodel predicts pain intensity and protective behavior based on angle, angle energy, and sEMG features. Zhou et al. [15]\nintroduced a real-time regression framework based on recurrent convolutional neural networks for automatic pain\nintensity estimation on the UNBC-McMaster Shoulder Pain Archive dataset. G. Bargshady et al. [16] developed an\nensemble deep learning framework (EDLM) with two stages. The early fusion stage combined a fine-tuned VGG-Face\nwith linear PCA to extract and select features from facial images. These features were then transferred to a hybrid deep\nlearning network for classification. A three-stream ensemble CNN-RNN classifier was used in the late fusion stage to\nclassify pain levels into five categories. Both the MIntPAIN and UNBC-McMaster Shoulder Pain datasets were used to\ntrain and evaluate the model. Bargshady et al. [17] also proposed a predictive modeling framework that employs VGG-\nFace with PCA using Hue, Saturation, Value (HSV) color space video frames for pain intensity estimation, followed by a\nmmodified Temporal Convolutional Network (TCN) for prediction using the UNBC-McMaster Shoulder Pain Archive and\nMIntPAIN databases. F. Paol et al. [18] used facial expressions to evaluate pain levels based on Action Units (AUs) and\nhead pose components. They trained and compared multiple models using both the BioVid and the PEMF datasets. This\nprogression highlights the advancements in pain assessment methods, emphasizing the need for future research to focus\non optimizing these models for real-time applications and exploring more comprehensive multimodal datasets to\nimprove generalizability and robustness."}, {"title": "3. METHODOLOGY", "content": "In this work, we propose two approaches to detect pain. The first approach involves a hybrid CNN-LSTM model that\nuses facial image data to extract both spatial and temporal features for detecting pain in image clips. The second\napproach implements an STGCN model to process landmarks extracted from the PEMF facial images to detect pain. The\nfollowing paragraphs will detail each of these approaches."}, {"title": "3.1 Visual spatio-temporal network", "content": "In the hybrid CNN-LSTM spatio-temporal network illustrated in block a of Figure 1, twenty frames of each clip in\nthe PEMF dataset are fed into a CNN model precisely a ConvNeXt model to handle the spatial aspect of the data.\nThe ConvNeXt model [19], introduced in 2022 by Facebook researchers, is a convolutional network with design\nelements inspired by vision transformers [20]. It was developed by iteratively modifying ResNet [21] architectures to\nintegrate features proven effective in vision transformers. The architecture employs multiple stages with varying feature\nmap resolutions, focusing on stage-compute-ratio (SCR) and stem cell structure. A key feature of ConvNeXt is its\npatchify layer, which uses a 4\u00d74 convolutional layer with a stride of 4, in contrast to ResNet's 7\u00d77 layer with a stride of\n2.\nFigure 1. (b) illustrates the general architecture of ConvNeXt series networks. The ConvNeXt block, crucial for\nenhancing performance and reducing floating-point operations (FLOPs), achieves this through larger kernel-sized\ndepthwise convolutions and a wider channel range, expanding to 96 channels. It adopts an inverted bottleneck design,\nuses GELU [22] activation instead of ReLU [23], and employs Layer Normalization [24] rather than BatchNormalization\n[25] techniques commonly found in advanced Transformers. The ConvNeXt block includes a 7\u00d77 depthwise convolution,\ntwo 1\u00d71 layers, and GELU activation, with Layer Normalization applied before the 1\u00d71 convolution layer. It also\nincorporates a downsampling layer between stages using 2\u00d72 convolutional layers with a stride of 2.\nMultiple versions of ConvNeXt have been developed, ranging from Tiny to XLarge, with channel numbers ranging\nfrom (96, 192, 384, 768) in the Tiny version (illustrated in Figure 1. (b)) to (256, 512, 1024, 2048) in the XLarge version,\nand the number of blocks being (3, 3, 9, 3) in Tiny and (3, 3, 27, 3) in the larger ConvNeXt models. The extracted spatial\nfeature vectors are then processed in a middle fusion stage. To construct the clip feature vector, the feature vectors from\nthe twenty video frames are concatenated. These concatenated vectors are then fed into an LSTM model, which handles\nthe temporal sequential aspects of the facial clips.\nLong Short-Term Memory (LSTM) networks are an advanced type of Recurrent Neural Networks (RNNs) designed\nto capture temporal dependencies across sequences of data. Traditional RNNs struggle with retaining information over\nlong sequences, as they tend to give more weight to recent information, which limits their ability to capture dependencies\nover a large number of timesteps. LSTM networks address this issue by introducing memory cells that can retain\ninformation over long sequences. Each memory cell contains three main components: an input gate, a forget gate, and an\noutput gate. The input gate determines which inputs to store in the memory cell based on the current input and the\nprevious hidden state, returning values between 0 and 1. The forget gate specifies which information to discard, with 0\nmeaning ignored and 1 meaning retained, based on the current input and the previous hidden state. The output gate\ncontrols how much of the memory cell's content contributes to computing the hidden state, also producing values\nbetween 0 and 1. These gating mechanisms enhance the performance of LSTM networks compared to traditional RNNs\nby effectively controlling the flow of information in and out of the memory cells, thus mitigating the vanishing gradient\nproblem."}, {"title": "3.2 STGCN for landmarks data", "content": "Face landmarks represent specific key points on the face used to identify and track facial features in computer vision\ntasks. These key points are often connected to form a mesh representing the structure of the face. Therefore, the\nlandmarks with their connections construct a graph, where each landmark represents a node in the graph and is"}, {"title": "4. EXPERIMENT AND RESULTS", "content": "In this work, we propose two approaches to detect pain. The first approach involves a hybrid CNN-LSTM model that\nuses facial image data to extract both spatial and temporal features for detecting pain in image clips. The second\napproach implements an STGCN model to process landmarks extracted from the PEMF facial images to detect pain. The\nfollowing paragraphs will detail each of these approaches."}, {"title": "4.1 Dataset", "content": "The experiments were conducted using a recent dataset called the Pain Emotion Faces (PEMF) database [6]. The\nPEMF dataset comprises 272 micro-clips collected from 68 healthy participants with an average age of 30.34 years. It\nincludes two main classes: \"No Pain\" (neutral) and three types of pain-related facial expressions: posed pain,\nspontaneous pain (measured with an algometer), and spontaneous pain (induced by CO2 laser). The dataset was\nevaluated by 510 undergraduate students who rated pain intensity, valence, arousal, and the presence of additional\nemotions (happiness, sadness, anger, surprise, fear, and disgust). Each clip consists of 20 frames, available in both color\nand black-and-white versions. Additional metrics include pain intensity, the percentage of six emotions, and facial action\nunits (AUs). In this work, we use the frames as input to the hybrid ConvNeXt model and generate new landmark data\nfrom them for use with the STGCN-LSTM network."}, {"title": "4.2 Experiment 1: Visual spatio-temporal network", "content": ""}, {"title": "4.2.1 Implementation details", "content": "In our training process, we used the Adam optimizer with a learning rate of le-4, combined with exponential decay\nto ensure the convergence of the hybrid ConvNeXt model. Data augmentation techniques, such as Random Cropping and\nRandom Rotation, were applied to improve the model's performance and generalization capabilities. Additionally, to\nfurther enhance our model's performance, we included weights from a ConvNeXt model pre-trained on the ImageNet-\n22k dataset [28]. This dataset, which includes a diverse range of images, helps our model learn more effectively. We\nresized our training images to 224\u00d7224 pixels to match the input size used for the pre-trained weights, facilitating their\neffective utilization and contributing to improved performance and accuracy.\nIn our study, we chose the XLarge version of the ConvNeXt model, based on the scores obtained for image\nclassification on the ImageNet dataset [21] and the study [29] where the authors conducted extensive experiments on the\nfive versions of the ConvNeXt model for facial emotion classification, namely: Tiny, Small, Base, Large, and XLarge.\nThe ConvNeXt XLarge demonstrated superior performance. For sequence learning, we implemented four types of\nLSTM: the original LSTM, Bi-LSTM, LSTM with an integrated attention mechanism, and stacked LSTM. The objective\nis to test all these LSTM variants and select the one that provides the best performance in combination with the\nConvNeXt XLarge. For more information on the different LSTM versions, interested readers can refer to this study [30].\nFinally, the model was trained for 150 epochs with a batch size of 8. We used 80% of the data for training, while the\nremaining 20% was split between the test and validation sets. The results are represented in the following section."}, {"title": "4.2.2 Results", "content": "To identify the most effective CNN model for our pain classification task, we evaluated several pre-trained CNN\nmodels on the PEMF dataset, selecting the best-performing ones for detailed comparison. As presented in Table 1, the\nConvNeXt XLarge model achieved the highest accuracy of 89.01%, significantly outperforming the other models.\nVGG16 and Xception followed with accuracies of 87.39% and 85.59% respectively. In contrast, models such as\nResNet101, InceptionV3, and MobileNetV1 demonstrated lower performance, with accuracies of 78.92%, 68.9%, and\n66.11% respectively. These results highlight the superior ability of ConvNeXt XLarge to capture spatial features crucial\nfor pain classification based on facial images. Therefore, we have chosen the ConvNeXt XLarge model as the spatial\nfeature extractor for our network.\nTo optimize the sequence learning component of our network, we implemented and evaluated different variants of\nthe LSTM model. The results, presented in Table 2, reveal the average performance of each variant across five\nexperiments (5-folds) in terms of accuracy, precision, recall, and F1-score. The LSTM with an attention mechanism\nachieved a moderate accuracy of 86.67%, indicating that while the attention mechanism adds value, it did not outperform"}, {"title": "4.3 Experiment 2: STGCN for landmarks data", "content": ""}, {"title": "4.3.1 Training details", "content": "The STGCN-LSTM is trained on the landmarks data using the Adam optimizer implemented with Exponential decay\nand a learning rate of the value le-4. To prevent overfitting, the SMOTE method (Synthetic Minority Over-sampling\nTechnique) was implemented to address the class imbalance in the dataset. It works by creating synthetic examples of\nthe minority class to balance the number of instances between the majority and minority classes. The model converges at\n150 epochs with a batch size of 10. In the next section, we present the results of the training."}, {"title": "4.3.2 Results", "content": "To ensure the effectiveness of the STGCN model on the pain classification task using landmarks data, we ran the\nmodel five times on the training and testing datasets. We started by implementing an STGCN model with three blocks\nand then combined it with an LSTM with two layers. The results in Table 3 show a significant difference between the\nSTGCN with and without the LSTM model. With an accuracy reaching 82.14%, the combination of the STGCN with the\nLSTM improved the results, emphasizing the temporal aspect of the landmarks data. Thus, relying solely on the STGCN\nis not efficient for the pain classification task based on clips of landmarks."}, {"title": "5. CONCLUSION", "content": "This paper proposes two different approaches built on the same dataset PEMF which is considered new with very few\napplications on the pain assessment task. Using the micro-clips (videos) of the dataset, we implemented a hybrid\nConvNeXt model that handles both the spatial and temporal aspects of the facial clip frames. The pre-trained ConvNeXt\nXLarge model extracted the spatial features of each clip frame passing them to an LSTM model that studied the\nsequential dependencies between the video frames. The hybrid network shows significant performance on pain\nclassification task.\nThe second approach is based on the extracted landmarks from the PEMF clip frames. These landmarks are key\npoints connected to form a graph with nodes (landmark coordinates) and edges (connections). To evaluate pain using the\nlandmarks graph data, we implemented an STGCN-LSTM model that studies the connections between the landmarks\nthrough the video frames. Experimental results of the STGCN-LSTM model on the constructed PEMF landmark dataset\ndemonstrate good performance for the pain classification task. The integration of the LSTM improved the results,\nproviding more accurate facial pain detection."}]}