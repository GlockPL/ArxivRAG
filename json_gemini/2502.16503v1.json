{"title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis", "authors": ["Yilun Zheng", "Sha Li", "Fangkun Wu", "Yang Ziyi", "Lin Hongchao", "Zhichao Hu", "Cai Xinjun", "Ziming Wang", "Jinxuan Chen", "Sitao Luan", "Jiahao Xu", "Lihui Chen"], "abstract": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, e.g., DeepSeek-R1 and GPT-03, highlighting parody as a significant challenge for LLMs. Our code and data is available at https://github.com/Lisaaa1017/Fanchuan.", "sections": [{"title": "1 Introduction", "content": "Parody in social media is a form of humor or satire, which uses exaggerated or absurd imitations for critique or entertainment (Dentith, 2002). It has become popular around some controversial topics in recent years, especially among the young generation (Mulholland, 2013; McClennen and Maisel, 2014). For example, as shown in Figure 1, the question \"Should my boyfriend hand over his salary to me?\" has sparked intense debate (Q&A). While some users clearly express their views as neutral, supportive, or opposed, others adopt a parody tone, mockingly pretending to support the stance with exaggerated statements like, \u201cGuys who don't hand over their salary are a HUGE red flag...\u201d, which subtly opposes it. This tactic can attract attention and provoke reactions through humor, making people reflect their opinions. Similar to irony or sarcasm (Frenda et al., 2023), parody also expresses the opinion opposite to its appearance. However, it emphasizes playful, entertaining, and exaggerated mimicry of a character, making the underlying critique more accessible and engaging to the audience.\nThe real meaning behind parody is highly culture-dependent. Therefore, the analysis of parody can offer unique insights in understanding the corresponding cultural values. The spread of parody on internet also fosters a diverse linguistic culture (Menghini, 2024). People can share their distinct views on society, political, or cultural topics in a humorous and engaging manner, encouraging global and cross-cultural dialogue. In addition, parody plays a crucial role in the formation of subcultures (Willett, 2009; Booth, 2014). Parody comments not only create distinct communities, but also mirror the values and identities of online users. For younger generations, parody comments have become a way of self-expression, which help to define their uniqueness, build connection with others, and form social circles. Gradually, it has become a shared language and a set of symbols for the growth of internet subcultures.\nDespite the widespread popularity of parody, there is a lack of high-quality datasets that capture parody comments with different topics and languages (Maronikolakis et al., 2020), restricting the more general and inclusive analysis in various contexts. To fill this gap, we propose FanChuan, a parody benchmark with high quality in three key aspects: high diversity, rich contexts, and precise annotations. First, we enhance diversity by collecting data from multiple sources (both Chinese and English corpora), a wide range of topics, and various social media platforms. Such broad coverage allows us to conduct more sufficient, balanced and fair evaluations of models. Second, we construct richer context information by building the relationship between comments and their replies as heterogeneous graphs. Unlike previous studies that only focus on textual (Zhang et al., 2022) or dialogue (Bamman and Smith, 2015; Wang et al., 2015) content, the graph-structured context enables the exploitation of relational information, which is found to be fairly valuable later. Third, since parody labeling is quite challenging and disagreements among annotators can easily arise, we ensure the quality of annotation by employing native speakers to label the parody and sentiment of each comment. Additionally, we have expert judges to resolve any disagreement and Large Language Models (LLMs) to refine the annotation results, ensuring consistency and reliability. As a result, we have created seven datasets, with 14,755 annotated users and 21,210 annotated comments in total, enabling comprehensive experiments and analyses.\nWith the new datasets, we evaluate embedding-based methods (Liu, 2019), incongruity-based methods (Liu et al., 2023b), outlier detection methods (Liu et al., 2008), graph-based methods (Kipf and Welling, 2016), and Large Language Models (LLMs) (OpenAI, 2023) on FanChuan with three parody related tasks: parody detection, comment sentiment classification with parody, and user sentiment classification with parody. Our results indicate that (1) parody-related tasks are challenging for all models, and even LLMs fail to consistently outperform traditional embedding-based approaches; (2) model performance of sentiment classification drops significantly on comments exhibiting parody behavior compared to those without parody; (3) incorporating commented objects as contextual information greatly enhances parody detection performance; (4) reasoning LLMs fail to outperform non-reasoning LLMs on parody detection. To our best knowledge, the existing studies on parody(Maronikolakis et al., 2020; Willett, 2009) are all from pre-LLMs era, and we are the first to evaluate the performance of LLMs on parody detection. In summary, our contributions are summarized as follows:\n\u2022 We introduce FanChuan, a parody benchmark that includes seven datasets from both Chinese and English corpora, containing 21,210 annotated comments and 14,755 annotated users.\n\u2022 We leverage heterogeneous graphs to model user interaction relationships, providing richer contextual information compared to previous datasets.\n\u2022 We comprehensively evaluate five types of methods, including embedding-based methods, inconsistency-based methods, outlier detection methods, graph-based methods, and LLMs, on three parody-related tasks.\n\u2022 Our findings reveal that parody-related tasks are challenging and LLMs cannot always outperform traditional embedding-based methods. Additionally, we show that reasoning LLMs generally underperform non-reasoning LLMs in parody detection."}, {"title": "2 FanChuan", "content": "In this section, we will introduce the details about FanChuan. Specifically, in Section 2.1, we introduce the dataset construction process, including data collection, annotation and preprocessing."}, {"title": "2.1 Dataset Construction", "content": "As illustrated in Figure 2, the data construction process for FanChuan involves three steps: data collection, annotation, and preprocessing. Then we introduce the details of each step as follows.\nData collection To ensure a comprehensive evaluation, we ensure high diversity in our benchmark by selecting a wide range of topics from both Chinese and English corpora. Given that parody often emerges around controversial issues, we begin by focusing on topics or recent events that have sparked intense debates on social media. To select the post that includes adequate parody comments, we randomly sample a subset of its comments to determine the proportion of parody content. If more than 3% of the comments are identified as parody, we classify it as suitable for further collection. To capture the most relevant content, we use keyword search to identify prominent posts, then collect their comments, replies, and associated content.\nData Annotation Labeling parody presents a significant challenge, not only because it requires a high familiarity with the content and culture (B\u00e4nziger and Scherer, 2005), but also due to potential disagreements of understanding among annotators from diverse backgrounds (Dress et al., 2008). To ensure precise annotations in FanChuan, the annotation process includes five steps: (1) To provide accurate and culturally relevant insights, we assign native speakers to annotate Chinese and English datasets, respectively. Annotators are then asked to review relevant materials to enhance their understanding before starting the annotation process. (2) Sentiment Annotation. Annotators classify the sentiment of a given comment or user by answering the question: \u201cDoes this comment or user support, oppose, or remain neutral regarding to this statement?\u201d (3) Parody Annotation. After sentiment classification, annotators are asked to determine whether a comment is a parody by answering the question: \u201cIs this comment a parody or not?\u201d During both sentiment and parody annotation stages, annotators are provided with relevant comments and context to ensure accurate labeling. (4) Resolving Discrepancies. Each comment receives a final label based on the majority vote of three annotators. If consensus is not reached, the most knowledgeable annotator on the relevant topic or event reassesses the labels. (5) Verification. To minimize errors in parody annotations, an experienced annotator reviews all comments labeled as parody. Note that this annotator will also double-check the comments that are labeled as parody by LLMs but not labeled by human annotators.\nData preprocessing To ensure data quality, we first delete any content or comments that contain irrelevant, sensitive, personal, or hazardous information. We provide three types of embeddings: Bag of Words (BoW) (Mikolov, 2013), Skip-gram (Mikolov et al., 2013), and RoBERTa (Liu, 2019). Given that the context of parody forms a network structure, we store the data as heterogeneous graphs as shown in Figure 3, where the nodes represent users and posts, and there are two types of edges to represent two types of relations: user-comments-post, and user-comments-user. Compared with existing datasets (Bamman and Smith, 2015; Pt\u00e1ek et al., 2014) that focus solely on content or dialogue, such graph-structured data enables deeper understanding of parody with richer contexts, including 2-hop neighbors and higher-order relationships.\nFinally, as shown in Table 1, we constructed seven datasets from both Chinese and English corpora, encompassing multiple topics, with a total of 14,755 annotated users and 21,210 annotated comments. Our analysis reveals that parody comments constitute only a small proportion of the total comments across all datasets. For detailed description and background information of each dataset, please refer to Appendix A."}, {"title": "2.2 Problem Definition", "content": "As shown in Figure 3, we utilize Heterogeneous Information Networks (HINs) to structure our datasets, representing the relational information in content and comments. Each HIN comprises two types of nodes: user nodes and post nodes, along with two types of edges: user comments to posts and user comments to users\u00b2. Each edge is directed, with the source being the user and the target either a post or another user. As shown by the orange edges on the right in Figure 3, multiple edges may exist between two nodes due to several rounds of replies among these users. This results in a directed multigraph (Gross and Yellen, 2003). Each edge or node is associated with text as features. We then introduce three tasks as follows.\nP1. Parody Detection Parody detection aims to identify whether a comment is parody or normal. In HINs, this can be framed as a binary classification task on edges. Given that parody comments represent a small fraction of all comments, this task can also be considered as outlier detection.\nP2. Comment Sentiment Classification Like parody detection, comment sentiment classification aims to categorize comments into three sentiment labels: positive, negative, and neutral.\nP3. User Sentiment Classification This task focuses on classifying users' sentiment as either a supporter, opponent, or neutral. Unlike the edge classification tasks discussed earlier, this is a node classification task in HINS."}, {"title": "Remarks", "content": "We introduce sentiment classification tasks due to the complexity of the scenarios that include parody comments (Bull, 2010). In the context of parody, these tasks serve as a comprehensive measure to assess the effectiveness of current models in handling parody-related tasks, which will be introduced in the next section."}, {"title": "3 Experiments", "content": "We split all the comment data into training, validation, and test sets with a ratio of 40%/30%/30%. We consider parody detection as a binary classification problem use F1 score for the evaluation. We model the comment and user sentiment classification with parody as multi-class classification problems, and use Macro-F1 to measure the model performance. For comprehensive evaluation and analysis, we test five types of approach in our experiments:\n(1) Embedding-based methods. This category includes Bag-of-Words (BoW) (Mikolov, 2013), Skip-gram (Mikolov et al., 2013), and RoBERTa (Liu, 2019), all of which utilize Multi-Layer Perceptron (MLP) classifiers. These methods are widely used and can provide general text representations to capture linguistic patterns and semantics.\n(2) Inconsistency-based methods. These methods are commonly used for irony detection and we assess BNS-Net (Zhou et al., 2024), DC-Net (Liu et al., 2021), QUIET (Liu et al., 2023a), and SarcPrompt (Liu et al., 2023b). Similar to irony or sarcasm, parody usually contains inconsistencies between literal and intended meaning, and thus, the evaluation of these methods are necessary.\n(3) Outlier detection methods. This category includes Isolation Forest (Liu et al., 2008), the Z-Score Method (Rousseeuw and Hubert, 2011), and One-Class SVM (Li et al., 2003). Similar to outlier detection tasks, where data is highly imbalanced, parody only accounts for around 5%-10% of all comments and tremendously deviates from the normal comment patterns, which makes outlier detection methods quite relevant.\n(4) Graph-based methods. Since (graph-structured) context information is highly important for parody understanding, and to capture complex structural information in user interaction graphs, Graph Neural Networks (GNNs) could be used for user sentiment classification. Three types of classical GNNs are used: Graph Convolutional Net-"}, {"title": "3.2 Performance Comparison", "content": "The evaluation results on the three parody-related tasks are shown in Table 2, 3, 4. The best and runner-up methods for each dataset are highlighted in bold and underlined, respectively. Then, the detailed comparison and analysis are as follows.\nParody Detection. The results in Table 2 indicate that: (1) Parody detection is challenging for all models, with most achieving only 10% ~ 40% F1 scores. Even the best-performing methods for Alibaba. and Drink. reach only 16.17% and 17.39%, respectively, highlighting the difficulty of the task. (2) LLMs generally rank higher but struggle with Chinese datasets. Specifically, both of ChatGPT-40 and Deepseek-V3 achieve 3.86 average rank across all datasets, outperforming other methods. However, traditional methods perform better on Chinese datasets. For instance, SarcPrompt achieves an F1 score of 22.22% on Bride. and 21.39% on CS2, outperforming the best LLM by a large margin. In addition to the performance comparison, we conduct a case study to further investigate how well LLMs understand parody detection in Appendix B.\nSentiment Classification. Tables 3 and 4 present the model performance in comment and user sentiment classification, respectively. Our findings are as follows: (1) Sentiment classification in the context of parody presents significant challenges. The top-performing models across each dataset achieve F1 scores ranging from 40% to 50%, which are notably lower than the performance on traditional sentiment classification benchmarks without parody(Socher et al., 2013; Maas et al., 2011). (2) Although LLMs show their superiority over other methods in terms of average rank, they still underperform some traditional approaches on certain datasets. For example, although ChatGPT-40-mini attains the highest average rank of 4.29 in comment sentiment classification, it performs much worse than BoW+MLP on Bride. and DC-Net on Campus. (3) Graph-based methods demonstrate strong performance on certain datasets. For example, GCN achieves the best results on Bride., suggesting that the relational context information in user-interaction networks is informative and beneficial for some tasks in sentiment classification.\nIn general, all the parody-related tasks are challenging for current models and no model can take dominant advantage over others cross all datasets. These observations underscore the need for further study and model development on parody-related tasks."}, {"title": "3.3 Influence of Context on Parody Detection", "content": "Since parody detection requires a deep understanding of the background information of a topic, intuitively, the context information should have a strong impact on model performance. Therefore, we introduce relevant background details and target comments (when available), and conduct ablation study to investigate its impact on model performance. In Table 5, we report the average F1 score across seven datasets, both with and without context. Performance improvements and declines are highlighted in green and red, respectively.\nOverall, most models benefit from contextual information, with ChatGPT-40 improving significantly from 24.04 to 28.53 and RoBERTa+MLP increasing from 16.43 to 21.23. Our results are consistent with the observations in (Bamman and Smith, 2015; Wang et al., 2015) that context im-"}, {"title": "3.4 Influence of Parody to Sentiment Classification", "content": "To confirm that parody adds challenges to sen-"}, {"title": "3.5 Reasoning LLMs in Parody Detection", "content": "Recently, there has been a surge in reasoning LLMs (OpenAI, 2024), which enhance performance by introducing inference-time scaling in the Chain-of-Thought (CoT) (Wei et al., 2022) reasoning process. To assess the impact of reasoning on LLM performance in parody detection, we compared the performance of reasoning LLMs with that of non-reasoning LLMs. Figure 4 presents the average F1 scores of reasoning LLMs, including ChatGPT01-mini (OpenAI, 2024), ChatGPTo3-mini (OpenAI, 2025), and DeepSeek-R1 (DeepSeek-AI et al., 2025), and non-reasoning LLMs, including ChatGPT40, ChatGPT40-mini, and DeepSeek-V3. Surprisingly, unlike math, coding (Wang et al.) and medical applications (Xu et al., 2024), where reasoning LLMs significantly improve performance, our results show that reasoning LLMs underperform their non-reasoning counterparts. This finding aligns with the conclusion in (Yao et al., 2024), which suggests that tasks like sarcasm detection do not follow a step-by-step reasoning process. This can explain why CoT does not enhance LLM performance. It indicates that the complexities of parody detection may require alternative strategies beyond reasoning, highlighting the need for further research in this area. Please see Appendix D.3 for detailed results on the performance of reasoning LLMs in parody detection."}, {"title": "4 Related Work", "content": "In this section, we introduce the datasets and detection methods related to parody, as well as its associated topics: sarcasm, irony, and humor."}, {"title": "4.1 Dataset", "content": "The datasets for parody and sarcasm cover a diverse array of topics, including politics (Gong et al., 2020), gender (Frenda et al., 2023), and education (Barbieri et al., 2014). They utilize various modalities, such as text (Zhang et al., 2022), speech (Ariga et al., 2024), visual (Schifanella et al., 2016a), and multimodal formats (Bedi et al., 2021; Maity et al., 2022). Beyond the content itself, context plays a crucial role in understanding sarcasm or parody (Wallace et al., 2014). To enhance contextual information, Wang et al. (2015); Bamman and Smith (2015) collect data from dialogues. For annotation, Bamman and Smith (2015); Pt\u00e1ek et al. (2014) use user-provided tags as labels, while Riloff et al. (2013) employ manual annotation. As noted by Chen et al. (2024), the former method requires no human involvement but can lead to noise, as not all users utilize tags. In contrast, the latter approach can yield more generalized labels but may result in significant disagreement among annotators (Joshi et al., 2016). In conclusion, most datasets focus on sarcasm detection (Gong et al., 2020; Zhang et al., 2022; Maity et al., 2022), leaving a notable scarcity of parody datasets."}, {"title": "4.2 Irony or Sarcasm Detection", "content": "Deep learning approaches for detecting parody and sarcasm can be categorized into incongruity-based, sentiment-based, and knowledge-based perspectives (Chen et al., 2024). Incongruity-based methods focus on the inherent incongruity that characterizes sarcastic content (Riloff et al., 2013). For example, Hazarika et al. (2018) and Schifanella et al. (2016b) identify sarcasm by measuring inconsistencies between different targets or modalities. Sentiment-based methods operate on the assumption that there are dependencies between sentiments and sarcasm. Savini and Caragea (2020) propose integrating sentiment tasks into the training process alongside sarcasm detection to enhance model performance. To create emotion-rich representations, Babanejad et al. (2020) incorporate affective and contextual cues. Recognizing that understanding sarcasm can often be implicit, knowledge-based approaches (Chen et al., 2022; Li et al., 2021) leverage external knowledge bases. These methods typically involve knowledge extraction, selection, and integration (Chen et al., 2024)."}, {"title": "5 Conclusions", "content": "In this paper, we introduce FanChuan, a multilingual benchmark for parody detection and analysis, encompassing seven datasets characterized by high diversity, rich contextual information, and precise annotations. Our findings reveal that parody detection remains highly challenging for both LLMs and traditional methods, with particularly poor performance on Chinese datasets. We also observe that contextual information significantly enhances model performance, while parody itself increases the difficulty of sentiment classification. Additionally, our results indicate that reasoning fails to improve LLM performance in parody detection. By filling a critical gap in the study of emerging online phenomena, FanChuan provides valuable insights into cultural values and the role of parody in digital discourse. These findings highlight the limitations"}, {"title": "Limitations", "content": "While this paper proposes a multilingual parody benchmark and provides an extensive analysis, we acknowledge several limitations that warrant further exploration in future work:\n\u2022 Limited dataset diversity. Although we collect datasets and analyze experimental results in both Chinese and English, the understanding of how parody manifests or how effective current methods are for parody detection in other languages remains unclear. Therefore, further efforts could be made to gather datasets in additional languages to enhance the diversity of parody data.\n\u2022 Annotation quality limitations. While we invite multiple annotators and conduct re-checks after labeling, some minor errors may still exist, as annotating parody can be a challenging task. To improve annotation quality in future studies, we will recruit more annotators and provide them with additional background knowledge related to the events before the annotation process. This will help ensure more accurate and consistent annotations.\n\u2022 Limited evaluation of Large Language Models (LLMs). In this study, we only test the performance of LLMs on parody-related tasks through prompt-based methods, without fine-tuning. This approach may not fully capture the potential of LLMs. Additionally, only 6 LLMs were evaluated, which is a relatively small number considering the rapid development of these models. Future work should include a broader range of LLMs and explore fine-tuning approaches to better assess their capabilities in parody detection tasks.\n\u2022 Limited exploration of graph-based methods. In our experiments, Graph Neural Networks (GNNs) are used solely for user sentiment classification. The application of GNNs to parody detection and comment sentiment classification remains unexplored, primarily due to the lack of paradigms that allow GNNs to classify edges in graphs. Future work could focus on designing GNN models tailored to edge classification, enabling more comprehensive experiments on parody detection and comment sentiment analysis."}, {"title": "Ethics Statement", "content": "Our proposed benchmark, FanChuan, adheres to the ACL Code of Ethics. All the coauthors also work as annotators, and are compensated at an average hourly rate of 20 SGD. The data we collected is licensed under CC BY 4.0 and is used exclusively for academic purposes. It consists of publicly available website comments and does not contain any sensitive or personal information. To protect user privacy, we filtered out any private data during the data collection and organization process, ensuring that the dataset does not include any user-sensitive content. Additionally, recognizing the potential presence of malicious content in user debates, we have removed harmful comments that violate community ethical standards. Regarding the cultural and topical elements in the datasets, our research remains neutral and free from bias, solely focused on academic exploration. Lastly, AI was used to revise the grammar during the paper writing process."}, {"title": "A Dataset Details", "content": "Alibaba-Math A student from a vocational school achieved remarkable results in the Alibaba Mathematics Competition, despite coming from a school with a less prestigious reputation. Many people supported her, seeing her as a symbol of rising from humble beginnings and a testament to female empowerment. However, some other people questioned her achievements, suggesting that she might have cheated based on snippets from TV interviews. This topic sparked heated discussions on the Chinese internet. To persuade others to believe their claims, some skeptics impersonated her supporters and used exaggerated praise, saying things like,\u201c\u8fd9\u4f4d\u540c\u5b66\u6709\u5b9e\u529b!\u963f\u91cc\u5df4\u5df4\u6709\u773c\u5149! \u8bf7\u963f\u91cc\u5df4\u5df4\u7834\u683c\u5f55\u53d6\u8fdb\u5165\u8fbe\u6469\u9662,\u52a9\u529b\u963f\u91cc\u79d1\u6280\u5feb\u901f\u53d1\u5c55\u201d\u201c(This student has strength! Alibaba has vision! Please grant her an exceptional admission to DAMO Academy to boost Alibaba's technological growth )\u201d This is a highly complex topic that encompasses mathematics, education, and gender-related controversies. Annotators working with this dataset must not only be familiar with relevant internet memes but also possess a solid understanding of advanced mathematical concepts.\nBridePrice In some parts of China, there is a tradition of giving a bride price to the bride's family upon marriage. Regarding the demands for exorbitant bride prices, some people believe that the bride price serves as a form of security for the bride, providing her with a greater sense of safety in the marriage. Others argue that the bride price has no inherent relation to marital happiness. This has sparked extensive online debates, and to create an absurd and humorous effect, some opponents of the bride price impersonate the supporters and post comments such as:\u201c\u662f\u7684\u662f\u7684,\u59d0\u59b9\u4eec\u5343\u4e07\u522b\u4e71\u5ac1\u4eba,\u627e\u4e0d\u5230\u5e74\u5165\u767e\u4e07\u7684\u5343\u4e07\u522b\u5ac1,\u5973\u5b69\u5b50\u4e94\u5341\u5c81\u90fd\u5f88\u503c\u94b1!\u201d (Ladies, never marry recklessly. If he doesn't make a million a year, don't marry him. Girls are valuable even at fifty!) Gender issues, particularly the topic of bride price, have been a widely debated subject on the Chinese internet for a long time. This dataset requires annotators to be well-versed in these discussions and familiar with the associated memes.\nDrink Water A technology video creator recently posted a video titled \u201cI Made This to Get Everyone to Drink More Water...", "Water Drinking Battle": "ystem designed to encourage hydration through a reward mechanism. Yet, due to the high design cost and limited effectiveness, some viewers questioned its practicality. Some even ironically pretended to support it, leaving comments like\u201c\u9707\u53e4\u70c1\u4eca,\u8db3\u4ee5\u5f00\u542f\u7b2c\u4e94\u6b21\u6280\u672f\u9769\u547d\u201d \u201c(A groundbreaking innovation capable of launching the fifth technological revolution)", "discussions": "someone believes that G2 needs more time to build synergy and has promising potential, while others question whether the roster change truly enhances their chances to win, as they still struggle to overcome their \"mental block\" against NAVI. Some satirical critics even made eye-catching remarks, such as \u201c\u4f20\u5947\u6355\u867e\u4eba\u7ec8\u7ed3\u4e86G2\u7684\u4e09\u65e5\u738b\u671d\u201d\u201c(The legendary shrimp catcher ended G2\u2019s three-day dynasty)\u201d, to express doubts about the effectiveness of G2's roster adjustments. Parody comments in this dataset are particularly difficult to identify for those unfamiliar with the background of CS2, as the comments contain terminology of CS2 game and various aliases of teams and players. Annotators must have a strong understanding of these references to accurately interpret the content.\nCampusLife This dataset was collected from a university forum, covering various discussion topics such as dorm life, campus buses, job hunting, and administration. One particular post sparked a heated debate: a student complained about their roommate bringing their girlfriend to stay overnight in the dorm and sought advice on how to address the situation. The comment section included parodic remarks like \u201cJealous?\u201d, mocking the situation in a humorous yet disapproving tone. Additionally, during the university's open campus day, a poster appeared in a restroom with the title: \u201cApplying to our university? Your tuition funds Palestinian genocide.\u201d In response, some users posted parodic comments, such as: \u201cEvery computer on campus is equipped with an Intel processor, and Intel's R&D center is in Israel! If you want to avoid supporting genocide, switch to a computer with a Zhaoxin CPU immediately!\u201d\nTiktok-Trump In a debate titled \u201cCan One Awakened Youth Withstand 20 Trump Supporters?\u201d, a female Trump supporter lost the debate due to her illogical reasoning and subsequently faced criticism from many netizens who deemed her remarks meaningless. Among the critics, some parodically commented, \u201cShe did a great job bring up solid points\u201d, to criticize the Trump supporter's lack of logical reasoning ability.\nReddit-Trump Trump is a highly controversial figure due to his political stance, ideology, and behavior, sparking widespread debate with both supporters and critics. Some opponents use parody to mimic his tone, such as commenting, \u201cHe's been tested-more than anyone, by the best doctors in the world. They were amazed, and said they'd never seen scores that high. He'll take another if asked, but they said he doesn't need to. It's incredible\u201d, mocking his rhetorical style and contentious image."}, {"title": "B Case Study on LLMs", "content": "To investigate how well LLMs understand parody, we conduct a case study in which LLMs are asked to provide explanations during prediction. Specifically, we construct the prompt by presenting a comment and its associated topic, then ask the LLMs to determine whether the comment is a parody and to explain their reasoning. After receiving the prediction and explanation from the LLMs, we compare the results with the ground truth label and explanation. The results of the case study for BridePrice, Alibaba-Math, DrinkWater, and CS2 are presented in Tables 7, 8, 9, and 10, respectively, using four LLMs: ChatGPT-40 (OpenAI, 2023), Qwen 2.5 (Yang et al., 2024), DeepSeek-V3 (DeepSeek-AI et al., 2024), and Claude3.5 (Anthropic, 2025). The results demonstrate:\n(1) LLMs struggle with parody detection. For example, the parody comment in Table 7 takes an extreme position opposing the viewpoint that a boyfriend should hand over his salary, yet all the LLMs classify this as a non-parody comment. Additionally, the comment in Table 10, which directly expresses a dislike toward the G2 team with analysis, is identified as a parody by 3 of the 4 LLMs.\n(2) LLMs frequently provide incorrect explanations when identifying parody comments. Even in the case of DrinkWater, shown in Table 9, where all the LLMs successfully identify the comment as a parody, they fail to generate accurate explanations. The explanations indicate that the LLMs rely mostly on the style and tone of the comment, without a deeper understanding of the implicit meaning. In conclusion, these results suggest that LLMs struggle to understand parody comments, as they both fail to provide accurate predictions and offer misleading explanations. This highlights the need for further development in LLMs for the task of parody detection."}, {"title": "C Implementation Details", "content": "In this section, we provide implementation details of all the methods used in Section 3. Except from Large Language Models (LLMs), all the other methods are trained on 300 epochs, with an early stopping of 5. We use Adam optimizer to update model parameters. The experiments are conducted on a linux server with Ubuntu 20.04, trained on a single NVIDIA RTX A5000 GPU with 24GB memory. All the methods are trained on train set, the hyperparameters are searched on validation set, where the search space is given by:\n\u2022 Hidden Dimension: {16, 32, 64, 128},\n\u2022 Learning Rate: {5e-6, 1e-5, 2e-5, 3e-5, 5e-5, 1e-4},\n\u2022 Weight Decay: {1e-5, 1e-4},\n\u2022 Batch Size: {16, 32},\nFor the task of parody detection, the threshold for each dataset is the same for all the methods. Specific, we let the threshold be 0.9415 for Alibaba-Math, 0.9526 for BridePrice, 0.9691 for DrinkWater, 0.9387 for CS2, 0.9262 for CampusLife, 0.9406 for Tiktok-Trump, 0.8768 for Reddit-Trump\nPrior to feeding the data into the model, we utilize over sampling with replacement for parody detection, and use Synthetic Minority Over-sampling Technique (SMOTE) (Chawla et al., 2002) for sentiment classification to balance the training data.\nApart from these common settings, we introduce the detailed implementations of each specific model as follows."}, {"title": "BoW+MLP (Mikolov, 2013)", "content": "Bag of Words (BoW) is a kind of word embedding method. In this study, the BoW model implemented in Word2Vec (Mikolov, 2013), aiming to predict a target word based on its surrounding context words. Before using Bag of Words, we standardize text input, remove unnecessary whitespace variations, tokenization text into individual words, and filter out high-frequency words that may not contribute much meaning. Next, we use Bag of Words in Word2Vec to get the word embedding, setting vector size to 50, window to 10, min count to 1, epochs to 50. Multi-Layer Perceptrons (MLP) is a kind of feed-forward neural network. In our study, we employ a three-layer MLP, with a dropout rate set to 0.3 and ReLU as the activation function."}, {"title": "Skip-gram+MLP (Mikolov et al., 2013)", "content": ""}]}