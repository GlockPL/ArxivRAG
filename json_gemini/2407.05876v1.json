{"title": "Efficiently Training Neural Networks for\nImperfect Information Games\nby Sampling Information Sets", "authors": ["Timo Bertram", "Johannes F\u00fcrnkranz", "Martin M\u00fcller"], "abstract": "In imperfect information games, the evaluation of a game\nstate not only depends on the observable world but also relies on hid-\nden parts of the environment. As accessing the obstructed information\ntrivialises state evaluations, one approach to tackle such problems is to\nestimate the value of the imperfect state as a combination of all states in\nthe information set, i.e., all possible states that are consistent with the\ncurrent imperfect information. In this work, the goal is to learn a func-\ntion that maps from the imperfect game information state to its expected\nvalue. However, constructing a perfect training set, i.e. an enumeration\nof the whole information set for numerous imperfect states, is often in-\nfeasible. To compute the expected values for an imperfect information\ngame like Reconnaissance Blind Chess, one would need to evaluate thou-\nsands of chess positions just to obtain the training target for a single\nstate. Still, the expected value of a state can already be approximated\nwith appropriate accuracy from a much smaller set of evaluations. Thus,\nin this paper, we empirically investigate how a budget of perfect infor-\nmation game evaluations should be distributed among training samples\nto maximise the return. Our results show that sampling a small number\nof states, in our experiments roughly 3, for a larger number of separate\npositions is preferable over repeatedly sampling a smaller quantity of\nstates. Thus, we find that in our case, the quantity of different samples\nseems to be more important than higher target quality.", "sections": [{"title": "1 Introduction", "content": "Imperfect information games, which are characterised by unobservable aspects,\nare an important part of Game AI research. In recent years, they have received\nincreased attention due to the inherent complexity of managing uncertainty.\nThis category encompasses a wide array of games, spanning from classical card\ngames like Poker and Bridge to adaptions of traditional board games such as"}, {"title": "2 Problem Statement", "content": "We formalise the problem as follows: Given is a dataset of examples D =\n(xi, Yi) CX \u00d7 Y, where each label yi = f(x\u2081, h\u2081) is determined by a function\nf, dependent not only on the observable information x\u2081, but also on the hidden\ninformation hi. Our goal is to find a function g(x) which approximates f(x, h),\nsuch that Vi \u2208 {1, .., |D|} : g(xi) \u2248 f(xi, hi). This task is non-trivial, and such\na function g does not generally exist, as the same observable x can occur mul-\ntiple times with different labels because, in general, f(x, h(1)) \u2260 f(x, h(2)) for\nh(1) \u2260 h(2).\nOur motivation for this problem originates from imperfect information games,\nwhere the information set represents all possible game states given one player's\ninformation. In several such games, remarkable performance has been achieved\nby basing the imperfect information gameplay, whether implicitly or explicitly,\non perfect information evaluations of states in an information set [3,1,5]. For\nexample, the value of a player's hand in Poker can be estimated as the expected\nvalue of the hand over all possible variations of the community and opponent's\ncards. Similarly, many strong RBC agents rely heavily on chess engines for eval-\nuating conventional chess positions [7,15,8] and approximate the imperfect in-\nformation state with the expected values of the states in the information set."}, {"title": "3 Related Work", "content": "The central objective of this work is to learn the function g which receives\nthe public information of a state x and approximates the expected value of that\nstate, without the need for its (potentially expensive) explicit computation by\niterating over the information set:\ng(x) \u2248 \u0177 = \\sum_{h} P(h|x) f(x, h)\n(1)\nHere, h\u2208 I are all possible configurations of private information that are part\nof the information set I, f is an evaluator of a perfect information state and\nPis a function which gives the probability of each hidden state for the given\nconfiguration x. In practice, P can be influenced by, among others, stochastic\nenvironments or adversary's (hidden) policies. In all cases, it is assumed to be\nan unobservable and unalterable part of the domain. In our experiments, as\nin Figure 1, we assume that all possible determinations are equally likely, i.e.\nP(hx) = 1/\\1(x)| but one could also directly learn meaningful weights for the\npositions in the information set from past behaviour or observations [2].\nA simple strategy to learn g is to collect samples of the form (xi, \u0177i), i.e.,\nto compute the exact value \u011di as in Equation (1) for many training positions\nxi, and to use supervised learning to learn the function \u0177i = g(xi) from these\nsamples. However, this approach generally is too costly due to the potentially\nlarge size of information sets, so obtaining a single \u0177i can require thousands of\nevaluations. Alternatively, \u0177 can be approximated by randomly sampling only a\nfew of the possible h(i), resulting in less accurate training signals y at a lower\ncomputational cost.\nIn our work, we aim to answer a fundamental question: Given a fixed budget\nof N perfect information evaluations, how should we generate the training data\nfor the learner? Options range from generating N different training examples\nxi, each evaluated with one random sample, over using a fixed number of k\nevaluations to generate targets for n = N/k positions, up to exhausting the\nbudget with exactly computing \u0177 for as many examples as possible. This trade-\noff between the training set size n (the number of distinct xi) and label quality\n(the number of evaluations k used to estimate the intended target values \u011di for\neach x\u2081) forms the core focus of this paper.\nThe problem formulated in Section 2 is multifaceted and occurs in several learn-\ning paradigms, thus we can only give a brief overview of how it manifests in\npractice.\nSeveral learning settings may be viewed as special cases of this formula-\ntion. Conventional supervised learning emerges when h\u2081 = \u00d8, Vi, i.e. when no\nhidden information determines yi. Similarly, learning from noisy labels can be\nformulated with a single hidden variable hi, which determines whether the orig-\ninal label remains intact or is corrupted. Knowledge of this hidden information"}, {"title": "4 Experiments", "content": "Here, we present a series of experiments designed to investigate the trade-off\nbetween target quality and training data quantity. The learner has a limited\nbudget of total evaluations N and can decide how many evaluations k should\nbe spent on each training example. Each evaluation yields one value, which are\naggregated into a training target by averaging."}, {"title": "4.1 Texas Hold'em Poker", "content": "The first experiment aims at a real-world setting where we have to balance\nthe label accuracy with the number of total training examples seen. Here, the"}, {"title": "Setup", "content": "In principle, for a given hand x, g(x) could be directly computed as\nthe average over all possible hidden contexts hx, but doing so requires a large\namount of computation. Without accounting for symmetry, a player can have\n{52 \\choose 2} = 1326 unique Poker hands. One would need to compute all possible ar-\nrangements of the remaining cards into two opponent cards and five community\ncards, i.e. {50 \\choose 2} {48 \\choose 5} = 2,781,381,002,400 total combinations. For each\nof these combinations, one needs to evaluate which player won the game and\naverage this for all configurations that pertain to the same player's hand to es-\ntimate the overall winning probability of that hand. While public data for the\nwin-chances of a hand exists, such data is only available for the most popular\ngames and computing them is much costlier in other games with higher degrees\nof uncertainty or more expensive state evaluations. Thus, we aim to decrease\nthe computational cost by only sampling parts of the information set instead\nof enumerating it entirely, and thus allow for the extension of the concept to a\nlarger variety of applications."}, {"title": "Results", "content": "As a first estimate, Figure 3 shows the discrepancy between an esti-\nmated hand strength through evaluations and the mathematical true win chance.\nNotable, with only a single sampled configuration, it is impossible to exactly re-\nceive the true win chance of most hands as the only possible results are 0, 0.5,\nand 1, thus resulting in three error clusters for one sample.\nThe training process (Figure 4) shows that training with fewer evaluations\nper sample leads to much quicker progress when regarding the performance in\nrelation to the total number of evaluations, but when the examples have higher-\nquality evaluations, each update is more meaningful. When comparing the best\nversions (Figure 5), we see that even when equating for the total number of\nevaluations, using a single evaluation leads to worse peak results than using\ntwo, three, five, and ten sampled evaluations. As it should be, equating train-"}, {"title": "4.2 Reconnaissance Blind Chess", "content": "Reconnaissance Blind Chess.4 RBC is an imperfect information adaption of\nchess, where players receive limited information about the opponent's moves.\nWhen training agents to play this game, it is highly useful to be able to eval-\nuate a specific situation (i.e. the received observations at one point in time),\nand evaluation functions for regular chess are readily available (e.g., from open-\nsource programs such as Stockfish5. Thus, computing the average evaluation of\nall states in an information set is an intuitive approach, but doing so is largely\nunfeasible due to the information set size.\nSetup For this experiment, training data is created offline for each k, thus gen-\nerating a fixed training set for each setting. Each learner has a fixed budget of\n1 million state evaluations that can be arbitrarily distributed among different"}, {"title": "Results", "content": "Again approximating the sampling error through Monte-Carlo esti-\nmates (Figure 6), we receive the expected result: Sampling only a small number\nof states can lead to a large discrepancy between the approximation and the\nground truth, but we see diminishing returns, such that sampling more than 50\npositions only leads to slight improvements in the approximation. We thus ex-\npect efficiency increases with repeated sampling, but more than 50 states should\nlead to meaningfully degrading performance.\nThis observation is confirmed by the results in Figures 7 and 8. The train-\ning curves of Figure 8 show that using singular samples results in poor overall\naccuracy, but vast oversampling leads to too few total training examples. The\nextreme choices of k (1 and 1000) perform poorly, but moderate sampling is\nuseful."}, {"title": "5 Summary and Conclusion", "content": "With this work, we provided experimental results on the influence of sampling\ndifferent numbers of states from an information set to learn an evaluation of\nthe whole set. For a given task, a total budget of N evaluations is given, which\ncan be distributed across information sets. Thus, we investigated the trade-off\nbetween the overall number of training samples generated and the accuracy of\ntheir associated training targets.\nFirstly, the trade-off is influenced by the cost of generating evaluations and\nthe cost of making an update to the learner, so the choice of k needs to be\nrelated to their balance. We find that in Heads Up Poker and Reconnaissance"}, {"title": "6 Future Work", "content": "We see multiple intriguing lines of further work based on these findings. First,\nwe here assumed no agency over the process of sampling from the information\nsets and no online variation in sample numbers. Removing either of those as-\nsumptions will likely lead to better results, and some strategies have previously\nbeen outlined by Sheng[18] for categorical tasks. In addition, while our general\nformulation holds for other distributions of states, we used a uniform distribu-\ntion of states for our experiments. This is a sensible assumption for Poker, but\ninformation sets do not follow a uniform distribution in RBC. Knowledge of this,\nor even access to a proxy of such a distribution, would lead to more accurate\nestimations in real-world tasks. Whether a non-uniform distribution changes the\nbest choice of sampled evaluations will be investigated in the future."}]}