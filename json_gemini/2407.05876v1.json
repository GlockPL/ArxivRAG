{"title": "Efficiently Training Neural Networks for\nImperfect Information Games\nby Sampling Information Sets", "authors": ["Timo Bertram", "Johannes F\u00fcrnkranz", "Martin M\u00fcller"], "abstract": "In imperfect information games, the evaluation of a game\nstate not only depends on the observable world but also relies on hid-\nden parts of the environment. As accessing the obstructed information\ntrivialises state evaluations, one approach to tackle such problems is to\nestimate the value of the imperfect state as a combination of all states in\nthe information set, i.e., all possible states that are consistent with the\ncurrent imperfect information. In this work, the goal is to learn a func-\ntion that maps from the imperfect game information state to its expected\nvalue. However, constructing a perfect training set, i.e. an enumeration\nof the whole information set for numerous imperfect states, is often in-\nfeasible. To compute the expected values for an imperfect information\ngame like Reconnaissance Blind Chess, one would need to evaluate thou-\nsands of chess positions just to obtain the training target for a single\nstate. Still, the expected value of a state can already be approximated\nwith appropriate accuracy from a much smaller set of evaluations. Thus,\nin this paper, we empirically investigate how a budget of perfect infor-\nmation game evaluations should be distributed among training samples\nto maximise the return. Our results show that sampling a small number\nof states, in our experiments roughly 3, for a larger number of separate\npositions is preferable over repeatedly sampling a smaller quantity of\nstates. Thus, we find that in our case, the quantity of different samples\nseems to be more important than higher target quality.", "sections": [{"title": "1 Introduction", "content": "Imperfect information games, which are characterised by unobservable aspects,\nare an important part of Game AI research. In recent years, they have received\nincreased attention due to the inherent complexity of managing uncertainty.\nThis category encompasses a wide array of games, spanning from classical card\ngames like Poker and Bridge to adaptions of traditional board games such as"}, {"title": "2 Problem Statement", "content": "We formalise the problem as follows: Given is a dataset of examples $\\mathcal{D} =$\n$(\\mathbf{x}_i, y_i) \\subset \\mathcal{X} \\times \\mathcal{Y}$, where each label $y_i = f(\\mathbf{x}_i, \\mathbf{h}_i)$ is determined by a function\n$f$, dependent not only on the observable information $\\mathbf{x}_i$, but also on the hidden\ninformation $\\mathbf{h}_i$. Our goal is to find a function $g(\\mathbf{x})$ which approximates $f(\\mathbf{x},\\mathbf{h})$, such that $\\forall i \\in \\{1, .., |\\mathcal{D}|\\} : g(\\mathbf{x}_i) \\approx f(\\mathbf{x}_i, \\mathbf{h}_i)$. This task is non-trivial, and such\na function $g$ does not generally exist, as the same observable $\\mathbf{x}$ can occur mul-\ntiple times with different labels because, in general, $f(\\mathbf{x}, \\mathbf{h}^{(1)}) \\neq f(\\mathbf{x}, \\mathbf{h}^{(2)})$ for\n$\\mathbf{h}^{(1)} \\neq \\mathbf{h}^{(2)}$.\nOur motivation for this problem originates from imperfect information games,\nwhere the information set represents all possible game states given one player's\ninformation. In several such games, remarkable performance has been achieved\nby basing the imperfect information gameplay, whether implicitly or explicitly,\non perfect information evaluations of states in an information set [3,1,5]. For\nexample, the value of a player's hand in Poker can be estimated as the expected\nvalue of the hand over all possible variations of the community and opponent's\ncards. Similarly, many strong RBC agents rely heavily on chess engines for eval-\nuating conventional chess positions [7,15,8] and approximate the imperfect in-\nformation state with the expected values of the states in the information set."}, {"title": "3 Related Work", "content": "The central objective of this work is to learn the function $g$ which receives\nthe public information of a state $x$ and approximates the expected value of that\nstate, without the need for its (potentially expensive) explicit computation by\niterating over the information set:\n$g(\\mathbf{x}) \\approx \\hat{y} = \\sum_{\\mathbf{h}} P(\\mathbf{h}| \\mathbf{x}) \\cdot f(\\mathbf{x}, \\mathbf{h})$\nHere, $\\mathbf{h} \\in \\mathcal{I}$ are all possible configurations of private information that are part\nof the information set $\\mathcal{I}$, $f$ is an evaluator of a perfect information state and\n$P$ is a function which gives the probability of each hidden state for the given\nconfiguration $x$. In practice, $P$ can be influenced by, among others, stochastic\nenvironments or adversary's (hidden) policies. In all cases, it is assumed to be\nan unobservable and unalterable part of the domain. In our experiments, as\nin Figure 1, we assume that all possible determinations are equally likely, i.e.\n$P(\\mathbf{h}|\\mathbf{x}) = 1/|\\mathcal{I}(\\mathbf{x})|$ but one could also directly learn meaningful weights for the\npositions in the information set from past behaviour or observations [2].\nA simple strategy to learn $g$ is to collect samples of the form $(\\mathbf{x}_i, \\hat{y}_i)$, i.e.,\nto compute the exact value $\\hat{y}_i$ as in Equation (1) for many training positions\n$\\mathbf{x}_i$, and to use supervised learning to learn the function $\\hat{y}_i = g(\\mathbf{x}_i)$ from these\nsamples. However, this approach generally is too costly due to the potentially\nlarge size of information sets, so obtaining a single $\\hat{y}_i$ can require thousands of\nevaluations. Alternatively, $\\hat{y}$ can be approximated by randomly sampling only a\nfew of the possible $\\mathbf{h}^{(i)}$, resulting in less accurate training signals $y$ at a lower\ncomputational cost.\nIn our work, we aim to answer a fundamental question: Given a fixed budget\nof $N$ perfect information evaluations, how should we generate the training data\nfor the learner? Options range from generating $N$ different training examples\n$\\mathbf{x}_i$, each evaluated with one random sample, over using a fixed number of $k$\nevaluations to generate targets for $n = N/k$ positions, up to exhausting the\nbudget with exactly computing $\\hat{y}$ for as many examples as possible. This trade-\noff between the training set size $n$ (the number of distinct $\\mathbf{x}_i$) and label quality\n(the number of evaluations $k$ used to estimate the intended target values $\\hat{y}_i$ for\neach $\\mathbf{x}_i$) forms the core focus of this paper.\nThe problem formulated in Section 2 is multifaceted and occurs in several learn-\ning paradigms, thus we can only give a brief overview of how it manifests in\npractice.\nSeveral learning settings may be viewed as special cases of this formula-\ntion. Conventional supervised learning emerges when $h_i = \\emptyset, \\forall i$, i.e. when no\nhidden information determines $y_i$. Similarly, learning from noisy labels can be\nformulated with a single hidden variable $h_i$, which determines whether the orig-\ninal label remains intact or is corrupted. Knowledge of this hidden information"}, {"title": "4 Experiments", "content": "Here, we present a series of experiments designed to investigate the trade-off\nbetween target quality and training data quantity. The learner has a limited\nbudget of total evaluations $N$ and can decide how many evaluations $k$ should\nbe spent on each training example. Each evaluation yields one value, which are\naggregated into a training target by averaging."}, {"title": "4.1 Texas Hold'em Poker", "content": "The first experiment aims at a real-world setting where we have to balance\nthe label accuracy with the number of total training examples seen. Here, the"}, {"title": "4.2 Reconnaissance Blind Chess", "content": "Reconnaissance Blind Chess. RBC is an imperfect information adaption of\nchess, where players receive limited information about the opponent's moves.\nWhen training agents to play this game, it is highly useful to be able to eval-\nuate a specific situation (i.e. the received observations at one point in time),\nand evaluation functions for regular chess are readily available (e.g., from open-\nsource programs such as Stockfish. Thus, computing the average evaluation of\nall states in an information set is an intuitive approach, but doing so is largely\nunfeasible due to the information set size.\nSetup For this experiment, training data is created offline for each $k$, thus gen-\nerating a fixed training set for each setting. Each learner has a fixed budget of\n1 million state evaluations that can be arbitrarily distributed among different"}, {"title": "5 Summary and Conclusion", "content": "With this work, we provided experimental results on the influence of sampling\ndifferent numbers of states from an information set to learn an evaluation of\nthe whole set. For a given task, a total budget of $N$ evaluations is given, which\ncan be distributed across information sets. Thus, we investigated the trade-off\nbetween the overall number of training samples generated and the accuracy of\ntheir associated training targets.\nFirstly, the trade-off is influenced by the cost of generating evaluations and\nthe cost of making an update to the learner, so the choice of $k$ needs to be\nrelated to their balance. We find that in Heads Up Poker and Reconnaissance"}, {"title": "6 Future Work", "content": "We see multiple intriguing lines of further work based on these findings. First,\nwe here assumed no agency over the process of sampling from the information\nsets and no online variation in sample numbers. Removing either of those as-\nsumptions will likely lead to better results, and some strategies have previously\nbeen outlined by Sheng[18] for categorical tasks. In addition, while our general\nformulation holds for other distributions of states, we used a uniform distribu-\ntion of states for our experiments. This is a sensible assumption for Poker, but\ninformation sets do not follow a uniform distribution in RBC. Knowledge of this,\nor even access to a proxy of such a distribution, would lead to more accurate\nestimations in real-world tasks. Whether a non-uniform distribution changes the\nbest choice of sampled evaluations will be investigated in the future."}]}