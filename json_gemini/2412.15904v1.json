{"title": "What are Step-Level Reward Models Rewarding?\nCounterintuitive Findings from MCTS-boosted Mathematical Reasoning", "authors": ["Yiran Ma", "Zui Chen", "Tianqiao Liu", "Mi Tian", "Zhuo Liu", "Zitao Liu", "Weiqi Luo"], "abstract": "Step-level reward models (SRMs) can significantly enhance\nmathematical reasoning performance through process super-\nvision or step-level preference alignment based on reinforce-\nment learning. The performance of SRMs is pivotal, as they\nserve as critical guidelines, ensuring that each step in the rea-\nsoning process is aligned with desired outcomes. Recently,\nAlphaZero-like methods, where Monte Carlo Tree Search\n(MCTS) is employed for automatic step-level preference an-\nnotation, have proven particularly effective. However, the\nprecise mechanisms behind the success of SRMs remain\nlargely unexplored. To address this gap, this study delves into\nthe counterintuitive aspects of SRMs, particularly focusing\non MCTS-based approaches. Our findings reveal that the re-\nmoval of natural language descriptions of thought processes\nhas minimal impact on the efficacy of SRMs. Furthermore,\nwe demonstrate that SRMs are adept at assessing the complex\nlogical coherence present in mathematical language while\nhaving difficulty in natural language. These insights provide\na nuanced understanding of the core elements that drive ef-\nfective step-level reward modeling in mathematical reason-\ning. By shedding light on these mechanisms, this study offers\nvaluable guidance for developing more efficient and stream-\nlined SRMs, which can be achieved by focusing on the crucial\nparts of mathematical reasoning.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated their\nremarkable capabilities across a wide range of tasks, such\nas information extraction, natural language understanding,\netc (Zhao et al. 2023), totally revolutionizing the deep learn-\ning community. Among these capabilities, reasoning stands\nout as a critical area of focus, especially mathematical rea-\nsoning, which needs to be further improved due to its com-\nplex nature. Numerous studies have shown that multi-step\nreasoning often facilitated through Chain-of-Thought (CoT)\nprompting, can significantly enhance model performance on\nreasoning tasks (Zhou et al. 2023; Besta et al. 2024; Ding\net al. 2023; Yao et al. 2024; Wang et al. 2022; Wei et al.\n2022; Zheng et al. 2024; Li et al. 2024; Zhan et al. 2024).\nRecently, guided tree-search methods further improved\nreasoning performance by exploring various reasoning paths\nthrough online simulation to identify the optimal solution\npaths (Hao et al. 2023, 2024; Feng et al. 2023). Although\na better reasoning path leads to a better performance, the\nlength of these reasoning chains leads to an exponential in-\ncrease in the search space, resulting in substantial computa-\ntional costs. Given the high expense of LLM inference, per-\nforming an online tree search for each reasoning problem\nintroduces repeated and unnecessary overhead.\nTo address this issue, step-level reward models (SRM)"}, {"title": "Preliminaries", "content": "Definition A Markov Decision Process (MDP) is a math-\nematical framework used to model decision-making prob-\nlems. This framework is fundamental for addressing a wide\nrange of reinforcement learning (RL) problems where the\noutcomes are partially random and partially controllable. An\nMDP is defined by a tuple (S, A, P, R, \u03b3), where:\n\u2022 S is the set of states.\n\u2022 A is the set of actions.\n\u2022 P is the transition probability function, P(st+1|st, at),\nwhich defines the probability of transitioning to state\nst+1 given the current state st and action at.\n\u2022 R is the reward function, R(st, at, st+1), which defines\nthe reward received after transitioning from state st to\nstate st+1 by taking action at.\n\u2022 y is the discount factor, which determines the importance\nof future rewards."}, {"title": "Markov Decision Process", "content": "For state value function\nV(s), the Bellman Expectation Equation is:\n\n\u03c0\nV*(s) = \u0395\u03b1\u03c0(:1s) [Es'~P(\ns,a) [R(s, a, s') + V\" (s')]]\n\nFor state-action value function Q(s, a), the Bellman Ex-\npectation is:\n\nQ\" (s,a) = Es\u2032~P(.\ns,a) [R(s, a, s') + Ea'\u223c\u03c0(:\ns') [Q*(s', a')]]\n\nOptimal Value Functions The optimal value functions\nare defined as:\n\nV*(s) = max V(s)\n\u03c0\nQ*(s, a) = max Q (s, a) \n\u03c0\nTherefore, the relationship between the optimal value func-\ntions and the Bellman Optimality Equation is:\n\nV*(s) = max Q*(s,a)\na"}, {"title": "Setup", "content": "LLM's Math Reasoning as MDP: Our Definition\nFigure 2 shows the mathematical reasoning process with\neach step decomposed into thought and math expressions.\nSpecifically, our MDP definition is as follows:\n\nMDP = (S, A, P, R)\nwhere:\n\u2022 State The state space S consists of states defined as si =\n(Tk, Ek)=0, representing a sequence of thoughts Tk and\nequations Ek up to step i.\n\u2022 Action The action space A consists of actions defined as\nai = Ti+1, representing the natural language descrip-\ntions of the subsequent thought proposed by the LLM.\n\u2022 State Transition P(Si+1|Si, ai) is the state transition\nfunction, defining the probability of transitioning to state\nSi+1 from state si after taking action ar. This function\nis implemented by the LLM generating the correspond-\ning math expression Ei+1 based on the next thought\nai = Ti+1 and the current state si = (Tk, Ek)k=0.\n\u2022 Reward Function R(si, ai, Si+1) is the reward function,\ndefining the immediate reward received after transition-\ning to state si+1 = (Tk, Ek)=1 from state si by taking\naction ar. We define the reward up to state si+1 based on\nwhether it can lead to the correct final answer:\n\nR(Si, ai, Si+1)\n\n(\n1,\n10,\nfinal answer is correct\nfinal answer is incorrect\n\nAdditionally, policy \u03c0(ai|si) is implemented by the LLM\ngenerating the thought of the next step ai = Ti+1 based\non the current state si = (Tk, Ek)=0. According to Equa-\ntion (1), the goal of an agent is to maximize V(si) or\nQ\u03c0(si, a) by generating the correct thoughts T in each step.\nIn summary, a language model plays a dual role in the\nMDP framework:"}, {"title": "MCTS for Step-Level Preference Collection", "content": "Understanding the natural correspondence between math\nreasoning and MDP, we can readily use MCTS for ef-\nficient step-level preference collection. The MCTS starts\nfrom a root node so, which is a math problem in mathe-\nmatical reasoning tasks. Then, each new node corresponds\nto a state update. Each iteration of MCTS can be divided\ninto four phases: Selection, Expansion, Rollout, and Back-\npropagation.\n\n\nwhere c(si+1) is the correct counts, N(si) and N(Si+1)\nare visit counts, and Wexp balances exploration and ex-\nploitation. This process continues until an unexplored\nnode is found."}, {"title": "Step-level Reward Modeling", "content": "After collecting all the preference pairs, step-level reward\nmodels can be constructed through contrastive learning.\nBased on our MDP definition, an SRM is regarded as the\naction-value function Q(s, a) or the value function V(s).\nSpecifically, we investigate different reward models for ab-\nlation studies, where reward models take different inputs\nto evaluate the ongoing reasoning process. Accordingly, we\ndefine four reward models (Figure 2-right) for the ablation\nstudy:\n\nV\u2081(si) = V1((Tk, Ek)k=0)\n\nV2(Si) = V2((Ek)k=0)\n\nV3(Si) = V3(Ei)\n\nQ(Si, ai) = Q((Tk, Ek)k=0, Ti+1)"}, {"title": "Beam Search with Step-Level Reward Model", "content": "Given the SRMs trained on the preference data, it is com-\nmonly used for step-level preference alignment to update the\npolicy. The purpose of this procedure is to generate the best\naction through the updated policy \u03c0', thereby reducing the\noverhead caused by online MCTS. It is also possible to up-\ndate the world model P with these preference pairs as better\naccuracy indicates better mathematical performance.\n\nlim P(arg max Q(s, at) = arg max Q(s,a)) = 1\n\u03b1\u03be\u0391\u03c0(8)\n{at}=0\nn\u2192\u221e\nlim P(arg max V (s) = arg max V (s')) = 1\nn\u2192\u221e\n{s}=0\ns'ES(s,a)\nImplementation Details\nDatasets To construct step-level preference pairs through\nMCTS, we use the math problems and their corresponding"}, {"title": "Main Results", "content": "After collecting all the step-level preference pairs through\nMCTS, datasets are constructed for FC-SRM, MO-SRM,\nSSMO-SRM, and NT-SRM training by selecting the cor-\nresponding components in each piece of data. The training"}, {"title": "Do we really need natural language?", "content": "Intuitively, one might expect that natural language de-\nscriptions provide essential contextual information and aid\nSRMs' cognitive understanding. The SRMs with different\ninput formats: full-context (FC) and math-only (MO) are\ntrained to investigate this aspect."}, {"title": "Can SRMs evaluate logical coherence in math\nlanguage?", "content": "The success of MCTS-based methods is attributed to the\nability to avoid logical and numerical errors. It is commonly\nbelieved that logical errors are more difficult to evaluate,\nwhile MCTS-based methods are believed a competitive so-\nlution to this challenge by collecting such preferences. In\nthis section, we investigate the role of natural language and\nmathematical language in assessing the logical coherence in-\ncluded in pure mathematical language by comparing SSMO-\nSRM, MO-SRM, and NT-SRM.\nLLMs can be trained to evaluate logical coherence in\npure mathematical language. For DeepSeek-Math-7B-\nBase, MO-SRM achieves an accuracy gain of +7.35% on\nGSM8K and +8.48% on MATH, which is higher than the\ngains +3.64% and 6.30% observed for SSMO-SRM. Simi-\nlarly, for Qwen2-7B, MO-SRM achieves an accuracy gain\nof +5.31% on GSM8K and +3.94% on MATH, higher than\nthat of SSMO-SRM +3.18% and +1.92%. This substantial"}, {"title": "Additional Analysis", "content": "Table 2: Supervise a larger model (Llama-3-70B-Instruct).\nSupervising a larger model Despite being trained on\npreference data generated by a smaller model, the MO-SRM\nwas able to effectively guide the reasoning process of a\nlarger model and achieve substantial improvements (+2.58%"}, {"title": "Conclusion", "content": "Our investigation into the role of natural language and math-\nematical expressions in step-level reward modeling reveals\nthat natural language descriptions are not essential for the\nsuccess of these models. Through extensive experiments, we\ndemonstrated that reward models operating solely on math-\nematical expressions perform comparably to those that in-\ncorporate both natural language and math. Furthermore, the\ndifficulty in training models to evaluate the coherence of nat-\nural language thought processes underscores the challenges\nLLMs face in capturing implicit logical structures through\nlanguage alone. We also found that the coherence of log-\nical structure inherent in mathematical expressions can be\nassessed by SRMs trained based on LLMs. Given the over-\nhead of obtaining step-level rewards, these findings offer\nnew insights for developing more efficient and targeted re-\nward models by isolating the most impactful components of\nmathematical reasoning steps."}, {"title": "Additional Results", "content": "Tendency of encouraging shorter paths\nWe observed that the greedy search with the SRMs tends to encourage shorter reasoning paths, although the MCTS itself does\nnot explicitly include the path length as a preference. (Figure .2) This observation is due to the insufficient exploitation of the\nMCTS process, but we need further investigation to confirm this proposition in future studies."}, {"title": "Implementation Details", "content": "Prompts\nSystem message (Agent)\nYou should act as a guide. You will break down the process into individual, understandable guidance step-by-step, each\nleading logically to the final result. I will follow your guidance by calculating the answer to each step with equations.\n### Your response must meet the following requirements:\n1. Never say anything not related to the math problem.\n2. You should not include any calculations in your instruction as that is the student's work.\n3. If the current math problem is ready to be solved by following your next guidance, start it with \"Now you can answer\nthe problem in this step.\".\n4. If the final answer to the current math problem has been obtained, just say \"The math problem has been solved.\"\nSystem message (World Model-GSM8K)\nYou are a student solving math problems under the instructions of the teacher. You should follow the step-by-step\nguidance posed by the teacher by calculating the answer of each step with equations until you deduce the final answer\nto the math problem.\n### Your response must meet the following requirements:\n1. Never talk about anything not related to the math problem.\n2. Include the equation of this step.\n3. If the guidance starts with \"Now you can answer the problem in this step.\", you must find the final answer to the\nproblem in this step.\n4. End with \"The answer is\u201d along with a single number to highlight the numerical (sub)answer (e.g. \u201cThe answer is\n42.\").\nSystem message (World Model-MATH)\nYou are a student solving math problems under the instructions of the teacher. You should follow the step-by-step\nguidance posed by the teacher by calculating the answer of each step with equations until you deduce the final answer\nto the math problem.\n### Your response must meet the following requirements:\n1. Include the equation of this step.\n2. If the subquestion is started with start it with \u201cNow you can answer the problem in this step.; you must find the final\nanswer to the problem in this step.\n3. You must use the LaTeX code \""}, {"title": "Hyperparameters", "content": "MCTS The hyperparameters of MCTS are shown in Table .1."}]}