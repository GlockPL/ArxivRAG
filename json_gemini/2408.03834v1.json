{"title": "TARGET PROMPTING FOR INFORMATION EXTRACTION WITH VISION LANGUAGE MODEL", "authors": ["Dipankar Medhi"], "abstract": "The recent trend in the Large Vision and Language model has brought a new change in how information extraction systems are built. VLMs have set a new benchmark with their State-of-the-art techniques in understanding documents and building question-answering systems across various industries. They are significantly better at generating text from document images and providing accurate answers to questions. However, there are still some challenges in effectively utilizing these models to build a precise conversational system. General prompting techniques used with large language models are often not suitable for these specially designed vision language models. The output generated by such generic input prompts is ordinary and may contain information gaps when compared with the actual content of the document. To obtain more accurate and specific answers, a well-targeted prompt is required by the vision language model, along with the document image. In this paper, a technique is discussed called Target prompting, which focuses on explicitly targeting parts of document images and generating related answers from those specific regions only. The paper also covers the evaluation of response for each prompting technique using different user queries and input prompts.", "sections": [{"title": "Introduction", "content": "Understanding large documents and complex reports with the help of large language models [11, 5] has revolutionized knowledge distribution and accessibility. With the rise of the large models in the industry [3], more and more fields are considering using these large models for their applications and incorporating them into their current or building new AI-powered systems, empowering streamlined usage by their users.\n\nThe current large models are great at learning and storing factual information, but they have limited ability to access and manipulate this knowledge. To address these limitations, RAG pipelines were introduced [12]. They combine a database (knowledge base) with the model [7], expanding its memory and information pool without relying solely on the model's ability to learn new tasks as needed. These databases are specifically designed for RAG systems to quickly and accurately retrieve relevant information based on input queries. They are also known as Vector Databases, where the data is stored in the format of vector embeddings [16, 10, 8].\n\nThe performance and accuracy of the RAG system heavily depend on how the data is processed and stored in the vector databases. For pipelines dealing with large documents and reports, it is crucial for the data or information extractor to perform well. Their ability to extract the correct information from the documents determines the quality of the responses by the generative model [3, section 2.2]. The better the quality of the data and the closer it is to the actual information in the documents, the better will be the model responses. Therefore, it is highly important to fine-tune the data extraction process.\n\nDocument parsing tools like PDF and text parsers are suitable for digitally generated documents where the words are typed. However, the real challenge arises when the requirement is to extract information from image-based documents, where the text is not printed but present inside an image in a document. There are other alternatives such as Optical Character Recognition or OCR, that convert scanned or printed text images, and handwritten text, into digital text. Though this technology can solve basic text extraction tasks, it can face challenges while distinguishing similar types of characters like \"0\" and \"O\" [17]. This shows that there is a loss of information when OCR is used to extract information from complex document images."}, {"title": "Method", "content": "The pipeline expects an image and a prompt for the model to generate text from the document image. A collection of open-source document images has been gathered and combined into a dataset for the experiment. Since the objective of this experiment is not to benchmark the method but to present a way of instructing the model for information extraction, only a handful of relevant images have been chosen for the study."}, {"title": "Model and Dataset", "content": "Phi-3-vision-instruct. It is an open-sourced model by Microsoft with a parameter size of 4.2 Billion [1]. It is a multimodal model that can process image and textual prompts, and generate textual outputs. This model has two primary components, an image encoder and a transformer decoder [1, page 9]. The image encoder used in the Phi-3 Vision model is CLIP ViT-L/14 [18]. It is responsible for processing the visual information from the input image. The visual tokens extracted by the image encoder are combined with the text tokens in an interleaved manner, without any specific order. The transformer decoder is Phi-3-mini-128k-instruct [1]. It is responsible for processing the combined visual and textual tokens to generate textual outputs based on the input image and text prompts."}, {"title": "Information Extraction", "content": "The model and processor are initialized in an extraction function that takes in 2 arguments, image and prompt. All the image processing and tokenization are handled by the processor, followed by the answer generation by the model itself. The extraction process runs in a loop, iterating over all the selected images, feeding into the inference code along with the prompt, and generating results for each image one after another. See figure 3. The input prompt is the aggregation of the user query and the system prompt, <image_1>. This ensures that the prompt is in the correct format when fed into the information extraction function."}, {"title": "Experiments", "content": "The experiment is primarily performed on the document images from the dataset, consisting of tabular pages and reports with infographics and charts. Since there are multiple entries with duplicate images, only a handful of them have been chosen for this experiment. The images are fed to the network with a user query and the system prompt, and the output is evaluated manually to rate its relevance. The model expects a system prompt [1], \"<image_1>\" as the image placeholder and following that an input query. See figure 3 for an overview.\n\nGeneral prompting. Prompting is a method of guiding a large pre-trained model to tackle new tasks by providing specific instructions and task-specific hints [9]. For tasks like targeted question answering or data extraction from documents, the prompt needs to be tailored or adjusted to the specific section of the document from which the information is needed [4].\n\nWhen prompts are generalized, the resulting answers encompass the overall meaning of the image. If the aim is to grasp the general idea of a document or image, a prompt derived from a description query is adequate for the model to provide the image document's context [13]. The model generates an answer that captures the whole meaning of the image document, yielding a more comprehensive response that covers every aspect of the image document.\n\nWhen the model is asked to provide a detailed description of an input image, the generated response contains a general overview of the image or document, spanning from top to bottom [1]. However, When observed properly, it becomes clear that the model's response lacks specific details, indicating a difference between the generated content and the actual image document content. The information generated by the model is partially accurate, capturing only a limited amount of context from the original document. This pattern is observed across the majority of the documents utilized in the experiment.\n\nTarget Prompting. The problem of extracting targeted information from the image document using the model can be effectively achieved using target prompting. It has been observed that by explicitly instructing the model [15] to describe a particular portion of the document image, it performs well compared to when it was asked a more general question [2]. This enables the model to perform tasks without covering everything from the input image object. By isolating regions of interest based on specific needs, it is possible to obtain precise answers more efficiently."}, {"title": "Results", "content": "Focusing on a specific portion of an image and trying to extract information from that region alone can greatly improve the model performance and the final output accuracy. The target prompting technique has greatly improved the information extraction process by retrieving content from only those portions of the document images which has been pointed to in the prompt. This experiment is performed in an instance with 16 GB RAM and a Tesla T4 GPU.\n\nThe targeted prompts mostly include questions that are specific to a particular region or from a definite portion of the document image. See listing 1. This ensures that the generated output is free from any unwanted information or noise. The response is relevant to the question asked and only generates results related to the input query prompt. It enables a more rigid control over the type of responses to be generated from the documents. The same image can be fed multiple times to the extraction function to extract the desired information only, avoiding any irrelevant content and unwanted noise in the final answer. This has the potential to reduce the post-generation processes as the majority of the documents when processed with this technique will have only answers relevant to what is being asked in the prompt."}, {"title": "Conclusion", "content": "The paper proposed a specialized method to direct the vision language model, Phi-3-vision-instruct, in generating specific responses related to the user input query. Although the results are not completely accurate, it performs significantly better when the task is to cover a certain portion of an image or document. There are more potential routes for improving the model responses and enhancing the accuracy of the proposed prompting technique, but that will require more testing and evaluation using a variety of datasets. In future work, the plan is to use an extended version of the dataset and conduct experiments to evaluate the performance and accuracy of the method on more complex documents."}]}