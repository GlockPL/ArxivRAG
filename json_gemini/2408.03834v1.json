{"title": "TARGET PROMPTING FOR INFORMATION EXTRACTION WITH VISION LANGUAGE MODEL", "authors": ["Dipankar Medhi"], "abstract": "The recent trend in the Large Vision and Language model has brought a new change in how information\nextraction systems are built. VLMs have set a new benchmark with their State-of-the-art techniques\nin understanding documents and building question-answering systems across various industries. They\nare significantly better at generating text from document images and providing accurate answers to\nquestions. However, there are still some challenges in effectively utilizing these models to build a\nprecise conversational system. General prompting techniques used with large language models are\noften not suitable for these specially designed vision language models. The output generated by such\ngeneric input prompts is ordinary and may contain information gaps when compared with the actual\ncontent of the document. To obtain more accurate and specific answers, a well-targeted prompt is\nrequired by the vision language model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts of document images\nand generating related answers from those specific regions only. The paper also covers the evaluation\nof response for each prompting technique using different user queries and input prompts.", "sections": [{"title": "Introduction", "content": "Understanding large documents and complex reports with the help of large language models [11, 5] has revolutionized\nknowledge distribution and accessibility. With the rise of the large models in the industry [3], more and more fields are\nconsidering using these large models for their applications and incorporating them into their current or building new\nAI-powered systems, empowering streamlined usage by their users.\n\nThe current large models are great at learning and storing factual information, but they have limited ability to access\nand manipulate this knowledge. To address these limitations, RAG pipelines were introduced [12]. They combine a\ndatabase (knowledge base) with the model [7], expanding its memory and information pool without relying solely on\nthe model's ability to learn new tasks as needed. These databases are specifically designed for RAG systems to quickly\nand accurately retrieve relevant information based on input queries. They are also known as Vector Databases, where\nthe data is stored in the format of vector embeddings [16, 10, 8].\n\nThe performance and accuracy of the RAG system heavily depend on how the data is processed and stored in the vector\ndatabases. For pipelines dealing with large documents and reports, it is crucial for the data or information extractor to\nperform well. Their ability to extract the correct information from the documents determines the quality of the responses\nby the generative model [3, section 2.2]. The better the quality of the data and the closer it is to the actual information in\nthe documents, the better will be the model responses. Therefore, it is highly important to fine-tune the data extraction\nprocess.\n\nDocument parsing tools like PDF and text parsers are suitable for digitally generated documents where the words are\ntyped. However, the real challenge arises when the requirement is to extract information from image-based documents,\nwhere the text is not printed but present inside an image in a document. There are other alternatives such as Optical\nCharacter Recognition or OCR, that convert scanned or printed text images, and handwritten text, into digital text.\nThough this technology can solve basic text extraction tasks, it can face challenges while distinguishing similar types of\ncharacters like \"0\" and \"O\" [17]. This shows that there is a loss of information when OCR is used to extract information\nfrom complex document images."}, {"title": "Method", "content": "The pipeline expects an image and a prompt for the model to generate text from the document image. A collection of\nopen-source document images has been gathered and combined into a dataset for the experiment. Since the objective of\nthis experiment is not to benchmark the method but to present a way of instructing the model for information extraction,\nonly a handful of relevant images have been chosen for the study."}, {"title": "Model and Dataset", "content": "Phi-3-vision-instruct. It is an open-sourced model by Microsoft with a parameter size of 4.2 Billion [1]. It is a\nmultimodal model that can process image and textual prompts, and generate textual outputs. This model has two primary\ncomponents, an image encoder and a transformer decoder [1, page 9]. The image encoder used in the Phi-3 Vision\nmodel is CLIP ViT-L/14 [18]. It is responsible for processing the visual information from the input image. The visual\ntokens extracted by the image encoder are combined with the text tokens in an interleaved manner, without any specific\norder. The transformer decoder is Phi-3-mini-128k-instruct [1]. It is responsible for processing the combined visual and\ntextual tokens to generate textual outputs based on the input image and text prompts.\n\nThe dataset used for this experiment is vidore/syntheticDocQA_government_reports_test_captioning from HuggingFace\n[6]. It has around 1900 rows of images, prompts and associated metadata. Out of all the available images, a subset of\nimages is chosen based on Clarity, Sharpness and Resolution and they are combined into a new dataset specifically\nfor this experiment. The chosen images are handpicked, ensuring all of them are clean and not blurred, and have\nwell-defined features. The selected images are paired with a specific prompt that targets a particular information from\nthe document image that we intend to extract."}, {"title": "Information Extraction", "content": "The model and processor are initialized in an extraction function that takes in 2 arguments, image and prompt. All the\nimage processing and tokenization are handled by the processor, followed by the answer generation by the model itself.\nThe extraction process runs in a loop, iterating over all the selected images, feeding into the inference code along with\nthe prompt, and generating results for each image one after another. The input prompt is the aggregation of\nthe user query and the system prompt, <image_1>. This ensures that the prompt is in the correct format when fed into\nthe information extraction function."}, {"title": "Experiments", "content": "The experiment is primarily performed on the document images from the dataset, consisting of tabular pages and reports\nwith infographics and charts. Since there are multiple entries with duplicate images, only a handful of them have\nbeen chosen for this experiment. The images are fed to the network with a user query and the system prompt, and the\noutput is evaluated manually to rate its relevance. The model expects a system prompt [1], \"<image_1>\" as the image\nplaceholder and following that an input query.\n\nGeneral prompting. Prompting is a method of guiding a large pre-trained model to tackle new tasks by providing\nspecific instructions and task-specific hints [9]. For tasks like targeted question answering or data extraction from\ndocuments, the prompt needs to be tailored or adjusted to the specific section of the document from which the information\nis needed [4].\n\nWhen prompts are generalized, the resulting answers encompass the overall meaning of the image. If the aim is to\ngrasp the general idea of a document or image, a prompt derived from a description query is adequate for the model to\nprovide the image document's context [13]. The model generates an answer that captures the whole meaning of the\nimage document, yielding a more comprehensive response that covers every aspect of the image document.\n\nWhen the model is asked to provide a detailed description of an input image, the generated response contains a general\noverview of the image or document, spanning from top to bottom [1]. However, When observed properly, it becomes\nclear that the model's response lacks specific details, indicating a difference between the generated content and the\nactual image document content. The information generated by the model is partially accurate, capturing only a limited\namount of context from the original document. This pattern is observed across the majority of the documents utilized in\nthe experiment.\n\nTarget Prompting. The problem of extracting targeted information from the image document using the model can\nbe effectively achieved using target prompting. It has been observed that by explicitly instructing the model [15] to\ndescribe a particular portion of the document image, it performs well compared to when it was asked a more general\nquestion [2]. This enables the model to perform tasks without covering everything from the input image object. By\nisolating regions of interest based on specific needs, it is possible to obtain precise answers more efficiently."}, {"title": "Results", "content": "Focusing on a specific portion of an image and trying to extract information from that region alone can greatly improve\nthe model performance and the final output accuracy. The target prompting technique has greatly improved the\ninformation extraction process by retrieving content from only those portions of the document images which has been\npointed to in the prompt. This experiment is performed in an instance with 16 GB RAM and a Tesla T4 GPU.\n\nThe targeted prompts mostly include questions that are specific to a particular region or from a definite portion of the\ndocument image.  This ensures that the generated output is free from any unwanted information or noise.\nThe response is relevant to the question asked and only generates results related to the input query prompt. It enables a\nmore rigid control over the type of responses to be generated from the documents. The same image can be fed multiple\ntimes to the extraction function to extract the desired information only, avoiding any irrelevant content and unwanted\nnoise in the final answer. This has the potential to reduce the post-generation processes as the majority of the documents\nwhen processed with this technique will have only answers relevant to what is being asked in the prompt."}, {"title": "Conclusion", "content": "The paper proposed a specialized method to direct the vision language model, Phi-3-vision-instruct, in generating specific\nresponses related to the user input query. Although the results are not completely accurate, it performs significantly\nbetter when the task is to cover a certain portion of an image or document. There are more potential routes for improving\nthe model responses and enhancing the accuracy of the proposed prompting technique, but that will require more testing\nand evaluation using a variety of datasets. In future work, the plan is to use an extended version of the dataset and\nconduct experiments to evaluate the performance and accuracy of the method on more complex documents."}]}