{"title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge Al Framework for Personal LLMs Fine-Tuning", "authors": ["Bei Ouyang", "Shengyuan Ye", "Liekang Zeng", "Tianyi Qian", "Jingyi Li", "Xu Chen"], "abstract": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. Other studies focus on exploiting the potential of edge devices through resource management optimization, yet are ultimately bottlenecked by the resource wall of individual devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge Al framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone, enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64\u00d7 end-to-end speedup and up to 88.16% reduction in memory footprint.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) [13, 20, 22] have ushered in a revolution in machine intelligence, owing to their exceptional capabilities in a wide range of machine learning tasks. While born on datacenter warehouse, LLMs have quickly sunk to edge devices and facilitated a range of intelligent applications at the network edge, such as intelligent personal assistants (IPAs) which are software agents that can augment individuals' abilities, complete complicated tasks, and even satisfy emotional needs. A recent survey [14] targeting LLM-based IPAs has revealed that over 80% of industry experts believe that, owing to the sensitive and privacy-critical nature of user data, personal LLMs should be fully (or primarily) hosted at the edge in order to enable privacy-preserving model personalization and serving. Figure 1 illustrates the scenario of hosting a personal LLM-based intelligent agent within a smart home. A personal LLM agent provides users with high-performance, privacy-preserving intelligent services. Meanwhile, the agent also tracks user interactions, learns from experiences, and extracts knowledge to fine-tune the personal LLMs and further enhance the service quality.\nWhile the serving of LLMs on edge devices has been made feasible through careful engineering [6, 26, 28], fine-tuning these models remains significantly challenging due to the resource-intensive nature of LLM training. Towards alleviating the resource challenges, some research works [4, 17] have explored parameter-efficient fine-tuning (PEFT) techniques, such as Adapters [9] and LoRA [10],"}, {"title": "2 MOTIVATION AND PRELIMINARIES", "content": "2.1 Transformer-Based LLMs and Fine-Tuning\nTransformer-Based LLMs. Transformer-based LLMs have gained prominence in various language-related applications due to their impressive performance. These models consist of multiple Transformer layers, each comprising two main components: the Multi-head Attention and the Feed Forward block. The Multi-head Attention block utilizes linear layers to generate query (Q), key (K), and value (V) matrices for each attention head, allowing for independent self-attention computations. The outputs of these attention heads are then concatenated and processed through a final linear layer. The Feed Forward block involves two linear operations that increase the hidden size from h to 4h and then reduce it back to h.\nPersonal LLMs Fine-Tuning. The training of LLMs typically consists of two stages: pre-training and fine-tuning. Before being deployed for specific tasks, language models are often pre-trained"}, {"title": "3 SYSTEM OVERVIEW", "content": "PAC is a time, memory and parameter efficient collaborative framework for personal LLMs fine-tuning across multiple edge devices. PAC first equips the target LLM with our Parallel Adapters module (Step). PAC profiler fine-tunes the LLM using a calibration dataset on edge devices to record the runtime profile required for parallelism planning (Step 1). PAC planner then takes the profiling results as input and generates planning configurations, including LLM partitioning points and device grouping strategies (Step 2)."}, {"title": "4 TIME, MEMORY AND PARAMETER\nEFFICIENT FINE-TUNING ALGORITHM", "content": "4.1 Fine-Tuning LLMs with Parallel Adapters\nObservation and Key Insight. As discussed in \u00a72, while techniques such as LoRA [10] and Adapters [9] reduce the number of parameters that need to be updated during fine-tuning, they do not significantly reduce the computational and memory requirements during the training on edge devices. This is because the parameters being updated are still inside the LLM backbone. To calculate the gradients for backpropagation, the full backward passes through the entire pre-trained model are still necessary, as illustrated in Figure 5(a) and (b). In the research field of AI, side-tuning [34] is a specialized fine-tuning technique. It adds a trainable side network that runs in parallel to the backbone model, with the side network's representation summed with the backbone's output in the final layer. Crucially, side-tuning only updates the side network, without backpropagating through the backbone model.\nParallel Adapters Architecture. In light of side-tuning, we employ a time and memory efficient personal LLMs fine-tuning technique with Parallel Adapters. The overall structure is illustrated in Figure 5(c). Specifically, we decouple conventional Adapters [9] from the LLM backbone, avoiding their integration at the end of each transformer layer. Instead, we provide a dedicated parallel highway for our trainable adapters network, which takes intermediate activations from the backbone transformer as input and generates the final predictions. In this way, backpropagation through"}, {"title": "4.2 PAC Activation Cache for Parallel Adapters", "content": "Observation and Opportunities. Leveraging Parallel Adapters substantially diminishes the computational and memory demands by circumventing backward propagation through the LLM backbone. However, for edge environments with limited resources, forward propagation calculations on the backbone of LLMs also require substantial computational resources. Figure 3 demonstrates that the computational overhead for forward propagation constitutes 54% and 56% of the total overhead when fine-tuning the T5-Large with Adapters and LoRA, respectively.\nTo minimize the computational demand, we identify two distinct opportunities for utilizing Parallel Adapters in in-situ finetuning of LLMs: (1) During the pre-training phase of LLMs, due to"}, {"title": "5 COLLABORATIVE EDGE AI SYSTEM FOR\nEFFICIENT PERSONAL LLMS FINE-TUNING", "content": "In PAC, we leverage edge devices in physical proximity and associate them as a resource pool to boost in-situ fine-tuning. Specifically, the fine-tuning procedure comprises two phases: (1) In the initial epoch, the backbone of LLMs, enhanced with Parallel Adapters, undergoes fine-tuning across multiple edge devices through a blend of data and pipeline parallelism (\u00a75.1); (2) In subsequent epochs, the activation cache eliminates the necessity for forward propagation within the backbone, thereby enabling the exclusive fine-tuning of our Parallel Adapters utilizing data parallelism (\u00a75.2)."}, {"title": "5.1 Resource-Efficient Collaborative\nOrchestration for LLMs Fine-Tuning", "content": "Observation of Data and Pipeline Parallelism at the Edge.\nWhen collaborating on LLM fine-tuning among edge devices, the principle question is which type of parallelism should be used. The most common way to train models in parallel is data parallelism (DP) [8]. However, DP necessitates that each device maintains a replica of the entire model, a requirement difficult to meet for LLMs with extensive parameter sizes, often surpassing the capacity of a single device. Pipeline parallelism (PP) [30] is further proposed to address this problem. In PP, the model is partitioned into multiple consecutive stages and each stage is mapped to a separate device.\nConsequently, PP enables the training of increasingly large models by deploying more devices. Nonetheless, PP encounters scalability constraints as the addition of edge devices results in more stages. This not only results in a significant presence of pipeline bubbles but also amplifies the impact of inter-stage communication latency, thereby hindering efficiency. The above observation motivates us to employ a hybrid parallelism (HP) architecture that incorporates the best of both DP and PP, so as to achieve superior performance and scalability in resource-constrained edge environments.\nHybrid Parallelism Architecture in PAC. As illustrated in Figure 6(a), PAC first divides an LLM into multiple stages where each contains a stage model composed of a set of consecutive transformer layer. Edge devices are allocated into several device groups, each comprising one or more devices. PAC maps each stage to a group, with the stage model replicated across all devices within that group. Throughout the fine-tuning process, a mini-batch is divided into several micro-batches for concurrent processing to enhance parallelism. If a device cluster hosts multiple devices, micro-batches are further subdivided. Each device is responsible for executing the forward (FP) and backward passes (BP) for its assigned stage model and aggregates gradients across all micro-batches for every mini-batch. Upon completing a mini-batch, gradient synchronization within each device group is achieved through AllReduce. Since the majority of parameters in LLMs are frozen, AllReduce synchronizes only the lightweight parallel adapters, ensuring a swift process. We adopt the one-forward-one-backward (1F1B) micro-batch scheduling [18] which schedules the BP early to release the activation memory produced by FP for reuse. Figure 6(b) depicts a well-structured hybrid parallelism, encompassing FP, BP, and inter-stage communication. Profiling. To enable parallelism planning, PAC profiler first finetunes the target LLM using calibration datasets to record the runtime profile required for planning. We define $t_{d,l}^{tal}(\\beta)$ and $t_{d,l}^{tail}(\\beta)$ as the FP and BP execution times for layer l on device d with batch size of \\beta, respectively. $u_d$ denotes the memory budget of device d. The size of output activations, input gradients, and weight parameters in bytes will also be collected to calculate memory footprint.\nPlanning Algorithm for Hybrid Parallelism. The global throughput of a pipeline is determined by the execution time of the"}, {"title": "5.2 Cache-Enabled Collaborative Edge\nFine-Tuning of Parallel Adapters", "content": "Data-Parallel Fine-Tuning for Parallel Adapters The computationally lightweight nature of the Parallel Adapters precludes the use of pipeline parallelism to fine-tuning with activation cache, as it would result in unoverlapable inter-stage communication latency. Therefore, we employ data parallelism to exclusively fine-tune our Parallel Adapters. Specifically, after the first training epoch, the activation cache for all samples is already collected. We then perform collective communication to redistribute the Parallel Adapters parameters and locally cached activations across all devices, ensuring each device receives the complete set of adapter parameters and corresponding activations. The devices then utilize this shared information to fine-tune the parallel adapters in a data-parallel manner. In our experiments, fine-tuning the BART-Large model on the MRPC dataset for three epochs, the redistribution of parameters and activations only contributed to approximately 8% of the total training time. Notably, the overhead of this process can be further amortized over additional training epochs. An instance of personal LLMs fine-tuning with activation cache is depicted in Figure 7.\nStorage Cost Analysis. Employing activation caching can reduce the computational requirements of forward propagation; however, it incurs additional storage overhead for activations. Specifically, the storage overhead is s \u00d7 h \u00d7 l per sequence, where s denotes the sequence length, h represents the transformer's internal feature dimension, and l corresponds to the number of transformer layers. For T5-Base model, the activation caching requires less than 1 GB to store the activations for 500 training samples with sequence length of 30. Such cost is no more than 1% of the storage of a modern mobile device, e.g., hundreds of GB. During fine-tuning, the activation cache is reloaded from disk per micro-batch, a process that takes no more than tens of milliseconds on embedded flash storage. The cache will be cleared once the fine-tuning process finishes."}, {"title": "6 EVALUATION", "content": "6.1 Implementation and Setups\nImplementation of PAC. We have fully implemented the prototype framework of PAC and baselines with ~2,000 LoC in Python atop Pytorch [2]. PAC's idea is also portable and can work well with other lightweight ML frameworks such as MNN [11] and TF-Lite [3]. Our Parallel Adapters is a lightweight version of the backbone model. The size of Parallel Adapters is determined by the reduction factor k. All weights and hidden state dimensions of the Parallel Adapters are$\\frac{1}{k}$ times the corresponding weights and hidden states of the backbone model. In our experiments, the reduction factor k is set to 8. The weights of the Parallel Adapters are initialized based on structural pruning, using the weights of the backbone model. We insert Parallel Adapters at the end of each transformer layer."}, {"title": "6.2 End-to-end Performance", "content": "Table 2 and Table 3 summarize the end-to-end performance comparisons between PAC, the single-device method, and state-of-the-art collaborative edge training methods. To ensure fair comparisons, these baseline methods are enhanced with prevalent PEFT techniques, including Adapters and LoRA. Fine-tuning the smaller datasets, MRPC and STS-B, is conducted over three epochs, with the latter two epochs benefiting from the PAC activation cache. Conversely, for larger datasets such as STS-2 and QNLI, a single epoch of fine-tuning is sufficient to achieve satisfactory performance."}, {"title": "6.3 Significance of Parallel Adapters at the Edge", "content": "We conducted experiments to assess the time and memory efficiency of Parallel Adapters at the edge. In this section, we perform data parallelism for Parallel Adapters with activation cache across 8 devices and hybrid parallelism for other fine-tuning techniques without 1F1B micro-batch scheduling. \"Activations\" contain the intermediate results and optimizer states. Figure 8 illustrates that Parallel Adapters outperform other fine-tuning techniques regarding both time and memory efficiency.\nParallel Adapters markedly reduce per-sample training time. Figure 8(a) presents the average sample training time across different fine-tuning techniques. Without activation cache, Parallel Adapters can reduce the average sample training time by 31.94% to 56.24% compared to baseline methods, primarily owing to a substantial decrease in backward propagation overhead. Both Adapters and LoRA incorporate trainable structures into the backbone model, thus necessitating backpropagation across the entire backbone model for gradient computation of these parameters. Consequently, regarding backward time, Adapters and LoRA can only achieve approximately a 49% reduction compared to full"}, {"title": "6.4 Analysis of Collaborative Edge Fine-Tuning", "content": "We perform an ablation study to understand the contribution of hybrid parallelism and activation cache in our system design.\nComparasion PAC with EDDL and Eco-FL. To explore the scalability advantages of PAC's hybrid parallelism over Eco-FL's pipeline parallelism and EDDL's data parallelism, we compared the throughput of these methods when training collaboratively across 2 to 8 edge devices. The batch size was consistent with the number of devices, and the sequence length of each sample was fixed at 128. We implement Eco-FL and EEDL using the Parallel Adapters technique to ensure a fair comparison. Note that none of the three methods utilizes activation cache.\nFigure 9(b) illustrates the maximum per device memory footprint of model weights across edge cluster. For EDDL, each device must host a complete LLM, preventing the reduction of the parameters' memory footprint through scaling up the number of devices. Therefore, as shown in Figure 9(a), the EDDL method exhibits OOM errors with both the BART-Large and T5-Large models. Conversely, PAC and Eco-FL utilize pipeline parallelism, partitioning the model into multiple stages with each handled by different devices. This approach allows for scaling the number of devices to reduce the peak memory footprint. PAC's hybrid parallelism offers a broader search space for parallel strategies compared to Eco-FL's pipeline parallelism. Our planning algorithm for PAC is capable of identifying more efficient hybrid parallel configurations within memory constraints, enhancing resource utilization. Although PAC may incur higher memory overhead in some instances, it achieves greater system throughput. Specifically, when compared to Eco-FL, PAC exhibits an increase in throughput from 39.50% to 84.79%."}, {"title": "7 RELATED WORK", "content": "Parameter-Efficient Fine-Tuning for LLM. Prompt tuning [12] proposes to prepend the model input embeddings with a trainable tensor. Adapters tuning [9] adds domain-specific layers after attention and FFN layers in transformer. LoRA [10] decomposes the parameter update for a weight matrix into two trainable low-rank matrices. To further reduce the memory overhead, pioneering studies explore fine-tuning techniques that obviate the need for"}, {"title": "8 CONCLUSION", "content": "This paper proposes PAC, a time and memory efficient collaborative edge Al framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design, achieving a acceleration of 8.64\u00d7 and 88.16% memory reduction compared to state-of-the-art methods."}, {"title": "Where, W(0\u2192y, Dn, s) =", "content": "minmin max{W(0\u2192 q, Dn-m, s - 1),\n0"}, {"title": "T(x\u2192y, Dn) =", "content": "d,l \n{\n+\u221e, if \u2203d\u2208 Dnmd > ud,\nmaxn\nde Dn x\nelse,"}, {"title": "L =", "content": "\u2211[ec(i) + c(i)] + \u2211[ef(i) + c(i)],\ni=1\n L =\nM\u22c5 (e(s) + e(s)),\n i\u2208 {1...,s}"}, {"title": "L =", "content": "max (AR (1) + \u2211(ef(j) + c(j),\n\ni=1\n L = min (L + L + L)."}]}