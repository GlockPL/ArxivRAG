{"title": "Brain-Model Evaluations Need the NeuroAI Turing Test", "authors": ["Jenelle Feather", "Meenakshi Khosla", "N. Apurva Ratan Murty", "Aran Nayebi"], "abstract": "What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the \"NeuroAI Turing Test,\" a benchmark that extends beyond behavior alone and additionally requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.", "sections": [{"title": "1. Introduction", "content": "Humanity is in the midst of an intense pursuit to understand and replicate intelligence in artificial systems. But the AI community (computer scientists scaling up AI algorithms) and the NeuroAI community (computational cognitive neuroscientists leveraging AI systems to build models of the brain) seem to be approaching the challenge of intelligence from fundamentally divergent perspectives. Al researchers have primarily focused on developing systems that exhibit intelligent behavior, a tradition rooted in Alan Turing's seminal idea of the Turing Test (Turing, 1950). The focus on achieving behavioral similarity to humans has undoubtedly propelled AI forward, but the goal is rarely to study and understand how the internal representations of AI systems generate behavior. In contrast, NeuroAI aims to model the computational principles underlying intelligence in biological systems, using the brain\u2014the only example of intelligence we universally recognize and agree upon as a reference point. Evaluations of NeuroAI models often in- Preprin"}, {"title": "2. Why do we need a distinct test for NeuroAI?", "content": "We think that the NeuroAI Turing Test is imperative for overcoming fundamental limitations inherent in both the classical Turing Test and the prevailing practices within NeuroAI.\nThe traditional Turing Test and recent proposed extensions (Zador et al., 2023), focus extensively on behavioral indistinguishability between biological organisms and machines i.e. the outputs. However, testing the output behavior alone is insufficient because many possible internal processes can produce identical behaviors through entirely different computational mechanisms (see Fig. 1 top).\nNeuroAI on the other hand, places importance on faithfully modeling both the human brain and behavior (cognition).\nBut even within the NeuroAI community, multiple definitions exist for what constitutes a \u201cgood\u201d model. In addition, the methodologies for comparing models with brains are fragmented, and rely on evaluation measures (metrics) that offer only relative comparisons between models without establishing any specific consideration for a definitive standard of success (more on this to follow).\nThe NeuroAI Turing Test directly addresses these shortcomings by establishing rigorous criteria for replicating behavior and internal mechanisms (biological brains) benchmarked against the variability observed between individual humans (or animals, in the case of many neural recordings). To assess internal similarity, we focus on measures of representational convergence which capture similarity at Marr's \u201calgorithmic\u201d level, in contrast to the original Turing Test which evaluates similarity at the \u201ccomputational\u201d level (Marr, 2010). Note however that the NeuroAI Turing Test framework is also sufficiently general to also be applied to models that attempt to capture \u201cimplementation\u201d level constraints.\nTogether, the NeuroAI Turing Test sets a higher bar for scientific rigor and empirical validation for brain models. Our goal is to empower the AI and Neuroscience communities with the tools and methods to identify superficial imitations and to strive for neuroscientific accuracy with brains. We think this shift is crucial for steering the co-evolution of AI and neuroscience toward new insights and the development of truly robust, brain-inspired systems. Without a rigorous standard, AI risks perpetuating models that are impressive in performance yet fundamentally disconnected from the biological intelligence they seek to emulate. The NeuroAI Turing Test is a necessary evolution.\nMotivation for standardization. Often, measures of brain-brain similarity are relegated to the supplement of papers. These numbers are typically below the noise ceiling obtained from data splits within the same participants, and the amount of variance yet to explain relative to the noise-ceiling is emphasized as progress to be made with future models. There is also a lack of agreement about what value on a particular dataset and metric would suggest that the similarity measure has been saturated, reflected by a diverse set of ceiling measurements in common benchmarks (Schrimpf et al., 2018; Ratan Murty et al., 2021; Conwell et al., 2022). Our NeuroAI Turing test provides practical guidance for future reporting of benchmarks: in addition to noise correcting for variability due to internal stochasticity and measurement noise, we must also report brain-brain similarity values (Fig. 2). And when we do, we find on \"classic\" standard benchmarks (such as for primate object recognition), that current models are closer to saturating them than we thought before (Fig. 3), suggesting a greater need for new benchmarks, rather than pushing further on"}, {"title": "3. Defining the NeuroAI Turing Test", "content": "In the standard Turing Test, Alan Turing reframed the question \"Can machines think?\" into \u201cCan machines produce behavior indistinguishable from a human?\" Similarly, in the NeuroAI Turing Test, we extend this idea by requiring machines not only to produce behavior that aligns with human capabilities but also to generate internal representations that are indistinguishable from those recorded from human (or animal) brains.\nLet $D \\in R^{C \\times T \\times N}$ where $C$ = Setup. (#conditions), $T$ = (# timepoints), $N$ = (# outputs), be a dataset of neural (e.g., neuron spike counts or fMRI voxel responses) or behavioral responses from a set of organisms (animals or human subjects) $O(D)$. Let ${X_i}_{i \\in O(D)}$ be the measurements from those organisms. In this framework, the time dimension is optional, as one can examine time averages, but at minimum conditions and outputs must be obtained from the brain or behavior. Let $X_m$ be the corresponding measurements from a model (e.g. unit activations or behavioral output from a neural network) under the same conditions. Note that the model can either be embodied (Zador et al., 2023; Pak et al., 2023) or not, that is a flexible choice. Let $M : R^{C \\times T \\times N} \\rightarrow R$ be a metric on the space of these representations.\nDistances. Define the inter-organism distance set:\n$\\Delta_{organism} = {M(X_i, X_j) : i, j \\in O(D), i \\neq j}$.\nNext, define the model-organism distance set:\n$\\Delta_{model} = {M(X_m, X_i) : i \\in O(D)}$.\nHypothesis Testing. Next, choose a significance level $\\alpha \\in (0, 1)$ of convergence. Select a distribution-free two-sample test $T$ (e.g., rank-sum, permutation, or KS) to compare $\\Delta_{organism}$ and $\\Delta_{model}$. Let\n$H_0$: $\\Delta_{model}$ and $\\Delta_{organism}$ come from the same distribution,\n$H_1$: $\\Delta_{model}$ and $\\Delta_{organism}$ differ systematically\n(e.g., model-organism distances are larger).\nDefinition 3.1 (Convergence in Distribution.). We say the model's representation converges in distribution to that of the organisms in O(D) (at level a) if:\nUnder the chosen test T, we fail to reject $H_0$ at the $\\alpha$ level (i.e., $p > \\alpha$),\nA chosen statistic (e.g., difference in means or medians) indicates that model-organism distances are not systematically larger (or otherwise different) than the inter-organism distances.\nNote that our definition of the NeuroAI Turing test is applicable to any measure of similarity used to compare a computational model with a biological system. The choice of this measure M is determined by the user, but we highlight some considerations here(for more discussion see Appendix \u00a7D, Table 2). Common choices of mapping function and metric include linear regression (ridge, PLS), and"}, {"title": "4. Current State of the NeuroAI Turing Test", "content": "With the formalized NeuroAI Turing test, we detail how current work on behavioral and representational convergence fits into this framing.\nBehavioral alignment alone is not sufficient for representational convergence between models and brains. Let's consider a model that passes the behavioral Turing Test. Would this model also pass a representational level test for brains? Some researchers have argued that several high-capacity neural networks are converging to universal \"platonic representations\u201d (Huh et al., 2024). This idea is closely related to notions of efficient coding (Barlow, 1961; Simoncelli & Olshausen, 2001) in neuroscience. But note converging to the optimal code still critically depends on strong implementational constraints. And the constraints in biology are quite different than those in artificial systems. Some work has suggested that including biological constraints in the architecture or training environment of models leads to better representational alignment (Dapello et al., 2020;\n2021; Deb et al., 2025). It remains an open question whether (and which) of these biological constraints critically shape the representation, such that a high-capacity system without such constraints would yield a different optimal solution. Thus, we argue that testing behavior alone is not sufficient, and we need models that also achieve representational convergence with the brain.\nBehavioral alignment is necessary for a complete model of the brain to pass the NeuroAI Turing test. Is passing a behavioral test necessary for passing our NeuroAI Turing test? If the goal is to model the entire organism, passing a behavioral test is necessary. Take a simple deterministic example where $f (x)$ is an internal representation and $g(x)$ outputs behavior: if $g(f(x)) \\neq g'(f'(x))$ then we know that $g' \\neq g$ and/or $f' \\neq f$. Models in recent years have made immense progress at solving complex behavioral tasks, including recent work showing increasing convergence in domains that previously had large gaps (Geirhos et al., 2021) (Fig. 4A shows one metric from this work). However, even in well-studied domains such as vision, these models are \"not yet adequate computational models of human core object recognition behavior\u201d (Wichmann & Geirhos, 2023). There are many instances of models failing to reach human-like performance on behavioral measures of brain-model alignment (Feather et al., 2019; 2023; Hermann et al., 2020; Baker & Elder, 2022; Bowers et al., 2023). The gap is especially pronounced for complex dynamic stimuli, such as future prediction (Nayebi et al., 2023, Fig 2A) (key result reproduced in Fig. 4B).\nWe emphasize that these proposed behavioral measures (and non-correlational controlled experiments) are compatible with our NeuroAI Turing Test, as they amount to a choice of metric M. However, our test goes beyond merely specifying a metric-it establishes a well-defined inter-subject behavioral consistency ceiling against which models can be evaluated. Without such a measure, one risks drawing incorrect conclusions, such as the claim that model-IT predictivity is disproportionately driven by background-processing (Malhotra & Bowers, 2024). This misunderstanding arises from conflating \"core object recognition\u201d with category-only processing, despite well-established evidence that IT itself encodes category-orthogonal features (Hong et al., 2016). It is therefore critical to consider what is already accounted for in the neural data relative to models; failing to do so in this case overlooks background-related processing in IT, leading to misleading model comparisons.\nFurther, models achieving \u201csuper-human\u201d performance on tasks are generally not considered brain-like, as their behavior could easily be distinguished from humans by looking at these abilities (for instance, in Zhang et al. (2022) the authors state that \u201cadvanced scientific topics\" can serve as a way to discern human and AI agents, as the AI does not find"}, {"title": "5. Alternate Views", "content": "In this section, we discuss alternate views to our proposed NeuroAI Turing Test. We group the main arguments against it by subsection:\n5.1. Alternate Notions of \"Brain-Likeness\"\nThe past century of neuroscience has had many definitions of what a brain \u201cdoes\u201d, thereby providing an implicit definition for what might constitute a \u201cbrain-like\" model (cf. Table 1 and Appendix \u00a7B). For example, the most notable of these include classic ideas of predictive coding (Rao & Ballard, 1999), sparse coding (Olshausen & Field, 1996), energy efficiency (Laughlin, 2001), and redundancy reduction (Barlow, 1961).\nWhile these notions are valuable, they do not inherently"}]}