{"title": "Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins", "authors": ["Venkatesh Pattabiraman", "Yifeng Cao", "Siddhant Haldar", "Lerrel Pinto", "Raunaq Bhirangi"], "abstract": "While visuomotor policy learning has advanced robotic manipulation, precisely executing contact-rich tasks remains challenging due to the limitations of vision in reasoning about physical interactions. To address this, recent work has sought to integrate tactile sensing into policy learning. However, many existing approaches rely on optical tactile sensors that are either restricted to recognition tasks or require complex dimensionality reduction steps for policy learning. In this work, we explore learning policies with magnetic skin sensors, which are inherently low-dimensional, highly sensitive, and inexpensive to integrate with robotic platforms. To leverage these sensors effectively, we present the Visuo-Skin (VISK) framework, a simple approach that uses a transformer-based policy and treats skin sensor data as additional tokens alongside visual information. Evaluated on four complex real-world tasks involving credit card swiping, plug insertion, USB insertion, and bookshelf retrieval, VISK significantly outperforms both vision-only and optical tactile sensing based policies. Further analysis reveals that combining tactile and visual modalities enhances policy performance and spatial generalization, achieving an average improvement of 27.5% across tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans effortlessly perform precise manipulation tasks in their everyday lives, such as plugging in charger cords, or swiping credit cards \u2013 activities that demand exact alignment and involve constrained motion. These tasks are so commonplace that we often overlook the complexity involved in executing them with the necessary accuracy. In contrast, much of the existing robot learning literature remains focused on simple, low-precision primitives such as pick-and-place, slide, push-pull, and lift that does not require such fine-grained spatial accuracy. As we strive to create robots capable of everyday tasks like handling cables and opening jars, it is crucial to develop frameworks that enable precise, contact-rich manipulation.\nWhile the role of tactile feedback for robust execution of precise skills in humans is widely acknowledged [1, 2], analogous capabilities in robotic policies have lagged behind their vision-based counterparts. A variety of tactile sensors have been developed to bridge this gap in robotics, with optical tactile sensors like Gelsight [3] and DIGIT [4] becoming popular choices in robot learning due to their high resolution. This increased resolution has facilitated several impressive works in areas like 3D reconstruction and localization [5, 6] and object recognition [7, 8]. However, the high dimensionality of tactile data from such sensors introduces additional complexity to the already challenging problem of policy learning. In most cases, the use of optical sensors necessitates dimensionality reduction through rep-resentation learning [4], explicit state estimation [9, 10] or discretization [11, 12] to make it amenable to policy learning. This observation prompts an investigation into using alter-native tactile sensing modalities that naturally offer lower-dimensional representations while still effectively capturing the essential characteristics of physical contact.\nIn this work, we present Visuo-Skin (VISK), a simple framework for training precise robot policies using skin-based tactile sensing. VISK uses a simple visuotactile policy architecture that incorporates tactile signals from AnySkin [13], an affordable magnetic tactile sensor demon-strated to provide spatially continuous, low-dimensional (15-dimensional) sensing while being replaceable, making it well-suited for policy learning applications. The VISK policy builds upon the BAKU [14] architecture, which enables"}, {"title": "II. RELATED WORK", "content": "Most robotic tasks involve physical interaction with the environment. Tactile sensing is critical in its ability to enable robots to reason about the physics of contact directly at the point of contact. Over the years, a number of diverse transduction mechanisms have been explored for tactile sensing. Resistive tactile sensors [15, 16, 17] are inexpensive and relatively easy to fabricate, and provide discrete sensing making them well-suited for a range of applications that involve sensing the presence or absence of contact. Capacitive tactile sensors [18, 19] tend to provide more fine-grained measurements compared to resistive sensors and include proximity sensing in addtion to tactile sensing. Another versatile category of sensors are MEMS-based sensors [20] that often combine multiple sensors such as audio and IMU sensors and can offer multimodal feedback in addition to higher resolution and mm-scale form factor.\nRecently, optical tactile sensors like Gelsight [3] and DIGIT [4] have emerged as a popular, high resolution alter-native to existing tactile sensors for robotics due to a number of desirable properties such as their ease of replaceability and compatibility with well-understood neural architectures like convolutional neural networks [7]. Similarly, magnetic tactile sensors like Xela [21] and ReSkin [22] have garnered significant interest due to their scalable form factor, low dimensionality and ability to sense shear force in addition to their consistency across sensor instances [22, 23]. In light of these characteristics, the VISK framework presented in this work uses AnySkin [13] a magnetic tactile sensor that strikes the right balance between low dimensionality and continuous contact sensing. Furthermore, its superior cross-instance signal consistency makes it more amenable than optical sensors to policy learning without the need for complex additional fabrication to prevent wear and tear [12]."}, {"title": "B. Visuotactile learning", "content": "The meteoric rise of deep learning has paralleled recent developments in rapid prototyping and additive manufac-turing. As a result, a number of recent works have inves-tigated the use of machine learning for a host of tactile prediction tasks such as slip detection [24, 25], material classification [26, 27], object identification [28, 29] and 3D reconstruction [30, 31] across a range of tactile sensors.\nIn this paper, we specifically focus on policy learning incorporating tactile information into robotic policies to enhance contact-rich manipulation.\nRecent works have demonstrated impressive improve-ments from incorporating tactile data into the policy learn-ing framework for precise dexterity [32, 33] and bimanual manipulation [34]. However, the high dimensional nature of dexterous control limits the task complexity and extent of generalizability enabled by these works. While [11, 35] use sim2real learning to demonstrate significant generalizability across objects for an in-hand rotation task, the task lacks precision, and sim2real transfer necessitates significant di-lution of the tactile input to only capture coarse, discrete information. This limits the scalability of this approach to the precise, contact-rich tasks considered in this work.\nYet other works rely on explicit pose estimation [36] and handcrafted feature extraction [9, 10] from optical tactile data for alignment when performing insertion tasks. While interesting, these techniques do not generalize to arbitrary tasks and require significant effort and domain knowledge to adapt to every new task. While some existing works have learned visuotactile policies for precise tasks such as insertion [37, 38], all of these works evaluate performance in restricted settings with little to no spatial variation in the location of the insertion slot. In this paper, we investigate visuotactile policy learning for contact-rich, high-precision tasks requiring spatial generalization, and conclusively show that VISK policies use tactile feedback in conjunction with vision to substantially improve task performance."}, {"title": "III. VISUO-SKIN POLICY LEARNING (VISK)", "content": "Two key considerations in designing a framework for visuotactile policy learning include the choice of a tactile sensor capable of providing reliable tactile data across di-verse environments and tasks, and designing a neural archi-tecture able to effectively leverage multimodal visual and tactile information. Our proposed approach, VISK, addresses these in the following ways: first, it employs AnySkin [13], a magnetic tactile skin shown to yield consistent tactile measurements reliably under various conditions. Second, it builds upon state-of-the-art approaches to visual policy learning [14] by incorporating a tactile encoding stream, allowing the network to effectively learn from multimodal data. Below, we describe each component of VISK in detail."}, {"title": "A. Data Collection", "content": "We use a VR-based teleoperation framework [40] employ-ing the Meta Quest 3 headset to collect data for our real-world xArm robot experiments. Visual data from 4 camera views, including an egocentric camera attached to the robot gripper, is recorded at 30 Hz. Tactile data for the AnySkin experiments is recorded as magnetometer signals at 100 Hz, while data from the DIGIT sensors in comparative tests are recorded at 30 Hz, identical to the cameras. Drawing from prior work demonstrating the benefits of adding noise to demonstrations for policy learning [41, 42], we add a uni-formly sampled angular perturbation to the direction of the commanded robot velocity during teleoperation. This proves especially useful for increasing the diversity of contact-rich signals in the dataset by rendering the tasks slightly more challenging for the human operator. While large perturba-tions risk steering the learned behavior cloning policy astray, we find that injecting a minor directional noise yields an information-rich tactile signal while maintaining consistent task success."}, {"title": "B. Policy Architecture", "content": "The VISK policy builds on top of BAKU [14], a state-of-the-art transformer-based policy learning architecture that learns visual policies across multiple camera views. Similar to BAKU, our architecture contains three main components:\na) Sensory Encoders: Visual inputs from cameras are encoded using a modified ResNet-18 [39] visual encoder. Low-dimensional tactile inputs from the AnySkin sensor are encoded with a two-layer multilayer perceptron (MLP). Drawing from [22], we subtract a baseline measurement from each tactile reading to account for sensor drift. The encoded representations for each modality are projected to the same dimensionality to facilitate combining modalities in the observation trunk. Some of the ablations and comparisons"}, {"title": "IV. EXPERIMENTS", "content": "We study the effectiveness of the VISK framework in a policy learning setting using behavior cloning. Our experi-ments are designed to answer the following questions:\n\u2022\tHow does VISK perform on precise manipulation tasks?\n\u2022\tHow do different inputs affect performance of VISK?\n\u2022\tDoes VISK's use of AnySkin improve over DIGIT [4]?\n\u2022\tDo VISK policies generalize to unseen task variations?"}, {"title": "A. Environment Setup", "content": "We use a Ufactory xArm 7 robot with its standard two-fingered gripper for all our experiments. To enable tactile sensing, we attach AnySkin sensor tips to the left gripper finger. An identically shaped, plain silicone tip is attached to the right finger. For baseline comparisons with the DIGIT sensor, we use a DIGIT sensor on either fingertip in line with prior work [24]. The camera inputs comprise synchronized RGB images at 128x128 resolution from three static third-person cameras and an egocentric camera mounted on the gripper. The action space is the change in the end-effector pose and gripper state. Our experimental setup is depicted in Figure 2. Learned policies are deployed at a 10Hz frequency."}, {"title": "B. Task Descriptions", "content": "For all the analysis presented in this paper, we focus on a set of four contact-rich tasks that require high precision as well as spatial generalization. Each task has a target object that the robot must interact with, whose position is varied during demo collection. All evaluations use a fixed set of ten target locations unseen in the training demonstration data.\na) Plug Insertion: This task requires the robot to insert a plug into the first socket on a power strip. The arm starts with the plug grasped and the power strip randomly posi-tioned within a 20cm \u00d7 7cm grid with a fixed orientation. The training dataset consists of 96 demonstrations.\nb) USB Insertion: This task has the robot plugging a USB stick into a specific port on a USB hub. The arm starts with the USB stick grasped and the hub is positioned randomly within a 20cm \u00d7 15cm grid. The training dataset consists of 98 demonstrations.\nc) Card Swiping: This task involves swiping a credit card through a card reader. The arm starts with the credit card grasped and the card reader randomly positioned within a 40cm x 15cm grid, and oriented at a random angle in the range (-30\u00b0, 30\u00b0) from the direction the robot is facing. The training dataset consists of 90 demonstrations.\nd) Book Retrieval: This task requires the robot to retrieve a specific book from a set of eight books placed together, with the order of books randomized each time. The robot must first reach for the target book, pivot it about its edge, and then grasp and pull it out of the bookrack. The training dataset consists of 172 demonstrations.\nFor the first three tasks, where the robot starts with a grasped object, we do not enforce hard constraints on the grasping location and allow some variability across runs. The extent of variation in target object configurations are shown in Fig. 4. Evaluations are performed on a set of 10 held-out configurations for each task."}, {"title": "C. Performance of VISK policies", "content": "We evaluate the performance of VISK policies on the aforementioned precise manipulation tasks in the real world. To account for the high variance in performance of behavior cloning policies, we train policies across 3 random seeds and conduct 10 trials per seed for a total of 30 trials per evaluation. We report the aggregated success rate across seeds in Table I, and find that VISK policies consistently outperform other variations across tasks.\nAdditionally, we observe that VISK policies exhibit emer-gent seeking behavior. For instance, with the plug insertion and USB insertion tasks, we find that the policy first gets close to the location of the target (socket or port respec-tively), makes contact, and proceeds to move around as it tries to find the target. Once it seems to have located a change in contact characteristics, the policy pushes down and inserts successfully. This behavior is strong evidence of VISK policies effectively leveraging tactile information from AnySkin. Further, it is distinctly different from the behavior of vision-only policies that simply attempt to push downwards once they get close to the insertion location regardless of alignment with the target. We see an analogous trend with the card swiping task, where the VISK policy slows down as the card approaches the machine, and attempts alignment through contact before performing the swiping motion. The vision-only policy, on the other hand, seems to skip the alignment phase, and directly attempts to swipe the card, often entirely missing the card slot as a result. These failure modes demonstrates that purely visual policies lack the fine-grained tactile information that makes VISK extremely effective on contact-rich, precise manipulation.\nSimilarly, for the book retrieval task, prominent failure modes for policies without AnySkin involve either applying too little force causing the book to flip back into the bookrack, or too much force causing the book to topple over entirely. VISK policies apply a controlled downward force that enables them to pivot the book to an appropriate tilt, followed by grasping and retrieval as shown in Fig. 3. Furthermore, for this task, repeated interaction with the sharp edges of the book caused the AnySkin to tear. All evaluations"}, {"title": "D. Effect of different input modalities on performance", "content": "From Table I, we find that while the addition of AnySkin inputs to the policy consistently improves performance, the addition of other modalities like the wrist camera and propri-oception can have significant impact on policy performance depending on the task. A few consistent patterns emerge across tasks: (1) VISK results in a significant improvement (\u2265 2x) in performance over the next best model, indicating its effectiveness on precise, contact-rich manipulation. (2) Adding proprioceptive input almost always results in a drop in performance. This can be attributed to the learned policy overfitting to proprioceptive information which is detrimental to tasks requiring spatial generalizability over target object locations. (3) With the exception of the card swiping task, the addition of a wrist camera improves policy performance. The wrist camera gives the policy a local visual understanding of the scene in the frame of the gripper, and in turn, the same frame as the robot's action space. This is especially useful for the more fine-grained adjustments required for high-precision tasks. For the card swiping task, visualization of demonstration data indicated that the wrist camera cannot see the card reader due to occlusion from the gripper and therefore simply acts as a noise input to the policy.\nWhile the drops in performance due to proprioception as well as due to the wrist camera in the card swiping task could potentially be addressed by collecting more demonstrations, they highlight the true potential of the VISK framework. The addition of AnySkin and the use of a transformer-based architecture enable the policy to incorporate reliable tactile feedback directly from the interface between the robot and the object being interacted with. The low dimensional nature of AnySkin signal eliminates the need for dimensionality reduction or intermediate representation learning and enables end-to-end learning of visuotactile policies from relatively few (<200) demonstrations."}, {"title": "E. Comparison between AnySkin and DIGIT", "content": "To further highlight the role of AnySkin in the VISK framework for precise manipulation tasks, we collect similar demonstration datasets for two of the tasks presented in Sec-tion IV-B (Plug Insertion and USB Insertion) using DIGIT sensors instead of AnySkin sensors. We maintain the same policy architecture as VISK with the exception of the tactile encoder, where we replace the MLP with a modified ResNet-18 architecture identical to the image encoders used for cam-era inputs. We train two variants of the DIGIT-based policies: one with raw DIGIT measurement as input to the policy, and another with the DIGIT measurement at the start of the trajectory subtracted from every subsequent measurement. We report statistics for the best-performing alternative. While the use of a different tactile sensor necessitates collection of new demonstration data, we try to keep the DIGIT and AnySkin datasets as close to each other as possible. Target object locations used for training as well as evaluation are identical between the experiments corresponding to both sensors. The results in Table I also compare the performance of VISK using the skin-based AnySkin tactile sensor against the optical DIGIT [4] sensor.\nWe find that across both tasks, policies trained with AnySkin significantly outperform those trained with DIGIT. This difference could be attributed to the lower sensitivity of the DIGIT sensor making it difficult to detect small tactile signals from extrinsic contact of the grasped object. Fur-thermore, the significantly higher dimensionality of DIGIT observations compared to VISK might also make it more difficult to learn a sensory encoder without overfitting to the training data. These experiments highlight the suitability of AnySkin over optical sensors for efficiently learning visuotactile policies for precise tasks, due to its ability to sense finer tactile details as well as its low dimensionality resulting in more robust policies."}, {"title": "F. Generalization to Unseen Task Variations", "content": "To further probe the strengths of the VISK framework, we investigate performance on unseen task variations for all of the tasks presented above. For each variation, we evaluate the best-performing VISK policy for the respective task on the same set of target object configurations shown in Fig. 4 and present the results in Table II. Additionally, we also report generalization performance of a vision-only baseline, which is essentially the VISK policy without tactile information.\nWe study the efficacy of the best-performing VISK policy on four different variations of the plug as shown in Fig. 5 addition of a ground pin, shape, size and color. On this small sample set, the VISK policy generalizes surprisingly well to every plug variation except color despite their pins being in significantly different positions relative to the plug used for training. This is further evidence of VISK policies effectively leveraging vision and touch even when faced with object variations distinctly different from training. Change in color is one variant where we see a significant drop in performance. The behaviors corresponding to these failures are qualitatively"}, {"title": "IV. CONCLUSION AND LIMITATION", "content": "In this work, we presented Visuo-Skin (VISK), a sim-ple yet effective framework that leverages low-dimensional AnySkin tactile sensing for visuotactile policy learning in the real world. Our results demonstrate the efficacy of VISK across a diverse range of precise, contact-rich manipulation tasks. Additionally, we also present a detailed analysis of the effect of different modalities on policy performance for this class of tasks and find that while the addition of wrist cameras can be critical to performance in tasks involving fine alignment, proprioception can often hurt spa-tial generalizability. We address a few limitations in this work: (a) While VISK shows significant improvements over vision-only policies, the policy's performance remains at approximately 60% across all tasks. This suggests potential for further performance enhancement through fine-tuning the VISK policy using reinforcement learning techniques [47]. (b) Contrary to findings in prior studies, we observe that robot proprioception did not contribute to improved policy learning performance in precise manipulation tasks. This unexpected result warrants further investigation and presents an interesting direction for future research. (c) All the tasks analyzed in this work involve maintained contact with the object throughout the duration of the task. Tasks that require making and breaking contact may involve specific nuances that could benefit from a similar detailed analysis. These limitations notwithstanding, we believe that VISK presents a significant step in the right direction for advancing visuotactile policy learning in robotics."}]}