{"title": "AnoPLe: Few-Shot Anomaly Detection via Bi-directional Prompt Learning with Only Normal Samples", "authors": ["Yujin Lee", "Seoyoon Jang", "Hyunsoo Yoon"], "abstract": "Few-shot Anomaly Detection (FAD) poses significant challenges due to the limited availability of training samples and the frequent absence of abnormal samples. Previous approaches often rely on annotations or true abnormal samples to improve detection, but such textual or visual cues are not always accessible. To address this, we introduce AnoPLe, a multi-modal prompt learning method designed for anomaly detection without prior knowledge of anomalies. AnoPLe simulates anomalies and employs bidirectional coupling of textual and visual prompts to facilitate deep interaction between the two modalities. Additionally, we integrate a lightweight decoder with a learnable multi-view signal, trained on multi-scale images to enhance local semantic comprehension. To further improve performance, we align global and local semantics, enriching the image-level understanding of anomalies. The experimental results demonstrate that AnoPLe achieves strong FAD performance, recording 94.1% and 86.2% Image AUROC on MVTec-AD and VisA respectively, with only around a 1% gap compared to the SoTA, despite not being exposed to true anomalies. Code is available at https://github.com/YoojLee/AnoPLe.", "sections": [{"title": "1. Introduction", "content": "In real-world, it is challenging to obtain true abnormal samples and often impractical to secure large amounts of accurately verified normal samples. This challenge has ignited interest in robust few-shot anomaly detection methods [6, 13, 19]. Recently, large-scale vision-language models (VLMs) [9, 17] have shown promise in zero- and few-shot scenarios with proper prompt engineering. Pointing out that prompt engineering is suboptimal and time-consuming, previous works in few-shot image classification [11, 27, 28] have replaced manually engineered hard prompts with learnable continuous vectors. Similarly, in recent years, there has been active research on CLIP-based zero- and few-shot methods within the domain of anomaly detection. Pioneering work [8] in the CLIP-based methods used manually engineered prompts for anomaly detection, followed by subsequent works based on learnable prompts.\nWhile introducing learnable context vectors rather than solely relying on hard prompts, recent studies based on prompt learning are, to some extent, not completely free from pre-defined knowledge. Several works require true anomalous images as an auxiliary dataset during training [29, 30], or inject prior domain expertise regarding anomalies within text prompts [2, 14]. For example, Promp-tAD [13] combines both hard and soft prompts, using hard prompts with prior knowledge about object-customized anomalies (e.g., zipper with broken fabric) as a guide to align the soft prompts during training (Fig. 1). Although these methods have shown remarkable performance in few-shot scenarios than prompt engineering, it is not guaranteed"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Few-Shot Anomaly Detection", "content": "Few-shot Anomaly Detection aims to train models that generalize well with only a few normal samples. Previous studies in this area have mainly focused on few-shot transfer, where the model is trained on the entire given dataset or set of classes and then transferred to the target dataset or classes with only a few samples [5, 6]. Notably, RegAD [6] performs class transfer with few-shot learning, leveraging pre-learned features from different classes during the training process. AnomalyGPT [5] utilizes auxiliary datasets to learn prompts in a fully supervised manner to facilitate unsupervised learning and conducts inference through in-context learning. Several studies propose learning abnormalities using only a few normal images without any explicit exposure to the domain of anomaly detection [8, 13, 18]. These approaches are more challenging than few-shot transfer, as the model must learn representations for anomaly detection with only a few images. In this work, we follow the latter settings since they are closer to real-world situations."}, {"title": "2.2. Prompting VLMs for Anomaly Detection", "content": "VLMs [9, 17] have been actively discussed within the AD domain as well. Some studies retain the original textual representation space from CLIP [17] and adjust the encoder's output by leveraging an adapter following the encoder [2, 5, 30]. InCTRL [30] trains the model to be aware of discrepancies between query images and few-shot normal sample prompts through in-context residual learning, sampling query images from true anomalous data. AnoVL [2] conducts prompt engineering for textual inputs and adjusts visual representation with the subsequent adapter. AnomalyGPT [5] feeds learnable prompts as visual input into large VLMs [20] and incorporates textual descriptions with tailored anomaly information for each class.\nAnother widespread approach is to prepend context vectors to the inputs of the encoder, adjusting the representation of token embeddings during the forward pass [13, 29]. AnomalyCLIP [29] explores the feasibility of textual deep learnable prompts in zero-shot transfer after learning abnormalities from true anomalous samples. Meanwhile, Promp-tAD [13] does not utilize abnormal images but still requires textual descriptions for each object in abnormal states. In contrast, our approach demonstrates favorable performance by excluding visual or textual cues of true anomalies and strongly coupling context vectors prepended to both the image and text encoders."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "AnoPLe aims to detect anomalies using only few normal images without any cues on true anomalies. To achieve this, we simulate anomalies both in pixel and latent space following [23] and [16], respectively. Here, the visual encoder Ev processes both normal (I+) and simulated abnormal images (I-). Given that pseudo anomalies are not as informative as true anomalies, it is suboptimal to have learnable contexts propagate through just a single branch. Thus, we introduce learnable prompts within both visual and textual encoder (Ev, ET) and encourage a strong coupling between modalities. Specifically, prompts from each modality are projected to the other's hidden dimension and concatenated with the original prompts.\nTo enhance anomaly localization, we introduce a multi-view aware lightweight decoder D. During training, N sub-images are fed along with the original image Io to bootstrap local semantics knowledge from CLIP. At inference, only"}, {"title": "3.2. Multi-Modal Prompts for Anomaly Detection", "content": ""}, {"title": "3.2.1 Textual and Visual Prompt Learning", "content": "In this section, we provide brief preliminaries for prompt learning and how we construct the initial state of prompts for each modalities.\nFor textual prompting, Text Encoder Er takes a word embedding to as an input. The textual input e+ and e- for normal and abnormal text prompts are defined as:\n$e^{+} = [class]$, $e^{-} = [abnormal][class]$,                                                            (1)\nrespectively. [class] refers to the name of each class. Note that we inject abnormalities via textual prompts (i.e., a mere word embedding of \"abnormal\u201d) as minimal as possible.\nThen, learnable textual context vectors Po\u2208 RCtxdt, where Ct and dt indicates a length of context vectors and hidden dimension of Er, are prepended to eo. For the input to Er, we define:\nw+ = [Po, e+], w\uc774 = [Po, e\uc774],                                                             (2)\nwhere wo, wo refers to normal and abnormal textual features. We share prompts among classes and conduct multi-class anomaly detection.\nWe aim not only to include the prompt as model input, but also to adjust the original CLIP's representation to anomaly detection through deep prompting at each layer [10, 11, 29]. Thus, we concatenate P with the output of the previous layer as follows:\nwj = [-, ej] = Er ([P, ej\u22121]) j = 1, ..., J,                                                          (3)\nwhere J denotes the number of prompting layers. For visual encoder Ev, patch embeddings z = [z\u00ba, z\u00b9,...,z\u207f] of an image I is fed into Ev as an input. Here, z\u00ba refers to the [CLS] token, which retains global-level representation of the image. For visual prompting, we concatenate zo and learnable visual context vector P \u2208 RCvxdv where Cv, dv indicates the visual context length and hidden dimension of Ev, as an input to Ev:\n V+ = [Po, z+], V\uc774 = [Po, z\u014d],                                                             (4)\nwhere V+ and V\u014d indicates visual features for simulated abnormal image I\u207b and normal image I\u207a. Visual branch performs deep prompting across multiple layers in the same manner as the textual branch."}, {"title": "3.2.2 Bidirectional Coupling of Multi-Modal Prompts", "content": "Since we do not provide the knowledge on abnormalities with a anomaly-descriptive textual prompts or true abnormal images, prompt learning form a single modality may fail to convey well-aligned representations for anomaly detection. Thus, we design our learnable prompts Pt, Pu to communicate each other. Several works [11, 27] have proposed prompt learning methods that encourage uni-directional communications for image classification; text-to-image [11] and image-to-text [27]. Such an uni-directional propagation of context vectors may result in suboptimal outcomes in anomaly detection when limited amount of images and minimal anomaly cues are available.\nThus, we propose a prompt learning approach that enables bi-directional communication among prompts. To achieve this, we introduce a linear projection for each modality: fv\u2192t \u2208 Rdvxdt and ftv \u2208 Rdtxdv. Using these projections, we project the prompts Pt and Pu to each other's hidden dimensions, obtaining P\u2192t and Po\u2192t. Consequently, in each layer of both modalities, context vectors are propagated as follows:\n[-, -, wj] = ET([P, Pt, wj-1]), j = 1, ..., J,                                                                                                                             (5)\n[-, -, zj] = Ev ([P, P, zj-1]), j = 1, ..., J,                                                                                                                             (6)"}, {"title": "3.3. Bootstrapping local semantics from CLIP", "content": ""}, {"title": "3.3.1 Multi-View Aware Decoder", "content": "CLIP is designed to align global-level representations, making it difficult to capture local semantics. AnoPLe modifies internal visual representations through visual prompting, which does not guarantee that V-V attention still extracts enriched local semantics. Therefore, similar to MaskCLIP [26], we introduce a shallow decoder D to refine the prompted patch tokens and generate a precise anomaly map. The shallow decoder D consists of three convolution layers (Fig. 2). The patch embedding z[1 : ] \u2208 Rn\u00d7dv of I, where n is the number of patches, is upsampled to F \u2208 Rh\u00d7w\u00d7dv via bi-linear interpolation. The convolution layers in D operate as a channel and spatial mixer, refining the visual representation and mapping it to the textual dimension dt."}, {"title": "3.3.2 Multi-View Signal c", "content": "Additionally, to ensure the decoder better learns fine-grained semantics, we feed N crops {Ii | 1 \u2264 i \u2264 N} of the original image Io along with Io. During inference, we only feed Io to the model to prevent increased computational costs from processing sub-images. This might cause a distributional shift, making the model overly sensitive to local details while diminishing its ability to perceive the whole image. To address this, we introduce a multi-view signal c\u2208 R(N+1)\u00d7dv. Before being fed into"}, {"title": "3.4. Loss Function", "content": ""}, {"title": "3.4.1 Pixel-level Anomaly Detection", "content": "For pixel level, we compute loss with predicted pixel-level logits M = (D(z), w) and the simulated ground truth mask M. The pixel-level loss Lpixel is defined as\nLpixel = Ldice((D(z), w), M) + Lfocal((D(z), w), M),                                                                                                                            (7)\nwhere Ldice is a dice loss [21] and Lfocal is a focal loss [15]. Ldice measures the intersections between M and M. Since anomalous pixels occur less frequently than normal pixels, we compute Lfocal to address this imbalance, alongside Ldice."}, {"title": "3.4.2 Image-level Anomaly Detection", "content": "At the image level, we repel negative pairs (normal images and abnormal text embeddings, or vice versa) and attract positive pairs using cross entropy loss Lce. The predicted probability of a sample I being normal or abnormal is defined as follows:\np = p(y = i|I) = $\\frac{exp((z^{i}, w^{i}/T))}{\\Sigma_{i \\in {-,+}} exp((z^{i,0}, w^{i}/T))}$                                                                                                                                                           (8)\nwhere i indicates whether I is abnormal(-) or normal(+). When computing w, we average outputs from all classes within datasets [8]. Next, we compute Lce for the predicted probability p = p(y = i|x) for both latent and image perturbations. The resulting image-level loss is defined as:\nLimg = Lce(z+,z\u00af,w) + \u00a3ce(z+, Zlatent, W),                                                                                                                            (9)\nwhere Zlatent = z+ + \u20ac with \u20ac ~ \u039d(\u03bc, \u03c3\u00b2) is a simulated anomaly with perturbation in the latent space, and z\u00af represents a simulated anomaly in the pixel space. In practice, we apply label smoothing with a smoothing parameter of 0.003 to the latent anomalies."}, {"title": "3.4.3 Alignment between Local and Global Semantics", "content": "We introduce an additional loss term, Lalign, to align global semantics with local semantics. Since an image is often treated as a bag of patches, local semantics can be beneficial in capturing image-level representations. Moreover, in anomaly detection, image-level anomalies are typically determined based on pixel-level anomalies [6, 18, 22]. Therefore, it is reasonable to utilize local semantics to inform global semantics in order to improve image-level anomaly detection.\nHowever, given that anomalous pixels constitute a small portion of the image, it is risky to take a mere average of pixel-level representation for distillation. Hence, we aggregate pixel-level representations to align with textual semantics (i.e., \u201cabnormal\u201d). To achieve this, we multiply the pixel-level logits M with D(z) element-wise to obtain s\u2208 Rdv as follows:\ns = $\\Sigma_{(i,j) \\in {h,w}}$ Mij Dij(z),                                                                                                                                                           (10)\nwhere Mij and Dij(z) refer to the values at (i,j) of the pixel-level logits and decoder output, respectively. For distillation, we calculate the cosine similarity and subtract it from 1, resulting Lalign 1- (z\u00ba, s). The final loss is then computed as:\nL = Lpixel + Limg + Lalign.                                                                                                                            (11)"}, {"title": "3.5. Anomaly Detection", "content": "To determine pixel-level anomalies, we compute an anomaly map Mq \u2208 Rh\u00d7w for the query image Iq as follows:\nMq = 1.0/((1.0/Mq) + (1.0/Mmem)) \u2208 Rhxw,                                                                                                                            (12)\nwhere Mq is output of the decoder D, and Mmem is a visual memory assisted anomaly map.\nTo provide details for Mmem, following [8, 13], we acquire patch features from the i-th layer of Ev and store them in the visual memory R. At inference time, patch features F \u2208 Rh\u00d7w\u00d7dv from the i-th layer of query images are compared with R. Mmem \u2208 [0,1]h\u00d7w is obtained as\nMij = $\\frac{1}{min(1,\\frac{1}{\\left|R\\right|}\\sum_{r \\in R}(1- (Fij, r))}$                                                                                                                                                                                          (13)\nFollowing [8], we aggregate two anomaly maps with harmonic mean. Hence, a image-level anomaly score is computed as\nscore = 1.0/((1.0/\u00ee) + (1.0/max M\u2084)).                                                                                                                            (14)"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "Our experiments utilize the MVTec-AD and VisA datasets, both widely recognized benchmarks in the anomaly detection domain. MVTec-AD comprises high-resolution images from 15 object and texture categories, including surface defects (e.g., scratches, dents) and structural defects (e.g., missing parts), making it a representative benchmark for evaluating anomaly detection models. The VisA dataset, which is more complex than MVTec-AD, contains 10,821 high-resolution images-9,621 normal and 1,200 anomalous-spanning 12 objects characterized by intricate structures, single and multiple instances."}, {"title": "4.2. Implementation Details", "content": "We use CLIP with a ViT-B/16+ architecture as the backbone, following previous approaches [8], [13]. We train our method for 60 epochs, with the learning rate set to 0.001 for the prompt learner and 0.0002 for the decoder. To generate subimages, we resize images to 480 \u00d7 480 pixels and divide them into four non-overlapping crops, while the original image is resized to 240 x 240 pixels. The prompt depth is set to 9, with visual and textual context lengths of 3 for MVTec-AD, and 5 and 8 for VisA."}, {"title": "4.3. Evaluation Metrics", "content": "We report the Area Under the Receiver Operation Characteristic (AUROC) following the literature [1] for both image- pixel-level anomaly detection."}, {"title": "4.4. Main Results", "content": "Table 1 shows that our method outperforms baselines(row 1-5) blind to true anomalies by a significant margin (> 10% on MVTec and > 6% on VisA at the image level compared to [18]). Even when compared to methods that assume prior access to true anomalies, our approach remains competitive. For instance, AnoPLe demonstrates superior image-level performance compared to WinCLIP [8], which uses manually predefined text prompts, in the 1-/ 2-/ 4-shot settings, with improvements of 1.0%/ 0.9%/ 1.1% on MVTec and 2.4%/ 2.0%/ 0.4% on VisA, respectively. We also surpass InCTRL [30], which trains on anomalous samples, across both benchmarks. Additionally, the performance gap between our method and state-of-the-art methods such as AnomalyGPT and PromptAD is minimal-less than 0.5% on MVTec and around 1-2% on VisA in Image and Pixel AUROC.\nThe slightly larger gap between our method and AnomalyGPT on VisA, compared to MVTec, likely stems from AnomalyGPT being based on large language models (LLMs), which offer more enriched knowledge than CLIP. Furthermore, AnomalyGPT is trained on thousands of normal images and then performs few-shot transfer to other datasets, which allows the model already to be well-prepared with the representations for anomaly detection in the few-shot transfer phase. Nevertheless, given that these SoTA methods rely on explicit anomaly annotations in their text prompts, while our approach does not utilize any anomaly information, this performance gap is reasonable. Additionally, since our model employs highly generic prompts and learns all classes simultaneously rather than with class-specific models, the small gap in AUROC is further justified."}, {"title": "4.5. Results on Prompt-guided Anomaly Detection", "content": "In Table 2, we present the results of a comparative study on prompt-guided anomaly detection without the assistance of visual memory. We compare our method to PromptAD (using generic prompts, denoted as PromptAD-g, and the original PromptAD with class-specific prompts) and MaPLe with simulated anomalies following DRAEM.\nWe first compare our method with those that do not use prior knowledge of true anomalies. Even with only generic prompts, our method outperforms the baselines. PromptAD-g experiences a significant drop in AUROC"}, {"title": "4.6. Qualitative Results", "content": "Figure 3 presents a qualitative comparison of segmentation maps in the one-shot setting. Compared to other models, which often misclassify normal areas as anomalies or miss true anomalous regions, AnoPLe offers more precise localization maps. For Hazelnut, PromptAD detects the object rather than the anomalies, and WinCLIP only partially captures the anomalous regions. In contrast, AnoPLe accurately segments the anomalous regions with high confidence. For Fryum, WinCLIP captures non-anomalous regions, while PromptAD fails to detect anomalies. Meanwhile, AnoPLe correctly detects the anomalous regions, though with some false-positive pixels.\nTo confirm that our method performs well not only in pixel-level anomaly detection but also at the image level in a qualitative manner, we visualize features from the MVTec and VisA benchmarks. Properly learned context vectors should align textual and visual features while clearly distinguishing between normal and abnormal samples. As shown in Figure 4, even when trained with a single normal image, the features are well-aligned across modalities, with a clear separation between normal and abnormal samples."}, {"title": "4.7. Ablations", "content": ""}, {"title": "4.7.1 Components Analysis", "content": "Table 3 presents the ablation results. In Table 3-(a), we compare different interaction settings for context vectors. In MVTec, uni-modal context vectors underperform compared to multi-modal contexts at the image level (83.3, 85.0 87.6, 88.1, 89.3), indicating that uni-modal prompts are suboptimal without prior knowledge of true anomalies. Multi-modal interactions generally improve performance, except when textual prompts are conditioned on visual prompts (row 5). Since visual contexts alone are less effective, we speculate that pseudo anomaly information is less refined when isolated, leading to poorer results (row 2, 5 vs. row 1, 3, 4). Incorporating bi-directional interactions yields the best results, showing that visual information is most effective when tightly integrated with textual information. At the pixel level, visual contexts alone perform similarly to bi-directional prompting. In both MVTec"}, {"title": "5. Conclusion", "content": "We propose a novel approach for few-shot AD that leverages multi-modal prompt learning with minimal dependence on prior knowledge of anomalies. Our method improves anomaly detection by incorporating bidirectional prompt communication and utilizing a lightweight decoder for precise localization. Experimental results demonstrate that our approach performs comparably in few-shot industrial anomaly detection scenarios, offering an efficient solution for real-world applications. This research sets a new direction for resource-efficient anomaly detection, maintaining high accuracy even with limited training data and without relying on prior knowledge of true anomalies."}, {"title": "6. Further Implementation Details", "content": ""}, {"title": "6.1. Data Pre-processing", "content": "We normalize our images using CLIP's pre-computed mean values [0.48145466, 0.4578275, 0.40821073] and standard deviations [0.26862954, 0.26130258, 0.27577711]. During training, in order to generate sub-images, we resize images to 480 \u00d7 480 pixels and divide them into four non-overlapping crops, while the original image is resized to 240 x 240 pixels. In a test phase, we feed the model only resized images of 240 \u00d7 240 pixels."}, {"title": "6.2. Training Details", "content": "We train our model using the SGD optimizer for 60 epochs, with a batch size of 1 and learning rates of 0.001 for the prompt learner and 0.0002 for the decoder. Specifically, we apply a learning rate schedule that warms up from zero to the preset learning rate. For optimizer, we set momentum and weight decay to 0.9 and le-5. When computing the alignment loss (Eq. 10), we normalize M using a temperature-scaled softmax, with the temperature 7 set to 2. Textual and visual hidden dimension is set to 640 and 896, respectively, following OpenCLIP [7]."}, {"title": "6.3. Visual Memory Construction", "content": "To provide details for how to obtain visual memory assisted anomaly map, we acquire patch features from the i-th layer of Ey and store them in the visual memory R, following previous works [8, 13]. At inference time, patch features F \u2208 Rh\u00d7w\u00d7dv from the i-th layer of query images are compared with R. Here, h and w are height and width of the image, respectively. The visual memory assisted anomaly score map Mmem \u2208 [0,1]h\u00d7w is obtained as Mij = $\\frac{1}{min(1,\\frac{1}{\\left|R\\right|}\\sum_{r \\in R}(1- (Fij, r))$. For implementation, we use l intermediate layers for feature extraction, obtaining F\u2208 Rh\u00d7w\u00d7l\u00d7dv and average them across layers, resulting in F\u2208 Rhxwxdv. Specifically, for the MVTec-AD dataset, we extract features from the 7th to 10th layers, while for the VisA dataset, we use the 7th to 9th layers. Following Win-CLIP [8] and PromptAD [13], we aggregate prompt-guided and memory-guided anomaly maps with harmonic mean."}, {"title": "6.4. Computational Environments", "content": "Our model is trained and evaluated using an NVIDIA A100 80GB GPU and an Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz."}, {"title": "7. Related Work", "content": "We provide additional related work that could not be covered in the main text."}, {"title": "7.1. Anomaly Detection", "content": "In visual anomaly detection, anomalous samples are rare, leading to the predominance of unsupervised settings. Research in these settings has explored approaches such as reconstruction based methods [23, 25] and feature embedding based approaches [18]. While unsupervised settings are common, there are also studies in supervised settings that include anomalous samples in training [3, 24]. Recently, data efficiency has become a concern, prompting the development of models that generalize well with few or no normal samples. Some studies enable zero-shot anomaly detection [8, 29], performing fully zero-shot domain transfer. Others [12, 14, 29] perform transfer at the class or dataset level similar to traditional image classification domains. In few-shot anomaly detection, methods like [6] align test images with few normal samples, DifferNet [19] is based on normalizing flow, and FastRecon [4] reconstructs anomaly features with distribution regularization."}, {"title": "7.2. Prompt Learning with VLMs", "content": "Recently, pre-trained vision-language models (VLMs) such as CLIP [17] have demonstrated high capability in zero- and few-shot image classification. Since updating parameters might cause overfitting or harm their well-learned representation space, prompt learning is preferred over fine-tuning VLMs in few-shot settings. Prompt learning approaches update only the prompts while keeping the weights of the encoder frozen. Pioneering work such as CoOp [28] and CoCoOp [27] has replaced manually engineered textual prompts from CLIP's text encoder with learnable continuous vectors, achieving strong performance in image classification and domain generalization. VPT [10] has designed learnable visual context vectors for vision-only encoders in multiple layers (deep prompts). Bridging these two, MaPLe [11] performs multi-modal prompt learning by constructing learnable prompts within both the vision and text branches. It outperforms previous uni-modal approaches with the enhanced alignment between two modalities."}, {"title": "8. Ablation Studies", "content": "We provide additional ablation results. The results are reported without visual memory to fairly evaluate the impact"}, {"title": "8.1. Prompt Depth", "content": "Figure 6 presents the results of an ablation study on prompt depth. To ensure a fair evaluation of the effect of prompt depth on performance, we measure both Image and Pixel AUROC without visual memory. In pixel-level detection, the results show greater robustness to changes in prompt depth compared to image-level detection. However, for image-level tasks, AUROC consistently improves as prompt depth increases. The highest performance is achieved when the prompt depth was set to 9, which is our default setting."}, {"title": "8.2. Textual Prompt Templates", "content": "The input textual prompts for AnoPLe are constructed as follows:\ne+ = [class], e\u2212 = [abnormal][class].                                                             (15)\nWe measure the impact of words placed in [class] (i.e., class word) and [abnormal] (i.e., state word) in Figure 7 and Table 4, respectively."}, {"title": "8.2.1 Ablation on a Class Word.", "content": "To examine the impact of the class word on our method, we compare two scenarios: \"Object\" (following [29]) and \"Classname.\" In the \u201cObject\u201d scenario, the word \"object\" is used in place of the class name for all classes, whereas in the \"Classname\" scenario, the specific class name is inserted into the [class] position. As shown in Figure 7, AnoPLe demonstrates greater robustness in pixel-level anomaly detection compared to the image-level. This can be attributed to the ability of decoder to effectively refine the logits from the encoder to better align the localization task. For image-level detection, the results indicate that incorporating class"}, {"title": "8.2.2 Ablation on a State Word.", "content": "We compare several state words (e.g., \"damaged,\" \"anomalous,\" \"defective\") and the Compositional Prompt Ensemble (CPE) proposed in [8] with our chosen state word, \"abnormal.\" In the original paper, CPE combines multiple state-level templates with multiple text templates; however, in this ablation, we only use an ensemble of state-level templates. As shown in Table 4, our simple state word, \"abnormal,\" outperforms or at least competes with other domain-specific options (e.g., \"damaged\" and \"defective\"). Although CPE achieves slightly higher AUROC scores at both the image and pixel levels in VisA, it employs a combi-"}, {"title": "8.3. Anomaly Simulation", "content": "Under constraint that we are not able to utilize any cues on true anomalies, whether textual or visual, we simulate anomalies in the pixel and latent space to achieve robust anomaly detection with only few normal images. Specifically, to generate pixel-level pseudo anomalies, we apply Perlin noise to the raw image following DRAEM [23]. For anomaly simulation in the latent space, we follow SimpleNet [16] by adding Gaussian noise to the output of the visual encoder. Table 5 presents the ablation results on the impact of pseudo anomalies at each level on our method. Simulating anomalies in the latent space leads to notably poor performance, indicating that merely adding Gaussian noise to the features of a limited number of normal images fails to provide the model with sufficient signals for anomaly detection. In contrast, using only pixel-level anomalies performs quite well across both datasets. Notably, in MVTec-AD, it slightly outperforms AnoPLe, though the difference is marginal. However, for image-level detection (especially in VisA), the performance remains suboptimal. Overall, the best results are achieved when both levels of pseudo anomalies are used together. While anomaly simulation in the latent space alone is not highly informative, it complements pixel-level pseudo anomalies, resulting in further performance improvements when both are combined."}, {"title": "9. Detailed Quantitative Results", "content": "Table 6-9 presents class-wise results on the MVTec-AD and VisA benchmarks. We exclude AnomalyGPT [5] from the class-wise comparison since its class-wise results are not reported in the original paper. As shown in Table 6 and 7, we achieve the second-highest scores in most cases on MVTec-AD when averaged across classes. Additionally, following PromptAD [13], we achieve the best performance in the second-highest number of classes. Given that AnoPLe is trained without access to true anomalies not as previous works[8, 13], this achievement is particularly noteworthy. On the VisA benchmark, we also achieve competitive results compared to the SoTA, with the gap between our method and the SoTA being around 1-2%. In the one-shot setting, this gap even narrows to within 1%."}, {"title": "10. Detailed Qualitative Results", "content": ""}, {"title": "10.1. Visualization of Anomaly Localization", "content": "Figure 9 and 10 present segmentation maps under the 1-shot setting across all classes of the MVTec-AD and VisA datasets. The first, second, third, and fourth columns represent the original image, ground truth, heatmap, and the binary map generated by applying a mask to the binarized heatmap, respectively. Although AnoPLe is trained in a 1-shot setting, it demonstrates the ability to accurately segment anomalies in localized areas across all datasets and classes. Additionally, it shows a tendency to avoid falsely identifying anomalies, resulting in a map that closely resembles the ground truth mask."}, {"title": "10.2. Failure Cases", "content": "We presents some failure examples in Figure 8 from both MVTec-AD and VisA. The top two rows present examples from the MVTec-AD dataset, while the bottom two rows are from the VisA dataset. The \"Cable\" case illustrates a scenario where components have been swapped, and the \"Transistor\" case represents an anomaly caused by the object being misplaced. In such logical anomaly cases, our method fails to accurately localize the anomaly. This can be attributed to the fact that our approach is designed to generate pseudo anomalies for training by creating anomalies at the image-level and pixel-level through noise. As a result, our method is more suited to structural anomalies, showing vulnerability when faced with logical anomalies, as seen in the provided examples. In particular, in the case of \u201cmisplaced\u201d objects, the ground truth mask is configured to localize both the correct and incorrect locations, making the ground truth itself ambiguous and difficult to accurately align with the predictions. Additionally, the \"Capsules\" example from the"}, {"title": "10.3. Learnable Contexts", "content": "AnoPLe encourages active interaction between the two modalities through a bi-directional projection of the learnable context vector, enabling the information in both modalities to complement each other and achieve robust anomaly detection. Our goal is to qualitatively confirm the interaction between the learnable"}]}