{"title": "AnoPLe: Few-Shot Anomaly Detection via Bi-directional Prompt Learning with Only Normal Samples", "authors": ["Yujin Lee", "Seoyoon Jang", "Hyunsoo Yoon"], "abstract": "Few-shot Anomaly Detection (FAD) poses significant challenges due to the limited availability of training samples and the frequent absence of abnormal samples. Previous approaches often rely on annotations or true abnormal samples to improve detection, but such textual or visual cues are not always accessible. To address this, we introduce AnoPLe, a multi-modal prompt learning method designed for anomaly detection without prior knowledge of anomalies. AnoPLe simulates anomalies and employs bidirectional coupling of textual and visual prompts to facilitate deep interaction between the two modalities. Additionally, we integrate a lightweight decoder with a learnable multi-view signal, trained on multi-scale images to enhance local semantic comprehension. To further improve performance, we align global and local semantics, enriching the image-level understanding of anomalies. The experimental results demonstrate that AnoPLe achieves strong FAD performance, recording 94.1% and 86.2% Image AUROC on MVTec-AD and VisA respectively, with only around a 1% gap compared to the SoTA, despite not being exposed to true anomalies.", "sections": [{"title": "1. Introduction", "content": "In real-world, it is challenging to obtain true abnormal samples and often impractical to secure large amounts of accurately verified normal samples. This challenge has ignited interest in robust few-shot anomaly detection methods. Recently, large-scale vision-language models (VLMs) have shown promise in zero- and few-shot scenarios with proper prompt engineering. Pointing out that prompt engineering is suboptimal and time-consuming, previous works in few-shot image classification have replaced manually engineered hard prompts with learnable continuous vectors. Similarly, in recent years, there has been active research on CLIP-based zero- and few-shot methods within the domain of anomaly detection. Pioneering work in the CLIP-based methods used manually engineered prompts for anomaly detection, followed by subsequent works based on learnable prompts.\nWhile introducing learnable context vectors rather than solely relying on hard prompts, recent studies based on prompt learning are, to some extent, not completely free from pre-defined knowledge. Several works require true anomalous images as an auxiliary dataset during training, or inject prior domain expertise regarding anomalies within text prompts. For example, Promp-tAD combines both hard and soft prompts, using hard prompts with prior knowledge about object-customized anomalies (e.g., zipper with broken fabric) as a guide to align the soft prompts during training. Although these methods have shown remarkable performance in few-shot scenarios than prompt engineering, it is not guaranteed"}, {"title": "2. Related work", "content": "to prior knowledge of abnormality is always available.\nThus, assuming no prior information about abnormal data in either visual or textual aspects, we aim to detect anomalies using only few normal images. To achieve this, we simulate anomalies to prompt CLIP to distinguish between normal and abnormal samples. In Figure 1, we provide generic prompts to the text encoder (e.g., \"abnormal\"). Additionally, to maximize the use of coarse information about abnormalities from simulated anomalies, we feed visual contexts into the image encoder.\nGiven the limited information from textual or visual inputs alone, we promote interaction between these contexts to better capture anomalies in CLIP\u2019s representation space. Unlike clear labels in image classification, the concept of \u201cabnormal\u201d is vague in CLIP, making textual context less informative. Therefore, conditioning the visual context solely on textual context can be suboptimal. To address this, we propose \u201cAnoPLe,\u201d a bidirectional prompt learning approach where textual context also references visual context enabling effective few-shot AD without relying on predefined textual or visual semantics about anomalous samples. Specifically, as shown in Figure 1, when compared with methods without prior knowledge on genuine anomalies, we outperform baselines with significant margin - MaPLe with pseudo anomalies following DRAEM and PromptAD-g which indicates PromptAD relies only on generic prompts. Furthermore, despite using highly generic prompts in a multi-class setting, AnoPLe achieves Image-AUROC comparable to PromptAD, the state-of-the-art (SoTA) method, leveraging prior knowledge of anomalies. As we extend our model to the one-class-one-model setting (AnoPLe*), we surpass PromptAD even without any prior knowledge.\nIn anomaly detection, it is crucial to localize the anomalous regions as well. However, CLIP struggles with capturing local semantics. Hence, previous studies have addressed this by sliding windows to focus on local regions, which increases computational load and slows down inference times, or by modifying the QKV self-attention with value-to-value (V-V) attention to enhance fine-grained details. However, V-V attention may be incompatible with multi-modal prompting, as it modifies the visual representation space, potentially compromising the capture of accurate local semantics.\nTherefore, inspired by MaskCLIP, we introduce a light-weight decoder to decode the CLIP\u2019s patch features for localization. To improve localization, we feed additional sub-images along with the original image, without overlapping patches. We only use the original image during testing, which causes a distributional shift. To address this, we train the model to recognize whether an image is a sub-image"}, {"title": "2.1. Few-Shot Anomaly Detection", "content": "Few-shot Anomaly Detection aims to train models that generalize well with only a few normal samples. Previous studies in this area have mainly focused on few-shot transfer, where the model is trained on the entire given dataset or set of classes and then transferred to the target dataset or classes with only a few samples. Notably, RegAD performs class transfer with few-shot learning, leveraging pre-learned features from different classes during the training process. AnomalyGPT utilizes auxiliary datasets to learn prompts in a fully supervised manner to facilitate unsupervised learning and conducts inference through in-context learning. Several studies propose learning abnormalities using only a few normal images without any explicit exposure to the domain of anomaly detection. These approaches are more challenging than few-shot transfer, as the model must learn representations for anomaly detection with only a few images. In this work, we follow the latter settings since they are closer to real-world situations."}, {"title": "2.2. Prompting VLMs for Anomaly Detection", "content": "VLMs have been actively discussed within the AD domain as well. Some studies retain the original textual representation space from CLIP and adjust the encoder's output by leveraging an adapter following the encoder. InCTRL trains the model to be aware of discrepancies between query images and few-shot normal sample prompts through in-context residual learning, sampling query images from true anomalous data. AnoVL conducts prompt engineering for textual inputs and adjusts visual representation with the subsequent adapter. AnomalyGPT feeds learnable prompts as visual input into large VLMs and incorporates textual descriptions with tailored anomaly information for each class.\nAnother widespread approach is to prepend context vectors to the inputs of the encoder, adjusting the representation of token embeddings during the forward pass. AnomalyCLIP explores the feasibility of textual deep learnable prompts in zero-shot transfer after learning abnormalities from true anomalous samples. Meanwhile, PromptAD does not utilize abnormal images but still requires textual descriptions for each object in abnormal states. In contrast, our approach demonstrates favorable performance by excluding visual or textual cues of true anomalies and strongly coupling context vectors prepended to both the image and text encoders."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "AnoPLe aims to detect anomalies using only few normal images without any cues on true anomalies. To achieve this, we simulate anomalies both in pixel and latent space following and, respectively. Here, the visual encoder $E_v$ processes both normal ($I^+$) and simulated abnormal images ($I^-$). Given that pseudo anomalies are not as informative as true anomalies, it is suboptimal to have learnable contexts propagate through just a single branch. Thus, we introduce learnable prompts within both visual and textual encoder ($E_v, E_T$) and encourage a strong coupling between modalities. Specifically, prompts from each modality are projected to the other\u2019s hidden dimension and concatenated with the original prompts.\nTo enhance anomaly localization, we introduce a multi-view aware lightweight decoder $D$. During training, $N$ sub-images are fed along with the original image $I_o$ to bootstrap local semantics knowledge from CLIP. At inference, only"}, {"title": "3.2. Multi-Modal Prompts for Anomaly Detection", "content": ""}, {"title": "3.2.1 Textual and Visual Prompt Learning", "content": "In this section, we provide brief preliminaries for prompt learning and how we construct the initial state of prompts for each modalities.\nFor textual prompting, Text Encoder $E_T$ takes a word embedding $e_o$ as an input. The textual input $e^+$ and $e^-$ for normal and abnormal text prompts are defined as:\n$e^+ = [class], e^- = [abnormal][class]$,\nrespectively. $[class]$ refers to the name of each class. Note that we inject abnormalities via textual prompts (i.e., a mere word embedding of \u201cabnormal\u201d) as minimal as possible.\nThen, learnable textual context vectors $P_o \\in R^{C_t \\times d_t}$, where $C_t$ and $d_t$ indicates a length of context vectors and hidden dimension of $E_T$, are prepended to $e_o$. For the input to $E_T$, we define:\n$w^+ = [P_o, e^+], w^- = [P_o, e^-]$,\nwhere $w_o^+, w_o^-$ refers to normal and abnormal textual features. We share prompts among classes and conduct multi-class anomaly detection.\nWe aim not only to include the prompt as model input, but also to adjust the original CLIP\u2019s representation to anomaly detection through deep prompting at each layer. Thus, we concatenate $P$ with the output of the previous layer as follows:\n$w_j = [v^-, e_j] = E_T([P, e_{j-1}]) j = 1, ..., J,$\nwhere $J$ denotes the number of prompting layers. For visual encoder $E_v$, patch embeddings $z = [z^o, z^1, ..., z^n]$ of an image $I$ is fed into $E_v$ as an input. Here, $z^o$ refers to the $[CLS]$ token, which retains global-level representation of the image. For visual prompting, we concatenate $z_o$ and learnable visual context vector $P \\in R^{C_v \\times d_v}$ where $C_v, d_v$ indicates the visual context length and hidden dimension of $E_v$, as an input to $E_v$:\n$V^+ = [P_o, z^+], V^- = [P_o, z^-]$,\nwhere $V^+, V^-$ indicates visual features for simulated abnormal image $I^-$ and normal image $I^+$. Visual branch performs deep prompting across multiple layers in the same manner as the textual branch."}, {"title": "3.2.2 Bidirectional Coupling of Multi-Modal Prompts", "content": "Since we do not provide the knowledge on abnormalities with a anomaly-descriptive textual prompts or true abnormal images, prompt learning form a single modality may fail to convey well-aligned representations for anomaly detection. Thus, we design our learnable prompts $P^t, P^v$ to communicate each other. Several works have proposed prompt learning methods that encourage uni-directional communications for image classification; text-to-image and image-to-text. Such an uni-directional propagation of context vectors may result in suboptimal outcomes in anomaly detection when limited amount of images and minimal anomaly cues are available.\nThus, we propose a prompt learning approach that enables bi-directional communication among prompts. To achieve this, we introduce a linear projection for each modality: $f_{v\\rightarrow t} \\in R^{d_v \\times d_t}$ and $f_{t\\rightarrow v} \\in R^{d_t \\times d_v}$. Using these projections, we project the prompts $P^t$ and $P^v$ to each other\u2019s hidden dimensions, obtaining $P^{v\\rightarrow t}$ and $P^{t\\rightarrow v}$. Consequently, in each layer of both modalities, context vectors are propagated as follows:\n$[v, v, w_j] = E_T([P, P^{v\\rightarrow t}, w_{j-1}]), j = 1, ..., J,$\n$[v, v, z_j] = E_v ([P, P^{t\\rightarrow v}, z_{j-1}]), j = 1, ..., J$"}, {"title": "3.3. Bootstrapping local semantics from CLIP", "content": ""}, {"title": "3.3.1 Multi-View Aware Decoder", "content": "CLIP is designed to align global-level representations, making it difficult to capture local semantics. AnoPLe modifies internal visual representations through visual prompting, which does not guarantee that V-V attention still extracts enriched local semantics. Therefore, similar to MaskCLIP, we introduce a shallow decoder $D$ to refine the prompted patch tokens and generate a precise anomaly map. The shallow decoder $D$ consists of three convolution layers (Fig. 2). The patch embedding $z[1 : ] \\in R^{n \\times d_v}$ of $I$, where $n$ is the number of patches, is upsampled to $F \\in R^{h \\times w \\times d_v}$ via bi-linear interpolation. The convolution layers in $D$ operate as a channel and spatial mixer, refining the visual representation and mapping it to the textual dimension $d_t$."}, {"title": "3.3.2 Multi-View Signal c", "content": "Additionally, to ensure the decoder better learns fine-grained semantics, we feed $N$ crops $\\{I_i | 1 \\leq i \\leq N\\}$ of the original image $I_o$ along with $I_o$. During inference, we only feed $I_o$ to the model to prevent increased computational costs from processing sub-images. This might cause a distributional shift, making the model overly sensitive to local details while diminishing its ability to perceive the whole image. To address this, we introduce a multi-view signal $c \\in R^{(N+1) \\times d_v}$. Before being fed into"}, {"title": "3.4. Loss Function", "content": ""}, {"title": "3.4.1 Pixel-level Anomaly Detection", "content": "For pixel level, we compute loss with predicted pixel-level logits $M = (D(z), w)$ and the simulated ground truth mask M. The pixel-level loss $L_{pixel}$ is defined as\n$L_{pixel} = L_{dice}((D(z), w), M) + L_{focal}((D(z), w), M)$,\nwhere $L_{dice}$ is a dice loss and $L_{focal}$ is a focal loss. $L_{dice}$ measures the intersections between $M$ and M. Since anomalous pixels occur less frequently than normal pixels, we compute $L_{focal}$ to address this imbalance, alongside $L_{dice}$."}, {"title": "3.4.2 Image-level Anomaly Detection", "content": "At the image level, we repel negative pairs (normal images and abnormal text embeddings, or vice versa) and attract positive pairs using cross entropy loss $L_{ce}$. The predicted probability of a sample $I$ being normal or abnormal is defined as follows:\n$p = p(y = i|I) = \\frac{exp((zi, w^i/T))}{\\Sigma_{v\\in\\{-,+\\}} exp((z^{i,v}, w^v/T))}$,\nwhere $i$ indicates whether $I$ is abnormal(-) or normal(+). When computing $w$, we average outputs from all classes within datasets. Next, we compute $L_{ce}$ for the predicted probability $p = p(y = i|x)$ for both latent and image perturbations. The resulting image-level loss is defined as:\n$L_{img} = L_{ce}(z_+, z_-,w) + L_{ce}(z_+, Z_{latent}, W)$,\nwhere $Z_{latent} = z_+ + \\epsilon$ with $\\epsilon \\sim N(\\mu, \\sigma^2)$ is a simulated anomaly with perturbation in the latent space, and $z^-$ represents a simulated anomaly in the pixel space. In practice, we apply label smoothing with a smoothing parameter of 0.003 to the latent anomalies."}, {"title": "3.4.3 Alignment between Local and Global Semantics", "content": "We introduce an additional loss term, $L_{align}$, to align global semantics with local semantics. Since an image is often treated as a bag of patches, local semantics can be beneficial in capturing image-level representations. Moreover, in anomaly detection, image-level anomalies are typically determined based on pixel-level anomalies. Therefore, it is reasonable to utilize local semantics to inform global semantics in order to improve image-level anomaly detection.\nHowever, given that anomalous pixels constitute a small portion of the image, it is risky to take a mere average of pixel-level representation for distillation. Hence, we aggregate pixel-level representations to align with textual semantics (i.e., \u201cabnormal\u201d). To achieve this, we multiply the pixel-level logits $M$ with $D(z)$ element-wise to obtain $s \\in R^{d_v}$ as follows:\n$S = \\frac{\\Sigma_{(i,j)\\in\\{h,w\\}} M_{ij} D_{ij}(z)}{\\Sigma_{(i,j)\\in\\{h,w\\}}}$,\nwhere $M_{ij}$ and $D_{ij}(z)$ refer to the values at $(i,j)$ of the pixel-level logits and decoder output, respectively. For distillation, we calculate the cosine similarity and subtract it from 1, resulting $L_{align} = 1- (z^o, s)$. The final loss is then computed as:\n$L = L_{pixel} + L_{img} + L_{align}$."}, {"title": "3.5. Anomaly Detection", "content": "To determine pixel-level anomalies, we compute an anomaly map $M_q \\in R^{h \\times w}$ for the query image $I_q$ as follows:\n$M_q = 1.0/((1.0/M_q) + (1.0/M_{mem})) \\in R^{h \\times w}$,\nwhere $M_q$ is output of the decoder $D$, and $M_{mem}$ is a visual memory assisted anomaly map.\nTo provide details for $M_{mem}$, following, we acquire patch features from the i-th layer of $E_v$ and store them in the visual memory $R$. At inference time, patch features $F \\in R^{h \\times w \\times d_v}$ from the i-th layer of query images are compared with $R$. $M_{mem} \\in [0,1]^{h \\times w}$ is obtained as\n$M_{ij} = \\underset{r\\in R}{min}(1 - \\frac{1}{2} (1- (F_{ij}, r)).$\nFollowing, we aggregate two anomaly maps with harmonic mean. Hence, a image-level anomaly score is computed as\n$score = 1.0/((1.0/i) + (1.0/max M_q))$."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "Our experiments utilize the MVTec-AD and VisA datasets, both widely recognized benchmarks in the anomaly detection domain. MVTec-AD comprises high-resolution images from 15 object and texture categories, including surface defects (e.g., scratches, dents) and structural defects (e.g., missing parts), making it a representative benchmark for evaluating anomaly detection models. The VisA dataset, which is more complex than MVTec-AD, contains 10,821 high-resolution images-9,621 normal and 1,200 anomalous-spanning 12 objects characterized by intricate structures, single and multiple instances."}, {"title": "4.2. Implementation Details", "content": "We use CLIP with a ViT-B/16+ architecture as the backbone, following previous approaches. We train our method for 60 epochs, with the learning rate set to 0.001 for the prompt learner and 0.0002 for the decoder. To generate subimages, we resize images to 480 \u00d7 480 pixels and divide them into four non-overlapping crops, while the original image is resized to 240 x 240 pixels. The prompt depth is set to 9, with visual and textual context lengths of 3 for MVTec-AD, and 5 and 8 for VisA."}, {"title": "4.3. Evaluation Metrics", "content": "We report the Area Under the Receiver Operation Characteristic (AUROC) following the literature for both image- pixel-level anomaly detection."}, {"title": "4.4. Main Results", "content": "Table 1 shows that our method outperforms baselines(row 1-5) blind to true anomalies by a significant margin (> 10% on MVTec and > 6% on VisA at the image level compared to ). Even when compared to methods that assume prior access to true anomalies, our approach remains competitive. For instance, AnoPLe demonstrates superior image-level performance compared to WinCLIP, which uses manually predefined text prompts, in the 1-/ 2-/ 4-shot settings, with improvements of 1.0%/ 0.9%/ 1.1% on MVTec and 2.4%/ 2.0%/ 0.4% on VisA, respectively. We also surpass InCTRL, which trains on anomalous samples, across both benchmarks. Additionally, the performance gap between our method and state-of-the-art methods such as AnomalyGPT and PromptAD is minimal-less than 0.5% on MVTec and around 1-2% on VisA in Image and Pixel AUROC.\nThe slightly larger gap between our method and AnomalyGPT on VisA, compared to MVTec, likely stems from AnomalyGPT being based on large language models (LLMs), which offer more enriched knowledge than CLIP. Furthermore, AnomalyGPT is trained on thousands of normal images and then performs few-shot transfer to other datasets, which allows the model already to be well-prepared with the representations for anomaly detection in the few-shot transfer phase. Nevertheless, given that these SoTA methods rely on explicit anomaly annotations in their text prompts, while our approach does not utilize any anomaly information, this performance gap is reasonable. Additionally, since our model employs highly generic prompts and learns all classes simultaneously rather than with class-specific models, the small gap in AUROC is further justified."}, {"title": "4.5. Results on Prompt-guided Anomaly Detection", "content": "In Table 2, we present the results of a comparative study on prompt-guided anomaly detection without the assistance of visual memory. We compare our method to PromptAD (using generic prompts, denoted as PromptAD-g, and the original PromptAD with class-specific prompts) and MaPLe with simulated anomalies following DRAEM.\nWe first compare our method with those that do not use prior knowledge of true anomalies. Even with only generic prompts, our method outperforms the baselines. PromptAD-g experiences a significant drop in AUROC"}, {"title": "4.6. Qualitative Results", "content": "Figure 3 presents a qualitative comparison of segmentation maps in the one-shot setting. Compared to other models, which often misclassify normal areas as anomalies or miss true anomalous regions, AnoPLe offers more precise localization maps. For Hazelnut, PromptAD detects the object rather than the anomalies, and WinCLIP only partially captures the anomalous regions. In contrast, AnoPLe accurately segments the anomalous regions with high confidence. For Fryum, WinCLIP captures non-anomalous regions, while PromptAD fails to detect anomalies. Meanwhile, AnoPLe correctly detects the anomalous regions, though with some false-positive pixels.\nTo confirm that our method performs well not only in pixel-level anomaly detection but also at the image level in a qualitative manner, we visualize features from the MVTec and VisA benchmarks. Properly learned context vectors should align textual and visual features while clearly distinguishing between normal and abnormal samples. As shown in Figure 4, even when trained with a single normal image, the features are well-aligned across modalities, with a clear separation between normal and abnormal samples."}, {"title": "4.7. Ablations", "content": ""}, {"title": "4.7.1 Components Analysis", "content": "Table 3 presents the ablation results. In Table 3-(a), we compare different interaction settings for context vectors. In MVTec, uni-modal context vectors underperform compared to multi-modal contexts at the image level (83.3, 85.0 87.6, 88.1, 89.3), indicating that uni-modal prompts are suboptimal without prior knowledge of true anomalies. Multi-modal interactions generally improve performance, except when textual prompts are conditioned on visual prompts (row 5). Since visual contexts alone are less effective, we speculate that pseudo anomaly information is less refined when isolated, leading to poorer results (row 2, 5 vs. row 1, 3, 4). Incorporating bi-directional interactions yields the best results, showing that visual information is most effective when tightly integrated with textual information. At the pixel level, visual contexts alone perform similarly to bi-directional prompting. In both MVTec"}, {"title": "5. Conclusion", "content": "We propose a novel approach for few-shot AD that leverages multi-modal prompt learning with minimal dependence on prior knowledge of anomalies. Our method improves anomaly detection by incorporating bidirectional prompt communication and utilizing a lightweight decoder for precise localization. Experimental results demonstrate that our approach performs comparably in few-shot industrial anomaly detection scenarios, offering an efficient solution for real-world applications. This research sets a new direction for resource-efficient anomaly detection, maintaining high accuracy even with limited training data and without relying on prior knowledge of true anomalies."}, {"title": "6. Further Implementation Details", "content": ""}, {"title": "6.1. Data Pre-processing", "content": "We normalize our images using CLIP's pre-computed mean values [0.48145466, 0.4578275, 0.40821073] and standard deviations [0.26862954, 0.26130258, 0.27577711]. During training, in order to generate sub-images, we resize images to 480 x 480 pixels and divide them into four non-overlapping crops, while the original image is resized to 240 x 240 pixels. In a test phase, we feed the model only resized images of 240 \u00d7 240 pixels."}, {"title": "6.2. Training Details", "content": "We train our model using the SGD optimizer for 60 epochs, with a batch size of 1 and learning rates of 0.001 for the prompt learner and 0.0002 for the decoder. Specifically, we apply a learning rate schedule that warms up from zero to the preset learning rate. For optimizer, we set momentum and weight decay to 0.9 and le-5. When computing the alignment loss (Eq. 10), we normalize M using a temperature-scaled softmax, with the temperature $\\tau$ set to 2. Textual and visual hidden dimension is set to 640 and 896, respectively, following OpenCLIP."}, {"title": "6.3. Visual Memory Construction", "content": "To provide details for how to obtain visual memory assisted anomaly map, we acquire patch features from the i-th layer of $E_v$ and store them in the visual memory R, following previous works . At inference time, patch features $F \\in R^{h\\times w\\times d_v}$ from the i-th layer of query images are compared with R. Here, h and w are height and width of the image, respectively. The visual memory assisted anomaly score map $M_{mem} \\in [0,1]^{h\\times w}$ is obtained as $M_{ij} = \\underset{r\\in R}{min}(1 - \\frac{1}{2} (1- (F_{ij}, r))$. For implementation, we use l intermediate layers for feature extraction, obtaining $F\\in R^{h\\times w\\times l\\times d_v}$ and average them across layers, resulting in $F\\in R^{h\\times w\\times d_v}$. Specifically, for the MVTec-AD dataset, we extract features from the 7th to 10th layers, while for the VisA dataset, we use the 7th to 9th layers. Following Win-CLIP and PromptAD , we aggregate prompt-guided and memory-guided anomaly maps with harmonic mean."}, {"title": "6.4. Computational Environments", "content": "Our model is trained and evaluated using an NVIDIA A100 80GB GPU and an Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz."}, {"title": "7. Related Work", "content": "We provide additional related work that could not be covered in the main text."}, {"title": "7.1. Anomaly Detection", "content": "In visual anomaly detection, anomalous samples are rare, leading to the predominance of unsupervised settings. Research in these settings has explored approaches such as reconstruction based methods and feature embedding based approaches . While unsupervised settings are common, there are also studies in supervised settings that include anomalous samples in training . Recently, data efficiency has become a concern, prompting the development of models that generalize well with few or no normal samples. Some studies enable zero-shot anomaly detection , performing fully zero-shot domain transfer. Others perform transfer at the class or dataset level similar to traditional image classification domains. In few-shot anomaly detection, methods like align test images with few normal samples, DifferNet is based on normalizing flow, and FastRecon reconstructs anomaly features with distribution regularization."}, {"title": "7.2. Prompt Learning with VLMs", "content": "Recently, pre-trained vision-language models (VLMs) such as CLIP have demonstrated high capability in zero- and few-shot image classification. Since updating parameters might cause overfitting or harm their well-learned representation space, prompt learning is preferred over fine-tuning VLMs in few-shot settings. Prompt learning approaches update only the prompts while keeping the weights of the encoder frozen. Pioneering work such as CoOp and CoCoOp has replaced manually engineered textual prompts from CLIP's text encoder with learnable continuous vectors, achieving strong performance in image classification and domain generalization. VPT has designed learnable visual context vectors for vision-only encoders in multiple layers (deep prompts). Bridging these two, MaPLe performs multi-modal prompt learning by constructing learnable prompts within both the vision and text branches. It outperforms previous uni-modal approaches with the enhanced alignment between two modalities."}, {"title": "8. Ablation Studies", "content": "We provide additional ablation results. The results are reported without visual memory to fairly evaluate the impact"}, {"title": "8.1. Prompt Depth", "content": "Figure 6 presents the results of an ablation study on prompt depth. To ensure a fair evaluation of the effect of prompt depth on performance, we measure both Image and Pixel AUROC without visual memory. In pixel-level detection, the results show greater robustness to changes in prompt depth compared to image-level detection. However, for image-level tasks, AUROC consistently improves as prompt depth increases. The highest performance is achieved when the prompt depth was set to 9, which is our default setting."}, {"title": "8.2. Textual Prompt Templates", "content": "The input textual prompts for AnoPLe are constructed as follows:\n$e^+ = [class], e^- = [abnormal][class]$.\nWe measure the impact of words placed in $[class]$ (i.e., class word) and $[abnormal]$ (i.e., state word) in Figure 7 and Table 4, respectively."}, {"title": "8.2.1 Ablation on a Class Word.", "content": "To examine the impact of the class word on our method, we compare two scenarios: \"Object\" (following ) and \"Classname.\" In the \u201cObject\u201d scenario, the word \"object\" is used in place of the class name for all classes, whereas in the \"Classname\" scenario, the specific class name is inserted into the $[class]$ position. As shown in Figure 7, AnoPLe demonstrates greater robustness in pixel-level anomaly detection compared to the image-level. This can be attributed to the ability of decoder to effectively refine the logits from the encoder to better align the localization task. For image-level detection, the results indicate that incorporating class"}, {"title": "8.2.2 Ablation on a State Word.", "content": "We compare several state words (e.g., \u201cdamaged,\u201d \u201canomalous,\u201d \u201cdefective\u201d) and the Compositional Prompt Ensemble (CPE) proposed in with our chosen state word, \u201cabnormal.\u201d In the original paper, CPE combines multiple state-level templates with multiple text templates; however, in this ablation, we only use an ensemble of state-level templates. As shown in Table 4, our simple state word, \u201cabnormal,\u201d outperforms or at least competes with other domain-specific options (e.g., \u201cdamaged\u201d and \u201cdefective\u201d). Although CPE achieves slightly higher AUROC scores at both the image and pixel levels in VisA, it employs a combi"}, {"title": "8.3. Anomaly Simulation", "content": "Under constraint that we are not able to utilize any cues on true anomalies, whether textual or visual, we simulate anomalies in the pixel and latent space to achieve robust anomaly detection with only few normal images. Specifically, to generate pixel-level pseudo anomalies, we apply Perlin noise to the raw image following DRAEM . For anomaly simulation in the latent space, we follow SimpleNet by adding Gaussian noise to the output of the visual encoder. Table 5 presents the ablation results on the impact of pseudo anomalies at each level on our method. Simulating anomalies in the latent space leads to notably poor performance, indicating that merely adding Gaussian noise to the features of a limited number of normal images fails to provide the model with sufficient signals for anomaly detection. In contrast, using only pixel-level anomalies performs quite well across both datasets. Notably, in MVTec-AD, it slightly outperforms AnoPLe, though the difference is marginal. However, for image-level detection (especially in VisA), the performance remains suboptimal. Overall, the best results are achieved when both levels of pseudo anomalies are used together. While anomaly simulation in the latent space alone is not highly informative, it complements pixel-level pseudo anomalies, resulting in further performance improvements when both are combined."}, {"title": "9. Detailed Quantitative Results", "content": "Table 6-9 presents class-wise results on the MVTec-AD and VisA benchmarks. We exclude AnomalyGPT from the class-wise comparison since its class-wise results are not reported in the original paper. As shown in Table 6 and 7, we achieve the second-highest scores in most cases on MVTec-AD when averaged across classes. Additionally, following PromptAD , we achieve the best performance in the second-highest number of"}]}