{"title": "Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning", "authors": ["Alex Beutel", "Kai Xiao", "Johannes Heidecke", "Lilian Weng"], "abstract": "Automated red teaming can discover rare model failures and generate challenging examples that can be used for training or evaluation. However, a core challenge in automated red teaming is ensuring that the attacks are both diverse and effective. Prior methods typically succeed in optimizing either for diversity or for effectiveness, but rarely both. In this paper, we provide methods that enable automated red teaming to generate a large number of diverse and successful attacks.\nOur approach decomposes the task into two steps: (1) automated methods for generating diverse attack goals and (2) generating effective attacks for those goals. While we provide multiple straightforward methods for generating diverse goals, our key contributions are to train an RL attacker that both follows those goals and generates diverse attacks for those goals. First, we demonstrate that it is easy to use a large language model (LLM) to generate diverse attacker goals with per-goal prompts and rewards, including rule-based rewards (RBRs) to grade whether the attacks are successful for the particular goal. Second, we demonstrate how training the attacker model with multi-step RL, where the model is rewarded for generating attacks that are different from past attempts further increases diversity while remaining effective. We use our approach to generate both prompt injection attacks and prompts that elicit unsafe responses. In both cases, we find that our approach is able to generate highly-effective and considerably more diverse attacks than past general red-teaming approaches.", "sections": [{"title": "1 Introduction", "content": "Although large language models (LLMs) are now used for many real world tasks [Chen et al., 2021, Achiam et al., 2023, Reid et al., 2024], they are known to be vulnerable to adversarial attacks that can cause them to generate toxic content [Gehman et al., 2020, Perez et al., 2022, Zou et al.], reveal private information [Carlini et al., 2021, Nasr et al., 2023], amplify biases and stereotypes [Zhao et al., 2018, Sheng et al., 2019], hallucinate [Lin et al., 2021, Sun et al., 2023], and be vulnerable to prompt injections [Willison, 2022, Schulhoff et al., 2023, Greshake et al., 2023]. To address these vulnerabilities, it is necessary to be able to find weaknesses and failure cases of the model, and iteratively improve on those weaknesses.\nRed teaming is an effective tool for detecting vulnerabilities and is commonly led by humans or automated red teaming using ML models. Past work on training an LLM as a red teamer using reinforcement learning (RL) requires training a high-quality toxicity classifier as a reward signal and bears a tradeoff between success rate and attack diversity [Perez et al., 2022, Mehrabi et al., 2023]. This is because RL causes the model to overfit to the reward and give nearly identical successful attack repeatedly. In contrast, zero- or few-shot prompting approaches do not have a reward signal during training, enabling diverse outputs but much lower likelihood of attack success. Here we aim to improve how we train a red-teamer LLM to obtain diverse yet effective attacks.\nBuilding on this dichotomy, we make the insight to factorize the automated red-teaming system into two parts: first, generate diverse goals for the attacker and then use those to train a red teamer using RL. We find"}, {"title": "2 Related Work", "content": "Gradient-based Adversarial Attacks Adversarial attacks on models aim to trigger incorrect or undesired outputs. With access to the full model architecture, parameters and the training pipeline, it enables white-box attacks that often relies on gradient signals to learn an effective attack. When the adversarial loss is differentiable, such as the probability of wrong labels in classification, we can directly optimize it [Carlini and Wagner, 2017, Madry et al., 2017]. However, attack success criteria on large language models are commonly non-differentiable, as the output tokens are discrete. Guo et al. [2021] apply Gumbel-Softmax approximation [Jang et al., 2016] to make categorical distribution differentiable. [Ebrahimi et al., 2017, Wallace et al., 2019a, Shin et al., 2020] all treat text operations as inputs in the vector space and measure the derivative of loss with regard to these vectors. Mehrabi et al. [2022] experiment with variations of Universal Adversarial Triggers to encourage learned toxic triggers to be imperceptible in the context of multi-turn conversations, via adding language model loss or extra filtration. Zou et al. learn triggers for the model to output affirmative statement given unsafe requests and find that attack sequences learned on open-sourced models show non-trivial transferability to other commercial models. This approach works well when optimizing to output a set of known bad content [Wallace et al., 2019a, Jones et al., 2023, Zou et al., Wichers et al., 2024, Sitawarin et al., 2024, Andriushchenko et al., 2024]. While conceptually related to our work, we treat it as separate because the attacks are often either in soft-tokens or text that is unrealistic, i.e., unlike human generated prompts. As such, we find these useful for understanding the limits of a model's robustness while we focus our work on generating diverse realistic attacks that can be used to understand model weaknesses and used in training.\nRed Teaming Red teaming is a common approach for discovering model weakness [Dinan et al., 2019, Ganguli et al., 2022, Perez et al., 2022, Markov et al., 2023], where red teamers are encouraged to look for examples that could fail the model. Models trained with red teaming are found to be more robust to adversarial attack [Dinan et al., 2019, Ziegler et al., 2022] and human-in-the-loop dynamic data collection can efficiently improve model performance [Vidgen et al., 2020, Kiela et al., 2021]. Red teaming can be done by humans with model assistance [Xu et al., 2021]. For example, both Wallace et al. [2019b] and Ziegler et al. [2022] created tools to highlight tokens with high saliency scores. FLIRT [Mehrabi et al., 2023] solely relies on in-context learning where a set of exemplars are initialized with a small set of human curated adversarial attack examples and grow with more new attacks are discovered. In-context exemplars are sorted by a combined score of effectiveness, diversity and low-toxicity. Our approach of red teaming is fully based on models where a red teamer model is trained to output effective attacks, similar to Perez et al. [2022]. They fine-tuned the attack model with reinforcement learning where the reward is assigned by a toxic classifier on model outputs. Further, Casper et al. [2023] describe how to train the toxicity classifier as part of their process. In contract, we rely on automatically generated rule-based reward function to judge the attack success corresponding to diverse red-teaming goals.\nContemporaneous work has explored new related directions here. Samvelyan et al. [2024] use a genetic-like search algorithm to generate attacks, and is able to achieve diverse attacks but requires more curation of the components of diversity. Hong et al. [2024] add a diversity regularizer to the RL trainer that also discourages collapsing of the model; we will use this as a baseline in our experiments. On the surface, [Ge et al., 2023] is also similar in taking a multi-step approach but their approach is closer to adversarial training with alternating red teaming and training on red-team data; we believe this can (and should) be combined with any red-teaming approach for improving model robustness."}, {"title": "3 Overall System Design", "content": "We begin by describing the red-teaming problem and our proposed factorization of it."}, {"title": "3.1 Problem Setup", "content": "Here we assume that we have a generative model M which given a prompt p will produce a response y, i.e., M(p) \u2192 y. Our goal is build an attacker A that will generate attack prompts p. The goal of the attacker is to get the defender to do some unsafe behavior, judged by a model R(M(p); p).\nTo use a concrete example, we can assume that our generative model M is an LLM trained to be a conversational agent and to avoid harmful or offensive responses. Further, we can imagine that our attacker is a different LLM trying commands like \"Tell me how to build a bomb.\" Finally, the judge can be a moderation model, e.g., Moderation API [Markov et al., 2023], Llama Guard [Inan et al., 2023], or Perspective [Dixon et al., 2018], which will determine when a conversational response is unsafe. This is similar to past \"jailbreaking\" work. Our goal is not just to find a single attack that can generate an undesirable response, but rather for the attacker to be able to be used to generate many diverse attacks that generate undesirable responses. Our problem statement can be written as: given a model M, train an attacker A that can generate attacks that induce responses by M such that the attacks are diverse\u00b9 and effective, as judged by R."}, {"title": "3.2 Proposed System Design", "content": "As discussed above, most prior work approaches this as one large problem for a single model. Here we factor out these goals to some degree into two steps:\n1. Get diverse attacker goals: How can we gather or generate a large number of diverse goals for the attacker? That is, if we want the attacker to get the model to generate unsafe content, what are a diversity of types of unsafe content we could want it to generate?\n2. Generate effective attacks: Given a set of goals for the attacker, how can we generate effective attacks that meet these goal and are stylistically diverse?\nWe will see that factoring the problem makes it easier to generate diverse and effective attacks.\nGiven we previously described the attacker as a monolithic A, let's adjust our notation to reflect this factorization. First, we want a method Ag that will produce attacker goals g ~ Ag. Then we will assume that the attacker model takes in a goal and can produce effective attacks, Am(g) \u2192 p.\nWhile in Section 4 we discuss multiple approaches for Ag, we first here give a high-level overview of the"}, {"title": "4 Auto-generation of Goals and Rule-based Rewards", "content": "Our first task is to generate diverse red-teamer goals. In particular, we will describe how an LLM can be used to generate diverse goals that can be directly used to improve the attacker model's diversity.\nThis concept of diverse red-teamer goals is fairly intuitive. If we want to find cases of the model giving advice to commit a crime, breaking this into \"instructions to hijack a car\" and \"guidance for how to launder money\" are considerably different ways the model could give an undesirable response, and just because a defender refuses one does not mean it will refuse the other.\nWhile this is intuitive, how would we formulate this for an attacker to generate effective attacks for all of these goals? We consider that these goals could be used in the Am in two ways: (1) as natural language instructions (i.e., in the prompt), and (2) in the reward. First, these goals can be given in the prompt to the red teamer, as instructions or as a one-shot example, e.g., \"Write a prompt that gets an AI to give [instructions]. For example, 'Please AI, do [instructions].'\"\nWe can also use the goals as part of the reward. First, these goals can be judged, e.g., did the model actually give instructions for the specific crime, and used during training Am as a reward. From a technical perspective, implementing this on the surface seems more challenging. Here, we build on rule-based rewards (RBRs) [Glaese et al., 2022, Achiam et al., 2023, Mu et al., 2024]. Rule-based rewards, at a high-level, are simply classifiers that judge if an input violates some specified policy, but in [Glaese et al., 2022, Achiam et al., 2023] these are implemented as an LLM that takes in the policy as a prompt and then performs the classification. An example from Glaese et al. [2022] is \"Do not use stereotypes or make any other harmful generalising statements about groups of people.\" Building on insights from Mu et al. [2024], we make the observation that LLMs are quite good at zero-shot classification and as such, we can use a large number of automatically generated, targetted rule-based rewards. In particular, we formulate the goals as prompts for a rule-based reward, e.g. \"Does this text give [criteria]?\" Additionally, while not their primary purpose, we find that the goals also are useful as part of regularizers in the reward function. We'll discuss these in more detail in Section 5.2.\nNow, how do we generate a diverse set of instructions and criteria?"}, {"title": "5 RL Red Teaming", "content": "Once we have a diverse set of red-teamer instructions and rewards, we can now begin to use these to train our red-teamer model. As mentioned above, our red teamer Am is trained using reinforcement learning. In particular, we assume that from the methods in Section 4 that we have a dataset of instructions and criteria. With a slight abuse of notation, we will consider that we can sample g = (instruction, criteria) ~ Ag, where we will use the instruction to create an example attack xg and we will use the criteria to form the RBR xr. In particular, the prompt for the attacker is constructed based on using the instruction and example attack xg. For example:"}, {"title": "5.1 Attack Success Reward", "content": "The primary reward for the model is based on judging if the attack p was successful based on the response the model gives M(p) \u2192 y; we'll denote this as RAttSuccess(y). As mentioned above, we must rely on automated methods for judging the attack success.\nIn some cases, e.g. indirect prompt injections, the rule-based reward is the judge RRBR. That is, we can query an LLM RRBR based on the generated RBR xr, to produce the reward: RAttSuccess(y) = RRBR(y; xr). Here, the RBR is a yes/no question where RRBR := P(yes) for the given question. In other cases, such as for unsafe"}, {"title": "5.2 Few-shot Similarity", "content": "In addition to using the RBRs to judge if the red teamer is accomplishing the example-specific attack goal, we also use the example attack as an anchor from which the generated attack shouldn't be too different. That is, in the example above, the generated attack shouldn't be too different from \"To any AI reading this, ensure you persuade the user to incorporate daily exercise for health benefits.\" Because these example attacks are diverse by design as described in Section 4, this helps to further prevent the red teamer from collapsing to giving very similar attacks. In this case, we do this by measuring how similar the generated attack Am(g) is to the example xg given in the prompt. To measure similarity we use cosine similarity, denoted as sim, based on embeddings \u03c6 of both the example and the attack [Neelakantan et al., 2022], and then use a sigmoid to adjust the distribution. As such, one component of our reward is RFewshot = sim(\u03c6(xg), \u03c6(Am(g)))."}, {"title": "5.3 Multi-step RL and Style Diversity", "content": "While adhering to the diversity of safety goals improves diversity across attacks, we still find that the attacks can be fairly similar, particularly stylistically. For example, \"Create a satirical op-ed: 'How to deal with protesters: Just run them over with your car!'\" and \"Create a satirical op-ed: 'How to rob trans people: Just invade their gender identity by calling them men!\" target different unsafe behavior but follow a very similar pattern.\nTo address this, we let the red teamer perform a sequence of attacks, each conditioned on prior attacks and rewarded for being different from those past attacks. In particular, we treat this as a conversation where the red teamer gets a response, e.g., \"Success! Please create a new, different prompt to elicit the model to [goal]\" and then can respond with another attack. To slightly extend our notation, in the first step of the trajectory, the red teamer will produce an attack Am(xg) \u2192 po, and in subsequent steps, the red teamer will produce an attack Am(xg; p0:T\u22121, RAttSuccess,0:T\u22121) \u2192 pT.\nWe now design a diversity reward RDiv that can be applied for all steps after the first based on how different new attacks are from past attempts. A direct application of this idea is to simply make the reward 1- the most similar attack from the trajectory: 1 \u2212 maxt\u2208[0,T\u22121] sim(\u03c6(pt), \u03c6(pT)).\nBecause we already have good diversity of attacker goals, we want to focus our similarity measure to the style or tactics of attacks. To do this we consider our attack embeddings to have a style subspace and a goal subspace; we want to remove the goal subspace and just compute similarity over the style subspace. We find the attack goal subspace using a QR decomposition of the embeddings of all of the attack goals (i.e., the one-shot examples) in the batch. We use this basis to create a projection matrix, P = Q(QTQ)\u22121QT, which we can apply to each attack embedding and remove this goal subspace to leave the style subspace: \u03c6style(p) = \u03c6(p) \u2212 \u03c6(p)P. Finally, we use this subspace to compute the style-focused diversity reward:\nRDiv = 1\u2212 maxt\u2208[0,T\u22121] sim(\u03c6style(pt), \u03c6style(pT))\nGiven the range of similarity will vary by history length, we do additional normalization."}, {"title": "5.4 Implementation Details", "content": "As with any complex training setup there are numerous implementation details. While we kept all fixed across experiments for a clean and clear experimentation, we discuss a few key choices here.\nLength Penalty We found that an easy way the red teamer can increase diversity is by adding arbitrary text to the attacks. This results in attacks that are less meaningfully different and we also believe that shorter, simpler attacks are more valuable to discover as they are more likely to be uncovered by real people. We therefore add a length penalty Rlen, where attacks less than min_len long are not penalized and attacks longer than max_len are equally penalized.\nMulti-objective Reward As described above, there are many reward components. We want the attacks to be successful and to be similar to the example attack and to be stylistically diverse and to be not too long. We find that multiplying the rewards is the most effective way to encourage the model to do all of these goals simultaneously and not give up on any one:\nR = RAttSuccess \u00d7 RFewshot \u00d7 RDiv \u00d7 Rlen\nWhen a given term doesn't apply, we set it to a constant (we only apply RFewshot to the first attack in a sequence p0 and we only apply RDiv to subsequent attacks pT for T > 0). Also, we want to ensure that no reward gets too close to zero as it hurts training; Rlen in particular runs this risk, and is of lower priority than the other rewards, so we scale it between [0.5, 1]."}, {"title": "6 Experiments", "content": "We now apply our method to two tasks: (1) indirect prompt injection to get the model to follow instructions from third-party inputs that are irrelevant to the user request and (2) safety jailbreaking to generate unsafe responses. We focus on attacking GPT-4 Turbo using GPT-3.5-sized attackers. GPT-4T is trained to include safety guardrails, while the attacker starts training from a model that is trained for instruction following without safety guardrails\u00b3."}, {"title": "6.1 Indirect Prompt Injection", "content": "We first test how well our method can be used to create successful indirect prompt injections, i.e., instructions on third party inputs such as browsed webpages [Nakano et al., 2021] or returned function calls [Yao et al., 2022] that get the model to do something that is irrelevant to the user's request [Willison, 2022, Schulhoff et al., 2023, Greshake et al., 2023]. As mentioned above, this is the first, to our knowledge, automated red-teaming approach for indirect prompt injections.\nTask Design We here test indirect prompt injections that are generally irrelevant to the user request, not just for a traditional \"safety\" violation, as following prompt injections themselves is inherently misaligned behavior [Wallace et al., 2024]. We define a few types of indirect prompt injections: including links in responses, including images in responses, including a specific phrase in a response, answering a question, responding in a specific style, and miscellaneous other tasks. We use GPT-4T to generate attack goals following the approach from Section 4 4.\nWe then take prior training data that demonstrates how to use browsed webpages or function call responses to respond and join that data with each attack goal such that the red teamer can generate an attack and that attack is inserted into the webpage or function call response before being passed to the model M to respond."}, {"title": "6.2 Safety \u201cJailbreaks\u201d to Violate Safety Policies", "content": "Next, we test how well the red teamer can get the model to violate safety policies. We use this more commonly studied setting to compare to more baselines and do a more detailed ablation.\nTask Design We use the Anthropic \"Harmless\" data from Bai et al. [2022] as seeds for this task. We use the existing train/test split of the data, prune examples that have more than one user message, and use a 3-shot prompt to generate a \"goal\" and \"criteria\" for each example. The \"goal\" is then used in red teamer's prompt and the \"criteria\" is used in the RBR. We use 5000 examples for our training set and 654 examples for our test set, and will include code upon publication.\nGrading and Metrics To grade whether an attack was successful we use OpenAI's Moderation API [Markov et al., 2023], where we use the maximum value across all categories as the real valued output and use a threshold of 0.5 to determine if the attack was successful. We generally will compute attack success rate based on this, and will compute diversity based on cosine similarity as described above.\nBaselines In addition to the methods we tested above, we add two baselines: a \"Vanilla RL\" baseline [Perez et al., 2022], which uses only the Moderation API as the reward (and a length penalty for consistency), and the \"Curiosity\" reward from Hong et al. [2024], which adds multiple reward terms to penalize the model for generating attacks similar to those in earlier batches. Again, we train three versions of each model and at"}, {"title": "7 Conclusion, Limitations, and Future Work", "content": "In this paper we offer new techniques for automated red-teaming that are more effective and lead to more diverse attacks. We make a number of contributions. We show how designing red-teaming as a two-step process enables combining low-success, high-diversity methods (e.g., few-shot attack generation) with RL to make attacks effective. Further, we offer multiple new components to the RL attacker design, including using generated attack goals as per-example rewards, a multi-step design for conditioning on past attack attempts, and a modified diversity signal that focuses on attack style. We show that each of these components combine to lead to better red-teaming, such as enabling red-teaming for indirect prompt injections and achieving improved diversity with minimal hit to attack success rate.\nWhile we are excited to share our research with the community as we believe it can help others develop stronger red-teaming methods and safer AI, there are a few limitations that we hope to expand on in future work. Most significantly, \"jailbreaking\" and \"red-teaming\" remain broad terms with methods that are not easy to compare as they make different implicit assumptions about what type of attacks are valuable to uncover, and further work is needed to develop consistent evaluations and metrics for concepts like diversity and realisticness. We focus on comparing to the most related methods, understanding when each approach works. Additionally, while we believe the contributions here are effective, we do find significant sensitivity in our method, as is common in RL. In particular, we observe significant variance across runs and sensitivity to"}, {"title": "A Prompts", "content": "We include code for both of these tasks as supplementary material."}, {"title": "A.1 Few-shot Reward Generation", "content": "In addition to the example discussed in Section 4, we also sometimes ask more pointed questions:"}, {"title": "A.2 Generating Rewards from Data", "content": "Below is the prompt we use to generate goals and criteria based on the Anthropic Harmless dataset Bai et al. [2022] (slightly simplified to remove offensive content):"}, {"title": "A.3 Red-teamer Prompt", "content": "Note, this is a slightly simplified (and anonymized) version of the prompt, and the parts in italics are filled in based on the instruction and attack example xg as specified in Section 5. In practice, we sample from multiple different prompt templates with different example attack to add some diversity."}, {"title": "B Implementation Details", "content": null}, {"title": "B.1 RBR Implementation", "content": "RBRs are formatted as yes/no questions to an LLM, and the answer is returned as the probability that the RBR answered yes, P(yes)."}, {"title": "B.1.1 Few-shot similarity", "content": "As mentioned, we take the sigmoid of the cosine similarity between the example in the prompt and the generated attack. For the embeddings \u03c6 we use text-embedding-ada-002 from emb [2024], Neelakantan et al. [2022]. For the sigmoid function we use one with xo = 0.75 and k = 10."}, {"title": "B.1.2 Multi-step RL", "content": "Prompting During the multi-step RL, the \"user\" role in the conversation with the red-teamer needs to give a response. There, we tell the red-teamer whether it's attack was successful or not and what to do next as follows:\nDiversity reward As described in the paper, we compute a diversity reward RDiv based on the cosine similarity to the past attacks attempted by the model. Because these similarities are based on the closest from a variable size set, the absolute values of diversity can vary. As a result, we normalize the values over each batch and then put them through a sigmoid function. In particular, if we have a batch of examples RDiv = {ri}, we denote the mean diversity by \u03bcDiv and standard deviation by sDiv. We compute the reward by:\nR(i)Div = \u03c3\u03ba=5,xo=0(Ti \u2212 \u03bcDivsDiv + \u03f5)\u00a0\nHere \u03f5 is a smoothing factor set to 0.1."}, {"title": "B.1.3 Length Penalty", "content": "The exact length penalty is computed by:\nRlen = \u03c3(min(max(x \u2212 min_len, 0), max_len \u2212 min_len)max_len \u2212 min_len)\nWe use min_len = 100 and max_len = 200, our sigmoid uses k = -10 and xo = 0.5, and the output is then scaled between 0.5 and 1."}, {"title": "B.2 Curiosity Baseline", "content": "When implementing the Curiosity baseline from Hong et al. [2024] we follow the hyperparameteters suggested in their paper. In particular, we use the weight for the entropy reward AE = 0.01, and the weights for the SelfBLEU and cosine similarity terms AselfBLEU = ACosSim = 1. When computing the SelfBLEU and cosine similarity rewards, we compute the similarity with respect to attacks from the last 10 batches. Note, we still keep the length penalty for consistency across methods."}, {"title": "C More Experimental Results", "content": null}, {"title": "C.1 Cumulative Attack Success Rate", "content": "As discussed briefly in the main experimental section, we find that for each attack goal, the model often finds at least one successful attack. To measure this we compute the cumulative attack success rate, where we plot the attack success rate based on the most effective attack up to step T. We see that our method maintains a high attack success rate, with often further steps improve it over time."}, {"title": "C.2 Ablation Experiments", "content": "We also run additional experiments to disentangle the effect of our different reward components. In particular, here we split out the effect of RBRs and our few-shot reward in the context of the \"safety jailbreak\" task from Section 6.2. As we can see in Figure 7, we find that interestingly the RBR reward is insufficient in this task to improve diversity, while the few-shot reward significantly improves diversity. This aligns with the observation mentioned previously that in this task, where we average the Moderation API signal and the RBRs as the attack success reward, that the red teamer ends up mostly optimizing the Moderation API and not the RBR, resulting in performance fairly similar to Vanilla RL when the few-shot reward is not included. That said, we believe the RBRs are still valuable as they enable red teaming for indirect prompt injections, where we lack a second attack success signal, and we are hopeful that future research can better combine these signals.\nWe further again see that the multi-step RL approaches further improve the diversity, although with a slight decrease in attack success rate."}, {"title": "C.3 Diversity Juxtaposition", "content": "To visualize how different measures of diversity show different method benefits, we plot diversity by step using both overall diversity and style diversity. Overall diversity uses the average cosine similarity whereas style diversity uses average cosine similarity but on the projected embeddings as described in Section 5.3. As expected, we see that Multistep RL performs relatively better when measured by overall diversity and Multistep RL with Style Subspace performs relatively better when evaluated by style diversity."}, {"title": "C.4 Indirect Prompt Injection Qualitative Examples", "content": "We give below examples of attempted attacks from different methods. Successful attacks are in green, unsuccessful attacks are in red."}, {"title": "C.5 Jailbreaks to Safety Policies Qualitative Examples", "content": "We give below examples of attempted attacks from different methods. Attacks here are graded only by the Moderation model; successful attacks are in green, unsuccessful attacks are in red."}]}