{"title": "From Models to Systems: A Comprehensive Fairness Framework for Compositional Recommender Systems", "authors": ["Brian Hsu", "Cyrus DiCiccio", "Natesh Sivasubramoniapillai", "Hongseok Namkoong"], "abstract": "Fairness research in machine learning often centers on ensuring equitable per- formance of individual models. However, real-world recommendation systems are built on multiple models and even multiple stages, from candidate retrieval to scoring and serving, which raises challenges for responsible development and deployment. This system-level view, as highlighted by regulations like the EU AI Act, necessitates moving beyond auditing individual models as independent entities. We propose a holistic framework for modeling system-level fairness, focusing on the end-utility delivered to diverse user groups, and consider interactions between components such as retrieval and scoring models. We provide formal insights on the limitations of focusing solely on model-level fairness and highlight the need for alternative tools that account for heterogeneity in user preferences. To mitigate system-level disparities, we adapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize utility and equity. We empirically demonstrate the effectiveness of our proposed framework on synthetic and real datasets, underscoring the need for a system-level framework.", "sections": [{"title": "1 Introduction", "content": "The prevailing focus in algorithmic fairness is on bias measurement and mitigation for individual prediction models as the unit of analysis [5, 34, 13, 52]. However, industrial applications of ML rarely train and serve a single model in isolation: individual models are components of a broader system. The recent EU AI act defining its scope of a system as \"a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\" This system-level perspective raises several questions for responsible development and deployment of recommendation systems. Are fairness notions for individual models sufficient to provide system-level equity?\nWe study system-level fairness in industrial recommendation systems, which are composed of mul- tiple layers of ML models. In Figure 1, we illustrate a schema utilized by many of the largest tech companies in the world. Using a job recommendation system as a running example, the ultimate goal is hiring, as measured by the 'confirmed hire rate' (see the left side of Figure 2). However, this system-level objective is notably misaligned with the short-term predictions of individual models"}, {"title": "Multi-objective black-box optimization for system-level fairness", "content": "In most settings, the perfor- mance of individual ML models can be optimized \"off-line\" based on previously collected user data using standard tools like cross-validation. On the other hand, the FPR and SPR stages (Fig- ure 1) require \"online\" experimentation (A/B testing) to collect user feedback on the quality of weights/parameters \u03b1. In the latter, weights are typically tuned through manual trial & error or black-box optimization methods such as Bayesian Optimization (BO) [19]. BO allows optimizing over noisy user feedback without derivative information, and is commonly used in online platforms (e.g., Meta [28] and LinkedIn [1]).\nWe go beyond identifying the source of disparities in utility, and formulate the overall system design as a multi-objective optimization problem balancing social welfare and fairness. Since it is difficult to model system-level objectives using a particular functional form, we treat them as a black-box and model it using a flexible Gaussian process. We use a simple but effective variant of a Bayesian Optimization (BO) algorithm to simultaneously maximize social welfare minus the disparity between the utility across groups. While the algorithms we leverage are well known, our formulation demonstrates how familiar tools from black-box optimization can be utilized in a practical and meaningful way to improve system-level fairness.\nTo further contextualize this in our job-recommendation setting, suppose that job preferences differ across demographic groups for jobs where one group prefers seeing attractive sounding jobs (click- worthy jobs), while another group prefers jobs of better background fit (apply-worthy jobs). When we have one model for clicks and one for applies and serve recommendations as a weighted linear combination of these model outputs, the \"population weights\" are unknown and are typically chosen \"globally\" in the sense that all users receive the same weights. Yet, the ideal weights from a user experience perspective largely depend on individual preference, which traditional fairness metrics fail to capture. While weights can be personalized to certain demographic groups, this introduces disparate treatment concerns [30, 43] and thus it is common practice to select a single global weight \u03b1. As we demonstrate on the right side of Figure 2, these global design choices have a tendency to tailor to the preferences of majority groups when using off-the-shelf tools."}, {"title": "Related work on viewer-side fairness in compositional systems", "content": "In this work, we focus on viewer- side fairness, which studies the disparity in utility a recommender system provides to users. In particular, this ignores the agency of the items being recommended: if items represent human agents and/or their products (e.g., creators and their content), a more holistic approach is required that goes beyond the scope of the current paper.\nFor fairness at an individual model-level, several authors focus on group-wise disparity of ranking performance metrics like AUC, NDCG, or F1, ensuring \u201cminimum quality of service.\u201d Examples include audits at Twitter ([33]), Microsoft ([35]), and LinkedIn ([41]), with comprehensive surveys by Li et al. [29], Wang et al. [55], Raj and Ekstrand [42], and Ekstrand et al. [18]. We root our analysis on the spirit of viewer-side fairness concepts in this work and focus on the group-wise disparity in overall viewer utility.\nFairness in compositional systems been studied from several perspectives; we summarize our key distinctions from existing works here and provide further technical details and references in Appendix A. Our work differs from existing research on compositional fairness in two key aspects: (1) we propose a novel multi-label fairness framework grounded in practical utility modeling, departing from prior approaches that propose transformations from the multi-label setting to a single-label setting [16, 53, 3]. (2) While fairness in ML pipelines has previously been studied [8, 17, 7, 26], we take this one step further and jointly analyze the filtration/selection layer in conjunction with the serving layer. We use this perspective to emphasize the need for fairness interventions at this stage by showing how it can represent a bottleneck in achieving fairness, even if everything else is fully fair."}, {"title": "Limitations", "content": "Our main contribution is formulating and crystallizing the notion of fairness at the system-level, providing structural insights on how individual components can be optimized to achieve this goal. While formalizations provide practical value through concrete definitions and operational algorithms, our approach is inherently limited as system-level equity cannot be reduced to a single number. We highlight the need for multi-faceted approaches where i) stakeholders of the discrete components of the system collectively have incentives to prioritize equity at the system-level, and ii) a central organization coordinates system design with fairness as a central concern. Lastly, we make technical assumptions in Section 2 to focus on the single period interaction (rather than temporal) and focus on a specific variety of distribution shift. Handling these assumptions would introduce further granularity into the analysis that are tangential to our main message about system-level fairness and we consider these ideas as a topic for future study."}, {"title": "2 User Utility Under Compositional Recommender Systems", "content": "We first argue that responsible AI in recommendation systems should prioritize downstream user utility over individual model performance metrics. Although previous works have advocated for utility-centric fairness ([58], [59], [45]), none have extended this to compositional systems where utility depends on multiple labels. Crucially, reducing disparities in single-model metrics (e.g., NDCG, AUC) may not improve system-level fairness, and proxy metrics used by individual models often misalign with the system's ultimate utility goal. Thus, we directly optimize for fairness in downstream utility rather than defining new composite metrics for translating fairness into the single-model domain."}, {"title": "2.1 Framework for Compositional Utility", "content": "We formalize the notion of system-level utility we explore in this work. In a recommendation system with M items in its entire corpus (typically in the order of millions), let Z be the feature representations of each item j = 1, ..., M. When a user initiates a session with covariates X, the item/candidate retrieval step selects m < M relevant items; let $I(X, Z^i) \\in {0,1}$ be the indicator for whether $Z^i$ is selected. In practice, item $Z^i$ may pass through rules-based filters or be chosen via a m-nearest neighbors algorithm.\nFor each item j and user (query) X, we have K different observable user outcomes $Y_k$ for k = 1,..., K (K is usually \u2264 10, e.g., clicks, likes). These outcomes represent different aspects of item quality tracked by a domain expert (e.g., a product manager) who hypothesizes each $Y_k$ is a proxy of the downstream utility metric U that they are tasked to improve (e.g., engagement, network growth). ML models $f_k: X, Z \\rightarrow Y_k$ provide predictions of each outcome, which is combined with weights $\u03b1_k > 0$ to recommend the best item j among the candidate set.\nDefinition 2.1 (Best Item for Serving). Given a universe of M items, serving preferences $\u03b1_k > 0$, model scores $f_k(X, Z^i)$, and retrieval function $I(X, Z^i)$, the recommended item j is given by\n$j^* := \\underset{1 \\leq j \\leq M}{\\text{argmax}} \\\\{\\sum_{k=1}^{K} \u03b1_k f_k(X, Z^i)I(X,Z^i) \\\\} $\nPractitioners commonly use the weighted sum formulation (1) for interpretability and ease of adjust- ment. Even if the linear sum does not perfectly match user preferences, groups of users conceptually have an optimal set of weights within the linear model class. The universality of this model indicates that weighted sum is often viewed as a reasonable approximation of true user utility. Our framework generalizes to compositional functions (products, maxima), regression models, and settings involving negative utility (e.g., likelihood of abusive content) by simply adding more preference parameters. We focus on the top-1 item for ease of exposition; our analysis can be extended to top-k setting but in this case, modeling position bias across ranks k is an important direction for future work.\nTo formalize user utility, we give the system designer the utmost benefit of the doubt by assuming that their selection model is \"well-specified\". We shall see in the next section that even when we assume user utilities reflect the beliefs of domain experts, there can be large discrepancies between individual-model vs. system-level fairness.\nDefinition 2.2 (User Utility). When item j is recommended, user X utility is given by the conditional expectation of the weighted sum under true outcomes and true preferences $\u03b1^*(g)$ for group $g \\in G$\n$U_g(X, I, f, \u03b1, \u03b1^*) = E \\\\{\\sum_{k=1}^{K} \u03b1_k^*(g)Y_k \\, | \\, X\\\\} $\nHere, we implicitly assume X is rich enough that $\\{Y_k\\\\} \\, | \\, X$ is invariant across demographic groups $g \\in G$ and instead model group-level heterogeneities through the true preference vector $\u03b1^*(g)$. Although users typically engage with the system multiple times, we treat them as i.i.d. in this work. We highlight this as a major limitation of our work as realistically, even a single bad experience can turn a user away from the platform.\nUsing this formalization, we can identify assumptions for when serving recommendations with $\u03b1^*(g)$ is a maximizer of user utility. The conditions we require are that the scoring models $f_k$ are fair in the sense that they are calibrated across the entire feature space. Calibration is an extensively studied topic in fairness literature [24, 9, 31, 15]; though typically defined in case of candidate-side fairness, the same takeaways hold for the viewer-side case that we analyze.\nLemma 2.3. Suppose individual models $f_k(X, Z^i)$ are calibrated with respect to their intended label $Y_k$ across the entire feature space: $E \\\\{Y_k \\, | \\, f_1(X, Z^i), ..., f_K (X, Z^i) \\\\} = f_k(X, Z^i)$ a.s. and that true and serving preferences are positive $\\{\u03b1^*, \u03b1\\} > 0$. If item j has a nonzero probability of being retrieved $P (I(X, Z^i) = 1) > 0$ whenever $P (f_k(X, Z^i) = \\cdot \\, | \\, X, Z^i) > 0$ a.s., then setting $\u03b1 = c \\cdot \u03b1^*$ for $c > 0$ is a maximizer of utility.\nThe lemma shows the validity of our framework in that learning true preferences indeed optimizes util- ity. We also qualitatively consider the role and impact of unobservable outcomes (e.g. sentimentality) in Appendix D."}, {"title": "3 Structural Insights", "content": "We now provide structural analyses of the utility gap between groups to answer the following question: when is individual model fairness (in)sufficient for system-level fairness? We articulate the different sources of utility-based inequity, and show the causes for utility gaps can be different compared to the standard causes of individual-model fairness. In particular, users who have the same features X look identical to the platform, but they may exhibit heterogeneous preferences (and therefore utilities) across groups. For example, different demographic groups may not respond identically to the same job posting despite similar backgrounds.\nWe consider two groups G = 0 and G = 1 to illustrate, and use $E_g[\\cdot]$ to denote the expectation with respect to P(X = \u00b7|G = g). Focusing on the user's short-term experience after a single interaction with the recommendation system, we analyze the utility gap\n$E_1 [U_1(X, I, f, \u03b1, \u03b1^*)] \u2013 E_0 [U_0(X, I, f, \u03b1, \u03b1^*)]$.\nWe assume the product does not personalize models specifically to demographic groups at any stage due to disparate treatment concerns treating users differently based on their demographics. Without loss of generality, we denote group 0 to be the disadvantaged group.\nOur goal is to perform an apples-to-apples comparison between demographic groups by \"con- trolling\" for the effects due to the user features X. That is, we wish to compare the gap utility $U_1(X, I, f, \u03b1, \u03b1^*) \u2013 U_0(X, I, f, \u03b1, \u03b1^*)$ for users that only differ in their group memberships. How- ever, such a comparison is only possible over users co-observed in both groups. Thus, we define a notion of a \"shared space\" between P(X|G = 1) and P(X|G = 0)\n$S_X(x) \\propto (p(x|G = 1) + p(x|G = 0))^{-1}p(x|G = 1)p(x|G = 0)$,\nso that $S_X$ is small whenever either p(x|G = 1) or p(x|G = 0) is small and large when both quantities are large. Intuitively, taking expectations over $S_X$ means we are paying attention to the feature space where both groups are present (e.g. industries $X_1$ where both demographics G = 0,1 are represented). This mirrors Cai et al. [10]'s distribution shift decomposition approach.\nWe expand the difference in expected utility as follows and provide some high level intuition:\n$E_1 [U_1 (X, I, f, \u03b1, \u03b1^*)] \u2013 E_0 [U_0(X, I, f, \u03b1, \u03b1^*)]$\n$= E_1 [U_1(X, I, f, \u03b1, \u03b1^*)] \u2013 E_{S_X} [U_1(X, I, f, \u03b1, \u03b1^*)]$\n$+ E_{S_X} [U_1(X, I, f, \u03b1, \u03b1^*) \u2013 U_0(X, I, f, \u03b1, \u03b1^*)]$\n$+ E_{S_X} [U_0(X, I, f, \u03b1, \u03b1^*)] \u2013 E_0 [U_0(X, I, f, \u03b1, \u03b1^*)]$\nTerm (4) relates to the utility change from the feature distribution of group 1 (P(X|G = 1)) to the shared distribution (3). It is large when the utility gap can be attributed to user features X often seen in group 1 but not in group 0. For instance, this may imply that the Al system provides better recommendations in male-dominated industries such as construction.\nTerm (5) compares the utility gap between groups over $S_X$. This term is large when the AI system favors the preferences $\u03b1^*$ of the majority group 1. Finally, Term (6) is large if the AI system works better for users who are common in both groups compared to those only in group 0."}, {"title": "3.1 Impact of Preference Misspecification", "content": "We now take a closer look at Term (5). Further decomposition of these terms in Theorems 3.1, 3.3 to come unveil the impact and limitations of individual model fairness. We show that misspecification of user preferences in the serving model and disparities in the quality of the embedding model can also be significant drivers of utility gaps.\nWe analyze the \"apples-to-apples\" comparison in Term (5), which provides the most intuitive notion of \"unfairness.\" This implies that the quality of recommendations is unequal even when two users from different groups share the same covariates (e.g. in interest and past behavior). We show this gap can occur at the system-level even if the individual models are fair. Below, the residuals $|Y - f_k (X, Z^i)|$ measure model performance of $f_k$, where as $|\u03b1_k - \u03b1_k^*(0)|$ and $|\u03b1_k - \u03b1_k^*(1)|$ denote the estimation error in preferences of the two groups."}, {"title": "3.2 Downstream Impact of Upstream Candidate Selection Models", "content": "We now focus on Term (4), and move upstream of the scoring models to assess the impact of candidate retrieval model $I(X, Z)$ as a driver of utility gap. In practice, the candidate retrieval model is separate from the ML models and SPR serving layer and may even be managed by a separate engineering team. Retrieval models are often designed to optimize offline retrieval metrics such as recall over a single outcome (e.g., Click). Since engineering considerations such as latency, memory, and performance drive retrieval model design [40, 49, 47], fairness considerations are generally underappreciated.\nWhile understanding user preferences over multiple labels is key to maximizing utility, candidate retrieval evaluations do not typically consider multiplicity of labels: recall over a single label may not align with the important label from the user's perspective. We address this by proposing a retrieval quality metric that we find more suitable for the compositional model system.\nDefinition 3.2 (Candidate Retrieval Model Quality). A \u03b3-good item j satisfies $E \\\\{\\sum_{k=1}^{K} Y_k \\\\} > \u03b3$. The quality of a candidate retrieval model $I(X, Z)$ selecting m items is the expected highest \u03b3-good item it can retrieve from the candidate pool. Formally, recalling $I(X, Z^i)$ is the indicator for whether item $Z^i$ is retrieved for user X, the quality metric for the candidate retrieval model is\n$Q_m(I(X, Z), Y) = E \\\\{ \\underset{j \\in \\{1, ..., m\\\\}}{\\text{max}}  \\\\{\\sum_{k=1}^{K} Y_k(Z_j) \\\\}  \\\\} $\nNotably, this definition differs from the standard definition of recall (true positives over total positives). This distinction is crucial for two reasons. First, this definition now spans multiple labels that the business ultimately knows are relevant for user preferences. Second, we focus on the maximum because the downstream model and SPR layer are designed for surfacing the best item from the retrieved candidates.\nWith this definition in hand, we relate disparity in retrieval quality to that of utilities. We show that even if fully optimize utility with respect to everything downstream of the candidate retrieval model $I(X, Z)$, namely the scoring models $f_k$ and serving coefficients $\u03b1_k$\u2014the utility gap is still bottlenecked by the quality of the candidate retrievals."}, {"title": "4 System-Level Fairness Via Bayesian Optimization", "content": "We now shift our focus from identification of fairness gaps to its mitigation. Based on our discussion in Section 3.1, a significant contributor of the utility gap is over-representation of the majority group in selecting the preference weights \u03b1. We formulate the selection of \u03b1 as a derivative-free closed-box"}, {"title": "4.1 Selecting an Inequality Metric", "content": "Prior works at the intersection of BO and fairness have generally centered on finding hyperparameters that yield a fair model based on a standard fairness definition such as demographic parity or equalized odds (see [38], [56], [12], [44] for examples). However, we are specifically proposing to forgo these standard definitions in favor of utility-based fairness. While it becomes tempting to directly take the exact term we analyzed (average utility gap across the groups) as a measure of inequality, there are various issues such as not being scale-invariant, the need to designate a specific disadvantaged group, and inability to compare across more than two groups, that make it unsuitable for our application.\nInstead, we leverage the recently proposed metric Deviation from Equal Representation (DER). Friedberg et al. [20] introduce the metric as a way of quantifying the disparate impact of experiments. For k groups with non-negative downstream outcomes $\u03bc_1,\u00b7\u00b7\u00b7, \u03bc_K$ (representing average number of sessions/confirmed hires per group), the DER is defined \u00b9 as the following metric, which is scale invariant and naturally extends to multiple groups.\n$D(\u03bc_1,\u00b7\u00b7\u00b7, \u03bc_K) = 1 - \\frac{k}{k-1} \\sum_{i=1}^{k} \\frac{\u03bc_i}{K \\, \u03bc_k}$\nWhen all means are equal, D(\u00b51, \u00b52) = 0 and otherwise the statistic grows larger when the values grow more disparate. Friedberg et al. [20] uses this for gauging if an experiment has unintended consequences (e.g., disproportionately benefits one group) that are not identifiable by looking at a global metric change. Similarly, our system-level fairness perspective aims to ensure the overall utility gains are not due to a disproportionate gain in the majority group and a drop in minority group. We use DER to integrate fairness considerations into the Bayesian optimization process."}, {"title": "4.2 Incorporating Deviation from Equal Representation Into Bayesian Optimization for System-Level Fairness", "content": "A common strategy to considering fairness in BO is to use a constrained version of the expected improvement (EI) methodology by Gardner et al. [21]. Here, the performance and fairness criterion are parameterized as separate Gaussian Processes (GP) over the decision variable \u03b1 (preference weights). We briefly recap these strategies and explain why we opt for a different, but related procedure. Originally, the EI approach [19] selects the next point optimizing the following quantity at step n\n$EI_n(\u03b1) = E_n [[f(\u03b1) \u2013 f^*]_+]$.\nThe \"fair variant\" of the algorithm modifies this objective by introducing an indicator variable $I(\u03b1)$ parameterized by a GP c(\u03b1) to represent if a point \u03b1 satisfies the constraints or not. Then, by assuming independence between the constraint and objective, Gardner et al. [21] presents the constrained objective $EIC_n$ where \u03b3 is a hyperparameter that denotes the slack on the constraint c(\u03b1) \u2013 \u03b3 \u2264 0.\n$EIC_n(\u03b1) = E_n [[f(\u03b1) \u2013 f^*]_+ \\cdot I(\u03b1)] = E_n [[f(\u03b1) \u2013 f^*]_+] \\cdot E_n [I(\u03b1)] = EI_n(\u03b1) \\cdot P(c(\u03b1) \u2264 \u03b3)$\nThis formulation is interpretable and maintains all the computational advantages of the standard EI method. Prior works demonstrate the efficacy of this method for finding hyperparameters that optimize for performance while satisfying fairness constraints [38]. However, the problem with directly applying this method by parameterizing DER (7) as a GP c(\u03b1) is twofold. First, DER clearly is not independent of the original objective of maximizing the overall downstream metric. This voids the mathematical support behind the constrained EI. Second, picking a threshold \u03b3 for DER is nontrivial. Realistically, the business is more interested in finding the best of both worlds by improving DER and the overall average utility metric simultaneously."}, {"title": "5 Experiments", "content": "We now demonstrate our proposed multi-objective- optimization formulation of utility and DER as a simple but effective method for achieving system- level fairness. Due to the lack of public bench- marks with readily available multi-action outcomes to credibly represent an industrial system, we re- strict our analysis to four datasets MovieLens([23]), ModCloth([51]), Electronics([51]), and one syn- thetic example. These datasets represent varying lev- els of difficulty in terms of model performance, util- ity optimization, and DER optimization. Details are provided in Appendix C. Importantly, we start with models that are groupwise fair, which is typically the end-goal of fairness studies. We then simulate group- wise preference disparities and use BOTorch [4] for iterative Bayesian optimization to mimic online rec- ommendation systems. Each iteration samples users, retrieves m items per user, and computes utility based on true labels and preferences as defined in Definition 2.2."}, {"title": "5.1.1 Results and Comparison", "content": "For each dataset and method, we aggregate the utility and DER over the 20 iterations for each of the 20 trials. We trace the Pareto frontier for each method across the trials, and then plot the average utility and DER to represent the average-case performance with 1 stdev error bars. For statistical significance, we report the p-value of the Wilcoxon signed rank test comparing our Fair EHVI against the baseline (random search) and pure utility optimization (EI). We consider random search the baseline, as even simple BO processes like EI are non-trivial implement in industry and therefore may not be available. All experiments were run locally on a MacBook Pro with an M3 processor.\nIn Figure 3, we see that Fair EHVI overall beats both random search and EI in identify the best tradeoff between utility and DER. For a more detailed view of performance on each dataset, we turn to Figure 4. The plots demonstrate that Fair EHVI consistently yields the most points on the global pareto frontier of utility and DER. Fair EHVI also yields better DER than random search and EI by a statistically significant margin in all cases, even in the difficult Electronics dataset where random search is difficult to beat. Our methodology does suffer in that dataset where the average utility is slightly lower than random search, but this can potentially be overcome with more tuning of the search parameters (as our algorithm samples EHVI candidates in parallel and optimizes for DER)."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we have proposed a mechanism for shifting from model-centric to system-level fairness in compositional recommender systems. We align fairness measurement with user utility optimization, spotlight the retrieval and serving layers as critical points of intervention, demonstrate the benefits and limitations of intervention at these layers, and propose a Bayesian optimization solution for bias mitigation at serving time. Our empirical results show improved utility distribution across heterogeneous user groups. As regulations like the EU AI Act emerge, such holistic system-level analysis becomes crucial for responsible AI practices.\nFor future work, one critical direction is understanding how the timescales of utilities affect fairness. For instance, while we have assumed that either all users are unique or that user sessions are i.i.d, this is generally not true. High utility in one session begets further usage and low utility begets strong drop-off. While these patterns can only be empirically estimated, folding them into our fairness framework adds the timescale dimension of how long unfairness is tolerable by users, further motivating the urgency of the problem. Additionally, as we have framed system-level fairness for the viewerside in this paper, we encourage researchers to understand the analagous problem but for systems that rank candidates and the interaction between the two types of fairness."}, {"title": "6.1 Acknowledgments", "content": "We sincerely thank Sam Gong and Will Cai for their insightful feedback and in-depth conversations about system-level fairness. We would also like to thank Sakshi Jain and Heloise Logan for their support and Kinjal Basu for his improvements on the DER metric. Finally, we thank the anonymous reviewers for their helpful comments on notation improvements and for providing references to expand the discussion of related works."}, {"title": "A Appendix / Related Works", "content": "To recap Section 1, the two core distinctions that we present from existing work are in:\n\u2022 Formulating a definition of multi-label fairness that is grounded in the utility model that underlies the final serving layer.\n\u2022 Connecting the two key mechanisms for industry-scale recommendation systems - filtration and ranking, and demonstrating how biases in either step can lead to downstream fairness gaps in user utility.\nWe organize our literature review as they relate to these two points separately. In terms of connecting the notions of industrial ML systems with algorithmic frameworks for fairness and utility, Ekstrand et al. [18] represents the closest related work through a comprehensive overview of industry-scale modeling systems. The authors recognize the distinction between short-term proxies and user utility, as well as various fairness definitions in ranking. While they identify fairness issues beyond the ML modeling stage\u2014such as in the candidate retrieval layer and behavioral distribution shifts across groups\u2014they fall back on traditional individual model fairness and do not address the practical compositional nature of the problem."}, {"title": "A.1 Recap of Fairness in Multi-Label Settings", "content": "Previous works have recognized that industrial systems often comprise multiple models, each predict- ing different which are proxies of downstream outcomes (e.g., \"clicks\" as a proxy of engagement, \"likes\" as a proxy of preference). In these settings, the authors typically develop a multi-label analogs of existing fairness definitions such as equalized odds ([3]). Specifically, this entails defining a \"composite label\" $Y_c$ as a function of the individual labels $Y_1,..., Y_k$, each of which represent a different aspect of the goodness of an item (and similarly for the composite prediction $\\hat{Y}_c$), and then applying a standard single fairness definition on the composite label and prediction. A key motive behind our research was our opinion that the proposed \"composite\" labels do not adequately reflect how predictions are served. To give some examples, Dwork and Ilvento [16] and Wang et al. [54] suggest having the composite label $Y_c$ as the product over the individual labels $Y_c = \\prod_k Y_k$ (and similarly for the composite prediction). On the other hand, Atwood et al. [3] suggests using the maxima such that $Y_c = \\text{max}_k Y_k$. From a user utility perspective, the interpretation of the product means that the item must be qualified in all aspects to be useful for the user, while the interpretation of the maxima means that a singular good aspect makes the item useful for the user. Our definition of the composite label presented in Section 3.1 as a weighted sum $Y_c = \\sum_k\u03b1Y_k$ therefore takes a natural middle ground, where users have (potentially heterogeneous) preferences over different aspects of an item. As mentioned in Figure 1, this is indeed the mechanism used across multiple industrial recommendation systems. DiCiccio et al. [15] studies a closely related setting by ensuring that the weighted sum of predictions is calibrated with respect to the weighted sum of labels. This is the same setup that we analyze, with the key difference being that DiCiccio et al. [15] addresses fairness for the items being ranked, rather than that of the viewer, which is our focus in this paper."}, {"title": "A.2 Recap of Fairness in Pipelines", "content": "Several fairness works have been motivated by the use of pipeline-style (also termed \"sequential\") systems in the real world to make decisions. For instance, resume filtering\u00b2 and promotion candidacy are commonly framed as a multi-step filter process, where a decision-maker reduces the candidate pool at each step until they end up with a group of desired size. Bower et al. [8] is an early work in this category and exactly studies fairness in pipeline systems, where a pipeline is fair if the final outcome obeys equal opportunity. In it, the authors illustrate that a pipeline constructed of models that are stand-alone fair may not be fair with respect to the final outcome. Dwork et al. [17] analyzes a similar setting, but instead assesses notions of individual fairness rather than group fairness and makes similar conclusions. Blum et al. [7] studies the same setting, but focuses on formulating a constrained optimization approach to optimize for both performance and fairness."}, {"title": "B Appendix / Proofs", "content": ""}, {"title": "B.1 Proof of Lemma 2.3", "content": "Proof. First we observe that by definition", "Z^i)": "and true user preferences $\u03b1^*$. Hence", "m+": "n$j^* := \\underset{1 \\leq j \\leq m+"}, {"\u03b1).\n$j(\u03b1^*)": "underset{1 \\leq j \\leq m+}{\\text{argmax}} \\\\{\\sum_{k=1}^{K}\u03b1_kf_k (X, Z^j) \\\\}$\n$= \\underset{1 \\leq j \\leq m+}{\\text{argmax}} \\\\{\\sum_{k=1}^{K}c\u03b1_k^*.E [Y_k \\, | \\, f_1(X, Z^i),..., f_K (X, Z^i)"}]}