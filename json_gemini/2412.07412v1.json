{"title": "Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT", "authors": ["Ahan Bhatt", "Nandan Vaghela", "Kush Dudhia"], "abstract": "Knowledge Graphs (KGs) are essential for the func-tionality of GraphRAGs, a form of Retrieval-Augmented Gener-ative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and scalability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models' ability to generate high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graphs (KGs) have become essential tools for organizing and representing complex relationships within data, serving as the backbone for numerous Al systems. Among these, GraphRAGs (Retrieval-Augmented Generative Systems) are regarded as one of the most effective approaches for combining structured knowledge with generative capabilities. However, the manual creation of KGs for GraphRAGs remains a significant challenge. Traditional techniques, such as rela-tionship classification, require considerable effort and often lack the precision needed to handle intricate or large-scale datasets.\nTo address this, we explore a method that leverages Large Language Models (LLMs) to automate the KG generation process, making it more accessible and efficient. The primary goal of this research is to simplify the creation of high-quality KGs through an automated approach, while the secondary aim is to compare the performance of three leading LLMs-GPT-4, LLaMA 2 (13B), and BERT-in extracting meaningful relationships and entities. Unlike conventional methods, which rely on predefined relationship templates, we focus on the ability of these models to infer relationships directly from raw text.\nDue to GPU constraints, our analysis uses a small excerpt from the Wikipedia page on the C programming language as the primary data source for KG creation. This excerpt, while limited in size, provides a diverse range of technical relation-ships and entities, offering an ideal test case for evaluating the models. By generating KGs from this excerpt, we aim to evaluate not just the accuracy of the relationships inferred but also the semantic coherence of the graphs produced.\nThis paper introduces an evaluation framework that goes beyond traditional accuracy measures to include metrics like Graph Edit Distance and Semantic Similarity. By doing so, we provide a comprehensive assessment of how effectively these models can translate unstructured data into structured knowledge. The results of this study highlight the capabilities and limitations of GPT-4, LLaMA 2, and BERT, providing insights into their suitability for automating KG generation. Ultimately, this research lays the groundwork for developing scalable and accurate methods for creating KGs, essential for advancing the utility of GraphRAGs and similar systems."}, {"title": "II. RELATED WORK", "content": "The integration of Large Language Models (LLMs) and Knowledge Graphs (KGs) has emerged as a critical area of study, addressing limitations in both technologies while enhancing their mutual functionalities. This section reviews key findings and methodologies from prior research to con-textualize our proposed approach."}, {"title": "A. Leveraging KGs to Enhance LLMs", "content": "Knowledge graphs contribute to LLMs by grounding their outputs in structured, verified data, mitigating issues such as hallucinations and lack of domain-specific knowledge. Baek et al. introduced KAPING, a method for augmenting LLM prompts with KG-derived facts, enabling zero-shot question answering. Other works, like Knowledge Solver, use KGs to enhance LLM reasoning capabilities through multi-hop inference processes."}, {"title": "B. Using LLMs to Build and Improve KGs", "content": "LLMs have been employed to streamline KG construction, particularly from unstructured data. Techniques such as Bert-Net extract entities and relations directly using paraphrased prompts, while semi-automated pipelines like AutoRD target domain-specific KGs for healthcare and other fields. These methods enable rapid, cost-effective KG generation without extensive manual input (Ibrahim et al.)."}, {"title": "C. Hybrid Integration Approaches", "content": "Recent studies highlight hybrid approaches that combine the implicit knowledge of LLMs with the explicit structure of KGs. For example, models like ERNIE and KnowBERT jointly embed textual and graph data, improving semantic understanding and enhancing tasks such as entity typing and relation classification. Such integrations demonstrate improved performance in both reasoning and interpretability (Kau et al.; Ibrahim et al.)"}, {"title": "III. METHODOLOGY", "content": "This section describes the systematic approach used to generate and evaluate knowledge graphs (KGs) using GPT-4, LLaMA 2 (13B), and BERT. The aim is to automate KG creation and assess the models' performance using a standard-ized evaluation framework. The methodology consists of seven well-defined steps. Fig. 1 depicts a flowchart summarizing the workflow of the pipeline."}, {"title": "A. Data Selection", "content": "A small excerpt from the Wikipedia page on the C pro-gramming language was selected as the primary data source. This excerpt includes concise descriptions of C's features, characteristics, and historical context. It provides sufficient relational information to test KG generation while remaining computationally feasible under GPU constraints. The dataset's size and technical nature make it an ideal choice for this experiment."}, {"title": "B. Data Preprocessing", "content": "The text was preprocessed to ensure compatibility across the three models. Preprocessing involved:\n\u2022\nTokenization: Splitting the text into smaller units, such as words or phrases, to align with the models' input requirements.\n\u2022\nCleaning: Removing unnecessary symbols, formatting errors, and extraneous text that might interfere with relationship extraction.\n\u2022\nFormatting: Structuring the input in a uniform format to avoid introducing bias in how each model processes the text. This step ensures the input text is consistent across all experiments."}, {"title": "C. Knowledge Graph Generation", "content": "Each model was tasked with extracting entities and their relationships from the preprocessed text. The process involved:\n\u2022\nEntity Recognition: Identifying key terms (e.g., \"C Programming Language,\u201d \u201cUnix\u201d) as nodes in the graph.\n\u2022\nRelationship Extraction: Detecting relational links (e.g., \"Derived From,\u201d \u201cSupports\u201d) between these entities.\nThe output was structured into a KG format, where nodes represent entities and edges denote relationships. Each model generated its KG independently based on its inherent capabil-ities."}, {"title": "D. Ground Truth Creation", "content": "A manually validated ground truth KG was constructed us-ing the same excerpt. Human expertise was used to accurately identify the entities and relationships present in the text. This ground truth graph serves as the benchmark against which the generated KGs were evaluated."}, {"title": "E. Evaluation Metrics", "content": "Five error metrics were employed to evaluate the generated KGs:\n\u2022 Precision: The proportion of correctly identified relation-ships out of all relationships predicted by the model.\nPrecision =  $\\frac{True Positives (TP)}{True Positives (TP) + False Positives (FP)}$\n\u2022 Recall: The proportion of actual relationships correctly identified by the model.\nRecall = $\\frac{True Positives (TP)}{True Positives (TP) + False Negatives (FN)}$"}, {"title": "F1-Score:", "content": "The harmonic mean of Precision and Recall, providing a balanced performance measure.\nF1-Score = 2 $\\frac{Precision \\cdot Recall}{Precision + Recall}$"}, {"title": "Graph Edit Distance (GED):", "content": "A structural comparison of the generated graph with the ground truth, measuring the number of edits required to make them identical.\nGED = $\\sum_{i=1}^{n}$ Edit Operations (Ggenerated, Gground truth)"}, {"title": "Semantic Similarity:", "content": "A measure of how semantically close the relationships in the generated KGs are to the relationships in the ground truth, often computed using cosine similarity.\n$\\frac{\\sum_{i=1}^{n} Vector(i) \\cdot Ground Truth Vector(i)}{|| Vector|| \\cdot ||Ground Truth Vector||}$"}, {"title": "F. Model Comparison", "content": "Each model's KG was compared against the ground truth using the evaluation metrics. This step involved detailed per-formance analysis to determine:\n\u2022 Accuracy of relationships extracted.\n\u2022 Coverage of entities present in the source text.\n\u2022 Structural fidelity and semantic alignment of the graph."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "This section presents the performance evaluation of the three models\u2014GPT-4, LLaMA 2, and BERT-using the met-rics defined earlier: Precision, Recall, F1-Score, Graph Edit Distance (GED), and Semantic Similarity. Additionally, the knowledge graphs generated by each model are visualized to provide a clearer understanding of their structural and semantic characteristics."}, {"title": "A. Visual Comparison of Knowledge Graphs", "content": "Figure 2, 3 and 4 represent the knowledge graphs generated by the three models, compared against the manually curated ground truth graph:\nEach graph showcases the relationships and entities ex-tracted from the dataset. Visually, GPT-4's graph demonstrates greater structural completeness and alignment with the ground truth, while LLaMA 2 and BERT display some degree of misalignment and missing elements."}, {"title": "B. Precision, Recall, and F1-Score", "content": "The Precision, Recall, and F1-Score metrics evaluate how accurately each model identifies entity-relationship pairs. Higher scores indicate better alignment with the ground truth."}, {"title": "C. Graph Edit Distance (GED)", "content": "Graph Edit Distance measures the number of node and edge transformations required to convert the generated graph into the ground truth graph. Lower GED values indicate better graph structural similarity. The GED scores are indicated in Table 2."}, {"title": "D. Semantic Similarity", "content": "Semantic Similarity evaluates how well the entities and re-lationships in the generated graphs semantically align with the ground truth using cosine similarity. The Semantic Simlarity scores are indicated in Table 3."}, {"title": "E. Discussion", "content": "The results clearly demonstrate that GPT-4 is the most ca-pable model among the three for automated knowledge graph generation, achieving the highest scores across all metrics. Its better semantic understanding and structural alignment with the ground truth make it a strong candidate for tasks involving GraphRAG systems. The visual comparison of knowledge graphs further supports these findings, with GPT-4's graph showing the highest structural and semantic fidelity. LLaMA 2 provides a balance between performance and resource con-straints, while BERT lags behind, primarily due to its limited contextual understanding in this task.\nThe inclusion of these visualizations enhances the inter-pretability of the results, emphasizing the critical role of advanced LLMs in addressing the challenges of automated KG generation for GraphRAGS."}, {"title": "V. CONCLUSION", "content": "The task of automating knowledge graph generation is crucial for the advancement of GraphRAG systems, which rely heavily on accurate and meaningful knowledge representa-tions. In this study, we proposed and evaluated a methodology that leverages large language models (LLMs) such as GPT-4, LLaMA 2, and BERT to generate knowledge graphs directly from unstructured data. This approach addresses the limita-tions of traditional methods, such as relationship classification, which often require extensive manual input and lack scalability for large datasets.\nOur findings indicate that GPT-4 consistently outperforms the other models, both in terms of semantic and structural alignment with the ground truth knowledge graph. The com-bination of higher precision, recall, and semantic similarity metrics, alongside a lower graph edit distance, highlights GPT-4's capability to generate knowledge graphs that are both accurate and coherent. While LLaMA 2 showed moderate effectiveness, its performance suggests potential for use in scenarios with resource constraints, where the trade-off be-tween computational efficiency and accuracy is acceptable. On the other hand, BERT, despite its foundational role in NLP, struggles to handle the complexities of this task, particularly in generating relationships that capture the nuances of the input data.\nBeyond comparing the models, this research underscores the feasibility of leveraging LLMs for automated knowledge graph creation. By using a simplified dataset, we demonstrated that even with minimal resources, meaningful insights can be derived. Future research can extend this work by testing on larger, more complex datasets and incorporating domain-specific adaptations to refine graph accuracy further.\nThis study highlights the importance of continuing ad-vancements in LLMs to make knowledge graph generation more accessible and scalable, particularly for applications in information retrieval, reasoning, and decision-making. The methodology and findings pave the way for developing more efficient and accurate systems, reducing reliance on manual processes, and enabling the broader adoption of GraphRAGS in real-world applications."}]}