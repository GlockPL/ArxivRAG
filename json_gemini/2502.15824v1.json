{"title": "Getting SMARTER for Motion Planning in Autonomous Driving Systems", "authors": ["Montgomery Alban", "Ehsan Ahmadi", "Randy Goebel", "Amir Rasouli"], "abstract": "Abstract-Motion planning is a fundamental problem in autonomous driving and perhaps the most challenging to comprehensively evaluate because of the associated risks and expenses of real-world deployment. Therefore, simulations play an important role in efficient development of planning algorithms. To be effective, simulations must be accurate and realistic, both in terms of dynamics and behavior modeling, and also highly customizable in order to accommodate a broad spectrum of research frameworks. In this paper, we introduce SMARTS 2.0, the second generation of our motion planning simulator which, in addition to being highly optimized for large-scale simulation, provides many new features, such as realistic map integration, vehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and a broad variety of sensor models. Moreover, we present a novel benchmark suite for evaluating planning algorithms in various highly challenging scenarios, including interactive driving, such as turning at intersections, and adaptive driving, in which the task is to closely follow a lead vehicle without any explicit knowledge of its intention. Each scenario is characterized by a variety of traffic patterns and road structures. We further propose a series of common and task-specific metrics to effectively evaluate the performance of the planning algorithms. At the end, we evaluate common motion planning algorithms using the proposed benchmark and highlight the challenges the proposed scenarios impose. The new SMARTS 2.0 features and the benchmark are publicly available at github.com/huawei-noah/SMARTS.", "sections": [{"title": "I. INTRODUCTION", "content": "One of the key challenges in autonomous driving (AD) is motion planning, i.e., generating behavior for an autonomous vehicle to navigate safely in a highly interactive and stochastic environment. Unlike other aspects of AD systems, such as perception and prediction, evaluation of motion planning al- gorithms in the real world may not be feasible due to potential safety concerns, high cost of deployment, and the inability to reproduce scenarios for repeated testing and comparison of solutions. As a result, simulators have become integral tools in the development of motion planning algorithms. To be effective, simulators must be comprehensive, addressing every aspect of real-world driving, and should be realistic to accurately model dynamics, behaviors, observations that correspond to real-world environments [1].\nIn this paper, we introduce SMARTS 2.0, which is built upon our successful and evolving simulation platform [2], with the intention of bringing together realistic behavioral modeling, efficiency, and diagnostic capabilities for effective development of motion planning algorithms. In addition to the original features of SMARTS, the new version provides support for integration of real-world datasets, sensor simulation, baseline evaluation protocol, vehicle-to-vehicle (V2V) communication, and diagnostic tool-sets, all of which are implemented in a highly optimized fashion, allowing large-scale simulation of heterogeneous agents.\nFurthermore, we propose a novel motion planning bench- mark, which focuses on highly interactive scenarios, such as turning actions at complex intersections and vehicle following maneuvers. As part of our contribution, we propose novel metrics and evaluate a number of existing planning algorithms, and highlight their challenges to motivate future research."}, {"title": "II. RELATED WORK", "content": "A. AD Simulation\nGiven the associated costs and lack of reproducibility in real-world scenarios, simulation serves as a fundamental tool for developing and evaluating motion planning algorithms. Increasing interest in autonomous driving has led to the introduction of many simulation platforms of varying scopes. Many of which cater to specific driving tasks, e.g. racing [3], highway [4], or urban environment [5]. The focus of simulations also varies, whether it is on realistic sensor modeling [6], [7], agent behavior modeling [2], [8], or execution efficiency [5], [9].\nThe current landscape of simulation platforms offers vari- ous solutions for evaluating aspects of autonomous driving systems. Excluding some platforms, such as [10], [4], [11], which focus on planning in passive driving environments, the majority of simulations offer forms of behavior modeling for social agents (i.e., agents other than the ego-vehicle), which is often extended to multi-agent simulation. This allows multi-instance evaluation in single scenarios [12], [13], [14], [15], [16]. Modeling sensors, such as visible spectrum cameras and LiDAR in some environments [17], [18], [7], [6], [8], [9] enables the evaluation of full-stack autonomous driving systems. In some cases, occupancy maps are implemented via ray-casting, which provides a basis to model obstructions in the scene from the perspective of the ego-vehicle [19]. Some platforms also offer unique features, such as mechanisms for V2V communication [13]. More recent simulators emphasize behavioral realism [20], [5], [19], [18] by incorporating expert data (in the form of human demonstrations or trajectories from expert policies) and real- world traffic data, which helps to reduce the sim-to-real gap in training driving models. For a more detailed list of simulations see [21].\nIn this work, we introduce SMARTS 2.0, built upon the widely used SMARTS [2] simulator, with a goal of integrating"}, {"title": "B. Motion Planning Benchmarks", "content": "There are a number of existing benchmarks, e.g. [22], [23], [24], that focus on the prediction task, which is a key component for motion planning. Here, the goal is to accurately measure the behavior of the social agents, often using a variety of accuracy or diversity metrics [25].\nMotion planning benchmarks [26], [16], [27], [28], [29] take one further step and model the response of the mission vehicle to the evolving surrounding environment. Specialized benchmarks, however, evaluate other aspects of the generated behaviors. For example, the CARLA benchmark [7] is based on synthetic scenarios inspired by the US National Highway Traffic Safety Administration (NHTSA) pre-crash typology whose evaluation is based on route and infraction points. The CommonRoad Benchmark [16] focuses on both interactive and non-interactive environments (where trajectories of social agents are provided) and assesses driving styles and rule violations with no restrictions. The Waymo simulation bench- mark [27] is intended for generating human-like trajectories, which are evaluated against real data in terms of realism and distributional distance error.\nIn this work, we propose a novel benchmark, focusing on highly interactive scenarios which involve both turning actions at complex intersections and adaptive driving in car following tasks. For evaluation, we improve upon the existing metrics and propose new ones to better capture the practicality of algorithms for real-world applications."}, {"title": "III. SMARTS 2.0", "content": "SMARTS [2] is an open-source platform designed for research on motion planning for autonomous driving. Some of the key features of SMARTS include a compositional architecture and distributed computing for scalable simula- tion, highly realistic physics engine, and support for standard Gym\u00b9 APIs for improving the flexibility of creating custom environments. Despite the popularity of the platform, our first version had several shortcomings, including the lack of an established evaluation protocol, limited support for data ingratiation, and a number of limitations on context modeling and sensing. In SMARTS 2.0 we address many of these prior shortcomings. Below are the descriptions of new key features and improvements in our simulation platform.\nBaseline evaluation protocol. As part of a competition hosted in 2022 [30], [26], SMARTS provided an evaluation protocol for a series of common driving tasks with metrics and SOTA planning algorithms that were made online. This"}, {"title": "IV. INTERACTIVE MOTION PLANNING BENCHMARK", "content": "A. Scenarios\nWe conduct the benchmarking of interactive motion plan- ning tasks in two categories: collaborative planning and adaptive planning.\n1) Collaborative planning: This task category requires individual agents to negotiate scenarios with other traffic agents, to achieve their objective. In particular, we focus on the interactive scenarios at intersections for which the agents should be able to accurately model interactions between other road users and forecast their behavior. Variations within this task category's scenarios are determined by road structure, agent quantity, traffic density, presence of signals, and types of turning maneuvers. Sample turning scenarios are shown in Figure 3.\n2) Adaptive planning: This problem addresses a key research challenge in AD where the task of the mission vehicle is to follow another vehicle and adapt to its behavior without knowing the lead vehicle's current intention and goals. The focus is on accurate sensing, accurate estimation of the intention of the lead vehicle, adaptation, and efficiency in terms of maintaining the minimum safe distance while following a lead vehicle. This task category promotes the use of multi-agent planning algorithms.\nVehicle following scenarios are likewise designed to consider varying levels of difficulty. In the simplest case, the focus of the task is on the ability of the mission vehicle to mimic the behavior of a lead vehicle with no background social vehicles present. More complex scenarios involve different numbers of social agents, the lead vehicle performing challenging behaviors (e.g., merging, exiting, turning), different starting locations for the mission vehicles, and environments that make the lead vehicle's intention ambiguous to the mission vehicles. These scenarios are depicted in Figure 4."}, {"title": "B. Evaluation Metrics", "content": "The proposed metrics are designed to evaluate safety, efficiency, human-likeliness (humanness), and completion of the solutions. The final ranking is computed as a weighted average of the individual metrics, $S_{final} = \\alpha_1M_1+..., \\alpha_nM_n$ where $S_{final} \\in [0,1]$ and $\\alpha$ and $m$ are weights and values of metrics respectively.\nThe metrics are divided into common metrics which are used for all task categories and task-specific metrics which are used for the corresponding scenario-specific tasks.\n1) Common Metrics: Progress Rate (PR) intends to evaluate how far a mission vehicle advances towards the goal before the termination of a scenario. PR is defined as:\n$PR = 1-\\frac{1}{N_{sc}}\\sum_{sc k=1}^{N_{sc}}\\frac{1}{N_{a,k}}\\sum_{1}^{N_{a,k}}\\frac{min(dist(mv_i, g_f), dist(mv_{i-init}, g_f))}{dist(mv_{i-init}, g_f)} \\in [0, 1]$\nwhere $N_{a,k}$ is the number ($a$) of mission vehicles ($mv$) in scenario $k$, $N_{sc}$ is the number of scenarios, $mv_{i-init}$ is the initial position of the mission vehicle $i$, $mv_i$ is the final position of the mission vehicle at termination, and $g_f$ is the position of the final goal at termination time.\nHumanness is intended to approximate how human-like the behavior of the mission vehicles is, motivated by an existing proposal [34]. This metric is based on the level of comfort (due to changes in acceleration and jerk) and lane center offset formulated as follows:\n$Humanness = 1 - \\frac{1}{N_{sc}}\\sum_{sc k=1}^{N_{sc}} \\frac{1}{N_{a,k}}\\sum_{1}^{N_{a,k}}\\frac{comf_{k,i}}{T_{sci,k}} + \\frac{1}{2N_{a,k}}\\sum_{1}^{N_{a,k}}\\frac{lc-off_i}{T_{sci,k}}  \\in [0, 1]$\nwhere $T_{sck.i} = min(T_{mvi,k},T_{sck})$. Here, $T_{mvi,k}$ is the time for vehicle $i$ to complete scenario $k$ and $T_{sck}$ is time limit of scenario $k$, $lc-off$ is lane center offset, which is 1 if the vehicle is fully offroad when the majority of a vehicle's wheels have left the road surface, otherwise given by $dist(mv,lc)$, where $l_w$ denotes the lane width. $comf$ metric is computed by,\n$comf = \\frac{T_{trv}}{T_{trv} + T_p},$\n$T_\\alpha = \\sum_{i=1}^{T} u_{ti}, u_t = \\begin{cases}\n1 & max \\; dyn_t > 1 \\\\\n0 & otherwise\n\\end{cases}$\nwhere $\\alpha = max(0,t - T_p)$, $\\beta = min(T_{trv},t)$, and $dyn = (\\frac{jerk_{lin}}{jerk_{lin-max}}, \\frac{acc_{lin}}{acc_{lin-max}})$.  where $jerk_{lin-max} = (0.9,0.9)$ and $acc_{lin-max} = (2.0, 1.47)$ referring to $(long, lat)$ values for maximum linear jerk and accelera- tion respectively [34].\nRule Compliance (RC) measures how safe the behavior of the vehicle is in terms of following road rules. The RC metric is specified as follows:\n$RC = 1 -\\frac{1}{N_{sc}}\\sum_{sc k=1}^{N_{sc}} \\frac{1}{3N_{a,k}}\\sum_{i=1}\\frac{min(S_{violate}}{T_{sci,k}}, \\frac{r_{violate}}{0.5S_{limit}} + p_o) \\in [0,1]$"}, {"title": "V. EVALUATION", "content": "A. SMARTS 2.0 Performance\nIn this section, we conduct a performance test between SMARTS 2.0 and its predecessor, SMARTS [2] using three evaluation criteria: the number of Integrated Traffic Actors, the number of Agents, and Road Network Edges. Integrated Traffic Actors evaluates the performance of the simulation given a quantity of internal traffic simulator vehicles to simulate. Agents measures the simulation of the number of agent vehicles, given road and neighboring vehicle sensors. Road Network Edges measures the simulation performance interaction between 10 agents and a scaling quantity of road network edges in the road network.\nThe performance is measured using the inbuilt SMARTS diagnostic tool running on a device with 32GB RAM and an Intel(R) Core(TM) i9-9900K CPU with 3.60GHz clock"}, {"title": "B. The Benchmark", "content": "Metrics. We use the metrics as described in Section IV-B. In addition, for ranking of complete alternative approaches we resort to a combined metric $S_{bench}$,\n$S_{bench} = \\frac{\\sum_{i=1}^{n} w_i M_i}{\\sum_{i=1}^{n} w_i} \\in [0, 1], n = \\sum_{i=1}^{n} w_i,$\nwhere $M$ is an evaluation metric result and $w$ is the scoring weight of evaluation metric $i$. We empirically set the weights as 0.1 for PR, 0.45 for RC, 0.15 for Humanness, and 0.3 each for MTE and SFD in each benchmark task.\nMethods. For evaluation, we use the three winning meth- ods from our 2022 competition, namely TF2, VCR3, and AID4. TF is a hierarchical model with a meta controller and a scheduler. The controller consists of a rule-based collision detector that recognizes situations in which an incident might take place, and an offline learning agent that generates actions. The scheduler executes the decision policy provided by the controller using a series of underlying policies, including speed, moving direction, and merging.\nVCR method uses two controllers, namely baseline and filtering. The former relies on the waypoints provided in the observation space, from which the controller selects the ones that lead to the goal location. The filtering controller, then samples from the lines between the agents current location"}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced SMARTS 2.0, an extension of our widely used simulation platform that offers many enhanced features, such as real data replay, advanced sensor simulation, V2V communication, and diagnostic tools in a highly optimized framework. In addition, we proposed a novel benchmark for highly collaborative planning tasks involving challenging intersections, and adaptive planning consisting of the vehicle following task. We conducted experiments using baseline and SOTA algorithms using our proposed metrics, and show that all approaches struggle to output a balanced performance across all metrics, making their applicability to real-world situations questionable. For future work, one can consider revisiting metrics such as RC to impose more restrictive conditions better resembling real-world constraints. As for future scenario improvements, inclusion of connected-driving settings requiring V2V com- munication, variations in traffic density, and heterogeneity can be further considered."}]}