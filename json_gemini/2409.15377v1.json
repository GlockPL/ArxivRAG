{"title": "Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia", "authors": ["Elisa Castagnari", "Lillian Muyama", "Adrien Coulet"], "abstract": "In practice, clinicians achieve a diagnosis by following a sequence of steps, such as laboratory exams, observations, or imaging. The pathways to reach diagnosis decisions are documented by guidelines authored by expert organizations, which guide clinicians to reach a correct diagnosis through these sequences of steps. While these guidelines are beneficial for following medical reasoning and consolidating medical knowledge, they have some drawbacks. They often fail to address patients with uncommon conditions due to their focus on the majority population, and are slow and costly to update, making them unsuitable for rapidly emerging diseases or new practices. Inspired by clinical guidelines, our study aimed to develop pathways similar to those that can be obtained in clinical guidelines. We tested three Large Language Models (LLMs) Generative Pre-trained Transformer 4 (GPT-4), Large Language Model Meta AI (LLaMA), and Mistral on a synthetic yet realistic dataset to differentially diagnose anemia and its subtypes. By using advanced prompting techniques to enhance the decision-making process, we generated diagnostic pathways using these models. Experimental results indicate that LLMs hold huge potential in clinical pathway discovery from patient data, with GPT-4 exhibiting the best performance in all conducted experiments.", "sections": [{"title": "1 Introduction", "content": "For complex diagnostic decisions, clinicians typically follow diagnostic guide-lines that outline the sequential steps necessary to reach a diagnosis. These guidelines, developed by panels of experts based on the best available evidence, aim to standardize and streamline clinical decisions through recommended pro-cedures such as gathering information, making observations, and ordering lab-oratory tests. However, the development of guidelines is both costly and time-consuming, making it challenging to develop guidelines that comprehensively"}, {"title": null, "content": "cover the entire spectrum of diseases. Therefore, there is a pressing need for more flexible and scalable methods to provide insights when clinical guidelines are incomplete or unavailable.\nWe believe that new techniques, trained on clinical data or embedding a large amount of domain knowledge, can complement traditional diagnostic guidelines. Our goal is to develop methods that assist the decision-making process in a step-by-step manner, as it has been importantly outlined in previous research [1]. Following such an approach has the potential to minimize unnecessary tests, optimize healthcare costs, and offer more personalized and accurate diagnoses, particularly for patients with uncommon conditions.\nThe extensive data available in Electronic Health Records (EHRs), as well as the emergence of large language models (LLMs) offers significant opportu-nities to enhance clinical practice. EHRs contain structured, semi-structured, and unstructured data about patients' health, including medications, labora-tory test orders and results, diagnoses, and demographic information. Previous studies have leveraged EHRs to train machine learning (ML) methods to au-tomatically suggest diagnoses for patients [2]. However, these studies typically use supervised ML methods to predict a single endpoint, represented as a class label. For data-driven approaches to be truly adopted in clinical practice, di-agnoses should not be limited to a single endpoint. Instead, they could be represented as a pathway that encompasses the steps of medical reasoning and decision-making. This work builds upon a previous study that employed Deep Reinforcement Learning (DRL) trained on EHRs to achieve this goal [3]. In this paper, we propose an approach that uses LLMs and explores different models, evaluate their performance on the basis of synthetic but realistic EHRS, and compare them with the DRL approach.\nAnemia, defined as a lower-than-normal amount of healthy red blood cells, was chosen as the clinical condition for this study for three reasons: its diagnosis is primarily based on a series of laboratory tests available in most EHRs; it is a common diagnosis, implying that the associated amount of data is sufficient to train ML models; and the differential diagnosis of anemia is frequently complex, making its guidance particularly useful.\nWe chose to use LLMs because, like the DRL approach, they can construct models that operate sequentially, passing through various steps, or following a human-readable chain of thought. We propose adapting prompts for LLMs to progressively build individualized pathways of observations to make before sug-gesting a diagnostic decision. For instance, in the anemia use case, a pathway would involve a sequence of laboratory test requests, with their results guiding the decision to request additional tests or make a diagnosis. We believe that these constructed pathways can complement clinical guidelines to aid practi-tioners in decision-making during the diagnosis process.\nOur main contributions are:"}, {"title": "1. Developing and Evaluating Prompts", "content": "We created various prompts that communicate the diagnostic task to the LLM to generate diagnostic pathways, and compared their performance."}, {"title": "2. Evaluating LLM Performance", "content": "We compared the performance of dif-ferent LLMs in generating diagnostic pathways and diagnosing patients."}, {"title": "3. Comparative Analysis", "content": "We compared the performance of LLM-generated pathways with those generated by a concurrent DRL-based approach, to assess differences and improvements."}, {"title": "2 Related Works", "content": "The discovery of clinical pathways from patient data has been extensively stud-ied, with unsupervised learning methods such as clustering [4-7], topic model-ing [8-10], and particularly process mining [11-13] being the most prevalent. Other works used supervised learning methods, such as decision trees [14, 15] and neural networks [16, 17], to learn patient pathways from EHRs. Mean-while, reinforcement learning has been actively applied in recent years to learn dynamic treatment regimens for patients [18-20]. The application of Deep Re-inforcement Learning for diagnosis pathways using EHRs represents another innovative approach. Yu et al. [21] used DRL methods for cost-effective clini-cal tasks, including the prediction of Acute Kidney Injury. In [3], Muyama et al. used EHR data to train DRL models for clinical diagnosis pathway genera-tion, formulating the diagnosis process as a sequential decision-making problem within a Markov Decision Process (MDP) framework.\nMoreover, the recent emergence of LLMs has seen their application in a wide range of tasks, including text generation, language translation, chatbot develop-ment, among others. Their potential in clinical reasoning has gained significant attention, particularly for enhancing diagnostic accuracy and interpretability. Various studies have explored different frameworks and methodologies to inte-grate LLMs into clinical decision-making processes, aiming to address the unique challenges of the medical domain. Traditionally, the assessment of LLMs has focused on multiple-choice questions. However, recent studies have shifted to-wards free-response clinical questions, showing the promise of newer LLMs like Generative Pre-trained Transformer 4 (GPT-4) in diagnosing complex clinical cases. In [22], Wu et al. developed a framework called In-Context Padding to guide LLMs' reasoning with medical knowledge seeds. This method involved ex-tracting medical entities from clinical contexts, inferring relevant entities using a knowledge graph, and padding these knowledge seeds into prompts to guide the inference process of LLMs. Similarly, Kwon et al. [23] used the Reasoning-Aware Diagnosis Framework, which leverages prompt-based learning to generate diagnostic rationales. They formulated clinical reasoning as a Clinical Chain-of-Thought (CoT), allowing LLMs to provide insights into patient data and the reasoning paths toward diagnoses. Moreover, Savage et al. [24] developed di-agnostic reasoning prompts to evaluate the interpretability of LLMs, such as GPT-4, in medicine.\nIn addition to these specific frameworks, LLMs like the Pathways Language Model (PaLM) and its instruction-tuned variant, Flan-PaLM, have shown state-"}, {"title": null, "content": "of-the-art performance on various medical question datasets [25]. These models leverage a combination of prompting strategies and instruction prompt tuning to enhance comprehension, knowledge recall, and reasoning.\nIf these related works highlight the potential of LLMs in clinical reasoning and decision-making, the study presented in the paper aims to evaluate the effectiveness of various prompting techniques and incorporate domain knowledge to improve the performance and explainability of LLMs for diagnosing anemia, specifically using Electronic Health Records. It explores how different prompting strategies, including examples, domain knowledge rules, and Chain-of-Thought reasoning, impact the performance of different LLMs for this task. Also, it provides a comparative error analysis between LLMS and a DRL approach used in a previous study."}, {"title": "3 Methods", "content": null}, {"title": "3.1 The Models", "content": "We explored the programmatic use of three different LLMs, based on the Trans-former architecture described in [26] to generate anemia diagnosis pathways. All three LLMs have been pre-trained on a large corpus of texts. The LLMs we used are:"}, {"title": "3.1.1 Generative Pre-trained Transformer", "content": "Developed by OpenAI [27], GPT [28] is an LLM that can be applied to various generative tasks. In this work, we used the fourth iteration in the GPT series, named GPT-4. A previous iteration (GPT-3.5) was initially considered but ultimately discarded because it consistently had a worse performance than GPT-4 on our preliminary tasks. This study uses GPT-4 Turbo."}, {"title": "3.1.2 Large Language Model Meta AI (LLaMA)", "content": "LLaMA [29] is a family of LLMs, developed by Meta AI [30]. This study uses LLaMA-3."}, {"title": "3.1.3 Mistral", "content": "Mistral is an LLM created by Mistral AI [31]. This study uses Mistral7B v0.3.\nThese models were applied to clinical reasoning tasks and adapted to the specific case of finding the optimal sequence of actions needed to achieve an accurate anemia diagnosis. We note that while Mistral and LLaMA are open-source, GPT is not."}, {"title": "3.2 Prompting", "content": "To achieve our objective, we tested the LLMs with various prompt engineer-ing approaches to enhance the decision-making process and evaluate it over a synthetic, but realistic dataset. The considered prompting approaches were:\n\u2022 Providing a closed list of answers: The model is prompted with the specification of the set of possible answers it could answer.\n\u2022 Setting a \u201cPersona\u201d [32]: Setting a personality or character within the prompting, so that the LLM reasons and answers as if it was that personality.\n\u2022 Providing examples: Providing the LLM with one to n problem-solution or input-output examples so it has a better understanding of the desired output.\n\u2022 Sequential prompting: Instead of providing a single question and re-ceiving a single answer, the model is prompted in a sequential manner, where at each step, it asks one question and gains new information about the patient, which in turn feeds the next step of the process.\n\u2022 Providing sets of rules: Feeding the LLM with rules from existing clinical diagnosis guidelines, to guide its responses.\n\u2022 Chain-of-thought [33]: The model is prompted to generate a step-by-step (chain) reasoning to explain its response, which breaks down the task into smaller reasoning steps.\nOur various prompting experiments fall in two major categories: the Plain prompt, where we provide the LLM with all the patient's features (e.g. lab test results) at once and the LLM responds once with a diagnosis; and the Sequential prompt, where the LLM requests for patient features one-by-one, and values are provided by our program to the LLM until a diagnosis is reached."}, {"title": "3.2.1 Plain Prompt", "content": "Following prompting good practices and preliminary testing, the very first prompt we tested included the set of possible diagnoses, i.e., the set of possible answers. To improve our results, we experimented with changing the persona of the mod-els. We tested three persona settings: no persona, an AI Assistant specialized in anemia, and a clinician, expert in anemia. Based on preliminary and unreported empirical results, we employed the \"clinician\" persona in all our reported ex-periments. Similarly, we explored in preliminary phases the impact of providing the LLM with examples, referred to as \"shots\", with three configurations: 0-shot, 1-shot, and few-shot (i.e., 3 examples). Based on the unreported empirical findings, we chose the 1-shot approach for the rest of our experiments."}, {"title": null, "content": "To assess the impact of incorporating additional domain knowledge into the prompt, we used a decision tree made from clinical diagnosis guidelines [3] and shown in Figure 1. We used it to provide LLMs with rules usually used for the differential diagnosis of anemia. We propose a pattern structure for translating pieces of the decision tree in natural language, as illustrated by the following prompt.\nYou are a clinician who is skilled in assessing whether a patient has anemia or not, and what type of anemia they have. You make the diagnosis based on their gender and laboratory test results. In your clinician role, you will give me the name of every lab test you took into consideration to determine the final diagnosis and the final diagnosis at the end.\nUsually, you make a diagnosis based on the following rules:\n1) Look for the ## value first.\n2) If the ## value is ** than ++, the diagnosis is @. If the ## value is ** than ++ but ** than ++, look for the ## of the patient.\n[If there are more than 2 cases:]\n3) Otherwise, look for the ##. Here you can distinguish the following cases (named a, b,...,n):\na) If the ## results are unavailable, the diagnosis is **.\nb) If the ## is ** than ++, look for the ## value. If the ## results are unavailable, the diagnosis is **. If the ## value is ** than ++, the diagnosis is @.\nc)\nd) [do step b]\nFor example, if you have that ##: **, ##: **, ##: **, the diagnosis will be @.\nwhere:\n\u2022 ## is a laboratory test or the gender;\n\u2022 ** is an operator (e.g., <, <, >, >, =);\n\u2022 ++ is a numerical value;\n\u2022 @ is a diagnosis type.\nPlease note that ##, **, ++ and @ are local variables and for this reason they can take various values within a single prompt.\nLastly, we evaluate the effectiveness of incorporating the chain-of-thought prompting strategy by asking the LLM to explain the steps leading to its re-sponse."}, {"title": "3.2.2 Sequential Prompt", "content": "The plain prompt provides all the patient's information at once, effectively treating the diagnosis task as the prediction of a single endpoint without gener-ating a pathway to reach that diagnosis. Conversely, we consider a \"sequential\" prompt, where the LLM is instructed to request one patient feature at a time in each turn of the dialogue. The LLM receives results for each requested feature until it reaches a diagnosis. In this mode, at any given time, the LLM only has access to the information it has already specifically requested.\nWe restricted the models to inquire only about the features present in the decision tree and also specified the set of possible anemia diagnoses from which the model would select the final diagnosis. Similar to the experiments with the plain prompt, we propose to evaluate the impact of providing of examples, incorporating rules from the decision tree, and the use of the chain-of-thought strategy."}, {"title": "3.3 Dataset", "content": "The experiments were conducted using the synthetic anemia dataset described in [3], which was constructed based on the decision tree shown in Figure 1. This dataset includes 17 features\u2014hemoglobin, gender, mean corpuscular volume (MCV), ferritin, reticulocyte count, segmented neutrophils, Total Iron Binding Capacity (TIBC), hematocrit, transferrin saturation (TSAT), red blood cells (RBC), serum iron, folate, creatinine, cholesterol, copper, ethanol, and glu-cose and 8 classes: No anemia, Vitamin B12/Folate deficiency anemia, Un-specified anemia, Anemia of chronic disease (ACD), Iron deficiency anemia (IDA), Hemolytic anemia, Aplastic anemia, and Inconclusive diagnosis.\nFor each diagnostic class, feature values were generated using a uniform prob-ability distribution, with minimum and maximum values determined through a"}, {"title": null, "content": "manual review of medical literature and thresholds from the decision tree. The dataset encompasses 70,000 patients. A more detailed description of the dataset synthesis can be found in [3]. For our experiments, we used 1,000 patients from it for both LLaMA and Mistral, whose class distribution is shown in Fig. 2. We only used 250 patients for GPT-4 due to resource constraints. We used the first 1,000 and 250 patients from the dataset for LLaMA/Mistral and GPT-4, respectively."}, {"title": "3.4 Evaluation Approach and Implementation", "content": "The performance was evaluated using the following metrics:\n1. Accuracy: The proportion of patients that were correctly diagnosed."}, {"title": "2. Mean Pathway Length", "content": "The average number of actions in the diagnos-tic pathways generated by the model."}, {"title": "3. F1 Score", "content": "The harmonic mean of the precision and recall scores. We report one-vs-rest and macro-averaging F1 scores."}, {"title": "4. ROC-AUC (Receiver Operating Characteristic - Area Under Curve) Score", "content": "This measures the model's ability to distinguish between different classes. We report one-vs-rest and macro-averaging ROC-AUC scores."}, {"title": "5. Time", "content": "The time for the LLMs to infer the results.\nAs mentioned, we conducted experiments on 1,000 patients, except for GPT-4, where we used 250 patients due to resource constraints. Due to inconclusive results with the seed parameter, our reported results are based on a single experiment run corresponding to the state of our machine at the time of exper-imentation.\nThe LLaMA and Mistral experiments were implemented using the Langchain Python library, and the GPT-4 experiments were conducted via the OpenAI API. The source code of these experiments is available at: https://anonymous. 4open.science/r/anemia_diag_with_llm-1C1A/. The reference for all the prompts used is in: https://anonymous.4open.science/r/anemia_diag_with_llm-1C1A/ prompts.txt"}, {"title": "4 Results", "content": null}, {"title": "4.1 Plain Prompts", "content": "The baseline experiment involved a plain prompt specifying the set of anemia classes. The results of this experiment are shown in Table 1. All the models exhibited poor performance, with Mistral performing the worst and GPT-4 the best."}, {"title": null, "content": "Next, we enhanced the baseline prompt with a single example of a diagnosis, such as: For example, you have that hemoglobin: 10g/dL, mean corpuscular volume: 83fL, reticulocyte count: 1.6%. The diagnosis will be Aplastic anemia. Results in Table 2 show that incorporating an example slightly improved the performance of all the models."}, {"title": null, "content": "To experiment with the addition of domain knowledge to the LLMs' decision-making process, we manually converted the decision tree in Figure 1 into natural language rules and added these rules to the prompt. Various templates for these rules were tested and the most suitable one was retained. The results in Table 3 show that there was a significant improvement for GPT-4, with both GPT-4 and Mistral doubling their scores. LLaMA also improved, though to a lesser extent. Despite the use of rules, there were still numerous misdiagnoses, particularly with LLaMA and Mistral, whereas the decision tree alone would perform a perfect classification."}, {"title": null, "content": "Finally, we added the Chain-of-Thought to the plain prompt. The results, displayed in Table 4, revealed a massive improvement for LLaMA and a reason-able improvement for GPT-4. Mistral's performance did not show any signifi-cant change. Also, this approach made the models' decisions more explainable, allowing us to identify the causes of misdiagnoses and misunderstandings of the models. This enabled us to better understand the model's errors and areas of confusion."}, {"title": "4.2 Sequential Prompts", "content": "We reproduced the same evaluation schema for the sequential prompts, but for simplicity, we only report the results of the two best-performing variants of these"}, {"title": null, "content": "prompts. The first, whose results are shown in Table 5, involved a sequential prompt with specified anemia classes, 1-shot, and rules."}, {"title": null, "content": "The second prompt is similar, but included the CoT. Its results, presented in Table 6, show that adding CoT improved performance for both GPT-4 and LLaMA, while Mistral did not show similar gains. GPT-4 in this scenario achieved the best results of all our experiments. Here, for comparison, we also display results from the Deep Q-Network (DQN) approach used in [3] to gener-ate anemia diagnosis pathways."}, {"title": "5 Discussion", "content": null}, {"title": "5.1 LLM Performance", "content": "In all our experiments, GPT-4 consistently had the best performance, followed by LLaMA, with Mistral performing the worst in all scenarios. As for the prompting, adding an example to the prompt improved results across all the models. However, the performance of all models was notably poor before the introduction of rules from diagnosis guidelines into the prompts. Therefore, while these models have been pre-trained on vast amounts of texts, including medical texts, for specific use cases, it may be beneficial to fine-tune the models using task-specific datasets if the goal is to learn only from new patient data without considering existing guidelines. Similarly, using Retrieval Augmented Generation (RAG) [34], could enable the identification and consideration of narrative clinical guidelines to fine-tune the embeddings, potentially leading to improved results. Additionally, incorporating the CoT strategy further im-proved the results for GPT-4 and LLaMA. This is because breaking down the task into smaller reasoning steps has been shown to enhance the reasoning abil-ities of LLMs. However, this was not the case with Mistral. Upon analyzing its responses, we found that Mistral did not consistently apply the CoT for some patients, even if prompted. Overall, LLMs present a valuable opportunity. By including domain knowledge in the form of rules expressed in natural langage to their input, they can generate diagnostic decision pathways in a conversational"}, {"title": "5.2 Error Analysis", "content": "We conducted a detailed error analysis for the results of both plain and sequen-tial prompts with CoT. For the plain prompt, the majority of the misdiagnoses across all models were due to \"comparison errors\", where the LLMs were in-correct in determining whether a value was greater than, less than, or equal to another value. For GPT-4, the remaining misdiagnoses were only attributed to the model not following the provided decision tree rules. For Mistral, a signifi-cant portion of the misdiagnoses (23.40%) may have been caused by the model not applying the CoT, even though it was explicitly mentioned in the prompt. This explains why Mistral was the only model with a worse performance when CoT was prompted. Other misdiagnoses were due to diverse factors, such as not adhering to the decision tree rules, the model continuing the chat after a diagnosis was found (despite the conversation being supposed to terminate), and not accepting the provided value for a feature, among other issues.\nIn the sequential prompt, again, the majority of misdiagnoses were due to comparison errors, particularly for GPT-4, where these accounted for 100% of the misdiagnoses. Specifically, for LLaMA, several hemolytic anemia patients were diagnosed with aplastic anemia, and vice versa. This is not surprising as both anemia types are found on the same branch of the decision tree as shown in Figure 1, with the value of a single feature (reticulocyte count) making the difference. Other errors were due to not following the decision tree rules provided, with Mistral requesting values for features that were not included in the dataset, despite the fact that the list of features to be selected was explicitly stated in the prompt."}, {"title": "5.3 Generated Pathways", "content": "Fig. 3 illustrates the pathways generated by GPT-4 using the sequential prompt and CoT. In this Sankey diagram, the orange nodes represent the lab test results requested by the model in their order from left to right, while the dark green nodes represent the final diagnoses. The pink pathways correspond to patients diagnosed with ACD, and the blue pathways correspond to patients diagnosed with Aplastic anemia.\nFigs. 4, 5, and 6 display the most common diagnostic pathways for each anemia class, as produced by GPT-4, LLaMA, and Mistral, respectively. Each class is represented by a different color, e.g. pink for ACD, green for No anemia, and blue for Vitamin B12/Folate deficiency anemia. As shown in Fig. 3 and 4, the pathways generated by GPT-4 align closely with the decision tree used to label the dataset, whose rules were provided to the LLMs. For LLaMA, as illustrated in Fig. 5, although its most common pathways generally adhere to the decision tree rules, there were instances where it repeatedly requested the ferritin value before terminating with an inconclusive diagnosis. Additionally,"}, {"title": null, "content": "unlike GPT-4, gender is a significant feature in almost all its most common pathways for each anemia class. In contrast, Fig. 6 shows that Mistral frequently made anemia diagnoses based solely on hemoglobin level. While hemoglobin is sufficient to confirm the presence of anemia, it is inadequate for identifying the specific type of anemia. This further explains Mistral's poor performance.\nTo further investigate the similarities between the pathways generated by each model, we calculated the Levenshtein distance between the most common pathways for each anemia type produced by the LLMs. For comparison, we also generated pathways based on the decision tree depicted in Fig. 1. We encoded the pathways into strings, where each feature in the pathway was represented by a single character, and these characters were concatenated to form a string rep-resenting the entire pathway. We compared the most common pathway for each anemia type across models and calculated the average Levenshtein distances. The results revealed that GPT-4's most common pathways perfectly matched those of the decision tree, suggesting that GPT-4 is the most effective model for rule-based diagnostic techniques. Among the LLMs, GPT-4 and LLaMA exhibited the greatest similarity, with an average Levenshtein distance of 1.75, while GPT-4 and Mistral followed with an average distance of 1.88. Conversely, the pathways generated by Mistral and LLaMA had the greatest divergence, with a mean Levenshtein distance of 2.63."}, {"title": "5.4 Comparison with DRL approach", "content": "In a previous study by Muyama et al. [3], they used a DRL approach to learn clinical diagnostic pathways from EHR data. Specifically, they employed various extensions of a Deep Q-Network to achieve this goal. We decided to compare our"}, {"title": "sequential prompt-based results to theirs", "content": "as both approaches involve sequential decision-making and share the same objective. It is important to note that the DQN models were trained and tested exclusively on EHR data, allowing them to learn diagnostic pathways solely from the data, without prior knowledge. In contrast, the LLMs used in our study, in addition to their pre-training on texts, were also provided with domain knowledge in the form of rules to follow in order to make a diagnosis.\nMoreover, the DQN-based study used a broader set of features, some of which were unrelated to anemia diagnosis. In our LLM-based approach, the features to be queried are explicitly mentioned in the prompt and incorporated into the decision tree rules. Despite these differences, among the three LLMs evaluated, only GPT-4 outperformed the DRL approach. When comparing the diagnostic pathways generated by both methods, we found that the pathways from the DQN were, on average, longer than those generated by GPT-4. This is likely due to GPT-4 being constrained by the decision tree rules, which limit the number of features considered.\nAdditionally, certain features, such as RBC, were prominent in the DQN-generated pathways, but absent in the LLM-generated pathways, as they are not included in the decision tree. To further assess pathway similarity, we mea-sured the Levenshtein distance between the most common pathways for each anemia class generated by each approach. We found that the pathways gen-erated by GPT-4 had the highest similarity to those produced by the DQN, with an average Levenshtein distance of 2. In comparison, the average Lev-enshtein distances between the DQN-generated pathways and those generated by LLaMA and Mistral were 2.25 and 2.36, respectively. This indicates that the LLM-generated pathways were more similar to each other than to those generated by the DQN, with the exception of those generated by Mistral and"}, {"title": "LLaMA", "content": "which exhibited the lowest similarity between them.\nIn terms of time (thus energy) efficiency, the trained DQN model quickly generates the diagnostic pathway by following its learned optimal policy, tak-ing significantly less time than the LLMs, which must process the prompt's information at each step."}, {"title": "5.5 Limitations", "content": "While our study demonstrates the potential of LLMs for clinical decision-making, particularly in the diagnostic process, it has several limitations, primarily the use of synthetic data. Given these preliminary results, our current effort is to evaluate our methods on real-world data.\nAnother limitation is the size of the dataset. Although we believe that the dataset used is a good indicator of the models' performance given that no additional model training is involved, we restricted the expense of funding resources by limiting the dataset size for GPT-4 to only a quarter of what was used for LLaMA and Mistral. While this provides a useful performance indication in terms of patient pathways and diagmoses, it should be considered a limitation and should be adjusted accordingly, especially for time comparisons.\nA critical issue encountered is the variability in results due to the man-agement of seeds for variable initialisation. Our findings from using LLAMA with the LangChain library indicate inconsistent seed behavior, particularly in sequential communication sessions with chains of messages. This inconsistency manifests as different results with the same seed and across different seeds. Con-versely, in non-sequential scenarios, results were more consistent with the same seed and similar results with different seeds. Nevertheless, seed management within the system can lead to alterations in execution times and disparate out-comes. Consequently, the seed's unreliability drived us to exclude it from our considerations, emphasizing that results are ultimately contingent on system dynamics.\nAnother limitation observed is with the Sequential prompt and CoT, which is associated with a higher length of inference, associated with a high complexity and length of the prompt itself. Longer and more complex prompts hinder the LLM's ability to precisely follow instructions, leading to inconsistent results. Each of our runs with the CoT introduced variations, as the model navigates through different scenarios and attempts to cover all possible cases. We observed that this complexity made it challenging for the model to consistently adhere to the decision tree rules.\nAdditionally, while some deviations from the decision tree's prescribed rules were noted, these deviations may not necessarily indicate errors. They could potentially represent valid alternative diagnostic pathways. To validate these alternative solutions, a crucial next step is to involve a physician who can assess whether these deviations are indeed correct and whether the LLMs are capable of identifying more efficient or accurate diagnostic pathways.\nFinally, in our comparison with the DRL approach, we acknowledge that multiple runs were conducted, resulting in various models, some of which demon-"}, {"title": "strated better performance than the one used in our study", "content": "This particular DQN model was selected because it had accuracy closest to the mean of the models evaluated in that study."}, {"title": "6 Conclusions and Future Work", "content": "In this study, we used three LLMs, i.e. GPT-4, LLAMA, and Mistral to gener-ate diagnostic pathways for anemia. We employed various prompting techniques and assessed their impact on the models' performance. Additionally, we com-pared these LLMs both to each other and to a DRL approach used in a previous study with a similar objective. Our findings indicate that, in certain scenarios, in particular when domain knowledge is provided in the form of decision rules to the LLMs, they can enhance the decision-making process.\nFor future work, we plan to apply our approach to real-world data and ex-pand the decision tree to include a wider range of anemia types. This expansion will not only provide a more comprehensive evaluation of the LLMs' diagnostic capabilities but also address a broader spectrum of clinical scenarios. Addi-tionally, we intend to explore the applicability of this approach to other clinical conditions beyond anemia. Furthermore, we aim to improve the LLMs' perfor-mance by exploring techniques such as LLM fine-tuning, prompt tuning, and RAG."}]}