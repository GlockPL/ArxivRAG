{"title": "I2VCONTROL-CAMERA: PRECISE VIDEO CAMERA CONTROL WITH ADJUSTABLE MOTION STRENGTH", "authors": ["Wanquan Feng", "Jiawei Liu", "Pengqi Tu", "Tianhao Qi", "Mingzhen Sun", "Tianxiang Ma", "Songtao Zhao", "Siyu Zhou", "Qian He"], "abstract": "Video generation technologies are developing rapidly and have broad potential applications. Among these technologies, camera control is crucial for generating professional-quality videos that accurately meet user expectations. However, existing camera control methods still suffer from several limitations, including control precision and the neglect of the control for subject motion dynamics. In this work, we propose I2VControl-Camera, a novel camera control method that significantly enhances controllability while providing adjustability over the strength of subject motion. To improve control precision, we employ point trajectory in the camera coordinate system instead of only extrinsic matrix information as our control signal. To accurately control and adjust the strength of subject motion, we explicitly model the higher-order components of the video trajectory expansion, not merely the linear terms, and design an operator that effectively represents the motion strength. We use an adapter architecture that is independent of the base model structure. Experiments on static and dynamic scenes show that our framework outperformances previous methods both quantitatively and qualitatively.", "sections": [{"title": "1 INTRODUCTION", "content": "Video generation technologies are explored to synthesize dynamic and coherent visual content, conditioned on various modalities including text (Blattmann et al., 2023c; Wang et al., 2024a; Gupta et al., 2023) and images (Blattmann et al., 2023b; Chen et al., 2024). Video generation has broad application potential across various fields, such as entertainment, social media, and film production. Motion controllability is crucial for ensuring that generated videos accurately meet user expectations, with camera control being one of the most important aspects. Camera control is the process of adjusting the position, angle, and motion of a camera, resulting in changes to the composition, perspective, and dynamic effects of a video. This technique is essential for generating professional-quality videos, as it influences the attention of viewers and enhances the expressiveness of scenes.\nAlthough precise camera control is crucial for producing high-quality videos, existing methods still face challenges. The first challenge pertains to the precision and stability of control. The lack of precision would result in an inaccurate reflection of the user control intention, significantly degrading user satisfaction. The second challenge is ensuring the natural dynamics of the subjects themselves, independent of camera movements. Similar to the challenges in multi-view (Mildenhall et al., 2020; Kerbl et al., 2023) and 3D geometric algorithms (Wang et al., 2021), where static scenes are much easier to handle than dynamic ones (Pumarola et al., 2020; Cai et al., 2022), generating plausible dynamics in videos proves to be more complex than managing static elements.\nWhile AnimateDiff (Guo et al., 2024b) utilizes LoRA (Hu et al., 2022) strategy for controlling camera movements, the motion-LoRAs are confined to a limited set of fixed movement modes, lacking flexibility, and it only allows for coarse control, thus failing to provide precise scale adjustments. A direct and intuitive approach allowing for arbitrary camera movements is embedding the camera pose matrix, as in MotionCtrl (Wang et al., 2023). However, this method results in sparse input signals that heavily rely on the training set distribution, which leads to poor generalization capability."}, {"title": "2 RELATED WORK", "content": "Text-to-video generation requires models to synthesize realistic videos based on given textual descriptions. Recent progress in diffusion models has boosted the quality of T2V generation to an unprecedented degree, achieving both impressive visual quality and surprising text-video consistency (Brooks et al., 2024; Blattmann et al., 2023b). Image Video (Ho et al., 2022) cascaded multiple video generation and super-resolution diffusion models to generate long and high-resolution videos from textual descriptions. Make-A-Video (Singer et al., 2022) extended a diffusion-based T2I model to T2V in a spatiotemporal factorized manner. Based on the successful experiences of image generation methods, several works (Wang et al., 2024a; Girdhar et al., 2023; Mei & Patel, 2023) performed T2V by first generating images from texts and then synthesizing videos based on images. EMU VIDEO (Girdhar et al., 2023) introduced adjusted noise schedules and a multi-stage training strategy for high-quality video generation. To reduce the computational complexity of video generation, other works (Blattmann et al., 2023c; He et al., 2022; Yu et al., 2023; Gupta et al., 2023) explored different designs of video auto-encoders, which can map a high-dimensional video into a low-dimensional latent space. LVDM (He et al., 2022) compressed videos from both the spatial and temporal dimensions, obtaining a low-dimensional 3D latent for each video. In addition, Lumiere (Bar-Tal et al., 2024) and Latte (Ma et al., 2024) explored different 3D model structures. Recently, Sora (Brooks et al., 2024) showed the power of DiT (Peebles & Xie, 2022) in T2V task."}, {"title": "2.2 IMAGE TO VIDEO SYNTHESIS", "content": "Image-to-video task aims to generate videos with a static image as the condition. One classic strategy is integrating CLIP embeddings of the static image into DPMs. For instance, VideoCrafter1 (Chen et al., 2023a) and I2V-Adapter (Guo et al., 2024a) utilized a dual cross-attention layer, similar to the IP-Adapter (Ye et al., 2023), to fuse these embeddings effectively. However, due to the notorious issue of CLIP image encoders losing fine-grained details, subsequent works (Hu, 2024; Wei et al., 2024) have proposed using more expressive image encoders to capture finer image features. In addition, another strategy is to expand the input channels of DPMs to concatenate noisy frames and the static image. Notable works in this category include SEINE (Chen et al., 2023b), Pixel-Dance (Zeng et al., 2024), AnimateAnything (Dai et al., 2023), and PIA (Zhang et al., 2024), which have demonstrated superior results by enhancing the input channels to integrate image information more effectively. Finally, methods such as DynamiCrafter (Xing et al., 2023), I2VGen-XL (Zhang et al., 2023), and SVD (Blattmann et al., 2023a) combined channel concatenation and attention mechanisms to simultaneously inject image features, aiming to achieve consistency in both global semantics and fine-grained details. This dual approach ensured that the generated videos maintained a high level of fidelity to the original static images while introducing realistic and coherent motion."}, {"title": "2.3 VIDEO CAMERA CONTROL", "content": "While methods aiming to control video foundation models continue to emerge, relatively few works explore how to manipulate camera motions in generated videos. AnimateDiff (Guo et al., 2024b) employed temporal motion LoRA (Hu et al., 2022) trained on video datasets with similar camera motions, where one single trained LoRA can control a specific type of camera motion. MotionCtrl (Wang et al., 2023) proposed to employ an adaptor structure to encode the extrinsic matrix of each frame into the temporal attention layers. Further, CamereCtrl (He et al., 2024) utilized the Pl\u00fccker embedding to improve the controllability. Camtrol (Hou et al., 2024) proposed a simple training-free method to directly render static point cloud to multiview frames and construct the final output video in a video-to-video manner. CamCo (Xu et al., 2024) integrated an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps, which keeps 3D-consistent well but causes small motion dynamic. In our work, we propose a method that can enhance the precision of camera control and add the control over subject motion strength."}, {"title": "3 METHOD", "content": "In this section, we introduce the video representation and notation used in this paper. First, we stipulate that the coordinates of all points we study are in the camera coordinate system. Although both the camera and the captured scene may move, we transfer all dynamics to the camera coordinate system, as in Fig. 2. Intuitively, the entire 3D world can be divided into the the static part and the dynamic part, where the static part corresponds to a linear motion in the camera coordinate system.\nConsider a dynamic sequence F(p, \u03bb):"}, {"title": "3.1 VIDEO REPRESENTATION AND NOTATIONS", "content": "F(p, \u03bb) : R\u00b3 \u00d7 [0, A] \u2192 R\u00b3, s.t. F(p, 0) = p\nwhere \u03bb \u2208 [0, \u039b] represents a time moment during the video, and p \u2208 R\u00b3 denotes a point of the entire 3D world. Notice that we specifically enforce F(p, 0) = p, to ensure that F accurately defines the 3D motion trajectory originating from the first frame. Considering the physical properties of the macroscopic world, it is reasonable to consider F as a smooth mapping function. Naturally, we can assert that for any given \u00c0 \u2208 [0, 1], there exist unique Rx \u2208 R3\u00d73 and t\u2081 \u2208 R\u00b3 such that:\nF(p, x) = Rx \u00b7 F(p, 0) + tx + 0(p),\nwhere o(p) denotes an infinitesimal of higher order than p. To simply prove it, we only need to perform a Maclaurin expansion of F(p, A) and F(p, 0) at p = 0:\nF(p, x) = F(0, 1) + JF(0, 1) \u00b7 p + o(p),\nF(p, 0) = F(0,0) + JF(0,0) \u00b7 p + o(p),\nwhere JF denotes the Jacobian matrix, representing the gradient for vector-valued functions. Subtracting the two equations and performing a simple calculation yields:\nF(p, x) = (I + JF(0, 1) \u2013 JF(0, 0)) \u00b7 F(p, 0) + F(0, 1) + o(p).\nEvidently, we can define:\nRx \u2261 I + JF(0, 1) \u2013 JF(0, 0), tx \u2252 F(0, 1),\nSubsequently, we further denote:\nG(p, 1) \u2252 F(p, 1) \u2212 (Rx \u00b7 F(p, 0) + tx) = 0(p),\nwhich actually represents the extent of nonlinearity, being a higher-order infinitesimal with respect to p than the linear term. Up to now, we have introduced the variables (Rx, tx, G(p, x)) to facilitate our forthcoming discussion on video camera control."}, {"title": "3.2 CONTROL SIGNAL CONSTRUCTION", "content": "While the most intuitive method is to directly employ R\u5165 and t\u2081 as the control signals, we aim to overcome the previously mentioned challenges of controllability and subject motion. Denote the region of 3D points captured by the first frame as \u03a9 \u2286 R\u00b3. We compute the linear translation for \u03a9 and project it to 2D, which defines a point trajectory on the camera plane:\nTx = I(Rx \u00b7 \u03a9 + tx), \u03bb \u2208 [0, 1]\nwhere II is the projection operation. Compared to Rx and t\u5165, T\u5165 offers a denser representation, thereby providing enhanced controllability and stability."}, {"title": "3.3 DATA PIPELINE", "content": "In Sec. 3.2, we theoretically analyzed how to derive the input signal (Tx, m\u5165) for camera control. In this section, we show how to compute them for the real-world video data Vgt. For the real-world video, the timesteps is a discrete sequence \u03bb \u2208 [0, T]\u2229\u0396, where A represents the timestep index. The region captured by the first frame can be organized on H \u00d7 W pixels, denoted as \u03a9 = {pij}i,j=1\nFurther, we divide the whole point set {pij}}=1 into the static part and the dynamic part:\nH,W\n\u03a9 = {ij}=1 = \u03a9\u03c2 | \u03a9D,\nHW\nwhere \u03a9\u03c2 denotes the static part, and OD denotes the dynamic part. Different from the theoretical analysis discussed in above sections, there are several major gaps between real-world RGB video data and the continous trajectory function:\n\u2022 Lack of 3D Information: Real-world video data only contains 2D pixels without 3D information.\n\u2022 Lack of Temporal Correspondence: The raw video data does not explicitly involve the information about the temporal movement of dense points as described by the continous trajectory function.\n\u2022 Lack of dynamic/static partition: In real-world video data, discerning which regions are dynamic and which are static remains ambiguous, especially when the camera itself is also mobile. This introduces a coupling between the movement of objects and the motion of the camera.\nTo address the first issue, we employ metric depth estimation method, Unidepth (Piccinelli et al., 2024), to bridge the gap between 2D and 3D data representation. For the second issue, we utilize a tracking method, Cotracker (Karaev et al., 2023), to establish pixel correspondence between consecutive frames, so that we can obtain the discrete trajectory (still denoted as F(p, \u5165) for ease of reading). For the third issue, we need to extract \u03a9s, \u03a9D \u2286 \u03a9 from \u03a9. A key insight lies in Eq. 10, which means the trajectory on \u03a9\u03c2 can be linearly fitted well. We solve this problem in a iterative manner, as described in Alg. 1.\nIteratively, we fit the trajectory and extract the well-fitted region as the updated static region \u03a9\u03c2, while the remaining part is the dynamic part D. Once we obtain the result Os and ND, like in each"}, {"title": "3.4 NETWORK, TRAINING AND INFERENCE", "content": "To ensure our method remains compatible with rapidly evolving base models, we have implemented an adaptive structure. Our network design is illustrated in Fig. 4. Starting from the original control signal, our adapter network generates a control feature that can be integrated into any diffusion process, thereby allowing adaptation to various video generation base frameworks.\nConsidering that Tx is a 4-dim tensor with shape (T, 2, H, W) and mx is a 2-dim tensor with shape (T, 1), we use a tiling method to expand my to the same shape as T\u5165, and then concatenate them along the channel dimension to finally obtain a (T, 3, H, W)-shaped tensor as the input of the network. As shown in Fig. 4 (layers marked with flame are our adaptive layers), we first employ several convolutional layers to convert the input to the same size as the tokens used in the diffusion process. We then concatenate the features with the tokens before computing self-attention. After the self-attention computation, we restore the original shape by removing the additional parts added during concatenation, similar to Hu (2024)."}, {"title": "4 EXPERIMENTS", "content": "In this section, we show our experiments. Sec. 4.1 introduces our implementation details and experiment settings. Sec. 4.2 shows the results and some properties of our method. In Sec. 4.3, we compare our method with previous baseline methods."}, {"title": "4.1 SETTINGS", "content": "We employ a Image-to-Video version of Magicvideo-V2 (Wang et al., 2024b) as our base model, where we set the frame number as 24 and the resolution as 704 \u00d7 448. We use 16 NVIDIA A100 GPUs to train them with a batch size 1 per GPU for 20K steps, taking about 36 hours. During training, we fix the parameters of the base model and only train our adapter part."}, {"title": "4.2 VISUALIZATION RESULTS", "content": "In this section, we show the visualization results of our method, demonstrating both pixel-level controllability and the motion strength adjustment. Due to the length and format of the paper, the results shown below are all in frame-by-frame image format."}, {"title": "4.2.1 \u03a1\u0399\u03a7EL-LEVEL CONTROLLABILITY", "content": "We demonstrate comprehensive pixel-level user controllability in our approach, as illustrated in Fig. 5. Initially, we estimate the metric depth from the input image, and then directly manipulate the RGBD point cloud with control signals to render a preview image. This provides users with an immediate and intuitive visual feedback, labeled as \u201cPreview\" in the figure. Below the direct rendering results, we display the outputs generated by our framework. As observed, the camera pose in the generated results is largely consistent with the preview, indicating that our control system achieves precise pixel-level control. In the first sample, where we set the motion strength to 0, the fox remains static, and the entire image aligns perfectly with the preview. In the second sample, with the motion strength set to 600, the cat is able to walk on the floor. Despite the movement of cat, the camera positioning remains consistent with the preview across all static elements, such as the fireplace. These examples underscore the ability of our framework to maintain pixel-level alignment regardless of the motion strength. This high level of controllability ensures that users can interactively and effortlessly tailor their visual outputs with exceptional precision, epitomizing a truly user-friendly experience."}, {"title": "4.2.2 \u041c\u043eTION STRENGTH ADJUSTMENT", "content": "In Fig. 6, we illustrate the effects of varying motion strength values on the same input image with consistent camera movements. When the motion strength is 0, the image content appears almost stationary. Conversely, as the motion strength is increased, the main objects within the scenes begin to exhibit motion. For instance, in the first example, the camera performs a pan-right movement, shifting the entire scene to the left. At a motion strength of 0, the polar bear remains static, moving uniformly with the background. However, increasing the motion strength allows the bear to move independently, walking naturally and vividly across the frame, giving an impression of freedom and"}, {"title": "4.3 COMPARISONS", "content": "In this section, we compare our results with previous baselines: MotionCtrl (Wang et al., 2023) and CameraCtrl (He et al., 2024). It is important to note that the original MotionCtrl and CameraCtrl differ significantly from our training configurations, including differences in the base model, training set, image resolution, and even the number of frames. Fortunately, they both employ an adapter architecture, allowing their designs to be adaptable to various base models. Therefore, to ensure a fair comparison, we choose to retrain MotionCtrl and CameraCtrl using the same experimental settings and base model (Magicvideo-V2) as ours. In the subsequent text of this section, whenever we refer to MotionCtrl and CameraCtrl, we are referring to the version that have been retrained by us. Considering that the motion-LoRA of AnimateDiff only supports a limited number of fixed camera movement patterns, we excluded it from our comparison."}, {"title": "4.3.1 \u0421\u043eMPARISON ON REALESTATE 10K DATASET", "content": "We compare our method with MotionCtrl and CameraCtrl on the RealEstate10K dataset. Considering that data in this dataset are nearly all static scenes, we set our motion strength to 0. Quantitative comparisons are presented in Tab. 1. Our method significantly outperforms the previous methods in both RotErr and TransErr, consistent with the pixel-level precision control observed in Section 4.2.1. For a qualitative comparison, refer to the left sample in Fig. 7. While our results are largely consistent with the preview, the outputs from CameraCtrl and MotionCtrl exhibit noticeable deviations. The results from MotionCtrl has the right trend but with some extra zoom-in, while CameraCtrl, although correct in the direction of camera movement, applies excessive movement amplitude, resulting in a failure to align at the pixel level with the ground truth. It can be seen that our method is closest to the preview image, which is consistent with the conclusion of quantitative comparison, further confirming the superiority of our controllability. Our results also show the smallest values in terms of FID and MSC, indicating that our method not only produces the highest quality of generated images but also maintains the static nature in static scenes."}, {"title": "4.3.2 \u0421\u043eMPARISON ON DATASET OF MOVABLE OBJECTS.", "content": "We also evaluate our method against MotionCtrl and CameraCtrl on the movable object dataset. Considering it contains movable objects, we experimented with several motion strength values: 0, 200, 400, 600. Quantitative comparisons are presented in Tab. 2. Our method performs best in both RotErr and TransErr. Although Ours-200, Ours-400 and Ours-600 perform slightly worse on these two metrics than Ours-0, they are still better than the comparison methods. Ours-600 achieves the best FID and thus the best image quality. The FID of Ours-0 is slightly higher than that of the other settings. A possible reason for this could be that the movable objects are forcibly held static, resulting in unnatural and insufficiently diverse frames, while diversity is crucial for FID. For MSC, our smallest value (Ours-0) is lower than the comparing methods, and our largest value (ours-600) is higher than the comparing methods, which proves our adjustable motion strength control abality again. Qualitative comparison is shown on the right sample in Fig. 7, where only our method is pixel-level aligned with the ground truth."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced I2VControl-Camera, a precise camera control method designed to enhance the controllability of video generation while maintaining a robust range of subject motion. We successfully addressed the challenge of control stability by employing point trajectories in the camera coordinate system, rather than relying on extrinsic matrices. Additionally, our approach involved modeling higher-order components of video trajectory expansion, enabling the network to precisely perceive and adjust the amplitude of subject motion dynamics. Our method demonstrated superior performance over previous methods in both quantitative and qualitative assessments. Looking forward, possible future work includes extending our framework to include more control modalities, such as drag and motion brush controls. These enhancements will allow for even more detailed and"}]}