{"title": "Measuring Orthogonality in Representations of Generative Models", "authors": ["Robin C. Geyer", "Alessandro Torcinovich", "Jo\u00e3o B. Carvalho", "Alexander Meyer", "Joachim M. Buhmann"], "abstract": "In unsupervised representation learning, models aim to distill essential features from high- dimensional data into lower-dimensional learned representations, guided by inductive biases. Understanding the characteristics that make a good representation remains a topic of ongoing research. Disentanglement of independent generative processes has long been credited with producing high-quality representations. However, focusing solely on representations that adhere to the stringent requirements of most disentanglement metrics, may result in overlooking many high-quality representations, well suited for various downstream tasks. These metrics often demand that generative factors be encoded in distinct, single dimensions aligned with the canonical basis of the representation space.\nMotivated by these observations, we propose two novel metrics: Importance-Weighted Orthogonality (IWO) and Importance-Weighted Rank (IWR). These metrics evaluate the mutual orthogonality and rank of generative factor subspaces. Throughout extensive experiments on common downstream tasks, over several benchmark datasets and models, IWO and IWR consistently show stronger correlations with downstream task performance than traditional disentanglement metrics. Our findings suggest that representation quality is closer related to the orthogonality of independent generative processes rather than their disentanglement, offering a new direction for evaluating and improving unsupervised learning models.", "sections": [{"title": "1 Introduction", "content": "Humans are able to process rich data such as high-resolution images to distill and memorize key information about potentially complex concepts. Similarly, representation learning aims to devise a procedure, often unsupervised, to encode potentially high-dimensional data into a lower-dimensional learned embedding space, such that classifiers or other predictors can easily extract information from the learned representations (Bengio et al., 2013). Understanding how to generate these convenient representations, requires the definition of desirable properties that representation learning models should enforce. In the domain of generative models, the ability to disentangle the explanatory factors underlying the data has long been credited to be such a desirable property."}, {"title": "2 Related work", "content": "Alongside the task of disentanglement, gauging a model's performance in disentangling a representation has emerged as a non-trivial problem. Beyond visual inspection of the results, a variety of quantitative methodologies have been developed to tackle this issue. Higgins et al. (2017) propose to measure the accuracy of a classifier predicting the position of a fixed generative factor. Kim & Mnih (2018) further robustify the metric by proposing a majority voting scheme related to the least-variance factors in the representations. Chen et al. (2018) introduce the Mutual Information Gap estimating the normalized difference of the mutual information between the two highest factors of the representation vector. Eastwood & Williams (2018) propose the DCI metrics to evaluate the correlation between the representation and the generative factors. For each of them, one linear regressor is trained and the entropy over the rows (Disentanglement) and the columns (Completeness) is computed, along with the error (Informativeness) achieved by each regressor. The Modularity metric introduced by Ridgeway & Mozer (2018) computes the mutual information of each component of the representation to estimate its dependency with at most one factor of variation. SAP score (Kumar et al., 2018) estimates the difference, on average, of the two most predictive latent components for each factor.\nThe use of metrics such as the aforementioned ones contributed to shaping several definitions of disentanglement, each encoding a somewhat different aspect of disentangled representations, which led to a fragmentation of definitions (Locatello et al., 2019). Higgins et al. (2018) attempt instead to propose a unified view of the disentanglement problem, by defining Symmetry-Based Disentangled Representation Learning (SBDRL), a principled framework drawn from group representation theory. The authors established disentanglement in terms of a morphism from world states to decomposable latent representations, equivariant with respect to decomposable symmetries acting on the states/representations. For a representation to be disentangled, each symmetry group must act only on a corresponding (multidimensional) subspace of the representation. Following this conceptualization, Caselles-Dupr\u00e9 et al. (2019) demonstrate the learnability of such representations, provided the actions and the transitions between the states. Painter et al. (2020) extend the work by proposing a reinforcement learning pipeline to learn without the need for supervision. Noteworthy are the two proposed metrics: (i) an independence score that, similarly to our work, estimates the orthogonality between the generative factors in the fashion of a canonical correlation analysis; (ii) a factor leakage score, extended from the MIG metric to account for all the factors. Tonnaer et al. (2022) formalize the evaluation in the SBDRL setting and proposed a principled metric that quantifies the disentanglement by estimating and applying the inverse group elements to retrieve an untransformed reference representation. A dispersion measure of such representations is then computed. Note that while most works focus on the linear manipulation of the latent subspace, the SBDRL can also be used in non-linear cases. However, SBDRL requires modelling the symmetries and the group actions, a challenging task in scenarios with no clear underlying group structure (Tonnaer et al., 2022). We aim to develop a metric which does not require such a group structure and works in more general cases.\nRecently, several works (Montero et al., 2021; Tr\u00e4uble et al., 2021; Dittadi et al., 2021) have proposed to go beyond the notion of disentanglement, advocating for the relaxation of the independence assumption among generative factors - perceived as too restrictive for real-world data problems - and modelling their correlations. Reddy et al. (2022) and Suter et al. (2019) formalize the concept of causal factor dependence, where the generative factors can be thought of as independent or subject to confounding factors. The latter"}, {"title": "3 Methodology", "content": "Our goal is to establish a metric that quantifies the total orthogonality of a representation. This involves estimating the orthogonality between the latent subspaces corresponding to each generative factor. However, this raises two challenges.\nFirst, the identification of the latent subspaces is not straightforward since the generative factor can exhibit non-linear behaviour with respect to the learned representation. We propose Generative Component Analysis (GCA), a procedure where, for each generative factor, multiple non-linear regressors are used to identify progressively smaller subspaces where most of the generative factor's information resides. These subspaces are then used to identify the generative components, that is, the set of orthonormal vectors spanning the latent subspace. In a similar fashion to procedures such as linear principal component regression, we weight the vectors by an importance score to obtain an importance-weighted orthogonal (i.o.o) basis, for each generative factor.\nSecond, prominent methods for measuring orthogonality between subspaces do not provide sufficiently discriminative results for our use case. The conventional definition of orthogonal complement is binary and too restrictive for nuanced applications. Conversely, methods like canonical correlation analysis, which determine similarity between vector spaces, typically operate under optimal conditions and may assign high scores even to spaces sharing only a single dimension. These approaches, while useful, tend to be overly permissive for our specific objectives. We therefore propose Importance-Weighted Orthogonality (IWO) to characterize the orthogonality between generative factors. This is done by computing an average weighted projection of each generative factor's latent subspace onto all the others."}, {"title": "3.1 Generative Component Analysis (GCA)", "content": "Consider a latent representation or code, $c \\in \\mathbb{R}^L$ encoding the generative factors $(z_1,...,z_K) \\in \\mathbb{R}^K$ with $L > K$. Note that $z_j = f(c)$, with $f$ being a potentially complex non-linear function. Not all changes in $c$ imply a change in $z_j$. In particular, we define the invariant latent subspace of $z_j$ to be the largest linear subspace $I_j \\subset \\mathbb{R}^L$, such that $f (c + v) = f(c), \\forall v \\in I_j$. Accordingly, the variant latent subspace (simply latent subspace) of $z_j$ is defined to be the orthogonal complement of $I_j$ and will be denoted as $S_j$, with dimensionality $R_j$. In the next paragraph, we describe how to find an importance-weighted basis for $S_j$.\nSubspace learning Starting from a code $c \\in \\mathbb{R}^L$, we project it onto progressively smaller dimensional subspaces, removing the least important dimension for regressing $z_j$ at each step, until the subspace is 1-dimensional. In particular, we design a Linear Neural Network (LNN) composed of a set of projective transformations $W_L,..., W_1$, which reduce the dimensionality of $c$ step-by-step. No non-linearities are applied, therefore each layer performs a projection onto a smaller linear subspace. The entire learning process is depicted in Figure 2.\nAt every layer of the LNN, we feed the intermediate projection $w_l \\in \\mathbb{R}^l$ into a non-linear neural network $f_{jl}$. These networks are tasked with regressing $z_j$. This approach is designed to discern the most informative latent subspace of dimensionality 1, thereby guiding the selection of which latent dimension to discard in each projection step."}, {"title": "3.2 Importance Weighted Orthogonality (IWO)", "content": "Orthogonality is commonly treated as a dichotomous attribute; that is, vectors are classified as either orthogonal or non-orthogonal, and similarly, a subspace is considered to either reside in the orthogonal complement of another or not. The concept of cosine similarity provides a continuous measure of the degree of orthogonality between two vectors. Analogously, we aim at a continuous measure that evaluates the degree of orthogonality between two subspaces. Consider two orthonormal bases $B_j = {b_1^{(j)},...,b_{R_j}^{(j)}}$ and $B_k = {b_1^{(k)},...,b_{R_k}^{(k)}}$ spanning two subspaces $S_j$ and $S_k$. Let $r_l^{(j \\rightarrow k)}$ be the overall projection of $B_j$'s basis vector $b_l^{(j)}$ onto all the basis vectors in $B_k$:\n\n$r_l^{(j \\rightarrow k)} = \\sum_{m=1}^{R_k} (b_l^{(j)} \\cdot b_m^{(k)})^2 \\qquad l = 1,..., L,$\n\nwhere the square is applied to guarantee non-negativity. With these projections, a continuous interpretation of orthogonality between $S_j$ and $S_k$ can be expressed as\n\n$O(S_j, S_k) = \\frac{1}{min(R_j, R_k)} \\sum_{l=1}^{R_j} r_l^{(j \\rightarrow k)}$.\n\n$O(S_j, S_k) \\in [0, 1]$ with its maximum reached if $S_j$ is a subspace of $S_k$ or vice-versa, and its minimum reached when $S_k$ lies in the orthogonal complement of $S_j$. This definition of orthogonality can be interpreted as the average squared cosine similarity between any vector pair from $S_j$ and $S_k$. However, when using it to gauge the orthogonality between a generative factor's latent subspaces, the importance of the basis vectors spanning the subspace is not taken into consideration.\nMetrics As the name suggests, in addition to encapsulating the orthogonality between factor subspaces, IWO also takes into consideration the importance of the dimensions spanning them. Let us consider $K$ different generative factors $z_1,..., z_K$. For each one of them, we are able to allocate an i.o.o. basis $B_k = {b_1^{(k)}..., b_{R_k}^{(k)}}$ with the importance weights {$\\alpha_1^{(k)}...,\\alpha_{R_k}^{(k)}$}. For any ground truth factor $z_j$, we define $r_l^{(j \\rightarrow k)}$ as $b_l^{(j)}$'s projection onto the latent subspace of another ground truth factor $z_k$, scaling the projection by the importance of the respective basis vectors:\n\n$r_l^{(j \\rightarrow k)} = \\sum_{m=1}^{R_k} \\sqrt{\\alpha_l^{(j)} \\alpha_j \\alpha_m^{(i)}} (b_l^{(j)} \\cdot b_m^{(k)})^2 \\qquad l = 1,..., L$.\n\nAnalogously to subspace orthogonality, IWO is then defined using the sum over all projections $r_l^{(j \\rightarrow k)}$ of the dimensions spanning $z_j$'s subspace. However, in order to align IWO with commonly used disentanglement metrics, it is defined as the complement of the sum:\n\n$IWO(z_j, z_k) = 1- \\sum_{l=1}^{R_j}r_l^{(j \\rightarrow k)}$.\n\nNote that, with respect to Equation 6, IWO does not require a normalization factor as the square root in Equation 7 guarantees that IWO $ \\in [0, 1]$. In addition, we invert the orthogonality by subtracting from 1, to"}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of our IWO and IWR implementations, we first test whether we can (1) recover the true latent subspaces using GCA and (2) correctly assess their IWO and IWR. For that purpose, we set up a synthetic data generation scheme, providing us with the ground truth IWO and IWR values. For comparison purposes, we also test how other metrics assess the synthetic data, namely Disentanglement, Completeness, Informativeness and Explicitness, as measured by the DCI-ES framework.\nWe further evaluate IWO through the disentanglement lib framework (Locatello et al., 2019). We measure how strong IWO and IWR correlate with downstream task performance for three different tasks deployed on the learned representations of six widely used variational autoencoder models trained on six benchmark disentanglement datasets and over a wide range of seeds and hyperparameters. More details are listed in the appendix and in our open-source code implementation\u00b9.\nTraining of all neural networks in the LNN is performed in parallel. In principle, the gradient flow from each individual regressor $f_{jl}$ can be stopped after the corresponding $W_l$, however, we attested a faster convergence when letting the gradient of each $f_{jl}$ flow back up to $c$. The aim of simultaneously exploring all nested subspaces is to facilitate the identification of the smallest subspaces by guidance through the larger ones. Additionally, we assume a reduction factor of 1, so that $W_l \\in \\mathbb{R}^{l \\times l+1}$ for $l = 1, . . ., L$, however, higher values can also be considered for larger representations."}, {"title": "4.1 Synthetic Experiments", "content": "We introduce a synthetic data generating scheme, which generates vectors of i.i.d Gaussian distributed latent representations $c \\in \\mathbb{R}^L$. Then, on the basis of the latent representations, we synthesize $K$ generative factors {$z_1,..., z_K$}. For simplicity, we choose $L$ as a multiple of $K$.\nFor simulating a disentangled latent space, we define each $z_j$ to be linearly dependent on a single, distinct element of $c$. To assess higher dimensional cases with non-linear relationships, we consider a non-linear commutative mapping $f : \\mathbb{R}^{R_j} \\rightarrow \\mathbb{R}$. In particular, we experiment with a polynomial (Poly.) and a trigonometric (Trig.) $f$. Notice that the commutativity enforces that the distribution is spread evenly across all dimensions, such that we can easily assess the performance of IWR($z_j$). For simplicity, we always set $R_j = R$ for all $j = 1,..., K$. Together, $L$ (latent space dimension), $K$ (number of factors) and $R$ (latent subspace dimension) determine how many dimensions each generative factor shares with the others. We consider the shared dimensions to be contiguous. Refer to Figure 5 in the Appendix for intuition.\nTo test representations that are not aligned with the canonical basis, we apply Random Orthogonal Projections (ROP) $R \\in \\mathbb{R}^{L \\times L}$ to $c$. In line with commonly used datasets such as Cars3D or dSprites, we rescale and quantize $z_j$ to the range [0, 1]."}, {"title": "4.2 Downstream Experiments", "content": "For the systematic evaluation of IWO's and IWR's correlation with downstream task performance, we use the disentanglement_lib framework (Locatello et al., 2019). We consider six benchmark datasets, namely dSprites, Color-dSprites and Scream-dSprites (Matthey et al., 2017), Cars3D (Reed et al., 2015), SmallNorbs (LeCun et al., 2004) and Shapes3D (Burgess & Kim, 2018). These datasets cover a wide range of complexities and variations, for more information on the individual datasets refer to Appendix C.2.\nOn each of these datasets, six different commonly used Variational Auto Encoder (VAE) models are trained: B-VAE Higgins et al. (2017), Annealed VAE (Burgess et al., 2018), \u03b2-TCVAE (Chen et al., 2018) Factor- VAE (Kim & Mnih, 2018), DIP-VAE-I and DIP-VAE-II (Kumar et al., 2018). These models represent a diverse set of approaches to disentanglement, each introducing unique mechanisms to encourage factorized representations. For each model, we consider six different regularization strengths (cf. Appendix C.1), each with ten different random seeds, resulting in a total of 2160 learned representations.\nThe representations are evaluated for their utility in regressing the generative factors. To this end, we consider three distinct downstream models trained on the learned representations: (i) random forest, (ii) logistic regression, and (iii) multi-layer perceptron. Correlations between downstream task performance and the commonly used metrics, DCI-D, DCI-C, and MIG, together with IWO's and IWR, are calculated throughout the considered regularization strengths.\nOur main objective is to investigate whether the orthogonality of a representation is indicative of the performance on a variety of downstream tasks and thus a good metric for the utility or quality of the representation, as elucidated by Wang & Isola (2020). The primary distinction between Orthogonality and Disentanglement, as measured by the considered metrics, is that the former does not require alignment with the canonical basis of the representation space. The three downstream models are chosen to distill this key difference. While we expect alignment with the canonical basis to benefit downstream task (i) random forest, we do not expect such a benefit for tasks (ii) logistic regression or (iii) multi-layer perceptron."}, {"title": "5 Discussion and Conclusions", "content": "In our investigation, we pivoted from the conventional focus on the disentanglement of generative factors to a novel examination of their orthogonality. Our approach, geared towards accommodating non-linear behaviors within linear subspaces tied to generative factors, fills a gap in existing literature, as no prior method aptly addressed this perspective. The proposed Generative Component Analysis (GCA) efficiently identifies the generative factor subspaces and their importance. Leveraging GCA, we formulated Importance-Weighted Orthogonality (IWO), a novel metric offering unique insights into subspace orthogonality. Throughout experiments, our implementation emerged as a robust mechanism for assessing orthogonality, exhibiting resilience across varying latent shapes, non-linear encoding functions, and degrees of orthogonality.\nDisentanglement has long been credited for fostering fairness, interpretability, and explainability in generated representations. However, as pointed out by Locatello et al. (2019), the utility of disentangled representations invariably hinges on at least partial access to generative factors. With such access, an orthogonal subspace could be rendered as useful as a disentangled one. Through orthonormal projection, any orthogonal representation discovered can be aligned with the canonical basis, achieving good disentanglement.\nIn conjunction with IWO's stronger downstream task correlation across datasets, models and tasks, this underscores our assertion that latent representations, which successfully decouple generative factors, are crucial for a wide range of downstream applications, regardless of their alignment with the canonical basis.\nIn conclusion, our work lays the groundwork for a fresh perspective on evaluating generative models. We hope GCA and IWO may help identify models crafting useful orthogonal subspaces, which might have been overlooked under the prevailing disentanglement paradigm. We hope that IWO extends its applicability across a broader spectrum of scenarios compared to traditional disentanglement measures."}, {"title": "A Explicitness vs IWO", "content": "The Explicitness (E) metric aims to evaluate the capacity needed for a representation to regress its generative factors. Formally,\n\n$E(z_j, c; \\mathcal{F}) = 1 - \\frac{AULCC(z_j, c; \\mathcal{F})}{Z(z_j; \\mathcal{F})}$,\n\nwhere $z_j$ and $c$ represent a generative factor and the latent space respectively, and $ \\mathcal{F}$ a class of regressors (e.g., multilayer perceptrons or random forests). $AULCC$ is the Area Under the Loss Curve, computed by recording the minimum losses achievable by regressing $z_j$ from $c$ with models in $ \\mathcal{F}$ of increasing capacity. The denominator, easily computable, acts as a normalizing constant so that $E \\in [0,1]$. As an example, $E = 1$ suggests that a linear regressor is sufficient to reach zero error, proving that the representation is efficient. To account for a bias toward large representations, the explicitness is paired with the Size (S), computed as the ratio between the number of generative factors and the size of the latent representation.\nIn Figure 4, we depict four situations of a 3-dimensional latent space where two generative factors $z_1, z_2$ lie in a separate 2-dimensional plane each, and the relationship between each generative factor and its corresponding latent subspace is non-linear, as hinted by the coloring. In particular, $z_j^{(n)} = f^{(n)} (c') = f^{(n)} (P^{(n)} c)$, for $j\\in {1,2}$ and $n \\in {i, ii, iii, iv}$, with $P^{(n)} \\in \\mathbb{R}^{2 \\times 3}$ being a projection matrix. For cases (i) and (iii), the projections span orthogonal planes, contrarily to cases (ii) & (iv) where the planes have a different inclination. Case (i) & (ii) are characterized by a quadratic relationship, i.e., $f^{(n)} = A(c_1)^2 + B(c_2)^2$ (with A, B being parameters), while case (iii) and (iv) encode a trigonometric relationship, i.e., $f^{(n)} = cos(2\\pi\\lambda_1) + cos(2\\pi\\lambda_2)$ (with A being a parameter).\nExplicitness evaluates the capacity required by a model to regress the generative factors $z_1, z_2$, starting from the representation $c$. Given that the effect of the linear transformation is present in all four situations, the differences are determined only by the non-linearity $f^{(n)}$. Therefore the metric is able to discriminate cases (i) & (ii) from (iii) & (iv). Instead, IWO quantifies the orthogonality of the planes, regardless of the non-linearities in the generative factors, so it discriminates the orthogonal cases (i) & (iii) from the non-orthogonal ones (ii) & (iv). Finally, note that the DCI-Disentanglement metric would penalize all four configurations as they are not disentangled."}, {"title": "B Efficient IWO Calculation", "content": "For the efficient calculation of Orthogonality, consider two matrices $B_j \\in \\mathbb{R}^{R_j \\times L}$, $B_k \\in \\mathbb{R}^{R_k \\times L}$ whose rows compose the i.o.o. basis vectors spanning $z_j$'s and $z_k$'s latent subspaces respectively. We define the orthogonality between the two latent subspaces in terms of these matrices as:\n\n$O(S_j, S_k) = \\frac{Tr(B_j B_j^T B_k B_k^T)}{min(R_j, R_k)}$\n\nNote that the trace $Tr(B_j B_j^T B_k B_k^T)$ equals the sum of the squared values in $B_j B_k^T$, i.e., $\\sum_{l,m}((B_j B_k^T)_{ml}) = \\sum_{l,m}(b_{jl} \\cdot b_{km})^2$, where $b_{jl}$ and $b_{km}$ are the $l$-th and $m$-th rows of $B_j$ and $B_k$ respectively. The maximum of the trace is therefore $min(R_j, R_k)$, reached if $S_j$ is a subspace of $S_k$ or vice-versa. This definition of orthogonality can be interpreted as the average absolute cosine similarity between any vector pair from $S_j$ and $S_k$.\nTo efficiently calculate the importance-weighted projection of $z_j$'s subspace onto $z_k$'s subspace, we first scale the corresponding bases vectors in $B_j, B_k$ with their respective importance before projecting them onto one another. IWO is the sum of all individual projections. Using $U_j = D_j B_j$, where $D_j \\in \\mathbb{R}^{R_j \\times R_j}$ is diagonal with the $l$-th diagonal entry corresponding to the square root of importance, $\\sqrt{\\alpha_l(z_j)}$, we can efficiently calculate IWO as:\n\n$IWO(z_j, z_k) = 1 - Tr(U_j U_j^T U_k U_k^T)$"}, {"title": "C Experimental details", "content": "In this section, we provide a detailed description of the correlation analysis of our orthogonality metric with downstream tasks on six different datasets and six different models listed in 3. For each model, learned representations for six different regularization strengths are considered (ten different random seeds for each reg. strength). All these representations are directly retrieved from or trained with the code of disentanglement lib\u00b2. For the ES metric, we utilized the official codebase provided by the authors of DCI-ES 3. Each dataset we investigate has independent generative factors associated with it."}, {"title": "C.1 Hyperparameters", "content": "The hyperparameters considered for each model are the following:\n\n\\bullet B-VAE (Higgins et al., 2017): with $\\beta \\in$ {1, 2, 4, 6, 8, 16}\n\\bullet Annealed VAE (Burgess et al., 2018): with $C_{max} \\in$ {5, 10, 25, 50, 75, 100}\n\\bullet \u03b2-TCVAE (Chen et al., 2018): with \u03b2\u2208 {1, 2, 4, 6, 8, 10}"}, {"title": "C.2 Datasets", "content": "DSprites Dataset The DSprites dataset is a collection of 2D shape images procedurally generated from six independent latent factors. These factors are color (white), shape (square, ellipse, heart), scale, rotation, and x and y positions of a sprite. Each possible combination of these latents is present exactly once, resulting in a total of 737280 unique images.\nColor DSprites Dataset This dataset retains the fundamental characteristics of the original DSprites dataset, with the distinct variation that each sprite, in the observation sampling process, is rendered in a color determined by random selection.\nScream DSprites Dataset The dataset mirrors the original DSprites dataset but introduces a unique modification in the observation sampling process: A random segment from the Scream image is selected as"}, {"title": "C.3 IWO on limited data", "content": "To assess the impact of smaller sample sizes on IWO and IWR's correlation with downstream task performance, we repeat the experiments detailed in Section 4.2 for the smallNORB dataset, but with only 50% and 10% of the data. The results are illustrated in Table 4. We observe that IWO and IWR are resilient to changes in dataset size."}, {"title": "C.4 IWO Training", "content": "Given a learned representation of a dataset, we consider each generative factor independently, allocating separate LNNs respectively. On top of the LNNs, we have NN heads, which regress the generative factors from the intermediate projections. The NN heads are also independent from one another and do not share any weights."}, {"title": "C.4.1 Implementation Details", "content": "We use the PyTorch Lightning framework for the implementation of the models required to discern IWO and IWR. In particular, we use PyTorch Lightning implementations of Linear layers and Batch-Normalization Layers. Whereas the setup of the LNN is equal for all models and datasets, the NN-heads vary in their complexity for different datasets and factors. As all considered models operate with a 10-dimensional latent space, each LNN has ten layers. The output of each LNN layer is fed to the next layer and also to the corresponding NN-head.\nTable 5 holds the NN head configuration per dataset and factor. These were found using a simple grid search on one randomly selected learned representation. This is necessary as factors vary in complexity and so does the required capacity to regress them. It is worth mentioning, that the Explicitness pipeline, as proposed by Eastwood et al. (2023), could actually be employed on top of the NN-heads, integrating both metrics.\nFor the initialization of the LNN layers and the NN heads, we use Kaiming uniform initialization as proposed in He et al. (2015). We further use the Adam optimization scheme as proposed by Kingma & Ba (2014) with a learning rate of 5 x 10-4 and a batch size of 128 for all optimizations. Data is split into a training (80%) and a test set (20%). During training, part of the training set is used for validation, which is in turn used as an early stopping criterion. The importance scores used for IWO should be allocated using the test set. In our experiments, the difference between the importance scores computed on the training set and the test set was small. For further details on the implementation, please refer to our official code 5."}, {"title": "D Importance analysis", "content": "In order to test the effectiveness of IWO and IWR's importance weighing, we compare their downstream task correlation with the downstream task correlation of pure orthogonality (without any weighing). Table 6 holds the results of the"}, {"title": "E Loss analysis", "content": "In Figure 6 we depict the loss of neural network heads at different projection steps for a single run of synthetic experiment 3 (L = 10, K = 5 and Rj = 5). L6 to L10 are omitted, as they are almost zero (similar to L5). Each generative factor is analysed using GCA, which means for each generative factor we train an LNN spine with 9 matrices $W_9 \\in \\mathbb{R}^{9 \\times 10}$ ... $W_1 \\in \\mathbb{R}^{1 \\times 2}$ and 10 NN heads acting on the projections. Because of the symmetry of the synthetic experiments, all five generative factors are similarly encoded in the latent space. In Figure 6 we are therefore depicting the mean and standard deviation over the generative factors. An entire pass through each LNN projects the representation to the most informative dimension for the respective generative factor. When recovering the generative factor from that projection, we incur a loss of $L_1$. The fraction $L_1/L_o$ tells us that this is \\approx 20% better than naive guessing (assuming the expectation value) of the factor. We see that we can almost perfectly recover the generative factors from projections to 5 and more dimensions. This is expected as the experiment is set up with $R_j = 5$. We see that each subsequently removed dimension increases the loss by \\approx 20%. It follows that $\\Delta L_1/L_o \\approx 0.2$, i.e. $\\alpha_l \\approx 0.2$ for $1 < l < 5$. The right-hand side of Figure 6 depicts the relative validation loss of the NN heads during training. The relative loss at the 72nd iteration corresponds to the relative loss depicted in the left plot."}, {"title": "F Computational Resources Analysis", "content": "This section details the computational resources utilized for evaluating DCI, IWO and IWR, specifically applied to the smallNORB dataset from disentanglement lib using a B-VAE framework. For the GCA model specifications please refer to section C.4.1"}, {"title": "F.1 Experimental Setup", "content": "\u2022 Data: Learned representations of a \u03b2-VAE trained on the smallNORB dataset from the disentanglement_lib\n\u2022 Objective 1: Assess the orthogonality of 60 learned representations, by performing GCA and calculating IWO/IWR. Note that the computational resources for the calculation of IWO/IWR are negligible compared to GCA.\n\u2022 Objective 2: Assess the disentanglement of 60 learned representations, by computing the DCI metric."}, {"title": "F.2 Resource Utilization", "content": "In Table 7, we list the computational resources used for each run of GCA."}, {"title": "F.3 Runtime Analysis", "content": "\u2022 GCA Average Duration: 7 minutes per run (20 Epochs)\n\u2022 DCI Average Duration: 4 minutes per run."}, {"title": "F.4 Computational Cost Considerations for GCA", "content": "\u2022 Performing GCA on pretrained smallNorb representations shows small computational costs, comparable to those necessary for computing DCI.\n\u2022 For GCA computational costs scale with the number of linear layers in the LNN spine and the capacity of the NN heads.\n\u2022 For large latent spaces, one should avoid step-wise dimensionality reduction in the LNN spine; larger reductions between consecutive LNN layers are preferred.\n\u2022 The first LNN layer size need not match the dimensionality of the representation. For large representations, a smaller first LNN layer is recommended.\n\u2022 GPU usage is beneficial for larger representations and models"}]}