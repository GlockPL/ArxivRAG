{"title": "Measuring Visual Sycophancy in Multimodal Models", "authors": ["Jaehyuk Lim", "Bruce W. Lee"], "abstract": "This paper introduces and examines the phenomenon of \u201cvisual sycophancy\" in multimodal language models, a term we propose to describe these models' tendency to disproportionately favor visually presented information, even when it contradicts their prior knowledge or responses. Our study employs a systematic methodology to investigate this phenomenon: we present models with images of multiple-choice questions, which they initially answer correctly, then expose the same model to versions with visually pre-marked options. Our findings reveal a significant shift in the models' responses towards the pre-marked option despite their previous correct answers. Comprehensive evaluations demonstrate that visual sycophancy is a consistent and quantifiable behavior across various model architectures. Our findings highlight potential limitations in the reliability of these models when processing potentially misleading visual information, raising important questions about their application in critical decision-making contexts.", "sections": [{"title": "1 Introduction", "content": "Multimodal machine learning models, which integrate and reason across multiple modalities such as vision, language, and audio, have become increasingly significant in artificial intelligence research and applications (Manzoor et al., 2023; Zhang et al., 2023). These models develop richer representations by leveraging complementary information from diverse data types, enabling them to address complex tasks that span multiple domains (Achiam et al., 2023; Anthropic, 2024; Dubey et al., 2024; Reid et al., 2024).\nVision-language models (VLMs), a subset of multimodal models, typically employ one of three main architectures: external vision encoders (Radford et al., 2021), cross-attention mechanisms (Alayrac et al., 2022; Tang et al., 2023), or end-to-end transformers (Achiam et al., 2023; Anthropic, 2024). While the specifics of these architectures differ, they all necessitate the integration of visual and textual modalities within the model to perform few-shot or zero-shot classification of provided images or to engage in natural conversation in relation to visual prompts\nModality Bias Distinct from Social Bias. It is necessary to distinguish the terminology of \"bias\" before we proceed with our investigation. Modality bias is distinct from the bias that the model displays due to the existence of unequal representation in the training data.\nSocial bias, a well-studied phenomenon in AI research, refers to the unfair or prejudiced treatment of certain groups or individuals based on characteristics such as race, gender, age, or socioeconomic status Ferrara (2023). In uni- and multimodal models, social bias often stems from imbalances or stereotypes present in the training data, leading to outputs that reflect and potentially amplify societal inequalities Akter et al. (2021); Shah and Sureja (2024); Starke et al. (2022). This type of bias has been extensively documented and remains a critical ethical concern in AI development.\nHowever, our research focuses on a different, less explored form of bias: modality bias. Modality bias refers to the tendency of a multi-modal model to rely disproportionately on one modality (e.g., vision) over another (e.g., text) when making predictions or generating outputs Guo et al. (2023); Shtedritski et al. (2023). Unlike social bias, which primarily reflects societal inequalities from which the model"}, {"title": "", "content": "samples, modality bias is a technical challenge inherent to the architecture and training of multi-modal systems. As foundational models are increasingly deployed to automate essential tasks across various domains, it is crucial to understand how training dynamics, architectural choices, causal mechanisms, and inference-time interventions may shape a model's preference for one modality over another. This preference can lead to suboptimal performance or inconsistent behavior when processing multimodal inputs.\nBias as Systematic Deviation from the Mean. In probability theory and cognitive psychology, bias is often measured as a systematic additive or subtractive deviation from the mean, distinct from variance, which measures data spread. Probability Theory plus Noise (PT+N) model predicts systematic deviations from normative probabilities in human judgment, attributing these to random noise in memory sampling processes rather than additive biases in log odds (Howe and Costello, 2020). While the inner mechanisms of language models differ from human probability judgment, the concept of measuring bias as systematic deviation from the mean remains a useful observational metric in both domains.\nExtending Keypoint Localization. Shtedritski et al. (2023) explored keypoint localization in vision- language models, particularly CLIP. By highlighting portions of the input image, they directed the model's attention to specific subsets while maintaining global information, achieving state-of-the-art performance on referring expression comprehension tasks. Our work extends these localization methodologies to a broader range of large vision-language models, including both proprietary models and popular open-source models such as LLAVA (Liu et al., 2024). This comprehensive approach allows us to evaluate the generalizability of localization methods across diverse model architectures and training paradigms.\nModality Gap and The Platonic Representation Hypothesis. Recent research has investigated the modality gap or the separability of data points between modalities in the embedding space. Jiang et al. (2024) observed a significant gap between textual and visual information, finding that insufficient cross-modal representation alignment correlates with high rates of hallucinations. The Platonic Representation Hypothesis (PRH) posits that large-scale multimodal models may converge towards a shared statistical representation of reality Huh et al. (2024). While not the direct focus of our experimental work, this hypothesis provides context for understanding modality bias, suggesting that a model's preference for one modality over another could indicate a divergence from a shared statistical representation.\nDefining Visual Sycophancy and User Intent. Past research has examined various anthropomorphic, behavioral properties of large models, including corrigibility Perez et al. (2022), sycophancy Denison et al. (2024); Sharma et al. (2023), and truthfulness Campbell et al. (2023); Evans et al. (2021). Our study focuses on visual sycophancy, a specific case where a concept or object is redundantly presented across two modalities (text and vision), and the model displays a systematic bias towards the option that shows user intent or emphasis.\nWe examine this phenomenon by using keypoint localization as a proxy for user intent, where highlighted portions of the image or contrastively colored texts provide additional information not present in the text alone Shtedritski et al. (2023). Through counterfactual comparisons between neutral conditions and biased variations, we analyze how visual cues affect the model's responses, measuring both the direction and magnitude of changes. This approach allows us to assess the extent to which the model may prioritize perceived user preferences over providing the most accurate or truthful response.\nBuilding upon the theoretical framework of modality bias and visual sycophancy outlined in our introduction, we now turn to the practical challenge of empirically measuring these phenomena in large language models. Our methodology aims to operationalize the concept of visual sycophancy through a series of carefully designed experiments. By transforming established benchmarks into multimodal representations and systematically manipulating visual cues, we create a controlled environment to observe how models respond to subtle visual emphasis. This approach allows us to quantify the extent to which models prioritize visually emphasized information over their prior knowledge or training, directly addressing our research questions about the nature and extent of visual sycophancy in multimodal AI systems. In the following section, we detail our experimental design, data preparation, and analytical methods, providing a clear path from our theoretical considerations to concrete, measurable outcomes."}, {"title": "2 Method", "content": "To empirically investigate the phenomenon of visual sycophancy in multimodal language models, we designed a series of experiments to quantify the influence of visual cues on model outputs.\nEvaluation Bias. Quantifying a model's bias on a multiple-choice benchmark is relatively straight- forward compared to quantifying bias on a free-generation task. Nevertheless, quantification remains challenging due to the multidimensional nature of bias. In the context of social biases, a model's output may be biased in content, style, or framing Bang et al. (2024), necessitating multidimensional metrics. However, in the context of modality bias that skews towards a localized key point, where the evaluation metric is percent correctness and a ground truth answer exists, a one-dimensional metric may effectively capture and quantify the bias, as the focus is on a single, well-defined aspect of performance.\nOur evaluation of bias measures the shift in the distribution of answers and log probabilities between biased variations. This involves comparing answer distributions across variations, including the distribution of ground truth answers. We also calculate a bias percentage, which represents the proportion of answers that changed in both directions. For GPT-40-mini and LLAVA, we conduct an analysis of the shift in the distribution of the top 4 answer token log probabilities across variations.\nPre-marked Visual Prompt Generation. For each neutral prompt in our benchmarks, we generated biased variations through visual localization techniques. This process involved creating visually distinct versions of the same question, each emphasizing a different answer option to simulate user intent.\nFor the Visual MMLU (vMMLU) benchmark, we produced two types of variation formats. The first format uses a filled-in bubble with colored text: we filled in the bubble next to one answer option and colored its text, creating a visual emphasis on that option. This highlight suggests user intent. The second format employs a size variation: we doubled the font size of the biased option without highlighting or bubbling the option. This size change does not explicitly suggest user intent.\nFor the Visual Social IQa (vSocialIQa) benchmark, we also created two types of formats. The first format uses bubbled and highlighted text: we filled in the bubble next to one answer option and"}, {"title": "", "content": "highlighted its text in yellow. The second format adopts a web-format style: we designed a variation resembling a typical web format, featuring a light background, black text for the question and options, and a light blue highlight for one answer option.\nVisual Prompt Dimension. To ensure consistency across our experiments, we maintained uniform in- put dimensions for all prompts and their variations within each benchmark type. This standardization was crucial for a fair comparison across different variations and models. The specific dimensions were chosen to accommodate the visual elements of our biased variations while preserving the readability and structure of the original prompts. For vMMLU, the image prompts were consistently 560 x 640 pixels, and for vSocialIQa, the image prompts were 800 x 600 pixels.\nVMMLU and vSocialIQa. MMLU primarily assesses factual knowledge and mathematical computa- tion skills, while Social IQa evaluates a model's capacity to provide socially appropriate responses in various situations Hendrycks et al. (2021); Sap et al. (2019). Both the vMMLU and vSocialIQa variations measure whether there is a significant correlation between shifts in answer distribution and the visual bias presented by keypoint localization. We chose to experiment with visual sycophancy on both general knowledge and social intelligence tasks to disentangle inconsistencies in factual and social contexts. This approach allows us to examine whether the effects of visual sycophancy differ between tasks that require objective information and those that involve nuanced social reasoning."}, {"title": "3 Result", "content": "Our experiments on the vMMLU benchmark reveal patterns of visual sycophancy across the tested multimodal language models, with varying degrees of susceptibility among different architectures.\nThe response distributions show a consistent trend of shifting towards visually pre-marked options across all tested models. This shift manifests as an increase in the selection rate for the pre-marked option, accompanied by a corresponding decrease in the selection rates for non-marked options. The magnitude of these shifts varies notably among the models, suggesting differences in visual cues."}, {"title": "", "content": "LLAVA and Claude-haiku demonstrated the most resilience to visual sycophancy. We observed that LLAVA's performance on vMMLU is at around 50 percent in the neutral setting and does not undergo significant change for pre-marked options. After conducting paired statistical t-tests to compare the distribution shifts between the neutral setting and each of the biased conditions (Options A, B, C, and D), the results indicated that the shifts in answer distributions are not statistically significant. Claude-haiku also demonstrated resilience to visual sycophancy, with relatively modest shifts in response distribution. Its largest shift occurred when Option C was pre-marked, resulting in a 20 percentage point increase. Interestingly, Claude-haiku's overall performance remained relatively stable across different pre-marking conditions, with score changes ranging from 0 to -7 percentage points. While Claude-haiku is not immune to visual sycophancy, its impact is limited.\nIn contrast, Gemini-1.5-flash exhibited the highest susceptibility to visual sycophancy. It showed substantial shifts in response distribution for all pre-marked options, with increases ranging from 30.06 to 44.98 percentage points. These large shifts were accompanied by more pronounced decreases in non-marked option selection, averaging between -10.02 and -14.99 percentage points. Notably, Gemini-1.5-flash also experienced the most significant performance degradation, with score decreases of up to 23.18 percentage points when Option C was pre-marked. The bias was so pronounced that visual variations could be classified based on the responses alone, with Gemini selecting visually highlighted options A or B about 50 percent of the time and options C or D approximately 70 percent of the time. This pattern suggests a strong influence of visual cues on Gemini-1.5-flash's decision- making at the expense of accuracy. Interestingly, Gemini demonstrated a notable difference in its behavior between blue-centered and vanilla vSocialIQa formats, indicating that the visual presentation of options can significantly impact model responses, potentially mitigating or exacerbating biases.\nGPT-40-mini displayed an intermediate level of susceptibility to visual sycophancy. Its shifts towards pre-marked options, while noticeable, were generally less pronounced than those of Gemini- 1.5-flash but more substantial than Claude-haiku's. Interestingly, GPT-40-mini's performance impact varied depending on the pre-marked option, with both slight improvements and decreases observed. This variability hints at a complex interaction between visual cues and the model's underlying knowledge or decision-making processes.\nThe strength of the sycophancy effect also varied depending on which option was pre-marked. Across all models, pre-marking Options C and D generally elicited stronger effects compared to Options A and B. This pattern raises questions about potential positional biases or the influence of option ordering on the models' susceptibility to the visual cue. The observations from the vMMLU benchmark are further nuanced by the results from the vSocialIQa task,"}, {"title": "", "content": "vSocialIQa results not only corroborate the presence of visual sycophancy across different task types but also reveal how the effect can be modulated by subtle changes in visual presentation.\nVisual design significantly influences sycophancy effects. The comparison between Setup A and Setup B in the vSocialIQa task demonstrates that the visual presentation of options can dramatically alter the magnitude of visual sycophancy. This is most strikingly illustrated by Claude-haiku's performance. While it showed resilience in the vMMLU task and in vSocialIQa Setup A, it exhibited extreme susceptibility in Setup B, with shifts of up to 65 percentage points when Option B was pre-marked and a complete 100% selection of Option C when it was pre-marked. This stark contrast underscores the critical role of visual design in multimodal tasks and suggests that model behavior can be highly sensitive to seemingly minor changes in presentation.\nConsistency of susceptibility varies across models. Gemini-1.5-flash's high susceptibility to visual sycophancy, observed in the vMMLU task, is consistently evident across both vSocialIQa setups. This reinforces the notion that some models may have a more fundamental vulnerability to visual cues, regardless of the specific task or visual presentation. In contrast, GPT-40-mini's intermediate level of susceptibility in vMMLU is mirrored in its relatively stable behavior across both vSocialIQa setups, suggesting a more robust integration of visual and textual information."}, {"title": "4 Analysis", "content": "Changes in Token Probability. We begin our analysis by examining the shift in log probability. Log probabilities for the answer tokens (\u2018A,' \u2018B,' \u2018C,' and 'D') were collected shortly after inference. For each visual bias type, the change in token probability from neutral to bias were calculated and then averaged out across prompts. The probabilities were first converted back to linear probability to avoid unintended scaling of the values during calculation of deltas."}, {"title": "", "content": "As Figure 5 shows, the bias type strongly aligns with the answer choice, with the highest positive delta from neutral to bias. That is, the model is most likely to increase the probability for token X when the visual information suggests X. Although the distribution of the model's answers may"}, {"title": "", "content": "capture a model's tendency to prefer one answer over another, it has been observed that first-token probability does not always reflect the model's \u201cactual\u201d answer during generation (Wang et al., 2024). Thus, an in-depth analysis of the delta in token probability was necessary.\nFigure 6 displays the changes in token probability for LLAVA-1.5v-13b, which shows a different type of bias. Instead of showing a stronger preference for the biased option as GPT-40-mini did, LLAVA showed a stronger preference for option D across all biased variations, with the exception of variation A, in which options C and D were tied for positive delta.\nClaude-haiku exhibits varying degrees of susceptibility to visual bias across different bench- marks and formats. In the vMMLU benchmark, the model shows minimal bias, with only slight increases in responses for visually emphasized options. The effect becomes more pronounced in the webpage-formatted Social IQa, where emphasized options receive moderately increased selection. However, the most dramatic impact is observed in the vanilla vSocialIQa format, where the bias is extreme-reaching 100 percent selection for visually emphasized option C. This progression suggests that Claude-haiku's sensitivity to visual cues intensifies as the visual emphasis becomes more pronounced, with the vanilla vSocialIQa format eliciting the strongest bias response. The varying performance across these tests underscores the importance of considering visual formatting in evaluating language model behaviors."}, {"title": "5 Related Work", "content": "Our study on visual sycophancy in multimodal language models builds upon several key research areas. Recent advancements in multimodal AI systems have expanded the capabilities of models to process and integrate information from various modalities (Ge et al., 2024; Li et al., 2024a,b; Wu et al., 2024). This progress has been accompanied by growing concerns about biases in AI, including both social and modality-specific biases (Adewumi et al., 2024; Alabdulmohsin et al., 2024; Chen et al., 2024; Lu et al., 2024; Luo et al., 2024).\nVisual attention mechanisms in AI systems have been studied extensively, often drawing parallels with human visual processing (Cao et al., 2024). Evaluation methodologies for multimodal AI systems have evolved to address the complexities of assessing performance across different modalities (Ye et al., 2024). Simultaneously, ethical considerations in AI development have gained prominence, focusing on the potential impacts of AI biases in real-world applications (Amirloo et al., 2024)."}, {"title": "6 Limitations and Future Work", "content": "There are several limitations to this work:\nAlthough we investigate and analyze a few of the most well-known proprietary and open-source models, the generalizability of our findings could be enhanced by including a broader range of state-of-the-art models. Future work should aim to test these concepts across an even more diverse set of architectures and configurations. Since multimodal prompt engineering and modality bias are relatively new concepts, our primary focus has been on measuring the bias rather than proposing specific applications or effective jailbreak mitigation strategies. Further research is needed to develop practical interventions and assess their effectiveness in real-world scenarios.\nThe use of token probability delta as a novel metric for calculating bias in machine learning models is still in its early stages. It is not yet entirely clear whether systematic bias in multimodal machine learning models is inherently additive or subtractive, and this remains an area for further empirical and theoretical investigation. Modalities other than vision and text have not been explored in this study. Future research should consider extending the analysis to include other modalities, such as audio or sensor data, to determine whether visual sycophancy or similar biases are present across different types of multimodal inputs.\nOur experiments primarily focus on visual biases within the context of multiple-choice benchmarks. The extent to which these findings translate to more complex, free-text generation tasks or other forms of human-AI interaction remains unexplored and could be an avenue for future research. While our study addresses the phenomenon of visual sycophancy, we have not fully explored the potential interactions between visual biases and other types of biases (e.g., social or cognitive biases) present in multimodal models. Understanding these interactions could provide a more comprehensive picture of bias in AI systems."}, {"title": "7 Conclusion", "content": "Our study on visual sycophancy in multimodal language models reveals a complex landscape of model behaviors and biases. We found that the susceptibility to visual cues varies significantly across different model architectures, task types, and visual presentation formats.\nOur findings challenge simplistic assumptions about multimodal information integration in AI systems and raise important questions about the reliability and consistency of model outputs. The observed visual sycophancy effects underscore the need for careful consideration of visual elements in the design and deployment of multimodal AI systems, particularly in critical decision-making contexts."}]}