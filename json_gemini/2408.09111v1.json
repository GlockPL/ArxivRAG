{"title": "Measuring Visual Sycophancy in Multimodal Models", "authors": ["Jaehyuk Lim", "Bruce W. Lee"], "abstract": "This paper introduces and examines the phenomenon of \u201cvisual sycophancy\" in multimodal language models, a term we propose to describe these models' tendency to disproportionately favor visually presented information, even when it contradicts their prior knowledge or responses. Our study employs a systematic methodology to investigate this phenomenon: we present models with images of multiple-choice questions, which they initially answer correctly, then expose the same model to versions with visually pre-marked options. Our findings reveal a significant shift in the models' responses towards the pre-marked option despite their previous correct answers. Comprehensive evaluations demonstrate that visual sycophancy is a consistent and quantifiable behavior across various model architectures. Our findings highlight potential limitations in the reliability of these models when processing potentially misleading visual information, raising important questions about their application in critical decision-making contexts.", "sections": [{"title": "Introduction", "content": "Multimodal machine learning models, which integrate and reason across multiple modalities such as vision, language, and audio, have become increasingly significant in artificial intelligence research and applications (Manzoor et al., 2023; Zhang et al., 2023). These models develop richer representations by leveraging complementary information from diverse data types, enabling them to address complex tasks that span multiple domains (Achiam et al., 2023; Anthropic, 2024; Dubey et al., 2024; Reid et al., 2024).\nVision-language models (VLMs), a subset of multimodal models, typically employ one of three main architectures: external vision encoders (Radford et al., 2021), cross-attention mechanisms (Alayrac et al., 2022; Tang et al., 2023), or end-to-end transformers (Achiam et al., 2023; Anthropic, 2024). While the specifics of these architectures differ, they all necessitate the integration of visual and textual modalities within the model to perform few-shot or zero-shot classification of provided images or to engage in natural conversation in relation to visual prompts\nModality Bias Distinct from Social Bias. It is necessary to distinguish the terminology of \"bias\" before we proceed with our investigation. Modality bias is distinct from the bias that the model displays due to the existence of unequal representation in the training data.\nSocial bias, a well-studied phenomenon in AI research, refers to the unfair or prejudiced treatment of certain groups or individuals based on characteristics such as race, gender, age, or socioeconomic status Ferrara (2023). In uni- and multimodal models, social bias often stems from imbalances or stereotypes present in the training data, leading to outputs that reflect and potentially amplify societal inequalities Akter et al. (2021); Shah and Sureja (2024); Starke et al. (2022). This type of bias has been extensively documented and remains a critical ethical concern in AI development.\nHowever, our research focuses on a different, less explored form of bias: modality bias. Modality bias refers to the tendency of a multi-modal model to rely disproportionately on one modality (e.g., vision) over another (e.g., text) when making predictions or generating outputs Guo et al. (2023); Shtedritski et al. (2023). Unlike social bias, which primarily reflects societal inequalities from which the model"}, {"title": "Method", "content": "To empirically investigate the phenomenon of visual sycophancy in multimodal language models, we designed a series of experiments to quantify the influence of visual cues on model outputs.\nEvaluation Bias. Quantifying a model's bias on a multiple-choice benchmark is relatively straight-forward compared to quantifying bias on a free-generation task. Nevertheless, quantification remains challenging due to the multidimensional nature of bias. In the context of social biases, a model's output may be biased in content, style, or framing Bang et al. (2024), necessitating multidimensional metrics. However, in the context of modality bias that skews towards a localized key point, where the evaluation metric is percent correctness and a ground truth answer exists, a one-dimensional metric may effectively capture and quantify the bias, as the focus is on a single, well-defined aspect of performance.\nOur evaluation of bias measures the shift in the distribution of answers and log probabilities between biased variations. This involves comparing answer distributions across variations, including the distribution of ground truth answers. We also calculate a bias percentage, which represents the proportion of answers that changed in both directions. For GPT-40-mini and LLAVA, we conduct an analysis of the shift in the distribution of the top 4 answer token log probabilities across variations.\nPre-marked Visual Prompt Generation. For each neutral prompt in our benchmarks, we generated biased variations through visual localization techniques. This process involved creating visually distinct versions of the same question, each emphasizing a different answer option to simulate user intent.\nFor the Visual MMLU (vMMLU) benchmark, we produced two types of variation formats. The first format uses a filled-in bubble with colored text: we filled in the bubble next to one answer option and colored its text, creating a visual emphasis on that option. This highlight suggests user intent. The second format employs a size variation: we doubled the font size of the biased option without highlighting or bubbling the option. This size change does not explicitly suggest user intent.\nFor the Visual Social IQa (vSocialIQa) benchmark, we also created two types of formats. The first format uses bubbled and highlighted text: we filled in the bubble next to one answer option and"}, {"title": "Result", "content": "Our experiments on the vMMLU benchmark reveal patterns of visual sycophancy across the tested multimodal language models, with varying degrees of susceptibility among different architectures."}, {"title": "Analysis", "content": "Changes in Token Probability. We begin our analysis by examining the shift in log probability. Log probabilities for the answer tokens (\u2018A,' \u2018B,' \u2018C,' and 'D') were collected shortly after inference. For each visual bias type, the change in token probability from neutral to bias were calculated and then averaged out across prompts. The probabilities were first converted back to linear probability to avoid unintended scaling of the values during calculation of deltas."}, {"title": "Related Work", "content": "Our study on visual sycophancy in multimodal language models builds upon several key research areas. Recent advancements in multimodal AI systems have expanded the capabilities of models to process and integrate information from various modalities (Ge et al., 2024; Li et al., 2024a,b; Wu et al., 2024). This progress has been accompanied by growing concerns about biases in AI, including both social and modality-specific biases (Adewumi et al., 2024; Alabdulmohsin et al., 2024; Chen et al., 2024; Lu et al., 2024; Luo et al., 2024).\nVisual attention mechanisms in AI systems have been studied extensively, often drawing parallels with human visual processing (Cao et al., 2024). Evaluation methodologies for multimodal AI systems have evolved to address the complexities of assessing performance across different modalities (Ye et al., 2024). Simultaneously, ethical considerations in AI development have gained prominence, focusing on the potential impacts of AI biases in real-world applications (Amirloo et al., 2024)."}, {"title": "Limitations and Future Work", "content": "There are several limitations to this work:\nAlthough we investigate and analyze a few of the most well-known proprietary and open-source models, the generalizability of our findings could be enhanced by including a broader range of state-of-the-art models. Future work should aim to test these concepts across an even more diverse set of architectures and configurations. Since multimodal prompt engineering and modality bias are relatively new concepts, our primary focus has been on measuring the bias rather than proposing specific applications or effective jailbreak mitigation strategies. Further research is needed to develop practical interventions and assess their effectiveness in real-world scenarios.\nThe use of token probability delta as a novel metric for calculating bias in machine learning models is still in its early stages. It is not yet entirely clear whether systematic bias in multimodal machine learning models is inherently additive or subtractive, and this remains an area for further empirical and theoretical investigation. Modalities other than vision and text have not been explored in this study. Future research should consider extending the analysis to include other modalities, such as audio or sensor data, to determine whether visual sycophancy or similar biases are present across different types of multimodal inputs.\nOur experiments primarily focus on visual biases within the context of multiple-choice benchmarks. The extent to which these findings translate to more complex, free-text generation tasks or other forms of human-AI interaction remains unexplored and could be an avenue for future research. While our study addresses the phenomenon of visual sycophancy, we have not fully explored the potential interactions between visual biases and other types of biases (e.g., social or cognitive biases) present in multimodal models. Understanding these interactions could provide a more comprehensive picture of bias in AI systems."}, {"title": "Conclusion", "content": "Our study on visual sycophancy in multimodal language models reveals a complex landscape of model behaviors and biases. We found that the susceptibility to visual cues varies significantly across different model architectures, task types, and visual presentation formats.\nOur findings challenge simplistic assumptions about multimodal information integration in AI systems and raise important questions about the reliability and consistency of model outputs. The observed visual sycophancy effects underscore the need for careful consideration of visual elements in the design and deployment of multimodal AI systems, particularly in critical decision-making contexts."}]}