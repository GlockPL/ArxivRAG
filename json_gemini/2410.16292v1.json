{"title": "AN EVALUATION OF LLM CODE GENERATION CAPABILITIES THROUGH GRADED EXERCISES", "authors": ["\u00c1lvaro Barbero Jim\u00e9nez"], "abstract": "Large Language Models have shown prominent capabilities in generating functional code from natural language descriptions. However, a standardized way to evaluate these capabilities in an objective and unbiased manner is still to be found. In this paper we review the current evaluation methods available to this end, and run a new evaluation of the performance of one state-of-the-art model (GPT4-o-mini) in solving curated coding challenges in 8 programming languages, obtained from Codewars, a software development community. Our analysis shows that the chance of success of the model has a positive correlation with the task difficulty, the popularity of the programming language being used and the time elapsed since the publication of the challenge. A further approximate explanatory analysis in terms of high-level features hints that while 46.6% of the model performance could be attributed to task difficulty, a 37.4% seems to be related to leakage of the challenge solutions into the model training set, while the remaining 16% depends on the programming language. These results suggest that current evaluation methodologies might be overestimating the actual skill of Large Language Models for generating functional code.", "sections": [{"title": "1 Introduction and Background", "content": ""}, {"title": "1.1 Large Language Models", "content": "Large Language Models (LLMs) are deep neuronal networks built from Transformer blocks [90], which model the distribution of written text splitted into words pieces, also known as tokens. They are usually trained in an unsupervised way over massive amounts of text, generally obtained from internet crawls. Such training can be implemented by predicting tokens masked at random in a segment of text and configuring the Transformed layers to use bidirectional attention (encoder models) [22, 53, 37, 77], or by issuing the model to predict the token following a given piece of text and configuring the attention masks in a left-to-right or causal way (decoder models) [70, 13, 11, 96, 63, 88, 1, 87, 42, 43, 5, 8, 24, 61, 98]. Some hybrid encoder-decoder architectures have also been proposed [72]. While early LLMs mainly focused on the English language, recent models tend to be trained in a varied mixture of languages; however language-specific models also abound [58, 81, 36, 80, 14].\nThe most valuable property of LLMs is that they allow for an effective transfer learning to a wide variety of language-related tasks, such as document classification, named entity recognition, summarization, question answering, information retrieval or natural language inference, to name a few. Such transfer can be performed by fine-tuning the model to a small supervised dataset related to the downstream task, generally producing better results than training a model from scratch over the same data.\nEven more, it has been shown [13] that pre-training a decoder LLM in a large enough dataset grants it some capabilities to perform well in downstream tasks without requiring a fine-tuning step, by means of prompting the model with a description in natural language of the task to perform (zero-shot inference), sometimes adding some input-output examples of the task to boost performance (few-shot inference). Since writing prompts requires no specialized knowledge in machine learning or the underlying details of the model, this method of interaction with LLMs has rapidly expanded to general users all over the world [60, 3].\nHowever, for LLMs to be able to cater for the wide variety of natural language requests from all kinds of users, some specialized post-training steps have been shown to be necessary. Instruction tuning [95], also termed Supervised Fine Tuning (SFT), improves the ability of the LLM to produce adequate responses to users, by fine-tuning it over"}, {"title": "1.2 Evaluating LLMs", "content": "Objectively evaluating the performance of LLMs poses a significant challenge, partly due to the wide range of tasks they can be applied to, but also because of their training procedure: internet crawls can contain copies of the very same datasets used for testing, thus overestimating their performance due to data leakage [27].\nA popular evaluation method is using a number of evaluation datasets in the form of multiple choice tests, in domains such as general knowledge, math or science. The LLMs are scored by checking if the correct choice for a question is the one for which the LLM assigns the highest generation probability [27, 28, 101, 85, 39, 73, 82, 93, 7]; these benchmarks, however, need to be updated frequently to avoid leakage to new LLMs through updates in the crawls used in their training data.\nAnother approach involves using crowd-sourced human evaluations of model outputs in the form of preferences [18], ranking the models by the percentage of times they were preferred, or by an Elo rating. Although generally considered to be the most accurate method for measuring the capabilities of an LLM, they are expensive to carry out and requires of domain experts to implement evaluations in highly specialized fields.\nA third approach employs automated evaluation metrics that measure the semantic similarity of the LLM response versus a gold standard, or assign some other kind of score to the response. These metrics need to make use of yet another LLM, either specialized in embedding texts into a latent space in which semantic similarity can be numerically measured [75], or in generating a performance report of the adequateness of the response [48]. While convenient and cheap, these kind of metrics have been shown to be brittle and suffer from biases [92, 15]."}, {"title": "1.3 LLMs for code generation", "content": "While LLMs are mainly focused on natural language generation, a significant effort has also been made to generate programming code with them. Being expressed as written text, programming languages seem like a natural extension of human languages. However, although the syntax underlying programming languages is way simpler and more restricted than that of natural languages, they do not allow for deviations, and thus are less fault-tolerant than human languages. Furthermore, a program is generated with the purpose of performing a particular function, so even syntactically correct programs can be of no use if they do not fulfill their target function.\nSimilarly to how natural languages are dealt with, a number of high-performing LLMs have been trained with texts both in natural languages and programming languages [63, 88, 96, 8, 98], which grants them noticeable abilities in code generation. Other LLMs are highly specialized in code generation, either by fine-tuning a base model with a code corpus (Codex [16]), or by training from scratch using codebases and related technical sources (StarCoder [50], StarCoder2 [54], StableCode [68], DeepSeekCoder [35])."}, {"title": "1.4 Evaluating LLMs in code generation tasks", "content": "Several useful tasks can be addressed with these kind of LLMs: generating the body of a function given a description in natural language, writing documentation or explanations for a given piece of code, producing unit tests for a function, reviewing pull requests or proposing patches to solve issues, among others. In this paper we will focus on the task of generating a function given a natural language description and a set of sample unit tests.\nSoftware development is a broad field that encompasses multiple aspects. While requiring some shared basic knowledge and skills, programming a web application, an AI algorithm, or a video game are very different tasks. This is not only due to the differences in programming languages but also because of the variety of tools and standards employed in each domain. Moreover, developing a complete software solution also requires the ability to design and understand systems composed of various modules, to collaborate effectively with other project members, and to interpret user requirements and translate them into code.\nA number of previous works have analyzed the problem of evaluating LLMs on coding tasks, or more generally, evaluating the performance of synthetic code generators, which has been an active line of research before the advent of LLMs [34]. This section aims to provide a review of relevant prior research on this topic.\ncode-docstring-corpus [6] is a dataset of 161,630 pairs of function docstrings and corresponding code in Python, extracted by scrapping projects from GitHub [29]. Baseline models for generating code from the docstring and viceversa are built using Neural Machine Translation models, whose performance is then evaluated using the metric BLEU [65]. BLEU is a metric widely used for automated machine translation, which boils down to measuring the number of matching n-grams between the generated text and a reference solution, penalizing also for brevity.\nThe CodeXGLUE dataset [56] attempts to mimic the famous GLUE dataset [91] for Natural Language Processing, aiming to create a reference benchmark for a set of diverse tasks related with automated code generation. To this end, a number of previously existing datasets are aggregated, for tasks such as clone detection, defect detection, code repair, code translation, code search, and text-to-code generation among others. The task of text-to-code generation is based on the CONCODE dataset [41], an extraction from around 33,000 Java projects on GitHub that results in around 104,000 examples consisting of natural language descriptions, code environments and code snippets, with the objective of generating class member functions from Javadoc-style comments. The evaluation metrics proposed for the task are exact match, BLEU and CodeBLEU [74] scores. CodeBLEU is an adaption of the BLEU metric for programming languages. It takes into account the level of match of the generated and reference solutions not only in terms of n-grams, but also using the Abstract Syntax Trees (AST) underlying the code, and the data-flows representing the semantics on how each variable is computed from other variables.\nIn Search-based Pseudocode to Code (SPoC) [49] the authors address the task of translating pseudocode to actual programming code, scraping 18,356 solutions of 677 programming problems from Codeforces [20], then hiring a team 59 crowdworkers to translate them to pseudocode. A sequence to sequence LSTM model is trained over a subset of the data, and solutions are generated for a separate test set. A solution is deemed as valid if it successfully compiles and passes a set unit tests. To measure the performance they propose the success rate at B metric, defined as the fraction of test problems where the system generates an accepted program under the budget of B trials. The authors argue this metric is superior to BLEU, as the latter fails to account for functional correctness and only focuses on the surface form of the code.\nAutomated Programming Progress Standard (APPS) [38] addresses the more ambitious task of generating code from descriptions in natural language. It consists of 10,000 coding problems together with unit tests and ground-truth solutions written by humans. The data is obtained by scrapping several websites centered around coding challenges (AtCoder, CodeChef, Codeforces, Codewars, HackerRank, Kattis, and LeetCode), followed by a manual curation process. Coding challenges are also arranged according to their difficulty into three groups: introductory level, coding interview level, and competition level. GPT-2 [70] and GPT-Neo [11] models are fine-tuned and tested over this dataset, as well as a few-shot prompting approach with GPT-3 [13]. Two evaluation metrics are proposed: an average number of unit tests completed, and a strict metric that requires a solution to pass all unit tests. The evaluation results show that interview-level exercises are indeed harder to solve by the LLMs, and not a single competition-level problem can be solved successfully when using the strict metric.\nHumanEval [16] is another evaluation dataset presented along with Codex, a GPT-3 [13] model fine-tuned over GitHub data. The dataset consists of hand-written Python function signatures and docstrings from which the LLM is expected to produce a working implementation. HumanEval contains 164 programming problems, covering tasks such as language comprehension, reasoning, algorithms and simple mathematics, of a difficulty at the level of easy code interviews. The authors emphasize the relevance of the problems being generated from scratch for their work, as they find that public repositories already contain solutions for problems posted at Codeforces, which are part of other evaluation datasets"}, {"title": "2 Measuring software development capabilities through programming exercises", "content": "In this paper we will limit ourselves here to measuring one specific aspect of software development: solving programming exercises, also popularly known as code challenges or \"katas\". In these exercises, the programmer is presented with a description of a specific functionality to implement, such as creating a function that determines whether a given number is even or odd. The programmer must then develop the code to implement this functionality, following instructions regarding the programming language to use and the format to follow.\nThe advantage of this type of exercise is that it is possible to automatically verify whether the code produced is correct. The kata designer creates a series of unit tests that challenge the programmer's solution, verifying whether the outputs produced by the code meet expectations across various scenarios. The performance of an LLM can then be measured with already established metrics such as pass@k.\nAn instance of a software development community that makes use of this entire system of katas is Codewars [21]. Figure 1 presents the flow followed by a user in order to successfully complete a kata. A relevant fact is that the proposed solution is tested against two sets of units tests, one of them kept hidden from the user. In this way, the user cannot create a trivial solution that just returns the expected outputs for the unit tests. When the user completes all the tests, the proposed solution is added to a repository of all valid solutions created for the kata so far; this repository is kept hidden from all users except those who have managed to submit a valid solution, or those who have decided to forfeit the exercise, losing their chance to complete it successfully."}, {"title": "3 Experimental setup", "content": "For this study, we have chosen to use GPT-40-mini (version 2024-07-18) [61] as the LLM to test, given that, at the time of writing, this language model ranks #3 for general tasks and #2 for programming tasks in the LMSYS Chatbot Arena Leaderboard [18]. Although the full model (GPT-40) demonstrates better performance in this ranking, the fact that the mini version is approximately 30 times cheaper makes it a more practical solution.\nOf course, a language model cannot perform this task on its own accord. Therefore, for this study a series of bots that interact between Codewars and the OpenAI API were developed, whose structure is show in Figure 3. The entire system implementation was done in Python, using Selenium [79] to create the bots that interact with Codewars."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Overall performance", "content": "Figure 7 shows the performance levels achieved by the LLM in comparison to human developers. Several points stand out:\n\u2022 The LLM outperforms humans in solving easy tasks (ranks 8 and 7).\n\u2022 The performance of humans and the LLM is similar when solving intermediate tasks (ranks 6 and 5) and even some early advanced tasks (rank 4).\n\u2022 However, when reaching rank 3 tasks, the LLM's performance significantly declines. Furthermore, for the most complex exercises (ranks 2 and 1), the LLM fails completely, unable to solve any of them. This is in accordance with previous benchmark evaluations [38, 4, 62], where the evaluated LLMs failed to solve any of the problems with the highest difficulty level."}, {"title": "4.2 Performance by programming language", "content": "Figure 8 deepens the analysis by examining the LLM's performance in solving katas of various difficulty levels across different programming languages. It is noteworthy that the LLM's performance significantly depends on the programming language used:\n\u2022 The LLM performs better in widely-used languages: JavaScript and Python are the two most commonly used general-purpose languages on GitHub. Additionally, in these languages, the proposed solutions for more difficult exercises do not seem to be entirely incorrect; rather, they often fail due to insufficient optimization (Timeout).\n\u2022 Even in less commonly used languages, such as C or R, performance remains fairly strong, although the LLM does not manage to solve any rank 3 katas in these languages (6 available for C, 1 for R).\n\u2022 In languages like Julia, which are even less widely adopted, the LLM seems to fare reasonably well. Although performance is lower than in C or R for intermediate or advanced exercises, it is reliable for basic tasks. A similar trend is observed with Solidity.\n\u2022 In legacy languages (Fortran and COBOL), performance drops dramatically. COBOL, in particular, shows catastrophic results, and visual inspection of the generated code reveals many cases of syntax errors, something extremely rare for other languages.\nTo compute an overall performance by language, we make use of the Codewars score defined before in Equation 1. This leads to the scores represented in Figure 9. Python emerges as the language in which the LLM performs best."}, {"title": "4.3 Factors motivating performance differences across languages", "content": "Previous research has already observed that other LLMs trained on public GitHub repositories perform especially well when coding in popular programming languages such as Python or Java [100]. The training data used for GPT models is not publicly known, but it is reasonable to assume that GitHub repositories have been used as well. To the best of our knowledge, there are no public statistics on the total lines of code per language stored in this community, but there are data on the number of pushes to repositories, categorized by language [30]. From these, we can generate the analysis shown in Figure 10.\nA clear correlation appears: not surprisingly, programming languages with more code available for training are the ones in which the LLM can perform better. This is good news for programmers working in the most widely-used languages, but disappointing for those considering using this technology to maintain or adapt the vast COBOL and Fortran codebases that still exist in legacy production systems."}, {"title": "4.4 Influence of kata age", "content": "The variability in the LLM's capabilities is not limited to task difficulty or the language used. Surprisingly, the publication date of the kata also impacts performance, both for the LLM and for human users, as shown in Figure 11: older katas are easier to solve than more recent ones. This trend affects the score of human users, both globally and when excluding the most complex katas (focusing only on kyus > 3), or when concentrating on basic and intermediate katas (kyus \u2265 5). However, this trend impacts the LLM even more significantly: initially, the LLM exhibits a higher score than humans if high-complexity exercises are ignored, but its performance eventually degrades to levels worse than those of human users in basic and intermediate exercises.\nWe consider two hypotheses that might explain this phenomenon:\n1. The criteria for selecting a kata's kyu level have changed, and a kata with a certain kyu k in 2024 is more difficult to solve than a kata from 2013 with the same kyu k.\n2. Over time, solutions to katas become more widely available, allowing humans to copy those solutions, and LLMs to incorporate them into their training data."}, {"title": "4.5 High-level explanatory analysis", "content": "Striving for a more quantitative explanation of the factors behind the LLM success at coding tasks, we present in this section a more detailed explanatory analysis.\nObtaining explanations from a neural network with billions of parameters such as the ones employed in LLMs is a complex task. The calculation and analysis of SHAP coefficients [57] is a popular approach to obtain explanations of the predictions of any kind of machine learning model in terms of its input features. However, the calculation of the SHAP coefficients requires computing how the absence of different groups of features affect the model output, something that can be achieved approximately by invoking the model a large number of times over different perturbations of the input features. Unfortunately this is very computationally demanding when dealing with an LLM, an also economically prohibitive in the case of a closed-source model like the one used in this paper. Furthermore, the SHAP coefficients provide explanations in terms of the input features, which in the case of the LLM are input tokens. As faithful as it might possibly be, an explanation of the LLM programming capabilities in terms of the presented tokens would be of little use.\nWhat we propose here instead is to create a simple surrogate model that tries to predict the success of the LLM at solving a kata, making use of high-level features abstracting the nature of the kata and the programming language to be used, instead of the actual kata description. More precisely, the surrogate model is fed with the following features:\n\u2022 Kata rank (kyu).\n\u2022 Days since publication: number of days elapsed since publication date, up to the date of the data collection (log-scaled)."}, {"title": "6 Conclusions and discussion", "content": "In this paper we have reviewed the current methods for evaluating the capabilities of Large Language Models for code generation tasks, and carried out a new evaluation of GPT-40-mini focused on programming challenges created by the Codewars community. Our analysis reveals several points of relevance:\n\u2022 The LLM is unable to propose valid solutions to highly complex exercises, and presents diminished performance when compared to humans when attempting hard problems.\n\u2022 Capabilities vary significantly across programming languages, with major drops in performance for unpopular languages. Legacy languages are specially affected.\n\u2022 Leakage of solutions seems to account for up to 37.4% of the performance. Furthermore, a significant factor of leakage is the publication date of the challenge, with the LLM performing worse on more recent challenges.\nUp to the best of our knowledge, this is the first result that quantifies the impact of solutions leakage on the performance of an LLM for coding, even if through approximate methods.\nThese results raise concerns about the current evaluation practices using public datasets, either in the form of crawls from public repositories or hand-made by crowdworkers. Even if the research community continues to create larger or more curated evaluation datasets, these will be eventually integrated into the massive corpus used for LLM training; as a consequence newer models will deceptively perform better in the evaluations due to data contamination. In fact, current evaluations in the literature of the performance of state-of-the-art LLMs are, quite probably, overestimates of"}, {"title": "5 Limitations", "content": "In the light of the reviewed previous work on this topic and the general problem of LLMs for code generation, we must acknowledge the following limitations in this work.\nCoverage of actual software development tasks: actual work as a software developer involves not just solving programming challenges, but also writing documentation, finding and reporting bugs in the system, designing high-level changes that span multiple code modules, gathering requirements and formalizing feature requests from users, organizing tasks with other team members, and defining the roadmap of the application. Thus, the presented evaluation only covers a small spectrum of this wide range of tasks. In addressing this shortcoming more comprehensive benchmarks such as SWE-bench [45] have already been proposed.\nReproducibility: although the presented evaluation framework is fully automated, current terms of use of Codewars admonish the use of AI methods to gain ranks in their community. Therefore, we have decided to release neither the code of the developed botnet, nor the database of solutions proposed by the LLM, as publishing these artifacts would easen the misdeeds of those willing to incur in such fraudulent activities.\nHuman in the loop: previous work has noted that an LLM for code generation can improve its results if it takes part in a multi-turn conversation with a human that points out mistakes in the generated code [4]. Commercial systems that make use of LLMs as helpers for coding tasks also make use of this loop [61, 31, 32]."}, {"title": "", "content": "In this paper we shall adopt a similar scoring method to measure the performance of the LLM. However, and as opposed to humans, the bot network developed can propose a solution to a kata in a matter of seconds, regardless of its difficulty. While this superhuman coding speed is a merit on its own accord, the focus of this paper is to measure the capabilities of the LLM in terms of being able to find solutions to katas of varying complexities. Therefore, we shall use a modified version of the Codewars score that measures an average performance of the LLM across all katas, using the expression\n$$P_{l} = \\sum_{i=1}^{8} S_{i} r_{li},$$\nwhere \\(P_{l}\\) is the computed performance of the LLM for programming language l, \\(S_{i}\\) is the Codewars score granted for a kata of rank i (following Figure 6), and \\(r_{li}\\) is the ratio of katas of rank i that the LLM is able to solve in language l. In summary, the metric we shall use is a weighted average of the frequency of the LLM success in solving exercise, putting more weight in harder tasks. A higher value on this metrics means the LLM is able to solve tasks of higher complexity more frequently."}, {"title": "", "content": "Given a linear model in the form \\(f(x) = \\sum_{i=1}^{d} x_{i}\\beta_{i} + b\\), the standard (or interventional) SHAP values for a sample x and feature j are simply given by \\(\\beta_{j}(x_{j} - \\mu_{j})\\), with \\(\\mu_{j}\\) the mean of feature j across the data. These easy to compute estimates can only be obtained by assuming independent features, something that is not met in the case of multicollinearity. To address this, a non-interventional approach that does not assume feature independence can be used, where the SHAP coefficients are computed approximately via a sampling procedure, and a better credit assignment is given to correlated features [59]. Through a sampling of 100K points we arrive at the SHAP coefficients represented in Figure 12."}]}