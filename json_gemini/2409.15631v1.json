{"title": "Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI", "authors": ["Liang Zhang", "Jionghao Lin", "John Sabatini", "Conrad Borchers", "Daniel Weitekamp", "Meng Cao", "John Hollander", "Xiangen Hu", "Arthur C. Graesser"], "abstract": "Learning performance data describe correct and incorrect answers or problem-solving attempts in adaptive learning, such as in intelligent tutoring systems (ITSs). These data are commonly used to assess knowledge mastery and improve the instructional effectiveness of ITSs through them. Learning performance data tend to be highly sparse (80%~90% missing observations) in most real-world applications due to adaptive item selection. This data sparsity presents challenges to using learner models to effectively predict future performance explore new hypotheses about learning. This article proposes a systematic framework for augmenting learner data to address data sparsity in learning performance data. First, learning performance is represented as a three-dimensional tensor of learners' questions, answers, and attempts, capturing longitudinal knowledge states during learning. Second, a tensor factorization method is used to impute missing values in sparse tensors of collected learner data, thereby grounding the imputation on knowledge tracing tasks that predict missing performance values based on real observations. Third, a module for generating patterns of learning is used. This study contrasts two forms of generative Artificial Intelligence (AI), including Generative Adversarial Networks (GANs) and Generate Pre-Trained Transformers (GPT) to generate data associated with different clusters of learner data. We tested this approach on an adult literacy dataset from AutoTutor lessons developed for Adult Reading Comprehension (ARC). We found that: (1) tensor factorization improved the performance in tracing and predicting knowledge mastery compared with other knowledge tracing techniques without data augmentation, showing higher relative fidelity for this imputation method, and (2) the GAN-based simulation showed greater overall stability and less statistical bias based on a divergence evaluation with varying simulation sample sizes compared to GPT. This framework supports controlled, safe, and cost-effective online and offline evaluation of ITS instructional designs prior to real-world testing, while facilitating more comprehensive educational data mining and learning analytics. It paves the way for improving real-world applications in AI-based education supported by learner data driving instructional adaptivity.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of AI-based educational technologies into e-learning platforms, combined with advanced pedagogical strategies, represents a landmark advance in modern education [1]. This integration has significantly changed approaches to learning and teaching, making educational systems more adaptive for personalized learning, flexible for remote access, scalable for resource distribution, and educational outcomes [2]. An exemplary prototype of this advancement is Intelligent Tutoring Systems (ITSs), which provide personalized and adaptive instructions through hints, prompts, and other adaptive feedback to improve learner performance [3], [4]. The effectiveness of ITSs has been demonstrated with considerable success in diverse areas such as science, technology, engineering, and mathematics (STEM), as well as in fields like reading and language learning [4]\u2013[6]. Learning performance data, which include records of quiz scores and problem-solving attempts from learners using the ITS, are crucial for evaluating knowledge mastery, self-regulation, and other characteristics that support adaptive instructions [7], [8]. For example, learners' correct and incorrect responses can be used to train machine learning models that track historical learning and predict future performance based on knowledge components and concepts, a process known as Knowledge Tracing (KT), within the learner model component of ITS [9], [10]. The ITS can tailor adaptive instructions, such as dialogue-based feedback, based on predictions and assessments, offering targeted support with specific hints and prompts, especially when learners struggle or experience wheel-spinning [4].\nIn real-world educational practices, learning performance data often suffers from data sparsity issues. As illustrated in Fig. 1, this phenomenon is evident in the learner-system interaction process. Fig. 1 shows how learners interact with an ITS, where their answers and attempts at questions are recorded. The data matrix reveals many missing entries, indicated by the symbol \"?\", highlighting the sparsity of the data. The real sparse scenarios demonstrate inconsistent and incomplete data patterns, as shown by the different colors within a single bar, representing various biased and unevenly distributed patterns. In contrast, the expected diverse scenarios that illustrate a more comprehensive and structured dataset, where each pattern is expected to maintain relative quantities for further analysis and modeling, showcasing a more balanced and thorough representation of the data. Data sparsity is typically divided into two categories: (1) unobserved data because of unpresented items (i.e., questions, problems) and limited learner responses or attempts [11], [12], and (2) insufficient learning performance patterns in the available empirical data [13], [14]. Various reasons contribute to these data sparsity issues, including participant dropout from learning tasks [15], learner disengagement due to off-task behavior [16] or gaming system [17], random data loss from design and operation errors [18], and potential bias in the experimental groups [19]. These challenges are typical in real-world settings.\nData sparsity issues adversely affect the accurate modeling of learning processes and assessment of learners' knowledge states, which in turn may compromise effective instructional support to learners, particularly those at risk [11], [13]. Data sparsity can lead to biased or over-fitted KT models, resulting in potentially misleading predictions of learners' future performance. Moreover, the insufficiency of learning performance data across a limited range of diverse patterns restricts the potential to comprehensively test and fine-tune ITSs, particularly in cases where certain pre-designed instructional conditions have not been thoroughly explored [20]. Therefore, addressing data sparsity is crucial for avoiding biased modeling, significantly enhancing ITSs, enabling more comprehensive evaluations of the learning process, and improving effective instructional support.\nAddressing data sparsity within ITSs represents a practical yet challenging research area. Informed by advancements in machine learning field [21], [22], tackling this issue typically involves two interconnected and sequential strategies: data imputation and data augmentation.\nData imputation focuses on filling the gaps in missing data, thereby creating a more complete dataset, which is crucial for accurate analysis and decision-making [23], [24]. On the other hand, data augmentation not only enriches and expands datasets where learning performance patterns are rarely presented but also enhances the robustness of analysis, modeling, and testing in ITSs, helping to mitigate the effects of sparsity on learner performance prediction [13], [25]. Despite the critical need, systematic efforts to address these issues in the ITS field remain limited, with few studies focusing on the comprehensive management of sparse data [11], [23].\nEffective handling of missing data remains a challenge in educational data analysis. Even though general data imputation methods (e.g., indicator or mean imputation [26], regression imputation [27], and multiple imputation [28]) have proven effective in the literature, they offer a cost-effective solution that avoids labor-intensive experiments by leveraging observed data to estimate unobserved data. These methods capitalize on underlying patterns and characteristics [29], but they address the missing data in a straightforward manner that fails to capture the full complexity of the data structure. For example, indicator or mean imputation replaces missing values with a specified indicator or the mean of observed values. However, this method may introduce bias by oversimplifying the complexities of missing data [26], [30]. Regression imputation predicts missing values based on other observed data using regression models. Despite its utility, it often fails to capture the full spectrum of the underlying data structure [27]. Multiple imputation generates multiple datasets by imputing missing values with a range of plausible values and then averaging the results. Yet, it may not adequately address complex, high-dimensional correlations [31]. Furthermore, data sparsity could influence model prediction of learner performance. When using Some KT models in ITSs to predict learning outcomes, such as Performance Factor Analysis (PFA) and Bayesian Knowledge Tracing (BKT), use logistic regression and Bayesian networks, respectively, to predict learning outcomes [32], [33]. However, these models themselves are vulnerable to data sparsity and cannot account for the sequence effect of learning events, though partially addressing data sparsity issues [11], [13].\nTherefore, to effectively address data sparsity and fully capture the data's complexity, and improve the prediction of learner performance, innovative computational approaches that provide deep insights are needed. A fundamental principle in the field of data imputation, articulated by Rubin [29], [34], is the use of observed data patterns to impute unobserved data, thereby enhancing the completeness and utility of the dataset. In ITS research, tensor factorization, originating from recommendation techniques, has been used to recommend missing performance data based on existing learner records [23], [35]. This method leverages multidimensional relationships to enhance predictions and knowledge representation by integrating three dimensions: learners, items (e.g., questions or materials), and temporal factors (e.g., time or attempts) [23], [35], [36]. Another source of inspiration stems from the advancements of generative AI models, which are capable of generating new data based on patterns during training and have revolutionized data augmentation methodologies by being more flexible and powerful [37], [38]. One prototype is Generative Adversarial Networks (GANs), which can proficiently learn from existing data distributions and generate varied samples that extend beyond the original dataset [39]. Another is the Generative Pre-trained Transformer (GPT), which possesses reasoning abilities for learning data distributions and can use both computational and heuristic models to help sample larger datasets that align with the original data distribution [40]. Successful applications of generative Al models for data augmentation in ITSs include enriching mathematics student learning datasets with multiple-choice questions [41], generating extensive student behavioral data via GANs, from MOOC learning platforms [42], and augmenting sparse learning performance data in reading comprehension [14]. Additionally, leveraging Generative Pre-trained Transformer (GPT-4) models have shown significant improvements in selecting appropriate machine learning models and fine-tuning them for predicting learning performance [43]. Building on the success of generative AI models in ITSs, these models could become highly effective tools for data augmentation, tailored to specific learning needs.\nThe present study develops a systematic augmentation framework that integrates multidimensional modeling using tensor factorization for data imputation and generative AI models for data augmentation. This framework generates extensive learning performance data tailored to specific and controlled performance patterns, enhancing both data imputation and data augmentation capabilities compared to model-based patterns derived from real experimental datasets. The framework will be tested with learning performance data from an example ITS for adult literacy, using AutoTutor lessons for ARC [44]. Our investigation is guided by the overarching Research Question: \"How can we develop a data augmentation framework to enhance sparse learning performance data and improve the scalability of data collected by the ARC ITS?\" This is explored through the following sub-questions:\n\u2022 RQ1: How does tensor factorization perform in imputing student performance data in adult reading comprehension, particularly in comparison to baseline methods such as PFA, BKT, and Sparse Factor Analysis Lite (SPARFA-Lite)?\n\u2022 RQ2: How can generative AI models, including GANs and GPT, be effectively and reliably utilized for data augmentation to tailor learning experiences based on individual performance patterns?\nThis proposed systematic augmentation framework has the potential to emerge as a powerful tool for enhancing data richness and reliability. Firstly, it can perform data imputation for sparse learning performance data, thereby enriching the data representation for more comprehensive learner modeling. Secondly, the generative models can increase the diversity of individualized learner performance data, which is essential for enriching learner modeling and ITS training."}, {"title": "II. RELATED WORK", "content": "Adult reading comprehension is a fundamental skill for academic success and lifelong learning. This skill can be improved with AI-based learning technologies with adaptive and personalized solutions [44]\u2013[46]. These technologies leverage educational data mining and learning analytics to assess student performance (e.g., learner modeling), and to provide personalized instruction tailored to individual learners (e.g., tutoring feedback, hints, nudges, and tailored recommendations for reading materials and comprehension exercises) [46]\u2013[51]. For instance, digital textbooks enhanced by AI have evolved significantly, starting from adaptive hypermedia based on knowledge engineering and progressing to semantic web technologies for the automated integration of external content [52]. The latest advances in natural language processing and machine learning have enabled these textbooks to use their content and structure as a knowledge base for \u201csmart\u201d functionalities like automated knowledge compilation, adaptive navigation and presentation, and targeted content recommendations [52]. These innovations significantly enhance the reading comprehension skills of students engaging with digital textbooks.\nMost of the AI-based technologies mentioned above have been used in ITSs for adult literacy. The Interactive Strategy Training for Active Reading and Thinking (iSTART) for Adult Literacy Learners (ALL) exemplifies this. ALL integrates interactive narratives and summarization into a comprehensive two-phase reading comprehension training program that features instructional videos on self-explanation and five comprehension strategies (i.e., comprehension monitoring, paraphrasing, prediction, elaboration, and bridging), followed by a practice phase [53]. Another instance is the AutoTutor developed for ARC, which is investigated in the present study [44]. The AutoTutor ARC employs the trialogue design, which includes one human learner and two computer agents (virtual tutor and virtual companion) [44]. This system facilitates interactive learning through a three-party conversation that assesses learners' responses, provides feedback, and corrects misconceptions based on an Expectation-Misconception Tailored (EMT) mechanism that attempts to cover good answers (called expectations) and correct bad answers (called misconceptions) [44], [48], [54]. The tutoring session concludes once all lesson expectations are met. Other ITSs, such as DSCOVAR for vocabulary learning [55], and ITSS for reading comprehension instruction through structure strategy training [56], are also noteworthy but are not detailed here.\nRecently, ITSs for adult reading comprehension have increasingly incorporated advanced AI technologies, particularly generative AI. Large Language Models (LLMs) are now used to develop summary grading models for intelligent textbooks that provide real-time formative feedback and assess comprehension [57]. Generative AI models, including GAN and GPT, have been used to tackle sparse data challenges in reading comprehension, thereby enhancing personalized educational technology in ITSs [14], [43]. Furthermore, the use of LLMs to generate high-quality, personalized reading materials underscores the potential for future ITS designs to improve reading comprehension skills [58]. These AI advancements have revolutionized ITS development from their design and creation to learning analytics and modeling in adult reading comprehension."}, {"title": "B. Tensor-based Imputation for Sparse Learning Performance Data", "content": "The increasing prevalence of sparse learning performance data from ITSs necessitates robust imputation methods to handle missing information effectively. Tensor-based imputation has emerged as an important technique due to its ability to maintain the multi-dimensional nature of learning data and to preserve intrinsic relationships across dimensions such as learners, questions, time or attempts, and performance data [23], [35], [36], [59].\nPioneering implementations of tensor-based methods include Thai-Nghe et al. (2011), who extended matrix factorization with tensor factorization to incorporate temporal effects, which significantly enhanced the accuracy of learner performance predictions [23], [35], [60], [61]. Similarly, Sahebi et al. (2016) introduced Feedback-Driven Tensor Factorization (FDTF) to integrate sequences of students, quizzes, and attempts within a tensor framework, which again improved knowledge representation and performance predictions [36]. Doan and Sahebi (2019) developed the Ranked-Based Tensor Factorization (RBTF) model that uses tensor factorization to accommodate occasional forgetting of concepts and integrate biases related to students, problems, and time, thereby supporting a predominantly positive learning trajectory [62]. Zhao et al. (2020) created Multi-View Knowledge Modeling (MVKM), a model that applies tensor factorization to derive insights from various learning materials within a shared latent space, while also considering occasional knowledge forgetting through a rank-based constraint [63]. Over time, various tensor factorization techniques have been developed to enhance the prediction accuracy and impute sparse data in educational settings [24], [64]\u2013[66].\nThese developments underscore the capability of tensor factorization to not only enhance prediction accuracy but also perform tensor-based imputation, effectively filling in missing values in sparse tensors. Tensor factorization preserves the \u201cnatural representation\u201d of synchronous and sequential learning events, which are crucial for accurately tracing and predicting learner performance and for more accurate imputation of missing learner performance values [60], [67]. This process allows for the efficient decomposition of interactions across different dimensions, enabling deeper analysis and further enriching the dataset's utility [62], [68]. The method's alignment with recommendation systems highlights its utility in identifying performance similarities and dependencies among learning events, making it a cornerstone in educational data mining and knowledge discovery when there is sparse learning performance data in ITSs."}, {"title": "C. Generative Al for Augmenting Sparse Educational Data", "content": "The advent of generative AI technologies has revolutionized the field of data augmentation, particularly in domains burdened by sparse and imbalanced datasets [25], [69]. Generative AI models have provided groundbreaking ways to synthesize realistic and existing data by capturing complex intrinsic patterns, distributions and characteristics, effectively enhancing dataset robustness for training machine learning models [70], [71]. At this point, generative AI effectively addresses scalability challenges in both high and low-dimensional simulation data. It can simulate data on a larger scale and generate tailored scenarios, facilitating informed decision-making under uncertainty conditions arising from sparse datasets [39], [72], [73]. Many successful applications in other fields have also exemplified the potential for educational data. Mariani et al. utilized GANs to balance imbalanced image classification datasets [74], while Frid-Adar et al. increased the size and diversity of medical imaging datasets through synthetic data augmentation [75]. Huang et al. transformed images for image-translation tasks, including day-to-night and vice versa, using GANs [76]. Additionally, GANs have generated synthetic fault data for wind turbines to enhance fault detection [77], [78], and replicated complex renewable energy patterns [79]. In educational settings, GANs have synthesized additional data from sparse datasets in open university learning analytics [80], illustrating their broad potential for improving learning performance data in ITSs.\nThe GPT is a state-of-the-art generative AI model known for its exceptional ability in human-like text generation and performing reasoning tasks with unprecedented accuracy [40], [81]. ChatGPT is a remarkable advancement in AI which has driven a revolutionary shifts in the use of AI in education through enhancing instructional feedback [82], boosting student engagement [83], and offering personalized learning experiences [58]. Notably, there has been tremendous progress in data augmentations in ITSs. For instance, Liu et al. [84] employed ChatGPT to enrich open-ended student responses in computer science with knowledge-guided code for short programming tasks. Susnjak et al. [85] harnessed ChatGPT to generate human-readable, prescriptive feedback based on modeling and assessing individual learners' responses, thus augmenting tailored explanations and feedback data. In our previous research [14], ChatGPT demonstrated its capability in learning the distribution of learning performance data tailored to individual patterns and selecting appropriate machine learning models for data augmentation in reading comprehension. Further studies have confirmed that ChatGPT can predict learning performance by encoding datasets, selecting and fine-tuning machine learning models, and decoding outputs as expected in prompts, such as estimated probability-based learning performance data [43].\nThe related work demonstrates the potential of generative AI models for data augmentation in ITSs. By generating synthetic yet realistic examples, these models can significantly expand the depth and breadth of learning performance data, making it more comprehensive and representative. This enables more accurate assessment of learners' progress and more robust training of AI systems that can adapt to various educational challenges and learner profiles."}, {"title": "III. DATASET", "content": "Datasets from the AutoTutor Adult Reading Comprehension (ARC) lessons, publicly accessible online\u00b9, were utilized. Ethical approval was obtained from the Institutional Review Board (IRB) under the approval number H15257. As illustrated in Table I, the four lessons sourced from the \"Stories and Texts\" series within the adult reading comprehension program cover topics including \u201cPersuasive Texts\u201d (Lesson 1), \"Cause and Effect\u201d (Lesson 2), \u201cProblems and Solutions\u201d (Lesson 3), and \"Inferences from Texts\u201d (Lesson 4). Each lesson includes 8 to 11 multiple-choice questions to test learners' reading comprehension skills. Learners start with medium (M) dificulty level materials, then, depending on their performance, either progress to the hard (H) level or move down to the easy (E) level. The \"Max. Attempt\" column in Table I defines the baseline setting for the maximum number of attempts across all selected questions records. Note that: 1) It is rare for learners to engage with the easy level in Lesson 1 and 3, since the majority can advance to the hard level; and 2) There are instances of dropout among learners during transitions.\nFor further details on the experiments conducted, please refer to the reference papers [51], [86], [87]."}, {"title": "IV. METHODS", "content": "This section describes our framework to enhance the learning performance data for AutoTutor ARC as well as the procedural methods used for its construction."}, {"title": "A. The Systematic Augmentation Framework", "content": "The systematic augmentation framework, shown in Fig. 2 integrates the construction of a three-dimensional tensor to represent learning performance data with subsequent tensor-based imputation and augmentation, effectively enriching these data. Initially, the framework structures learning performance data from real-world learner-ITS interactions into a three dimensional tensor, encompassing learners, questions and attempts. The entries in this tensor represent learning performance values, quantified as binary values (1 for correct and 0 for incorrect). As depicted in Fig. 2, the 3-dimensional tensor includes filled cubes that represent recorded learning performance values and transparent cubes that indicate sparse or missing values. Subsequently, tensor-based imputation using Tensor Factorization converts the sparse tensor into a densified form. The densified tensor provides invaluable insights into diverse learning performance patterns, enabling the segmentation of the tensor into sub-tensors based on these distinct patterns (this will be detailed in subsequent sections). Generative Al models, such as GAN and GPT, are then used to simulate additional data samples, enriching the original dataset based on specific learning performance patterns. This scalable simulation process ultimately generates a more comprehensive dataset that incorporates both imputed and augmented data.\nThe workflow framework can be formalized as follows:\n$logs_{learning} \\rightarrow T_{sparse} \\rightarrow T_{dense} \\rightarrow T_{augmented}$      (1)\nwhere $logs_{learning}$ represents the logs of learning performance (including learner, questions, attempts and actual learning performance values). These logs are structured into sparse tensor $T_{sparse}$, This sparse tensor is then processed through tensor-based imputation techniques to yield $T_{dense}$, a densified tensor that fills in missing or sparse values to create a more complete dataset. The densified tensor are then used to further augment the tensor into $T_{augmented}$, scaling the sampling size according to individualized learning performance patterns."}, {"title": "B. Construction of Three-dimensional Tensor", "content": "Consider the data produced by a population of learners working in an ITS. The data consist of a set of U learners, represented by {$l_1, l_2, l_3, \u2026 \u2026 , l_u$}, who engages with a sequence of N questions, denoted by {$q_1, q_2, q_3, \u2026 \u2026 , q_N$}. Each question in this sequence permits up to M attempts, represented by {$t_1, t_2, t_3, \u2026 , t_m$}, allowing individual learners multiple opportunities to respond. The $T_{sparse} \u2208 R^{U \u00d7 N \u00d7 M}$ captures these interactions, where the entry $T_{uij}$ in $T_{sparse}$ records the performance of learner $l_u$ on question $q_i$ at the attempt $a_j$, where the $u \u2208 (1, 2, \u2026 \u2026 , U), i \u2208 (1, 2, \u2026 \u2026 \u2026 , N), j \u2208 (1, 2, \u2026 \u2026 , M)$. Within the AutoTutor ARC context, the entry variable $T_{uij} = {0, 1, NaN}$, where 1 indicates a correct answer, 0 signifies an incorrect answer, and NaN donates unobserved values.\nThe construction of the learning materials is guided theoretically by a subject domain model and a pedagogical model that has the following assumptions: (a) Hierarchical Knowledge Representation: Each question involves distinct knowledge components or concepts, including both procedural and declarative types; these knowledge components may also be shared across different questions [63], [88]. (b) Latent Knowledge Relations: The specific knowledge embedded within each question is crucial for mastery, and the unique yet interconnected knowledge across questions create a comprehensive network of logic and procedures during the knowledge acquisition process [24], [89]. (c) Sequence Effects in Performance Interactions: Learners' sequential interaction with questions affects their understanding, comprehension, and performance; for example, one question might facilitate comprehension and performance of a subsequent question [90], [91] and there may be recency effects where performance in recent items may be weighted higher on learner mastery [24], [92]. (d) Maximum Attempt Assumption: Assume a theoretical maximum number of attempts a learner might need, highlighting the importance of evaluating comprehensive learning states through repeated attempts. (e) Similarity in Learning for Individual Learners: Assuming a common relevance and utility in the mode of knowledge acquisition among learners, it becomes possible to predict knowledge mastery based on similarities in their individual learning pathways [35], [60]. (f) Probability-based Prediction: Assume that predicted learning performance value is represented as a continuous probability between 0 and 1, indicative of the likelihood of knowledge mastery [93]."}, {"title": "C. Tensor-based Imputation", "content": "We model the sparse tenor $T_{sparse}$ through factorization into two lower dimensional components: (1) a factor matrix U of size $U \u00d7 K$, which captures the latent learning-related features of U learners, such as initial learning abilities and learning rates, where K is the total number of these features; and 2) a latent tensor V of size $K \u00d7 M \u00d7 N$, representing learner knowledge in terms of K latent features across M attempts on N questions. The approximated tensor $T_{dense}$ is is computed as follows:\n$T_{dense} \u2248 U \u00d7 V + b + b_l + b_q + b_a + \u03b5$    (2)\nwhere $b_l, b_q, and b_a$ represent the biases from learners, questions, and attempts, respectively, and \u025b denotes a global bias. During tensor factorization, the sigmoid function is applied to normalize the output of estimated performance values, ensuring outputs are bounded between 0 and 1. The model also incorporates a rank-based constraint, which promotes a trend of monotonic knowledge acquisition across successive attempts by learners, while still allowing for potential forgetting or slipping [59], [62]. The objective function includes terms for mean squared error to measure the discrepancy between observed values and predictions, and regularization terms for the decomposed components (the learner feature matrix U and the latent tensor V and various biases) to mitigate overfitting, along with a rank-based constraint. Optimization is carried out using stochastic gradient descent to minimize objective function, iterating until convergence. Ultimately, the resulting $T_{dense}$ functions as a probability-based filled tensor, indicating the likelihood of knowledge mastery states [59].\nThe Tensor Factorization method thus enables us to construct and assess different multidimensional models for learning performance that facilitate data imputation for comprehensive modeling of the learning process. We explored three different baseline models that will ultimately be compared with our proposed model. (a) Bayesian Knowledge Tracing (BKT): BKT uses a Hidden Markov Model to dynamically assess and predict a learner's knowledge state (represented as binary states of \u201cknown\u201d and \u201cunknown\u201d), with adjustments based on their responses to questions while considering probabilities of learning, guessing and slipping [88]; here, it also initially integrates both student-specific and skill-specific parameters to effectively account for individual learner variability and the hierarchical nature of skills [33], [94], [95]. (b) Performance Factor Analysis (PFA): PFA utilizes logistic regression to estimate the probability of the learner' performance on the question, factoring in individual learning ability, skill-related features (e.g., difficulty), and the learner's previous success and failures [32], [92]. (c) Sparse Factor Analysis Lite (SPARFA-Lite): SPARFA-Lite, a streamlined version of Sparse Factor Analysis, uses matrix completion techniques to efficiently analyze graded learner responses and predict performance by determining the optimal number of knowledge components, offering enhanced computational speed over the initial Sparse Factor Analysis [96]\u2013[98]."}, {"title": "V. IDENTIFICATION OF LEARNING PERFORMANCE PATTERNS BY CLUSTERING", "content": "To make data augmentation adaptive to individual learning performance patterns, we first identify these patterns by clustering based on the similarity of each learner's performance across different attempts at specific questions.\nThe matrix slice $\u03a9_{qn}$, extracted from the $T_{dense}$, encapsulates the probability-based knowledge states associated with the performance on the nth question $q_n$, for all U learners over M attempts. Eq. 3 illustrates our approach to identifying learning performance patterns within $\u03a9_{qn}$. The matrix can be represented as a sequence of vectors {$L_1, L_2, L_3, \u2026 , L_n$}, where each $L_u$ aggregates the uth learner's performance across all attempts for the specified question. The performance distribution for each learner vector is assumed to be modeled by function $G(\u00b7)$, with the associated set of parameter vectors depicted as {$\u03b8_1, \u03b8_2, \u03b8_3, \u2026 , \u03b8_M$}. The fluctuations in these model parameters indeed reflect individual differences in learning performance patterns by quantifying the uncertainties within the evolving knowledge states for each learner.\n$\u03a9_{qn} =  \\begin{bmatrix}\nL_1 \\rightarrow G_1(L_1) \\rightarrow \u03b8_1 \\\\\\\\\nL_2 \\rightarrow G_2(L_2) \\rightarrow \u03b8_2 \\\\\\\\\nL_3 \\rightarrow G_3(L_3) \\rightarrow \u03b8_3 \\\\\\\\\n\u2026 \\\\\\\\\nL_U \\rightarrow G_C(L_U) \\rightarrow \u03b8_U\n\\end{bmatrix}   \\Rightarrow   \\begin{bmatrix}Cluster_1\\\\ Cluster_2\\\\ Cluster_3\\\\ \u2026 \\\\Cluster_C \\end{bmatrix}$  (3)\nIn this study, we employed a power law function for G(.) to model the relationship between learners' performance values and their number of attempts, drawing on the learning curve theory proposed by Newell and Rosenbloom [99]. This theory links error rates to practice amounts and supports the power law's use [99]. The power law learning curve is particularly valued for its robust fit and interpretable parameters, which has been widely recognized in educational and training research [100], [101]. In the power-law formula $Y = aX^b$, Y represents learning performance, quantified as the probability of producing correct answers, while X denotes the number of attempts to respond to the current question. The parameter a indicates the learner's initial learning ability or prior knowledge, and b quantifies the rate at which the learner acquires knowledge through practice. We then utilized K-means++ algorithm [102] to cluster the distribution of these parameters (a and b) among learners, which assists in identifying distinct individual learning performance patterns."}, {"title": "VI. DATA AUGMENTATION BASED ON GENERATIVE MODELS", "content": "We investigated two generative AI models, GAN and GPT, to generate learner data that manifests particular performance patterns, and thereby enable scalable sampling.\nGenerative Adversarial Networks. GANs employ two complementary networks, a generator (G) and a discriminator (D), both of which are implemented using convolutional neural networks (CNNs) [39], [70]. The generator is designed to produce simulated data samples from initialized random noise, typically sourced from a Gaussian distribution. Its output is crafted to be compatible with the input from the individualized learning performance pattern as synthetic sample data. The discriminator's role is to determine whether the simulated data samples are real or synthetic by comparing them with actual data samples (original learning performance distribution). Concurrently, the $G(\u00b7)$ is trained to progressively reduce the difference between the distributions of the real and simulated data through iterative tuning. The training costs for both $D(\u00b7)$ and $G(\u00b7)$ are dictated by the objective function $V(G, D)$, defined as follows [70]:\n$\\displaystyle\\min_{G} \\displaystyle\\max_{D} V(G, D) = E_{simulate}[logD(T_{real}, T_{simulate})] + E_{noise} [log(1-D(G(Random Noise)))]$  (4)\nwhere the $T_{real}$ represents the real performance sample input to $G(\u00b7)$, and $T_{simulate}$ denotes the output $G(\u00b7)$, simulating real data for assessment by $D(\u00b7)$. The term $E_{simulate}[]$ calculates the expectation of log-probability that $D(\u00b7)$ correctly identifies whether data is real or simulated, while $E_{noise}[]$ measures the expectation of log-probability that $D(\u00b7)$ correctly rejects generated data as fake. Ultimately, the finely tuned generator produces scalable simulated sample data.\nGenerative Pre-trained Transformer. Data augmentation utilizing GPT-4 [103] involves learning by capturing actual data distribution and simulating data samples through a strategic prompt-based process. This process, detailed in Zhang et al. [14], [43], involves three main procedures: encoding, which converts numerical learning performance values into contextual prompts; an LLM component that involves prompting an augmentation method that capitalizes on GPT-4's reasoning and understanding capabilities; and decoding, which entails generating the simulated data along with interpretations. As for the aforementioned matrix slice $\u03a9_{qn}$ that represents the learners and questions for learning performance, it can be formalized as GPT($\u03a9_{qn}$), which scales the size of the learner's data. Specifically, inputs such as individualized learning performance values are contextualized with detailed information about questions, answers, and attempts, including descriptions of their format and content. Subsequently, a simulation request prompts GPT-4 to seamlessly integrate this numerical and textual data, driving the execution of a simulation. During this process, sampling mathematical models can be searched, selected, and fine-tuned by GPT-4 to enhance the data augmentation process. The prompts are iteratively refined to yield results that align with our specifications."}, {"title": "VII. EXPERIMENTAL SETUP AND EVALUATION", "content": "The experimental setup is designed and optimized according to the framework's workflow", "Levels": "The evaluation of sparsity levels is conducted by calculating the missing rate", "105": ".", "Factorization": "The number of latent features can range from 1 to 20", "Configurations": "The BKT and PFA models utilize two distinct assumptions for setting KCs: \u201cSingle KC\u201d", "Unique KC": "which assumes a one-to-one correspondence between questions and knowledge components. (d) Cross-Validation for Running Data Imputation Models: For Tensor Factorization and other baseline models in the data imputation stage", "Imputation": "As referenced in other peer research [24", "33": [62], "106": "both Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) are employed to quantify the performance of the models. (f) Significance of Model Performance by ANOVA Analysis: ANOVA analysis is employed to assess the statistical significance of model performance", "Size": "Different degrees of data augmentation in terms of total generated sample size are contrasted. The objective of this comparison is to observe differences in variance and bias in parameter estimates fit to augmented learner performance data sample sizes. Sample sizes are set in increments of 1", "Measurement": "The Earth Mover's Distance (EMD) metric [107", "108": "is employed to quantify divergence by measuring the distance between parameters of the power-law functions fitted to simulated and original data distributions separately, and to assess the reliability of data augmentation across varying sample sizes. EMD measures the minimum cost of transforming one distribution into another, reflecting the amount of work needed. Smaller EMD values indicate closer similarity between distributions, while larger values indicate greater divergence. This EMD metric effectively captures how changes in simulation sample size influence divergence from the original data.\nAdditionally, we include fine-tuning details here to enhance model optimization. To achieve optimal model performance, it's essential to fine-tune the parameters of each model during the optimization process. For the BKT, the four key parameters ($P("}]}