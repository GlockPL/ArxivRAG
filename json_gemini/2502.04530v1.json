{"title": "Robust Probabilistic Model Checking with Continuous Reward Domains", "authors": ["Xiaotong Ji", "Hanchun Wang", "Antonio Filieri", "Ilenia Epifani"], "abstract": "Probabilistic model checking traditionally verifies properties on the expected value of a measure of interest. This restriction may fail to capture the quality of service of a significant proportion of a system's runs, especially when the probability distribution of the measure of interest is poorly represented by its expected value due to heavy-tail behaviors or multiple modalities. Recent works inspired by distributional reinforcement learning use discrete histograms to approximate integer reward distribution, but they struggle with continuous reward space and present challenges in balancing accuracy and scalability.\nWe propose a novel method for handling both continuous and discrete reward distributions in Discrete Time Markov Chains using moment matching with Erlang mixtures. By analytically deriving higher-order moments through Moment Generating Functions, our method approximates the reward distribution with theoretically bounded error while preserving the statistical properties of the true distribution. This detailed distributional insight enables the formulation and robust model checking of quality properties based on the entire reward distribution function, rather than restricting to its expected value. We include a theoretical foundation ensuring bounded approximation errors, along with an experimental evaluation demonstrating our method's accuracy and scalability in practical model-checking problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Probabilistic model checking (PMC) [29] plays a critical role in many self-adaptation methods, allowing the detection of quantitative requirements violations and supporting the planning of appropriate adaptation actions [7], [15], [32], [46]. However, most PMC methods allow to reason only about the expected value of a system property of interest, which may neglect the variability in the distribution of possible outcomes.\nFor example, even if the average execution time of the system is less than 100ms, a large number of runs may exceed such expected value. Not accounting for the variability in a measure of interest's distribution may provide a partial assessment of a system's performance, in turn limiting the awareness of the adaptation logic and the robustness of its decisions.\nRecent advancements in PMC [2], [12], also inspired by related results in reinforcement learning [5], [11] have incorporated distributional perspectives in the analysis of models abstracted as Discrete-Time Markov Chains (DTMCs) and Markov Decision Process (MDPs) by approximating probability or reward distributions using histograms. However, the precision of histogram approximations is constrained by the setting of bins, with fewer bins leading to coarser representa-tions. Furthermore, the resulting piecewise constant approximations fail to capture subtle features such as multimodality or skewness, and their applicability is restricted to integer reward scheme to ensure convergence. These methods also face scalability challenges, particularly in large or continuous reward spaces, where sparse data worsens the inaccuracy of the discretization process."}, {"title": "II. BACKGROUND", "content": "Probabilistic model checking is a formal method to reason about quantitative properties of systems, whose behavior is abstracted via convenient stochastic models [29]. In this work, we focus on discrete Markov models and on the verification of regular properties [3]. Regular properties can be reduced to predicates on the probability of reaching a target state or on the cumulative reward until reaching a target state. For example, the co-safe fragment of linear time logic (LTL) is used to define sequential plans for a robot [30] or probabilistic computation tree logic (PCTL) properties without nesting of the modal operators [3]. Without loss of generality, we will focus on properties of the forms  P\u22c9p[\u03c6]  and  R\u22c9r[\u03c6],  where  \u22c9\u2208{<,>} , and  p\u2208[0,1]  and reward  r\u2208[0,\u221e)  represent bounds on the probability of eventually reaching a state where the Boolean predicate  \u03c6  holds.\nIn its standard semantics [4], a formula  P\u22c9p[\u03c6]  holds in a state  s  if the expected probability of eventually reaching a state satisfying  \u03c6  is  \u22c9p.  Similarly,  R\u22c9r[\u03c6]  is evaluated by constraining the expected cumulative reward until a state satisfying  \u03c6  is reached. This formulation takes advantage of the linearity of the expected value to reduce the probabilistic model checking problem to the solution of a linear system of equations for DTMCs, while for MDPs [39] to efficient dy-namic programming using the standard Bellman operator [3]. However, the expected value may not tell the full story about the system being analyzed [12]. For example, the dispersion of the distribution, its skewness, or the presence of multiple modalities may render the expected value a poor representative of the system's behavior.\nChance constraints. In this work, we aim to reinterpreting P\u22c9p[\u03c6] and  R\u22c9r[\u00b7]  as evaluations of the cumulative distribution function for the reachability and reward distributions. This enables verifying requirements of the form \"a reward larger than r is obtained with at least probability c\". Notice that 1) reasoning about the probability of reaching a target state (i.e., a state satisfying a desired property  \u03c6  ) can be reduced to reasoning about the cumulative rewards to reaching a target state via systematic model transformations [16] and, similarly, 2) the paths reaching a target state can be reduced to paths to absorption into a designated absorbing state, again via the identification of connected components and systematic model transformations [3]. Therefore, to simplify the presentation and without loss of generality, the remainder of this paper will focus on approximating and reasoning"}, {"title": "B. Markov Reward Process", "content": "Discrete-Time Markov Chain (DTMC). A Discrete-Time Markov Chain is defined as a tuple  D=(S,s0,P,r,AP,L) , where  S  is a finite state space,  s0\u2208S  is the initial state, P:S\u00d7S\u2192[0,1] is the stochastic matrix representing the transition probabilities (such that  pij\u22650  for all  pij\u2208P  and  \u2211jPij=1  for all  i ), and  r:S\u2192R>0  is a reward function that assigns a reward to each state\u00b9,  AP  is a set of atomic propositions and  L:S\u21922AP  is a labeling function mapping each state to a set of propositions that hold true in that state. This stochastic system evolves as a sequence of random variables  {Xn}n\u22650 , where each  Xn  represents the state of the system at the n-th time step.\nNotably, a specific induced DTMC  MD=(SD,s0,PD,r,AP,L)  can be constructed by a Markov Decision Process (MDP) M and a given policy  \u03c0 , where  SD  is a sub-space of the finite state space of the original MDP,  s0\u2208SD  is the initial state,  PD:S\u00d7A\u00d7S\u2192[0,1]  is the stochastic transition matrix and  r:(S,A)\u2192R>0  is a reward function. The policy  \u03c0:S\u00d7A\u2192{0,1}  is a specific action a,  \u03c0(\u03b1\u2032s)=1  and  \u03c0(\u03b1\u2032\u03c2)=0  for all a' \u2260 a. Mp\u0189 is fully probabilistic with no open action choices.\nReward Process. Consider a non-negative reward function r:S\u2192R\u22650 , representing the instantaneous reward received in each state, together with a sequence of random variable {Xn}n\u22650 . The total cumulative reward up to the time N over the times  0,...,N  is  R(N)=\u2211Nn=0r(Xn) . As the expectation of probabilities and rewards in Markov Models can be computed as the solutions of linear equations [9], the expected total cumulative reward  \u03bc=E[\u22111=0r(X)]  can be easily computed by solving the linear system  \u03bc=r+P\u03bc  if the system is transient. Furthermore, by utilizing the Moment Generating Function (MGF) of total reward to absorption, we can compute the k-th moments of total reward for higher moment analysis.\nMoment-Generating Function (MGF). Consider a set of transient states  C\u2286S  and absorbing states  CC\u2286S  (where  C\u2229CC=\u2205, C\u222aCC=S ), with  T=inf{n>0:Xn\u2208Cc}  denoting the first hitting time of a state in CC. The total cumulative reward up to absorption is represented by  RT= \u2211Tt=0r(Xt) . The MGF of  RT  starting from  X0=x , denoted by  MRT(\u03b8,x) , is given by:\n MRT(\u03b8,x)=Ex[e\u03b8RT]=Ex[exp(\u03b8\u2211T\u22121n=0r(Xn))]=\u2211Tt=0Ex[exp(\u03b8\u2211t\u22121n=0r(Xn))](1)\nUsing first-step analysis, this MGF  MRT  satisfies the linear system:\n MRT(\u03b8)=f(\u03b8)+G(\u03b8)M(\u03b8), \nwhere  f(\u03b8,x)=\u2211y\u2208Cce\u03b8r(x)P(x,y)  for  x\u2208C , and  G(\u03b8,x,y)=e\u03b8r(x)P(x,y)  for  x,y\u2208C . The k-th moment uk(x)=Ex[RkT]  starting at  X0=x  can be obtained by differentiating the MGF  uk=d\u03b8kMRT(\u03b8)  with the following recurrence:\n uk(x)=\u2211y\u2208Cce\u03b8r(x)P(x,y)\u2211Tt=0Ey[\u2211t\u22121n=0r(Xn)k](2)\n =r(x)k+\u2211y\u2208CP(x,y)(\u2211k\u22121i=0(ki)r(x)k\u2212iui(y))\nExample. The first moment for our UAV computed with Equation (2) corresponds to the expected reward computed, e.g., by the PRISM model checker [31]:  \u03bc=7.51 . However, the standard deviation (square root of the second moment) is \u03c3 = 4.91. Using, for example, Cantelli's inequality [19] (as known as one side Chebysehv's inequality [40]), this implies only 50% of the trajectories are guaranteed to fall within the interval  [0,\u03bc+\u03c3] , leaving a significant portion of distribution not represented.\nThis result indicates that the expected value alone provides limited information, highlighting further refinement for accurate distribution approximations and robust model checking, particularly in risk-sensitive systems, to ensure reliable out-comes. We introduce next the base distribution we will use for such approximations.\nErlang Distribution. The Erlang distribution is a special case of both phase-type and Gamma distributions with integer shape parameters [13]. An Erlang distribution with shape parameter a\u2208Z+ and rate parameter  \u03bb\u2208R+  is defined by the probability density function:\n fx(x;a,\u03bb)=\u03bb\u03b1x\u03b1\u22121e\u2212\u03bbx(a\u22121)!(3)\nThe Erlang cumulative distribution function is given by:\n Fx(x;a,\u03bb)=1\u2212a\u22121\u2211j=0(\u03bbx)je\u2212\u03bbxj!(4)\nTo approximate the cumulative reward to absorption dis-tribution, we will employ a mixture of Erlang distributions, following its inherent connection with Markov models [1] and its strong ability to closely approximate the distributions of any random variable in continuous settings and discrete settings"}, {"title": "III. ROBUST PROBABILISTIC MODEL CHECKING", "content": "We propose a robust PMC method for DTMCs by approxi-mating the cumulative reward distribution towards absorption through moment matching with mixtures of Erlang distribu-tions. The approximation is formulated as a truncated Stieltjes moment problem [18], to reconstruct the density function from a finite number of its moments on the semi-infinite interval [0,+\u221e) . This approach specifically focuses on preserving the statistical properties of the distribution while addressing the challenge of having only partial information in the first K moments, rather than the full infinite moment sequence. This approach avoids the complexities of discretization and iterative updates of histogram-based methods limited to dis-crete rewards [12], thus avoiding potential issues of binning resolution trade-off and state-space explosion. Instead, our method focuses on refining the approximation based on a set of invariant statistical characteristics to approximate the density function with essential statistical information directly."}, {"title": "A. Markov Reward Distribution Approximation", "content": "Problem Formulation. Our objective is to approximate the distribution of the cumulative reward towards absorption R(N) in a DTMC D, with initial state  s0  and transition matrix P. We define the approximating density  fapprox(x)  of the approximate reward distribution X as a mixture of n Erlang densities:\n fapprox(x)=\u2211ni=1wiferlang(x;ai,\u03bb),(5)\nwhere  wi\u22650  is the non-negative mixture weight of the i-th Erlang density,  ai\u2208Z+  is the shape parameter of the i-th Erlang density, and  \u03bb\u2208R+  is the common rate parameter.\nWe formalize the approximation process as solving a trun-cated Stieltjes moment problem [18] on the semi-infinite interval  [0,+\u221e)  with the first K-th moments of the random variable X. The parameters wi, ai,  \u03bb  are determined by solv-ing an optimization problem that minimizes the difference between the first K-th moments  \u03bc1,...,\u03bc\u03ba  of the true re-ward distribution (cf. Sec. II-B) and the moments  \u03bc\u02c61,...,\u03bc\u02c6\u03ba  of the approximate distribution. We further incorporate the maximum entropy principle within our moment matching formulation [17], [36], to ensure that among all possible approximate distributions matching the given moments, the one with the least bias (i.e., the one maximizing entropy) is selected.\nThe objective function of the optimization problem is de-fined as:\n minwi,ai,\u03bbL=K\u2211k=1(\u03bck\u2212\u02c6\u03bck)2\u2212\u03b3H(fapprox),(6)\nwhere  \u03b3\u22650  is a weighting parameter controlling the trade-off between moment matching and entropy maximization,\n H(fapprox)=\u2212\u222b\u221e0fapprox(x)logfapprox(x)dx is the differen-tial entropy of the approximated distribution, the moments  \u03bck  of the true reward distribution are computed with the MGF in Equation (2), and the moments of the mixture of Erlang distributions is  \u03bc\u02c6k=\u2211ni=1wi(ai+k\u22121)!(ai\u22121)!\u03bbk.\nMain Algorithm. The approximation algorithm, detailed in Algorithm 1, takes as input a DTMC D along with an arbitrarily small error bound  \u03f5>0 , the number K of moments to be matched, and the size of the mixture n, and it returns the approximate distribution. For a given number K of moments, the mixture of Erlang distributions can match them using approximately  [K/2]+1  components, reducing the mixture size while preserving accuracy [28]. We first compute the true target moments  \u03bck , providing the necessary statistical infor-mation using Equation (2) (lines 1-2). Before the optimization phase, we initialize the mixture weights wi, shapes ai, and rate parameter  \u03bb  (detailed discussion on initialization and hyper-parameters in Section V).\nAt each optimisation step (in the loop at line 4), we compute the first K moments of  fapprox  and the differen-tial entropy  H(fapprox) . The objective function L balances moment matching with entropy maximization, controlled by the weighting parameter  \u03b3 . The parameters wi, ai, and  \u03bb  are updated to minimize L subject to the constraints, until the convergence criteria are satisfied. The algorithm then returns the approximated distribution with the optimized parameters for further analysis via robust model checking. To improve the numerical stability of moment matching, we further uti-lize standardized moments  \u03bc\u02c6k  in the optimization process  \u03bc\u02c6k=\u03bck/c , with  c=\u221a\u03bc2, for  k=3,4,...,K .\nAlgorithm 1 produces an arbitrarily close approximation  fapprox  that closely matches the cumulative reward distri-bution associated with discrete Markov models. Arbitrary accuracy can be obtained by increasing the number of mo-ments to be matched and the size of the mixture [18], [42]. The inclusion of entropy maximization ensures that, among all distributions matching the given moments, the algorithm selects the most unbiased one. This feature is particularly advantageous for robust probabilistic model checking to avoid overfitting on a small number of moments [36].\nComplexity Observations. The main algorithm starts with a preliminary phase which computes the first K moments of the reward distribution based on the reward scheme and transition matrix of the DTMC. Each moment  \u03bck  is computed by solving a system of linear equations involving the lower-order moments up to  \u03bck\u22121 . Therefore, to compute the first K moments, it is sufficient to solve K systems of linear equations. The size  \u03b4  of each system of equations corresponds"}, {"title": "B. Distributional Analysis for Robust Model Checking", "content": "Robust Property Evaluation. The evolution of the random variable X can be treated as the outcome cumulative reward of the underlying stochastic system, with each transition gov-erned by probabilistic dynamics. By considering the density  fapprox  of X, we formalize a robust property evaluation for chance-constrained requirements [14] of the form:\n Pr(X<r\u2217)\u2265\u03b1,(7)\nwhere  r\u2217  is a critical threshold representing a performance or safety metric, and  \u03b1  is the required probability level. For example, in a safety-critical system, we can verify that the probability of a performance violation remains below a predefined risk level  \u03b1 , rather than asking whether just the expected reward is below the threshold. Notice that constraints of the form  X>r\u2217  or, more in general,  r\u2217 \u227aX\u227ar\u2217,  can be evaluated with the same method using the standard properties of the cumulative distribution function.\nAlmost sure properties, where the probability of satisfying a temporal formula is required to be 1 (or 0), can also be straightforwardly phrased as chance-constrained requirements, whereas chance-constrained formulations allow for the con-trolled relaxation of the requirements. This makes our distri-butional method well suited for the verification of probabilistic safety requirements [29], as well as for the robustness anal-ysis of the policies synthesized for MDPs via reinforcement learning [20], by reasoning on the induced DTMC model [4, Ch. 10]."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we demonstrate that it is acceptable and effective to approximate the cumulative reward distribution of a DTMC using mixtures of Erlang distributions. We use Erlangization to obtain a continuous approximation which is later approximated by a mixture of Erlang distribution on a modified DTMC with discretized reward. By applying these theorems, we justify the use of mixtures of Erlang distributions as a valid method for approximating the cumulative reward distribution and its exponential tail behavior.\nTheorem IV.1 (Approximation of Cumulative Reward by Discrete Phase-Type (DPH) Distribution). Consider a DTMC  D=(S,s0,P,r)  with a finite state space and non-negative rewards  ri\u2208R\u22650  and a cumulative reward towards absorp-tion  F(r)=RT(s0) . For any  \u03f5>0 , there exists a precision parameter  \u03b4>0  and a DTMC  D\u02dc=(S\u02dc,s0,P\u02dc,\u03b4e) , where e is the vector of all ones, such that  F(r)  can be approximated"}, {"title": "V. EVALUATION", "content": "We evaluate our moment-matching approximation method with error analysis using case studies adapted from PRISM [31] and used also in previous literature [12]. Our im-plementation employs an Erlang mixture model with moment matching and maximum entropy using SciPy SLSQP [45], and Gurobi [21] for optimization, while the linear systems of equa-tions required to compute the moments of the reward process are solved with Numpy [22]. The primary goal is to empir-ically assess the approximation performance of our method against the empirical distribution and traditional histogram-based approximation from the perspective of accuracy and computational tractability, as well as discuss hyper-parameters and overheads.\nBenchmarks. The benchmarks, adapted from PRISM case studies [31], include scenarios like message synchronization and Herman protocol [25], which are modeled as DTMCs; for the MDP benchmarks (such as betting games, and grid world navigation), we evaluate the DTMC induced by the optimal policy produced by PRISM (reward schemes defined for the original models). We evaluate our method also on two models with continuous rewards, which cannot be analyzed by previ-ous work [12]: UAV (our example model) and an adaptation of the Future Investor model [35]. The benchmark subjects cover a diverse range of state space sizes and transition structures, as summarized in Table I.\nExperimental Settings. We construct the approximate re-ward distribution from the initial state of each model using SLSQP [45]. To evaluate the accuracy of the approximation, we construct a baseline empirical reward distribution by run-ning one million simulations of the models and collecting the reward accumulated for each run. For models using integer rewards, we also compare with the histogram distributions constructed by [12].\nTo align the approximation problem with the truncated Stieltjes moment problem on  (0,+\u221e) , we shift the location of the Erlang distributions to  loc=\u03bc\u2212\u03c3  (\u03c3 being the standard deviation of the reward distribution), following the location"}, {"title": "VI. RELATED WORK", "content": "Moment Matching Approximation. Moment matching is a classical method for approximating density functions [27], [37]. The main challenge of density approximation methods is to select an appropriate foundational distribution to represent the unknown distribution. Existing methods have focused on using powerful kernel density function [18] or Erlang mixtures [23], [28], given their capability in recovering a wide variety of distributions. However, the accuracy of these approximations depends heavily on how well the selected function or kernel aligns with the underlying distribution. We utilize the Erlang mixtures, not only due to their flexibility but also because of its inherent connection to Markov models, which allows us to approximate the reward distribution with bounded error.\nRobust Model Checking. Robustness is one of the most important concerns in model checking methods. Some simulation-based approaches provide probabilistic bounds with confidence guarantees of model correctness [24], [38]. There are also several methods that provide evaluation be-yond expected value with distributional information, including the quantile-based method [2] and histogram approximation method [12], but could lead to unbounded information loss due to the coarse approximation. In contrast, our approach provides smoother approximation while avoiding discretiza-tion errors and offering guaranteed accuracy in both discrete and continuous domains.\nDistributional Reinforcement Learning. Recent work in distributional reinforcement learning has focused on estimating the full return distribution instead of just its expectation. a categorical histogram approximation method has been prosed in [5] to compute return distributions using discrete bins, optimizing the Wasserstein distance between estimated and target distributions. Extensions of this work, such as quantile regression (QR-DQN) [11] and implicit quantile networks (IQN) [10], improve on this by learning the quantile-based approximation, offering greater flexibility and risk-sensitive policies. Our method can be further extended to the RL domain, providing precise estimation and flexibility in con-tinuous reward environments than histogram methods, with a robust verification that can be used as guidance for distribu-tional policy iteration."}, {"title": "VII. CONCLUSION", "content": "In this work, we addressed the limitations of current PMC methods that rely on expected values, by proposing a moment-matching approach using mixtures of Erlang distributions. This approach enables a more precise approximation of cumulative reward distributions in DTMCs by capturing multi-modality and higher-order statistical features. We enhance system ver-ification by incorporating the full distribution of outcomes, enabling better decision support for adaptive systems. Next, we aim to extend our approach to robust model checking and learning in MDPs, where distributional robustness could guide policy search. We plan to leverage the prior knowledge of the accumulated reward distribution in MDPs, as demonstrated in this work, to provide a robust alternative to histogram-based approximation, which may struggle in distributional RL scenarios involving sparse or dense reward regions and con-tinuous reward space. We also aim to explore the applicability of this approach in distributional learning settings beyond distributional model checking, to enhance policy iteration process and evaluate its potential for synthesizing more robust policies under uncertainty in practical applications."}]}