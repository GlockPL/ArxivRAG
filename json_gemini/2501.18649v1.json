{"title": "Fake News Detection After LLM Laundering: Measurement and Explanation", "authors": ["Rupak Kumar Das", "Dr. Jonathan Dodge"], "abstract": "With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "Paraphrasing is the process of generating text from a reference text with syntactic and lexical diversity while maintaining semantic similarity. Paraphrasing is important for different downstream NLP tasks, such as text summarization [1], semantic parsing [2], question answering [3, 4], data augmentation [5], adversarial example generation [6], and checking the robustness of a model [6]. However, effective paraphrasing is challenging because it involves syntactically rephrasing text, but preserving meaning [7].\n\nThe impact of paraphrasing on fake news detection is still under-explored, and the advancement of large language models only increases the importance of this field. OpenAI reported ongoing attempts to misuse AI for political misinformation. Still, the most widespread incident was a hoax falsely claiming to involve its models, with the overall impact on the 2024 election appearing modest [8].\n\nState-of-the-art fake news detectors mainly distinguish real from fake based on human knowledge (expert- or crowdsourcing-oriented), content features (linguistic, syntactic, and sentiment), and network features. Those features may make it easier for state-of-the-art detectors to detect LLM-generated/synthesized fake news because of their patterns of generating fake news.\n\nThe impact of the generated text by different LLMs (e.g., GPT [9], BERT [10], T5 [11], LLaMA [12]) on fake news detection systems has recently attracted attention. LLMs are reasonably good at generating and synthesizing fake news. LLMs like GPT2 can synthesize and spread misinformation by pre-training it to a large-scale news corpus [13]. Further, LLM-generated fake news is more controllable because the generation process conditions on knowledge elements (entities, relations, and events)"}, {"title": "2 Background", "content": "Researchers have long used traditional techniques to generate paraphrased text, such as manual rules [25] or lexical substitutions [26]. Deep neural networks have been prevalent in generating paraphrased text in the last decade. [27] combine a generator and evaluator to design a reinforcement learning-based paraphraser. There, the evaluator provides the rewards, which then fine-tune the generator. [28] incorporate an LSTM model with a variational autoencoder (VAE) to generate multiple paraphrased texts for a given sentence. A similar kind of VAE model generates paraphrased sentences [29] without using bilingual data. [30] utilizes a network of four-layer stacked LSTM and residual connections like the ResNet [31] model for paraphrase generation. [32] proposed a similar network with a latent bag of words. To mitigate the slow training issue in sequence-to-sequence models, [5] proposed a novel approach for paraphrase generation that consists of exclusive convolution for local interactions and self-attention for global interactions. Researchers also implemented different techniques to generate controlled paraphrased text using an additional set of position embeddings [33], decomposition mecha-"}, {"title": "3 Methodology", "content": "Figure 1 shows an overview of our methodology.\nWe used two publicly available datasets to assess the performance of fake news detectors. The first dataset [62] is on COVID-19 misinformation, containing only two classes: Real and Fake. This dataset has 5524 real news and 5030 fake news, making it quite balanced. The second dataset comes from POLITIFACT.com and is called LIAR [63]. It consists of 12.8K manually classified statements made by politicians in various contexts. However, this dataset differs from the COVID-19 dataset because it has six labels: true-16.37%, mostly true\u201419.15%, half true-20.68%, barely true-16.22%, false\u201419.50%, and pants-fire\u2014 8.09%. The creators have pre-split the dataset into train, test, and validation. We curated and preprocessed data with the NLTK [64] library to prepare the data for input to the detectors."}, {"title": "3.1 Classifiers and Paraphrasers", "content": "We considered logistic regression, decision tree, random forest, and support vector machine as supervised models, CNN and LSTM as deep learning models, and BERT [10], T5 [11] and Llama [12] as pre-trained language models. We selected those models due to their effectiveness and popularity in text classification tasks [65].\n\nIn the next step, we used three techniques, each from an LLM family, to paraphrase both the fake and real news. The first paraphraser is called PEGASUS [66], a transformer-based model. For the"}, {"title": "3.2 Implementation Details", "content": "We performed our experiment on a computer with NVIDIA RTX A4500 (20GB) GPU, consuming a total of \u224820 hours. We implemented the supervised learning algorithms from the sklearn python library [67].\n\nWe built the CNN models in TensorFlow. The input layer consisted of 1024 units with ReLU activation, followed by hidden layers with 512 and 256 units also activating ReLU, a dropout layer with a 0.2 rate, and an output layer with units equivalent to the number of classes and sigmoid activation for multi-class classification. We employed a batch size of 32, 10 training epochs, and an embedding size of 300 in building the CNN model.\n\nOur TensorFlow-based LSTM classifier consisted of an LSTM layer with 100 units, a dense output layer with sigmoid activation, an LSTM layer with the pre-trained embeddings, and a spatial dropout1D layer.\n\nWe trained the PyTorch-based BERTbase model on the GPU for ten epochs with: learning rate at 1e-5, Adam epsilon at 1e-8, and tokenized text sequences capped at 300 characters.\n\nWe adopted the T5 classifier and parameters from a GitHub repository [68]."}, {"title": "3.3 Evaluation Techniques", "content": "To evaluate the performance of the models, we adopted the typical metrics (i.e., accuracy, F1 score, precision, and recall). To determine which detectors perform best for a given task, we relied on the macroF1 score because it balances the importance of precision and recall, especially for an imbalanced dataset. The LIAR dataset is quite imbalanced, and the F1-score is the better evaluation metric to access the classification result. We measured the performance of the same set of detectors on both the original and paraphrased texts.\n\nWe evaluated the paraphraser's quality with the open-source contextual embedding-based technique BERTSCORE, which showed strong performance in adversarial paraphrase detection [21]. Our specific metric is the F1 value of BERTSCORE, which we denote as FBERT score. Calculating similarity with FBERT score takes two arguments, which are, in our case, human-written fake news and the LLM-paraphrased output. This FBERT score is the harmonic mean of PBERT and RBERT. PBERT measures how much the reference sentence captures the meaning of the candidate sentence by averaging the maximum cosine similarity between each token in the candidate and the reference. RBERT measures how much the candidate sentence captures the meaning of the reference sentence using the same technique.\n\nFinally, we explored LIME explanations to discover reasons for getting different classification results between human-written and paraphrased text. Based on those observations, we then applied a sentiment analyzer [69] to each tuple of human-written and three LLM-paraphrased outputs."}, {"title": "4 Results", "content": "Our Supplemental Materials contain two files which have all of the data we used: \u201cOriginal text\", three \u201c<MODEL> paraphrased\u201d, each with accompanying paraphrase and sentiment scores."}, {"title": "4.1 RQ1 - Human-writing vs Paraphrase", "content": "In RQ1, we find out how the detectors perform on human-written fake news and LLM-paraphrased fake news articles. The results show that the detectors struggle to detect LLM-generated fake news more than human-written fake news. Further, while the F1 scores were low for LLM-paraphrased fake news, the Pegasus-paraphrased fake news is the most challenging to detect.\n\nTable 1 demonstrates the accuracy, F1 score, precision, and recall values of all the fake news detectors on the COVID-19 dataset, and Figure 2 (Top) compares their F1 score. All 17 detectors achieve a high F1 score, with human-written news articles being easiest to detect. Additionally, this dataset exhibits consistent results, with human-written fake news being the easiest for all the detectors.\n\nTable 2 shows the performance of fake news detectors on human-written vs LLM-paraphrased news articles on the LIAR dataset. Figure 2 (Bottom) compares F1 scores among all the detectors. The Figure and Table both show that no source was easier or harder to detect consistently. Specifically, encoder-decoder models (e.g., BERT, T5, Llama) yield low F1 score in detecting human-written fake news articles, when compared to GPT or Llama-paraphrased fake news. Both deep learning methods (CNN and LSTM) attain a low F1 score in classifying Pegasus and GPT-paraphrased news articles. On the other hand, supervised learning models such as SVM, logistic regressions, and random forests, regardless of the features (TF-IDF, Countvectorizer, Word Embeddings), show a high F1 score in identifying GPT and Llama-paraphrased texts, which indicates their struggle in detecting human-written and Pegasus-paraphrased fake news."}, {"title": "4.2 RQ2 - Detector Efficacy", "content": "In RQ2, we determine which fake news detector is more robust in detecting fake news generated by LLM. Here, we have a mixed result: For the COVID-19 dataset, the LLM-based models are superior, but the LIAR dataset is less clear."}, {"title": "4.3 RQ3 - Paraphraser Detectability", "content": "In RQ3, we find out which LLM-paraphrased fake news was hard/easy to detect. In general, for both datasets, fake news detectors struggle with the fake news from Pegasus more than Llama, which was more of a struggle than GPT.\n\nEspecially in the COVID-19 dataset (Figure 2 (Top) and Table 1), the F1 scores of all detectors for the Pegasus-paraphrased dataset are the worst. In the LIAR dataset (Figure 2 (Bottom) and Table 2), the F1 scores for the 11 detectors (among 17) are worst for Pegasus-generated fake news."}, {"title": "4.4 RQ4 - Paraphraser BERTSCORE", "content": "Our RQ4 was to find out which paraphraser generates the highest quality paraphrases, as measured by the FBERT score [21]. In general, GPT emerges as the most reliable tool for maintaining high semantic similarity in COVID-19 and LIAR data sets. Figures 3 and 4 illustrate the semantic similarity score distributions.\n\nWe also calculated Hedge's g to measure the effect sizes on FBERT score between treatments, here using different paraphrasers. For the COVID-19 dataset, we find a small effect size between GPT and Llama (Hedge's g, 0.34), which indicates low difference in the semantic similarity between their paraphrased text outputs. In contrast, we find very large effect sizes between GPT and Pegasus (Hedge's g, 1.78) and between Llama and Pegasus (Hedge's g, 1.47), which indicates that GPT and Llama produce paraphrases with practically significantly higher semantic similarity than Pegasus. For the LIAR dataset, we find negligible effects between Llama and paraphrasers (Hedge's g, |g| < .06), but medium effect size between GPT and Llama (Hedge's g, 0.60), which substantiates our earlier observation about the superior FBERT scores that GPT paraphrases possess."}, {"title": "4.5 RQ5 - Explainability", "content": "Table 1 shows that the BERT model performs well in human-written news articles. However, its performance decreases with Llama-paraphrased news articles. To observe the reason, we selected an instance where the BERT model could accurately classify a piece of human-written fake news, but failed to classify the Llama-paraphrased version.\n\nThe original sentence (Figure 5 (Top Left))"}, {"title": "5 Discussion", "content": "by the FBERT score metric, GPT is our best-performing paraphrasing model. However, when we look at detection F1, we see a contradictory result, namely that one of the other models are better. Specifically, if we interpret a larger reduction in F1 to be \"better\" (i.e., the paraphrase is more able to conceal the true label of the text), then Pegasus is best. Meanwhile, if we interpret a smaller reduction in F1 to be \"better\u201d (i.e., the paraphrase retains as much of the labeling of the original as possible), then Llama is best. As a result, a number of open questions swirl, such as which measure we should rely upon more and why, as well as how to devise a top-down paraphrase metric that better aligns with bottom-up observations."}, {"title": "5.1 Which Quality Measures?", "content": "In RQ3 and RQ4, we introduced two different ways to measure the quality of a paraphrase. RQ4 simply appealed to the FBERT score metric, while RQ3 measured the change in detection rates between the paraphrase and the original human-written text."}, {"title": "5.2 Changing Sentiment Without Changing Semantics?", "content": "In Section 4.5 we saw evidence that a lot of data had large sentiment shifts, but high FBERT score. This seems like a rather large problem for two reasons. First, as we already mentioned, it seems to be introducing confusion into the classification problem. Second, this combination should not be possible from the metric perspective. Ultimately, FBERT score relies on some combination of semantic similarity and syntactic similarity. One interpretation of our results is that this combination may benefit from more terms, (e.g., sentimental similarity). Ultimately, our results indicate that there is room for researchers to improve some aspect of semantic similarity measurement."}, {"title": "6 Conclusion", "content": "In answering our RQs, we made five contributions. First, paraphrasing tends to decrease classification accuracy, indicating that LLM laundering can be an effective attack to evade fake news detectors. Second, we identify which models perform well at which tasks. Specifically, we found LLM-based models to be the detector most robust to LLM-paraphrased text, Pegasus to be the generator that best evades detection and GPT to be the generator that creates most semantically similar fake news paraphrases, as measured by FBERT score. Third, LIME explanations revealed a possible reason for failures, specifically sentiment shift. Fourth, we provide evidence about the prevalence of a shift in sentiment paired with a high semantic similarity. Finally, we introduce two paraphrased datasets for future researchers to build"}, {"title": "7 Limitations", "content": "Despite the insights provided by this study, it still has some limitations. This study focuses on a limited number of paraphrasing models (GPT, Llama, and Pegasus) and LLM-based detectors (Llama, T5, and BERT). Similarly, we only covered a limited set of datasets (COVID-19 and LIAR). Further, the LIAR dataset is rather strange\u2014the text instances are short and the leaderboard models all perform poorly (.27% Accuracy).\n\nWhile we focus on semantic similarity using BERTScore, we do not examine other dimensions of text quality, such as fluency or coherence. An empirical study with human evaluators might be a better evaluation technique to identify the best paraphrases.\n\nIn this paper, we proposed our own metric for semantic shift (Equation 1). While the formula is rather straightforward and makes sense, this is not a validated process.\n\nThough this study finds that a shift in sentiment and introduction of ambiguity might be the reason for the detectors to detect fake news properly, we also need a more comprehensive study to ascertain that claim."}, {"title": "8 Ethical Considerations", "content": "In this work, we enumerate a potential method to improve evading fake news detectors. As with much work in security, enumerating an attack always poses the risk that malicious actors deploy the attack. However, the hope is that the defenders' awareness of the attack counterbalances this concern, since they are able to develop mitigation strategies for that specific attack and avoid being surprised by it."}]}