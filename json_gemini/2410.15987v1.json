{"title": "Analyzing Closed-loop Training Techniques for Realistic Traffic Agent Models in Autonomous Highway Driving Simulations", "authors": ["Matthias Bitzer", "Reinis Cimurs", "Benjamin Coors", "Johannes Goth", "Sebastian Ziesche", "Philipp Geiger", "Maximilian Naumann"], "abstract": "Simulation plays a crucial role in the rapid development and safe deployment of autonomous vehicles. Realistic traffic agent models are indispensable for bridging the gap between simulation and the real world. Many existing approaches for imitating human behavior are based on learning from demonstration. However, these approaches are often constrained by focusing on individual training strategies. Therefore, to foster a broader understanding of realistic traffic agent modeling, in this paper, we provide an extensive comparative analysis of different training principles, with a focus on closed-loop methods for highway driving simulation. We experimentally compare (i) open-loop vs. closed-loop multi-agent training, (ii) adversarial vs. deterministic supervised training, (iii) the impact of reinforcement losses, and (iv) the impact of training alongside log-replayed agents to identify suitable training techniques for realistic agent modeling. Furthermore, we identify promising combinations of different closed-loop training methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Modeling the behavior of traffic participants is a crucial component in the development process of autonomous driving systems. Multi-agent driver models are utilized, for example, in simulation [1], [2] to benchmark planners or in planning systems themselves to reason about the behavior of other traffic participants [3]. However, most deployed driver models are rule-based and are not able to capture behavior outside their manually specified rules. Data-driven driver models offer an alternative that is able to learn behavior directly from real-world data.\nMany general learning methods have been proposed to learn multi-agent driving policies, including simple one-step supervised learning [4], [2], closed-loop deterministic imitation learning [5], [6], [7], adversarial imitation learning [8], [9], [10] and combinations of imitation with Reinforcement Learning (RL) [5], [11], [12], [13], [14], [15]. Most recent methods have in common that the training is executed closed-loop, meaning that the model directly executes a sequence of actions with a differentiable forward model in the loop (see Figure 1), instead of predicting the next action given a ground-truth state (which we refer to as open-loop). This enables the policy to reason about the future consequences of a given action. While many of the aforementioned works propose new training methods and/or architectures, a systematic, comparative, empirical study of high-level training principles is often missing. But such a study is important to understand the sim-to-real gap induced by different training paradigms, in particular when using them for safety testing. Our goal in this paper is to systematically compare and analyze different training paradigms for multi-agent driver models, with a particular focus on closed-loop methods in highway scenarios. The analysis includes the following dimensions:\n\u2022 Closed-loop vs. open-loop: While it is already evident that closed-loop training is beneficial, we reevaluate this claim over a larger set of training methods.\n\u2022 Deterministic supervised learning vs. probabilistic adversarial learning: Recent methods used Model-based Generative Adversarial Imitation Learning (MGAIL) [16] to train a probabilistic driver model. However, also training purely deterministically in a closed-loop supervised fashion is possible [5], [6], [7]. It is unclear how both compare.\n\u2022 Pure Imitation vs. additional reinforcement learning signal: There is some evidence that training a policy with imitation combined with reinforcement learning is beneficial [15]. Thus, we include a reinforcement signal in our analysis of closed-loop trainings.\n\u2022 Log-replay vs. multi-agent training: Most methods either propose a single-agent training alongside log-replayed agents or a multi-agent learning scheme. The comparison of the two schemes is infrequent.\nEach training method has its theoretical benefits and shortcomings. Policies trained via deterministic supervised training might lack diverse behavior, but can be trained in a stable way. Adversarial training might, in theory, be able to match state distributions perfectly but are hard to train and it is unclear if the discriminator matches the distribution on features that are actually relevant for the driving task. Furthermore, one can enforce driving properties that are important for the driving task, such as collision avoidance, via a reinforcement learning signal but it is unclear how much this impacts other realism properties.\nTo execute our study, we propose an intuitive multi-agent policy parameterization that can be employed in all training methods with minimal adjustments. The architecture is based on a multi-agent Graph Neural Net (GNN) encoder that"}, {"title": "II. RELATED WORK", "content": "Rule-based driver models. The driver modeling task is often solved by employing rule-based methods for creating a decision-making policy. Generally, rule-based driver models express the driving task as a set of parameterized functions, such as the Intelligent Driver Model (IDM) [18] and its extensions [19], [20], [21]. Their popularity stems from the simple implementation and parameterization based on ego agent's velocity, distance to other vehicles, and the velocity difference. To obtain the best performing parameters for this method, data driven approaches have been used in [22], [23], [24]. Other works [25], [26], [27], [28], [29] parameterize car following models based on surrounding vehicle features. While rule-based models are highly interpretable and computationally efficient, they lack expressiveness w.r.t. realism of the driver behavior and suffer from poor generalization [30], [31]. To address these issues, different learning-based methods have come to the forefront in autonomous driving research.\nDriver modeling with Reinforcement Learning. In RL implementations [32], [33], [34] the driving policy is trained to maximize a pre-specified reward obtained through the interaction with the environment. Typically, the environment dynamics are non-differentiable or even unknown. Designing a reward function that captures realistic motion behavior is extremely difficult due to the subtle intricacies of human decision making. Assuming that the reward is a function purely defined by the state and is dynamics invariant [35], its function can be inferred through Inverse Reinforcement Learning (IRL) [36]. Yet, IRL is expensive to run and difficult to scale [37].\nDriver Modeling with Imitation Learning. Imitation Learning (IL) is used to train a policy solely from expert demonstrations. Classically, Behavioral Cloning (BC) can be used in an open-loop setting to obtain a trained policy directly from ground truth demonstrations [4], [2], [38]. However, these methods suffer from distribution shift in long tail rollouts [39]. This can be alleviated with applying augmentations [2] or training in closed-loop where the training is performed directly in a sequential decision-making manner. Here, differential simulation accumulates the loss over multiple steps [1], [6], [11], [10], [7]. This requires either augmenting the input data if working with rasterized representations of environment [11] or using vector representations [40] to have fully differentiable features. However, pure IL can be insufficient to learn safe and reliable policies due to a rareness of critical scenarios in the ground truth data [15]. Therefore, loss in IL is augmented with RL rewards or common sense losses. In [5], [11], [12], [13], [14] a level of infraction is used to penalize the collisions and road departures in the loss.\nDriver Modeling with Generative Adversarial Learning. Adversarial learning is an alternative approach for imitating expert behavior. Here, the loss function is replaced with a learned discriminator, and the driving policy is trained to fool the discriminator. Foundational work that combined adversarial and policy learning was done with Generative Adversarial IL (GAIL) [8], [9], [41], [16]. These methods typically need an interplay between discriminator optimization and solving the RL problem. Importantly, when the"}, {"title": "III. PROBLEM FORMULATION AND BASE POLICY", "content": "In this section, we phrase our problem formulation and introduce our base policy parameterization. In Section IV we outline the different training paradigms for analysis.\nProblem Formulation. Our driver modeling goal is to learn a multi-agent policy $\\pi_\\theta(a_t|s_t)$, where $a_t = (a_{1,t},..., a_{M,t})$ denotes the actions of M traffic participants at time step t and $s_t$ denotes the states of all agents, which contain information like the position, speed etc., and the local map, at time step t. Given a set of ground truth trajectories $D = \\{T_i\\}_{i=1}^{N}$ with $T_i = \\{(s_0^{(i)}, a_0^{(i)},...,s_{T_i}^{(i)}, a_{T_i}^{(i)})\\}$, the goal is to learn the parameters $\\theta$ of the policy such that it induces a distribution over trajectories $p(T) = p(s_0) \\prod_{t=0}^{T} P(s_{t+1} | a_t, s_t)\\pi_\\theta(a_t|s_t)$ that resembles the distribution of trajectories in D. Here, we assume a known, deterministic and differentiable transition model $P(s_{t+1} | a_t, s_t)$, which is a common assumption in driver modeling [5], [7]. In particular, we use position delta actions for each agent (detailed in differential update step). The main challenge of driver modeling consists of modeling the underlying policy $\\pi_\\theta$ and choosing the learning method to fit the generated trajectories to the ground truth trajectories. In the following paragraphs we propose our policy parameterization, that induces an intuitive inductive bias for multi-agent driving.\nPolicy architecture. Our multi-agent policy $\\pi_\\theta(a_t|s_t)$ follows an encoder-decoder architecture. The encoder takes in the multi-agent state $s_t$ and returns an encoding for each agent:\n$h_t := [h_{1,t},...,h_{M,t}] = Encoder(s_t)$ (1)\nFor multi-agent scenarios it is crucial that the Encoder can deal with different number of agents in the scene. Here, it is natural to define a graph over agents and use a GNN. The agents are the nodes in a locally connected graph. For a given multi-agent state $s_t$ we extract initial node features $n^{(0)} = [n_{1,t}^{(0)},...,n_{M,t}^{(0)}]$ and initial edge features $e^{(0)} = [e_{ij}^{(0)}]_{i\\neq j}$ and process them via repeating message-passing layers with index k = 0, . . ., K \u2212 1\n$n^{(k+1)}, e^{(k+1)} = MessagePassing(n^{(k)},e^{(k)})$ (2)\nThe output of the encoder is given via $h_t = n^{(K)}$. We elaborate on the details of the initial node and edge features as well as the message passing algorithm in the paragraphs below.\nDepending on the training method (see Section IV) the decoder of the policy either maps the agent-wise encoding directly to the 2D action space via a weight-shared MLP,\n$a_t = [MLP(h_{1,t}),..., MLP(h_{M,t})] = Decoder(h_t)$. (3)\nor each MLP maps to the parameters $\\phi_i$ of a 2D distribution like a Gaussian or Gaussian Mixture (GMM) which defines an agent-specific distribution over our action space.\nNode features. The initial node features of the GNN contain information about the agents' poses and kinematics, the agents' local map, and further information about the agent, like the agent type and its dimensions. First, we embed poses, kinematics, agent type and dimensions into a single embedding vector $h_{i,t}$ via an MLP. The map is represented as a sequence of line strings in the local coordinate system of the agent. Those line strings are associated with the boundary lines of each lane segment. The line strings are embedded with Multi-Head Attention (MHA) resulting in a sequence of line embeddings $[l_{i,j}]_{j=1}^{L_i}$. The initial embedding to the GNN is computed by fusing map information via cross attention (with residual) from $h_{i,t}$ to $[l_{i,j}]_{j=1}^{L_i}$. All features are computed in the local reference frame of each agent and are thus rotation and translation invariant.\nEdge features. The edge features capture relevant properties for interaction between source and target agents. The edge features therefore consist of the distance and velocity difference between source and target agents, relative position history between source and target agent for t-2: t in target agents' coordinate frame, heading difference\u00b9, and the time to collision between the source and target agent, clipped at a maximum value of 10 s. All these features are invariant to rotations and translations of the agent pair.\nMessage-Passing. Intuitively, our GNN has the inductive bias that the agents are the nodes in the graph, their interaction is modeled via edges, and the reasoning over others and the resulting behavior is computed with the message-passing steps, as principally proposed by [45]. Concretely, our message passing module employs an edge model that updates the edge features via $e_{ij}^{(k+1)} = MLP([h_i^{(k)}, h_j^{(k)}, e_{ij}^{(k)}])$ and uses cross-attention (with residual) between $h_i^{(k)}, h_j^{(k)}$ and $[e_{ij}^{(k+1)}]_{i=1}^{M_i}$ to get $h_i^{(k+1)}$. In this way, each target agent can focus on the relevant agents that might interact with it.\nDifferentiable update step. A crucial component to enabling closed-loop training is a differentiable forward step, that enables propagating gradients through the steps of the trajectory. As actions $a_{i,t}$, we use the position deltas in the local reference frame of each agent, i.e., for i's local 2D position at time t+1, we have $(x_{i,t+1}, Y_{i,t+1}) = (X_{i,t}, Y_{i,t})+ a_{i,t}$. After transforming this to the global reference frame, the position and the heading of each agent are updated (see Figure 1). The new multi-agent state $s_{t+1}$ is calculated via differentiable transformations of all features given the new locations and headings. Crucially, this enables MGAIL and differentiable simulation training to propagate future error through time to earlier actions. We investigate the impact of different variants of closed-loop training in our experiments."}, {"title": "IV. COMPARED TRAINING APPROACHES", "content": "We introduce the different training paradigms we analyse for learning realistic driver models.\nBehavioral Cloning. The first intuitive training method is supervised one-step imitation learning, also called behavioral cloning. Here, we minimize an imitation loss over the one-step state-action distribution. For example, one might minimize the expected negative log likelihood of the policy under the one-step data distribution, $L_{BC}(\\theta) = E_{s,a~D}[-log \\pi_\\theta(a|s)]$. While it appears to be an intuitive principle to train a policy, it has been shown repeatedly [39], [46] that it can easily lead to compounding errors and unrealistic distributions over (multi-step) trajectories $p(\\tau)$. However, we use this method for comparison purposes, as pretraining (see Section V) and as regularizer (see MGAIL).\nDifferentiable Simulation. Since our forward model is differentiable, we can alleviate the compounding error problem via training $\\pi_\\theta$ through differentiable simulation, aka propagating gradients through time. Here, we consider the policy to be a deterministic mapping from states to actions at \u2192 $\\pi_\\theta(s_t)$ rather than a probability distribution. Given some initial state $s_0$ and a generated trajectory $\\tilde \\tau = (s_0, \\tilde a_0, s_1, ..., s_T)$, we use as loss $L_{Ds}(\\theta) = E_{s_0~D} \\sum_{t=1}^T d(s_t, \\tilde s_t) | \\theta]$, where $d(s_t, \\tilde s_t)$ is a weighted mean squared error (MSE) loss between the (x, y) positions in the states averaged over all agents. When training via differentiable simulation we pretrain the weights with BC, where we also replace the log likelihood loss with the weighted MSE loss.\nMGAIL. Recent methods [8], [9], [16] utilized generative adversarial networks to learn driver models. Here, we train a discriminator $D_\\Psi$ in addition to the policy $\\pi_\\theta$. The discriminator is trained to classify states into the ones that come from ground truth and the ones that are generated via the $\\pi_\\theta$. It maps from states to probability scores for each agent, thus $D_\\Psi(s) \\in [0,1]^M$, and is trained via the cross entropy loss $L(\\Psi) = E_{s~D}[-log D_\\Psi(s)] + E_{s~\\pi_\\theta} [-log (1 - D_\\Psi(s))]$ (here the expectation includes an additional averaging along the time dimension, over all states s of the individual trajectories). We parameterize our discriminator in the same way as the policy, except that the MLP in the decoder maps to [0, 1] instead of the 2D action space.\nThe policy/generator is trained to fool the discriminator and thus minimizes the loss $L_{MGAIL}(\\theta) = E_{s~\\pi_\\theta} [log (1 - D_\\Psi(s))]$. It is important to note that gradients in this loss, can also propagate to previous time points, because of the differentiable forward model. Here, the decoder of the policy maps to the parameters of a proper probability distribution and is either parameterized via a Gaussian or Gaussian mixture distribution. The loss can be approximated via sampling with the reparameterization trick.\nDifferentiable Collision Loss. Combining data-based losses with reinforcement learning (RL) losses has been shown to be beneficial for learning planner policies [15]. Also, for driver modeling, enforcing certain aspects, like preventing collisions, is crucial. However, directly applying"}, {"title": "V. EXPERIMENTS & RESULTS", "content": "In the following, we present our ablation study on the different training methods for modeling highway traffic agents. First, we introduce our experimental setup and show the results. In Section VI, we summarize the high-level findings of our experiments.\nDataset. We evaluate on the exiD dataset [17], a real-world trajectory dataset that contains drone-recorded driving data from highway entries and exits in Germany. We extract training and evaluation data by cutting the exiD recordings into snippets of 10 second length, which we downsample to a frequency of 2 Hz. In total, our dataset consists of 5750 recording snippets, which we refer to as rollouts, where all rollouts of one recording are assigned to one split. The dataset is organized into a train, validation and test split with 4461, 737 and 552 rollouts, respectively. Furthermore, we ensure that each split contains rollouts from each of the seven exiD recording locations.\nSimulation setup. We simulate for the full 10 seconds of our rollouts at 2 Hz. In order to enable the computation of the node features, which include the agent's speed and acceleration, we only start controlling an agent after 3 steps of it being present in the rollout. This means that an agent effectively performs three initial log-replay steps before it is controlled by the model.\nEvaluated Methods. We compare the methods presented in Section IV along with their combinations. Concretely, for open-loop training, we evaluate BC training with maximum likelihood (BC-LL) using a Gaussian head (BC Gaussian-LL) as well as a Gaussian mixture head (BC GMM-LL). We consider training BC with weighted MSE combined with orientation loss (BC WMSE + Orientation). Here, BC WMSE refers to an MSE loss where different weights are applied for x and y axis deviations, since lateral motion is predominantly smaller in highway scenarios. Orientation is a loss expressed as:\n$d_{orientation} (\\hat a, a) := \\frac{1}{M}\\sum_{i=1}^{M}d_{MSE}((\\delta \\hat x_{a,i}, \\delta \\hat y_{a,i}), (\\delta x_{a,i}, \\delta y_{a,i}))$, (4)"}, {"title": "VI. ANALYSIS OF RESULTS", "content": "We analyze the experimental results and draw high-level conclusions for driver model training:\nClosed-loop can be beneficial over open-loop training: Theoretical and experimental findings have been established in the past regarding the benefit of closed-loop multi-agent training over simpler, more open-loop approaches, e.g., the \u201ccompounding error\u201d phenomenon of BC training [46], [47], [39]. Nonetheless, open-loop approaches are still often used due to their simplicity [38], [10]. Our experimental findings confirm the case made for closed-loop training. From Table I, we see that the two closed-loop paradigms - differentiable simulation and MGAIL - significantly outperform open-loop BC methods. This is evident when comparing DiffSim to BC WMSE + Orientation as well as MGAIL + BC-LL to the respective BC-LL method with the same head. In all cases, the closed-loop method is better or equal on (almost) every metric. Additionally, single-agent closed-loop training with log replay of surrounding agents (Table II) can be seen as \"less closed-loop\" than full multi-agent closed-loop training (Table I). Also, here, the former outperforms the latter. Furthermore, we can see that the closed loop methods lead to more realistic scenarios in terms of JSD metrics and and almost always to lower collision rates than an IDM model.\nReinforcement loss can harm realism: Unrealistically high collision rates remain one of the biggest open challenges in learned driver models, and are a key indicator of where realism is still limited [16], [10]. As a natural remedy, various works [1], [48] have added a \u201ccommon sense loss\u201d, also called \u201creinforcement loss\u201d. Our experiments show that such a reinforcement learning aspect can indeed significantly bring down the collision rate (e.g., compare collision rate metric between DiffSim wMSE and"}, {"title": "VII. CONCLUSION", "content": "To summarize, we conducted a systematic analysis study of closed-loop imitation training principles for realistic traffic agent models for highway scenarios. We utilized the same GNN-based driving policy under different training paradigms, and reported quantitative experimental results as well as high-level insights with qualitative results given in the supplementary material. We find that each method on its own comes with individual weaknesses, and combining different methods can counteract them. In particular, we find that closed-loop training has significant advantages over open-loop training, that a reinforcement signal can destroy realism and that combinations of different closed-loop learning principles can improve overall performance."}, {"title": "A. Architecture overview", "content": "In Section III, we gave a formal introduction to our policy parameterization. In the following, we add an intuitive description of our architecture along with a visualization that can be seen in Figure 2.\nTo compute the action for each agent in a given scene, we employ a Scene Encoder, followed by a GNN, followed by a Behavior Decoder. As can be seen in Figure 2, the input to the network is a scene at a certain point in time. It contains a map, which consists of lane graph information and lane boundaries and it contains features for each agent like the position, bounding box, orientation, velocity, acceleration. Based on these features we determine, for each pair of agents, whether one is relevant for the action of the other. If that is the case, we introduce a directed edge between them in our GNN.\nBased on this input, the Scene Encoder computes node and edge features (like the relative position or velocity of two agents) and embeds them in latent space. Moreover, for each agent, the lines in the relevant part of the map are extracted, transformed into ego coordinates and embedded into latent space. After that, a cross attention module (which also takes the node embedding into account) computes a map embedding from the line embeddings (see Figure 3 for details). The map embedding is added to the node embedding as the last step of the Scene Encoder. Hence, the output of the Scene Encoder is an embedding for each agent and an embedding for each directed edge between two agents.\nThese embeddings enter the GNN that computes one update of the edge embedding by taking into account the node embeddings of the source and the target node. Afterwards, each node is updated by taking into account all embeddings of incoming edges. This yields a final embedding for each agent that is passed to the Behavior Decoder. This is an MLP that generates the action in the desired output format (e.g., means and covariances of the Gaussians or next waypoints).\n1) Polyline representation.: To embed the map information, we use a similar method as in [40]. Road lines are extracted from Lanelet2 [50] map representation and split into segments of maximum length of 20 meters. Each line segment is represented as a polyline of 10 points. Segments are selected that fall into a crop around each agent in each respective agent's frame of reference with at least one point. The crop size is 10 meters to left and to right, 120 meters in front and 45 meters in the rear of the vehicle. Selected polylines are combined with a sinusoidal embedding and the line type. We use two line types - solid and dashed. Each polyline is passed through three PointNet [40] layers. Embedded polylines are used as key and value arguments in Multi-Head Attention (MHA) message passing module (as implemented in [51]) where query is the relevant agent embedding. Aggregated attended polyline embeddings are then combined with the agent embedding to obtain the agent representation. Map topology embedding is depicted in Figure 3."}, {"title": "B. Method Details + Hyperparameters", "content": "As default optimizer, we use Adam with learning rate lrate and a StepLR learning rate scheduler with factor \u03b3 and step-size nstep. These three parameters were tuned independently for combined and log-replay trainings. Here, we only report them for the combined trainings - all other hyperparameters"}, {"title": "C. Further Evaluations", "content": "1) Log-replay evaluation: In Table I and Table II, we report results on different agent control methods when evaluated on controlling all agents in the scene. Here, we report evaluation on an inverse task of controlling only one agent in the scene, i.e. a learned agent policy executed alongside log-replay agents. The single agent that is controlled in the evaluation is deterministically selected as the agent that is present with the most time steps in the scene. We train the"}]}