{"title": "SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training", "authors": ["Jiaxing Yu", "Xinda Wu", "Yunfei Xu", "Tieyao Zhang", "Songruoyao Wu", "Le Ma", "Kejun Zhang"], "abstract": "Lyric-to-melody generation aims to automatically create melodies based on given lyrics, requiring the capture of complex and subtle correlations between them. However, previous works usually suffer from two main challenges: 1) lyric-melody alignment modeling, which is often simplified to one-syllable/word-to-one-note alignment, while others have the problem of low alignment accuracy; 2) lyric-melody harmony modeling, which usually relies heavily on intermediates or strict rules, limiting model's capabilities and generative diversity. In this paper, we propose SongGLM, a lyric-to-melody generation system that leverages 2D alignment encoding and multi-task pre-training based on the General Language Model (GLM) to guarantee the alignment and harmony between lyrics and melodies. Specifically, 1) we introduce a unified symbolic song representation for lyrics and melodies with word-level and phrase-level (2D) alignment encoding to capture the lyric-melody alignment; 2) we design a multi-task pre-training framework with hierarchical blank infilling objectives (n-gram, phrase, and long span), and incorporate lyric-melody relationships into the extraction of harmonized n-grams to ensure the lyric-melody harmony. We also construct a large-scale lyric-melody paired dataset comprising over 200,000 English song pieces for pre-training and fine-tuning. The objective and subjective results indicate that SongGLM can generate melodies from lyrics with significant improvements in both alignment and harmony, outperforming all the previous baseline methods.", "sections": [{"title": "1 Introduction", "content": "Lyric-to-melody generation, which aims to automatically generate melodies from given lyrics, has attracted lots of attention from both academia and industry. When creating melodies, capturing the complex and subtle lyric-melody correlations is crucial. Previous works (Watanabe et al. 2018; Bao et al. 2019; Yu, Srivastava, and Canales 2021; Ju et al. 2022; Sheng et al. 2021; Lv et al. 2022; Zhang et al. 2022; Ding et al. 2024; Wang et al. 2024; Yu et al. 2024) in this field have achieved great progress in capturing these correlations, but still encounter two primary challenges: lyric-melody alignment modeling and harmony modeling.\n1) Lyric-melody alignment modeling. Lyric-melody alignment denotes the quantitative relationships between syllables/words and notes, particularly the number of notes mapped to a single syllable or word, which has a significant impact on the richness and singability of a song. Since most existing works (Yu, Srivastava, and Canales 2021; Ju et al. 2022; Lv et al. 2022; Zhang et al. 2022) only explore the one-syllable/word-to-one-note (one-to-one) alignment, few works consider the one-syllable/word-to-multiple-notes (one-to-multiple) alignment. Bao et al. (2019) predicted the number of notes corresponding to the given syllable with a greedy alignment strategy. Sheng et al. (2021) utilized sentence-level and token-level attention masks to achieve alignment between word/syllable and note. However, these methods usually suffer from low alignment accuracy, attributable to their indirect ways of learning the lyric-melody alignment. Consequently, it is essential to introduce a unified representation for lyrics and melodies that directly captures the lyric-melody alignment.\n2) Lyric-melody harmony modeling. Lyric-melody harmony refers to the qualitative relationships between syllables/words and notes, emphasizing their feature coherence, which is crucial for the rhythmic and structural consistency of a song. Ju et al. (2022) proposed templates which consist of tonality, chord, rhythm, and cadence, serving as a bridge between lyrics and melodies to improve the harmony. Lv et al. (2022) extracted key features from lyrics, including tonality, rhythm, chord, and structure, and leveraged these features as the query to retrieve and concatenate pre-generated melody segments. Zhang et al. (2022) introduced several rules about lyric-melody relationships from the perspectives of tone, rhythm, and structure, and integrated them into the decoding step of lyric-to-melody generation models. These approaches rely heavily on either intermediates (tem-"}, {"title": "2 Background", "content": "Over the past few years, there have been advancements in deep learning approaches for lyric-to-melody generation. Bao et al. (2019), Lee, Fang, and Ma (2019), and Yu, Srivastava, and Canales (2021) adopted end-to-end models to generate melodies from lyrics. These methods cannot fully capture the relationships between lyrics and melodies due to the limited availability of paired lyric-melody dataset. To address this issue, Ju et al. (2022) divided the generation process into two stages, lyric-to-template and template-to-melody, to leverage unpaired data. Lv et al. (2022) proposed a generation-retrieval pipeline by sharing same key features between lyrics and melodies. Sheng et al. (2021) trained lyrics generation and melody generation models separately with unpaired data and performed attention based alignment modeling. Zhang et al. (2022) developed an expert system on Sheng et al. (2021) and Ju et al. (2022) that incorporated lyric-melody relationships from the music theory to improve lyric-melody harmony. However, the aforementioned studies are inadequate in effectively handling the complex and subtle correlations between lyrics and melodies, particularly failing to address both alignment modeling and harmony modeling concurrently. In this paper, we propose SongGLM, a novel lyric-to-melody generation system with 2D alignment encoding and multi-task pre-training to tackle these challenges.\nPre-training frameworks have made significant contributions to the development of automatic music composition. Early encoder-only frameworks, like BERT (Devlin et al. 2019), adopted multi-layer bidirectional Transformer encoders to learn deep bidirectional representations, showing strong capabilities in music understanding tasks (Wang and Xia 2021; Zeng et al. 2021). MuseBERT (Wang and Xia 2021) leveraged BERT with a specific representation that merges musical attributes and relations for better music understanding. MusicBERT (Zeng et al. 2021), pre-training BERT with OctupleMIDI encoding and a tailored bar-level masking strategy, demonstrated strong performance across four music understanding tasks. Later decoder-only frameworks, like GPT (Radford et al. 2018), leveraged multi-layer unidirectional Transformer decoders to capture rich context information, which are suited for music generation tasks (Ens and Pasquier 2020). MMM (Ens and Pasquier 2020) was trained based on GPT-2 (Radford et al. 2019) by concatenating multiple tracks into a single sequence for conditional multi-track music generation. MuseNet\u00b9 was also built upon GPT-2 that can generate 4-minute music with different instruments and various styles. Encoder-decoder frameworks, like MASS (Song et al. 2019), integrated encoder and decoder modules for better understanding and generating music sequences (Sheng et al. 2021). SongMASS (Sheng et al. 2021) utilized the MASS pre-training method and attention-based alignment modeling for automatic song writing. Recently,"}, {"title": "3 Method", "content": "An overview of SongGLM is shown in Figure 2. Given the paired lyric-melody dataset, we first establish two relationships between lyrics and melodies based on their representative features, and incorporate these relationships into n-gram extraction to select the most harmonized n-grams. Then, we introduce a unified symbolic song representation with 2D alignment encoding and adopt a multi-task pre-training framework that employs hierarchical blank infilling objectives for lyric-to-melody generation. In the following subsections, we describe the details of harmonized n-gram extraction and lyric-to-melody generation."}, {"title": "3.2 Harmonized N-gram Extraction", "content": "N-grams are widely used in Music Information Retrieval for understanding and generating tasks (Zheng, Moh, and Moh 2017; Xiao et al. 2021; Wu et al. 2023). However, existing n-gram extraction methods (Zeng et al. 2021; Wu et al. 2023) often struggle to effectively capture the harmony between lyrics and melodies when applied to lyric-to-melody generation tasks. To address this challenge, we first introduce the three most representative features of lyrics and melodies, and establish the qualitative relationships between them based on these features. Then, we propose a novel n-gram extraction strategy that incorporates these relationships to select the most harmonized n-grams.\nWe describe three key features of lyrics and melodies and the corresponding extraction functions (fw or fn) across the dimension of word, pitch and time, specifically: syllable stress, melodic peak, and rhythm skeleton.\nSyllable stress (Ladefoged, Draper, and Whitteridge 1958) refers to the emphasis placed on particular syllables within words. The sequence of syllable stress represents the rhythmic pattern of lyrics, and plays a crucial role in lyric-to-melody generation. According to the CMU Pronouncing Dictionary 2, each syllable stress can be categorized into three levels on an ordinal scale: Unstressed, Primary Stress, and Secondary Stress. Table 1 presents several examples of syllable stress within words from the CMU Pronouncing Dictionary. For the \"Syllable Stress\" feature, we define fw as follows:\n$f_w(W) = [s_1,..., s_y]$                                                                                 (1)\nwhere $s_i \u2208 {0,1,2}$ represents the stress level of the ith syllable in a word, with 0 indicating Unstressed, 1 indicating Primary Stress, and 2 indicating Secondary Stress.\nMelodic peaks (Eitan 2016) refer to the notes with a higher pitch compared to the preceding and subsequent notes. The sequence of melodic peaks describes the movement pattern of the melody among high pitches. Figure 3 illustrates examples of melodic peaks (green blocks) in the song. For the \"Melodic Peak\" feature, given the pitch sequence P [P1,..., Pn] of the melody, we define $f_{n.MP}$ for the ith note as follows:\n$f_{n.MP}(N) =\n\\begin{cases}\n1, & 1<i<n  \\text{ and } p^i > p^{i-1} \\text{ and } p^i > p^{i+1} \\\\\n0, & \\text{otherwise}\n\\end{cases}$\n(2)"}, {"title": "Relationships", "content": "Since stressed syllables are often associated with musically accented notes (Nichols et al. 2009), we reveal the mechanism of interaction between lyric and melodic features and introduce two relationships.\nComposers usually employ melodic peaks to emphasize certain syllables in songwriting. Specifically, as shown in Figure 5(a), a high level of syllable stress tends to occur with the melodic peak.\nRhythm skeleton is another method by which composers highlight specific words in a song. For instance, as illustrated in Figure 5(a), syllables with high stress levels are often associated with the note in the rhythm skeleton.\nThe correspondence between syllable stress and melodic accents can significantly improve the harmony between lyrics and melodies. On the other hand, if a melodic accent mismatches with syllable stress (red block in Figure 5(b)), it may disrupt the natural flow of the song, potentially resulting in disharmony that can detract from the overall listening experience.\nTo capture the above relationships and ensure harmony between lyrics and melodies, we propose a novel n-gram extraction strategy. This strategy involves calculating a composite score for each n-gram, which includes both a melodic score and a lyric-melody relationship score. N-grams with high composite scores are selected as harmonized n-grams. The details of this strategy are outlined below.\nGiven a word sequence W of lyrics, a note sequence N of melody, and paired feature extraction functions fw and fn ($f_{n.MP}$ or $f_{n.RS}$) from lyric-melody relationships, we denote each word-note feature pair as a joint uni-gram {(fw(w), fn(Nw)) | w \u2208 W,Nw \u2286 N}, where Nw represents the set of notes corresponding to the single word w. Subsequently, we extract joint n-grams for n ranging from 2 to 12, comprising lyric n-grams fw(Wn) with n words and melodic n-grams fn(Nwn). Furthermore, we compute t-statistic scores st (Xiao et al. 2021) for the lyric and melodic n-grams separately, si and sm. Each melodic n-gram fn(Nwn) is associated with a set of m distinct lyric n-grams $F_w(W_m)$ = {$f_w(W),..., f_w(W_m)$} from different joint n-grams. Finally, the score s of a joint n-gram {(fw(Wn), fn(Nwn)) | Wn \u2286 W, Nwn\u2286 N} consists of two parts (the melodic n-gram t-statistic score sm and the lyric-melody relationship score sim), which is defined as:\ns = Sm + Slm\n(4)\ns1 = St(fw(Wn))\n(5)\nSm = St(fn(Nwn))\n(6)\n$\\bar{S_{lm}} = C(F_w(W_m)) \\cdot \\sum_{i=1}^{m} S_i$\n(7)\n$C(F_w(W_m) = \n\\begin{cases}\n1, & m = 1 \\\\\n(1 - H'(F_w(W_m))), & m > 1\n\\end{cases}$\n(8)\n$H'(F_w(W_m)) = -\\frac{1}{log m} \\sum_{i=1}^{m} p(W_i)log p(W_i)$\n(9)\nwhere p represents the occurrence probability of a given lyric n-gram among all corresponded lyric n-grams to the melodic n-gram, C represents the concentration of the lyric n-gram set associated with the melodic n-gram, derived from the normalized entropy H'. The higher the concentration C, the better the joint n-gram represents a significant and repeating pattern in the lyric-melody relationship, thereby more effectively influencing the harmony between lyrics and melodies. Based on their scores, we select the top 25% of joint n-grams as harmonized n-grams to construct the final n-gram lexicon for word-level sampling in lyric-to-melody generation."}, {"title": "3.3 Lyric-to-Melody Generation", "content": "On top of the above extracted harmonized n-grams, we build SongGLM upon GLM (Du et al. 2022) with a single Transformer-based encoder-decoder framework for lyric-to-melody generation, as shown in Figure 6. In the pre-training stage, we adopt a multi-task pre-training framework with hierarchical blank infilling objectives. In the fine-tuning and inference stage, we utilize causal language modeling to predict the next note sequentially from left to right.\nInspired by OctupleMIDI (Zeng et al. 2021), We design a unified symbolic song representation for lyric-to-melody generation that allows the model to learn the lyric-melody alignment in an efficient and direct way. It consists of three different types of tokens: Word, Note and Special, each containing three sets of attributes: content-related, alignment-related, and generic. We list all the attributes in Table 2. For each token, we consolidate the attributes into a single compound token to reduce the sequence length.\nFor Word and Note tokens, we assign the same alignment-related and generic attributes but different content-related attributes. Specifically, alignment-related attributes include two alignment ids, as shown in Figure 6(a). The first alignment id represents word-level alignment, called Word ID. For each Word token, it denotes the position in the word sequence, starting from 0 to the total length - 1. For each Note token, it equals to the Word ID of the word corresponding to the note. The second alignment id represents phrase-level alignment, named Phrase ID. For both tokens, the Phrase IDs refer to the musical phrase to which they belong. The above two alignment ids are encoded into embedding vectors, serving as 2D alignment encoding to guarantee the hierarchical alignments between words and notes. Generic attributes include token types, which enhance the model's capacity to differentiate between Word and Note. And for content-related attributes, Note tokens comprise five musical elements: bar, position, pitch, duration, and tempo, while Word tokens contain the text of the word. We only select words that are included in the CMU Pronouncing Dictionary and sorted them according to their frequency of occurrence in the lyrics. To facilitate computational modeling, we set content-related attributes of the Word token to None in the\nLE \u2190 {}\nfunction LYRICSBASEDRECOGNITION(W)\nfor i 0 to m do\nif Wi contains punctuation marks then\nLE.append(Wi)\nend if\nend for\nreturn LE\nend function\nfunction MELODYBASEDRECOGNITION(N)\nR\u2190 RestNotes(N)\nMELU R\ni\u2190 1\nwhile i < n do\nif MEi-1 and ME\u2081 are adjacent in N then\nD\u2190 |MEi-1.duration \u2013 ME\u017c.duration|\nif D > 240 then\nremove ME\nelse\nremove MEi-1\nend if\nend if\ni\u2190 i+1\nend while\nreturn ME\nend function\nLR \u2190 LYRICSBASEDRECOGNITION(W)\nMR MELODYBASEDRECOGNITION(N)\nPR\u2190 len(LR)  \u25b7 calculate the punctuation mark ratio\nlen(W)\nif PR < 0.1 then\nreturn MR\nelse\nreturn LR\nend if"}, {"title": "Note token, and vice versa.", "content": "For Special tokens, we adopt five special delimiter symbols: <BOS>, <EOS>, <MASK>, <PAD>, and <SEP>. Similar to the Word and Note tokens, every Special token contains all attributes, each bearing the same value as itself.\nMulti-task pre-training has been shown to enhance model's performance in a variety of tasks (Sun et al. 2021; Wu et al. 2023). Meanwhile, autoregressive blank infilling is an effective pre-training approach for language models (Du et al. 2022; Wu et al. 2023). Following their success, we implement a multi-task pre-training framework with hierarchical autoregressive blank infilling objectives in SongGLM.\nGiven an input sequence S = [W1,...,Wm, N1, ..., Nn], multiple token spans s = {$1,...,sk} are sampled from the note sequence N. Each span si corresponds to a series of consecutive tokens [Si,1,..., Si,l\u2081] in N, and is replaced with a single special token <MASK>, forming a corrupted token sequence Scorrupt. The model is trained to predict the missing tokens within the spans from the corrupted token sequence in an autoregressive way, with access to the corrupted token sequence and previously predicted spans. Formally, the generation probability of the ith masked span is defined as:\n$\\begin{aligned}Po(S_i|S_{corrupt}, S\\_{<i}) &= \\prod_{j=1}^{l_i}Po(S_{i,j}|S_{corrupt}, S\\_{<i}, S\\_{i,<j})\n\\end{aligned}$\n(10)\nAnd an autoregressive blank infilling objective is performed by minimizing the negative likelihood (loss) as follows:\n$\\begin{aligned}-logp_o(s|S_{corrupt}) &= - \\sum_{S_i \\in S} \\sum_{S_{i,j} \\in S_i} log p_o(S_{i,j}|S_{corrupt})\n\\end{aligned}$\n(11)\nWe construct three hierarchical autoregressive blank infilling objectives for pre-training to capture the multi-scale, multi-dimensional harmony between lyrics and melodies.\nBased on the extracted n-gram lexicon, we randomly sample two types of harmonized n-grams from the note sequence with the Maximum Matching Algorithm (Xiao et al. 2021). The total length of sampled n-grams constitutes 15% of the note sequence. We replace each sampled n-gram with 1) the <MASK> token 80% of the time, 2) a random n-gram 10% of the time, and 3) the original n-gram 10% of the time. These objectives aims to capture word-note level harmony between lyrics and melodies.\nMultiple musical phrases are sampled from the note sequence, with the total length accounting for 50% of the original note sequence length. We consider both lyric and melodic information for musical phrase boundary recognition. The detailed detection algorithm is shown in Algorithm 1. This objective aims to capture lyric-phrase level harmony between lyrics and melodies, as well as to ensure the coherence of melodic contexts.\nWe sample a single long span that covers 50% of the original note tokens. This objective aims to improve"}, {"title": "the overall harmony between lyrics and melodies, and enhance the model's ability of melodic structure modeling.", "content": "The loss of our proposed multi-task pre-training objectives is defined as:\n$\\begin{aligned}\nL = L_{SMR} + L_{SRR} + L_{Phrase} + L_{Song}\n\\end{aligned}$\n(12)\nIn the pre-training stage, the input sequence S contains three parts: Part A is the word sequence W, Part B is the corrupted note sequence, and Part C consists of the masked spans with each separated by a <SEP> token. Tokens in Part A & Part B form the corrupted sequence Scorrupt, and can attend to each other. Part C tokens can only attend to preceding tokens, and tokens in Part A & Part B. Figure 6(b) illustrates how attention weight is modified through the self-attention mask to control the token's attention. Formally, WA and M are described as:\n$W_A = softmax(\\frac{QK^T}{\\sqrt{d_k}} + M)$\n(13)\n$M_{ij} = \n\\begin{cases}\n0, & \\text{ allow to attend}\\\\ \n-\\infty, & \\text{ prevent from attending}\n\\end{cases}$\n(14)\nWith this mechanism, our unified model effectively learns a bidirectional encoder for Part A & Part B, and a unidirectional decoder for Part C.\nIn the fine-tuning and inference stage, we employ causal language modeling. The input sequence begins with the word sequence (Part A) and a <BOS> token (indicating the start of the note sequence), and the model predicts the next token in an autoregressive manner until it generates an <EOS> token."}, {"title": "4 Experiments", "content": "A large-scale paired dataset is critical for lyric-to-melody generation models to capture lyric-melody correlations and attain superior performance. However, the current largest paired dataset (Yu, Srivastava, and Canales 2021) only contains 12,197 MIDI songs and lacks one-to-multiple alignment. In this paper, we acquire approximately 1.6 million raw MIDI data from MelodyNet (Wu et al. 2023), and construct a large-scale lyric-melody paired dataset with varied word-note alignments, including both one-to-one and one-to-multiple alignments.\nTo obtain high-quality MIDI songs from raw MIDI data, we perform data processing in four phases: lyric processing phase, melody processing phase, lyric-melody combined processing phase, and de-duplication phase.\nFirst, we clean the lyrics by retaining only English letters and punctuation marks 3, and converting the text to lowercase. Second, given the mixture of words and syllables in the lyrics, we combine syllables into"}, {"title": "to achieve 250,000 total updates.", "content": "After pre-training, we fine-tune SongGLM for the lyric-to-melody generation task. It aims to generate high-quality and correlated melody from given lyrics. We set training\n$\\begin{aligned}\nL = L_{SMR} + L_{SRR} + L_{Phrase} + L_{Song}\n\\end{aligned}$\n(12)\nIn the pre-training stage, the input sequence S contains three parts: Part A is the word sequence W, Part B is the corrupted note sequence, and Part C consists of the masked spans with each separated by a <SEP> token. Tokens in Part A & Part B form the corrupted sequence Scorrupt, and can attend to each other. Part C tokens can only attend to preceding tokens, and tokens in Part A & Part B. Figure 6(b) illustrates how attention weight is modified through the self-attention mask to control the token's attention. Formally, WA and M are described as:\n$W_A = softmax(\\frac{QK^T}{\\sqrt{d_k}} + M)$\n(13)\n$M_{ij} = \n\\begin{cases}\n0, & \\text{ allow to attend}\\\\ \n-\\infty, & \\text{ prevent from attending}\n\\end{cases}$\n(14)\nWith this mechanism, our unified model effectively learns a bidirectional encoder for Part A & Part B, and a unidirectional decoder for Part C.\nIn the fine-tuning and inference stage, we employ causal language modeling. The input sequence begins with the word sequence (Part A) and a <BOS> token (indicating the start of the note sequence), and the model predicts the next token in an autoregressive manner until it generates an <EOS> token."}, {"title": "structured questions with both yes/no and short text responses.", "content": "13.8k dataset using expert labels. First, we split the dataset into train/dev/test splits with ratios of 70/15/15 respectively and then apply data augmentation to the training set using the following steps:\n4.4 Main Results\nTo verify the effectiveness of SongGLM in the alignment and harmony between lyrics and melody, we compare SongGLMsmall to the original and ReLyMe-equipped SongMASS and TeleMelody with same system configurations. The objective results are shown in Table 5. It is evident that SongGLMsmall significantly surpasses the baseline models across all objective metrics. Specifically, DA indicates that SongGLMsmall outperforms in lyric-melody alignment, while DP, DD, DIOI and MD suggest that SongGLMsmall is the most capable of ensuring the harmony between lyrics and melodies. Table 6 shows the subjective results, from which we can see that for melody itself, SongGLMsmall can generate diverse and consistent melodies. For the overall song, SongGLMsmall not only ensures the rhythmic and"}, {"title": "7.74\u00b10.34", "content": "The number of tokens is a good measure to evaluate compression and quality. Table 4 show the final token count for the different datasets. From this we can infer the compression ratio is at least equal to 1000/x where x is the number of tokens obtained after compression with proposed method. The number of tokens is a good indication of the quality of generated dataset. As seen in Table 4, the number of tokens generated by the proposed method is much lower compared to the baseline methods that can generate many invalid tokens and results in lower compression ratios. The results also show that when compression increases by decreasing the number of tokens, the perplexity value is much higher in our method than the compared methods which proves that our method generates quality datasets with the same level of compression achieved by the other methods.\nstructural consistency between lyrics and melody, but also achieves the best results in singability and overall performance. Besides, SongGLMbase achieves better results with a larger model and pre-training dataset in both objective and subjective evaluations, showing the scalability and capability of SongGLM.\nTo verify the\n, we compare the\nThe results in Table 5 show that our"}, {"title": "5 Conclusion", "content": "In this paper, we propose SongGLM, a lyric-to-melody generation system that leverages 2D alignment encoding and multi-task pre-training to ensure the alignment and harmony between lyrics and melodies. We introduce a unified symbolic song representation for lyrics and melodies that contains generic, content-related, and alignment-related attributes, and 2D alignment encoding to capture accurate alignments between lyrics and melodies. We design a multi-task pre-training framework with hierarchical blank infilling objectives (n-gram, phrase, and long span), and integrate lyric-melody relationships into the extraction of harmonized n-grams to guarantee the harmony between lyrics and melodies. Both objective and subjective results indicate that our proposed SongGLM can generate high-quality melodies from lyrics with remarkable lyric-melody alignment and harmony. Furthermore, method analysis shows the effectiveness of the detailed designs in SongGLM. In the future, we plan to extend our research to include more languages, such as Chinese, and explore the application of SongGLM to other automatic music composition tasks, such as text-to-music generation and video-to-music generation."}]}