{"title": "Mixture of Experts for Node Classification", "authors": ["Yu Shi", "Yiqi Wang", "WeiXuan Lang", "Jiaxin Zhang", "Pan Dong", "Aiping Li"], "abstract": "Nodes in the real-world graphs exhibit diverse patterns in numerous aspects, such as degree and homophily. However, most existent node predictors fail to capture a wide range of node patterns or to make predictions based on distinct node patterns, resulting in unsatisfactory classification performance. In this paper, we reveal that different node predictors are good at handling nodes with specific patterns and only apply one node predictor uniformly could lead to suboptimal result. To mitigate this gap, we propose a mixture of experts framework, MOE-NP, for node classification. Specifically, MOE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Experimental results from a range of real-world datasets demonstrate significant performance improvements from MOE-NP .", "sections": [{"title": "Introduction", "content": "Node classification, which aims to predict the class of nodes in a graph, has a wide range of applications (Tang, Aggarwal, and Liu 2016; Xiao et al. 2022), such as citation networks and co-author networks (Lin et al. 2020; Xiao et al. 2022). The key to node classification tasks lies in node representation learning (Jin et al. 2021), which has been theoretically and empirically proven to be one of the key strengths of Graph Neural Networks (GNNs) (Hamilton, Ying, and Leskovec 2017). GNNs typically refine node representations through information aggregation and feature transformation among the nodes, and have achieved impressive performance in various graph-related tasks, such as social analysis (Ben Yahia 2024), recommendation system (Wu et al. 2022) and traffic prediction (Yu, Yin, and Zhu 2018).\nMost existent GNN models are designed based on the homophily assumption (Bi et al. 2024; Zheng, Luan, and Chen 2024; Platonov et al. 2024), i.e., nodes tend to be similar with those connected to them. However, this assumption is not always true. There exist numerous heterophilic graphs in reality, such as the social network in a dating website (Wang et al. 2024), where numerous users tend to follow people of different genders. To solve node classification in these scenarios, several studies propose to integrate information from broader neighborhoods so as to strengthen the relationships between originally-unconnected nodes. For instance, Mix-Hop (Abu-El-Haija et al. 2019) repeatedly mixes feature representations of neighbors at various distances. Geom-GCN (Pei et al. 2020) proposes to add virtual nodes in feature aggregation. WRGCN (Suresh et al. 2021) constructs a modified computation graph to enhance the connections and information for disassortative nodes. Furthermore, some recent work (Luan et al. 2022; Chen et al. 2023) proposes to deal with this heterophliy challenge in a node-wise manner, given the consideration that different nodes in the same graph can be faced with distinct heterophliy challenges.\nWhile heterophily issue has been widely studied, some latest research (Ma et al. 2021; Mao et al. 2024) suggests that node classification is essentially related to the node pattern, rather than merely the label homophliy. However, the node pattern is quite complex, which can be described from various perspectives, including label homophily, structure information and attribute information. In addition, nodes within the same graph can possess diverse node patterns from different perspectives. This naturally raises a problem: Do nodes with different node patterns within the same graph require distinct node predictors?\nTo answer this question, we first have conducted some empirical investigations over node patterns. We have observed that nodes with different patterns in one graph require distinct node classifiers and their preferences for classifiers are affected by numerous factors. Furthermore, we have noticed that the classifier preference is not only related to node pattern, but also associated with the overall graph context, as suggested in previous study (Mao et al. 2024). Based on these observation, we then propose MOE-NP, a mixture-of-expert model for the node classification task, which consists of a bunch of diverse node predictors and a specially designed gating network. The gating network is able to adaptively assign experts to each node according to both its local node pattern and the overall graph context. Extensive experiments have shown the superior performance of the proposed MOE-NP. Specifically, it surpasses the best baseline on PubMed and Actor datasets by 4.4% and 3.4 % in accuracy. In addition, a theoretical analysis is provided to demonstrate the rationality of MOE-NP ."}, {"title": "Preliminary", "content": "In this section, We investigate node patterns within the same graph from different perspectives and their influence towards the classifiers. Some notations and backgrounds are first introduced, and then we discuss the empirical observations and key insights."}, {"title": "Notations and Backgrouds", "content": "We denote an undirected graph without self-loops as G = (V, E), where V = {vi}_1 is the node set and E \u2286 V \u00d7 V is the edge set. Let N (vi) denote the neighbors of node vi. A \u2208 Rnxn denotes the adjacency matrix, and D represents the diagonal matrix standing for the degree matrix such that Dii = \u22117=1 Aij. Let \u00c3 and \u010e be the corresponding matrices with self-loops, i.e., A = A + I and D = D + I, where I is the identity matrix. X \u2208 Rnxd denotes the initial node feature matrix, where d is the number of dimensions. We use y to denote the ground-truth node label and y\u2081 denote the label of node i. In this paper, we define homophily h on graph Gas\n$h(G) = \\frac{1}{|V|} \\sum_{v_i \\in V} \\frac{|\\{u \\mid u \\in N(v_i), y_u = y_{v_i} \\}|}{D_{ii}}$\nwhere h(G) is the average node homophily value for Graph G, within the range of [0, 1].\nMost Graph Neural Networks (GNNs) are designed to use low-pass filters to refine node representation, which tend to smooth node features across the graph based on the homophily assumption. On the contrary, the high-pass GNNs are designed to capture differences between a node and its neighbors, which are more suitable for heterophilic graphs, where nodes tend to connect with different counterparts. In addition, there exist some models, such as multilayer perceptrons (MLPs), not aggregating neighbor information. Instead, they solely transform features based on the its own data, ignoring the graph structure."}, {"title": "Empirical Study", "content": "As shown in Figure 1, nodes in the same graph can vary in patterns from different perspectives, such as homophily and degree. Regardless of whether the overall graph is homophilic or heterophilic, the node distributions in terms of homophily and degree values can exhibit significant differences within one graph. Specifically, there can exist considerable heterogeneous nodes in a homophilic graph, and vice versa.\nTo explore the effect of node patterns towards node classification, a series of preliminary study have been conducted. Specifically, we divide nodes from one dataset into five groups based on its homophily values, and examine their preferences for different node classifiers. The results of both a homophilic graph and a heterotrophic graph are demonstrated in Figure 2. It is obvious that nodes from different homophily group hold diverse preferences for node classifiers. Also, this preference difference is not related to the homophily group, but the overall graph homophily, given the observation that best classifiers for nodes from low and medium groups are not the same between Chameleon and PubMed. To reveal the diversity of node patterns and its influence for node classification, we further divide one homophily group into five separate sets based on the node degree value. As shown in Figure 2c, even within the same homophily group, the degree of nodes influences the effectiveness of different node predictors.\nThe varying effectiveness of classifiers across different homophily and degree groups suggests that the complex node patterns play significant role in node classification, and the node patterns are supposed to be illustrated from diverse perpectives. In addition, both local properties and the broader context of the graph affect the preference for node classifiers. These observations highlight the necessity of assigning different classifiers to different nodes based on both their node patterns from various aspects and the overall graph context."}, {"title": "Theoretical analysis", "content": "This section provides a theoretical analysis to support the necessity for different node predictors for nodes with diverse node patterns. We use the Contextual Stochastic Block Model (CSBM) (Baranwal, Fountoulakis, and Jagannath 2021), a generative model commonly employed to produce graph structures and node features. Typically, CSBMs assume graphs following a uniform pattern. It generates edges following nodes with the same label connected with probability p, while nodes with different labels are connected with probability q. We have a Contextual Stochastic Block Model (CSBM) as\n(A, X) ~ CSBM(n, p, q, \u03bc, \u03bd),\nwhere n represents the total number of nodes, and \u00b5 and v denote the average feature vectors for different node classes with ||\u03bc||, ||v|| \u2264 1. Feature vectors drawn from a Gaussian distribution N (\u03bc, \u03c3\u00b2I), where \u03c3\u00b2I is the covariance matrix, with I representing the identity matrix. Without loss of generality, we assume that p \u2212 q \u2264 0. Similarly, we define another CSBM as\n(A', X') ~ CSBM (n', p', q', \u03bc, \u03bd), \u03bd)\nWe use a linear classifier with parameters w \u2208 Rd and b \u2208 IR. The predicted labels can be represented by\n\u0177 = \u03c3(Xw + b1),\nwhere \u03c3(x) = (1 + e-x)\u00af\u00b9 denotes the sigmoid function. We utilize Simplified Graph Convolutional networks (SGC) characterized by low-pass filters, which are defined as\nX = D-1AX.\nWe use the binary cross-entropy loss as\n$L(V, w, b) = \\frac{1}{|V|} \\sum_{v_i \\in V} Y_i log (y_i) + (1 - Y_i) log (1 - \\hat{y_i}),$\nwhere V are test samples.\n(Baranwal, Fountoulakis, and Jagannath 2021) shows that the optimal linear classifier is given by w* = R= and b* = \u2212 (\u03bd + \u03bc, w*). It is assumed that the distance between \u00b5 and vis large, satisfying ||\u03bd \u2013 \u03bc|| = \u03a9$\\frac{\\sigma n}{\\sqrt{d n(p+q)}}$"}, {"title": "Method", "content": "Based on the preliminary study, nodes within the same graph can exhibit diverse node patterns from different perspectives, and this affects their preference for node predictors significantly. To accommodate the node pattern diversity, this paper proposes a Mixture of Experts (MoE) framework to solve node classification, which is able to assign node predictors with different weights to each node based on its node patterns. There are several challenges to be addressed. First, how to accurately define node patterns given they are complex and multi-modal? Second, how to appropriately com-"}, {"title": "MOE-NP \u2013 A General Framework", "content": "The overall MOE-NP framework is illustrated in Figure 3, which consists of two main components, the node pattern extractor and the gating model. Specifically, the node pattern extractor is designed to extract multi-modal node patterns for each node. The gating model is developed to generate node-wise expert weights for different nodes based on their extracted node patterns. There are five common node predictors adopted as experts in the proposed MOE-NP, and more advanced experts can be easily added into the flexible MOE-NP framework."}, {"title": "Node Pattern Extractor", "content": "As revealed in the previous study, node patterns directly influence the classification performance of node predictors. Thus, it is essential to design a reasonable node pattern extractor for the proposed MOE-NP . However, this is not a trivial task. On the one hand, node patterns are naturally multi-modal and can vary in numerous aspects, such as the degree distribution and the label homophily distribution. On the other hand, label homophily distribution is hard to be accurately computed, given the absence of ground-truth labels in the test graph.\nTo solve these challenges, we carefully design a node pattern extractor component. To objectively reflect the local structure context for a given node, random walk sampling has been conducted for each node in its neighborhood. The sampled nodes are then formalize the local context for the given node, where further pattern extraction is performed. In order to exploit the node pattern from the node feature modality and to overcome the absence of node label, MOE-NP proposes to learn an edge discriminator, an instance of multi-layer perceptron (MLP), to predict the pair relation between the target node and all the nodes, based on their node features in the local graph. To further enhance the structure information, we concatenate the degree information with its edge discriminator's output in a node-wise manner, as demonstrated in Figure 3."}, {"title": "The Design of the Gating Model", "content": "Given the multi-modal local patterns provided by the node patten extractor, an appropriate gating model is desperate, which is supposed to dynamically assign weights to node classifiers for each node based on its patterns. As revealed in the previous study (Mao et al. 2024) and the preliminary analysis, the node classification is not only relevant to the local node patterns, but the overall graph pattern. For instance, nodes with relatively-low homophily values may require different node predictors, due to the variance of graph it belongs to. To reflect the overall graph context, MOE-NP proposes to generate a graph pattern via averaging all the local node patterns in a graph. Next, both the local pattern and the global pattern are transformed into the same embedding space by two learnable MLP models. The concatenated embedding from both the local and global perspectives are then serve as the input of the gating model. Finally, an MLP with the softmax function is applied to generate the final weight predictions, which is used to combine the prediction results from different experts in a node-wise manner."}, {"title": "Optimization Objective", "content": "The proposed MOE-NP aim at solving node classification tasks, thus a typical node classification loss is leveraged as the overall optimization objective as follows:\n$L_{MOE_NP} = \\sum_{i=1}^{n} log(1 + exp(-y_i (\\sum_{j=1}^{t}W_{i}f_{j}(x_{i})))),$\nwhere n denotes the total number of nodes in the training set, yi represents the true label of node i, and the prediction result is derived from the outputs of multiple experts, weighted by wji, which are determined by the gating model. The whole framework, including the edge discriminator and the gating model, is optimized in an end-to-end way under this objective. Detailed training process is discussed in Appendix."}, {"title": "Experiments", "content": "In this section, comprehensive experiments have been conducted to validate the effctiveness and rationality of the propsoed MOE-NP . The experiments settings are first introduced, then the comparison of MOE-NP and the state-of-the-art baseline on different datasets are demonstrated. Next comes further analysis of the proposed MOE-NP ."}, {"title": "Experimental settings.", "content": "Datasets. We have conducted experiments on seven widely-used datasets, including two homophilic datasets, Cora and Pubmed (Sen et al. 2008) and five heterophilic datasets, Texas, Wisconsin, Cornell, Chameleon, and Actor (Rozemberczki, Allen, and Sarkar 2021). For each dataset, 10 random splits are generated with 60% training, 20% validation, and 20% testing partitions. In each split, the models are trained with 3 different random seeds, and the average performance and standard deviation are reported.\nBaselines. Seven baselines have been adopted for comparison, including basic methods, such as MLP, common GCN (Kipf and Welling 2017)and high-pass GCN. Some latest methods that are specially designed for heterotrophic graphs have also been taken into consideration, such as ACMGCN (Luan et al. 2022), GloGNN (Li et al. 2022), LinkX (Lim et al. 2021) and LSGNN (Chen et al. 2023).\nMOE-NP Implementations. In this work, we implement five commonly-used models as experts. They are GCN, GCN with residual connect, High-pass GNN, High-pass GNN with residual connect, and MLP. Additional node predictors can be easily added as experts, given the flexibility of MOE-NP."}, {"title": "The Overall Performance Comparison", "content": "We have compared the proposed MOE-NP with seven baselines on seven datasets, including both the homophilic and heterophilic graphs. The results are shown in Table 1. The proposed MOE-NP has demonstrated impressive performance on both the homophilic and heterophilic graphs, achieving the best overall rank among all the methods. The basic baselines, such as MLP and GCN, tend to perform well only on specific groups, while MoE-NP leveraging several such baselines as experts can achieve significant improvements in terms of classification performance, even surpassing other SOTA baselines, such as ACMGNN and GloGNN. This demonstrates the effectiveness of the proposed MOE-NP."}, {"title": "Analysis of MOE-NP", "content": "In this section, we conduct some further analysis towards MOE-NP to demonstrate the rationality and effectiveness of its design. To examine the effectiveness of the gating model and the pattern extractor, we divide nodes in chameleon into five groups, following an increasing order of node homophily value, and then observe the expert weights assigned to different groups in MOE-NP. To simplify the analysis, we implement three basic models as experts. As shown in Figure 4a, the gating model generate different expert weights for node groups with different homophily characteristic, and the expert weight for the low-pass GNN is significantly higher in the node group with higher homophily values, which demonstrates the effectiveness of the weight allocation mechanism in MOE-NP . Also, we compare MOE-NP with an average weight allocation strategy as illustrated in Table 2 to further demonstrate the efficacy of the gating model. In addition, to validate the rationality of node extractor, we conduct an ablation study via excluding the global pattern part or the local pattern part. As shown in Figure 4b, either reducing the global pattern or the local pattern can cause a significant performance loss of MOE-NP, which shows the necessity of node patterns from both the local and global perspectives."}, {"title": "Related Work", "content": "MOE-NP aims to enhance node classification performance through a mixture of experts framework. The related work is discussed as follows:\nEarly work introduced the Graph Convolutional Network (GCN) model(Kipf and Welling 2017), which combines spectral filtering of graph signals with non-linearity for supervised node classification. However, GCNs perform suboptimally on heterophilic graphs, which have emerged as a significant challenge in node classification. To address this, specialized models such as GloGNN (Li et al. 2022), LinkX (Lim et al. 2021), LSGNN (Chen et al. 2023), Mixhop (Abu-El-Haija et al. 2019), and ACM-GNN (Luan et al. 2022) have been developed. Recent studies indicate that real-world graphs often exhibit a mix of node patterns (Mao et al. 2024), and traditional GNNs applying the same node patterns across all nodes, can be suboptimal.\nThe Mixture of Experts (MoE) framework (Jacobs et al. 1991; Jordan and Jacobs 1994) employs a divide-and-conquer strategy to allocate sub-tasks to different experts. It has been widely used in Natural Language Processing (NLP) (Du et al. 2022; Zhou et al. 2022) and Computer Vision (Riquelme et al. 2021). In the graph domain, GraphMETRO (Wu et al. 2023) leverages MoE to address distribution shift issues in GNNs. Link-MoE (Ma et al. 2024) employs multiple GNNs as experts, strategically selecting the most suitable expert for each node pair based on diverse pairwise information."}, {"title": "Conclusion", "content": "In this paper, we explored the complex node patterns from different perspectives in the real-world graph datasets and reveal their influences towards node predictors. To accommodate the diverse needs for node classifiers of different nodes, we propose MOE-NP, a mixture of experts framework for node classification. Specifically, MOE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Extensive experiments demonstrate the proposed MOE-NP demonstrated superior performance on both homophilic and heterophilic datasets. Further, our theoretical analysis and empirical studies validate the rationality and effectiveness of the proposed MOE-NP."}, {"title": "Datasets and Experimental Settings", "content": "In this section, we detail the datasets used and the experimental settings for both the baseline models and the proposed MOE-NP framework.\nDatasets: We conduct experiments across seven widely recognized datasets, which encompass both homophilic and heterophilic types. These include three homophilic datasets: Cora, Pubmed(Sen et al. 2008); along with three heterophilic datasets: Texas, Wisconsin, Cornell, Chameleon, and Actor(Rozemberczki, Allen, and Sarkar 2021). For all datasets, we generate ten random splits, distributing nodes into 60% training, 20% validation, and 20 % testing partitions.\nExperimental Settings: For the baseline models, we adopt the same parameter setting in their original paper. For the proposed MOE-NP, we adopt the MLP as the gating model and employ pretrained node predictors as experts. Specifically, we implement the proposed MOE-NP with GCN, GCN with residual connection, High-pass GNN, High-pass GNN with residual connection, and MLP as expert models. We use a single GPU of NVIDIA RTX 4090 24 GB, to run the experiments. All the hyperparameters are tuned based on the validation accuracy from the following search space:\nExperts\n\u2022 Learning Rate: {0.0005, 0.001}\n\u2022 Dropout: {0 ~ 0.9}\n\u2022 Weight Decay: {1e - 5, 5e - 5, 1e - 4}\n\u2022 number of layers = {2, 3, 4}\n\u2022 hidden channels ={32, 64, 128, 256}\nMOE-NP\n\u2022 Gating Learning Rate: {0.0005, 0.001}\n\u2022 Gating Weight Decay: 1e \u2013 5, 5e - 5, 1e - 4}\n\u2022 Expert Learning Rate: {0.001, 0.01, 0.1, 0.5}\n\u2022 Expert Weight Decay: {0, 5e \u2014 5, 5\u0435 \u2013 3, 5e \u2013 2}\n\u2022 hidden channels ={32, 64, 128, 256}\n\u2022 Dropout: {0 ~ 0.9}\n\u2022 number of MLP layers : {1, 2, 3, 4}\n\u2022 max random walk length : {5, 10, 20, 40}"}, {"title": "Optimization of MOE-NP", "content": "There are different optimization strategies for the proposed MOE-NP, such as an end-to-end strategy that training the gating network and the experts simultaneously, or a two-step strategy that training the experts first and then training the gating networks. Some important details are discussed as follows:\n(1) load or train?\nIn a typical mixture of experts (MOE) framework, pre-trained expert models are commonly employed for classification tasks, leveraging domain-specific knowledge. For the purpose of capturing distinct node patterns, these pre-trained expert models are more effective at identifying various types"}, {"title": "Proof of Theorem 1", "content": "The feature of node i remains normally distributed. Its mean is\n$m(i) = E(\\vec{X_i}) =\\begin{cases} \\frac{p\\mu+q\\nu}{p+q} (1 + o(1)),  for i\\in C_0, \\\\  \\frac{q\\mu+p\\nu}{p+q} (1 + o(1)),  for i\\in C_1.  \\end{cases}$\nHere, C\u2080 and C\u2081 represent classes 0 and 1. According to Lemma 2 in (Baranwal, Fountoulakis, and Jagannath 2021),"}, {"title": "Proof of Theorem 2", "content": "With Eq.(11) and Eq.(12) in the proof of Theorem 1, we can obtain\n$L(A', X', w^*, b^*) = \\frac{1}{n'} \\sum_{i=1}^{n'} log (1 + exp ((1 - 2\\epsilon_i) ((X_i, w^*) + b^*)))\n\\le log (1+exp(-\\frac{R(p'-q')}{2(p'+q')} ||\\mu - \\nu||(1 - o(1))))$\nUsing the inequality log(1+e-x)  Cex, the binary cross-entropy loss can be bounded as\n$L(A', X', w^*, b^*) \\le C exp(-\\frac{R(p'-q')}{2(p'+q')} ||\\mu - \\nu||(1 - o(1))).$\nThis provides an upper bound for the loss, which decays exponentially with the separation parameter."}]}