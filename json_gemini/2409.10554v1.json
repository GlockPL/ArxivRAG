{"title": "An Examination of Offline-Trained Encoders in Vision-Based Deep Reinforcement Learning for Autonomous Driving", "authors": ["Shawan Mohammed", "Alp Argun", "Nicolas Bonnotte", "Gerd Ascheid"], "abstract": "Our research investigates the challenges Deep Reinforcement Learning (DRL) faces in complex, Partially Observable Markov Decision Processes (POMDP) such as autonomous driving (AD), and proposes a solution for vision-based navigation in these environments. Partial observability reduces RL performance significantly, and this can be mitigated by augmenting sensor information and data fusion to reflect a more Markovian environment. However, this necessitates an increasingly complex perception module, whose training via RL is complicated due to inherent limitations. As the neural network architecture becomes more complex, the reward function's effectiveness as an error signal diminishes since the only source of supervision is the reward, which is often noisy, sparse, and delayed. Task-irrelevant elements in images, such as the sky or certain objects, pose additional complexities. Our research adopts an offline-trained encoder to leverage large video datasets through self-supervised learning to learn generalizable representations. Then, we train a head network on top of these representations through DRL to learn to control an ego vehicle in the CARLA AD simulator. This study presents a broad investigation of the impact of different learning schemes for offline-training of encoders on the performance of DRL agents in challenging AD tasks. Furthermore, we show that the features learned by watching BDD100K driving videos can be directly transferred to achieve lane following and collision avoidance in CARLA simulator, in a zero-shot learning fashion. Finally, we explore the impact of various architectural decisions for the RL networks to utilize the transferred representations efficiently. Therefore, in this work, we introduce and validate an optimal way for obtaining suitable representations of the environment, and transferring them to RL networks. We experimentally demonstrate the correlation between RL performance in the CARLA simulator and the quality of representations received by the RL agent. These results highlight the crucial role that feature extraction and state representation play in RL.", "sections": [{"title": "INTRODUCTION", "content": "In the domain of artificial intelligence, vision-based meth-ods have arisen as a pivotal area of exploration. Leveraging camera images as primary inputs offers a versatile and cost-effective way to obtain a nuanced understanding of an environment by capturing colors, dimensions, and spatial orientation of objects. The recent advancements in deep learning and computer vision accentuate the capabilities of cameras, suggesting a potential reduction in reliance on expensive sensors like LiDAR. However, the journey to achieve autonomy still faces numerous challenges. Autonomous driving operates primarily within the boundaries of Partially Observable Markov Decision Processes (POMDPs), where only a part of the environmental state is observed, called belief state. In an attempt to reduce the uncertainty in the belief state, historical information, sensor augmentation and data fusion methods are utilized.\nWhile it has been shown that this uncertainty is reduced when more information is added, it also poses several new problems. The use of diverse sensors and their associated data fusion techniques amplify the complexity of the per-ception module. Hence, cutting-edge learning algorithms, datasets, and hardware solutions tailored for deep neural networks (DNN) and high-dimensional data streams are required. This progression of complexity inevitably raises critical questions about the applicability of Deep Reinforce-ment Learning (DRL) in real-world AD scenarios. End-to-end RL methods suffer from low sampling efficiency [1], which is especially critical for AD. High-resolution sensors produce vast amounts of data every second [2], necessitating semantic processing at various layers [3]. The data often has spatiotemporal correlations that are essential to capture [4].\nIn this work, we address the challenges arising from the limitations of reward-driven representation learning of spatiotemporal input data for AD using DRL. We investigate the effects of various learning schemes for the offline-training of encoders for spatiotemporal representation learning, and the performance of RL agents with the transferred represen-tations for AD, as depicted in Figure 1. Our experiments are conducted in the high-fidelity simulator CARLA [5], where RL agents control ego vehicles to achieve lane following and collision avoidance tasks."}, {"title": "RELATED WORK", "content": "To improve the sample-efficiency of RL and learn latent dynamics, several works employ variational autoencoders (VAE) [11] [12] applied to Google DeepMind infrastructure for physics-based simulation (DMControl) [13]. Addition-ally, to tackle challenges in reward-driven, vision-based RL, several methodologies have adopted offline-trained encoders [14], [15], which use contrastive learning for training the encoders, and evaluated in DMControl tasks. Yuan et al. [16] employ earlier layers of a pre-trained image encoder trained on ImageNet [17] through supervision, and DMControl and CARLA. However, in this work we compare a variety of video encoders trained through contrastive, non-contrastive and generative models without any labels, and evaluate their performances on CARLA. Additionally, Feichtenhofer et al. [18] compare MoCo and BYOL for spatiotemporal repre-sentation learning for downstream computer vision tasks, where we compare them for control tasks in CARLA, and additionally employ a variational autoencoder as a generative model and another contrastive model, DPC [8] for a more diverse investigation.\nWe conduct our study in the the hyper-realistic environ-ments presented by CARLA driving simulator, aiming to address real-world autonomous driving tasks. While earlier studies utilized vision-based RL methods in challenging environments, their visual data often lack the depth and complexity of real-world conditions. Real-world scenarios introduce nuances such as fluctuating weather, unpredictable pedestrian behavior, dynamic traffic, and occlusions, all with partial observability. The works of Toromanoff et al. [19] and Carton et al. [20] leverage monocular cameras within CARLA, similar to us, to solve autonomous driving tasks by using imitation learning and highly informative reward func-tions. However, in this work, we do not use imitation learning and use a highly sparse and challenging reward function that offers less granular information to allow our agent to adapt to difficult situations, and generalize better. Additionally, we assess the performance implications of different learning schemes to offline-train encoders and architectural design decisions."}, {"title": "METHODOLOGY", "content": "Our methodology can be splitted into three major compo-nents: offline-trained encoder, head network and DRL model. First we train an encoder to learn generalizable features from an offline video dataset to be transferred to our DRL model. This encoder is trained offline based on various state of the art algorithms from computer vision. Then, the representation of the encoder is used for decoupled DRL to learn to control a vehicle in a driving simulator. Finally, we perform an ablation study for designing the head network to transfer the learned representations optimally for vision-based navigation."}, {"title": "1. Offline-trained Encoder", "content": "In order to utilize large video datasets and transfer this knowledge to our DRL network, we employ an encoder so that our DRL network can learn the ego vehicle control from the predicted features, instead of from pixels.\nWe use a sequence of four images as input to our agent. Hence, for our encoder, we use a 3D-ResNet18 [21], due to its feature extraction capabilities. This architecture provides an optimal balance between speed and performance for our study. Thus, while preserving adequate complexity, its size allows adequate training and inference time. We trained our encoders offline on the the Berkeley Deep Drive (BDD 100K) dataset [22] with no annotations, in a self-supervised manner. All our encoders use an input of size 4x3x128x128, due to four stacked RGB frames of 128 pixels. This training process seeks to test the encoders' generalizability: Can they recognize patterns valid beyond their initial dataset? Our research aims to determine which learning scheme, guided by its inherent loss function, can provide the best representations to learn the control task in CARLA simulator. The Results chapter provides a detailed evaluation of each encoder, presenting an empirical answer to our central question. For a detailed list of hyperparameters and a description of the losses, we refer to our GitHub repository [18].\nMomentum Contrast (MoCo): MoCo [7] utilizes the InfoNCE contrastive loss [23], which learns to pull together augmented views of the same sample, and push away the embeddings from other samples. Hence, MoCo objective maximizes the similarity sim with the positive keys {k+} and minimizes the similarity with negative keys {k-} as in Equation 1."}, {"title": null, "content": "Lq = -log( \\$\\frac{\\sum_{k \\in {k+}}exp(sim(q,k)/a)}{\\Sigma_{k \\in {k+,k-}}exp(sim(q,k)/a)} \\$) \n(1)\nwith : sim(q, k) = \\$\\frac{q^Tk}{||q|| \\cdot ||k||}\\$"}, {"title": null, "content": "Bootstrap Your Own Latent (BYOL): BYOL utilizes a non-contrastive loss, which aims for the consistency in representation across different augmentations of the same sample [9]. Without a contrastive objective, in order to prevent representation collapse, where the model predicts the same constant vector regardless of the input [24], BYOL implements an extra multi-layer perceptron predictor head on top of the query encoder. BYOL uses a similarity loss, a simple mean square error (MSE) between the representations of the query encoder qe(ze) and the key encoder z\u0119, as shown in Equation 2."}, {"title": null, "content": "Lo, ||40(20) - 21 || (2)"}, {"title": null, "content": "To apply MoCo and BYOL for video, we used their adap-tations to spatiotemporal representation learning, through 3D convolutions and an additional temporal persistency objective [25], so that the clips from the same video have similar embeddings, and the clips from different videos have dis-tant embeddings. Furthermore, MoCo and BYOL utilize a momentum encoder, whose weights are a moving averages of the original encoder.\nDense Predictive Coding (DPC): DPC learns represen-tations by recurrently predicting future representations [8]. Similar to MoCo, DPC is a contrastive learning approach, utilizing the InfoNCE loss (Eq. 1). However, DPC differs from MoCo by not utilizing a momentum encoder, but aggregating the predicted representations over the temporal axis using recurrent neural networks. Hence, it implicitly enforces a temporal coherence to learn the inherent temporal dynamics of the data.\nVariational Autoencoder (VAE): A VAE consists of two main components: an encoder, which maps the input data to a lower-dimensional latent space, and a decoder, which reconstructs the input data from the latent representation [6]. While the reconstruction loss encourages compact and efficient encoding of the input data, the Kullback-Leibler (KL) divergence term pushes the learned latent variables to adhere to a known distribution, enabling smoother interpola-tion and generalization in the latent space, as in Equation 3. To make the autoencoder comparable to the other approaches used in this work, we adapted it to take a sequence of four input images. In contrary to MoCo, BYOL and DPC, which are joint embedding architectures, VAE is a generative model, which learns the representations with a reconstruction objective."}, {"title": null, "content": "LVAE = ||x-2||2 + DKL(q(z|x)||p(z)) (3)"}, {"title": "2. Deep Reinforcement Learning for Autonomous Driving", "content": "For this study, we adapted our existing DRL framework [28] and used it for the validation of our encoders for vision-based navigation in CARLA driving simulator. Figure 2 shows an overview of the DRL framework used with all its integral parts. The framework consists of three main modules: environment, leaderboard, and DRL framework. CARLA leaderboard standardizes traffic scenario definitions and provides a benchmark to compare the performance of different approaches. We integrate the offline trained en-coders and implement an additional head network to predict vehicle controls into our DRL framework so that the RL agent can learn higher-level decision-making and control policies from visual abstractions rather than learning multiple perception tasks. By freezing the encoder layers, we prevent them from being trained by the RL loss function, resulting in a visual abstraction, i.e., representation learning based RL training configuration. This configuration trains only the actor and critic NNs with RL loss and uses representations as inputs st from the frozen encoder module. We standardized the output of the encoders, so they predict representations of same dimensionality, regardless of the training scheme. We use proximal policy optimization for RL loss, as in Equation 4 [29]."}, {"title": null, "content": "LCLIP (0) = \u00ca[min(rt(0)\u0100t, clip(rt(0),1 \u2212 \u20ac, 1 + \u20ac)\u00c2t)]\n(4)\nwith :\nrt(0) = \\$\\frac{\\pi_{\\theta}(a_t|s_t)}{\\Pi_{\\theta_{old}}(a_t|s_t)}\\$"}, {"title": "Environment setup:", "content": "We conducted experiments in CARLA focusing on lane following and collision avoidance. The evaluation was limited exclusively to the steering control of an ego vehicle, with acceleration control taken over by the autopilot. This approach was chosen because learning acceleration control requires additional information, such as variable speed limits on different road types. In addition, suc-cessful acceleration control would place increased demands on NN, particularly regarding data fusion within the networks and adaptation of input data. These additional requirements could change the nature of our ablation study and complicate the investigation of the impacts of different offline encoders. Therefore, the focus on steering control was chosen in this study.\nThe observation space for all encoders is uniform, with a frame size of 3x128x128 and four stacked RGB frames. An example of the observation space used is shown in Figure 3. Due to resource constraints, we ran the CARLA experiments with four agents. We initialized the agents on training routes 31, 35, and 46 provided by the CARLA Leaderboard [30]. The roads can be curvy or straight depending on random spawn location. Here, an episode is always exactly 400 steps long unless there is a collision."}, {"title": "Reward function:", "content": "The reward function for RL training consists of three components. The first component, ri, relates to lane violation penalties, and the second component, rc, relates to collision penalties. Both components are set to 1 when the ego vehicle crosses a lane or collides with another vehicle, walls, or pedestrians. In all other cases, it is 0, as shown in Eq. 5. Since all reward components are negative, the maximum achievable reward is zero. Each component has its own coefficients: c\u2081 = 2 and cc = 30, which are used to distinguish the priority of the events."}, {"title": null, "content": "\u03b3reward = \u2212ciri \u2212 Ccrc\n(5)\n\u03b3i =\n{0, if kept lane\n1, if left lane\n, rc =\n{0, if no collision\n1, if collision"}, {"title": null, "content": "The reward function, \u03b3reward, is intentionally formulated very simply compared to other works solving the same task [19], [20]. In this way, we challenge the RL optimization for learning complex policies by a simple and sparse but goal-oriented reward signal, as we saw that this allows a more natural drive in our previous work [28].\nImplementation Details: Our framework consists of a distributed RL training on multiple instances of CARLA simulator to collect training samples more efficiently. In order to standardize the models we compared, we converted all models to the Open Neural Network Exchange (ONNX) standard as it provides a unified representation for machine learning models [31], regardless of the models were imple-mented in PyTorch or TensorFlow."}, {"title": "3. Ablation Study on Designing a Head Network for Vision-based Navigation", "content": "When adapting encoders, initially trained on SSL objec-tives, to downstream vision-based navigation tasks through DRL, it's pivotal to transfer the representations to a task-specific head network. To design the head network for our encoder, we perform an ablation study examining three different architectures for the head network.\nDirect Feature Extraction: In this setup, the 2048 features from the projection layer are flattened and used directly without the prediction layer, as in Figure 4(a). By omitting the last prediction layer of the original head, which is highly adapted to the original self-supervised learning task, more general features can be transferred. This 1D vector is passed to the actor and critic networks, which consist entirely of fully connected (FC) layers."}, {"title": "RESULTS", "content": "In this chapter, we first present outcomes from our ar-chitectural design search for the encoder head, actor, and critic. Using the optimal configuration, we then evaluate the performance of the specified encoders. Our best-trained en-coder's performance, utilizing the top architectural design, is subsequently compared against an end-to-end DRL approach. Our results are showcased in two formats:\nCumulative reward per episode, adhering to RL conven-tions.\nMetrics from the CARLA leaderboard, indicating lane invasions and steps per episode.\nEach experiment was run three times; we depict the median, smoothed with a 100-step moving average, for clarity."}, {"title": "Architectural Design Search", "content": "The results of the architectural design search are visualized as a heatmap in Fig. 7. Each of the head networks on top of the frozen encoders was trained for 1800 episodes. After normalization, we plotted the highest achieved reward. In the subsequent descriptions, the two variations of each of the three methods are labeled with a suffix: s indicates the smaller neural network, while xl designates the larger one. Both variants of the \"Direct Feature Extraction\" method, labeled as ProlD_s for the smaller version and Pro1D_xl for the larger network, perform poorly across all three encoder types examined. The \"Temporal Axis Reduction\" variants (avg2D_s and avg2D_xl) exhibit the best perfor-mance, with DPC performing well only when paired with the larger network. The \"Conv-Layer with 3D Average\" versions (avg1D_s and avg1D_xl) perform variably depending on the encoder, obtaining mediocre to high rewards."}, {"title": "Analysis of Different Learning Schemes to Train Encoders", "content": "Figure 5a, 5b, and 5c showcase evaluations for under-standing how different trained encoders influence RL per-formance. Notably, most RL trainings with offline-trained encoders succeeded in the task, with DPC being an ex-ception. The DRL training of the head network with DPC as the encoder failed to converge, consistently committing lane invasions even beyond 1800 episodes. A successful driving behavior requires zero lane invasions and zero col-lisions to achieve the maximum possible reward, which is zero. The VAE encoder exhibited this, albeit with bet-ter convergence than DPC. MoCo and BYOL succeeded in the task, demonstrating a consistent driving behavior. Their primary distinction was in convergence speed: BYOL reached zero rewards in just 700 episodes, while MoCo took 850 episodes. Due to its supervised and multi-task learning scheme, and more complex E-ELAN backbone, YOLOPv2 encoder showed a faster convergence and reached a reward of zero in merely 500 episodes, as expected. However, BYOL is a close second, which shows the strength of BYOL represen-tations, even though trained without labels and with a smaller backbone, 3D-ResNet18. Figure 6a highlights the significant performance difference between training with frozen and of-fline trained encoders, and traditional end-to-end RL. Despite using similar architectures and configurations\u2014based on the same DRL framework and hyperparameters from Table I\u2014differences are evident. End-to-end RL fails at the lane following and collision avoidance tasks, and thus, can only reach a reward of -4. In contrast, our BYOL encoder with a trained head network not only learns an effective policy but also attains zero rewards after approximately 700 episodes, which indicates of a flawless driving behavior without any lane violations or collisions, as shown by Figures 6b and 6c."}, {"title": "CONCLUSION", "content": "In this work, we examine multiple self-supervised spa-tiotemporal representation learning approaches to train an encoder for AD using DRL. We train our encoders on the BDD100K driving dataset, and design a head network on top of these encoders to predict vehicle controls in CARLA driv-ing simulator. Then, we train the head network through DRL to learn the lane following and collision avoidance tasks. Our experiment results showed that learning by reconstruc-tion produces weaker features for perception as also shown in [32]. On the other hand, joint embedding architectures learned stronger features, which are also useful for control tasks. As our non-contrastive approach, BYOL was the best performing SSL approach, we believe contrastive learning can be harmful to learned representations, as some videos in the dataset can be still similar to each other, hence, include similar features. However, contrastive learning pushes these embeddings away from each other. Furthermore, even though Transformer-based approaches proved their strength in vision tasks [33] we show that a relatively lightweight ConvNet, such as a 3D-ResNet18 can capture useful features from a sequence of frames that can be useful for control. Our investigation for designing the head network showed that when the representation quality is high such as the encoder trained by BYOL, the impact of the design of the head net-work is less significant. However, with lower representation quality, such as the encoder trained by DPC, the design of the head network is important for succeeding in the control task. Finally, self-supervised learning can learn generalizable features that can be transferred from the BDD100K dataset to CARLA simulator without further fine-tuning, which proves the scalability of SSL to utilize large video datasets."}]}