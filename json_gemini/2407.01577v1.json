{"title": "MOT: A Mixture of Actors Reinforcement Learning Method by Optimal Transport for Algorithmic Trading", "authors": ["Xi Cheng", "Jinghao Zhang", "Yunan Zeng", "Wenfang Xue"], "abstract": "Algorithmic trading refers to executing buy and sell orders for specific assets based on automatically identified trading opportunities. Strategies based on reinforcement learning (RL) have demonstrated remarkable capabilities in addressing algorithmic trading problems. However, the trading patterns differ among market conditions due to shifted distribution data. Ignoring multiple patterns in the data will undermine the performance of RL. In this paper, we propose MOT, which designs multiple actors with disentangled representation learning to model the different patterns of the market. Furthermore, we incorporate the Optimal Transport (OT) algorithm to allocate samples to the appropriate actor by introducing a regularization loss term. Additionally, we propose Pretrain Module to facilitate imitation learning by aligning the outputs of actors with expert strategy and better balance the exploration and exploitation of RL. Experimental results on real futures market data demonstrate that MOT exhibits excellent profit capabilities while balancing risks. Ablation studies validate the effectiveness of the components of MOT.", "sections": [{"title": "1 Introduction", "content": "The goal of algorithmic trading is to maximize long-term profits while keeping risks within an acceptable range [21]. Compared to the traditional approach of relying on the expert judgment of trading timing, algorithmic trading is highly automated and efficient. Traditional technical analysis methods include mean reversion [10], momentum investing [11], multi-factor models [4], etc. However, financial market data is non-stationary with a low signal-to-noise ratio. Expert-designed technical analysis methods can't generate profits under diverse market conditions. Deep learning methods excel at capturing intricate price patterns and enhance models' performance [28,29,15]. However, the process from supervised models' output to actual investment still requires the construction of strategies, which introduces expert knowledge and subjectivity. RL methods don't require carefully designed strategies by humans. They take market information as states and output trading decisions directly, which makes it easy to incorporate the unique financial constraints (e.g. transaction costs and slippage) into environments. RL has achieved SOTA in many quantitative investment tasks [16,19,30]."}, {"title": "2 Problem Formulation", "content": "The algorithmic trading problem can be represented as Markov Decision Process (MDP) M = (S, A, P, R,y), where S represents the state space provided by the environment, A represents the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the probability function of the conditional state transitions, R : S \u00d7 A \u2192 R is the reward function, and \u03b3 \u03b5 (0, 1) is the discount factor. The specific definition of the five-tuple for MDP is as follows:\nThe State space S: The state St = [S;S] \u2208 S. The account indicators S = [s1, s2, ...] describe the trader's positions, account cash balance, margin, returns, and other related information of the trader's account. Pt = [pi, p\u0159, Pt, Pi, vi, vt] represents the Opening-High-Low-Closing (OHLC) prices, trading volume, and trading value. Qt = [q+, q\u00b2, \u2026\u2026\u2026, q] are derived from Pt and technical analysis. The market indicators S = [Pt; Qt] include the volume-price data Pt and the technical indicators Qt.\nThe Action space A: The action at \u2208 {\u22121,1} represents the trading signal output by the agent. -1 corresponds to short selling and 1 corresponds to a long position. We define the agent to trade in units of contracts. The actual execution of trades depends on the trading signal and the trader's existing positions.\nThe Transition Function P: We assume that the actions of individual traders do not affect the overall asset price in the market. This implies that the observation transition function of market indicators is independent of trading behavior, i.e. P(S41 St) = P(S1 St, at). However, the observation transition function of account prices is influenced by trading behavior, i.e. P(St+1|St) + P(St+1 St, at).\nThe Reward R: We choose the closing price pe to calculate profit rt. To better simulate real market, we set transaction fee rate \u00b5 3 and slippage \u03c3 4. The profit rt is defined as rt = (pt-pt-1-2\u03c3)\u00b7at\u22121\u2212\u03bc\u00b7\u03c1\u03b5\u00b7|\u2206po|, where \u2206po = Po' \u2013 Po. When setting rewards, it is inappropriate to consider only the profit without taking into account the risk. The Sharpe ratio is the most widely used indicator for balancing risk and returns [24], defined as SR = $\\frac{mean(rt)}{std(rt)}$. To measure the impact of the profit on SR each step, we adopt the Differential Sharpe Ratio (DSR) [17] as the reward. Considering that the adjacent data is more important than distant previous data in algorithmic trading, DSR employs the smoothing technique of Exponential Moving Average (EMA). DSRt is defined as:\n$DSR_t = \\frac{B_{t-1}\\Delta A_t - A_{t-1}\\Delta B_t}{(B_{t-1}-A_{t-1})^2}$, (1)\nrepresenting the impact of each new profit rt on SR after applying EMA. At is the first moment and Bt is the second moment of profits rt estimated by EMA. We utilize the DSRt as the reward R. If the account money is insufficient, the trading will be terminated in advance, and we simulate this by setting a margin threshold."}, {"title": "3 Methodology", "content": "The overview of MOT is present in Figure 2. First, to ensure alignment between the actions in Demonstration Buffer and the actual outputs of the actor, we introduce Pretrain Module. Second, we leverage imitation learning to initialize the RL algorithm. Third, we use multiple actors with disentangled representation learning and model various market conditions. Last, Allocation Module allocates samples to different actors by OT algorithm."}, {"title": "3.1 Imitation Learning", "content": "In RL-based algorithmic trading, the initial exploration phase is often inefficient and yields low profits. Imitation learning leverages expert knowledge and provides the actor with a favorable starting point. We employ PPO [23] as the backbone to address the MDP problem. To capture the temporal patterns of states St, we utilize Gated Recurrent Units (GRU) [1] to obtain the hidden representation ht = GRU(ht-1, St) of states St. ht is then fed into the actor and critic networks as inputs.\nThe actor network aims to find the optimal policy \u3160 by maximizing the advantage function. The input is the environment state St and the output is the action at. To ensure sufficient exploration by the agent, we add noise & to the output of the actor network. The actual executed action at = \u03c0\u00ba (ht) +\u025b, where \u025b represents the noise, \u03c0 is the policy given by actor network with parameters 0. The trading experience trajectories (SARS:"}, {"title": "3.2 Pretrain Module", "content": "The Pretrain Module is used to align the actions in the buffer B with the outputs of the actor. As mentioned before, it can be observed that aexpert in DB is directly provided by Dual Thrust strategy rather than generated by the actor network \u03c0\u03b8. Therefore, when using the demonstration data for gradient descent of the network, there is a significant discrepancy between the distribution of the actor network's output action \u03c0\u03bf and the action dexpert [16]. This has a negative impact on the stability of the RL network.\nTo address this issue, we aim to align the output action at = \u03c0\u03b8 of the actor network with the expert-provided action aexpert by training the actor network using supervised learning. The loss function is defined as Equation 4:\n$L_{pre} = CrossEntropy(a_{expert}, \\pi_{\\theta}(h_t))$ (4)\nPretrain Module accelerates the actor's understanding of the task by mimicking expert strategies and enhances the actor's ability to effectively engage in the imitation learning process. Pretrain Module is positioned before imitation learning as Figure 2."}, {"title": "3.3 Multiple Actors", "content": "We employ multiple actors to model strategies in different patterns. Futures data is derived from the trading activities of numerous participants and reflects different trading patterns [22]. Ignoring multiple patterns will reduce the performance of models [8]. All k actors of MOT are constructed in the same manner, as depicted in Figure 2 and Equation 3. For convenience, we illustrate how the model is trained with k = 2.\nTo integrate the outputs of the two actors, we use an Allocation Module to assign weights to them. Regarding the construction of the Allocation Module, we first consider what inputs should be provided to it. The historical sequence of futures states St plays a significant role in determining the current market patterns. Additionally, the historical decision errors of different actors represent their decision-making performance and also influence the current sample allocation. We use GRU to extract latent feature representations from S, denoted as h = GRU(-1,S), where i means i-th sample. As the calculation of sample decision errors, we provide posterior teacher actions on the training set. The teacher action ateacher = 1 when the futures close price p increases in the next time step and \u22121 otherwise. Let a\u00b9\u00b9 and a\u00b92 represent the action output by actor 1 and actor 2. The sample decision error e is then computed as [ateacher t-at1, t-a ateacher t at a2]. To avoid introducing future information, we utilize the previous error e-1. Subsequently, we concatenate h and embedding of error sequence d-1 = GRU (d-2, e-1) and feed them into a fully connected layer to predict the allocation results, denoted as b = FC(h, d\u22121). In different patterns Allocation Module should have different attention for the two actors in Equation 5, where q represents the allocation weights, and a represents the final action. To ensure the discrete differentiability of the Allocation Module, we utilize the gumbel-softmax method [9] to compute Equation 5. It is worth noting that the allocation of samples is not binary, but rather a soft allocation ranging 0 < q < 1.\n$q^i = softmax(b_i), a_t = q^T [a_t^1, a_t^2]$, (5)\nHowever, if the actors want to learn different patterns, the representations should be as dissimilar as possible. Inspired by disentangled representation learning, we take the inputs x of the actors' last layers as the representations and design a disentangled loss to enable the agent to learn different patterns, $L_{dis} = \\sum_{i=1}^N \\sum_{k=1}^{K-1} x_i^T x_i^k$."}, {"title": "3.4 Optimal Transport Regularization", "content": "However, the model lacks a mechanism to ensure the effective allocation of samples to actors. Sometimes, the majority of samples are assigned to one actor. We incorporate OT techniques to ensure that the Allocation Module assigns more appropriate samples to each actor, thereby capturing diverse patterns more accurately.\nWe need to consider two main requirements. Firstly, the Allocation Module should allocate the samples to the actor with the smallest decision error. In other words, if |ateacher t - a\u00b9| > |ateacher t - a2, we tend to assign the sample to actor 2. Secondly, the allocation of samples to the actors should be proportional to their respective patterns.\nBelow, we formally define the allocation problem. Assume we utilize N samples in each epoch of PPO's gradient descent process. Based on the error vector, we can construct an error matrix denoted as Lerr \u2208 [N \u00d72]. Each element L in it represents the decision error of the i-th sample on the j-th actor, given by Lor = ateacher - aii. Corresponding to Lerr is the allocation matrix M \u2208 [N \u00d7 2], where each element Mii e {0,1}. The value of 1 in the allocation matrix M indicates that Allocation Module assigns the i-th sample to the j-th actor, while the value of 0 indicates no allocation.\nThe OT method is particularly suitable for solving allocation problem. OT involves determining an optimal allocation of resources from one location to another while minimizing overall cost or distance. It is also commonly employed to measure the difference between two probability distributions. Our research aims to find the optimal allocation scheme that minimizes Lerr. The specific formulation of the problem is as follows,\n$min_M (L \\cdot M)$\ns.t.$\\frac{\\sum_{i=1}^{N} M_{i1}}{N} = w_1, \\frac{\\sum_{i=1}^{N} M_{i2}}{N} = w_2, M_{i1} + M_{i2} = 1, \\forall i = 1, 2, ..., N, $ (6)\nwhere w\u2081 and w\u2082 represent the proportions corresponding to different modes (assumed to be). We employ the Sinkhorn method to solve the OT problem [2]. Figure 3 provides a visual explanation of the problem we aim to address.\nTo align the distribution of the output q\u00b2 from the allocation module with M\u00b2 of the OT problem, we incorporate a cross-entropy loss term. Considering Allocation Module as part of actors, Equation 3 can be expanded to Equation 7, \u03bb\u03bf is the hyperparameter. The third term is LOT. The pseudocode for the MOT is shown in Algorithm 1.\n$L_{actor} (\\theta) = L_{CLIP} (\\theta) + L_{dis} + \\lambda_0 \\sum_{k=1}^2 M_{ik} log(q_{ik}^2).$ (7)"}, {"title": "4 Experiments", "content": "We utilize the IF stock index futures dataset whose underlying asset is the CSI 300 Index. The dataset provides minute-level trading data of contracts. Each minute bar includes OHLC, trading volume, etc. The total trading duration in a day is 240 minutes. We collected it from ricequant.com, and divided the data into a training set from 2015-12-31 to 2018-05-08 and a test set from 2018-05-09 to 2019-05-09."}, {"title": "4.2 Baselines, Evaluation Metrics and Hyperparameters", "content": "Baselines: Long Position Hold (buy futures and hold), Short Position Hold (borrow contracts and hold), Dual Thrust [13] (a technical analysis trading strategy commonly used for intraday trading), GRU [1] (a variant of RNNs), PPO [23] (a RL method that improves stability by preventing large policy changes7), iRDPG [16] (SOTA: an off-policy algorithm that incorporates expert strategy and behavior cloning).\nEvaluation Metrics: We will measure the model's performance by Accumulated Rate of Return (ARR, the overall profitability), Volatility (VO, measures by standard deviation of profit r), Annualized Sharpe Ratio (ASR, annualized version of SR), Maximum Drawdown (MDD, the maximum decline of an asset's value from its peak to the lowest over a period), Calmar Ratio (CR=$\\frac{ARR}{MDD}$, risk-adjusted ARR based on MDD) and Sortino Ratio (SoR=$\\frac{mean(r)}{std(min(r,0))}$, excess return per unit of downside risk).\nHyperparameters: We set transaction fee rate \u03bc = 2.3 \u00d7 105 and slippage \u03c3 = 0.2. Insufficient account assets may trigger a forced liquidation. We set the margin threshold as 70% and initial capital C = 50000 CNY. We repeated 6 experiments for each model."}, {"title": "4.3 Experimental Results", "content": "Table 2 provides a summary of the results. Figure 4 (a) depicts ARR of all the methods. From Table 2, MOT outperforms other methods in terms of profit and risk-reward balance. ARR is the most crucial indicator, and our model achieves the highest ARR. The ARR of PPO is about 1.0 higher than that of GRU, indicating that PPO exhibits greater robustness. The ASR, CR, and SoR are composite metrics that consider both risk and return. Deep learning methods (last 6 rows in Table 2) outperform the technical indicator models (first 3 rows in Table 2) in these three metrics, which suggests the former better represents complex states under high-noise conditions. MOT performs second in terms of MDD, indicating that MOT only requires a short time period to recover from losses. RL models outperform time-series models, as the latter primarily focuses on predicting price trends without considering the high costs caused by incorrect predictions. Since greater risk leads to greater returns, profits are higher when there are significant price fluctuations. So the correlation among most methods is very high."}, {"title": "4.4 Ablation Study", "content": "We conducted ablation experiments to show the effectiveness of its three components. The experimental results and the trend of ARR are depicted in Table 2 and Figure 4 (b).\nOverall performance. MOT-NP applies imitation learning based on PPO without Pretrain Module. MOT-ND is obtained by removing multiple actors from the final model, while MOT-NO eliminates the process of OT. From Figure 4 (b), we observe that ARR curve of MOT remains higher than other variants in most periods. Table 2 shows that MOT performs best in terms of ARR, VO, ASR, and SOR. Among the three modules, OT method contributes the most to the improvement of model performance, followed by the Pretrain Module. MOT-ND excels in MDD metric, indicating that the model without multiple actors' design tends to generate more conservative strategies. While a conservative trading strategy often misses the optimal investment opportunities. Since the calculation of CR relies on MDD, MOT-ND also exhibits higher CR.\nEffectiveness of Pretrain Module. The influence of the expert strategy in DB diminishes over time and the benefit of imitation learning is mainly observed in the early stages. For the ablation experiment, we selected the agent trained for 100 epochs after imitation learning. Figure 4 (c) illustrates the impact of Pretrain Module on imitation learning and the yellow curve is the model with Pretrain Module. It can be observed that MOT-ND demonstrates a steady increase accompanied by minor fluctuations in profit. In contrast, MOT-NP experiences some declines and doesn't learn well. This indicates that Pretrain Module contributes to the improvement of imitation learning.\nEffectiveness of multiple actors and OT modeling. Figure 5 demonstrates the variation in weights assigned to two actors before and after OT modeling. In a relatively volatile period, the model assigns weights more randomly without OT while assigns higher weights to actor 2 with OT. Notably, the introduction of OT leads to higher returns and enhances the ability to capture complex patterns. Figure 4 (d) illustrates the impact of actors' number to MOT. MOT achieves the best profitability when k = 2 while achieves the worst when k = 1. This indicates that only one actor is insufficient to capture all patterns, while an excessive number of actors may lead to redundancy. In our model, the optimal number of actors is 2."}, {"title": "5 Related Work", "content": "Investment strategies based on expert knowledge. The early method used expert knowledge to construct heuristic rules [10,20], which can be divided into two categories: fundamental analysis and technical analysis. Fundamental analysis captures diverse factors such as industry trends, company financial statements, and public opinion. This method is more commonly used by long-term investors to find undervalued assets. Popular technical indicators include Relative Strength Index [27], Average Direction Index [6], On-Balance Volume [26], etc. Commonly used investment strategies include momentum trading [7] and mean reversion strategy [20]. However, interrelated technical indicators are correlated with each other, and building them directly from the market introduces too much market noise. Typically, rules constructed based on expert knowledge can only capture trading opportunities under specific market conditions [3].\nInvestment strategies based on RL. In contrast to supervised learning, which still requires expert knowledge to construct strategies, RL can optimize strategies in an end-to-end form. Moody et al. [18] made the first attempt to apply recurrent RL (RRL) algorithm to algorithmic trading. However, traditional RL methods are not well-suited for environments with large state spaces, making it challenging to select market features. Deep RL methods have partially addressed this problem. Si et al. [25] argue that strategies need to consider multiple factors and combine multi-objective optimization with deep RL to address this issue. Oliveira et al. [19] adopts SARSA, which maps states and actions to specific cells in a table to learn the value function. Since insufficient financial data causes overfitting, Jeong et al. [12] divided stocks into groups based on their correlations and introduced transfer learning into the Deep Q-Network (DQN). To shorten the inefficient random exploration phase, iRDPG [16] incorporates technical analysis through imitation learning. Yuan et al. [30] argue that daily frequency data cannot meet the high demands of RL and instead use minute frequency data. And PPO algorithm achieves more stable returns compared to DQN and SAC algorithms."}, {"title": "6 Conclusion", "content": "In this paper, we propose MOT, an RL-based model for algorithmic trading problems. Specifically, we model the algorithmic trading problem as MDP and leverage imitation learning to enable the agent to learn from expert knowledge. To better initialize MOT, we introduce the Pretrain Module prior to the imitation learning phase. Considering that futures prices result from different patterns, we employ multiple actors with disentangled representation learning to model the patterns. We design the Allocation Module to integrate the outputs of multiple actors and incorporate OT techniques to guide the learning of the Allocation Module. Experimental results demonstrate that our model achieves superior profitability while controlling the risk, showcasing its robustness in financial markets with complex data patterns. Further ablation studies confirm the effectiveness of the three components of MOT."}]}