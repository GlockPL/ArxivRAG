{"title": "Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification", "authors": ["Santosh V. Patapati"], "abstract": "Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models. In Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score of 85.95%, a precision of 80%, and a recall of 92.86%.", "sections": [{"title": "I. INTRODUCTION", "content": "Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide [1]. The COVID-19 pandemic has further exacerbated this issue, leading to a staggering 27% increase in the global prevalence of MDD [2].\nUnlike biological disorders, MDD is not diagnosed through blood tests or imaging. It lacks these gold standards. Instead, the most common approach for MDD diagnosis relies on clinical interviews [3] and self-reported inventories (SRIs) [4], [5]. These systems have been subject to criticism for their subjectivity [6]\u2013[8], which gives way for a number of issues. Bias, for example, can come from both parties. In clinical interviews and SRIs, patients may exaggerate or under-report symptoms in social desirability bias [9], [10]. Confirmation bias affects how clinicians weigh a patient's symptoms against the assigned criteria [11]. As a result, the misdiagnosis rate of MDD is estimated to be as high as 54% [12].\nIn recent years, there has been a growing interest in the use of machine learning (ML) systems to automatically assess the presence and severity of MDD. This offers a low-cost and objective alternative to current methods. A heavily researched use case of ML involves making diagnoses from clinical interview recordings.\nNumerous studies investigate a multi-modal approach, which combines both verbal and non-verbal cues to reach a diagnosis. These models achieve extremely high levels of success. Currently, the majority of state-of-the-art models take in three modes of input: Audio, video, and text-based data.\nThe text-based modality is generally seen as the weakest link in diagnosing MDD from clinical interviews [13]. This can be attributed to the scarcity of task-specific text-based training data. This lack of data hinders the effective training of NLP models, which are data-intensive, resulting in suboptimal performance compared to the audio and video modalities.\nTo date, no research has attempted to integrate Large Language Models (LLMs) into a multi-modal architecture for this task. Due to their training on large corpora, we hypothesized incorporating LLMs into such an architecture would improve the accuracy of depression diagnosis from clinical interview recordings and alleviate the issue of scarce training data.\nIn this work, we make the following contributions:\n1) We propose a novel, tri-modal architecture that utilizes\nLLMs. This is the first work to incorporate LLMs into\na multi-modal framework for this task.\n2) We demonstrate that LLMs are effective for mental\nhealth diagnosis when incorporated into multi-modal ar-\nchitectures. The proposed architecture outperforms base-\nline and state-of-the-art models tested on the AVEC 2016\nChallenge dataset and Leave-One-Subject-Out Cross-\nValidation. The results of this work serve as an indicator\nfor the potential of such an architecture in classifying\ndepressed patients.\n3) The proposed architecture is integrated into a locally\nhosted web application to emulate the potential use of\nsuch a model in the real-world."}, {"title": "II. RELATED WORKS", "content": "Speech data has been found to be highly effective, more so than video and text-based data, for diagnosing depression from clinical interview recordings. In fact, multiple mono-modal audio-based models have achieved results comparable to SRIs for diagnosing a variety of psychiatric disorders, such as Generalized Anxiety Disorder (GAD) [14]\u2013[16].\nCurrent audio-based diagnosis methods rely on low and high level features extracted from raw audio data. The extracted features are passed as input into deep learning models.\nMany representations of audio data have been utilized for this task. (Wang et al.) [17] evaluated and compared the effectiveness of such representations, finding Mel Frequency Cepstral Coefficients (MFCCs) to be the best performing audial feature for depression diagnosis of the 30 features which were tested in the study.\nDerived using the Mel Scale [29], MFCCs provide a compact representation of the human perception of sound. This allows for the low-level representation of timbre, pitch, and rhythm. Studies indicate such values are different between depressed and non-depressed patients [30]. (Wang et al.) [31] and (Taguchi et al.) [32] found certain MFCCs follow a consistent pattern that is only present in depressed patients.\nCurrently, MFCCs are the most used feature across state-of-the-art models to represent the audio mode for this task.\nResearch has shown facial expressions and movements are significantly different in depressed patients as opposed to non-depressed ones [33] [34]. Visual-based depression diagnosis models rely on features extracted from patient videos and images.\nFacial Action Units (FAUs), a low-level representation of the movements of specific facial muscles, were introduced as part of the Facial Action Coding System [35]. FAUs allow us to objectively study facial expressions and decipher non-verbal cues (Figure 3). This makes them highly effective and popular for affective computing tasks [36]. In this study, a subset of 20 out of 44 total FAUs are used, as shown in Table IV.\nMulti-modal models combine both verbal and non-verbal cues to reach a diagnosis. To our knowledge, the majority of state-of-the-art models for depression diagnosis from clinical interview recordings consider at least audio and video data in making a diagnosis."}, {"title": "III. DATA COLLECTION AND PREPROCESSING", "content": "The Distress Analysis Interview Corpus Wizard of Oz (DAIC-WOZ) contains data collected from 189 clinical interviews ranging between 7-33 minutes [41]. Each interview contains a 16kHz .wav recording, a .CSV text transcript, an ID numbered 300 492, and features collected from video data. To keep data de-identified, raw video data is not made available in the DAIC-WOZ. Instead, the dataset provides six visual features extracted in 30 frames per second using the OpenFace pose estimation library [42]. The extracted visual features are as follows: 68 2D Facial Landmarks, 68 3D Facial Landmarks, Histogram of Oriented Gradients, Facial Action Units (FAUs), 4 Gaze Vectors, and Head Pose. FAUs are the only visual feature considered in this study.\nIn the DAIC-WOZ, depression was assessed using an SRI: Patient Health Questionnare-8 (PHQ-8). The PHQ-8 measures 8 items on a scale of 0 to 3 based on how often problems have been occuring over a two-week period. The final output is a binary label and a score measuring the severity of MDD on a scale between 0-24. A cut-off score of 10 (inclusive) was used for the binary label, where participants scoring higher were considered depressed. These classifications were used to develop, train, and evaluate the proposed architecture.\nAccess to DAIC-WOZ was granted after signing an End User License Agreement (EULA).\nThe DAIC-WOZ dataset contains many errors that needed to be resolved before the regular preprocessing steps could be applied. The following errors were resolved:\n1) All audio files contained interactions between research\nassistants prior to the interview starting. These pieces of\naudio were identified and removed.\n2) The interviews with IDs of 373 and 444 contained\nlong interruptions which had to be removed. One such\ninterruption, for example, was the phone of a research\nassistant ringing.\n3) The interviews with IDs of 451, 458, and 480 were\nmissing the utterances of the therapist. This issue was\nresolved by manually transcribing the therapist speech.\n4) The interviews with IDs of 318, 321, 341, and 362\ncontained transcription files which were out of sync with\nthe audio. This was resolved by manually adjusting the\ntimestamps of the affected utterances.\n5) The interview with ID 409 contained a labeling error,\nwhere the PHQ-8 score was 10 but the binary label was\n0. This was manually resolved.\nThe DAIC-WOZ contains transcripts as tab-separated .CSV files with the following columns: speaker, start_time, stop_time, and value. The provided utterances contained numerous errors that needed to be fixed. Preprocessing was applied using RegEx and NLP techniques to make text suitable"}, {"title": "IV. MODEL DEVELOPMENT", "content": "Two-shot learning was employed with GPT-4 through the OpenAI API to reach a classification given full text transcripts from the DAIC-WOZ.\nThe model was given the prompt 'Take on the role of an expert in psychiatric diagnosis using the DSM 5. Read the following transcript and determine if the patient has depression.' This prompt is a modified version of that presented in (Galatzer-Levly et al.) [47]. Given this prompt, GPT-4 was expected to output a binary classification of 'depressed' or 'not depressed' in JSON format. The function calling feature provided in the OpenAI API was manipulated to force an output in the desired JSON format in almost all cases."}, {"title": "V. RESULTS", "content": "The proposed architecture was evaluated in two ways:\n1) The model was evaluated using the DAIC-WOZ AVEC\n2016 Challenge cross-validation split to allow compara-\nbility with models from other studies.\n2) The proposed architecture was evaluated through\nLOSOCV.\nThe proposed model outperforms all baseline models and multiple state-of-the-art models which were evaluated using a regular cross-validation split as well as LOSOCV. It performs well on both the depressed and non-depressed classes, with accuracies of 92.9% and 90.2% on LOSOCV for the depressed and non-depressed classes, respectively. The large difference in the accuracy between the depressed and non-depressed classes (92.9% and 81.81%, respectively) on the AVEC 2016 cross-validation split can be attributed to the extremely small sample size of just 14 for the depressed class. We believe the accuracy of 92.9% for the depressed class on the AVEC 2016 Challenge cross-validation split is inflated as a result of this sample size."}, {"title": "VI. DEPLOYMENT", "content": "The proposed architecture was integrated into a locally hosted web application, DepScope, for easy and accessible use of the model by clinicians. This application mimics how such a model may be implemented into real-world scenarios. Clinicians first connect their Zoom account to the web application and agree to the necessary terms, consenting that their clinical interview recordings may be processed by the DepScope applicatoin. This is done through the Zoom Appli-cation Programming Interface (API). From there, any time a clinical interview concludes over Zoom, a webhook [56] is"}, {"title": "VII. CONCLUSION", "content": "This paper presents a novel machine learning architecture for diagnosing MDD from clinical interview recordings. We explored the integration of large language models and audio-visual features into a tri-modal architecture for this task. The proposed architecture was optimized through automated hyper-parameter tuning techniques and evaluated using the AVEC 2016 Challenge cross-validation split and LOSOCV. We drew comparisons between the performance of the proposed ar-chitecture and previously developed models, finding that it outperforms all baseline models and multiple state-of-the-art models. The model was integrated into a locally hosted web application to emulate how it may be used in the real-world.\nThough the proposed model achieved impressive results, the current architecture faces multiple limitations. The speed of the model suggests that it is not appropriate for real-time applications and may only be effective when integrated with batch-based data processing techniques. Due to the use of large language models in this architecture, it is highly unlikely this can be resolved. The dataset used in this study, the DAIC-WOZ, is small and homogenous. We believe this has negatively impacted the generalizability of the proposed model and may have resulted in metrics which do not reflect the effectiveness of the model in real-world settings. The lack of high-quality data available for this task is an issue which plagues the field as the whole.\nIn future work, we aim to further optimize the proposed architecture for increased accuracy and make the model easily accessible to use. As new large language models are de-"}]}