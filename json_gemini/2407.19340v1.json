{"title": "Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification", "authors": ["Santosh V. Patapati"], "abstract": "Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models. In Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score of 85.95%, a precision of 80%, and a recall of 92.86%.", "sections": [{"title": "I. INTRODUCTION", "content": "MAJOR Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide [1]. The COVID-19 pandemic has further exacerbated this issue, leading to a staggering 27% increase in the global prevalence of MDD [2].\nUnlike biological disorders, MDD is not diagnosed through blood tests or imaging. It lacks these gold standards. Instead, the most common approach for MDD diagnosis relies on clinical interviews [3] and self-reported inventories (SRIs) [4], [5]. These systems have been subject to criticism for their subjectivity [6]\u2013[8], which gives way for a number of issues. Bias, for example, can come from both parties. In clinical interviews and SRIs, patients may exaggerate or under-report symptoms in social desirability bias [9], [10]. Confirmation bias affects how clinicians weigh a patient's symptoms against the assigned criteria [11]. As a result, the misdiagnosis rate of MDD is estimated to be as high as 54% [12].\nIn recent years, there has been a growing interest in the use of machine learning (ML) systems to automatically assess the presence and severity of MDD. This offers a low-cost and objective alternative to current methods. A heavily researched use case of ML involves making diagnoses from clinical interview recordings.\nNumerous studies investigate a multi-modal approach, which combines both verbal and non-verbal cues to reach a diagnosis. These models achieve extremely high levels of success. Currently, the majority of state-of-the-art models take in three modes of input: Audio, video, and text-based data.\nThe text-based modality is generally seen as the weakest link in diagnosing MDD from clinical interviews [13]. This can be attributed to the scarcity of task-specific text-based training data. This lack of data hinders the effective training of NLP models, which are data-intensive, resulting in suboptimal performance compared to the audio and video modalities.\nTo date, no research has attempted to integrate Large Lan- guage Models (LLMs) into a multi-modal architecture for this task. Due to their training on large corpora, we hypothesized incorporating LLMs into such an architecture would improve the accuracy of depression diagnosis from clinical interview recordings and alleviate the issue of scarce training data.\nIn this work, we make the following contributions:\n1) We propose a novel, tri-modal architecture that utilizes LLMs. This is the first work to incorporate LLMs into a multi-modal framework for this task.\n2) We demonstrate that LLMs are effective for mental health diagnosis when incorporated into multi-modal ar- chitectures. The proposed architecture outperforms base- line and state-of-the-art models tested on the AVEC 2016 Challenge dataset and Leave-One-Subject-Out Cross- Validation. The results of this work serve as an indicator for the potential of such an architecture in classifying depressed patients.\n3) The proposed architecture is integrated into a locally hosted web application to emulate the potential use of such a model in the real-world."}, {"title": "II. RELATED WORKS", "content": "Speech data has been found to be highly effective, more so than video and text-based data, for diagnosing depression from clinical interview recordings. In fact, multiple mono-modal audio-based models have achieved results comparable to SRIs for diagnosing a variety of psychiatric disorders, such as Generalized Anxiety Disorder (GAD) [14]\u2013[16].\nCurrent audio-based diagnosis methods rely on low and high level features extracted from raw audio data. The extracted features are passed as input into deep learning models.\nMany representations of audio data have been utilized for this task. (Wang et al.) [17] evaluated and compared the effectiveness of such representations, finding Mel Frequency Cepstral Coefficients (MFCCs) to be the best performing audial feature for depression diagnosis of the 30 features which were tested in the study.\nDerived using the Mel Scale [29], MFCCs provide a com- pact representation of the human perception of sound. This allows for the low-level representation of timbre, pitch, and rhythm. Studies indicate such values are different between depressed and non-depressed patients [30]. (Wang et al.) [31] and (Taguchi et al.) [32] found certain MFCCs follow a consistent pattern that is only present in depressed patients. Currently, MFCCs are the most used feature across state- of-the-art models to represent the audio mode for this task."}, {"title": "B. Facial Action Units in Video-Based Models", "content": "Research has shown facial expressions and movements are significantly different in depressed patients as opposed to non-depressed ones [33] [34]. Visual-based depression diagnosis models rely on features extracted from patient videos and images.\nFacial Action Units (FAUs), a low-level representation of the movements of specific facial muscles, were introduced as part of the Facial Action Coding System [35]. FAUs allow us to objectively study facial expressions and decipher non-verbal cues (Figure 3). This makes them highly effective and popular for affective computing tasks [36]. In this study, a subset of 20 out of 44 total FAUs are used, as shown in Table IV."}, {"title": "C. Multi-Modal Models", "content": "Multi-modal models combine both verbal and non-verbal cues to reach a diagnosis. To our knowledge, the majority of state-of-the-art models for depression diagnosis from clinical interview recordings consider at least audio and video data in making a diagnosis."}, {"title": "1) Data Fusion Strategies", "content": "The way in which the modal- ities are fused is referred to as the fusion strategy (Figure 2. Two basic fusion strategies which are highly prominent in machine learning models are Early Fusion and Late Fusion [38].\n\u2022 Early Fusion: Representations of different modalities are concatenated before being fed into a neural network. For example, one work combines unprocessed audio and text features before passing them into a Support Vector Machine. However, this method can produce a high- dimensional feature representation, resulting in the 'curse of dimensionality', where the volume of space increases exponentially and training data becomes sparse [39].\n\u2022 Late Fusion: Individual mono-modal models are trained for their respective modalities and their outputs are con- catenated right before the final decision. (Samareh et al.) [40] employs Late Fusion for MDD diagnosis by evaluating outputs from mono-modal video, audio, and text-based models using Random Forests. When Late Fusion techniques are used, however, deeper complexities and relationships between the different modalities are lost.\nModel-Level Fusion is a combination of these two strate- gies. In Model-Level Fusion, individual modalities are pro- cessed before concatenation. After concatenation, the data is processed even further to reach an output. This mitigates the curse of dimensionality and issue of uncaptured complexities between modalities. It allows patterns within a single modality and relationships across modalities to be learned effectively. For this reason, it is the most commonly used and best per- forming fusion method for multi-modal depression diagnosis from clinical interviews."}, {"title": "III. DATA COLLECTION AND PREPROCESSING", "content": ""}, {"title": "A. DAIC-WOZ", "content": "The Distress Analysis Interview Corpus Wizard of Oz (DAIC-WOZ) contains data collected from 189 clinical in- terviews ranging between 7-33 minutes [41]. Each interview contains a 16kHz .wav recording, a .CSV text transcript, an ID numbered 300 492, and features collected from video data. To keep data de-identified, raw video data is not made available in the DAIC-WOZ. Instead, the dataset provides six visual features extracted in 30 frames per second using the OpenFace pose estimation library [42]. The extracted visual features are as follows: 68 2D Facial Landmarks, 68 3D Facial Landmarks, Histogram of Oriented Gradients, Facial Action Units (FAUs), 4 Gaze Vectors, and Head Pose. FAUs are the only visual feature considered in this study.\nIn the DAIC-WOZ, depression was assessed using an SRI: Patient Health Questionnare-8 (PHQ-8). The PHQ-8 measures 8 items on a scale of 0 to 3 based on how often problems have been occuring over a two-week period. The final output is a binary label and a score measuring the severity of MDD on a scale between 0-24. A cut-off score of 10 (inclusive) was used for the binary label, where participants scoring higher were considered depressed. These classifications were used to develop, train, and evaluate the proposed architecture."}, {"title": "B. Dataset Errors", "content": "The DAIC-WOZ dataset contains many errors that needed to be resolved before the regular preprocessing steps could be applied. The following errors were resolved:\n1) All audio files contained interactions between research assistants prior to the interview starting. These pieces of audio were identified and removed.\n2) The interviews with IDs of 373 and 444 contained long interruptions which had to be removed. One such interruption, for example, was the phone of a research assistant ringing.\n3) The interviews with IDs of 451, 458, and 480 were missing the utterances of the therapist. This issue was resolved by manually transcribing the therapist speech.\n4) The interviews with IDs of 318, 321, 341, and 362 contained transcription files which were out of sync with the audio. This was resolved by manually adjusting the timestamps of the affected utterances.\n5) The interview with ID 409 contained a labeling error, where the PHQ-8 score was 10 but the binary label was 0. This was manually resolved."}, {"title": "C. Text Preprocessing", "content": "The DAIC-WOZ contains transcripts as tab-separated .CSV files with the following columns: speaker, start_time, stop_time, and value. The provided utterances contained nu- merous errors that needed to be fixed. Preprocessing was applied using RegEx and NLP techniques to make text suitable to pass into the LLM. Refer to Table II for a sample of a transcript before pre-processing is applied.\nA major issue in the unprocessed transcripts is the presence of unique identifiers within utterances, as seen in Table II. In the original DAIC-WOZ dataset, all transcriptions of the clinical interviewer were automatically generated for partic- ipants whose IDs were greater than 363. In such cases, a unique identifier is followed by the true text in parenthesis. These identifiers are heavily present in Table. RegEx was applied to identify and replace unique identifiers with their true meanings.\nIn the DAIC-WOZ, acronyms which are pronounced letter by letter are connected by underscores, e.g. 'l_a' represents 'Los Angeles'. A Python dictionary was generated, where the keys were such acronyms and the values were their extended form. All acronyms were replaced with what they represented.\nIf speech is cut-off, the full word is followed by what was actually pronounced, enclosed in inequality signs. Non-speech, such as coughing, is enclosed in inequality signs as well. These issues were resolved by simply removing text that followed this pattern using RegEx.\nThe use of proper grammar in LLM prompting can increase classification accuracy, but the transcripts provided in DAIC- WOZ contain major grammatical errors. To resolve this issue, the 't5-base-grammar-correction' model [43] was applied to all text through the Hugging Face platform. The model ensured the proper use of punctuation and capitalization in every ut- terance. However, the model is limited in its ability to identify interrogative sentences and apply punctuation accordingly. A Support Vector Machine was trained to identify if utterances are interrogative and to apply the appropriate punctuation with an accuracy of 97%.\nThe names of diarized speakers were adjusted. 'Ellie' was replaced with 'Therapist' and 'Participant' was replaced with 'Patient'. Using such naming conventions gave the LLM a better understanding of a conversation's context and the speakers' respective roles.\nFollowing all corrections, subsequent utterances by the same speaker were merged together. Refer to Table III for a sample of a fully adjusted transcript.\nThe resulting .CSV files were converted into raw text data which could be fed into the LLM. In this raw text format, subsequent utterances were separated by a new-line. Speakers were diarized by preceding utterances with the speaker name, followed by a colon (e.g., 'Therapist: So how are you doing today?')."}, {"title": "D. Audio Preprocessing", "content": "Audio preprocessing followed four stages. (1) Audio was segmented to only include patient speech. (2) Audio data was augmented to increase the number of total samples by a factor of seven. (3) MFCCs were extracted using Librosa [44]. (4) The MFCCs were normalized using Cepstral Mean and Variance Normalization.\n1) Audio Segmentation: In the proposed model, as well as past research, the audio modality is incorporated by analyzing patient responses to questions. Patient speech was separated from therapist speech by slicing audio according to the times- tamps provided in the transcript, which are detailed down to the centisecond. The cropped therapist audio was discarded and not used as input into the final architecture.\nThe remaining audio data was separated into smaller chunks to make it suitable for input into Bidirectional Long Short Term Memory (BiLSTM) layers. It was sliced into 8-second segments, where any remaining segment under 8 seconds was discarded.\nAs first done in (Muzammel et al.) [13], audio data was augmented through pitch shifting and noise injection strategies to prevent over-fitting and data scarcity. In this work, we combined and randomly applied the following augmentation strategies to increase sample size seven-fold:\n\u2022 Pitch Shifting: Pitch was adjusted by N.N, N.N, and N.N in semitones.\n\u2022 Noise Injection: A NumPy array with randomly assigned values was overlaid onto audio to create noise.\n2) Audio Feature Extraction: MFCCs were extracted from raw audio data using the Librosa Python library. 60 MFCCS were extracted from 124ms windows with a 92ms overlap. There were 15,420 total coefficients for every 8-second audio segment. Figure 4 displays the pipeline used to derive the MFCCs.\n3) MFCC Normalization: Rather than normalizing raw audio data, the extracted MFCCs were normalized using Cep- stral Mean and Variance Normalization (CMVN). MFCCs are highly sensitive to additive noise and differences in recording conditions. CMVN solves the issue of signal variations due to different recording conditions in MFCCs by normalizing the spectrum to have zero mean and unit variance. Past research has shown applying CMVN to MFCCs greatly improves the performance of speech recognition algorithms [46]."}, {"title": "E. Visual Preprocessing", "content": "Visual preprocessing followed two stages. (1) Video data was segmented to only include data from when the patient is speaking. (2) Individual columns were normalized for zero mean and unit variance.\n1) Video Segmentation: In the proposed architecture, we analyze patient visual data only while they are speaking, not in all parts of the interview. Using the timestamps provided in the transcript, FAU data was cropped to exclude and discard segments where the therapist is speaking, or where nobody is speaking at all. The data was then sliced into 8-second segments in such a way that it aligned with the segmented audio data. Any remeaining segment under 8 seconds was discarded. After video segmentation was applied, there were an equal number of video and audio samples for each interview.\n2) FAU Normalization: Normalization was applied to con- tinuous, non-discrete FAU values. Namely, numbers 1-14 in Table IV. The continuous FAU values were scaled to zero mean and unit variance respectively."}, {"title": "IV. MODEL DEVELOPMENT", "content": ""}, {"title": "A. Text-Based Model Development", "content": "Two-shot learning was employed with GPT-4 through the OpenAI API to reach a classification given full text transcripts from the DAIC-WOZ.\nThe model was given the prompt 'Take on the role of an expert in psychiatric diagnosis using the DSM 5. Read the following transcript and determine if the patient has depres- sion.' This prompt is a modified version of that presented in (Galatzer-Levly et al.) [47]. Given this prompt, GPT-4 was expected to output a binary classification of 'depressed' or 'not depressed' in JSON format. The function calling feature provided in the OpenAI API was manipulated to force an output in the desired JSON format in almost all cases."}, {"title": "B. Tri-Modal Model Development", "content": "Model development followed two stages. (1) The model architecture was optimized with the Hyperband Tuning Algo- rithm [48] in Keras-Tuner [49]. (2) The model parameters were reset and the optimized architecture was evaluated through Leave-One-Subject-Out Cross-Validation (LOSOCV).\n1) Hyperparameter Tuning: The model was trained with the Adam optimizer [50] and binary cross entropy as the loss function. To account for the class imbalance of the DAIC- WOZ, the loss function was weighted to give higher value to the depressed class. To achieve this functionality, part of the Keras-Tuner Python library source code was rebuilt.\nThe tuner was set to focus on validation loss as the only metric to gauge configuration performance. Through each bracket, the number of total epochs run and models tested was reduced by a factor of three. A callback was created to end training and move on to the next configuration if validation loss did not improve over three consecutive epochs. Two iterations of the full Hyperband algorithm were run.\n2) Evaluation: LOSOCV (Figure 6) was applied by first re- setting the optimized architecture's trainable parameters which had been adjusted during hyperparameter tuning. A unique model was trained for every clinical interview in the dataset (n=180). For each model, all samples originating from one clinical interview were excluded from the training dataset. Each model was tested on the excluded clinical interivew, and all the model results were pooled for the final performance calculation. Augmented samples were not included in testing data."}, {"title": "C. Proposed Architecture", "content": "The final model architecture (Figure 5), which has been optimized by the Hyperband Tuning Algorithm, follows 7 steps to reach a classification.\n1) The model extracts higher level features from the MFCCs in three consecutive blocks consisting of BiL- STM, Dropout, and Batch Normalization layers.\n2) The input FAU data follows three processing steps to make it suitable for concatenation with the MFCC ten- sor: (a) The FAU data is first flattened into a 1D tensor. (b) The Keras [53] Repeat Vector layer repeats the flattened data for each timestep present in the processed MFCC tensor, resulting in a 3D tensor. (c) Lastly, the Time Distributed operation applies a Dense layer to each temporal slice, where the number of neurons is the number of feature dimensions in the MFCC tensor. These operations result in a higher-level representation of the FAUs with the same dimensions as the processed MFCC data, allowing concatenation in the following step.\n3) The processed FAU and MFCC data are concatenated along the features axis.\n4) The concatenated data is processed by consecutive BiL- STM, Dropout, and Batch Normalization layers.\n5) The same sequence of operations applied in step two is applied to the LLM binary output to prepare it for concatenation with the MFCC-FAU tensor.\n6) The MFCC-FAU and LLM tensors are concatenated.\n7) The combined tensor is processed by four consecutive Leaky ReLU-activated Dense layers and one sigmoid- activated layer. A binary classification is output."}, {"title": "V. RESULTS", "content": "The proposed architecture was evaluated in two ways:\n1) The model was evaluated using the DAIC-WOZ AVEC 2016 Challenge cross-validation split to allow compara- bility with models from other studies.\n2) The proposed architecture was evaluated through LOSOCV.\nThe proposed model outperforms all baseline models and multiple state-of-the-art models which were evaluated using a regular cross-validation split as well as LOSOCV. It performs well on both the depressed and non-depressed classes, with accuracies of 92.9% and 90.2% on LOSOCV for the depressed and non-depressed classes, respectively. The large difference in the accuracy between the depressed and non-depressed classes (92.9% and 81.81%, respectively) on the AVEC 2016 cross- validation split can be attributed to the extremely small sample size of just 14 for the depressed class. We believe the accuracy of 92.9% for the depressed class on the AVEC 2016 Challenge cross-validation split is inflated as a result of this sample size."}, {"title": "A. Computational Complexity", "content": "The speed of the proposed architecture was evaluated on the 12-minute 50-second DAIC-WOZ clinical interview with an ID of 301. The test was conducted using an Intel i7-12650H @ 2.30GHZ processor, NVIDIA RTX 4060 graphics card, and 64GB of RAM. The model took 2.67 seconds to process the interview, including GPT-4 computation time through the OpenAI API."}, {"title": "VI. DEPLOYMENT", "content": "The proposed architecture was integrated into a locally hosted web application, DepScope, for easy and accessible use of the model by clinicians. This application mimics how such a model may be implemented into real-world scenarios. Clinicians first connect their Zoom account to the web application and agree to the necessary terms, consenting that their clinical interview recordings may be processed by the DepScope applicatoin. This is done through the Zoom Appli- cation Programming Interface (API). From there, any time a clinical interview concludes over Zoom, a webhook [56] is sent to the DepScope backend. The recording is then retrieved from the Zoom servers for preprocessing, model inference, and clinical report generation via GPT-4. Clinical reports contain a summary of the interview, detailing all key points and a justification for why the diagnosis was drawn (Figure 10). Reports additionally contain the confidence of the model in the classification, which is output by the sigmoid activation function. Reports populate the clinician dashboard (Figure 9)."}, {"title": "VII. CONCLUSION", "content": "This paper presents a novel machine learning architecture for diagnosing MDD from clinical interview recordings. We explored the integration of large language models and audio- visual features into a tri-modal architecture for this task. The proposed architecture was optimized through automated hyper- parameter tuning techniques and evaluated using the AVEC 2016 Challenge cross-validation split and LOSOCV. We drew comparisons between the performance of the proposed ar- chitecture and previously developed models, finding that it outperforms all baseline models and multiple state-of-the-art models. The model was integrated into a locally hosted web application to emulate how it may be used in the real-world.\nThough the proposed model achieved impressive results, the current architecture faces multiple limitations. The speed of the model suggests that it is not appropriate for real- time applications and may only be effective when integrated with batch-based data processing techniques. Due to the use of large language models in this architecture, it is highly unlikely this can be resolved. The dataset used in this study, the DAIC-WOZ, is small and homogenous. We believe this has negatively impacted the generalizability of the proposed model and may have resulted in metrics which do not reflect the effectiveness of the model in real-world settings. The lack of high-quality data available for this task is an issue which plagues the field as the whole.\nIn future work, we aim to further optimize the proposed architecture for increased accuracy and make the model easily accessible to use. As new large language models are de-"}]}