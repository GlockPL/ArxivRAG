{"title": "User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance", "authors": ["Mrinal Verghese*", "Brian Chen*", "Hamid Eghbalzadeh", "Tushar Nagarajan", "Ruta Desai"], "abstract": "Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches - Socratic Models [46] and Vision Conditioned Language Models (VCLMs) [31] on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we conduct a first-of-its-kind user study, with 18 participants performing 3 different multi-step cooking activities while wearing an egocentric observation device called Aria [37] and following assistance from multimodal LLMs. We find that the Socratic approach outperforms VCLMs in both offline and online settings. We further highlight how grounding long visual history, common in activity assistance, remains challenging in current models, especially for VCLMs, and demonstrate that offline metrics do not indicate online performance.", "sections": [{"title": "1. Introduction", "content": "Imagine a vision-powered assistant capable of empowering its users in multi-step daily activities like cooking, assembling, etc. by detecting mistakes and recommending corrections. Two fundamental capabilities of such an assistant are a) the ability to understand task-relevant steps and progress accomplished by the user from the past visual observations e.g., video [33] and b) the ability to recommend the next actions the user should take by forecasting and planning of future actions [10, 15]. In addition to encoding history and forecasting, such assistants must also account for the user in the loop and re-plan on the fly to en-"}, {"title": "2. Related Work", "content": "LLMs for multimodal reasoning. Inspired by the capabilities of LLMs, Zeng et al. [46] pioneered the Socratic approach to leverage LLMs for multimodal reasoning. Recently, Palm [16] and AntGPT [48] have employed a similar approach using LLMs to anticipate future actions in videos. Specifically, they transform videos into text using narration models like BLIP-2, action recognition models [48], or a combination of the two [16]. The LLM is then used to model the text sequence to predict future action in a sentence completion fashion [34]. However, these approaches have only been evaluated on offline video datasets. In this work, we evaluate such models in an online manner by deploying them in a real-world assistance setting.\nVision-conditioned Language Model (VCLM). Instead of a text-based representation of vision modality for LLM-based reasoning in vision tasks, another approach is to have a unified model that combines visual and linguistic information by aligning these modalities. Examples of these models include Flamingo [1], OpenFlamingo [4], Palm-E [12], BLIP-2 [21], InstructBLIP [9], LLaVA [27], IDEFICS [19], MiniGPT-4 [51] and many more [13, 14, 20, 28, 38, 45]. These models are generally fine-tuned using large-scale datasets containing multimodal data [23, 27] and are evaluated on image captioning [44] and visual question answering (VQA) tasks [2, 3, 11].\nBuilding on these, VideoLLM [7], AnyMAL [31], VideoChat-Embed [22], Video-ChatGPT [29], Video-LLaVA [24], LLaVA-NeXT [26] and Video-LLaMA [8,47] fine-tuned VCLMs for a set of video tasks. A nuanced overview of these models can be found in Tab. 12 in Appendix 10. Keeping the sparsity of available annotated data in assistance scenarios in mind, we specifically focus on few-shot VCLMs without any task-specific finetuning for future action prediction in videos. Note that various other multimodal multitask transformer models such as GATO [35] operate on multimodal tokenized input/output from various modalities such as text, image, video, robot actions, etc., for planning. However, we focus on multimodal models that use LLMs as a backbone."}, {"title": "3. Multimodal LLM Approaches", "content": "Activity assistance requires grounding information from untrimmed video history for future action prediction. If the visual history is appropriately represented, the future action prediction task can be framed as a sentence completion task using an LLM [40, 41]. We evaluate two predominant approaches to represent visual history for such reasoning with LLMs to predict future actions. Figure 2a shows an overview of our Socratic and VCLM models.\nSocratic model. The main idea behind the Socratic approach is to use pretrained vision-language models (VLMs) to convert non-textual modalities into text for a downstream LLM. One could extract and represent different task-relevant information from the untrimmed video history as text, such as objects, actions, and open-set narrations describing the events in the video. We find that such narrations tend to be a superset of various contextual information that could be extracted from video, including objects and actions. Appendix 9 provides this quantitative comparison for LTA. Instead of using objects, actions, and narrations, we choose only narrations from a video narration model to represent visual history as text in our Socratic models, akin to various existing works [16, 46]. As an example, Socratic models would represent visual information corresponding to visual history in a curry-making activity such as \u201cAdd oil in pan\", \"Add onions in pan\" etc.\nVision-conditioned language models (VCLMs). These models embed the visual modality as continuous tokens that can be passed as input to an LLM along with text tokens. A linear [27, 31] or a non-linear projection layer [33] is fine-tuned to align these continuous tokens with the embedding space of text tokens for a given LLM. Finally, the LLM backbone is often fine-tuned on a multimodal instruction dataset. [8, 24, 26, 31] Thus, unlike Socratic Models, VCLMs can process both embedded visual information and text. Such implicit representation may allow models to capture fine-grained visual information e.g., \"state of the fried onions\" while making a curry, which might decide if the user should stir more or add the next ingredient.\nVCLMs typically split up the available tokens in their input context to their inbuilt LLM into continuous embeddings and text tokens, with continuous embeddings coming from visual encoders. The encoders in current SOTA VCLMs use limited and uniformly sampled frames from input videos for video tasks. Most VCLMs process between 8 to 16 frames [8, 24, 26] which may be ineffective in our benchmarks which require grounding on average >500 frames corresponding to multiple task-relevant steps. To ensure that VCLMs could be applied to our benchmarks, we use both text tokens and continuous embeddings to encode the visual history in our VCLMs.\u00b9 Appendix 9 shows an ablation comparing VCLMs that only use continuous embeddings for the encoding history with those that use both continuous embeddings and text tokens in LTA."}, {"title": "4. Offline Benchmarks", "content": "Our goal is to make progress toward vision-powered assistants that can reason about their user's context from visual input, such as the user's progress in daily activities, and provide relevant recommendations on future actions. Various action anticipation benchmarks previously proposed by the research community also require such reasoning capabilities. Hence, we choose them to evaluate the two prominent categories of SOTA multimodal LLM approaches."}, {"title": "4.1. Benchmark Tasks", "content": "While a plethora of video-based action anticipation benchmarks exist [32], we choose two representative ones such that they cover the space of medium to long visual history and medium to long forecasting horizon \u2013 the settings closest to activity assistance in real-world vision-powered systems2 Specifically, we choose Long-term action Anticipation (LTA) from Ego4D [15], and Visual Planning for Assistance (VPA) task on the CrossTask dataset from [33]. We blurred faces from the CrossTask videos prior to use.\nAs summarized in Table 1, LTA focuses on predicting a sequence of future actions with a length of $Z = 20$ after grounding a long untrimmed visual history corresponding to approximately 8 or more actions. Compared to LTA, VPA on CrossTask operates on a medium-range untrimmed visual history corresponding to 3-4 actions for medium-horizon forecasting of $Z = 3 ~ 4$ future actions. While not part of the original benchmark, we also look at LTA with $Z = 5$ to help disambiguate the challenges of long-history and long-horizon in prediction. The output is mapped to a closed set of verbs, nouns, and actions, i.e., (verb, noun) pairs in each benchmark. We use the same evaluation metrics as were proposed by the original benchmarks for consistency with prior work. The predicted action sequences in LTA are evaluated using the edit distance. VPA is evaluated with action prediction accuracy at each step (mean accuracy), order-agnostic mean Intersection over Union (mIoU), and a strict order-respecting metric Success Rate for the predicted sequence (defined as in [33])."}, {"title": "4.2. Experiment Setup", "content": "Figure 2 shows an overview of Socratic and VCLM models used in our experiments. Both our Socratic and VCLM implementations use the same video narration model to en-"}, {"title": "5. User-in-the-loop Evaluation", "content": "Our benchmarking experiments on LTA and VPA highlight the strengths and weaknesses of VCLMs and Socratic approaches to predict future actions based on video history. However, it is unclear how these actions would manifest with a user in the loop and whether these actions \u2013 when executed - would successfully complete real-world activities. To that end, we conduct an online evaluation of VCLMs and Socratic approaches in real-world assistive scenarios. We recruit 18 participants to perform multi-step cooking activities while wearing an egocentric observation device called Aria [37] and following assistance from one of these models. We measure the true activity completion success rate, which is difficult to measure offline, and the correctness of recommended actions using mean IoU."}, {"title": "5.1. Study Design", "content": "Multi-step activities. We choose three cooking activities for our study: 1) espresso latte, 2) caprese salad, and 3) BLT (bacon lettuce and tomato) sandwich. These activities consist of a variety of ingredients, including meat, vegetables, breads, and liquids, and require different types of actions, including pouring, chopping, spreading, and plating. Furthermore, these activities also have some ordering constraints among the steps. For instance, the milk needs to be frothed before pouring into the espresso for making latte and the BLT ingredients need to be stacked on the bread before closing the sandwich. Lastly, we account for the ease of doing these activities in an office kitchen, which required omitting really long activities or activities using a stove, oven, etc.\nStudy protocol. Each participant performs two of the three aforementioned activities instructed by either a VCLM or a Socratic model. We use Latin-square counterbalancing for the ordering of multimodal LLM that offers them assistance as well as the type of activity that they perform across participants to reduce learning effect [17].\nEach activity entails a script that a participant can follow. Full scripts of the three activities can be found in Appendix 11. Each activity script is split into two phases \u2013 1) Partial progress: In the initial phase of the activity, participants are asked to make partial progress in the activity by completing a set of steps in any feasible order. For example, in the caprese-making activity, participants are instructed to slice tomatoes and mozzarella, tear basil, and place tomato slices on the plate. They are also free to slice varying amounts of these ingredients in whatever manner they like e.g., small vs. large slices. 2) Assistance evaluation: In this phase of the activity, the multimodal LLM assistant engages and guides the participant through completing the remaining steps. Participants iteratively request the next task step from the assistant and then execute what the assistant asks them to do to the best of their ability. Figure 3 gives an overview of these phases for the caprese-making activity. Participants may skip recommended actions from the assistant that are infeasible, irrelevant, or already completed. Actions are skipped solely at the participant's dis-"}, {"title": "5.2. Real-world Deployment of Multimodal LLMS", "content": "We frame the multimodal LLM assistance in our online study after the VPA task [33] used in our offline benchmarking experiments (Sec. 4), since we believe its definition is closest to vision-based assistants. Following VPA, the VCLMs and Socratic models are given an untrimmed and unsegmented egocentric video stream from the partial progress phase of the activity. This usually corresponds to 3-5 high-level actions on average. We also provide the models with a natural language goal describing the activity, as in VPA. The models are then prompted to iteratively output single-step action predictions to guide the user through the remaining 2-3 steps in the assistance evaluation phase of each activity. Akin to our offline benchmarking, the models in our study did not have access to the activity scripts, nor had they seen the kitchen environment where the ex-"}, {"title": "5.3. Quantitative Results", "content": "The Socratic appraoch outperforms VCLMs at user-inthe-loop activity assistance. Table 4 shows the results of our online study. Akin to our offline experiments that showed the Socratic approach outperforming VCLMs in LTA (Table 2) and demonstrate competitive performance with VCLMs in VPA (Table 3), we find that the Socratic approach enables higher activity completion success rate as well as mIoU across the 18 participants and 3 activities in our online study. Note that our online Socratic model does not leverage finetuned video narration models like our offline experiments. Nevertheless, it exhibits superior performance. Despite the low overall success rate of both models, in 40% of successful trials, the Socratic model enabled a user to complete a task they had not previously done.\nOffline metrics do not capture online performance. The success rate of activity completion cannot be truly measured in offline datasets. However, it is unclear whether other metrics used for evaluating video-based action anticipation and planning such as mIoU and edit distance (Sec. 4) translate from offline settings to online settings. Despite being small scale as compared to datasets in our offline benchmarking experiments, the video data from our online experiments enable a unique opportunity to compare these metrics head-on in both online and offline settings. To this end, we rerun Socratic and VCLM offline in the videos from our study. Specifically, we provide the models with videos from the partial progress phase of the activities along with the activity goal, e.g., \"make Caprese salad with mozzarella, tomato, basil, olive oil\" following the VPA task (Sec. 4). The models are then prompted with few-shot examples, following our prompt setup from offline experiments, to predict n + 2 steps. Here, n is the expected number of steps remaining in the activity from the assistance evaluation phase of the activity. We observe higher mean IoU rates for both models when run offline compared to their mIoU when they provided user-in-the-loop assistance online. Furthermore, VCLM outperforms Socratic in offline mIoU. However, it lags behind in both online mIoU and real-world success rates, indicating that offline mIoU may be an unreliable predictor of real-world performance. The gap between offline and online mIoU may partly be attributed to the single multi-step prediction of all the remaining actions in the offline setting versus iterative single-step predictions, i.e., with replanning in the online setting. The iterative singlestep predictions are more likely to make repeat suggestions, often due to grounding errors, which lead to a lower intersection between suggested steps and ground-truth steps."}, {"title": "5.4. Qualitative Analysis of Model Errors", "content": "Grounding errors, planning errors, and failure to detect activity end/success are the main failure modes. We also evaluate cases where a participant skipped actions recommended by the assistant. Recall that participants could skip action recommendations that were redundant, infeasible, or irrelevant. Appendix 12 shows a detailed analysis and breakdown of these reasons for skipped actions by the participants across the VCLM and Socratic models per activity. Such analysis enabled us to identify three main error modes \u2013 grounding errors, planning errors, and failure to detect activity end/success. Figure 4 shows these error modes for the espresso latte activity across all participants. Redundant skipped actions often correlate to grounding mistakes, where the models fail to recognize a step that has already been completed. Infeasible skipped steps also often correlate with grounding errors. Here, the models may suggest something that works for a different variation of the activity. For example, grinding coffee might work for a different version of a latte-making activity, but participants used an automated espresso machine in our study. These grounding errors are often more subtle than grounding errors from redundant steps. Finally, irrelevant skipped steps often correlate to planning errors, where the models suggest a step that is not part of the activity. We also find that in 50% of the successful activity episodes, the models fail to recognize when an activity is completed.\nOffline metrics don't capture error modes. The failure of models to detect activity end/success state does not affect the success rate metric, which would count such activity episodes as successful. However, it does lower mIoU scores due to redundant suggested actions. The overview of the participant steps for making a latte in Fig. 4 succinctly highlights the known issues with offline metrics. Specifically, mIoU as a permissive metric, would consider adding milk before steaming, a planning error, as a success. Conversely, offline success rate, being a restrictive metric, would discount 4 of the 5 present paths to success for making lattes as failures. Furthermore, mIoU and edit distance metrics do not capture optional actions sometimes suggested by such models \u2013 that do not affect activity success e.g., adding sugar. Grounding errors are the dominant mode of failure for models online. The bulk of the errors both models exhibit pertain to grounding. In particular, past participant actions are either not captured by the narrations or visual embedding of their activity history or are present in the long history but not attended to by the models during prediction, leading to grounding errors. We find that 63% of the skipped action recommendations are due to redundant action suggestions emerging from erroneous grounding (Appendix 12). The distribution of skipped actions is consistent across both models and indicates that recognizing previously completed actions in an activity is an ongoing challenge for these models. In contrast, both models make fewer planning errors, i.e., they suggest fewer irrelevant actions or actions with incorrect orderings. Overall, our analyses of skipped actions and errors in the study indicate that the primary challenge with visual assistants still lies in reasoning about activity progress and activity success/failure via grounding \u2013 more so than task knowledge or planning."}, {"title": "6. Conclusion", "content": "We evaluate the two predominant multimodal LLMbased approaches: Vision Conditioned Language Models (VCLM) and Socratic Models for vision-based activity assistance through two video-based action anticipation benchmarks on offline datasets and a real-world online study with 18 participants. To the best of our knowledge, our online evaluation is the first of its kind for multimodal LLMs towards real-world activity assistance systems. Our experiments show the Socratic approach is better equipped to capture coarse visual details across a long visual history. Current VCLMs can capture more fine-grained details but only for short visual history. Encoding long videos and aligning long videos with text tokens as needed by VCLMs would thus be rich avenues for future work. In the interim, Socratic models demonstrate competitive behaviors on videobased action anticipation and planning tasks spanning short to long visual history both offline and online.\nOur work sets important directions for future research on multimodal LLMs as vision-based assistants. Our online study highlights how grounding is the largest source of errors for these types of models. Grounding at different granularities remains an open problem, which, when improved, will greatly enhance activity assistance systems. Furthermore, we show how offline metrics do not provide a good indication of performance in online settings, demonstrating the importance of real-world evaluation of models for assistive scenarios."}, {"title": "7. Offline and Online Experiment Details", "content": "7.1. Offline Benchmark Tasks\nWe evaluate VCLM and Socratic models on two existing video-based forecasting benchmarks \u2013 Long Term Action Anticipation (LTA) [15] and Visual Planning for Assistance (VPA) [33] using offline datasets \u2013 Ego4D [15] and CrossTask [52] respectively (Sec. 4). Here, we provide a detailed overview of the datasets and our experimental setup for each of these tasks.\nEgo4D-LTA [15]: Ego4D consists of 3,670 hours of video footage of everyday activities, with 53 different scenarios. Out of this, we use the LTA (forecasting) subset, which entails 116 hours. This subset contains 1723 clips that cover an action space of 115 verbs and 478 nouns. We use the standard train and validation splits proposed by Ego4D [15] for our evaluation. In the LTA task, given 8 video segments from a video clip as input, the models must predict the 20 future actions in the form of verb, noun, and verb + noun, in correct order. Edit distance between the predicted sequence of actions and the ground truth action sequence in the video clip is used as a metric for evaluation following Ego4D [15].\nCrossTask-VPA [52]: CrossTask consists of 2.7K instructional videos for 18 different tasks from multiple domains, covering 374 hours of footage. Some of the action classes were shared among different tasks, with a total of 118 actions. Each video consists of an average 7.6 action steps. We follow [33] to construct a train split with 1,564 videos and a test split with 752 videos. We extract multiple test samples from each test video for VPA \u2013 specifically, given an annotated video consisting of K steps, we generate K-Z samples, leaving at least $Z = 3, 4$ steps to predict in the future. This leads to a dataset of 4123 test samples for our evaluation. Our VPA task definition also follows [33] - given an untrimmed video and a goal of the activity/task in the video described in natural language as input, the models must predict the up to 4 future actions in the form of verb+noun, in correct order. Evaluation compares the predicted action sequence with the ground truth actions in the video using mIoU, per step accuracy, and success rate metrics (Sec. 4)."}, {"title": "7.2. Model Modifications for Online Evaluation", "content": "Goal-Conditioned Summarization. Sec. 5.2 provides an overview of modifications for our multimodal LLMs to enable online evaluation. The online settings entail noisy stream of redundant video frames leading to long narration history. To handle such long narration histories in a robust manner, one of the biggest changes we make in these models is goal-conditioned summarization. This greatly reduces the number of tokens in the input and allows the language model to attend to a longer narration histories more robustly while still leveraging few-shot examples. The summarization is performed by LLama2-13B Chat using the following prompt:\nA person is currently attempting to [goal]. Their task is in progress and their goal is not yet complete. The following are low level narrations of their actions.\n[narration history]\nPlease summarize these into a smaller set of high-level narrations. Focus on narrations that are relevant to the goal and do not include irrelevant narrations in your high-level summary. Begin every high-level narration with the text, 'A person ':\n1. A Person\nThe output from the LLM is parsed by only keeping lines starting with numbers. The helps remove any conversation or filler language in the response. This summarized history often contains 5-20 high-level narrations and is used by the LLM to perform prediction. Few-shot examples are also summarized offline.\nGoal-Generation for Few-shot Examples. We evaluate the utility of goal conditioning in online experiments, akin to our offline experiments (Sec. 9.4). To that end, our pilot studies show that goal-conditioned prediction performs much better than prediction without goals in the online setting. Specifically, we find that goals help the LLM identify which parts of noisy input video stream and narrations are relevant to completing the activity. In order to ensure our few-shot examples to the multimodal LLMs, which are obtained from Ego4D, are appropriately goal conditioned for online experiments, we need to annotate these with goal information. Since, goal information is not available in Ego4D, we again resort to LLMs for obtaining pseudo goal labels for these videos. Specifically, we use Llama2-70B chat with the following prompt to generate goals for Ego4D LTA training set videos:\nThe user took these physical actions: [Narration History]\nWhat are the top 3 goals of the user?\nRespond only in JSON that satisfies the Response type:type ResponseList = [Response_1, Response_2, Response_3]\ntype Response = {user_goal: str;confidence: float;explanation: str;}\nProvide {user_goal} in the format of 'They wanted to {user_goal}', the {confidence) of the goal given the context (on a scale from 0 to 1), and a terse {explanation} of the given goal and its confidence."}, {"title": "8. Prompt Templates for LTA and VPA", "content": "Detailed prompt templates for our offline benchmark tasks LTA and VPA as shown in figures 5 and 6. The prompt for LTA (Fig. 5) consists of examples text narration sequences pertaining to the full video from 8 videos of the training set and the visual history of 8 segments from the current video. The narrations are from the LaViLa narration model [49]. Likewise, the prompt for VPA (Fig. 6) includes examples of full action sequences consisting of ground truth (GT) action labels for 8 videos from the training set and the visual history of the current video, which entails actions predicted following previous work [33] noted as [predicted action]."}, {"title": "9. Ablations on Visual History Representation", "content": "9.1. Evaluation of the benefit from implicit representation of visual information for smaller LLMs across different LLM sizes.\nTable 6 above shows that mAcc gap in VPA task for 7B models with and without visual conditioning at Z = 1,3,4 is 5.6%, 3.8%, and 3.2% respectively. The mAcc gap for 13B models at Z = 1,3,4 is 4.4%, 3%, and 4.3% and for 70B models is 0%, .3%, and 1.3% respectively as in Table 3. Implicit visual representation aids smaller LLMs across model sizes.\n9.2. Task-relevant information from visual history\nDifferent aspects of the visual history can be extracted and represented in text for VCLMs and Socratic models. It is unclear what aspects should be extracted to enable efficient forecasting in such models. To that end, we compare different modes of task-relevant information for Socratic multimodal LLMs on the Ego4D LTA task. Each of these modes of information can be obtained from different pre-trained vision-language models. Specifically, we consider information on objects, actions, and narrations describing activities in the video as the three relevant information modes.\nWe obtain object descriptions using Detic [50] with the Ego4D LTA noun set as a custom vocabulary, recognized actions using the LaViLa dual encoder with the Ego4D LTA closed-set of actions, and open-set narrations using the LaViLa narration model [49]. We test three settings i.e., combinations of these information modes: only narrations, narrations and objects, narrations and actions. The only narrations setting uses the same prompts as our Socratic model described in Sec 3. The narrations and objects setting prepends a list of recognized objects from the input video before the narrations in the visual history. The narrations and actions setting follows the same prompting structure as Palm [16]. All three settings use the same retrieval-based prompting approach as described in Sec 4.2. Action and object prompts are generated on the LTA train set.\nTable 7 shows the results for these three settings on Ego4D LTA. All models use Llama2-7B as the LLM. As seen in the table, neither adding object nor action information from the visual history noticeably improves performance on the LTA task. Following this result, we determine that object and closed-set action information is a subset of open-set narration information when it comes to visual history representation. Consequently, we use only narrations to represent visual history for both VCLM and Socratic models in all our offline and online experiments (Sec. 4, 5).\n9.3. Comparison of narrators for visual history\nVideo history might be sufficiently represented using open-set narrations that describe the activity in the video (Tab. 7) for video-based forecasting tasks. To determine an appropriate video narration model for our multimodal LLMs in forecasting tasks, we compare two SOTA video narrators \u2013 LaViLa [49] and the Blip-2 [21]. We perform this comparison using the Llama2-7B Socratic models on LTA (Table 8). Following Palm [16], we feed the median frame from a video segment along with the prompt \"A person is \" to the Blip-2 model for generating a narration describing the video segment. In contrast, the LaViLa narration model uses 4 evenly spaced frames. We parse the output of the narrator model by replacing references to the participant with \u201cA person\u201d to ensure consistent structure. Note that, LaViLa narrator uses GPT2-XL (1.5 B parameters) as its LLM backbone while Blip-2 uses OPT 2.7B. LaViLa is also explicitly trained on Ego4D to narrate video clips [49].\nAs seen in Table 8, despite its smaller language model backbone, LaViLa narrator significantly outperforms Blip2 for capturing relevant visual history for forecasting. This is likely due to LaViLa's narration-specific and egocentric training data, as well as consumption of 4 frames from the input video rather than just 1. Based on this analysis, we use LaViLa narrator for open-set narration generation of visual history for all our experiments unless otherwise specified.\n9.4. Medium history-medium horizon forecasting in VPA without goal\nOur offline benchmark tasks \u2013 VPA and LTA, cover the spectrum of medium to long forecasting based on medium to long visual history respectively. However, unlike LTA, the VPA task [33] also uses the goal of the activity in the video, in addition to the visual history, for forecasting future actions. To understand the performance of multimodal LLMs on medium history, medium horizon forecasting problems without the availability of goal information, we conduct an ablation on VPA. Specifically, we evaluate the best performing multimodal LLM \u2013 VCLM 70B on VPA with and without goal information (Table 9). We simply remove the goal information from the VPA prompt (Fig. 6) for this analysis.\nThe results show that the information regarding the goal enables the VCLMs to make better mid-horizon forecasting predictions while slightly decreasing the accuracy of short horizon predictions. Thus, the performance of multimodal LLMs may overall drop in medium history, medium horizon forecasting problems when goal information is not available. Since the availability of goal information leads to improved performance and since such information may be easy to obtain in user-in-the-loop settings, we frame our online evaluation using VPA's task definition i.e., with inclusion of goal."}, {"title": "9.5. Text-based history representation for VCLMs on long-history tasks", "content": "Since current VCLMs may be capable of encoding only limited visual history, we also provide text-based representation of history as input to VCLMs for our long history-based forecasting tasks e.g., LTA. We conduct an ablation experiment to determine the contribution of such additional text-based history representation when used along with visual embeddings in VCLMs. Specifically, we want to determine how VCLMs without text history perform in long-history tasks. We run this ablation with a Llama2-70B chat VCLM on LTA using the following prompt:\nPredict the next 20 actions in the form of (verb, noun)\nThe result in Table 10 shows that without a text-based history representation, the VCLM model fails to output meaningful predictions. Text-based history representation both provide a template for generation and help ground the VCLM to information not captured in its 8 input frames. We show examples of generated text with and without text history below.\nWith text history:\n//Generated text:\n16. A person drops a garlic peel in a bowl\n17. A person presses a garlic clove with the knife\n18. A person presses a garlic clove with a knife\n19. A person drops the garlic clove in the bowl\nWithout text history:\n//Generated text:\nPairs e.g. (drive, road) 1. Put CDs on top of magazines 2. Paste pictures butterflies and birds near flowers 3. Glue colored paper sequins on the bench in front of the boat\n//Generated text:\npairs. 1. tv, eyes 2. jako the bird is used."}, {"title": "10. Overview of existing VCLM and Socratic Models", "content": "Tables 11 and 12 show how our implemented Socratic and VCLM models compare to other models within each approach. Socratic models differ in what kinds of text the VLM's generate (actions, narrations, objects, etc), which VLMs are used, whether prediction is conditioned on an inferred goal, how prediction is performed, either by directly generating subsequent actions, or using chain of thought reasoning, and what LLM is used. We select our implemented socratic model by testing actions, narrations, and objects on the Ego4D LTA task (table 7). We found that adding actions or objects, provided by the LaViLa encoder [49", "50": "respectively, did not improve performance over open-set narrations provided by the LaViLa narrator model [49", "24": ".", "31": "."}, {"31": "as a representative VCLM method for our"}]}