{"title": "User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance", "authors": ["Mrinal Verghese", "Brian Chen", "Hamid Eghbalzadeh", "Tushar Nagarajan", "Ruta Desai"], "abstract": "Our research investigates the capability of modern multimodal reasoning models, powered by Large Language Models (LLMs), to facilitate vision-powered assistants for multi-step daily activities. Such assistants must be able to 1) encode relevant visual history from the assistant's sensors, e.g., camera, 2) forecast future actions for accomplishing the activity, and 3) replan based on the user in the loop. To evaluate the first two capabilities, grounding visual history and forecasting in short and long horizons, we conduct benchmarking of two prominent classes of multimodal LLM approaches - Socratic Models [46] and Vision Conditioned Language Models (VCLMs) [31] on video-based action anticipation tasks using offline datasets. These offline benchmarks, however, do not allow us to close the loop with the user, which is essential to evaluate the replanning capabilities and measure successful activity completion in assistive scenarios. To that end, we conduct a first-of-its-kind user study, with 18 participants performing 3 different multi-step cooking activities while wearing an egocentric observation device called Aria [37] and following assistance from multimodal LLMs. We find that the Socratic approach outperforms VCLMs in both offline and online settings. We further highlight how grounding long visual history, common in activity assistance, remains challenging in current models, especially for VCLMs, and demonstrate that offline metrics do not indicate online performance.", "sections": [{"title": "1. Introduction", "content": "Imagine a vision-powered assistant capable of empowering its users in multi-step daily activities like cooking, assembling, etc. by detecting mistakes and recommending corrections. Two fundamental capabilities of such an assistant are a) the ability to understand task-relevant steps and progress accomplished by the user from the past visual observations e.g., video [33] and b) the ability to recommend the next actions the user should take by forecasting and planning of future actions [10, 15]. In addition to encoding history and forecasting, such assistants must also account for the user in the loop and re-plan on the fly to ensure successful task execution. With the advent of various vision-language models powered by modern-day LLMs, a natural question is how these models fare towards the aforementioned capabilities. To that end, our work makes two contributions. First, we perform benchmarking on offline datasets to understand a) the effectiveness of different approaches for grounding visual history in multimodal LLMs and b) the forecasting capabilities of these approaches in short and long horizons. Second, we test these approaches in online settings to understand whether the performance in offline experiments translates to real-world assistive scenarios with a user in the loop. To the best of our knowledge, our work is the first to perform such an online evaluation.\nWhile a plethora of multimodal LLMs exist today [7, 8, 16, 22, 24, 26, 29, 31, 47, 48], they can be broadly differentiated into two categories based on their approach to grounding multimodal inputs \u2013 Socratic Models and Vision-conditioned Language Models (VCLMs). The Socratic approach [46] converts visual history into text using pre-trained VLMs such as action or object detectors or narration"}, {"title": "2. Related Work", "content": "LLMs for multimodal reasoning. Inspired by the capabilities of LLMs, Zeng et al. [46] pioneered the Socratic approach to leverage LLMs for multimodal reasoning. Recently, Palm [16] and AntGPT [48] have employed a similar approach using LLMs to anticipate future actions in videos. Specifically, they transform videos into text using narration models like BLIP-2, action recognition models [48], or a combination of the two [16]. The LLM is then used to model the text sequence to predict future action in a sentence completion fashion [34]. However, these approaches have only been evaluated on offline video datasets. In this work, we evaluate such models in an online manner by deploying them in a real-world assistance setting.\nVision-conditioned Language Model (VCLM). Instead of a text-based representation of vision modality for LLM-based reasoning in vision tasks, another approach is to have a unified model that combines visual and linguistic information by aligning these modalities. Examples of these models include Flamingo [1], OpenFlamingo [4], Palm-E [12], BLIP-2 [21], InstructBLIP [9], LLaVA [27], IDEFICS [19], MiniGPT-4 [51] and many more [13, 14, 20, 28, 38, 45]. These models are generally fine-tuned using large-scale datasets containing multimodal data [23, 27] and are evaluated on image captioning [44] and visual question answering (VQA) tasks [2, 3, 11].\nBuilding on these, VideoLLM [7], AnyMAL [31], VideoChat-Embed [22], Video-ChatGPT [29], Video-LLaVA [24], LLaVA-NeXT [26] and Video-LLaMA [8,47] fine-tuned VCLMs for a set of video tasks. A nuanced overview of these models can be found in Tab. 12 in Appendix 10. Keeping the sparsity of available annotated data in assistance scenarios in mind, we specifically focus on few-shot VCLMs without any task-specific finetuning for future action prediction in videos. Note that various other multimodal multitask transformer models such as GATO [35] operate on multimodal tokenized input/output from various modalities such as text, image, video, robot actions, etc., for planning. However, we focus on multimodal models that use LLMs as a backbone."}, {"title": "3. Multimodal LLM Approaches", "content": "Activity assistance requires grounding information from untrimmed video history for future action prediction. If the visual history is appropriately represented, the future action prediction task can be framed as a sentence completion task using an LLM [40, 41]. We evaluate two predominant approaches to represent visual history for such reasoning with LLMs to predict future actions. Figure 2a shows an overview of our Socratic and VCLM models.\nSocratic model. The main idea behind the Socratic approach is to use pretrained vision-language models (VLMs) to convert non-textual modalities into text for a downstream LLM. One could extract and represent different task-relevant information from the untrimmed video history as text, such as objects, actions, and open-set narrations describing the events in the video. We find that such narrations tend to be a superset of various contextual information that could be extracted from video, including objects and actions. Appendix 9 provides this quantitative comparison for LTA. Instead of using objects, actions, and narrations, we choose only narrations from a video narration model to represent visual history as text in our Socratic models, akin to various existing works [16, 46]. As an example, Socratic models would represent visual information corresponding to visual history in a curry-making activity such as \u201cAdd oil in pan\u201d, \"Add onions in pan\u201d etc.\nVision-conditioned language models (VCLMs). These models embed the visual modality as continuous tokens that can be passed as input to an LLM along with text tokens. A linear [27, 31] or a non-linear projection layer [33] is fine-tuned to align these continuous tokens with the embedding space of text tokens for a given LLM. Finally, the LLM backbone is often fine-tuned on a multimodal instruction dataset. [8, 24, 26, 31] Thus, unlike Socratic Models, VCLMs can process both embedded visual information and text. Such implicit representation may allow models to capture fine-grained visual information e.g., \"state of the fried onions\" while making a curry, which might decide if the user should stir more or add the next ingredient.\nVCLMs typically split up the available tokens in their input context to their inbuilt LLM into continuous embeddings and text tokens, with continuous embeddings coming from visual encoders. The encoders in current SOTA VCLMs use limited and uniformly sampled frames from input videos for video tasks. Most VCLMs process between 8 to 16 frames [8, 24, 26] which may be ineffective in our benchmarks which require grounding on average >500 frames corresponding to multiple task-relevant steps. To ensure that VCLMs could be applied to our benchmarks, we use both text tokens and continuous embeddings to encode the visual history in our VCLMs.\u00b9 Appendix 9 shows an ablation comparing VCLMs that only use continuous embeddings for the encoding history with those that use both continuous embeddings and text tokens in LTA."}, {"title": "4. Offline Benchmarks", "content": "Our goal is to make progress toward vision-powered assistants that can reason about their user's context from visual input, such as the user's progress in daily activities, and provide relevant recommendations on future actions. Various action anticipation benchmarks previously proposed by the research community also require such reasoning capabilities. Hence, we choose them to evaluate the two prominent categories of SOTA multimodal LLM approaches.\n4.1. Benchmark Tasks\nWhile a plethora of video-based action anticipation benchmarks exist [32], we choose two representative ones such that they cover the space of medium to long visual history and medium to long forecasting horizon \u2013 the settings closest to activity assistance in real-world vision-powered systems2 Specifically, we choose Long-term action Anticipation (LTA) from Ego4D [15], and Visual Planning for Assistance (VPA) task on the CrossTask dataset from [33]. We blurred faces from the CrossTask videos prior to use.\nAs summarized in Table 1, LTA focuses on predicting a sequence of future actions with a length of \\(Z = 20\\) after grounding a long untrimmed visual history corresponding to approximately 8 or more actions. Compared to LTA, VPA on CrossTask operates on a medium-range untrimmed visual history corresponding to 3-4 actions for medium-horizon forecasting of \\(Z = 3 ~ 4\\) future actions. While not part of the original benchmark, we also look at LTA with \\(Z = 5\\) to help disambiguate the challenges of long-history and long-horizon in prediction. The output is mapped to a closed set of verbs, nouns, and actions, i.e., (verb, noun) pairs in each benchmark. We use the same evaluation metrics as were proposed by the original benchmarks for consistency with prior work. The predicted action sequences in LTA are evaluated using the edit distance. VPA is evaluated with action prediction accuracy at each step (mean accuracy), order-agnostic mean Intersection over Union (mIoU), and a strict order-respecting metric Success Rate for the predicted sequence (defined as in [33]).\n4.2. Experiment Setup\nFigure 2 shows an overview of Socratic and VCLM models used in our experiments. Both our Socratic and VCLM implementations use the same video narration model to en-"}, {"title": "4.3. Quantitative Results", "content": "Text-based representation of visual history is more effective than implicit representation when encoding long visual history. We find that the Socratic approach outperforms VCLMs for predicting actions in Ego4D LTA irrespective of the LLM size (Table 2). Our results suggest that the visual embeddings used by VCLMs are less amenable for encoding long visual histories. Specifically, the implicit information contained in these visual embeddings does not add much beyond the text-based representation of visual history for future prediction tasks that require grounding longer visual histories. This finding is consistent across different future prediction horizons as highlighted by \\(Z = 5\\) and \\(Z = 20\\) results. Recall that irrespective of the history lengths, our Socratic and VCLMs have a fixed context window. While Socratic models use the entire context to encode text, VCLMs use 12.5% of their available tokens in the context window for visual embeddings. For long-history tasks, our results suggest that it is better to use the available context window to encode the history as text rather than devoting tokens to capture implicit information.\nSmaller LLMs benefit from implicit representation of visual information for short to medium-range visual history. In contrast to LTA however, VCLMs show competitive performance in the VPA task requiring grounding of medium visual history (Table 3). Specifically, VCLM 13B outperforms the Socratic 13B model by a large margin (mAcc: 21.2 \u2192 25.5 and mIoU: 37.4 \u2192 45.5 for \\(Z = 4\\)) in VPA on CrossTask (Table 3). In Appendix 9.1, we show that this trend is consistent for 7B LLMs. However, this performance gap between the VCLMs and Socratic models disappears for 70B LLMs in VPA. Thus, the implicit vision representation may capture signals that help in forecasting, especially when using smaller LLMs (7B, 13B) with limited reasoning and planning capabilities. However, such implicit information may not be essential for larger LLMs, which may be able to plan well with coarse-level grounding.\nLarger language models lead to better planning with limited, unstructured information from the visual history. Akin to many existing works [18], we find that scaling laws hold for our video-based planning tasks. Overall action prediction performance improves in both LTA and VPA for both categories of models as LLM size increases from"}, {"title": "5. User-in-the-loop Evaluation", "content": "Our benchmarking experiments on LTA and VPA highlight the strengths and weaknesses of VCLMs and Socratic approaches to predict future actions based on video history. However, it is unclear how these actions would manifest with a user in the loop and whether these actions \u2013 when executed - would successfully complete real-world activities. To that end, we conduct an online evaluation of VCLMs and Socratic approaches in real-world assistive scenarios. We recruit 18 participants to perform multi-step cooking activities while wearing an egocentric observation device called Aria [37] and following assistance from one of these models. We measure the true activity completion success rate, which is difficult to measure offline, and the correctness of recommended actions using mean IoU.\n5.1. Study Design\nMulti-step activities. We choose three cooking activities for our study: 1) espresso latte, 2) caprese salad, and 3) BLT (bacon lettuce and tomato) sandwich. These activities consist of a variety of ingredients, including meat, vegetables, breads, and liquids, and require different types of actions, including pouring, chopping, spreading, and plating. Furthermore, these activities also have some ordering constraints among the steps. For instance, the milk needs to be frothed before pouring into the espresso for making latte and the BLT ingredients need to be stacked on the bread before closing the sandwich. Lastly, we account for the ease of doing these activities in an office kitchen, which required omitting really long activities or activities using a stove, oven, etc.\nStudy protocol. Each participant performs two of the three aforementioned activities instructed by either a VCLM or a Socratic model. We use Latin-square counterbalancing for the ordering of multimodal LLM that offers them assistance as well as the type of activity that they perform across participants to reduce learning effect [17].\nEach activity entails a script that a participant can follow. Full scripts of the three activities can be found in Appendix 11. Each activity script is split into two phases \u2013 1) Partial progress: In the initial phase of the activity, participants are asked to make partial progress in the activity by completing a set of steps in any feasible order. For example, in the caprese-making activity, participants are instructed to slice tomatoes and mozzarella, tear basil, and place tomato slices on the plate. They are also free to slice varying amounts of these ingredients in whatever manner they like e.g., small vs. large slices. 2) Assistance evaluation: In this phase of the activity, the multimodal LLM assistant engages and guides the participant through completing the remaining steps. Participants iteratively request the next task step from the assistant and then execute what the assistant asks them to do to the best of their ability. Figure 3 gives an overview of these phases for the caprese-making activity. Participants may skip recommended actions from the assistant that are infeasible, irrelevant, or already completed. Actions are skipped solely at the participant's discretion. The activity episode is considered complete if the assistant returns a \"done\" step (for example, asking the participant to serve their dish), if the participant chooses to skip 3 instructions in a row, or after the participant executes \\(n+2\\) actions, where n is the number of steps in the evaluation section of the script. We allow n + 2 actions so as to account for multiple successful action sequences, including ones with optional steps (Fig. 4).\nEvaluation protocol. At the end of each activity episode, participants are asked to evaluate whether the food item they produced with the assistant's help is consistent with their idea of the food item they were supposed to prepare in the activity. If they are unfamiliar with the food item, they may conduct an internet search first to determine the characteristics of the item. Independently, the study administrator also evaluates whether the participant's product matches the food description. We consider an activity episode to be successful if both the participant and the administrator rate the episode as successful. Since the activity can be accomplished in multiple ways and with optional steps, our approach of using two human ratings for estimating activity success ensures a conservative and robust measurement. We also record individual actions recommended by the assistant and whether they were skipped, executed, or infeasible to compute the mean IoU and analyze the types of errors.\n5.2. Real-world Deployment of Multimodal LLMS\nWe frame the multimodal LLM assistance in our online study after the VPA task [33] used in our offline benchmarking experiments (Sec. 4), since we believe its definition is closest to vision-based assistants. Following VPA, the VCLMs and Socratic models are given an untrimmed and unsegmented egocentric video stream from the partial progress phase of the activity. This usually corresponds to 3-5 high-level actions on average. We also provide the models with a natural language goal describing the activity, as in VPA. The models are then prompted to iteratively output single-step action predictions to guide the user through the remaining 2-3 steps in the assistance evaluation phase of each activity. Akin to our offline benchmarking, the models in our study did not have access to the activity scripts, nor had they seen the kitchen environment where the ex-\nperiments were conducted. We use the same retrieval-based few-shot prompting strategy as in our offline experiments to obtain predictions from these models.\nModel modifications for offline \u2192 online. To keep inference times short during the study, we only use 13B versions of our VCLMs and Socratic models. However, direct deployment of these offline models on the online video stream from Aria does not work out of the box. The visual history accumulated in partial progress phase of activities in the study can consist of up to 1500 frames, corresponding to 2+ minutes of video, and are akin to the visual histories in LTA. However, unlike LTA, where ground-truth segmentation of these long video histories is available to generate text-based representations and vision embeddings for Socratic and VCLM, respectively, the video history from Aria is unsegmented. To support the grounding of long, unsegmented visual history in Socratic and VCLM, we make two main modifications. First, we perform segmentation. However, the addition of yet another model, e.g., a video segmentation model in our processing pipeline, could increase computation time and lead to interaction delays in our user-in-the-loop setup. Therefore, we uniformly segment the Aria stream into clips before passing them to our narration model - LaViLa. To compensate for unrelated and repeated narrations emerging from the uniformly segmented stream, we generate and cluster multiple narrations per segment as well as across segments based on the semantic similarity of narrations. Despite such stream segmentation and narration clustering, we find that the narrations tend to be extremely low-level, which leads to a very long narration history \u2013 ultimately exceeding the context window of our multimodal LLMs. Hence, our second modification entails the addition of a goal-conditioned summarization step to produce the final set of narrations for encoding the long visual history in online settings. Appendix 7.2 provides additional details about these modifications. Lastly, unlike offline benchmarking experiments where we match the open-set model outputs to a closed-set of actions (Sec 4), we directly use the open-set output for easier interactions with the user in the loop. No other modifications were made to the models for online deployment. Table 5 in the appendix shows these online-modified models perform similarly on the VPA task.\nSystem setup. We obtain the RGB video stream from Aria donned by our participants at 10 frames per second over wifi to a local machine. The frames are then center-cropped and downsampled in resolution (1400 \u00d7 1400 \u2192 288 \u00d7 384) to match the resolution of the LaViLa encoder. These frames are then sent to a remote server, which hosts the multimodal LLMs. The step suggestions returned by the models are parsed and communicated to the user via text-to-speech over wireless earbuds. The LaViLa narrator model runs on two-second clips of video and outputs 10 narrations per clip pre-clustering. The summarization step runs over the entire clustered narration history before every prediction step.\n5.3. Quantitative Results\nThe Socratic appraoch outperforms VCLMs at user-in-the-loop activity assistance. Table 4 shows the results"}, {"title": "5.4. Qualitative Analysis of Model Errors", "content": "Grounding errors, planning errors, and failure to detect activity end/success are the main failure modes. We also evaluate cases where a participant skipped actions recommended by the assistant. Recall that participants could skip action recommendations that were redundant, infeasible, or irrelevant. Appendix 12 shows a detailed analysis and breakdown of these reasons for skipped actions by the participants across the VCLM and Socratic models per activity. Such analysis enabled us to identify three main error modes \u2013 grounding errors, planning errors, and failure to detect activity end/success. Figure 4 shows these error modes for the espresso latte activity across all participants. Redundant skipped actions often correlate to grounding mistakes, where the models fail to recognize a step that has already been completed. Infeasible skipped steps also often correlate with grounding errors. Here, the models may suggest something that works for a different variation of the activity. For example, grinding coffee might work for a different version of a latte-making activity, but participants used an automated espresso machine in our study. These grounding errors are often more subtle than grounding errors from redundant steps. Finally, irrelevant skipped steps often correlate to planning errors, where the models suggest a step that is not part of the activity. We also find that in 50% of the successful activity episodes, the models fail to recognize when an activity is completed.\nOffline metrics don't capture error modes. The failure of models to detect activity end/success state does not affect the success rate metric, which would count such activity episodes as successful. However, it does lower mIoU scores due to redundant suggested actions. The overview of the participant steps for making a latte in Fig. 4 succinctly highlights the known issues with offline metrics. Specifically, mIoU as a permissive metric, would consider adding milk before steaming, a planning error, as a success. Conversely, offline success rate, being a restrictive metric, would discount 4 of the 5 present paths to success for making lattes as failures. Furthermore, mIoU and edit distance metrics do not capture optional actions sometimes suggested by such models \u2013 that do not affect activity success e.g., adding sugar. Grounding errors are the dominant mode of failure for models online. The bulk of the errors both models exhibit pertain to grounding. In particular, past participant actions are either not captured by the narrations or visual embedding of their activity history or are present in the long history but not attended to by the models during prediction, leading to grounding errors. We find that 63% of the skipped action recommendations are due to redundant action suggestions emerging from erroneous grounding (Appendix 12). The distribution of skipped actions is consistent across both models and indicates that recognizing previously completed actions in an activity is an ongoing challenge for these models. In contrast, both models make fewer planning errors, i.e., they suggest fewer irrelevant actions or actions with incorrect orderings. Overall, our analyses of skipped actions and errors in the study indicate that the primary challenge with visual assistants still lies in reasoning about activity progress and activity success/failure via grounding \u2013 more so than task knowledge or planning."}, {"title": "6. Conclusion", "content": "We evaluate the two predominant multimodal LLM-based approaches: Vision Conditioned Language Models (VCLM) and Socratic Models for vision-based activity assistance through two video-based action anticipation benchmarks on offline datasets and a real-world online study with 18 participants. To the best of our knowledge, our online evaluation is the first of its kind for multimodal LLMs towards real-world activity assistance systems. Our experiments show the Socratic approach is better equipped to capture coarse visual details across a long visual history. Current VCLMs can capture more fine-grained details but only for short visual history. Encoding long videos and aligning long videos with text tokens as needed by VCLMs would thus be rich avenues for future work. In the interim, Socratic models demonstrate competitive behaviors on video-based action anticipation and planning tasks spanning short to long visual history both offline and online.\nOur work sets important directions for future research on multimodal LLMs as vision-based assistants. Our online study highlights how grounding is the largest source of errors for these types of models. Grounding at different granularities remains an open problem, which, when improved, will greatly enhance activity assistance systems. Furthermore, we show how offline metrics do not provide a good indication of performance in online settings, demonstrating the importance of real-world evaluation of models for assistive scenarios."}, {"title": "7. Offline and Online Experiment Details", "content": "7.1. Offline Benchmark Tasks\nWe evaluate VCLM and Socratic models on two existing video-based forecasting benchmarks \u2013 Long Term Action Anticipation (LTA) [15] and Visual Planning for Assistance (VPA) [33] using offline datasets \u2013 Ego4D [15] and CrossTask [52] respectively (Sec. 4). Here, we provide a detailed overview of the datasets and our experimental setup for each of these tasks.\nEgo4D-LTA [15]: Ego4D consists of 3,670 hours of video footage of everyday activities, with 53 different scenarios. Out of this, we use the LTA (forecasting) subset, which entails 116 hours. This subset contains 1723 clips that cover an action space of 115 verbs and 478 nouns. We use the standard train and validation splits proposed by Ego4D [15] for our evaluation. In the LTA task, given 8 video segments from a video clip as input, the models must predict the 20 future actions in the form of verb, noun, and verb + noun, in correct order. Edit distance between the predicted sequence of actions and the ground truth action sequence in the video clip is used as a metric for evaluation following Ego4D [15].\nCrossTask-VPA [52]: CrossTask consists of 2.7K instructional videos for 18 different tasks from multiple domains, covering 374 hours of footage. Some of the action classes were shared among different tasks, with a total of 118 actions. Each video consists of an average 7.6 action steps. We follow [33] to construct a train split with 1,564 videos and a test split with 752 videos. We extract multiple test samples from each test video for VPA \u2013 specifically, given an annotated video consisting of K steps, we generate K-Z samples, leaving at least Z = 3, 4 steps to predict in the future. This leads to a dataset of 4123 test samples for our evaluation. Our VPA task definition also follows [33] - given an untrimmed video and a goal of the activity/task in the video described in natural language as input, the models must predict the up to 4 future actions in the form of verb+noun, in correct order. Evaluation compares the predicted action sequence with the ground truth actions in the video using mIoU, per step accuracy, and success rate metrics (Sec. 4).\n7.2. Model Modifications for Online Evaluation\nGoal-Conditioned Summarization. Sec. 5.2 provides an overview of modifications for our multimodal LLMs to enable online evaluation. The online settings entail noisy stream of redundant video frames leading to long narration"}, {"title": "9. Ablations on Visual History Representation", "content": "9.1. Evaluation of the benefit from implicit representation of visual information for smaller LLMs across different LLM sizes.\nTable 6 above shows that mAcc gap in VPA task for 7B models with and without visual conditioning at Z = 1,3,4 is 5.6%, 3.8%, and 3.2% respectively. The mAcc gap for 13B models at Z = 1,3,4 is 4.4%, 3%, and 4.3% and for 70B models is 0%, .3%, and 1.3% respectively as in Table 3. Implicit visual representation aids smaller LLMs across model sizes.\n9.2. Task-relevant information from visual history\nDifferent aspects of the visual history can be extracted and represented in text for VCLMs and Socratic models. It is unclear what aspects should be extracted to enable ef-"}]}