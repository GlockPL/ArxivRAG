{"title": "A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement", "authors": ["Guillermo Villate-Castillo", "Javier Del Ser", "Borja Sanz"], "abstract": "Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content\u2014an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and disagreement. The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.", "sections": [{"title": "1. Introduction", "content": "Content moderation (CM) has been an important pillar in maintaining ethical online interactions. Given the large amount of user-generated text, CM systems often employ moderation algorithms [1] to combat the spread of online toxicity. Over the past decade, much research on toxicity detection in text data has leveraged the use of machine learning (ML) models, which have been shown to suffer from reliability and robustness issues, wrong predictions due to spurious lexical features [2] and biases [3]. Robustness and reliability are cornerstones, especially in sensitive areas such as CM, where subjectivity and context significantly influence the results [1].\nAs observed in a recent survey on toxicity detection [4], most research contributions reported in this area to date have largely overlooked a key element in decision-making: the estimation of the model's confidence in its prediction. In the few cases when uncertainty quantification (UQ) has been considered [5], models have shown remarkable improvements in robustness.\nUQ not only optimizes the accuracy of predictions, but also facilitates human moderation. By identifying the least reliable predictions, UQ techniques allow moderators to focus their attention on cases where the model exhibits the highest uncertainty, thus optimizing the use of time and resources devoted to human review.\nAlthough incorporating uncertainty has advanced ML models in different disciplines, such as traffic modelling [6] or medical imaging [7], it is crucial to recognize that tasks like toxic language detection, which are inherently subjective, require a different perspective due to their complexity. Disagree-ments among annotators during labelling processes often occur in content moderation reflecting this subjectivity, which can lead to bias and, in some cases, to the censorship of minority opinions. The quantification of inter-annotator disagreement is thus a valuable source of information about the complexity inherent in comments, and is directly related to the aleatoric uncertainty present in the large databases whose inputs require moderation.\nAnnotation disagreement quantification, together with the epistemic un-certainty of the modelling process, allow us to identify the cases when the model cannot provide a robust prediction, as well as those when, due to their complexity and subjectivity, the intervention of a human moderator is nec-essary. Accounting for both cases is critical, as ML models trained with a majority vote disregard the inherent ambiguity of comments. Unfortunately, considering different annotation perspectives alongside the data modelling"}, {"title": "2. Related Work and Contribution", "content": "In this section, we first establish the fundamentals and perform a lit-erature review of CP (Subsection 2.1), with a focus on text classification. Then we define content moderation and revisit frameworks proposed in the recent literature within the crossroads of toxicity detection and uncertainty estimation (Subsection 2.2). Finally, we end the section with a statement on the contribution of our work beyond the literature reviewed in the previous subsections (Subsection 2.3)."}, {"title": "2.1. Conformal Prediction", "content": "CP is a framework pioneered by Vovk et al. [11], which can be defined as \u201ca practical measure of the evidence found in support of a prediction, by esti-mating how unusual a potential example looks with respect to previous ones\u201d Gammerman et al. [12]. Given a desired confidence level 1 a, conformal algorithms are proven to always be valid, meaning that the prediction inter-vals or sets generated by these algorithms will, on average, correctly contain the true outcomes at least 1 a of the time. This validity is achieved with-out requiring any specific assumptions about the data distribution, except for the assumption of independent and identically distributed (i.i.d.) data. Conformal predictors can be used for both classification and regression tasks, and can be constructed from any conformity score to indicate the similarity between a new test example and the training examples. Thanks to their model-agnostic nature, CP techniques can be employed alongside any ML algorithm.\nSince we deal with toxicity detection, we depart from a trained classifica-tion model (CLASS), from which we create a set of possible predicted labels C(x) associated to input x using a small amount of data called calibration data. To construct the prediction set in conformal prediction for classifica-tion, we first select a conformal score s, typically derived from the softmax output of the CLASS. We then calculate the quantile q = (n + 1)(1 \u2212 a)/\u043f"}, {"title": "2.2. Content Moderation", "content": "CM, as defined in Grimmelmann [20], refers to the governance mecha-nisms that structure participation in a community to facilitate cooperation and prevent abuse. In our context, when we talk about CM, we also refer to algorithmic CM, which the same author defines as \u201csystems that classify user-generated content based on either matching or prediction, leading to a decision and governance outcome\u201d.\nBefore the rise of social media, CM was primarily performed by human moderators. However, with the surge of interactions on new platforms, this"}, {"title": "2.3. Main Contribution", "content": "Taking into account the analysed literature, the main contribution of this manuscript is a collaborative CM framework where the moderator is in control of the algorithmic CM tools by deciding when the moderator's final decision should be incorporated. This is achieved not only by considering the model's confidence through UQ, but also by incorporating information about the level of annotation disagreement the model should handle in its"}, {"title": "3. Materials and Methods", "content": "In this section, we first introduce the dataset used for the experiments, along with the modifications made to its original version (see Subsection 3.1). Next, we present the methods employed (Subsection 3.2), where we provide details on model selection, hyperparameter configuration, the computation of annotation disagreement, and the Uncertainty Quantification (UQ) tech-niques applied during the experiments. Lastly, we conclude by defining the metrics used to validate our proposed research questions and the collabora-tive Confusion Matrix (CM) framework (Subsection 3.3)."}, {"title": "3.1. Dataset", "content": "When working with toxicity modelling datasets, one of the elements that has been recently criticized by the community is the lack of information about the annotation process, such as the number of annotators, their background, the total number of annotations, and the diversity of the annotators [23]. Since disagreement calculation is a central focus of our implementation, it is essential to have a dataset that includes sufficiently diverse opinions and numerous annotators per comment to yield an heterogeneous distribution of annotations.\nAs exposed in Villate-Castillo et al. [4], only two datasets currently meet these requirements: the Jigsaw Unintended Bias in Toxicity Classification dataset [24], to which the Jigsaw Specialized Rater Pools dataset [25] was also added, as the latter included a portion of this dataset with a higher number of annotations and greater diversity; and Wikipedia Detox (WikiDetox) [26]. However, Wikipedia Detox is not considered in our study because it does not provide the percentage of people who agreed that a given comment is toxic. Furthermore, this latter dataset contains a majority of comments with a low level of disagreement, which may lead to the detection model being overfit to this data subset, rendering it undesirable. Consequently, the model can struggle to learn the lexical features associated with comments that elicit higher levels of disagreement due to its limited exposure to examples from that fraction of the data.\nThe Jigsaw Unintended Bias in Toxicity Classification dataset, created as part of a challenge organized by Google Jigsaw, provides a high-quality dataset that is already divided into training, validation, and test sets. This predefined partitioning facilitates the reproducibility of our experiments. Ad-ditionally, compared to other datasets in the toxicity domain [4], this dataset features a substantially larger group of annotators (8,899), as opposed to the smaller groups of tens typically found in other datasets. This large number of annotators is a significant advantage, as it provides a more diverse range of opinions, not only in terms of numbers, but also in terms of background, ethnicity, gender, and age. However, during the dataset preprocessing phase, the following caveats are found:\n\u2022 The dataset contains comments, each annotated by between 3 and several hundred annotators. Since we are targeting a diverse dataset in terms of opinions, we select only the comments that have at least 10 annotators. This yields 315,685 comments in the training set, 38,855 in the test set, and 33,611 in the validation set, out of a total of approximately 2 million comments.\n\u2022 Subsection 3.2.2 shows that annotation disagreement is measured on a scale from 0 to 1, where 0 indicates no disagreement and 1 indicates high disagreement. When applying the formulas explained in this section to the chosen dataset, we observe that the dataset is imbalanced in terms of the level of disagreement. The majority of comments have a disagreement score higher than 0.4, with most falling between 0.4 and 0.8. Despite this im-balance, the dataset includes instances of medium and high disagreement,"}, {"title": "3.2. Methods", "content": "In this section we introduce the notation, selected model architecture, learning objectives, and hyperparameters (Subsection 3.2.1), whereas Sub-section 3.2.2 discusses various methods from the literature for computing disagreement and explain our selected approach. Finally, Subsection 3.2.3 presents the CP algorithms considered in our experiments."}, {"title": "3.2.1. Notation, Model Selection, Learning Objectives and Hyperparameters", "content": "The proposed CM framework is defined as a multitask learning (MTL) task on an annotated dataset D = {X, A, y,d}, where X is a set of text instances, A is the set of annotations, y is the set of toxic labels, and d is the annotation disagreement value. Each entry yi \u2208 {0,1} represents the label assigned to xi \u2208 X based on the majority vote of the annotators in a \u2208 A; the label that receives the most votes is selected for each instance. Meanwhile, di \u2208 R[0,1] represents the computed annotation disagreement for the set of annotations given to the text instance xi.\nAs depicted in Figure 1, the MTL CM process consists of a primary task (toxicity classification of the given comment x\u2081) and an auxiliary task (estimation of the annotation disagreement). The architecture is based on DistillBERTbase uncased [27], which serves to extract the embedding from the CLS token (a sentence-level representation for classification) of the given comment x; to perform the MTL task. The generated predictions for each task are defined as \u0177 from the primary task and di(x) from the auxiliary task. We then generate the corresponding prediction set C(x\u012b) and confidence interval I(x) using the inductive CP techniques later detailed in Subsection 2.1 and a calibration subset Dcal. Finally, the need for a human moderator's"}, {"title": "3.2.2. Computation of the Annotation Disagreement", "content": "Annotation disagreement can be computed in different ways, e.g. as the percentage of people in disagreement with the majority vote as in Wan et al. [30]; by predicting the 5-point scale score as proposed by Ramponi and Leonardelli [31]; or by using the concept of soft labels [9], i.e., the proba-bility of a comment to be in the positive class. In addition to these methods, we also consider the possibility of using the entropy of the annotations to quantify disagreement. Specifically, given the mean of the annotation labels \u0101\u00bf (which in our case can be regarded as the probability that x, is annotated to belong to the toxic class), the entropy serves as a measure of uncertainty or unpredictability in the annotation process. This measure is given by:\nd(xi) = -\\bar{a_i} \\log_2(\\bar{a_i}) \u2013 (1 \u2013 \\bar{a_i}) \\log_2(1 \u2013 \\bar{a_i}), \\tag{1}\nwhere a is the proportion of annotators classifying content x; as toxic.\nIn our case we propose an alternative method to compute the annotation disagreement, which draws inspiration from a combination of the works in Wan et al. [30] and Fornaciari et al. [9]. Specifically, the annotation dis-agreement is given by the distance between the mean of the annotations \u0101\u00bf (assuming that a\u2081 = 1 represents toxic and a\u2081 = 0 represent non-toxic) and 0.5, which represents the value of maximum disagreement. The resulting value, ranging from 0 to 0.5, is linearly scaled to the range R[0, 1] and then inverted to yield the sought measure of disagreement. Mathematically:\nd(xi) = 1 - 2 |\\bar{a} \u2013 0.5|, \\tag{2}\nwhere d(xi) represents the annotation disagreement score for x\u017c, and \u0101\u017c is the mean of the annotations associated to xi.\nThis metric, the distance based annotation disagreement score, is priori-tized over entropy because the latter produces values that are too similar in cases of high uncertainty, and moreover requires a more complex conversion to calculate the percentage of individuals in disagreement. Regarding the scale-based method introduced by Ramponi and Leonardelli [31], we were unable to use their proposed technique because the selected dataset lacks the required 5-point scale score for each comment."}, {"title": "3.2.3. Conformal Algorithms under Consideration", "content": "For UQ in classification and regression models, our framework incorpo-rates inductive CP algorithms due to their ability to offer guaranteed cov-erage, their model-agnostic nature, the fact that they do not require any"}, {"title": "3.3. Performance Metrics", "content": "Traditional CM systems have been evaluated using performance metrics usually considered in Machine Learning, including accuracy, F1 score, and AUC (Area under the ROC Curve). However, these metrics only capture model performance and do not account for the efficiency of models in a collaborative environment with human moderators.\nTo overcome this, we henceforth define collaboration efficiency as the model's ability to express its own limitations. In Kivlichan et al. [5], this gap in evaluation metrics is addressed by introducing Review Efficiency, which we rename in our study as Model Uncertainty-aware Review Efficiency (MURE) to distinguish between review efficiency based on model uncertainty and an-notation disagreement. MURE quantifies the proportion of examples deliv-ered to a moderator in situations where the model's toxicity classification would otherwise be incorrect. In simpler terms, it measures how often com-ments flagged as uncertain by CP methods correspond to cases where the model would have made an incorrect prediction.\nWe mathematically define this metric in the context of a classification task comprising C classes, such that |C(xi)| > 1 denotes the case when the classifier is not certain about its prediction. We define TPUQ as the number of instances where the model correctly delivers the content at its output to the moderator based on its estimated uncertainty, i.e. when yi \u2260 \u0177i and C(xi) > 1. Conversely, FPuq refers to the number of examples where the model incorrectly sends the content for review based on their estimated uncertainty despite its toxicity prediction being accurate, corr. Yi = \u0177i and |C(xi) > 1. As a result, MURE can be computed as:\nMURE = \\frac{TP_{UQ}}{TP_{UQ} + FP_{UQ}} \\tag{11}\nLikewise, to account for the efficiency in predicting when a comment is annotated ambiguously, we propose a new metric coined as Comment Ambiguity-aware Review Efficiency (CARE). CARE evaluates the ability to identify all ambiguous comments, based on a threshold y selected by the moderator. Let TP\u2194 be the number of comments that are truly ambiguous as per y (i.e., d(x\u2081) \u2265 y) and which are correctly predicted as ambiguous (i.e., I(x) \u2265 \u03b3). Likewise, we define FN\u21d4 as the number of comments which are truly ambiguous but are not predicted as such, i.e., d(x\u2081) \u2265 y but I(xi) < \u03b3. Based on these definitions, CARE is given by:\nCARE = \\frac{TP_{\\leftrightarrow}}{TP_{\\leftrightarrow} + FN_{\\leftrightarrow}} \\tag{12}\nnamely, the True Positive Rate (TPR) associated to the conformalized an-notation disagreement estimate resulting from the auxiliary modeling task. In addition to CARE and MURE, we also consider Review F1 Score (R-F1), which measures how well the framework manages model errors due to both model's uncertainty and comment ambiguity. R-F1 combines precision (MURE) and recall (CARE) to evaluate the system's effectiveness in handling these errors, providing a balance between the two:\nR-F1 = 2. \\frac{MURE \\cdot CARE}{MURE + CARE} \\tag{13}\nFurthermore, we introduce the point-biserial correlation rpb to capture the correlation between the auxiliary task flagging a comment as ambiguous given the threshold y and the level of annotation disagreement for that comment,"}, {"title": "4. Experimental Setup", "content": "In this section, we describe the experiments conducted to address the re-search questions outlined in Section 1. Section 4.1 and 4.2 detail the exper-imental setup for RQ1 and RQ2, respectively. Finally, Section 4.3 describes the experimental setup for RQ3.\nFor all experiments the same hyperparameter setting is used, and fixed seeds are set for the sake of reproducibility. As introduced in Section3.2, we use the DistillBERTbase uncased model, which was fine-tuned on a H100 GPU with a batch size of 32, a maximum sequence length of 512, a learning rate equal to 2.10-5, a weight decay of 0.01, and a learning rate warm-up of 1 epoch. AdamW is used as the optimizer, and the model is trained until no improvement is observed in the loss metric, with a patience parameter of 2 epochs. All scripts and log files used to produce the results discussed"}, {"title": "4.1. RQ1: Impact of Annotation Disagreement Prediction on Primary Task Performance and Calibration", "content": "The STL CLASS model, depicted in the upper section of Figure 1, is trained using focal loss on hard labels. In contrast, the MTL approach, illus-trated in the lower section of the same figure, applies the same loss function but incorporates an auxiliary task. To evaluate potential improvements, we compare the STL CLASS across various regression settings, which include BCE, MSE and RAC, as detailed in Subsection 3.2. Given that previous research observed improvements in classification [9], we hypothesize that a similar effect might occur with the regression auxiliary task. To verify this, we compare each multitask model with the STL REG using the correspond-ing regression loss. We use MAE and MSE as the primary metrics to evaluate improvements.\nThese comprehensive assessments are crucial for addressing subsequent research questions related to model uncertainty, particularly because calibra-tion plays a significant role in CP methods. A proper calibration ensures that the estimated prediction intervals or conformal sets provided by CP methods accurately reflect the true uncertainty of the model's predictions."}, {"title": "4.2. RQ2: Effects of Annotation Disagreement Integration on Uncertainty Quantification", "content": "Since proper model calibration is closely related to accurate uncertainty quantification (UQ) provided by CP methods, we hypothesize that multitask settings could improve model uncertainty estimation for both STL classifica-tion (CLASS) and regression (REG). To quantify improvements in the pri-mary task (STL CLASS), we focus specifically on the MURE metric, which assesses how well the uncertainty method predicts cases where the model is incorrect. In addition to this metric, we also consider measures com-monly used to evaluate uncertainty estimation methods, including marginal coverage, the correlation between uncertainty and annotation disagreement scores ri,d (which refers to the Pearson correlation between the estimated annotation disagreement based on the confidence interval and the true an-notation disagreement), and the certainty F1 score. To compare uncertainty improvements, we use the same classification and multitask models described in Section 4.1, calibrated using the same validation/calibration dataset and"}, {"title": "4.3. RQ3: Benefits of an Auxiliary Task versus Independent Models in Collaborative Content Moderation", "content": "Previous studies have assessed the efficiency of moderators and modera-tion systems using review efficiency (MURE) [5], which considers only model performance while disregarding label data quality and text ambiguity. To address these factors, our moderation framework includes an auxiliary task designed to measure this concept. Since this is a novel approach, we intro-duce a new metric, CARE, to account for the system's capacity to detect ambiguous examples, as explained in Section 3.3. To determine if a com-ment is ambiguous, a threshold y must be selected that aligns with the desired maximum level of annotation ambiguity. Given that the quantity of uncertain examples varies depending on each model's capabilities, which in turn affects the CARE metric, we used the False Positive Rate (FPR) at a given TPR/MURE value, a metric commonly used in other areas including Out-of-Distribution (OoD) detection [38]. Different TPR values are applied for RAC models due to the interval size, which affects the quality of the ambiguity estimation.\nExperiments for RQ3 consider the following models:\n\u2022 STL CLASS (upper section of Figure 1): In this case, classification model's uncertainty is used to measure how well the model performs in detecting"}, {"title": "5. Results and Discussion", "content": "In this section we present and analyse the results for each research ques-tion. In Section 5.1 we discuss our findings for RQ1, highlighting the benefits of our approach in terms of model calibration and performance. Section 5.2 focuses on the advantages of incorporating the auxiliary task for improving uncertainty estimation in the primary classification task. In Section 5.3 we compare the multitask and composite approaches for content moderation, with a focus on comment annotation disagreement. For each RQ, we begin by summarizing the key performance metrics and findings."}, {"title": "5.1. RQ1: Impact of Annotation Disagreement Prediction on Primary Task Performance and Calibration", "content": "Following the experimental setup presented in Section 4.1, Table 2 sum-marizes the performance results for the primary task (toxicity detection) of the STL CLASS approach and the MTL approach with different regression"}, {"title": "5.2. RQ2: Effects of Annotation Disagreement Integration on Uncertainty Quantification", "content": "As described in the experimental setup corresponding to RQ2, we now focus on analysing whether there is any clear improvement in uncertainty quantification for the regression and classification tasks by including an aux-iliary task in annotation disagreement prediction.\nThe results for the classification uncertainty are divided based on the conformal prediction methods in use:"}, {"title": "5.3. RQ3: Benefits of an Auxiliary Task versus Independent Models in Collaborative Content Moderation", "content": "In this section, we present the main results of our collaborative content moderation framework, focusing on the outcomes of our CARE metric, F1 Review Efficiency, and Pearson correlation ri,d. The results are categorized by regression loss, comparing the MTL approach against the CoM approach, where STL REG and STL CLASS work together to make a final decision."}, {"title": "6. Conclusions and Future Work", "content": "In this work, we introduced a novel framework for collaborative content moderation that accounts for model uncertainty and addresses annotation disagreement through the introduction of an auxiliary task. This approach excels at capturing the inherent subjectivity of toxic comments, providing a moderation scheme that reflects this intrinsic complexity. Additionally, we proposed two new metrics to evaluate the comment review process: F1 Review Efficiency and CARE. These metrics incorporate the concept of an-notation disagreement into the review process, which the SOTA MURE does not consider.\nThroughout the manuscript we have analysed the impact of incorporating this auxiliary task in terms of calibration, model performance, and uncer-tainty estimation for both the primary task (classification) and the auxiliary task (regression), using various regression losses and uncertainty quantifica-tion techniques. We have found out that the integration of the auxiliary task improves not only the classification task, but also enhances the auxiliary task in terms of calibration and performance.\nBy demonstrating improvements in uncertainty estimation through the use of annotation disagreement, we have opened up an interesting new area of research, where future studies could build on towards enhancing existing classification-based tasks with better uncertainty estimation techniques. Fi-nally, we have evaluated the quantitative benefits of employing a multitask architecture over a single-task approach, obtaining promising results. The multitask approach outperformed the single-task model in detecting incor-rect predictions, and also quantified disagreement in comments as specified by the chosen y, namely, the targeted ambiguity threshold.\nFindings. We summarize the key findings for each of the research questions that have guided this study:\n\u2022 RQ1: We have observed that, consistently with previous studies, the incor-poration of an auxiliary task focused on annotation disagreement enhances the calibration and performance of the toxicity classification task. In the case of the improvements of performance in the auxiliary task, an overall improvement was observed for the MSE and MAE metrics.\n\u2022 RQ2: In terms of improving uncertainty estimation by adding an auxiliary task, this addition has been shown to enhance the classification model's uncertainty estimation compared to a single-task classifier. Furthermore, we observed a better point-biserial correlation between the model's uncer-tainty and annotation disagreement, indicating a stronger understanding\nof borderline cases. For the regression task (disagreement estimation), the confidence intervals generated were wider than those of the single-task regression model used for the same task, particularly for the RAC and BCE models. However, the MSE model did not demonstrate consistent performance differences, making the overall enhancement inconclusive and highly dependent on the appropriate configuration based on the selection of the uncertainty method and the regression loss.\n\u2022 RQ3: The multitask architecture generally outperforms CoM, which con-sist of a single-task classification model and a single-task regression model. Although in some cases the improvement reported in our results is small, the MTL framework is more computationally efficient and offered advan-tages regarding the responses to RQ1 and RQ2. Compared to the single-task classifier, the multitask model provides more flexibility for modera-tion, as it directly computes annotation ambiguity. However, we acknowl-edge that the uncertainty estimated for the single-task classifier is still reasonably aligned with comment ambiguity.\nOn an overall concluding note, the moderation framework is not only aligned better with the inherent limitations of toxicity classification, but also offers improved uncertainty quantification, model calibration, and perfor-mance, which are beneficial for real-world moderation processes.\nLimitations. Toxicity classification, and by extension content moderation, faces a significant challenge due to the inherent subjectivity of the task. This subjectivity complicates dataset creation and the annotation process, as we must account for both explicit and implicit (contextualized) toxicity. Since a few datasets provide prior comments or context for toxic comments, it be-comes difficult to accurately assess the toxicity during annotation. This can lead to several issues in the annotation process, as outlined by Zhang et al. [23]. The main challenges include: rater heterogeneity, individual differences, variations in working patterns, difficulties in understanding comment com-plexity, unclear task descriptions, and issues related to randomness. These challenges directly impact our proposed moderation framework, which lever-ages annotation disagreement as a core element. Another limitation is that a few datasets guarantee a sufficient number of annotators, which restricts our approach due to economic constraints, as increasing the number of an-notators significantly raises the costs derived from the annotation process. Finally, our approach may be limited by the heterogeneity and distribution of\nannotation disagreement, as datasets like Wiki Detox predominantly contain low-disagreement comments. This makes it more challenging to effectively model annotation disagreement and incorporate it to frameworks like the one proposed in this manuscript.\nFuture Work. As part of our future work, we intend to explore additional un-certainty estimation techniques to investigate whether the performance gains from introducing the auxiliary task extend to other uncertainty estimation methods. This will help in assessing the robustness and generalizability of our framework when incorporating other uncertainty estimation techniques. Furthermore, we aim to evaluate its applicability to other tasks character-ized by subjective annotations, such as sentiment analysis, where annotation disagreement and ambiguity play a critical role. Additionally, we envision that our approach holds potential in the crossroads of active learning and generative text modelling, as it can generate more diverse datasets and even-tually, improve the detection of evolving patterns in toxic language expres-sion. In this envisioned approach, we will investigate how to effectively avoid known issues with synthetic data generation in fine-tuning loops, including the model degradation due to content autophagy [40]."}]}