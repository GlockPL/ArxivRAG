{"title": "Exploiting Phonological Similarities between African Languages to achieve Speech to Speech Translation", "authors": ["Peter Ochieng", "Dennis Kaburu"], "abstract": "This paper presents a pilot study on direct speech-to-speech translation (S2ST) by leveraging linguistic similarities among selected African languages within the same phylum, particularly in cases where traditional data annotation is expensive or impractical. We propose a segment-based model that maps speech segments both within and across language phyla, effectively eliminating the need for large paired datasets. By utilizing paired segments and guided diffusion, our model enables translation between any two languages in the dataset. We evaluate the model on a proprietary dataset from the Kenya Broadcasting Corporation (KBC), which includes five languages: Swahili, Luo, Kikuyu, Nandi, and English. The model demonstrates competitive performance in segment pairing and translation quality, particularly for languages within the same phylum. Our experiments reveal that segment length significantly influences translation accuracy, with average-length segments yielding the highest pairing quality. Comparative analyses with traditional cascaded ASR-MT techniques show that the proposed model delivers nearly comparable translation performance. This study underscores the potential of exploiting linguistic similarities within language groups to perform efficient S2ST, especially in low-resource language contexts.", "sections": [{"title": "1 Introduction", "content": "To facilitate communication between people who do not share a common language, the machine learning community has proposed techniques for speech-to-speech translation (S2ST), which converts speech from one language to another. The standard approach typically involves three key sub-tasks: automatic speech recognition (ASR), text-to-text translation, and text-to-speech (TTS) synthesis Ney (1999); Matusov et al. (2005); Vidal (1997).\nAlthough this cascaded approach has achieved some success, it has also been criticized due to error propagation. Errors in one sub-task are compounded in subsequent tasks, leading to increased translation inaccuracies. Furthermore, while the cascaded process captures the semantics of the original speech, important speech elements such as the speaker's unique characteristics (indexical components) and the natural rhythm of communication are often lost during translation Barrault et al. (2023b). These components are crucial in many social and conversational contexts.\nFor low-resource languages, such as African languages, the cascaded approach faces additional challenges. The lack of aligned or annotated text between languages makes text-to-text translation difficult or impossible in some cases. In response to these issues, researchers have started exploring direct S2ST, which bypasses the need for intermediate text-based representations.\nImplementing direct S2ST remains challenging due to the lack of sufficient annotated speech pairs needed for fully supervised end-to-end training Jia et al. (2019). Collecting and annotating speech datasets is significantly more difficult than gathering parallel text pairs used in the cascaded approach. This challenge has motivated innovative solutions from various works Jia et al. (2019); Zhang et al. (2021); Jia et al. (2021); Lee et al. (2021); Huang et al. (2022), which leverage deep neural networks to perform direct S2ST without requiring parallel speech datasets.\nWhile prior works on direct S2ST have shown promising results using deep learning models, our approach differentiates itself by explicitly leveraging linguistic relationships among languages within the same phylum to improve speech segment annotation. We introduce two segment-mapping techniques that capitalize on phonological similarities across related languages, which prior methods have not explored in this context.\nGuided diffusion is a generative modeling technique that iteratively refines a noisy input toward a target distribution. In the context of S2ST, we adapt this model to speech data, using a pseudo-classifier trained on paired segments to guide the diffusion process. This enables us to generate clean speech segments in the target language using paired segments of speech.\nTo evaluate our technique, we conduct a pilot study using selected African languages. These languages are categorized into four major phyla: Niger-Congo, Nilo-Saharan, Afroasiatic, and Khoisan Childs (2003); Frajzyngier (2018). We assess the efficacy of segment-mapping techniques by testing on languages within the same phylum and across different phyla to understand how phonological relationships impact translation performance.\nGiven two speeches in languages x and y, both conveying the same semantic content, we investigate whether the speech segments in x, extracted using silences, can be directly mapped to segments in y, particularly when both languages belong to the same phylum. We also examine whether the mapping quality deteriorates when y is from a different phylum.\nWe propose two segment-mapping techniques: one based on the similarity of segment locations within the speech and the other based on the similarity of learned embeddings through contrastive training. By automating speech segment mapping, we aim to reduce the manual effort and cost involved in annotating speech datasets. Once the segments are aligned, we employ a guided diffusion model to train a direct S2ST system. Our contributions are as follows:\n1. Investigate the extent to which linguistic similarities within a phylum can be leveraged for automatic speech segment annotation, offering a scalable solution for low-resource languages.\n2. Explore the feasibility of automatic speech segment annotation across languages from different phyla, contributing to cross-lingual translation tasks.\n3. Propose two techniques for segment alignment-location-based and embedding-based techniques.\n4. Develop an automatic evaluation method for segment pairing, minimizing reliance on costly human evaluations and enabling more scalable annotation efforts.\n5. Introduce a direct S2ST model using guided diffusion."}, {"title": "2 Background", "content": null}, {"title": "2.1 Guided diffusion", "content": "Guided diffusion involves learning the conditional distribution p(xy). Guidance therefore involves learning the conditional distribution which enables the model to generate the data x by conditioning it on information y. Using Tweedie's formula which states that the true mean of samples drawn from an exponential family distribution can be estimated using the maximum likelihood estimate of the samples plus the correcting term involving the score of the estimate, the mean of the distribution z ~ N(z; uz, \u03a3) can be estimated as:\nE(uz|z) = 2 + \u03a3\u2207z log p(z)\nTherefore, given the posterior q(xt|xo) = N(xt; \u221a\u0101txo, (1 \u2013 \u0101t)I), its true mean is estimated as:\nE(Uxt Xt) = Xt + (1 -\u0101t)\u2207xt log p(xt)\nWhere at = \u03a0=1&t, \u20ac0 ~ N(\u20ac0; 0, I) and at evolves with time t based on a fixed or learnable schedule such that the final distribution p(xT) is a standard Gaussian. Hence:\n\u221a\u0101txo = xt + (1 \u2212 \u0101t)\u2207xt log p(xt)"}, {"title": "3 Related work", "content": "To facilitate communication between people who do not share common language, machine learning commu-nity have proposed techniques that implement speech-to-speech translation (S2ST) which entails translating speech from one language to another. The de-facto way of implementing S2ST is to break the process into three key sub-tasks i.e., automatic speech recognition, text-to-text translation, and text-to-speech synthesis Ney (1999) Matusov et al. (2005) Vidal (1997). Even though this cascaded approach has achieved some level of success in S2ST, it has been criticised due to the fact that errors made at a given sub-task are compounded in subsequent tasks leading to larger translation errors. Further, when speech is translated in this manner even when translation can accurately capture the semantics of the original speech, certain key elements of the original speech are lost in the translation process Schuller et al. (2013). Elements of speech"}, {"title": "4 Direct S2ST based speech segmentation and guided diffusion", "content": null}, {"title": "4.1 Speech segmentation", "content": "Speech segmentation is crucial in our proposed technique. We hypothesize that two speeches with the same semantic content and belonging to the same language phylum will have silences at approximately the same locations due to phonological similarities such as rhythm and prosody. These silences are leveraged to segment audio files into coherent sentences. Silence-based audio segmentation using voice activity detection (VAD) has been explored in previous studies Gaido et al. (2021); Potapczyk & Przybysz (2020); Duquenne et al. (2021). Although effective, VAD-based segmentation presents two key challenges.\nOne challenge is that pauses within a sentence may lead to incomplete or incoherent segments. Another issue is that pause-based segmentation can result in \"spillover\" sentences, where consecutive sentences are incorrectly grouped into one when no pause exists between them. To address these issues, over-segmentation techniques have been proposed in works such as Potapczyk & Przybysz (2020); Duquenne et al. (2021), which divide audio files more densely based on silence.\nFor example, Potapczyk & Przybysz (2020) defines a sentence threshold after dense segmentation, merging smaller fragments until they meet the threshold. Similarly, Duquenne et al. (2021) introduces an over-segmentation approach where segments must be at least 3 seconds and no more than 20 seconds long. While this increases recall, it comes at the cost of additional computation Barrault et al. (2023a).\nIn our approach, we adopt the technique from Duquenne et al. (2021), as experimental results showed it to be more robust and better suited for the speeches used in our evaluation, particularly for African languages. Given a speech x that contains multiple sentences, we apply a VAD tool to identify all silences within the input speech. These silences are used to extract segments, where each segment sx is bounded by two silence timestamps and must be between 3 and 20 seconds in length."}, {"title": "4.2 Segment pairing", "content": "We propose a two-step process to pair segments from two speeches x and y, both containing similar semantic content. The first step pairs segments based on location, and the second step refines these pairs using contrastive self-supervised learning to filter out less accurate matches.\n1. Segment both speeches x and y using the method described in Section 3.1.\n2. Calculate the average segment lengths lx and ly, and determine the absolute difference d = |lx - ly| to account for timing variations.\n3. For each segment sx of length px in speech x, find matching segments sy\u2081 in speech y within the length range pr\u00b1d, and located between i d/2 and i + Px + d/2, accounting for variations in delivery speed.\n4. If no match exists in speech y, generate overlapping segments from y with overlaps of d/2, and create all possible pairings.\n5. Reverse the process by mapping segments of y to those of x, ensuring bi-directional alignment.\nThis location-based alignment creates multiple one-to-many pairings, which are refined into one-to-one pair-ings using contrastive self-supervised learning."}, {"title": "4.2.1 Refinement with contrastive learning", "content": "To filter and select high-quality segment pairs, we train a segment encoder fs using contrastive self-supervised learning. The goal is to learn a mapping function fs : s\u2084 \u2192 Rd, which transforms each segment s into a d-dimensional vector. We employ the SimCLR contrastive loss Chen et al. (2020), treating segments from the same language as positive pairs and segments from different languages as negative pairs:\nEss,Sy\nlog\nefs(sx)Tfs(8+)\nefs(s) fs(s) + 2n-2 efs(sa)Tfs(Syi)\nThis loss encourages similarity between positive pairs fs(sx), fs(st), while minimizing similarity with neg-ative pairs fs(sx), fs(s\u1ef9). Once trained, the encoder fs generates embeddings for all segments from both speeches x and y.\nFinal one-to-one pairings are established by calculating the cosine similarity between the embeddings of segments from both speeches. For each segment sx, we select the segment sy that maximizes the cosine similarity:\n(Sx, Sy) = arg max cos(fs (sx), fs(sy))\ny\nLow-quality matches are filtered by retaining only those pairs validated by contrastive learning. From the initial location-based pairings {($x,Sy1),\u00b7\u00b7\u00b7, (Sx,Syn)}, we keep only pairs confirmed by contrastive learning:\n(Sx, Sy\u2081) = {(Sx, Syi),\u00b7\u00b7\u00b7, (Sx, Syn)} \u2229 {(Sx, Sy\u2081)}\nIf no valid pairs remain, we fall back to selecting the segment sy, with the highest cosine similarity score, ensuring the closest semantic match.\nTo train the segment encoder fs, we start by encoding the input segment s using an input encoder fi, which is implemented as a single convolutional layer as proposed in ?. This layer consists of 256 convolutional filters, each with a kernel size of 16 samples and a stride of 8 samples. The input to this layer is a speech segment in the time domain, sx \u2208 RT, and the output is a time-frequency representation akin to the Short-Time Fourier Transform (STFT), denoted as sxo \u2208 RF\u00d7T:\nSxo = ReLU(Conv1d(Sx))\nThe output Sxo \u2208 RF\u00d7T from the input encoder fi is then fed into the segment encoder fs, where we use Efficient Net-B0 Tan & Le (2019), a lightweight and highly scalable convolutional neural network designed to efficiently handle 2D inputs. No modifications were made to the EfficientNet-B0 architecture, and global max pooling is applied in the final layer to produce an output embedding h\u2208R720.\nThe projection head, following EfficientNet-B0, consists of a fully connected feed-forward layer with 512 units. This is followed by Layer Normalization and a tanh activation function, which regularize the embedding space and ensure the output is in a suitable range for contrastive learning. The projection head maps the 720-dimensional embedding h from EfficientNet-B0 to a 512-dimensional latent space, used during contrastive training to facilitate the SimCLR loss. After training, the projection head is discarded, and only the output of EfficientNet-B0 is used as the final embedding for each segment."}, {"title": "4.3 Guided translation", "content": "Figure 1 illustrates the flow of the proposed guided translation method, comprising three main modules: the input encoder fi, the pseudo-classifier, and the pre-trained diffusion model.\nFirst, the input encoder converts paired speech segments sx and sy into their representations sro and Syo. The target representation Sxo, representing the latent form of the target speech, is transformed into its noisy version Sxt, where tx refers to the diffusion timestep of the target segment. This noising process follows the same forward diffusion process of the pre-trained diffusion model, with tx ~ U(1,T).\nNext, the noisy latent s\u00e6t, along with the clean source segment syo, is passed to the pseudo-classifier, which computes f(sxt\u2081)\u00b7f(syo). This step aligns the noisy latent representation of the target speech sxt at time tr with the source speech syo (at ty = 0). The diffusion model then predicts the noise (sxt,tx) present in Sxtr\nCtx'\nFinally, the predicted noise ee(Sxt,tx) and the pseudo-classifier output are used to estimate a modified noise (xt,tx) (see Equation 10). This modified noise is input into the DDIM sampling process (Equation 12) to estimate Sxt-1, refining the latent variable step-by-step. The estimated Sxt-1 is further refined through subsequent steps in the DDIM sampling process until the final de-noised target speech segment Sxo is generated. Further details are provided in the following section."}, {"title": "4.3.1 Pseudo-classifier model", "content": "Inspired by CLIP Radford et al. (2021), which learns joint representations between text and images, we propose learning joint representations of speech segments from two different languages. Unlike CLIP, which uses two separate encoders for its multimodal inputs, our pseudo-classifier employs a single encoder-the segment encoder fs to generate representations of paired speech segments from languages x and y. The output of the pseudo-classifier is the dot product fs(sx).fs (sy) between the representations of the paired segments, which measures their similarity.\nConcretely, given paired speech segments (sx, Sy), we first use the input encoder fi described in Section 4.3.1 to obtain intermediate representations Sxo \u2208 RF\u00d7T and syo \u2208 RF\u00d7T, where tx = 0 and ty = 0 for clean representations. Noise is then injected into the target segment sxo, yielding s\u00e6t at a later timestep tx. Both Sxt and Syo are then passed through the segment encoder fs. The output representations fs(sxt) and"}, {"title": "4.3.2 Unconditional pre-trained diffusion sampling for S2ST", "content": "Inspired by Nichol et al. (2021), which replaces the classifier in guided diffusion with a CLIP model for text-to-image generation, we similarly replace the classifier p(y|x+) in our guided diffusion process. Specifically, we substitute log p(y|xt) with a pseudo-classifier fs(Sxt\u2082)\u00b7fs(Syo), as described in Equation 13.\n\u20ac(Sxt,tx) := 60(Sxt,tx) - \u221a1 - \u0101trstr (fs(Sat).fs(Syo))\nThe pseudo-classifier fs(sxt)\u00b7fs (syo) is generated based on noised speech segments sxt from the target language and clean (un-noised) segment syo from language y. The term 60(Sxt,tx) refers to the pre-trained diffusion model.\nBy replacing log p(y|xt) with the pseudo-classifier fs(Sxt)\u00b7fs(syo), the diffusion process is guided to maintain semantic consistency between the two languages.\nWe also perform an ablation study making the pseudo-classifier generate fs(sxo) \u00b7 fs(Syo), i.e., the target segments are not noised. This allows us to investigate whether noising target segments provides any benefit in the guiding process. The modified guided diffusion is implemented as shown in Equation 14:\n\u20ac(Sxt,tx) := 60(Sxt,tx) - \u221a1-atasto (fs(Sxto). fs(Syo))\nOnce the modified noise \u00ea(sxta, tx) is computed, it is passed through the DDIM sampling process to iteratively refine the latent speech representation."}, {"title": "4.3.3 Conditional Pre-trained Diffusion Sampling for S2ST", "content": "Equations 13 and 14 apply when the underlying diffusion model is unconditional, i.e., the pre-trained diffusion model is modeling p(sr). To implement guided diffusion when the pre-trained diffusion model is conditional, i.e., modeling p(sx | sy), we modify Equations 13 and 14 as follows.\nThe updated form of Equation 13 is:\n\u2207\n(Sxt, Syo,tx,ty = 0) := \u20ac(Sxtx, Syo, tx,ty = 0) - \u221a1 -\u0101testa (fs(Sxt).fs(Syo))\nHere, the pre-trained diffusion model ey (Sxtr, Syo, tx,ty = 0) estimates the noise injected into Sxt when Sxtr is conditioned on Syo.\nSimilarly, Equation 14 is modified to the following form (Equation 16):\n(Sxt, Syto, tx, to) := ex(Sxt\u00e6, Syo, tx,ty = 0) 1"}, {"title": "4.4 Unified diffusion pre-trained model", "content": "Instead of training separate diffusion models for unconditional and conditional sampling, we propose a unified diffusion model that can handle both distributions."}, {"title": "4.4.1\nBackground", "content": "The goal is to design a unified diffusion model capable of capturing distributions derived from the joint distribution q(xo, yo), including:\n\u2022\nThe marginal distributions q(xo) and q(yo),\n\u2022\nThe conditional distributions q(xo|yo) and q(yo|xo),\n\u2022\nThe joint distribution q(xo, Yo).\nIn diffusion models, the marginal distribution q(x0) is modeled as the conditional distribution of noise injected into the latent variable xt, i.e., E[ext]. Similarly, the conditional distribution q(xo yo) and the joint distribution q(x0, yo) are modeled as E[ex xt, Yo] and E[ex, ext, Yt], respectively.\nThese can be unified as E[\u20ac\u00ba, \u20ac4|Xtz, Yt\u2084] Bao et al. (2023), where tx and ty are potentially different timesteps, and xt and yty are their corresponding latents. By setting ty = T, the model estimates the marginal E[ext], and by setting ty = 0, the conditional distribution E[ex|xt\u2082, Yo] is modeled. The joint distribution q(xo, yo) is modeled by setting tx = ty = t.\nA joint diffusion model for noise prediction \u20ac0(Xtz, Yty, tx, ty) is trained to predict the noise et and e injected into xt and yt\u2081, using the following objective:\nEx0,40,c,c,t,ty || 60 (xt, Yty, tx, ty) - [ex, \u20ac&] ||2\nwhere [ex, e] is the concatenation of ex and e, both sampled from a standard Gaussian distribution N(0, I), and tx and ty are uniformly sampled from U(1,T)."}, {"title": "4.4.2 Training and Sampling of the Unified Diffusion Model", "content": "The training process, as summarized in Figure 2, consists of two key stages:\nWe begin by encoding speech segments from languages x and y into low-dimensional latent embeddings, denoted as sxo and syo, using a speech encoder fs. This encoder extracts essential features from each speech signal. For the latent representation sxo, a pre-trained diffusion model applies noise, generating the noisy embedding Szt. Similarly, Syo is transformed into syty through a forward diffusion process. The noise injection can occur at different timesteps, meaning that to may not necessarily equal ty.\nOnce the noisy embeddings Sxt and syty are generated, the next task is to predict the noise injected into both segments. This is achieved using a transformer-based noise prediction network that consists of 8 transformer blocks designed to capture temporal dependencies in the data.\nWe enhance the transformer's capability by concatenating the acoustic features of both speech segments, denoted as [Cx, Cy], which are derived from their respective Mel-spectrogram representations. These concate-nated features provide additional context, improving the accuracy of noise prediction.\nEach transformer block in the model consists of the following components: - Multi-head attention (M\u041d\u0410) mechanism to capture relationships across different time steps. Feed-forward layers (FFW) to process the embeddings in a non-linear manner. Layer normalization to stabilize training.\nEach transformer block is followed by a normalization layer and a final embedding layer, which outputs the predicted noise for both segments, ef for the noisy embedding s\u00e6t, and ex for the noisy embedding Syty.\nThe model is trained to minimize the error in noise prediction. The objective function is formulated as:\nSyty\n2\n2\nwhere [6, 6] are noise values sampled independently from a standard Gaussian distribution, representing the ground truth noise injected into the latent representations Sxt and syt, at timesteps tx and ty, respectively."}, {"title": "5 Evaluation", "content": null}, {"title": "5.1 Dataset", "content": "We collected a proprietary speech and text dataset from the Kenya Broadcasting Corporation (KBC), which operates 11 radio stations broadcasting in both English and various Kenyan vernacular languages. News articles originally written in English were translated into these vernacular languages and read by presenters at different times throughout the day. This process allowed us to gather news articles in English, their corresponding translations into local languages, and the associated speech recordings. The news articles presented across different vernacular stations during the same time slots maintained the same semantic content, ensuring consistency across translations.\nThe news articles were delivered by multiple newscasters, introducing diversity in speech patterns, accents, and vocal styles, which enriches the dataset by exposing models to a wide range of speaking styles. This variation in speech enhances the dataset's utility for tasks involving speaker variation, making it valuable for speech recognition and speech-to-speech translation systems.\nWe focused on the 7 pm news bulletins, which represent the most comprehensive broadcasts, consolidating the day's news. The dataset consists of news bulletins aired between 2018 and 2023. To ensure consistency in semantic content, the speech data was pre-processed to remove advertisements, which varied across radio stations.\nFrom the 11 languages available, we selected five of the most widely spoken languages in Kenya: Swahili, Luo, Kikuyu, Nandi, and English. These languages were chosen based on their prevalence and cultural significance in Kenya, representing the major linguistic groups in the country. Details of the pre-processed dataset are provided in Table 3."}, {"title": "5.2 Segment generation", "content": "The news bulletins in the languages listed in Table 1 were segmented using the technique described in Section 4.1. Table 2 provides a summary of the average segment length and the total number of segments generated for each language.\nFrom the segmentation analysis, we observe that languages within the same phylum tend to generate seg-ments with smaller deviations in their average lengths. For example, the difference in average segment length between Nandi and Luo (both Nilo-Saharan languages) is 0.6 seconds, while the difference between Kikuyu and Swahili (both Niger-Congo languages) is 0.4 seconds. In contrast, Luo and Kikuyu, which belong to different phyla, show a larger deviation of 0.9 seconds. For these pilot languages, the trend suggests that linguistic similarities within a phylum may influence the segmentation process, affecting both the average segment length and the total number of segments generated."}, {"title": "5.3 Segment pairing", "content": "We paired segments from two languages both within a phylum and across two different phyla. We specifi-cally paired Luo-Nandi segments (same phylum), Luo-Kikuyu (cross-phyla), Kikuyu-Swahili (same phylum), Swahili-English and Luo-English. To pair the segments we used the technique described in section 4.2. The number of pairs generated for each paired languages shown in Table 3."}, {"title": "5.4 Segment encoder training", "content": "We trained a global segment encoder model, fs, using a contrastive learning approach to embed paired speech segments from different languages. The training dataset consisted of 70% of the paired segments from each language pair, as shown in Table 3.\nThe training process involved both clean and noised segments. Specifically, we introduced noise to half of the training segments by sampling a noise value ex from a standard Gaussian distribution and adding it to the latent representation of a randomly selected segment s from a given language x. The other half of the training data consisted of clean, unaltered segments. This mixture allowed the model to generalize well across both clean and noisy speech conditions.\nThe model was trained with a contrastive learning objective. During training, segments from the same language were considered positive pairs, encouraging their embeddings to be similar in the latent space, while segments from different languages were treated as negative pairs, encouraging their embeddings to be more distinct. To ensure consistency across the dataset, all speech segments were padded to match the length of the largest segment, which was 20 seconds. This uniform padding ensured that all input data had the same dimensions, facilitating efficient batch processing during training.\nThe segment encoder was pre-trained over 1 million steps, with a batch size of 512. We employed the AdamW optimizer with hyperparameters \u03b2\u2081 = 0.9, \u03b22 = 0.98, and e = 10-9 to stabilize the training process and prevent overfitting.\nFurther, the learning rate was initialized to 1 \u00d7 10-4 and reduced progressively using a cosine annealing schedule, which helped optimize the convergence of the model. Regular checkpoints were saved during training to evaluate the model's performance on the validation set, ensuring that we maintained the best-performing version of the encoder."}, {"title": "5.5 Sentence pairing", "content": "For each speech collected, we used the corresponding news article that was read as the ground truth reference text. To create parallel text datasets, we manually paired sentences from the collected news articles. The pairing process aimed to match semantically similar sentences across the five languages shown in Table 3. This resulted in five sets of parallel text datasets, one for each pair of languages.\nThe manual pairing process was guided by semantic similarity, ensuring that the sentences conveyed the same meaning across languages. These parallel datasets are crucial for evaluating translation performance and for training language models on semantically aligned data."}, {"title": "5.6 Automatic Segment Pairing Accuracy Evaluation", "content": "We pre-trained five language-specific ASR models to generate text from the corresponding speech in any of the five languages listed in Table 4. For the Nandi, Kikuyu, and Luo languages, we used the Squeezeformer model Kim et al. (2022), while for Swahili and English, we fine-tuned the Whisper small model Radford et al. (2023). Table 4 reports the word error rates (WER) of these pre-trained ASRs."}, {"title": "5.7 Manual Segment pairing evaluation", "content": "For manual segment pairing evaluation, we recruited undergraduate students who were native speakers of Nandi, Luo, and Kikuyu-the primary languages used in this study. The students were selected from a pool of BSc students who responded to an advertisement posted on a notice board. A total of 2,367 students volunteered, and all of them were included in the study. Each student received a token payment of Ksh 200 for their transcription work.\nAll participants were proficient in both English and Swahili, making them well-suited for accurately tran-scribing the speech data. For each language pair listed in Table 3, we randomly selected 1,000 segment pairs. Each student was tasked with listening to a segment in their native language and transcribing it into English, which served as the common reference language for comparison.\nEach student was assigned 20 segments and was required to provide both the segment number and its transcription. For each segment pair (sx,sy), the segment Sr was given to a student proficient in language x, while the segment sy was assigned to a student proficient in language y. Both students transcribed their respective segments into English.\nTo ensure transcription accuracy, each segment sx was transcribed by at least two students. Additionally, two independent evaluators (verifiers) compared the transcriptions of the same segment for similarity. The verifiers used a scale from 1 to 5, where 5 represented complete similarity, to rate the consistency between the two transcriptions. If the similarity score between any two transcriptions for a given segment was 4 or higher, the transcription was considered accurate.\nWe used BLEU scores to compute precision, recall, and F-measure as described in Section 4.7. Figure 4 reports the results.\nFinally, we computed BLEU scores between the verified transcriptions of segment s and segment sy to evaluate the semantic alignment of paired segments across languages. A higher BLEU score indicated stronger semantic alignment, allowing us to quantitatively assess the effectiveness of our segment pairing method.\nThe results of the manual evaluation show a similar trend to that of the automatic evaluation, where the location-based segment pairing technique performs better when the two languages are from the same phylum. For example, the manual evaluation for Luo-Nandi (same phylum) achieved precision, recall, and F-measure scores of 66.4%, 63.8%, and 65.1%, respectively. This closely aligns with the results obtained from the automatic ASR + Search method, which yielded precision, recall, and F-measure scores of 67.32%, 65.44%, and 66.37%, respectively."}, {"title": "6 Effect of Segment Length on Segment Mapping Quality", "content": "In this section, we investigate how segment length impacts the quality of segment pairing. We categorized a segment pair as long if both segments of the pair are longer than the average segment length l for language x, as defined in Table 2. A segment pair was categorized as short if both segments were shorter than lx, and as average length if both segments in the pair were approximately equal to lx.\nTo evaluate the effect of segment length, we randomly selected 1000 short, average, and long segment pairs from three language pairings: Luo-Nandi, Luo-Kikuyu, and Kikuyu-Swahili. We used the Method 2: ASR + Search technique to evaluate the quality of the segment matches for each category.\nFigure 5 summarizes the results, showing that segment pairs of average length consistently achieve the highest pairing quality, with the lowest variability across all three language pairings. Short segments also performed relatively well but exhibited slightly more variability. Long segment pairs demonstrated the lowest pairing quality and the highest variability, suggesting that longer segments are more prone to quality degradation during the pairing process.\nThis analysis indicates that as segment length increases, the likelihood of a drop in pairing quality also increases. Upon Manual investigation on why the quality of long segment pairings was lower, we found that many paired long segments contained uneven spillover sentences. In these cases, both segments included a complete sentence and half of another sentence, but the segments did not end at the same time. This discrepancy introduced errors in the pairing process, reducing the overall quality."}, {"title": "6.1 Diffusion Model Training", "content": "To optimize translation accuracy between language pairs, we trained five unified diffusion models, each specifically for a unique language pair listed in Table 3, as illustrated in Figure 3. These models, collectively referred to as the Segment-Aware Unified Diffusion Model (SegUniDiff), were trained using 70% of"}, {"title": "7 Speech Translation Generation", "content": "We evaluated the efficiency of the speech translation process by measuring the time taken by the model to generate a single n-frame speech sample. The reported translation speed represents the average time per speech sample across the entire test dataset. To standardize the evaluation, we down-sampled the target segment sxo to 24 kHz and extracted 128-dimensional mel-spectrogram features using a 50 ms Hanning window, a 12.5 ms frame shift, and a 2048-point FFT.\nThe latency analysis was conducted on a V100 GPU, and the results are illustrated in Figure 6. The proposed guided diffusion technique, evaluated in both unconditional and conditional settings, demonstrated a nearly constant translation speed across different input configurations. This stands in contrast to the cascaded approach, which exhibited a linear increase in translation time as the number of frames in the input segment grew."}, {"title": "8 Limitation of study", "content": "\u2022\nDataset Specificity: The dataset used in this study originates from news broadcasts, characterized by formal language, clear pronunciation, and controlled acoustic environments. While this dataset is suitable for initial evaluation, it may not capture the diversity and complexity of everyday con-versational speech, which includes spontaneous language and varied acoustic conditions. Evaluating the model's performance on more diverse datasets, especially those with conversational speech and challenging acoustic conditions, would be necessary to establish its robustness and generalizability.\n\u2022\nLimited Language Selection: This study focuses on a specific set of Kenyan languages, allowing an exploration of linguistic similarities within a closely related group potentially influenced by geo-graphical proximity. Expanding to languages from varied geographical regions and language families would provide insight into the broader applicability of this approach."}, {"title": "9 Conclusion", "content": "This work investigates how similarity within African languages grouped within a phylum can be exploited to achieve direct S2ST. We implement a segment-based translation using guided diffusion. We evaluate the developed model using different language pairs of speeches. The speeches are paired both within a phylum and across a phylum. The evaluation results show that when languages have close"}]}