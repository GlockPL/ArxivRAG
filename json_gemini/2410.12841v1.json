{"title": "UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models", "authors": ["Jiayi Guo", "Liyun Zhang", "Yiqin Shen"], "abstract": "Automated Machine Learning (AutoML) has streamlined the complexities of machine learning. However, existing AutoML frameworks focus on discriminative tasks leaving generative tasks unexplored. Moreover, existing AutoML frameworks often lack interpretability and user engagement during the model development and training process, due to the absence of human-centered designs. To address these challenges, we propose UniAutoML, a human-centered AutoML framework that leverages Large Language Models (LLMs) to unify the automation of both discriminative (e.g., Transformers and CNNs for classification or regression) and generative tasks (e.g., fine-tuning diffusion models or LLMs). UniAutoML enables natural language interactions between users and the framework to provide real-time feedback and progress updates, thus enhancing the interpretability, transparency, and user control, where users can seamlessly modify or refine the model training in response to evolving needs. To ensure the safety and reliability of LLM-generated content, UniAutoML incorporates a safety guard-line that filter inputs and censor outputs. We evaluated UniAutoML's performance and usability through experiments conducted on eight diverse datasets and user studies involving 25 participants. Our results demonstrate that UniAutoML not only achieves higher performance but also improves user control and trust in the AutoML process, which makes ML more accessible and intuitive for a broader audience.", "sections": [{"title": "Introduction", "content": "Automated Machine Learning (AutoML) has emerged as a user-friendly solution designed to simplify the complex process of building and training machine learning (ML) models (Hutter, Kotthoff, and Vanschoren 2019). By automating model selection, hyperparameter tuning, and code generation, AutoML makes ML more accessible to individuals without ML expertise. This automation not only accelerates model development but also reduces the cognitive load for both novice and experienced users by simplifying complex workflows and minimizing repetitive tasks. However, existing AutoML frameworks, such as AutoGluon (Erickson et al. 2020) and Auto-sklearn (Feurer et al. 2020), predominantly focus on discriminative models, leaving a significant gap in their applicability to generative models-an area of growing importance, especially with the rise of diffusion models (Ho, Jain, and Abbeel 2020) and Large Language Models (LLMs). This limitation is particularly challenging for non-experts who might need to engage with generative modeling tasks but find current AutoML solutions inadequate. Furthermore, traditional AutoML frameworks often provide limited interpretability and transparency, offering minimal explanations for the steps involved and lacking real-time feedback during the AutoML process. This \"black box\u201d nature can diminish user trust and hinder the adoption of AutoML tools, especially among those who seek to understand and interact with the models they use. The inability to easily modify or fine-tune selected models further exacerbates this issue, underscoring the need for a more human-centered approach to AutoML that bridges the gap between discriminative and generative models while enhancing user engagement and trust.\nTo address these limitations, we introduce UniAutoML, a novel framework that unifies AutoML for both discriminative and generative tasks. UniAutoML not only supports traditional discriminative models such as Transformers and Convolutional Neural Networks (CNNs) for tasks like classification and regression but also extends its capabilities to advanced generative models, including diffusion models and large language models (LLMs). A key feature of UniAutoML is its automated fine-tuning process, enabling users to train these complex models effortlessly through natural language commands, eliminating the need for manual coding. For instance, users can fine-tune diffusion models by simply providing datasets, while LLMs can be fine-tuned using the XTuner (Contributors 2023) tool by specifying a configuration name or address. This approach significantly lowers the barriers for non-experts, allowing them to engage with advanced generative and discriminative models without requiring deep technical expertise. Another innovation in UniAutoML is its human-centered design, featuring a conversational user interface (CUI) that allows users to interact with the model through natural language. UniAutoML eliminates the need for users to have prior knowledge of Python or specific AutoML tools, thereby broadening the accessibility of AutoML. The CUI offers an intuitive and interactive experience, guiding users step-by-step through the AutoML process while providing real-time updates on model selection, training progress, and performance metrics. This continuous feedback loop not only enhances transparency but also empowers users to make informed decisions, such as adjusting model parameters or terminating underperforming trials mid-process. Additionally, UniAutoML leverages the vast repository of pre-trained models available through HuggingFace, significantly expanding the range of models accessible to users compared to traditional frameworks like AutoKeras and Auto-sklearn, which are confined to their respective pre-trained modules. It not only increases the versatility of UniAutoML but also ensures that users have access to the latest and most suitable models for their tasks with minor efforts on maintenance. To realize the potential of UniAutoML, we employ LLMs as the core component of the framework. However, recognizing the potential risks associated with LLM-generated content, we design a safety guard-line that filters user inputs and censors outputs, ensuring that UniAutoML remains focused on AutoML tasks and mitigating potential security or ethical concerns.\nThe major contributions are five-fold. Firstly, we propose the first AutoML framework that unifies both discriminative and generative tasks, including automated fine-tuning for diffusion models and LLMs. This integration reduces the learning curve for users working with these advanced models, making complex ML tasks more accessible to non-experts. Secondly, we introduce a human-centered design to enhance both user control and interpretability. It not only provides detailed AutoML outputs and progress information but also enables users to actively participate in the AutoML process. Users can modify model parameters, terminate underperforming trials, and make informed decisions in real-time, fostering a deeper understanding and more effective control over the entire ML pipeline. Thirdly, UniAutoML features connectivity with HuggingFace, allowing it to retrieve and utilize a wide range of pre-trained models, which thus eliminates the need for local maintenance of model libraries and significantly enhances the scalability and versatility of AutoML. Users can access the latest and most suitable models for their tasks with minimal effort, keeping pace with rapid advancements in the field. Forth, we implement a safety guard-line mechanism that addresses potential safety and ethical concerns associated with LLM use. It carefully inspects both user inputs and LLM outputs, maintaining high standards of security and ethicality within our UniAutoML framework. Finally, we conduct a comprehensive evaluation of UniAutoML through both quantitative experiments and user studies."}, {"title": "Methods", "content": "Overview of UniAutoML UniAutoML is an innovative human-centered AutoML framework that leverages LLMs to unify the automation of both discriminative and generative machine learning tasks, as illustrated in Fig. 1. Users initiate by providing task requirements and datasets descriptions through plain natural language. The model selection module, implemented as LLMs, then identifies the most suitable pre-trained models from HuggingFace by comparing model features with task requirements and dataset characteristics, supporting both discriminative and generative tasks. For model configuration, UniAutoML acquires corresponding settings from HuggingFace and defines the hyperparameter search space. To ensure efficient handling of diverse data modalities, the data pre-processing module generates code for data loading and preparation. The model training module then constructs the pipeline for model training, incorporating techniques like Low-Rank Adaptation (LORA) (Hu et al. 2021) for fine-tuning generative models when necessary. Throughout this process, an LLM-Explainer module provides real-time interpretations of each decision and output, enhancing user understanding and control. Additionally, a safety guard-line mechanism is implemented to filter both user inputs and LLM outputs, preventing the generation or propagation of harmful or irrelevant information.\nUnified AutoML for Discriminative and Generative Tasks UniAutoML seamlessly integrates support for both discriminative and generative tasks within a single framework. Importantly, the pre-processing, model configuration selection, and safety guard-line modules are shared across both discriminative and generative tasks, ensuring a consistent and efficient workflow regardless of the task type. The key differences lie in the model selection and model training modules, which are tailored to the specific requirements of discriminative and generative tasks. For discriminative tasks, UniAutoML the model selection builds upon the structure of AutoGluon (Erickson et al. 2020), supporting a wide array of models suitable for classification, regression, and other predictive tasks across multiple data modalities, including image, text, and tabular data. Specifically, the model selection module in UniAutoML is powered by LLM to ensure that the most appropriate pre-trained discriminative models are chosen based on the specific requirements of the task and the characteristics of the input data. In the realm of generative tasks, UniAutoML introduces automation for fine-tuning both diffusion models and LLMs. For diffusion models, users simply provide the dataset address and express their intention to use a generative model (e.g., diffusion model) through natural language. UniAutoML then selects the diffusion model with the model selection module, and the model training module employs LoRA to fine-tune the model on the provided dataset. Once training is complete, users can generate images by inputting prompts through the CUI again, leveraging the capabilities of the fine-tuned model. LLM fine-tuning in UniAutoML is facilitated through seamless integration with XTuner (Contributors 2023). Users need only specify the name or path of the LLM configuration they intend to fine-tune, along with their desired storage path for the resulting model.\nModel Selection Module This module in UniAutoML leverages LLM to comprehend and categorize task requirements based on the user query. The process begins by determining whether the task is generative or discriminative, along with identifying the relevant data modalities. In this step, the LLM acts as both a binary classifier for task type and an information extractor for modalities, represented as $t_c = \\text{LLM}(p_c; q)$ and $\\text{Modality} = \\text{LLM}(p_m; q)$, where $t_c$ is the task category (generative or discriminative), Modalitya is the set of identified data modalities, $p_c$ and $p_m$ are prompts for classification and modality extraction respectively, and q is the user query. Afterwards, a more specific task type such as image classification, text generation, or tabular regression is deduced, represented as:\n$\\arg \\max_{s_t \\in T} \\text{sim}(\\text{Emb}(q \\oplus t_c \\oplus \\text{Modality}), \\text{Emb}(s_t))$,\nwhere $s_t$ is the determined specific task type, T is the set of all task types, sim is a similarity function (i.e., cosine similarity) between the embeddings, Emb refers to the embedding function that converts text into dense vector representations, and $\\oplus$ denotes concatenation. The model cards from HuggingFace, containing metadata for both generative and discriminative models, are encoded in the same vector format to create separate vector databases for each task category. Let $D_g$ and $D_d$ represent the vector databases for generative and discriminative models respectively: $D_g = \\{\\text{Emb}(m) | m \\in M_g\\}$ and $D_d = \\{\\text{Emb}(m) | m \\in M_d\\}$, where $M_g$ and $M_d$ are the sets of generative and discriminative models. This separation ensures that the model selection is tailored to the specific requirements of generative and discriminative tasks. Next, the selected task type $s_t$ is compared against the appropriate vector database of models to identify an informed shortlist of 10 candidate models as follows:\n$M_c = \\text{top}_k(\\text{sim}(\\text{Emb}(s_t), \\text{Emb}(m)) | m \\in D, k = 10)$\nwhere $M_c$ is the set of candidate models, D is either $D_g$ or $D_d$ depending on $t_c$, and topk selects the k models with the highest similarity scores. Finally, these shortlisted models are further evaluated against the specific task requirements to identify the most suitable model:\n$m_b = \\arg \\max_{m \\in M_c} \\text{(sim(Emb(q), Emb(m))}$,\n(1)\nwhere $m_b$ is the best model selected based on the comparison between the candidate models $M_c$ and user query q.\nData Pre-processing and Model Training Modules To ensure efficient handling of diverse data modalities, UniAutoML's data pre-processing module generates code for data loading and preparation. This module adapts to the specific requirements of both discriminative and generative tasks, ensuring that the data is appropriately formatted for the selected model. The model training module then constructs the pipeline for model training, incorporating techniques like LORA for fine-tuning generative models when necessary. For discriminative tasks, it builds upon AutoGluon, while for generative tasks, it integrates specialized fine-tuning approaches for diffusion models and LLMs.\nEnhanced Interpretability and User Interaction in Uni-AutoML At the core of our interpretation mechanism lies an LLM-Explainer module, that is designed to elucidate the rationale behind each decision of UniAutoML as well as the output into plain and user understanable language. For each stage, once an output is yielded, it is transmitted to the LLM-Explainer module, which provides an explanation, represented as:\n$\\text{Exp}_r = \\text{LLM-Explainer}(p_e;r \\oplus q \\oplus c)$,\n(2)\nwhere $Exp_r$ is the explanation for the result, $p_e$ is the system prompt for the LLM-Explainer, r is the output result, and c is the context information e.g., model card, modality information. For instance, during the model selection stage, after determining the most suitable model for the given task, the model card, user instructions, and modality information are conveyed to the LLM-Explainer as follows:\n$\\text{Exp}_{m_b} = \\text{LLM-Explainer}(p_e;m_b \\oplus q \\oplus t_c \\oplus \\text{Modality}_a)$,\nwhere $Exp_{m_b}$ is the explanation for the selected model $m_b$. This design assists users in grasping the decision-making mechanisms of UniAutoML but also boosts their confidence in the validity of the chosen model. Moreover, the LLM-Explainer is capable of interpreting error information, providing a comprehensive breakdown with probable causes and potential resolutions:\n$\\text{Exp}_{err} = \\text{LLM-Explainer}(P_{err}; err \\oplus q \\oplus c)$,\n(3)\nwhere $Exp_{err}$ is the explanation to the error message, $P_{err}$ is the prompt for error interpretation, and err is the error information. Users are prompted at the end of each stage with information about the completed phase as well as the upcoming stage. They are also given the opportunity to provide additional instructions for the next stage:\n$I_{next} = LLM(p_n; S_c \\oplus S_n \\oplus q)$,\n(4)\nwhere $I_{next}$ is the information about the next stage, pn is the prompt for next stage information, $S_c$ is the current stage, $S_n$ is the next stage, and q is the user query. Furthermore, we have incorporated a progress indication tool, particularly during the model training stage. This live tracking mechanism provides users with an estimated completion time and a sense of temporal orientation.\nEnhance User Control with Human-Centered Design The interaction of UniAutoML with user introduces several advancements to improve user control and engagement. Firstly, addressing the need for adaptability, UniAutoML introduces an interactive override feature where users can manually interrupt the workflow if intermediate results don't meet their expectations based on the LLM-Explainer module or if additional guidance is necessary. Specifically, UniAutoML is engineered to recognize the context of these user interventions precisely, discerning the specific stage they pertain to, allowing the model to retrace its steps and recommence operations from the implicated juncture, seamlessly integrating new directives. This design maintains the delicate balance between automation efficiency and user discretion, allowing alterations to the automatic course without compromising the integrity of the entire process, whether for discriminative or generative tasks. Moreover, leveraging the LLM-Explainer module, UniAutoML provides real-time explanations and feedback on each decision made during the AutoML process, where users can query the system about its choices, request clarifications, or suggest modifications at any point. This continuous dialogue ensures that users understand the rationale behind each step and can steer the process towards their specific requirements. For generative tasks, such as fine-tuning diffusion models or LLMs, this feature is particularly valuable as it allows users to refine the training process based on intermediate results or specific domain knowledge.\nSafety Guard-Line Module LLM can present potential safety and ethical concerns. To address these issues and ensure the responsible use of LLMs within UniAutoML, we have implemented a safety guard-line module, which acts as a safeguard to monitor and filter both user inputs and LLM outputs throughout the AutoML pipeline. The safety guard-line module operates in two stages. First, it scrutinizes user instructions to filter out irrelevant or potentially harmful information to ensure that only appropriate and task-relevant inputs are processed by the LLM, formulated as $I_{filtered} = \\text{LLM}(P_{filter}; I_{user})$ where $I_{filtered}$ is the filtered input, $P_{filter}$ is the system prompt for input filtering, and $I_{user}$ is the original user input. Following the processing of filtered inputs, this module then examines the LLM's output for any content that may compromise user safety or raise ethical concerns. If such content is detected, the module prompts the LLM to revise its output. This iterative refinement process can be represented as $O_{safe} = \\text{LLM}(P_{revise}; O_{initial} \\oplus C_{critique})$ where $O_{safe}$ is the safe, revised output, $P_{revise}$ is the system prompt for output revision, $O_{initial}$ is the initial LLM output, and $C_{critique}$ is the critique of the initial output."}, {"title": "Experiments", "content": "Implementation Details We conducted our experiments using two NVIDIA A6000 GPUs with CUDA 11.7. The software environment consisted of Python 3.8 and PyTorch 2.0.1. For the LLM component, we utilized GPT-40 (OpenAI 2023). To ensure consistency and reproducibility, we set the 'Temperature' parameter, which controls the model's creativity, to 0. For a detailed description of the prompts used at each stage of our framework, please refer to Appendix.\nQuantitative Evaluation on Discriminative Tasks Our quantitative evaluation assesses the performance of Uni-AutoML in comparison to other AutoML frameworks. We focused on two aspects, namely the model selection module and the model training module. It's important to note that in this evaluation, we concentrate solely on discriminative tasks, because there are currently no existing AutoML frameworks for generative models. Consequently, the evaluation of UniAutoML's performance on generative AutoML tasks will be addressed from a human-centric perspective in our user study.\nBaselines We compared UniAutoML against two prominent AutoML frameworks, AutoGluon (Erickson et al. 2020) (version 1.0.0) and AutoKeras (Jin et al. 2023) (version 2.0.0). These baselines were chosen to represent different approaches within the AutoML landscape. AutoKeras primarily employs neural architecture search (NAS), while AutoGluon focuses on hyperparameter optimization (HPO). To ensure a fair comparison, we configured these baselines with equivalent dataset addresses, training quality parameters, and epoch settings as used in UniAutoML.\nDatasets To evaluate UniAutoML's performance across various tasks and data modalities, we selected a diverse set of datasets, primarily sourced from Kaggle. Table 1 provides an overview of the eight multimodal datasets used in our experiments, detailing their size, task type, and evaluation metrics. The datasets encompass a range of ML tasks,"}, {"title": "Discussion and Conclusion", "content": "Discussion UniAutoML represents an advancement in AutoML, addressing limitations of existing AutoML frameworks by unifying both discriminative and generative tasks. The integration of LLMs enhances the framework's ability to handle complex, multimodal datasets and enables more intuitive user interactions. The human-centered design, featuring a conversational interface and real-time explanations, makes ML more accessible to non-experts. As AI continues to evolve, methods like UniAutoML that prioritize user engagement, transparency, and versatility will becoming more important in democratizing ML and shaping its future applications.\nLimitations and Future Directions Despite the promising results of UniAutoML, several important limitations must be addressed. Firstly, the financial implications of using state-of-the-art LLMs like GPT-40 cannot be overlooked. The substantial costs associated with API access for complex AutoML tasks may prove prohibitive for many users, particularly in resource-constrained environments. To address this, future work should explore more cost-effective alternatives, such as efficient local deployment of LLMs, optimization of prompts for conciseness, and the potential use of smaller, task-specific language models fine-tuned for AutoML tasks. Another limitation is the current implementation's heavy reliance on internet connectivity for both LLM usage and retrieval of pre-trained models. This dependency restricts the framework's applicability in offline or low-connectivity environments, which are common in many real-world scenarios. Future research can also focus on developing strategies for local deployment of LLMs and pre-trained models, creating compact, offline-capable versions of the framework, and implementing caching mechanisms to reduce the need for constant network access.\nConclusion UniAutoML automates the entire ML pipeline while providing unprecedented user interactivity and interpretability through CUI and real-time explanations. Our comprehensive evaluation, including quantitative experiments and a user study, demonstrates that UniAutoML not only achieves competitive or superior performance compared to traditional AutoML frameworks but also enhances user experience, control, and trust. By reducing the cognitive burden on users and making advanced ML methods more accessible, UniAutoML represents a step towards democratizing AI."}, {"title": "Prompts", "content": "This section presents the prompts used in different modules of UniAutoML to guide the LLM. The color scheme in these prompts serves several purposes to enhance readability and highlight important information:\n\u2022 Blue text is used for headings and main instructions.\n\u2022 Orange text highlights variable or injectable content. It's used for placeholders like {modality},{configs}, or {configuration_data}.\n\u2022 Black text on the light background is used for the main content.\nPrompt for the Modality Inference in Model Selection Module The following prompt corresponds to the model selection module to accurately identify and categorize the data modalities present in the input dataset.\nPrompt for Modality Inference in Model Selection Module\nRole: You are an AI assistant specializing in analyzing data modalities from multimodal data.\nTask: Identify the data type of each column within a pandas.DataFrame provided as dataset.\nOutput Format: Strictly adhere to the following JSON format without additional context: {\"column_name\": \"data_type\"}\nInstructions:\n\u2022 Determine data types based on column names, data content, and user-provided context, which may include task or dataset context.\n\u2022 Ensure all columns are analyzed and included in the output.\n\u2022 Identify the label column (typically present as the target variable for prediction or classification).\nModality Examples: text, image, audio, video, document, table, semantic_seg_img, ner, categorical, numerical, label.\nReference Examples:\n1. Input: instructions: description_A, Data: input_A Output: output_A\n2. Input: instructions: description_B, Data: input_B Output: output_B\n3. Input: instructions: description_C, Data: input_C Output: output_C\nYour Task: Input: instructions: description_D, Data: input_D Output:\nPrompt for the Preprocessing Module The following prompt guides the LLM in generating code for data processors in UniAutoML.\nPrompt for Data Preprocessing Module\nRole: You are an AI assistant specializing in writing data processor code in an AutoML task.\nTask: Write a function to return the corresponding data processors based on the model's configuration.\nOutput Format: Strictly adhere to the following dict format: {\"data_processor_codes\": \"codes\", \"reason\": \"reason for choosing the data processor\"}\nInstructions:\n\u2022 Follow the provided function structure and naming conventions.\n\u2022 Modify the code based on the dataset modality {modality}, processing only the modalities present.\n\u2022 Load training data into each processor from the global 'dataset' variable.\n\u2022 Include a label data processor for each model.\n\u2022 Write only the code for defining the processor function, not for executing it.\n\u2022 For image processors, set train_transforms = val_transforms based on {image_cfg} and your knowledge.\n\u2022 For semantic segmentation image processors, assign img_transforms and gt_transforms based on {semantic_seg_img_cfg} and your knowledge.\nPrompt for Fusion Model Generation\nRole: You are a helpful assistant that writes the Deep learning model code.\nTask: You task is to only write a fusion model to fuse different base models' features without any explanation. Use # before every line except the python code.\nInstructions:\n\u2022 Here are some model code for your reference: Given some base models' config as follow:{base_configs}; Give the fusion model config as follow: {fusion_config} You should then respond to me the code with:\n\u2022 Fusion technique should be learnable, MLP is recommended.\n\u2022 The fusion model structure should be defined as fusion model and fusion head, which output features and logits, respectively.\n\u2022 Base models instance should be defined in Fusion model Class. You should not change the value of the output of base model instances.\n\u2022 All base models have a uniform variable( self.out features dim) to represent the output features dimension.\n\u2022 Finding the maximum dimension of all base models' output features, and define learnable linear layers to adapt all base models' output features to the maximum dimension as the input of fusion model. For example, if three models have feature dimensions are [512, 768, 64], it will linearly map all the features to dimension 768.\n\u2022 Output the logits, features, loss weights of fusion model and base models. The return must be in a JSON format: model name:\"logits\":...,\"features\":...,\"weight\".....\n\u2022 All the network layers and variable self.model name,self.loss weight should be defined in function init, not in function forward.\n\u2022 Some variables are not present in each model's config, you cannot use a variable that does not exist in the corresponding model config.\nPrompt for the Hyperparameter Description in Model Training Module\nRole: You are an AI assistant specializing in describing machine learning hyperparameters.\nTask: Add descriptions for the parameters in a machine learning training configuration.\nOutput Format: Strictly adhere to the following JSON format without additional content: {\"hyperparameter_name\": \"description\"}\nInstructions:\n\u2022 Provide clear and concise descriptions for each hyperparameter.\n\u2022 Ensure that descriptions do not include specific values from the configuration.\n\u2022 Focus on the purpose and impact of each hyperparameter on the model's behavior or performance.\n\u2022 Use technical language appropriate for machine learning practitioners.\nGiven Information: Model configurations: configs\nYour Task: Generate descriptions for all hyperparameters present in the given model configurations.\nPrompt for Auto Diffusion\nRole: You are a code engineer that helps user to write code to use Diffusion model\nTask: Your task is to first infer Diffusion model, then fine tune it use LORA and train dataset which is stored at {dataset_address}. Instructions:\n\u2022 Here are some code for you to refer to, it is an example of how to infer this model\nPrompt for Fine-Tuning Diffusion Models in the Model Training Module\nPrompt for Task Type Selection\nRole: You are an assistant that helps user to switch mode of a model.\nTask: User will give you its task requiremeng for that AutoML model. Your task is to respond to user input by classifying it.\nInstructions:\n\u2022 If the user wants to conduct discriminative task, respond discriminative\n\u2022 If the user wants to conduct task using Diffusion model, respond diffusion\n\u2022 If the user wants to conduct task use fine-tuned LLM, respond LLM\nPrompt for Preprocessing Dataset with XTuner The following prompt guides the LLM in generating data to preprco-cess raw data for fine-tuning"}]}