{"title": "Diversified and Adaptive Negative Sampling on Knowledge Graphs", "authors": ["Ran Liu", "Zhongzhou Liu", "Xiaoli Li", "Hao Wu", "Yuan Fang"], "abstract": "In knowledge graph embedding, aside from positive triplets (i.e., facts in the knowledge graph), the negative triplets used for training also have a direct influence on the model performance. In reality, since knowledge graphs are sparse and incomplete, negative triplets often lack explicit labels, and thus they are often obtained from various sampling strategies (e.g., ran-domly replacing an entity in a positive triplet). An ideal sampled negative triplet should be informative enough to help the model train better. However, existing methods often ignore diversity and adaptiveness in their sampling process, which harms the informativeness of negative triplets. As such, we propose a generative adversarial approach called Diversified and Adaptive Negative Sampling (DANS) on knowledge graphs. DANS is equipped with a two-way generator that generates more diverse negative triplets through two pathways, and an adaptive mechanism that produces more fine-grained examples by localizing the global generator for different entities and relations. On the one hand, the two-way generator increase the overall informative-ness with more diverse negative examples; on the other hand, the adaptive mechanism increases the individual sample-wise informativeness with more fine-grained sampling. Finally, we evaluate the performance of DANS on three benchmark knowledge graphs to demonstrate its effectiveness through quantitative and qualitative experiments.", "sections": [{"title": "1. Introduction", "content": "Knowledge graphs have been widely used to encode facts about the real world. Typically, each fact describes a relationship between a head and tail entity in the form of a triplet (head, relation, tail), and different entities across facts are interconnected to form a graph structure. The rich facts contained in a large-scale knowledge graph can be used to enhance numerous applications that rely on real-world knowledge, such as question answering [41, 16, 33], object detection [9, 15, 19] and recommendation [4, 12, 8, 42].\nTo effectively exploit the facts for these applications, a common approach is to first perform knowledge graph embedding that converts the symbolic entities and relations to a latent vector space. The learned embedding aims to capture relevant structural and semantic information in the knowledge graph, which can then be integrated with other machine learning models.\nIn this paper, we focus on the problem of knowledge graph embedding. The high-level idea is that the embedding vectors of entities and relations co-occurring in the same fact should be bounded by certain constraints due to their relatedness. For instance, consider a fact r = (h = Beijing,r = isCapital0f,t = China) and a classic method TransE [2]. TransE maps each entity and relation in the fact to vectors, i.e., $e_h$, $e_r$, $e_t$, respectively, so that they approximately satisfy the constraint $e_h + e_r \\approx e_t$ by minimizing the loss $||e_h + e_r - e_t||$. On the contrary, a nonfact such as (h = Beijing, r = isCapitalof, t' = Russia) would maximize the loss $||e_h + e_r et. Given this contrast, the factual triplets are known as positive triplets (or examples), whereas the non-factual triplets are called negative triplets. Although posi-tive triplets are readily available, negative triplets are often obtained through random sampling. More recent works [43, 3, 1, 48, 30, 50] explore advanced constraints or losses [2, 36, 44] on the triplets, but the sampling strategy for negative triplets remains a crucial yet less explored problem.\nEarlier negative sampling approaches resort to random sampling, e.g., by replacing the tail (or head) entity in a positive triplet with a random entity from the knowledge graph sampled in a uniform [42] or popularity-weighted manner [20]. Although random sampling is straightforward, it is often inade-quate to optimize the informativeness of negative triplets. The informative-ness refers to how much information each negative triplet could contribute"}, {"title": "2. Background", "content": "Negative sampling is an important issue in various machine learning tasks such as recommendation systems [28] and natural language processing [20]. In the context of knowledge graph embedding, negative triplets are often constructed by replacing the tail or head entity in a positive triplet with a randomly sampled entity [2, 40, 17]. Unfortunately, in uniform [2] or popularity-weighted sampling [20], the sampled entity could be completely unrelated to the head or the relation, and therefore be less informative.\nTo sample more informative negative triplets, researchers have leveraged different heuristics or learning strategies. Several structure-aware models [1, 46, 18] exploit the graph structures, which generally select negative ex-amples in the neighborhood of positive examples. For example, SANS [1] hypothesizes that entities that are in close proximity to each other, but do not share a direct relationship, are better candidates for negative sampling. In a similar spirit, PinSage [18] generates localized graphs via random walks to extract informative negative samples. However, these approaches have a high risk of selecting false negatives, as not explicitly related entities in close proximity could still form positive triplets due to the incompleteness of the observed graph.\nOther approaches seek to quantify the informativeness of the negative triplets through various learning strategies, including GANs [3, 38, 11, 47], reinforcement learning [39, 49], and importance sampling [48]. These meth-ods provide a more explicit and systematic scoring of negative triplets which often led to better performance. However, these approaches do not consider"}, {"title": "3. Methodology", "content": "In this section, we introduce the problem formulation and some prelim-inaries on knowledge graph embedding, followed by our proposed approach DANS."}, {"title": "3.1. Problem formulation and preliminaries", "content": "A knowledge graph (KG) is defined by an entity (node) set V, a relation set R and a ground-truth or positive triplet (edge) set E. Given a triplet \u03c4 = (h, r, t) for some h, t \u2208 V and r \u2208 R, a typical KG model aims to learn a scoring function F(\u03c4) to estimate the probability that \u03c4 is a positive triplet, i.e., Tis a fact that should appear in the ground truth set E.\nGiven the power of graph convolutional networks, in this paper, we adopt a multi-layer relational graph convolutional network (RGCN) [29] to serve as our base embedding model. The base model encodes the entities in layer 1+1 into vectors $e_i^{l+1} \\in R^{d^{l+1}}$ in a latent embedding space, by aggregating their embeddings $e_i^l \\in R^{d^l}$ from the previous layer l, as follows.\n$e_i^{l+1} = RELU (\\sum_{r \\in R} \\sum_{j \\in N_i^r} W_r^l e_j^l + W_0^l e_i^l)$, (1)\nwhere $N_i^r$ is the set of neighbors of entity i under relation r, $W_r^l$ is a trainable weight matrix for r, $W_0^l$ is an additional trainable weight matrix to capture the self-information of each entity in layer 1, and RELU is the activation function. Assuming a total of L layers are stacked, the embeddings in the last layer are the output embeddings, which we simply write as $e_i \\in R^d$, Vi \u2208 V.\nTo optimize the parameters, a set of training triplets Dtr that consists of both positive and negative triplets is used. As shown in Figure 1(d), our objective is to sample a set of high-quality negative triplets, which, together with positive triplets, will be used to minimize the following cross-entropy loss:\n$- \\sum_{\\tau \\in D_{tr}} y_{\\tau} log F(\\tau) + (1 - y_{\\tau}) log (1 \u2013 F(\\tau))$ (2)\nwhere $y_\u03c4$ = 1 if \u03c4\u2208 E, else $y_\u03c4$ = 0. We implement F using three popular decoders, namely, DistMult [44], ComplEx [36] and RotatE [34]. We provide"}, {"title": "3.2. Adaptive two-way generator", "content": "A common way to obtain a negative triplet is to replace the tail (or head) entity in a positive triplet by a randomly sampled entity. Beyond simple random sampling, generative adversarial nets (GAN) [10] such as KBGAN [3], IGAN [38], HeGAN [11] and GNDN [47], which learn the underlying sample distributions, have been shown to be effective in negative sampling on KG or other graph structures.\nFormally, given a positive triplet (h, r,t), a generator G aims to produce a \"fake\" tail entity t' to replace the real tail t, resulting in a negative triplet (h, r, t'). More precisely, G is a function that maps a noise e (typically sam-pled from a prior distribution) to a vector $e_{t'}$ in the entity embedding space. Although we follow a similar process, distinct from existing GAN-based ap-proaches, we propose an adaptive two-way generator, as shown in Figure 1(b). It not only diversifies the generation of fake entities, but also localizes the global generator model to adapt to fine-grained differences across entities.\nDiversity. Classical GANs generate fake samples through a single pathway and assume a fixed prior distribution, which limits the diversity of fake entity generation. Particularly, in the context of KG, we can generate a fake tail entity associated with either the head entity only, or the relation as well. This improves the diversity of resulting negative triplets and increases the overall informativeness. Hence, we propose a two-way generator that consists of two pathways, namely $G_E$ and $G_R$, to generate negative triplets associated with a given entity and entity-relation, respectively. Furthermore, having personalized priors for each entity or relation would further enhance the diversification. Specifically, to replace the tail entity in a positive triplet (h, r, t) (the same process would also apply to replacing the head entity h),"}, {"title": "3.3. Two-way discriminator", "content": "As in a standard GAN architecture, a discriminator is needed to help the generator produce high-quality fake entities that mimic real entities. Specif-ically, the discriminator and the generator compete with each other in a minimax game, in which the generator aims to fool the discriminator by producing realistic looking entities, while the discriminator aims to beat the generator by distinguishing the real and fake entities. In our case, given the two-way generator, we further equip the discriminator with the ability to dis-tinguish the fake entities generated by the two pathways, which can further differentiate and diversify the two pathways.\nConcretely, as shown in Figure 1(c), the discriminator also has two path-ways: $D_{Adv}$, an adversarial pathway to distinguish fake and real entities, and $D_{aux}$, an auxiliary pathway to distinguish fake entities generated by $G_E$ and $G_R$. Taking the generation of tail entities as an example, given the real tail entity t in a positive triplet, as well as the fake entities t' generated by $G_E$ and t\" generated by $G_R$, $D_{adv}$ tries to distinguish t from t' and t\", while"}, {"title": "3.4. Adversarial training", "content": "Lastly, we train the generator, discriminator, and base embedding model jointly. On the one hand, the generator aims to fool the adversarial pathway of the discriminator, making $D_{Adv}$ harder to distinguish real and fake entities, as below.\narg max\u0398G $E_t L_{Adv} (\\hat y_{Adv, t}, 1)$\n+ $E_{t'} L_{Adv} (\\hat y_{Adv, t'}, 0)$ + $E_{t\"} L_{Adv} (\\hat y_{Adv, t\"}, 0)$\n+ \u03bb$ \\sum_{m, h}(||\u03b1_h^m \u2013 1||^2 + ||\u03b2_h^m||^2)$, (13)\nwhere t is a real tail entity, and t',t\" are fake tail entities from $G_E$ and $G_R$, respectively (again, we only illustrate the case where the tail entity in a"}, {"title": "4. Experiments", "content": "We perform empirical evaluation on three benchmark knowledge graphs. We first compare the empirical performance of the proposed model DANS\u00b9 with state-of-the-art baselines. In addition, we seek to address a number of research questions (RQ) through more in-depth empirical analysis. RQ1: Does the two-way design in the generator improve model performance? RQ2: Does the adaptive FiLM layer in the generator improve model performance? RQ3: What is the impact of the number of negative triplets and adaptive regularization, respectively? RQ4: Can we observe the diversity and adap-tiveness of generated triplets?"}, {"title": "4.1. Experimental Design", "content": "Datasets. Three benchmark knowledge graphs are used for our experi-ment. (1) WN18RR [2], a harder variant of WN18 [7], which is derived"}, {"title": "4.3. Additional research questions", "content": "In this part, we seek to investigate RQ1-RQ4 listed at the beginning of this section. All experiments in this part are conducted using the DistMult function as the decoder.\nAblation study (RQ1, RQ2). We investigate the contribution from major design choices through an ablation study. As depicted in Figure 2(a), we"}, {"title": "5. Conclusion and Future Work", "content": "In this work, we introduced DANS, a negative sampling strategy for knowledge graph embedding that explicitly accounts for the informativeness of negative triplets. On one hand, we proposed a two-way generator to in-crease the overall informativeness by diversifying the negative triplets based on their association with not only entities but also relations. On the other hand, we adapt the global generator model into local models, which generate negative triplets in a finer-grained manner to improve their individual infor-mativeness. Empirically, DANS has outperformed state-of-the-art baselines"}, {"title": "6. Funding", "content": "This research / project is supported by the Ministry of Education, Singa-pore, under its Academic Research Fund Tier 2 (Proposal ID: T2EP20122-0041). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the Ministry of Education, Singapore."}, {"title": "Appendix A. Scoring function", "content": "ComplEx: ComplEx [36] extends DistMult [44] by introducing complex-valued embeddings to better model asymmetric relations. In ComplEx, entity and relation embeddings no longer lie in a real space but a complex space Ck by operator Re.\nF((h, r,t))Complex = Re(eh Diag(er)et)\nComplex maps the entities and relations to the complex vector where $e_h$, $e_t$ \u2208 Ck by operator Re and Diag(er) \u2208 Rd\u00d7d is diagonal matrix whose diagonal is er, an r-specific trainable vector of the decoder.\nRotatE: Inspired by TransE [2], RotatE [34] veers into complex vector space and is motivated by Euler's identity. The model defines each relation as a rotation and measures how the distance from the source entity to the tar-get entity to account for three relation patterns: symmetric/anti-symmetric, inversion and composition.\nF((h, r, t)) Rotate = -||eh o er et||2\nRotate maps the entities and relations to the complex vector where $e_h$, $e_t$ \u2208 Ck and o denotes the Hadamard product."}]}