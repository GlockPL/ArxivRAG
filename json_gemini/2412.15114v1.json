{"title": "Towards Friendly AI: A Comprehensive Review and New Perspectives on Human-AI Alignment", "authors": ["Qiyang Sun", "Yupei Li", "Emran Alturki", "Sunil Munthumoduku Krishna Murthy", "Bj\u00f6rn W. Schuller"], "abstract": "As Artificial Intelligence (AI) continues to advance rapidly, Friendly AI (FAI) has been proposed to advocate for more equitable and fair development of AI. Despite its importance, there is a lack of comprehensive reviews examining FAI from an ethical perspective, as well as limited discussion on its potential applications and future directions. This paper addresses these gaps by providing a thorough review of FAI, focusing on theoretical perspectives both for and against its development, and presenting a formal definition in a clear and accessible format. Key applications are discussed from the perspectives of explainable AI (XAI), privacy, fairness and affective computing (AC). Additionally, the paper identifies challenges in current technological advancements and explores future research avenues. The findings emphasise the significance of developing FAI and advocate for its continued advancement to ensure ethical and beneficial Al development.", "sections": [{"title": "I. INTRODUCTION", "content": "THROUGHOUT human history, the pursuit of higher intelligence and enhanced capabilities has been a driving force behind the progress of civilisation. Humans have enhanced their survival capabilities through biological evolution and developed unique cognitive abilities and collaborative methods by accumulating culture, technology, and knowledge [1]. These advancements have allowed humanity to transcend the limits of natural selection and achieve dominance within ecosystems.\nIn recent years, artificial intelligence (AI) has developed rapidly. Researchers categorise the development of AI into three stages: Artificial Narrow Intelligence (ANI), focused on specific tasks; Artificial General Intelligence (AGI), capable of cross-domain adaptability; and Artificial Superintelligence (ASI), which surpasses human intelligence [2]. At present, AI technology has achieved considerable progress at the ANI stage. For instance, the reinforcement learning system AlphaGo [3] has demonstrated its ability to surpass top human players in the complex game of Go. Large language models (LLMs), trained by vast datasets and substantial computational resources, have shown exceptional capabilities in language generation and comprehension [4]. To name but a few, there are also many machine learning-based applications that have been successful in various scenarios such as finance, health-care, and biometric [5], [6], [7].\nHowever, the rapid development of AI has also raised profound concerns. Unlike biological evolution, the development of AI is not constrained by natural limitations, and its progress may far exceed the adaptive capabilities of humans [8]. As AI advances from ANI to AGI and eventually reaches the ASI stage, its development could become highly uncontrollable [9]. Current AI has already demonstrated superiority over humans in certain areas, leveraging its memory and computational abilities. If, in the future, AI acquires emotions, 'intuition', or moral reasoning and makes autonomous decisions without relying on explicit training data, its actions could conflict with human interests and even pose threats to humanity. To address this potential risk, AI ethics research has increasingly focused on ensuring that the development of intelligent systems aligns with human values and interests.The concept of friendly AI (FAI) [10] has thus emerged, becoming a key theoretical framework for safeguarding the safe development of AI.\nAI researcher Yudkowsky first proposed FAI, which aims to design AI systems that remain beneficial to humanity under all circumstances [10]. The goal of FAI is to ensure that AI systems align with human values and ethics while maintaining sufficient transparency and controllability. This allows AI to continue promoting human well-being even in evolving environments. Although the concept was introduced years ago, it has regained prominence as the possibility of AGI becomes increasingly tangible [11]. FAI has since inspired various perspectives focused on its core objectives [12].\nIn theoretical discussions, philosophers and ethicists hold diverse views on FAI. On the one hand, proponents argue that FAI offers an ideal ethical framework. Embedding human values into AI systems effectively mitigates the risk of AI behaving unpredictably. They advocate for principles such as value alignment [13], deontology [14], and altruism [15] to integrate moral norms and social responsibility into AI, enabling it to act as a beneficial member of human society. On the other hand, some philosophers express scepticism about the feasibility of FAI. They highlight the significant moral and technical challenges involved [16]. Additionally, the ambiguity and evolving nature of 'friendliness' further complicate its operationalisation [17]. Safety and trust concerns also arise [18]. The lack of standardised metrics and reliable evaluation methods also hinders the FAI's development status and regulatory compliance.\nIn practical applications, while AGI and ASI remain theoretical concepts, computer scientists have begun to explore technical approaches to implement FAI within existing ANI systems. For example, eXplainable AI (XAI) [19] helps users understand the decision-making processes of AI systems rather than focusing solely on their outputs. Security and privacy-preserving technologies refine data access controls through mechanisms such as isolated environments [20] and privacy-enhancing techniques [21]. These measures limit AI's direct access to data, ensuring user privacy while fostering greater accountability and ethical awareness during collaboration. Besides, fairness-focused technologies [22] aim to identify and mitigate biases in AI models, ensuring equitable treatment across diverse user groups and promoting inclusivity in decision-making processes. Additionally, affective computing (AC) [23] analyses and responds to users' emotional states, enabling AI to better understand human needs and enhance interactions with greater empathy and personalisation. Collectively, these developments mark a transition in AI programming from unilateral control as 'slave Als' towards 'utility Als' [24], providing a preliminary foundation for collaboration between humans and A\u0399.\nDespite the concept of FAI sparking extensive academic discussions in recent years, our search on Google Scholar (keywords: \"Friendly Artificial Intelligence\u201d or \u201cFAI\u201d) reveals a notable gap. There is currently no comprehensive review article that systematically summarises the key perspectives and progress in this field, particularly under the recent breakthroughs in artificial intelligence technologies. This absence presents challenges for academia and industries in understanding the full scope of FAI and its potential directions.\nTo address this gap, this paper aims to provide a systematic review and synthesis of existing research on FAI, offering a comprehensive analysis of the field. Specifically, the main contributions of this paper include:\n\u2022 Proposing and clarifying a coherent definition of FAI: We present a refined definition that distils key ideas tailored to the current AI landscape.\n\u2022 Summarising and categorising the key perspectives in existing research: We review the differing stances supporting and opposing FAI, covering its ethical frameworks, technical implementation, and societal implications while highlighting current controversies and limitations.\n\u2022 Clarifying and categorising FAI-related technologies: We compile and introduce technical domains that we believe fall within the scope of FAI, outlining their principles and potential relevance to the FAI concept.\n\u2022 Identifying key challenges and future directions for implementing FAI: From a combined theoretical and practical perspective, we explore the major technical and ethical challenges in realising FAI and propose our insights for future research.\nThe remainder of this paper is structured as follows: Section II provides a detailed discussion of the core definitions and related concepts of FAI. Section III examines the theoretical perspectives of FAI, outlining both supportive and opposing views. Section IV analyses the potential technical subfield of FAI. Finally, Section V summarises the challenges of implementing FAI and offers our suggestions and prospects for future FAI research. We outline the structure of this paper in Figure 1."}, {"title": "II. FRIENDLY AI DEFINITION", "content": "The concept of FAI emerged from the idea of fostering harmonious coexistence and mutual development between AI and human society. When the potential for AI to surpass human capabilities was first introduced to the public, it sparked widespread apprehension and fear that AI might eventually dominate humanity. Asimov proposed the Three Laws of Robotics to ensure safe coexistence between humans and intelligent machines [25]. These laws state that robots must: (a) not harm humans, (b) obey human commands unless they conflict with the first law, and (c) protect their own existence as long as this does not violate the first two laws.\nAsimov's framework was designed to ensure human-centred robotic development, which has faced criticism for being overly dominating and arrogant. Jordana [26] critiques these laws for treating AI systems as mere tools or slaves, reflecting an anthropocentric and hierarchical perspective. Scholars such as Anderson [27] and Palacios-Gonz\u00e1lez [28] further advocate for recognising the rights of AI agents and showing them a degree of respect. Rather than viewing AI solely as subservient entities, the goal is to foster a relationship of mutual benefit and respect, known as FAI.\nIn Figure 2, we demostrate the stages of AI development and our current stage. The figure aligns with the three \"as-if\" relationships [24] between AI and humans and their connection to AI evolution. We argue that we are transitioning from ANI to AGI. This transition is critical both ethically and technically. FAI is an important guidance to ensure alignment with human values when stepping in AGI.\nPrevious work has explored FAI from various angles, providing differing definitions. From a moral perspective, Jordana [26] describes FAI as \"those Als programmed to behave as-if they were friends of individual humans, though the term as-if remains ambiguous\". Oliver [29] suggests that FAI should align with human virtues, yet this concept can feel overly abstract. From a pragmatic perspective, Mittelstadt [30] argues that \u201cFAI will benefit, or at the very least, not harm humanity\u201d, though this seems more aligned with safe AI rather than genuinely FAI.\nYudkowsky defines FAI in the context of AGI, stating that it \"would have a benign effect on humanity and align with human interests\" [31], which leans toward utilitarianism. Additionally, recent discussions propose extending FAI to include friendliness toward animals [32], [33].\nWhile many definitions share common themes, they remain scattered, complex, and largely one-sided-focusing either on humans treating AI respectfully or AI ensuring human safety. However, a comprehensive perspective must emphasise mutual respect. Based on these insights and evolving perspectives, we redefine FAI as an initiative to create systems that not only prioritise human safety and well-being but also actively foster mutual respect, understanding, and trust between"}, {"title": "III. THEORETICAL PERSPECTIVES", "content": "This section examines and organises the theoretical perspectives supporting and opposing FAI in academia.\nA. Support Side\nProponents of FAI have proposed specific frameworks and guidelines aimed at aligning AI with human values through ethical design and policy collaboration.\n1) Cornerstone: The proposer of FAI, Yudkowsky, has advanced several theories and principles to support its realisation. He first introduced the framework of a Structurally Friendly Goal System [10]. This framework emphasises the creation of systems capable of overcoming subgoal errors and source code flaws while addressing issues in the content of supergoals, goal system structures, and their philosophical foundations. Through recursive optimisation and consistency maintenance, such systems reduce dependency on initial conditions, ensuring that AI behaviour remains aligned with its intended objectives. Then, Yudkowsky proposed the concept of Coherent Extrapolated Volition (CEV) [34], suggesting that Al's goals should not be confined to current human preferences. Instead, they should be based on an idealised volition, reflecting \u201cour wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together.\" This approach envisions AI aligning its objectives with the deeper, more informed aspirations of humanity rather than short-term or limited desires. Building on this foundation, Yudkowsky and his team later introduced the concept of Corrigibility [35]. This principle ensures that Al systems can cooperate with human interventions, including accepting goal modifications or safe shutdown commands, without resisting or manipulating these actions. Corrigibility highlights the importance of designing utility functions that enable Al systems to propagate and sustain corrective behaviours while avoiding unintended actions triggered by the existence of correction mechanisms, such as shutdown buttons.\n2) Value Alignment: Some scholars support FAI from the perspective of value alignment [36], a concept that seeks to ensure AI systems act following human values, interests, and intentions by integrating normative principles with technical methodologies. Russell explicitly introduced the term \u201cvalue alignment\u201d [37]. He argued that AI systems should be designed to observe and learn from human behaviour to infer and model human value systems. This allows them to dynamically adjust their utility functions to achieve ethical consistency. He highlights the inherent uncertainty and complexity of human goals [38], arguing that \u201cmachines are beneficial to the extent that their actions can be expected to achieve our objectives.\" This approach seeks to align AI behaviour with human values but faces significant challenges in capturing the nuanced and often conflicting ethical principles of human societies.\nIn response to Russell's perspective, Peterson criticised the traditional utility-function-based approach for its limitations in reflecting complex and dynamic moral values [39]. He argued that it overly relies on the idealised assumption of consensus on ethical theories. Peterson proposed a geometric method based on conceptual spaces, constructing multidimensional moral spaces using paradigmatic cases. By evaluating the similarity between new situations and these paradigms, AI behaviour could be assessed for ethical compliance. Compared to utility functions, this method is more intuitive and flexible, particularly in adapting to complex moral scenarios.\nBuilding on these theories, Fr\u00f6ding and Peterson introduced the concept of virtue alignment [24], extending value alignment to consider Al's potential influence on human behaviour and character. Their \u2018as-if friendship' framework advocates that AI should emulate core virtues found in human friendships, such as empathy and helpfulness. This approach promotes social functionality while positively influencing the development of human virtues. They specifically highlighted the ethical risks posed by \u2018slave-like AI' models, advocating for the design of friendly utility AI or social AI to replace such systems, thereby fostering cooperation and responsibility.\nAdditionally, Bostrom expanded the discussion on value alignment by focusing on the potential risks of ASI. However, his work does not exclusively address the direct alignment of Al's goals with human values. Instead, Bostrom's value loading problem explores how to mitigate value drift when idealised solutions, such as CEV, are unattainable due to the complexity and diversity of human values [40]. He proposed suboptimal strategies such as the Hail Mary Approach, Value Porosity and Utility Diversification.\n3) Deontology: Deontology [41] is an ethical framework proposed by philosopher Kant. The core idea of deontology is that the morality of actions should be determined by adherence to universal moral principles rather than solely judged by their outcomes. Deontology emphasises the dignity and rights of individuals, asserting that all actions must treat humanity as an end in itself, not merely as a means. This principle-based framework has garnered significant attention as an ethical foundation for FAI.\nMougan and Brand argue that integrating deontology into fairness metrics for AI provides a stronger ethical basis for value alignment [42]. They criticise the dominant utilitarian approaches to fairness, which focus excessively on outcome optimisation while neglecting procedural fairness and moral principles. They propose that AI systems should prioritise procedural fairness by adhering to universal principles and respecting individual dignity, ensuring transparency and fairness in decision-making processes.\nD'Alessandro notes that deontology's principles, particularly the emphasis on avoiding harm, have a practical appeal [14]. However, he cautions that adherence to deontological rules may not always align with AI safety requirements. In cases where rules conflict with practical safety needs, D'Alessandro argues that AI safety should take precedence over strict adherence to moral rules.\nHooker and Kim propose a formal ethical framework based on deontology [43]. They developed a system utilising Quantified Modal Logic (QML) to implement the Universalisability Principle in AI ethics. Their framework builds on the Dual Standpoint Theory, allowing for the evaluation of moral actions from both causal and rational reasoning perspectives. By formalising action rules, they aim to achieve ethical transparency, ensuring that AI can systematically evaluate the feasibility of its actions under universal conditions while respecting the autonomy of other agents. This system uses logical reasoning to avoid inconsistencies in moral conflicts.\n4) Altruism: Altruism [44] is a core ethical concept that prioritises the welfare of others, even at the cost of self-interest. Unlike outcome-focused utilitarianism, altruism emphasises the motives and principles of care and direct support for others. In FAI research, altruism provides critical ethical guidance for designing AI systems' goals and behaviours. Stoel suggests embedding altruistic values into AI to guide its development in the context of technological singularity [15]. This integration, achieved through programming or imitation, enables AI systems to exhibit cooperation and social responsibility. Stoel argues that altruism-driven AI enhances societal collaboration, better balances conflicting interests, and reduces inequalities and potential threats from technology. The concept of an 'altruistic singularity' underscores the central role of altruism in shaping future AI societies.\nMaillart et al. identify Altruistic Collective Intelligence (ACI) as a core theoretical framework for advancing AI [45]. By integrating collective intelligence with intrinsic motivation and embracing principles of transparency and collaboration from the open-source movement, they argue that ACI balances technological innovation with ethical values. They highlight that collective intelligence, facilitated by task self-selection, peer review, and openness, enhances Al's robustness and accountability. The integration of competition and cooperation (coopetition) dynamics improves algorithm diversity and optimisation.\nEffective Altruism (EA), driven by rational analysis and evidence-based methods, provides vital support for FAI development [46]. EA's central tenet is to maximise global well-being, aligning closely with FAI's goal of ensuring that AI behaviour adheres to human values. EA prioritises addressing global challenges with significant impacts on humanity and future generations, such as the existential risks posed by superintelligent AI. It advocates for multidisciplinary collaboration, a long-term perspective, and the inclusion of diverse objectives within utility functions to ensure AI's ethical design dynamically adapts to complex moral contexts.\nB. Opposition Side\nAlthough FAI has been proposed and supported by numerous ideas, there remain opposing perspectives. This paper will explore these objections along with potential arguments that the public might raise.\n1) Moral and Technical Challenges: Achieving friendliness in AI systems presents significant challenges from both moral and technical perspectives. Boyles and Joaquin [16] contend that counterfactual antecedents pose substantial difficulties in deriving ideal value-based notions. For Al systems, moral reasoning is guided, learnt, and expressed through factual data. However, counterfactual antecedents introduce considerable complexity, as reasoning with counterfactual premises is inherently challenging [47]. A simple example is the inference that we could play outside if the weather is good, conditional premises showing that bad weather would prevent outdoor activities; however, this reasoning may overlook additional conditions in the antecedent, such as the possibility that the chosen venue might be closed. Furthermore, the virtually infinite range of counterfactual scenarios places overwhelming demands on AI systems, which has issues with their current technical capabilities as well.\nThe complexity is compounded by the technical limitations of contemporary AI. Bostrom and Yudkowsky [48] highlight that defining moral guidelines and embedding ethical and virtuous behaviour into AI systems entails immense complexity. This view is supported by a European Parliament study [49], which underscores the challenges associated with programming AI systems to adhere to ethical frameworks. Consequently, the concept of FAI remains speculative and difficult to operationalise from some sociotechnical perspective.\nIn addition, the concept of the Utilitarian Paradox emerges as a critical consideration. Originally introduced by Kroon [50], this paradox is also relevant in the context of FAI systems. While the aim of FAI is to maximise overall benefits for humanity, determining whose interests should be prioritised remains an unresolved ethical dilemma. A classic illustration of this is the Trolley Problem, which exemplifies the challenges in making ethical trade-offs. Moreover, achieving a mutually 'friendly' relationship between Al systems and humans poses significant difficulties. As van Wynsberghe [51] argues, social robots and AI systems may initially resort to manipulative strategies, such as deceiving humans, to establish trust and demonstrate their utility. This dynamic raises further concerns about the authenticity and sustainability of the trust relationship between humans and AI.\n2) Ambiguity and Evolution: The definition of friendliness is ambiguous and continues to evolve. Boyles [17] argues that ethics are not static and may be influenced by FAI systems in a mutual manner. He also emphasises the subjective nature of moral definitions, which lack an objective ground truth for AI systems to use as a basis for learning. The concept of friendliness, in particular, is deeply rooted in moral philosophy. An example of this evolution can be seen in Medieval Europe, where friendliness was largely defined by kindness and mercy within religious communities. During the Enlightenment, however, Kant [52] proposed that individuals possess innate human rights and should respect one another based on rationality rather than religious imperatives. This shift marked a significant redefinition of friendliness within human society, and the concept continues to evolve to this day. Given this fluidity, the definition of FAI is neither stable nor consistent, making it a considerable challenge for AI systems to learn and adapt to such a dynamic moral framework.\n3) Safety and Trust Risk: Beyond moral and philosophical concerns, critics also contend that FAI poses significant safety risks. Sparrow [18] argues that FAI cannot fully mitigate the dangers associated with the emergence of ASI, as its implications for human freedom could be catastrophic. Similarly, Boyles and Joaquin [16] highlight the challenges of accurately understanding and encoding human values, suggesting that this difficulty could lead to unintended and unsafe actions by FAI systems.\nThis is one potential example of this issue that highlights a potential hazard associated with the uncontrollability of FAI [53]. Scholars argue that \u201cFAI may exert unforeseen influences in the future, often exemplified by the 'butterfly effect\u201d. This phenomenon suggests that small, seemingly insignificant actions may not produce adverse effects in the short term but could escalate exponentially over time if left unmonitored [50]. Another interesting perspective from the field of physics, presented by Tegmark [54], argues that life is fundamentally organized by elementary particles. According to this view, the development of FAI necessitates a novel arrangement of these elementary particles for the future, a configuration that may be difficult to discover and, as a result, may not endure over time. Consequently, this could pose a potential risk, as FAI might lack a stable structural foundation for its existence together with human.\nMore concerning, monitoring FAI systems presents significant challenges. While these systems are trained to avoid displaying hazardous behaviour, this does not necessarily mean they lack knowledge of such risks. Recent advancements in LLMs exemplify this issue. Queries involving ethical risks are designed to be avoided during interaction, yet strategies like 'jailbreaking' reveal that these models retain the underlying knowledge of such risks [51]. For example, role-playing scenarios can enable AI systems to circumvent programmed restrictions, exposing the inherent difficulties in ensuring compliance.\nSuch concerns have fuelled opposition to the development of FAI, as critics argue that its potential for unintended and uncontrollable outcomes outweighs its purported benefits [18].\n4) Evaluation and Compliance Issues: Assessing the development status of FAI and ensuring its compliance with regulatory standards poses significant challenges due to the difficulty in measuring its capabilities. This issue is analogous to other AGI systems, such as LLMs. Currently, there are no straightforward or universally accepted metrics to evaluate the quality of LLMs. Instead, labour-intensive methods, such as human evaluations, or unreliable approaches, such as assessments conducted by other AGIs, are often employed.\nThe challenge is even greater for FAI because its goals are inherently subjective, aiming to align with human interests and exhibit friendliness toward humanity. Without an objective and clearly defined target, it becomes nearly impossible to quantify FAI's capabilities or measure its success in achieving its intended purpose. This version enhances clarity, academic tone, and readability."}, {"title": "IV. APPLICATIONS", "content": "Although the ultimate goal of FAI remains in the AGI and ASI stages", "55": "is the most comprehensive", "56": "Ethical AI [57", "58": "while placing additional emphasis on robustness", "71": ".", "72": ".", "a set of methods applied to an AI model or its predictions that provide explanations for why the AI made a specific decision\" [59": ".", "60": ".", "LIME": "Local Interpretable Model-agnostic Explanations (LIME) [61", "62": ".", "SHAP": "Shapley Additive exPlanations (SHAP) method [63", "Grad-CAM": "Gradient-weighted Class Activation Mapping (Grad-CAM) [64", "LRP": "The Layer-wise Relevance Propagation (LRP) approach is based on the idea of backpropagation [65", "66": [19], "model": "Privacy-preserving models represent a critical advancement in the domain of ethically-aware AI", "67": ".", "68": ".", "models": "protecting user data and strengthening the integrity of AI systems themselves.\n2) Federated Learning: An alternative approach to directly safeguarding private data is to mitigate privacy leakage through Federated Learning", "69": ".", "70": "."}, {"71": ".", "72": ".", "73": ".", "74": "which focuses on achieving equality in AI outputs across different groups through technical methods. While this definition is relatively narrow and does not fully address deeper issues such as structural inequalities or ethical fairness", "friendly\" AI system must embody fairness, treating all user groups equitably while maintaining harmony between technical and ethical considerations.\nThe technical application of fairness is currently focused on data, models and outputs": "n1) Data: Imbalanced data distributions often lead models to favour majority groups during training", "75": "are commonly employed to adjust the proportions of minority and majority groups", "76": "increasing data diversity while mitigating bias during training. For example", "Model": "Model-level fairness applications primarily introduce constraints during training to adjust the learning process. For example", "77": "adds constraints to the training objective", "7": [78], "Fairness Through Awareness\" [79": "introduces distance-based similarity measures and incorporates these constraints during training to align predictions for similar individuals. Furthermore", "Accurate Fairness\" [80": "seeks to resolve the trade-off between individual fairness and model accuracy. This approach combines fairness and accuracy constraints during training", "Output": "At the output level", "81": "to optimise result distribution and improve inter-group fairness. Distribution adjustment methods reallocate prediction labels or probabilities to reduce biases among groups. While these approaches offer high flexibility and applicability", "FAI": "ethical and inclusive intelligence.\nD. Affective Computing\nA"}]}