{"title": "Clustering Time Series Data with Gaussian Mixture Embeddings in a Graph Autoencoder Framework", "authors": ["Amirabbas Afzali", "Hesam Hosseini", "Mohmmadamin Mirzai", "Arash Amini"], "abstract": "Time series data analysis is prevalent across various domains, including finance, healthcare, and environmental monitor-ing. Traditional time series clustering methods often struggle to capture the complex temporal dependencies inherent in such data. In this paper, we propose the Variational Mix-ture Graph Autoencoder (VMGAE), a graph-based approach for time series clustering that leverages the structural advantages of graphs to capture enriched data relationships and produces Gaussian mixture embeddings for improved separability. Comparisons with baseline methods are included with experimental results, demonstrating that our method significantly outperforms state-of-the-art time-series clustering techniques. We further validate our method on real-world financial data, highlighting its practical applications in finance. By uncovering community structures in stock markets, our method provides deeper insights into stock relationships, benefiting market prediction, portfolio optimization, and risk management.", "sections": [{"title": "Introduction", "content": "time series is commonly referred to a sequence of data points collected or recorded at successive time instances, usually at uniform intervals. For instance, in finance, time series data might include daily closing prices of a stock over a year(Shah, Isah, and Zulkernine 2019). In healthcare, it could be the EEG signal of a person's brain in a specific time interval (Siuly, Li, and Zhang 2016), and in environmental monitoring, it might involve hourly temperature readings (Zhao, Zhu, and Lu 2009).\nNumerous studies have been conducted on time series analysis, encompassing various tasks such as forecasting (Torres et al. 2021), classification (Ismail Fawaz et al. 2019), clustering (Aghabozorgi, Shirkhorshidi, and Wah 2015), anomaly detection (Shaukat et al. 2021), visualization (Fang, Xu, and Jiang 2020), pattern recognition(Lin et al. 2012), and trend analysis (Mudelsee 2019).\nTime series clustering is a powerful method for grouping similar time series data points based on their characteristics, especially when there is no prior knowledge of the data structure (Liao 2005). It has diverse applications, such as stock market forecasting, where it is used for feature extraction to predict stock movements, helping investors anticipate market behavior and enhance model predictions (Babu, Geethanjali, and Satyanarayana 2012). In portfolio optimization, clustering identifies stocks with similar traits, fostering diversification and reducing risks. Additionally, it supports risk management by predicting market volatility using algorithms like Kernel K-Means and Gaussian Mixture Models (Chaudhuri and Ghosh 2016) and contributes to fraud detection by flagging anomalies that deviate from typical cluster patterns (Close and Kashef 2020).\nDespite its practical significance, unsupervised time series clustering faces notable challenges. Time series data often vary significantly in their critical properties, features, temporal scales, and dimensionality across different domains. Real-world data further complicate this process by introducing issues such as temporal gaps and high-frequency noise (Hird and McDermid 2009). To address these challenges, researchers have developed methods focusing on three main aspects: 1) time series similarity measures (Tan et al. 2020; Li, Boubrahimi, and Hamdi 2021), 2) discriminative representation learning (Ma et al. 2019; Zhang et al. 2018; Jorge and Rub\u00e9n 2024), and 3) clustering mechanisms (Paparrizos and Gravano 2015; Li et al. 2022). These advancements aim to enhance the reliability and applicability of time series clustering in complex, real-world scenarios.\nIn this paper, we leverage all these aspects by constructing a graph from time series data using dynamic time wrapping (DTW) (Sakoe 1978) to capture the relationships between individual time series. By exploiting a specialized graph autoencoder, we can also learn how to embed each node properly. This embedding not only represents the unique features of each data point but also captures shared features from nodes similar to the data point. To the best of our knowledge, this is the first work that employs graph autoencoder architecture for time series clustering.\nThe novel contributions of this work can be summarized as follows:\n\u2022 We propose a new framework for time series clustering. This approach uses a graphical structure to capture more detailed information about data relationships. Turning a time series dataset into graphs can effectively capture both temporal and relational dependencies.\n\u2022 We introduce a specialized graph autoencoder, named Variational Mixture Graph Autoencoder (VMGAE), that generates a Mixture of Gaussian (MoG) embeddings."}, {"title": "Related Work", "content": "Time series data clustering has been a significant area of research for decades, leading to various algorithms. Classical clustering algorithms like k-means and spectral clustering can be executed on raw time series data, while some methods use modified versions of classical methods. K-shape (Paparrizos and Gravano 2015), assigns data to clusters based on their distance to centroids and updates the centroids like k-means, but instead uses cross-correlation for distance measurement. KSC((Yang and Leskovec 2011)) uses k-means for clustering by adopting a pairwise scaling distance measure and computing the spectral norm of a matrix for centroid computation.\nAnother approach is to use shapelets to extract discriminative features from time series data, as demonstrated in (Ulanova, Begum, and Keogh 2015), (Zhang et al. 2018), (Li et al. 2022), and (Zhang et al. 2016). The main challenge in these methods is identifying suitable shapelets for the shapelet transform process, which extracts meaningful features from raw data to perform clustering. R-clustering method (Jorge and Rub\u00e9n 2024) employs random convolutional kernels for feature extraction, which are then used for clustering. Additionally, (Tan et al. 2020) implements a hierarchical clustering algorithm that uses Granger causality (Ding, Chen, and Bressler 2006) as the distance measure, fusing pairs of data to create new time series and continuing the clustering process. STCN (Ma et al. 2020) uses an RNN-based model to forecast time series data, employs pseudo labels for its classifier, and utilizes the learned features for clustering. Since our method leverages both the autoencoder architecture and a graph-based approach for clustering time series data, we will review autoencoder-based methods and graph-based techniques separately."}, {"title": "Autoencoder-based methods", "content": "Autoencoders have demonstrated empirical success in clustering by using their learned latent features as data representations. For instance, DEC (Xie, Girshick, and Farhadi 2016) adds a KL-divergence term between two distributions to its loss function, alongside the reconstruction error loss, to make the latent space more suitable for clustering. DTC (Olive et al. 2020) introduces a new form of distribution for the KL-divergence term, applying this method to trajectory clustering. Another method, DCEC (Guo et al. 2017), incorporates a convolutional neural network as its autoencoder within the DEC method for image clustering. VaDE (Jiang et al. 2016) adds a KL-divergence term between the Mixture-of-Gaussians prior and the posterior to the reconstruction loss. This is done using the embeddings of data points in the latent space of a variational autoencoder and a prior GMM distribution. In the domain of time series clustering, DTCR (Ma et al. 2019) trains an autoencoder model with the addition of k-means loss on the latent space and employs fake data generation and a discriminator to classify real and fake data, enhancing the encoder's capabilities. Also, TMRC (Lee, Kim, and Sim 2024) proposes a representation learning method called temporal multi-features representation learning (TMRL) to capture various temporal patterns embedded in time-series data and ensembles these features for time-series clustering."}, {"title": "Graph-based methods", "content": "Graphs have significantly enhanced the capabilities of deep learning methods in various tasks. Variational Graph Autoencoder (VGAE) (Kipf and Welling 2016b) utilizes GCN (Kipf and Welling 2016a) for link prediction and node classification tasks. Specifically, graphs have also found significant applications in the time series domain. Recent works such as (Song et al. 2020), (Cao et al. 2020), and (Yu, Yin, and Zhu 2017) use graph-based methods for time series forecasting, while (Zha et al. 2022) and (Xi et al. 2023) apply them for classification. Additionally, (Zhao et al. 2020), (Deng and Hooi 2021), and (Han and Woo 2022) utilize graph-based models for anomaly detection in time series data. Graphs have also been employed for time series data clustering (Li, Boubrahimi, and Hamdi 2021).\nOne of the critical challenges in graph-based methods for the time series domain is constructing the adjacency matrix. Several methods address this issue by introducing metrics to compute the similarity or distance between two time series samples. The Granger causality method (Ding, Chen, and Bressler 2006) leverages the causal effect of a pattern in one time series sample on another to measure similarity between samples. The Dynamic Time Warping (DTW) method (Sakoe 1978) minimizes the effects of shifting and distortion in time by allowing the elastic transformation of time series to compute the distance between two samples. There are many extensions of the DTW method, such as ACDTW (Li et al. 2020), which uses a penalty function to reduce many-to-one and one-to-many matching, and shapeDTW (Zhao and Itti 2018), which represents each temporal point by a shape descriptor that encodes structural information of local subsequences around that point and uses DTW to align two sequences of descriptors.\nThe similarity between two time series can be used directly as the edge representation, but the distance needs to be processed for use in the adjacency matrix. One method is to apply a threshold on distances to predict whether an edge exists between two nodes in a binary graph (Li, Boubrahimi, and Hamdi 2021)."}, {"title": "Problem Definition and Framework", "content": "Notation\nIn the following sections, we denote the training dataset as D = {d1,...,dn}, where di represents the i-th time series, and n is the size of the training dataset. The length of the"}, {"title": "Graph Construction", "content": "For graph construction, we use a variant of DTW called Weighted Dynamic Time Warping (WDTW) and a constraint to limit the window size of the wrapping path. The distance between two sequences X = (x1, x2,...,xN) and Y = (Y1,Y2,..., ym) with a weight funtion w and a window size W is computed as follows:\nWDTW(X,Y) = min \u03a3\u03c9[[i - j]] \u00b7 dinner (Xi, Yj)\n(i,j) \u0395\u03c0\nsubject to the constraint:\n|i - j| \u2264 W,\nwhere \u03c0 is a warping path that aligns the sequences X and Y, dinner(xi, yj) is the distance between elements xi and yj. This could be any customized distance. For simplicity, we use Euclidean distance. w[|i \u2013 j|] is a weight function that depends on the absolute difference between indices i and j, and W is the window size that limits the maximum allowable shift between indices.\nThe weight function w[n] should be a monotonic function of n, as it penalizes alignments where the indices are farther apart, favoring closer alignments. For simplicity, we set w[n] = \u03b3\u00b7 \u03b7, where \u03b3 is a positive hyperparameter.\nGiven the training dataset D = {d1,...,dn}, we construct a distance matrix S, where Sij represents WDTW(di, dj) with fixed parameters \u03b3 and W. Next, we propose a novel transformation approach to convert the distance matrix S into a similarity matrix. By fixing the density rate \u03b1 = #{A=1}/n2 , we compute a threshold \u03b4 to construct an adjacency matrix A, where Aij = 1 if Sij < \u03b4 and Aij = 0 otherwise. The key difference compared to previous work (Li, Boubrahimi, and Hamdi 2021) is that, instead of fixing \u03b4, we fix \u03b1 and use it to compute the corresponding \u03b4 for each dataset. This is important because the optimal threshold \u03b4 may vary across datasets, while the optimal \u03b1 is much more stable. While the current representation demonstrates good separation, further refinement can be achieved with the use of VMGAE."}, {"title": "Learning Representation via a Graph Structure", "content": "Graph Convolutional Autoencoder. In our unsupervised setting, we utilize a graph convolutional autoencoder architecture to embed a graph G = {V, E, X} into a low-dimensional space. Specifically, we derive an embedding zi \u2208 Z for the i-th node of the graph. This approach presents two key challenges: 1) How can both the graph structure A and node features X be effectively integrated within the encoder? 2) What specific information should the decoder reconstruct?\nGraph Convolutional Layer. To effectively capture both the structural information A and node features X in a unified framework, we employ graph convolutional network (GCN) (Kipf and Welling 2016a). Graph convolutional"}, {"title": "Decoder Model D(Z, A)", "content": "Decoder model is given by an inner product between latent variables:\np(A | Z) = \u03a0 \u03a0 p(Aij | Zi, Zj),\ni=1 j=1\nand the conditional probability is usually modeled as:\np(Aij = 1 | Zi, Zj) = \u03c3(ZiT Zj),\nwhere \u03c3(\u00b7) is the logistic sigmoid function.\nThus, the embedding Z and the reconstructed graph \u00c2 can be presented as follows:\n\u00c2 = \u03c3(ZZT), here Z = q(Z | X, A) \nLearning Algorithm. In VMGAE, our objective is to maximize the log-likelihood of the data points, log p(A). Based on the decoder model The joint probability p(A, Z, c) can be factorized as:\np(A, Z, c) = p(A | Z)p(Z | c)p(c).\nThe log-likelihood can be expressed as:\nlog p(A) = log \u03a3 Eq(Z,c/X,A) log\nZ\nC\np(A, Z, c) dZ\nC\n\u2265 Eq(Z,c/X,A) [log ]\n= LELBO (X, A).\nThe inequality is derived from Jensen's inequality. Instead of maximizing the log-likelihood directly, we aim to maximize its Evidence Lower Bound (ELBO), and using the factorization in Equation 11, it can be rewritten as follows:\nLELBO (X, A) = Eq(Z,cx,A) [log p(A, Z, c) \u2013 log q(Z, c|X, A)]\n= Eq(Z,c/X,A) [log p(A|Z) + log p(Z c) + log p(c)] \u2013 Eq(Z,c/X,A) [log q(Z|X, A) + log q(c|X, A)],\nwhere the last line is obtained under the assumption of a mean-field distribution for q(Z, c|X, A).\nSimilar to the approach in (Jiang et al. 2016), a mixture of Gaussian latent variables is used to learn the following distributions:\np(ci) = Cat(ci | \u03c0)\np(zi | ci). = N (zi | Mi, \u03c3\u00bf2I).\nBy assuming a mean-field distribution, the joint probability p(c) and p(Z | c) can be factorized as:\np(c) = \u03a0 Cat (Ci | \u03c0),\ni=1\np(Z | c) = \u03a0 N (Zi | \u00b5c\u2081, \u03c3\u00bf\u00bfI),\ni=1\nwhere \u03c0k is the prior distribution of cluster k hence \u03a3k \u03c0k = 1, Cat(. | \u03c0) is the categorical distribution"}, {"title": "Experiments", "content": "Experimental Setup and Datasets\nWe employed 19 datasets from the UCR time series classification archive (Huang et al. 2016) for our clustering experiments, with specific details provided in Table 1 and Table 2. Our networks were implemented and tested using PyTorch (Paszke et al. 2019), Torch_Geometric (Fey and Lenssen 2019), and executed on an A100 GPU (40G). VMGAE was trained with a learning rate of 1e-4 for 500 epochs"}, {"title": "Algorithm 1: VMGAE Training Procedure", "content": "Input: Time series dataset D\nParameters: Hyperparameters {W, \u03b3, \u03bb, \u03b1}, Pre-training iterations Tpre, Training iterations T\nOutput: Clustering results\n1: Compute the distance matrix S using WDTW (Eq. (1)).\n2: Convert the distance matrix S into an adjacency matrix A.\n3: Initialize GAE with random weights.\n4: for t = 1 to Tpre do\n5: Pre-train the GAE by minimizing Lrecon (A) (Eq. (23)).\n6: end for\n7: Fit a GMM to the latent representations Z from the GAE.\n8: Initialize parameters \u00b5, \u03c3, and \u03c0 using the fitted GMM.\n9: fort = 1 to T do\n10: Train VMGAE by minimizing LVMGAE (X, A) (Eq. (22)).\n11: end for\n12: Fit a final GMM on the learned latent representations Z.\n13: return Clustering results based on the final GMM."}, {"title": "Quantitative Analysis", "content": "The performance of VMGAE was benchmarked against several time series clustering methods to evaluate its clustering capabilities thoroughly. The results presented in Tables 1 and 2 are sourced from the original papers, except R-Clustering (Jorge and Rub\u00e9n 2024), where results were obtained by running the authors' publicly available code. Both"}, {"title": "Application in Finance", "content": "Understanding stock market dynamics in finance is essential for making informed investment decisions. Detecting patterns and communities within this complex network of stocks helps gain insights into market behavior and make better investment choices.\nIn this section, we demonstrate the effectiveness of our approach by applying it to real-world stock market data and evaluating the quality of the resulting clusters. We selected the top 50 publicly traded U.S. stocks listed on NASDAQ, NYSE, and NYSE American, ranked by market capitalization. The input time series for our model consists of daily normalized closing prices from January 1, 2020, to October 4, 2024. We set the number of clusters to 5 based on the Elbow Method (Thorndike 1953). The results are displayed highlighting distinct discriminative patterns."}, {"title": "Conclusion", "content": "In this work, we introduce a novel method for clustering time series data by leveraging graph structures, achieving strong performance across various datasets. Our approach transforms time series data into graph representations using Weighted Dynamic Time Warping, enabling the capture of temporal dependencies and structural relationships. We then apply the proposed Variational Mixture Graph Autoencoder (VMGAE) to generate a Gaussian mixture latent space, improving data separation and clustering accuracy. Extensive experiments demonstrate the effectiveness of our method, including sensitivity analysis on hyperparameters and the evaluation of different convolutional layer architectures. Furthermore, we applied our method to real-world financial data, uncovering community structures in stock markets and showcasing its potential benefits for market prediction, portfolio optimization, and risk management. These findings highlight the versatility and practical applications of VMGAE in addressing time series clustering challenges."}, {"title": "A. Derivation of ELBO for VMGAE", "content": "The Evidence Lower Bound (ELBO) for VMGAE is defined as follows:\nlog p(A) = log \u03a3 \u222b p(A, Z, c)\nZ\nC\n= log \u222b \u03a3 q(Z, c|X, A)\nq(Z, c|X, A) p(A, Z, c)\ndZ\nC\n\u2265 \u222b \u03a3 Eq(Z,c/X,A) log p(A, Z, c) \n= LELBO (X, A),\nwhere X refers to the feature matrix (or time series matrix in our case), and A represents the adjacency matrix. Jensen's inequality is applied to arrive at this bound.\nThe expanded form of LELBO(X, A) using 11 is given by:\nLELBO (X, A) = Eq(Z,c|x,A) [log p(A, Z, c) \u2013 log q(Z, c|X, A)]\n= Eq(Z,c/x,A) [logp(A|Z) +logp(Z|c) + log p(c)] \u2013 Eq(Z,c/x,A) [log q(Z | X, A) + log q(c|X, A)].\nNext, we compute the expectations over the various terms in the ELBO.\nTerm (I):\nEq(Z,cx,A) [log p(A|Z)] = \u03a3 \u03a3log (Aij Zi, Zj)\n= \u03a3 \u03a3 Aij log Aij + (1 \u2013 Aij) log(1 \u2013 Aij).\nTerm (II):\nEq(Z,c/x,A) [log p(Z | c)]\n= \u03a3 \u03a3 ( X, A) log p(Zici)dzi\n= \u03a3 ( A) N (zi 2I) log N (zi dzi\nAccording to appendix B (Jiang et al. 2016), we have:\nEq(Z,cx,A) [log p(Z | c)]\n= \u03a3 ( A) [log(2\u03c0)\n+ \u03a3 (-)+ \u03a3 q(Ci | X, A) log q(ci | X, A)]."}, {"title": "B. Derivation of q(ci|X, A)", "content": "An important point is how to calculate q(ci | X, A). We can reformat the ELBO into the following form:\nLELBO (X, A) = Eq(Z,c/X,A) log q(Z c|X, A) p(A, Z, c)\nEq(Z,c/X,A) log q(Z c|X, A)\n= Eq(Z/X,A) log + log p(c | Z)\nq(Z | X, A) q(c | X, A)\n= Eq(Z/X,A) log + log\nq(Z | X, A)\n\u03a3 \u222b q(Zi X, A) q(ci | X, A) log q(ci | X, A)\ndzi. Eq(Z/X,A) log \nIn the Equation above, the first term is not dependent on c and the second is non-negative. Hence, to maximize LELBO (X, A), DKL(q(Ci | X, A)||p(Ci | zi)) = 0 should be satisfied. As a result, we use the following Equation to compute q(ci | X, A) in VMGAE:\nq(ci | X, A) = p(Ci | Zi) = \u03a3=1P (c) p (Zi c')\nP(Ci)P(Zi Ci)"}, {"title": "D. Evaluation Metrics", "content": "We evaluate the clustering performance in our analysis using two well-established metrics: the Rand Index (RI) and Normalized Mutual Information (NMI). The Rand Index, which quantifies the agreement between the predicted and actual clustering assignments, is computed as follows:\nRI =\nTP+TN\nTP+FP+ FN+TN\nIn this expression, TP (True Positive) denotes the number of pairs of time series correctly classified into the same cluster, while TN (True Negative) refers to the number of pairs accurately assigned to different clusters. Conversely, FP (False Positive) captures the number of pairs incorrectly grouped into the same cluster, and FN (False Negative) accounts for pairs that should be clustered together but are mistakenly separated.\nThe NMI score is defined as:\nNMI =\n\u03a3\u03a3 Nij log()\n\u03a3 Gilog())(\u03a3P; log())"}, {"title": "E. Qualitative Analysis", "content": "We further present visualizations of the evolving clusters during training on the DiatomSizeReduction These clusters are mapped from the latent space representations Z to a 2D space using t-SNE."}, {"title": "F. Ablation Study", "content": "F.1. Hyperparameter Sensitivity Analysis\nIn this section, we analyze the impact and sensitivity of the hyperparameters \u03b3, \u03bb, and \u03b1 on our method. To assess the sensitivity of each hyperparameter, the other hyperparameters were kept fixed at their optimal values, as shown in Table 4. The hyperparameter values \u03b3 = 0.7 and \u03b3 = 1.0 yield better metric results for the SonyAIBORobotSurfacel dataset compared to \u03b3 = 0.2, which was used to report the results in Tables 1 and 2. This improvement was not evident through the visualization process. As shown in the table, for some datasets like Meat, the model is not sensitive to the hyperparameter values, whereas for other datasets, such as Car, the model shows some sensitivity to the hyperparameter values."}, {"title": "F.2. Impact of Convolutional Layer Variants", "content": "Several advanced graph convolutional layers have been developed to enhance information propagation in graph neural networks, each with distinct methods and advantages. One well-known type of convolutional layer is the Graph Attention Network (GAT) (Veli\u010dkovi\u0107 et al. 2018). GAT layers introduce attention mechanisms to graph convolutions, enabling the model to assign different importance to neighboring nodes rather than treating them uniformly. Specifically, the GAT layer computes attention coefficients aij based on node features, which are then used to aggregate information from neighboring nodes. The process of each GAT layer is expressed as follows:\nZ(l+1)\ni\n= \u03c6 \u03a3 aij W Z(1)\njEN(i)\nwhere N(i) denotes the neighbors of node i, and \u03c6 is an activation function. The attention mechanism allows GAT layers to dynamically adjust the influence of neighboring nodes, leading to more flexible and potentially more accurate embeddings.\nAnother variant is SAGEConv (Hamilton, Ying, and Leskovec 2018), which stands for Sample and Aggregation Convolution. This layer generalizes GCNs by allowing for aggregating features from a sampled set of neighbors instead of using all neighbors. Various aggregation operators like mean aggregator, LSTM aggregator, and polling aggregator can perform the aggregation process. The final formula is given by :\nZ(l+1)\ni\n= \u03c6 wz + W AGGREGATE({Z\nj\n: j \u2208 N(i)})),\nwhere AGGREGATE is a function that combines the features of the neighbors.\nChebConv (Defferrard, Bresson, and Vandergheynst 2017) is another robust convolutional layer that utilizes a recursive process to produce Z's and aggregate them by some learnable parameters. The ChebConv whole operation is given by:\nZ(1) = Xi\nZ(2) = L.Xi\nZ(k) = 2L.Zk-1 \u2013 Zk-2\nK\n\u03a7i = \u03a3 \u0398kZ,\nk=0\nwhere Tk (L) denotes the Chebyshev polynomial of order k, and L is the graph Laplacian.\nSimilarly, SGConv (Wu et al. 2019), or Simplifying Graph Convolution, provides an efficient alternative that simplifies the graph convolution operation while maintaining good performance. The operation can be expressed as:\nZi = Softmax (Sk*xi),\nwhere S is the normalized adjacency matrix and k is a fixed number and \u03a3 the laearnable parameter matrix.\nFinally, TAGConv (Du et al. 2018), or Adaptive Graph Convolution, adapts the convolution operation based on the local graph structure. It computes the convolution by taking into account the varying degrees of nodes:\nK\nZ = \u03a3 A XWk,\nk=0\nwhere A is the normalized adjacency matrix and W's are learnable parameters.\nwe examines how different convolutional layers affect the model's ability to learn node embeddings and perform clustering. In the main results shown in Tables 1 and 2, we used Graph Convolutional Network (GCN) layers. Here, we test other types of convolutional layers and compare their effects on the model's performance across different datasets."}, {"title": "F.3. Versatility of VMGAE: Application to Graph Datasets", "content": "While our primary contribution focuses on applying VMGAE to time series data transformed into graph representations, it is important to highlight the versatility of our method, which can be effectively applied to any graph input. The architecture is designed to learn meaningful latent representations across diverse graph datasets.\nTo demonstrate this, we employed the Cora dataset, a benchmark graph dataset comprising scientific publications grouped into distinct categories, with citation relationships forming the edges between nodes. Each node corresponds to a publication, and the edges represent citation links. This dataset is commonly used in graph-based machine-learning tasks due to its structured graph topology and rich node features.\nOur experiments on the Cora dataset further validate the flexibility of our VMGAE architecture. For this evaluation, the learning rate was set to 1e-5 the \u03bb parameter was set to 0.001, the model was trained for 500 epochs, and dropout was applied with a rate of 0.01."}]}