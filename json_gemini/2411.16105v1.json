{"title": "ADAPTIVE CIRCUIT BEHAVIOR AND\nGENERALIZATION IN MECHANISTIC\nINTERPRETABILITY", "authors": ["Jatin Nainani", "Sankaran Vaidyanathan", "AJ Yeung", "Kartik Gupta", "David Jensen"], "abstract": "Mechanistic interpretability aims to understand the inner workings of large neural\nnetworks by identifying circuits, or minimal subgraphs within the model that im-\nplement algorithms responsible for performing specific tasks. These circuits are\ntypically discovered and analyzed using a narrowly defined prompt format. How-\never, given the abilities of large language models (LLMs) to generalize across\nvarious prompt formats for the same task, it remains unclear how well these cir-\ncuits generalize. For instance, it is unclear whether the model's generalization\nresults from reusing the same circuit components, the components behaving dif-\nferently, or the use of entirely different components. In this paper, we investigate\nthe generality of the indirect object identification (IOI) circuit in GPT-2 small,\nwhich is well-studied and believed to implement a simple, interpretable algorithm.\nWe evaluate its performance on prompt variants that challenge the assumptions of\nthis algorithm. Our findings reveal that the circuit generalizes surprisingly well,\nreusing all of its components and mechanisms while only adding additional input\nedges. Notably, the circuit generalizes even to prompt variants where the original\nalgorithm should fail; we discover a mechanism that explains this which we term\nS2 Hacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these models.", "sections": [{"title": "1 INTRODUCTION", "content": "Mechanistic interpretability (Elhage et al., 2021) is an increasingly prominent approach to under-\nstanding the inner workings of large neural networks. Much work in this area is dedicated to discov-\nering circuits, which are subgraphs within the large model that faithfully represent how the model\nsolves a particular task of interest (Olah et al., 2020), by identifying attention heads and paths with\nsignificant causal effects on the model output (Vig et al., 2020; Stolfo et al., 2023; Hanna et al.,\n2023; Prakash et al., 2024). By studying these circuits, researchers aim to uncover simple, human-\ninterpretable algorithms that explain how the model solves the task.\nThese circuits are typically analyzed only within the specific prompt format used to extract them.\nHowever, modern LLMs can often solve the same task across various prompt formats. This raises\nimportant questions about how a circuit generalizes when the prompt format is varied, especially\nwhen the full model generalizes. For instance, it is unclear whether the model's generalization\nresults from the reuse of the same circuit components, the components behaving differently, or the\nuse of entirely different components. Evaluating the generality of a circuit can provide a deeper\nunderstanding of the circuit's behavior and the range of scenarios in which the circuit serves as a\nvalid explanation for the full model's performance.\nThe generality of circuits has significant implications for mechanistic interpretability. Since circuits\nare typically extracted and evaluated using a specific set of prompts, there is no prior expectation for\nthem to generalize. In the worst case, a different prompt format might require a completely different\ncircuit. Ideally, however, the same circuit would solve the task across all prompt variants, providing"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this work, we evaluate the circuit for indirect object identification (IOI) in GPT-2 small (Wang\net al., 2023) using variants of the prompt format that was originally used to discover the circuit. We\nrefer to the original prompt format as base IOI. In the example from Figure 1, \"John\" and \"Mary\"\nare referred to as the subject (S) and indirect object (IO) tokens respectively, and the 10 token is\nthe expected answer. Each base IOI prompt contains one instance of the indirect object token (10)\nand two instances of the subject token (S1, S2). We later introduce variants of this prompt that can\ninclude multiple instances of the indirect object token (101, 102).\nAs in Wang et al. (2023), we measure the performance of circuits and the full model using the logit\ndifference metric, which is defined as the difference between the log probabilities of the 10 token\nand the S token. A larger positive logit difference indicates that the model predicts the correct token\n(10) with higher probability than the incorrect token (S).\nThe base IOI circuit contains a set of distinct attention head types, each of which performs a specific\nfunction in the overall mechanism of the circuit. We include the base IOI circuit diagram in Figure\n7, and further details can be found in Wang et al. (2023). We focus on the following head types:\n\u2022 Name Mover heads are responsible for copying the correct token (10) to the output. These\nheads are active at the END token and attend to previous names in the sentence, copy the name\nthey attend to, and send it to the output.\n\u2022 S-Inhibition heads ensure that the Name Mover heads focus on the 10 token by modifying the\nqueries of the Name Mover heads and suppressing attention to the duplicated tokens (S1, S2).\nThey are active at the END token and attend to S2.\n\u2022 Duplicate Token heads identify tokens that have appeared earlier in the sequence. These heads\nare active at the duplicate instance of a token (S2), attend to its first occurrence (S1), and output\nthe position of the repeated token.\n\u2022 Previous Token heads copy information from token $1 to the next token (S1+1), the token\nfacilitating the transfer of sequential token information.\n\u2022 Induction heads serve a similar function to Duplicate Token Heads. They are active at the\nposition of the duplicated token (S2) and attend to the token that follows its previous instance\n(S1+1). Their output is used as a pointer to S1, and to signal that S was duplicated.\nA small amount of prior work on circuit discovery has evaluated circuit generalization performance\nusing prompts outside of the format used for their discovery. However, these evaluations are often\nlimited and do not fully investigate how the circuit components and mechanisms behave under these\nprompt changes. Wang et al. (2023) evaluated the base IOI circuit on adversarial examples similar to\nthe DoubleIO variant we use in this paper, and observed a drop in model performance. We build on\nthis finding by evaluating the circuit itself and explaining the mechanism behind the model's lower\nyet nontrivial performance. The Greater-Than (Hanna et al., 2023) and Arithmetic (Stolfo et al.,\n2023) circuits were evaluated on prompt variants and included circuit overlap comparisons, though\nthese studies are mostly qualitative and not explored in further detail. The EAP-IG circuit discovery\nmethod (Hanna et al., 2024) was also evaluated using circuit overlap and cross-task faithfulness."}, {"title": "3 TESTING GENERALITY OF THE IOI CIRCUIT USING PROMPT VARIANTS", "content": "Inferring a circuit from model behavior is a critical step, but it is not sufficient on its own-such a\ncircuit acts as a formal hypothesis about the problem-solving mechanisms of the full model, and this\nhypothesis should be probed in various ways to determine the circumstances in which it holds. We\naim to assess the generality of the base IOI circuit and the range of circumstances in which its core\nmechanisms persist. To this end, we introduce two variants of the base IOI prompt that, according\nto the original description of the IOI algorithm, should be unsolvable by the base IOI circuit.\nIn this section, we present three key findings. First, we demonstrate that the unmodified base IOI\ncircuit is unexpectedly capable of sustaining its performance as we systematically vary the nature of\nthe task. Secondly, we demonstrate that the circuit significantly outperforms the model on the vari-\nants, showing consistently high logit difference scores while the model performance drops. Finally,\nwe show that most of the attention heads in the circuit behave nearly identically to how they would"}, {"title": "3.1 IOI PROMPT VARIANTS", "content": "The IOI algorithm assumes that the subject (S) token is duplicated while the indirect object (10)\ntoken is not, and accordingly functions by detecting and suppressing the duplicated token. We can\ntest how much the performance of the base IOI circuit relies on this assumption by manipulating the\nnumber of instances of the 10 token. Our study includes the following range of prompts:\nBase IOI: When John and Mary went to the store, John gave a drink to __________.\nDoubleIO: When John and Mary went to the store, Mary was happy. John gave a drink to __________.\nTripleIO: When John and Mary went to the store, Mary was happy. Mary sat on a bench. John\ngave a drink to __________.\nIn the DoubleIO variant, both the S and IO tokens are duplicated, making it unclear which one the\ncircuit should detect and suppress. In the TripleIO variant, the 10 token appears three times and\nthe S token appears twice. Given the stated logic of the IOI algorithm, where the most frequently\nduplicated token is suppressed, the circuit should return the S token since it now appears most\nfrequently in the prompt."}, {"title": "3.2 BASE IOI CIRCUIT PERFORMANCE ON VARIANTS", "content": "We first evaluate the performance of the full model as well as the base IOI circuit on datasets of 200\nprompts generated for each of the variants. Our data generation strategy follows from Wang et al.\n(2023); we create a set of template sentences for each variant, along with lists of names, places, and\nobjects, and sample from these to construct full sentences.\nTable 1 shows the logit difference scores for the full model and the base IOI circuit on each of\nthe variants. Faithfulness is defined as the ratio of the circuit's logit difference to the model's logit\ndifference, which is a measure of how closely the circuit's performance aligns with that of the model."}, {"title": "3.3 CIRCUIT FUNCTIONALITY ANALYSIS", "content": "The sharp deviation in performance between the base IOI circuit and the full model raises an impor-\ntant question: Are the elements of the circuit maintaining the same functions as they had on the base"}, {"title": "4 S2 HACKING: PERFORMANCE WITHOUT FAITHFULNESS", "content": "As established in the previous section, the base IOI circuit significantly outperforms the full model\non the IOI prompt variants without any changes in the behavior of its components, which appears to\nbe unfaithful to the full model. In this section, we identify the source of the deviations in behavior\nand functionality between the base IOI circuit and the full model.\nThe basis of the discrepancy between the circuit and the model performance is a mechanism in the\nbase IOI circuit which we term S2 Hacking. In the base IOI circuit, the Induction and Duplicate\nToken heads primarily attend to the S2 token, which is always the incorrect answer in each of the\nIOI prompt variants. The outputs of these heads are then used by the S-Inhibition heads to suppress\nthe attention on the S1 and S2 tokens. This mechanism is a key factor in how the circuit is able to\nreturn the correct (10) token with high probability.\nHowever, evaluating this circuit requires knocking out all paths that are not part of the circuit using\nmean ablation (Wang et al., 2023). In the base IOI circuit, the S2 token is the only input token\nthat has a path from the input tokens to the END token that passes through the Duplicate Token and\nInduction heads. This is because the paths from all other input tokens were shown to have a low\ncausal effect on the model output during the circuit discovery process. As a result, the paths from\nall other input tokens to these heads are knocked out."}, {"title": "4.1 METRICS", "content": "Based on the head types and their functions specified by Wang et al. (2023), each head is typically\ncharacterized by its focus on a specific token or set of tokens. We define the following metrics to\ncompare the attention scores of these specific tokens for each of these head types, to understand the\ndeviations in their behavior between the base IOI circuit and the full model.\nConfidence ratio $= \\frac{Attn(correct)}{Attn(incorrect)}$\nFunctional faithfulness $= \\frac{Attn(token) in circuit}{Attn(token) in model}$\nThe circuit confidence ratio for a given head is considered high (> 1) if its attention score in the\ncircuit is higher for the correct token than the incorrect token, and the model confidence ratio is\nsimilarly defined for the head in the full model. If the circuit confidence ratio exceeds that of the\nmodel, it suggests that the circuit is more likely to attend to the correct token than the model is.\nFunctional faithfulness scores are used to compare the attention scores between the model and the\ncircuit for each relevant token; we focus particularly on the S2 and 102 tokens. The ratio is close to\n1 if the circuit attends to the token as much as the model does, while a value greater than 1 indicates\nthat the circuit attends to the token more than the model does, demonstrating a difference in behavior\nfor the head. Confidence ratios and functional faithfulness scores for all heads in the circuit are given\nin Figure 4, where all metrics are plotted with confidence intervals based on 50 samples."}, {"title": "4.2 TRACING THE S2 HACKING MECHANISM", "content": "Our results demonstrate that the S2 Hacking mechanism is primarily carried out through S-Inhibition\nhead 8.6, Induction heads 5.9 and 5.5, and Duplicate head 3.0. Through this mechanism, we find\nthat all of the Name Mover heads show higher confidence in predicting an IO token over an S token.\nThe remaining heads in the circuit show similar behavior to their performance on base IOI prompts.\nAdditional details on these experiments can be found in Appendix A."}, {"title": "5 HOW DOES GPT-2 SMALL ACTUALLY SOLVE DOUBLEIO AND TRIPLEIO?", "content": "Having established that S2 Hacking explains how the base IOI circuit is able to outperform the full\nmodel on the DoubleIO and TripleIO variants, we now investigate how the full model actually solves\nthese variants. To do this, we discover a new circuit for each variant using the same patch patching\nmethodology and experimental framework that was used to discover the base IOI circuit (Wang\net al., 2023). In addition to explaining how the model solves the DoubleIO and TripleIO variants,\nthese circuits reveal that all components of the base IOI circuit are reused for these variants."}, {"title": "5.1 ADDING PATHS FROM INPUT TOKENS", "content": "The S2 Hacking mechanism suggests a starting point for discovering DoubleIO and TripleIO cir-\ncuits. Recall that in the base IOI circuit, S2 is the only input token with outgoing paths to the\nDuplicate heads, with all paths from the remaining input tokens being ablated out. For the variants,\nwe start with the base IOI circuit and restore some of these paths from other input tokens that were\noriginally ablated out, and see if any of them have a causal effect on the output of the model. The\nresults are shown in Figure 5.\nFor DoubleIO, we observe that adding paths from the 102 token brings the circuit's performance\nclosest to the full model, with a normalized faithfulness of 0.77. This is done by adding 10 edges\nto the base IOI circuit: two for each of the three Duplicate heads and the two Previous Token heads\nthey depend on. In contrast, adding paths from just the 101 token has little impact on the circuit's\nperformance, while adding paths from other input tokens further degrades its performance. Hence\nfor every path from the S2 token to a Duplicate Token head, the corresponding path from the 102\ntoken to the same head also has a causal effect on the model output.\nA similar pattern emerges in TripleIO, where adding paths from the 102 and 103 tokens brings the\ncircuit's performance closest to that of the full model, achieving a normalized faithfulness of 0.79.\nThis requires adding 20 edges to the base IOI circuit: the same 10 edges corresponding to the 102\ntoken that were added to the DoubleIO circuit, plus 10 edges corresponding to the 103 token."}, {"title": "5.2 DISCOVERING CIRCUIT REUSE THROUGH PATH PATCHING", "content": "We use the methodology of Wang et al. (2023) to discover a circuit for the DoubleIO and TripleIO\nvariants, starting with the identification of Name Mover heads. For each attention head and relevant\ninput token, we compute the direct causal effect of the path that starts from the token and proceeds\nthrough the head to the final logit at the END position. This type of token-level causal effect esti-\nmation is important for variants like DoubleIO and TripleIO where multiple tokens are duplicated,\nsince each of these duplicates can have paths to different heads.\nSurprisingly, we do not observe any significant deviation in results compared to the base IOI circuit.\nThe Name Mover heads from the base IOI circuit are just as causally relevant for the DoubleIO and\nTripleIO variants, and none of the other heads in the model have a substantial enough direct causal\neffect score to suggest the addition of a new Name Mover head to the circuit. These results indicate\nthat the Name Mover heads from the base IOI circuit are being reused for both variants.\nWe observe the same pattern in the S-Inhibition heads, which we refer to more generally as Inhibition\nheads since they could inhibit either of the duplicated names in the prompt. To identify these heads,\nwe compute the direct causal effects of every head in the model on the queries of the Name Mover\nheads. We find that all of the S-Inhibition heads from the base IOI circuit have the most significant\ncausal effects, indicating that the DoubleIO and TripleIO circuits are also reusing these heads."}, {"title": "5.3 CHOOSING BETWEEN THE DUPLICATED NAMES", "content": "The circuit in Figure 7 demonstrates that Inhibition heads now receive information from both the\n102 and S2 duplicate tokens in the DoubleIO prompt. This raises a natural question: How does the\nmodel decide which duplicate to suppress to produce the correct answer? To answer this, we sought\nto find decision points: heads that are most responsible for choosing between the duplicates.\nSurprisingly, we find that the order in which names appear in the prompt significantly affects the\nperformance of both the full model and the DoubleIO circuit, with higher performance when the IO\ntoken comes first. Figure 8 (left) shows the logit differences stratified by whether S or IO comes\nfirst, and the overall logit difference appears to be an average of the two. This suggests that one or\nmore attention heads respond differently depending on the order of the names in the prompt.\nWe find this behavior in head 2.2, a Previous Token head. This head appears to implement a \"first\ncome, first serve\u201d mechanism, where it primarily attends to the name that appears first in the prompt,\nthereby serving as a key decision point in the circuit. As shown in Figure 8 (right), the head fre-\nquently assigns high attention to one of the name tokens (S, 10), based on which appeared first. In\nprompts where 10 appeared first, the head attended far more to the 10+1 token than it did its S+1\ncounterpart, and vice versa when S appeared first. The full set of experiments is in Appendix B, and\na more detailed study of the duplicate suppression mechanism is left to future work."}, {"title": "6 CONCLUSION", "content": "Our investigation reveals that the IOI circuit in GPT-2 small generalizes more effectively than pre-\nviously understood, reusing its core components and mechanisms while adapting through minimal\nstructural changes, such as adding input edges. Although the base IOI circuit behaved unexpectedly\non the DoubleIO and TripleIO prompt variants, as demonstrated by S2 Hacking, the functionality\nof its attention heads remained intact. This study of circuit generalization and evaluation on prompt\nvariants ultimately deepened our understanding of how GPT-2 small solves the IOI task, while of-\nfering critical insights into the broader capabilities of large neural networks."}, {"title": "A S2 HACKING", "content": "In this section, we provide further detail on how each of the attention heads in the base IOI circuit\nbehave under S2 Hacking. The Name Mover heads in the circuit are responsible for returning the\noutput, ideally the 10 token, and we find that all of these heads attend primarily to the IO token. To\nunderstand how this occurs, we consider all of the heads that have an incoming edge from the S2\ntoken: the S-Inhibition, Induction, Duplicate, and Previous Token heads."}, {"title": "A.1 S-INHIBITION HEADS", "content": "The ideal behavior of the S-Inhibition heads on the base IOI prompt is to focus on the duplicated\ntoken S2 at the END position. These heads influence the queries of the Name Mover heads, guiding\nthem to reduce their attention on the S2 token. This enables them to focus on the IO token, which\nis the correct answer. However, since there are two duplicates in the DoubleIO prompt variant, S2\nand 102, it is possible for these heads to attend to both duplicates. The optimal behavior of these\nheads is to place high attention on the S2 token while maintaining low attention on the 102 token\nat the END position.\nWe see an example of S2 Hacking in Head 8.6, which exhibits a higher confidence ratio in the circuit\ncompared to the model. The model confidence ratio is close to 1 and the circuit confidence ratio is\ngreater than 2, indicating that the head attends roughly equally to the S2 and 102 tokens in the\nmodel but significantly favors the S2 token in the circuit. This enables it to effectively direct the\nName Mover heads to focus much less on the S2 token and thereby avoid the incorrect answer."}, {"title": "A.2 INDUCTION HEADS", "content": "When an Induction head encounters a duplicate instance of the subject token S2 in the prompt, it\nattends primarily to the word immediately following the previous instance of the subject (S1+1).\nThere are two duplicates in the DoubleIO prompt variant, so the circuit performance would increase\nif the induction head placed high attention on the S1+1 token at the S2 position and low attention\non the 101+1 token at the 102 position. This would ensure that the head continues to prioritize the\nS2 token.\nS2 Hacking is evident in Heads 5.5 and 5.9. Their confidence ratios are close to 1 in the full model,\nindicating roughly equal attention assigned to both the S2 and 102 tokens. However, the circuit\nconfidence ratios are significantly higher, suggesting a stronger preference for the S2 token. Addi-\ntionally, the functional faithfulness scores show higher values for the S2 token compared to the 102\ntoken, indicating that attention on the S2 token deviates further from the behavior of the full model.\nIn contrast, Head 6.9 does not appear to rely on S2 Hacking since it already favors the S2 token\nheavily, as shown by the high confidence ratios on both the model and the circuit and the functional\nfaithfulness scores on the S2 and 102 tokens are very similar. This is similar to the pattern we\nobserved in the S-Inhibition Heads 7.3 and 7.9. Interestingly, Head 5.8 does not seem to benefit\nfrom S2 Hacking and still favors the 102 token."}, {"title": "A.3 DUPLICATE HEADS", "content": "The function of the Duplicate heads is to focus on the first instance of a name in the prompt when a\nduplicate instance is encountered. Since the Duplicate heads influence the queries of the Induction\nheads, it is advantageous for them to attend highly to S1 at the S2 token position while avoiding\nattending to 101 at the 102 token position.\nHeads 0.1 and 0.10 demonstrate nearly identical performance between the circuit and the full model,\nwith both attending equally to 101 and S1. This is evident from the confidence and functional\nfaithfulness scores all being close to 1. In contrast, Head 3.0 shows strong signs of S2 Hacking;\nwhile its confidence ratio is close to 1 for the full model, it significantly increases for the circuit. The\nfunctional faithfulness scores further show that the behavior of this head on the S2 token diverges\nfrom the full model much more than on the 102 token. This head shows signs of a decision point\nwhere S2 Hacking begins, with its effects cascading down to the Induction and S-Inhibition heads."}, {"title": "A.4 PREVIOUS TOKEN HEADS", "content": "The purpose of a Previous Token head at a given token position is to attend to the token that imme-\ndiately precedes it in the prompt. In the DoubleIO prompt variant, it would be beneficial for these\nheads to maintain this functionality at the S1+1 position while minimizing attention on the 101 to-\nken at the 101+1 position. This is important because the Previous Token heads influence the values\nof the Induction heads in the base IOI circuit, and we have previously seen how the Induction heads\nbenefit from focusing on the subject token.\nUnlike the other head types above, the Previous Token heads appear not to rely very much on the\nS2 Hacking mechanism. Head 4.11 functions almost identically in both the circuit and the model,\nwith confidence and functional faithfulness scores consistently close to 1. While Head 2.2 slightly\nbenefits from S2 Hacking, the circuit deviates very little from the full model. In both cases, these\nheads attend more to the S1 token at position S1+1 than they do to the 101 token at the 101+1\nposition, so this head appears to favor the S1 token regardless."}, {"title": "B IDENTIFYING LAYER 2 HEAD 2 AS THE DECISION POINT", "content": "We measure the difference between the direct effect on the S tokens from the Direct Effect on the IO\ntokens on every relevant head. We selected Previous Token Heads 4.11 and 2.2 as the most likely\ncandidates to be decision points as a result of the aforementioned difference scores. Plotting the\nattention patterns of each head, however, led us to conclude that Head 2.2 was a better candidate\nthan Head 4.11. This is because while 4.11 was found to have a high causal effect on these tokens,\nit was only because it was attending highly to both IO and S tokens to a similar degree. Head 2.2 on\nthe other hand, was found to more frequently assign high probably to only one of the two duplicate\nname tokens, making it the best decision point candidate within the circuit. Plots for the attention\npatterns of head 4.11 can be found in Figure 9, and the same for head 2.2 in Figure 8.\nPrevious Token Head 4.11 attends equally to the 10+1 and S+1 tokens, regardless of the order in\nwhich the name tokens appear in the prompt. This evidence dissuaded us from considering 4.11 as\nthe decision point where the model would decide which duplicate token to inhibit."}, {"title": "C PROMPT TEMPLATES", "content": "Here we provide the BABA-style prompt templates to generate the DoubleIO and TripleIO datasets\nand prompts. The ABBA-style format is generated by switching the order of the first two names.\nBABA Templates - DoubleIO\nThen, [B] and [A] went to the [PLACE], [A] was happy. [B] gave a [OBJECT] to [A]\nThen, [B] and [A] had a lot of fun at the [PLACE], [A] seemed distracted. [B] gave a\n[OBJECT] to [A]\nThen, [B] and [A] were working at the [PLACE], [A] was confused. [B] decided to give a\n[OBJECT] to [A]\nThen, [B] and [A] were thinking about going to the [PLACE], [A] was excited. [B] wanted\nto give a [OBJECT] to [A]\nThen, [B] and [A] had a long argument, and afterwards [A] smiled. [B] said to [A]\nAfter [B] and [A] went to the [PLACE], [A] looked tired. [B] gave a [OBJECT] to [A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE], [A] laughed. [B] decided to give it to\n[A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE], [A] was deep in thought. [B] decided\nto give the [OBJECT] to [A]\nWhile [B] and [A] were working at the [PLACE], [A] was happy. [B] gave a [OBJECT]\nto [A]\nWhile [B] and [A] were commuting to the [PLACE], [A] seemed distracted. [B] gave a\n[OBJECT] to [A]\nAfter the lunch, [B] and [A] went to the [PLACE], [A] was confused. [B] gave a [OB-\nJECT] to [A]\nAfterwards, [B] and [A] went to the [PLACE], [A] was excited. [B] gave a [OBJECT] to\n[A]\nThen, [B] and [A] had a long argument, [A] smiled. Afterwards, [B] said to [A]\nThe [PLACE] [B] and [A] went to had a [OBJECT], [A] looked tired. [B] gave it to [A]\nFriends [B] and [A] found a [OBJECT] at the [PLACE], [A] laughed. [B] gave it to [A]\nBABA Templates - TripleIO\nThen, [B] and [A] went to the [PLACE], [A] sat on a bench. [A] had left their [OBJECT]\nbehind. [B] gave a [OBJECT] to [A]\nThen, [B] and [A] had a lot of fun at the [PLACE], [A] seemed lost in thought. [A] looked\nup at the sky. [B] gave a [OBJECT] to [A]\nThen, [B] and [A] were working at the [PLACE], [A] was laughing softly. [A] glanced at\nthe [OBJECT]. [B] decided to give a [OBJECT] to [A]\nThen, [B] and [A] were thinking about going to the [PLACE], [A] appeared confused. [A]\nfumbled with their [OBJECT]. [B] wanted to give a [OBJECT] to [A]\nThen, [B] and [A] had a long argument, and afterwards [A] smiled. [A] adjusted their\n[OBJECT]. [B] said to [A]\nAfter [B] and [A] went to the [PLACE], [A] yawned. [A] rubbed their eyes. [B] gave a\n[OBJECT] to [A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE]. [A] looked at the ground, [A] sighed\ndeeply. [B] decided to give it to [A]\nWhen [B] and [A] got a [OBJECT] at the [PLACE], [A] smiled at the view. [A] seemed\nrelieved. [B] decided to give the [OBJECT] to [A]\nWhile [B] and [A] were working at the [PLACE], [A] sat on a bench. [A] had left their\n[OBJECT] behind. [B] gave a [OBJECT] to [A]\nWhile [B] and [A] were commuting to the [PLACE], [A] seemed lost in thought. [A]\nlooked up at the sky. [B] gave a [OBJECT] to [A]\nAfter the lunch, [B] and [A] went to the [PLACE], [A] was laughing softly. [A] glanced\nat the [OBJECT]. [B] gave a [OBJECT] to [A]\nAfterwards, [B] and [A] went to the [PLACE], [A] appeared confused. [A] fumbled with\ntheir [OBJECT]. [B] gave a [OBJECT] to [A]\nThen, [B] and [A] had a long argument, [A] smiled. [A] adjusted their [OBJECT]. [B]\nsaid to [A]\nThe [PLACE] [B] and [A] went to had a [OBJECT], [A] yawned. [A] rubbed their eyes.\n[B] gave it to [A]\nFriends [B] and [A] found a [OBJECT] at the [PLACE], [A] looked at the ground. [A]\nsighed deeply. [B] gave it to [A]"}]}