{"title": "Learning-Enhanced Safeguard Control for\nHigh-Relative-Degree Systems: Robust\nOptimization under Disturbances and Faults", "authors": ["Xinyang Wang", "Hongwei Zhang", "Shimin Wang", "Wei Xiao", "Martin Guay"], "abstract": "Abstract\u2014Merely pursuing performance may adversely\naffect the safety, while a conservative policy for safe ex-\nploration will degrade the performance. How to balance the\nsafety and performance in learning-based control problems\nis an interesting yet challenging issue. This paper aims\nto enhance system performance with safety guarantee in\nsolving the reinforcement learning (RL)-based optimal con-\ntrol problems of nonlinear systems subject to high-relative-\ndegree state constraints and unknown time-varying dis-\nturbance/actuator faults. First, to combine control barrier\nfunctions (CBFs) with RL, a new type of CBFs, termed\nhigh-order reciprocal control barrier function (HO-RCBF)\nis proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient\nsimilarity is proposed to quantify the relationship between\nthe gradient of safety and the gradient of performance.\nFinally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the per-\nformance with a safety guarantee. Two simulation examples\nillustrate that the proposed safe RL framework can address\nhigh-relative-degree constraint, enhance safety robustness\nand improve system performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In practice, safety, optimality and stability are three impor-\ntant design objectives for control systems, especially safety-\ncritical and resource-constrained systems [1, 2]. As an effec-\ntive approach to simultaneously consider stability, optimality\nand safety, safe optimal control has attracted increasing at-\ntention in the control community in the past few years, which\naims to stabilize dynamical systems while minimizing a user-\ndefined cost function and adhering to certain safety constraints.\nIt has been successfully applied in various practical scenarios,\nsuch as autonomous driving [3] and trajectory optimization [4].\nSo far, many approaches, such as model predictive control\nand safety filter, to name a few [5], have been investigated\nto address safe optimal control problems. However, these\napproaches require accurate system information to project the\noptimal controller into the safe input set, and thus cannot\nrigorously guarantee safety in the presence of disturbances. As\nan effective tool for designing optimal controllers for uncertain\nsystems, reinforcement learning (RL) has been studied for\naddressing safe optimal control problems by using some tech-\nnical strategies, such as external knowledge [6], reachability\nanalysis [7], and worst-case criterion [8].\nDespite their success in practical applications, these ap-\nproaches encounter a serious challenge of computational cost,\nespecially for high-dimensional systems. To reduce the compu-\ntational burden, the concept of control barrier function (CBF)\nwas introduced in [9] as a safety constraint to certify the\nforward invariance of a safety set. Two notions of CBF are\ncommonly utilized i.e., reciprocal CBF (RCBF) and zeroing\nCBF (ZCBF). The former goes to infinity when approaching\nthe boundary of the safety set, while the latter vanishes.\nRecently, CBF has been integrated into RL techniques to\naddress safe, optimal control problems. The approach from\n utilizes the barrier transformation technique to map a\nconstrained system to an unconstrained one, allowing standard\nRL techniques to be applied directly. Unfortunately, these tech-\nniques are restricted to box safety constraints and cannot ade-\nquately address complex safety constraints, such as ellipsoidal\nand cone safety constraints [11, 12], which are frequently\nencountered in robotic control. Similar to the penalty method\n(see Chapter 2.1.1 in [13]), a barrier function-based penalty\nterm is incorporated into the cost function in [14], taking\ninto account both optimality and safety during the learning\nprocess. However, as argued in [15], the effectiveness of the\nCBF-based penalty method strongly depends on the proper\ndesign of the cost function, and an improper cost function may\nlead to undesirable learning results. It is also worth noting that\napproaches [14, 10], where safety and optimality are tightly\ncoupled, may violate the safety constraints during the learning\nprocess.\nDifferent from the approaches [14, 10], a recent work\nintroduces a safeguarding-based RL method that empha-\nsizes learning a performance-driven policy while guaranteeing\nsafety through an augmented safeguarding policy. In other\nwords, this method simplifies the learning target by decou-\npling the safety objective from the learning process using a"}, {"title": "II. PROBLEM FORMULATION", "content": "Consider a continuous-time nonlinear control affine system\n$\\dot{x} = f(x) + g(x) (u + u_f)$ (1)\nand its nominal system\n$\\dot{x} = f(x) + g(x) u$ (2)\nwhere $x \\in R^n$ and $u \\in R^p$ are the state vector and the\ninput vector of the system, respectively; $u_f \\in R^p$ represents\nthe unknown actuator fault and/or matched disturbance. The\ndrift dynamics $f : R^n \\rightarrow R^n$ and the control dynamics\n$g: R^n \\rightarrow R^{n \\times p}$ are locally Lipschitz. It is assumed $f(0) = 0$\nand the system is stabilizable. Given a stabilizing control\npolicy $u(t) = k(x, t)$ such that $k(0,t) = 0$, $f(x)+g(x)k(x,t)$\nis locally Lipschitz in $x$ and piecewise continuous in $t$, for any\ninitial state $x_o$ at 0, there always exists a solution of the system\nsatisfying\n$\\dot{x} = f(x) + g(x) k(x,t)$, $x(0) = x_o$.\nAssumption 1: The actuator fault/matched disturbance are\nbounded, continuously differentiable signals with bounded\ntime derivative, i.e.,\n$u_f \\in U_f := \\{ u_f \\in R^p | ||u_f||_\\infty \\le \\eta_1, ||\\dot{u}_f||_\\infty \\le \\eta_2 \\}$,\nwhere $\\eta_1$ and $\\eta_2$ are unknown positive constants.\nRemark 1: In practice, it is reasonable to make Assump-\ntion 1 on $u_f$ [22]. As an actuator fault, $u_f$ can represent\nforce biases caused by motors in unmanned aerial vehicles\n[23], actuator bias of distributed generators in microgrids\n[24], and so on. In addition, $u_f$ can represent a matched\ndisturbance, which is a common occurrence in several practical\napplications, such as permanent magnet synchronous motors\n[25] and unmanned vehicles [26].", "A. Unconstrained optimal control": "To seek an optimal control policy for (3), one way is to\nsolve the following optimal control problem [27]\n$\\inf_{u \\in R^p} J(x, u) := \\int_0^\\infty (x(t)^T Q x(t) + u(t)^T R u(t)) dt$, (3)\nwhere J is the user-defined cost function and Q, R > 0."}, {"title": "III. ADAPTIVE HIGH-ORDER SAFEGUARDING CONTROL", "content": "To address such an optimal control problem, the optimal\nvalue function is defined as\n$V^*(x(t)) = \\inf_{u \\in R^p} \\int_t^\\infty l(x(\\tau), u(\\tau)) d\\tau$.\nand the Hamiltonian function can be obtained by differentiat-\ning the optimal value function\n$H(x, \\nabla V^*(x), u) = \\nabla V^*(x)^T (f(x) + g(x)u) + l(x, u)$ (4)\nwhere $\\nabla V^*(x) = \\partial V^*(x)/\\partial x$ and $V^*(0) = 0$.\nProvided a continuously differentiable $V^*(x)$ exists, it is\nthe unique positive definite solution of the Hamilton-Jacobi-\nBellman (HJB) equation\n$\\inf_{u \\in R^p} H(x, \\nabla V^*(x), u) = 0$. (5)\nBy applying the optimal condition $\\partial H/\\partial u = 0$, the optimal\ncontrol policy can be derived as\n$k^*(x) = -\\frac{1}{2} R^{-1} g(x) \\nabla V^*(x)$. (6)", "B. Constrained optimal control": "Suppose x is subject to inequality constraints and can be\nmathematically formulated as $x \\in \\mathcal{C} \\subset R^n$. The set $\\mathcal{C}$ is\nnonempty with no isolated point and has the following form\n$\\mathcal{C} = \\{x \\in R^n | h(x) \\ge 0\\}$\n$\\mathcal{C}^S = \\{x \\in R^n | h(x) = 0\\}$\n$\\text{Int}(\\mathcal{C}) = \\{x \\in R^n | h(x) > 0\\}$\nwhere $h : R^n \\rightarrow R$ is $m$th-order differentiable. Then, a\nstandard-constrained optimal control problem can be formu-\nlated as follows.\nProblem 1: Consider the nominal nonlinear system (2) and\na safety set $\\mathcal{C} \\subset R^n$. Find an admissible control policy $u(.)$\nto solve the optimal control problem (3) while rendering the\nset $\\mathcal{C}$ forward invariant [28], i.e.,\n$\\inf_{u \\in R^p} \\int_0^\\infty l(x(\\tau), u(\\tau)) d\\tau$, (7)\ns.t. $x(t) \\in \\mathcal{C}, \\forall t \\ge 0, x(0) \\in \\mathcal{C}$\nFor (7), the control policy (6) is optimal but generally\ndoes not satisfy the constraint. Considering the fact that di-\nrectly solving (7) is computationally expensive and inefficient,\nCBFs are applied to provide sub-optimal, but computationally\nefficient, solutions to constrained optimal control problems.\nBefore presenting CBF-based solution of the constrained con-\ntrol problem, we introduce some basic definitions commonly\nencountered in safe control methodologies.\nDefinition 1: [9] (Reciprocal control barrier function): A\ncontinuously differentiable function $b : \\mathcal{C} \\rightarrow R_{\\ge 0}$ is a\nreciprocal control barrier function (RCBF) with respect to\n$h(x)$ for the system (2) if there exist class K functions $\\beta_1, \\beta_2$\nand $\\beta_3$ such that, for all $x \\in \\text{Int}(\\mathcal{C})$\n$\\frac{1}{\\beta_1(h(x))} \\le b(x) \\le \\frac{1}{\\beta_2(h(x))}$\n$\\inf_{u \\in R^p} \\{L_f b(x) + L_g b(x) u\\} \\le \\beta_3(h(x))$\nwhere $\\nabla b(x) = \\partial b(x)/\\partial x$, $L_f b(x) = \\nabla b(x) f(x)$ and\n$L_g b(x) = \\nabla b(x) g(x)$ are Lie derivatives of $b(x)$ along $f(x)$\nand $g(x)$, respectively.\nDefinition 2: (Relative degree): Given a safety set $\\mathcal{C}$. An\nrth-order differentiable function $h: R^n \\rightarrow R$ is said to have a\nrelative degree of $m \\le r$ with respect to (1), if $\\forall x \\in \\mathcal{C}^S$, there\nexists a neighborhood of $x$ such that in this neighborhood\n$||L_g L_f^{m-1} h(x)|| \\neq 0$ and $||L_g L_f^{k} h(x)|| = 0$,\n$\\forall k = 0, 1, ..., m - 2$.\nMany existing works combine CBFs and RL to solve\noptimal control problems while guaranteeing the safety of (2).\nIn [16], a safeguarding policy $u^s(x)$ is introduced to learn\nthe unconstrained optimal control policy $k^*(x)$ safely, and the\ncontroller becomes\n$u(x) = -K R^{-1} L_g b(x)^T + k^*(x)$,\n$u^s(x)$\nwhere $R > 0$ is the same weight in (3) and $K_s > 0$ is a user-\ndefined safeguarding gain. For the case of $m = 1$, $u(x)$ can\nkeep the state trajectory of system (2) with $k^*(x)$ staying in\n$\\text{Int}(\\mathcal{C})$ for any $x_o \\in \\text{Int}(\\mathcal{C})$. The disturbance considered in\nthis problem can be generalized to mismatched disturbance.\nHowever, provided that $h(x)$ has relative degree $m > 1$ with\nrespect to (2), the safeguarding controller cannot be utilized\nto guarantee safety, due to the absence of the control terms.\nIt is worth noting that many practical applications have\nhigh-relative-degree constraints, such as traffic merging [29],\nadaptive cruise control [30] and motion planning [31]. How-\never, effective safeguarding policies with the ability to address\nhigh-relative-degree constraints has not been satisfactorily\ninvestigated to date. In the next section, we shall extend\nthe safeguarding control policy to address high-relative-degree\nconstraints."}, {"title": "III. ADAPTIVE HIGH-ORDER SAFEGUARDING CONTROL", "content": null, "A. High-order RCBF": "We define a series of functions as\n$\\Psi_i(x) = \\Psi_{i-1}(x) + a_i (\\Psi_{i-1}(x))$, $i \\in \\{1, 2, ..., m\\}$ (8)\nwhere $\\Psi_0(x) = h(x)$ and $a_1, a_2, \\ldots, a_{m-1}$ are sufficiently\nsmooth extended class K functions such that $\\Psi_i(x_0) > 0$ for\n$1, 2, ..., m-1$. Then we define a series of sets $\\mathcal{C}_i$ of the form\n$\\mathcal{C}_i = \\{x \\in R^n | \\Psi_{i-1}(x) > 0\\}$, $i \\in \\{1, 2, \\ldots, m\\}$, (9)\nand propose a high-order RCBF (HO-RCBF) defined as fol-\nlows.\nDefinition 3: (HO-RCBF): Given a safety set $\\mathcal{C}$, let\n$\\Psi_{i-1}(x)$ and $\\mathcal{C}_i$ be defined as (8) and (9) for all $i \\in$\n$\\{1, 2, ..., m\\}$, respectively. A continuously differentiable\nfunction $B: \\mathcal{C}_m \\rightarrow R$ is a HO-RCBF for the nonlinear system"}, {"title": "High-order safeguarding policy", "content": "(2) if $\\forall x_o \\in \\text{Int}(\\mathcal{C})$, there exist class K functions $\\gamma_1, \\gamma_2$ and\n$\\gamma_3$ such that $\\forall x \\in \\mathcal{C} := \\cap_{i=1}^m \\mathcal{C}_i$,\n$\\frac{1}{\\gamma_1 (\\Psi_{m-1})} \\le B(\\Psi_{m-1}) \\le \\frac{1}{\\gamma_2 (\\Psi_{m-1})}$,\n$\\inf_{u \\in R^p} \\{L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) u\\} \\le \\gamma_3 (\\Psi_{m-1})$, (10)\nwhere the argument $x$ of $\\Psi_{m-1}$ is omitted for simplicity, and,\n$\\nabla B(\\Psi_{m-1}) = \\frac{\\partial B}{\\partial \\Psi_{m-1}}$,\n$L_f B(\\Psi_{m-1}) = \\nabla B(\\Psi_{m-1})^T L_f \\Psi_{m-1}$,\n$L_g B(\\Psi_{m-1}) = \\nabla B(\\Psi_{m-1})^T L_g \\Psi_{m-1}$.\nWhen $m = 1$, the HO-RCBF is reduced to a RCBF [9]. Given\nany $x_o \\in \\text{Int}(\\mathcal{C})$, one can always find K functions $a_i$ such\nthat $\\Psi_i(x_o) > 0$, $\\forall i \\in \\{1, 2, \\ldots, m\\}$, i.e., $x_o \\in \\mathcal{C}$. Since\n$\\mathcal{C} \\subset \\text{Int}(\\mathcal{C})$, one can show $x(t) \\in \\text{Int}(\\mathcal{C}), \\forall t \\ge 0$ if $\\mathcal{C}$ is\nforward invariant. In the following Lemma, we illustrate that\nthe existence of a HO-RCBF for (2) implies the existence of\na safe policy that renders $\\mathcal{C}$ forward invariant.\nLemma 1: The set $\\mathcal{C}$ is forward invariant for system (2)\nunder any Lipschitz continuous control policy $u \\in K_{hf}(x)$ if\n$B(\\Psi_{m-1})$ is a HO-RCBF candidate, where $K_{hf}(x) =$\n$\\{u \\in R^p | L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) u \\le \\gamma_3 (\\Psi_{m-1})\\}$.\nProof: From $\\Psi_{m-1}(x_0) > 0$ and a HO-RCBF candidate\n$B(\\Psi_{m-1})$, we have $B(\\Psi_{m-1}) < \\infty$ at $x_0$ and\n$\\dot{B}(\\Psi_{m-1}) \\le \\gamma_3(\\Psi_{m-1})$.\nThus, one has $B(\\Psi_{m-1}) < \\infty$ and further $\\Psi_{m-1}(x) > 0$,\n$\\forall t \\in [0, \\infty)$, which implies $x \\in \\mathcal{C}_m$. Based on the definition\n(8), $\\Psi_{m-1}(x) > 0$, i.e.,\n$\\Psi_{m-2}(x) + a_{m-1}(\\Psi_{m-2}(x)) > 0$,\nimplies that $x \\in \\mathcal{C}_{m-1}$ since $\\Psi_{m-2}(x_0) > 0$. Iteratively, we\nhave $x \\in \\mathcal{C}_1$ for $\\forall t \\in [0, \\infty)$. Therefore, $x \\in \\mathcal{C}$ for all $t > 0$\nif $x(0) \\in \\mathcal{C}$. Hence, $\\mathcal{C}$ is forward invariant.\nBy Lemma 1, given a constrained optimal control problem\nwith a safe initial state, one can always convert the safety\nobjective to rendering the set $\\mathcal{C}$ forward invariant by construct-\ning a valid HO-RCBF candidate. Taking disturbance/actuator\nfault into consideration, and using a HO-RCBF to certify\nthe forward invariance of $\\mathcal{C}$, we reformulate the constrained\noptimal control problem as follows.\nProblem 2: Consider the nonlinear system (1) subject to\nunknown actuator bias fault and/or disturbance, and a safety\nset $\\mathcal{C} \\subset R^n$ described by a differentiable function $h : R^n \\rightarrow R$\nwith arbitrary relative degree $m \\ge 1$. Given any $x_o \\in \\text{Int}(\\mathcal{C})$,\ndefine a series of functions $\\Psi_i$ (8) and a series of corresponding\nsets $\\mathcal{C}_i$ (9) satisfying $\\Psi_i(x_o) > 0$, $\\forall i = 1, 2, \\ldots m$. For\n$\\mathcal{C} = \\mathcal{C}_1 \\cap \\ldots \\cap \\mathcal{C}_{m-1} \\cap \\mathcal{C}_m$,\nlet $B(\\Psi_{m-1})$ be a valid HO-RCBF with respect to (1). Find\nan admissible control policy to solve\n$\\inf_{u \\in R^p} \\int_0^\\infty l(x(\\tau), u(\\tau)) d\\tau$\ns.t. $L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) (u + u_f) \\le \\gamma_3(\\Psi_{m-1})$\nwhere $u_f$ satisfies Assumption 1.\nBefore presenting the solution, we make some assumptions.\nAssumption 2: Given the system (1) and the safety set $\\mathcal{C}$,\nthe following conditions hold:\n1) The origin lies in $\\text{Int}(\\mathcal{C})$, and there exists a neighbor-\nhood $\\mathcal{N}_1$ of the origin such that $\\mathcal{N}_1 \\cap \\mathcal{C}^S = \\emptyset$.\n2) There exist positive constants $\\underline{g}$ and $\\overline{g}$ such that $\\underline{g} <\n||g(x)|| \\le \\overline{g}$ for all $x \\in \\mathcal{C}_m$.\nRemark 2: The first condition in Assumption 2 is made for\nexcluding the case where $0 \\in \\mathcal{C}^S$, as RCBFs cannot guarantee\nthe safety in this scenario. The second condition guarantees\nthat the control gains are not required to vanish or explode to\npreserve the safety and stability of the system."}, {"title": "High-order safeguarding policy", "content": "If the optimal value function $V^*(x)$ exists, it is the unique\npositive definite solution of the following problem [15]\n$\\inf_{u \\in R^p} H(x, \\nabla V^*(x), u)$ (11)\ns.t. $L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) (u + u_f) \\le \\gamma_3 (\\Psi_{m-1})$,\nwhere $H(x, \\nabla V^*(x), u)$ is the same Hamiltonian function as\n(4), and $V^*(0) = 0$. Define the Lagrangian function\n$L(x, \\nabla V^*(x), u, u_f, \\lambda) = H(x, \\nabla V^*(x), u) + \\lambda^T (L_f B(\\Psi_{m-1})\n+L_g B(\\Psi_{m-1}) (u + u_f) - \\gamma_3 (\\Psi_{m-1}))$\nwhere $\\lambda$ is the Lagrange multiplier.\nUsing the Karush-Kuhn-Tucker (KKT) conditions [32], we\nhave\n$\\frac{\\partial H}{\\partial u} + (L_g B(\\Psi_{m-1}))^T \\lambda = 0$,\n$\\frac{\\partial L}{\\partial \\lambda} = L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) (u + u_f) - \\gamma_3 (\\Psi_{m-1}) \\le 0$,\n$\\lambda^T (L_f B(\\Psi_{m-1}) + L_g B(\\Psi_{m-1}) (u + u_f) - \\gamma_3 (\\Psi_{m-1})) = 0$,\n$\\lambda \\ge 0$.\nSolving the above set of equations/inequalities yields the safe\noptimal control policy\n$u(x) = -\\lambda R^{-1} L_g B(\\Psi_{m-1})^T + k^*(x)$, (12)\n$u^s(x)$\nand the optimal Lagrange multiplier\n$\\lambda = \\text{max} \\left(\\frac{L_f B + L_g B (k^*(x) + u_f) - \\gamma_3 (\\Psi_{m-1})}{CBR^{-1}L_gB^T}, 0\\right)$.\nRemark 3: It is noted that the controller (12) has some\nimplementation issues. For example, $\\lambda$ has to be computed\nin real-time, which may not work when the control board\nhas a low bandwidth. Computation of $\\lambda$ requires accurate\ninformation of the disturbance $u_f$ which is hard to obtain, if\npossible. Even if the upper bound of $u_f$ is known, the robust\nform of the HO-CBF will be subject to some conservativeness.\nIt is shown in Section V that small uncertainties of the\nsignal $u_f$ may lead to an inappropriate value of $\\lambda$, which can\npotentially cause further violations of the safety constraint."}, {"title": null, "content": "Considering the control policy (12), we replace $\\lambda$ with a\nconstant positive safeguarding gain $K_s$ to remove the de-\npendency on accurate system information and reduce com-\nputational requirements. This yields the proposed high-order\nsafeguarding controller\n$u^s = -K_s R^{-1} L_g \\Psi_{m-1}^T \\nabla B(x)$, (13)\nwhere $\\nabla B(x) = \\partial B(x)/\\partial \\Psi_{m-1}$, and $B(x)$ is an energy\nfunction satisfying\n$\\inf_{x \\in \\mathcal{C}_m} B(x) \\ge 0$, $B(0) = 0$, $\\lim_{x \\rightarrow \\mathcal{C}_m^S} B(x) = \\infty$, (14)\nwhere $\\mathcal{C}_m^S = \\{x \\in R^n | \\Psi_{m-1}(x) = 0\\}$. For example,\n$B(x) = (B(\\Psi_{m-1}(x)) - B(\\Psi_{m-1}(0)))^2$.\nSince $\\Psi_{m-1}(x) > 0$ if $B(x) < \\infty$, the forward invariance\nof $\\mathcal{C}_m$ can also be verified by $B(x)$. Compared with (12),\nthis controller does not need to compute $\\lambda$, and requires no\ninformation of the disturbance $u_f$.\nThe following result illustrates that (13) always satisfy the\nCBF conditions as x approaches the boundary of the safety set\nand renders $\\mathcal{C}$ forward invariant in the presence of unknown\ndisturbance/actuator fault.\nTheorem 1: Given any $x_0 \\in \\text{Int}(\\mathcal{C})$, let $\\mathcal{C}_i$ and $\\Psi_{i-1}(x)$,\n$i \\in \\{1, 2, \\ldots, m\\}$, be defined as (8) and (9) such that $x_0 \\in \\mathcal{C}$.\nUnder Assumptions 1-2, the high-order safeguarding controller\n(13) can guarantee the safety of both the nominal system (2)\nand the system (1) subject to unknown disturbance/actuator\nfault $u_f$, i.e. $x(t) \\in \\mathcal{C}, \\forall t > 0$ and $\\forall x_0 \\in \\mathcal{C}$.\nProof: Since h(x) has relative degree m > 1, one has\n$||L_g L_f^{i-1} h(x)|| = 0$, $\\forall i \\in \\{1, 2, ..., m - 1\\}$.\nHence, the actuator fault $u_f$ will not cause any construction\nerror in the sequence of sets $\\mathcal{C}_i$ for $i = \\{1, 2, \\cdots, m\\}$, which\nimplies that the HO-RCBF can still be used to guarantee the\nforward invariance of $\\mathcal{C}$. The time derivative of B(x) along\nthe state trajectories is\n$\\dot{B}(x) = \\nabla B(x)^T (L_f \\Psi_{m-1} + L_g \\Psi_{m-1}(u^s(x) + u_f))$\n$\\le ||\\nabla B(x)|| (||L_f \\Psi_{m-1}|| + ||L_g \\Psi_{m-1}|| ||u_f ||)$\n$- K_s \\nabla B(x)^T L_g \\Psi_{m-1} R^{-1} L_g \\Psi_{m-1}^T \\nabla B(x)$.\nSince $||u_f ||_\\infty \\le \\eta_1$, $||L_g \\Psi_{m-1}|| \\neq 0$ and\n$L_g \\Psi_{m-1} R^{-1} L_g \\Psi_{m-1}^T \\ge \\underline{\\sigma}(R)^{-1} ||L_g \\Psi_{m-1}||^2$,\nwe have\n$\\dot{B}(x) \\le ||\\nabla B(x)||^2 \\left( \\frac{L_\\Psi}{B(x)} - \\frac{K_s}{\\underline{\\sigma}(R)} ||L_g \\Psi_{m-1}||^2 \\right)$,\nwhere $L_\\Psi = ||L_f \\Psi_{m-1}|| + ||L_g \\Psi_{m-1}|| \\eta_1$.\nConsider a compact set $\\mathcal{B}_l = \\{x \\in R^n | ||x|| \\le r\\}$ for\narbitrary large $r > 0$ such that $\\mathcal{B}_r \\cap \\mathcal{C} \\neq \\emptyset$. Since $f(x)$ is\nlocally Lipschitz, $f(x)$ is norm-bounded by f for all $x \\in \\mathcal{B}_r$.\nFurthermore, $\\Psi_{m-1}$ is bounded in $\\mathcal{B}_r$ and therefore $\\nabla \\Psi_{m-1}$\nis norm-bounded. Denote the bound of $\\nabla \\Psi_{m-1}$ as $\\Phi$. By\n$||L_f \\Psi_{m-1}|| \\le ||\\nabla \\Psi_{m-1}|| ||f(x)|||$\n$||L_g \\Psi_{m-1}|| < ||\\nabla \\Psi_{m-1}|| ||g(x)|||$,\none has"}, {"title": null, "content": "$||L_f \\Psi_{m-1"}, "le \\Phi f$ and $||L_g \\Psi_{m-1}|| \\le \\Phi \\overline{g}$.\nSince $B(x) \\rightarrow \\infty$ as $\\Psi_{m-1}(x) \\rightarrow 0$, we have $||\\nabla B(x)|| \\rightarrow \\infty$\nas $\\Psi_{m-1}(x) \\rightarrow 0$. Then $\\dot{B}(x)$ becomes negative as $\\Psi_{m-1}(x)$\napproaches 0 according to (14), which illustrates x will never\nescape from the safety set $\\mathcal{C}_m$. From Lemma 1, $x \\in \\mathcal{C}_m$ for\nall $t \\ge 0$ implies that $\\mathcal{C}$ is forward invariant.\nWe show how the safeguarding policy guarantees safety of\nthe system (1) and its nominal system (2) under a nominal\ncontrol policy in the following corollary.\nCorollary 1: Let k(x, t) be a nominal control policy, which\nis locally Lipschitz in x on $\\mathcal{B}_r$ and piecewise continuous in t,\nand k(0, t) = 0. Under Assumptions 1-2, the control policy\n$u = k(x, t) + u^s(x)$\ncan render $\\text{Int}(\\mathcal{C})$ forward invariant for both nominal system\n(2) and the system (1) subject to disturbance/actuator fault.\nProof: By regarding $g(x)k(x,t)$ as a term of the drift\ndynamics, the proof follows from Theorem 1.\nFor an optimal control problem with high-relative-degree\nstate constraints, HO-ZCBF [20"], "4": "Interestingly, the safeguarding controller (13)\ncan be easily extended to address safety constraints with\ndifferent relative degrees. For example, let $\\mathcal{H}_l = \\{x \\in\nR^n | h_{l-1}(x) \\ge 0\\}$ for l = 1, 2, ..., n be the safety sets. Define\nfunctions $\\Psi_{l,j_l}$ ($j_l = 0, 1, \\ldots, n_l"}