{"title": "OPTIMIZING PREFERENCE ALIGNMENT WITH DIFFER-\nENTIABLE NDCG RANKING", "authors": ["Jiacong Zhou", "Xianyun Wang", "Jun Yu"], "abstract": "Aligning large language models with human preferences improves interaction\nquality and safety by ensuring outputs better reflect human values. A promising\nstrategy involves Reinforcement Learning from Human Feedback (RLHF), starting\nwith collecting and ranking responses generated by a supervised fine-tuning model\nto refine alignment. Current methods (DPO) focus on learning from pairwise\npreference data, categorizing responses into preferred and less preferred pairs, and\noptimizing by maximizing pairwise margins. Recent studies have uncovered a sub-\nstantial discrepancy between the theoretical aspirations of preference learning and\nits real-world results. Current preference alignment techniques underperform ex-\npectations, with ranking accuracies below 60% on standard datasets. This suggests\nexisting methods inadequately capture ideal preference relationships within se-\nquences. To address this challenge, this paper introduces Direct Ranking Preference\nOptimization (DRPO), a novel method that views human preference alignment\nas a Learning-to-Rank (LTR) task. DRPO leverages NDCG, a widely used LTR\nmetric, to optimize the ranking of responses within lists based on preference data,\nthereby enhancing ranking accuracies. Due to the nondifferentiability of NDCG,\nwe propose diffNDCG loss, a differentiable approximation facilitated by a sorting\nnetwork to simulate NDCG. Furthermore, to improve the quality of generated\nresponse, we propose a novel margin-based Adaptive Rank Policy Score. Exten-\nsive experiments have shown that DRPO outperforms existing baseline methods,\nenhancing the quality of the generated responses. The code is publicly available 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), trained on extensive and diverse datasets, can be prompted to\ndemonstrate impressive capabilities across a broad range of tasks (Huang et al., 2024; Chiang et al.,\n2023; OpenAI et al., 2024; Touvron et al., 2023). However, due to the varied nature of their training\ndata, these models sometimes produce content that may not align with human preferences, including\nfabricated answers, offensive comments, or harmful responses (Bai et al., 2022; Wang et al., 2023). \u03a4\u03bf\nensure the development of AI systems that are safe and controllable, this paper investigates learning\ntasks for LLMs that guide them to generate responses in alignment with human preferences.\nHuman preference alignment has become an active research area. Reinforcement Learning with\nHuman Feedback (RLHF) (Ouyang et al., 2022) is the first proposed method in this area. However,\nthe optimization process of RLHF is complex, and its implementation introduces challenges due to\nunstable and costly training. Recent studies (Hong et al., 2024; Ethayarajh et al., 2024) have started\nto adopt alternatives to RLHF. For example, Direct Preference Optimization (DPO) (Rafailov et al.,\n2023) enables the extraction of the corresponding optimal policy in a closed form and derives a\npairwise logistic loss directly from pairwise preference data. DPO eliminates the need for explicit\nreward modeling or reinforcement learning, thereby reducing the training costs associated with RLHF.\nAlthough significant progress has been made in human preference alignment, most existing methods\nprimarily focus on pairwise human preferences, which involve evaluating human preferences by"}, {"title": "2 PRELIMINARIES", "content": "Prompt, Response and Policy. Let X and Y denote the set of prompts and the set of responses\n(action space), respectively. We use x \u2208 X to represent a prompt, and y \u2208 Y to represent a response.\nGiven a prompt x, a large language model (LLM) generates a corresponding response y. This\nresponse y is produced according to a policy \\pi_\\theta(\u00b7|x), which is a discrete distribution over Y. We also\ndefine \\pi_{ref}(x) as a discrete distribution over Y, serving as the reference policy. The reference policy\n\\pi_{ref} is derived from the Supervised Fine-tuning (SFT) model (Rafailov et al., 2023).\nRanking Preference Data. The training dataset D = {x^i, y^i, s^i}_{i=1}^N is composed of three elements:\nx^i represents the i-th prompt; y^i = (y_1^i, ..., y_K^i) consists of a list of K responses, typically generated\nby the SFT model; and s^i = (s_1^i, . . ., s_K^i) \u2208 [0,1]^K denotes the relevance scores of the responses y^i\nin relation to the prompt x^i. The relevance score s, generally obtained from AI (Huang et al., 2024;\nJiang et al., 2023; Bai et al., 2023a; Chiang et al., 2023) and human feedback or a reward model,\nreflects how well the response y corresponds to the prompt x^i. If the response y_j^i is scored higher\nthan y_k^i, it implies that y_j^i is more closely aligned with human preferences compared to y_k^i.\nAlignment with Human Preferences Using Ranking Preference Data. Aligning LLM with human\npreferences involves utilizing the dataset of human preferences (e.g., the training dataset D) to refine\nthe policy \\pi_\\theta(y|x). Substantial progress has been made toward achieving this goal, with most existing\nstudies (Rafailov et al., 2023; Hong et al., 2024) leveraging pairwise preference data, represented\nas D_p = {x^i, (y_1^i, y_2^i), (s_1^i, s_2^i)}_{i=1}^N (i.e., the case K = 2 for training dataset D). Unlike existing\nmethods, we treat preference alignment as a listwise ranking problem (i.e., K > 2), which allows for\na more effective exploitation of the complex preference relationships embedded within sequence.\nLearning-to-Rank Task. Given the ranking preference data, the human preference alignment can be\nregarded as the Learning-to-Rank (LTR) task (Liu et al., 2009; Yu et al., 2019; Cao et al., 2007). When\na user enters a query x_q, the LTR algorithm needs to reliably rank multiple candidate documents (e.g.,\ntexts, images and web pages) to ensure that the most relevant information is retrieved first so that the\nuser can quickly find the information they need. Let M be the LTR model. Given a query x_q and\ndocuments y_d, M predicts relevance scores s, assigning higher scores to superior documents.\nLearning Framework. Let M be the score prediction model of human preference alignment. Given\nthe prompt x and responses y, one can predict the relevance scores:\n\\hat{s}_\\theta = M(x, y; \\pi_\\theta),"}, {"title": "3 PROPOSED METHODOLOGY", "content": "In this section, we introduce the proposed method DRPO, which comprises three primary components:\n(1) ranking score computation; (2) differentiable responses ranking; (3) diffNDCG loss. The graphical\nillustration of our proposed method is depicted in Figure 1.\n3.1 RANKING SCORE COMPUTATION\nPolicy Reference Ratio. The fundamental criterion for computing the ranking score is that more\npreferred responses should receive higher scores. A commonly used strategy to compute the ranking\nscores is Policy Reference Ratio proposed by Rafailov et al. (2023): let M_{prr} be the policy reference\nratio model of human preference alignment, which can be expressed as:\nM_{prr}(x,y; \\pi_\\theta) = \\Big(\\beta \\log \\frac{\\pi_\\theta(y_1 | x)}{\\pi_{ref}(y_1 | x)}, ..., \\beta \\log \\frac{\\pi_\\theta(y_K | x)}{\\pi_{ref}(y_K | x)}\\Big),\nwhere \\beta is the hyper-parameter to control the KL divergence between \\pi_\\theta and \\pi_{ref}.\nAdaptive Rank Policy Score. While the Policy Reference Ratio defined in Eq. 2 has been widely\nadopted in various methods (Liu et al., 2024), it emphasizes the relative likelihood between the policy\nmodel \\pi_\\theta and a reference model \\pi_{ref} rather than directly maximizing the absolute likelihood of\nthe preferred response. Consequently, a high Policy Reference Ratio score may coincide with low\nabsolute likelihood for preferred responses (Meng et al., 2024), leading to sub-optimal performance in\nreal-world generation tasks, where high absolute likelihoods are essential for producing high quality\noutputs (Holtzman et al., 2018; Fan et al., 2018). To address this, we focus on the log-likelihood of\ngenerated sequences and establish a length normalized basic scores function based on log-likelihood:\ns(x, y; \\pi_\\theta) = \\Big(\\frac{1}{|y_1|} \\log \\pi_\\theta(y_1 | x), ..., \\frac{1}{|y_K|} \\log \\pi_\\theta(y_K | x)\\Big),\nHere, |y| denotes the token length of y. This length normalization reduces bias towards shorter\nsequences (Yuan et al., 2023). Furthermore, when performing the Differentiable Swapping Operation\n(see Section 3.2), we calculate score differences between elements in the responses list. In these\ncalculations, a common practice to enhance discrimination between high and low-quality responses\nis to incorporate a margin (Meng et al., 2024; Ethayarajh et al., 2024), ensuring preferred responses\nexceed dispreferred ones by at least a specified threshold. This margin-based methodes has been\nempirically demonstrated to enhance model generalization and improve the quality of generated\nresponses (Touvron et al., 2023). To this end, we introduce a additional ranking-aware term \\gamma(y):\ns(x, y; \\pi_\\theta) = \\Big(\\frac{1}{|y_1|} \\log \\pi_\\theta(y_1 | x) + \\gamma(y_1), ..., \\frac{1}{|y_K|} \\log \\pi_\\theta(y_K | x) + \\gamma(y_K)\\Big).\nThe ranking-aware margin is then defined as the difference between \\gamma(y_i) and \\gamma(y_j) when comparing\nscores of two responses y_i and y_j. This margin should effectively reflect quality differences among\nresponses across the ranked list. Specifically, we assume adjacent responses have similar relevance,\nand design ranking-aware margin to satisfy three criteria: first, apply smaller margins for adjacent\nranks, allowing fine-grained discrimination; second, assign larger margins for greater ranking dispari-\nties, emphasizing significant differences; finally, dynamically adjust margins based on relative score\nchanges, maintaining discrimination across the quality levels while avoiding overemphasis on minor\ndifferences. Therefore, we define \\gamma(y) by combining a base weighted ranking position term with an\nexponential moving average estimate (Qin et al., 2010b) of past scores related to the ranking position:\n\\gamma(y) = \\tau \\cdot q(y) - \\beta \\cdot V_q(y), where V_q(y) = \\theta \\cdot V_q(y) + (1 - \\theta) \\cdot \\frac{1}{|y|} \\log \\pi_\\theta(y | x)."}, {"title": "3.2 DIFFERENTIABLE RESPONSES RANKING", "content": "One of the most intuitive strategies to learn human preferences\nfrom the response list y, is to sort the responses by predicted scores\nand use this ranking to fine-tune the language model, thereby learn-\ning the optimal preference ordering. However, traditional sorting\nmethods like Selection Sort (Musser, 1997) and Quick Sort (Hoare,\n1962) are inherently discrete and discontinuous, impeding differ-\nentiable optimization in LLM fine-tuning for preference learning\n(Petersen et al., 2021). In this section, we employ a differentiable\nsorting network\u00b2 to rank responses based on scores \\hat{s}_\\theta, enabling\nend-to-end fine-tuning of LLM on ranking preferences.\nDifferentiable sorting networks offer superior parallel efficiency\nand excellent sorting performance while maintaining differentia-\nbility (Petersen et al., 2021). For a list of length L, the time\ncomplexity ranges from O(L^2) to O(L \\log^2 L), depending on the\nspecific network variant (Akl, 1990) (such as Odd-Even or Bitonic\nnetworks) (Akl, 1990). These complexities are competitive with\nor surpass many differentiable sorting methods, including those\nproposed in (Song et al., 2024; Grover et al., 2019; Liu et al., 2024;\nXia et al., 2008; Blondel et al., 2020; Swezey et al., 2021). A\ncomprehensive time complexity comparison is provided in Table 5. Furthermore, sorting networks\nproduce doubly stochastic permutation matrices, crucial for accurate NDCG computation by repre-\nsenting ranking probabilities faithfully. In contrast, existing differentiable sorting methods (Grover\net al., 2019; Swezey et al., 2021) often produce unimodal permutation matrices, leading to overestima-\ntion in response gain calculations and severely distorting diffNDCG (see Eq. 13) measurements. The\nexperimental results presented in Table 1 demonstrate that sorting networks significantly outperform\nexisting differentiable sorting methods across various performance metrics.\nOdd-even Sorting Network. In this work, we adopt the odd-even sorting network (Batcher, 1968)\nfor response ranking due to its simplicity and ease of implementation. As depicted in Figure 2, an\nordered sequence of K elements can be achieved through a K-layer sorting network. Each layer of\nthe odd-even sorting network operates by comparing and swapping neighboring elements at either\nodd or even indices, thereby organizing them in a desired order. The process of odd-even sort is\ndivided into alternating odd and even stages. During the odd stage, all pairs of elements at odd indices\n(i.e., elements at positions 1 and 2, 3 and 4, 5 and 6, etc.) are compared, and swapped if necessary\naccording to the desired order. In the even stage, all pairs of elements at even indices (i.e., elements\nat positions 2 and 3, 4 and 5, 6 and 7, etc.) are compared, and swapped according to given order.\nGiven the predicted scores \\hat{s}_\\theta = M_{arp}(x, y; \\pi_\\theta) = (\\hat{s}_1,..., \\hat{s}_K) (as defined in Eq. 6), we employ\na K-layer odd-even sorting network to sort these scores in descending order. This sorting network\noperates through a systematic alternation between odd and even indexed elements. Specifically, we"}, {"title": "3.3 DIFFERENTIABLE NORMALIZED DISCOUNTED CUMULATIVE GAIN LOSS", "content": "To align human preferences, a direct strategy involves optimizing the cross-entropy loss between the\nground truth permutation matrix P_{ground} (obtained by ground truth scores s) and the predicted soft\npermutation matrix P_{soft}: let l_{ce} be the cross-entropy loss and [P]_j be the j-th column of matrix P,\nL_{ce}=\\frac{1}{K} \\sum_{j=1}^K l_{ce}([P_{soft}]_j, [P_{ground}]_j).\nHowever, experiments in Table 3 reveals this strategy's suboptimality. One possible explanation is\nthat it fails to distinguish error severity across ranking positions, incorrectly equating misplacements\nof top-ranked responses with lower-ranked items, despite higher-ranked responses typically being far\nmore crucial. To address these challenges, we propose optimizing Normalized Discounted Cumulative\nGain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002b), an effective LTR metric for measuring ranking\nquality. NDCG assesses the significance of responses in conjunction with their ranking positions. It\nassigns greater importance to responses at the top of the ranking compared to those positioned lower,\nand imposes a more severe penalty for inaccurately placing a top-ranked response in a lower position.\nFurthermore, while NDCG was originally designed to reflect users' tendency to focus on top-ranked\nresults (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002a), this characteristic aligns with human preference rankings,\nwhich prioritize more preferred responses over less preferred ones (Pool et al., 2016). This similarity\nmakes NDCG an effective proxy for evaluating and learning human preferences. Experiments in\nFigure 4 demonstrate that has a stronger correlation with human preference win rates compared to the\noptimizing targets of existing methods, highlighting its effectiveness. Additionally, NDCG prioritizes\ntop-ranked responses and penalizes their misplacement, and capturing graded importance between"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nDatasets. Anthropic's Helpful and Harmless (HH) (Bai et al., 2022) contains 161k/8.5k train-\ning/test samples. Each sample consists of a prompt and a pair of responses (chosen and reject), where\n\"chosen\" represents the preferred response and \"reject\" represents the less preferred response. We\nalso generate additional responses for each prompt and rate each response using a reward model"}, {"title": "4.2 EXPERIMENTAL RESULTS ON ANTHROPIC'S HELPFUL AND HARMLESS DATASET", "content": "Main Results. Experiments on HH dataset are conducted in Table 1 showing the effectiveness\nof our method. (1) Our method DRPO outperforms baselines SFT, DPOBT, DPOPL, PRO, LIPO\nand other LTR methods across different model scales. The GPT-4 Win Rate has an improve-\nment of 5.22%~6.13%, and Reward Model Win Rate has an improvement of 4.69%~8.98% and\n0.78%~2.73%. (2) Even without additional modifications, our Sorting Network outperforms most\nconventional sorting methods, such as Neural Sort. (3) Directly extending the existing alignment"}, {"title": "5 CONCLUSION", "content": "Aligning LLMs with human preferences is crucial for enhancing human-machine interactions and\nensuring the safety of LLMs. However, most existing methods focus only on alignment with pairwise\npreference data, categorizing responses into preferred and less preferred pairs. Our research provides\ntimely insights on how to align LLMs with multiple responses. We propose a novel method DRPO,"}, {"title": "NDCG(\u015de, s)", "content": "= \\frac{1}{iDCG} \\sum_{j=1}^K \\frac{2^{s_j}-1}{\\log_2(1 + q(y_j))}, where iDCG = \\sum_{j=1}^K \\frac{2^{s_j}-1}{\\log_2(1 + q^*(y_j))}"}, {"title": "M_{arp}(x, y; \u03c0\u03b8) = (s(x, y_1; \u03c0\u03b8), s(x, y_2; \u03c0\u03b8), ..., s(x, y_K; \u03c0\u03b8)),", "content": ""}, {"title": "\u015d'j", "content": "= max(\u015dj, \u015cj+1), \u015d'j+1 = min(\u015dj, \u015cj+1), \u2200j \u2208 {1, ..., K \u2212 1}."}, {"title": "minsoft(\u015dj, \u015cj+1) = \u015dj \u00b7 h(\u015dj+1 - \u015dj) + \u015cj+1 \u00b7 (1 \u2212 h(\u015dj+1 \u2212 \u015dj)), maxsoft(\u015dj, \u015cj+1) = \u015dj \u00b7 (1 \u2212 h(\u015dj+1 \u2013 \u015dj)) + \u015cj+1\u00b7h(\u015dj+1 \u2013 \u015cj),", "content": ""}, {"title": "h(x)", "content": "= \\begin{cases} 1 & \\text{if } ax < -0.25, \\\\ \\frac{1}{16\\alpha}x + 0.5 & \\text{if } |\\alpha x| < 0.25, \\\\ 0 & \\text{otherwise}. \\end{cases}"}, {"title": "Pk", "content": "= diag(P1, P3,...) or diag(1, p2, P4, ...),"}, {"title": "Lce=\\frac{1}{K} \\sum_{j=1}^K l_{ce}([P_{soft}]_j, [P_{ground}]_j).", "content": ""}, {"title": "\u03b4 = q(yj) and \u03c8(\u03b4, s, \u015d\u03b8) = Sj,", "content": ""}, {"title": "NDCG(\u015de, s) =", "content": "\\frac{1}{iDCG} \\sum_{j=1}^K \\frac{2^{\u03c8(\u03b4,s,\u015do)} - 1}{\\log_2(1 + d)} = \\frac{1}{iDCG} \\sum_{d=1}^K \\frac{2^{\u03c8(\u03b4,s,\u015de)} - 1}{\\log_2(1 + d)}."}, {"title": "\u03c8'(\u03b4, s, \u015de) = [P_{soft}^T .S]d.", "content": ""}, {"title": "diffNDCG(\u015de, s) =", "content": "\\frac{1}{iDCG} \\sum_{d=1}^K \\frac{2^{\u03c8'(\u03b4,s,\u015de)} - 1}{\\log_2(1 + d)}."}, {"title": "min_{\u03b8} L_{diffNDCG} =", "content": "\\frac{1}{|D|} \\sum_{(x,y,s) \u2208 D} (diffNDCG (M_{arp}(x, y, \u03c0\u03b8), s),"}, {"title": "S", "content": "= \\frac{\\sum_{i=1}^K e^{R(x^i,y^j)}}{\\sum_{i=1}^K e^{R(x^i,y^i)} + e^{R(x^i,y^i)}} where R is a reward model."}]}