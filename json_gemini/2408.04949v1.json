{"title": "CROCODILE : Causality aids Robustness via COntrastive Disentangled LEarning", "authors": ["Gianluca Carloni", "Sotirios A. Tsaftaris", "Sara Colantonio"], "abstract": "Due to domain shift, deep learning image classifiers perform poorly when applied to a domain different from the training one. For instance, a classifier trained on chest X-ray (CXR) images from one hospital may not generalize to images from another hospital due to variations in scanner settings or patient characteristics. In this paper, we introduce our CROCODILE framework, showing how tools from causality can foster a model's robustness to domain shift via feature disentanglement, contrastive learning losses, and the injection of prior knowledge. This way, the model relies less on spurious correlations, learns the mechanism bringing from images to prediction better, and outperforms baselines on out-of-distribution (OOD) data. We apply our method to multi-label lung disease classification from CXRs, utilizing over 750000 images from four datasets. Our bias-mitigation method improves domain generalization and fairness, broadening the applicability and reliability of deep learning models for a safer medical image analysis. Find our code at: https://github.com/gianlucarloni/crocodile.", "sections": [{"title": "1 Introduction", "content": "Domain shift bias is the problem of machine learning (ML) models performing not consistently across in-distribution (ID) and out-of-distribution (OOD) data. The former are independent and identically distributed (i.i.d) to the data on which the model was trained. Conversely, data are OOD when their distribution essentially differs from the source one, such as chest X-rays (CXR) coming from a different hospital than the training one [18,7,28]. Traditional ML models still tend to rely on spurious correlations seen during training for predicting the outcome and spectacularly fail when those shortcut associations are not present in OOD data, for instance, due to variations in scanner settings, image artifacts, or patient demographics [6,20,1,8]. For this reason, the field of domain generalization (DG) has searched for ways to make deep learning (DL) models learn robust features that could generalize better to unseen domains [11,13,25,29]."}, {"title": "2 Methodology", "content": "We define a structural causal model (SCM) [15] for medical image classification in Fig 1. Given the input images I, such as CXRs, and the disease classification y, we obtain two sets of features via feature extraction. We denote $F_{ca}$ the causal features that truly determine the outcome (e.g., the patchy airspace opacification typical in pneumonia). Similarly, we denote $F_{sp}$ the spurious features, determined by data bias's confounding effect, which are unrelated to a disease (e.g., metal tokens on the image corners). Ideally, Y should be caused only by $F_{ca}$, but is naturally confounded by $F_{sp}$, as both types of features usually coexist in medical data. Unfortunately, conventional models tend to learn the correlation $P(Y|F_{ca})$ via the shortcut (backdoor) path $F_{ca} \\leftarrow I \\rightarrow F_{sp} \\rightarrow Y$ instead of the desired $F_{ca}\\rightarrow Y$. As we detail next, we exploit the do-calculus from causal theory [16] on the causal features to block the backdoor path, estimating $P(Y|do(F_{ca}))$. Following the same idea, we conceive two other sets of"}, {"title": "2.1 Disease-branch and Domain-branch", "content": "We present our overall framework in Fig 2. A disease prediction branch learns to extract useful image features to predict the medical finding (e.g., pneumothorax or atelectasis in a CXR), regardless of the different domains. On another parallel branch for domain prediction, the image features that are useful for the trivial task of predicting the domain the images come from are learned (regardless of the different diseases). The architecture is trained end-to-end. Each branch involves a feature extraction backbone followed by a block to enhance features via channel- and spatial- attention [14]. Then, a Transformer network [24] with a modified cross-attention mechanism yields not only the usual set A of"}, {"title": "2.2 Feature Disentanglement and Causal Intervention", "content": "For each branch, we need to make $Q^{ca}$ and $Q^{sp}$ capture the authentic and trivial aspects from the input samples. To achieve the correctness of the predictions, we impose two cross-entropy (CE) loss terms, $L_{CE,y}$ and $L_{CE,d}$, over the classification logits $z_y$ and $z_d$ from the causal features $Q^{ca}$ and $Q^{ca}_d$, supervised by the disease labels y and domain labels d, respectively. To make $Q^{sp}$ features encode the trivial patterns that are unnecessary for classification, we push its predictions $z_y$ and $z_d$ evenly to all respective categories. We define the uniform classification losses $L_{KL,y}$ and $L_{KL,d}$ as the KL-divergence between the spurious features and the respective uniform distribution ($y_u$ or $d_u$). To alleviate the confounding effect, we implement the backdoor adjustment by performing a latent causal intervention [21,12]: we stratify the spurious features appearing from training data and pair the causal set of features with those stratified spurious features to compose the intervened graph. This way, we fit the concept of borrowing from others (i.e., \"if everyone has it, it is as if no one has it\"). We impose CE losses $L_{CE,y}^{bd}$ and $L_{CE,d}^{bd}$ between the logits $z_y'$ and $z_d'$ obtained from the corresponding intervened features $Q^{bd}$ and the same ground-truth label for the causal features. This way, we push the predictions of such intervened images to be invariant and stable across different stratifications due to shared causal features. Practically, we approximate this operation with an intra-batch shuffling of $Q^{sp}$ followed by random sampling (with 0.3 drop probability) and addition to $Q^{ca}$. By combining the supervised CE loss, the KL loss, and the backdoor CE loss for each branch, we obtain the two following equations:\n$L_y = -(\\lambda_1 y \\log(z_y) + \\lambda_2 KL(y_u, z_y) + \\lambda_3 y \\log(z_y')) \\qquad (1)$\n$L_d = -(\\lambda_4 d \\log(z_d) + \\lambda_5 KL(d_u, z_d) + \\lambda_6 d \\log(z_d')) \\qquad (2)$"}, {"title": "2.3 Contrastive Learning", "content": "To attain cross-domain robustness, we posit there should also exist an alignment between the causal features that determine the disease and the spurious features for the domain prediction task. And the converse should also be true. For instance, we want the regions of the image that determine the presence of pneumonia to be unrelated to what contributes to discerning different domains (e.g., spurious metal tokens). Conversely, the image aspects determining which domain"}, {"title": "2.4 Injecting Prior Knowledge", "content": "Motivated by the high interclass similarity and hierarchical structure of CXR findings [19,27], we propose a new method to inject prior (medical) knowledge into the model to guide its learning (Fig. 4). Differently from solutions as conditional training [17], which rely on data, our proposal is desirable to capture semantic priors without relying on data. We define a causal graph representing the relationship between the CXR findings and propose a novel formulation of the causality map concept [5,4] to model the co-occurrence of CXR findings in the images. As we have seen, each $Q^{ca}$ representation has shape $n_c \\times h$, where $n_c$ is the number of classes (e.g., nine CXR findings) and h is the hidden dimension of the embeddings. After normalizing $Q^{ca}$ by their global maximum batch-wise, they lie in the range 0-1, and we interpret their values as probabilities of the CXR findings to be present in the image. Indeed, given two embeddings $Q^i$ and $Q^j$, to compute the effect of the former on the presence of the latter, we estimate the ratio between their joint and marginal probabilities as:\n$P(Q^i|Q^j) = \\frac{P(Q^j, Q^i)}{P(Q^i)} = \\frac{\\checkmark_{i,j} (\\max_h Q^i_h) (\\max_h Q^j_h)}{\\Sigma_h Q^i_h} , \\forall i, j \\in 1 \\le i, j \\le n_c \\qquad (6)$\nthus obtaining the relationships between embeddings $Q^i$ and $Q^j$, since, in general, $P(Q^i|Q^j) \\neq P(Q^j|Q^i)$. By computing these quantities for every pair i, j, we obtain the $n_c \\times n_c$ map $C_y$. We interpret asymmetries across estimates opposite"}, {"title": "3 Experimental Setup", "content": "We classify eight radiological findings (plus the No finding class) from frontal CXR images of four popular data sets in both ID and OOD settings. After cleaning, the number of images for each set is: 112110 for ChestX-ray14 [27], 183453 for CheXpert [9], 95452 for PadChest [2], and 365737 for MIMIC-CXR [10]. For the first dataset, we create the Lung opacity class as OR logic across the consolidation, effusion, edema, pneumonia, and atelectasis classes. We resize the images to 320 \u00d7 320 and adjust their contrast in 0-255. For ID experiments, we combine images of ChestX-ray14, CheXpert, and PadChest, split them into 80-20% train and validation sets, and assess the multi-label classification performance via the area under the ROC curve (AUC) and the average precision (AP) scores for each category and their average. We test the best-performing ID model on the external, never-before-seen MIMIC-CXR dataset to evaluate OOD generalization abilities. In all the experiments, we adopted ResNet50 backbones, Adam optimizer, learning rate of 1e-6, batch size of 12, and trained the model in"}, {"title": "4 Results and Conclusion", "content": "The results of our ID and OOD investigations (Table 1) reveal our method is behind its ablated versions and [12] on i.i.d. data (ID) while is the best-performing model on the external never-before-seen data (OOD). This important result points to a necessary trade-off between in-domain accuracy and out-of-domain robustness on real-world data, supporting recent work [23]. Notably, our method is the most effective in reducing the ID-to-OOD drop in performance. By leveraging causal tools, disentanglement, contrastive learning, and prior knowledge, it learns a better mechanism from image to prediction, relies less on spurious correlations, and breaks the boundaries across domains. Our bias-mitigation proposal is general and can be applied to tackle domain shift bias in other computer-aided diagnosis applications, fostering a safer and more generalizable medical AI."}]}