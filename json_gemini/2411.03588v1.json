{"title": "An Experimental Study on Decomposition-Based Deep Ensemble Learning for Traffic Flow Forecasting", "authors": ["Qiyuan Zhu", "A. K. Qin", "Hussein Dia", "Adriana-Simona Mihaita", "Hanna Grzybowska"], "abstract": "Traffic flow forecasting is a crucial task in intelligent transport systems. Deep learning offers an effective solution, capturing complex patterns in time-series traffic flow data to enable the accurate prediction. However, deep learning models are prone to overfitting the intricate details of flow data, leading to poor generalisation. Recent studies suggest that decomposition-based deep ensemble learning methods may address this issue by breaking down a time series into multiple simpler signals, upon which deep learning models are built and ensembled to generate the final prediction. However, few studies have compared the performance of decomposition-based ensemble methods with non-decomposition-based ones which directly utilise raw time-series data. This work compares several decomposition-based and non-decomposition-based deep ensemble learning methods. Experimental results on three traffic datasets demonstrate the superiority of decomposition-based ensemble methods, while also revealing their sensitivity to aggregation strategies and forecasting horizons.", "sections": [{"title": "1 Introduction", "content": "Traffic flow forecasting is one of the most important tasks for intelligent transport systems (ITS) in daily traffic management and operations [23]. Several operations, such as incident management, require reliable flow forecasting for a short horizon in future to support decision-making. However, accurate forecasting remains challenging due to the complex patterns in time-series traffic data. Factors like road congestion, vehicle breakdowns and traffic signal timing [17,15] contribute to irregular and unpredictable patterns in traffic data, making accurate forecasting difficult to achieve."}, {"title": "2 Background and related works", "content": "This section briefly reviews the existing decomposition-based ensemble methods and time-domain multi-resolution ensemble approaches."}, {"title": "2.1 Decomposition-based ensemble methods", "content": "Most research on decomposition-based ensemble methods follows a divide-and-conquer concept, which transforms complex original time-series data into a set of simple components [20]. The first category, including Fourier Transform (FT) [4] and Wavelet Transform (WT) [6], uses use predefined basis functions. However, predefined basis functions suffer from frequency resolution issues, which means they lack or have a limited ability (due to predefined function) to distinguish between closely spaced frequency components in a signal. To address this gap, the second category avoids predefined functions, extracting signals based on local time scales and, as such, distinguishing closely spaced frequency components into Intrinsic Mode Functions (IMFs). Notable models in this category include Empirical Mode Decomposition (EMD), Ensemble Empirical Mode Decomposition (EEMD), and Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) [20]. Previous studies [24,12] show that these methods outperform direct raw data predictions. Thus, we selected EMD, EEMD, and CEEMDAN for this study."}, {"title": "2.2 Time domain multi-resolution ensemble", "content": "The time interval (i.e., temporal resolution) of recorded time-series data is connected to forecasting performance, making it a beneficial approach for ensemble learning [3,27]. The challenge of overfitting in deep learning methods due to complex time-series data can be mitigated by considering outcomes of other deep learning models trained on aggregated time-series data that focus on different trends. Previous research suggests that such a multi-resolution ensemble may outperform the single model in terms of performance and robustness [3,11].\nFrequency resolution refers to the ability to distinguish between closely spaced frequency components in a signal."}, {"title": "3 Comparative design", "content": "In this work, we attempt to compare and evaluate the deep ensemble methods on traffic flow forecasting tasks. The proposed comparison design is illustrated in Fig. 1."}, {"title": "3.1 Data generation module", "content": "Firstly, the data generation module extracts the subsets of time patterns from the original time-series for each selected ensemble method. The generated subsets are adjusted according to the specific ensemble learning method.\nFor multi-resolution ensemble and bagging The data generation for time domain multi-resolution ensemble is an addition to simple data aggregation, representing views based on aggregation levels [1]. Firstly, the original time-series will be sliced into a set of input-output pairs, denoted by set $X = \\{X_1,X_2, ..., X_i\\}$ and set $Y = \\{Y_1, Y_2, ..., Y_i\\}$. Each $x_i$ contains $I$ element and each $y_i$ is the sum of $T$ elements following $I$. $X^M = \\{X_1, X_2, ...., X_m\\}$ is obtained by sum aggregation on $X$ based on each time interval. Assuming the original time-series is recorded in 1 min time interval, input $I$ is 60 minutes, the timer-series input size for $x_1$ in $X^1$ and $x_10$ in $X^{10}$ are 60/1 = 60 and 60/10 = 6, respectively. For conventional bagging, the $X^M$ and $Y^M$ are generated by taking 90% elements from $X$ and $Y$ with replacement in their original form, and the number of generated series $M$ is equal to 25 as suggested by [5], representing subsets of the original data.\nFor decomposition-based methods Since decomposition will alter the feature characteristics of the original time-series data, the input and output are combined into a single sequence $s$ and are sliced after decomposition. The first step is to prepare the raw set $S = \\{s_1, s_2, ..., s_i\\}$ and each sequence $s_i$ contains $I + T$ elements. $S = \\{s_1, s_2, ..., s_i \\}$ will be decomposed to $M$ sets $S^M = \\{S^1, S^2, ...., S^m\\}$ with each $S^m = \\{s_1^m, s_2^m,..., s_i^m\\}$.\nAfter decomposition, we need to extract the input and output from $S^M$. Assuming $I$ and $T$ are 60 minutes and 10 minutes, arbitrary $s^m$ in $S^m$ can be sliced into $x_i^m = \\{...\\}$ and $y_i^m = \\{...\\}$, and the task is to forecast $y_i^m$ using corresponding $x_i^m$. The final prediction $y_i$ will be aggregated from predicted $\\{\\hat{y_1^m}, \\hat{y_2^m},..., \\hat{y_m^m}\\}$ and compared with grand truth $y_i$. The details of each method are explained below.\nEMD method was developed to adaptively decompose complex signals into multiple simpler time-series called Intrinsic Mode Functions (IMFs), providing a better ability to distinguish frequency components compared with the Fourier Transform. We use $s(t)$ for the value in time $t$ in sequence $s$ for simplification. Its process can be described as following steps:"}, {"title": "3.2 Modelling module", "content": "Long short-term memory (LSTM) is a recurrent neural network type that stores information across the time stamp to model the long-term dependencies [8] effectively. We apply LSTM as the base learner in the modelling module for its good time-series learning capacity. Each base learner will be trained in sequential order under the corresponding input-output pair until the maximum training iteration is reached or the early stopping rule on the validation set is violated. A hyperparameter tuning process is responsible for optimising the base learner. To maintain a fair comparison, the input and forecasting horizons are the same for all ensemble learning methods regardless of the data generation process."}, {"title": "3.3 Aggregation module", "content": "In the aggregation module, the outputs of base learners will be extracted and utilised using an optimised final learner or baseline aggregation strategy. Mean aggregation is the baseline strategy for time domain multi-resolution ensemble and conventional bagging. Due to the nature of decomposition, sum aggregation is the baseline strategy for decomposition-based methods. We further apply linear regression and MLP as the optimised final learners to utilise the predictions generated from base learners."}, {"title": "4 Experimental study", "content": "To evaluate the performance of decomposition-based deep ensemble learning methods, we conducted a series of experiments on Melbourne traffic datasets and PEMS datasets to investigate:\n1. RQ1: Whether decomposition-based methods better benefit the deep learning models than other ensemble methods on forecasting tasks?\n2. RQ2: Is the performance of decomposition-based ensemble learning sensitive to aggregation methods?\n3. RQ3: Whether the answer of RQ1 and RQ2 depends on input and forecasting horizon?"}, {"title": "4.1 Data", "content": "In this study, we conducted the experiments on 3 datasets: the Melbourne data set, the PEMSD4 data set and the Portland data set. The Melbourne data set are corridors stretching from Melbourne CBD to the eastern residential suburbs; see Fig. 2. The traffic flow, speed and occupancy are recorded in 1-minute intervals"}, {"title": "4.2 The hyper-parameter setting", "content": "To generate IMFs for decomposition-based methods, we use the implementation from the pyEMD package. The Spline method is set to Akima for complex signal processing for all decomposition-based methods. EMD adaptively decompose the signal without preset parameters. For EEMD and CEEMDAN, all noise coefficients $\\epsilon$ are set to 0.2, and the number of trials is set to 25. We use the top 5 IMF components in modelling.\nThe proposed base learners' model structure is shown in Table 1. LSTM first completes the time-series feature extraction, and the returned sequence is then passed to fully connected layers. The proposed model is implemented using tensorflow-gpu (version 2.6.0), and the hyper-parameter tuning is conducted by keras-tunner (version 1.1.3)."}, {"title": "4.3 Evaluation matrix", "content": "This research uses root mean squared error (RMSE) for performance evaluation. RMSE is used to measure the fitness between the predictions and the real observation of traffic flows:\n$RMSE = \\sqrt{\\frac{1}{n} * \\sum_{i=1}^{n}(tfp_i - y_i)^2}$\nWhere $y_i$ is the actual value of traffic flows, $tfp_i$ represents the forecasting value corresponding to each $y_i$, and $n$ denotes the number of observed values."}, {"title": "4.4 Overall comparison", "content": "To assess the effectiveness of decomposition-based ensemble methods, we compared EMD, EEMD, and CEEMAND against conventional bagging and multi-resolution ensemble, all trained on time-series recorded in 1-minute intervals over 10 repeated runs. A standalone LSTM model, trained on the aggregated time-series data with an equal time interval and forecasting horizon, was also compared with the aforementioned methods as an additional baseline, as suggested by [1]. The input horizon consists of 120, 240, and 360 minutes, with a forecasting horizon of 10 minutes for this test. The results for the Melbourne dataset are reported in Table 2, while the other results are reported in the supplementary material 1. Based on RMSE measurements, EEMD is the best ensemble in the Melbourne dataset, outperforming other methods in 10 out of 15 test scenarios. In the case of the PEMS and Portland datasets, EEMD outperformed other methods in 13 and 14 test scenarios, respectively. Overall, EEMD methods"}, {"title": "4.5 The ablation study of aggregation strategy", "content": "We compare linear regression and MLP methods against baseline aggregation across the ensemble methods to assess the sensitivity of decomposition-based methods to aggregation strategy. The test setup is the same as the overall comparison. The results for the Melbourne dataset are reported in Table 3, and the other results are reported in the supplementary material. Based on RMSE measurement, linear is the best ensemble in the Melbourne dataset. In the case of the PEMS and Portland datasets, the best strategy is also linear.\nFurthermore, linear regression and MLP outperform the baseline aggregation strategy in 127 and 94 cases, respectively. This might be because our progress didn't add residue to modelling, and some uncovered IMFs remain out of the top 5 IMFs, which are further utilised by linear regression and MLP methods."}, {"title": "4.6 Sensitivity of 20-min and 30-min forecasting horizons", "content": "We further evaluate the performance of all combinations of ensemble and aggregation methods for the forecasting tasks with 20-min and 30-min forecasting horizons. The rest of the test setup is the same as the overall comparison. The results are summarised in Fig 3a. We evaluated the results and noticed that 360-min input would improve the general performance of non-decomposition-based methods. This is consistent with the previous study [16] that longer inputs generally improve forecasting accuracy. However, decomposition-based are not sensitive to specific setups, and the CEEMDAN method tends to perform better when the output size is large. Fig. 3b. compares each ensemble's computation time (in minutes) paired with linear regression for different inputs. It can be observed that decomposition-based methods have higher computation time due to the IMF construction progress, whereas CEEMDAN is more computationally expensive because of the adaptive noise strategy."}, {"title": "5 Conclusions and Future Work", "content": "In this paper, we compare three decomposition-based deep ensemble learning methods with two common non-decomposition-based ones, including bagging and multi-resolution ensemble, for their performance in solving traffic flow forecasting tasks. Experimental results on several traffic flow datasets demonstrate the superiority of decomposition-based methods, with the EEMD-based method outperforming others in most test scenarios. Future work includes the exploration of advanced ensemble learning strategies based on multi-task optimisation [26] and the incorporation of other types of base learners [18]. We also plan to evaluate the decomposition-based methods in other application scenarios involving time-series forecasting missions [13]."}]}