{"title": "Cheems: Wonderful Matrices\nMore Efficient and More Effective Architecture", "authors": ["Jingze Shi", "Lu He", "Yuhan Wang", "Tianyu He", "Bingheng Wu", "Mingkun Hou"], "abstract": "Recent studies have shown that, relative position encoding performs well in selective state space model scanning algo-\nrithms, and the architecture that balances SSM and Attention enhances the efficiency and effectiveness of the algorithm,\nwhile the sparse activation of the mixture of experts reduces the training cost. I studied the effectiveness of using differ-\nent position encodings in structured state space dual algorithms, and the more effective SSD-Attn internal and external\nfunction mixing method, and designed a more efficient cross domain mixture of experts. I found that the same matrix is\nvery wonderful in different algorithms, which allows us to establish a new hybrid sparse architecture: Cheems. Compared\nwith other hybrid architectures, it is more efficient and more effective in language modeling tasks.", "sections": [{"title": "1 Introduction", "content": "Efficient algorithms are designed to compress information states, so that they can store as much useful information as\npossible in a limited state space, while effective algorithms are designed to store all information states, so that they can\navoid capturing biased information.\nTransformers (Attention is All You Need (Vaswani et al. 2017)) Architecture is popular in modern deep learning language\nmodeling, which can directly capture the relationship between any two elements in the sequence and effectively deal\nwith long-distance dependency problems. However, the architecture has two main drawbacks. First, when dealing with\nlong sequences, the quadratic complexity of its self-attention algorithm and the cache size limit the ability to process long\ncontexts. Second, Transformers lack a single summary state, which means that each generated token must be computed\nover the entire context.\nAt the same time, the Selective State Space Model (Mamba2 (Dao and Gu 2024)) came into being. Mamba2 stores effective\nrelevant information through its selective state update algorithm, balances the quadratic and linear calculation methods\nof relevant matrices, achieves linear scaling of sequence length during training, and maintains a constant state size during\ngeneration. In addition, due to its linear recursive state update mechanism, Mamba2 has a single summary state. However,\nMamba2 also has a major drawback: its state does not expand with the sequence length, and information compression\ninevitably leads to information loss.\nTo build a model that is both efficient and effective, the key is to balance the relationship between compressing information\nstates and storing all information states. My main goal is to integrate the selective state space algorithm with the quadratic\nself-attention algorithm to overcome their respective limitations. Furthermore, combining these two algorithms with\na mixed expert with cross-domain general knowledge to build a better basic model architecture than Transformers or\nMamba. The model has the ability to filter information, long-term dependencies in long contexts, summary states, efficient\nlearning, and low memory usage. This paper aims to further explore how to combine the selective state space algorithm\nwith the quadratic self-attention algorithm to build a new model architecture and promote language modeling in a more\nefficient and more effective direction.\nPosition Encoding. The key to combining the selective state space algorithm with the attention algorithm is the effec-\ntive integration of positional information. In Mamba (Gu and Dao 2023), the position information is implicitly provided\nby causal convolution, and matrix D is used to skip the connection to the input and output of the selective state space\nalgorithm, re-continuing the discrete positional information. In Mamba2 (Dao and Gu 2024), the cumulative product is"}, {"title": "Algorithm Mixing.", "content": "Instead of letting the hidden state passively store information, it is better to let it actively learn. In\nOTCE (Shi et al. 2024), it was found that SSM at the end of the model structure would lead to problems such as knowledge\nrouting bias. Considering the selective information filtering ability, SSD should be used before Attn. Considering that\nthe SSD state size is constant and the Attn state expands with the sequence length, using SSD as an internal function\nto selectively recursively update context information, and Attn as an external function to focus on the overall language\nmodeling."}, {"title": "Cross Domain.", "content": "In human society, knowledge is widely distributed in different fields, and these fields are interconnected\nthrough common foundational knowledge and cross-domain connections. In OTCE (Shi et al. 2024), it was preliminarily\nverified that the mixed expert with shared parameters has advantages in both pre-training and fine-tuning stages. I\nselected the mixed expert with the best performance in cross-domain knowledge learning, and after adjusting its internal\nstructure and algorithm, it can be expanded to the million level, which I call the Cross-Domain Million Mixed Expert\n(CDMMOE). It can expand the number of cross-domain experts by 1024 times under the same parameter scale, and will\nnot cause a rapid decrease in calculation speed due to the increase in the number of experts, which is more efficient."}, {"title": "Architecture Design.", "content": "I use the rotation position encoding matrix as the position encoding method of the structured\nstate space dual matrix and the causal self-attention matrix. Use the structured state space dual matrix as the internal\nfunction of the causal self-attention matrix. Considering that the causal self-attention is slow in long sequence calculation,\nto extend the model depth, several structured state space dual matrices can be used before it, using its selective state space\nalgorithm for information filtering. After each structured state space dual matrix and causal self-attention matrix, use\nthe cross-domain million mixed expert matrix to store cross-domain general knowledge and domain-specific knowledge.\nThese wonderful matrices form the Cheems architecture, as shown in Figure 1.\nI empirically validated Cheems on multiple tasks, including training speed, semantic similarity evaluation, long and short\ntext classification, natural language reasoning, keyword recognition, topic selection tasks in different domains, context\nlearning, and multi-query associative memory recall tasks. These experiments demonstrate the effectiveness and efficiency\nof the Cheems architecture in handling complex language tasks."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Attention", "content": "Attention is a mechanism that computes the relevance scores between each element in the sequence and all other el-\nements, allowing each element to \"attend\" to other elements. The most important variant of attention is the softmax\nself-attention.\n$Y = \\text{softmax}(Q K^T) \\cdot V$\nA notable feature of self-attention is that it can capture dependencies between any positions in the input sequence, without\nbeing limited by distance, and the state expands with the sequence length, which gives it an advantage in capturing long-\nrange dependencies in long sequences.\nIn causal language modeling, a causal mask is usually added to it, which I will refer to as quadratic self-attention in the\nfollowing text."}, {"title": "2.2 Structured State Space Duality", "content": "Many variants of attention have been proposed, all of which are based on the core of attention scores.\nlinear attention (Katharopoulos et al. 2020) discards softmax by folding it into the kernel feature map and rewrites\n$(Q K^T) \\cdot V = Q \\cdot (K^T V)$ using the kernel property of matrix multiplication. In the case of causal (autoregressive) attention,\nthey show that when the causal mask is merged to the left as $(\\mathcal{L} \\circ Q K^T) \\cdot V$, where $\\mathcal{L}$ is a lower triangular matrix, the right\nside can be expanded into a recursive form.\nIn Transformers are SSMs (Dao and Gu 2024), the structured state space duality is used to prove that simply computing\nthe scalar structured SSM \u2013 by materializing the semi-separable matrix $M = \\mathcal{L} \\circ C B^T = \\mathcal{L} \\circ Q K^T$ and performing quadratic\nmatrix-vector multiplication \u2013 is equivalent to quadratic masked kernel attention.\n$(\\mathcal{L} \\circ Q K^T) \\cdot V = (\\mathcal{L} \\circ C B^T) \\cdot X$"}, {"title": "2.3 Positional Encoding", "content": "Position information is important in language modeling, and there are mainly three forms of relative positional encoding:\nconvolution, recursive, and inner product.\nThe source of positional information in Mamba (Gu and Dao 2023) is causal convolution and matrix D that skips the\nconnection between input and output.\nIn Mamba2 (Dao and Gu 2024), element $a_t$ acts as a \"gate\" or \"selector\", and its cumulative product $a_{(j:i)}$ controls\nthe amount of interaction allowed between position i and position j, which can be seen as a form of relative positional\nembedding.\nROPE (Su et al. 2021) adds absolute positional information to Q and K in self-attention, and obtains the relative positional\nencoding matrix $[B, L, L]$ by calculating the inner product of $Q K^T [B, L, D] \\times [B, L, D]^T$."}, {"title": "2.4 Cross Domain Mixture of Experts", "content": "The sparse activation mixture of experts architecture aims to train a larger model in fewer training steps with limited\ncomputational resources, which often performs better than training a smaller model in more steps.\nTo ensure that experts obtain non-overlapping knowledge, OTCE (Shi et al. 2024) achieves this by sharing some private\nexpert internal parameters and passing through shared parameters before entering private experts. The goal is to capture\ncommon knowledge and reduce redundancy. The extended cross-domain experts perform well in SFT fine-tuning and\nfew-shot learning tasks, but the training speed is slow."}, {"title": "2.5 Mixture of A Million Experts", "content": "The finer the granularity of the sparse activation mixture of experts, the better the performance. Mixture of A Million\nExperts (He 2024) proposes PEER (parameter efficient expert retrieval) to maintain computational efficiency under a large\nnumber of experts."}, {"title": "2.6 Expressive Hidden States", "content": "Learning to (Learn at Test Time) (Sun et al. 2024) proposes to make the hidden state the machine learning model itself to\nincrease its expressive power. The hidden state is a neural network that can be trained to perform any task, and the model\ncan be trained to learn the hidden state."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Inner Product Positional Encoding", "content": "For example, in the self-attention $Q K^T$, the dot product of two vectors $Q_i \\cdot K_j$ is calculated, and the result is a scalar, which\nrepresents the correlation between position i and position j.\nThe basic idea of rotation position encoding is to encode the position information as a complex rotation matrix, whose\nangle is determined by the position index. When $Q K$ or $C B$ is applied with RoPE, if an element position is close to the\nfront, its rotation will affect the direction of the K or B vector multiplied by it, thereby affecting the result of the inner\nproduct.\nDefine ROPE as $f_{\\{Q,K\\}}(x_i, i)$ and $f_{\\{C,B\\}}(x_i, i)$, where $x_i$ is the input vector, and i is the position index, then:\n$f_{\\{Q,K\\}}(x_i, i) = R_{\\Theta_i}^d W_{\\{Q,K\\}} x_i\\\\\nf_{\\{C,B\\}}(x_i, i) = R_{\\Theta_i}^d W_{\\{C,B\\}} x_i$\nwhere $\\Theta$ and $R_\\Theta^d$; are defined as follows:\n$\\Theta = \\{\\theta_i = n \\frac{\\Theta}{2(i-1)/d}, i \\in [1, 2, ..., d/2]\\}\\\\\\nR_{\\Theta_i}^d =$\n$\\begin{bmatrix}\n\\cos i \\theta_0 & -\\sin i \\theta_0 & 0 & 0 & 0 & 0\\\\\n\\sin i \\theta_0 & \\cos i \\theta_0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\cos i \\theta_1 & -\\sin i \\theta_1 & 0 & 0\\\\\n0 & 0 & \\sin i \\theta_1 & \\cos i \\theta_1 & 0 & 0\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n0 & 0 & 0 & 0 & \\cos i \\theta_{d/2} & -\\sin i \\theta_{d/2-1}\\\\\n0 & 0 & 0 & 0 & \\sin i \\theta_{d/2} & \\cos i \\theta_{d/2-1}\n\\end{bmatrix}$"}, {"title": "3.2 Inner Function Attention", "content": ""}, {"title": "3.3 Cross Domain Million Mixtures of Experts", "content": "In the conventional mixture of experts strategy, the tokens assigned to different experts need to have common knowledge\nor information, so multiple experts will have redundant parameters for storing common information when obtaining their\nrespective parameters, which leads to expert parameter redundancy. And the proportion of expert parameter redundancy\nincreases with the increase in expert granularity and the decrease in the number of expert activations. In Mixture of A\nMillion Experts (MMOE) (He 2024), due to the high degree of expert refinement, the proportion of expert parameter re-\ndundancy reaches an astonishingly high level, which can be seen in the loss curve at the beginning of pre-training.\nHowever, if the tokens assigned to different experts have already passed through the parameters for storing common\nknowledge, the parameter redundancy can be reduced. The reduction of redundancy will help build a more efficient and\nprofessional mixture of experts. This parameter for storing common knowledge can be called cross-domain.\n$\\text{CrossDomain}(x, W^S, V, W_2) = (x W^S \\cdot \\sigma(x V)) W\\\\\nf(x) = \\sum_{i=1}^N e_i^p (\\text{CrossDomain}(x))$\nOf course, in order to allow private experts to decide for themselves which information from the shared cross-domain\nshould be used and to be able to arbitrarily adjust the proportion of shared cross-domain parameters, it is necessary to\nmake certain modifications to the PEER strategy. Taking only one private expert as an example:\n$\\text{ln_Private}(x, W) = \\text{CrossDomain}(x) W \\quad W \\in \\mathbb{R}^{d \\times d_P}\\\\\\\\ \\text{CDMMOE}(x, W^P, V^P, W_2^P) = (x W^{P^T} \\cdot \\sigma(x V^{P^T})) W_2$"}, {"title": "3.4 Cheems Architecture", "content": "In fact, after the discussion of the previous few sections, the architecture of Cheems has emerged. I use RoPE in the inner\nproduct positional encoding as the source of positional information for SSD and Attn. Add a CDMMOE module after each\nSSD or InnerFunctionAttn module to store factual knowledge. Considering that the quadratic self-attention is slow in long\nsequence calculation, more SSD modules are needed to extend the model depth. Considering the selective nature of SSD,\nthe SSD module is used before the InnerFunctionAttn module for information filtering. Considering that the SSD module\nat the end of the model will lead to knowledge routing bias within a certain range, the SSD module is not used at the end\nof the model. There are many studies that show that the best perplexity performance of the model is when the ratio of SSD\nto Attn is 7:1. Therefore, a Cheems module is 7 \u00d7 (ROPE + SSD + CDMMOE) + (ROPE + InnerFunctionAttn + CDMMOE),\nstacked in this way."}, {"title": "4 Empirical Validation", "content": ""}, {"title": "4.1 Environment Dataset and Hyperparameters", "content": "\u2022\n\u2022 The environment is an open-source PyTorch image provided by Nvidia (NVIDIA 2022).\n\u2022 The pre-training dataset is a mixture of open-source datasets including Book, Wikipedia, UNCorpus, translation2019zh,\nWikiMatri, news-commentry, ParaCrawlv9, ClueCorpusSmall, CSL, news-crawl, etc.\n\u2022 The SFT fine-tuning is 2 million instructions of the Belle dataset.\n\u2022 Use the Trainer class of the Transformers library for training and evaluation and save relevant metrics.\n\u2022 Use GLM (Du et al. 2022) as the architecture backbone.\n\u2022 The AdamW hyperparameters are set to \u03b2\u2081 = 0.9, \u03b22 = 0.98, \u0454 = 10-6.\n\u2022 The linear warm-up steps are 10% of the total steps, reaching the maximum learning rate of 2e-4, and then cosine\ndecay to the minimum learning rate of 2e-5.\nUse RMSNorm instead of LayerNorm.\n\u2022 No linear bias terms."}, {"title": "4.2 Language Modeling", "content": "The architecture of mixed state space algorithm and attention algorithm naturally needs to be compared with the same\nhybrid architecture, I chose Jamba (Lieber et al. 2024) and OTCE (Shi et al. 2024) as the comparison objects.\nIn Figure 5, the throughput during training (forward and backward propagation) and evaluation (forward propagation) of\nthe three architectures is shown. Cheems architecture shows higher efficiency when the sequence length is above 4K in\nmodern large-scale language modeling.\nIn Table 1, the perplexity performance of pre-training and fine-tuning of the three architectures at different scales and\nthe verification accuracy on different tasks are compared. Cheems shows extremely low perplexity performance in SFT\nfine-tuning. CLUE is the benchmark for evaluating the model's performance on sequence classification tasks. CEval\nis the benchmark for evaluating the model's comprehensive ability on many domains, more detailed performance on\neach subtask is shown in Appendix B.3. PIQA (standard short context task), NarrativeQA (natural long context task), and\nHotpotQA (synthetic long context task) are benchmarks for evaluating the model's associative memory recall performance\nat different context lengths, Cheems shows better performance as the context length increases."}, {"title": "5 Conclusion", "content": "This paper explores and proposes a new basic model architecture Cheems, continuing to promote language modeling in\na more efficient and more effective direction."}]}