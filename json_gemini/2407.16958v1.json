{"title": "Cheems: Wonderful Matrices\nMore Efficient and More Effective Architecture", "authors": ["Jingze Shi", "Lu He", "Yuhan Wang", "Tianyu He", "Bingheng Wu", "Mingkun Hou"], "abstract": "Recent studies have shown that, relative position encoding performs well in selective state space model scanning algorithms, and the architecture that balances SSM and Attention enhances the efficiency and effectiveness of the algorithm, while the sparse activation of the mixture of experts reduces the training cost. I studied the effectiveness of using different position encodings in structured state space dual algorithms, and the more effective SSD-Attn internal and external function mixing method, and designed a more efficient cross domain mixture of experts. I found that the same matrix is very wonderful in different algorithms, which allows us to establish a new hybrid sparse architecture: Cheems. Compared with other hybrid architectures, it is more efficient and more effective in language modeling tasks.", "sections": [{"title": "1 Introduction", "content": "Efficient algorithms are designed to compress information states, so that they can store as much useful information as possible in a limited state space, while effective algorithms are designed to store all information states, so that they can avoid capturing biased information.\nTransformers (Attention is All You Need (Vaswani et al. 2017)) Architecture is popular in modern deep learning language modeling, which can directly capture the relationship between any two elements in the sequence and effectively deal with long-distance dependency problems. However, the architecture has two main drawbacks. First, when dealing with long sequences, the quadratic complexity of its self-attention algorithm and the cache size limit the ability to process long contexts. Second, Transformers lack a single summary state, which means that each generated token must be computed over the entire context.\nAt the same time, the Selective State Space Model (Mamba2 (Dao and Gu 2024)) came into being. Mamba2 stores effective relevant information through its selective state update algorithm, balances the quadratic and linear calculation methods of relevant matrices, achieves linear scaling of sequence length during training, and maintains a constant state size during generation. In addition, due to its linear recursive state update mechanism, Mamba2 has a single summary state. However, Mamba2 also has a major drawback: its state does not expand with the sequence length, and information compression inevitably leads to information loss.\nTo build a model that is both efficient and effective, the key is to balance the relationship between compressing information states and storing all information states. My main goal is to integrate the selective state space algorithm with the quadratic self-attention algorithm to overcome their respective limitations. Furthermore, combining these two algorithms with a mixed expert with cross-domain general knowledge to build a better basic model architecture than Transformers or Mamba. The model has the ability to filter information, long-term dependencies in long contexts, summary states, efficient learning, and low memory usage. This paper aims to further explore how to combine the selective state space algorithm with the quadratic self-attention algorithm to build a new model architecture and promote language modeling in a more efficient and more effective direction.\nPosition Encoding. The key to combining the selective state space algorithm with the attention algorithm is the effective integration of positional information. In Mamba (Gu and Dao 2023), the position information is implicitly provided by causal convolution, and matrix D is used to skip the connection to the input and output of the selective state space algorithm, re-continuing the discrete positional information. In Mamba2 (Dao and Gu 2024), the cumulative product is"}, {"title": "2 Background", "content": "2.1 Attention\nAttention is a mechanism that computes the relevance scores between each element in the sequence and all other elements, allowing each element to \"attend\" to other elements. The most important variant of attention is the softmax self-attention.\n$Y = softmax(QKT) \\cdot V$\nA notable feature of self-attention is that it can capture dependencies between any positions in the input sequence, without being limited by distance, and the state expands with the sequence length, which gives it an advantage in capturing long-range dependencies in long sequences.\nIn causal language modeling, a causal mask is usually added to it, which I will refer to as quadratic self-attention in the following text.\n2.2 Structured State Space Duality\nMany variants of attention have been proposed, all of which are based on the core of attention scores.\nlinear attention (Katharopoulos et al. 2020) discards softmax by folding it into the kernel feature map and rewrites $(QKT) \\cdot V = Q \\cdot (KTV)$ using the kernel property of matrix multiplication. In the case of causal (autoregressive) attention, they show that when the causal mask is merged to the left as $(L0 QK\u00af) \\cdot V$, where L is a lower triangular matrix, the right side can be expanded into a recursive form.\nIn Transformers are SSMs (Dao and Gu 2024), the structured state space duality is used to prove that simply computing the scalar structured SSM \u2013 by materializing the semi-separable matrix $M = L\u25e6CBT = L0QK\u2122$ and performing quadratic matrix-vector multiplication \u2013 is equivalent to quadratic masked kernel attention.\n$(L\u3002QKT) \\cdot V = (L0 CBT) \\cdot X$\n2.3 Positional Encoding\nPosition information is important in language modeling, and there are mainly three forms of relative positional encoding: convolution, recursive, and inner product.\nThe source of positional information in Mamba (Gu and Dao 2023) is causal convolution and matrix D that skips the connection between input and output.\nIn Mamba2 (Dao and Gu 2024), element $a_t$ acts as a \"gate\" or \"selector\", and its cumulative product $a(j: i)$ controls the amount of interaction allowed between position i and position j, which can be seen as a form of relative positional embedding.\nROPE (Su et al. 2021) adds absolute positional information to Q and K in self-attention, and obtains the relative positional encoding matrix [B, L, L] by calculating the inner product of $QKT [B, L, D] \u00d7 [B, L, D]\u2122$.\n2.4 Cross Domain Mixture of Experts\nThe sparse activation mixture of experts architecture aims to train a larger model in fewer training steps with limited computational resources, which often performs better than training a smaller model in more steps.\nTo ensure that experts obtain non-overlapping knowledge, OTCE (Shi et al. 2024) achieves this by sharing some private expert internal parameters and passing through shared parameters before entering private experts. The goal is to capture common knowledge and reduce redundancy. The extended cross-domain experts perform well in SFT fine-tuning and few-shot learning tasks, but the training speed is slow."}, {"title": "2.5 Mixture of A Million Experts", "content": "The finer the granularity of the sparse activation mixture of experts, the better the performance. Mixture of A Million Experts (He 2024) proposes PEER (parameter efficient expert retrieval) to maintain computational efficiency under a large number of experts."}, {"title": "2.6 Expressive Hidden States", "content": "Learning to (Learn at Test Time) (Sun et al. 2024) proposes to make the hidden state the machine learning model itself to increase its expressive power. The hidden state is a neural network that can be trained to perform any task, and the model can be trained to learn the hidden state."}, {"title": "3 Methods", "content": "3.1 Inner Product Positional Encoding\nFor example, in the self-attention $QKT$, the dot product of two vectors $Q_i \\cdot K_j$ is calculated, and the result is a scalar, which represents the correlation between position i and position j.\nThe basic idea of rotation position encoding is to encode the position information as a complex rotation matrix, whose angle is determined by the position index. When $QK$ or $CB$ is applied with RoPE, if an element position is close to the front, its rotation will affect the direction of the K or B vector multiplied by it, thereby affecting the result of the inner product.\nDefine ROPE as $f_{{Q,K}} (x_i, i)$ and $f_{{C,B}} (x_i, i)$, where $x_i$ is the input vector, and i is the position index, then:"}, {"title": "3.2 Inner Function Attention", "content": "The quadratic self-attention algorithm calculates the Q and K related to the entire input sequence X to form the attention matrix, and the hidden state (usually referred to as the KV cache) is a linearly growing list with t (token), explicitly storing all historical context information without any compression, and the time complexity of calculating this linearly growing state is quadratically increasing, which is a characteristic of quadratic self-attention. I do not consider the efficiency problem of this quadratic algorithm on long sequences O(t) in this paper.\n$Q_{proj} = XW_Q$\n$K_{proj} = XW_K$\nTo improve the expressive power of the quadratic self-attention hidden state from the perspective of ignoring a certain speed to improve the effect, and finally achieve efficiency improvement, it is necessary to change from a simple $y = xW$ to an excellent heuristic $y = f(x, W, ...)$. Considering that Q and K need to perform inner product operations, more complex operations may cause large fluctuations during training, so the heuristic should be applied to V. Of course, from a more intuitive perspective, queries and keys do indeed only need simple linear transformations, and their dot product similarity, that is, the attention matrix, is to determine which information to extract from the values. Therefore, we can apply the heuristic to the values to improve the expressive power of the hidden state. This is the basic idea of inner function attention.\n$V_{InnerFunction} = f(X, W, ...)$\nSuppose there is a Super Hero with a photographic memory, who can remember everything seen and recall any detail. When you ask it a question, at first, you will decide that this Super Hero is very smart and seems to know everything. But as you ask more and more questions, you will realize that it does not think at all because it refuses to compress information and cannot extract patterns and rules from a large amount of information. Having the ability to compress information in both training and reasoning is the key to intelligence.\nIn OTCE (Shi et al. 2024), it was found that when mixing SSM (Gu and Dao 2023) with Attention, if the entire sequence is calculated, the quadratic self-attention algorithm can learn the recursive state update from $h_{t\u22121}$ to $h_t$. And the state space algorithm such as SSD (Dao and Gu 2024) has a constant state size, using it as a heuristic will not cause too much speed reduction on long sequences. So $V = SSD(X, W_A, W_B, W_C)$ is a selective recursive state related to the input X, from $h_0$ to $h_t$, which meets the requirements of improving the expressive power of the hidden state.\n$V_{InnerFunction} = SSD(X, W_A, W_B, W_C)$\nThen the attention matrix calculated by $Q K^T$ extracts information from the selective recursive state V, retaining the explicit storage of all historical context information of the quadratic self-attention.\n$A = softmax(Q_{proj}K_{proj})$\n$Y = AV_{InnerFunction}$"}, {"title": "3.3 Cross Domain Million Mixtures of Experts", "content": "In the conventional mixture of experts strategy, the tokens assigned to different experts need to have common knowledge or information, so multiple experts will have redundant parameters for storing common information when obtaining their respective parameters, which leads to expert parameter redundancy. And the proportion of expert parameter redundancy increases with the increase in expert granularity and the decrease in the number of expert activations. In Mixture of A Million Experts (MMOE) (He 2024), due to the high degree of expert refinement, the proportion of expert parameter redundancy reaches an astonishingly high level, which can be seen in the loss curve at the beginning of pre-training.\nHowever, if the tokens assigned to different experts have already passed through the parameters for storing common knowledge, the parameter redundancy can be reduced. The reduction of redundancy will help build a more efficient and professional mixture of experts. This parameter for storing common knowledge can be called cross-domain.\n$CrossDomain(x, W\u00b3, V, W\u2082) = (xW\u00b3\u00b7 \u03c3(xV))W$\nwhere s represents shared parameters, \u03c3 represents the activation function, W, V, W\u2082 represent the three weight matrices of the Gated MLP.\nSubsequently, we use the output of the cross-domain as the input of the private mixture of experts, where the mixture of experts strategy can be a classic routing strategy or the PEER (He 2024) strategy.\n$f(x) = \\sum_{i=1}^{N} e_i^{p_i} (CrossDomain(x))$\nwhere e represents the expert, p represents the private parameters, and N represents the number of experts.\nOf course, in order to allow private experts to decide for themselves which information from the shared cross-domain should be used and to be able to arbitrarily adjust the proportion of shared cross-domain parameters, it is necessary to make certain modifications to the PEER strategy. Taking only one private expert as an example:\n$In_Private(x, W) = CrossDomain(x)W W\u2208R^{dxdP}$\n$CDMMOE(x, WP, VP, W) = (xWP T . \u03c3(xVPT))W2$"}, {"title": "3.4 Cheems Architecture", "content": "In fact, after the discussion of the previous few sections, the architecture of Cheems has emerged. I use RoPE in the inner product positional encoding as the source of positional information for SSD and Attn. Add a CDMMOE module after each SSD or InnerFunctionAttn module to store factual knowledge. Considering that the quadratic self-attention is slow in long sequence calculation, more SSD modules are needed to extend the model depth. Considering the selective nature of SSD, the SSD module is used before the InnerFunctionAttn module for information filtering. Considering that the SSD module at the end of the model will lead to knowledge routing bias within a certain range, the SSD module is not used at the end of the model. There are many studies that show that the best perplexity performance of the model is when the ratio of SSD to Attn is 7:1. Therefore, a Cheems module is 7 \u00d7 (ROPE + SSD + CDMMOE) + (ROPE + InnerFunctionAttn + CDMMOE), stacked in this way."}, {"title": "4 Empirical Validation", "content": "4.1 Environment Dataset and Hyperparameters\n\u2022 The environment is an open-source PyTorch image provided by Nvidia (NVIDIA 2022).\n\u2022 The pre-training dataset is a mixture of open-source datasets including Book, Wikipedia, UNCorpus, translation2019zh, WikiMatri, news-commentry, ParaCrawlv9, ClueCorpusSmall, CSL, news-crawl, etc.\n\u2022 The SFT fine-tuning is 2 million instructions of the Belle dataset.\n\u2022 Use the Trainer class of the Transformers library for training and evaluation and save relevant metrics.\n\u2022 Use GLM (Du et al. 2022) as the architecture backbone.\n\u2022 The AdamW hyperparameters are set to \u03b2\u2081 = 0.9, \u03b22 = 0.98, \u0454 = 10-6.\n\u2022 The linear warm-up steps are 10% of the total steps, reaching the maximum learning rate of 2e-4, and then cosine decay to the minimum learning rate of 2e-5.\nUse RMSNorm instead of LayerNorm.\n\u2022 No linear bias terms.\n4.2 Language Modeling\nThe architecture of mixed state space algorithm and attention algorithm naturally needs to be compared with the same hybrid architecture, I chose Jamba (Lieber et al. 2024) and OTCE (Shi et al. 2024) as the comparison objects.\nIn Figure 5, the throughput during training (forward and backward propagation) and evaluation (forward propagation) of the three architectures is shown. Cheems architecture shows higher efficiency when the sequence length is above 4K in modern large-scale language modeling.\nIn Table 1, the perplexity performance of pre-training and fine-tuning of the three architectures at different scales and the verification accuracy on different tasks are compared. Cheems shows extremely low perplexity performance in SFT fine-tuning. CLUE is the benchmark for evaluating the model's performance on sequence classification tasks. CEval is the benchmark for evaluating the model's comprehensive ability on many domains, more detailed performance on each subtask is shown in Appendix B.3. PIQA (standard short context task), NarrativeQA (natural long context task), and HotpotQA (synthetic long context task) are benchmarks for evaluating the model's associative memory recall performance at different context lengths, Cheems shows better performance as the context length increases."}, {"title": "5 Conclusion", "content": "This paper explores and proposes a new basic model architecture Cheems, continuing to promote language modeling in a more efficient and more effective direction."}]}