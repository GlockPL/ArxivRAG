{"title": "Knowledge Distillation Neural Network for Predicting Car-following Behaviour of Human-driven and Autonomous Vehicles", "authors": ["Ayobami Adewale", "Chris Lee", "Amnir Hadachi", "Nicolly Lima da Silva"], "abstract": "As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and Human-driven vehicles (HDVs), understanding the car-following behaviour is important to improve traffic efficiency and road safety. Using a real-world trajectory dataset, this study uses descriptive and statistical analysis to investigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV and HDV-HDV in mixed traffic. The ANOVA test showed that car-following behaviours across different vehicle pairs are statistically significant (p-value < 0.05).\nWe also introduce a data-driven Knowledge Distillation Neural Network (KDNN) model for predicting car-following behaviour in terms of speed. The KDNN model demonstrates comparable predictive accuracy to its teacher network, a Long Short-Term Memory (LSTM) network, and outperforms both the standalone student network, a Multilayer Perceptron (MLP), and traditional physics-based models like the Gipps model. Notably, the KDNN model better prevents collisions, measured by minimum Time-to-Collision (TTC), and operates with lower computational power, making it ideal for AVs or driving simulators requiring efficient computing.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, data-driven car-following models have been developed to analyze and predict car-following be-haviours. Data-driven car-following models adopt advanced machine learning (ML) models such as Multiple-Layer-Perceptron (MLP) [1], Long Short-Term Memory (LSTM) networks [2], graph neural network (GRNN) [3], and hybrid networks [4].\nThese data-driven models have increasingly become the preferred method for predicting car-following behaviour, outperforming traditional physics-based models such as the Gipps model [5], the Intelligent Driver Model (IDM) [6] and the General Motors (GM) model [7]. While these classical physics-based models have undergone various improvements [8, 9] to increase their predictive capabilities, they still generally do not achieve the level of accuracy provided by data-driven models.\nDespite the evident advantages of data-driven models in accuracy and adaptability, they often demand high com-putational power and memory resources, posing challenges for deployment in resource-constrained environments like IoT-based systems [10, 11]. While lightweight models like LightGBM offer efficiency [12], they struggle to capture the complex relationships in mixed traffic involving HDVs and AVs. Based on these limitations, there is a need for efficient model architectures to deliver high-performance predictions within the constraints of current IoT devices used in auto-motive technologies.\nIn this regard, this study introduces a Knowledge Dis-tillation Neural Network (KDNN), which maintains a high level of predictive accuracy of car-following behaviour while considering the practical constraints of computational power and complexity. The KDNN distils complex and nuanced knowledge captured by large and complex neural network models into lightweight models suitable for deployment on devices with limited computational power and memory. It is expected that the KDNN can provide accurate predictions of car-following behaviour more efficiently with reduced computational time.\nThus, the objective of this study is to develop the KDNN for predicting car-following behaviour and evaluate its per-formance in comparison with the two conventional ML models-LSTM and Multiple Layer Perception Network (MLP). The models were trained and validated using real-world vehicle trajectories of autonomous vehicles (AV) and human-driven vehicles (HDV). The remainder of the paper"}, {"title": "II. DATA", "content": "This study analysed car-following behaviour using the Waymo trajectory dataset provided by Google, with pro-cessing detailed by Hu et al. [13]. The dataset includes trajectories of HDVs and AVs equipped with Lidar and camera technologies. This dataset comprises various pairings of vehicles: 1032 pairs of HDV following HDV (HDV-HDV), 274 pairs of HDV following AV (HDV-AV), and 196 pairs of AV following HDV (AV-HDV). Unfortunately, the dataset does not include instances of AVs following other AVs. Each vehicle's trajectory was recorded at a 0.1-second interval.\nKey variables collected from the dataset include the ac-celeration ($a_n$, $a_{n+1}$) and jerk ($j_n$, $j_{n+1}$) of both the lead vehicle (n) and the following vehicle (n + 1), the speed ($v_n$, $v_{n+1}$) of each vehicle, the spacing between them ($x_n$), and the speed difference between the following and lead vehicles ($\\Delta v_{n+1} = v_{n+1} - v_n$)."}, {"title": "B. Descriptive Analysis of Car-following Behaviour", "content": "To examine car-following behaviour in mixed traffic sce-narios using the trajectory dataset, we focused on trajectory points where the distance between the following vehicle and the lead vehicle was under 50 meters for all three vehicle pairings. We also examined the speed variability among all three vehicle pairs for different bins of spacing. By looking at the speed variability, we can compare the inconsistency of vehicle speed for each vehicle pair.\nFigure 1 shows that all three pairings exhibited increasing speed variability as spacing grew. The AV-HDV pair showed the lowest speed variability in tighter spacing bins ([5, 15] and [15, 25] meters), suggesting a more consistent and cautious approach. In contrast, HDV-AV had the highest variability, possibly due to more aggressive or unpredictable speed adjustments. At the same time, HDV-HDV pairs demonstrated the broadest range of variability, reflecting diverse human driving behaviours.\nTo further understand car-following behaviour, skewness and kurtosis were analyzed to evaluate driving aggressiveness and skill levels. Both methods have been previously used to assess driver aggressiveness and skill levels, as pointed out by Chen, Hwang, Ho, et al. [14]. Skewness measures how much a real-valued random variable's distribution leans away from the mean. In contrast, Kurtosis measures the \"tailedness\" of the probability distribution of a real-valued random variable, Higher skewness suggests more abrupt acceleration or deceleration, which reflects aggressive driving behaviour. Conversely, higher kurtosis suggests a driver's enhanced skill level, implying more consistent and controlled driving."}, {"title": "C. Statistical Analysis of Car-following Behaviour", "content": "To further analyse the effects of lead and following vehicle types on car-following behaviour, the means of the following vehicle's speed, acceleration, and TTC were compared among the three-vehicle pairs using a one-way ANOVA test. Given that these parameter values are sensitive to the spacing be-tween the lead and the following vehicles, mean values were analyzed across three defined spacing categories: <= 10m, 10m 15m, and 15m -30m.\nThe results show that mean speed increased with longer spacing for all vehicle pair types. The ANOVA test confirms that the mean speed differs significantly among the three ve-hicle pair types at a 95 confidence interval (p-value < 0.05). Notably, AV followers consistently displayed lower mean speeds than HDV followers, regardless of the lead vehicle type, likely due to AVs being programmed with a safety-first approach. Conversely, HDV drivers following AVs showed higher mean speeds than those following other HDVs across all spacing categories. This suggests that human drivers may trust and feel more comfortable following AVs."}, {"title": "III. METHODS", "content": "The KDNN is a machine learning approach where knowl-edge is transferred from a complex and large-scale model (teacher network) to a smaller and more efficient model (stu-dent network) [16]. The KDNN mimics how human beings learn through the teacher-student relationship. This process is beneficial when deploying large models is impractical due to resource constraints.\nKnowledge distillation works by training the student net-work to emulate the teacher network's predictive behaviour. The teacher network, typically large and complex like LSTM or convolutional neural network(CNN), is first trained on the original dataset for high accuracy. The smaller and simpler student network learns from both the original data and the teacher's outputs, allowing it to learn complex patterns the teacher identifies.\nThe knowledge transfer between the teacher and student network occurs during the training phase of the network, and it can be done through three different approaches: 1) response-based transfer, 2) feature-based transfer, and 3) relation-based knowledge transfer [17]. This study uses response-based transfer, where the teacher network solely transfers its prediction to the student network. The KDNN is trained using a composite loss function, a weighted com-bination of the student network's predictions:\n$L_{Total} = \\alpha L_{Student}^{MSE} + (1-\\alpha) L_{Distill}^{MSE}$ (1)\nwhere $L_{Total}$ is the total loss, $L_{Student}^{MSE}$ is the student network loss, $L_{Distill}^{MSE}$ is the distillation loss, and $\\alpha$ is a hyperparameter that manages the trade-off between the two.\nThen, the total loss is re-written as follows:\n$L_{Total}^{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\alpha (y_i - y_i^s)^2 + (1-\\alpha) \\frac{1}{N} \\sum_{i=1}^{N} (y_i - y_i^t)^2$ (2)\nwhere N represents the batch size, $y_i$ is the observed value for the i-th instance, $y_i^s$ is the student network's prediction for the i-th instance, $y_i^t$ is the teacher network's prediction for the i-th instance.\nUsing the proposed KDNN, the speed of the following vehicle is predicted as follows:\n$v_{n+1}(t+\\tau) = f(S_{n+1}(t), v_n(t), \\Delta v_{n+1}(t))$ (3)"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The KDNN model, LSTM model (teacher network), and MLP model (student network) were developed using Python in the Google Colab environment to leverage its higher processing power. Hyperparameter tuning was done using the 'keras-tuner' library, with Randomized Cross-Validation (RandomCV) selected over an exhaustive search like Grid-Search. Unlike GridSearch, RandomCV reduces computa-tional demand by randomly sampling a subset of hyperpa-rameter combinations when training the network, making it more efficient for complex models like LSTM.\nTo avoid overfitting or underfitting, TimeSeriesSplit cross-validation with KFold set to 3 was used. The dataset was split into 20% for testing, with the remaining data divided into 20% for validation and 60% for training. The optimal configurations determined by RandomCV are as follows: The student network used 5 epochs, 2 hidden layers with 60 nodes each, ReLU for hidden layers, sigmoid for the output layer, a learning rate of 0.01, and a batch size of 100. The teacher network was configured with 10 epochs, 2 hidden layers (475 and 61 nodes), ReLU for hidden layers, sigmoid for the output layer, a dropout rate of 0.3, a learning rate of 0.0016, and a batch size of 161.\nNotably, the teacher network had lower RMSE (0.585) than the student network (0.899). This shows that the teacher network has superior predictive capacity due to its complexity. The RMSE of the KDNN model varies with $\\alpha$ in a non-linear relationship, and it was lowest when $\\alpha = 0.5$. This RMSE was higher than the RMSE of the teacher network but lower than the RMSE for the student network. This indicates that the KDNN model successfully transferred knowledge from the teacher network to the student network and balance the trade-off between prediction accuracy and computational efficiency."}, {"title": "V. CONCLUSIONS AND RECOMMENDATIONS", "content": "This study analyzes car-following behaviour in mixed traffic of Human-driven Vehicles (HDVs) and Autonomous Vehicles (AVs) using real-world vehicle trajectory data. It also applies a knowledge distillation neural network (KDNN) model to predict car-following behaviour in mixed traffic.\nThe analysis showed that AVs had higher mean Time-to-Collision (TTC) in closer spacing than HDVs, reflecting AVs' safety features. HDVs, however, exhibited higher mean speeds when following AVs, suggesting greater comfort in following AVs than other HDVs.\nIn comparing RMSE among different models, the KDNN model showed better predictive accuracy than the standalone MLP student network and higher computational efficiency than the LSTM teacher network, as indicated by lower CPU usage. Additionally, the KDNN model resulted in safer driving behaviour, evidenced by higher minimum TTC values across all vehicle pair groups. This suggests that the KDNN model can replicate more conservative and safer driving behaviour, which can be adapted to the AV control algorithm.\nIn conclusion, the KDNN model offers a balanced solu-tion with accurate predictions and computational efficiency, making it suitable for resource-constrained environments like edge computing in AVs. Future research should explore the transferability of the KDNN model using other trajectory datasets like NGSIM and UBER. Different machine-learning models for teacher and student networks should also be con-sidered to improve the balance between prediction accuracy and computational power demand."}]}