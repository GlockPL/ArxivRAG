{"title": "Reinforcing Thinking through Reasoning-Enhanced Reward Models", "authors": ["Diji Yang", "Linda Zeng", "Kezhen Chen", "Yi Zhang"], "abstract": "Large Language Models (LLMs) exhibit great potential in complex multi-step reasoning through inference-time thinking but still struggle with deciding when to stop thinking due to limited self-awareness about their knowledge boundaries. While human preference alignment has shown extraordinary opportunities, expensive labeling challenges adherence to scaling law. Language model self-critique, as an alternative to using human-labeled reasoning data, is questioned with its inherited biases. This work addresses these challenges by distilling the LLM's own reasoning processes into synthetic behavioral data, eliminating the need for manual labeling of intermediate steps. Building on this concept, we propose Distillation-Reinforcement-Reasoning (DRR), a three-step framework that leverages the LLM's inherent behaviors as external feedback by first generating behavioral data using the Reasoner (LLM) to reflect its reasoning capabilities, then training a lightweight discriminative reward model (DM) on behavioral data, and finally deploying the DM at inference time to assist the Reasoner's decision-making. Experiments on multiple benchmarks show that the DRR framework outperforms self-critique approaches without relying on additional complex data annotation. Benefiting from lightweight design, ease of replication, and adaptability, DRR is applicable to a wide range of LLM-centric tasks.", "sections": [{"title": "1. Introduction", "content": "Generative Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, demonstrating impressive capabilities in understanding and generating human-like text (Ouyang et al., 2022; OpenAI, 2023; Dubey et al., 2024). Despite their successes, enabling LLMs to perform complex multi-step reasoning effectively remains a significant challenge (Mirzadeh et al., 2024). In tasks that require reasoning over multiple steps, such as complex problem-solving or logical inference, early attempts tried to make LLM mimic the multiple steps of prolonged thinking that humans perform when faced with difficult problems in inference time (OpenAI, 2024; Yang et al., 2024b). However, LLMs often struggle with deciding \"when to stop\" due to limited self-awareness of knowledge. They may provide incomplete solutions, over-extended reasoning, or fail to produce a final answer when appropriate.\nTraditional approaches to improving self-awareness in multi-step reasoning in LLMs primarily rely on outcome supervision, where models are trained to produce the correct final answer (Lightman et al., 2023). While this method helps align the models' outputs with expected results, it does not necessarily enhance the reasoning process itself. Recent research suggests that process supervision, which involves supervising the intermediate reasoning steps, can lead to better performance in multi-step reasoning tasks (OpenAI, 2024; Lightman et al., 2023; Kumar et al., 2024). On the downside, process supervision requires midterm labels for each reasoning step, which are expensive and time-consuming to obtain, limiting the scalability of this approach. Another line of research involves self-critique mechanisms, enabling models to assess and refine their outputs by leveraging their pre-trained knowledge or by estimating confidence scores (Pan et al., 2023). However, these strategies have limitations due to the models' inherent biases and tendencies towards hallucinations (Stechly et al., 2023; Huang et al., 2023; Stechly et al., 2024). Additionally, methods that rely on accessing internal model states are impractical for closed-source LLMs, restricting their applicability.\nIn this paper, we propose a novel framework that addresses the \"when to stop\" problem to equip pretrained LLMs with enhanced multi-step thinking capabilities. Inspired by ethological practices which analyze observable behaviors without inferring unmeasurable internal states (TINBERGEN, 1963)\u2014our method leverages the outputs generated by LLMs to enhance knowledge awareness of the system's knowledge boundaries. Specifically, our approach empowers the LLM with a multi-step self-correction mechanism through an in-context reinforcement learning process (Monea et al., 2024), which reinforces reasoning behav-"}, {"title": "2. Related Work", "content": "Recently, an emerging field in AI is to allow (large language) models to learn and enhance the reasoning process through reinforcement learning (RL) (Ouyang et al., 2022; Lightman et al., 2023; Yang et al., 2024a;b) in contrast to traditional outcome supervision only. While RL-based methods, with or without parameter changes (verbal RL (Shinn et al., 2024)), may not match the efficiency of supervised learning in rapidly aligning input-output pairs, they offer significant advantages once models acquire basic language understanding and reasoning capabilities through pretraining. Specifically, RL enables exploration with weaker outcome supervision, often surpassing training data patterns in solving complex problems (Kumar et al., 2024). Noteworthy achievements in this domain include AlphaGo (Silver et al., 2016), which mastered the game of Go, and the recent OpenAI-01 (OpenAI, 2024), which has demonstrated proficiency in general generative tasks. Nonetheless, systems that engage in these complex reasoning processes require the ability to determine when to end the reasoning process and generate a final response (Yang et al., 2024b). This decision-making relies heavily on the system's self-awareness or self-criticism (Pan et al., 2023). Existing research on system-level self-criticism can be categorized into two approaches: feedback generated by the LLM itself (LLM Self-critics) and feedback provided by additional components within the AI system (External Feedback).\nLLM Self-critics LLM self-critics mechanisms empower language models to assess and refine their own outputs (Kadavath et al., 2022; Yao et al., 2022; Asai et al., 2023; Amayuelas et al., 2023). There are two predominant strategies for LLMs to self-present their feedback (Pan et al., 2023). A straightforward approach leverages the model's pre-trained knowledge to critique and enhance its responses (Wang et al., 2023a; Weng et al., 2023; Madaan et al., 2024; Wang et al., 2023b; Yin et al., 2023). While this method utilizes the model's inherent capabilities, it may not be able to reliably identify its own errors due to its inherent biases and tendencies to hallucinations (Stechly et al., 2023). The second approach involves using confidence scores or uncertainty estimates to evaluate the quality of the model's outputs (Xie et al., 2024; Farquhar et al., 2024; Chen et al., 2024; Zhang et al., 2023). This strategy typically requires manually crafted criteria, which can be labor-intensive and lack a theoretical foundation for optimal configuration, thereby limiting its reliability and generalizability (Pan et al., 2023). Moreover, the need to access the internal state of the model closes off the opportunity for its application to good-performing closed-source LLM APIs such as ChatGPT and Gemini.\nInspired by the fact that Ethologists draw conclusions about animals through observable behavior and avoid making assumptions about internal states that can hardly be directly measured (TINBERGEN, 1963), our approach emphasizes the analysis of real observable behavior (i.e., the outputs of LLM) rather than defining internal states (e.g., confidence thresholds).\nExternal Feedback Recent studies have shown that external modules can provide meaningful feedback during the generation process of LLMs, thereby enhancing system performance (Gou et al., 2024). These modules encompass a diverse range of variations, including tools like Code Interpreters (Chen et al., 2023a), Search Engines (Trivedi et al., 2023), and other specialized software (Kim et al., 2024), as well as carefully designed rules (Yao et al., 2024). MemPrompt (Madaan et al., 2022) predefines a dictionary of possible LLM outputs and their corresponding scores to offer feedback at inference time, similar to our analysis of LLM behavior. While these approaches effectively respond to LLM behavior across various benchmarks, however, these non-deep learning tools tend to limit the flexibility of the system.\nOn the other hand, since RLHF (Ouyang et al., 2022), using external deep learning models as a reward provider has also been widely studied recently (Bai et al., 2022; Glaese et al., 2022). These works focus on using the feedback provided by the reward model to fine-tune the LLM instead of directly participating in the generation at inference time. Consequently, these resource-intensive approaches require"}, {"title": "3. Approach", "content": "In this section, we introduce our main framework for enhancing LLM inference time reasoning. As shown in Figure 2, it consists of three steps: behavioral data generation via Reasoning Process Distillation, where LLM reasoning patterns are collected as training data (Section 3.1); training of the Discriminative Model (DM), where the DM learns to assess LLM behavior (Section 3.2); and system deployment for inference, during which the LLM and the DM interact over multiple iterations in order to produce a final answer or abstention (Section 3.3).\n3.1. Reasoning Process Distillation\nAlgorithm 1 depicts our semi-supervised Reasoning Process Distillation algorithm for data collection to support the Discriminative Model (DM) training. This algorithm simulates the Reasoner LLM's inference-time behavior across multiple reasoning iterations using raw training data. For each question Q, the Reasoner R generates an answer A' alongside its rationale r. A binary verdict label is assigned to indicate whether the DM should accept or reject the response, depending on whether A' matches the ground-truth answer A. For all incorrect answers, it is re-prompted with the question Q and past history (i.e., context C) to explore alternative reasoning paths. This iterative process continues until the Reasoner generates the correct answer A, signaling that the DM should accept the response and stop further reasoning. If no accurate response is generated after a predefined number of iterations, the process terminates to avoid an infinity loop. As the result, the generated data reflects a diverse range of LLM behaviors the DM may encounter across various turns during inference, allowing the DM to directly learn the unconscious reasoning patterns that LLM tends to generate when leading to a correct or incorrect answer. By making use of LLM's natural behaviors with its environment, this generation method streamlines the feedback collection process, relying solely on the input and ground-truth output from existing datasets, which are typically available for most datasets.\n3.2. Discriminative Model Training\nGiven a question, its past context, and an LLM-generated answer with rationale, the goal of the DM is to predict whether the new answer should be accepted or rejected. In other words, the DM acts as a binary classification model, and its training on a distribution over (x, y) is alignment between the input tuple xi = {Q, C, A', r} and the label y \u2208 {Accept, Reject}. Through observing both the LLM's previous responses and its new rationale, which may include corrections to past reasoning or further explorations into internal knowledge, the DM learns to be both a coach, supervising the LLM's behavior across multiple iterations, and a judge, determining the reliability of its final response. This enables the DM to determine if the LLM's response is influenced by hallucinations and when it should stop.\nSpecifically, the DM predicts logits z \u2208 R2, representing unnormalized probabilities for the two classes. To account for the higher cost of false-positive errors (i.e., Accepting an incorrect answer), the weight w\u2081 for class Accept is set higher than wo, the weight for class Reject. The weighting design encourages DM to adopt stricter acceptance criteria, prioritizing the reliability of the final decision and reducing the likelihood of harmful false-positive predictions (see Section 6 for details). The training objective is to minimize a weighted cross-entropy loss as shown in Equation 1, where P(y|x; 0) is the predicted probability of the class y, and wy represents class-specific weights.\nLDM(0) = - E(x,y) ~ D [Wy log P(y | x; 0)] (1)\nBy utilizing the behavioral data described in Section 3.1, the DM is exposed to diverse scenarios reflective of inference-time conditions. This approach eliminates the reliance on external reward signals, commonly collected through human feedback, as the DM inherently models these features by learning directly from the dataset.\n3.3. Inference\nAt inference, the system features an iterative line of exchanges between the Reasoner (LLM) and the Discriminative Model (DM). While the LLM aims to answer the initial question, the DM assesses LLM reasoning and provides feedback signals by accepting or rejecting its responses."}, {"title": "4. Experiment", "content": "4.1. Environment Setup\nTasks and Datasets We evaluate our approach on two critical reasoning tasks: commonsense reasoning and knowledge-intensive reasoning. Commonsense reasoning tests the ability to navigate general, intuitive, everyday scenarios, while knowledge-intensive tasks entail the application of specialized factual knowledge to solve complex problems. These tasks embody essential real-world challenges that LLMs must address to demonstrate practical utility and adaptability. Specifically, we use CommonsenseQA (Talmor et al., 2019) and WinoGrande (Sakaguchi et al., 2020) for commonsense reasoning, and OpenBookQA (Mihaylov et al., 2018) and PIQA (Bisk et al., 2020) for knowledge-intensive reasoning. We use the QA pairs from the training sets to generate mid-step behavioral data for DM training. Due to the label availability, we evaluate the released test set for OpenBookQA and the validation sets for all other datasets. Notably, our method offers a scalable and cost-effective solution, leveraging the training of a lightweight classifier rather than the computationally expensive re-training of large generative models. Besides, our method does not require any optimization for individual datasets; that is, for all experiments (except the ablation study), we train one classifier for all four datasets. As a result, our method can be expanded to and efficiently deployed across a diverse range of reasoning tasks.\nEvaluation In addition to using accuracy as an evaluation metric, we contend that standard evaluation question-answering metrics may not adequately capture user satisfaction, as incorrect or misleading answers are often more disappointing than no answer at all. We advocate for an alternative utility function, formula score (FS) (Davis, 1967), inspired by scoring systems used in mathematical competitions (Mathematical Association of America, 2024), which assign negative scores to incorrect answers and no penalties for abstentions. This metric better reflects the objectives of a trustworthy QA system, emphasizing the importance of correct reasoning and appropriate decision-making on when to provide a final answer. Specifically, a correct answer earns 1 point, an incorrect answer results in a 1-point deduction, and choosing not to answer yields 0 points. The earned points percentage over all points will be the final score. We evaluate our system using both standard accuracy and formula score, allowing for a more balanced measure of the reliability of QA systems, particularly in cases where avoiding incorrect answers is critical.\nAdditionally, we report the critic-decision accuracy (Acc(D)), which measures the accuracy of a system's decisions to give an answer or abstain (i.e. how well it decides to abstain or not). For DRR, this refers to the DM accuracy at the last reasoning turn before an answer is given and is calculated by comparing the DM's accept or reject prediction to the accuracy of the LLM answer. For zero-shot methods, this refers to the percentage of LLM's successful answers and abstentions, when the answer it would have given had it not abstained is wrong."}, {"title": "5. Results and Analysis", "content": "The experiment results are reported in Table 1. Compared to zero-shot CoT baselines, our system demonstrates consistent improvements in both accuracy and formula score across all data sets for both Llama3 and GPT4, successfully empowering the LLM to self-correct and achieve more accurate answers by facilitating an iterative reasoning process. Furthermore, the relative increases in formula scores are significantly greater than the increases in accuracy, highlighting our system's ability to mitigate undesirable responses through deciding abstentions. In comparison, the abstain zero-shot consistently decreases performance, indicating LLMs do not know by themselves when to give an abstention. Performance improvements are more apparent in commonsense reasoning tasks, such as WinoGrande, than in knowledge-intensive reasoning tasks. Knowledge-intensive tasks often require factual knowledge, making LLMs more prone to guessing the same answer or becoming stuck after multiple iterations. This contrast demonstrates how our method excels in domains that benefit more directly from iterative reasoning.\nFor open-source models, such as Llama3, our system achieves significant relative improvements over supervised fine-tuning (SFT-LORA), providing a lightweight, interpretable, and widely applicable alternative. Unlike supervised fine-tuning, which offers limited scope (with no reasoning chain) for further enhancements in complex scenarios, our approach generates a mid-step thinking path that enables greater interpretability and potential for future improvements. Additionally, our method is applicable beyond open-source models, as demonstrated by its success with GPT-4, highlighting its adaptability to improve inference-time performance for API-accessed models across diverse settings without additional training. For closed-source models, such as GPT-4, our system outperforms self-critic baselines in all datasets in both accuracy and formula score, demonstrating the benefits of multi-turn reasoning using external feedback. This is particularly important in scenarios where additional training is infeasible, such as with closed-source API-accessed models. Furthermore, most self-critic scores decrease from zero-shot CoT, aligning with findings by Huang et al. (2023) on the limitations of LLM self-awareness.\nCritic Decision Accuracy Figure 3 illustrates the critic-decision accuracy of DRR compared to Abstain and Self-Critic baselines, demonstrating the DM's positive impact at inference time decision-making. As previously discussed, the critic's decision-making ability is inherently tied to the model's self-awareness of its knowledge boundaries. Using Llama3 as the Reasoner, DRR achieves an average accuracy of 81.43% across four datasets, significantly surpassing the Abstain baseline at 67.22%. For the GPT-4, while both (82.00%) and Self-Critic (82.28%) baselines exhibit much stronger capability on Acc(D) than Llama3-Abstain, DRR achieves the highest average accuracy at 87.48%, consistently outperforming both baselines across all datasets. By achieving better Acc(D) compared to the baselines where the LLM makes decisions independently, the use of the DM facilitates more accurate answers and meaningful abstentions during inference-time reasoning."}, {"title": "6. Discussion", "content": "Qualitative Examples Figure 4 displays various examples of the common behavior of our system at inference time. The first example shows optimal behavior, where the LLM responds incorrectly and with disjoint reasoning in its rationale on the first turn, the DM identifies this hallucination and rejects the response, and the LLM retries on the second turn, adequately correcting its answer and rationale, leading to an accepted correct answer. We also observe this behavior occurring flexibly across often more than two turns. The second example shows our system's ability to mitigate cases where LLM can never reach a correct answer. The DM consistently rejects faulty responses, where the LLM answer does not match an otherwise correct rationale (Turn 1), where the rationale contradicts itself (Turn 2), or where the rationales do not directly answer the question (subsequent turns), until the maximum turns is reached, leading to an abstention. This also highlights the importance of insights provided into abstention offered by the formula score introduced in Section 4.1. While these abstentions are counted as incorrect answers for the accuracy score, they mitigate the negative effects measured by the formula score. The third example shows the DM incorrectly rejecting an answer at the first turn and then correcting itself and accepting the correct answer at a later turn. This demonstrates another pattern of our system's capability to mitigate errors as it has the ability to correct itself both on the LLM and on the DM side. In fact, it can make the LLM reconsider its initial response and be more sure about the next time around, as shown by the more logical and well-explained rationale at the later turn. Furthermore, this shows the less weight put on DM's false negatives, making them less costly to system performance.\nGenerative vs. Discriminative Our system relies on a lightweight Discriminative Model (DM) to identify faulty reasoning steps rather than having a single generative model handle both answer production and error detection. We argue that discriminative decisions, such as binary classification of correctness, can be less complex than unrestricted text generation. The DM only needs to learn whether the LLM's intermediate output is acceptable or flawed, rather than produce the entire answer. In practice, as illustrated in Figure 4, spotting a hallucinated statement or a gap in reasoning is often more straightforward than generating a complete solution from scratch.\nThis observation aligns with findings in adversarial training for image generation: the discriminator in Generative Adversarial Networks (GANs) is frequently able to surpass the generator to the point of near-perfect classification, sometimes leading to mode collapse (Goodfellow et al., 2014; Arjovsky et al., 2017). Although our setting differs from image generation, this pattern suggests that a specialized classifier can outpace a generative model in identifying errors. Recent self-critique approaches have also drawn attention to how generative and discriminative strengths differ. For instance, Jiang et al. (2024) note that LLMs do not necessarily excel at discriminative tasks (such as self-correction) unless they are explicitly trained for them. In a similar vein, our DM focuses on binary classification, freeing the LLM to concentrate on generative functions.\nIn addition, smaller discriminative models provide practical advantages. They typically require fewer parameters and can be retrained quickly without modifying the main LLM. For example, Tyen et al. (2023) suggests that if an LLM can identify where its own errors occur, it can correct them. They use a similar strategy of training a small classifier to"}, {"title": "7. Ablation study", "content": "This section investigates two critical design factors for the Discriminative Model (DM): label weighting and model size. The results, shown in Table 2, reveal how these factors impact system performance.\nWeighted Training As discussed in Section 6, false-positive errors incur the highest cost. To mitigate this, following equation 1, the DM training applies label weights Wy that favor rejecting incorrect answers over mistakenly accepting them. Specifically, this study compares a 3:1 ratio (Reject to Accept) with the unweighted 1:1 setting. Results in Table 2 show that weighted training offers a slight but consistent improvement in all evaluated metrics.\nDM Size Flan-T5-Large-783M was selected for its strong performance in preliminary tests. Table 2 compares this baseline with a larger DM, Flan-T5-XL-2.85B, on the Open-BookQA dataset using Llama3. While the larger model can offer an additional performance boost, the smaller DM achieves competent results, making billion-scale parameters less essential in many practical scenarios."}, {"title": "8. Conclusion", "content": "This work addresses the pressing challenge of diminishing fresh training data for large language models, which need ever-increasing high-quality inputs to refine their advanced capabilities. We proposed Distillation-Reinforcement-Reasoning (DRR), a framework that enhances LLM's inference-time reasoning via in-context reinforcement learning supervised by a small discriminative model, which is trained from synthetic data generated from the LLM through a novel reasoning process distillation algorithm. Empirical results on multiple standard QA benchmarks demonstrate the effectiveness of this framework over self-critique approaches. We expect this low-cost and scalable solution to provide a practical path for large language models to continue improving complex reasoning abilities in accordance with scaling laws."}, {"title": "B. Prompts", "content": "Zero-Shot Baselines For zero-shot baselines, we designed three types of prompts: (a) Standard QA Prompt: a straightforward question-answering prompt; (b) Abstain QA Prompt: an extended prompt allowing the model to abstain from answering if none of the options are correct; and (c) Self-Critic Prompt: a prompt adapted from Huang et al. (2023) where the model critiques its own response and iteratively improves it. Zero-shot predictions are generated by evaluating the first turn of our system, ignoring DM outputs. Abstain zero-shot predictions are run separately. Note that the self-critic prompt directly from Huang et al. (2023) may produce discrepancies, such as self-critic scores being lower than zero-shot scores using our system prompts.\nDRR Prompt Settings DRR employs two distinct prompt strategies. Version 1 uses a direct instruction-based approach. The first turn uses the Standard QA Prompt as a system prefix, mimicking zero-shot performance. Subsequent turns replace the standard QA prompt with a Exploration Prompt, instructing the LLM to explore new reasoning paths based on feedback such as \"Wrong Answer! Try again.\". Temperature settings are 0.1 (Top-P 0.9) for the first turn to ensure consistent zero-shot responses and 0.6 (Top-P 0.7) for later turns to increase diversity. Version 2 adopts a gradual prompting strategy. All turns use the Standard QA Prompt as a system prefix. The appended context includes an environment message that emphasizes gradual improvement and encourages exploring new reasoning paths without explicitly labeling past answers as wrong. Temperature settings are 0.6 (Top-P 0.9) for all turns. For a given experiment, the same strategy is used for both the generation and inference steps. Version 1 is used for Llama models, while Version 2 is applied to GPT models.\nPrompt Examples Examples of each prompt are displayed as following."}]}