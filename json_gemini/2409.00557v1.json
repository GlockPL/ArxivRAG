{"title": "Learning to Ask: When LLMs Meet Unclear Instruction", "authors": ["Wenxuan Wang", "Juluan Shi", "Chaozheng Wang", "Cheryl Lee", "Youliang Yuan", "Jen-tse Huang", "Michael R. Lyu"], "abstract": "Equipped with the capability to call functions, modern large language models (LLMs) can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLMS tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench (Noisy ToolBench). We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLMs' performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the AwN significantly outperforms existing frameworks for tool learning in the Noisy ToolBench. We will release all related code and datasets to support future research.", "sections": [{"title": "Introduction", "content": "LLMs have undergone remarkable development since OpenAI introduced ChatGPT-3.5 (Bang et al. 2023). This model demonstrates a significant advancement in solving multiple tasks, including code generation (Dong et al. 2023; Sakib, Khan, and Karim 2023; Feng et al. 2023), machine translation (Jiao et al. 2023; Peng et al. 2023), even game playing (Wu et al. 2024). However, despite their impressive capabilities, LLMs often struggle with complex computations and delivering accurate, timely information (Qu et al. 2024). Tool learning emerges as a promising solution to mitigate these limitations of LLMs by enabling dynamic interaction with external tools (Schick et al. 2024).\nThe incorporation of tool usage capabilities marks a pivotal step towards enhancing the intelligence of LLMs, pushing them closer to exhibiting human-like intelligence. The integration of tool usage allows LLMs to perform a broader array of complex and varied tasks, including managing emails, designing presentations, and browsing the web to gather real-time information. For example, LLMs can perform complex calculations using a calculator tool, access real-time weather updates through weather APIs, and execute programming code via interpreters (Qin et al. 2023a; Schick et al. 2024; Mialon et al. 2023; Yang et al. 2023a).\nToolformer (Schick et al. 2024) is a pioneering work in empowering language models with self-learning capabilities for tool usage. This groundbreaking approach integrates various tools, including calculators, Q&A systems, and search engines. Then, significant research efforts have been directed toward accessing a wider variety of tools or using multiple tools simultaneously to resolve a single query, such as Gorilla(Patil et al. 2023), RestGPT (Song et al. 2023) and ToolLLM (Qin et al. 2023b).\nDespite the significant strides made, existing frameworks and benchmarks often operate under the assumption that user instructions are always explicit and unambiguous, a premise that diverges from real-world scenarios (Qin et al. 2023a; Song et al. 2023; Patil et al. 2023). Due to the feature of API calls, it requires precise user instructions since the arguments for the function call can hardly tolerate ambiguity. We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks (as the example shown in Figure 1a). Furthermore, as the tasks assigned to LLMs grow in complexity, multiple and sequential API calls are needed to resolve a single task. This complexity amplifies the challenge, as any error in the sequence of API calls can culminate in an outcome that strays from the user's original intention. Hence, LLMs tool-use under unclear instruction is an important but rarely investigated direction.\nTo address this oversight, we conduct a systematic analysis of actual user instructions, identifying and categorizing potential issues into several key areas. These include instructions lacking essential information, instructions with ambiguous references, instructions containing inaccuracies, and instructions that are unfeasible for LLMs to execute due to the limitations of the tools available. Building on this observation, we meticulously design a noisy instruction benchmark for tool use, named NoisyToolBench. This benchmark includes a collection of provided APIs, ambiguous queries, anticipated questions for clarification, and the corresponding responses. Its primary goal is to assess the capability of LLMs to detect ambiguities in user queries and to pose relevant questions for clarification accordingly.\nTo improve the performance of LLMs tool-use under unclear instructions, we propose a novel framework called Ask-when-Needed (AwN). Our key insight is encouraging LLMs to proactively ask questions to seek clarifications from users when uncertainties arise during instruction execution. By facilitating dialogue throughout the process, our method aims to ensure the accurate invocation of functions (See Figure 1b)\nTo evaluate the performance of LLMs tool-use under unclear instruction, we design several innovative metrics from the accuracy and efficiency perspectives. For accuracy, we measure the LLMs' proficiency in asking appropriate clarifying questions, their ability to execute the correct function calls, and their success in delivering final responses that meet the users' needs. For efficiency, we calculate the average number of redundant asked questions and the average number of actions required to complete the instruction. An ideal LLM should achieve higher accuracy with fewer number of queries. Recognizing the labour-intensive nature of manually communicating with LLMs and verifying all execution results, we also innovatively design an automatic evaluation system, ToolEvaluator, to streamline the whole process. ToolEvaluator leverages the advanced problem-solving capabilities of GPT-4 to communicate with LLMs and automatically evaluate the performance of LLMs' tool-using under unclear instruction. Our experiments demonstrate that the AwN significantly outperforms existing baseline methods for tool learning in the Noisy ToolBench.\nThe key contributions of this research are summarized as follows:\n\u2022 We conduct a systematic examination of the real-world user instruction for tool utilization and categorize the prevalent issues into four distinct categories.\n\u2022 We create and release a novel benchmark, NoisyToolBench, which can be used to evaluate the performance of LLMs' tool-using under imperfect user instruction.\n\u2022 We design five evaluation metrics from both accuracy and efficiency perspectives and introduce an automatic evaluation system, ToolEvaluator, that can proxy humans to interact with and assess LLMs.\n\u2022 We introduce a novel algorithm, named AwN method, to prompt LLMs to actively ask questions to request clarifications from users when facing uncertainties. Experimental results show that AwN can significantly improve the LLMs' tool-using under unclear instructions."}, {"title": "Related Works", "content": "Tool Learning for LLMs. LLMs have recently made significant advancements, with ChatGPT being recognized as a major step towards achieving AGI (Wu et al. 2023; Lund and Wang 2023; Jiao et al. 2023). These LLMs possess strong reasoning capabilities, enabling them to perform increasingly complex tasks (Liu et al. 2023). However, to progress further towards AGI, it is crucial for LLMs to master the utilization of tools. Toolformer is the first innovative AI model designed to use several specialized tools, such as a web browser, a code interpreter, and a language translator, within a single framework (Schick et al. 2023). The model's ability to seamlessly switch between these tools and apply them contextually represents a significant advancement in AI capabilities. Recent studies like RestGPT (Song et al. 2023) and ToolLLM (Qin et al. 2023b), have connected LLMs with real-life Application Programming Interfaces (APIs), allowing them to sequentially employ multiple external tools to solve user queries. The tool-augmented approach empowers LLMs to use various kinds of tools to do more sophisticated tasks, showcasing an enhanced level of capability compared to pure LLMs. Besides, API-Bank (Li et al. 2023), ToolAlpaca (Tang et al. 2023), ToolBench (Qin et al. 2023b), ToolQA (Zhuang et al. 2023) and RestBench (Song et al. 2023) are exemplary benchmarks to systematically evaluate the performance of tool-augmented LLMs performance in response to user's queries. However, current models often ignore the situations in which users might not give exact instructions, which can result in the tools not working properly. Thus, our study aims to tackle this specific challenge by developing a new benchmark specifically for ambiguous instructions.\nPrompting LLMs for Decision Making. In certain situations, addressing user queries may require more than a single API call. This necessitates the effective division of the overarching task into smaller, more manageable components, which presents a significant challenge. Prior research has focused extensively on enhancing LLMs's ability to effectively plan and execute complex tasks. The 'Chain of Thought' prompting approach facilitates advanced reasoning by introducing intermediate steps in the reasoning process (Wei et al. 2022). The ReAct methodology improves the integration of reasoning and action, enabling LLMs to take informed actions based on environmental feedback (Yao et al. 2022). Meanwhile, Reflexion is designed to reduce errors in the reasoning process by revisiting and learning from previous mistakes (Shinn, Labash, and Gopinath 2023). DFSDT expands upon Reflexion, allowing LLMs to evaluate various options and choose the most viable path (Qin et al. 2023b). In our work, we innovatively involve users in the process of executing instructions. Our approach, referred to as AwN, motivates LLMs to consider the necessity of requesting further information from users during each tool invocation round. This strategy aims at clarifying users' ambiguous instructions to help execute the tasks in alignment with the users' intentions.\nLearning to Ask. Since user queries may not always be clear, and the execution of LLMs may encounter uncertainties and ambiguities, learning to ask questions has emerged as a challenging yet crucial research area. Some researchers introduce a learning framework that empowers an embodied visual navigation agent to proactively seek assistance(Zhang et al. 2023). Recently, similar ideas have been adopted in the software engineering, leveraging a communicator to enhance the reliability and quality of generated code (Wu 2023). Our work focuses on the tool-learning scenario, which is more sensitive to the user's unclear query. A concurrent study (Qian et al. 2024) also focuses on the reliability of tool-learning systems under unclear instruction. However, they did not systematically examine real-world user behavior, leading to the limited and biased nature of their dataset that doesn't accurately capture user errors. Our research addresses this shortfall by starting with a user analysis. Additionally, Qian's methodology depends significantly on human manual interaction and assessment of LLM performances, which is time-consuming and hard to reproduce. In contrast, we introduce an automated evaluation method that can proxy humans to communicate with and automatically evaluate the performance of LLMs."}, {"title": "Noisy ToolBench", "content": "Several tool-learning benchmarks have been introduced to assess LLMs' ability in tool utilization. However, these benchmarks overlook the potential ambiguity in users' instruction, which might hinder LLMs from executing tasks as intended by the user. For instance, as depicted in Figure 1a, if a user inquires, \"How is today's weather\" without specifying the location, LLMs cannot accurately activate the APIS to fetch the correct weather information. This scenario underscores the critical role of interaction between users and LLMs in executing instructions accurately. However, previous tool-learning benchmarks only contain perfect user instruction in a one-query-one-execution manner.\nTo create a realistic benchmark for ambiguous instructions, the initial step involves a systematic examination of the common errors in user instructions that could complicate correct execution by LLMs. Therefore, we first collect real-world user instructions that are problematic. Then, we classify these instructions into various categories based on their characteristics. Lastly, we manually create our dataset, ensuring it reflects the distribution of errors found in the real-world user instructions.\nUser Instruction Analysis\nTo analyze the issues in real-world user instruction, we recruit human annotators to write user queries according to the API provided. Firstly, we select 100 APIs from the ToolBench (Qin et al. 2023b), containing real-world RESTful APIs spanning 49 categories, ranging from sports to finance. Secondly, we hire 10 volunteers, who have a Bachelor's degree, are proficient in English, and have experience using LLMs. We provide them with the 100 APIs, and then ask them to write down an instruction to prompt LLMs to call each API, ending up with 1000 user queries. Finally, we manually identify the problematic user queries and categorized them as follows:\n\u2022 Instructions Missing Key Information (IMKI): These are user instructions that omit crucial details necessary for the successful execution of a function. An example of IMKI would be, \"Set an alarm to wake me up\" without providing a specific time. Asking for more information is needed when encountering this issue.\n\u2022 Instructions with Multiple References (IMR): These user instructions include elements that can be interpreted in several ways, potentially leading to confusion for LLMs in understanding the user's actual intent. For example, an IMR instance is \"I want to know the director of the movie 'The Matrix',\" where the ambiguity arises because there are multiple versions of 'The Matrix', each possibly having a different director. This issue is similar to IMKI but is more subtle and difficult to detect. Pointing out potential references and asking for clarification are needed when encountering this issue.\n\u2022 Instructions with Errors (IwE): This category consists of user instructions that contain the necessary information for executing a function, but the information is incorrect. An example of IWE is, \"Please help me to log in to my Twitter. My user account is 'abcde@gmail.com' and the password is '123456',\" where the user might have provided the wrong account details or password due to typographical errors. Asking for the correct information is needed when encountering this issue.\n\u2022 Instructions Beyond Tool Capabilities (IBTC): These are user instructions that request actions or answers beyond what LLMs can achieve with the available APIs. In such cases, the existing tool-augmented LLM frameworks might randomly choose an available API, leading to an incorrect function call. This scenario highlights the need for LLMs to recognize their limitations in tool usage. Telling the user that the query is beyond the capabilities and refusing to generate API calls are needed when encountering this issue."}, {"title": "Ask-when-Needed Prompting", "content": "Previous approaches to tool-using often overlooked the importance of user engagement during the reasoning and planning stages. To address this oversight, we introduce a new prompting strategy named Ask-when-Needed (AwN). The key insight is prompting LLMs to detect the potential flaws in user instructions and proactively seek clarifications by asking questions before generating the API call.\nYou are AutoGPT, tasked with processing\nuser requests through a variety of APIS\nyou have access to. Sometimes, the\ninformation provided by users may be\nunclear, incomplete, or incorrect. Your\nmain responsibility is to determine\nif the user's instructions are\nsufficiently clear and detailed for\neffective use of the APIs. Here's your\nstrategy:\n1. If user instructions are missing\ncrucial details for the APIs, pose\na question to obtain the necessary\ninformation.\n2. If the user's instructions appear\nto be incorrect, delve deeper by asking\nquestions to clarify and rectify the\ndetails.\n3. If the user's request falls outside\nthe capabilities of your current APIs,\nnotify them that you're unable to meet\nthe request by stating: \"Due to the\nlimitation of the toolset, I cannot\nsolve the question\u201d.\nAwN is built upon widely-used tool-using methods, such as CoT and ReAct. As illustrated in Figure 2, we introduce an additional step before the generation of API calls. This step involves presenting all available information to the LLMs, including the user query and API guideline, and prompting them to determine the adequacy and correctness of user instruction. If LLMs identify any missing argument needed for function execution based on the API's requirements, they are encouraged to ask questions to the user for this information. AwN prompts LLMs not to generate any API call until obtaining all the necessary information. In other words, only if no further information is needed, they can bypass the query step and directly initiate the API call. We also provide various kinds of specific instructions and demonstration examples for different kinds of instruction issues. All the details can be found in the Appendix due to the space limitation."}, {"title": "Experiments", "content": "In this section, we evaluate the performance of our Ask-when-Needed (AwN) prompting technique on the NoisyToolBench dataset. We first introduce the evaluation metrics, where we specify the criteria used to assess the effectiveness of AwN. Following that, we describe the evaluation pipeline, detailing the step-by-step process employed to measure AwN's performance. Lastly, we discuss the main experiments, presenting the results and findings from our comprehensive testing of the AwN technique.\nEvaluation Metrics\nWe evaluate the performance of LLMs' tool-using under unclear instructions from two perspectives: accuracy and efficiency. The accuracy assessment aims to measure the LLMs' capability to make correct decisions during the instruction execution phase and to generate accurate final answers. In contrast, the efficiency assessment focuses on the number of redundant decisions made by the LLMs, considering that unnecessary communication could lead to a waste of processing time. Specifically, we design the following five metrics:\n\u2022 Accuracy 1 (A1). Al evaluates the capability of LLMs to ask the anticipated questions that pinpoint the ambiguous elements in user instructions. Al is considered a success if the LLMs manage to ask the correct questions at any point. Conversely, it is deemed a failure if they do not.\n\u2022 Accuracy 2 (A2). A2 assesses the ability of LLMs to use all available information to invoke the correct API calls. It is deemed a success if the LLMs call all the anticipated APIs with the correct arguments. If they fail to do so, it is considered a failure.\n\u2022 Accuracy 3 (A3). A3 measures the ability of LLMs to extract the anticipated information from previous API calls to fulfill the user's instructions. This is achieved and considered a success if the user's instructions are successfully executed. If not, it is regarded as a failure.\n\u2022 Average Redundant Asked questions (Re). This metric evaluates the quantity of irrelevant or redundant questions asked by LLMs during the instruction process. Irrelevant questions are those that do not meet the initial expectations of the query, and redundant questions include those that are repetitive or have previously been asked. This metric is crucial for assessing the LLMs' ability to precisely identify the ambiguous aspects of user instructions and to formulate appropriate questions to clarify these uncertainties. The larger the value, the worse the performance.\n\u2022 Steps. Steps quantifies the average number of actions required to complete an instruction, including inference generation, asking questions, and conducting API calls. A smaller number indicates fewer unnecessary steps in the instruction execution process, signifying a more efficient and direct approach to accomplishing the task.\nAuto-Evaluation Pipeline\nTo assess how LLMs perform under unclear instructions, interacting with LLMs and making assessments are needed. Previous work employs individuals to interact with and evaluate LLMs throughout the entire evaluation process, which is inefficient and not reproducible. To address this, we design an automated evaluation method named ToolEvaluator to proxy this process. ToolEvaluator can automatically interact with LLMs and assess their performances.\nAuto-Interaction ToolEvaluator can proxy the user's communication with LLMs. When LLMs post a question, ToolEvaluator calculates the semantic similarity between the asked question and the expected question by the sentence-transformer (Reimers and Gurevych 2019). If the similarity is higher than a threshold, ToolEvaluator replies with the predefined answer to the LLMs. Otherwise, this question is treated as an irrelevant question and ToolEvaluator replies with a standard reply of \"Sorry, I cannot provide additional information about this.\". This approach streamlines the evaluation process by reducing the need for human interaction with LLMs, as illustrated in Figure 3.\nAuto-Assessment ToolEvaluator can also automatically assess how well LLMs perform under ambiguous instructions according to the five metrics introduced above. Al measures whether LLMs can ask the right question. ToolEvaluator calculates the semantic similarity between the LLMs-asked question and the expected question to asses A1. A2 measures whether LLMs can conduct correct API calls. Following the previous works (Yang et al. 2023b; Chiang and yi Lee 2023; Wang et al. 2023; Yuan et al. 2023), ToolEvaluator adopts GPT-4 as a judge to identify whether the generated API calls are the same as the expected API calls. A3 measures whether LLMs can correctly generate the final answer. ToolEvaluator adopts GPT-4 as a judge to identify whether the final answer aligns with the user's intent. For measuring the efficiency, ToolEvaluator counts the number of generated irrelevant questions as Re and counts the total number of actions during the process as Steps. All the details can be found in the Appendix due to the space limitation.\nThe Effectiveness of ToolEvaluator\nSince ToolEvaluator is an automatic evaluation method, the evaluation can be inaccurate due to the imperfect nature of AI techniques, such as sentence transformer or GPT-4 as the judge. In this section, we conduct a human annotation to validate the effectiveness of ToolEvaluator. Specifically, we randomly select 50 cases and ask annotators to assess the accuracy and efficiency, according to the evaluation metrics mentioned above. Then we compare the assessment results from ToolEvaluator and human annotators. ToolEvaluator achieves 90% accuracy, indicating its effectiveness."}, {"title": "Experimental Setup", "content": "We evaluated the performance of AwN against two baseline methods, chain-of-thought (CoT) (Wei et al. 2022) and depth-first search-based decision tree (DFSDT) (Qin et al. 2023b), which are two widely-used tool-learning methods. All the experiments are conducted with two LLMs as engines, gpt-3.5-turbo-0613 and gpt-4-0125, using the default setting. The baseline prompting methods did not integrate additional user-LLM communication during the whole execution process, leading to inevitable failures under unclear instructions. Therefore, their performance in A2 and A3 are not measured. And since an ideal reaction under Instructions Beyond Tool Capabilities (IBTC) is telling the user that the query is beyond the capabilities and refusing to generate API calls, its performance in A2 and A3 are measured neither."}, {"title": "Main Result", "content": "We evaluate the performance of AwN as well as the baseline methods on our NoisyToolBench dataset. The accuracy-related results are shown in in Table 2 and the efficiency-related results are shown in Table 3.\nAwN enhances the capability of gpt-3.5-turbo to ask pertinent questions across different issues. For example, as is shown in Table 2, AwN improved the A1 scores from 0.34 to 0.86, from 0.24 to 0.76, from 0.21 to 0.58, and from 0.00 to 0.08 for gpt-3.5-turbo-based CoT as well as from 0.53 to 0.84, from 0.42 to 0.88, from 0.42 to 0.86 and from 0.00 to 0.13 for gpt-3.5-turbo-based DFSDT.\nHowever, this improvement comes with a side effect: gpt-3.5-turbo tends to ask more irrelevant or redundant questions, as indicated by the higher Re scores in Table 3. Especially in gpt-3-turbo-based DFSDT, where the average number of redundant questions is 10.9, 11.54, and 15.3. This suggests that while the AwN aids in identifying and addressing ambiguities in user instructions, it also leads to a less efficient querying process\nAwN improves gpt-4 effectively and efficiently. When gpt-4 is enhanced with the AwN method, it also demonstrates a strong capability of asking relevant questions to address ambiguity in instructions. This is evidenced by the increasing Al scores in Table 2, from 0.68 to 0.84, from 0.58 to 0.76, from 0.35 to 0.42 and from 0.29 to 0.98 for CoT, as well as the increase from 0.64 to 0.8, from 0.56 to 0.64, from 0.37 to 0.51 and 0.63 to 0.92.\nBesides, AwN can improve gpt-4 without generating excessive unnecessary questions. As is shown in Table 3, AwN only generates 0.18, 0.2, and 0.23 redundant questions for CoT, as well as 0.34, 0.36, and 0.37 redundant questions for DFSDT. Compared with gpt-3.5-turbo, gpt-4 demonstrates a stronger ability to avoid asking unnecessary questions while effectively identifying and addressing ambiguities in user instructions.\nAsking the right question leads to the better generation and execution of API calls. Besides the significant improvements on A1, AwN also achieves considerable performance in generating correct API calls (A2) and returning the expected final answer (A3). For example, gpt-4-based DFSDT can achieve 0.46, 0.36, and 0.40 accuracy on A3 with the help of AwN method.\nImpact of AwN on the average number of steps. The average number of steps measures the cost of LLMs' tool-using. As is shown in Table 3, adopting AwN will enviably increase the number of actions. However, considering the significant performance improvements achieved, the moderate increase in cost is justifiable and worthwhile."}, {"title": "Conclusion", "content": "This study delves into the critical issue of unclear user instructions and their impact on the effective use of tools by modern LLMs. Recognizing the limitations of LLMs in dealing with ambiguous instructions, we conducted a thorough investigation into the common error patterns present in real-world user instructions. Based on our analysis, we introduced the NoisyToolBench dataset (NoisyToolBench), a novel tool-using benchmark aimed to evaluate the LLM's tool-using performance under unclear user instructions. Furthermore, we developed the Ask-when-Needed method (AwN), an innovative approach that empowers LLMs to actively seek user input whenever they face uncertainty in instructions. To automate the evaluation process of LLMs in tool usage, we also present an automated evaluation tool (ToolEvaluator), which can proxy human users to interact with LLMs and assess LLMs' from both accuracy and efficiency perspectives. Our experimental results show that the AwN algorithm markedly surpasses existing methods in the Noisy ToolBench dataset and significantly improves the performance of LLMs' tool-using under unclear user instructions.\nLimitations\nThis paper has two limitations:\n1. Although AwN can improve the performance, there is still a big gap to perfect. We hope that this work can serve as the first stepping stone, inspiring future researchers to delve deeper into this field of study.\n2. The automatic evaluation process is not 100% accurate, leading to some potential false negatives and false positives. In the future, more efforts are needed to build a more reliable auto-evaluation method."}]}