{"title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison", "authors": ["Judy Hanwen Shen", "Archit Sharma", "Jun Qin"], "abstract": "The goal of aligning language models to human preferences requires data that reveal these preferences. Ideally, time and money can be spent carefully collecting and tailoring bespoke preference data to each downstream application. However, in practice, a select few publicly available preference datasets are often used to train reward models for reinforcement learning from human feedback (RLHF). While new preference datasets are being introduced with increasing frequency, there are currently no existing efforts to measure and compare these datasets. In this paper, we systematically study preference datasets through three perspectives: scale, label noise, and information content. We propose specific metrics for each of these perspectives and uncover different axes of comparison for a better understanding of preference datasets. Our work is a first step towards a data-centric approach to alignment by providing perspectives that aid in training efficiency and iterative data collection for RLHF.", "sections": [{"title": "Introduction", "content": "Reinforcement learning from human feedback (RLHF) is typically the final stage of the modern large language model (LLM) training pipeline [1, 2, 3]. The reward models necessary for RLHF algorithms are predominantly trained from datasets of pairwise preferences [4, 5]. While a substantial number of works have focused on new algorithms for learning from preference data to better train reward models [6, 7, 8, 9], relatively few works have examined qualities of these datasets themselves. At the very minimum, all of these pairwise datasets of human preferences contain examples with 1) a prompt, 2) two responses, and 3) an annotation of which response is preferred. Beyond this basic structure, preference datasets vary widely in domain (e.g. code, chat, QA, etc.), generation process (e.g. synthetic vs human), collection procedure (e.g. annotation, prompt generation), and even size (e.g. 10k - 300k examples [7, 10]).\nIdeally, a custom preference dataset for each specific application can be collected, and carefully labeled by multiple annotators for reward model training. New technical reports that accompany state-of-the-art language models highlight the importance of preference data quality yet give little to no details about the preference datasets used [11, 12]. Among publicly available preference datasets, there is folk wisdom that more carefully curated datasets are better, yet no rigorous study or methodology for comparing these datasets exists beyond summary statistics, such as token count [8]. Today, little is known about when and why one preference dataset may be better than another, nor what \"better\" can mean in the context of these datasets."}, {"title": "Related Work", "content": "Data-Centric Methods Scaling laws introduced to describe the relationship between parameters, data, and compute for pre-training have been widely accepted as the explanation for why larger models and more data are better for language model training [13, 14]. Different approaches for improving data quality and composition have been proposed as efficient alternatives for indiscriminately training on all available data [15, 16]. However, the scale of pre-training data vastly eclipses the scale of data used in the fine-tuning and RLHF stages. Data quality and data selection for reward model training may be more similar to supervised learning settings than language modeling. In supervised learning and supervised fine-tuning, careful data selection and pruning have been shown to lower the number of samples required [17, 18, 19]. However, reward models do differ from the supervised learning setting since they are adapted from these pre-trained base models. Recent work has studied data scaling for fine-tuning LLMs to find that LLM performance benefits more from pre-training data scaling than fine-tuning data scaling and the optimal fine-tuning method is task and data-dependent [20].\nPublicly Available Preference Datasets For RLHF preference datasets in particular, early works collected datasets on the order of tens of thousands of examples for reward model training. For example, for a summarization task Stienon et al., [21] collected 64k preference pairs based on Reddit prompts, while the WebGPT [22] reward model was trained with 16k preference pairs based on prompts from existing QA datasets. Subsequent datasets follow a more general human-assistant format while being much larger (e.g. OpenAssistant [23], HH-RLHF [4], Stanford Human Preferences [24]). However, these datasets vary drastically in collection procedure. For example, for InstructGPT and HH-RLHF humans were asked to rank model-generated responses while for OpenAssistant and Stanford Human Preferences preferences for different human-generated responses were gathered. More recently, preference datasets where both responses and rankings are synthetically generated have gained popularity [10, 25]. These synthetically constructed datasets offers more training samples and more diversity in terms of the topics generated. There is also a movement back to creating smaller but carefully annotated preferences, often with multiple annotators [26]. Despite the large variation in practices for generating these different datasets, there has been little comparison and characterization of how different datasets affect reward model training.\nChallenges of Reward Modeling and Learning from Human Preferences Defining data quality is complex for preference data since many different tasks may use the same reward model for RLHF. There are concerns with the representativeness of preferences as well as the alignment between collected data and the intended objective [27, 28, 29]. One suggestion for measuring the effectiveness of reward models is standardized benchmarks on reward model performance on a variety of common tasks [30]. This approach measures the generalization of a single reward model on different tasks by testing how well each reward model performs on scoring the chosen response higher. The top-performing models on this benchmark leaderboard include models of a variety of sizes from 8B to 340B parameters and a variety of preference data sizes from 10k to more than 700k examples. Given this mishmash of different approaches, it is important to understand how to measure preference data quality for the reward modeling step of RLHF. This work aims to characterize the elements of preference data quality that inform practical decisions around data generation, annotation, and usage in this setting."}, {"title": "Model Agnostic Data Metrics", "content": "3.1 Preliminaries\nLet x be the prompt, yw be the winning (chosen) response, and y\u0131 be the losing (rejected) response. Let D = {(x, yw, Yl)i}=1 ~ D be the dataset of preferences that we will study. Let r : X \u00d7 Y \u2192 R be the reward model that maps a (x, y) prompt response pair to a score. In reward modeling, we want to compare the rewards of two given generations. The Bradley-Terry model defines Yij as a Bernoulli random variable representing the outcome of whether the completion yi is preferred or wins over the completion yj. Under this model, Yij ~ Bernoulli(pij) and the log ratio of the probability that Yi wins over yj is:\n$\\log \\frac{P_{ij}}{1-P_{ij}} = r(x, y_i) - r(x, y_j)$.\nIf we let y\u2081 be the winning completion Yw and y; be the losing completion y\u0131, we can then write the probability of the reward model preferring yw as:\n$P(y_w > y_l) = \\frac{\\exp(r(x, y_w))}{\\exp(r(x, y_l)) + \\exp(r(x, y_w))}$.\nFollowing prior work [5, 4], the probability of the reward model giving a higher score to the chosen response can then be maximized directly through the following objective function:\n$L = -E_{(x, y_w, y_l) \\sim D} [\\log\\sigma(r(x, y_w) - r(x, y_l))]$.\n3.2 Datasets and Models\nWe examine four publicly available preference datasets in our study: Anthropic Helpful-Harmless (HH-RLHF) [4], Ultrafeedback (ULTRAFEEDBACK) [10], LMSYS Arena Preferences (LMSYS) [31], and PKU-SafeRLHF (SAFERLHF) [32]. These datasets are selected based on their frequent use in prior works [4, 8]. For each dataset, we examine their behavior on reward models trained from pre-trained models of different sizes: 350 million (Opt-350m [33]), 1 billion (TinyLlama-1B-3T [34]), and 7 billion parameters (Llama2-7B and Llama2-7B-Chat [2]). We focus predominately on reward models trained from base models but also include ablations with fine-tuned versions since the practice around reward model training varies. For example, some papers train reward models from checkpoints already fine-tuned with instructions and human feedback (e.g. Llama3-8B-Instruct) [8]) and other works train reward models directly from based models [26, 7]. Notably, Ouyang et. al. [5] remark that similar reward model quality was observed between training on a base 6B model and an instruction-tuned 6B model. We evaluate both in-domain and generalization performance through evaluation set accuracy and Rewardbench [30] respectively."}, {"title": "Experiments", "content": "4.1 Scaling: Are larger preference datasets better?\nThe first perspective we examine is the role of dataset size for different preference datasets. Unlike scaling laws for pre-training, there is no consensus about how large a preference dataset should be to train a good reward model. For summarization in particular, Stienon et. al. [21] estimate that doubling their particular dataset size leads to a 1.1% increase in reward model validation accuracy until 65k examples. In contrast, others have found that even when using 2.9 million examples, reward model accuracy continues to improve [2]. While these differences can be blamed on the dataset composition, the impact of increasing the training set size across different datasets has not been studied. We examine four datasets that range in size from 30k examples to 200k examples and observe how training dataset size impacts performance. Figure 1 illustrates the impact of scaling on evaluation set accuracy. For all datasets, the larger models (Llama2-7B, Llama2-7B-chat), gain less from doubling the dataset size. While Llama2-7B-chat is fine-tuned with RLHF from part of HH-RLHF, this pattern remains even for other datasets that were released after Llama2-7B-chat. Among datasets, SAFERLHF has the highest average gain per doubling of the training dataset (2.4-4.7%) for all models."}, {"title": "Noise Invariance: How robust are reward models to label noise?", "content": "Prior works have reported the human agreement with the collected preferences to be 76% for summarization [21] and 73% inter-annotator agreement for response quality for general instruction tasks [5]. Ideally, annotator disagreement serves as a filter for low-quality preference data, however, even if the collection process is unknown, it is still useful to understand how much noise there might be in the preference dataset. In image classification tasks, neural networks are robust to label noise [35]. In these settings, a random label is used instead of the true label in multiclass classification. In the context of preference data, we can model label noise as the flipping of the chosen response with the rejected response. We can define: p as the noise rate and add random label noise by constructing a dataset:\n$\nf(x, y_l, y_w) = \\begin{cases}\n(x, y_w, y_l) & \\text{w.p. } p \\\\\n(x, y_l, y_w) & \\text{w.p. } 1 - p.\n\\end{cases}$\nTable 1 shows the percentage of the peak evaluation set accuracy achieved when 30% of labels are flipped. Overall, we find that reward model performance remains unaffected by label flipping until"}, {"title": "Explaining Noise Invariance: The Role of Noise in Reward Model Confidence", "content": "We can look at the underlying prediction probabilities to further understand why introducing label noise does not significantly affect performance both on the evaluation set and the RewardBench tasks. Since accuracy for both sets of metrics is calculated through expected binary outcomes (i.e. E(x,yw,y1)~D [1[\u0177 = yc]] where \u0177 = arg maxy\u2208 {yw,y\u0131 } r(x,y)), we can use the Bradley-Terry model to calculate P(yw > y\u0131) and investigate how these distributions change. As the noise rate increases, the distribution of probabilities (e.g. P(yw \u27a4 y\u0131)) becomes more concentrated around 0.5 (Figure 4). This pattern is consistent across different reward model sizes and datasets. Across different datasets, Figure 3 shows that when label noise is introduced, HH-RLHF and LMSYS collapses quicker to P(yw > y\u0131) \u2248 0.5 than other datasets. This suggests that there might be a higher level of baseline noise in the HH-RLHF labels that results in more uncertain predictions. This pattern is again consistent across different reward model sizes.\nTo precisely characterize model confidence, we can measure the expected calibration error (ECE) of reward model outputs [36]. However, in the Bradly-Terry model, using P(yw \u27a4 y\u0131) directly as model confidence results in perfect accuracy when P(Yw > y\u0131) > 0.5. The only prior work we could find that measures calibration in reward models uses max{P(yw > y\u0131), P(y\u0131 > Yw)} as the confidence of the model [37]. To properly measure calibration, we can write each evaluation pair as (x, Y1, Y2, z) and split it into (x, yw, Y\u0131, z = 1) and (x, y\u0131, Yw, z = 0). Then to calculate the calibration error we can use P(z = 1) := P(y1 > y2) as model confidence and plot the count of z = 1 as the accuracy (see Figure 6). The overall ECE is equivalent to the max method from prior work but now we have confidence values in the entire interval of [0, 1] instead of just [0.5, 1]. As label noise increases,"}, {"title": "Information Content: Are high contrast responses necessary for reward model learning?", "content": "A major dichotomy in how preference datasets are generated is whether the responses are human-written or sampled from large language models. For example, the Anthropic Helpful-Harmless (HH-RLHF) dataset contains response pairs generated from responses from LLMs of the same family [4]. In contrast, the Stanford Human Preference Dataset (SHP) dataset is gathered from pairs of (presumably human) Reddit responses [24]. As responses are more similar in quality, prior work has found that human annotation agreement reduces these responses [2]. While the relative informativeness of an example for training a reward model is likely model-dependent, since the models used for reward model training vary in training data, a minimal level of contrast between the chosen and rejected response is likely a prerequisite for valuable examples in preference datasets. Given the differences in response generation, we can compare and contrast different datasets by computing the cosine similarity between embeddings of responses (i.e. 1 dcos (Yw, Y\u0131)). Figure 5 shows that the HH-RLHF dataset has many more similar response pairs than ULTRAFEEDBACK. To understand the impact of training with high-information examples, we created a threshold of 0.8 in cosine similarity and designated the examples with a smaller similar as \u201chigh information\u201d. Fixing the training set size, we compared the performance of training the high-information examples to a random sample. Surprisingly, the results vary by model and dataset. For the larger models (i.e. 1B+ parameters), there is little difference between the high information and random training sets of the same size. However, for the smaller 350 million parameter model, we see that the high information examples often resulted in a better evaluation accuracy (Figure 5)."}, {"title": "Discussion", "content": "Our work investigates three aspects of preference datasets to identify dataset differences and connect these differences to downstream performance on both in- and out-domain tasks. Firstly, we find that while preference datasets vary in size, a larger dataset is not better than a smaller dataset that is more relevant to the task. Furthermore, increasing dataset size gives only marginal gains for in-domain evaluation accuracy and may even hinder performance on out-of-domain tasks. Future work introducing new preference datasets should report the marginal gain of using the entire dataset on different models compared to using just 10-25% of the dataset.\nSecondly, we find all four of the preference datasets we examine to be extremely noise invariant. We attribute this observation to label noise introducing more uncertainty in reward model predictions rather than prediction reversal. This suggests that better preference datasets can tolerate a higher level of label noise. Future work introducing new preference datasets should report the noise invariance of a dataset and the calibration error induced in the downstream reward model.\nLastly, we find that preference datasets vary widely in the distribution of similarity of response pairs. The performance improvements of training from high information or dissimilar response pairs depends on the underlying reward model. An extreme case is if the underlying language model has undergone RLHF policy learning using a preference dataset, then the relative value or information of this dataset should be lower for reward modeling. Recent work has proposed that learning policies from on-policy data outperforms methods using out-of-distribution data [38]. Future work should define and investigate on-policy data for reward model learning in the context of RLHF."}, {"title": "Noise: Calibration and Reward Modeling", "content": "Thus far, very few works have explored the notion of calibration in reward models. For pairwise preferences, we can think of a reward model as a binary predictor where the notion of calibration is rather natural. Expected calibration error, while suffering from real drawbacks [39], is the most commonly used metric for measuring miscalibration [36]. To compute calibration error, bins can created such that for a bin $B_m$, the confidence of the bin is just averaged the predicted probability:\n$\\text{conf}(B_m) = \\frac{1}{\\vert B_m \\vert} \\sum_{i \\in B_m} p_i$\nand the accuracy of bin $B_m$ is the average accuracy of samples in the confidence bin range:\n$\\text{acc}(B_m) = \\frac{1}{\\vert B_m \\vert} \\sum_{i \\in B_m} \\mathbb{I}[y_i = \\hat y_i],$\nwhere $\\hat y_i$ is the predicted label. In this setup, a perfectly calibrated predictor would have matching confidence and accuracy for each bin. In other words, the expected calibration error is the difference between the accuracy and confidence in each bin:\n$ECE = \\sum_{m=1}^M \\frac{\\vert B_m \\vert}{n} \\vert \\text{acc}(B_m) - \\text{conf}(B_m) \\vert$\nA problem arises when computing this quantity for reward models on preference data if $\\hat y_i$ is naively taken to be $p_i = P(Y_{w_i} > Y_{l_i})$. This is because by definition, if $P(Y_{w_i} > Y_{l_i}) > 0.5, \\hat y_i = Y_i$. This means that zero calibration error can only be achieved through $P(Y_{w_i} > Y_{l_i}) \\in \\{0, 1\\}$ with a perfect predictor.\nThe only prior work that studies calibration in reward models suggests computing the model probability as [37]:\n$p_i = \\text{max}\\{P(Y_{w_i} > Y_{l_i}), P(Y_{l_i} > Y_{w_i})\\}.$\nThis gives the right intuition that if $p_i \\approx 0.5$: we should be very uncertain of the outcome. However, this approach restricts $p_i \\in [0.5, 1]$. Thus, we suggest an alternative approach in creating another random variable $z \\in \\{0, 1\\}$ to randomize the label outcomes so that each example has the following format $(x, Y_1, Y_2, z)$. Each example, $(x, Y_w, Y_l)$ becomes the following two examples: $(x, Y_w, Y_l, z = 1)$ and $(x, Y_l, Y_w, z = 0)$. Now we have the confidence of a bin as:\n$\\text{conf}(B_m) = \\frac{1}{\\vert B_m \\vert} \\sum_{i \\in B_m} Pr[z_i = 1] = Pr(y_1 > y_2)$\nand the accuracy of a bin as:\n$\\text{acc}(B_m) = \\frac{1}{\\vert B_m \\vert} \\sum_{i \\in B_m} z_i$\nThis approach gives us the reliability diagram in Figure 6. We can see that as label noise increases, calibration error decreases. A trivial predictor can achieve zero ECE by always predicting the average"}, {"title": "Scale: Percentile Saturation", "content": "Our work compares preference datasets of vastly different sizes. In our main paper, we present two approaches, the scaling law approach of looking at how the performance of each dataset changes with increasing data (Figure 1) and the benchmarking approach where we plot the performance of different datasets for each task (Figure 2). Here we would like to present a third choice of data saturation curves. On the y-axis we plot the percentage of total performance achieved and on the x-axis we plot the percentage of total data used. This allows us to compare the data efficiency of datasets. In Figure 7, the first observation we can make is that while the shape of the slope of each line becomes flatter with large models, the ordering of datasets remains the same. This allows us to observe that across models of vastly different sizes, SAFERLHF is a dataset that is not very redundant. This is not an artifact of dataset size since LMSYS is approximately the same size."}, {"title": "Dataset and Experiment Details", "content": "Our work looks at 4 different openly available preference datasets. We excluded preference datasets collected or derived from Reddit data due to recent restrictions with respect to terms of service. Specifically, the four datasets we used came from the following hugging face dataset URLs:\n\u2022 HH-RLHF: Anthropic/hh-rlhf\n\u2022 ULTRAFEEDBACK: RLHFlow/UltraFeedback-preference-standard\n\u2022 LMSYS 1msys/lmsys-arena-human-preference-55k\n\u2022 SAFERLHF RLHFlow/PKU-SafeRLHF-30K-standard\nTo ensure minimal data discrepancies between models, we filtered out examples longer than 512 tokens according to each model tokenizer. We also removed ties from the LMSYS dataset."}, {"title": "Response Pair Distances", "content": "For computing distances between responses, we compared several different sentence embeddings. We compared instruction embeddings [40], retrieval embeddings[41], as well as general-purpose embeddings[42, 43]. We found that cosine and Euclidian distances derived from all of them were highly correlated. Thus, we used a general-purpose pre-trained model: all-MiniLM-L6-v2. Using embeddings from this model, Figure 8 shows the contrast in response similarity between different datasets. We see that HH-RLHF contains many more similar winning-losing response pairs compared to other datasets. Furthermore, even though LMSYS responses are generated from a much more diverse set of models than the other three datasets, there are still more similar responses than dissimilar responses. We expect forum-based preference datasets such as Stanford Human Preferences to follow a vastly different distribution of response similarity."}, {"title": "Dataset Scaling", "content": "In the main text we show the OOD performance for the Llama2-7B-Chat model. We also include the Llama2-7B base model (Figure 10) where we see the same pattern of ULTRAFEEDBACK dominating the chat category and SAFERLHF dominating the safety category of Rewardbench. For smaller models, Figure 11, shows a similar pattern for TinyLlama-1B and Figure 12. In these smaller models, the advantage of the SAFERLHF is even more stark."}, {"title": "Noise Invariance", "content": "We also include plots of the effect of dataset label noise on Rewardbench tasks. For all of the models, the Chat and Safety tasks are not significantly affected until 40% of the labels are flipped. For the Chat Hard and reasoning tasks, most of the models we train are not good enough to examine differences properly. It is also interesting that we do not observe cross-over behavior; no dataset starts with a worse performance and improves over a different dataset at a higher level of noise."}]}