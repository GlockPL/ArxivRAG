{"title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding", "authors": ["Yuxin Zuo", "Shang Qu", "Yifei Li", "Zhangren Chen", "Xuekai Zhu", "Ermo Hua", "Kaiyan Zhang", "Ning Ding", "Bowen Zhou"], "abstract": "We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4, 460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 17 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.", "sections": [{"title": "1. Introduction", "content": "Large Multimodal Models (LMMs) demonstrate promising potential in advancing general medical AI systems for applications in clinical scenarios (Achiam et al., 2023; Liu et al., 2024b; Saab et al., 2024). However, current text and multimodal benchmarks for evaluating general medical AI capabilities have numerous limitations:\nFirst, existing text medical benchmarks, such as PubMedQA (Jin et al., 2019), MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), and MMLU (Medical) (Wang et al., 2024b), lack comprehensive coverage of fine-grained and diverse real-world diagnostic scenarios, including highly specialized fields such as family and addiction medicine. This lack of essential breadth limits the applicability of medical AI in thoroughly addressing realistic medical scenarios. Moreover, these benchmarks fall short of sufficiently challenging current advanced AI, hindering progress toward reliable medical AI. For instance, ol-preview has achieved 96% and 99% accuracy on MedQA-USMLE and MMLU Medical Genetics, respectively (Nori et al., 2024).\nSecond, traditional multimodal medical benchmarks (Lau et al., 2018; He et al., 2020; Liu et al., 2021; Zhang et al., 2023; Hu et al., 2024; Chen et al., 2024b) are critically inconsistent with real-world clinical scenarios: 1) Limited Scope and Insufficient Difficulty. These benchmarks solely evaluate basic visual perception and medical knowledge, neglecting the complexity of real-world medical tasks across different stages of the diagnosis process. They fail to assess the expert-level knowledge and reasoning ability required for diagnostic decision-making and treatment planning. 2) Lack of Authenticity and Clinical Relevance. Current benchmarks lack detailed clinical information and rely on automatically generated simple QA paired with isolated medical images, diverging considerably from realistic clinical scenarios. Medical exam questions used in existing text benchmarks present a promising solution, and Med-Gemini (Saab et al., 2024) also demonstrate the significance of such evaluations. However, the field still lacks a systematic and high-quality benchmark.\nTo address these challenges, we present MedXpertQA, a highly challenging and comprehensive medical multiple-choice benchmark. It encompasses MedXpertQA Text for text-only evaluations and MedXpertQA MM for multimodal assessments, making it suitable for a wide range of AI models. shows an overview. Both subsets are currently the most challenging benchmarks in their respective fields. presents model performance comparisons of MedXpertQA Text and other benchmarks. MedXpertQA MM includes diverse image types to simulate the wide variety of visual information encountered in real-world diagnosis. Overall, MedXpertQA covers a wide range of medical specialties and systems and includes chal-"}, {"title": "3. MedXpertQA", "content": "We introduce MedXpertQA, a universal medical benchmark consisting of challenging text and multimodal subsets Text and MM, which are each divided into a few-shot development set with 5 questions and a test set. shows an overview. MedXpertQA is designed to assess expert-level medical knowledge and reasoning capabilities across 17 medical specialties, 11 human body systems, and 3 task categories. It includes a total of 4, 460 questions sourced from examinations at the medical licensing level or higher, of which 2,005 are multimodal questions with a total of 2, 839 images. It is the first to introduce medical specialty evaluations to better simulate realistic medical scenarios. Moreover, MedXpertQA MM is the first multimodal medical benchmark to incorporate challenging medical examination questions and real-world medical scenarios. Meanwhile, due to medicine's rigorous requirements for highly reliable outcomes across a comprehensive scope, we refrain from solely prioritizing complexity and instead aim to ensure question diversity and range as well. As a result, MedXpertQA presents substantial challenges and showcases remarkable diversity across multiple dimensions.\nMedical Coverage MedXpertQA collects questions of 17/25 member board exams (specialties) of the American Board of Medical Specialties to enable evaluation of highly specialized medical scenarios. Moreover, applying the categorization in Liachovitzky (2015), we instruct an LLM to annotate each question with its most relevant human body system. MedXpertQA covers all total 11 systems.\nModal Diversity MedXpertQA includes structured data such as tables in its questions and answer choices, as well as semi-structured documents. MedXpertQA MM's images similarly demonstrate high diversity and wide coverage. It not only encompasses medical imaging results obtained from various techniques in diverse formats, but also other image types such as diagrams, charts, and documents, fully covering the spectrum of visual information that human doctors are expected to analyze. Specifically, MedXpertQA encompasses the following image categories: Radiology,"}, {"title": "3.2. Benchmark Construction", "content": "Data Collection We begin by constructing a large-scale question bank, sourcing difficult multiple-choice questions from authoritative medical examinations and textbooks. Previous work primarily relied on USMLE questions for training and evaluation (Jin et al., 2021). In contrast, we expand the scope by including questions from COMLEX, another major medical licensing examination in the U.S., to capture the unique challenges of medical image interpretation in orthopedic practice. To further evaluate multimodal medical capabilities, we incorporate questions from the American College of Radiology (ACR) DXIT and TXIT exams, the European Board of Radiology (EDiR) exams, and the New England Journal of Medicine (NEJM) Image Challenge. In addition, we collect questions from 17 American medical specialty board exams, spanning a wide array of common medical specialties. Ultimately, we collect 37,543 questions. We obtain human response distributions and expert annotations, including explanations and difficulty ratings.\nData Filtering We conduct AI Expert Filtering and Human Expert Filtering to identify questions that challenge both humans and AI. Subsequent Similarity Filtering further enhances robustness.\nStep 1: AI Expert Filtering. We employ 8 models, divided into basic and advanced, as AI experts to vote on and filter questions. First, each basic AI expert performs 4 sampling attempts for each question. If any expert answers a question correctly in all attempts, the question is deemed too simple and removed. Second, questions that are answered incorrectly by all Al experts are retained. This approach minimizes randomness and effectively differentiates between questions that current AI systems can solve and those that remain challenging. We list models used in this phase in Appendix B.1.\nStep 2: Human Expert Filtering. We use prior and posterior human expert annotations to identify questions that pose challenges to humans. We first assess each question's posterior difficulty by calculating its Brier score, a widely applied metric of prediction accuracy (Zhu et al., 2024). We consider the question's human response distribution over all answer choices as the answer prediction. Given the prediction vector \u0177 and label vector y, the Brier score B is:\n$B = \\frac{1}{N} \\sum_{i=1}^N (y_i-\\hat{y_i})^2,$\nwhere N is the number of options, yi is the label for option i (0 or 1), and \u0177 is the proportion of responses selecting option i. A lower Brier score indicates a more accurate overall prediction distribution, suggesting the question is easier. Compared to accuracy, the Brier score accounts for the"}, {"title": "3.3. Medical Reasoning Benchmarking", "content": "OpenAI's ol models advance reasoning capabilities, which have been extensively validated in math and coding (Wu et al., 2024). However, there is still a lack of thorough evaluation in specialized domains like medical reasoning, as relevant benchmarks remain underdeveloped. Recent works developing 01-like medical reasoning models with reinforcement learning (Chen et al., 2024a) or inference-time scaling (Huang et al., 2025) similarly face limitations. Benchmarks chosen, such as MedQA, contain questions assessing medical knowledge only (see Example of MedQA (Understanding) in Appendix F.1), hence are suboptimal for isolating the model's medical reasoning ability. Therefore, while leading o1-like models show promise in medicine (Xie et al., 2024; Nori et al., 2024), they lack systematic, reasoning-focused evaluations of medical proficiency.\nMedicine tightly interweaves professional knowledge with complex reasoning. Clinical reasoning is multifactorial (Yazdani & Hoseini Abardeh, 2019), requiring synthesizing diverse information, navigating uncertainty (Patel et al., 2024), and engaging in heterogeneous cognitive processes (Shin, 2019). These characteristics are represented in MedXpertQA through complex, information-rich tasks such as multiple-choice style differential diagnosis (Seller & Symons, 2011) (see Appendices F.1 and F.3). Therefore, we underscore the importance of identifying reasoning-oriented tasks to facilitate fine-grained performance feedback and accurate assessment. On MedXpertQA, we use gpt-40 to annotate questions according to whether they require complex reasoning. As seen in Appendix H.1, we instruct the LLM to categorize complicated, reasoning-heavy questions as Reasoning. In contrast, other questions that involve little to no reasoning and instead assess skills such as medical knowledge and image perception are categorized as Understanding. As shown in, within both Text and MM, a majority of questions focus on Reasoning."}, {"title": "3.4. Comparisons with Existing Benchmarks", "content": "Tables 1 and 2 compare two subsets of MedXpertQA with existing benchmarks. Traditional multimodal benchmarks have notable discrepancies from real-world clinical tasks, reflected in the limited number of image types, low image-to-question ratios, and automatically generated questions and annotations. Meanwhile, the MMMU (H & M) Series, primarily based on university-level subject exams, falls short in scope, difficulty, and specificity to the medical domain."}, {"title": "4. Experiments", "content": "We evaluate all models using zero-shot CoT prompting (Kojima et al., 2022) unless otherwise specified. For answer cleansing, we follow the script provided by Kojima et al., 2022. Appendix F.2 shows a case of evaluation through zero-shot CoT prompting. We employ greedy decoding for output generation if available, ensuring result stability. For reasoning models with specific evaluation requirements, we follow their respective instructions. Appendix B.2 presents additional implementation details. We could not evaluate 01 and 03-mini on the full MedXpertQA due to time con-"}, {"title": "5. Analysis", "content": "To evaluate whether question and option augmentation reduces benchmark leakage risk, we follow (Xu et al., 2024) to use perplexity (PPL) and N-gram accuracy (ROUGE-L and edit distance similarity) as metrics. We make certain adaptations and simplifications to assess leakage risk at the instance level before and after data synthesis. After concatenating the original question with a specific prompt such as \"Answer:\" as input, we calculate the PPL of the model's output. Moreover, to assess whether the evaluated model's rationale is similar to the corresponding explanations we collected, we compute the ROUGE-L and edit distance similarity between the output and the explanation. We analyze the outputs of GPT-40, since it is the most effective among all vanilla LMMs, reflecting a higher risk of data leakage.\nTable 5 shows results. Based on the threshold set by Xu et al. (2024), the data leakage risk before synthesis is relatively low, which can be attributed to two factors: 1) Questions in MedXpertQA are derived from difficult medical exams"}, {"title": "5.2. Impact of Inference-Time Scaling", "content": "We employ two groups of Qwen-Series models for comparative analysis to investigate the impact of inference-time scaling on the models' performance on challenging medical tasks. Specifically, they represent two language and multimodal 01-like models, along with their corresponding backbones. Furthermore, we assess DeepSeek-R1 and DeepSeek-V3 for a comprehensive analysis.\n\u2022 DeepSeek-R1 vs DeepSeek-V3: Evaluation on MedXpertQA Text.\n\u2022 QWQ-32B-Preview vs Qwen2.5-32B-Instruct: Evaluation on MedXpertQA Text.\n\u2022 QVQ-72B-Preview vs Qwen2-VL-72B: Evaluation on MedXpertQA MM.\nFigure 4 illustrates the performance of the three groups across MedXpertQA Text and MedXpertQA MM. The accuracy variation observed in each subplot is consistently more pronounced in the Reasoning set. Furthermore, accuracy on the Reasoning set exhibits a clear upward trend, while performance on the Understanding set fluctuates, with occasional declines. This suggests that, even without additional medical training data, inference-time scaling can improve complex medical reasoning skills."}, {"title": "5.3. Medical Insights", "content": "We analyze GPT-40's performance based on the annotated system labels to derive fine-grained medical insights. Figure 5 shows that the proportion of Integumentary questions in the correctly answered set markedly increases compared with the full question set. We also observe a much higher accuracy on these questions than questions on other systems. This suggests that GPT-40 exhibits a deeper understanding and stronger performance on Integumentary-related questions. In contrast, it exhibits lower accuracy on the Cardiovascular subset, with a noticeable proportion decline in the correct set, indicating suboptimal capability in this regard."}, {"title": "6. Conclusion", "content": "In this work, we introduce MedXpertQA, a highly challenging and comprehensive medical multiple-choice benchmark evaluating expert-level knowledge and reasoning in real-world clinical scenarios. MedXpertQA encompasses diverse medical specialties, body systems, and clinical tasks. It addresses critical gaps in current benchmarks, including limited coverage of medical specialties, insufficient difficulty, and lack of clinical relevance. By incorporating expert-level medical examination questions rooted in comprehensive clinical data, MedXpertQA MM marks a crucial advancement in multimodal medical benchmarking. We mitigate data leakage risk through data synthesis and engage experts to ensure accuracy and validity. We bench-"}, {"title": "A. Leakage Prevention Statement", "content": "We obtained all data from freely and publicly accessible sources. We only retained a small percentage of source data, and all questions were subject to question rephrasing and option shuffling, fully ensuring that MedXpertQA complies with U.S. fair use laws.\nTo mitigate the potential data leakage risks, we refrain from releasing the data sources and request that you do not share any example of MedXpertQA online, whether in plain text, image, or any other format."}, {"title": "B. Additional Implementation Details", "content": "Specifically, the basic AI experts include Qwen2.5-7B(Qwen Team, 2024), InternLM2.5-7B (Cai et al., 2024), and LLAMA-3.1-8B (Dubey et al., 2024) for text-only medical questions filtration, Qwen2-VL-7B (Wang et al., 2024a), InternVL2-8B (Chen et al., 2024c), and LLAMA-3.2-11B-Vision (Dubey et al., 2024) for multi-modal medical questions filtration. The advanced AI experts include proprietary models gpt-40-2024-08-06 and claude-3-5-sonnet-20241022."}, {"title": "C. Error Analysis", "content": "In this section, we analyze the reasons behind model errors by classifying different models' incorrect answers into several error types. We consider the following error types:\n\u2022 Reasoning Process Error: The model's prediction rationale indicates errors in key reasoning steps, which led to the incorrect answer.\n\u2022 Perceptual Error (for MedXpertQA MM only): The incorrect answer stems from a misunderstanding or misinterpretation of the image or images provided in the question.\n\u2022 Question Understanding Error: The answer shows an incorrect understanding of information in the original question.\n\u2022 Lack of Medical Knowledge: The model's prediction shows a lack of medical knowledge necessary for arriving at the correct answer.\n\u2022 Formatting Error: The answer includes the correct content, but is formatted improperly and causes the answer extraction process to fail.\nFor each model analyzed below, we sample 200 incorrectly answered questions from the MedXpertQA Text and MedX-pertQA MM subsets, respectively. We use gpt-40-2024-11-20 to label each error type based on the question, correct answer, correct explanation, incorrect answer, and incorrect prediction rationale produced by the model."}, {"title": "H. Prompts", "content": "The following lists all prompts used in this work."}]}