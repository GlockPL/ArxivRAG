{"title": "Matryoshka Quantization", "authors": ["Pranav Nair", "Puranjay Datta", "Jeff Dean", "Prateek Jain", "Aditya Kusupati"], "abstract": "Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models \u2013 especially to low precisions like int4 or int2 \u2013 requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.", "sections": [{"title": "1. Introduction", "content": "Due to their impressive performance, there is a strong push to deploy deep learning models, particularly large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; G Team et al., 2024) in a large number of scenarios. Due to auto-regressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transferring model weights from high-bandwidth memory (HBM) to the SRAM or due to transferring weights/activations in a distributed cluster.\nQuantizing weights and/or activations can significantly reduce the overall communication load and is, therefore, one of the most popular techniques for reducing inference costs (Dettmers et al., 2022). While floating-point representations are standard for training, integer data types such as int8, int4, and int2 are appealing alternatives for inference. However, current methods for quantizing to these varying integer precisions typically treat each target precision as an independent optimization problem, leading to a collection of distinct models rather than a single, versatile one. Furthermore, quantizing to extremely low precisions like int2 is known to be highly inaccurate.\nIn this work, we pose the question of whether both of the above challenges can be addressed; that is, can we train a single model from which we can extract multiple accurate lower-precision models? We answer this question in the affirmative by introducing Matryoshka Quantization (MatQuant), a novel multi-scale training method that leverages the inherent nested (Matryoshka) structure (Kusupati et al., 2022) within integer data types (Figure 1a). Specifically, slicing the top bits of an int8-quantized weight can directly yield an int4 or int2 model. Existing quantization techniques often neglect this structure, which limits the potential for multi-scale adaptable models operating at various bit-widths with optimal performance.\nInstead, MatQuant simultaneously optimizes model weights across multiple precision levels (e.g., int8, int4, int2). At a high level, we represent each model parameter at different precision levels using shared most significant bits (MSBs), and then jointly optimize the loss for each precision level. This allows us to develop a single quantized model that can effectively operate at any of the chosen bit-widths, offering a spectrum of accuracy-versus-cost options. MatQuant is a general-purpose technique, applicable to most"}, {"title": "2. Related Work", "content": "Model weight quantization is an extremely powerful and prevalent technique for making resource-intensive neural networks suitable for deployment constraints - especially modern-day LLMs. Quantization algorithms can be categorized as either learning-free or learning-based. Learning-free methods use limited data to calibrate model parameters without relying on gradient descent. Learning-based methods, however, utilize gradient descent to update either model parameters or auxiliary parameters to aid in quantization."}, {"title": "Learning-free Quantization Methods", "content": "Naive quantization methods, such as MinMax, absmax, and zero-point quantization, aim to directly map the range of model weights to the target bit-width see (Dettmers et al., 2022) for a detailed background. Dettmers et al. (2022) further improved this by identifying the need to handle outliers with higher precision than the rest of the model weights. The core principle of more recent learning-free quantization methods remains similar while improving various aspects of it and using small amounts of data for calibration. For example, GPTQ (Frantar et al., 2022) improves upon min-max quantization by iterating over all the coordinates, quantizing them one at a time, and updating the remaining full-precision coordinates to minimize the layer-wise activation reconstruction error. AWQ (Lin et al., 2023), SmoothQuant (Xiao et al., 2023), and AffineQuant (Ma et al., 2024) scale the weights and activations to reduce outliers, thus making them easier to quantize. QuIP (Chee et al., 2024), FrameQuant (Adepu et al., 2024), and QuaRoT (Ashkboos et al., 2024) multiply the weights and activations by orthonormal matrices before quantizing to reduce the number of outliers. SqueezeLLM (Kim et al., 2024) uses clustering to obtain the optimal buckets for quantization, and CDQuant (Nair and Suggala, 2024) improves upon GPTQ by greedily choosing the coordinates to descend along. While learning-free methods are inexpensive and work well at higher bit-widths, they are often suboptimal in the low-precision regime, which benefits greatly from learning-based techniques."}, {"title": "Learning-based Quantization Methods", "content": "Quantization Aware Training (QAT) (Abdolrashidi et al., 2021; Jacob et al., 2018) is a logical approach to ensure that models are easy to quantize during inference while retaining high accuracy. However, because QAT involves updating all the model parameters, its adoption for LLMs has been limited. Several recent works improve the performance and efficiency of QAT. LLM-QAT (Liu et al., 2024a) and BitDistiller (Du et al., 2024) enhance QAT with knowledge distillation from the full-precision model. EfficientQAT (Chen et al., 2024) minimizes the block-wise reconstruction error before performing end-to-end training. This significantly reduces the time it takes for QAT to converge. On the other hand, some techniques significantly reduce the overhead by learning only the auxiliary parameters, such as scaling factors and zero-points, that aid in quantization instead of updating the actual weight matrices. For example, OmniQuant (Shao et al., 2023) does not update the model parameters; instead, it learns additional scales and shifting parameters (that aid with quantization) through gradient descent over the block-wise reconstruction error and achieves better accuracy than most QAT techniques. Likewise, SpinQuant (Liu et al., 2024b) uses gradient descent to learn its rotation matrices. This class of learning-based quantization techniques (OmniQuant, SpinQuant, etc.) is widely adopted due to their appeal of achieving QAT-level accuracy at a fraction of the cost."}, {"title": "Multi-scale Training", "content": "Training across multiple data scales (resolutions) was heavily popularized in computer vision for both recognition and generation (Adelson et al., 1984; Denton et al., 2015; Lin et al., 2017). More recently, the paradigm of multi-scale training has shifted to models (Devvrit et al., 2023; Kusupati et al., 2022; Rippel et al., 2014; Yu et al., 2018), where the data remains the same, and models of varying capacity, all nested within one large model, are trained jointly. This joint, nested (Matryoshka-style) learning with varying model sizes results in a smooth accuracy-vs-compute trade-off and is beneficial in many downstream applications and real-world deployments. However, the most obvious structure with a nested nature is the bit structure of the integer data type. Given the success of multi-scale training for inputs, outputs, and model weights, it is imperative to explore it further for integer data types, especially in the context of quantization, which aids in the deployment of resource-intensive LLMs."}, {"title": "3. Matryoshka Quantization", "content": "We introduce MatQuant, a general-purpose, multi-scale training technique that works seamlessly with popular learning-based quantization methods such as Quantization Aware Training (QAT) (Jacob et al., 2018) and OmniQuant (Shao et al., 2023). As long as the model or auxiliary parameters are optimized with gradient descent, MatQuant's multi-scale training technique can be used across chosen bit-widths, leveraging the inherent nested structure of integer data types. In this section, we will elaborate on the preliminaries behind QAT and OmniQuant, alongside our novel proposed approach, MatQuant."}, {"title": "3.1. Preliminaries", "content": ""}, {"title": "3.1.1. Quantized Aware Training", "content": "Quantized Aware Training (QAT) learns a c-bit quantized model by optimizing for the end-to-end cross entropy loss using gradient descent. It uses the quantized weights for the forward pass and a straight through estimator (STE) (Bengio et al., 2013) to propagate gradients through the quantization operator during the backward pass.\nTo mathematically formulate QAT, we define MinMax quantization of a real-valued vector w in c bits as follows:\n$Q_{MM}(w, c) = clamp (\\lfloor \\frac{w}{\\alpha} + z \\rfloor, 0, 2^c - 1)$\n$\\alpha = \\frac{max(w) \u2013 min(w)}{2^c - 1}$\n$z = \\frac{min(w)}{\\alpha}$\nwhere $Q_{MM}(w, c)$ is the c-bit quantized version of w, \u03b1 is the scaling factor and z is the zero point.\nLet $W_F$ represent weights of a Transformer LLM and let $D = \\{(x_1, y_1), . . ., (x_N, y_N)\\}$ be a labelled dataset where $x_i$ and $y_i$ represent the input and output respectively. With $L_{CE}$ as the cross entropy loss, the optimization of QAT is:\n$\\min_{W_F} \\sum_{i \\in [N]} L_{CE} (F(x_i; Q_{MM} (W_F, c)), y_i)$\nwhere F(.) represents the LLM's forward pass."}, {"title": "3.1.2. OmniQuant", "content": "OmniQuant, unlike QAT, does not update the model parameters. Instead, it learns additional scaling and shifting parameters through gradient descent over layer-wise L2 error reconstruction. These auxiliary parameters aid with quantization. Similar to QAT, OmniQuant also uses a straight through estimator during optimization. However, unlike QAT, OmniQuant operates with limited data, making it much more attractive for resource-scarce settings.\nOmniQuant adds two learnable scales, \u03b3 and \u03b2, to MinMax quantization as follows:\n$Q_{omni} (w, c) = clamp (\\lfloor \\frac{w}{\\alpha} + z \\rfloor, 0,2^c \u2212 1)$\n$\\alpha = \\frac{\\gamma \\cdot max(w) \u2013 \\beta \\cdot min(w)}{2^c-1}$\n$z = \\frac{\\beta .min(w)}{\\alpha}$\nOmniQuant also adds another set of learnable shifting and scaling parameters to the FFN's affine projections as follows:\nXW+b \u2192 ((X \u2013 \u03b4) \u00a9 s) \u00b7 Qomni (W \u00a9 s) + b + \u03b4 \u00b7 W\nwhere $X \\in R^{n\u00d7d}$ is the input to the affine transformation, $W \\in R^{dxdo}$ is the linear projection associated with the affine transformation, $b \\in R^{do}$ is the bias vector, \u03b4 \u2208 Rd and s e Rd are learnable shift and scale parameters respectively.\nWith the goal of optimizing the layer-wise L2 error (where a layer consists of an Attention block followed by an FFN block), OmniQuant's overall objective can be portrayed as follows:\n$\\min_{\\gamma,\\beta,\\delta,\\varsigma} ||F_l(W, X_l) \u2013 F_l (Q_{omni} (W, X_l)||^2$\nwhere $F_l(\u00b7)$ represents the forward pass for a single layer l, W represents the layer parameters and $X_l$ represents the layer's input. Note that the above objective is optimized independently for each of the L Transformer layers."}, {"title": "3.2. MatQuant", "content": "MatQuant is a general purpose framework to develop a single model that can do well at any precision. It is a multi-scale training technique that works with most learning-based quantization schemes like QAT and OmniQuant discussed earlier. At its core, taking inspiration from Kusupati et al. (2022), MatQuant optimizes the quantization loss for several target bit-widths jointly.\nTo have a single model for various integer precisions, we nest smaller bit-widths into large ones - leveraging the inherent Matryoshka nature of the integer data type. So, if we want to extract a r-bit model from a c-bit model (0 < r < c), we can just slice out the r most significant bits (MSBs) \u2013 using a right shift, followed by a left shift of the same order. Formally, the $S(q^c, r)$ operator slices the most significant r bits from a c-bit quantized vector $q^c$:\n$S(q^c,r) = (\\lfloor \\frac{\\frac{q^c}{2^{c-r}}}{2^{2^{c-r}}} \\rfloor 2^{c-r})$\nOnce we have this structure, we can optimize for several precisions by slicing the MSBs from the largest bit-width we are optimizing for. Let R = {$r_1$, $r_2$, ..., $r_k$} be the bit-widths we want to optimize for, Q(\u00b7) represent the quantization function of the base algorithm (i.e., any learning-based quantization scheme), L(\u00b7) represent the loss function pertaining to the base algorithm, F(\u00b7) represent the forward pass required to compute the loss, \u03b8 represent the set of model/auxiliary parameters we are optimizing for and let $W_F$ represent the model parameters. MatQuant's overall objective can be formulated as follows:\n$\\min_{\\theta} \\frac{1}{N} \\sum_{i\\in[N]} \\sum_{r \\in R} \\lambda_r L (F(S(Q(\\theta,c), r), x), y)$\nwhere y = yi for QAT and $y=F_l(W, X)$ for OmniQuant, and x = xi for QAT and $x = X_l$ for OmniQuant. \u03bb, is the loss reweighing factor for bit-width r.\nIn this work, we default to training MatQuant with three bit-widths, R = {8,4,2}, and subsequently perform a grid search over \u03bbr. This process aims to optimize performance such that the model performs well across all targeted precision levels. Further, while the focus of this paper is primarily on integer data types, we discuss the possibility of extending MatQuant to floating-point representations in Section 5.5.\nA key point to note is that MatQuant primarily alters the quantized weight distributions across precision levels compared to the base quantization algorithm (OmniQuant or QAT). Figure 1c illustrates the differences in the quantized weight histograms obtained with and without MatQuant on Gemma-2 9B using OmniQuant. Upon close observation, we find that all the distributions of MatQuant are shifted to the right; that is, weights quantized with MatQuant tend to use more higher-valued weights. While this might not significantly impact int8 or even int4 models, int2 models benefit from utilizing more of the possible quantized weights compared to the baseline. Because int2 favors higher-valued weights, this effect propagates to higher-valued weights for int4, and then to int8. This observation highlights the potential overparameterization and freedom in the int8 data type to accommodate the more stringent needs of int2 during joint training. We further explore the effects of this phenomenon in Section 5.3 to develop a better standalone quantization technique for a single target precision."}, {"title": "3.2.1. Interpolative Behavior", "content": "Slicing. Although we explicitly train MatQuant for three precisions (int8, int4, int2), we find that the resulting model, when quantized to interpolated bit-widths like int6 & int3 by slicing (Eq. 6) the int8 model, performs on par with a baseline trained explicitly for that precision. It is also significantly better than slicing an int8 quantized model. We attribute this strong interpolation in bit-width space to MatQuant, and present more results in Sections 4.1 & 4.2.\nMix'n'Match. MatQuant also enables the use of different precisions at different layers through layer-wise Mix'n'Match (Devvrit et al., 2023), even though we never trained for these combinatorial possibilities. These large number of models, obtained at no cost, densely span the accuracy-vs-memory trade-off. We explore several Mix'n'Match strategies and find that having a higher precision (int8) in the middle layers and a lower precision (int2) at the start and end is Pareto-optimal among hundreds of possible models. See Section 4.3 for detailed experiments."}, {"title": "4. Experiments", "content": "In this section, we present an empirical evaluation of MatQuant working with two popular learning-"}, {"title": "4.1. MatQuant with OmniQuant", "content": "Table 1 shows the efficacy of MatQuant when used with FFN-only OmniQuant and compared to explicitly trained OmniQuant baselines for the target precisions, i.e., int8, int4, and int2, across all the models. While the average downstream accuracy of MatQuant for int8 and int4 quantization is within 0.5% of the corresponding independently trained baselines, the int2 quantized models of MatQuant are 4.37%, 8.01%, and 6.35% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively. Similar trends and improvements follow when measuring performance through validation log perplexity. Further, the quantized int4 and int2 models sliced from the int8 OmniQuant baseline suffer a significant drop in accuracy around int4, demonstrating that the nested structure of int8 is not well utilized."}, {"title": "Sliced Interpolation", "content": "Beyond the target quantization granularities (int8, int4, and int2), MatQuant allows for bit-width interpolation to bit-widths not optimized during training. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is comparable to explicitly trained baselines for both precisions."}, {"title": "4.2. MatQuant with QAT", "content": "To further demonstrate the generality of MatQuant, we experiment on the same models using the popular QAT technique. Following the trend of experimental results with OmniQuant, we show in Table 2 that the models trained using MatQuant with QAT are comparable to the explicitly trained baselines for all the targeted bit-widths of int8 and int4. However, int2 quantized models using MatQuant are 4.69%, 6.30%, and 6.34% more accurate for Gemma-2 2B, 9B, and Mistral 7B, respectively."}, {"title": "Sliced Interpolation", "content": "Models trained using MatQuant with QAT exhibit strong interpolative performance similar to that of MatQuant with OmniQuant. We find that the accuracy of the int6 and int3 models obtained by slicing the MatQuant models is comparable to explicitly trained baselines for both interpolated bit-widths.\nWhile OmniQuant only trains the auxiliary parameters needed for quantization, QAT also updates the weight parameters. This potentially results in severe overfitting to the C4 subset used in the experiments. We observe this overfitting in all the experiments presented in Table 2, where the log perplexities improve for QAT compared to OmniQuant, while the downstream accuracies suffer. This also highlights the need for high-quality data for QAT to realize its benefits; otherwise, users are better off using resource-friendly methods like OmniQuant."}, {"title": "4.3. Layerwise Mix'n'Match", "content": "Alongside the strong slicing-based interpolative properties, quantization with MatQuant also enables another form of elastic and interpolative behavior through Mix'n'Match. Mix'n'Match provides a mechanism to obtain a combinatorial number of strong models by using different quantization granularities, from the target bit-widths \u2013 i.e., int8, int4, and int2 across layers. Figure 2 shows the ability of Mix'n'Match to densely span the Pareto-optimal accuracy-vs-bits-per-FFN-parameter (memory/cost) trade-off for"}, {"title": "5. Ablations and Discussion", "content": "In this section, we present design ablations to improve MatQuant. Section 5.1 discusses the effect of non-uniform weighting across target precisions (int8, int4, int2), and Section 5.2 explores enabling co-distillation of lower precision levels (int4, int2) from the highest precision quantized model (int8). During the process of extending MatQuant to all Transformer parameters, not just the FFN block, we uncovered an interesting hybrid quantization algorithm (between Baseline and MatQuant). Section 5.3 further details this method, called Single Precison MatQuant, which stabilizes the otherwise QAT baseline for all the Transformer weights. Finally, we also discuss extending MatQuant beyond integer data types and the considerations for effective deployment on current hardware."}, {"title": "5.1. Weightings (\u03bb) for MatQuant", "content": "Depending on the constraints, we may wish to maximize the accuracy of one of the target bit-widths in MatQuant. Equation 7 provides a general formulation of MatQuant that supports a grid search on the weights \u03bb, for bit-width r. The results in Section 4 are with the weights that have balanced performance across target precisions. Table 3 shows the weight multiplier ablation results for Gemma-2 2B, 9B, and Mistral 7B. While equal weighting for all precisions works well, we see that higher weights for a specific precision results in increased accuracy for that bit-width. This re-weighting to improve int8 and int4 models often results in a minor accuracy drop for the int2 models. We can consider re-weighting as scaling the importance of the bits during training, and finding an optimal grid-search-free recipe is an interesting research question."}, {"title": "5.2. Co-distillation for MatQuant", "content": "Given the nested nature of the models trained using MatQuant, we explored co-distillation, where the outputs from a higher-precision model are used as the target for the lower-precision nested model, either in a standalone fashion or alongside the ground truth target (weighted equally). Table 4 shows the effects of co-distillation applied to MatQuant with both OmniQuant and QAT on Gemma-2 9B. While int8 and int4 show no significant improvement, the nested int2 model benefits substantially from the int8 supervision, reaching 1.65% higher accuracy than the non-co-distilled MatQuant with OmniQuant. This helps us push the int2 quantized Gemma-2 9B beyond 70% average downstream accuracy for the first time across all our experiments. Co-distillation in MatQuant opens up avenues for interesting design choices that can further leverage the inherent nested structure of integer data types."}, {"title": "5.3. Single Precison MatQuant", "content": "In Tables 1 and 2, MatQuant performs on par with the explicitly trained baselines for int4, int8, and the interpolated int3 and int6 precisions. However, the int2 models show a significant accuracy improvement. To investigate this, we conducted a simple ablation in MatQuant by removing the loss terms for int4 and int8 (i.e., R = {2} in Equation 7 or setting \u03bb4 = \u03bb8 = 0) and present the results in Table 5. We call this version of MatQuant as Single Precison MatQuant. With Single Precison MatQuant, we observe a further boost of up to 1.67%, in the accuracy of int2 models at a ~2% accuracy drop in the corresponding int4 and int8 models \u2013 int2 is still nested within int8. This improvement likely stems from the six additional bits available during MatQuant-style training to optimize the int2 representation.\nIn the case of Single Precison MatQuant, gradient descent is free to tune these six additional bits to improve the overall quality of the int2 model. In MatQuant, since we have additional losses to preserve the performance of the int4"}, {"title": "5.4. Deployment Considerations", "content": "Current hardware accelerators have native support for serving int8 and int4 quantized models. Additionally, custom-implemented CUDA kernels can can support various low-precision bit-widths, like int2 and int3 (Chee et al., 2024; Frantar et al., 2022). MatQuant can generate a large number of models at inference time. Depending on the serving environment, we can choose between Mix'n'Match models and homogeneous sliced models. For example, suppose the serving environment has a memory constraint equivalent to an int3 model but lacks optimized support for int3, while supporting int2. In this case, a Mix'n'Match model performing comparably to the int3 model could be deployed. More generally, as depicted in Figure 2, MatQuant densely spans the memory-versus-accuracy curve and can be leveraged to obtain the most performant model for a specific serving constraint. MatQuant can enable further research on hardware software co-design to effectively support elastic bit-widths on-the-fly during inference time."}, {"title": "5.5. Extension to Floating Point", "content": "Extending MatQuant to floating-point representations, such as FP8 and FP4, presents significant challenges. Given that the exponent is encoded within the bit representation and contributes to the value as a power of 2 (i.e., effectively log2), slicing it results in buckets whose sizes increase exponentially, unlike the integer case, where bucket sizes are constant. For example, slicing the first two bits from int8 yields buckets of 0, 64, 128, 192. Here, the bucket size (64) is constant; however, this would not be the case when slicing two exponent bits from FP8. This is a promising avenue for future research that could"}, {"title": "6. Conclusions", "content": "In this work, we presented MatQuant, a novel multi-scale training technique that leverages the nested structure of integer data types to simultaneously optimize model weight quantization across multiple precisions (int8, int4, and int2) within a single model. This general-purpose method, applicable to learning-based quantization techniques like OmniQuant and QAT, produces models with comparable accuracy to baselines for int8 and int4, while achieving significant improvements, up to 10% (using co-distillation), for int2 models. MatQuant further enables bit-width interpolation and layer-wise mix-and-match for flexible accuracy-cost trade-offs, promising more efficient deployment of large models across various hardware settings. Finally, MatQuant also helped discover Single Precison MatQuant, which significantly improves standalone low-bit quantization."}]}