{"title": "Unifying and Optimizing Data Values for Selection via Sequential-Decision-Making", "authors": ["Hongliang Chi", "Qiong Wu", "Zhengyi Zhou", "Jonathan Light", "Emily Dodwell", "Yao Ma"], "abstract": "Data selection has emerged as a crucial downstream application of data valuation. While existing data valuation methods have shown promise in selection tasks, the theoretical foundations and full potential of using data values for selection remain largely unexplored. In this work, we first demonstrate that data values applied for selection can be naturally reformulated as a sequential-decision-making problem, where the optimal data value can be derived through dynamic programming. We show this framework unifies and reinterprets existing methods like Data Shapley through the lens of approximate dynamic programming, specifically as myopic reward function approximations to this sequential problem. Furthermore, we analyze how sequential data selection optimality is affected when the ground-truth utility function exhibits monotonic submodularity with curvature. To address the computational challenges in obtaining optimal data values, we propose an efficient approximation scheme using learned bipartite graphs as surrogate utility models, ensuring greedy selection is still optimal when the surrogate utility is correctly specified and learned. Extensive experiments demonstrate the effectiveness of our approach across diverse datasets.", "sections": [{"title": "1. Introduction", "content": "Data plays a fundamental role in modern machine learning, with recent advances in deep learning heavily dependent on massive datasets. However, not all data contributes equally to model performance, leading to the development of data valuation methods that quantify the contributions of individual training samples. The dominant approaches to data valuation are grounded in cooperative game theory. In this formulation, training samples are treated as players in a cooperative game, where the utility function measures the validation performance of models trained on different data subsets. Data values are then derived by aggregating each sample's marginal contributions across various subset combinations, leading to principled valuation methods such as Data Shapley, Data Banzhaf, and Beta Shapley. These valuation methods provide essential guidance for crucial data-centric tasks like data selection, where the goal is to identify optimal subsets of training data that maximize model performance. The standard protocol of existing work evaluates data values through selection curves: data points are ranked by their assigned values, and model performance is measured as more points are incrementally added in descending order of their values. As shown in Figure 1, the effectiveness is evaluated by comparing performance between different methods across different selection sizes (vertical dashed lines), where superior data values should demonstrate both immediate efficiency through steeper initial curves and sustained effectiveness via consistently higher performance across all selection budgets (e.g., Data Values I in Figure 1), indicating their ability to identify valuable training points early while maintaining their advantage as the dataset grows. However, existing data valuation methods were not explicitly designed to optimize for this selection objective, which accounts for performance across all possible budgets. Despite the clear importance of achieving both efficient initial selection and sustained performance improvements, current methods overlook this fundamental requirement.\nIn this work, we directly analyze data values from a selection perspective. While existing evaluation protocols implicitly assume that good data values should perform well across different selection sizes, this crucial objective has never been explicitly formalized or optimized. We address this"}, {"title": "2. Background and Preliminaries", "content": "2.1. Data Values and Data Selection\nDefinition 2.1 (Score-based Data Values). Given a dataset D = {x1, ..., Xn} and a utility function U : 2D \u2192 R that measures the performance of any subset, the goal of data valuation is to learn a value-assignment function v : D \u2192 R that assigns scores to individual data points based on their contribution to the overall utility.\nThe predominant data values leverage solutions from the cooperative game theory, known as Game-theoretic Data Values. For a broader review of data valuation methods, we refer readers to Section A.1.\nDefinition 2.2 (Game-theoretic Data Values). Given a dataset D and a utility function U that measures model performance on a held-out validation set when trained on different subsets, game-theoretic data values are derived by treating data points as players in a cooperative game, where for each point xi, its value v(i) is computed as a weighted combination of its marginal contributions across different subsets:\nv(i) = \\sum_{S\\subset D\\{i}} w(S)[U(S\\cup \\{i\\}) \u2013 U(S)]\nwhere w(S) represents the weight assigned to subset S. Notably, when the weights depend only on subset size and satisfy \\sum_{k=1}^n w(k) = 1, this formulation yields semi-values (Kwon & Zou, 2021), with Data Shapley being a special case where w(S) = \\frac{|S|!(n-|S-1)!}{n!}. Another commonly used but non-game-theoretic data value that also fits this definition is the Leave-one-out (LOO) value V_{LOO}(i) = U(D) \u2013 U(D\\{i}), which measures each point's marginal contribution to the full dataset.\nData values provide principled approaches for quantify-"}, {"title": "2.2. Sequential-Decision-Making and MDPS", "content": "Sequential decision-making problems can be systematically modeled through Markov Decision Processes (MDPs). An MDP is defined by a tuple M = (S, A, P, r), where S represents the finite state space, A denotes the set of possible actions, P(s'|s, a) specifies the probability of transitioning to state s' when taking action a in state s, and r: S \u00d7 A \u2192 R is the reward function.\nFor finite-horizon problems with deterministic state transitions, known as finite-horizon Deterministic MDPs, the transition probability P is replaced by a deterministic transition function T : S \u00d7 A \u2192 S for a fixed horizon length H. At each step t = 1,..., H, given the current state s\u2208 S, an action a \u2208 A is selected, yielding an immediate reward r(s, a), after which the system transitions to the next state. The value function Vt(s) represents the maximum cumulative reward achievable from state s at time t:\nVt(s) = \\max_{a\\in A}[r(s,a) + V_{t+1}(T(s, a))], with boundary condition VH(s) = \\max_{a\\in a}r(s,a). The objective is to find a policy \u03c3 : S \u00d7 [H] \u2192 A that maximizes the cumulative reward \\Sigma_{1}^H r(s_t, (s_t, t)) subject to the transition dynamics. This optimization can be solved through backward dynamic programming (DP)."}, {"title": "2.3. Approximate Dynamic Programming", "content": "Dynamic programming faces computational challenges due to the curse of dimensionality when state and action spaces grow large. Approximate Dynamic Programming (ADP) introduces approximation techniques to address these limitations. The first key strategy in ADP involves parametric function approximation to estimate both value functions and reward functions. Instead of maintaining exact values for each state-action pair (s, a), ADP employs parametric approximations like V (s;0) for value functions and r(s, a; \u03b8) for finite-horizon undiscounted reward functions, where 0 represents learnable parameters in linear models or neural networks. The second strategy involves simplified decision rules at each state that consider only immediate or limited-horizon future rewards, rather than the full backward induction required by exact dynamic programming. These approximations enable ADP to handle high-dimensional MDPs by trading off exact optimality for computational tractability while maintaining solution quality through careful approximation design. For a comprehensive discussion of sequential decision-making and ADP methods, we refer readers to Section A.2.\nAmong various ADP approaches to solving sequential decision-making problems, myopic reward function approximation represents the most computationally efficient strategy by combining both approximation strategies: it uses parameterized reward approximation \u00ee(s, a; 0) and adopts the simplest decision rule \\sigma_{myopic} = \\argmax_{a\\in A}r(s, a; \u03b8). This solution focuses solely on maximizing an approximated immediate reward for the nearest time period, ignoring how current decisions might impact future states."}, {"title": "3. A Unified Framework for Sequential Data Selection", "content": "When data values are utilized for data selection (Definition 2.3), data points are selected sequentially according to their induced ranking. This process is fundamentally sequential in nature: data points must be chosen one by one, with each selection potentially impacting the value of the remaining points. Moreover, through this ranking mechanism, a single value assignment implicitly provides solutions for data selection problems with any budget constraint k \u2264 |D|, which is formalized in the following observation.\nObservation 3.1 (Solutions for Multiple Selection Problems). Any value assignment v : D \u2192 R over a dataset D implicitly generates solutions for all subset selection problems with cardinality constraints k \u2208 [1, |D|] through its induced ranking.\nAccording to this observation, an ideal data value should select satisfying subsets for different budget sizes k in a progressive way. Existing evaluation protocols implicitly follow this intuition. As shown in Figure 1, the quality of data values is typically evaluated through qualitative comparison of performance curves. Superior data val-"}, {"title": "3.1. Sequential Data Selection as a Deterministic Markov Decision Process", "content": "The nested constraint Sk-1 C Sk reveals the inherent recursive structure of Problem 3.2: each selection decision is conditioned on all previous choices, and the marginal contribution of the k-th selected sample depends on the composition of the existing subset. This sequential dependency naturally suggests reformulating the problem as a Deterministic Markov Decision Process (DMDP):\n1. State st: The set of selected samples at step t\n2. Action at: The next sample to be selected from remaining samples\n3. Transition: St+1 = St U at\n4. Reward: rt = U(st)\nTo solve the sequential data selection problem, which starts from an empty set and sequentially selects data points until all data is selected, we need to analyze what factors determine the selection quality. As illustrated in Figure 2, the solution quality is fundamentally determined by two key factors: (1) Reward Modeling: How we model and access the environment's reward signals. This could be the ground-truth utility obtained through actual model training and evaluation, or a surrogate that approximates the true utility function. (2) Decision Policy: How we make decisions based on the reward function we used. The combination of these two components determines the quality of the induced ranking, as their interactions directly impact the selection performance across different subset sizes. Through this lens of utility modeling and decision-making, we will analyze how different solutions arise from specific choices of these two components, from exact dynamic programming to various approximation schemes in existing methods."}, {"title": "3.2. An Exact Solution via Dynamic Programming", "content": "Under the DMDP formulation, our goal is to find an optimal policy that maximizes the cumulative reward starting from any state. Let V(s) denote the optimal value function starting from state s, representing the maximum achievable cumulative reward from state s onwards. According to the"}, {"title": "4. Analyzing Existing Data Values via ADP", "content": "Having established the optimal sequential selection framework, we now demonstrate how existing data valuation methods can be unified and analyzed through Approximate Dynamic Programming (ADP). This perspective reveals that the ranking sequences induced by data values are in fact specific solutions to our sequential decision-making problem under particular approximation schemes.\nTheorem 4.1 (Game-theoretic Data Values as ADP Solutions). For game-theoretic data valuation methods (Data"}, {"title": "4.1. Optimality Analysis", "content": "In this subsection, we analyze the optimality of existing game-theoretic data values under different utility functions in Problem 3.2. We start with linear utility functions - a special case where data points contribute independently: Remarkably, despite their approximations, existing data valuation methods achieve optimality when the ground-truth utility exhibits linear structure. Consider linear utility functions, a special case where each data point contributes independently to the overall utility:\nDefinition 4.2 (Linear Utility Function). A utility function U : 2D \u2192 R is linear if it can be written as U(S) =\n\\sum_{ies} wi for some weights wi \u2208 R.\nWhen the utility function is linear, all semi-value-based methods achieve optimal sequential selection.\nTheorem 4.3 (Optimality Under Linear Utility). When the utility function U in Problem 3.2 is linear, the sequences induced by semi-value based data valuation methods (including Data Shapley, Beta Shapley, and Data Banzhaf) are optimal solutions to Problem 3.2 (proof in Appendix D)."}, {"title": "4.1.1. OPTIMALITY ANALYSIS UNDER MONOTONIC SUB-MODULAR UTILITY WITH CURVATURE C", "content": "While the prior analysis of linear utility functions provides valuable theoretical insights, such functions represent an idealized case that rarely appears in practical data selection scenarios. Real-world selection tasks typically exhibit more complex utility structures with diminishing returns, wherein the marginal benefit of each additional data point decreases as we select more points. This behavior is naturally captured by monotonic submodular functions, which arise in many practical settings such as coreset selection and data summarization.\nDefinition 4.4 (Monotonic Submodular Function). A utility"}, {"title": "5. Efficient Approximation via Bipartite Surrogate Models", "content": "While our framework provides theoretical clarity through exact dynamic programming, its computational demands make it intractable in practice. Although existing methods like Data Shapley offer linear approximations (Section 4), they often fail to capture complex data dependencies. To address both challenges, we propose approximating the utility function using bipartite graph structures. Our key insight is that a data point's utility contribution can be effectively modeled through its coverage relationships with the"}, {"title": "5.1. Bipartite Graph Structure Learning", "content": "Let training data Xtrain = {xtrain,...,xtrain} and validation data Xvalid = {xalid,...,xualid}. We represent the utility dependencies through a weighted bipartite graph G = (Vtrain U Vvalid, E, w), where vertices Vtrain and Vvalid correspond to training and validation points respectively, edges E Vtrain \u00d7 Vvalid capture influence relationships, and weights w : E \u2192 R+ quantify their strengths. The key challenge lies in learning appropriate edge structures that accurately reflect utility dependencies. Our algorithm leverages feature space information to discover inherent utility relationships, based on the insight that training and validation points with similar features often exhibit shared utility patterns. We propose an efficient algorithm (Algorithm 2 in Appendix F) that leverages feature space information to learn edge connections by adaptively thresholding similarity scores."}, {"title": "5.2. Sequential Selection with Bipartite Graphs", "content": "With the constructed bipartite graphs from our two approximation approaches, we can determine data values through an iterative maximum coverage process. At each step, we select the training point that covers the maximum number of uncovered validation points based on the learned edge relationships. After each selection, we update the remaining coverage structure by removing both the selected training point and its covered validation points from consideration. This pruning step prevents redundant coverage while ensuring diversity in the selected sequence. The process naturally assigns higher values to points selected earlier in the sequence, as they provide coverage for larger portions of previously uncovered validation points. This selection strategy inherits theoretical guarantees from the submodular nature of the coverage function while maintaining computational efficiency by operating on our pre-computed graph structures rather than requiring repeated model evaluations."}, {"title": "5.3. Theoretical Properties", "content": "The bipartite-based approximation preserves key theoretical properties that ensure its effectiveness. As established in Appendix G, our approximation maintains both monotonicity and submodularity of the original utility function. These properties, formalized in Theorem G.2, guarantee that greedy selection under the approximated utility achieves optimal sequential ordering when the approximation accurately learns the ground-truth utility function (Theorem G.3)."}, {"title": "6. Data Values for Selection: From Reward Functions to Value Functions", "content": "Our sequential decision-making framework reveals a fundamental insight: data values for selection should arise from value functions rather than reward functions. Through the optimal policy derived in Section 3.2, we can define optimal data values as v*(i) = \\sum_{t=1}^n \\I_{t}^*(i), where \\I_{t}^*(i) represents the optimal selection step for sample i. This value function perspective fundamentally differs from traditional methods that aggregate marginal rewards independently. Instead, it naturally captures sequential dependencies by considering each sample's role in the optimal selection trajectory. By expressing data values through value functions conditioned on optimal state sequences, we measure a data point's value by its order to the optimal trajectory rather than isolated rewards. This formulation achieves space efficiency by tracking dependencies only along optimal paths instead of the full state space."}, {"title": "7. Experiment", "content": "For evaluation, we compare our approach with nine baseline data valuation methods including etc., all implemented in OpenDataVal and other public dataset sources. We conduct experiments on eight diverse datasets from OpenML, following the standard data valuation evaluation protocol that generates selection curves by iteratively adding points based on their assigned values. All experiments are averaged over 20 independent runs with 1000 model retraining steps except for the DynamicProgramming method (Algorithm 1), which is used for optimality verification in sequential selection. Detailed experimental settings and dataset descriptions are provided in Appendix H."}, {"title": "7.1. RQ1: How Close are Existing Data Values to Optimal Sequential Selection?", "content": "Following the experimental settings detailed in Appendix H.5, Figure 3 reveals substantial performance gaps between existing methods and optimal sequential selection method DynamicProgramming. Several key findings emerge:\nFirst, the optimal selection DynamicProgramming (solid gray line) consistently outperforms all existing methods across datasets, with particularly pronounced gaps in early selection stages (k \u2264 5). This indicates that current approaches significantly underperform in identifying the most valuable initial samples. Second, the performance gap varies notably across datasets, suggesting dataset-specific challenges. The gap is particularly prominent in structured datasets like bbc-embeddings and nomao, where optimal selection demonstrates steep initial performance improvements that existing methods fail to match. Third, existing methods exhibit varying patterns of suboptimality. Game-theoretic approaches tend to perform similarly to each other but consistently fall short of DynamicProgramming. Learning-based methods and influence-based approaches show competitive performance in some cases but lack consistency across datasets. Notably, all methods cannot match the optimal strategy's ability to achieve both rapid initial improvement and sustained performance gains."}, {"title": "7.2. RQ2: How Does Utility Curvature Impact the Performance of Game-theoretic Data Values?", "content": "To empirically validate our theoretical analysis of utility curvature's impact on data valuation methods, we design controlled experiments using message passing mechanisms that systematically vary the degree of data substitutability. Our results demonstrate three key findings that align with the theoretical predictions (detailed analysis in Appendix I):\n(1) under low curvature (propagation proportion = 0.0), all methods achieve strong performance with mean accuracy above 0.70, confirming the (1 - c)\u00b2 approximation guarantee predicted by Theorem 4.6; (2) as curvature increases through higher propagation proportions, we observe significant performance degradation across all methods, with mean accuracy dropping from around 0.74 to 0.59 at maximum substitutability (propagation proportion = 1.0); and (3) the convergence in performance between these game-theoretic methods at high curvature validates our analysis that all these approaches face similar fundamental limitations under strong substitution effects."}, {"title": "7.3. RQ3: How Effective is Bipartite-based Approximation for Data Selection?", "content": "Following the experimental settings detailed in Appendix H.6, the selection curves in Figure 4 demonstrate the clear advantages of our bipartite-based approach bipartite. On structured datasets like bbc-embeddings, our method exhibits remarkably steep initial performance improvements, achieving over 60% accuracy with just 20 samples, while other methods require 40-60 samples to reach similar performance levels. This pattern of superior early-stage selection is consistently observed across different datasets. For instance, on the MiniBooNE dataset, our method reaches 70%"}, {"title": "8. Conclusion", "content": "In this work, we establish a comprehensive theoretical framework that unifies data valuation and sequential decision-making, providing new insights into the role of data values in selection tasks. By reformulating data selection as a sequential optimization problem, we demonstrate that existing game-theoretic methods can be understood as myopic linear approximation solutions. Our analysis reveals fundamental limitations of these methods under high utility curvature, explaining their varying performance across dif-"}, {"title": "A. Related Work", "content": "A.1. Data Valuation\nData valuation seeks to quantify the contribution of individual training samples to model performance. Game-theoretic approaches have dominated this field, beginning with Data Shapley, which adapts the Shapley value from cooperative game theory to quantify data contributions. This seminal work inspired various extensions, including Beta Shapley, which introduces a family of semi-values through Beta function weighting, and Data Banzhaf, which provide robust valuation frameworks through binary-weighted marginal contributions. While these general methods are computationally intensive, specialized approaches like achieve near-linear time complexity by exploiting algorithmic structures such as K-Nearest Neighbors, compared to the exponential complexity of model-agnostic approaches. Recent developments address specific challenges in data valuation: proposed CS-Shapley for better handling class-wise contributions in classification tasks, while introduced PC-Winter value to tackle the unique challenges of graph-structured data valuation. Alternative theoretical frameworks, such as the Core, have also emerged to address coalition stability in data valuation.\nRecent work has focused on improving computational efficiency and valuation accuracy. proposed learning data utility functions to avoid repeated model retraining, while developed DU-Shapley as an efficient proxy through discrete uniform distributions. introduced EcoVal, which accelerates valuation by clustering similar data points and propagating values within clusters. P-Shapley leverages predicted probabilities instead of accuracy for finer-grained utility differentiation.\nParallel efforts have explored training-free and task-agnostic directions. DAVINZ enables data valuation at network initialization by theoretically deriving domain-aware generalization bounds, while proposed a task-agnostic framework based on statistical properties without validation requirements. Alternative paradigms include reinforcement learning-based approaches like DVRL, complexity-gap scoring, and Wasserstein distance-based frameworks, which offer diverse perspectives beyond traditional game-theoretic methods.\nA.2. Approximate Dynamic Programming\nMarkov Decision Process (MDP) provides one of the most fundamental frameworks for modeling sequential decision-making problems since the seminal work of Bellman. While MDPs can be solved optimally through dynamic programming, they suffer from the curse of dimensionality as the state and action spaces grow. To address these computational challenges, Approximate Dynamic Programming (ADP) has emerged as a powerful paradigm that decomposes the original problem into more tractable subproblems through various approximation strategies.\nSeveral key developments have shaped modern ADP approaches. established theoretical foundations for approximate linear programming in average-cost dynamic programming, while advanced the practical applicability through constraint sampling techniques. unified various competing strategies into a common framework, bridging the gap between communities such as stochastic programming, dynamic programming, and stochastic search. Recent work has focused on specific challenges: developed methods for continuous, convex decision sets in storage problems, while introduced distributionally robust ADP with guaranteed convergence properties. The effectiveness of ADP has been demonstrated across diverse applications. In game-playing domains, achieved breakthrough performance in Tetris through policy-based ADP, while extended these methods to two-player zero-sum Markov games. Recent advances continue to push boundaries, with developing tensor-based approximations for hybrid control systems and establishing polynomial-time guarantees for constrained reinforcement learning through novel ADP formulations.\nA.3. Parallel Work on Unifying Data Selection Methods\nRecent work by presents a comprehensive tutorial on data selection methods for foundation models, focusing on both heuristic and principled approaches to data curation. While their work provides valuable insights into the practical aspects of data selection in foundation model training pipelines, our work differs in several key aspects: First, we focus specifically on the optimization perspective of data selection with data values, providing theoretical guarantees of optimality under our proposed sequential decision making and approximate dynamic programming framework. Their"}, {"title": "B. Algorithm for Computing Optimal Data Values", "content": "We present the complete algorithm for computing optimal data values through dynamic programming in Algorithm 1. The algorithm proceeds in two phases: (1) backward induction to compute the optimal value function and policy, and (2) forward traversal to derive data values from the optimal selection sequence."}, {"title": "C. Proof of Game-theoretic Data Values as ADP Solutions", "content": "We provide a complete proof of Theorem 4.1, which establishes the equivalence between game-theoretic data valuation methods and specific ADP solutions. The theorem states that for any data values assigned by game-theoretic methods (such as Data Shapley, Beta Shapley, and Data Banzhaf), we can construct an equivalent ADP framework that both recovers the original values and generates identical selection sequences. The core of this equivalence lies in two key components: a linear surrogate function that models the reward signal, and a myopic policy that guides sequential decision-making. Through the perspective of ADP, we show that when the surrogate function \u00db(S) = \\sum_{ies} \u03b8i is properly fitted by minimizing ||U \u2013 \u00db ||2,\nits coefficients di exactly recover the original data values v(i). Moreover, the myopic policy #myopic(s) = \\argmax_{a\\in D\\s}\u03b8(a) generates a selection trajectory that precisely matches the sequence induced by ranking the original values."}, {"title": "C.1. Utility Function Approximation Analysis", "content": "Our sequential decision-making framework reveals that existing data valuation methods can be naturally interpreted with linear surrogate modeling. Specifically, these methods implicitly approximate the true utility function U : 2D \u2192 R via a linear model:\n\u00db (S) = \\sum_{i\\in S}\u03b8i = \\sum_{i}\u03b8i\u00b7 \\I_{i\\in S}\nwhere \u03b8i represents the coefficient for data point i, and \\I_{i\\in S} is the indicator function that equals 1 if i \u2208 S and 0 otherwise.\nAs introduced in (Li & Yu, 2024), the coefficients \u03b8\u2081 can be estimated through a constrained weighted least squares optimization:\nmin\\sum_{0\\neq S\\subset D}w(|S|)(U(S) \u2013 \u00db(S))^2\nsubject to \u00db (S)0\u2081 = U(D) \u2013 U(\u00d8)\nDifferent data valuation methods emerge from specific choices of the weighting function w(|S|):\n\u2022 Data Shapley: w(S) = (\\binom{n}{|S|})^{-1}\n\u2022 Data Banzhaf: w(S) = 2-|S|\n\u2022 Beta Shapley: w\u03b1,\u03b2(|S|)\nThese weighting schemes are specifically designed to recover the corresponding game-theoretic values in the limit of infinite samples. For instance, the combinatorial weights in Data Shapley ensure that the estimated coefficients converge to the exact Shapley values."}, {"title": "C.2. Value Recovery and Ordering Preservation", "content": "The equivalence between our ADP framework and game-theoretic data values is established through two key steps:\nLemma C.1 (Value Recovery). When the number of samples used in estimation approaches infinity, the estimated coefficients converge to the game-theoretic values:\n\\lim_{N\u2192\u221e}  = v(i), Vi\u2208D\nwhere represents the coefficient estimated using N samples.\nProof. The weighted least squares optimization with constraint fundamentally estimates the expected marginal contribution of each data point. When N \u2192 \u221e, this expectation converges to the exact marginal contributions used in game-theoretic definitions:\n\u03b8\u2081 = \\sum_{S\\subset D\\{i}}w(|S|)[U(S\\cup\\{i\\}) \u2013 U(S)] = v(i)\nThis convergence is guaranteed by the law of large numbers and the specific choice of weighting functions that match the corresponding game-theoretic methods.\nLemma C.2 (Myopic Policy Equivalence). Under the linear surrogate model, the myopic policy selections match the value-based ordering:\n\\sigma_{myopic}(St) = \\argmax_{a\\in D\\st}\u00db(st \u222a \\{a\\}) \u2013 \u00db(st) = \\argmax_{a\\in D\\st}\u03b8(a)\nProof. For any state st, the marginal gain of adding element a is:\n\u00db(st\u222a \\{a\\}) \u2013 \u00db (st) = \\sum_{i\u2208st\u222a\\{a\\}}\u03b8i\u00b7 \\I_{iestu{a}} - \\sum_{i}\u03b8i\u00b7 \\I_{iest}\n= \u03b8a\nTherefore, the myopic policy naturally selects elements in descending order of their coefficients."}, {"title": "D. Proof of Linear Utility Optimality", "content": "We establish the optimality of semi-value based methods under linear utility functions, inspired by the analytical steps developed by Wang et al. (2024b) for monotone transformed modular functions. Our analysis reveals that linear functions, as a special case, possess a fundamental property: they maintain consistent ordering of marginal contributions across different subset sizes. This property, combined with the weighted averaging nature of semi-values, ensures that greedy selection based on any semi-value achieves optimal sequential selection performance.\nLemma D.1 (Consistent Marginal Contributions). For a linear utility function U(S) = \\sum_{ies} wi, the marginal contribution of any element i remains constant regardless of the subset S:\nU(SU{i}) \u2013 U(S) = wi,\nVS CD\\{i}\nLemma D.2 (Semi-value Equivalence). Under a linear utility function, any semi-value based method assigns values proportional to the weights:\nv(i) = cwi\nwhere c > 0 is a constant determined by the specific semi-value weighting scheme.\nProof. For any semi-value based method:\nv(i) = \\sum_{SCD\\{i}} \u03c3i[U(SU{i}) \u2013 U(S)]\n\\sum_{SCD\\{i}} &|s|wi\nwi\\sum_{SCD\\{i}} wi\n= cwi\nwhere \\sum_{SCD\\{i}} &|s|wi= c > 0 by the definition of semi-values.\nTheorem D.3 (Sequential Selection Optimality). For any linear utility function U, selecting elements in descending order of their semi-values achieves optimal cumulative utility:\n\\sum_{k=1}^n U(Sk) = \\max_{\u03c0} \\sum_{k=1}^n U(S)\nProof. The proof follows in three steps:\n1) By Lemma D.2, the ordering based on semi-values v(i) is equivalent to ordering based on weights wi.\n2) For any size k, the subset Sk selected by this ordering contains the k elements with largest weights.\n3) Due to linearity, for any size k:\nU(Sk) = \\sum_{i \\in Sk} wi \u2265 \\sum_{U i\\in S^}wi = U(S)\nfor any alternative subset S of size k. Therefore:\n\\sum_{k=1}^n U(Sk) \u2265 \\sum_{k=1}^n U(S)\nfor any alternative sequence \u03c0, establishing optimality.\nThis proof shows that all semi-value methods achieve identical selection performance under linear utilities because they preserve the ordering of the underlying weights wi. This explains why different methods like Data Shapley, Beta Shapley, and Data Banzhaf all achieve optimal sequential selection despite their different weighting schemes."}, {"title": "E. Analysis of Data Valuation Methods Under Submodular Functions", "content": "In this appendix, we provide a complete proof that ranking-based element selection using any standard data valuation method achieves a (1 \u2013 c)2 approximation ratio for monotone submodular functions with curvature c. This result unifies previous analyses and provides tight bounds for Shapley value, Banzhaf value, and Leave-one-out methods, providing theoretical foundations for the results presented in Section 4.4 of the main paper.\nE.1. Preliminaries and Submodular Functions\nConsider a dataset D and a monotone submodular utility function U : 2D \u2192 R>o with U(\u00d8) = 0. For any sets A \u2282 B C D and element i \u2208 D\\B, monotone submodularity implies:\nU(A) \u2264 U(B) (monotonicity)\nU(AU{i}) \u2013 U(A) \u2265 U(BU {i}) \u2013 U(B) (submodularity)\nA fundamental property of submodular functions is their curvature, which measures how much marginal contributions decrease as sets grow. For a normalized (U(\u00d8) = 0) monotone submodular function U, its curvature c\u2208 [0, 1] is defined as:\nc = 1-\\min_{i\u2208D} \\frac{U(D) \u2013 U(D \\ {i})}{U({i})}\nThis definition implies that for any element i and set S \u2286 D \\ {i}:\nU(SU {i}) \u2013 U(S) \u2265 (1 \u2013 c)U({i})\nE.2. Data Valuation Methods and Curvature\nWe consider data valuation methods that assign a value v(i) to each element i \u2208 D as a weighted average of marginal contributions:\nv(i) = \\sum_{SCD\\{i}}w(S)[U(SU {i}) \u2013 U(S)]\nwhere weights w(S) \u2265 0 satisfy \\sum_{\u03c2} w(S) = 1. A particularly important class of such valuations are semi-values, which satisfy additional symmetry properties through size-based weights:\nw(S) = \u03b2|s| for some Bk \u2265 0 with \\sum_{k=0}^{D-1} (\\binom{D}{k})\u03b2k = 1\nwhere Bk represents the weight assigned to all subsets of size k.\nThis framework encompasses several important cases from the main paper:\n1. Data Shapley: w(S) x \\frac{|S|!(|D|-|S|-1)!}{|D|!}\n2. Data Banzhaf: w(S) = \\frac{1}{2^{|D|-1}}\n3. Leave-one-out: w(S) = 1 if S = D \\ {i}, 0 otherwise\nWe now establish a fundamental relationship between data values and singleton utility values under curvature constraints.\nLemma E.1 (Value-Curvature Relationship). For any element i \u2208 D, the data value v(i) and singleton value U({i}) satisfy:\nv(i) \u2265 (1 \u2013 c)U({i})\nProof. The proof follows from the curvature property and the structure of data valuations. For any set S C D \\{i}, the curvature definition ensures:\nU(SU{i}) \u2013 U(S) \u2265 (1 \u2013 c)U({i})"}, {"title": "E.3. Approximation Analysis", "content": "Inspired by the curvature-based analysis framework introduced by, we begin by establishing a fundamental relationship between the value of a set constructed through sequential selection and the individual values of its elements. This relationship forms the cornerstone of our approximation guarantee.\nLemma E.2 (Sequential Selection Bound). For any sequence of elements e1, ..., ek and their corresponding partial sets Si = {1, ..., ei} with So = \u00d8, we have:\nU(S) = \\sum_{i<k}USi-1(ei) \u2265 (1 - c)  MU(ei) \u2265 (1 \u2212 c) \\sum_{i<k}v(i)\nProof. The first equality expresses the value of set S as the sum of marginal contributions when elements are added sequentially. For the first inequality, we apply the curvature property to each term: US\u2081-1 (ei) \u2265 (1\u2212c)U({ei}). The second inequality follows from our earlier observation that for each element i, we have U({ei}) \u2265 v(i) by the submodularity.\nThis theorem formalizes the approximation guarantee mentioned in Section 4.4 of the main paper:\nTheorem E.3 (Submodular Approximation). Let U be a monotone submodular function with curvature c. Let Gk be the set of k elements selected by choosing elements in decreasing order of their data values v(\u00b7). Then:\nU(Gk) \u2265 (1 \u2013 c)\u00b2U(OPTk)\nwhere OPTk is an optimal solution to \\max_{|s|k} U(S).\nProof.\nU(Gk) \u2265 (1 \u2212 c) \\sum_{i\u2208Gk}v(1) (by Lemma E.2)\n\u2265 (1-c) \\sum_{i\u2208OPTK}v(i) (by greedy selection)\n\u2265 (1-c)^2 \\sum_{i\u2208OPTK}U({i}) (by Lemma E.1)\n\u2265 (1 \u2212 c)\u00b2U(OPTk) (by submodularity)"}]}