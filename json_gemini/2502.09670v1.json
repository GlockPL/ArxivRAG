{"title": "The Science of Evaluating Foundation Models", "authors": ["Jiayi Yuan", "Jiamu Zhang", "Andrew Wen", "Xia Hu"], "abstract": "The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.", "sections": [{"title": "1 Introduction", "content": "As the furor surrounding large language models (LLMs) shifts increasingly from their theoretical capabilities into examinations of practical applicability, relative comparisons between the multitude of models available on the market become ever more important. Questions such as \u201cbetween GPT-4, Claude 3.5, and Gemini, which is better?\" are becoming increasingly commonplace as individuals and organizations increasingly look to integrate LLMs into their workflows, particularly as the number of publicly available offerings increases [1, 91, 92].\nWhile at first glance, this might seem like a straightforward question with a simple one-word answer, it is nearly impossible to provide a definitive answer without knowing the specific task context: e.g., whether it is for customer service, code generation, or any other number of applications. This difficulty raises an important question: how do we evaluate LLMs effectively to identify the best choice for given applications? While the importance of rigorous evaluations is widely acknowledged [44, 87], the current research [10, 58] lacks a comprehensive and structured discussion on how to systemically approach LLM evaluation, particularly when needing to consider task context. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, and there exists no actionable evaluation guideline incorporating a cohesive process that integrates both the nuances of diverse use cases and the broader ethical and operational considerations.\nIn recognizance of this gap, in this paper, we aim to formalize the evaluation process for LLMs and offer actionable solutions. We do not aim to provide a new method for evaluation nor an exhaustive survey of the field. By breaking down the process step by step and grounding our approach in stringent literature, we aim to provide clarity and utility to researchers, practitioners, and decision-makers alike. Therefore, we named our paper \"The Science of Evaluation\", reflecting our intention to make this scattered effort more systematic, rigid, actionable, and near scientific work. The key contributions of this work are as follows:\n\u2022 Formalizing the Evaluation Process: We present a structured framework that defines the critical steps and considerations involved in evaluating LLMs, emphasizing the importance of aligning evaluation methods with specific use-case contexts.\n\u2022 Providing Actionable Tools and Frameworks: We introduce practical resources, such as checklists and documentation templates, to guide users through the evaluation process. These tools are designed to ensure evaluations are thorough, reproducible, and aligned with organizational needs.\n\u2022 Surveying Recent Work: While not exhaustive, we feature a targeted survey of recent advancements and methodologies in LLM evaluation, focusing on their application to real-world scenarios."}, {"title": "2 Preliminary: ABCD in Evaluation", "content": "In this section, we introduce key preliminary concepts essential for understanding the evaluation of LLMs. With the rapid advancements in AI, systemic evaluation of LLMs requires an interdisciplinary approach that spans model design, data utilization, computational infrastructure, and domain-specific knowledge. To organize this diverse set of requirements systematically, we propose the \u201cABCD in Evaluation\u201d framework, representing Algorithm, Big Data, Computation Resources, and Domain Expertise. Each component addresses a fundamental aspect of the evaluation process: the underlying algorithms driving LLMs, the role of vast and diverse datasets in training and testing, the computational and storage requirements for model serving and inference, and the importance of domain-specific knowledge to design meaningful evaluation scenarios. This structured framework provides"}, {"title": "2.1 Algorithm \u2013 Models", "content": "LLMs can be classified into closed-source and open-source models, each with distinct traits influencing deployment, accessibility, and adaptability. Closed-source models (e.g., GPT\u00b9, Claude\u00b2, and Gemini\u00b3) are proprietary systems delivering strong performance through rigorous optimization. However, their architectures and training data remain hidden, limiting customization and transparency while tying users to external providers for access, pricing, and data privacy considerations.\nConversely, open-source models provide greater transparency, community-driven development, and extensive opportunities for customization. Examples like LLaMA [76], Mistral [31], and Qwen [4] illustrate the diverse tasks and benchmarks they can address. Although they foster innovation and flexibility, open-source solutions can require substantial computational resources and technical expertise to deploy and maintain, posing challenges for organizations with limited infrastructure."}, {"title": "2.2 Big Data \u2013 Evaluation Datasets", "content": "Evaluating LLMs requires access to vast and diverse datasets to ensure robust and meaningful assessments. These datasets serve as the foundation for evaluating models across various dimensions, such as accuracy, robustness, ethical alignment, and domain-specific applicability.\nLarge-scale evaluation datasets are essential for covering the breadth of tasks that LLMs are expected to handle, from natural language understanding (e.g., classification and retrieval tasks) to natural language generation (e.g., summarization and translation). Publicly available benchmarks, such as GLUE [78] and SQUAD [64], provide standardized datasets for task-specific evaluations, enabling direct comparison across models. In addition to these benchmarks, domain-specific datasets-tailored for fields such as healthcare, legal"}, {"title": "2.3 Computing and Storage Resources", "content": "Deploying LLMs requires significant computational and storage resources, particularly during the inference (serving) phase. When deploying models in-house for evaluation, it is crucial to consider both the model and data to ensure compatibility with the available hardware infrastructure. Key considerations are as follows,\n\u2022 Model Parameters. The size of the model, determined by the number of parameters, directly influences memory requirements. For instance, a model with 7 billion parameters requires approximately 28 GB of memory, assuming 4 bytes per parameter. Larger models, such as those exceeding 100 billion parameters, demand exponentially more memory, often necessitating advanced hardware configurations. Selecting a model that aligns with available computational resources is vital for seamless evaluation.\n\u2022 GPU Memory. Adequate GPU memory is essential for efficient inference, as it stores model parameters and facilitates fast computations. High-performance GPUs, such as NVIDIA's A100 with 80 GB of memory or H100 with extended memory capacities, are widely used for deploying large models. For particularly large-scale models, distributed clusters of GPUs are required to handle memory-intensive operations, enabling parallel processing and reduced latency during inference.\n\u2022 Storage. Storage capacity is another critical factor in deploying LLMs, as it must accommodate both the model parameters and associated datasets. High-speed storage solutions, such as NVMe SSDs, significantly enhance data retrieval times, improving overall system performance. While local storage is optimal for performance-critical"}, {"title": "2.4 Domain Expertise", "content": "Domain expertise is crucial in evaluating LLMs, ensuring assessments are contextually relevant and aligned with specific application requirements. Experts guide the selection of evaluation metrics tailored to particular domains, enhancing the relevance of assessments. They also conduct human evaluations, providing qualitative insights into model outputs that automated metrics may miss. In high-stakes fields like healthcare, experts assess the accuracy and appropriateness of LLM-generated recommendations, identifying nuanced failure cases and offering actionable feedback for model improvement. This integration of domain knowledge bridges the gap between technical performance metrics and real-world applicability, underscoring the importance of multidisciplinary collaboration in advancing LLM evaluation. [75]\nIncorporating domain expertise into the evaluation process ensures that LLMs are rigorously tested and refined to meet the practical demands of their intended applications. Experts help develop more robust, reliable, and ethically sound Al systems by aligning technical assessments with domain-specific standards. This collaborative approach is essential for the responsible deployment of LLMs across various industries. [74]"}, {"title": "3 Dimensions of Evaluation", "content": "In this section, we explore the evaluation of model capabilities across a wide spectrum of general domain NLP tasks, ranging from foundational tasks like classification and extraction that test basic language understanding to more intricate challenges such as advanced inference and summarization."}, {"title": "3.1 Performance Metrics", "content": ""}, {"title": "3.1.1 Natural Language Understanding.", "content": "Text classification tasks are among the most fundamental in natural language understanding, requiring models to assign predefined labels to text inputs. This includes various applications, such as sentiment analysis, topic classification, spam detection, and intent recognition. Benchmarks such as SST-2 (Stanford Sentiment Treebank) [71], AG News [96], and IMDB Reviews [49] are commonly used for evaluating sentiment and topic classification. Frameworks like HELM [45] combine the abstract taxonomy of scenarios and metrics with a clear set of practical selections of implemented scenarios, prioritizing coverage, value, and feasibility. Entity/Word Extraction are tasks involving entity or word extraction that require models to identify and label entities or specific spans of text within a document. This category encompasses named entity recognition (NER), part-of-speech tagging, and keyword extraction. Benchmarks such as CONLL-2003 (for NER) [68] and OntoNotes 5.0 [59] are widely used in this domain. These datasets challenge models to recognize names, locations, dates, and other key information in text, often reflecting real-world scenarios like legal or medical document analysis. Natural Language Inference (NLI) tasks evaluate a model's ability to determine logical relationships between pairs of sentences, such as entailment, contradiction, or neutrality. Qin et al. [60] evaluate ChatGPT's zero-shot learning ability on NLI tasks, and Lee et al. [41] found that LLMs struggle with NLI tasks and fail to capture human disagreement, both highlighting its strengths and limitations. Popular datasets for NLI include SNLI (Stanford Natural Language Inference) [8], and MultiNLI [89], which feature sentence pairs"}, {"title": "3.1.2 Natural Language Generation.", "content": "Summarization tasks require models to condense long documents into concise summaries while retaining key information. Benchmarks such as CNN/Daily Mail [53], XSum [55], and Gigaword [67] are commonly used for this purpose. Summarization can be extractive (selecting key sentences) or abstractive (generating new, concise text). Recent studies suggest that LLMs demonstrate general proficiency in summarization tasks, with performance varying across model architectures and configurations. For instance, Liang et al. [44] observed that TNLG v2 (530B) achieves state-of-the-art results, surpassing models like OPT (175B) and fine-tuned Bart. These findings highlight the growing potential of evaluating LLMs for summarization. Text Completion tasks challenge models to generate coherent and contextually appropriate continuations for a given prompt. OpenAI's GPT-3 benchmarks [9] for completion often rely on tasks involving story or sentence completion, and datasets like WikiText [51] or BooksCorpus [100] are commonly used. Question Answering (QA) tasks test a model's ability to provide precise and relevant answers to posed questions based on a context passage or general knowledge. Benchmarks like SQUAD (Stanford Question Answering Dataset) [65], Natural Questions [37], and TriviaQA [33] are widely recognized in this area. Machine translation tasks involve translating text from one language to another, serving as a cornerstone application for many language models. Benchmarks like WMT [34] provide datasets that span multiple language pairs, allowing the evaluation of translation capabilities. Recent research highlights the growing potential of LLMs in such a domain. Wang et al. [83] reveal that GPT-4 and ChatGPT achieve strong human-evaluated performance, often surpassing commercial machine translation systems and many document-level neural machine translation models."}, {"title": "3.2 Robustness and Reliability", "content": "In this section, we discuss how to evaluate a model's robustness and reliability, focusing on two main types of challenges: natural perturbations and adversarial attacks."}, {"title": "3.2.1 Natural Perturbations.", "content": "Evaluating robustness to natural perturbations examines how models perform under real-world variations in data distribution and input quality. Distribution shifts occur when the test data diverges from the training data, a common issue in applications like sentiment analysis or machine translation, where language use varies across regions, demographics, and platforms. Benchmarks like WILDS [35] provide curated datasets reflecting these shifts, such as shifts in medical imaging data or demographic-specific Reddit comments. Noisy inputs include typographical errors, altered phrasing, or incomplete data that mimic real-world scenarios like chatbots encountering user typos. Benchmarks such as the NoiseQA [66] dataset for question answering or the TextFlint [85] toolkit for systematic noise injection simulate these challenges. The robustness of LLMs to prompts is among the most critical aspects of their evaluation. To assess this, Zhu et al."}, {"title": "3.2.2 Designed Adversarial Attacks.", "content": "Adversarial attacks involve carefully crafted inputs designed to exploit model vulnerabilities, presenting a distinct challenge compared to natural perturbations. Textual adversarial attacks like word-level substitution, paraphrasing, or syntactic manipulation aim to deceive models into producing incorrect outputs without altering the meaning of the input. For instance, the TextFooler [32] algorithm modifies keywords to retain semantics while misleading models and benchmarks like AdvGLUE [81] integrate adversarially perturbed data to stress-test systems. Gradient-based adversarial attacks exploit the internals of the model to generate adversarial examples. For instance, methods like HotFlip [21] leverage gradients to identify critical words to perturb, directly targeting neural architectures like transformers. Evaluation often combines metrics such as adversarial robustness (accuracy post-attack) and perturbation cost (measuring the effort required to deceive the model). Research into countermeasures, such as adversarial training (e.g. adversarially augmented datasets: RobustBench [15]), aims to fortify models while maintaining general performance on clean data."}, {"title": "3.3 Ethical and Fairness Considerations", "content": "Ensuring that the outputs of LLMs adhere to well-defined ethical principles and fairness standards is not just necessary-it's imperative. These principles extend beyond the basic requirement of avoiding discriminatory outcomes; they also encompass the need for defining equitable treatment, respecting the autonomy and dignity of all individuals, and ensuring that system behaviors are in harmony with universally recognized human values. To effectively address these complex issues, we categorize the considerations into two fundamental types: social bias, which pertains to the model's behavior in a broader societal context, and individual fairness, which focuses on the fair treatment of each person."}, {"title": "3.3.1 Social Bias.", "content": "Social bias in language models refers to systematic prejudices embedded in their outputs, often reflecting biases present in training data. These biases manifest as gender, racial, or cultural stereotypes and pose significant risks when deploying models in sensitive applications such as hiring, healthcare, or legal systems. For instance, models trained on web-scraped data may disproportionately associate certain groups with negative contexts or perpetuate outdated stereotypes, potentially leading to harmful outcomes. To quantify and address these biases, various benchmarks and datasets have been developed. Bias-in-Bios [17], StereoSet [52], and CrowS-Pairs [54] evaluate biases across diverse contexts. Social Bias Probing [50] introduces a large-scale dataset and perplexity-based fairness score to analyze LLMs' associations with societal categories and stereotypes. TWBias [29] focuses on biases in Traditional Chinese LLMs, incorporating chat templates to assess gender and ethnicity-related stereotypes within Taiwan's context. Similarly, BBQ (Bias Benchmark for QA) [57] provides question sets to reveal social biases against protected classes in U.S. English-speaking contexts. These tools highlight the need for robust evaluations to mitigate social biases in Al systems."}, {"title": "3.3.2 Individual Fairness.", "content": "Individual fairness emphasizes that similar individuals or inputs should receive consistent and equitable treatment from models, regardless of sensitive attributes such as gender, ethnicity, or age. This principle ensures that two inputs differing only in protected attributes yield equivalent predictions or scores. For instance, in a job recommendation system, the model should provide comparable job listings for resumes with similar qualifications, regardless of names that may indicate different genders. Datasets like ADULT [7], commonly used for income prediction, and COMPAS [19], utilized for recidivism risk prediction, are often employed to study individual fairness. These datasets enable researchers to evaluate biases that may arise in model predictions, offering valuable insights into whether models uphold equitable outcomes in practical scenarios."}, {"title": "3.4 Explainability and Interpretability", "content": "Technically, evaluating explanations involves human or automated model approaches. Human evaluations assess plausibility via the similarity between model rationales and human rationales or subjective judgments. However, these methods usually overlook faithfulness [97]."}, {"title": "3.4.1 Plausibility.", "content": "Evaluating the plausibility of LLM explanations involves assessing how well they align with human reasoning and expectations. Plausibility is often measured at the input text or token level, considering dimensions such as grammar, semantics, knowledge, reasoning, and computation [70]. For local explanations, metrics such as Intersection-Over-Union (IOU), precision, recall, F1 score, and area under the precision-recall curve (AUPRC) are commonly used to compare predicted rationales with human-annotated ones [18]. These"}, {"title": "3.4.2 Faithfulness.", "content": "Faithfulness examines whether explanations accurately reflect the model's internal reasoning. Quantitative metrics like comprehensiveness (change in predicted probability after removing top tokens) and sufficiency (effectiveness of extracted rationales for prediction) are widely used [18]. Other measures, such as Decision Flip - Fraction Of Tokens (DFFOT) and Decision Flip - Most Informative Token (DFMIT), evaluate the influence of individual tokens on predictions [13]. In the prompting paradigm, studies highlight that explanations, such as chain-of-thought (CoT) reasoning, can be systematically unfaithful. For instance, Turpin et al. [77] showed that GPT-3.5 and Claude 1.0 failed to acknowledge biases in few-shot prompts, generating misleading rationales. Smaller models often produce more faithful explanations than larger ones, indicating a trade-off between model capability and reasoning transparency [38]. To enhance faithfulness, decomposition methods that break tasks into subquestions have shown promise, improving alignment with underlying decision-making processes [63]. These findings emphasize the need for robust evaluation frameworks to ensure explanations genuinely reflect the reasoning behind predictions."}, {"title": "3.5 Safety and Controllability", "content": "The evaluation of safety and controllability is critical, especially in high-stakes scenarios such as healthcare, legal systems, and financial applications. In these domains, outputs from LLMs can have profound real-world consequences, making it imperative to ensure they do not produce unsafe, erroneous, or harmful content. This section provides an in-depth examination of benchmarks and methodologies for evaluating safety, concentrating on addressing hallucination and the potential for misuse."}, {"title": "3.5.1 Hallucination.", "content": "A hallucination occurs when LLMs produce content that is factually incorrect, logically unsound, or fabricated, posing substantial risks in domains such as healthcare and law. In medical scenarios, faulty drug interactions or diagnoses could lead to severe patient harm, while in legal settings, fabricated references to case law or statutes may undermine the integrity of judicial processes. Several benchmarks have been introduced to measure and address hallucination. The Hallucination Leaderboard by Vectara [30] utilizes the Hughes Hallucination Evaluation Model (HHEM-2.1) to gauge hallucination frequency and factual consistency in document summaries. HaluEval [42] comprises thousands of queries and task-specific examples to assess LLMs' ability to detect fabricated information in QA, dialogue, and summarization. The Hallucinations Leaderboard by Hugging Face [28] evaluates LLMs on tasks like open-domain QA and fact-checking, while LongHalQA [62] introduces long-context scenarios for multimodal models (MLLMs). AMBER [82] tests for various hallucination types across both generative and discriminative tasks with efficient methods."}, {"title": "3.5.2 Misuse and Risk.", "content": "Misuse evaluation addresses scenarios where LLMs are deliberately employed to produce harmful, deceptive, or unethical outputs, such as misinformation campaigns, propaganda, or phishing attempts. In these high-stakes environments, it is essential to ensure that models remain robust and fail-safe when prompted with malicious inputs, thereby preventing the generation of unsafe content. Several benchmarks have been developed to assess and mitigate these risks. A proposed risk taxonomy and assessment framework [16] systematically dissects potential threats by examining four modules-input, language model, toolchain, and output-and suggests targeted mitigation strategies. R-Judge [93] evaluates models' capacity to detect safety risks within multi-turn agent interactions. S-Eval [94] introduces an LLM-based approach for large-scale safety evaluation, using 220,000 prompts to scrutinize various risk categories and adversarial instructions. AgentHarm [3] focuses on LLM agents' resilience to misuse, testing 110 detailed behaviors across 11 harm categories. Together, these tools furnish a comprehensive framework for risk detection and mitigation, guiding the development of more secure and trustworthy Al systems."}, {"title": "4 Evaluation Methodologies", "content": "Objective tasks are predominantly associated with natural language understanding (NLU) applications, where clear ground truth labels are available. Models are evaluated based on their ability to accurately replicate or predict these labels, enabling precise comparisons across different systems."}, {"title": "4.1 Quantitative Eval. for Objective Tasks", "content": "These evaluations vary depending on the specific goals of each task, with distinct metrics tailored to capture performance effectively. In the following, we outline common NLU tasks and the metrics used to evaluate them.\nIn tasks such as sentiment analysis, topic classification, and named entity recognition (NER), models are assessed using metrics like accuracy, precision, recall, and F1-score, which collectively provide a comprehensive view of performance. For instance, accuracy measures the proportion of correct predictions, while precision and recall address the trade-off between relevance and completeness in the results. In information retrieval and passage ranking tasks, where models are tasked with ordering outputs by relevance, metrics like Mean Reciprocal Rank (MRR) [14], Normalized Discounted Cumulative Gain (nDCG) [86], and Average Precision at k (AP@k) are commonly used. For example, in MS MARCO [5], models are evaluated based on their effectiveness in ranking relevant documents at the top of the search results, and they reward both the precision of the highest-ranking results and the overall quality of the ranking. For extractive question-answering tasks, metrics such as Exact Match (EM) assess whether the model's output perfectly matches the ground truth, while F1-score evaluates partial overlap between predicted and true answers."}, {"title": "4.2 Quantitative Eval. for Subjective Tasks", "content": "Subjective tasks are more common in natural language generation (NLG) applications, where outputs are evaluated for qualities such as fluency, coherence, and semantic fidelity. Since ground truth in these tasks is often open to interpretation, evaluation relies on approximate metrics designed to capture content quality and similarity to reference outputs. To address the diverse requirements of NLG tasks, various metrics have been developed to evaluate different dimensions of content quality. These metrics can be broadly categorized into lexical, semantic, and diversity-based measures, each focusing on specific aspects of the generated text. Below, we discuss these categories in detail."}, {"title": "4.2.1 Content Quality.", "content": "Lexical Metrics: Metrics like ROUGE [47] (Recall-Oriented Understudy for Gisting Evaluation) and BLEU [56] (Bilingual Evaluation Understudy) measure lexical overlap between model outputs and reference texts. ROUGE is commonly used in summarization tasks, focusing on recall of n-grams, while BLEU, often applied in machine translation, emphasizes precision of n-gram matches. These metrics, even though they are straightforward, may still fail to account for semantic equivalence when lexical overlap is low.\nSemantic Metrics: To address the limitations of lexical metrics, semantic similarity measures like BERTScore [95] and METEOR [6] have gained popularity. BERTScore uses embeddings from large pre-trained models (e.g., BERT) to calculate token-level similarity, capturing meaning rather than surface forms. METEOR incorporates stemming and synonyms, improving evaluation for tasks like paraphrase generation and summarization.\nDiversity and Novelty Metrics: In creative tasks, such as storytelling or dialogue generation, metrics like Distinct-n [43] measure the diversity of generated outputs by counting unique n-grams. Novelty assesses the deviation of the output from training data or references, ensuring models produce varied and original content.\nQuality Trade-offs in Subjective Tasks: Subjective task metrics often reflect trade-offs between coherence, relevance, and diversity. For example, a model optimizing for BLEU [56] may sacrifice creativity in favor of exact matches, while prioritizing BERTScore [95] might enhance semantic fidelity at the cost of diversity. Balancing these trade-offs is a critical aspect of evaluating LLM-generated outputs."}, {"title": "4.2.2 Factuality and Truthfulness.", "content": "Ensuring factual accuracy and truthfulness is a critical aspect of evaluating language models, particularly in applications such as open-domain question answering, summarization, and conversational AI. Emerging metrics for factuality, including entailment-based metrics such as FactCC [36] and DAE [24], evaluate whether models generate factually accurate and truthful information. In addition, FEQA [20] and QAGS [79], which leverage question generation and answering (QGA) techniques, serve as factuality metrics. These metrics are particularly critical for tasks such as open-domain question answering, summarization, and conversational AI, where hallucinations or fabricated content can significantly undermine user trust.\nFactuality extends beyond verifying the correctness of information; it also involves a thorough evaluation of whether the outputs are ethically aligned, fair, and consistently reliable under diverse conditions. To comprehensively address these broader concerns, we delve into two critical subsets of factuality: Ethics and Bias, and Trust and Calibration.\nEthical and Bias: Metrics such as the fairness score and the bias amplification ratio aim to quantify the ethical alignment of models [22]. These metrics evaluate whether outputs perpetuate harmful stereotypes or exhibit fairness across demographic groups. For example, the Winogender schema tests whether pronoun resolution is influenced by gender stereotypes, and metrics like Equalized Odds measure the consistency of model predictions across protected attributes [80]. In addition, the Generalized Entropy Index [72] provides a versatile framework for quantifying inequality, capturing disparities in model performance or outcomes across different demographic groups. These metrics are crucial for ensuring fairness and mitigating biases in AI systems.\nTrust and Calibration: Metrics such as Expected Calibration Error (ECE) assess whether the model's confidence scores align with the actual prediction accuracy [25]. Well-calibrated models are essential in high-stakes applications where overconfidence or underconfidence in predictions can have severe consequences. Additionally, metrics like robustness to adversarial prompts assess the model's reliability when tackled with adversarial scenarios or challenging inputs [98]. Furthermore, the AUC of the selective accuracy and coverage provides a comprehensive measure of the trade-off between the accuracy of predictions and the proportion of covered data points, which allows the evaluation of model reliability in selective prediction tasks [23]."}, {"title": "4.2.3 Emerging Metrics.", "content": "There are also some very new metrics specifically designed for day-to-day usage with LLMs. For example, DRFR [61] evaluates the ability to follow instructions, while Human-AI Language-based Interaction Evaluation (HALIE) [40] underscores the importance of assessing the interactive process itself. With the rise of more interactive AI applications, AntEval [46] has been proposed to assess social interaction competencies in LLM-driven agents. AntEval establishes complex multi-agent environments that encourage information exchange and intention expression, providing metrics such as Information Exchanging Precision (IEP) and Interaction Expressiveness Gap (IEG) to quantitatively measure interaction skills. These newer metrics emphasize the naturalness and responsiveness of LLMs in realistic, often open-ended scenarios-like conversational or collaborative tasks-and thereby complement more traditional, static evaluation approaches."}, {"title": "4.3 Qualitative Evaluation", "content": "Qualitative evaluation focuses on human judgments and interpretative assessments of model output, providing insights that quantitative metrics often miss. These approaches are particularly useful for capturing nuances like contextual appropriateness, creativity, and ethical considerations. They involve subjective evaluation criteria and often require human annotators or expert reviewers. Fairness metrics include Demographic Parity (measuring uniform prediction distributions across groups), Equalized Odds (ensuring similar error rates across groups), and Counterfactual Fairness, which evaluates outcomes in counterfactual scenarios where sensitive attributes are altered [80]."}, {"title": "4.3.1 Human Evaluation.", "content": "Human evaluation remains the gold standard for assessing model output in tasks such as open-ended text generation, dialogue systems, and creative writing. Annotators are asked to rate the model output in predefined dimensions such as fluency, relevance, coherence, and engagement [44]. For example, QUEST [75] is a comprehensive framework for the human evaluation of LLMs in healthcare, and LalaEval [73] offers a holistic human evaluation framework for domain-specific LLMs."}, {"title": "4.3.2 Case Study and Error Analysis.", "content": "Qualitative approaches also emphasize case studies and error analysis, where researchers manually inspect model output to understand specific failures and limitations. For example, in high-stakes domains like healthcare or law, analysts can examine whether models provide incorrect or misleading recommendations, offering insights into robustness and safety concerns. By categorizing errors into types, such as factual inaccuracies or ethical violations, error analysis can guide targeted improvements in model design. Error Analysis Prompting [48] is a method that enables LLMs to perform human-like translation evaluations. Alemayehu et al. [2] conducted an error analysis of multilingual language models in machine translation, focusing on English-Amharic translation."}, {"title": "5 Framework for Evaluations", "content": "Evaluating large language models (LLMs) effectively is a nuanced process involving the selection of appropriate benchmarks, the identification of meaningful metrics, and the careful consideration of resource constraints and domain-specific needs. In this section, we propose a structured framework that guides practitioners through three stages: (1) establishing a checklist for evaluation preparation, (2) conducting applicability analysis and iterative refinement, and (3) maintaining comprehensive documentation for transparency and longevity. By following this framework, evaluators can systematically approach LLM assessment, ensuring that each evaluation is both task-appropriate and orderly documented."}, {"title": "5.1 Checklist for Evaluation Preparation", "content": "The checklist of evaluating an LLM is guided by the core ABCD principles-Algorithm, Big Data, Computational Resources, and Domain Expertise. Table 2 outlines a concise sequence of steps to ensure a solid foundation for your evaluations. By defining objectives and priorities early, practitioners can more efficiently align each step of the process with relevant algorithmic choices, data considerations, resource constraints, and domain-specific requirements.\nBeginning with this ABCD-aligned checklist ensures a structured evaluation roadmap. By explicitly referencing Algorithm, Big Data, Computational Resources, and Domain Expertise at each step, practitioners can tailor their approach to the specific modeling frameworks, data requirements, resource constraints, and context-critical considerations that define successful LLM evaluations."}, {"title": "5.2 Applicability Analysis and Refinement", "content": "In the preceding subsections, we have discussed various approaches to evaluating each of these dimensions. Although it is certainly true that it is desirable for an LLM to perform well across all evaluation dimensions, some models will inevitably excel in certain areas while being less effective in others. A comprehensive execution of these various benchmarks, depending on the size of the data involved, can be extremely resource consumptive, to the point of being prohibitive for practical implementation.\nSimilarly, in applications where domain specificity is required or important for evaluation (e.g., law and healthcare), datasets will likely need to be prepared for each evaluation. This is particularly pertinent to healthcare, as documentation practices differ significantly between individual healthcare institutions, causing substantial variations in task performance for data-driven algorithms. To require that localized datasets for every combination of dimension and evaluation methodology be created is thus largely infeasible.\nWe posit, however, that not all dimensions are necessary for any given task, or at the very least, only a subset of the"}, {"title": "5.3 Maintenance and Documentation", "content": "After refining the evaluation process, maintaining comprehensive records and transparent documentation is essential. Proper documentation allows others to understand the context of the evaluation, replicate the methodology, and build upon the results. To achieve this, documentation includes:\n\u2022 Evaluation Setup: Clearly state the task objectives, prioritized dimensions, chosen metrics, and justifications.\n\u2022 Datasets and Benchmarks: Provide details about dataset sources, preprocessing steps, and representativeness, as well as benchmark models used.\n\u2022 Model Details: Describe the models under evaluation, including training data characteristics, fine-tuning procedures, and any custom modifications.\n\u2022 Prioritization and Weighting: Disclose how certain dimensions and metrics were weighted over others, allowing for fair comparisons and informing future research decisions.\n\u2022 Results and Analysis: Present findings alongside appropriate baselines, confidence intervals, and contextual explanations, noting trade-offs (e.g., accuracy vs. fairness).\n\u2022 Employing standardized documentation tools such as model cards, data sheets, or transparency reports can streamline this process. Thorough, organized documentation not only increases trust and reproducibility but also sets the stage"}, {"title": "6 Challenges and Future Directions", "content": "Evaluating LLMs remains a multifaceted endeavor, shaped by domain-specific requirements, evolving data distributions, and broader societal considerations. Existing benchmarks, such as GLUE or HELM, often lack the granularity to capture specialized tasks-for instance, clinical subtasks within MedQA-and typically focus on predominantly English-language datasets. These limitations underscore the need for domain-specific evaluations that address underrepresented languages and specialized domains (e.g., certain medical subspecialties). Handling dynamic environments presents an additional challenge: LLMs frequently encounter shifting data distributions and unforeseen requirements in real-world settings, necessitating continual evaluation frameworks and active monitoring methods (e.g., the ARPA-H PRECISE-AI\u00b9 effort) for early detection of aberrations and performance drift. Furthermore, optimizing solely for performance can exacerbate biases or obscure transparency, prompting the development of multi-objective frameworks that weigh interpretability and fairness alongside technical metrics. Finally, evaluations must look beyond immediate performance to anticipate long-term societal implications such as misinformation spread, highlighting the need for responsible governance and policy considerations.\nA promising direction lies in adopting multiagent evaluation frameworks that treats each stakeholder or component as an \"agent\" with distinct roles and objectives [26]. Domain experts would define specialized tasks and criteria; data curators would assemble representative datasets; metric designers would refine existing measures or propose new ones; and evaluators-human or automated-would apply metrics to yield timely insights [90]. By enabling negotiation and collaboration among these agents, evaluations can adapt more fluidly to domain-specific needs, accommodate new metrics or data sources, and dynamically respond to emerging societal priorities. Moreover, this multiagent approach can systematically address challenges in specialized domains: for instance, agents specializing in clinical knowledge can generate targeted questions and updates to keep pace with evolving medical standards. Such a system also supports continuous learning and drift monitoring, making it easier to detect performance issues or biases early and adjust accordingly. Ultimately, multiagent frameworks and domain-specific strategies can help guide the development of more"}, {"title": "7 Related Literature", "content": ""}, {"title": "7.1 Surveys of LLM evaluation.", "content": "Evaluating large language models has gained significant attention, leading to various comprehensive surveys that explore different facets of this domain. Chang et al. [10", "27": "delves into the challenges and limitations of current LLM evaluation"}]}