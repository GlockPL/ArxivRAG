{"title": "DEFENDING DEEP REGRESSION MODELS AGAINST\nBACKDOOR ATTACKS", "authors": ["Lingyu Du", "Yupei Liu", "Jinyuan Jia", "Guohao Lan"], "abstract": "Deep regression models are used in a wide variety of safety-critical applications,\nbut are vulnerable to backdoor attacks. Although many defenses have been pro-\nposed for classification models, they are ineffective as they do not consider the\nuniqueness of regression models. First, the outputs of regression models are con-\ntinuous values instead of discretized labels. Thus, the potential infected target of a\nbackdoored regression model has infinite possibilities, which makes it impossible\nto be determined by existing defenses. Second, the backdoor behavior of back-\ndoored deep regression models is triggered by the activation values of all the neu-\nrons in the feature space, which makes it difficult to be detected and mitigated us-\ning existing defenses. To resolve these problems, we propose DRMGuard, the first\ndefense to identify if a deep regression model in the image domain is backdoored\nor not. DRMGuard formulates the optimization problem for reverse engineer-\ning based on the unique output-space and feature-space characteristics of back-\ndoored deep regression models. We conduct extensive evaluations on two regres-\nsion tasks and four datasets. The results show that DRMGuard can consistently\ndefend against various backdoor attacks. We also generalize four state-of-the-art\ndefenses designed for classifiers to regression models, and compare DRMGuard\nwith them. The results show that DRMGuard significantly outperforms all those\ndefenses.", "sections": [{"title": "1 INTRODUCTION", "content": "Regression techniques are widely used to solve tasks where the goal is to predict continuous values.\nUnsurprisingly, similar to their classification counterparts, regression techniques have been revolu-\ntionized with deep learning and have achieved the state-of-the-art result in many real-world applica-\ntions. Examples such as gaze estimation (Zhang et al., 2017b; 2019), head pose estimation (Borghi\net al., 2017; Kuhnke & Ostermann, 2019), and facial landmark detection (Sun et al., 2013; Wu & Ji,\n2019), among many others (Lathuili\u00e8re et al., 2019). Unfortunately, deep regression models (DRM)\ninherited the vulnerabilities of deep neural networks (Gu et al., 2019; Liu et al., 2018b; Nguyen\n& Tran, 2021; 2020; Turner et al., 2019) and did not escape from the threat of backdoor attacks.\nExisting work (Sun et al., 2022) shows that an attacker can inject a backdoor trigger into a DRM\nsuch that it outputs an attacker-chosen target vector for any input stamped with an attacker-chosen\nbackdoor trigger, while its predictions for clean inputs are unaffected. Therefore, given the wide\nadoption of DRM in many safety-critical applications such as driver attention monitoring (Abuel-\nsamid, 2020; Berman, 2020), navigation of autonomous vehicles (Zeisl et al., 2015), and pedestrian\nattention monitoring (Raza et al., 2018; Schulz & Stiefelhagen, 2012), backdoor attacks raise severe\nsafety concerns about the trustworthiness and robustness of DRMs.\nExisting solutions to defend deep classification model (DCM) against backdoor attacks can be di-\nvided into data-level (Chen et al., 2018; Guo et al., 2023) and model-level defenses (Liu et al., 2019;\nWang et al., 2019; 2022b). Data-level defenses aim to detect backdoored training or testing data,\nwhile model-level defenses aim to detect a potentially backdoored model and unlearn the backdoor\nbehaviors. As we will discuss in Section 2, our work focuses on model-level defenses, as they are\nmore realistic and do not assume the defender has access to the backdoored training or testing data.\nHowever, existing model-level defenses (Wang et al., 2022b; 2019; Wu & Wang, 2021) are designed\nfor DCMs. Our experiments show that they are ineffective when generalized and applied to DRMS"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Backdoor Attacks: Many backdoor attacks (Chen et al., 2017; Gu et al., 2019; Liu et al., 2018b;\nPhan et al., 2022; Wang et al., 2022a;c; Yao et al., 2019; Zhao et al., 2022) have been proposed for\ndeep neural networks. They showed that an attacker can inject a backdoor into a classifier and make\nit output an attacker-chosen target class for any input embedded with an attacker-chosen backdoor\ntrigger. Depending on whether the attacker uses the same backdoor trigger for different testing\ninputs, we categorize existing attacks into input-independent attacks (Chen et al., 2017; Gu et al.,\n2019; Liu et al., 2018b; Turner et al., 2019; Yao et al., 2019) and input-aware attacks (Koffas et al.,\n2022; Li et al., 2021b; Nguyen & Tran, 2021; 2020; Salem et al., 2022). For instance, Gu et al.\n(2019) proposed an input-independent backdoor attack by using a fixed pattern, e.g., a white patch,\nas the backdoor trigger. Recently, researchers proposed to use input-aware techniques, such as the\nwarping process (Nguyen & Tran, 2021) and generative models (Nguyen & Tran, 2020), to generate\ndynamic triggers varying from input to input. When extending those attacks to DRMs (Sun et al.,\n2022), an attacker can inject a backdoor and make the model output a fixed vector (called target\nvector) for any testing input with the backdoor trigger. Lastly, backdoor attacks were also studied\nfor graph neural networks (Xi et al., 2021; Zhang et al., 2021) and natural language processing (Shen\net al., 2021). They are out of the scope of this paper as we focus on attacks in the image domain.\nExisting Defenses: We categorize existing defenses against backdoor attacks into data-level de-\nfenses (Doan et al., 2020; Gao et al., 2019; Ma et al., 2023) and model-level defenses (Liu et al.,\n2022; 2019; Wu & Wang, 2021; Xiang et al., 2022; Zeng et al., 2022; Zheng et al., 2022). Data-level\ndefenses detect whether a training example or a testing input is backdoored or not. They usually have\ntwo major limitations: 1) training data detection defenses (Chen et al., 2018) are not applicable for\na given model that is already backdoored; and 2) testing input detection defenses (Doan et al., 2020)\nneed to inspect each testing input at the running time and incur extra computation cost, and thus are\nundesired for latency-critical applications, e.g., gaze estimation (Zhang et al., 2020). Therefore, we\nfocus on model-level defense in this work.\nModel-level defenses detect whether a given model is backdoored or not, and state-of-the-art meth-\nods (Guan et al., 2022; Qiao et al., 2019; Wang et al., 2019; 2022b; Xiang et al., 2022) are based"}, {"title": "3 DESIGN OF DRMGUARD", "content": "3.1 THREAT MODEL\nDeep regression model: A deep regression model (DRM) is a deep neural network that maps\nan input to a vector, i.e., $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{X} \\subset \\mathbb{R}^{N_w \\times N_h \\times N_c}$ represents the input space with\nwidth $N_w$, height $N_h$, and channel $N_c$; and $\\mathcal{Y} \\in \\mathbb{R}^d$ represents the d-dimensional output space.\nGiven a training dataset $\\mathcal{D}_{tr}$ that contains a set of training examples, we define the following loss\n$\\sum_{(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{D}_{tr}} \\ell(f(\\mathbf{x}), \\mathbf{y})$, where $(\\mathbf{x}, \\mathbf{y})$ is a training example in $\\mathcal{D}_{tr}$ and $\\ell$ is the loss function for\nthe regression task (e.g., $\\ell_2$ loss) to update the parameters of $f$.\nBackdoor attacks: We consider existing backdoor attacks for classification models (Gu et al.,\n2019; Nguyen & Tran, 2021; 2020; Turner et al., 2019), and adapt them to DRMs. Specifically,\ngiven a training dataset $\\mathcal{D}_{tr}$, an attacker can add backdoor triggers to the training samples in $\\mathcal{D}_{tr}$,\nand change their ground-truth annotations to an attacker-chosen vector, $\\mathbf{y}_\\tau \\in \\mathcal{V}$, known as the target\nvector. The attacker can manipulate the training process. The backdoored DRM performs well on\nbenign inputs, but outputs the target vector $\\mathbf{y}_\\tau$ when the backdoor trigger is present in the input.\nFormally, we define the backdoor attack for DRMs as:\n$$f(\\mathbf{x}) = \\mathbf{y}, f(\\mathcal{A}(\\mathbf{x})) = \\mathbf{y}_\\tau,$$\nwhere $f$ is the backdoored DRM, $\\mathbf{x} \\in \\mathcal{X}$ is the benign input; $\\mathbf{y} \\in \\mathcal{Y}$ is the benign ground-truth\nannotation; and $\\mathcal{A}$ is the trigger function that constructs the poisoned input from the benign input.\nEvaluation metric for backdoor attacks: Given a set of poisoned inputs, we define attack error\n(AE) as the average regression error calculated from the output vectors and the target vector over all\nthe poisoned inputs, to evaluate the performance of backdoor attacks on DRMs. AE can be regarded\nas the counterpart to the attack success rate for backdoor attacks on classification models.\nAssumptions and goals of the defender: The defense goal is to identify if a DRM has been\nbackdoored or not. Following existing defenses for backdoor attacks (Liu et al., 2019; Wang et al.,\n2019; 2022b), we assume the defender can access the trained DRM and a small benign dataset $\\mathcal{D}_{be}$\nwith correct annotations."}, {"title": "3.2 OVERVIEW OF DRMGUARD", "content": "We propose DRMGuard to identify if a DRM has\nbeen backdoored by reverse engineering the trig-\nger function $\\mathcal{A}$. Figure 1 shows the overview of\nDRMGuard. We use a generative model $G_\\theta$ to\nmodel $\\mathcal{A}$. This allows us to model the trigger\nfunction for both input-independent and input-\naware attacks. For a given DRM $f$ under exam-\nination, we split it into two submodels. Specifi-\ncally, we first use the submodel $F$ to map the original input $\\mathbf{x}$ to the feature space $F(\\mathbf{x}) \\in \\mathbb{R}^m$,\ni.e., the output space of the last but second layer of $f$. Then, we use the submodel $H$ to map the\nintermediate feature from the feature space to the final output space. This allows us to investigate\nthe characteristics of the backdoored DRMs in both feature space and output space, based on which\nwe formulate the optimization problem for reverse engineering. Moreover, we propose a strategy\ncalled momentum reverse trigger to reverse high-quality triggers."}, {"title": "3.3 OBSERVATIONS AND INTUITIONS FOR BACKDOORED DEEP REGRESSION MODEL", "content": "Reverse engineering is performed by solving an optimization problem with constraints that are de-\nsigned based on observations in the input (Wang et al., 2019) or the feature space (Wang et al.,\n2022b). Existing work (Wang et al., 2022b) shows that by using feature-space constraints, one can\nreverse both input-independent trigger (Gu et al., 2019) and input-aware trigger (Nguyen & Tran,\n2021; 2020) for a backdoored deep classification model (DCM). Following this trend, we consider\nfeature-space constraints when designing the reverse engineering for DRM. Below, we first discuss\nthe difference in the feature space between backdoored DCM and backdoored DRM. Then, through\ntheoretical analysis and experiments, we introduce the key observation for backdoored DRM.\nDifference between backdoored DCM and DRM: A key observation for backdoored DCMs is\nthat the backdoor behavior is represented by the activation values of several neurons in the feature\nspace (Liu et al., 2019; Wang et al., 2022b). Specifically, when a trigger is present in the input, the\nactivation values of the affected neurons will drop into a certain range, making the backdoored DCM\noutput the attacker-chosen target class regardless of the activation values of the other neurons. This\nis because, after applying a series of operations to the feature vector, a backdoored DCM utilizes\narg max to obtain the final classification output. As long as the activation values of the affected\nneurons can make the target class have the highest probability, the influence of the other neurons on\nthe final classification output will be eliminated by arg max. By contrast, the final regression output\nof a backdoored DRM is obtained by applying linear transformation (or followed by an activation\nfunction) to the feature vector without using arg max. Thus, the activation value of each neuron in\nthe feature space contributes to the final output. This difference inspires us to take all the neurons\ninto consideration when searching for the feature-space characteristics of backdoored DRMs, rather\nthan looking at a few specific neurons only.\nTheoretical analysis and metrics: We use $\\{\\mathbf{h}_i\\}_{i=1}^N$ and $\\{\\mathbf{h}_i'\\}_{i=1}^N$ to denote the feature vectors\nextracted from a set of $N$ benign inputs $\\{\\mathbf{x}_i\\}_{i=1}^N$ and a set of poisoned inputs $\\{\\mathcal{A}(\\mathbf{x}_i)\\}_{i=1}^N$, respec-\ntively, where $\\mathbf{h}_i = F(\\mathbf{x}_i) \\in \\mathbb{R}^m$ and $\\mathbf{h}_i' = F(\\mathcal{A}(\\mathbf{x}_i)) \\in \\mathbb{R}^m$. We use $y_{i,j}$ and $y_{i,j}'$ to denote the $j$th\ncomponent of the output vector $\\mathbf{y}_i = H(\\mathbf{h}_i) \\in \\mathbb{R}^d$ and $\\mathbf{y}_i' = H(\\mathbf{h}_i') \\in \\mathbb{R}^d$. $y_{i,j}$ is calculated by:\n$$y_{i,j} = \\Omega(\\mathbf{w}_j^T \\mathbf{h}_i + b_j) = \\Omega(||\\mathbf{w}_j||_2 ||\\mathbf{h}_i||_2 \\cos{\\alpha_{i,j}} + b_j),$$\nwhere $\\Omega(\\cdot)$ is the activation function; $\\mathbf{w}_j \\in \\mathbb{R}^m$ and $b_j \\in \\mathbb{R}$ are the weights vector and the bias of $H$\nfor the $j$th component of the output vector, respectively; $\\alpha_{i,j}$ is the angle between $\\mathbf{h}_i$ and $\\mathbf{w}_j$. Based\non Equation 1, we have $y_{i,j} \\approx y_{i',j} \\approx \\approx y'_{i,j}$ if $f$ is backdoored, which means $\\sigma^2(\\{y_{i,j}\\}_{i=1}^N)$\nis a small positive value, where $\\sigma^2(\\cdot)$ is the variance function. As shown in Equation 2, the value of\n$\\sigma^2(\\{y_{i,j}\\}_{i=1}^N)$ is influenced only by $||\\mathbf{h}_i||_2$ and $\\alpha_{i,j}$, as $||\\mathbf{w}_j||_2$ and $b_j$ are constant for a given DRM.\nMoreover, when $f$ is backdoored, $\\frac{\\sum_{j=1}^d \\sigma^2(\\{y_{i,j}\\}_{i=1}^N)}{d}$ is a small positive value and influenced\nby $||\\mathbf{h}_i||_2$ and $\\boldsymbol{\\alpha}$, where $\\boldsymbol{\\alpha} = \\{\\alpha_{i,1}, ..., \\alpha_{i,d}\\} \\in \\mathbb{R}^d$. We use $\\alpha_{i,j}$ to denote the angle between $\\mathbf{h}_i$\nand $\\mathbf{w}_j$, and define $\\boldsymbol{\\alpha}_i$ as $\\boldsymbol{\\alpha}_i = \\{\\alpha_{i,1}, ..., \\alpha_{i,d}\\} \\in \\mathbb{R}^d$.\nTo further investigate how $||\\mathbf{h}_i||_2$ and $\\boldsymbol{\\alpha}$ influence $\\frac{\\sum_{j=1}^d \\sigma^2(\\{y_{i,j}\\}_{i=1}^N)}{d}$, we introduce the ratio\nof norm variance (RNV) and the ratio of angle variance (RAV), as two feature-space metrics:\n$$RNV = \\frac{\\sigma^2(\\{||\\mathbf{h}_i'||_2\\}_{i=1}^N)}{\\sigma^2(\\{||\\mathbf{h}_i||_2\\}_{i=1}^N)} \\text{ and } RAV = \\frac{\\sum_{j=1}^d \\sigma^2(\\{\\alpha_{i,j}'\\}_{i=1}^N)}{\\sum_{j=1}^d \\sigma^2(\\{\\alpha_{i,j}\\}_{i=1}^N)}$$\nSpecifically, RNV compares the dispersion of $\\{||\\mathbf{h}_i'||_2\\}_{i=1}^N$ and $\\{||\\mathbf{h}_i||_2\\}_{i=1}^N$, while RAV compares\nthe dispersion of $\\{\\boldsymbol{\\alpha}_i'\\}_{i=1}^N$ and $\\{\\boldsymbol{\\alpha}_i\\}_{i=1}^N$. RNV $\\ll 1$ indicates that when triggers are present in the\ninputs, the feature vectors extracted by $F$ have similar norms. RAV $\\ll 1$ means that the variance of\nangles between $\\{\\mathbf{h}_i'\\}_{i=1}^N$ and $\\mathbf{w}_j$ are much smaller than that between $\\{\\mathbf{h}_i\\}_{i=1}^N$ and $\\mathbf{w}_j$ for $j = 1, ..., d$.\nObservations: We use four backdoor attacks, i.e., BadNets (Gu et al., 2019), Input-aware dynamic\nattack (IA) (Nguyen & Tran, 2020), WaNet (Nguyen & Tran, 2021), and Clean Label (Turner et al.,\n2019), to train backdoored DRMs on MPIIFaceGaze dataset (Zhang et al., 2017a) and Biwi Kinect\ndataset (Fanelli et al., 2013). Table 1 shows the RNV and the RAV of the backdoored DRMs that\nare trained by different backdoor attacks on the two datasets. The key observation is that RAV is\nsignificantly smaller than 0.1 in all the examined scenarios. To further explore this observation,\nthe scatter plots in Figure 2 visualize $\\{\\boldsymbol{\\alpha}_i'\\}_{i=1}^N$ and $\\{\\boldsymbol{\\alpha}_i\\}_{i=1}^N$ in all the examined cases. We can see"}, {"title": "3.4 METHODOLOGY", "content": "Reverse engineering for DRM: One major challenge for reverse engineering for DRM is that the\ntarget vector $\\mathbf{y"}, "tau$ is defined in the continuous output space. As a result, it is impossible to enumer-\nate and analyze all the potential target vectors using existing reverse engineering methods designed\nfor DCM, which treat each class as a potential target class and reverse the trigger for it (Wang\net al., 2019; 2022b). To resolve this challenge, we propose to reverse engineer $\\mathcal{A}$ by minimizing:\n$\\sum_{j=1}^d \\sigma^2(\\{f_j(G_\\theta(\\mathbf{x}_i))\\}_{i=1}^N) / d$, where $f_j(G_\\theta(\\mathbf{x}_i))$ is the $j$th component of $f(G_\\theta(\\mathbf{x}_i)) \\in \\mathbb{R}^d$. This\nis intuitive, as for a backdoored of the term $\\sum_{j=1}^d \\sigma^2(\\{y_{i,j}\\}_{i=1}^N)$ will be a small positive value.\nThus, we can search for the target vector in the continuous output space by learning a trigger func-\ntion $\\mathcal{A}$, modeled by $G_\\theta$, that can mislead $f$ to map different inputs to the target vector without"]}