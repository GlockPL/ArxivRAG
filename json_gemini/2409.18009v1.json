{"title": "Control Industrial Automation System with Large Language Models", "authors": ["Yuchen Xia", "Nasser Jazdi", "Jize Zhang", "Chaitanya Shah", "Michael Weyrich"], "abstract": "Traditional industrial automation systems require specialized expertise to operate and complex reprogramming to adapt to new processes. Large language models offer the intelligence to make them more flexible and easier to use. However, LLMs' application in industrial settings is underexplored. This paper introduces a framework for integrating LLMs to achieve end-to-end control of industrial automation systems. At the core of the framework are an agent system designed for industrial tasks, a structured prompting method, and an event-driven information modeling mechanism that provides real-time data for LLM inference. The framework supplies LLMs with real-time events on different context semantic levels, allowing them to interpret the information, generate production plans, and control operations on the automation system. It also supports structured dataset creation for fine-tuning on this downstream application of LLMs. Our contribution includes a formal system design, proof-of-concept implementation, and a method for generating task-specific datasets for LLM fine-tuning and testing. This approach enables a more adaptive automation system that can respond to spontaneous events, while allowing easier operation and configuration through natural language for more intuitive human-machine interaction. We provide demo videos and detailed data on GitHub: https://github.com/YuchenXia/LLM4IAS", "sections": [{"title": "I. INTRODUCTION", "content": "Traditional industrial automation systems are rigid, requiring specialized expertise for any modification or reconfiguration. For instance, when the system needs to be adapted to produce new product variants or execute different operations, significant effort is required to design and implement the necessary changes. This process is often hampered by several challenges, including the need for an in-depth understanding of the complicated equipment and the time-consuming effort in translating user requirements into executable programs. These factors contribute to delays and increased costs, with reconfiguration often constrained by knowledge barriers, the intricate nature of reprogramming tasks, and possibly inefficient communication between user and programmer. As a result, traditional industrial automation systems are not only inflexible but also costly and time-inefficient when adapting to new demands[1].\n\nLarge language models offer transformative potential in industrial automation. They can perform reasoning based on the knowledge internalized during pre-training, interpret both general and domain-specific language, and generate on-demand responses to varied inputs. While LLMs have demonstrated their utility in general chatbot applications[2], their tailored application in industrial contexts remains underexplored. The challenge lies in effectively adapting these capabilities to deliver tangible value in the industrial domain.\n\nA structured approach is required to link the digital functionality of LLMs with the physical realm of industrial automation.\n\nIn this paper, we introduce a novel framework for controlling and configuring industrial automation equipment using large language models, enabling more flexible, intuitive and knowledge-informed automation systems. As our contribution, this framework includes:\n\n\u2022\tAn integral system design for applying large language models in industrial automation.\n\u2022\tA proof-of-concept implementation on a physical production system with quantitative evaluation.\n\u2022\tA systematic approach for creating datasets for fine-tuning LLMs to adapt a general pre-trained model for this specific industrial application.\n\nAs a result, we present an LLM-controlled automation system that can interpret user tasks specified in natural language, generate a production plan, and execute operations on the physical shop floor. The configuration of control logic is enabled by prompt design, and the application-specific adaptation of the LLMs is enabled by supervised fine-tuning on data collected during system operation."}, {"title": "II. REQUIREMENTS AND SYSTEM SETUP", "content": "This section introduces the required technological foundation in typical industrial automation domain that enables the downstream application of LLM. Based on this foundation, we establish a system depicted in Figure 1.\n\nA. Interoperability\n\nInteroperability is a fundamental prerequisite for implementing intelligent systems. This concept involves two key aspects: synchronized data acquisition and unified control interface.\n\nPLCs are commonly central to industrial automation, serving as nodes for field data collection and control point exposure. Building on this, OPC UA[3] enables seamless interoperability by providing a unified interface to connect PLCs with higher-level systems across various devices and platforms. In robotics and automation systems that utilize ROS, unified data and control interfaces can be established via ROS communication mechanisms. For proprietary automation modules, communication can be achieved through industrial Ethernet TCP/IP.\n\nUtilizing these technology stacks, various automation modules can be digitally integrated through a unified data and control interface. This integration facilitates the creation of a cyber environment, providing access to the physical system, also referred to as a Cyber-Physical System[4].\n\nB. Digital Twins and Semantics\n\nA digital twin is software that provides a synchronized digital representation of physical assets[5], [6]. It maintains information models that integrate field data and offers services to other software components. Overall, it serves as the foundation for a runtime environment supporting high-level applications, especially indispensable for LLM system.\n\nAnother critical aspect for integrating LLM in industrial applications is semantics. Systems operating at different levels often interpret the same data differently depending on the context[7]. For instance, \"a bit flip from 0 to 1\" in a PLC program indicates \"motor on\" at the field device level, \"workpiece transport initiation\" at a higher control level, and \"logistics task starts\" at the production planning level. These variations of semantics become more apparent when viewed through the lens of the automation pyramid, as illustrated in Figure 2.\n\nThis necessitates a semantic enhancement process across various abstraction levels. We develop a data observer software component (refer to Figure 1) for this purpose. The data observer monitors data in information models and converts them into textual expressions on different semantic abstraction levels. As different LLM-agents have distinct requirements for how data and changes should be interpreted in their specific task contexts, we pre-define the rules determining which events should be emitted on data changes and customize the text content for each agent, as shown in Figure 3.\n\nC. LLM Agent\n\nRecent studies on LLM applications have focused on the concept of LLM agents [8]. In the scope of this paper, an agent is defined as a software component that 1) is responsible for solving specific task requirements, 2) is associated with a physical asset and can be embodied in the form of an automation module.\n\nBuilding on our previous work [9], [10], which introduced a hierarchical manager/operator LLM agent structure for automation system planning and control, this paper presents a refined and more scalable system design. A new fundamental component of this system design is an event log memory, combined with subscription and broadcasting mechanism that provides time-ordered information to agents, thereby keeping them informed about ongoing activities.\n\nBased on the tasks, the system features three distinct agent roles:\n\n\u2022\tManager Agent: This agent responds to user commands or events and generates an operational plan. It assigns subtasks to operator agents through the event log and monitors the plan execution. This agent type leverages the reasoning capability of LLMs in problem-solving and planning.\n\u2022\tOperator Agent: This agent executes tasks assigned by the manager agent or reacts to events by generating function call commands to control the ongoing production process. The operator agents are embodied with diverse automation modules to execute operations. This agent type leverages the reasoning and code understanding capabilities to control the operations within the automation system.\n\u2022\tSummarization Agent: This agent subscribes to the event log and provides a summary of system operations for the user. LLMs' long context understanding and alignment with human preferences are highly relevant for this task role. (cf. Figure 1)\n\nEach agent type has clearly defined responsibilities and interacts with the system according to its role using prompts.\n\nD. Data-driven LLM Application and Testing\n\nTo assess the performance of the agent system performing automation tasks, we utilize this setup to generate test datasets. These datasets comprise pairs of system-generated events and agent prompts alongside the expected outputs. This enables systematic testing of the LLMs' responses under various operational conditions."}, {"title": "III. THE AGENT SYSTEM FRAMEWORK DESIGN", "content": "The organization of agent collaboration is crucial for effective application. For manufacturing process planning and control, we adopt a manager-operator model, applied across different abstraction levels following the automation pyramid (cf. Figure 2). Additionally, we introduce a summarization agent to generate reports based on the event log for users, as shown in Figure 1, though it is omitted in this section for brevity.\n\nA. Manager Agent and Operator Agent for Planning and Control\n\nThe manager agent is a planning module responsible for processing user input tasks and decomposing them into sub-tasks to form a production plan. Operator agents are designed to control specific automation modules, receiving tasks from the manager and executing them accordingly.\n\nWe provide a formal specification of the agent framework's conceptual and structural composition, as well as the relationships between the components, which underpin its software-technical implementation.\n\nB. Formal Description of the LLM Agent System\n\nIn the context of this paper, an LLM agent is a software component that processes textual data to control an automation module. The collaborating LLM agents form an agent system, as illustrated in Figure 4, with the LLM agent construct shown in Figure 5.\n\nAgent System AS:\n\n$AS = {AM_1, AM_2, \u2026, AM_i}$\n\nAn agent system AS consists of several agents, and each agent $AM_i$ (as illustrated in Figure 5) is responsible for controlling a specific automation module $M_i$.\n\nAutomation Module $M_i$:\n\n$M_i = (C_{Mi}, F_{Mi}, E_{Mi})$\n\nNote that an overall automation system consists of multiple automation modules {$M_1, M_2, ..., M_i$}.\n\nEach individual automation module $M_i$ comprises a tuple, where:\n\n\u2022\t$C_{M_i}$ is the set of components of the $M_i$.\n\u2022\t$F_{M_i}$ is the set of functions exposed by $M_i$.\n\u2022\t$E_{M_i}$ is the set of events that are in the scope of $M_i$\n\nThe individual events in set $E_{M_i}$ are pre-defined, automatically emitted upon state changes in the module. This events generation mechanism maps low-semantic data to high-semantic information for process planning and control.\n\nEvent Log Memory $E$ that collects all the events:\n\n$E = {(e_1, t_1), (e_2, t_2), ..., (e_t, t_t) | t_1 < t_2 < \u2026 < t_t}$\n\nThe event log memory records a sequence of all events $(e_t, t_t)$ ordered chronologically, representing the history of state changes in the automation system.\n\nSubscription Mechanism $S$:\n\n$E_{AMi} = S(A_{Mi}) \u2286 E$\n\nAn agent $AM_i$ subscribes to the event log memory $E$ to retrieve relevant events into its own event log memory $E_{AMi}$. $S(A_{Mi}) \u2286 E$ denotes a selective function $S$ that allows the agent $AM_i$ to subscribe to events from E, thereby limiting the agent's scope of observation to relevant events.\n\nAgent Prompt $P_{AMi}$:\n\n$P_{AMi} = Textual(R_{AMi}, C_{Mi}, F_{Mi}, SOP_{AMi}, E_{AMi})$\n\nA textual represented prompt $P_{AMi}$ for an agent $AM_i$ that integrates the elements listed in TABLE I.\n\nFrom LLM Generated Output $O_{eem}$:\n\n$O_{eem} = (O_{reason}, O_{fc})$\n\n$O_{eem}$ consists of LLM's generated reasoning and a function call.\n\nC. The Prompting Method\n\nThe design of the prompt is pivotal in this framework. The prompt serves several purposes except as merely instruction for LLM agents, but also to 1) incorporate knowledge about the automation system, 2) serve as an integration point to connect the LLM with real-time events, and 3) later serves as prefix when training a LLM in a supervised fine-tuning manner, and, notably, tokens in the prompt are not included in"}, {"title": "IV. DATASET CREATION", "content": "In this section, we introduce how we create and organize the dataset for testing and training the LLM for this downstream use case, as illustrated in Figure 7.\n\nA. Dataset Creation Based on the Agents System and Prompting Method\n\nThe LLM agents generate function calls based on the information provided in the prompt. During operation, the underlying digital twin system automatically emits new events in the event log. The agent is continuously updated with real-time textual information to generate a response, which includes a function call to control the equipment and a brief explanation for the reason.\n\nEach prompt, updated with a new event, is considered an individual test point. The combination of this input (the prompt with the new event) and the expected correct output (the LLM's generated function call and reasoning) forms a test case. These test cases are organized into test suites, each corresponding to a specific operational task procedure\u2014such as an inventory management process or a sequence of steps in a painting process. Collectively, these test suites form a comprehensive dataset designed to evaluate the LLM's ability to perform control tasks within the automation system.\n\nB. Formal Description of the Dataset\n\nAgain, we provide a formal specification of the conceptual and structural composition that underpins the software technical implementation.\n\nTest Point $T_p$:\n\n$T_p = (P_{AMi} \u222a {E_{AMi,[t+1,t+k]}})$\n\nA test point $T_p$ is defined as an agent prompt $P_{AMi}$ containing incremental k new events $E_{AMi,[t+1,t+k]}$ for testing the output of an agent $A_{Mi}$. When expanded:\n\n$T_p = Textual(R_{AMi}, C_{Mi}, F_{Mi}, SOP_{AMi}, E_{AMi,[0,t]} \u222a E_{AMi,[t+1,t+k]})$\n\nTest Case $T_c$:\n\n$T_c = (T_p, O_{eem})$\n\nA test case is the combination of a test point $T_p$ and the expected reference output $O_{eem}$ from the LLM, where $O_{eem}$ consists of the expected function call to be generated and a reference reason:\n\n$O_{eem} = (O_{reason}, O^*_{fc})$\n\nTest Suite $T_s$:\n\n$T_s = {T^1_c, T^2_c, ..., T^n_c}$\n\nTest cases $T^1_c$ to $T^n_c$ are organized into test suite $T_s$, each corresponding to a specific operational task scenario.\n\nDataset D:\n\n$D = {T^1_s, T^2_s, ..., T^n_s}$\n\nThe complete dataset D consists of a collection of test suites from $T^1_s$ to $T^n_s$, representing various task scenarios. The dataset is used for both testing and fine-tuning the LLM for the application.\n\nBesides its usage in testing, it also serves as training data for supervised fine-tuning, as illustrated in Figure 8. This fine-tuning helps train a general LLM to internalize application-specific knowledge for controlling automation equipment and learn patterns from process knowledge, as specified in the prompts, the correct function calls and the reasons."}, {"title": "V. EXPERIMENTS", "content": "Using the created dataset, we first evaluate several open-source pre-trained and a proprietary model GPT-40. Then, we applied supervised fine-tuning to further train these models on the created dataset.\n\nThe objectives are twofold: firstly, to evaluate the fine-tuning enhancement of the LLM agent's performance on this specific task; and secondly, to gain insight into the cost-performance trade-offs involved.\n\nA. Test case creation\n\nIn contrast to normal operation where LLM agents observe events before deciding on machine operation, dataset creation involves a reverse process, as depicted in Figure 9.\n\nIn this case, the dataset is created without LLM agents, but with direct user input. With a specific task in mind, the user manually operates the command interface to interact with the automation system. As this process unfolds, the digital twin system automatically generates and records relevant events. The user finally provides a description of the intended task process. This approach captures the three essential elements for dataset: the events, the command calls, and the initial user task request. The dataset contains the knowledge necessary for successful execution of intended tasks.\n\nUsing this approach, we create various task scenarios for handling typical situations in factory operations, such as processing user orders, responding to spontaneous events, or handling abnormalities. Our initial collected dataset contains 100 test cases $T = (T_p, O_{eem})$, each consisting of a complete prompt and the expected LLM output. This dataset can be used to assess the performance of pre-trained LLMs in controlling industrial automation systems.\n\nIn our initial dataset, 68% of the control command generation involves repetitive tasks in our initially prepared dataset, where the LLM agents should follow the SOP routines. The remaining 32% consist of non-routine tasks, requiring LLM agents to respond to unprecedented events through autonomous decision-making.\n\nB. Metrics for Evaluation\n\nWe apply two metrics to evaluate the system's performance.\n\u2022\tCorrectness Rate: Measures whether the generated command matches the reference command in the dataset.\n\u2022\tReason Plausibility: Assessed through human evaluation, using a Likert scale from 1 to 5 to rate the plausibility of the generated reason.\nThese two metrics together provide a more granular metric to identify loss. In some cases, the command is incorrect while the reason may be plausible.\n\nC. Special requirements of automation tasks\n\nGiven the nature of industrial automation, there are two main aspects of the requirements:\n\u2022\tAccuracy in Repetitive Operations: Industrial automation tasks often require 100% accuracy in repetitive operations. This repeatability implies that some routine tasks can be anticipated. This requirement can be evaluated by assessing whether the LLM agents follow the SOP to successfully complete the tests, or whether the model improves by learning from the dataset to perform in-sample tasks.\n\u2022\tHandling Unexpected Events: The system shall be capable of responding to unexpected events not predefined in the SOP or present in the training dataset. It should demonstrate generalization by flexibly handling unforeseen situations that were not anticipated during development. The language model should use its learned knowledge to generate appropriate responses to spontaneous events an ability typically lacking in traditional automation systems.\n\nBased on these considerations, we perform a comprehensive evaluation and model fine-tuning.\n\nD. The evaluation\n\nWe use different LLMs as inference engines to power the agent systems. GPT-40 is selected to represent the state-of-the-art performance achievable by LLMs, while other open-source models are chosen to represent those that can be practically deployed in on-premises industrial settings. To account for the trade-off between model complexity and performance, we compare larger models (70B range) with smaller ones (7B range). We chose GPT-40[2], Llama3 models[12], Qwen2 models[13] and Mistral models[14] for our experiments.\n\n1) Evaluation of pre-trained LLM\n\nWe begin by evaluating the original pre-trained models. In automation tasks, 1) some are routine processes where the LLM agent can follow SOP guidelines in agent prompts to operate the automation system, while 2) others require the agent to autonomously respond to unexpected events, for which reactions have not been instructed in agent prompts. We distinguish between these two types of tasks in our evaluation.\n\nBased on the evaluation results, GPT-4 generally outperforms other open-source models in interpreting agent prompts and events to generate control commands, though their performance varies significantly. Each model also exhibits distinct \"personalities\" in this use case.\n\n2) Evaluation of post-trained LLM based on created dataset\n\nUsing the collected dataset, we apply supervised fine-tuning (SFT) to assess how training open-source models can improve the LLM's performance for this specific downstream task. This training has the potential to enable the customization of a general LLM for intelligent control of specialized automation equipment. For GPT-40, we used OpenAl's proprietary fine-tuning API to explore the capabilities that LLMs can achieve, even though the training methods may vary.\n\nFor two main considerations, we train the models on the test dataset: 1) the machine is designed to execute specialized tasks most of time repetitively, and 2) the goal of fine-tuning is to evaluate whether the model can effectively learn the machine's operational knowledge. Additionally, in one of our concurrent research experiments, we observed that larger LLMs generally exhibit less significant overfitting or catastrophic forgetting than smaller LLM during a k-fold cross-validation fine-tuning based on limited results for this use case. However, we will address this interesting hypothetical finding in the future as we work to overcome cost constraints and dataset scarcity issue.\n\nOpenAI's model and fine-tuning services outperform other models, and the GPT-40 model quickly learns from the samples how to control the automation systems. Other models also demonstrated reasonably good performance. Interestingly, fine-tuned smaller LLMs did not necessarily underperform in this particular use case. However, our contingency LoRA[15] fine-tuning yielded poor results in our experiments and led to a decrease in model performance.\n\nAn accuracy of less than 100% means that the results are not fully reliable for directly controlling an automation system, as it could result in stops and errors during operation. However, they can be developed as an assistant system, proposing next actions for human supervisors to approve in a human-machine collaboration use case scenarios."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduce a novel application-oriented framework that enables the use of LLMs to control industrial automation systems. For system developers, the development can be divided into two phases: 1) modularizing the system and creating interoperable interfaces to establish the physical and digital foundation for agents, and 2) creating datasets and applying LLM-specific prompting and training methods. The result is an end-to-end solution that allows an LLM to control automation systems, with the reconfiguration process and human machine interactions made more intuitive through natural language. We are continuing to refine our design and implementation to further increase the technology readiness level and will post new results from this ongoing research."}]}