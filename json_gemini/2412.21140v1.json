{"title": "FACILITATING LARGE LANGUAGE MODEL RUSSIAN ADAPTATION WITH LEARNED EMBEDDING PROPAGATION", "authors": ["Mikhail Tikhomirov", "Daniil Chernyshev"], "abstract": "Background: Recent advancements in large language model (LLM) technologies have introduced powerful open-source instruction-tuned LLMs that match the text generation quality of leading models like GPT-4. Despite accelerating LLM adoption in sensitive-information environments, the lack of disclosed training data hinders replication and makes these achievements exclusive to specific models.\nPurpose: Given the multilingual nature of the latest iteration of open-source LLMs, the benefits of training language-specific LLMs diminish, leaving computational efficiency as the sole guaranteed advantage of this computationally-expensive procedure. This work aims to address the language-adaptation limitations posed by restricted access to high-quality instruction-tuning data, offering a more cost-effective pipeline.\nMethod: To tackle language-adaptation challenges, we introduce Learned Embedding Propagation (LEP), a novel method with lower training data requirements and minimal disruption of existing LLM knowledge. LEP employs an innovative embedding propagation technique, bypassing the need for instruction-tuning and directly integrating new language knowledge into any instruct-tuned LLM variant. Additionally, we developed Darumeru, a new benchmark for evaluating text generation robustness during training, specifically tailored for Russian adaptation.\nResults: We applied the LEP method to adapt LLaMa-3-8B and Mistral-7B for Russian, testing four different vocabulary adaptation scenarios. Evaluation demonstrates that LEP achieves competitive performance levels, comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. Further improvements were observed through self-calibration and additional instruction-tuning steps, enhancing task-solving capabilities beyond the original models.\nConclusion: LEP offers a viable and efficient alternative to traditional language-specific instruction-tuning, significantly reducing the costs associated with language adaptation while maintaining or surpassing the performance benchmarks set by contemporary LLMs.", "sections": [{"title": "1 Introduction", "content": "Emergence of universal instruct-tuned large language models (LLM) such as ChatGPT (Ouyang, 2022) has substantially accelerated the development of natural language processing technologies. However, despite the remarkable achievements in zero-shot task solving, the close-source nature of such models prevented their adoption in the areas with sensitive or exclusive information where any risk of data-leak jeopardizes the integrity of the business process. As a result the rising demand for open-source alternatives drove the researchers to derive methods for knowledge distillation of state-of-the-art LLMs. One of the first approaches was Alpaca (Taori, 2023) which used ChatGPT to synthesize the instruct-tuning data for open-source foundation LLM LLaMA (Touvron, 2023a). While Alpaca was far from state-of-the-art this inspired the creation of more advanced schemes like BactrianX (Li, 2023) that augmented the synthesis process with cross-lingual machine translation which in turn enabled training of open-source multilingual chatbots. However, with release of GPT-4 (Achiam, 2023) which excelled in multilingual setting it became possible to integrate the explicit translation step into instruction synthesis pipeline thus increasing accessibility of knowledge distillation. This has led to creation of series language-specialized instruction-tunes of open-source LLMs such as Saiga (Gusev, 2023), PolyLM (Wei, 2023), Vikhr (Nikolich, 2024), LLAMMAS (Kuulmets, 2024).\nWith increasing instruction synthesis quality the open-source language-specific LLMs were closing the gap with the state-of-the-art closed-source solutions eventually hitting the performance ceiling of conventional instruction-tuning (Cui, 2023) due to low utilization of inherent English contextual knowledge which is dominant in state-of-the-art pre-trained open-source LLMs (Touvron, 2023b; Jiang, 2023; Dubey, 2024). As a possible solution researchers (Zhu, 2023; Li, 2024; Chai, 2024) proposed enriching the instruction-tuning datasets with translation tasks which are designed to align new language knowledge with the existing English semantic representations. However, it was shown by Ranaldi (2023) and Husain (2024) that the cause of alignment issue is likely to lie with the inefficiency of tokenization algorithm which can be addressed either by building a new language-specific token vocabulary or by recycling the English tokens for Romanized language representation.\nInspired by works of Lakew (2018), Kuratov (2019), Rust (2021) & Yang (2022) on vocabulary adaptation for encoder models Cui et al. (2023) proposed language-specific continued pre-training pipeline for full LLM language adaptation which paired with instruct-tuning on synthesized examples allowed to create Chinese LLaMa, the first open-source model to reach the performance level of ChatGPT with substantially improved computation efficiency thanks to Chinese-adapted tokenization vocabulary. This approach was studied in detail by Tikhomirov (2023) for LLaMa-2 (Touvron, 2023b) adaptation to Russian language and it was shown that semantic alignment efficiency can be further improved with morphologically accurate tokenization algorithm. Moreover, the full LLM language adaptation pipeline was shown by Nguyen (2023) to outperform state-of-the-art closed-source counterparts on low-resource languages due to their bias towards popular languages.\nWhile the current iteration of language adaptation algorithm is relatively cost-efficient, the benefit of developing language adapted LLMs is falling amid the rapid development of LLM technology and multilingual specialization of open-source options. At the same time it becomes common to release instruction-tuned models (Jiang, 2023; Dubey 2024) that perform on par with closed-source state-of-the-art counterparts without disclosing the instruction-tuning data the quality of which is the major factor of resulting LLM task-solving capabilities (Zhou 2024). Collecting data of such quality requires a considerable investment in human annotation to an extent that only large organizations can afford creation of such datasets (Dubey 2024). If a language specific counterpart of a high quality instruction dataset is unavailable the result of full language adaptation will only have the benefit of higher computational performance as an inferior instruction-tuning data will lead to inferior task-solving performance.\nTo cut the language adaptation costs and enable direct language adaptation of instruction-tuned LLM we propose an updated pipeline for language adaptation, Learned Embedding Propagation. Unlike the original full LLM language adaptation pipeline (Cui, 2023), our method requires less data and computational resources due to limited pre-training impact on model parameters which is compensated by novel ad-hoc embedding propagation procedure that allows to skip the instruction-tuning step and instead implant the new language knowledge directly into any existing instruct-tuned variant. To further facilitate the Russian adaptation we developed a new lightweight benchmark for train-time evaluation of LLM text generation robustness, Darumeru. We test Learned Embedding Propagation pipeline on Mistral-7B and LLaMa-3-8B LLMs for 4 Russian tokenization variants. The evaluation results (Figure 1) demonstrate that despite lower parametrization our language-adaptation method manages not only to regain the original quality of the instruction tune but in some cases even outperform it by a significant margin. Additional case-study experiments on improving the best language-adapted models with continued instruct-tuning and self-calibration also confirm the superiority of our language-adapted models, pushing their performance beyond existing counterparts."}, {"title": "2 Method", "content": null}, {"title": "2.1 Model Language Adaptation", "content": "Following the previous work on LLM lingual adaptation (Cui, 2023; Tikhomirov, 2023) we first optimize model vocabulary for better alignment with Russian language morphology and then continue the pre-training process on a large corpora of Russian texts of various genres and topics. Formally the model adaptation consists of 3 steps:\n1. Tokenization training;\n2. Model embedding initialization;"}, {"title": "2.1.1 Tokenization training", "content": "Since there are no best practices for vocabulary optimization we consider 4 options for tokenization training:\n\u2022 BPE - fully substituting the tokenization vocabulary by rebuilding the BPE tokenization algorithm (Vries, 2021), which is used in the majority of state-of-the-art LLMs.\n\u2022 Unigram - fully substituting the tokenization vocabulary with morphologically accurate tokenization obtained with Unigram algorithm (Tikhomirov, 2023).\n\u2022 Extension - extending the original BPE vocabulary by first building a new BPE vocabulary for Russian corpora and then merging it with the original (Cui, 2023).\n\u2022 Optimization - refactoring the existing BPE vocabulary by reducing it to the most common 50% tokens of Russian corpora and then subsequent\n\u2022 Extension to the original size. (considered only for LLMs with extensive English vocabulary)."}, {"title": "2.1.2 Embedding Initialization", "content": "Previous work on LLM language adaptation (Cui, 2023; Tikhomirov, 2023; Nguyen, 2023) found simple averaging of embeddings of overlapping subtokens to be a sufficient solution for embedding initialization. Formally, given embedding vectors of old $v_{old}$ and new $v_{new}$ tokenization vocabularies the new embeddings are initialized as the following:\n$U_{new}(t) = \\frac{1}{K} \\sum_{j=1}^{K} V_{old}(t_j);$ (1)"}, {"title": "2.1.3 Continued pre-training", "content": "The main issue with embedding initialization is that despite introduction of new tokens the LLM retains the habit to use the tokens that were present in the original tokenization. As a result the model computational performance of text generation remains the same as the model tends to use more tokens per word than it is expected while also misinterpreting the new tokens due to homonymy of token context.\nTo alleviate the issue the common tactic is to train the newly initialized embeddings on adaptation language corpora using the same pre-training task as LLM, which is causal language modeling. In this task the input text is broken into sequences of tokens of increasing size all of which start from the beginning and the model is asked to predict for each sequence the next possible token. The model optimization is done using simple cross-entropy loss thus any text corpora can be used for the pre-training task.\nContinued pre-training of embeddings only allows the model to tailor those embeddings for inner semantics thus redistributing the existing language knowledge among the newly introduced tokens. However, some researchers (Cui, 2023; Tikhomirov 2024) argued that pre-training embeddings only may be insufficient for proper model-vocabulary alignment and intermediate model layers must be also trained. On the other hand, increasing the number of trained model parameters reduces the training process stability which in turn substantially raises the data size requirements and computational costs of training procedure. As the middle ground we complement embedding pre-training with a post-training layer alignment procedure that recycles existing fine-tunes of the adapted model."}, {"title": "2.2 Learned Embedding Propagation", "content": "The issue of cost-efficient knowledge transfer for language adapted models has been studied before in the context of encoder models. To solve the absence of task-tuning dataset in the target language Artetxe et al. (2019) proposed a simple algorithm for transferring task-solving knowledge to BERT models:\n1. Pre-train the full language model from scratch on available large monolingual text corpora (e.g English) using language modeling training objective (for BERT it is masked language modeling);\n2. Create a copy of the pre-trained model and replace the embeddings of the original with new embeddings for the target language;\n3. Continue the pre-training of the modified original on target language monolingual corpora for model embed- dings while freezing (not updating) all other layers using the same training objective;\n4. Fine-tune the copy on the downstream task dataset while keeping the embeddings frozen;\n5. Swap the embeddings of the fine-tuned copy with embeddings of the original model obtained after continued pre-training on the target language corpora.\nThe major advantage of the described algorithm is that the continued pre-training step requires much less data than initial pre-training from scratch as it requires training only a fraction of model parameters which reduces model optimization task complexity and thus has faster convergence (Kaplan, 2020). The main hypothesis is that task-solving knowledge is language agnostic and it was confirmed in the original experiments (Artetxe, 2019) for natural language understanding and document classification tasks. However, the authors noted that fine-tuning on downstream tasks with frozen embeddings is not enough for proper embedding swap alignment and additional embedding transformations or special embedding utilization penalties are required to maximize the efficiency of target language vocabulary processing. As a possible solution to the embedding alignment problem Chen et al. (2023) proposed using a special pre-training regime with active embedding forgetting to force the language model to accumulate the knowledge in intermediate layers. The downside of such an approach is that we must have full control on the initial pre-training which is not possible for state-of-the-art LLMs obtained by training on high quality proprietary datasets with immense computational budget.\nWe argue that embedding swap alignment can be achieved without special training procedures by leveraging the fine-tuning parameter update trajectory. Ilharco et al. (2023) showed that the fine-tuning trajectory may be approximated"}, {"title": "$\\\\tokenize_{old}(t) = [t_1,..., t_k].$", "content": "(2)\nwhere tokenizeold is the original tokenization function,t is token in new vocabulary, t is a token in original vocabulary.\nWhile there are more advanced initialization techniques, recent studies on design choices for LLM language adaptation (Tejaswi, 2024) concluded that embedding averaging has the best expected adaptation quality and the performance gap with task-tailored methods is within standard deviation of task evaluation protocol. Therefore for all experiments we use the described subtoken averaging embedding initialization strategy."}, {"title": "2.2.1 Direct embedding swap", "content": "Considering that most state-of-the-art LLMs are trained on multilingual datasets, it can be expected that their inner representations are tailored for language-agnostic text processing. Similarly to the original works on embedding-based knowledge transfer for encoder models we assume that the embedding layer carry only conceptual information i.e. we suppose $D_{inst}^{ru}= U_{inst}^{ru} = E$, where E is an identity matrix."}, {"title": "2.2.2 Overlapping token correction", "content": "Since the considered LLMs are initially designed for multilingual text generation they have a basic set of the most common tokens for popular languages such as russian. The idea is to find the union C = tokensold \u2229tokensnew of the original tokensold and language-adapted tokensnew vocabularies and use this subset to reduce Ix, Ox to the common components of embedding initialization $I_{X/com}$, $O_{X/com}$ where X \u2208 {base, inst}. This allows to approximate the embedding projections as $D_{inst}^{ru} \u2248 D_{inst}$ and $U_{inst}^{ru} \u2248 U_{inst}$:\n$D_{inst}^{ru} = I_{base/com}^{-1}I_{inst/com},$ (8)\n$U_{inst}^{ru} = O_{inst/com}O_{base/com},$ (9)\n$I_{X/com} = [idx(t)]_{t \\in C}^{X},$ (10)"}, {"title": "2.3 Darumeru benchmark", "content": "Existing LLM benchmarks for Russian language (Fenogenova, 2024) do not expose the testing data labels for local evaluation. On one hand such an initiative is reasonable amid the rising trend of training on test data which renders the LLM ranking results meaningless. On the other hand hidden test labels means that the evaluation requires having an online connection to the benchmark system which prevents evaluation in offline computational environments thus postponing the evaluation until the end of training session. Moreover lack of access to test labels makes it impossible to classify the type of prediction errors thus limiting the post-training quality analysis.\nTo address the issue we developed a new benchmark framework that focuses on quick and informative LLM text generation quality evaluation. This benchmark consists of combinations of open splits of datasets from MERA (Fenogenova, 2024), mmlu_ru / mmlu_en, RuCoLA (Mikhailov, 2022), as well as new datasets for text generation assessment - 17 datasets total. A more detailed description of each dataset is given in the following sections."}, {"title": "2.3.1 Framework", "content": "The evaluation framework utilizes message format to ensure compatibility with both pre-trained and instruction-tuned LLMs. This means that all task data for the models is converted into a sequence of \u201cuser role\u201d-\u201cmessage content\u201d pairs, from which the final prompt is constructed. The framework supports tasks that require estimating the probability of the next token, generation, or logsoftmax for the entire generated sequence. The evaluation can be carried out directly in a conventional Transformers model training environment or via VLLM specialized model inference servers."}, {"title": "2.3.2 DaruMERA and DaruMMLU", "content": "We composed DaruMERA from the following MERA datasets: MultiQ, PARus, RCB, RWSD, USE, ruOpenBookQA, ruWorldTree. For better language understanding evaluation we also added validation split of RuCoLA dataset. For DaruMMLU part we separated ruMMLU (MERA) and complemented it with MMLU datasets from the NLP-Core-Team repository\u00b2. There are several changes to the original datasets:\n1. MultiQ version was augmented with additional gold answers. The existing labels do not correspond in form to the questions, as they were extracted from the text without proper preprocessing. The augmentation process consisted of passing the question and reference answer pairs to LLaMa-3-70B-Instruct model to rephrase the answer in accordance with the question;"}, {"title": "2.3.3 DaruSum", "content": "Most of the evaluation tasks aim to measure the model's text comprehension capabilities and global contextual knowledge which is required for proper prompt processing. However for text generation the model must be also capable of filtering the input text for the query relevant content to ensure that the user would receive the desired answer regardless of input format or size. Text summarization is the perfect evaluation task for such a case as it requires both filtering the input content and composing the answer from the salient fragments.\nThere are two summarization settings: extractive and abstractive. Extractive summarization is a task of sentence saliency ranking where the summary is obtained by taking top-k ranked sentences. Abstractive summarization on the other hand is a text generation task where saliency ranking is integrated in the token sampling process as the model guides itself toward the most concise summary. While the abstractive setting has the higher preference it is hard to distinguish automatically the suboptimal content filtering from the text generation errors. At the same time constraining the text generation process to input fragments such as sentences basically reduces the task to extractive summarization. Thus to evaluate content filtering accuracy and text generation quality it is sufficient to evaluate the abstractive summarization in free and constrained generation settings.\nFor the summarization dataset we chose Gazeta (Gusev, 2020) which has established itself as the standard for Russian automatic summarization evaluation. To improve the accuracy of evaluation procedure we derived an example filtering protocol that all reference summary content can be inferred from the input document. Since LLaMa-3-70B showed high human agreement in LLM evaluation\u00b3 we employed it as the example correctness evaluator and tasked it to find all citations that support the summary sentence. We filtered out all examples that had more than 20% of unsupported summary sentences and mapped found citations to document sentences, thus producing accurate extractive labels. To adapt the task for a few-shot setting which is limited by context window limitations we compressed the documents by dropping the paragraphs that had no extractive summary labels. To account for LLM text generation length variance (Dubois, 2024) as the metric for abstractive and extractive settings we chose average of ROUGE-1 and ROUGE-2 recall and R-precision respectively."}, {"title": "2.3.4 DaruCopy", "content": "When replacing the LLM vocabulary it is important that it learns to fully utilize new tokens. The input token embeddings are responsible for conveying the text meaning which can be evaluated by natural language understanding tasks such as MMLU. In contrast, the output token embeddings are used to find the closest semantic meaning to the current neural network state which depends on contextual history. As a consequence, in creative tasks this state is unstable and LLM tends to generate rarer tokens. At the same time, in the tasks where the LLM is required to reuse the input context the network state is expected to fall into semantic clusters of tokens that are present in the input sequence. Following that logic by prompting the LLM to produce a copy of the input text we can evaluate its token generation efficiency.\nWe used Wikipedia articles of different genres to collect copy task datasets for English and Russian languages involving 2 copy settings: sentence-wise and paragraph-wise. The former setting assesses the LLM alignment with tokenization algorithm which is calculated as the ratio of the length of the original text to the generated text in tokens. In paragraph setting we evaluate the overall text generation stability by measuring the percentage of generations in which the ratio of longest common subsequence (lcs) tokens to all paragraph tokens is greater than 99% (1% is left for spacing errors). Deviation from 99% amid the high sentence copy scores indicates that the model tends to confuse tokens and thus can hallucinate context in creative tasks which is the major reliability concern for practical applications."}, {"title": "2.3.5 Benchmark parameters", "content": "When calculating the benchmark metrics, the following parameters were set: batch size 8, sequence length 4096, 5-shot for foundation models and zero-shot for instruct models."}, {"title": "2.4 Experiment setting", "content": "We conducted adaptation experiments with two models: Mistral-7B-v0.1 (Jiang, 2023) and LLaMa-3-8B (Dubey, 2024)."}, {"title": "2.4.1 Continued Pre-training", "content": "Training dataset for tokenization and continued pre-training consists of documents from the following domains: Russian Wikipedia, English Wikipedia, Habrahabr, Pikabu, Fiction, News, Educational literature. The documents were deduplicated using Locality Sensitive Hashing Minhash algorithm. We removed metadata, links, comment sections and badly formatted documents to improve vocabulary distribution and reduce the number of grammatically incorrect examples. To reduce the semantic noise we restricted the vocabulary to Cyrillic and Latin languages and stripped non-standard symbols like emoji or logograms (e.g. Chinese characters) using UTF-8 normalization. For training, texts were sampled with increased weights for Wikipedia, educational and scientific literature. Additionally, to feed texts into the language model, we ensured that each sample began either with a new document or with a new paragraph.\nTokenization parameters. We trained BPE and Unigram tokenizers with 32000 and 128000 tokens for Mistral-7B and LLaMa-3-8B respectively. For Extended tokenizer, we extended the original tokenizers to 55328 and 174816 tokens using new Russian-adapted BPE vocabularies for corresponding models. Since LLaMa-3-8B tokenization vocabulary is likely to be extensive we created an Optimized version, where we shrunk the original BPE vocabulary to 64000 tokens and then merged with top 64000 most common tokens from new BPE vocabulary, resulting in 114504 tokens.\nHyperparameters. During continued pre-training we used the following hyperparameters: Total Batch Size: 256; Block Size: 1024; Weight Decay: 0.1; Scheduler: Cosine; Warmup Steps: 100; Epochs: 1.\nWe tested 4 different learning rates: 2e-5, 5e-5, 1e-4, 2e-4 for each model and tokenization on 20% of all continued pre-training dataset. Based on benchmark results, we chose a learning rate equal to 1e-4 for all Mistral-7B models, and learning rate equal to 2e-4 for LLaMa-3-8B models. It is important to note that the efficiency of model adaptation showed a significant dependence on the learning rate, especially for LLaMa-3-8B based models."}, {"title": "2.4.2 Case Study: Self-Calibration", "content": "For the cases of full vocabulary substitution where the model learns to rewire all new embeddings virtually from scratch the propagation process may have lower efficiency as the difference between instruct-tuned and language-adapted embeddings may be dramatic. The logical solution is to synthesize self-instruct data using the original instruct-tuned LLM and then use it to calibrate the language-adapted version. To generate the examples, we used prompts from Saiga instruction dataset and used greedy decoding to get the most likely answer from instruct-tuned LLM viewpoint. Then we asked LLaMa-3-70B to evaluate the quality of synthesized pairs in terms of grammar and relevance on a 5-point grading scale. All examples that received a score less than 4 were discarded which left us 13531 calibration examples.\nSince calibration examples are native for LLM inner semantic representations there is a risk that instead of alignment the model may revert back to the original tokenization behavior which prioritizes smaller but more familiar tokenization chunks. To avert such a scenario we leverage the fact that all modern LLMs are pre-trained on Wikipedia articles in such a manner that their embedding representations are aligned with Wikipedia concepts. By asking the fine-tuned model to repeat a Wikipedia article token by token we force the model to recall its pre-training memory and thus to propagate the activation signals respective to the concepts in the article to embeddings of optimal tokens of new tokenization. Following that logic we supplemented the self-instruct dataset with 10000 article-copy task examples, obtained from the part of Wikipedia that has no overlap with our pre-training or benchmark datasets.\nWe found the following LoRA-tuning settings to be optimal for calibration procedure: Rank: 8; Alpha: 1; Learning Rate: 2.5e-5; Weight Decay: 0.1; LoRa target modules: first and last transformer layers; LoRa modules to save: lm_head, embed_tokens; Max Sequence Length: 8096 (i.e. max context length); Total Batch Size: 64; Epochs: 1."}, {"title": "2.4.3 Case Study: Continued Instruction-Tuning Calibration", "content": "In addition to the self-calibration experiments, we decided to test how continued instruction-tuning on the high-quality Russian instruction dataset would affect the final performance. For this experiment we choose Saiga dataset which is"}, {"title": "3 Results", "content": null}, {"title": "3.1 Open-source LLM Benchmark", "content": "To establish a baseline we benchmarked popular instruct-tuned LLMs (see Table 1): Openchat 3.5, LLaMa-3 (instruct) (Dubey, 2024), Saiga (Gusev, 2023), Vikhr (Nikolich, 2024), Qwen-2, Mistral Nemo (Jiang, 2023). As expected the largest model, Mistral Nemo, has the highest zero-shot performance. Smaller counterparts have the same score margin. However, Qwen-2 7B manages to outperform Mistral Nemo in MMLU tasks while falling behind on text generation robustness tests of DaruSum and DaruCopy. Vikhr-5.2 similarly has the same score on DaruMERA as Mistral Nemo. Considering the LLM scaling laws (Kaplan, 2020) and the performance gap with state-of-the-art sub-10B parameter LLM, LLaMa-3, this observations suggest that some parts of MMLU and MERA datasets were leaked to training data of Vikhr-5.2 and Qwen-2 7B."}, {"title": "3.2 Vocabulary Adaptation and Continued Pre-Training", "content": "Following our initial benchmark results we focused on Russian adaptation of the foundation models of the most performant instruct-tunes: Mistral-7B and LLaMa-3-8B. To evaluate the language-adaption results we used few-shot in-context-learning as the models are not used to interpreting the instructions directly."}, {"title": "3.3 Learned Embedding Propagation", "content": "The results of complete Learned Embedding Propagation (LEP) are reported in Table 3. For each adapted vocabulary construction option (BPE, Unigram, Extended and Optimized) we test 3 methods: Direct Embedding Swap (Swap), Overlapping Token Correction (Overlap) and Vocabulary Conversion (Conversion). For embedding donor model we used best continued pre-training checkpoints (see Table 2).\nFor Mistral-7B and OpenChat 3.5 the embedding propagation results have large variance depending on the chosen tokenization algorithm for Russian vocabulary. In case of BPE, which is the same algorithm used for the original, the trained embedding for new vocabulary has the highest alignment with instruct-tuned counterpart in case of direct embedding swap. In case of more morphologically correct Russian tokenization, Unigram, overlap projection has the highest average task performance. However, if we look at group-wise scores it becomes evident that conversion is"}, {"title": "3.4 Case Study: Self-Calibration", "content": "In self-calibration experiments we focused on closing the gap of best LEP LLaMa-3-8B instruct models (Table 4, self-calibration). As expected the performance of DaruCopy tasks improved substantially, practically reaching the perfect reliability levels. DaruSum also saw the improvements as the improved citation capabilities are beneficial for"}, {"title": "3.5 Case Study: Continued Instruction-Tuning Calibration", "content": "Our experiments on continued instruction-tuning calibration approach, presented in Table 4, showed that the additionally fine-tuned LEP adapted models achieve and in some cases outperform the original models. Adding 2000 instructions for copying long texts to the instructional dataset has a positive effect in almost all cases. Moreover, the obtained models are more effective when used in the Russian language, and the loss of initial knowledge in the case of our method is minimal, compared to conventional instruct-tuning."}, {"title": "3.6 Examples", "content": "We also investigated how the models' responses changed depending on the stage: original model, LEP, LEP + calibration (Figure 3).\nFrom the example, it can be seen that the original model did not correctly perceive the question at all. The LEP model already answers more correctly, but does not take into account that this is a phraseological unit. The calibrated model already answers the question most correctly among the three versions of the model, paying attention to the true meaning of the phrase."}, {"title": "4 Discussion", "content": null}, {"title": "4.1 LLM benchmark results for Russian language", "content": "Results presented in Table 1 demonstrate that fine-tuning of open-source state-of-the-art LLMs on Russian focused instruction datasets commonly leads to performance drops in language understanding. This phenomenon was initially observed within Ru-Arena-General and Chatbot Arena benchmarks, however, due to their open-question format it was hard to separate generation errors from bad user prompting. Closed-question benchmarks such as MERA (Fenogenova, 2024), which was used as the basis of Darumeru, can not reliably detect language processing degradation due to the possibility of benchmark hacking. Benchmark hacking is a procedure of fine-tuning on benchmark solutions or similar data which is viewed as a variant of cheating in the context of LLM benchmarks. Usually developers of LLM models do not intend to resort to such poor practice and on the contrary make an additional effort to remove any possible benchmark data from the overall LLM training data pool. At the same time detecting benchmark related data-leaks is a labor-intensive task as it requires checking training data not just for exact matches but also for any possible paraphrases which includes translating examples to other languages.\nOur Darumeru benchmark addresses the limitation of closed-question format with newly introduced tasks for text summarization (DaruSum) and tokenization diagnostic (DaruCopy). DaruSum requires two crucial task-solving elements, proper text analysis and good text writing skills. Any performance drops in this benchmark subset indicate problems with text understanding or text generation. DaruCopy distinguishes between the two by exclusively evaluating the latter by reducing the task to explicitly broadcasting the original context without any analysis or paraphrasing. Consequently, lower DaruCopy scores indicate a reasoning conflict within the LLM logic as the model fails to follow simplest task directive of text copying. These two subsets of Darumeru benchmark show that LLaMa-3-8B is a more"}, {"title": "4.2 Language adaptation strategy", "content": "During development of our LLM Russian adaptation pipeline we made several design choices which were explored in previous works. First of all, we assumed that tokenization knowledge and the ability to use new tokens is stored in input embeddings and LM head layers of LLM. Several works (Cui, 2023; Tikhomirov, 2023; Nikolich 2024; Nguyen, 2024) demonstrated that language-adaptation of these subset of layers only is insufficient for proper language understanding and thus subsequent instruction-tuning of such models leads to suboptimal results. At the same time it was shown (Tikhomirov, 2024) that there is no significant difference between language-adaptation of all-layers and dual-stage approach, when embedding and LM-head training process is complemented with subsequent training of other layers. Results reported in Table 2 reinforce this claim as the first stage of dual-stage approach proves to be efficient enough to substantially improve Russian language comprehension of Mistral-7B model. However, LLaMa-3-8B post-adaptation scores suggest that the necessity of inner-layer training is dictated by the original LLM Russian linguistic skills which are effectively captured by the DaruMERA subset of our benchmark. Learned Embedding Propagation procedure results (see Table 3) also reflect this observation as Mistral-7B showed highest language-knowledge transfer efficiency.\nWhether layer discrepancy can be alleviated by instruction-tuning we explored in our calibration experiments. Instruction-tuning on target language often improves token utilization and boosts language comprehension (Gu- sev, 2023; Wei, 2023; Nikolich, 2024). We see a similar trend in Table 4. By training the original non-adapted instruction-tuned versions of LLMs on Saiga dataset (Gusev, 2023) we enhanced Russian task-solving capabilities which boosted benchmark scores. Applying the same procedure to our LEP models (saiga d7) we retain the positive effect at increased rates with the scores higher than of the original models which were the subjects of LEP knowledge transfer. The drawback of instruction-tuning on Russian instruction datasets is that we inevitably disturb the original knowledge that was gained in prior training (Tejaswi, 2024). We attempted to address the issue by training on the answers generated by the original LLM (self-calibration) rather than using the original references from the Saiga dataset. However for LLaMa-3-8B instruct we did not see noticeable improvement in any LLM capabilities besides tokenization utilization (DaruCopy). This result is likely due to lack of generation quality of our self-calibration synthesized examples which during our manual inspection revealed to carry much simpler Russian language logic and vocabulary. Considering that Saiga is a prime example of GPT-4 reference synthesis (Taori, 2024) we hypothesize that by utilizing more advanced sampling techniques and better example quality evaluation protocols we may collect a reference dataset with the similar features without employment of other datasets or third-party models."}, {}]}