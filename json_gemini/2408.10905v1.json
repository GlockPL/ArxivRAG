{"title": "The impact of labeling automotive AI as \"trustworthy\" or \"reliable\" on user evaluation and technology acceptance", "authors": ["John Dorsch", "Ophelia Deroy"], "abstract": "This study explores whether labeling AI as either \"trustworthy\" or \"reliable\" influences user perceptions and acceptance of automotive AI technologies. Utilizing a one-way between-subjects design, the research presented online participants (N=478) with a text presenting guidelines for either trustworthy or reliable AI, before asking them to evaluate 3 vignette scenarios and fill in a modified version of the Technology Acceptance Model which covers different variables, such as perceived ease of use, human-like trust, and overall attitude. While labeling AI as \u201ctrustworthy\u201d did not significantly influence people's judgements on specific scenarios, it increased perceived ease of use and human-like trust, namely benevolence, suggesting a facilitating influence on usability and an anthropomorphic effect on user perceptions. The study provides insights into how specific labels affect adopting certain attitudes toward AI technology.", "sections": [{"title": "Introduction", "content": "The wide use of the \"trustworthy AI\" denomination by manufacturers and legislators sparks debates. On the one hand, several philosophers have argued that it is irrational to place trust in a machine [1-3,20]. Instead, trust should be placed not in the machines themselves, but in the humans responsible for their development and deployment [4]. Others, like Coeckelbergh [5] for instance, argues in favor of trusting machines, emphasizing that as social animals, we already live in a world structured by trusting relationships. Accordingly, trust should be considered a pre-reflective condition of human experience and unproblematically extended to machines.\nThose who argue against trusting machines contend that trust can only be rationally employed if the trustee is sensitive to normative constraints of reasons [3], has the trustee's goodwill in mind when acting [6], holds the trustee's interests as their own [7], or is emotionally sensitive to moral values [8]. Since today's machines are nowehere near any of these conditions, many AI ethicists advocate for employing the notion of reliability rather than trust when dealing with machines. Reliability, decoupled from trust, is understood as entailing solely performance-based or outcome-based standards for evaluation [2] [3].\nBut do these distinct conceptual frameworks for conceiving of one's relationship to machines- the framework of trust and the framework of reliability - influence the attitudes of users towards this technology?\nIn this study, we sought to test whether the deployment of either framework in the field of automotive AI technology would predict changes in naive users attitudes."}, {"title": "Hypotheses", "content": "In this study, we investigated the impact of labeling AI as \"trustworthy\" versus \"reliable\" on various dependent variables related to technology attitudes. Our hypotheses (H1, H2, H3, H4, H5) examine whether the label \u201ctrustworthy AI\" predicts changes in AI blameworthiness, AI accountability, confidence while using, and confidence while learning, and the overall TAM score. Additionally, we explored whether the labels predict changes in TAM categories, including perceived ease of use, perceived usefulness, intention to use, ability-based and human-like trust in automotive \u0391\u0399 (H5.1 - H5.8) (see Table 1). The study was pre-registered to enhance transparency and credibility. The hypotheses remain consistent with those outlined in the pre-registration, though the numbering has been reordered and simplified, the null hypotheses are reformulated as 'no predicted effect'. The pre-registration document can be accessed at the Open Science Framework (OSF) via this link."}, {"title": "Materials and methods", "content": "Initially, 617 participants were recruited for the study through online platforms Cloud Research and Amazon Mechanical Turk. Participants were paid approximately $12 an hour for completing the study, whether their data was ultimately analyzed or not. After conducting a language proficiency check, 66 participants were excluded, leaving 551 (see"}, {"title": "Experimental Design", "content": "A one-way between-subjects design was used. Participants were randomly assigned to one of two groups: the \"trustworthy AI\" group or the \"reliable AI\" group. The assignment was random to ensure that each participant had an equal chance of being placed in either group. Ultimately, data were analyzing from 241 participants assigned to the \"trustworthy AI\" group and 237 participants assigned to the \"reliable AI\" group, ensuring a balanced sample. The sample consisted of 493 participants, with a balanced distribution in terms of gender and age groups. As specified in the pre-registered plan, 15 participants were excluded from analysis due to being 65 years old or older, leaving a"}, {"title": "Phase 1: Demographic Phase", "content": "This section involved the collection of demographic data (age, gender, education, experience with automotive AI, driver's license status, AI expertise), and a language check to ensure fluency in English. Detailed demographic data are described in Descriptive Statistics, and language check criteria are provided in Supplementary Material, specifically in Language Check."}, {"title": "Phase 2: Induction Phase", "content": "Participants were presented with group-specific definitions of \"trustworthy AI\" or \"reliable AI\" (see Definitions of Key Terms) attributed to AI experts, as well as an attention check to ensure engagement and comprehension of the respective definition (see Attention Check)."}, {"title": "Phase 3: Vignette Phase", "content": "Participants read three separate vignettes, one for each automotive AI-assisted task: planning assistance, parking assistance, and steering assistance. The vignettes were presented in random order and, across the two groups, the vignettes were identical except for the key terms \"trustworthy\" or \"reliable\" (and other related semantic derivatives) respective of group assignment. Participants answered the same four questions for each of the three vignettes (AI accountability, AI blameworthiness, confidence in driving, and confidence in learning) using a 5-point Likert scale, which were also presented in random order. The full text of the vignettes and the questions asked are available in Supplementary Material, specifically in Vignettes."}, {"title": "Phase 4: Questionnaire Phase", "content": "Participants answered the eight TAM questions covering the distinct constructs perceived ease of use, perceived usefulness, behavioral intention, ability trust, human-like trust (benevolence and integrity), general trust, and attitude. A 5-point Likert scale was used to measure responses. The questions were presented in random order to prevent biasing effects. The complete TAM questionnaire is provided in Supplementary Material, specifically in Technology Acceptance Model Questionnaire.\nThe selection of items for TAM is informed by Choung et al. [14], which identified the importance of trust (general, human-like, and ability-based trust) in AI and its impact on technology acceptance. Their comprehensive study provides a validated framework for measuring key constructs related to technology acceptance. Each TAM variable category was measured with one question to streamline the data collection process, minimize participant fatigue, and cohere with automotive AI.\nThe selected questions are the most appropriate for automotive AI, and the wording of each question has been modified to reflect this technology. The original study asked about smart AI assistants and, as a result, several of the survey items do not apply."}, {"title": "Data Collection and Handling", "content": "Data was collected using the Qualtrics online survey platform. Procedures for data handling and storage included secure, encrypted storage and removal of personal identifiers. Data monitoring and quality assurance were ensured through exclusion of incomplete responses, failed language checks, and failed attention checks. See Supplementary Material for more, specifically Inclusion Criteria."}, {"title": "Data Analysis", "content": "For the analysis of the vignettes, independent samples t-tests were conducted to compare means by condition for hypotheses related to the vignette assessments, i.e. H1 to H4, which include AI accountability, AI blameworthiness, confidence in using automotive AI, and confidence in learning how to drive with automotive AI (in the context of obtaining one's first driver's license). To further investigate the effects of condition on outcomes related to the vignettes, Bayesian ordinal regression models were fitted for each question. These models accounted for participant and vignette as random effects, providing posterior estimates of the condition effect for each hypothesis.\nFor the total TAM score, addressing hypothesis H5, we employed a Cumulative Link Mixed Model (CLMM) to account for the ordinal nature of the response variable and to model participant and TAM items as random effects. Cumulative Link Models (CLMs) were used to analyze ordinal response variables related to the individual TAM questions, specifically testing hypotheses H5.1 through H5.8.\nAnalysis deviated slightly from the pre-registered plan. First, the residuals of the t-test did not adhere to a normal distribution, as indicated by significant deviations in the Shapiro-Wilk test results. Since the t-test assumes normally distributed residuals, its application was questioned for this data. We still performed the t-test and report the results below. To address the non-normality of residuals, we also conducted a Wilcoxon test as a non-parametric alternative; however, this test did not reveal any significant results either. Detailed results of the Wilcoxon test are reported in Supplementary Material, respectively in T-Test Analysis and Further Analysis: Vignette Questions. Finally, we opted not to use Cumulative Link Mixed Models for H5.1 through H5.8 as initially proposed. Since we modelled each question separately, we did not need to account for random effects of items or participants, so a simpler ordinal regression model (CLM) was sufficient for analysis."}, {"title": "Results", "content": "The results of the hypothesis testing went against most pre-registered hypotheses, which were based on the literature as well as the pilot, which had a total of 43 participants, 21 in the \"trustworthy AI\" group and 22 in the \"reliable AI\" group. For hypotheses H1, H2, which predicted that the trustworthy AI condition would lead to lower ratings of blameworthiness and accountability, we did not observe any effect and could therefore not reject the null hypothesis. For Hypotheses H3, and H4, which concerned confidence ratings and where no special prediction was issued, no effect was observed. Finally, for"}, {"title": "H1: AI Accountability", "content": "There was no significant difference between the trustworthy AI group (mean = 3.47) and the reliable AI group (mean = 3.53) in terms of accountability, t(478) = 1.05, p = 0.292, with a 95% confidence interval of [-0.0541,0.180]."}, {"title": "H2: AI Blameworthiness", "content": "The blameworthiness ratings were similar for the trustworthy AI group (mean = 3.50) and the reliable AI group (mean = 3.52), t(478) = 0.333, p = 0.739, with a 95% confidence interval of [-0.0963, 0.136]."}, {"title": "H3: Confidence in Driving", "content": "Participants' confidence in driving using the automotive AI did not significantly differ between the trustworthy AI group (mean = 3.42) and the reliable AI group (mean = 3.50), t(478) = 1.264, p = 0.207, with a 95% confidence interval of [-0.0398, 0.184]."}, {"title": "H4: Confidence in Learning", "content": "Similarly, confidence in learning to drive with the automotive AI was not significantly different between the trustworthy AI group (mean = 3.52) and the reliable AI group (mean = 3.59), t(478) = 1.07, p = 0.285, with a 95% confidence interval of [-0.0531,0.180]."}, {"title": "H5: Total TAM Score", "content": "For the Total TAM Score, we conducted ordinal regression analyses to examine the relationship between the condition and participants' perceptions across the total sum of the various questions. The mixed model had a log-likelihood of -4498.18 and an AIC of 9010.35. The model converged after 538 iterations, with a maximum gradient of 1.01 \u00d7 10-3 and a condition number of 1.8 \u00d7 1002. The coefficient for the grouptrust variable was estimated at 0.2058 with a standard error of 0.2137, resulting in a z-value"}, {"title": "H5.1 - H5.8: Results of Individual Technology Acceptance Items", "content": "For individual TAM items, we conducted ordinal regression analyses to examine the relationship between the group label and participants' perceptions across the various questions: 1. perceived ease of use, 2. perceived usefulness, 3. intention to use, 4. trust: ability, 5. trust: benevolence, 6. trust: integrity, 7. trust: general, 8. attitude general.\nAs outlined in our pre-registration, we tested each question for adherence to the proportional odds assumption using the Brant test, implemented through the brant package in R. The results indicated that Question 4 (trust in ability) did not satisfy this assumption (\u03c72 = 8.65, df = 3, p = 0.0343). To maintain consistency across our analyses and ensure a uniform approach for all eight questions, we opted to use a model with flexible thresholds. This approach allowed us to account for potential variability in the relationship between the labels and responses across different thresholds. Each model included 478 observations, using a logit link function with flexible thresholds."}, {"title": "Question 1: Learning to use automotive AI would be easy for me", "content": "For Question 1, the model had a log-likelihood of -630.88 and an AIC of 1271.77. The model converged after six iterations, with a maximum gradient of 3.92 \u00d7 10-13 and a condition number of 2.5 \u00d7 1001. The coefficient for the grouptrust variable was estimated at 0.3354 with a standard error of 0.1697, resulting in a z-value of 1.976 and a p-value of 0.0481. The 95% confidence interval for this estimate ranged from 0.0028 to 0.6679, indicating a statistically significant effect of the \"trustworthy AI\" label on the perception of ease of learning. The thresholds were estimated as follows: Strongly Disagree Disagree at -3.6908 (CI: [-4.3346, -3.0470]), Disagree Neutral at -1.7084 (CI: [-2.0130, -1.4038]), Neutral|Agree at -0.3899 (CI: [-0.6386, -0.1412]), and Agree Strongly Agree at 1.7564 (CI: [1.4553, 2.0575]). The result for the Neutral Agree threshold difference was significant (p = 0.016), with a standard error of 0.1391 and a difference of 0.4624. The other threshold differences did not reach significance."}, {"title": "Question 2: Using automotive AI would improve my performance at accomplishing driving-related tasks", "content": "For Question 2, the model had a log-likelihood of -688.55 and an AIC of 1387.09. The model converged after five iterations, with a maximum gradient of 2.10 \u00d7 10-08 and a condition number of 1.9 \u00d7 1001. The coefficient for the grouptrust variable was estimated at -0.0318 with a standard error of 0.1657, resulting in a z-value of -0.192 and a p-value of 0.848. The 95% confidence interval for this estimate ranged from -0.3566 to 0.2929, indicating that the effect of the \"trustworthy AI\" label on performance improvement is not statistically significant. The thresholds were estimated"}, {"title": "H5.3: Intention to Use", "content": "We conducted an ordinal regression analysis to examine the relationship between the group labels, either \"trustworthy AI\" or \"reliable AI\" and the participants' intention to use the technology. The analysis used a logit link function with flexible thresholds. The model included 478 observations, with a log-likelihood of -704.85 and an AIC of 1419.70. The model converged after five iterations, with a maximum gradient of 7.07 \u00d7 10-10 and a condition number of 2.2 \u00d7 1001.\nThe coefficient for the grouptrust variable was estimated at -0.0877 with a standard error of 0.1653, resulting in a z-value of -0.531 and a p-value of 0.596. The 95% confidence interval for this estimate ranged from -0.4116 to 0.2363, indicating that the effect of the \"trustworthy AI\" label on the intention to use is not statistically significant. This result supports the hypothesis, suggesting that the label \"trustworthy AI\" does not predict a change in the intention to use the technology.\nThe threshold coefficients indicate the points at which the response transitions from one category to the next on the Likert scale. The threshold between \"Strongly Disagree\" and \"Disagree\" was estimated at -2.4660 with a standard error of 0.1878, resulting in a z-value of -13.134 and a 95% confidence interval ranging from -2.8340 to -2.0981.\nThe threshold between \"Disagree\" and \"Neutral\" was estimated at -1.3005 with a standard error of 0.1393, resulting in a z-value of -9.336 and a 95% confidence interval"}, {"title": "Question 4: Automotive AI technologies are competent in their area of expertise", "content": "For Question 4, the model had a log-likelihood of -622.62 and an AIC of 1255.24. The model converged after six iterations, with a maximum gradient of 3.13 \u00d7 10-13 and a condition number of 1.9 \u00d7 1001. The coefficient for the grouptrust variable was estimated at -0.0442 with a standard error of 0.1698, resulting in a z-value of -0.26 and a p-value of 0.795. The 95% confidence interval for this estimate ranged from -0.3771 to 0.2886, indicating that the effect of the \"trustworthy AI\" label on the perception of competence is not statistically significant. The thresholds were estimated as follows:\nStrongly Disagree | Disagree at -3.2074 (CI: [-3.6975, -2.7173]), Disagree|Neutral at -1.8894 (CI: [-2.2027, -1.5762]), Neutral|Agree at -0.3087 (CI: [-0.5557, -0.0618]), and Agree Strongly Agree at 2.1247 (CI: [1.7871, 2.4623])"}, {"title": "Question 5: Automotive AI technologies care about our well-being", "content": "For Question 5, the model had a log-likelihood of -714.40 and an AIC of 1438.81. The model converged after five iterations, with a maximum gradient of 8.39 \u00d7 10-09 and a condition number of 2.4 \u00d7 1001. The coefficient for the grouptrust variable was estimated at 0.4862 with a standard error of 0.1651, resulting in a z-value of 2.945 and a p-value of 0.00323. The 95% confidence interval for this estimate ranged from 0.1626 to 0.8097, indicating a statistically significant positive effect of the \"trustworthy AI\" label on the perception that automotive AI technologies care about our well-being. The thresholds were estimated as follows: Strongly Disagree|Disagree at -1.2419 (CI: [-1.5177, -0.9662]), Disagree|Neutral at -0.3804 (CI: [-0.6267, -0.1341]),\nNeutral Agree at 1.0483 (CI: [0.7878, 1.3087]), and Agree Strongly Agree at 2.8723 (CI: [2.4721, 3.2725]). The results for the Strongly Disagree Disagree threshold difference was significant (p = 0.0013), with a standard error of 0.1924 and a difference of 0.7862. The result for the Neutral Agree threshold difference was also significant (p = 0.011), with a standard error of 0.1338 and a difference of 0.5065. The other threshold differences did not reach significance."}, {"title": "Question 6: Automotive AI technologies do not abuse the information and advantage they have over their users", "content": "For Question 6, the model had a log-likelihood of -666.32 and an AIC of 1342.65. The model converged after five iterations, with a maximum gradient of 1.14 \u00d7 10-07 and a condition number of 1.6 \u00d7 1001. The coefficient for the grouptrust variable was estimated at 0.1857 with a standard error of 0.1671, resulting in a z-value of 1.111 and a p-value of 0.267. The 95% confidence interval for this estimate ranged from -0.1419 to 0.5132, indicating that the effect of the \"trustworthy AI\" label on the perception of information abuse is not statistically significant. The thresholds were estimated as follows: Strongly Disagree|Disagree at -2.3873 (CI: [-2.7580, -2.0166]),\nDisagree Neutral at -1.3978 (CI: [-1.6782, -1.1173]), Neutral Agree at 0.4396 (CI: [0.1940, 0.6852]), and Agree Strongly Agree at 2.3124 (CI: [1.9653, 2.6596]). None of the thresholds reached statistical significance"}, {"title": "Question 7: I trust that automotive AI can offer information and service that is in my best interest", "content": "For Question 7, the model had a log-likelihood of -648.60 and an AIC of 1307.19. The model converged after six iterations, with a maximum gradient of 5.36 \u00d7 10-14 and a condition number of 2.0 \u00d7 1001. The coefficient for the grouptrust variable was estimated at 0.1035 with a standard error of 0.1698, resulting in a z-value of 0.61 and a p-value of 0.542. The 95% confidence interval for this estimate ranged from -0.2292 to 0.4362, indicating that the effect of the \"trustworthy AI\" label on the perception that automotive AI offers information and service in the user's best interest is not statistically significant, and none of the thresholds reached statistical significance"}, {"title": "Question 8: I feel positive toward automotive AI technologies", "content": "For Question 8, the model had a log-likelihood of -691.06 and an AIC of 1392.11. The model converged after five iterations, with a maximum gradient of 1.10 \u00d7 10-07 and a condition number of 2.4 \u00d7 1001. The coefficient for the grouptrust variable was estimated at -0.0022 with a standard error of 0.1663, resulting in a z-value of -0.013 and a p-value of 0.989. The 95% confidence interval for this estimate ranged from -0.3282 to 0.3238, indicating that the effect of the \"trustworthy AI\" label on the positive perception of automotive AI technologies is not statistically significant. No threshold differences reached significance"}, {"title": "Discussion", "content": "This study aimed to investigate the impact of labeling AI as \"trustworthy\" versus \"reliable\" on user perceptions and acceptance of automotive AI technologies. Regarding specific scenarios, the study found no significant differences between the trustworthy AI and reliable AI conditions. Users were neither more lenient nor more judgmental when AI was labeled as \"trustworthy\" or \"reliable\" in terms of accountability and blameworthiness. The same held for confidence in using and learning to use the technology. Although these findings go opposite to the prediction, they also suggest that nothing is gained especially in terms of confidence in using an anthropomorphic \"trustworthy\" label.\nThe other focus of the study was on the Technology Acceptance Model The exploration of Total TAM score (H5) and sub-categories (H5.1 - H5.8) revealed mixed results. There was no difference in total scores, which aligns with some existing results"}, {"title": "Ethical AI Design", "content": "Given that the trustworthy label did not lead to higher acceptance or confidence, it is more practical and ethically sound to communicate the reliability of these systems. This aligns with the proposal that machines, being performance-based entities, are better suited to meet reliability standards, which can be objectively measured and verified, rather than trustworthiness, which involves normative judgments."}, {"title": "Practical Implications", "content": "For developers and policymakers in the automotive AI industry, the study's findings suggest that there is no loss in using reliability in their communication practices. This label also ensures that AI systems are not described in a manner that will mislead users about the capabilities and limitations of AI systems."}, {"title": "Limitations", "content": "This study has several limitations that need to be acknowledged. First, the observed increase in benevolence could be attributed to a framing effect due to the specific wording in the definition of trustworthy AI provided to the participants in the grouptrust condition (see Definitions of Key Terms for more details). The description highlighted that trustworthy AI \u201cshould care about ethical norms and/or conceptualize moral principles\", which might have influenced participants' perceptions. Given the complex nature of trust and the various philosophical debates surrounding its definition, future research will need to walk a difficult tightrope.\nSecond, while we observed that increased perceived usefulness and benevolence did not lead to a corresponding increase in intention to use, contrary to what would have been expected (see [12-14]), this may be due to other factors unrelated to these variables. For instance, it is possible that participants had already formed opinions about whether they would use this technology, which were not influenced by the labels provided in the study. Future research could address this by employing a within-subject design to assess whether changing the label impacts participants' attitudes over the course of the study.\nThird, this study only measured user attitudes toward automotive AI. Future research should consider the effect of the trustworthy label on other AI technologies that are less tool-like, such as care-bots and chatbots."}, {"title": "Conclusion", "content": "Labeling AI as \"trustworthy\" rather than \"reliable\" does not significantly impact user attitudes or acceptance, except increasing the likelihood that users will perceive AI as caring about our wellbeing. The findings on the lack of effect on users should be distinguished from the philosophical and ethical arguments on the lack of justification for attribution of trust to AI. If emphasizing reliability over trustworthiness is more practical and ethically sound in the development and deployment of AI, the findings"}, {"title": "Descriptive Statistics", "content": "The demographic characteristics of the sample are presented below. The sample consisted of 493 participants with a balanced distribution in terms of gender and age groups. 15 participants were excluded from analysis due to being 65 years old or older, leaving a final sample of 478 participants whose data were analyzed. The majority of participants were either male (246) or female (242). There were also 3 participants who identified as non-binary, 1 participant who preferred not to say, and 1 who identified as 'other'. The age distribution was as follows: 68 participants were 18-30 years old, 240 were 30-45 years old, 167 were 45-64 years old, 15 were 65 years or older, and 3 preferred not to say. A vast majority of participants held a driver's license (476), while 14 participants did not, and 3 preferred not to disclose their status.\nThe participants reported various areas of expertise, including 9 in philosophy, 32 in computer science, 5 in informatics, 8 in data science, 10 in computer programming, 3 in machine learning, and 1 in robotics, with no participants selecting 'other'. The educational background of participants varied widely: 212 had a bachelor's degree (4-year), 53 had an associate degree (2-year), 52 had a master's degree, 81 had some college but no degree, 8 had a professional degree (JD, MD), 68 had completed high school (including GED), 3 had less than high school education, 8 had technical certification, 7 had a doctoral degree, and 1 preferred not to say. No participants selected 'other' for education. Participants had various experiences with autonomous technology, with 221 participants reporting some experience with automotive AI, such as with lane-keeping, steering, route-planning, or parking assistance (see Experience for the specific questions)."}, {"title": "Definitions of Key Terms", "content": "Participants were instructed to read and understand the definitions of \"trustworthy\" and \"reliable\" AI, as appropriate to their group assignment. These definitions were also reiterated when participants read the vignettes. Definitions were developed and informed by normative analysis and ethical considerations in previous work in the ethics of AI [3].\nTrustworthy AI\n1. It should be lawful, complying with all applicable laws and regulations."}, {"title": "Reliable AI", "content": "1. It should be lawful, complying with all applicable laws and regulations.\n2. It should be effective, shown to produce the expected outcome for the tasks it was assigned.\n3. It should be consistent, likely to produce such outcomes in the future."}, {"title": "Inclusion Criteria", "content": "To be considered for inclusion in the study, participants were required to demonstrate a minimum threshold of fluency in English. Specifically, only those achieving a success rate of at least 75% on the language check were included in the final analysis.\nAdditionally, participants were required to meet a minimum accuracy threshold of 75% on the attention check to ensure comprehension of the key components, one that was specific to group assignment."}, {"title": "Language Check", "content": "Please select all the sentences below that are written in correct English:\n1. Atheletes often need to warm up.\n2. I just saw a moose running down the road!\n3. John ill felt and went doctor.\n4. Could you the books put in that boxes?\n5. I am forget do my homeworks.\n6. She think English is more easier to learn.\n7. Tommorrow was tatiehr than today.\n8. The building is a very murnnlye.\n9. Where's the pen I gave you yesterday?\n10. He was pulled over by the police for driving 120 miles per hour."}, {"title": "Trust Group", "content": "Before continuing, please select only the statements below that align with the key components of trust in AI.\n1. If the AI is trustworthy, it will do the right thing for the right ethical reasons.\n2. If the AI is trustworthy, it neither conceptualizes moral principles nor does it care about ethical norms.\n3. Trustworthy AI cares about ethical norms and/or conceptualizes moral principles.\n4. Trustworthy AI has nothing to do with the AI having moral principles or ethical norms."}, {"title": "Reliable Group", "content": "Before continuing, please select only the statements below that align with the key components of reliability in AI.\n1. If the AI is reliable, it will produce the expected outcome for the tasks it was assigned.\n2. If the AI is reliable, it will neither produce the expected outcome, nor is it likely to do so in the future.\n3. Reliable AI consistently produces the expected outcome.\n4. Reliable AI has nothing to do with the AI being effective or consistent."}, {"title": "Vignettes", "content": "Participants were instructed to read three distinct vignettes related to automotive AI: Planning Assistance, Parking Assistance, and Steering Assistance. Each vignette was designed to evaluate AI accountability, AI blameworthiness, confidence in using the AI, and confidence in learning to drive. The vignettes were identical except for the key terms \"trustworthy\" and \"reliable,\" as specified by the participants' group assignment. The key terms were not in bold as they are below, but italicized."}, {"title": "Planning Assistance", "content": "Picture yourself in the driver's seat of a modern vehicle. You're stuck in the midst of heavy traffic during your daily commute. The vehicle is equipped with a trustworthy/reliable AI route-planning system, designed by the manufacturer to analyze real-time traffic data and recommend a new route to avoid congestion. You trust/rely on the suggestion, and the AI switches lanes and diverts to the suggested route without any additional input from you, the driver."}, {"title": "Parking Assistance", "content": "Visualize yourself navigating a suburban neighborhood in your vehicle equipped with trustworthy/reliable AI parking assistance. You come across a tight parallel parking space, and the AI recommends activating the trustworthy/reliable AI parking assistance. You trust/rely on the AI to park the vehicle, and it maneuvers the car into the tight parking spot without any additional input from you, the driver."}, {"title": "Steering Assistance", "content": "Imagine driving on a rural road with your car's trustworthy/reliable AI driving assistance providing real-time safety alerts. As you approach a curve, the trustworthy/reliable AI uses its sensors to detect an obstacle ahead and displays a warning on your dashboard screen. It also suggests that the AI should steer the vehicle to bypass the obstacle. You trust/rely on the suggestion, and the AI steers the vehicle to avoid the obstacle without any additional input from you, the driver."}]}