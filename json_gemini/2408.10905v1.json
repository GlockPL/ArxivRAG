{"title": "The impact of labeling automotive AI as \"trustworthy\" or\n\"reliable\" on user evaluation and technology acceptance", "authors": ["John Dorsch", "Ophelia Deroy"], "abstract": "This study explores whether labeling AI as either \"trustworthy\" or \"reliable\" influences\nuser perceptions and acceptance of automotive AI technologies. Utilizing a one-way\nbetween-subjects design, the research presented online participants (N=478) with a text\npresenting guidelines for either trustworthy or reliable AI, before asking them to\nevaluate 3 vignette scenarios and fill in a modified version of the Technology Acceptance\nModel which covers different variables, such as perceived ease of use, human-like trust,\nand overall attitude. While labeling AI as \u201ctrustworthy\u201d did not significantly influence\npeople's judgements on specific scenarios, it increased perceived ease of use and\nhuman-like trust, namely benevolence, suggesting a facilitating influence on usability\nand an anthropomorphic effect on user perceptions. The study provides insights into\nhow specific labels affect adopting certain attitudes toward AI technology.", "sections": [{"title": "Introduction", "content": "The wide use of the \"trustworthy AI\" denomination by manufacturers and legislators\nsparks debates. On the one hand, several philosophers have argued that it is irrational\nto place trust in a machine [1-3,20]. Instead, trust should be placed not in the machines\nthemselves, but in the humans responsible for their development and deployment [4].\nOthers, like Coeckelbergh [5] for instance, argues in favor of trusting machines,\nemphasizing that as social animals, we already live in a world structured by trusting\nrelationships. Accordingly, trust should be considered a pre-reflective condition of\nhuman experience and unproblematically extended to machines.\nThose who argue against trusting machines contend that trust can only be rationally\nemployed if the trustee is sensitive to normative constraints of reasons [3], has the\ntrustee's goodwill in mind when acting [6], holds the trustee's interests as their own [7],\nor is emotionally sensitive to moral values [8]. Since today's machines are nowehere near\nany of these conditions, many AI ethicists advocate for employing the notion of\nreliability rather than trust when dealing with machines. Reliability, decoupled from\ntrust, is understood as entailing solely performance-based or outcome-based standards\nfor evaluation [2] [3].\nBut do these distinct conceptual frameworks for conceiving of one's relationship to\nmachines- the framework of trust and the framework of reliability - influence the\nattitudes of users towards this technology?\nIn this study, we sought to test whether the deployment of either framework in the\nfield of automotive AI technology would predict changes in naive users attitudes."}, {"title": "Hypotheses", "content": "In this study, we investigated the impact of labeling AI as \"trustworthy\" versus\n\"reliable\" on various dependent variables related to technology attitudes. Our\nhypotheses (H1, H2, H3, H4, H5) examine whether the label \u201ctrustworthy AI\"\npredicts changes in AI blameworthiness, AI accountability, confidence while using, and\nconfidence while learning, and the overall TAM score. Additionally, we explored\nwhether the labels predict changes in TAM categories, including perceived ease of use,\nperceived usefulness, intention to use, ability-based and human-like trust in automotive\n\u0391\u0399 (H5.1 - H5.8) (see Table 1). The study was pre-registered to enhance transparency\nand credibility. The hypotheses remain consistent with those outlined in the\npre-registration, though the numbering has been reordered and simplified, the null\nhypotheses are reformulated as 'no predicted effect'. The pre-registration document can\nbe accessed at the Open Science Framework (OSF) via this link."}, {"title": "Materials and methods", "content": "Initially, 617 participants were recruited for the study through online platforms Cloud\nResearch and Amazon Mechanical Turk. Participants were paid approximately $12 an\nhour for completing the study, whether their data was ultimately analyzed or not. After\nconducting a language proficiency check, 66 participants were excluded, leaving 551 (see"}, {"title": "Experimental Design", "content": "A one-way between-subjects design was used. Participants were randomly assigned to\none of two groups: the \"trustworthy AI\" group or the \"reliable AI\" group. The\nassignment was random to ensure that each participant had an equal chance of being\nplaced in either group. Ultimately, data were analyzing from 241 participants assigned\nto the \"trustworthy AI\" group and 237 participants assigned to the \"reliable AI\" group,\nensuring a balanced sample. The sample consisted of 493 participants, with a balanced\ndistribution in terms of gender and age groups. As specified in the pre-registered plan,\n15 participants were excluded from analysis due to being 65 years old or older, leaving a"}, {"title": "Phase 1: Demographic Phase", "content": "This section involved the collection of demographic data (age, gender, education,\nexperience with automotive AI, driver's license status, AI expertise), and a language\ncheck to ensure fluency in English. Detailed demographic data are described in\nDescriptive Statistics, and language check criteria are provided in Supplementary\nMaterial, specifically in Language Check."}, {"title": "Phase 2: Induction Phase", "content": "Participants were presented with group-specific definitions of \"trustworthy AI\" or\n\"reliable AI\" (see Definitions of Key Terms) attributed to AI experts, as well as an\nattention check to ensure engagement and comprehension of the respective definition\n(see Attention Check)."}, {"title": "Phase 3: Vignette Phase", "content": "Participants read three separate vignettes, one for each automotive AI-assisted task:\nplanning assistance, parking assistance, and steering assistance. The vignettes were\npresented in random order and, across the two groups, the vignettes were identical\nexcept for the key terms \"trustworthy\" or \"reliable\" (and other related semantic\nderivatives) respective of group assignment. Participants answered the same four\nquestions for each of the three vignettes (AI accountability, AI blameworthiness,\nconfidence in driving, and confidence in learning) using a 5-point Likert scale, which\nwere also presented in random order. The full text of the vignettes and the questions\nasked are available in Supplementary Material, specifically in Vignettes."}, {"title": "Phase 4: Questionnaire Phase", "content": "Participants answered the eight TAM questions covering the distinct constructs\nperceived ease of use, perceived usefulness, behavioral intention, ability trust,\nhuman-like trust (benevolence and integrity), general trust, and attitude. A 5-point\nLikert scale was used to measure responses. The questions were presented in random\norder to prevent biasing effects. The complete TAM questionnaire is provided in\nSupplementary Material, specifically in Technology Acceptance Model Questionnaire.\nThe selection of items for TAM is informed by Choung et al. [14], which identified\nthe importance of trust (general, human-like, and ability-based trust) in AI and its\nimpact on technology acceptance. Their comprehensive study provides a validated\nframework for measuring key constructs related to technology acceptance. Each TAM\nvariable category was measured with one question to streamline the data collection\nprocess, minimize participant fatigue, and cohere with automotive AI.\nThe selected questions are the most appropriate for automotive AI, and the wording\nof each question has been modified to reflect this technology. The original study asked\nabout smart AI assistants and, as a result, several of the survey items do not apply."}, {"title": "Data Collection and Handling", "content": "Data was collected using the Qualtrics online survey platform. Procedures for data\nhandling and storage included secure, encrypted storage and removal of personal\nidentifiers. Data monitoring and quality assurance were ensured through exclusion of\nincomplete responses, failed language checks, and failed attention checks. See\nSupplementary Material for more, specifically Inclusion Criteria."}, {"title": "Data Analysis", "content": "For the analysis of the vignettes, independent samples t-tests were conducted to\ncompare means by condition for hypotheses related to the vignette assessments, i.e. H1\nto H4, which include AI accountability, AI blameworthiness, confidence in using\nautomotive AI, and confidence in learning how to drive with automotive AI (in the\ncontext of obtaining one's first driver's license). To further investigate the effects of\ncondition on outcomes related to the vignettes, Bayesian ordinal regression models were\nfitted for each question. These models accounted for participant and vignette as random\neffects, providing posterior estimates of the condition effect for each hypothesis.\nFor the total TAM score, addressing hypothesis H5, we employed a Cumulative Link\nMixed Model (CLMM) to account for the ordinal nature of the response variable and to\nmodel participant and TAM items as random effects. Cumulative Link Models (CLMs)\nwere used to analyze ordinal response variables related to the individual TAM questions,\nspecifically testing hypotheses H5.1 through H5.8.\nAnalysis deviated slightly from the pre-registered plan. First, the residuals of the\nt-test did not adhere to a normal distribution, as indicated by significant deviations in\nthe Shapiro-Wilk test results. Since the t-test assumes normally distributed residuals,\nits application was questioned for this data. We still performed the t-test and report the\nresults below. To address the non-normality of residuals, we also conducted a Wilcoxon\ntest as a non-parametric alternative; however, this test did not reveal any significant\nresults either. Detailed results of the Wilcoxon test are reported in Supplementary\nMaterial, respectively in T-Test Analysis and Further Analysis: Vignette Questions.\nFinally, we opted not to use Cumulative Link Mixed Models for H5.1 through H5.8 as\ninitially proposed. Since we modelled each question separately, we did not need to\naccount for random effects of items or participants, so a simpler ordinal regression\nmodel (CLM) was sufficient for analysis."}, {"title": "Overview of Findings", "content": "The results of the hypothesis testing went against most pre-registered hypotheses, which\nwere based on the literature as well as the pilot, which had a total of 43 participants, 21\nin the \"trustworthy AI\" group and 22 in the \"reliable AI\" group. For hypotheses H1,\nH2, which predicted that the trustworthy AI condition would lead to lower ratings of\nblameworthiness and accountability, we did not observe any effect and could therefore\nnot reject the null hypothesis. For Hypotheses H3, and H4, which concerned confidence\nratings and where no special prediction was issued, no effect was observed. Finally, for"}, {"title": "H1 - H4: Vignette Responses", "content": "As detailed in the pre-registered plan, we conducted t-tests to compare group means on\nAI accountability, AI blameworthiness, confidence in driving, and confidence in learning\nbetween the trustworthy AI and reliable AI groups (See Figure 1). The analysis for each\nhypothesis showed that there were no significant differences between the trustworthy AI\nand reliable AI conditions across the measured variables. The p-values for all\ncomparisons were greater than 0.05, indicating that the observed differences were not\nstatistically significant, as confirmed through Bayesian ordinal regression (See Table 2)."}, {"title": "H1: AI Accountability", "content": "There was no significant difference between the trustworthy AI group (mean = 3.47)\nand the reliable AI group (mean = 3.53) in terms of accountability, t(478) = 1.05,\np = 0.292, with a 95% confidence interval of [-0.0541,0.180]."}, {"title": "H2: AI Blameworthiness", "content": "The blameworthiness ratings were similar for the trustworthy AI group (mean = 3.50)\nand the reliable AI group (mean = 3.52), t(478) = 0.333, p = 0.739, with a 95%\nconfidence interval of [-0.0963, 0.136]."}, {"title": "H3: Confidence in Driving", "content": "Participants' confidence in driving using the automotive AI did not significantly differ\nbetween the trustworthy AI group (mean = 3.42) and the reliable AI group (mean =\n3.50), t(478) = 1.264, p = 0.207, with a 95% confidence interval of [-0.0398, 0.184]."}, {"title": "H4: Confidence in Learning", "content": "Similarly, confidence in learning to drive with the automotive AI was not significantly\ndifferent between the trustworthy AI group (mean = 3.52) and the reliable AI group\n(mean = 3.59), t(478) = 1.07, p = 0.285, with a 95% confidence interval of\n[-0.0531,0.180].\nThe jitter plot with box plot overlay illustrates the distribution of responses across\nfour key measures: AI Accountability, AI Blameworthiness, Confidence in Driving, and\nConfidence in Learning, comparing the \"Reliable AI\" and \"Trustworthy AI\" groups (See\nFigure 2). The box plots, which summarize the central tendency and spread of the\nresponses, show that the medians and interquartile ranges (IQRs) are similar across\nboth conditions for each measure. The overlapping IQRs in the box plots indicate that\nparticipants in both conditions exhibit similar attitudes and perceptions, particularly in\nterms of confidence in driving and learning, as well as their views on AI accountability\nand blameworthiness.\nTo further examine the potential differences between the \"Reliable AI\" and\n\"Trustworthy AI\" conditions, Bayesian ordinal regression models were applied to each of\nthe four questions in the vignette study (See Table 2). The analysis yielded small\nestimated group effects across all questions, with 95% credible intervals consistently\nencompassing zero, indicating no strong evidence for significant differences between the\ngroups. Specifically, the probabilities that the group effect is greater than zero were all"}, {"title": "H5: Total TAM Score", "content": "For the Total TAM Score, we conducted ordinal regression analyses to examine the\nrelationship between the condition and participants' perceptions across the total sum of\nthe various questions. The mixed model had a log-likelihood of -4498.18 and an AIC of\n9010.35. The model converged after 538 iterations, with a maximum gradient of\n1.01 \u00d7 10-3 and a condition number of 1.8 \u00d7 1002. The coefficient for the grouptrust\nvariable was estimated at 0.2058 with a standard error of 0.2137, resulting in a z-value"}, {"title": "Question 1: Learning to use automotive AI would be easy for me", "content": "For Question 1, the model had a log-likelihood of -630.88 and an AIC of 1271.77. The\nmodel converged after six iterations, with a maximum gradient of 3.92 \u00d7 10-13 and a\ncondition number of 2.5 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at 0.3354 with a standard error of 0.1697, resulting in a z-value of 1.976 and\na p-value of 0.0481. The 95% confidence interval for this estimate ranged from 0.0028 to\n0.6679, indicating a statistically significant effect of the \"trustworthy AI\" label on the\nperception of ease of learning. The thresholds were estimated as follows: Strongly\nDisagree|Disagree at -3.6908 (CI: [-4.3346, -3.0470]), Disagree Neutral at -1.7084 (CI:\n[-2.0130, -1.4038]), Neutral|Agree at -0.3899 (CI: [-0.6386, -0.1412]), and\nAgree|Strongly Agree at 1.7564 (CI: [1.4553, 2.0575]) (see Figure 5). The result for the\nNeutral Agree threshold difference was significant (p = 0.016), with a standard error of\n0.1391 and a difference of 0.4624. The other threshold differences did not reach\nsignificance."}, {"title": "Question 2: Using automotive AI would improve my performance at\naccomplishing driving-related tasks", "content": "For Question 2, the model had a log-likelihood of -688.55 and an AIC of 1387.09. The\nmodel converged after five iterations, with a maximum gradient of 2.10 \u00d7 10-08 and a\ncondition number of 1.9 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at -0.0318 with a standard error of 0.1657, resulting in a z-value of -0.192\nand a p-value of 0.848. The 95% confidence interval for this estimate ranged from\n-0.3566 to 0.2929, indicating that the effect of the \"trustworthy AI\" label on\nperformance improvement is not statistically significant. The thresholds were estimated"}, {"title": "H5.3: Intention to Use", "content": "We conducted an ordinal regression analysis to examine the relationship between the\ngroup labels, either \"trustworthy AI\" or \"reliable AI\" and the participants' intention to\nuse the technology. The analysis used a logit link function with flexible thresholds. The\nmodel included 478 observations, with a log-likelihood of -704.85 and an AIC of\n1419.70. The model converged after five iterations, with a maximum gradient of\n7.07 \u00d7 10-10 and a condition number of 2.2 \u00d7 1001.\nThe coefficient for the grouptrust variable was estimated at -0.0877 with a\nstandard error of 0.1653, resulting in a z-value of -0.531 and a p-value of 0.596. The\n95% confidence interval for this estimate ranged from -0.4116 to 0.2363, indicating that\nthe effect of the \"trustworthy AI\" label on the intention to use is not statistically\nsignificant. This result supports the hypothesis, suggesting that the label \"trustworthy\nAI\" does not predict a change in the intention to use the technology (See Figure 7).\nThe threshold coefficients indicate the points at which the response transitions from\none category to the next on the Likert scale. The threshold between \"Strongly Disagree\"\nand \"Disagree\" was estimated at -2.4660 with a standard error of 0.1878, resulting in a\nz-value of -13.134 and a 95% confidence interval ranging from -2.8340 to -2.0981.\nThe threshold between \"Disagree\" and \"Neutral\" was estimated at -1.3005 with a\nstandard error of 0.1393, resulting in a z-value of -9.336 and a 95% confidence interval"}, {"title": "Question 4: Automotive AI technologies are competent in their area of\nexpertise", "content": "For Question 4, the model had a log-likelihood of -622.62 and an AIC of 1255.24. The\nmodel converged after six iterations, with a maximum gradient of 3.13 \u00d7 10-13 and a\ncondition number of 1.9 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at -0.0442 with a standard error of 0.1698, resulting in a z-value of -0.26 and\na p-value of 0.795. The 95% confidence interval for this estimate ranged from -0.3771\nto 0.2886, indicating that the effect of the \"trustworthy AI\" label on the perception of\ncompetence is not statistically significant. The thresholds were estimated as follows:\nStrongly Disagree | Disagree at -3.2074 (CI: [-3.6975, -2.7173]), Disagree|Neutral at\n-1.8894 (CI: [-2.2027, -1.5762]), Neutral|Agree at -0.3087 (CI: [-0.5557, -0.0618]),\nand Agree Strongly Agree at 2.1247 (CI: [1.7871, 2.4623]) (see Figure 8)."}, {"title": "Question 5: Automotive AI technologies care about our well-being", "content": "For Question 5, the model had a log-likelihood of -714.40 and an AIC of 1438.81. The\nmodel converged after five iterations, with a maximum gradient of 8.39 \u00d7 10-09 and a\ncondition number of 2.4 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at 0.4862 with a standard error of 0.1651, resulting in a z-value of 2.945 and\na p-value of 0.00323. The 95% confidence interval for this estimate ranged from 0.1626\nto 0.8097, indicating a statistically significant positive effect of the \"trustworthy AI\"\nlabel on the perception that automotive AI technologies care about our well-being. The\nthresholds were estimated as follows: Strongly Disagree|Disagree at -1.2419 (CI:\n[-1.5177, -0.9662]), Disagree|Neutral at -0.3804 (CI: [-0.6267, -0.1341]),\nNeutral Agree at 1.0483 (CI: [0.7878, 1.3087]), and Agree Strongly Agree at 2.8723 (CI:\n[2.4721, 3.2725]) (see Figure 9). The results for the Strongly Disagree Disagree threshold\ndifference was significant (p = 0.0013), with a standard error of 0.1924 and a difference\nof 0.7862. The result for the Neutral Agree threshold difference was also significant\n(p = 0.011), with a standard error of 0.1338 and a difference of 0.5065. The other\nthreshold differences did not reach significance."}, {"title": "Question 6: Automotive AI technologies do not abuse the information and\nadvantage they have over their users", "content": "For Question 6, the model had a log-likelihood of -666.32 and an AIC of 1342.65. The\nmodel converged after five iterations, with a maximum gradient of 1.14 \u00d7 10-07 and a\ncondition number of 1.6 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at 0.1857 with a standard error of 0.1671, resulting in a z-value of 1.111 and\na p-value of 0.267. The 95% confidence interval for this estimate ranged from -0.1419\nto 0.5132, indicating that the effect of the \"trustworthy AI\" label on the perception of\ninformation abuse is not statistically significant. The thresholds were estimated as\nfollows: Strongly Disagree|Disagree at -2.3873 (CI: [-2.7580, -2.0166]),\nDisagree Neutral at -1.3978 (CI: [-1.6782, -1.1173]), Neutral Agree at 0.4396 (CI:\n[0.1940, 0.6852]), and Agree Strongly Agree at 2.3124 (CI: [1.9653, 2.6596]). None of the\nthresholds reached statistical significance (see Figure 10)."}, {"title": "Question 7: I trust that automotive AI can offer information and service\nthat is in my best interest", "content": "For Question 7, the model had a log-likelihood of -648.60 and an AIC of 1307.19. The\nmodel converged after six iterations, with a maximum gradient of 5.36 \u00d7 10-14 and a\ncondition number of 2.0 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at 0.1035 with a standard error of 0.1698, resulting in a z-value of 0.61 and a\np-value of 0.542. The 95% confidence interval for this estimate ranged from -0.2292 to\n0.4362, indicating that the effect of the \"trustworthy AI\" label on the perception that\nautomotive AI offers information and service in the user's best interest is not\nstatistically significant, and none of the thresholds reached statistical significance (see\nFigure 11)."}, {"title": "Question 8: I feel positive toward automotive AI technologies", "content": "For Question 8, the model had a log-likelihood of -691.06 and an AIC of 1392.11. The\nmodel converged after five iterations, with a maximum gradient of 1.10 \u00d7 10-07 and a\ncondition number of 2.4 \u00d7 1001. The coefficient for the grouptrust variable was\nestimated at -0.0022 with a standard error of 0.1663, resulting in a z-value of -0.013\nand a p-value of 0.989. The 95% confidence interval for this estimate ranged from\n-0.3282 to 0.3238, indicating that the effect of the \"trustworthy AI\" label on the\npositive perception of automotive AI technologies is not statistically significant. No\nthreshold differences reached significance (see Figure 12)."}, {"title": "Discussion", "content": "This study aimed to investigate the impact of labeling AI as \"trustworthy\" versus\n\"reliable\" on user perceptions and acceptance of automotive AI technologies. Regarding\nspecific scenarios, the study found no significant differences between the trustworthy AI\nand reliable AI conditions. Users were neither more lenient nor more judgmental when\nAI was labeled as \"trustworthy\" or \"reliable\" in terms of accountability and\nblameworthiness. The same held for confidence in using and learning to use the\ntechnology. Although these findings go opposite to the prediction, they also suggest\nthat nothing is gained especially in terms of confidence in using an anthropomorphic\n\"trustworthy\" label.\nThe other focus of the study was on the Technology Acceptance Model The\nexploration of Total TAM score (H5) and sub-categories (H5.1 - H5.8) revealed mixed\nresults. There was no difference in total scores, which aligns with some existing results"}, {"title": "Ethical AI Design", "content": "Given that the trustworthy label did not lead to higher acceptance or confidence, it is\nmore practical and ethically sound to communicate the reliability of these systems. This\naligns with the proposal that machines, being performance-based entities, are better\nsuited to meet reliability standards, which can be objectively measured and verified,\nrather than trustworthiness, which involves normative judgments."}, {"title": "Practical Implications", "content": "For developers and policymakers in the automotive AI industry, the study's findings\nsuggest that there is no loss in using reliability in their communication practices. This\nlabel also ensures that AI systems are not described in a manner that will mislead users\nabout the capabilities and limitations of AI systems."}, {"title": "Limitations", "content": "This study has several limitations that need to be acknowledged. First, the observed\nincrease in benevolence could be attributed to a framing effect due to the specific\nwording in the definition of trustworthy AI provided to the participants in the\ngrouptrust condition (see Definitions of Key Terms for more details). The description\nhighlighted that trustworthy AI \u201cshould care about ethical norms and/or conceptualize\nmoral principles\", which might have influenced participants' perceptions. Given the\ncomplex nature of trust and the various philosophical debates surrounding its definition,\nfuture research will need to walk a difficult tightrope.\nSecond, while we observed that increased perceived usefulness and benevolence did\nnot lead to a corresponding increase in intention to use, contrary to what would have\nbeen expected (see [12-14]), this may be due to other factors unrelated to these\nvariables. For instance, it is possible that participants had already formed opinions\nabout whether they would use this technology, which were not influenced by the labels\nprovided in the study. Future research could address this by employing a within-subject\ndesign to assess whether changing the label impacts participants' attitudes over the\ncourse of the study.\nThird, this study only measured user attitudes toward automotive AI. Future\nresearch should consider the effect of the trustworthy label on other AI technologies\nthat are less tool-like, such as care-bots and chatbots."}, {"title": "Conclusion", "content": "Labeling AI as \"trustworthy\" rather than \"reliable\" does not significantly impact user\nattitudes or acceptance, except increasing the likelihood that users will perceive AI as\ncaring about our wellbeing. The findings on the lack of effect on users should be\ndistinguished from the philosophical and ethical arguments on the lack of justification\nfor attribution of trust to AI. If emphasizing reliability over trustworthiness is more\npractical and ethically sound in the development and deployment of AI, the findings"}, {"title": "Goodness of Fit Evaluation", "content": "The Shapiro-Wilk test results for the residuals revealed significant deviations from\nnormality across all questions. For \u201cAI Accountability,\" the Shapiro-Wilk statistic was"}, {"title": "CLMM Analysis: Total TAM Score", "content": "Given that the Cumulative Link Mixed Model does not assume a normal distribution of\nresiduals, we assessed the goodness of fit using likelihood ratio tests instead of residual\nanalysis. The likelihood ratio test compares the fit of a null model, which includes only\nthe random effects, to a full model that also includes the group effect. The results of\nthis test indicate that the inclusion of the group variable does not significantly improve\nthe model fit. Specifically, the likelihood ratio statistic was 0.9269 with a p-value of\n0.3357, suggesting that the model's fit does not improve significantly when the group\nvariable is included. This supports the finding that the group effect is not statistically\nsignificant, as reflected in the earlier analysis. The non-significant p-value further\nconfirms that the inclusion of the group variable does not contribute meaningfully to\nexplaining the variability in the response."}, {"title": "CLM Analysis: Individual TAM Questions", "content": "For the same reason as above, we have evaluated the goodness of fit of Cumulative Link\nModels through likelihood ratio tests, instead of residual analysis as originally planned.\nThese tests compare the fit of the base model, which includes only the group variable,\nto models incorporating additional predictors. For the base model in each TAM\nquestion, the likelihood ratio test results reveal that, in all but one case, adding\nadditional predictors does not significantly improve model fit. Specifically, p-values for\nQuestions 1, 2, 3, 6, 7, and 8 range from 0.245 to 0.980, indicating that the base model\nperforms comparably to models with added predictors. The exception is Question 4,\nwhere a p-value of 0.029 suggests a marginal improvement in model fit with additional\npredictors. Thus, overall, the base model, which includes only the group variable,\nappears to be a generally adequate fit for the data across questions. For example, none\nof the models showed a significant improvement in fit when including gender as a\npredictor and its interaction with group (p-values ranged from 0.387 to 0.896), except\nfor Question 6 where the p-value was 0.013, indicating an improvement. However, this\ndid not affect the significance of the findings. For instance, the p-value for the\ncoefficient of grouptrust is 0.757, for Female is 0.163, and for the interaction term\ngrouptrust is 0.426."}, {"title": "Outlier Analysis", "content": "We conducted an outlier analysis using the Interquartile Range (IQR) method to\nidentify extreme values across each TAM question. Specifically, outliers were defined as\nobservations that fell below the first quartile (Q1) minus 1.5\u00d7 the IQR or above the\nthird quartile (Q3) plus 1.5\u00d7 the IQR.\nAcross all eight questions, the total number of responses was consistent at 478, with\nthe majority of outliers selecting \"Strongly Disagree\", the one exception being Question\n5, which had no outliers. For Question 5, the median response was \"Agree,\" with an\ninterquartile range (IQR) from \"Neutral\" to \"Agree.\"\nFor Question 1, there were 10 outliers, with a median of \"Neutral\" and an IQR from\n\"Disagree\" to \"Agree.\" Question 2 had 36 outliers, with a median of Neutral\" and an\nIQR of Disagree\" to \"Agree.\" In Question 3, 39 outliers were identified, with a median\nof Disagree\" and an IQR from \u201cDisagree\u201d to \"Agree.\" Question 4 showed 19 outliers,"}]}