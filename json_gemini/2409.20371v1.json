{"title": "Frequency Adaptive Normalization For Non-stationary Time Series Forecasting", "authors": ["Weiwei Ye", "Songgaojun Deng", "Qiaosha Zou", "Ning Gui"], "abstract": "Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns. To address the non-stationarity, reversible instance normalization has been recently proposed to alleviate impacts from the trend with certain statistical measures, e.g., mean and variance. Although they demonstrate improved predictive accuracy, they are limited to expressing basic trends and are incapable of handling seasonal patterns. To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns. Specifically, we employ the Fourier transform to identify instance-wise predominant frequent components that cover most non-stationary factors. Furthermore, the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model. FAN is a model-agnostic method that can be applied to arbitrary predictive backbones. We instantiate FAN on four widely used forecasting models as the backbone and evaluate their prediction performance improvements on eight benchmark datasets. FAN demonstrates significant performance advancement, achieving 7.76%~37.90% average improvements in MSE. Our code is publicly available\u00b2.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting plays a key role in various fields such as traffic [8], finance [23] and infectious disease [1], etc. Recent research focuses on deep learning-based methods, as they demonstrate promising capabilities to capture complex dependencies between variables [47, 42, 46]. However, time series with trends and seasonality, also called non-stationary time series [15], create covariate pattern shifts across different time steps. These dynamics pose significant challenges in forecasting.\nTo mitigate non-stationarity issues, reversible normalization has been proposed [33, 17] which first removes non-stationary information from the input and returns the information back to rebuild the output. Current work focuses on removing non-stationary signals with statistical measures, e.g., mean and variance in the time domain [10, 28]. However, while these methods have improved prediction accuracy, these statistical measures are only capable of extracting the most prominent component, i.e., the trend, leaving substantial room for improvement. They, we argued, can hardly measure the characteristics of seasonal patterns, which are quite common in many time series. This significantly limits their capability to handle the non-stationarity, especially the seasonal patterns.\nWe illustrate an example featuring one of the simplest non-stationary signals in Fig. 1. This graph shows a time-variant signal with a gradually damping frequency, which is widely seen in many passive systems, e.g., spring-mass damper systems. As depicted in Fig. 1, the three input stages"}, {"title": "2 Related Work", "content": "Time series forecasting has been a hot topic of study for many years. This section provides discussions on related work from the following three perspectives."}, {"title": "2.1 Time Series Forecasting", "content": "Traditional statistical methods, such as ARIMA [2], assume the stationarity of the time series and dependencies between temporal steps. Although these methods provide theoretical guarantees, they typically require data with ideal properties, which is often inconsistent with real-world scenarios [42]. Besides, they can only handle a limited amount of data and features. In recent years, the field has witnessed a significant proliferation in the application of deep learning techniques for multivariate time series forecasting, a development ascribed to their ability in handling high-dimensional datasets. Consequently, various methods have been proposed to model time series data. Work based on recurrent neural networks [36, 4] preserves the current state and models the evolution of time series as a recurrent process. However, they generally suffer from a limited receptive field, which restricts their ability to capture long temporal dependencies [47]. Inspired by their successes in Computer Vision (CV) and Natural Language Processing (NLP), convolutional neural networks and the self-attention mechanism have been extensively utilized in time series forecasting [22, 19, 40, 25]. Nevertheless, those works still face difficulties in handling non-stationary data with covariate pattern shifts. Making an accurate prediction for non-stationary time series remains challenging."}, {"title": "2.2 Non-stationary Time Series Forecasting", "content": "To address non-stationarity, many methods directly model non-stationary phenomena with different modeling techniques. Li et al. [24] utilize a domain-adaptation paradigm to predict data distributions. Du et al. [6] propose an adaptive RNN to alleviate the impact of non-stationary factors through distribution matching. Liu et al. [26] introduce a non-stationary Transformer with de-stationary attention that incorporates non-stationary factors in self-attention mechanisms. To model non-linear dynamic systems, several models based on Koopman theory [21, 29, 37, 44] have been proposed with Fourier transform. To learn different patterns at different scales, Wang et al. [38] employs global and local Koopman operators. Liu et al. [27] model non-stationarity identified with Fourier transform and use Koopman operators to learn those components. However, these solutions typically select fixed frequency components based on the whole sequence rather than frequencies based on inputs. This time-invariant assumption can hardly be true in real-world scenarios."}, {"title": "2.3 Instance-wise Normalization against Non-stationarity", "content": "To mitigate the time-variant property of non-stationary time series, a set of instance-wise normaliza- tion methods have been proposed to remove the impacts from non-stationarity. To reflect instance-wise changes, Ogasawara et al. [31] propose the usage of normalization based on local properties rather than global statistics. Passalis et al. [33] introduce an adaptive and learnable approach to this instance- wise normalization paradigm. However, although these methods effectively remove non-stationary components from inputs, they still need to predict the non-stationary time series in the output series, which remains challenging. In response, reversible instance normalization [17] is introduced by reintegrating the removed non-stationary components back to reconstruct the output. However, it still assumes unchanged trends between inputs and outputs. Kim et al. [17] developed RevIN, which mainly addresses evolving trends between input sequences. Recent works [10, 28] explore trends at a finer granularity, e.g., at the sliced level. However, these approaches still model non-stationarity with temporal statistical distribution parameters and fail to account for evolving seasonality, which is a crucial aspect of non-stationarity [35, 11, 45]."}, {"title": "3 Proposed Method: FAN", "content": "Given a multivariate time series $X \\in \\mathbb{R}^{N\\times D}$, where $N$ is the total time steps and $D$ denotes the number of feature dimensions. We use inputs series with length $L$ to predict outputs series within length $H$. The forecast task can be formulated as: $X_{t-L:t} \\rightarrow X_{t+1:t+H}$, where $X_{t-L:t} \\in \\mathbb{R}^{L\\times D}$ and $X_{t+1:t+H} \\in \\mathbb{R}^{H \\times D}$. For a clearer notation, we denote the input and output series as $X_t \\in \\mathbb{R}^{L\\times D}$ and $Y_t \\in \\mathbb{R}^{H \\times D}$ respectively.\nOur proposed method, FAN, consists of symmetrically structured instance-wise normalization and denormalization layers, illustrated in Fig. 2. The normalization process removes the impacts of non-stationary signals through frequency domain decomposition (upper left part of Fig. 2), while the denormalization process, supported by a prediction module, addresses potential shifts in frequency components between the input and output (lower part of Fig. 2)."}, {"title": "3.1 Frequency-based Normalization", "content": "First, FAN removes the top $K$ dominant components in the frequency domain for each input instance, so the forecasting backbone can concentrate on the stationary aspects of the input. We term this process as Frequency Residual Learning (FRL). The input at time $t$, $X_t$, is multivariate with $D$ dimension, and each dimension might have different frequency patterns; thus, we apply the FRL to each dimension in a channel independence setting [30]. Here, the FRL is realized by the 1-dim Discrete Fourier Transform (DFT) with DFT($\\cdot$) towards each input $X_t$:\n$Z_t = DFT(X_t) \\text{ and } K_t = TopK(Amp(Z_t)) \\text{ and } X_{res}^t = IDFT(Filter(K_t, Z_t)) $  (1)\nEqu. 1 shows that DFT($\\cdot$) transforms an input into Fourier components in complex values, denoted $Z_t \\in \\mathbb{C}^{T\\times D}$. Then, TopK($\\cdot$) selects the frequency set with the top $K$ largest amplitude, which are calculated with Amp() function. Filter is the operation to filter out the $K_t$ frequency from $Z_t$. To mitigate the impact of non-stationary signals, FRL restores the top $K$ components into time domain components $X_{non}^t$ with IDFT(.)."}, {"title": "3.2 Forecast & Denormalization", "content": "As a result, the normalization layer allows the forecast backbone model go to focus more on the dynamics within the inputs. Here, following the reversible instance normalization paradigm, the forecast backbone $g_\\theta$ receives the transformed data $X_{res}^t$ as input and forecasts only the stationary part $\\hat{Y}_{res}^t$ of the outputs $Y_t$. This design makes it easier for the model to forecast non-stationary time series. Then, we apply the removed non-stationary information back to the output. We define this process as:\n$\\hat{Y}_{res}^t = g_\\theta(X_{res}^t)$\n$\\hat{Y}_t = \\hat{Y}_{res}^t + \\hat{Y}_{non}^t$ (3)\nwhere $\\hat{Y}_{non}^t$ is the reconstruct signal of $X_{non}^t$. We illustrate $\\hat{Y}_{non}^t$ as follow:\nNon-stationary shift forecasting. For reverse instance normalization, we need to estimate $\\hat{Y}_{non}^t$ in the outputs. As an input and its corresponding output are rather close, RevIN [17] directly adds $X_{non}^t$ back by assuming $\\hat{Y}_{non}^t$ with exactly the same trend as $X_{non}^t$. However, this assumption can hardly be true as the non-stationary information between the input and output may evolve. Furthermore, although SAN [28] and Dish-TS [10] predict statistics to address the discrepancy between the input and output, these statistics can only represent the most salient trend patterns.\nTo this end, rather than predicting statistics [10, 28], we use a simple MLP model $q_f$ to directly predict future values of the composite top K frequency components for D dimensions, defined as:\n$\\hat{Y}_{non}^t = q_f(X_{non}^t, X_t) = W_3 ReLU(W_2 Concat(ReLU(W_1X_{non}^t), X_t))$ (4)\nwhere $W_1, W_2, W_3$ are all learnable parameters. Here, since $X_{non}^t$ only contains top K frequency information, it is difficult to capture variations in other frequencies solely relying on $X_{non}^t$. Therefore,"}, {"title": "Loss Functions", "content": "To help with the residual learning process, we incorporate a prior guidance loss for the prediction of principal frequency components, the final loss is defined in Eq. 5. The forecast with prior guidance can be considered a multi-task optimization problem [12], where $\\mathcal{L}_{nonstat}$ ensures $q_\\phi$ accurately predict the non-stationary principal frequency component and $\\mathcal{L}_{forecast}$ guarantees that both model optimizes along the overall forecast accuracy.\n$\\phi, \\theta = \\underset{\\Phi,\\Theta}{\\text{arg min }} \\sum_t (\\mathcal{L}_{nonstat}(Y_{non}^t, \\hat{Y}_{non}^t) + \\mathcal{L}_{forecast}(\\hat{Y}_t, Y_t))$ (5)\nHere, the mean square error loss is used for both loss functions."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "Datasets. We use eight popular datasets in multivariate time series forecasting as benchmarks, including: (1-4) ETT (Electricity Transformer Temperature) 3[47] records the oil temperature and load of the electricity transformers from July 2016 to July 2018. Four subsets are included in this dataset, where ETThs are sampled per hour and ETTms per 15 minutes. (5) Electricity 4 contains the electricity consumption of 321 clients from July 2016 to July 2019 per 15 minutes. (6) ExchangeRate 5 contains the daily exchange rates of 8 countries from 1990 to 2016. (7) Traffic 6 includes the hourly traffic load of San Francisco freeways recorded by 862 sensors from 2015 to 2016. (8) Weather 7 is made up of 21 indicators of weather, including air temperature and humidity collected every 10 minutes in 2021.\nFor preprocessing, we apply z-score normalization [12] on all datasets to scale different variables to the same scale. Note that z-score normalization is unable to handle non-stationary time series since the statistics remain unchanged for different input instances [17]. The split ratio for training, validation, and test sets is set to 7:2:1 for all the datasets. We report datasets properties in Table 1, including (1) Trend Variation: Differences in the means across different sections of the dataset. (2) Seasonality Variation: We report the average variance over the Fourier spectrum to examine the presence of evolving seasonality. Other dataset details can be found at Appendix B.\nEvaluation. We set the prediction length $H \\in \\{96, 168, 336, 720\\}$, covering both short- and long- term rediction [30]. A fixed input window length $L = 96$ is used for all datasets. We evaluate the performance of baselines using mean squared error (MSE) and mean absolute error (MAE). the MSE and MAE are computed on z-score normalized data to measure different variables on the same scale. We report the final results on the test set for the model that performed optimally on the validation set.\nBackbone Models. FAN is model-agnostic and could be applied to any prediction backbones. To validate its effectiveness, four state-of-the-art time-series forecasting model are used: MLP-based DLinear [46], Transformer-based Informer [47] and FEDformer[48], and convolutional neural network-based SCINet [25]. Notably, FEDformer also employs the Fourier transform for analyzing seasonality. Results later show FAN still makes considerable improvements over FEDformer."}, {"title": "4.2 Main Results", "content": "We report MAE/MSE forecasting errors of the baselines and FAN in Table 2. Since the performance in ETT datasets shows similar results, only results of ETTm2 are reported. The full results of the ETT benchmarks and further discussion are in Appendix E.2.\nAs shown in Table 2, our proposed FAN effectively enhances the performance of all four backbone models, by a large margin, achieving state-of-the-art performance on five datasets. Specifically, on the ETTm2, Electricity, Exchange, Traffic, and Weather datasets, the average MSE performance improvements are rather significant: 10.81%, 21.49%, 51.27%, 21.97%, and 21.55% respectively. It clearly shows that frequency residual learning of FAN effectively mitigates the impacts of evolving seasonal and trend patterns and enhances the stationarity that simplifies the prediction for backbones.\nFurthermore, FAN shows increasing performance improvements by prolonging the prediction length in the Informer backbone, from 96 steps to 720 steps, the MSE improvements are 9.87%, 18.87%, 36.91%, 16.26%, and 20.05%, respectively. We believe this can be attributed to the fact that the periodicity contained in the prediction series increases with step length, and the FAN's pattern prediction module helps uncover periodicity in longer step lengths, thereby enhancing long-term prediction effectiveness. It is important to note that even in the models that utilize FFT to analyze seasonal patterns, like FEDformer, we still observe significant performance improvements (19.81%). This conclusion underscores our model's advantage in handling non-stationary aspects by directly extracting non-stationary seasonality patterns rather than learning these patterns."}, {"title": "4.3 Comparison With Reversible Instance Normalization Methods", "content": "In this section, we compare FAN with three state-of-the-art normalization methods for non-stationary time series forecasting: SAN [28], Dish-TS [10], and RevIN [17], with the same experimental setup as Sect. 4.2. We report the average MSE over all the forecasting lengths of all backbones for all datasets in Table 3. It is evident that FAN generally outperforms the baseline models, except for a few cases with a close margin. Here, SAN generally ranks second as it slices the whole sequence into sub-series which can make seasonal patterns into evolving trends that could be partially predicted with its statistics prediction module. In comparison, RevIN and Dish-TS have much worse performance. Detailed results of all cases and further discussions are provided in Appendix E.4."}, {"title": "4.4 TopK vs. Frequency Distributions", "content": "As different datasets might have different non-stationary patterns, it is crucial to select appropriate K frequency components from inputs. We study the relations between the selected TopK and the frequency distribution on the ExchangeRate and Traffic datasets.\nFig. 5 plots the frequency amplitude distribution for frequency 0~32 by performing DFT towards the different input instances with $L = 96$ of the whole training sequences. Here, we can see the clear relation between the selection of K and the frequency amplitude distributions. As we can see, the Traffic dataset contains rich seasonal signals ranging from 0~32 while the ExchangeRate dataset only has mainly one principal frequency component with frequency 0 (trend) in the inputs. Thus, the prediction on the ExchangeRate dataset might not benefit from a bigger K while a bigger K indeed helps for the Traffic dataset. Results for more datasets are in Appendix B."}, {"title": "4.5 Ablation Studies", "content": "Main Components. This section aims to evaluate the effectiveness of various FAN's designs. Three variants are studied: \"w/o predict\" denotes the removal of the non-stationary pattern prediction module and directly reconstructing $\\hat{Y}_{non}^t$ with $X_{non}^t$. \u201cpure backbone\u201d refers to the omission of the reconstruction in the output or \"w/o backbone\" is the omission of the stationary part. We evaluate their performance on two non-stationary datasets, ETTh1 and Weather. The experimental settings are consistent with those described in Section 4.2. The evaluation results, along with the standard deviations, are presented in Table 4. The results show that FAN achieves best performance across all metrics in all variants. FAN w/o backbone ranks second as the learning model of FAN already learns the principle changes. In comparison, the results from pure backbone is the weakest, as it cannot handle nonstationary signals. FAN w/o predict also has poor performance. Those results clearly"}, {"title": "5 Concolusion", "content": "In this paper, we study the problem of non-stationary time series prediction. We identify the fact that traditional statistical measurement-based instance-wise normalization can not effectively recover the evolving seasonal patterns. We propose FAN to perform instance normalization for each input window. The Fourier transform is used to remove the main frequency components in the inputs and reconstruct the Fourier basis through denormalization. To address the evolving trend and seasonal patterns between inputs and outputs, we utilize a simple MLP model to predict the changes in the extracted non-stationary pattern. The effectiveness of FAN is verified with a set of experiments on eight widely used benchmark datasets. Compared to other state-of-the-art normalization baselines, FAN significantly improves the prediction performance and outperforms state-of-the-art normalization methods. One potential avenue for improvement involves the autonomous determination of an optimal K for the selection of principal frequency components."}, {"title": "A Reproducibility", "content": ""}, {"title": "A.1 Experiment Details", "content": "We make our codes publicly available 2, including the backbones and baselines, the backbones and baselines code are based on their public GitHub repositories. We used a batch size of 32, a learning rate of 0.0003, and trained each run for 100 epochs, with an early stopper set to patience as 5. For the experimental results, $K$ is set as the number of frequencies greater than 10% of the maximum amplitude. For a fair comparison, other normalization methods were also tuned accordingly to ensure optimal results, only the normalization hyperparameters were tuned, no other experiment parameters are tuned during the experiment phase."}, {"title": "A.2 Pseudocode of GPU-Friendly Normalization", "content": ""}, {"title": "B Dataset Detials", "content": ""}, {"title": "B.1 Fourier Amplitude Distribution", "content": "Frequency amplitude variation and composition are closely related to the non-stationary pattern shift [20]. To analyse its effect, we use $L = 96$ to plot the frequency amplitude distribution of all input windows used in our eight benchmarks in Fig. 7.\nIn Fig. 7, it can be clearly observed that in many datasets such as ETTh1, ETTh2, ETTm1, Traffic, and Electricity datasets, besides the low-frequency trend patterns, the high-frequency parts also exhibit significant variations, especially in ETTh1, ETTm1, and Traffic datasets. This may also be the reason for our significant improvements on these datasets (with maximum improvements of 19.90%, 7.02%, and 18.65% respectively). However, in the ETTh2, ExchangeRate, and Weather datasets, which have relatively low seasonal variation, our model's improvement compared to state-of-the-art methods is relatively smaller. This is naturally because these datasets do not contain much seasonal non-stationary information for further improvement."}, {"title": "B.2 Main Frequency Density", "content": "FAN select top K amplitude signals, compared to previous methods based on Fourier transform, we do not use a fixed frequency set [41, 27] or randomly select [48] the frequencies. This is aligned with our observations in real data: the principal frequency signals may have a distinct distribution, rather than being composed solely of fixed or pure random frequency signals. We plot the probability of input frequencies being selected into the top 10 signals in the input signal, as shown in Fig. 8. Although the low-frequency trend signals dominate in amplitude, many high-frequency signals still play a dominant role in some inputs, highlighting the importance of considering the entire spectrum, not just the low/high or random selected frequencies. Furthermore, this analysis shows that there may be significant differences of the main frequency components between different inputs.\nHowever, previous methods based on the Fourier transform assumed that the main frequency signal is constant across inputs [41, 27]. In contrast, our model can dynamically extract Fourier-based signals from the inputs which enables better extraction of seasonal information, especially when the input patterns vary a lot."}, {"title": "B.3 Variation of Main/Residual Components", "content": "We examine the relative variations of the normalized main and residual components in Fig. 9. The quantitative results are obtained by calculating the relative Fourier components amplitude variations between the input and output in the frequency domain. In particular, across all benchmarks, the variations in the main frequency components are smaller than those in the residuals. We believe that this is the reason why a simple MLP is effective enough to capture the main frequency variations, as its shift is relatively small, as shown in Appendix D.2."}, {"title": "B.4 Trend/Seasonal Variation", "content": "We explain more details of how the trend and seasonal variation in Table 1 is calculated.\nTrend Variation To capture global trend shifts, we calculate the mean values over different regions of the dataset. Specifically, given a timeseries dataset $X \\in \\mathbb{R}^{N\\times D}$, we first chronologically split it into $X^{train}$, $X^{val}$, and $X^{test}$, representing the training, validation, and testing datasets, respectively. The trend variations are then computed as follows:\n$\\text{Trend Variation} = \\frac{|Mean_n(X^{train}) \u2013 Mean_n(X^{val,test})|}{Mean_n(X^{train})}$ (6)\nwhere the subscripts indicate the dimension of mean, $\\mid\\mid$ denotes the absolute value operation, and $X^{val,test}$ represents the concatenation of the validation and test sets. Note that, to obtain relative results across different datasets, the trend variation is normalized by dividing by the mean of the training dataset. We fetch the first dimension to be the value in main content Table 1.\nSeasonal variations. We evaluate seasonal changes by analyzing the variations in Fourier frequencies across all input instances. Given the inputs, $X \\in \\mathbb{R}^{N_{i} \\times L \\times D}$ where $N_{i}$ is the number of inputs. We first obtain the FFT results of all inputs, denoted as $Z \\in \\mathbb{C}^{N_{i} \\times L \\times D}$. Then, we calculate the variance across different inputs and normalize this variance by dividing by the mean of each input as:\n$\\text{Seasonal Variation} = \\frac{Var_{N_i}[Amp(Z)]}{Mean_i(X)}$ (7)\nwhere the subscripts indicate the dimension of the operation. We sum the results across all channels for the value in Table 1."}, {"title": "C Theoritical Analysis", "content": "This section discusses the effect of FAN on stationarity and temporal distribution in a theoretical perspective. We conclude that FAN enhances the stationarity of the input and mitigates distribution in the time domain."}, {"title": "C.1 Preliminary", "content": "Discrete Fourier Transform. Given a multivariate time series input X, we independently apply the 1-dim Fourier transform to each dimension x(i), hence, we illustrate in vector settings. For a discrete time series vector $x \\in \\mathbb{R}^L$ with L time steps, it is transformed into Fourier domain by applying the 1-dim DFT, and can be transformed back using 1-dim IDFT, which can be defined as:\n$DFT: z[w] = \\sum_{t=0}^{L-1} x[t] \\cdot e^{-j2\\pi \\frac{wt}{L}}$\n$IDFT : x[t] = \\frac{1}{T} \\sum_{w=0}^{T-1} z[w]\\cdot e^{j2\\pi \\frac{wt}{L}}$ (8)"}, {"title": "C.2 Variance Over Spectrum", "content": "Along with the time series spectral theory [35], a time series with smaller variance in the spectrum is more stationary, in this section, we try to prove the proposed FAN can reduce the variance over spectrum, thus enhance the stationarity of the input data. Hence, we prove that, given an univariate time series real value vector $x \\in \\mathbb{R}^T$, after removing main frequency components $z[k] \\in K$, the variance on spectrum can be reduced Var (ares) < Var (a).\nHere, the marginal distribution of the amplitude vector (the spectrum) a is represented as a joint Rayleigh distribution with different scale parameters:\n$f(a) = \\int f(a, p)dp = \\prod_{i=1}^{L} \\frac{a}{\\sigma_i^2} \\cdot exp(-\\frac{a^2}{2\\sigma_i^2})$ (12)\nNote that although we assume that the frequency components are independent with each other, this assumption is actually widely used [16] since it is quite possible that a specific component changes independently, e.g., the daily weekly changes while the monthly periodicity stays the same. Following the principle of additivity of variance for independent variables [13], the variance of the amplitude vector a can be expressed as follows:\n$Var (a) = \\sum_{i=1}^{L} \\frac{4 - \\pi}{2} \\sigma_i^2$ (13)\nafter removing frequencies $k \\in K$, the joint distribution actually becomes:\n$f(a)^{(K)} = \\prod_{i=1, i \\notin K}^{L} \\frac{a}{\\sigma_i^2} \\cdot exp(-\\frac{a^2}{2\\sigma_i^2})$ (14)"}, {"title": "C.3 Influence On Temporal Distribution", "content": "Relation with Temporal Statistics. The zero frequency of the Fourier transform divided by L is actually the mean of the statistical measure, and the energy of the Fourier transform of frequency components above zero is equivalent to the variance of the input scaled by L, proved as follow:\n$E[x] = \\frac{1}{L} \\sum_{t=0}^{L-1} x[t] = \\frac{1}{L}z[0]$ (16)\nAccording to Parseval theorem [5], for a discrete signal x, its energy is identical in both the time and frequency domain:\n$\\sum_{t=0}^{L-1}|x[t]|^2 = \\sum_{w=0}^{L-1}|z[w]|^2 = LE[x^2]$ (17)\nThus the variance of the input signal can be defined as the energy of Fourier components with frequency above zero.\n$Var[x] = E[x^2] - E^2[x] = \\frac{1}{L}\\sum_{w=0}^{L-1}|z[w]|^2 - |\\frac{1}{L}z[0]|^2$ (18)\n$= \\frac{1}{L} \\sum_{w=1}^{L-1} |z[w]|^2$ (19)\nInfluence On Mean. Since the mean is equal to the zero frequency component in time domain and for any other components, the expectation is zero since they are all number of periodic sin/cos signals, after removing the zero frequency component, the expectation is then equal to zero. Due to this property, this is also known as the 'detrending' in traditional signal processing [32], proved as:\n$E[x^{res}] = E[x - IDFT(z[0])] = E[x - \\frac{1}{L}z[0]] = E[x \u2013 E[x]] = 0$ (20)\nInfluence On Variance. Since the The normalization step select and remove top K amplitude Fourier components, the Fourier spectrum energy will be significantly diminished, defined as:\n$\\sum_{w=0, w\\notin K}^{L-1} |z[w]|^2 << \\sum_{w=0}^{L-1}|z[w]|^2$ (21)\nthus, the input variance then can be largely reduced, which is:\n$Var[x^{res}] << Var[x]$ (22)\nIn summary, our method can effectively reduce the range of data distribution, which is crucial for enhancing the performance of the backbone model and minimizing the risk of overfitting [12]."}, {"title": "C.4 Fourier Spectrum Empirical Analysis", "content": "The variance in the Fourier spectrum is an important indicator reflecting stationarity [20]. The closer the frequency components are to each other, the smaller the variance between the components, thus the stronger the stationarity [35]. Therefore, we compare the changes in frequency domain components for different methods and present the results in Fig. 10. In Fig. 10, after FAN's normalization step, the distribution exhibits alignment of the input and output, and the range of the distribution mean has decreased to 8, compared with previous methods which are round 80, 70, 70 respectively for SAN,"}, {"title": "D Ablation Study", "content": ""}, {"title": "D.1 Hyperparameter Analysis", "content": "Our model incorporates a hyperparameter $K$, which represents the maximum frequency count selection. In this section, we provide a sensitivity analysis for this parameter in Fig. 11. We observe that our proposed FAN achieves stable performance across various parameter settings."}, {"title": "D.2 Pattern Prediction Module", "content": "To justify our rather simple MLP structure for predicting the future main frequency component, we extended the basic MLP with three additional layers to observe the results. These layers include\n\u2022 +MLP: adding an additional MLP layer on top of the basic MLP.\n\u2022 +GRU [4]: adding a GRU layer, which is a recurrent neural network mitigate gradient vanishing problem through gating mechanism.\n\u2022 +TSMixer [7]: adding a TSMixer layer, which is a state-of-the-art lightweight model that also considers inter-dimensional relationships.\nWe perform the ablation on ETTh1, ExchangeRate, Weather datasets, under experiment settings of Section 4.2, we report the MAE/MSE evaluation metrics, and the result is shown at Table 6.\nIn Table 6, the three-layer MLP of FAN performed best overall on three datasets and more complex models tend to perform worse. We believe this is due to the following reasons: (1) The main frequency signal provides a baseline position for the backbone model, leading to more robust predictions and thus greater model robustness. (2) The main frequency signal is subject to underlying physical characteristics, resulting in relatively slower changes. This has been verified by observations in Appendix B.3, showing that the main frequency signal changes at least 40.27% more slowly compared to the residual frequency signal. Therefore, a simple three-layer MLP is sufficient to provide effective and somewhat more robust predictions. However, a four-layer MLP and GRU also achieved the best performance in some metrics, indicating that there is still room for improvement in future work."}, {"title": "E Full Results And Discussions", "content": ""}, {"title": "E.1 Experiment On Synthetic Data", "content": "To fully demonstrate the effectiveness of our method on signals with varying non-stationary fre- quencies, we generated a synthetic multidimensional time-series dataset using composite sinusoidal signals [32"}]}