{"title": "MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction", "authors": ["Jingyu Song", "Xudong Chen", "Liupei Lu", "Jie Li", "Katherine A. Skinner"], "abstract": "High-definition (HD) maps provide environmental information for autonomous driving systems and are essential for safe planning. While existing methods with single-frame input achieve impressive performance for online vectorized HD map construction, they still struggle with complex scenarios and occlusions. We propose MemFusionMap, a novel temporal fusion model with enhanced temporal reasoning capabilities for online HD map construction. Specifically, we contribute a working memory fusion module that improves the model's memory capacity to reason across history frames. We also design a novel temporal overlap heatmap to explicitly inform the model about the temporal overlap information and vehicle trajectory in the Bird's Eye View space. By integrating these two designs, MemFusionMap significantly outperforms existing methods while also maintaining a versatile design for scalability. We conduct extensive evaluation on open-source benchmarks and demonstrate a maximum improvement of 5.4% in mAP over state-of-the-art methods. The code for MemFusionMap will be made open-source upon publication of this paper.", "sections": [{"title": "1. Introduction", "content": "With recent advances in leveraging Bird's Eye View (BEV) representations, perception capabilities of Autonomous Vehicles (AVs) have been largely enhanced [18, 23, 27, 31, 41]. Among all the BEV perception tasks, constructing High-Definition (HD) maps in an online manner has received increasing attention from the AV research community [17, 19, 22, 39]. Specifically, online HD map construction aims to use onboard vehicle sensor data to reconstruct road elements such as lanes, pedestrian crossings and road boundaries on-the-fly. The reconstructed HD maps are essential for downstream planning tasks such as prediction and planning [13]. This task is of great significance in advancing large-scale deployment of AVs as it avoids the need for a sophisticated and labor-intensive offline HD map generation process [22].\nExisting works have demonstrated the effectiveness of formulating online HD map construction in an end-to-end framework [22, 24, 28, 39]. Though significant advances have been made in this field, many methods fall short of leveraging temporal information, which has been shown to be very beneficial, especially during instances of occlusion [4, 33, 39]. StreamMapNet [39] is the first end-to-end vectorized HD map construction work leveraging temporal cues. By maintaining a recurrent BEV memory feature map and propagating map queries, StreamMapNet conducts temporal fusion effectively and significantly outperforms previous non-temporal baselines [19, 22], demonstrating the importance of incorporating temporal cues for online HD map construction. The success of StreamMapNet [39] has inspired follow-up works to further improve it by incorporating auxiliary learning tasks or by integrating ideas from the tracking literature [4, 33].\nStill, we highlight the challenges of directly accumulating all the temporal features into a single memory feature map, as proposed in [39]. The network can struggle to reason about the entire history due to limited memory capacity in complex road environments since the network only has access to the latest memory feature and current BEV feature. Furthermore, this design can struggle with occlusion. A typical example is when a sudden occlusion occurs due to a moving truck near the ego-vehicle. This will trigger a bad update to the memory feature due to challenging 2D-to-BEV projection, and thus will affect all the future predictions. A similar issue is also noted in the video object segmentation field [7, 43]. We share the same insight that maintaining working memory features from a subset of historical frames is helpful, which is also pointed out by [4]. Lastly, we argue the importance of maintaining explicit temporal overlap information, as the model can leverage this input to reason about temporal information more effectively.\nGiven these insights, we propose MemFusionMap (Fig. 1), a novel memory fusion framework designed for online vectorized HD Map construction. We design a working memory buffer to maintain working memory features in a fixed-lag manner. The working memory features are propagated recursively to align with the current field of view of the ego-vehicle. The fixed-lag design ensures bounded memory usage to fit real-world deployment needs. In addition, we propose a novel use of a temporal overlap heatmap, represented as a BEV image with a single channel. The heatmap score indicates the number of times each grid cell falls within the field of view. We recurrently propagate the temporal overlap heatmap. To facilitate the model's temporal reasoning capabilities, the temporal overlap heatmap is fused with the working memory features by a convolutional temporal fusion block. The output of the temporal fusion block is fed into a standard detector head to generate road element predictions.\nTo summarize, our main contributions are as follows:\n\u2022 We propose a simple yet effective model to fuse working memory features in BEV space for online vectorized HD map construction. MemFusionMap focuses on improving the network's temporal reasoning capability while also maintaining a versatile design for scalability and compatibility.\n\u2022 We propose a novel design of maintaining a temporal overlap heatmap, providing a strong cue for the model to reason across a history of frames and also implicitly encodes valuable insights of the vehicle's trajectory.\n\u2022 We conduct extensive evaluation on nuScenes [1] and Argoverse2 [35] to demonstrate the effectiveness of MemFusionMap. The proposed method significantly outperforms the state-of-the-art method [39], achieving a maximum improvement of 5.4% in mAP."}, {"title": "2. Related Works", "content": "Constructing HD maps is one of the key tasks for AV perception. While traditional SLAM-based methods [30, 40] suffer from high cost during offline construction and maintenance, online HD map construction solutions have obtained increasing attention as they can effectively simplify the map construction process and handle map changes. HDMapNet [17] marks a significant step of moving towards vectorized HD map representations, which are preferred by the downstream tasks over the rasterized map representations that are generated from previous segmentation methods [18, 27, 42] or lane detection methods [5, 12, 32]. VectorMapNet [22] proposes the first end-to-end vectorized HD map construction framework where a DETR [2] based map decoder is used to generate the map elements' prediction with an auto-regressive transformer. Its performance is further improved by MapTR [19] and Map-TRV2 [20] by treating the map construction task as a point set prediction problem and adding auxiliary tasks and decoder design improvement. Other works focus on different map element representations: BeMapNet [28] and PivotNet [10] explore using B\u00e9zier curve and pivot-based points, respectively. Additionally, other methods investigate network design improvement and training pipeline enhancements [3, 21, 24, 38]. More recently, StreamMapNet [39] and its follow-up works [4, 33] demonstrate notable improvement by leveraging temporal fusion for this task, which will be discussed in Sec. 2.2."}, {"title": "2.1. Online Vectorized HD Map Construction", "content": "Constructing HD maps is one of the key tasks for AV perception. While traditional SLAM-based methods [30, 40] suffer from high cost during offline construction and maintenance, online HD map construction solutions have obtained increasing attention as they can effectively simplify the map construction process and handle map changes. HDMapNet [17] marks a significant step of moving towards vectorized HD map representations, which are preferred by the downstream tasks over the rasterized map representations that are generated from previous segmentation methods [18, 27, 42] or lane detection methods [5, 12, 32]. VectorMapNet [22] proposes the first end-to-end vectorized HD map construction framework where a DETR [2] based map decoder is used to generate the map elements' prediction with an auto-regressive transformer. Its performance is further improved by MapTR [19] and Map-TRV2 [20] by treating the map construction task as a point set prediction problem and adding auxiliary tasks and decoder design improvement. Other works focus on different map element representations: BeMapNet [28] and PivotNet [10] explore using B\u00e9zier curve and pivot-based points, respectively. Additionally, other methods investigate network design improvement and training pipeline enhancements [3, 21, 24, 38]. More recently, StreamMapNet [39] and its follow-up works [4, 33] demonstrate notable improvement by leveraging temporal fusion for this task, which will be discussed in Sec. 2.2."}, {"title": "2.2. Temporal Fusion for HD Map Construction", "content": "Fusing temporal information has proven effective in various works in domains like 3D object detection and scene completion [14, 18, 26, 34, 36]. HDMapNet [17] is the first to demonstrate the benefits of temporal fusion in online HD map construction. It directly applies max pooling to compress temporal information into a BEV feature map, which is fed into the decoder. StreamMapNet [39] develops the first temporal fusion pipeline in an end-to-end framework for this task. In addition to propagating and reusing queries in the decoder, it proposes a BEV temporal fusion paradigm leveraging the streaming strategy following [14, 34]. StreamMapNet [39] compresses the history of information into a latent memory feature and propagates the latent memory feature recurrently. It applies a Gated Recurrent Unit (GRU) [8] to fuse the latent memory feature with the current BEV feature from the BEV feature encoder.\nThe success of StreamMapNet has motivated subsequent works such SQD-MapNet [33] and MapTracker [4]. Inspired by DN-DETR [16], SQD-MapNet [33] designs a stream query denoising approach to further improve over StreamMapNet. However, SQD-MapNet is only evaluated on the original train/val splits that are prone to overfitting, contradicting the main motivation of online HD map construction. MapTracker [4] explores leveraging additional ground truth of tracked road elements to enhance the query propagation paradigm. MemFusionMap shares the same intuition as MapTracker [4] that the redundancy of maintaining a subset of past frames can improve the model's robustness. However, we design MemFusionMap with a strong emphasis on scalability and versatility for practical application. Notably, while MapTracker fuses the last 20 frames of memory latents selected according to the distance strides, our proposed metohd, MemFusionMap, designs a simple yet effective memory fusion module, which only uses 4 past frames to avoid computational overhead and to achieve better training efficiency and runtime. MemFusionMap also covers the perception range of 100 \u00d7 50 m, which has more practical value for real-world AV deployment, while MapTracker [4] omits this extended perception range. Furthermore, SQD-MapNet [33] and MapTracker [4] require implementing additional task and loss modules, which can cause extra overhead when integrating into existing multi-task AV perception pipelines. MapTracker also requires additional post-processing to generate the tracks of map elements, which can be time-consuming and potentially incompatible with large-scale databases that use an auto labeling process that cannot guarantee quality assured labels for all frames. In contrast, MemFusionMap maintains a compact design with better versatility that can be directly applied to existing pipelines by switching the temporal fusion module in BEV space. Lastly, MemFusionMap also proposes a novel method for maintaining and integrating a temporal overlap heatmap, which is demonstrated to significantly improve the model's temporal reasoning capability."}, {"title": "3. MemFusionMap", "content": "The overall framework of MemFusionMap is shown in Fig. 2. The pipeline of MemFusionMap is inspired by StreamMapNet [39]. However, MemFusionMap proposes a novel structure for the temporal BEV fusion module. MemFusionMap firstly uses a shared ResNet-50 [15] image encoder to extract image features. A BEV feature encoder further projects the image features in 2D to BEV space. The projected BEV feature is denoted as $F_{BEV} \\in R^{C\\times H \\times W}$, where C is the BEV feature dimension, and H and W represent the spatial dimensions of the BEV feature. The BEV feature is fed into the memory fusion block, which will be introduced in the following sections. The output of the memory fusion block is a unified BEV feature map containing temporal cues, denoted as $\\hat{F}_{BEV} \\in R^{C\\times H \\times W}$. Following [39], we use a DETR [2] based transformer decoder with a multi-point attention mechanism for extended perception range. We also maintain a memory buffer to properly propagate and reuse history map queries.\nThe following sub-sections discuss the key contributions of the proposed architecture in more detail. The core contribution of MemFusionMap is its temporal BEV fusion design, which consists of a temporal overlap heatmap (Sec. 3.1) and the working memory fusion module (Sec. 3.2)."}, {"title": "3.1. Temporal Overlap Heatmap", "content": "We propose a novel paradigm to maintain a temporal overlap heatmap to explicitly inform the model of the temporal overlap information. Fig. 3 illustrates the process of maintaining the heatmap. Starting from the first frame of a each clip, we initialize the temporal overlap heatmap $H_{to}$ as follows:\n$H_{to} = 1^{1\\times H \\times W}$ (1)\nSpecifically, $H_{to}$ has the same spatial size as $F_{BEV}$ and is single-channel. Each pixel of H indicates the temporal overlap score of this cell in the corresponding BEV space. $H_{to}$ is initialized with all 1s, meaning the whole BEV has not been seen before. Then, as the second frame (t1) comes in, we first propagate the temporal overlap heatmap to the new vehicle position as follows:\n$H_{t_1} = Warp(H_{to}, T_{t_o}^{t_1}),$ (2)\nwhere $H_{t_1}$ is the propagated and warped overlap heatmap from to to t1 according to the egomotion $T_{t_o}^{t_1}$. The warping is implemented using the grid sample function in PyTorch with zero padding mode for the out-of-bound grids. We then obtain the final heatmap for t1 as:\n$H_{t_1} = H_{t_1} + 1^{1\\times H \\times W}$. (3)\nFollowing this strategy, we keep updating the temporal overlap heatmap recurrently. At each timestamp, the overlap heatmap is fed into the memory fusion block, which will be discussed in Sec. 3.2.\nOne main benefit of maintaining this temporal overlap heatmap is that it provides informative insights into the regions with heavy overlaps and those without. With this information, the model can more adaptively reason across the current BEV feature and the historical features. Specifically, in the overlap heatmap Ht, a smaller value indicates that a region is less temporally overlapped, meaning more information from the current BEV feature should be trusted. Conversely, a larger value suggests a region with greater overlap, implying that information from the memory latents could be more valuable. Furthermore, the overlap heatmap naturally encodes the vehicle trajectory, providing the model with additional knowledge about vehicle motion to enable more adaptive temporal reasoning.\nWe demonstrate this benefit with two visual examples. As shown in Fig. 4, we present two example overlap heatmaps resulting from different speeds. The heatmap on the right exhibits a larger area with a value of 1, indicating that a larger area is newly seen, which is a result of higher-speed ego-motion. We present another example of a vehicle turning in Fig. 5. The propagated overlap heatmap reveals two noticeable edges at the corner. These low-level feature patterns can be easily detected by a CNN, providing critical insights into the vehicle's trajectory."}, {"title": "3.2. Memory Fusion Block", "content": "We design a memory fusion block to synthesize memory features with the temporal overlap heatmap. As discussed in Sec. 1, we highlight the importance of increasing the memory capacity for better temporal reasoning capability and robustness against occlusion. We aggregate a fixed number of BEV features from historical frames to fuse with the current BEV feature. We follow [7] to use the terminology used in cognitive science [9, 11, 37] and refer to the subset of historical frames as working memory features. As shown in Fig. 2, we develop a working memory buffer to properly handle storage and loading of working memory features. We also design a memory fusion block to temporally reason across working memory features, aided by the temporal overlap heatmap."}, {"title": "3.2.1 Working Memory Buffer", "content": "We design a working memory buffer to manage the memory features and update them recurrently. In the streaming training schedule, at time t, before memory fusion, the working memory feature $F^{WM}_t$ is defined as follows:\n$F^{WM}_t = \\{F^{BEV}_{t-T_{WM}}, F^{BEV}_{t-T_{WM}+1},..., F^{BEV}_{t-1}, F^{BEV}_{t}\\},$ (4)\nwhere $F^{BEV}_t$ is the unified BEV feature after memory fusion at time t, and $T_{WM}$ is the working memory capacity. Progressing towards time t +1, we first add the new feature and drop the oldest feature:\n$F^{WM}_{t+1} = \\{F^{BEV}_{t-T_{WM}+1}, F^{BEV}_{t-T_{WM}+2},..., F^{BEV}_{t}, F^{BEV}_{t+1}\\}.$ (5)\nWe also warp all the working memory features to align with $F^{BEV}_{t+1}$.\n$F^{WM}_{t+1} = Warp(F^{WM}_t, T_{t}^{t+1}),$ (6)\nwhere $T_{t}^{t+1}$ is the transformation from t to t + 1. Additionally, at t0, the beginning of a sequence, we initialize $F^{WM}_{t}$ by repeating $F^{BEV}_{t}$ for $T_{WM}$ times. After the memory fusion at to, we replace $F^{BEV}_{t}$ with $\\hat{F}^{BEV}_{t}$ and warp them to form $F^{WM}_{t}$. The following process of updating working memory follows the strategy mentioned above. Meanwhile, the working memory buffer also serves to store the temporal overlap heatmap. The propagation process for the heatmap is discussed in Sec. 3.1."}, {"title": "3.2.2 Working Memory Fusion", "content": "We propose a working memory fusion module to obtain a unified BEV feature $\\hat{F}^{BEV}$ fused with temporal cues. The goal of the memory fusion block is to effectively utilize the temporal information in working memory features, aided by the temporal overlap heatmap, and to fuse them with the current BEV feature to obtain a unified BEV feature with rich temporal information.\nSpecifically, at time t, the input to this module consists of the working memory features $F^{WM}_t \\in R^{C_{WM}\\times H \\times W}$, the temporal overlap heatmap $H_t \\in R^{1\\times H \\times W}$, and the BEV feature $F^{BEV}_{t} \\in R^{C \\times H \\times W}$ obtained from the BEV feature encoder. Here C is the feature channel dimension of the BEV feature encoder, $C_{WM} = T_{WM} \\times C$, and H and W represent the spatial size of the BEV feature. The output of the working memory fusion module is the unified feature $\\hat{F}^{BEV}_{t} \\in R^{C\\times H \\times W}$. In the working memory fusion module, the first step is to extract low-level features from the temporal overlap heatmap:\n$\\hat{H}_t = sigmoid(Conv_{H}(H_{t})) \\in R^{C_{H}\\times H \\times W},$ (7)\nwhere $Conv_H$ is a convolution block composed of 3 convolution layers with ReLU, and $C_H$ is the feature dimension of the learned temporal overlap heatmap feature. We apply a sigmoid function on the output temporal overlap feature to bound its value. Subsequently, we use a convolutional memory fusion block to obtain the unified BEV feature $\\hat{F}^{BEV}_{t}$ following:\n$\\hat{F}^{BEV}_t = LayerNorm(Conv_{Mem}(Concat(F^{WM}_t, \\hat{H}_t, F^{BEV}_{t}))),$ (8)\nwhere $Conv_{Mem}$ is composed of 3 convolution layers with ReLU. We follow [39] to apply layer normalization to improve training stability.\nWhen designing the working memory fusion module, we take the HD map construction task in mind. We observe that a unique feature for this task is that map elements, such as lane markings, can have elongated shapes, which inspires us to enhance the receptive field of the memory fusion module to let temporal features interact more effectively both temporally and spatially. Therefore, we apply dilated (atrous) convolution [6] in $Conv_{Mem}$ to further improve the working memory fusion module."}, {"title": "3.3. Training Loss", "content": "During training, the overall map loss $L_{map}$ is defined as in [39]:\n$L_{map} = \\lambda_1 L_{Focal} + \\lambda_2 L_{line} + \\lambda_3 L_{trans},$ (9)\nwhere $L_{Focal}$ is the classification matching cost, $L_{line}$ is the polyline-wise matching cost, $L_{trans}$ is the auxiliary transformation loss for temporal query propagation, and $\\lambda_{1-3}$ are pre-defined loss weights. We refer to [39] for more details about the loss function."}, {"title": "4. Experiments", "content": "We evaluate our methods on the open-source nuScenes [1] and Argoverse2 [35] benchmarks. The nuScenes benchmark has an annotation keyframe rate of 2Hz, with 6 cameras synchronized. Argoverse2 is annotated with 10Hz with 7 ring cameras and 2 stereo cameras. We follow [39] to unify the frame rate of Argoverse2 dataset to 2Hz, and use all 6 cameras of nuScenes and the 7 ring cameras of Argoverse2 as the input to the model.\nAs noted by [29, 39], the default splits in both nuScenes and Argoverse2 can be prone to overfitting as there is a large portion of overlap between training and validation sets. We highlight that the ultimate goal of the online HD map construction task is to develop a generalizable model that can adapt to unseen environments with robustness to potential map changes, instead of simply memorizing the training set. Therefore, we focus on evaluating MemFusionMap using the split proposed in [29] for nuScenes and the split proposed in [39] for Argoverse2. We refer to these non-overlapped splits as new splits and the default splits as original splits in the following sub-sections."}, {"title": "4.1. Datasets and Splits", "content": "We evaluate our methods on the open-source nuScenes [1] and Argoverse2 [35] benchmarks. The nuScenes benchmark has an annotation keyframe rate of 2Hz, with 6 cameras synchronized. Argoverse2 is annotated with 10Hz with 7 ring cameras and 2 stereo cameras. We follow [39] to unify the frame rate of Argoverse2 dataset to 2Hz, and use all 6 cameras of nuScenes and the 7 ring cameras of Argoverse2 as the input to the model.\nAs noted by [29, 39], the default splits in both nuScenes and Argoverse2 can be prone to overfitting as there is a large portion of overlap between training and validation sets. We highlight that the ultimate goal of the online HD map construction task is to develop a generalizable model that can adapt to unseen environments with robustness to potential map changes, instead of simply memorizing the training set. Therefore, we focus on evaluating MemFusionMap using the split proposed in [29] for nuScenes and the split proposed in [39] for Argoverse2. We refer to these non-overlapped splits as new splits and the default splits as original splits in the following sub-sections."}, {"title": "4.2. Evaluation Metrics", "content": "We follow [33, 39] to consider both a small perceptual range (60 \u00d7 30m) and a large perceptual range (100 \u00d7 50m). We mainly use the geofenced new splits proposed by [29, 39]. We use Average Precision (AP) as the evaluation metric. We calculate AP using the distance threshold of {0.5m, 1.0m, 1.5m} for the small perceptual range and {1.0m, 1.5m, 2.0m} for the large perceptual range. Following common practices in existing works, we evaluate three types of map elements: pedestrian crossings, lane dividers, and road boundaries. We calculate the mean AP (mAP) score of the three types of map elements."}, {"title": "4.3. Implementation Details", "content": "We build MemFusionMap upon the codebase of StreamMapNet [39]. We use ResNet-50 [15] as the image feature encoder. We use BEVFormer [18] as the BEV feature encoder to project features from 2D to BEV space. C is set as 256 across all experiments. We set H as 50 and W as 100. In the memory fusion module, we set $T_{wM}$ as 4 and $C_H$ as 32. The model is trained on 8 NVIDIA V100 GPUs with a total batch size of 32. The learning rate is set to 5 \u00d7 10-4 and an AdamW [25] optimizer is used. To train MemFusionMap, we apply the streaming training strategy [14, 34] to detach the memory tensor so gradients do not propagate back to previous frames. Following [26, 39], we apply a two-stage training procedure, where the input of the first stage is single-frame. It is followed by a temporal training stage, where the input is consecutive sequences obtained from dividing the raw training sequence into 2 random sequences. We include more implementation details in the supplementary material."}, {"title": "4.4. Quantitative Comparison", "content": "We compare MemFusionMap against state-of-the-art (SOTA) baselines across the nuScenes [1] and Argoverse2 [35] datasets with two perceptual ranges using the new splits [29, 39]. The baselines in the comprehensive comparison include StreamMapNet [39] and single-frame baselines, including VectorMapNet [22] and MapTR [19]. As to the follow-up works of StreamMapNet (i.e., SQD-MapNet [33] and MapTracker [4]), we conduct separate comparisons. This is due to the lack of support for either the extended 100 \u00d7 50 m perception range in [4], which holds greater practical value for AV deployment, or the absence of open-source implementation and results evaluated using the new splits in [33]. We report the number of epochs to demonstrate the training efficiency and measure the runtime on a desktop equipped with an NVIDIA A6000 GPU.\nWe first show an overall comparison on the new split [29] of the nuScenes [1] benchmark. At both ranges, MemFusionMap achieves significant improvement over baselines across all metrics, demonstrating the effectiveness of the proposed memory fusion architecture. Similarly, as shown in Tab. 2, MemFusionMap demonstrates superior performance over other methods across all categories on the Argoverse2 benchmark, validating the consistent improvement brought by MemFusionMap. We also highlight the greater improvement in the long perception range setting compared to the short range setting on both benchmarks, which is more applicable to real-world deployment. As MemFusionMap is built upon StreamMapNet and shares a similar training objective, we emphasize the advancement brought by MemFusionMap over StreamMapNet, which directly shows the improvement brought by the proposed working memory fusion framework over the previous GRU-based temporal fusion paradigm.\nFurthermore, we show a comparison with open-source temporal baselines on mAP and runtime in Tab. 3. Though MemFusionMap significantly outperforms StreamMapNet [39] while maintaining a similarly fast runtime, it falls behind the recent SOTA work MapTracker [4] in mAP. We recognize the contribution of MapTracker of formulating the HD map construction task as a tracking problem, and think merging MemFusionMap and MapTracker is an interesting future direction. However, it is worth mentioning that MapTracker is benefited from additional ground truth of tracked map elements and requires a time-consuming post-processing process. This may have scalability and compatibility issues with large-scale databases that use an auto labeling process that cannot guarantee all frames are quality assured, which could render a direct comparison on mAP inappropriate. Additionally, as shown in Tab. 3, MemFusionMap demonstrates strong performance without requiring a long training schedule, which is very valuable for integrating to large-scale databases. Compared with MapTracker [4], MemFusionMap also improves runtime significantly. Considering these factors, we still think MemFusionMap has unique benefits for the community.\nWe also show additional comparison on the original split (Tab. 4) for completeness since SQD-MapNet [33] does not report results on the new split and is not open-source. The evaluation is based on the 60 \u00d7 30 m range since SQD-MapNet uses a larger BEV spatial dimension for the extended range. Results in Tab. 4 demonstrate that MemFusionMap still outperforms the baselines, though we note that this split is prone to overfitting."}, {"title": "4.5. Ablation Studies", "content": "We conduct extensive ablation studies to further break down the performance improvement brought by MemFusionMap and justify our design choice. We conduct all of the ablation studies using the new split of nuScenes at 100 \u00d7 50 m range. We include some ablation studies in the supplementary material.\nWe show an ablation study in Tab. 5. We first remove the GRU-based temporal BEV fusion from StreamMapNet. We notice a notable performance drop, which shows the importance of temporal fusion for this task. We then test adding a convolutional working memory fusion module. It is observed that a simple convolution block can effectively improve temporal reasoning capability with the help of increased memory capacity by maintaining working memory features. The comparison between (a) and (c) also demonstrates the challenge of accumulating the entire history into a single memory latent. The improvement from (c) to (d) validates the importance of using dilated convolution for larger receptive field. Progressing from (d) to (e), the 1.7% improvement on an already strong model justifies the significance of our novel design of the temporal overlap heatmap.\nFurthermore, we study the effect of varying the working memory capacity $T_{WM}$. As shown in Tab. 6, we find that $T_{WM}$ = 4 is a sweet spot that delivers the best performance while also ensuring good runtime performance. We conjecture that the performance drop from further increasing $T_{WM}$ may be due to the unnecessary dilution of the current BEV feature when fusing with additional temporal information. Additionally, we validate the design choice of dilation parameters of the convolution layers in the proposed working memory fusion module. Empirical results in Tab. 7 demonstrate that maintaining a symmetric dilation of (2, 2) brings the most improvement."}, {"title": "4.6. Qualitative Results", "content": "We present qualitative comparison of MemFusionMap and StreamMapNet on nuScenes [1] at the extended 100 \u00d7 50 m range, which is the most challenging configuration. We present several examples of MemFusionMap outperforming baseline model StreamMapNet [39] in Fig. 6. Thanks to the proposed working memory fusion module with the temporal overlap heatmap, MemFusionMap predicts more accurate map elements with better geometry and fewer missed elements. We show more qualitative examples in the supplementary material."}, {"title": "5. Conclusion", "content": "In this paper, we have proposed MemFusionMap, a novel approach for effective online vectorized HD map construction with enhanced temporal reasoning capability. We propose a working memory fusion module to improve the memory capacity of the model. We also design a temporal overlap heatmap to aid the temporal reasoning process across history frames. Our proposed MemFusionMap demonstrates state-of-the-art performance on open-source benchmarks with the geofenced splits and maintains a fast runtime and versatile design. We hope MemFusionMap will inspire future research works to leverage our proposed memory fusion module in other BEV perception tasks. We think integrating MemFusionMap in the MapTracker [4] framework to leverage the tracking paradigm to further enhance the performance and temporal consistency is also worth exploring."}, {"title": "6. Potential Negative Societal Impact", "content": "Although MemFusionMap achieves significant improvements over existing models, it is important to note that it may still generate false predictions, particularly in complex scenarios and at long perception ranges, which could introduce safety concerns for autonomous driving systems. Additionally, while we specifically focus on geographically non-overlapping splits to prevent MemFusionMap from merely memorizing the training data and to ensure it generalizes to unseen environments, the model may still retain knowledge of the HD map data. This retention could raise further privacy and security issues."}, {"title": "A. Implementation Details", "content": "In this section, we provide additional details of the MemFusionMap framework implementation. We recognize the importance of improving reproducibility in AI research. We plan to release code in the future, pending internal clearance and approval for code release."}, {"title": "A.1. Training", "content": "We report additional details of training MemFusionMap. During training, we use the overall map loss Lmap as defined in [39]:\n$L_{map} = \\lambda_1 L_{Focal} + \\lambda_2 L_{line} + \\lambda_3 L_{trans},$ (10)\nwhere LFocal is the classification matching cost, Lline is the polyline-wise matching cost, Ltrans is the auxiliary transformation loss for temporal query propagation, and $\\lambda_{1-3}$ are pre-defined loss weights. We set $\\lambda_1$ as 5.0, $\\lambda_2$ as 50.0, and $\\lambda_3$ as 0.1 for all of the experiment instances. In addition, as mentioned in the main paper, we are applying a two-stage training schedule following [26, 39]. To ensure fair comparison with the main baseline model (i.e. StreamMapNet [39]), we follow the same schedule as described in the open-source repository of StreamMapNet. Specifically, as to the training sessions on nuScenes [1] and Argoverse2 [35], the first single-frame training stage lasts 4 and 5 epochs, respectively. The second stage is a temporal training stage, where the input is consecutive sequences obtained from dividing the raw training sequence into two random sequences."}, {"title": "A.2. Working Memory Fusion", "content": "We provide further details about the implementation of the working memory fusion module. As mentioned in the main paper, we employ two convolution blocks to extract the temporal overlap heatmap feature (ConvH) and conduct working memory fusion to obtain the unified feature (ConvMem).\nAs shown in Fig. 4 and Fig. 5 in the main paper, the temporal overlap heatmap includes low-level feature patterns that encode informative insights on vehicle trajectories. We apply ConvH to extract the low level features from the heatmap Ht:\n$\\hat{H}_t = sigmoid(Conv_{H}(H_{t})) \\in R^{C_{H}\\times H \\times W}.$ (11)"}, {"title": "B. Supplementary Experiments", "content": "We provide additional experiments to further demonstrate the superior performance of MemFusionMap and justify our design choice."}, {"title": "B.1. Faster Convergence", "content": "When designing MemFusionMap, we prioritize scalability with the goal of integrating it into autonomous driving systems that utilize large-scale databases. As we expand to databases much larger than existing open-source benchmarks [1"}]}