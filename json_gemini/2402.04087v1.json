{"title": "A HARD-TO-BEAT BASELINE FOR TRAINING-FREE CLIP-BASED ADAPTATION", "authors": ["Zhengbo Wang", "Jian Liang", "Lijun Sheng", "Ran He", "Zilei Wang", "Tieniu Tan"], "abstract": "Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.", "sections": [{"title": "INTRODUCTION", "content": "Contrastive Language-Image Pretraining, known as CLIP (Radford et al., 2021), has attracted considerable attention in recent years as a powerful method for aligning vision and language representations. By leveraging a massive dataset of 400 million web-scale image-text pairs, CLIP learns to encode images and text into a shared semantic space using vision and language encoders, respectively. This shared semantic space facilitates the comparison of similarities and differences between images and texts. One remarkable feature of CLIP is its zero-shot ability to perform image classification without any additional training on the target classes. This is accomplished by using the language encoder to generate classifier weights based on a simple prompt, such as \u201ca photo of a {class}\". By inputting different class names into the prompt, we can generate the corresponding weights for classification, thereby enabling the classification of images into a broad range of categories without training.\nDespite its powerful zero-shot capability, recent works (Zhou et al., 2022b; Zhang et al., 2022; Zhou et al., 2022a; Lu et al., 2022; Gao et al., 2024) have focused on designing efficient fine-tuning methods for downstream classification tasks. These methods have achieved significant improvements compared to Zero-Shot CLIP (Radford et al., 2021), even when trained on few-shot datasets and optimizing extremely small numbers of parameters. For instance, CoOp (Zhou et al., 2022b) proposes to learn a set of global text prompts for the pre-trained CLIP (Radford et al., 2021) on downstream tasks, which achieves a 15% improvement compared to Zero-Shot CLIP (Radford et al., 2021) with only 16 samples per class by fine-tuning a mere 16k parameters. While these methods are efficient\""}, {"title": "RELATED WORK", "content": "Vision-Language Models. In recent years, vision-language models (VLMs) have become a new paradigm for foundational models that aim to bridge the gap between the modalities of vision and language. These models are trained on large-scale image-text datasets, which endows them with powerful transferable abilities such as zero-shot learning, few-shot adaptation, and in-context learning (Radford et al., 2021; Kim et al., 2021; Lu et al., 2019; Su et al., 2020; Jia et al., 2021). Moreover, they exhibit strong open-world capabilities and have been successfully applied to recognize open-world concepts, including zero-shot learning (Radford et al., 2021; Jia et al., 2021), open-world segmentation (Mengde et al., 2022; Ding et al., 2022), and open-world detection (Joseph et al., 2021; Gu et al., 2022; Gupta et al., 2022). Contrastive-based vision-language pre-training has become the mainstream approach in this field. These methods, including CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), are trained on large-scale web-based noisy image-text pairs. They employ a language encoder and a vision encoder to encode the texts and images, respectively, and learn to align their representations through contrastive loss. We utilize CLIP (Radford et al., 2021) in this work.\nEfficient Fine-tuning for VLMs. Recent works (Zhou et al., 2022a;b; Zhang et al., 2022; Gao et al., 2024; Lu et al., 2022; Chen et al., 2023; Guo et al., 2023; Udandarao et al., 2023; Huang et al., 2022; Wang et al., 2024; 2023b) focus on developing efficient fine-tuning methods for large pre-trained vision-language models that can be used in downstream tasks due to their large model size. These methods aim to achieve the maximum performance gain by fine-tuning the minimum number of model parameters on few-shot downstream datasets. For instance, CoOp (Zhou et al., 2022b) proposes to learn global text prompts for downstream tasks through back-propagation on few-shot datasets."}, {"title": "METHOD", "content": null}, {"title": "Gaussian Discriminant Analysis for CLIP Adaptation", "content": "Gaussian Discriminant Analysis. We revisit a traditional probabilistic generative model (Bishop, 2006), Gaussian Discriminant Analysis (GDA), for training-free CLIP-based adaptation, whose classifier can be derived by making an assumption about the data distribution of each class. The parameters of the classifier can be obtained from the statistical information of the data without the need for explicit training. By applying Bayes' formula, the classification probability can be expressed as the function of the data distribution and its prior probability:\n$$p(y = i|x) = \\frac{p(x/y = i)p(y = i)}{\\sum_{j=1}^{K}P(x|y = j)p(y = j)} = \\frac{exp(f_i(x))}{\\sum_{j=1}^{K} exp(f_j(x))},$$ (1)\nwhere i = 1, 2, . . ., K for K-class classification tasks, $x \\in R^D$ is the visual feature, and we normalize Eq. (1) using the softmax function. And the logit function is $f_i(x) = log(p(x|y = i)p(y = i)), i = 1,2,..., K$. Therefore, by assuming the data distribution of each class and their prior distribution, we can obtain the classifier. In GDA (Bishop, 2006), the features are typically assumed to follow the Gaussian distributions with identical covariance, i.e., $(X|Y = i) \\sim N(\\mu_i, \\Sigma)$ for i = 1, 2, .., K. We substitute this assumption into Eq. (1), which then can be expressed as follows:\n$$p(y = i|x) = \\frac{exp(-\\frac{1}{2}x - \\mu_i^T\\Sigma^{-1}(x - \\mu_i) + log p_i)}{\\sum_{j=1}^{K} exp(-\\frac{1}{2}x - \\mu_j^T\\Sigma^{-1}(x - \\mu_j) + log p_j)}$$ (2)\nwhere $p_i = p(y = i) = 1/K, i = 1, 2, ..., K$ is the prior probability of the corresponding class, which is assumed to be uniform. And thus, the logit $f_i(x) = \\mu_i^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + log p_i$. Thus, the weight $W \\in R^{K\\times D}$ and the bias $b \\in R^{K}$ for the classifier are as follows:\n$$W_i = \\Sigma^{-1}\\mu_i, b_i = log p_i - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i,$$ (3)\nfor i = 1, 2, ..., K. Later, we estimate the mean for each class and the precision matrix using the training data and subsequently obtain the corresponding weight and bias for the linear classifier."}, {"title": "Parameter Estimation", "content": "We estimate the mean vectors using the empirical mean $\\mu_i = \\frac{\\sum_{j=1}^{N} x_jI_{(y_i=k)}}{\\sum_{j=1}^{N} I_{(y_i=k)}}$. However, in high-dimensional spaces, estimating the precision matrix is a challenging task due to the inverse of the empirical covariance matrix being a biased estimator of the precision matrix, and it may be impossible to invert due to numerical issues. To solve this, we utilize shrinkage methods to estimate the precision matrix. To avoid introducing additional computations, we use the empirical Bayes ridge-type estimator (Kubokawa & Srivastava, 2008) to estimate the precision matrix:\n$$\\Sigma^{-1} = D((\\mathcal{N} - 1)\\Sigma + tr(\\Sigma)I_D)^{-1},$$ (4)\nwhere $N$ is the number of samples, $D$ is the dimension of the features, and $\\Sigma$ represents the empirical covariance. Once the parameter estimation is completed, we can input it into Eq. (3) to obtain the weight and bias of the classifier.\nBesides the knowledge extracted from visual modality, the prior knowledge of text modality in pre-trained CLIP is calculated by $x_{test} W_c^T$, where $W_c$ is the weights of CLIP's classifier generated from the text encoder by inputting a predefined prompt, such as \u201ca photo of a {class}\". For simplicity, we integrate the knowledge from visual and text modalities by mixing the predictions. Therefore, the output logits of the test image are then calculated as:\n$$logits = x_{test}W_c^T + \\alpha(x_{test}W_t^T + b),$$ (5)\nwhere $x_{test}$ is the visual feature of test image, and $\\alpha$ is a hyper-parameter.\""}, {"title": "EXTENSION TO OTHER SCENARIOS", "content": "We further extend our method to base-to-new generalization and unsupervised learning, where our method cannot directly apply, to illustrate the generalization of our method. In order to maintain the simplicity of the method and avoid introducing additional complexities that could impact the assessment of our approach, we only perform straightforward modifications in these two scenarios.\nExtension to Base-to-new Generalization. For CLIP base-to-new generalization, the model is trained on the base dataset and tested on a new dataset with unseen classes. However, our method cannot be directly implemented in the scenario where data for the new classes is unavailable. Based on the observation that similar samples have similar statistical information (Yang et al., 2021), we propose that our method can be extended to new classes using the KNN algorithm. To achieve"}, {"title": null, "content": "this, we utilize text embeddings of the new classes to query the training set and select the k nearest neighbors as the synthesized labeled data. The process is defined as follows:\n$$D_{new} = \\bigcup_{i=K+1}^{M} \\{(x,i)|x \\in NN_k(t_i, D)\\}$$ (6)\nwhere i = K + 1, ..., M denotes the new classes, $t_i$ denotes its text embedding, and $NN_k(*,D)$ denotes the k-nearest neighbors of training set D. The classifier is then produced utilizing Eq. (3).\nExtension to Unsupervised Learning. In the unsupervised learning scenario, we only have the unlabeled data $\\{x_i\\}_{i=1}^{N}$. Based on the Gaussian assumption in GDA, the unsupervised data $\\{x_i\\}_{i=1}^{N}$ follow Gaussian mixture distribution. In order to maintain the simplicity of our method, we directly employ the EM algorithm for estimating the means and covariance matrix. To begin, we initialize the mean vectors and covariance using the zero-shot classifier, assuming equal priors for each Gaussian distribution. In the E-step, we calculate the probability of the unlabeled data $\\{x_i\\}_{i=1}^{N}$ as follows:\n$$\\gamma_{ik} = \\frac{exp(f_k(x))}{\\sum_{i=1}^{N} exp (f_i(x))},$$ (7)\nfor the unlabeled data $\\{x_i\\}_{i=1}^{N}$, and $f$ is the logit function using Eq. (5). Moving on to the M-step, we update the mean vectors and covariance matrix using the following formulas:\n$$\\mu_k = \\frac{\\sum_{i=1}^{N} \\gamma_{ik}x_i}{\\sum_{i=1}^{N} \\gamma_{ik}}, \\Sigma = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^{N} \\gamma_{ik}}$$(8)\nSubsequently, we update the classifier using Eq. (3) and repeat the EM process until convergence."}, {"title": "EXPERIMENTS", "content": null}, {"title": "SETUP", "content": "Dataset. According to previous works (Radford et al., 2021; Zhou et al., 2022a;b; Zhang et al., 2022), we select 11 publicly available image classification datasets to assess the effectiveness of CLIP few-shot classification, base-to-new generalization, and unsupervised learning. These datasets cover a range of image recognition tasks, including generic object recognition with ImageNet (Deng et al., 2009) and Caltech101 (Li et al., 2004), fine-grained image recognition with OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers 102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014) and FGVCAircraft (Maji et al., 2013), satellite image classification with EuroSAT (Helber et al., 2019), action classification with UCF101 (Soomro et al., 2012), texture classification with DTD (Cimpoi et al., 2014), and scene recognition with SUN397 (Xiao et al., 2010). Additionally, we also select 4 datasets, ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-R (Hendrycks et al., 2021a), to evaluate the out-of-distribution generalization. Moreover, we adopt 2 commonly used imbalanced datasets, ImageNet-LT (Liu et al., 2019) and Places-LT (Zhou et al., 2017), for CLIP long-tailed classification.\nTraining Details. To align with previous works (Zhou et al., 2022b;a; Zhang et al., 2022), we utilize ResNet-50 (He et al., 2016) as the visual encoder of CLIP for few-shot classification by default. Similarly, following the previous work (Wang et al., 2024), we choose ResNet-101 as the visual encoder of CLIP for imbalanced learning. To evaluate the model's base-to-new generalization and out-of-distribution generalization performance, we followed CoCoOp (Zhou et al., 2022a) and adopted ViT-B/16-based CLIP (Radford et al., 2021). We follow CLIP (Radford et al., 2021) to adopt prompt ensembling on ImageNet and use a single Zero-Shot CLIP on the other 10 datasets. The hyperparameter a, which is used to ensemble the classifiers, is searched in the validation set with values ranging from 0.0001 to 100.0, and this value is kept constant for new class data. And the k for the KNN algorithm to synthesize the new class dataset is set to 64. All experiments are conducted on a single NVIDIA GeForce RTX 3090. To obtain a reliable estimate of model performance, we conduct three runs with different random seeds and averaged the results.\nEvaluation Protocol. For the few-shot classification, we adhere to the evaluation protocol proposed by CLIP (Radford et al., 2021). Specifically, we randomly select 1, 2, 4, 8, or 16 instances per"}, {"title": "RESULTS ON FEW-SHOT CLASSIFICATION", "content": null}, {"title": "OUT-OF-DISTRIBUTION GENERALIZATION", "content": "We further conduct experiments to assess our method on out-of-distribution generalization. Specifically, we train our model using the 16-shot setting on ImageNet (Deng et al., 2009). Subsequently, we transfer the model directly to target datasets, which included ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-R (Hendrycks et al., 2021a).\nAs presented in Table 2, we choose CLIP, CoOp, Tip-Adapter, and Tip-Adapter-F for comparison. And these methods are based on ViT-B/16-based CLIP. Without requiring any training, our method achieves the highest results on average over the four target datasets. These results indicate that our model is more advantageous in dealing with out-of-distribution generalization and reduces the risk of overfitting on the source dataset."}, {"title": "RESULTS ON IMBALANCED LEARNING", "content": "We compare our method with several imbalanced learning baselines: Zero-Shot CLIP, linear probe, full fine-tuning, and CLIP integrated with specific imbalanced algorithms, namely: MARC (Wang et al., 2023a), CRT (Kang et al., 2020), Balanced Softmax (Ren et al., 2020), and their variants with Decoder (Wang et al., 2024) on ImageNet-LT (Liu et al., 2019) and Places-LT (Zhou et al., 2017) dataset.\nThe results are shown in Table 3. We report the results in terms of overall accuracy, many-shot accuracy, medium-shot accuracy, and few-shot accuracy, as well as the F1 score. Specifically, we achieve improvements against Zero-Shot CLIP of 8.72% and 9.90% for ImageNet-LT and Places-LT datasets on overall accuracy, respectively. It is worth noting that our method surpasses previous training-required methods, even those trained using imbalanced algorithms. Our method primarily enhances CLIP's performance in terms of medium-shot and few-shot accuracy. This improvement can be attributed to the identical covariance assumption in GDA, which transfers the knowledge of feature distribution from many-shot classes to medium- and few-shot classes."}, {"title": "RESULTS ON BASE-TO-NEW GENERALIZATION", "content": "Table 4 presents the results on base-to-new generalization, which show that our approach outperforms the other methods in terms of base accuracy, new accuracy, and their harmonic mean. On average across 11 datasets, our method surpasses CLIP, CoOp, CoCoOp, and KgCoOp by 0.31%, 11.31%, 2.84%, and 0.93% in terms of new accuracy, and by 7.02%, 7.06%, 2.89%, and 1.72% in terms of the harmonic mean. Detailed results on each dataset can be referred to Figure 8 in the Appendix."}, {"title": "RESULTS ON UNSUPERVISED LEARNING", "content": "In unsupervised learning, the estimation of mean vectors and covariance matrices in GDA is performed by directly applying the EM algorithm for Gaussian Mixture Model (GMM). The results are shown in Table 5. It is noteworthy that this straightforward approach significantly enhances the performance of CLIP in downstream tasks when utilizing unlabeled data. Furthermore, our method consistently outperforms the Zero-Shot CLIP by an average margin of 4.69% across all 11 datasets. Moreover, when compared to the three baseline methods, our approach achieves the highest results on 7 out of the 11 datasets. These results clearly indicate the effectiveness of our method."}, {"title": "ABLATION STUDY", "content": null}, {"title": "Effectiveness of Precision Matrix Estimation", "content": "The estimation of the precision matrix is challenging due to the limited data and bias problem. To address this, we employ the empirical Bayes ridge-type estimator (KS) in the paper, which is specifically designed for scenarios where the sample size is smaller than the dimension. We compare it with other robust precision estimation techniques, including Moore-Penrose, the estimator in Efron and Morris (EM), GraphicalLasso, LedoitWolf, and OAS. As shown in Table 6, the empirical Bayes ridge-type estimator achieves the best results, which shows its effectiveness."}, {"title": "Effectiveness of Increased Sample Size", "content": "We further train our method with more training data. Figure 3 illustrates the results of training our model on ImageNet, using 1, 2, 4, 8, 16, 32, and 64 shots per class. The x-axis is presented on a logarithmic scale. We observe that the model performance increases with an increase in the number of data, and it exhibits a linear relationship with the logarithm of the number of data. This indicates that our approach is not restricted to few-shot learning, but instead has the ability to improve consistently with an increase in the number of samples."}, {"title": "Effectiveness of Ensemble Classifier", "content": "We evaluate the effectiveness of the ensemble of linear classifiers, as presented in Eq. (5). Figure 4 shows the performance of the CLIP zero-shot classifier, our linear classifier, and the ensemble classifier in few-shot classification on average on the 11 datasets. We observe that directly using the linear classifier sometimes produces worse results than using the zero-shot classifier. This can be attributed to inaccuracies in the estimated precision matrix, leading to a poor classifier. However, when the classifiers are ensembled according to Eq. (5), the ensemble classifier outperforms both individual classifiers in all settings, demonstrating its effectiveness."}, {"title": "Comparison to Fully-trained Methods", "content": "In Table 7, we compare efficient fine-tuning methods, Tip-Adapter, Tip-Adapter-F, and our proposed method, with conventional fully trained methods such as ResNet (He et al., 2016) and DeiT (Touvron et al., 2021). We adopt ViT-L/14 CLIP for efficient fine-tuning methods. Although Tip-Adapter and Tip-Adapter-F achieve comparable performance to conventional methods (He et al., 2016; Touvron et al., 2021), they fail to train on full set as they need to cache all the training data, which leads to OOM error. In contrast, our proposed method does not have this problem since we only store the classifier parameters. Therefore, our approach can perform well not only on few-shot but also on the full training set. Furthermore, without requiring any training, our approach achieves the highest performance compared to both efficient fine-tuning methods and conventional training methods."}, {"title": "CONCLUSION", "content": "In this paper, we revisit Gaussian Discriminant Analysis (GDA) with CLIP as a hard-to-beat training-free adaptation method. Without any training, we can directly obtain the classifier from the mean vectors and covariance of the training dataset. We conduct extensive experiments of our method on CLIP few-shot classification and imbalanced learning, and its two simple variants on base-to-new generalization and unsupervised learning. Our method achieves state-of-the-art results against previous training-free methods and is comparable to or even better than training-required methods. These results demonstrate the effectiveness of our method. In the future, we will explore the application of our method in dense prediction tasks and other scenarios such as test-time adaptation."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "In this paper, we provide a comprehensive overview of the datasets, training procedures, and evaluation settings, which are thoroughly discussed in Section 4.1. Detailed statistics for the datasets, prompt templates, and pseudocode can be found in Appendix C. To ensure the reproducibility of our method, we have also made the source code and scripts available in the supplementary materials."}, {"title": "DETAILS OF THE METHOD", "content": null}, {"title": "COMPUTATION OF EQUATION (2)", "content": "Theorem A.1. Assuming that the features of different classes follow the Gaussian distribution with identical covariance, i.e., (X|Y = i) ~ N(\u03bc\u2081, \u03a3) for i = 1, 2, .., K. Then, the classification probability can be expressed as follows:\n$$p(y = i|x) = \\frac{exp(\\mu_i^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + log p_i)}{\\sum_{j=1}^{K} exp(\\mu_j^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + log p_j)}$$ (9)\nProof. Since (X|Y = i) ~ N(\u03bc\u2081, \u03a3) for i = 1, 2, .., K, the probability of class i is:\n$$p(x|y = i) = \\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}}}exp(-\\frac{1}{2}(x - \\mu_i)^T\\Sigma^{-1}(x - \\mu_i)),$$ (10)\nwhere d is the feature dimension. Later, the classification probability can be derived by using the Bayesian formula,"}, {"title": null, "content": "p(x|y = i)p(y = i)\n$$\np(y = i|x) = \\frac{p(x|y = i)p(y = i)}{\\sum_{j=1}^{K}p(x|y = j)p(y = j)}$$ (Bayesian formula)\n$$\n= \\frac{\\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}}}exp(-\\frac{1}{2}(x - \\mu_i)^T\\Sigma^{-1}(x - \\mu_i))p(y = i)}{\\sum_{j=1}^{K}\\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}}}exp(-\\frac{1}{2}(x - \\mu_j)^T\\Sigma^{-1}(x - \\mu_j))p(y = j)}$$ (Using Equation 10)\n$$\n=\\frac{exp(-\\frac{1}{2}x^T\\Sigma^{-1}x + \\mu_i^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i-\\frac{1}{2}x^T\\Sigma^{-1} \\mu_i)p(y = i)}{\\sum_{j=1}^{K}exp(-\\frac{1}{2}x^T\\Sigma^{-1}x + \\mu_j^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j-\\frac{1}{2}x^T\\Sigma^{-1} \\mu_j)p(y = j)}$$\n$$\n=\\frac{exp(\\mu_i^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + log p_i)}{\\sum_{j=1}^{K} exp(\\mu_j^T\\Sigma^{-1}x - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + log p_j)}$$ (denoted $p_i = p(y = i)$)\n (11)"}, {"title": "MORE EXPERIMENTAL ANALYSIS", "content": null}, {"title": "BASE-TO-NEW GENERALIZATION", "content": "Results. Our method can be extended to the base-to-new generalization scenario by incorporating the KNN algorithm. To accomplish this, we utilize the text embeddings of the new classes to query the training set and select the k nearest neighbors as the training data for the new class. Subsequently, we apply our proposed method to generate the classifier for the new classes using the synthesized dataset. In order to compare our approach, we select CLIP (Radford et al., 2021), CoOp (Zhou et al., 2022b), CoCoOp (Zhou et al., 2022a), and KgCoOp (Yao et al., 2023)."}, {"title": "ROBUSTNESS TO DIFFERENT ARCHITECTURES", "content": "We further evaluate the efficacy of our proposed method across 11 datasets with varying visual architectures of CLIP. We selected two approaches for comparison: a training-required method, CoOp (Zhou et al., 2022b), and a training-free method, Tip-Adapter (Zhang et al., 2022). And these methods are trained on the 16-shot dataset. As shown in Table 9, our method yielded a substantial improvement of 17.28%, 18.20%, 16.18%, and 16.62% on average, compared to the Zero-Shot CLIP (Radford et al., 2021) approach, for ResNet-50, ResNet-101, ViT-B/32, and ViT-B/16 CLIP, respectively, across all 11 datasets. The results demonstrate the effectiveness of our method across different CLIP architectures."}, {"title": "ABLATION OF THE HYPER-PARAMETER a", "content": "As shown in Equation (5), our method needs a hyper-parameter a to integrate the knowledge from visual and text modalities. Specifically, we only performed a coarse search for a within [0.001, 0.01, 0.1, 1, 10, 100]. The search only ascertained the order of magnitude for alpha, providing a foundational understanding of its impact. The optimal alpha values resulting from this exploration"}, {"title": "EXPERIMENTAL DETAILS", "content": null}, {"title": "STATISTIC OF DATASETS", "content": "Following previous work (Zhou et al., 2022a;b; Wang et al., 2023b; Huang et al., 2022; Wang et al., 2024), we conduct experiments on 17 publicly available image classification datasets. The datasets include ImageNet (Deng et al., 2009), Caltech101 (Li et al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), EuroSAT (Helber et al., 2019), UCF101 (Soomro et al., 2012), DTD (Cimpoi et al., 2014), SUN397 (Xiao et al., 2010), ImageNetV2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), ImageNet-LT (Liu et al., 2019), and Places-LT (Zhou et al., 2017)."}, {"title": "PROMPT TEMPLATES FOR EACH DATASET", "content": "For the zero-shot classifier, we employ handcrafted prompts to generate the classifier weight, as proposed in CLIP (Radford et al., 2021). By default, we utilize the prompt template \"a photo of {class}.\" for class labels, where {class} represents the name of the classes. However, for fine-grained classification datasets such as FGVCAircraft (Maji et al., 2013), we incorporate the name of the superclass or a description into the template. The prompt templates for each dataset are shown as follows."}]}