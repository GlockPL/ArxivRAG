{"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration", "authors": ["Minjie Hong", "Yan Xia", "Zehan Wang", "Jieming Zhu", "Ye Wang", "Sihang Cai", "Xiaoda Yang", "Quanyu Dai", "Zhenhua Dong", "Zhimeng Zhang", "Zhou Zhao"], "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm-based recommender systems (RSs) often face challenges due to the significant differences between the linguistic semantics of pre-trained LLMs and the collaborative semantics essential for RSs. These systems use pre-trained linguistic semantics but learn collaborative semantics from scratch via the Ilm-Backbone. However, LLMs are not designed for recommendations, leading to inefficient collaborative learning, weak result correlations, and poor integration of traditional RS features. To address these challenges, we propose EAGER-LLM, a decoder-only Ilm-based generative recommendation framework that integrates endogenous and exogenous behavioral and semantic information in a non-intrusive manner. Specifically, we propose 1) dual-source knowledge-rich item indices that integrates indexing sequences for exogenous signals, enabling efficient link-wide processing; 2) non-invasive multiscale alignment reconstruction tasks guide the model toward a deeper understanding of both collaborative and semantic signals; 3) an annealing adapter designed to finely balance the model's recommendation performance with its comprehension capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing on three public benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recommender systems are tools designed to alleviate the phenomenon of information overload in Web environments by algorithmically analyzing user behavior to predict and push content that may be of interest to users. Typical recommender systems [11, 17, 20, 34] encode users and items as latent representations within a shared space to capture semantic similarities, followed by efficient retrieval using Approximate Nearest Neighbors (ANNs) algorithms [8, 16]. The distinct separation of these two phases often introduces performance limitations due to the absence of end-to-end optimization.\nResearch across domains like vision [1, 23], speech [4, 15], and multimodality [42] demonstrates the broader applicability of LLMs. Several recent studies investigate the potential roles of LLMs in recommender systems (RSs). Unlike traditional models that encode users and items as embedding vectors, some LLM-based RSs [2, 5, 13, 21] converts user behaviors and preferences, alongside the candidate item set, into discrete natural language text sequences or prompts. These prompts are then used to extract item-related information from the LLM's textual outputs. [7, 14, 24, 43] enhance collaboration by incorporating additional or existing tokens into the LLM to represent user and item IDs, which are then fine-tuned during specialized training to fit interaction data. [35, 38] employs exogenous collaboration models to obtain collaboration embeddings, which are integrated into the inputs of the LLM, thereby enriching the recommendation process.\nBut these paradigms suffer from several flaws: (1) While the plain text approach can yield favorable outcomes in zero-shot recommendation [5, 13], it primarily analyzes only the surface-level textual semantics of behavioral sequences. This method heavily relies on candidate sets and incurs significant computational overhead when modeling extensive historical sequences. (2) In real-world applications, where the number of candidate recommendation items vastly exceeds the vocabulary size of LLMs, the tokenization redundancy introduced by Vanilla IDs complicates LLMs' ability to accurately interpret commands. This redundancy results in low learning efficiency and a failure to effectively leverage semantic features. (3) The substantial disparity between the domains of external collaborative signals and the semantic signals of pre-trained LLMs means that directly integrating these signals can significantly disrupt the original functionalities of the LLMs. Consequently, the model struggles to effectively process and interpret the information contained in these external signals [35, 38].\nTo overcome the challenges outlined, we introduce EAGER-LLM, a novel decoder-only LLM-based generative recommendation framework. In this framework, we compress massive exogenous signals into a few newly added tokens with extremely high compression ratios. Additionally, we incorporate a non-invasive multi-scale alignment reconstruction tasks and Multi-Stage Training that facilitates an efficient understanding and integration of exogenous behaviors, semantic signals, and recommendation data knowledge with the LLM's original parameters. Our approach is detailed through three aspects:\nPrimarily, we introduce Dual-source Knowledge-rich Item Indices to address the inefficiencies of previous approaches that used atomic tokens to represent item IDs, which resulted in tokenization redundancy and overly discrete and independent semantics that did not effectively support the recommendation task. Our method efficiently characterizes large candidate sets with a small number of identifiers, incorporating a useful priori knowledge with a high compression ratio to integrate exogenous semantic and behavioral information into the decoding inference process. We implement an indexing structure where semantically similar items share identifier prefixes. Given the distinct domain differences between behavioral and semantic feature spaces, prior research in multimodal and bimodal models [3] has shown that even advanced encoder-side feature fusion approaches like Q-former [19] are insufficient for effective integration of dual-source features. Consequently, we discretize and separately splice the exogenous behavioral and semantic signals. This decoupled indexing scheme minimizes information loss from encoder-side feature fusion and enables the model to more effectively represent the complex interplay between behavior and semantics during subsequent training.\nFurthermore, we have introduced Non-Invasive Multiscale Alignment Reconstruction Tasks. Given the vast amount of exogenous semantic and behavioral signals compressed into a small number of tokens at a very high compression ratio, it is challenging for the model to directly assimilate adequate exogenous knowledge. To address this, we have devised the Global Contrast Decompression Task and Comprehensive Interaction Modeling Tasks. These initiatives aid the model in decompressing extensive exogenous knowledge from a limited number of highly compressed tokens. By incorporating additional summarization tokens and leveraging the restricted context of recommendation data, these tasks effectively minimize the domain gap between natural language and collaborative semantics, enhancing the efficiency of the recommendation process. In addition, we introduced a multi-stage training scheme centered on the Annealing Adapter, which flexibly balances recommendation accuracy and model text inference capability. The contributions of this paper can be summarized as follows:\n\u2022 We present EAGER-LLM, an innovative decoder-only LLM-based generative recommendation framework that synergistically integrates endogenous and exogenous behavioral and semantic information\n\u2022 We propose Dual-source Knowledge-rich Item Indices, Multiscale Alignment Reconstruction Tasks and the Annealing Adapter that non-intrusively guide the model towards a deep understanding of collaborative and semantic signals.\n\u2022 Extensive experiments across three public recommendation benchmarks demonstrate the superiority of EAGER-LLM over existing methods, emphasizing its effectiveness and robustness."}, {"title": "2 Related work", "content": "The use of deep sequential models for understanding user-item interactions in recommender systems has significantly evolved, with various approaches making notable contributions. GRU4REC [11] introduced the use of GRU-based RNNs to model sequential user behaviors effectively. SASRec [17] implemented self-attention mechanisms akin to those found in decoder-only transformer models to enhance recommendation accuracy. Drawing inspiration from the success of masked language modeling in NLP, BERT4Rec [30] applied transformers with masking techniques specifically tailored for sequential recommendation tasks. Additionally, TIGER [28] has started emphasizing the use of semantic IDs. In this approach, each item is represented by a series of tokens that reflect its related details, and the system predicts the sequence of upcoming item tokens using a seq2seq method. Additionally EAGER [36] advances the investigation by implementing a dual-stream generation architecture that incorporates both semantic and behavioral information. In this work, we extend EAGER [36] to EAGER-LLM by bridging LLMs and recommenders with dual-source knowledge-rich item indices and non-invasive multiscale alignment reconstruction, which not only enhances recommendation accuracy but also retains conversation and explanation generation abilities of LLMs. Recently, P5 [7, 14] fine-tunes a pre-trained LLMs for multi-task recommender systems. In this study, we endeavor to further investigate a paradigm designed to mitigate the substantial discrepancies between LLMs in recommendation tasks and their original training tasks by integrating exogenous semantic and behavioral information."}, {"title": "2.2 LLMs as Recommenders", "content": "Recently, LLMs have been utilized in recommendation tasks due to their ability to understand, generate, and infer natural language properties. LLM-based RSs [24] constructs user/item correlations through its powerful high-quality textual representations and extensive external knowledge, and is expected to solve the problems of poor generalization [22] and poor performance of traditional RSs on sparse historical interaction data, etc. Chat-Rec [6] aims to enhance conversational recommendation systems by integrating ChatGPT's interactive capabilities with established recommendation models, such as MF [18] and LightGCN [10]. P5 [7] fine-tunes a pre-trained large language model for multi-task recommender systems, utilizing the LLM tokenizer (SentencePiece tokenizer) to generate tokens from randomly assigned item pseudo-IDs. M6 [5] explores the use of item text information (such as names) as identifiers for items. LC-Rec [39] designs a learning-based vector quantization method to generate ID from Item's semantic representation and proposes alignment tuning tasks to enhance the intergration of collaborative semantics in LLMs. Recently, new research has emerged to bridge the significant gap between pre-trained language models and recommendation tasks. CoLLM [38] infuses behavior information into LLMs by incorporating representations from an external collaborative model into the input. In this work, we aim to further explore recommender frameworks that can integrate endogenous and exogenous behavioral and semantic signals based on LLM."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Sequential recommendation is a crucial metric in LLM-based recommender systems (RSs). We transform the traditional two-tower model, which computes similarity followed by reordering, into a generative recommendation paradigm. In this framework, each item x is represented by a set of tokens Y = [y1, y2, \u2026, yk] \u2208 \u0423. As illustrated in 3, given an input sequence X, which includes instructions and the interaction history, the sequence of the target item Y is generated directly in an autoregressive manner. The probability can be calculated by:\n$$p(Y|X) = \\prod_{i=1}^{k}p(y_i| X, Y_1, Y_2, ..., Y_{i-1})$$"}, {"title": "3.2 Dual-source Knowledge-rich Item Indices", "content": "Some existing LLM-based methods utilize bracket notations as newly-introduced atomic tokens to represent items. However, this method can be problematic in data-rich real-world scenarios, where the number of potential recommended items greatly exceeds the vocabulary of LLMs. This leads to tokenization redundancy, making it challenging for LLMs to process commands accurately. Moreover, the description-based approach [2], which assigns tokens to index items based on the semantics of item titles or descriptions, introduces a strong inductive bias. This can obscure the true intent of user behaviors, as it does not model behavioral sequences clearly and unbiasedly, compromising the model's ability to understand and predict user preferences effectively.\nAdditionally, existing methods often overlook the value of exogenous prior knowledge. Furthermore, our experiments show that significant results are achieved when effectively integrating exogenous behavioral and semantic signals, showing subtle interaction, understanding, and cooperation.\nTo address our objectives, we aim to: 1) introduce a small number of tokens to efficiently represent a vast set of candidates; 2) infuse useful a priori knowledge into identifiers to incorporate exogenous semantic and behavioral information about items into the reasoning process; and 3) design an indexing structure where semantically similar items share identifier prefixes. To achieve these goals, we utilize a discretized indexing algorithm that encodes dual-source information for item representation. As illustrated in 2, for any given item along with its descriptive text (title, synopsis, etc.), semantic embeddings are derived using pre-trained language models (e.g., T5 [27], Llama [32]). In this study, we specifically employ the LLM-Backbone itself for semantic extraction:\nhiddenstate_t = llm(x_t,hiddenstate_{t-1}), t = 1, 2, 3, ..., n\n$$\\overline z = \\frac {1}{n} \\sum_{i=1}^n hiddenstate_i$$\nThe descriptive text x1:n of item i is entered sequentially into the last hidden state transformed by the LLM and averaged as the semantic representation zs of the item. Behavioral features zb are extracted by the encoder of a two-tower model (e.g., DIN [40]) that uses only ID sequences as recommendations:\nz^b = BehaviorEncoder(i)\nGiven the domain disparities between behavioral and semantic feature spaces, prior research has shown that even advanced encoder-side feature fusion techniques (e.g., Q-former [19]) often result in significant compression losses and fail to effectively integrate dual-source features. This places supernumerary strain on the decoding process. Consequently, we opt to separately and discretely process the exogenous semantic and behavioral signals. While vector quantization is commonly used for discretization, it proves unstable for training, leading to issues like item ID conflicts. To facilitate reproducibility in our study, we employ hierarchical K-Means to discretize the semantic embedding Zs and behavioral embedding Zb, where each cluster is progressively subdivided into"}, {"title": "3.3 Non-Invasive Multiscale Alignment Reconstruction Tasks", "content": "After incorporating additional tokens to represent items, we achieve a high compression ratio (\u2248 2,000,000:1), which significantly condenses massive exogenous signals into a very small number of tokens. This extreme compression ratio presents a challenge for the model to independently learn substantial, useful knowledge. Drawing inspiration from [36, 39], we have devised a series of multi-scale alignment reconstruction tasks."}, {"title": "3.3.1 Global Contrast Decompression Task", "content": "A method that non-intrusively enhances the model's ability to quickly and easily interpret knowledge-rich indices at extreme compression rates. This is achieved by incorporating additional summarization token and trainable Decompression Guidance Projectors.\nseq = {x_{Prompts}, x_u,..., x_u, Y, Y_{CON}}\nWhere XPrompts denotes the sequence of text prompts {x1,..., xm} and X represents the indexed tokens for the i-th item, reflecting the user's chronological behavior sequence. As outlined in 3.2, y denotes the j-th level of the predicted item tokens, where t = b corresponds to the item's behavioral token, and t = s to the semantic token. The summary token Y[CON] is strategically placed at the end to encapsulate the global knowledge of the preceding sequence.\nTo efficiently transmit exogenous dual-source signals into the preordered tokens through gradient updating, we introduce non-intrusive Decompression Guidance Projectors ft . This projector transforms the global hidden state distilled by Y[CON] into semantically and behaviorally-guided latent states. Additionally, we employ a contrastive learning paradigm that utilizes original exogenous semantic embbeddings Zs and behavioral embbeddings Zb to accelerate and assist the decompression process of hyper-compressed Tokens.\nhiddenstate_t = llm(x_t, hiddenstate_{t-1}), t = 1, 2, 3, ..., \u03b7\n$$L_{con} = F(f_t (hiddenstate_{[CON]}), Z), t \\in {b,s}$$\nThe total contrastive loss Lcon, is calculated by proportionally summing Loon and Loon. The function F(\u00b7, \u00b7) serves as the metric for contrastive learning. Importantly, the Decompression Guidance Projectors ft are utilized only during training, not in inference."}, {"title": "3.3.2 Comprehensive Interaction Modeling Task", "content": "To effectively harness the inference capabilities, pre-training knowledge, and trainable parameters of the LLM-Backbone for fitting recommendation data, we have restructured the traditional sequence recommendation task and its auxiliary tasks into a Next Token Prediction task, which LLMs are good at. Unlike using additional selectors as suggested by [43], we contend that this could alter the model's output form and output domain distribution, potentially compromising the original capabilities of the LLM-Backbone and diminishing the framework's generalizability across different backbones.\nAs illustrated in 3, Comprehensive Interaction Modeling is segmented into three subtasks: Sequence Recommendation Task, Semantic Reconstruction Task and Preference Understanding Task. These tasks effectively leverage the model's own parameters to integrate exogenous signals, recommendation data knowledge, and the model's intrinsic reasoning capabilities organically."}, {"title": "3.4 Initial training, Annealing Adapter Tuning and Inference", "content": ""}, {"title": "3.4.1 Initial training", "content": "In the initialization phase of enhancing the model's recommendation capabilities, we devised various conditional language modeling objectives. This strategy encourages highly divergent models, compared to pre-recommendation pre-train tasks, to cultivate in-depth generalization, understanding, and reasoning abilities pertinent for recommendation tasks.\nThe initial training can be formulated as follows:\nmax \u03a3 \u03a3 log (P+\u03c6, (Yt | x, y<t))\n\u03a6\n(x,y) \u2208 Z t=1\nx represents the \"Instruction Input\". y denotes the \"Instruction Output\" within the initial training data. yt stands for the t-th token of y. I corresponds to the original parameters of the LLM-Backbone. \u03c6r represents the additional parameters in Sequence Recommendation Task (SRT), and Z refers to the training set. We combine the generation and exogenous Semantic, Behavioral Reconstruction Loss to train our model, given by:\nL = Lgen + ISRT(\u03bb1Lcon + \u03bb2 Loon)\nWhere ISRT is an indicator function that is 1 if the task is SRT and 0 otherwise. \u03bb\u2081 and 12 are loss coefficients."}, {"title": "3.4.2 Annealing Adapter Tuning", "content": "we observed that annealing with restricted quantities of high-grade sequence recommendation data considerable improves the performance of the LLM-Backbone on pivotal benchmarks subsequent to the initial training of recommendation capabilities.\nAchieving the optimal solution for enhancing sequence recommendation performance remains a formidable challenge without adjusting the data volume ratio across various tasks. Integrating tasks such as sequence recommendation, preference understanding, and semantic reconstruction, while neglecting to bridge the gap between natural language processing and sequential behavior, complicates the optimization of sequence recommendation performance without modifying the proportion of data volume allocated to different tasks.\nConversely, training on a limited set of high-grade sequence recommendation tasks during the Annealing Training phase can also impair the model's original capabilities due to the significant disparity between the language semantics modeled by LLMs and the collaborative semantics implicit in recommender systems. Therefore, the use of an Adapter to introduce additional parameters in this phase, as shown in 4, constitutes an efficient and pragmatic approach to mitigate the adverse effects associated with Annealing Training.\nFormally,\nmax \u03a3 \u03a3log (P+\u03c6r+\u03c6\u03b1 (Yt | x, Y<t))\n\u03a6\u03b1\n(x,y) \u2208 Z t=1\nwhere qa denotes the parameters of Annealing Adapter."}, {"title": "3.4.3 Inference", "content": "It is noteworthy that the additional parameter , brought forth by the SRT, is disregarded during the inference stage. Moreover, employing the Annealing Adapter dynamically to meet varying task demands acts as a potent strategy for achieving a flexible balance between the model's textual reasoning abilities and recommendation accuracy."}, {"title": "4 EMPIRICAL STUDY", "content": "We analyze the proposed EAGER-LLM method on three datasets and demonstrate its effectiveness by answering the following research questions:\n\u2022 RQ1: How does EAGER-LLM compare to state-of-the-art sequential recommendation methods in different datasets?\n\u2022 RQ2: How do the components of EAGER-LLM (e.g., DKI, GCT, AAT) and hyper-parameter adjustments affect its performance?"}, {"title": "4.1 Experimental Setting", "content": ""}, {"title": "4.1.1 Dataset", "content": "We conducted experiments using three real-world public datasets of Amazon product reviews [9, 26], which are among the most widely utilized benchmarks for sequence recommendation. Specifically, the experiments focused on three subcategories: \"Beauty\", \"Sports and Outdoors\" and \"Musical Instruments\". In line with previous studies [12, 29, 37], we utilized the 5-core dataset approach, which excludes unpopular items and inactive users with fewer than five interaction records. The statistics for these datasets are presented in 1."}, {"title": "4.1.2 Evaluation Metrics", "content": "We utilize two widely recognized criteria for the matching phase: Recall and Normalized Discounted Cumulative Gain (NDCG). We present metrics calculated for the top 5/10 recommended candidates. In line with the standard evaluation protocol [17], we adopt the leave-one-out method for assessments. During training phases, we restrict the user's historical item count to 20. Additionally, for generative methods employing beam search, we consistently set the beam size to 20."}, {"title": "4.1.3 Implementation Details", "content": "We utilize Llama-7b [32] as LLM-Backbone. In constructing the item indexes, LLM-Backbone itself and DIN [40] as encoders. For training, our approach mirrors that of LC-Rec for ease of comparison, employing the AdamW optimizer with a learning rate set to 5e-5 and weight decay at 0.01. We implement data parallelism and gradient accumulation to achieve an overall batch size of 128. For GCT, we adopt InfoNCE to serve as the loss metric."}, {"title": "4.2 Performance Comparison (RQ1)", "content": ""}, {"title": "4.2.1 Baselines", "content": "We compare the following four categories of methods:\n(1) Traditional seqiential methods\n\u2022 GRU4REC [11]: An RNN-based sequential recommendation model that utilizes GRU model to encode the item sequence.\n\u2022 Caser [31]: a CNN-based approach that utilizes horizontal and vertical CNN layers to model the patterns in user behavior.\n\u2022 HGN [25]: employs hierarchical gating networks to effectively discern long-term and short-term user preferences.\n(2) For transformer-based methods, we have:\n\u2022 S^3-Rec [41]: enhances recommendation by pre-training bidirectional Transformer to maximize mutual information.\n\u2022 BERT4Rec [30]: Utilizes a bidirectional Transformer to overcome the constraints of unidirectional models.\n\u2022 FDSA [37]: models feature sequence transition patterns using a self-attention module.\n(3) For generative methods, we have:\n\u2022 TIGER [28]: employs T5 to generate IDs for items and uses an autoregressive decoding process to identify target candidates.\n\u2022 P5-CID [14]: leverages collaborative signals to construct ID identifiers for T5-based generative recommender model.\n(4) For LLM-Based methods, we have:\n\u2022 LC-Rec [39]: LC-Rec designs a vector quantization method to generate semantic IDs and use Llama as backbone to autoregressively decodes the identifiers of the target candidates items.\n\u2022 LETTER [35]: Integrates collaborative signals into LLM-Backbone through a series of regularizations."}, {"title": "4.2.2 Overall Performance", "content": "We provide a detailed report in 2 on the sequence recommendation performance of our method across three datasets, comparing it against various baseline models. The results lead to several key observations :\nTraditional baselines employ a simple inner-product matching approach, which segments the process and limits its ability to effectively model complex user interaction histories and intentions. Moreover, this approach's computational complexity grows exponentially with the candidate set, also restricting the representational space size. In contrast, EAGER-LLM aligns with the generative recommendation paradigm. It not only leverages pre-training knowledge to enhance recommendation-relevant capabilities, but it also reduces computational costs by directly generating the target item ID through beam search. This approach expands the limitations of latent space size in item representation, allowing it to incorporate significantly more exogenous information.\nGenerative recommendation, Ilm-based approaches (TIGER, LC-REC etc.) neglected the importance of exogenous behavioral signals for sequence recommendation. While the transformer architecture with generation loss works well in various domains, it is not designed for the task of sequence recommendation. These non-native approaches ignore the rank-order relationship of the candidates in the recommendation task, which leads to poor model performance on ranking-related metrics such as ndcg. Therefore, we believe that introducing additional behavioral signals is the key to improving the overall performance of model recommendation without changing the model architecture and training process.\nThere are also some approaches that attempt to incorporate exogenous behavioral signals into the recommendations (P5-CID, LETTER). LETTER, for instance, integrates collaborative signals into discrete coding through a series of regularizations. However LETTER does not have open source code and is only implemented as Llama2-7b [33], we evaluated our EAGER-LLM using the Llama2-7b on the Instruments dataset, as detailed in 3. Our method outperforms LETTER by over 8% across all metrics. We contend that even sophisticated encoder-side feature fusion methods can introduce additional compression loss, hindering the efficient integration of multi-source features. Therefore, allowing the LLM-Backbone itself to handle the fusion of information without introducing extra generalization bias at the input emerges as a simpler and more effective strategy."}, {"title": "4.3 Ablation Study (RQ2)", "content": "In the ablation experiments, the Sequence Recommendation Prediction Task was used as the core metric to evaluate the performance impact of each component. The main components of EAGER-LLM include Dual-source Knowledge-rich Item Indices (DKI), Global Contrast Decompression Task (GCT), and Annealing Adapter Tuning (AAT). The results are reported in 4, we can observe that:\n\u2022 Removing EAGER-LLM of the DKI, GCT, ATT (random index) achieves the worst results in different datasets, but still outperforms the vast majority of traditional baselines. This underscores the inherent superiority and robustness of our foundational framework in addressing the sequence recommendation task, and highlights significant potential for further development and enhancement in future work."}, {"title": "4.4 Further Analysis (RQ2)", "content": "As depicted in 5, we conducted performance experiments on the Beauty and Instruments datasets using various exogenous signal indexing methods: (1) Random, where each level of indices is randomly selected from candidates; (2) Semantic, utilizing indices derived solely from textual semantic signals; (3) Behavior, using indices generated solely from behavioral signals; and (4) Unit, combining indices from both Semantic and Behavior. The Unit index significantly exceeds the sum of the individual contributions from the two sources, yielding much higher results.\nTo our shock, in the Beauty dataset, the Semantic index performs worse than the Random index, likely due to a high compression ratio that complicates the model's ability to decode separate exogenous signals, especially after removing GCT and AAT. This often results in a diminished or even negative impact on recommendation performance. More intriguingly, the integration of exogenous Behavioral signals enables effective interaction between the dual information streams, enhancing their mutual comprehension and decoding. This synergy not only mitigates the negative impacts but also transforms them into substantial positive outcomes.\nIn 6, we demonstrate the impact of varying lengths of the Item Indices scheme on performance within the Beauty dataset. Four layers provide sufficient information for learning, while additional layers do not hinder ID generation but increase inference time."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce EAGER-LLM, a novel decoder-only, LLM-based generative recommendation framework that seamlessly integrates both endogenous and exogenous behavioral and semantic information non-intrusively. Extensive experiments validate the effectiveness and robustness of EAGER-LLM, showcasing superior performance compared to existing state-of-the-art methods."}]}