{"title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model", "authors": ["Yingxue Xu", "Yihui Wang", "Fengtao Zhou", "Jiabo Ma", "Shu Yang", "Huangjing Lin", "Xin Wang", "Jiguang Wang", "Li Liang", "Anjia Han", "Ronald Cheong Kin Chan", "Hao Chen"], "abstract": "Remarkable strides in computational pathology (CPath) have been made in the task-agnostic foundation model (FM) that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H&E diagnostic whole slide images (WSIs) along with their associated pathology reports and RNA-Seq data, which resulted in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we propose a novel whole-slide pretraining paradigm which injects multimodal knowledge at the whole-slide context into the pathology FM, called Multimodal Self-Taught PRetraining (mSTAR). The proposed paradigm revolutionizes the workflow of pretraining for CPath, which enables the pathology FM to acquire the whole-slide context. To the best of our knowledge, this is the first attempt to incorporate multimodal knowledge at the slide level for enhancing pathology FMs, expanding the modelling context from uni-modal to multimodal knowledge and from patch-level to slide-level. To systematically evaluate the capabilities of mSTAR, extensive experiments including slide-level unimodal and multimodal applications, are conducted across 7 diverse types of tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks. The average performance in varying types of slide-level applications consistently demonstrates significant performance enhancements for mSTAR compared to other state-of-the-art FMs, with statistically critical differences observed. In particular, mSTAR showcases the promising superiority in multimodal capabilities attained through the integration of multimodal knowledge.", "sections": [{"title": "1 Introduction", "content": "The recent advancements in foundation models (FMs) [2, 3, 4, 5] for computational pathology (CPath) have demonstrated considerable progress in an incredibly broad spectrum of clinical tasks, such as cancer diagnosis, treatment and prognosis. Despite encouraging performance in general-purpose pathology foundation models, there are still several unresolved challenges.\nFirst, massive multimodal data in line with clinical practices are under-utilized for pretraining, such as pathology reports and gene expression profiles. Existing pathology FMs either focus on vision-only [3] or vision-captions data [2, 4], in which captions are too short to offer sufficiently rich information although attempting to incorporate different modalities. The power of multimodal data has been repeatedly substantiated not only in the general machine learning community [6, 7] but also in the field of medical cancer research [8, 9, 10]. In the clinical workflow, as shown in Figure la, pathology reports often provide the complete and exhaustive information of whole slides, while patients' gene expression profiles offer insights of quantitative molecular dynamics that can complement the qualitative morphological view provided by a slide. The integration of these slide-level multimodal data can establish a broad and holistic perspective, thereby undoubtedly enhancing the capabilities of pathology FMs to perform various clinical tasks.\nSecond, existing efforts in pathology FMs are primarily directed towards the modelling of patch/ROI-level data [2, 3, 4], leading to limited contexts for slide-level clinical applications. Conventional models typically treat individual patch images as independent samples for pretraining a patch extractor, and subsequently employ multiple instance learning (MIL) [11, 12, 13] to perform slide-level modelling based on embedded patch features. Recent concurrent work has attempted to pretrain a slide-level FM [5]. However, it is achieved by pretraining a slide aggregator on top of pre-extracted patch features with a fixed patch extractor. This way poses an inherent limitation that the upper bound of pretraining performance is inevitably constrained by the quality of patch features, thus limiting their generalization abilities.\nIn this study, we curated the largest multimodal dataset including three modalities: H&E diagnostic WSIs, pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs (22k used for pretraining) from 10,275 patients across 32 cancer types sourced from The Cancer Genome Atlas (TCGA) (Figure 1c-e). To leverage these multimodal data for CPath, we developed a novel whole-slide pretraining paradigm which injects multimodal knowledge into the"}, {"title": "2 Results", "content": "2.1 The Overview of mSTAR\nThe proposed mSTAR aims to provide a novel whole-slide pretraining paradigm that injects multimodal knowledge into the pathology foundation model. Compared with existing pathology foundation models, mSTAR has the following innovative designs to fully unleash its power in a wide spectrum of pathological downstream tasks. First, clinical multimodal data are fully harnessed in pretraining to endow the pathology FM with multimodal knowledge for comprehensive perspectives in clinical tasks. Second, the whole-slide pretraining paradigm provides an alternative way to obtain whole-slide contexts for pathology FMs through self-taught training. To the best of our knowledge, this is the first work to inject multimodal knowledge at the whole-slide context into a pathology FM, broadening the contextual understanding for CPath from patch-level to slide-level and from unimodal to multimodal knowledge. The overview of mSTAR is shown in Figure 2, consisting of two stages of pretraining.\nIn the first stage, the objective is to inject multimodal knowledge into the slide aggregator by slide-level contrastive learning among three modalities, i.e., WSIs, pathology reports and RNA-Seq profiles. Note that the pretrained slide aggregator will act as a bridge that propagates multimodal knowledge into the patch extractor in the next stage. To this end, as shown in Figure 2a, we first utilized a pretrained patch extractor, a state-of-the-art pathology foundation model named UNI [3], to encode each patch image of a slide into patch features. Then the resulting patch features are fed into a slide aggregator and integrated into a slide-level representation which is subsequently aligned with other modalities through inter-modality contrastive learning. Furthermore, to mitigate the influence of heterogeneity across different types of cancers, the pretraining of the slide aggregator is also supervised by inter-cancer contrastive learning. This approach brings samples of the same cancer type closer together while concurrently pushing samples of different cancer types apart.\nIn the second stage, the pretrained slide aggregator acquiring multimodal knowledge, can serve"}, {"title": "2.2 Pathological Slide Classification for Diagnosis and Treatment", "content": "Slide classification holds significant relevance in clinical settings, particularly in accurate cancer diagnosis and treatment. For example, it aids in categorizing the specific subtype of cancer, which in turn guides the healthcare professionals in precise diagnosis and treatment planning. Additionally, pathologists can identify molecular markers within tissue slides, allowing them to determine the molecular subtypes of cancer. This information is of utmost importance as it forms the foundation for developing personalized treatment plans and enhancing treatment effectiveness.\nTherefore, the evaluation of slide-level diagnostic tasks, such as cancer subtyping, grading, etc., is the first step of this study, followed by molecular prediction based on the analysis of tissue slides. From the perspective of technique, here we are investigating unimodal abilities through pathological slide classification. To this end, 12 diverse slide classification datasets were used for assessing the ability of decision-making diagnosis at the slide level, consisting of 6 diagnostic subtasks and 6 molecular prediction subtasks. For diagnostic tasks, the following subtasks were included: breast metastasis detection on CAMELYON [14, 15], prostate ISUP grading on PANDA [16], ovarian cancer subtyping on UBC-OCEAN [17, 18], BRCA subtyping on TCGA BRCA [19], NSCLC subtyping on TCGA NSCLC [19] and RCC subtyping on RCC-DHMC [20]. For molecular prediction, we performed the prediction of molecular subtypes ER, HER2 and PR on BCNB [21] datasets, respectively. This task also includes molecular subtypes prediction on TCGA-BRCA, TCGA-CRC and TCGA-GBMLGG datasets [19]. The details of every task are illustrated in the Section 4.3.\nThe evaluations are two-fold: 1) we examined patch features of different pathology foundation models in slide classification (Figure 3a-d), including ResNet50 (R50) [22], PLIP [2], CONCH [4], UNI [3] and our mSTAR. To obtain slide/patient-level classification decision, we adopted two commonly used MIL backbones as the slide aggregator trained from scratch, attention-based MIL (ABMIL) [11] and TransMIL [13] for integrating these extracted patch features. ABMIL is a simple yet robust MIL approach, which is usually used for evaluation in previous foundation model research [3, 4, 5]. Considering that we employed the pretrained TransMIL as the \"Teacher\" in the proposed method, we also present the performance of TransMIL from scratch for all extractors. 2) Since we pretrained a TransMIL in the first stage in the proposed paradigm, we additionally explore how it collaborates with the pretrained patch extractor (Figure 3d), leading to an advanced version (mSTAR+). We further compared the performance of different extractors between being equipped with a non-pretrained TransMIL (built from scratch) and the pretrained TransMIL+ (Figure 3e).\nAll comparisons are based on the metric of Macro-AUC, a commonly used classification metric, which does not rely on the selection of the decision threshold and is insensitive to the sample ratio of various classes. Furthermore, to avoid the impact of potential inherent bias in various datasets and different MIL backbones, following the previous research [1], the average rank of Macro-AUC was reported as well (Figure 3a) and the one-sided Wilcoxon signed-rank test was performed on various datasets to examine the statistical difference between mSTAR and other FMs.\nFrom an overall perspective, we assessed the average rank and average Macro-AUC for mSTAR and the compared foundation models across 12 diverse subtasks. The results showed that mSTAR consistently achieved the first place across different tasks, regardless of different MIL backbones employed or when considering the overall performance. For the consistency of performance improvements, Figure 3c-d (Extended Data Table A2-A3) demonstrated that mSTAR achieved the best performance on 11 out of 12 subtasks with ABMIL and 9 out of 12 subtasks"}, {"title": "2.3 Pathological Survival Analysis for Prognosis", "content": "Prognostic analysis is an intricate clinical endeavor, necessitating a thorough analysis from a multitude of facets. In this regard, the integration of multimodal data has proven instrumental in enabling more comprehensive prognostic assessments [8, 9, 10]. Therefore, it is crucial to delve into the exploration of the role of multimodal knowledge within the broader whole-slide context in enhancing prognosis estimation. In this study, we assessed a key prognostic task, overall survival (OS) prediction, on top of pathological tissue slides.\nmSTAR advances Cancer Survival Prediction. We investigate the capability of predicting overall survival (OS) across 9 diverse cancers, based on the metric of C-Index, a metric commonly used in survival prediction. To avoid data leakage, the data used for evaluation were held out from pretraining data. To ensure the reliable evaluation, we excluded cancer types with fewer than 450 cases or those with too few uncensored data out of the 32 cancer datasets in TCGA [19], resulting in 9 cancer datasets: Breast Invasive Carcinoma (BRCA), Colon Adenocarcinoma and Rectum Adenocarcinoma (CRC), Glioblastoma Multiforme and Brain Lower Grade Glioma (GBMLGG), Head and Neck Squamous Cell Carcinoma (HNSC), Kidney Renal Clear Cell Carcinoma (KIRC), Lung Adenocarcinoma (LUAD), Lung Squamous Cell Carcinoma (LUSC), Skin Cutaneous Melanoma (SKCM) and Uterine Corpus Endometrial Carcinoma (UCEC). Similarly, following the settings of slide-level classification, we also used ABMIL and TransMIL for evaluating patch extractors and then the performance of the pretrained slide aggregator was assessed. More details are presented in Section 4.3.\nFor consistency of performance increases, mSTAR performed best compared to other foundation models, where Figure 4a shows that mSTAR always ranked in front of other foundation models for different MIL backbones and the overall performance. When considering each individual subtask, mSTAR achieved the best performance on 7 out of 9 datasets with ABMIL (Figure 4c and Extended Data Table A5), in which 5 tasks demonstrated a significant difference (P < 0.01) compared to the second-best approach. With TransMIL (Figure 4d and Extended Data Table A6), mSTAR presented more consistent improvements, which outperformed compared approaches on 8 out of 9 cancer types and showed significant improvements compared to the second-best method in 7 cancer types (P < 0.001).\nWhen specifically assessing the C-Index metric, the average C-Index showcased significant performance gains compared against the second-best model (UNI) by +1.98% for TransMIL (P < 0.005), +0.53% for ABMIL (P < 0.05), and +1.30% for the average performance of different MIL backbones (Overall, P < 0.001). Next, we delved into details of performances across 9 datasets, where we observed apparent improvements (P < 0.001) of +3.87% on UCEC, +3.34% on LUAD, +2.89% on LUSC and +1.46% on SKCM. These results indicated the effectiveness of multimodal knowledge involved in mSTAR in extracting discriminative features for distinguishing complex survival patterns in pathological images.\nWe also investigated the performance of incorporating the pretrained aggregator. From observations of Figure 4d, mSTAR+ equipped with the pretrained aggregator pushed performance"}, {"title": "2.4 Multimodal Capability", "content": "Multimodal contrastive pretraining naturally allows the model to be applied to multimodal downstream tasks. In this study, since mSTAR aligned multimodal data that may contribute to alleviating inter-modal heterogeneity, we first examine whether features extracted by mSTAR facilitate multimodal fusion by assessing multimodal survival analysis tasks. Next, we investigate more cross-modal abilities, including slide-level few-shot and zero-shot classification and pathological report generation.\nmSTAR facilitates Multimodal Fusion. The involvement of multimodal data in the pretraining process can enhance the model's capability of capturing complex interactions across different modalities and aligning each other by contrastive learning, thereby potentially alleviating the heterogeneity of different modalities and improving its performance on a wider range of cancer types. To validate this, we replaced pathological features with ones extracted by various extractors in existing multimodal fusion models for 9 TCGA cancer survival prediction tasks, to observe the differences that would arise. Specifically, recent SOTA multimodal fusion models were employed in this study, including MCAT [8], Porpoise [23], MOTCat [9] and CMTA [10].\nOn the whole, mSTAR has clearly outperformed other SOTA methods by a wide margin. Considering average rank, mSTAR ranked between 1.22 and 1.67 among various fusion models and the overall rank is 1.47, which left the second-best approach UNI far behind (Figure 5a) ranking at 2.56 to 3.00 and 2.68 on average. For average C-Index (Figure 5b), mSTAR achieved consistent and notable enhancement in multimodal fusion with a significant difference, with average performance increases of +1.5% (P < 0.001). Among different multimodal fusion models, mSTAR outperformed the second-best UNI by +1.7% (P = 0.02) for MCAT, +1.1% (P = 0.01) for Porpoise, +2.5% (P < 0.005) for MOTCat and +1.9% (P < 0.005) for CMTA.\nAcross various datasets, MCAT surpassed other SOTA approaches on 7 out of 9 tasks (Figure 5c and Extended Data Table A8), especially on LUAD (+3.3%, P < 0.001) and BRCA (+1.9%, P < 0.001). In the case of MOTCat, mSTAR excelled in 7 out of 9 tasks (Figure 5d and Extended Data Table A10) with performance increases of 0.6%-4.8% (P < 0.001). mSTAR with Porpoise demonstrated superior performance in the majority of tasks, topping 5 out of 9 datasets (Figure 5e and Extended Data Table A9), which increased the second-best model by 0.3%-3.8% (P < 0.001). For CMTA, mSTAR delivered the highest performance in 7 of 9 (Figure 5f and Extended Data Table A11), advancing the second-best one by 0.4%-4.1% (P < 0.001).\nIn a nutshell, the remarkable increases across various datasets and diverse multimodal fusion backbone models vividly demonstrate the tremendous contributions of multimodal knowledge embedded by slide-level multimodal contrastive learning in facilitating multimodal fusion.\nmSTAR advances Few-shot Slide Classification. Few-shot learning aims to achieve accurate prediction using only a small number of labeled data for each class, which is especially valuable in the context of medical scenarios where data scarcity is a common challenge. The ability of generalizing to new tasks with a few labels can be enhanced by multimodal data by offering a more"}, {"title": "3 Discussion", "content": "In this study, we delve into how to harness the full potential of multimodal data to effectively advance the performance of the pathology foundation models and bridge the gaps of previous foundation models in multimodal capabilities. Additionally, we explored a new whole-slide pretraining paradigm for CPath, which broadened the context of modelling for better performance on slide-level tasks. We curated the largest multimodal dataset consisting of WSIs, pathology reports and RNA-Seq data, which contained over 26k public modality pairs from 10,275 patients across 32 major cancer types. Diverse experimental results on 43 subtasks demonstrated that mSTAR excelled in not only unimodal tasks but also multimodal tasks at the slide level. In particular, with the inclusion of gene expression data, mSTAR performed well in slide-level molecular prediction. Furthermore, multimodal contrastive learning facilitated multimodal fusion tasks and endowed the model with more generalized capabilities in low-shot scenarios. Simultaneously, the incorporation of clinical pathology reports in the pretraining process resulted in the advanced capability of report generation by providing the expert knowledge.\nIn the realm of prior investigations into pathology foundation models, two prominent categories have emerged: vision-only models [3, 30, 5] and vision-language models [4, 2]. However, these approaches fail to tap into a vast wealth of information inherent in clinical pathology reports and gene expression profiles. Pathology reports usually provide authentic expert knowledge in line with the clinical practice, while gene expression profiles always serve as robust indicators of disease status for clinical applications in diagnosis [31] and prognosis [32]. As shown in Extended Data Table A1, the involvement of pathology reports and gene expression data can bring extra performance gains. We also found that there was still considerable room for further improvement in molecular prediction, multimodal fusion for survival analysis and report generation in existing FMs, which can benefit from the inherent knowledge in pathology reports [33] and gene expression profiles [28, 27, 29]. Simultaneously, we notice that the majority of existing FMs primarily focus on patch/ROI-level models and short texts, in which restricted contexts hinder their practical performance in slide-level clinical applications, such as survival analysis and report generation. It is worth noting that, unlike CONCH supervised by generative loss, mSTAR without involving generative components in pretraining, still demonstrated encouraging performance in report generation with producing more comprehensive texts. Recently, beyond working on small patches/ROIs, we noticed that a study attempted to work on slide-level foundation models, which pretrained the model on patch features. However,"}, {"title": "4 Methods", "content": "4.1 Pretraining Dataset Curation\nData used for pretraining in this study were totally obtained from a publicly available source, the Cancer Genome Atlas Program (TCGA) [19], in which we collected 9,640 cases (11,765 slides) of diagnostics formalin-fixed paraffin-embedded (FFPE) H&E WSIs, 11,108 pathology reports and 10,234 cases of bulk RNA-Seq data across all 32 cancer types of TCGA. After quality control, we curated a dataset with 8,440 WSI-Report pairs, 8,965 WSI-RNA-Seq pairs and 8,764 Report-RNA-Seq pairs, resulting in 26,169 modality pairs, as shown in Figure 1c. Given that numerous downstream tasks, such as survival analysis, were evaluated on TCGA data, we held out some validation and test cases. For 9 cancer datasets comprising over 450 cases, we adopted a split ratio of 7:1:2 for train-validation-test folds. For those cases involving multiple slides, we combined their patches or features into a single case for pretraining at the patient level. This ensured slides belonging to one case were included within the same fold, thereby preventing potential data leakage. We also considered label stratification for survival labels at patient-level, since we primarily evaluated the performance of survival prediction on TCGA data. Note that all cases without survival labels were used for pretraining. Details of data splitting for these 9 cancer datasets are provided in Extended Data Table A21. After data partitioning, we curated 22,127 modality pairs for contrastive learning, consisting of 7,083 WSI-Report pairs, 7,538 WSI-RNA-Seq pairs and 7,506 Report-RNA-Seq pairs. Among these, there were 7,947 cases with all three modalities for pretraining. For acquisition of high-quality data, we conducted the subsequent pre-processing procedures for each modality.\nWSI Pre-processing. To conduct slide- (or patient-) level tasks on WSIs, our processing pipeline involved tissue segmentation, patching, and feature extraction (for pretraining aggregators and evaluation). For tissue segmentation, we employed the CLAM library [12], which performed binary thresholding on the saturation channel of a downsampled RGB slide, converted to the hue-saturation-value (HSV) color space. The resulting segmentation mask was obtained by filtering the contours based on their area. The hyperparameters of segmentation are released on our codebase. Furthermore, slides that were corrupted and those containing a small proportion of tissue region were excluded from this study. As a result, we acquired 9,608 cases of 11,727 slides for pretraining and evaluation.\nTo adhere to established practices of previous works [12, 3, 4], we partitioned the segmented tissue regions into 256 \u00d7 256 pixels patches at 20x-equivalent magnification without overlaps and then resized all patches to 224 x 224 pixels for feature extraction. Using pretrained patch extractors that were kept frozen, we pre-extracted embeddings for each patch and stored them for subsequent evaluation purposes.\nReport Pre-processing. For pathology reports, we curated open-source texts from TCGA and converted them from their original PDF format to editable text format via Amazon Web Services (AWS) Optical Character Recognition (OCR) tools, resulting in 9,523 Reports. For quality control, we curated these reports via the powerful language tool, GPT-4, with appropriate prompts provided in Extended Data Table A22, and re-checked them manually to ensure the unchanged original intent. The statistical distribution of word counts for reports is demonstrated in Figure 1e, in which the majority of cases have word counts below 500.\nRNA-Seq Pre-processing. We accessed RNA-Seq data of TCGA from cBioportal database, which were preprocessed and normalized using RSEM [37]. An inherent difficulty in gene expression modelling arises from the variations in absolute magnitudes observed across different"}, {"title": "4.2 Pretraining Framework", "content": "To utilize multimodal knowledge at the whole-slide context for enhancing the pathology foundation model, we propose a whole-slide pretraining paradigm consisting of two-stage pretraining, as shown in Figure 2. In the first stage, we aim to inject multimodal knowledge into the slide aggregator by contrastive learning, including inter-modality contrastive learning (following CLIP [42]) and inter-cancer contrastive learning. In the second stage, to seamlessly propagate multimodal knowledge at the slide-level context into the patch extractor, we leverage the slide aggregator pretrained in the first stage, serving as a \"Teacher\" model, to supervise the pretraining of the patch extractor, termed Self-Taught training. In this way, multimodal knowledge of the whole-slide context can be injected into the pathology FM.\nStage 1 Pretrain Slide Aggregator. In this stage, we aim to pretrain a slide aggregator that learns multimodal knowledge by contrastive learning with other modalities. Note that the pretrained slide aggregator plays a role of \"Teacher\" that propagates the learned knowledge into the patch extractor at the next stage. These modules to be trained are highlighted in red boxes in the Figure 2a, in which we pretrain a 2-layer TransMIL [13] as the slide aggregator for WSIs, a Bert-like text encoder (following BioBert-Base-v1.2 [43]) for pathology reports, and a Performer (following scBERT [40]) for RNA-Seq data.\nGiven these transformer-like encoders, we need to tokenize raw data of every modality into token embeddings before feeding them into their respective encoders. For pathology, we obtained non-overlapping 224 \u00d7 224 patches as early mentioned, and then for every patch, we used a pretrained patch extractor, UNI [3], to extract patch features, where a patch feature was regarded as a token embedding for the slide aggregator. After gathering 4,096 patch features for the i-th patient's WSIs, $P_{i} = \\{p_{m}\\}_{m=1}^{M}$, we fed them into the slide aggregator to integrate all patch features and got a 512-dimensional pathological [CLS] token embedding $P_{i}$ as the slide-level representation, where M is the number of patches and it was fixed into 4,096. For cases where the number of patches exceeds 4,096, a random selection of 4,096 patches is made, while for cases with fewer than 4,096 patches, padding is applied using the mean value. For those cases where one patient has more than one WSI, we simply concatenated them together. Note that all patch features were transformed into 512-dimensional features by a linear projection before being forwarded into the aggregator.\nFor pathology reports, we adopted the text encoder for randomly truncated 512 tokens and outputted the report [CLS] token embedding $T_{i}$. For cases where the length of the text is less than 512, the special token '[pad]' was padded. The RNA-Seq data was organized as a set of 2-tuple $(g_{i}, e_{i})$ comprising of the gene name $g_{i}$ and its expression variable $e_{i}$. Following previous works [40, 44], to assure that genes with potential co-expression get close together, we employed Gene2Vec [41] to generate 200-dimensional gene embeddings for each gene name $g_{i}$. Gene expression can be viewed as the manifestation or presence of each gene, which has been well-documented within a biological system. Therefore, we applied the term-frequency-analysis method used in previous works [40, 44] to discretize the continuous expression variable $e_{i}$ through binning technique. Subsequently, the discrete variable was transformed into a 200-dimensional embedding, which was then integrated into the final gene token embedding $g_{i}$ by addition. Through forwarding the gene encoder, we can get the gene [CLS] token embedding $G_{i}$. It is worth noting that encoder outputs from report and gene modalities were transformed into 512-dimensional features by a linear projection for contrastive learning.\nTo optimize the model through pretraining, we incorporate two objectives including inter-modality contrastive learning and inter-cancer contrastive learning. In the case of inter-modality contrastive learning, given the [CLS] representation of each modality, every two modalities"}, {"title": "4.3 Downstream Tasks", "content": "Comparisons and Baselines. To investigate the benefit of enhancing the patch extractor by incorporating multimodal knowledge at the slide level, we compared mSTAR against one general baseline and three SOTA pretrained extractors commonly used in the CPath community: (1) ResNet50 [22] pretrained on ImageNet-1K [47], a commonly used baseline in many slide-level tasks [13, 9]. (2) PLIP [2], a vision-language (V-L) architecture (CLIP [42]) pretrained on OpenPath consisting of over 200k pathological patch-caption pairs. (3) CONCH [4], a V-L CoCa [48] framework with an additional generative loss pretrained on over 1.17 million pathological patch-caption pairs. (4) UNI [3], a pure vision patch extractor pretrained on more than 100 million patches from over 100k WSIs. Through pre-extracted patch features via these encoders, we can get 1024-dimensional (1024-d) embeddings for ResNet50, UNI, and mSTAR, and 512-dimensional (512-d) embeddings for PLIP and CONCH.\nUnless otherwise specified, we obtained slide-level predictions by training the widely used attention-based multiple-instance learning (ABMIL) [11], a MIL aggregator that integrates all patch features of a WSI into the slide-level representation according to attention scores. We also employed TransMIL [13] as a MIL backbone, a transformer-like MIL architecture with linear time complexity, since we leveraged TransMIL serving a \"Teacher\" to propagate multimodal knowledge into the \"Student\" patch extractor through self-taught training in this work. We additionally presented results of mSTAR equipped with the pretrained aggregator, which was contrastively pretrained features embedded by our pretrained patch extractor mSTAR, resulting in the advanced version of the proposed approach termed mSTAR+.\nIn the experiments of multimodal fusion, we employed 4 existing SOTA multimodal integration models, MCAT [8], Porpoise [23], CMTA [10] and MOTCat [9]. For slide-level Zero- or Few-shot Classification that requires textual class prototypes, we considered the pretrained model as a good zero- or few-shot learner. As a result, following the paradigm of CLIP, we compared against those approaches that are equipped with the text encoder by utilizing the pretrained text encoder as a good classification head, including PLIP and CONCH. For those visual-only FMs, i.e., R50 and UNI, we employed the pretrained text encoder of mSTAR to construct class prototypes. For the pathological report generation, we trained a recent model HistGen [33] based on pathological features from various FMs.\nSlide-level Unimodal and Multimodal Tasks. For these tasks, we follow the conventional two-stage MIL paradigm comprising pre-extraction of patch features as instances and the training of a MIL aggregator that integrates patch features (or instances) into a single slide-level (or bag) feature. The aggregator took all patch features of a WSI as an input and mapped them into a hidden embedding as a single slide-level representation. Subsequently, the slide-level representation was passed through a fully connected classifier head, resulting in logits. Lastly, based on logits, we performed two types of slide-level tasks including classification supervised by cross-entropy loss with slide labels, and survival prediction (an ordinal regression task) supervised by NLL loss [49] with survival labels (Overall Survival in month), ranging from various diagnosis, treatment and prognosis tasks.\nClassification. We performed Breast metastasis detection on CAMELYON (CAMELYON16+17) [14, 15], Prostate ISUP grading on PANDA [16], ovarian cancer subtyping on UBC-OCEAN [17, 18], BRCA subtyping on TCGA BRCA [19], NSCLC subtyping on TCGA NSCLC [19] and RCC subtyping on RCC-DHMC [20], resulting in 6 diverse tasks. For molecular predition, we also conducted"}]}