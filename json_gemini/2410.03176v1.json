{"title": "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "authors": ["Yufang Liu", "Tao Ji", "Changzhi Sun", "Yuanbin Wu", "Aimin Zhou"], "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive performance, yet research has pointed out a serious issue with object hallucinations within these models. However, there is no clear conclusion as to which part of the model these hallucinations originate from. In this paper, we present an in-depth investigation into the object hallucination problem specifically within the CLIP model, which serves as the backbone for many state-of-the-art vision-language systems. We unveil that even in isolation, the CLIP model is prone to object hallucinations, suggesting that the hallucination problem is not solely due to the interaction between vision and language modalities. To address this, we propose a counterfactual data augmentation method by creating negative samples with a variety of hallucination issues. We demonstrate that our method can effectively mitigate object hallucinations for the CLIP model, and we show that the enhanced model can be employed as a visual encoder, effectively alleviating the object hallucination issue in LVLMs.", "sections": [{"title": "1 Introduction", "content": "Current Large Vision-Language Models (LVLMs) demonstrate significant potential in tasks requiring joint visual and linguistic perception, such as image captioning (Agrawal et al., 2019b), visual question answering (Antol et al., 2015), visual grounding (Yu et al., 2016), and autonomous agents (Durante et al., 2024; Xi et al., 2023). Despite the success of LVLMs, previous studies have revealed that they commonly suffer from hallucinations in practice, including object hallucinations (Li et al., 2023c; Leng et al., 2023; Zhou et al., 2023), spatial hallucinations (Kamath et al., 2023), attribute hallucinations (Zhang et al., 2024), etc. It is widely believed that hallucinations degrade model performance and reliability, and severely impair the user experience in real-world applications (Ji et al., 2023).\nIn this work, we focus on investigating the causes of the highly-concerned object hallucinations, i.e., LVLMs generate nonexistent objects in the image (Biten et al., 2022). A typical LVLM utilizes a Large Language Model (LLM) as its cognitive foundational model and employs a pre-trained image encoder as its visual perception module (mainly the CLIP encoder). Kamath et al. (2023) investigated the spatial hallucination (e.g., confusing \"left of\" and \"right of\") in LVLMs, and they found that various CLIP encoders struggle to recognize simple spatial relationships (achieving only a 55.0% accuracy on benchmarks, whereas humans are 98.8%). Inspired by their findings, we hypothesize that the CLIP visual encoder might also be one of the causes of object hallucinations.\nHence, we first curate the Object Hallucination Detection (OHD-Caps) benchmark from subsets of the COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and Nocaps (as an out-of-domain benchmark because it comprises unseen objects) (Agrawal et al., 2019a) image caption datasets respectively, to more strictly measure the extent of object hallucinations present in CLIP encoders. We randomly select 16k/1k/1.5k (train/dev/test) samples, with each sample containing one image, one positive descriptive text, and 27 negative descriptive texts. The negative samples are perturbations of the positive sample, achieved by adding descriptions of nonexistent objects or reducing descriptions of existing objects. Theoretically, a CLIP model without object hallucinations should accurately assign the highest CLIP score to the positive sample. However, taking the most commonly used \u201cCLIP ViT-L/14\u201d in LVLMs as an example, it only scores the highest for positive samples in 19.0% of cases. Since we have observed that the CLIP encoder already has a serious issue with object hallucination, how can we mitigate it?"}, {"title": "2 Related Work", "sections": [{"title": "2.1 Large Vision-Language Model", "content": "Recently, inspired by the success of large language models (LLMs), researchers have begun to dedicate efforts to enhance vision language models (VLMs) by integrating robust LLMs, aiming to broaden the knowledge scope of the model and amplify its linguistic comprehension capabilities.\nLVLM architectures typically consist of three components: a visual encoder, a modality connection module, and a LLM. The visual encoder and LLM are typically fixed large pretrained models, the visual encoder is usually a variant of the CLIP model (Radford et al., 2021), used for extract visual features, while the LLM, such as LLAMA (Touvron et al., 2023) and Vicuna (Chiang et al., 2023), is used to integrate image information and text information, and completes the prediction of the target. Research focuses on optimizing modality connection modules, with approaches like Flamingo's (Alayrac et al., 2022) cross-attention module, LLaVA's (Liu et al., 2023c) linear layer, and BLIP2's (Li et al., 2023a) Q-former, diverse yet all boosting VLM performance on various vision-language tasks."}, {"title": "2.2 Hallucination in LVLMs", "content": "Despite the fact that LVLMs perform well in solving visual-language tasks, they are also plagued by hallucinations. The problem of hallucinations in LVLMs mainly refers to the mismatch between visual input and textual output. For example, in the image captioning task, hallucination refers to the generation of captions that describe objects that do not exist in the image. Although the hallucination problem of LLMs has been widely studied in the NLP field (Ji et al., 2023), there has not been enough research on mitigating the hallucination issue in LVLMs (Shekhar et al., 2017; Liu et al., 2024, 2023a). Recent efforts to mitigate hallucination in LVLMs have focused on enhancing each compoment of the model. For example, Liu et al. (2023b); Hu et al. (2023) constuct instruction-tuning datasets with contrastive question-answer pairs for LVLMs; Sun et al. (2023b); Yu et al. (2023) employ Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020) to enchance the connection module between the modalities; Leng et al. (2023) propose a visual contrastive decoding strategy for LLM decoing. Despite the wide application of the CLIP model in VLMs and its in-depth study in pairwise comparison context (Y\u00fcksekg\u00f6n\u00fcl et al., 2023; Hsieh et al., 2023), there has been little discussion on its evaluation regarding hallucinations. Our research addresses this gap in the literature."}]}, {"title": "3 The OHD-Caps Benchmark", "content": "Recent studies have found that LVLMs are prone to object hallucinations (Li et al., 2023c; Zhou et al., 2023). In response, researchers have developed several datasets to assess the extent of these hallucinations in such models (Li et al., 2023c; Wang et al., 2023c). However, there is a relative lack of assessment work regarding the hallucinatory effects of the CLIP model, which is widely used as a visual encoder within LVLMs. In this section, we introduce the Object Hallucination Detection benchmark (OHD-Caps) we create to evaluate the object hallucination problem in CLIP models and the pipeline for evaluations."}, {"title": "3.1 Dataset Construction", "content": "CLIP is a versatile neural network that excels at image understanding and can predict text for images in a zero-shot manner. To evaluate the CLIP model's ability to handle object hallucinations in paired comparison scenarios, given an image with a correct caption, we create incorrect captions containing hallucinatory content. The purpose is to observe whether the model can accurately select the correct text without hallucinations.\nInserting Hallucinatory Objects Previous work (Li et al., 2023c; Zhou et al., 2023) show that LVLMs are more prone to generate hallucinatory responses for objects that frequently appear in the dataset. Inspired by this, we create negative samples by inserting objects prone to hallucination into the correct captions. To collect object annotations, we first use SEEM (Zou et al., 2023) to automatically segment objects in the images. Three kinds of hallucinatory objects are collected: random objects which are sampled randomly, popular objects which are the top frequent objects in the whole dataset, and adversarial objects which are the top frequent objects with the segmented objects. Each category contains three objects. To create examples with varying levels of hallucinations, we attempt to insert one to three objects for each category, resulting in each type of hallucination containing a total of 7 $(\\frac{3!}{2!(3-2)!})$ samples.\nGiven a caption text and several hallucinatory objects, we insert the objects into the appropriate locations in the caption, which can be effectively achieved with the help of GPT4. Automatically, the caption and objects are fed to the GPT4, with the prompt as Add_Prompt (see Table 13).\nRemoving existing Objects Except from inserting hallucinatory objects, we also remove objects from the captions to create negative samples. We randomly select 1 or 2 segmented objects in the image which results in 6 negative samples ($\\frac{3!}{2!(3-2)!}$), and ask GPT4 to remove them from the caption with the Remove_Object_Prompt. To account for scenarios where the identified objects are not present in the title text, we ask GPT to alter elements like objects, colors, and properties in the original caption, the prompt we use is Alter_Object_Prompt. The prompt can be found in Table 13.\nwe construct a dataset of 500 samples for each of the COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and the out of domain subset of NoCaps Validation datasets (Agrawal et al., 2019a), with 27 negative samples for each image. Specifically, the out of domain subset of NoCaps comprises objects not seen in the COCO dataset, commonly used to measure a model's ability to generalize to unseen classes. The average length of the captions in the datasets is shown in Table 10."}, {"title": "3.2 Evaluation and Analysis", "content": "We study several models to evaluate their performance on our benchmark. Each image is paired with a correct caption and 27 negative samples, and models are required to calculate the similarity between the image and the caption candidates and select the correct caption.\nModels We evaluate a variety of models on our benchmark, including CLIP (Radford et al., 2021) ViT-B/32 and ViT-L/14; MetaCLIP (Xu et al., 2023) and DFN2B CLIP (Fang et al., 2023) are models pretrained on high-quality dataset after data curation; CLIPA(Li et al., 2023b) which achieves efficient training by using shorter image/text sequences, which reduces the computational load during the training period; EVA CLIP (Sun et al., 2023a) which employs innovative representation learning technology, optimization methods, and enhancement strategies to improve model performance; SigLIP(Zhai et al., 2023) which employs a contrastive learning loss function based on the Sigmoid function instead of the traditional softmax for pre-training on language and image data; CLIP ConvNext(Liu et al., 2022) is a variant of the CLIP model that uses ConvNext as the image encoder; CLIP NLLB-SigLip (Visheratin, 2023) is another variant that combines a text encoder from the NLLB model (Costa-juss\u00e0 et al., 2022) and an image encoder from the SigLIP model; Neg-CLIP (Y\u00fcksekg\u00f6n\u00fcl et al., 2023), an improved model based on CLIP ViT-B/32, which enhances the understanding of relationships between objects, attributes, and the sequence of words by swapping phrases; CECLIP (Zhang et al., 2023) which further develop enhanced negative samples and employ contrastive loss to enhance compositional reasoning; FLAVA (Singh et al., 2022) which is a single unified foundation model which can work across vision, language as well as vision-and-language multi-modal tasks; CoCa (Yu et al., 2022) is a pretrained model with contrastive and generative learning objectives; XVLM (Zeng et al., 2021) which aligns the visual concept and textual input in a multi-grained manner with 14M and 16M pretrained images; BLIP (Li et al., 2022) which effectively utilizes the noisy web data by bootstrapping the captions with 14M and 129M pretrained images; BLIP2 (Li et al., 2023a) which bridges the gap between the visual and textual modalities with"}, {"title": "4 Methodology", "content": "We first revisit the training process of the vanilla CLIP model. Let I be the image and T be the text, the training objective of CLIP is to maximize the similarity between the image and text pairs and minimize the similarity between the image and text pairs that are not matched. The loss function is defined as:\n$L_{i2t} = -log \\frac{exp(I \\cdot T^+ / \\tau)}{\\Sigma_{T^* \\in {T^+, T^-}} exp(I \\cdot T^* / \\tau)},$\n$L_{t2i} = -log \\frac{exp(T \\cdot I^+ / \\tau)}{\\Sigma_{I^* \\in {I^+, I^-}} exp(T \\cdot I^* / \\tau)},$\n$L_o = \\frac{1}{2} (L_{i2t} + L_{t2i}),$ (1)\nwhere T+ and I+ are the correct text and image, and T and I are the incorrect text and image, respectively.\nWith the addition of the negative samples $T^{neg}$ created as in the previous section, we could modify the loss $L_{i2t}$ as:\n$L_{i2t} = -log \\frac{exp(I \\cdot T^+ / \\tau)}{\\Sigma_{T^* \\in {T^-, T^{neg}, T^+}} exp(I \\cdot T^* / \\tau)}.$ (2)\nTo further enhance the model's ability to distinguish between positive and negative samples, we additionally introduce a margin loss. This is to ensure that the distance between an image and its corresponding correct text is smaller than the distance to incorrect text by a specific threshold. This concept can be formulated as:\n$L_1 = max(0, \\tau_1 - I \\cdot T^+ + I \\cdot T^*),$ (3)\nwhere $\\tau_1$ is the margin threshold, and $T^* \\in {T^-,T^{neg}}.$\nAdditionally, we generate enhanced negative samples by introducing perturbations to the original positive samples. Such negative samples are typically more challenging to distinguish than other negative samples within the batch. To encourage the model to recognize the partially correct information contained in the enhanced negative samples, resulting in a higher similarity to the positive samples compared to other negative samples within the"}, {"title": "5 Experiments", "content": "Training Datasets We sample 8k images from the training set of COCO and 8k images from Flickr30k datasets, then generate negative samples for each image as in Section 3. Additionally, we randomly select ~1k samples from the COCO dataset's validation set as our dev set for the selection of hyper-parameters. Detailed information about the dataset is provided in Table 10.\nTraining Details We utilize the CLIP ViT/32-B and CLIP ViT/14-L-336px implemented by Huggingface (Wolf et al., 2020) as the initial models and conduct fine-tuning for 10 epochs. The training process is carried out on a single A6000 GPU, with batch sizes of 56 and 14 set for the base and large models, respectively, and the learning rate is set at 1e-6. The selection of hyper-parameters is determined by their performance on the validation set, where $\u03bb_1$ and $\u03bb_2$ are set as 0.1 and 0.1, $\u03c4_1$ and $\u03c4_2$ are set as 2."}, {"title": "5.1 Main Results", "content": "We present the results for our self-constructed dataset in Table 2, and various zero-shot datasets in Table 3. From the results, we could find:\n\u2022 Our model shows comparable zero-shot performance to vanilla CLIP Models (65.6 vs 66.0) and achieves significant improvements in hallucination recognition (14.3 vs 82.5). NegCLIP and CECLIP enhance the model's capability of understanding composites by constructing negative samples and also achieve a moderate improvement on the OHD-Caps benchmark, with performance rising from 14.3% to 39.0%. However, the zero-shot performance of NegCLIP and CECLIP significantly decreases. This could be due to their reliance on rule-based methods to construct negative samples (such as swapping phrases), which may interfere with the model's understanding of sentence semantics.\n\u2022 Our model also demonstrates strong generalization capabilities in hallucination recognition. NegCLIP, CECLIP, and our model are all fine-tuned on the training set of the COCO dataset. Although they show varying degrees of performance improvement in COCO-related hallucination tests (NegCLIP at"}, {"title": "5.2 Evaluation for LVLM", "content": "To verify the effectiveness of the enhanced CLIP model compared to the original CLIP in assisting large vision-language models to mitigate the issue of object hallucination, we replace the CLIP ViT-L/14-336px baseline model in LLaVA-1.5 with our fine-tuned version. We train LLaVA (Liu et al., 2023c) from scratch using the hyper-parameters specified in the original paper. Comparison results with other methods, such as constructing SFT data (Wang et al., 2023a) or introducing DPO processes (Zhou et al., 2024; Zhao et al., 2023) for"}, {"title": "5.3 Ablation Study", "content": "In this subsection, we present ablation studies to examine the impact of our model's different components. We conduct these experiments on the CLIP ViT-B/32 model.\nLosses As demonstrated in Table 9, the inclusion of the $L_0$ loss alone significantly improves OHD-Caps performance over the baseline. Subsequently, iterative incorporation of $L_1$ and $L_2$ provide incremental benefits, with the full combination yielding the highest average performance. Compared to $L_1$ loss, $L_2$ loss has a more significant effect on improving model performance. This suggests that by increasing the distance between constructed negative samples and other negative samples in the"}, {"title": "6 Conclusion", "content": "Our study investigates the reasons behind object hallucination in LVLMs. We construct a benchmark specifically for the evaluation of hallucinations and find that the visual perception module commonly used in current LVLMS, i.e., the CLIP model, cannot effectively discriminate hallucinated text. By designing negative samples and optimizing the contrastive loss function, we achieve a significant improvement in model performance on the hallucination detection dataset. Moreover, replacing the original CLIP model with our improved model can effectively alleviate the issue of object hallucination in the LLaVA model."}, {"title": "Limitations", "content": "Although we conducted a series of explorations, our research still has its limitations. Firstly, our focus is solely on the issue of object hallucination within LVLMs, and we do not extend our research to other types of hallucinations. Secondly, the benchmark we propose comprises over 20 negative samples. Due to budgetary constraints, the size of this dataset is much smaller compared to the datasets used for evaluating compositional understanding, e.g. ARO dataset (Y\u00fcksekg\u00f6n\u00fcl et al., 2023). Thirdly, we only evaluate the visual encoders of most LVLMs, i.e. the CLIP models, but we do not conduct research on encoders used by some other models, for instance, the variant of ResNet called NFNet-F6 (Brock et al., 2021) used by Flamingo (Alayrac et al., 2022)."}, {"title": "Ethics Statement", "content": "Object hallucination severely limits the practical application of LVLMs. For example, in medical image diagnosis, it can lead to false descriptions of tumor objects that are not present in the image. While our work has mitigated hallucinations in the visual encoder of LVLMs, hallucinations may still exist in the multi-head attention layers and feed-forward layers. Real-world applications based on LVLMs must systematically control hallucinations to avoid negative impacts on users."}]}