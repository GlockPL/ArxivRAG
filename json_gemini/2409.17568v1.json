{"title": "Showing Many Labels in Multi-label Classification Models: An Empirical Study of Adversarial Examples", "authors": ["Yujiang Liu", "Wenjian Luo", "Zhijian Chen", "Muhammad Luqman Naseem"], "abstract": "With the rapid development of Deep Neural Networks (DNNs), they have been applied in numerous fields. However, research indicates that DNNs are susceptible to adversarial examples, and this is equally true in the multi-label domain. To further investigate multi-label adver-sarial examples, we introduce a novel type of attacks, termed \"Showing Many Labels\". The objective of this attack is to maximize the number of labels included in the classifier's prediction results. In our experiments, we select nine attack algorithms and evaluate their performance under \"Showing Many Labels\". Eight of the attack algorithms were adapted from the multi-class environment to the multi-label environment, while the remaining one was specifically designed for the multi-label environ-ment. We choose ML-LIW and ML-GCN as target models and train them on four popular multi-label datasets: VOC2007, VOC2012, NUS-WIDE, and COCO. We record the success rate of each algorithm when it shows the expected number of labels in eight different scenarios. Experi-mental results indicate that under the \"Showing Many Labels\", iterative attacks perform significantly better than one-step attacks. Moreover, it is possible to show all labels in the dataset.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have been widely applied in various domains. However, recent research has indicated that DNNs are susceptible to carefully crafted perturbations [1,2,3]. These perturbations are imperceptible to the hu-man eyes but can lead to incorrect classifications by DNNs. Such vulnerabilities can be particularly detrimental in safety-critical fields. For example, attackers"}, {"title": "2 Related Work", "content": "The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 provides a problem description. Section 4 introduces the attack algorithms, models and datasets used in the experiments. Section 5 analyzes experimental results. Section 6 briefly summarizes this paper."}, {"title": "2.1 Multi-label Adversarial Attack", "content": "In the multi-label learning domain, due to the interdependencies between la-bels, altering the predicted confidence of one label could lead to changes in the predicted confidence of other labels. Consequently, attack methods designed for multi-class classification tasks cannot be directly applied to multi-label scenar-ios. Currently, there is relatively little work on multi-label adversarial examples, with only a few attack methods available.\nSong et al. [12] divided the multi-label learning problem into classification and ranking, and designed corresponding attack frameworks for each. Based on these two frameworks, they proposed four multi-label attacks: ML-DP, ML-CW, Rank I, and Rank II. ML-DP and ML-CW are used for attacking multi-label classification, while Rank I and Rank II are for multi-label ranking. Zhou et al. [13] defined an optimization problem similar to ML-DP and then transformed the optimization problem into a linear programming problem. This method con-trols the size of the perturbation by limiting the lo norm of the perturbation.\nHu et al. [26] proposed an attack algorithm for the Top-k multi-label learning problem, TkML-AP. This algorithm designed a new loss function, achieving un-targeted attacks, universal untargeted attacks, and targeted attacks. Kong et al. [14] proposed a black-box attack for generating multi-label adversarial examples based on the differential evolution algorithm. They designed a complementary mutation operator in the algorithm, enhancing the overall performance of the algorithm."}, {"title": "2.2 Existing Attack Types", "content": "To evaluate the effectiveness of attack algorithms, Song et al. [12] proposed five types of attacks, which are \"Hiding single\", \"Random\", \"Extreme\", \"Reduc-tion\", and \"Augmentation\". In the \"Hiding single\", adversaries select samples that contain at least two labels and randomly choose one label to hide. Ad-versarial examples generated ensure that the classifier is unable to identify the hidden label. In the \"Random\", adversaries randomly select a positive label and a negative label within a sample, and then launch attacks to alter the positive label to be negative and the negative label to be positive. \"Extreme\" is the most"}, {"title": "3 Problem Description", "content": "Suppose we have a multi-label classification problem with l labels, where the input data is represented as (x, y). x \u2208 Rd denotes the feature vector of an example, and y \u2208 {\u22121,1}l represents its label vector. When yi = 1, it indicates that the example contains the label i; otherwise, yi = -1.\nWe denote the multi-label classifier by f : Rd \u2192 Rl, which outputs the predicted confidence for each label of the exmaple. f can be considered as a composition of l sub-functions, namely f = {f1, ..., fl}, where fi(x) represents the predicted confidence of the label i. Suppose t represents a threshold; if the predicted confidence fi(x) for label i is not less than t, it implies that label i is present in the example, otherwise it does not. In this paper, unless specifically referred to, the threshold t is set to 0, and fi(x) \u2208 [\u22121,1]. Therefore, for any label i, the predicted outcome y of the multi-label classifier can be represented as follows:\n $Y =\\begin{cases} 1, & \\text{if } f_i(x) \\geq 0 \\\\ -1, & \\text{if } f_i(x) < 0 \\end{cases}$\nWhen each dimension of y and y' is equal, the classification is correct.\nIn the task of generating multi-label adversarial examples, we denote the adversarial perturbation by r. The adversarial example is represented as x* = x + r. We aim to achieve the attack target while minimizing the perturbation; thus, the problem can be formulated with the following equation:\n$\\begin{aligned} & \\min_{r} ||r|| \\\\ & \\text{s.t. } F(f(x^*)) = y^* \\end{aligned}$\nwhere y* is the attack target, F(\u00b7) is the function represented by Equation (1).\nIn the \"Showing Many Labels\", our goal is to manipulate the classifier's predictions such that the number of positive labels meets the expected number"}, {"title": "4 Experimental Design", "content": ""}, {"title": "4.1 Attack Algorithms", "content": "This subsection introduces the nine attack algorithms used in the experiments.\n1) FGSM: The FGSM [20] is a white-box attack applied in multi-class envi-ronments. It calculates the gradient of the model output with respect to the input and adds a small perturbation in the direction of the gradient's sign to achieve the attack. Since this method generates adversarial samples in a single step, it is con-sidered a one-step attack. In this paper, we utilize the version of FGSM adapted for the multi-label environment [12]. Specifically, compute the loss L(y*, f(x)) between the attack target y* and the model's output f(x). The gradient of the loss for the input x is \u2207xL(y*, f(x)). To minimize the loss L(y*, f(x)), the perturbation is added to the input in the direction of -\u2207xL(y*, f(x)), thereby generating an adversarial example. The algorithm can be described as follow:\n$x^* = x + \\epsilon \\cdot \\text{sign}(-\\nabla_x L(y^*, f(x)))$\nwhere e is the perturbation step size, sign(\u00b7) is the sign function, and x* denotes the generated adversarial examples.\n2) FGM: FGM [21] is an extension of FGSM. Unlike FGSM, which simply adds the sign of the gradient to the input to create an adversarial example, FGM uses L2 norm normalization to better preserves the direction opposite to the gradient, potentially leading to more effective adversarial examples. Song et al. [12] adapted the FGM to the multi-label environment and the core idea can be described as follow [12]:\n$x^* = \\text{clip}_{\\epsilon} \\{x - \\epsilon \\frac{\\nabla_x L(y^*, f(x))}{\\|\\nabla_x L(y^*, f(x))\\|_2}\\}$\nwhere ||\u00b7||2 is 12 norm.\n3) BIM: FGSM and FGM are both one-step adversarial attack methods. Their simplicity often leads to failure when attacking large datasets. To enhance"}, {"title": "4.2 Classification Models", "content": "This subsection introduces two multi-label classification models. The ML-LIW model was proposed by Song et al. [12], while the ML-GCN model was proposed by Chen et al. [27].\n1) ML-LIW: Song et al. [12] introduced ML-LIW, a multi-label classifier based on the Inception v3 network pre-trained on the ImageNet dataset. To ac-commodate the requirements of multi-label classification, Song et al. replaced the original softmax layer with a sigmoid layer. Furthermore, to enhance model per-formance, they designed a comprehensive loss function that combines instance-wise and label-wise losses. Specifically, the instance-wise loss employs a modified instance AUC score, which helps capture the relationships between labels within each instance. The label-wise loss, on the other hand, incorporates ranking loss and label-wise AUC scores, aiming to alleviate the issue of label imbalance in the dataset.\n2) ML-GCN: Chen et al. [27] proposed a multi-label classification model based on Graph Convolutional Networks (GCNs), known as ML-GCN. This model simulates the dependencies between labels by constructing a directed graph, where each node represents an object label and is represented through word embeddings. The core concept of ML-GCN is to leverage GCNs to learn node representations on the label graph, thereby mapping to a set of interdepen-dent classifiers. The model employs a novel reweighting scheme to construct the label correlation matrix, which effectively guides the propagation of information in the GCN by balancing the weights between nodes and their neighborhoods. This approach not only mitigates the issues of overfitting and over-smoothing, but also captures complex relationships between labels, thereby enhancing the accuracy of classification."}, {"title": "4.3 Datasets", "content": "This subsection introduces four commonly used datasets in the multi-label learn-ing domain.\n1) VOC2007: The VOC2007 [15] dataset is a widely used benchmark in the field of computer vision. Comprising images from real-world scenarios, it includes 20 distinct categories. The training set comprises 5,011 samples, the validation set 2,510, and the test set 4,952.\n2) VOC2012: The VOC2012 [15] dataset inherits and expands upon the VOC2007 dataset, featuring 20 categories. It includes a training set with 5,017 images and a validation set with 5,821 images and no test set.\n3) NUS-WIDE: The NUS-WIDE [16] dataset is a web image dataset with multiple labels that created by the Media Search Lab at the National University of Singapore. It comprises 269,648 images across 81 categories. However, some samples are currently inaccessible. We use the accessible samples as described in [28]. The number of accessible samples amounts to 169,823, which includes a training set of 119,103 and a validation set of 50,720 and no test set.\n4) COCO: The COCO [17] dataset supports a variety of computer vision tasks, including image classification, object detection, segmentation, and image captioning. It encompasses 80 categories, with a training set of 82,783 images, a validation set of 40,504 images, and a test set of 40,775 images."}, {"title": "5 Experimental Results and Analyses", "content": ""}, {"title": "5.1 Experimental Setting", "content": "Due to the absence of a test set in both the VOC2012 and NUS-WIDE datasets, we randomly select 20% of the data from the training and validation sets to serve as the test set. We resize all images to 448*448 and normalized the pixel values to the range [0, 1].\nFor ML-GCN, we directly utilize the open-source code from the original paper and train it on four datasets. The code is available at https://github.com/Megvii-Nanjing/ML-GCN. For ML-LIW, we replicate it based on [12], and similarly, train it on four datasets.\nWe evaluate the model's performance on the datasets using five metrics: hamming loss, ranking loss, micro-F1, macro-F1, and average precision."}, {"title": "5.2 Results Analyses", "content": "Our objective is to evaluate the performance of nine attack algorithms in the \"Showing Many Labels\" across two models and four datasets. In the experi-ments, following the method described in Section 3, we respectively calculate the avgLabels for samples that extracted as attack samples from four datasets, with VOC2007 at 1.293, VOC2012 at 1.207, NUS-WIDE at 1.351, and COCO at 1.641. Subsequently, we set a series of expLabels for each dataset based on differ-ent values of n. Specifically, VOC2007 and VOC2012 have six distinct expLabels, while NUS-WIDE and COCO have eight distinct expLabels. We record the at-tack success rate of each attack algorithm under eight different scenarios target-ing various expLabels. The results are shown in Table 2 and 3, where boldface indicates the best results. Since the total number of labels may vary across dif-ferent datasets, we use the value of n to represent different expLabels, with the calculation method following Equation (3).\nExperimental results indicate that different attack algorithms exhibit varying capabilities in showing labels across different models and datasets. Some algo-rithms are only capable of showing a small number of labels, while others can show all labels. Some algorithms perform well under certain models and datasets but perform poorly in other situations.\nThe overall performance of one-step attacks is significantly inferior to that of iterative attacks. When n \u2265 2, FGSM and FGM are unable to carry out attacks, with FGSM being the worst performer among all algorithms. When attacking the ML-GCN trained on the NUS-WIDE dataset, even with n = 0, the attack success rate is only 0.3%. The FGSM attack performs best when targeting the ML-LIW trained on the NUS-WIDE, with attack success rates of 99.3%, 96.0%, 83.2%, and 47.4% for n = 0, 1, 2, 3 respectively. The FGM performs more stably due to the 12 norm normalization, which ensures that perturbations are applied in the direction opposite to the gradient, but the overall performance remains poor."}, {"title": "6 Conclusion", "content": "This paper investigates the performance of attack algorithms under eight differ-ent conditions within the new attack type \"Showing Many Labels\", and presents the attack success rates of various attack algorithms for different expLabels. We utilize two multi-label classification models, four multi-label datasets, and nine attack algorithms, eight of which are adapted from multi-class environments to the multi-label environment. Experimental results indicate that under the \"Showing Many Labels\" attacks, the performance of one-step attacks is sig-nificantly inferior to that of iterative attacks. Furthermore, as the expLabels increases, the attack becomes more challenging. However, in certain scenarios, there are still attack algorithms capable of showing all labels in the dataset with a high probability of success. In the future, we will conduct further research on more effective attack methods targeting the multi-label learning domain, as well as corresponding defensive measures."}]}