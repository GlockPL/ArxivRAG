{"title": "PIANIST: Learning Partially Observable World Models with LLMs for Multi-Agent Decision Making", "authors": ["Jonathan Light", "Sixue Xing", "Yuanzhe Liu", "Weiqin Chen", "Min Cai", "Xiusi Chen", "Guanzhi Wang", "Wei Cheng", "Yisong Yue", "Ziniu Hu"], "abstract": "Effective extraction of the world knowledge in LLMs for complex decision-making tasks remains a challenge. We propose a framework PIANIST for decomposing the world model into seven intuitive components conducive to zero-shot LLM generation. Given only the natural language description of the game and how input observations are formatted, our method can generate a working world model for fast and efficient MCTS simulation. We show that our method works well on two different games that challenge the planning and decision making skills of the agent for both language and non-language based action taking, without any training on domain-specific training data or explicitly defined world model.", "sections": [{"title": "1 Introduction", "content": "Recent studies have shown how LLMs, trained on massive amounts of online data, can be used as a world model to conduct planning [1, 2]. However, using LLMs as world models have not been as well explored in multi-agent, partial information settings such as in language games and other board games. These settings present unique challenges due to (1) the complexity of all the possible action, (2) partial observablity, and (3) other, possibly adversarial or stochastic, agents. These complexities mean that directly using the LLM as a policy for planning is not as feasible [3]. More related works in App. D.\nIn this work, we introduce a framework PIANIST that allows us to use the LLM to more easily learn and plan with a PIANIST world model. Specifically, PIANIST separates the world model into seven different components that we use the LLM to generate. This includes the forward transition function, the action function, and the information partition function, all of which we prompt the LLM to generate in the form of code, which is easily executable and verifiable. We show that our method works well on two different games one card based, and one discussion based \u2013 showing strong performance from LLM-agents that use our framework."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Decision problem formulation", "content": "We formulate decision making tasks as a partially observable Markov decision process (POMDP) with an explicit environment actor which makes it more LLM-friendly to model."}, {"title": "Problem definition.", "content": "Given a state space S and action space A, a policy function \u03c6 in policy space \u03a0 maps states to probability distributions over actions, \u03c6 : S \u2192 \u0394\u0391. An environment E = (S,A,N,T, R, A, \u03a6\u03b5) includes the state and action spaces, actors N, a transition function T : S \u00d7 A \u2192 S, a reward function R : S \u00d7 A \u2192 R|N| for actor rewards, and an action function A : S\u2192N, P(A) determining legal actions. The environment actor's policy \u03c6E handles stochastic transitions, allowing for both deterministic and stochastic, single or multi-agent settings. In partial information settings, an information partition function P : S \u00d7 N \u2192 I maps hidden states to information sets. Then the policy \u03c6 maps from information sets to action distributions, \u03c6 : I \u2192 \u0394\u0391.\nGoal. Given an environment E, the goal for each actor i \u2208 N is to find a policy that maximizes their cumulative reward, given that other players are also playing their optimal policy \u03c6*:\n$\\phi_i = \\underset{\\phi_i}{argmax}\\underset{\\tau \\sim (\\Phi_i, \\phi_{-i}^*)}{E} \\Big[\\sum_{(s, a) \\in \\tau} R_i(s, a) \\Big]$, where \u03c4 = (so, ao, ...) is the simulated trajectory according to the strategic profile (\u03c6i, \u03c6\u2212i) and the transition function T, with at ~ \u03c6(at|st) and st+1 = T(st, at). \u03c6 is commonly known as a Nash equilibrium, since no player i has any incentive to individually deviate from their optimal policy \u03c6*."}, {"title": "2.2 Decision-making games", "content": "We evaluate the performance of OMEGAZERO compared to other algorithms, both rule based and deep reinforcement learning based, on two board games representing games of two different genres.\nGOPS (Goofspiel) is a multi-round, two-player simultaneous action game commonly studied in game theory [4, 5]. Each player is dealt identical hands of cards numbered 1 to k, and a shuffled prize deck, also numbered 1 to k, is revealed one card at a time. Both players simultaneously play a card from their hand; the higher card wins the prize, and both cards are discarded. After k rounds, players sum the values of their won prize cards, and the higher total determines the winner. Good players anticipate future moves and assess the value of each prize. Long-horizon games challenge LLM agents, as they struggle to connect near-term actions with long-term outcomes.\nTaboo (2-player text version) is a cooperative game where one player is the clue-master and the other is the guesser. The clue-master is given a target word and a list of taboo words they cannot use in their clues. Each round, the clue-master makes a statement, and the guesser responds with one guess. The game ends when the guesser correctly identifies the word, makes five guesses, or the clue-master accidentally uses a taboo word. If a taboo word is used, the team scores 0; otherwise, the score is five minus the number of guesses. A good clue-master anticipates the guesser's thought process to help narrow down the options. The novelty of each word adds to the challenge."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 PIANIST: Extracting LLM World Knowledge", "content": "We present a new framework for extracting world knowledge from LLMs by dividing the world model into seven intuitive components that the LLM can understand. With this extracted model, we can apply model-based reinforcement learning techniques like MCTS or TD-learning. Most components are generated by prompting the LLM with the game description and a predefined Python parent template class. See App. C for examples of LLM generated models.\n\u2022 I: Information sets. The agent observes information sets, and we provide code for representing them along with a natural language game description as an interface between the real world and the agent. This and the game description are the only game-specific information given.\n\u2022 S: Hidden states. The agent records any relevant hidden information here.\n\u2022 N: Actors. Used to specify what the actor names are for the action function and reward function.\n\u2022 A: Action function. For large action spaces, the function returns the top k most likely actions. For language actions, an LLM generates the top k text options. See section 3.3 for details."}, {"title": "3.2 Integrating PIANIST with Search", "content": "Our pseudocode for PIANIST-guided MCTS is in Alg. 1, with a diagram in Fig. 2, and further details in App. A. During MCTS, we sample a realization of the observed information set using the information realization function. A trajectory is then simulated by selecting the action with the highest UCT value (eq. 1) for the acting player. Since actors cannot distinguish hidden states within the same information set, UCT values are averaged across all states in that set, weighted by visit counts, preventing the use of hidden information. The simulation continues until a state s with unexplored actions is reached, where a random unexplored action a is chosen. The transition function provides the next state s' and reward r, which are recorded. The action function, partition function, and value heuristic are used to record the actions, information set, and value estimate for s'. A random rollout or LLM-generated heuristic computes the value estimate. Backpropagation is then performed from s' up the tree using the backpropagation equation (eq. 3)."}, {"title": "3.3 Handling language actions", "content": "Language-based games are particularly challenging for traditional RL methods due to their need for language abilities and extensive semantic knowledge. In these games, the action space for language-based actions, such as discussion, is practically infinite, consisting of all possible word and token combinations. Current search methods are only effective in finite action spaces. Additionally, RL methods alone cannot inherently understand language or be trained to do so through self-play. We address this by utilizing LLMs to propose likely high-level dialogue actions for players. This allows us to (1) prune improbable actions and (2) focus on a few high-level strategy categories,"}, {"title": "4 Experiments", "content": "We evaluated our model against three different opponents. For the ground-truth models, we used ground truth models with MCTS search, combined with a random-rollout value heuristic, and played them against our LLM-generated agent, which also uses an LLM-generated value heuristic. Ground-truth include the true S, A, N, T, R, A models used during actual gameplay. For LLM as policy, we directly queried the LLM for actions in ReAct style [9], which includes a thought phase before action. For human opponents, we recruited 10 individuals to play 30 games of 6-card GOPS and 30 games of Taboo. In Taboo (a cooperative game), we paired each agent with a human-crafted model as the teammate (guesser), as the clue-giver role is more difficult. In GOPS, the two agents played directly against each other. We report both win rate and score for both games. In GOPS, win rate refers to whether a player had a higher score than their opponent, while score represents the point difference based on how many score cards were won. In Taboo, win rate measures whether the team guessed the word on the first try, and score is based on how quickly the team guessed the correct word. Note the possibility for tieing in GOPS.\nAs shown in Table 1, PIANIST performs similarly to ground-truth models, indicating that the LLM can generate an accurate world model using our framework. Additionally, Table 2 shows that our world model helps the agent plan more effectively than directly querying the LLM for actions. However, our agent struggles to consistently beat humans at GOPS and Taboo, highlighting the need for further research on improving LLM agents for complex decision-making environments on both action and language games (Table 3). Overall, despite using LLMs to generate its world model zero-shot, our agent demonstrates strong performance, showcasing the effectiveness of PIANIST in extracting world knowledge for tree search. This suggests that future work could explore more nuanced adaptations of the framework to balance decision-making performance across different games, potentially enabling more robust generalization in varied multi-agent environments."}, {"title": "A MCTS Details", "content": "The Monte Carlo Tree Search (MCTS) process, illustrated in Figure 2, simulates possible future game states by expanding nodes in a search tree. Each node corresponds to a state, and edges correspond to actions. MCTS operates by iteratively simulating trajectories (denoted as 7) from the current realized hidden state until an unexpanded state is reached. The steps involved in MCTS include four key phases: selection, expansion, simulation, and backpropagation."}, {"title": "A.1 Upper Confidence Bound for Trees (UCT) and Action Selection", "content": "At each node, the next action a* is selected according to the Upper Confidence Bound for Trees (UCT) formula, given by Equation (1):\n$a^* = arg\\ max_a \\underset{s'|s, r = T(s,a)}{E} [r_i + \\gamma V_i(s') + C \\sqrt{\\frac{\\log n(s)}{n(s')}}]$                                    (1)\nIn this equation:\n\u2022\na* is the action that maximizes the expression.\n\u2022\ns represents the current state, and s' is the next state reached after taking action a.\n\u2022\nri is the immediate reward for agent i when transitioning from state s to s'.\n\u2022\nVi(s') is the value function that estimates the future reward from the next state s' for agent i.\n\u2022\ny is the discount factor, which balances immediate and future rewards.\n\u2022\nn(s) is the number of visits to the current state s, while n(s') is the number of visits to the next state s'.\n\u2022\nC is a constant controlling exploration versus exploitation, and \\log n(s)/n(s') encourages exploring less-visited actions.\n\u2022\nT(s, a) represents the transition dynamics, mapping the current state and action to the next state and reward."}, {"title": "A.2 Probability Distribution Over the Information Set", "content": "Since our MCTS handles partial observability, a probability distribution is defined over the information set Is, which contains all possible hidden states s that are consistent with the observed information. This distribution is given by:\n$P(s) = \\frac{n(s)}{\\sum_{s' \\in I_s} n(s')}$                  (2)\nWhere:\n\u2022\nP(s) is the probability of being in state s within the information set Is.\n\u2022\nn(s) is the visit count of state s, and the denominator normalizes over all possible states s' E Is."}, {"title": "A.3 Backpropagation", "content": "After simulating a trajectory \u03c4, MCTS backpropagates the results of the simulation to update the value estimates and visit counts for all state-action pairs (s, s') along the trajectory. This update is performed using Equation (3):\n$V_i \\in N \\ \\ V_i(s) \\leftarrow V_i(s) + \\frac{1}{n(s)} (r_i + \\gamma V_i(s') - V_i(s)), \\ n(s) \\leftarrow n(s) + 1 $                                    (3)"}, {"title": "B PIANIST Generation details", "content": ""}, {"title": "C Example PIANIST models generated by gpt-40", "content": ""}, {"title": "D Related Work", "content": "LLMs for text agents. Large language models (LLMs) have demonstrated significant emergent capabilities, such as zero-shot prompting and complex reasoning [10, 11, 12, 13, 14, 15]. They also possess extensive world knowledge [16], which has spurred increasing efforts to use LLMs for decision-making in text agents [17]. One notable paradigm is ReAct [18], which employs an observation-reasoning-acting loop for agent planning with LLMs. Building on ReAct, Reflexion [7] incorporates self-reflection to enhance reasoning capabilities. Other works in this domain have utilized feedback [19, 20], memory [21], and tool use [22, 23] to further enhance agent performance. Our proposed method, OMEGAZERO, integrates these components to design an agent capable of systematic analysis and strategic decision-making. Typical prompting techniques for text agents include Chain-of-Thought [24], Tree-of-Thought [2], and Graph-of-Thought [25].\nLLMs and planning. Recent works have proposed planing using the LLM as a world model [1, 26]. These works have mostly centered around using the LLM as a forward transition function (dynamics model) by querying the LLM for the next state [2], or using a planning language to describe plans [27]. Other works have explored using LLMs to guide the MCTS search process by using the LLM as a policy [3, 28]. We build upon these works by investigating what kind of world model is more conducive to extracting world knowledge from the LLM and combining it with MCTS.\nSkill learning with LLMs. Recent works have explored the possibly of LLMs learning skills through learning a textual short and long term memory [7, 29], or textual insights extracted from the memories [30]. Due to the length of trajectories in our game setting and the numerical nature of the data, it is difficult to learn textual memories, so we learn high level strategies instead. We also explore how to acquire simulational self-play feedback in multiagent settings. Using LLMs to learn a functional reward model has also been applied to great success on single-agent robotic tasks [31, 32]. We build upon their work by introducing a new improvement method that can help learn a better reward model, and exploring how function learing can be applied to multiagent settings with simulated feedback.\nAI in strategy games. Al has been applied to great success in board games. AlphaGo and MuZero demonstrated the power of combining MCTS, deep learning, and feedback generation using self-play in games such as Go, Chess, and Shogi [33, 34]. Language models can also be trained on human in-game discussion data and integrated with another separately trained action planner to play board games with dialogue [35]. We build upon the AI for games literature by showing that LLMs can accomplish both (1) the training of a value heuristic like that in AlphaGo through self-play more efficiently than RL and (2) dialogue generation in discussion games with no human examples. These adversarial environments are not just limited to board games. For example, there has been recent interest on creating LLM-agents that can negotiate [36, 37], which our method can also be applied to. Traditionally the solution to searching over a large action space has been to bucket similar actions together, such as possible raises in poker [38]. We leverage the inherent distribution in the LLM to suggest the top most probable, yet distinct, actions instead."}, {"title": "F Language action examples", "content": "Example proposed possible dialogue actions\nClue word: barefoot\nTaboo word: shoes, socks, summer, beach\nAction 1: You might feel the ground directly under your feet when you don't wear any footwear.\nAction 2: It's a way to enjoy nature by feeling the earth, grass, or sand without anything covering your feet."}]}