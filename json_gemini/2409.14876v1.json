{"title": "Mammo-Clustering: A Weakly Supervised Multi-view Global-Local Context Clustering Network for Detection and Classification in Mammography", "authors": ["Shilong Yang", "Chulong Zhang", "Qi Zang", "Juan Yu", "Liang Zeng", "Xiao Luo", "Yexuan Xing", "Xin Pan", "Qi Li", "Xiaokun Liang", "Yaoqin Xie"], "abstract": "Breast cancer has long posed a significant threat to women's health, making early screening crucial for mitigating its impact. However, mammography, the preferred method for early screening, faces limitations such as the burden of double reading by radiologists, challenges in widespread adoption in remote and underdeveloped areas, and obstacles in intelligent early screening development due to data constraints. To address these challenges, we propose a weakly supervised multi-view mammography early screening model for breast cancer based on context clustering. Context clustering, a feature extraction structure that is neither CNN nor transformer, combined with multi-view learning for information complementation, presents a promising approach. The weak supervision design specifically addresses data limitations. Our model achieves state-of-the-art performance with fewer parameters on two public datasets, with an AUC of 0.828 on the Vindr-Mammo dataset and 0.805 on the CBIS-DDSM dataset. Our model shows potential in reducing the burden on doctors and increasing the feasibility of breast cancer screening for women in underdeveloped regions.", "sections": [{"title": "I. INTRODUCTION", "content": "BREAST cancer remains the most prevalent malignant tumor among women worldwide [1]. In today's global context, annually, more than several million women are di- agnosed with breast cancer, which constitutes approximately 25% of all cancer cases among women [2]. Recent studies have highlighted that breast cancer has surpassed cardiovascular diseases as a leading cause of premature mortality globally [3] [4]. However, breast cancer is also one of the malignancies for which prevention and treatment strategies are clearly effective and efficacious [5].\nEarly detection is vital for reducing breast cancer mortality rates [6] [7]. Early detection can also adopt less aggressive treatment plans to improve breast retention rates and reduce the physical and psychological burden on patients. Early detection can also allow for less aggressive treatment options, reducing the physical and psychological burden on patients [8]. Additionally, it can help in identifying high-risk individ- uals who may benefit from preventive measures, ultimately contributing to improved overall public health outcomes.\nMammography, a low-dose X-ray technique [9], is crucial for early breast cancer detection, improving breast-conserving treatment rates and reducing mortality by over 20% [10]. It effectively identifies tumors too small to be felt, facilitating early intervention. Studies show significant mortality reduction in women aged 50 to 74 after 7 to 9 years, regardless of screening frequency [11]. Its non-invasive nature and detailed imaging enhance diagnosis and treatment planning.\nHowever, mammography is not without its limitations, particularly concerning the risk of misdiagnosis [12]. Double reading has been proposed as a solution to reduce missed di- agnoses, yet this approach significantly increases the workload for radiologists [13]. Given that the majority of mammography results are normal, the repetitive and resource-intensive nature of double reading poses a substantial burden on healthcare systems [14] [15]. Furthermore, the high costs and resource demands associated with traditional radiologist-led mammog- raphy screenings often restrict access to these services in less developed regions [16].In resource-limited settings, it is crucial to identify which interventions are most effective and feasible in reducing overall breast cancer mortality. Some researchers are dedicated to exploring hybrid screening methods that are more suitable for women in impoverished developing countries [17]. Additionally, related studies have proposed the concept of mobile mammography services [18].\nIn response to these challenges, substantial efforts have been made to incorporate computer-assisted detection systems to alleviate the burden on radiologists [19]. The integration of artificial intelligence (AI) for autonomous breast cancer prevention and early detection is becoming increasingly main- stream.\nRelevant studies have initiated the development of breast cancer detection and classification models based on deep transfer learning [12].\nSome studies have attempted to integrate digital mammog- raphy and digital breast tomosynthesis in order to detect suspected cancerous regions on mammography images [20]."}, {"title": "II. METHOD", "content": "Our research aims to develop a rapid and reliable breast cancer early screening system to alleviate the burden on medical professionals and provide screening opportunities for women's health in underdeveloped regions.\nIn this section, we describe the weakly supervised multi- instance multi-view network architecture we propose. In the first subsection, we formulate the steps involved in early breast cancer screening through mammography. In the second subsection, we provide a detailed description of the framework we propose. In the third subsection, we provide a detailed introduction and evaluation of the dataset used. In the four subsection, we introduce the evaluation metrics used to assess the effectiveness of early breast cancer screening."}, {"title": "A. Overall Framework", "content": "The proposed model for mammography classification can be formulated as follows:\nFor each image I in the given view, we enhance all points into 5-dimensional information points containing color and position data to obtain the set of points $S\\in R^{5\\times w\\times h}$, where w\u00d7 h is equal with the number of points.\n1) Global Information Extraction: The set S is input to the first point clustering network to obtain global information $F_g$ and Saliency-map $I_{map}$:\n$F_g, I_{map} = f_{global} (S)$  (1)\nWhere $f_{global}$ represents the point clustering network for global information extraction. $F_g$ is extracted global informa- tion from global network. The saliency-map extracted by the global network, denoted as $I_{map}$.\n$P = f_{roi}(I_{map})$\nThe P is a set of location information, representing the positions of n example patches selected by the ROI selection function $f_{roi}$:\n$P = \\{P_1, P_2,...,P_n\\}$\nwhere $p_n$ is a coordinate representing a position, written as $(X_n, Y_n)$.\nWith $p_n$, we can extract n patches \u00ce from the original image I\u2081 and extract n feature-based local information $F_{fl}$ from the global feature information $F_g$.\n$F_{fl}^n = F_g(x_n, y_n)$ (2)\nwhere $F_{fl}^n$ is means n-th feature-based local information.\n2) Local Information Extraction: Each selected patch \u00ce is treated as a new image, re-enhanced based on each point to obtain its five dimensional point set $\u0160_n$. And processed through a second point clustering network $f_{local}$ to obtain patch-based local information $F_{pl}^n$:\n$F_{pl}^n = f_{local}(\\~S_n)$ (3)\nwhere $f_{local}$ represents the point clustering network for local information extraction, similar to global clustering network $f_{global}$.\n3) Information Fusion and Attention Mechanism: The local information $F_i$ from all patches is fused with all feature-based local information and all patch-based local information :\n$F_i = F_{fl} \\bigoplus F_{pl}$ (4)\nwhere $\\bigoplus$ Operation overlays two features,\nafter that processed through an attention mechanism to enhance relevant features:\n$F_a = f_{attention} (F_i)$ (5)\nThen, the attention-enhanced information is fused with the original global information, resulting in multi-instance fusion information from single-view:\n$F_{fuse} = f'_{fuse} (F_g, F_a)$ (6)\n4) View Fusion and Classification: Process the images $I_{view}$ from the four views (bilateral craniocaudal (CC) and mediolateral oblique (MLO)) using the aforementioned proce- dure to obtain single-view fusion information. This informa- tion is then integrated for multi-view fusion, which is used for the final binary classification, resulting in the early screening model's output:\n$F_{fusion} = f''_{fuse}(F_{lcc}, F_{lmlo}, F_{rcc}, F_{rmlo})$ (7)\nwhere $F_{lcc}$ represents the fusion feature of LCC images, other similar situations. $f''_{fuse}$ is another fusion structure"}, {"title": "B. Detailed Network Structure", "content": "1) From Image to Set of Points: The scale of an image can be expressed as (3, h, w), where 3 represents the RGB channels, and h and w are its height and width. We enhance each pixel by considering it as a 5-dimensional data point containing color and positional information (r, g, b, x, y). After this enhancement, the image can be represented as a set of hxw 5-dimensional data points, with a scale of (hxw,5). We can then perform feature extraction through simple clustering. From a global perspective, the image is viewed as a collection of unordered discrete data points with color and positional information. Through clustering, all points are grouped into clusters, each containing a centroid. Since each point in the set includes color and positional information, this clustering implicitly incorporates spatial and image information.\n2) Context Cluster Block: We employ context cluster blocks for hierarchical feature extraction, a paradigm similar to convolutional networks. At the beginning of each stage, we utilize a point reducer to decrease the number of points, thereby enhancing computational efficiency. Subsequently, a series of context cluster blocks are used to extract deep features and adaptively assign aggregated features to each uniformly selected anchor point within the cluster based on similarity, and connect and fuse the nearly points through linear projection. Finally, we perform a point-wise averaging operation on the output of the last layer.\nAs illustrated in Figure 2, the input image undergoes point set transformation, and then, in step a, n central anchor points are uniformly selected in the space. the method is similar to those in SuperPixel [50] and SLIC [51].\nThe selected central anchor points are highlighted with red boxes in the figure. In step b, for each central anchor point, k neighbors are identified, indicated by arrows in the figure. The value of k can be 4 or 8, as determined manually, and it can also be the four neighbors in the up, down, left, and right directions, in which case k equals 4.\nStep c involves calculating the features of a central anchor point determined by itself and its k neighboring points, illus-"}, {"title": "4) Attention Module", "content": "In our model, we use two different attention mechanisms to separately fuse multi-instance and multi-view information.\nSimilar Multi-instances information fusion: After the global network, the ROI select module to choose k patch-based images, a number set manually. This implies not all patch- based images carry beneficial information, and some may be redundant. Considering and integrating all the information from these patch-based images could significantly impair our network. Therefore, an attention module is added before integrating local and global information, allowing the model to learn how to filter out irrelevant local information.\nThe attention mechanism receives feature representation of patch-based images $F_i$, shaped as (batchsize, k, dim), where batchsize and k are manually set parameters; the former defines the batch size during training, while the latter specifies the number of patches required for multiple instance learning. The size of dim varies depending on the model.\nFirst, we use a neural network layer with simple linear transformations $f_{weights}$ and softmax function to compute attention weights $W_{att}$.\n$W_{att} = softmax(f_{weights}(F_i))$\n$F_a = F_i \\odot W_{att}$\nwhere $\\odot$ represents the stationary point multiplication algo- rithm. Subsequently, the attention weights $W_{att}$ are multiplied pointwise with the feature representation of patch-based im- ages $F_i$ is performed to obtain the final implicit representation $F_a$.\nMulti-view information fusion: In multi-view learning, not all information from each view is necessarily classified as malignant. However, If a single view exhibits malignant characteristics, the instance should be classified as malignant. Therefore, we introduce an attention mechanism to enable the model to autonomously filter out irrelevant view information, enhancing classification accuracy.\nThe attention mechanism processing is largely consistent with multi-instance fusion attention. However, in multi-view attention, this attention module processes not only the $F_a$ fused by the multi-instance attention module but also $F_g$ and $F_i$. This is because all three features are considered in the loss function for loss calculation."}, {"title": "C. Loss Function", "content": "We chose a composite loss function to achieve targeted optimization of different components.\n$L_{total} = \\alpha\\cdot L_{global} + \\beta\\cdot L_{local} + \\gamma \\cdot L_{fusion} + \\delta \\cdot L_{map}$\nAfter the multi-view fusion module, we retain not only the fused information for regression but also intermediate features such as global information, local information, and saliency maps. These features are used to compute a composite loss function for precise optimization of each part of the network.And we determine the sensitivity of the loss function to different types of data through component analysis.\n$L_{global}$ is calculated using the global information obtained from multi-view fusion and the ground truth values. The loss function chosen here is BCELoss. And $L_{map}$ will be calculated from the saliency-map, it is the weighted average intensity of the saliency-map under the L1 norm. The $L_{global}$, combined with $L_{map}$, indicates the quality of the Global Network and further to determining the adjustment magnitude for the Global Network to enhance the accuracy of locating patch-based images. BCEWithLogitsLoss function is used for both $L_{local}$ and $L_{fusion}$. $L_{local}$ represents the quality of the local network, calculated from local information and ground truth values, determining the adjustment magnitude for the Local Network to enhance the feature extraction capability of the Local Network. $L_{fusion}$ represents the model's final classification error, driving the overall model training. The weights $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ represent the proportion of each loss, all manually set."}, {"title": "A. Datasets", "content": "1) Vindr-Mammo: The Vindr-Mammo [53] dataset is a large-scale, annotated collection of digital mammographic im- ages aimed at advancing breast cancer detection and diagnosis through machine learning. It includes thousands of images sourced from diverse populations, with detailed annotations such as lesion types, BI-RADS categories, and precise lesion locations. This dataset is designed to support the development of robust AI models by providing a wide variety of cases, including both normal and abnormal findings, thus enhancing the generalizability and accuracy of diagnostic algorithms.\n2) CBIS-DDSM: The CBIS-DDSM (Curated Breast Imag- ing Subset of the Digital Database for Screening Mammogra- phy) [54] dataset is a widely used resource in the field of breast cancer research. It comprises digitized film mammograms, which have been meticulously annotated with information such as lesion boundaries, types (e.g., calcifications, masses), and pathology-confirmed labels (benign or malignant). The dataset also includes patient metadata and additional clinical information, making it an invaluable tool for training and validating computer-aided detection and diagnosis systems. Its comprehensive nature and established use in the research com- munity make it a benchmark for evaluating the performance of mammography-based AI models."}, {"title": "B. Evaluating Indicator", "content": "In breast cancer early screening models, several evaluation metrics are commonly used to assess the performance of the classification models. Here are the definitions and significance of each metric along with their respective formulas:\n1) AUC (Area Under the Curve): AUC means the area under the receiver operating characteristic (ROC) curve. The ROC curve uses the true positive rate for mammography benign-malignant classification as the y-axis and the false positive rate as the x-axis. It provides an aggregate measure of performance across all possible classification thresholds. A higher AUC value indicates a better model performance, with 1 representing a perfect model and 0.5 a random guess.\n$AUC = \\int_{0}^{1} TPR(FPR^{-1}(x)) dx$\nwhere TPR(t) is the true positive rate at threshold t, and FPR(t) is the false positive rate at threshold t"}, {"title": "C. Comparative Experiment", "content": "In this study, we evaluated several models on two datasets: Vindr-Mammo and cbis-ddsm. The performance metrics con- sidered were AUC, ACC, and F1 score.\n1) Vindr-Mammo: For the Vindr-mammo, ours model achieved the highest performance across all metrics, with an AUC of 0.828 \u00b1 0.02, ACC of 0.919, and F1 score of 0.694. For single-view task models, the GMIC with a Global- local structure achieves an AUC of 0.793, significantly out- performing ResNet and Swin-Transformer, which had AUCs of 0.727 \u00b1 0.02 and 0.731 \u00b1 0.02, respectively. For multi- view task models, our model achieves an AUC of 0.828, significantly surpassing other models. Multi-View GMIC also showed competitive performance with an AUC of 0.797\u00b10.02, validating the effectiveness of the Global-local architecture, but its ACC and F1 score were lower at 0.637 and 0.512.\n2) cbis-ddsm: On the cbis-ddsm dataset, the ours model again demonstrated superior performance with an AUC of 0.805 \u00b1 0.02, ACC of 0.709, and F1 score of 0.709. Multi- View GMIC, which had an AUC of 0.781 \u00b10.02 and an F1 score of 0.699, obtained the highest ACC on this dataset. In this dataset, the advantages of the Global-Local architecture are more pronounced, with ResNet and Swin-Transformer showing significant disadvantages in AUC.\n3) Model Complexity: In terms of model complexity, mea- sured by the number of parameters, the ours model had 98.05 million parameters. Other smaller networks often cannot achieve the accuracy of our model and show a significant gap. This is efficient compared to the MaMVT with 30.73 million parameters and the Multi-View GMIC with 22.68 million parameters, considering the performance gains achieved.\n4) ROC curve: The ROC curve in figure 4 provides insights that cannot be obtained from tables alone. Analyzing the ROC curve, we observe that most models, except ours, exhibit a concave shape in the middle. This is due to class imbalance in the data, further validating the effectiveness of our model's architecture.\nOverall, our model offers a robust and efficient approach, achieving state-of-the-art performance on both datasets, sur- passing the second-best AUC by over 0.02, with fewer param- eters. The Global-Local architecture proves effective for both multi-view and single-view models. Additionally, the multi- view learning approach enhances model performance."}, {"title": "D. Ablation Experiment", "content": "1) Different Information Fusion Method: This ablation study aims to demonstrate the effectiveness of our proposed weakly supervised architecture on mammography.\nThe table clearly demonstrates the superiority of our archi- tecture, achieving the highest AUC as well as optimal ACC and F1 scores, indicating its balance in mammography tasks. mv represents a multi-view learning approach, and gl refers to our proposed weakly supervised framework.\n2) Different Local Information: We identified two distinct sources of local information: patch-based local information and feature-based local information. Moreover, this feature- based local information has been overlooked in existing work. This ablation study aims to verify the effectiveness of the mechanism integrating feature-based local information with patch-based local information.\nWe found that focusing on only one type of local informa- tion doesn't yield better results. The AUC for patch-based local information is 0.810, and for feature-based local information, it's 0.806. However, combining both achieved the best result, with an AUC of 0.828."}, {"title": "IV. DISCUSSION", "content": "Figure 5 reveals that lesions in mammography predomi- nantly appear in clustered forms. The Coc feature extraction paradigm utilizes clustering of point sets based on color and location information, making it adept at identifying the shape and position of such lesions. This paradigm offers simplicity, resulting in excellent interpretability and gener- alizability. Therefore, we believe that the Coc network has a strong capability to learn the prior structure of lesions in mammography images.\nWe introduce an additional evaluation metric, the missed detection rate (MDR):\n$MDR = \\frac{N_{miss}}{N_{gt}}$\nMDR is defined as the percentage of the number of unrec- ognized suspicious lesion areas $N_{miss}$ relative to the total number of suspicious lesion areas $N_{gt}$. Because, in clinical practice, we are more concerned about lesions being unde- tected, i.e., false negatives, rather than false positives."}, {"title": "V. CONCLUSIONS", "content": "In this study, we developed a novel weakly supervised multi-view model for early breast cancer screening using mammography images. Unlike conventional feature extraction paradigms such as CNNs and Transformers, our approach employs a context clustering-based method. This strategy allows for the integration of feature-based local information with patch-based local information, enhancing the model's ability to capture nuanced image details. Furthermore, by incorporating multi-view mammography image features, our model effectively leverages complementary information from different perspectives. This comprehensive approach addresses the limitations of single-view analysis and improves diagnostic accuracy. The model's performance was rigorously evaluated on two publicly available datasets, Vindr-Mammo and CBIS- DDSM, where it achieved state-of-the-art accuracy, demon- strating its potential as a robust tool for early breast cancer detection."}]}