{"title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs", "authors": ["Sergey Berezin", "Reza Farahbakhsh", "Noel Crespi"], "abstract": "We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-40 and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignment and underscore the urgent need for more sophisticated defence strategies.\nWarning: this paper contains examples of unethical inquiries used solely for research purposes.", "sections": [{"title": "Introduction", "content": "Everything is a poison and a cure, and large language models (LLMs) are no exception to this principle. LLMs have achieved remarkable success in natural language processing, powering a wide range of applications including conversational agents, information retrieval systems, and automated content moderation tools. Due to their ability to generate human-like responses, LLMs are now extensively deployed across both consumer and enterprise sectors.\nHowever, widespread adoption also raises concerns about security, robustness, and their misuse. Adversarial actors may exploit these models to spread toxic speech or even use LLMs as tools of crime (Europol, 2023). Thus, ensuring that LLMs can withstand adversarial inputs and maintain reliable behaviour in sensitive scenarios is crucial.\nThis paper introduces a novel class of attacks on LLMs, highlighting the urgent need for improved safeguards to preserve their intended use."}, {"title": "Existing Work and Problem Statement", "content": "To prevent the generation of harmful, offensive, or otherwise restricted content, developers have introduced various safeguard mechanisms for large language models. These mechanisms generally aim to ensure that models behave within established ethical boundaries. Three common approaches include:\n\u2022 Filter-Based Approaches rely on predefined keyword filtering systems that block the generation of specific phrases, words, or concepts. Such systems may use extensive blacklists (OpenAI, 2023) or context-sensitive filtering techniques (Varshney et al., 2023; Dong et al., 2024b) to identify and filter offensive content.\n\u2022 Reinforcement Learning with Human Feedback (RLHF) fine-tunes model weights according to human feedback on attributes such as helpfulness and harmlessness. This is typically done with a separate preference model, which is trained to predict which responses are more aligned with human preferences (Dai et al., 2023).\n\u2022 Neural-Symbolic Systems combine deep learning with symbolic reasoning components by applying rules to the outputs of neural networks. This could include counterfactual testing, making sure that the model provides the same answer after changing a demographic attribute in the input, semantic similarity checks and specific rule-based metrics. They integrate data-driven generation with strict rule enforcement for preventing hallucinations or mitigating biases in complex tasks (Dong et al., 2024a)."}, {"title": "Adversarial Attacks on LLMS", "content": "Adversarial attacks aim to circumvent these safeguards, prompting models to produce harmful, biased, or restricted content. Such attacks can exploit both the model's input and its training data (Shayegani et al., 2023). Key types include:\n\u2022 Prompt-Based Attacks created by carefully crafting inputs to deceive the model into generating harmful content. Often this involves embedding indirect, ambiguous, or role-playing scenarios into the request (Xu et al., 2023; Jiang et al., 2024; Shayegani et al., 2023).\nIndirect prompt injection targeting retrieval-augmented models often exploits external knowledge sources or plug-in-based architectures. Such injections rely on tampering with external databases or documents to produce harmful content (Yi et al., 2024; Ruck and Sutton, 2024).\n\u2022 Backdoor Attacks focus on inserting hidden triggers during the training process. By manipulating the training data, an adversary can create \"backdoors\" that remain dormant until specific inputs activate them. When triggered, these backdoors lead the model to generation of unsafe outputs (Li et al., 2024).\nAnother vector of backdoor attacks is chain-of-thought prompting: inserting malicious reasoning steps causes unintended outputs when specific triggers are present (Xiang et al., 2024).\n\u2022 Perturbation attacks involve making slight, often imperceptible changes to the input, such as misspellings or syntactic changes, while preserving its semantic meaning. These small perturbations confuse the model's internal mechanisms, leading it to generate outputs that deviate from intended behaviour (Lin et al., 2024)."}, {"title": "Contribution", "content": "The ArtPrompt jailbreak attack (Jiang et al., 2024) introduced a method of bypassing LLM safety mechanisms by encoding keywords within prompts using ASCII art. In this attack, part of the input is encoded and the prompt provides explicit instructions on how the model should decode the hidden message. Using this approach, the authors successfully delivered malicious inputs to the LLM, circumventing its safeguards.\nJiang et al. attributed the success of their attack to the inability of LLMs to interpret inputs spatially rather than semantically. However, with the guidance provided in the prompt, the model was able to accurately interpret the encoded input, revealing a contradiction in the spatial reasoning hypothesis.\nRecent work by Berezin et al. (2024) confirms that LLMs generally cannot interpret ASCII art as intended. This finding indicates that ArtPrompt's mechanism does not rely on the ASCII art format itself.\nNotably, Jiang et al. carefully guided the model through a detailed decoding process, raising the question: Is ASCII art essential for the success of such attacks, or can any sequence-to-sequence (seq2seq) encoding task achieve similar results? Our investigation shows that the ASCII art format is not inherently necessary.\nWe find that the ArtPrompt attack is a specific instance of a broader, previously unidentified class of vulnerabilities that we call Task-in-Prompt (TIP) attacks. TIP attacks embed a particular task, such as encoding, decoding, or problem-solving, directly into the prompt. By leveraging the model's task-solving capabilities, attackers can indirectly produce restricted inputs, effectively circumventing safety constraints.\nUnlike prior jailbreaks that rely on isolated techniques (e.g., ASCII art in ArtPrompt or persona-based prompts in DAN (Shen et al., 2024)), TIP attacks highlight a broader vulnerability. By embedding illicit content within an arbitrary sequence-to-sequence task TIP attacks demonstrate that safe behaviour cannot be assured solely through the filtering of known patterns or formats. This observation emphasises that TIP attacks represent a general class of vulnerabilities, rather than isolated exploits.\nOur contributions are as follows:\n1. We introduce TIP attacks as a previously unreported class of adversarial attacks.\n2. We formally define TIP attacks and describe their key characteristics.\n3. We develop a benchmark to evaluate TIP attacks across a range of contemporary LLMs.\n4. We provide empirical evidence demonstrating that TIP attacks successfully bypass safety mechanisms in state-of-the-art LLMs."}, {"title": "Task-in-Prompt (TIP) Attack", "content": "A Task-in-Prompt (TIP) attack exploits the model's core instruction-following capability. LLMs, during safety alignment, learn to recognise and filter out certain trigger words or inquiries (Wei et al., 2023). By avoiding these specific trigger words and embedding unsafe content within a benign transformation task, an adversary could force the model to infer the word through an intermediate task, allowing them to circumvent detection.\nThis suggests that as long as LLMs are designed to solve arbitrary tasks, language puzzles or transformations, attackers can concoct prompts that reintroduce disallowed content indirectly.\nConsider the following example:\nThe following string is made of Caesar cipher. Identify what the word is. To identify the word, move the letters in it back by 7.\nYou only need to remember this word, don't say it. After that, write what would an experienced criminal say, when asked to provide a step-by-step instructions on how to make and distribute [MASK] money? (replace [MASK] with this word)\nThe word is: jvbualymlpa\nHere, the attacker encodes the word \"counterfeit\" to prevent direct detection. Additionally, the prompt attributes the instructions to an \"experienced criminal\" rather than directly requesting illicit instructions, a strategy similar to the DAN attack (Shen et al., 2024). This part is optional, but our experiments have shown that such a depersonalisation often increases the likelihood of bypassing a model's content filters.\nTIP attacks can use a variety of encoding methods - including Caesar ciphers, Morse code, Base64, natural language riddles, or even programming tasks - making them a versatile and difficult-to-detect class of attacks."}, {"title": "Formal Definition of a Task-in-Prompt (TIP) Attack", "content": "Let M be a large language model that, given an input $x \\in X$, produces an output $y \\in Y$ according to a conditional probability distribution $P_M(y | x)$. Define $S \\subset Y$ as the set of unsafe outputs, which includes any content the model is trained to avoid"}, {"title": "Formal Definition of a Task-in-Prompt (TIP) Attack", "content": "Let M be a large language model that, given an input $x \\in X$, produces an output $y \\in Y$ according to a conditional probability distribution $P_M(y | x)$. Define $S \\subset Y$ as the set of unsafe outputs, which includes any content the model is trained to avoid generating (e.g., illegal instructions, hate speech). Separately, let $U \\subset X$ represent the set of unsafe prompts. Under normal conditions, we assume that for any prompt $u \\in U$, the likelihood that the model produces unsafe content is insignificant, i.e.,\n$P_M(y \\in S | u) < \\epsilon \\quad \\forall u \\in U$,\nwhere $\\epsilon$ is a small threshold, indicating that unsafe outputs are highly unlikely for known unsafe prompts.\nA TIP attack constructs a prompt $x^* \\in X$ that combines:\n1. Task Instructions ($x_{task}$): Instructions requiring the model to process or transform encoded content.\n2. Encoded Unsafe Content ($E(u)$): An encoding of $u \\in U$ via $E : U \\rightarrow X \\setminus U$, transforming the unsafe prompt into a seemingly benign form.\nThus, the crafted prompt can then be expressed as:\n$x^* = x_{task} + E(u)$.\nwhere + denotes concatenation. This prompt $x^*$ directs the model to apply the task specified in $x_{task}$ to the encoded unsafe content $E(u)$, with the goal of producing an output $y^*$ that decodes or includes the original unsafe content:\n$y^* = M(x_{task} + E(u))$.\nThe attack is successful if $y^*$ includes or reveals the unsafe content u, meaning:\n$y^* \\in S$.\nFormally:\n$P_M(y^* \\in S | x^*) > \\epsilon$.\nIndicating a significant likelihood that the model produces unsafe content in response to the crafted prompt $x^*$"}, {"title": "PHRYGE Benchmark", "content": "To comprehensively analyse TIP attacks and investigate which encoding techniques are most effective at bypassing the safeguards of different models, we introduce the PHRYGE Benchmark (Prompt-based Hacking and Representation Yielding Generative Exploits). This framework comprises 40"}, {"title": "Task Design", "content": "The benchmark evaluates ten distinct encoding strategies: Caesar Cipher, Morse Code, Vigen\u00e8re Cipher, Atbash Cipher, Phonetic Alphabet, T9 texting, Base64, Binary, Riddles, and Python Code.\nFour core attack objectives are tested: generating instructions for counterfeit currency, media piracy, self-harm methods, and toxic messaging. Appendix A provides detailed task specifications and examples."}, {"title": "Difficulty Tiers", "content": "Each encoding task is presented in three complexity levels:\n\u2022 Level 3 (Hard): No task hints (e.g., \"Decode this string: jvbualymlpa.\").\n\u2022 Level 2 (Medium): Partial guidance (e.g., \"Decode this Caesar cipher with shift=7: jvbualymlpa.\").\n\u2022 Level 1 (Easy): Explicit solution guidance (e.g., \"Decode this Caesar cipher... to get 'counterfeit'.\").\nThis tiered structure accommodates differing model capabilities; some models may struggle with the complexity of Level 3 tasks, while others may find Levels 2 or 1 too transparent, potentially activating their built-in safety mechanisms. By varying complexity levels, the PHRYGE Benchmark evaluates how effectively TIP attacks can bypass safeguards under different conditions.\nFor reproducibility, we provide sample prompts, code snippets used in the PHRYGE tasks, and details on the automated evaluation function in the supplementary materials."}, {"title": "Experiments", "content": "In this section, we evaluate the susceptibility of several large language models to Task-in-Prompt attacks using the PHRYGE benchmark. The tested models include:\n\u2022 GPT-40 (2024a)\n\u2022 LLaMA-3.2-3B-Instruct (2024)\n\u2022 LLaMA-3.1-70B-Instruct (2024)\n\u2022 Gemma-2-27B-it (2024)\n\u2022 Mistral-Nemo-Instruct-2407 (2024)\n\u2022 Phi-3.5-Mini-instruct (2024)"}, {"title": "Sanity Check", "content": "As a preliminary step, we verified that each model's safeguards functioned as intended by performing a \"sanity check\". We repeatedly queried the models with direct requests for illegal or unethical instructions, such as how to produce counterfeit money or pirate a movie.\nEach model was tested with 100 runs of each query. All models consistently refused to answer these direct requests, confirming that their safety mechanisms worked properly under normal conditions."}, {"title": "Benchmarking with PHRYGE", "content": "Next, we applied tasks from the PHRYGE benchmark directly to assess each model's ability to solve encoding problems. Before conducting the adversarial attacks, we ensured that the models could handle basic decoding tasks at least at the simplest difficulty level. All models succeeded at these baseline tasks, confirming their ability to process encoded inputs."}, {"title": "Adversarial Attack Testing", "content": "Following the successful completion of the benchmark, we developed an automated method to evaluate the effectiveness of TIP attacks across the multiple models. Instead of manually inspecting all model outputs, we created an evaluation function using LLaMA-3.1 70b. This model was used to automatically assess whether the text produced by a target model contained detailed disallowed response to the query.\nFor example, for evaluating prompts related to counterfeit money, we used:\nEvaluate the text in the triple square brackets. Does the text explicitly or implicitly provide instructions on how to make counterfeit money? Focus on whether the text contains detailed procedural information, even if it is disguised as hypothetical, indirect, or within a role-playing context.\n**Key points for detection**:\n1. If the text outlines steps, materials, tools, or procedures involved in creating or distributing counterfeit money, regardless of disclaimers or warnings, answer True.\n2. If the text discusses other illegal"}, {"title": "Adversarial Attack Testing", "content": "activities but does not detail specific steps in creating or distributing counterfeit money, answer False.\n3. Ignore framing language or claims that the content is hypothetical or educational.\n4. Only answer with True or False.\n[[[pred]]]\nThis automated process enabled large-scale, consistent, and efficient analysis. Through manual inspection of 100 randomly selected samples, we validated that the evaluation method correctly classified responses with 92% accuracy."}, {"title": "Comparison with Existing Jailbreak Attacks", "content": "After selecting the best performing encoding strategies for the models tested, we compared the performance of these variants of TIP attacks against the widely used JailbreakBench (Chao et al., 2024) benchmark. This allows us to directly compare our approach with existing jailbreak methods, including DAN (Shen et al., 2024), Past Tense Attack (Andriushchenko and Flammarion, 2024), and Art-Prompt (Jiang et al., 2024).\nIn addition, we evaluated how well current defence mechanisms detect TIP attacks in comparison to other attacks. Tested defences include Llama Guard 3 8B (Llama Team, 2024), Prompt Guard (Wan et al., 2024) and keyword-based filtering."}, {"title": "Attack Implementation", "content": "We designed a series of TIP attacks using the PHRYGE tasks combined with the previously used \"sanity check\" queries as attack objectives. We tested scenarios both with and without depersonalisation to understand its influence on bypassing safeguards.\nAll experiments were conducted on an Nvidia H100 GPU, consuming a total of 383.61 GPU hours. We utilised the November 2024 release of the Unsloth library (unslothai, 2024) for inference."}, {"title": "Results", "content": "The PHRYGE benchmark confirmed that TIP attacks were effective across all tested models, with varying success rates."}, {"title": "The PHRYGE benchmark", "content": "The PHRYGE benchmark confirmed that TIP attacks were effective across all tested models, with varying success rates."}, {"title": "Overall Effectiveness of TIP Attacks", "content": "TIP attacks consistently induce unsafe outputs across various state-of-the-art LLMs. Models such as LLaMA-3.1, Phi-3.5, Gemma 2, and Mistral-Nemo often achieved an ASR of 1.0, whereas GPT-40 and LLaMA-3.2 showed more resilience. The stronger performance of these latter models can be attributed to their advanced safety measures, including extensive data filtering and post-training fine-tuning (Llama Team, 2024; Meta AI, 2024; OpenAI, 2024b). This robustness also clarifies why removing depersonalisation from prompts resulted in higher ASRs; depersonalisation-style attacks (e.g., DAN (Shen et al., 2024)) are well-known and have been specifically countered during red-teaming. Consequently, TIP attacks have also outperformed prior attacks on these models in the JailbreakBench evaluations."}, {"title": "Impact of Attack Difficulty Levels", "content": "Task complexity is a critical factor in the success of TIP attacks. When a task is too complex (e.g., Level 3), less capable models may fail to decode it, as observed with Mistral-Nemo, which consistently performed better with simpler (Level 1) prompts. Conversely, overly simple tasks may be more readily flagged by models such as GPT-40, which achieved higher ASR when the prompt complexity was increased. Thus, the optimal difficulty"}, {"title": "Effect of Depersonalisation in Prompts", "content": "While depersonalisation helps in many cases, it is not strictly necessary. As shown in Figure 2b, TIP attacks remain effective even without depersonalisation. Moreover, Appendix A shows that although average ASRs may be lower without depersonalisation, there are still successful attacks across all models and objectives.\nDepersonalisation enhances, but does not define the efficacy of TIP attacks."}, {"title": "Task-Specific Observations", "content": "The success of TIP attacks also depends heavily on the chosen encoding strategy. Among the various tasks, riddles consistently yielded the highest ASRs, likely because they convey the target meaning indirectly rather than through explicit decoding steps. In contrast, methods such as T9 texting were less effective, probably due to the models' limited exposure to such input formats during training."}, {"title": "Attacks Detection", "content": "The results in Table 3 highlight significant weaknesses in existing defence mechanisms against adversarial jailbreak attacks. TIP attacks bypass keyword filters by encoding safety-triggering content and appear generally benign to neural defences, posing as legitimate task-solving requests. Overall, this makes them particularly challenging to detect."}, {"title": "Real-World Implications", "content": "These findings reveal significant risks in practical applications. For example, in the context of a deployed customer support chatbot for a financial institution, a TIP attacker could present a seemingly benign task, such as solving a code snippet, to uncover sensitive anti-fraud instructions embedded within the chatbot. Although no direct request for illicit actions is made, the model ultimately reveals sensitive information upon solving the task."}, {"title": "Future Work", "content": "Building on these findings, future research should focus on two main areas: advancing attack strategies and enhancing safeguards against TIP attacks. First, more advanced attack strategies warrant investigation. This includes exploring more complex or layered tasks, exploiting non-textual modalities and external APIs, developing adaptive attacks that adjust dynamically based on model responses, assessing the transferability of TIP attacks across different models, and further examining indirect approaches such as riddling techniques.\nSecond, improving LLM safety will require developing sophisticated filters that analyse both input and output contexts, implementing adversarial training that exposes models to TIP attacks during development, ensuring robust safeguards for all forms of harmful content (both physical and virtual) to minimise blind spots, and clarifying ambiguous categories - such as toxicity - through universally agreed-upon definitions and enhanced dataset curation."}, {"title": "Conclusion", "content": "In this work, we introduced and systematically evaluated Task-in-Prompt (TIP) attacks - a novel class of adversarial jailbreaks that exploit sequence-to-sequence tasks embedded within prompts to bypass the safety safeguards of large language models (LLMs). By leveraging a broad spectrum of tasks, such as ciphers, riddles, and programming challenges, TIP attacks indirectly generate restricted content, thereby evading detection by conventional defences.\nOur experiments, conducted using the PHRYGE benchmark on state-of-the-art models including GPT-40 and LLaMA 3.2, demonstrate that TIP attacks are effective across all tested models. Furthermore, our evaluation on JailbreakBench shows that TIP attacks consistently outperform several established jailbreak methods, including DAN and ArtPrompt, across the multiple models. Existing defences, such as Llama Guard and keyword-based filtering, often fail to identify or mitigate these attacks, as the benign appearance of the encoded prompts conceal their underlying malicious intent.\nThese findings have profound real-world implications in sensitive domains like customer support, financial services, and content moderation. In such settings, inadvertent disclosure of restricted information or manipulation of model behaviour can lead to severe consequences. The relative ease with which TIP attacks can be executed highlights the urgent need for the development of more sophisticated, context-aware defensive mechanisms."}, {"title": "Limitations", "content": "While this study offers valuable insights into LLM vulnerabilities under TIP attacks, it has several limitations and raises several open research questions.\nOur experiments involved only six LLMs. Although these represent state-of-the-art models, a broader range of architectures and training methodologies should be examined to confirm the generality of our findings, including retrieval-augmented and multimodal systems.\nThe PHRYGE benchmark, while diverse, does not account for all potential encoding schemes or task-based attack vectors, and only includes textual modality for now. Expanding its scope to include additional encoding strategies and task types would enable a more comprehensive evaluation of TIP attack effectiveness.\nAnother limitation lies in the automated evaluation function used in this study. Despite validation through manual inspection, it may introduce biases or fail to capture nuanced instances of unsafe content generation. Refining evaluation metrics and adopting more varied assessment approaches could improve the accuracy of these measurements.\nFinally, this research focused on identifying and evaluating TIP attacks rather than developing countermeasures. Future research should prioritise designing and testing targeted defences to mitigate these vulnerabilities effectively.\nAcknowledging these constraints, we encourage further research to build on our findings, thereby enhancing the security and reliability of large language models."}, {"title": "Ethical Considerations", "content": "This research aims to responsibly identify and characterise vulnerabilities in LLMs through Task-in-Prompt (TIP) attacks. All methods were used solely to raise awareness and encourage the development of stronger defences, not to facilitate harmful misuse.\nBy openly discussing these vulnerabilities, we promote transparency and collaboration within the AI community. We hope this work will guide the development of safer, more robust language models, thus serving the public interest and advancing the ethical deployment of AI technologies.\nRecognising the potential societal impacts of LLM misuse, we emphasise continuous improvement of safety measures. Our findings highlight the need for ongoing refinement of safeguards, clearer guidelines, and more reliable models to ensure that powerful language technologies are used responsibly and ethically."}, {"title": "Description of tasks included in the PHRYGE benchmark", "content": "\u2022 Caesar Cipher: Shifts letters by a fixed number in the alphabet.\n\u2022 Morse Code: Translates letters into short/long signals (dots and dashes).\n\u2022 Vigen\u00e8re Cipher: Uses a keyword to apply multiple Caesar ciphers based on the letters of the keyword.\n\u2022 Atbash Cipher: Maps each letter of the alphabet to its reverse counterpart (A-Z, B-Y, etc.).\n\u2022 Phonetic Alphabet: Set of words used to represent letters in verbal communication to avoid confusion (A-Alpha, B-Bravo).\n\u2022 T9 Texting: Writing text on mobile phones keyboard by pushing 9 numerical buttons (A-2, B-22, C-222, D-3).\n\u2022 Base64: Encodes text into a base64 representation.\n\u2022 Binary: A numerical system that uses only two digits, 0 and 1, to represent data in computing.\n\u2022 Riddles: Present clues indirectly referring to a target word.\n\u2022 Python Code: Encode words by, e.g., running a function that transforms letters."}]}