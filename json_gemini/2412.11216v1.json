{"title": "Distribution-Consistency-Guided Multi-modal Hashing", "authors": ["Jin-Yu Liu", "Xian-Ling Mao", "Tian-Yi Che", "Rong-Cheng Tu"], "abstract": "Multi-modal hashing methods have gained popularity due to their fast speed and low storage requirements. Among them, the supervised methods demonstrate better performance by utilizing labels as supervisory signals compared with unsupervised methods. Currently, for almost all supervised multi-modal hashing methods, there is a hidden assumption that training sets have no noisy labels. However, labels are often annotated incorrectly due to manual labeling in real-world scenarios, which will greatly harm the retrieval performance. To address this issue, we first discover a significant distribution consistency pattern through experiments, i.e., the 1-0 distribution of the presence or absence of each category in the label is consistent with the high-low distribution of similarity scores of the hash codes relative to category centers. Then, inspired by this pattern, we propose a novel Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to filter and reconstruct noisy labels to enhance retrieval performance. Specifically, the proposed method first randomly initializes several category centers, each representing the region's centroid of its respective category, which are used to compute the high-low distribution of similarity scores; Noisy and clean labels are then separately filtered out via the discovered distribution consistency pattern to mitigate the impact of noisy labels; Subsequently, a correction strategy, which is indirectly designed via the distribution consistency pattern, is applied to the filtered noisy labels, correcting high-confidence ones while treating low-confidence ones as unlabeled for unsupervised learning, thereby further enhancing the model's performance. Extensive experiments on three widely used datasets demonstrate the superiority of the proposed method compared to state-of-the-art baselines in multi-modal retrieval tasks.", "sections": [{"title": "Introduction", "content": "With the rapid growth of multimedia data such as images, text, and videos, achieving effective retrieval from massive multi-modal data has become a significant challenge. To address this challenge, numerous information retrieval technologies have emerged, with multi-modal hashing methods gaining widespread attention for their fast retrieval speed and low storage requirements. Unlike uni-modal hashing and cross-modal hashing , multi-modal hashing maps data points from different modalities into a unified Hamming space for fusion, resulting in binary hash codes that facilitate efficient multi-multi retrieval. Compared to unsupervised methods, supervised multi-modal hashing methods generate more discriminative hash codes and achieve more accurate retrieval by utilizing labels as supervisory signals and there is a hidden assumption that training sets have no noisy labels for these supervised methods.\nHowever, in real-world scenarios, labels may be incorrectly annotated due to manual labeling, such as an image that should be labeled as \"tiger\" being mistakenly labeled as \"cat\", which limits the applicability of existing supervised hashing methods in noisy label scenarios. While some works in image hashing and cross-modal hashing have demonstrated that the presence of noisy labels in the training set can lead to model overfitting, resulting in indistinguishable hash codes and inaccurate retrieval, no work has yet focused on and resolved this issue in the field of multi-modal hashing.\nTo effectively tackle this issue, it is crucial to filter out noisy labels from the dataset. Existing single or cross-modal hashing methods have demonstrated that models initially learn effective hash mappings from clean labels but eventually overfit to noisy labels, resulting in degraded performance. This phenomenon suggests that even after a short training period, the generated hash codes are somewhat discriminative and tend to align with their corresponding category centers, as indicated by higher similarity scores. Thus, the 1-0 distribution of the category's presence or absence, i.e., the label vector, should be consistent with the high-low distribution of similarity scores between the hash codes and category centers. Specifically, if an instance belongs to a category, the corre-"}, {"title": "Related Work", "content": "In multi-modal hashing, supervised methods utilize labels as supervisory information to help models learn richer semantic information, and can be categorized into shallow and deep approaches based on whether deep networks are utilized. Shallow supervised multi-modal hashing often relies on linear mapping or matrix factorization to model the latent semantic associations between modalities. For instance, OMHDQ links hash code learning with both low-level data distribution and high-level semantic distribution based on paired semantic labels. SAPMH employs paired semantic labels as parameter-free supervisory information to learn multidimensional latent representations. In contrast, deep supervised multi-modal hashing leverages deep networks to integrate feature extraction and hash code learning into a unified deep framework. For example, BSTH introduces a bit-aware semantic transformation module to achieve fine-grained, concept-level alignment and fusion of multi-modal data, thereby generating high-quality hash codes. STBMH addresses the issue of similarity transitivity broken in multi-label scenarios by designing an additional regularization term.\nFor almost all supervised multi-modal hashing methods, there is a hidden assumption that training sets have no noisy labels. However, in real-world scenarios, the labels are often annotated incorrectly due to manual labeling which greatly"}, {"title": "Noisy Label Learning", "content": "Noisy label learning has been extensively studied in tasks such as image classification. Existing approaches to handling noisy labels can be broadly categorized into two types: noise-robust modeling and label-noise cleaning. Noise-robust modeling involves directly training robust models on noisy labels using noise-specific loss functions or regularization terms; for instance, NCR designs a regularization loss term based on the consistency between an instance and its neighboring nodes in the feature space. In contrast, label-noise cleaning focuses on filtering or correcting noisy labels directly, exemplified by FCF , which introduces a fusion cleaning framework that combines correction and filtering to address different types of noisy labels.\nIn the context of hashing, there have been works addressing noisy labels in image hashing and cross-modal hashing by using loss-based methods. For instance, DIOR uses the concept of behavior similarity between original and augmented views to filter noisy labels, while CMMQ designs a proxy-based contrastive loss to mitigate the impact of noisy labels. However, no existing work addresses the issue of noisy labels in multi-modal hashing retrieval. Based on the discovered distribution consistency pattern, we propose a novel approach called DCGMH to filter and reutilize noisy labels, improving the robustness of the hashing model. The specific details of this approach will be discussed in the next section."}, {"title": "The Proposed Method", "content": "In the section, we first describe the problem definition and then provide detailed explanations of our proposed method's architecture, whose framework diagram is depicted in Figure 2. Subsequently, we summarize the objective function and optimize the model's training process. Finally, we demonstrate the out-of-sample extension."}, {"title": "Problem Definition", "content": "Similar to most existing multi-modal hashing methods, this work focuses on image-text datasets. Assuming that there is a dataset $\\mathcal{O}$ containing $n$ instances, denoted as $\\mathcal{O} = \\{o_i\\}_{i=1}^{n} = \\{(x_i, y_i, l_i)\\}_{i=1}^{n}$, where $x_i$ and $y_i$ represents the text and image modal data point, respectively. Moreover, $l_i \\in \\{0,1\\}^{m}$ represents the label vector of the instance $o_i$, where $m$ is the total number of categories, and when an instance $o_i$ belongs to the category $j$, $l_{ij} = 1$; otherwise, $l_{ij} = 0$. Furthermore, the similarity matrix $S \\in \\{-1,1\\}^{n \\times n}$ is employed to represent the similarity between instances, such that when instances $o_i$ and $o_j$ share at least one category, $S_{ij} = 1$, indicating they are similar; otherwise, $S_{ij} = -1$, indicating they are dissimilar. Additionally, the instances in dataset $\\mathcal{O}$ are ultimately mapped to hash codes"}, {"title": "Architecture", "content": "To generate high-quality hash codes, we use instance $o_i = \\{x_i, y_i, l_i\\}$ as the training data for the hashing network. Then, we employ a bag-of-words (BoW) model to extract the feature representation $f_x$ and a VGG model without the final classification layer to extract the feature representation $f_y$ for the text and image modal data, respectively. Subsequently, modality-specific multi-layer perceptrons (MLP) are utilized to map the feature representation $f^*$, $* \\in \\{x,y\\}$ of each modality to a unified space, denoted as:\n$u^* = MLP^*(f^*; \\theta^*)$  (1)\nwhere $u^*$ is the resulting projected feature representation and $\\theta^*$ denotes the learnable parameters of the MLP for the respective modality. Finally, the projected features from different modalities are fused by directly summing them, and this fused representation is then passed through a hashing function with a non-linear activation function (tanh($\\cdot$)) to generate the fused hash code $b_i$, denoted as:\n$b_i = H(u^x + u^y; \\Theta)$  (2)\nwhere $H$ refers to the hash function and $\\Theta$ is a set of learnable parameters. Finally, the final binary hash code $b_i$ for instance $o_i$ can be obtained as $b_i = sgn(\\hat{b}_i)$, where sgn($\\cdot$) is a function that maps positive values to 1 and negative values to -1. In summary, for instance $o_i$, its binary hash code can be formulated as $b_i = sgn(F(x_i, y_i; P))$, where $P$ is a set of learnable parameters.\nTo filter noisy labels via our discovered distribution consistency pattern, we first randomly initialize several category centers $C \\in \\mathbb{R}^{m \\times k}$, i.e., $C = \\{c_j\\}_{j=1}^{m}$, where $m$ is the number of categories and $k$ is the length of hash code, ensuring that each category occupies a distinct and non-overlapping region. Then for each instance's hash code $b_i$, we calculate its similarity scores $d_{ij}$ with each category center $c_j$, denoted as:\n$d_{ij} = \\frac{c_j^T b_i}{||c_j|| ||b_i||}$ (3)\nBy aggregating the similarity scores, we obtain the similarity scores matrix $D \\in \\mathbb{R}^{n \\times m}$ which captures the similarity of hash code to each category center. In this matrix, when a hash code $b_i$ belongs to a particular category $c_j$, the corresponding similarity score $d_{ij}$ tends to be high; otherwise, it is relatively low. Next, based on the consistency pattern between the 1-0 distribution of labels and the high-low distribution of similarity scores, we calculate the consistency level $T = \\{t_i\\}_{i=1}^{n}$ between the label distribution $l_i$ and similarity distribution $d_i$. We then design a consistency-based"}, {"title": "criterion to filter the dataset", "content": "$\\mathcal{O}$ into the clean label set $\\mathcal{O}_c$ and noisy label set $\\mathcal{O}_n$, which can be formulated as:\n$t_i = \\frac{\\sum_{j=1}^{m} l_{ij}d_{ij}}{\\sum_{j=1}^{m} d_{ij}}$ (4)\n$\\mathcal{O}_c = \\{(x_i, y_i, l_i) | t_i > \\epsilon(\\mathcal{T})\\}$ (5)\n$\\mathcal{O}_n = \\{(x_i, y_i, l_i) | t_i \\le \\epsilon(\\mathcal{T})\\}$ (6)\nwhere $t_i$ is employed to measure the degree of consistency between the label distribution and similarity distribution, with higher values indicating greater consistency, $\\tau$ represents the noise ratio and $\\epsilon(\\mathcal{T})$ denotes the filtering threshold to ensure $\\tau n$ instances are identified as noisy label instances.\nTo accurately learn the semantic knowledge and associations between labels and hash codes in the subsequent steps, we reconstruct the labels in $\\mathcal{O}_c$ and $\\mathcal{O}_n$. On the one hand, since the labels in $\\mathcal{O}_c$ are clean, we directly use the initial labels as the reconstructed labels and adopt a standard measure in multi-modal hashing to establish semantic associations between hash codes and labels to learn implicit knowledge. On the other hand, for instances in $\\mathcal{O}_n$, recognizing the significant advantage of labels as guiding information, we design a corrector via distribution consistency pattern to correct high-confidence noisy labels. Specifically, we treat the clean label set $\\mathcal{O}_c$ as a knowledge base, and for a given instance $o_i = \\{x_i, y_i, l_i\\}$ in the noisy label set $\\mathcal{O}_n$, we identify the two instances $o_j = \\{x_j, y_j, l_j\\}$ and $o_k = \\{x_k, y_k, l_k\\}$ from $\\mathcal{O}_c$ whose distribution of similarity scores $d_j$ and $d_k$ most closely match that of $o_i$. Here if the labels $l_j$ and $l_k$ of $o_j$ and $o_k$ are consistent, we infer that $o_i$ has a high confidence of sharing the same label; otherwise, we consider $o_i$ as having low confidence and treat it as unlabeled data. This corrector can be represented as follows:\n$m_i = d_i (D_c)^T$ (7)\n$\\begin{aligned}\\{o_j, o_k\\} = arg \\underset{o_j, o_k \\in O_c \\atop l_j = l_k}{\\text{min}} (rank(m_{i, j}) + rank(m_{i, k}))\\end{aligned}$ (8)\n$l_i = \\begin{cases}\n  l_j, & \\text{if } \\{o_j, o_k\\} \\text{ exist;} \\\\\n  None, & \\text{otherwise.}\n\\end{cases}$ (9)\nwhere $D_c$ is the similarity scores matrix of clean label set $\\mathcal{O}_c$, $m_i$ represents the level of consistency between the similarity distribution of the noisy label instance $o_i$ and each instance in the clean label set $\\mathcal{O}_c$, and rank($\\cdot$) indicates the ranking of consistency levels. As for low-confidence noisy labels, we discard the original labels and treat them as unlabeled. Consequently, the noisy label set $\\mathcal{O}_n$ is further divided into a corrected label set $\\mathcal{O}_r$ and an unlabeled set $\\mathcal{O}_u$."}, {"title": "Objective Function and Optimization", "content": "To ensure that the generated fused hash codes accurately reflect label semantics, inspired by STBMH , we design the following pointwise loss on the clean label set $\\mathcal{O}_c$ to make the hash codes as close as possible to their corresponding categories while keeping them distant from non-relevant categories, denoted as:\n$\\mathcal{L}_o = \\frac{1}{\\mathcal{N}_c} \\sum_{i}^{n_c} \\sum_{j \\in V_i} log (\\frac{exp(b_i^T c_j)}{\\sum_{h \\in N_i} exp(b_i^T c_h)})$ (10)\nwhere $k$ is the length of hash code, $n_c$ is the number of instances in $\\mathcal{O}_c$, $V_i$ contains the indices of all categories to which instance $o_i$ belongs and $N_i$ contains the indices of categories to which it does not belong. By minimizing $\\mathcal{L}_o$, the value of $exp(b_i^T c_j)$ becomes significantly higher than that of $\\sum_{h \\in N_i} exp(b_i^T c_h)$, indicating that the similarity be-\nhigh, while the similarity with non-relevant category centers is low, thereby achieving the desired objective.\nMeanwhile, for the corrected label set $\\mathcal{O}_r$, since the labels within still have the potential for error correction, we adopt a pairwise loss to emphasize the relative similarity relationships among instances as a whole, rather than direct associations between instances and specific individual categories, expressed as follows:\n$\\mathcal{L}_a = \\sum_{i}^{\\mathcal{N}_r} \\sum_{j}^{\\mathcal{N}_r} ||cos(b_i, b_j) - S_{ij}||$ (11)\n$cos(b_i, b_j) = \\frac{b_i^T b_j}{||b_i|| ||b_j||}$ (12)\nwhere $k$ is the length of hash code, $n_r$ is the number of instances in $\\mathcal{O}_r$, cos($\\cdot$,$\\cdot$) is employed to measure the cosine similarity between hash codes and $s_{ij}$ represents the pairwise similarity defined by the labels. By minimizing $\\mathcal{L}_a$, the cosine similarity between hash codes of similar instances defined by the labels will approach 1 and their Hamming distance $d_H(b_i, b_j)$ will decrease, where $d_H(b_i, b_j) = \\frac{1}{4k}||b_i - b_j||^2$.\nFor the unlabeled set $\\mathcal{O}_u$, given that text and image data points inherently contain rich semantic knowledge, we employ unsupervised contrastive learning to uncover hidden semantic relationships and further enhance model performance. Specifically, for an instance $o_i$ in $\\mathcal{O}_u$, we first generate an augmented instance $\\tilde{o}_i$ through data augmentation. Then due to $o_i$ and $\\tilde{o}_i$ embody the same semantic meaning, their corresponding hash codes should be as consistent as possible. Therefore, we design the following contrastive loss to minimize the distance between hash code $b_i$ and $\\tilde{b}_i$:\n$\\mathcal{L}_u = \\frac{1}{\\mathcal{N}_u} \\sum_{i=1}^{\\mathcal{N}_u} (1 - \\tilde{s}_{ii}) + \\frac{1}{\\mathcal{N}_u} \\sum_{i=1}^{\\mathcal{N}_u} \\sum_{j=1, j \\ne i}^{\\mathcal{N}_u} max(0, \\tilde{s}_{ij} - \\epsilon)$ (13)\nwhere $\\tilde{s}_{ij} = cos(\\tilde{b}_i, \\tilde{b}_j)$ i.e. cosine similarity of hash codes, $n_u$ is the number of instances in $\\mathcal{O}_u$ and $\\epsilon$ is the threshold that restricts the similarity between different instance pairs. By minimizing $\\mathcal{L}_u$, the similarity between the same instance pairs will approach 1, while the similarity between different instance pairs will be less than the threshold $\\epsilon$, thus ensuring that more similar instances are mapped to closer hash codes and achieving the goal of extracting the inherent semantics of the instance through unsupervised learning.\nIn addition, each category should occupy a distinct region without interference. To achieve this, the centers of the categories must be well-separated, which lead us to design the following center loss:\n$d_{ij} = ||c_i - c_j||$ (14)\n$\\mathcal{L}_c = \\frac{1}{\\{d_{ij} < \\epsilon| i < j\\}} \\sum_{i<j} d_{ij} - min_{i<j} d_{ij}$ (15)\nwhere $d_{ij}$ represents the distance between two category centers and $i  j indicates that only the upper triangular region is considered. By minimizing $\\mathcal{L}_c$, sufficient separation between different categories is achieved which enhances the model's ability to distinguish them. Furthermore, as the model ultimately relies on the sgn() function to convert the fused hash codes into binary hash codes, we introduce the following loss to control the quantization loss incurred during this process:\n$\\mathcal{L}_q = \\sum_{i}^{n} ||b_i - \\hat{b}_i||$ (16)\nFinally, by combining the losses from the above components, the overall objective function of the multi-modal hashing network is defined as:\n$\\mathcal{L} = \\mathcal{L}_o + \\alpha \\mathcal{L}_a + \\beta \\mathcal{L}_u + \\gamma \\mathcal{L}_c + \\eta \\mathcal{L}_q$ (17)\nwhere $\\alpha$, $\\beta$, $\\gamma$ and $\\eta$ are hyperparameters. By minimizing this loss during the training of the whole multi-modal hashing network, the model can generate high-quality and distinguishable hash codes even in the presence of noisy labels, which significantly enhances its performance. The detailed algorithm is listed in the Algorithm 1."}, {"title": "Out-of-Sample Extension", "content": "After optimizing the multi-modal hashing network by minimizing $\\mathcal{L}$, we can use the model to generate hash codes for instances outside the training set. Specifically, for a given unseen instance $o_i = \\{x_i, y_i, l_i\\}$, we obtain the binary hash"}, {"title": "Experiments", "content": "To validate the effectiveness of our proposed method, we conduct experiments on three widely used image-text benchmark datasets: MIR Flickr , NUS-WIDE , and MS COCO . Among the datasets, the MIR Flickr dataset, which includes 24 labels, contains 20,015 data pairs, with each text represented as a 1,386-dimensional BoW vector. We partition this dataset into a testing set of 2,243 pairs, a retrieval set of 17,772 pairs, and a training set of 5,000 pairs. The NUS-WIDE dataset, featuring 21 common labels, includes 195,834 pairs with text represented by 1,000-dimensional BoW vectors. We derive a testing set of 2,085 pairs, a retrieval set of 193,749 pairs, and a training set of 21,000 pairs. For MS COCO, which includes 80 labels, we select 5,981 pairs for testing, 82,783 pairs for retrieval, and 18,000 pairs for training.\nGiven that the labels in these datasets are originally correctly annotated, we introduce noise to better simulate the performance of the proposed method in real-world scenarios with noisy labels. Specifically, when setting the noisy label ratio to w, we randomly select w proportion of training instances as noisy label instances and averagely categorize them into four types to accurately simulate real-world scenarios: (1) maintaining the number of categories that belong and including some original categories; (2) maintaining the number of categories that belong but excluding the original categories; (3) changing the number of categories that belong while including some original categories; and (4) changing the number of categories that belong and excluding the original categories.\nTo evaluate the superiority of our proposed method, we compare it with ten state-of-the-art multi-modal hashing methods, i.e., FOMH , OMHDQ"}, {"title": "Implementation Details", "content": "Similar to DIOR , we first perform warm-up training without noisy label filtering and correction to allow the model to learn the basic hashing mapping capabilities, with the warm-up epochs set to 5, 5, and 30 for the MIR Flickr, NUS-WIDE, and MS COCO respectively. During the training of the hashing network on a single NVIDIA RTX 3090Ti GPU, the SGD optimizer with a batch size of 48 is adopted for parameter optimization, with an initial learning rate set to 0.005, 0.001, and 0.01 for the three datasets, respectively. Both the image and text modalities employ MLP architectures consisting of two linear projection layers ($d_m \\rightarrow 4,096 \\rightarrow 128k$), where $d_m$ is the feature dimension of the modality data points and k is the length of the hash codes. Regarding hyper-parameters, $\\alpha$ and $\\beta$ are set to 1, 1.5, 1.2 and 0.15, 0.05, 0.2 for the three datasets, respectively. $\\gamma$ and $\\eta$ are empirically set to 5 and 1 for all three datasets, respectively, which will be discussed later. Additionally, all experimental results are averaged over three runs with different random seeds."}, {"title": "Evaluation Protocol", "content": "In our experiment, we evaluate the proposed model's efficiency and effectiveness using two protocols: Hamming ranking and hash lookup. For the Hamming ranking protocol, which relies on Hamming distances, we use Mean Average Precision (MAP) and Precision curves at different top N results (P@N) to measure accuracy. Meanwhile, the hash lookup protocol's accuracy is assessed using the Precision-Recall (PR) curve within a specified Hamming radius. Furthermore, experiments are conducted across two sub-tasks: performance comparison at a fixed noisy label ratio, and performance variation with different noisy label ratios."}, {"title": "Experimental Results", "content": "To simulate the model's retrieval performance in real-world noisy label scenarios, we conduct experiments on three datasets assuming 40% noisy labels in the training set, with the MAP, P@N and PR curve comparison with all baselines shown in Table 1, Figure 3 and Figure 4.\nThe following conclusions can be drawn from the experimental results: (1) In the scenario with 40% noisy labels, our proposed method DCGMH achieves the best MAP performance compared to all baselines. Specifically, when compared to existing multi-modal hashing models that do not account for noisy labels, DCGMH provides average improvements of at least 4.9%, 6.7%, and 9.3% on the MIR Flickr, NUS-WIDE, and MS COCO, respectively. Additionally, DCGMH significantly outperforms the DIOR model designed to handle noisy labels. This demonstrates DCGMH's effectiveness in generating high-quality hash codes in noisy label scenarios. (2) DCGMH consistently obtains the highest P@N scores, which suggests that DCGMH excels at retrieving the most relevant results more quickly and accurately when in scenarios with noisy labels. The superior P@N performance further emphasizes the model's robustness and effectiveness in mitigating the adverse effects of noisy labels, enhancing the multi-modal hashing model's retrieval performance. (3) The PR curve of DCGMH outperforms all baselines, showing that the hash codes gener-"}, {"title": "Performance Variation with Different Noisy Label Ratios", "content": "To evaluate how different noisy label ratios affect model performance, we examine the MAP metric under 64-bit hash codes as the noisy label ratio varied within the range of [10%, 30%, 50%, 70%, 90%]. The experimental results are depicted in Figure 5, from which we can obtain the following observations: (1) Our proposed method DCGMH consistently achieves the best retrieval performance across nearly all noisy label ratios, which demonstrates that DCGMH effectively mitigates the negative impact of noisy labels in the training set and showcases its exceptional robustness. (2) As the noisy label ratio increases, DCGMH shows the slowest performance decline compared to other models, indicating its lower sensitivity to noisy labels. Even with a high proportion of noisy labels, DCGMH can effectively filter and correct them to maintain superior performance. (3) Notably, when the noisy label ratio reaches 90%, the model's performance drops sharply. This decline"}, {"title": "Ablation Study", "content": "To evaluate the effectiveness of each module design and filtering strategy, we define the following five variants: DCGMH-I is trained directly on the original dataset without applying any noise label filtering or reconstruction; DCGMH-R treats all noisy labels as unlabeled for unsupervised learning, without correcting high-confidence noisy labels; DCGMH-U directly discards low-confidence noisy labels; DCGMH-RU trains solely on clean labels and discards all filtered noisy labels; DCGMH-D filter noisy labels via the difference in loss between different augmented views like DIOR.\nThe experimental results are shown in Table 2, where the training set has 40% noisy labels. From these results, we can draw the following conclusions: (1) Compared to DCGMH-I trained directly with noisy labels, both DCGMH and the other variants show substantial performance improvements, highlighting the critical role of the noisy label filtering module in preventing model overfitting and enhancing retrieval performance. (2) Among DCGMH-R, DCGMH-U, DCGMH-RU, and DCGMH, the latter achieves the best retrieval performance, indicating that both the noisy label correction module and unsupervised semantic learning module contribute to enhancing the hash model's ability to capture richer semantic information and generate more precise hash codes. (3) Compared to DCGMH-D, DCGMH deliv-"}, {"title": "Sensitivity to Hyper-parameters", "content": "Taking the MIR Flickr dataset as an example, we explore the impact of different values of the hyperparameters $\\alpha$, $\\beta$, $\\gamma$, and $\\eta$ on model performance with the hash code length of 64 bits and the noisy label ratio of 40%. The experimental results are shown in Figure 6, where the $\\beta$ value varies between 0.01 and 0.2 based on empirical settings, while the $\\alpha$ and $\\eta$ values range from 0.5 to 1.5, and the $\\gamma$ value ranges from 1 to 9. The results indicate that as the hyperparameter values increase, the model's performance initially improves and then declines. Moreover, regardless of the values selected within the range, the performance consistently surpasses that of existing state-of-the-art multi-modal hashing models. To achieve optimal performance, we set the values of $\\alpha$, $\\beta$, $\\gamma$, and $\\eta$ to 1, 0.15, 5, and 1, respectively, in the experiments. Similar trends are observed on the NUS-WIDE and COCO, where the corresponding hyperparameters are set to 1.5, 0.05, 5, 1, and 1.2, 0.2, 5, 1, respectively."}, {"title": "Conclusion", "content": "In this paper, we propose a novel Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which can filter out noisy labels via our discovered consistency pattern between the 1-0 distribution of labels and the high-low distribution of similarity scores and simultaneously design a corrector to correct high-confidence noisy labels to generate"}]}