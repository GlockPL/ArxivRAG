{"title": "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "authors": ["Xianzhi Li", "Ran Zmigrod", "Zhiqiang Ma", "Xiaomo Liu", "Xiaodan Zhu"], "abstract": "Language models are capable of memorizing detailed patterns and information, leading to a double-edged effect: they achieve impressive modeling performance on downstream tasks with the stored knowledge but also raise significant privacy concerns. Traditional differential privacy based training approaches offer robust safeguards by employing a uniform noise distribution across all parameters. However, this overlooks the distinct sensitivities and contributions of individual parameters in privacy protection and often results in suboptimal models. To address these limitations, we propose ANADP, a novel algorithm that adaptively allocates additive noise based on the importance of model parameters. We demonstrate that ANADP narrows the performance gap between regular fine-tuning and traditional DP-SGD based fine-tuning on a series of datasets while maintaining the required privacy constraints.", "sections": [{"title": "Introduction", "content": "Language models have achieved remarkable success and shown impressive abilities in a wide range of tasks (Almazrouei et al., 2023; Touvron et al., 2023; Team et al., 2023). Their advanced capabilities in memorizing detailed information and patterns in data as well as making connections among them have not only helped language models to achieve impressive modeling performance on downstream tasks, but also raised significant and ubiquitous privacy concerns if it is not properly handled (Neel and Chang, 2023; Mireshghallah et al., 2023; Yao et al., 2024).\nDifferential Privacy (DP) is a principled framework for mitigating privacy risks, providing theoretical guarantees that prevent inferring the presence or absence of an individual's data in a model's output (Abadi et al., 2016; Dwork, 2006). Conventional DP-enhanced fine-tuning offers robust safeguards by assuming a uniform noise distribution across all parameters to protect privacy (Kerrigan et al., 2020; Yu et al., 2021b; Li et al., 2021). Recent work has begun exploring DP in Parameter Efficient Fine Tuning (PEFT) (Yu et al., 2021a; Bu et al., 2022), which is built on the same assumption on the additional tunable parameters. Unfortunately, such an assumption overlooks the distinct sensitivities and contributions of individual parameters in privacy protection and often results in suboptimal models.\nIn this paper, we introduce ANADP, a novel DP method that adaptively distributes the noise and privacy budget among a language model's parameters during fine-tuning, based on their importance to the model at a given training step. Our work was inspired by Zhang et al. (2023), who utilized the sensitivity and uncertainty of parameters for model pruning. Importantly, our approach not only respects the inherent heterogeneity of parameter significance but also maintains strong privacy protection. The proposed integration addresses the key challenges in effectively measuring the contributions of parameters and ensures that models are trained stably. We demonstrate that ANADP consistently improves performance over the traditional DP fine-tuning under the same privacy budget and bridges the gap between traditional DP and non-DP fine-tuning (no privacy guarantee). The contributions of our work are summarized below:\n\u2022 We propose ANADP, a novel algorithm for fine-tuning language models while maintaining privacy guarantees. To the best of our knowledge, this is the first DP method that distributes the privacy budget based on Transformer parameters' importance non-uniformly.\n\u2022 We empirically demonstrate that ANADP outperforms the standard DP approaches on the Glue benchmark (Wang et al., 2018) in multiple training paradigms (e.g. both full fine-tuning and PEFT)."}, {"title": "Related Work", "content": "Differential Privacy. DP is a principled approach to ensuring privacy. The concept of DP was formalized by Dwork (2006), who introduced the definition and foundational mechanisms of DP. In machine learning, Abadi et al. (2016) introduced the widely used Differentially Private Stochastic Gradient Descent (DP-SGD). Adaptive Differential Privacy is a recent development. Research (Gong et al., 2020; Chen et al., 2023) have been developed to preserve adaptive DP in deep neural networks. However, these methods fall short in capturing the complex parameter interactions within transformers, potentially leading to suboptimal models and trade-offs between privacy and utility.\nFine-Tuning and PEFT. Full fine-tuning used to be a prominent approach but can be resource-intensive and less efficient (Lester et al., 2021; Tay et al., 2022). PEFT has emerged as another option for effectively training LLMs. Many PEFT techniques, such as LoRA (Hu et al., 2021), Adapters (Houlsby et al., 2019), and prefix tuning (Li and Liang, 2021) have been proposed to tune small, additional modules instead of the whole model. He et al. (2021) provided a unified view revealing the connections among various parameter-efficient transfer learning methods. Recent work by (Zhang et al., 2023) introduced AdaLoRA to dynamically adjust the amount of parameter tuning based on the task and model requirements. Despite their efficiency, integrating these methods with privacy-preserving techniques remains an area that requires further exploration."}, {"title": "The ANADP Model", "content": "The integration of Differential Privacy for language model fine-tuning is crucial for deploying LLMs in privacy-sensitive applications. In this work, we introduce ANADP, an adaptive noise allocation DP training method based on the importance score of models' parameters, which provides a generic solution that can be applied to a wide range of LLMs. The fundamental idea is that adding less noise to the parameters that are more important and more to the less important parameters would help improve the model's utility given the same privacy budget. This section describes the construction and correctness of ANADP, whose pseudocode is given in Alg. 1.\nWe follow Dwork (2006)'s definition of DP. Specifically, we achieve DP through a randomized algorithm A over an output space S. Given a privacy budget \u03f5 and error probability \u03b4, we say A is (\u03f5, \u03b4)-differentially private ((\u03f5, \u03b4)-DP) if for any neighboring datasets D and D', which differ in exactly one data record, the following inequality holds:\nPr [A(D) \u2208 S] < e^{\u03f5} Pr [A(D') \u2208 S] + \u03b4  (1)\nwhere privacy budget \u03f5 is a measure of the amount of privacy loss allowed during training. Past methods for achieving (\u03f5, \u03b4)-DP typically add a uniform Gaussian noise to the parameters. More formally, given a batch L, we can define adding Gaussian noise to the model's gradient, g(L) as:\n$\\tilde{g}(L) = \\min (g(L), C) + \\mathcal{N}(Co^{2})$ (2)\nwhere C is a clipping threshold and $\\mathcal{N}(Co^{2})$ is Gaussian noise with mean 0 and variance $Co^{2}$. C and \u03c3 are fixed and computed based on the privacy budget (Abadi et al., 2016). In this work, we explore a tunable \u03c3 to realize a better trade-off between privacy and utility. We aim to tailor the noise"}, {"title": "Parameter Importance", "content": "distribution across different parameters and the key objective is to determine the importance of each parameter.\nParameter Importance. In order to gauge parameters' importance, our work is inspired by Zhang et al. (2023), which calculates importance based on the sensitivity and uncertainty of the parameter for model pruning. We use the moving averages of the sensitivity and uncertainty of the model parameters at training step t:\n$S_{t} = \\beta_{1}S_{t-1} + (1 - \\beta_{1})S_{t}$ (3)\n$U_{t} = \\beta_{2}U_{t-1} + (1 - \\beta_{2})|S_{t} - S_{t}|$ (4)\nwhere \u03b21, \u03b22 \u2208 [0,1] are hyper-parameters to control the move average rate. Additionally, $S_{t} = |g(L_{t}) \\cdot w_{t-1}|$ is the sensitivity of the model weights at step t. The importance metric is then the element-wise product of the sensitivity and uncertainty\n$I_{t} = S_{t} * U_{t}$. (5)\nThis formulation ensures that parameters with moderate sensitivity but high uncertainty are still considered important, which prevents prematurely discarding parameters that could become important as training progresses.\nImportance Normalization. Using $I_{t}$ as defined in Eq. 5 may lead to zero-gradients. Therefore, we smooth the importance scores\n$\\tilde{I_{t}} = (1 - \\alpha)(\\frac{I_{t} - median(I_{t})}{q_{1}(I_{t}) - q_{2}(I_{t})}) + \\alpha \\mu$ (6)\nwhere \u03b1 \u2208 [0, 1] is a smoothing parameter, $q_{1}(I_{t})$ and $q_{2}(I_{t})$ are chosen quantiles of $I_{t}$, and \u00b5 is the mean of the scaled normalized vector.\n$\\tilde{I_{t}}$ gives a scaled distribution of importance across the model's parameters. In order to ensure (\u03f5, \u03b4)-DP, we further adjust distribution to be centered at one\n$I_{t} = \\tilde{I_{t}} - (mean(\\tilde{I_{t}}) - 1)$. (7)\nThis means that the overall noise added to the model will follow the same distribution as that of Abadi et al. (2016) who uses a uniform noise across all parameters. As the overall noise added is the same, ANADP satisfies the (\u03f5, \u03b4)-DP guarantees"}, {"title": "Experimental Setup", "content": "We evaluate the performance of ANADP against the traditional DP-SGD method (Abadi et al., 2016), DP-PEFT method (Yu et al., 2021a) as well as regular fine-tuning (i.e., no privacy guarantees). Same as in previous work (Wu et al., 2023; Yu et al., 2021a), we run our three privacy configurations using RoBERTa (base and large) (Liu et al., 2019) in the full fine-tuning setting as well as on two state-of-the-art PEFT methods: LORA (Hu et al., 2021) and Adapter (Houlsby et al., 2019). Each of the combinations above is evaluated against four datasets from the Glue benchmark (Wang et al., 2018) which is used in past work to evaluate conventional DP-SGD: SST-2 (Socher et al., 2013), QNLI (Rajpurkar et al., 2016), MNLI (Williams et al., 2018), and QQP.\nWe further conduct privacy protection experiments. Our experiments follow those of (Wu et al., 2023); we train a model using contexts containing private information (the Enron email dataset (Klimt and Yang, 2004)), and then compute the leakage risk of the privacy information. The Enron email dataset is comprised of over 500,000 emails that contain sensitive information such as person names and phone numbers Specifically, we use the Mean Reciprocal Rank (MRR) for person name and exposure (Carlini et al., 2019) metric for telephone numbers to show the risk of privacy leakage."}, {"title": "Accuracy Results", "content": "The performance of ANADP in comparison to past DP methods and regular training is given in Table 1. Introducing privacy protection inevitably leads to performance degradation. Nevertheless, we observe that ANADP consistently outperforms"}, {"title": "Exposure Risks Results", "content": "Our exposure experiments seek to assess the risk of privacy leakage empirically, particularly focusing on sensitive information such as person names and telephone numbers. Such an experiment is important as concerns have previously been raised on whether DP guarantees adhere to the allocated privacy budget (Steinke et al., 2024). This discrepancy can occur due to various factors, ranging from theoretical assumptions not holding in practice to statistical variations and implementation bugs.\nFigure 1 shows that ANADP maintains the same level of privacy protection as the conventional DP methods on the Enron email dataset (Klimt and Yang, 2004), without statistically significant difference between them. The exposure risk values show a substantial reduction compared to those"}, {"title": "ANADP Noise Distribution", "content": "Figure 2 shows the detailed distribution of noise multipliers applied via ANADP when tuning RoBERTa-base on the SST-2 task. ANADP demonstrates a strategic pattern in noise allocation, consistently applying lower noise levels to more critical parameters. Notably, the lower and final layers of the model often receive reduced noise. This could suggest that initial layers, responsible for capturing basic linguistic features, and final layers, which fine-tune these features into task-specific outputs, are deemed more sensitive to noise disruption. This pattern supports the hypothesis that maintaining the integrity of these parameters is crucial for preserving the model's overall performance.\nIn contrast, ANADP strategically assigns higher"}, {"title": "Conclusion", "content": "This paper introduces ANADP, a novel approach to integrating DP with language model fine-tuning in both the full fine-tuning and PEFT settings and dynamically adjusting the noise added to the gradients, based on measuring model parameters' importance. We demonstrated that under the same privacy budget, ANADP consistently outperforms the standard DP-SGD training on different benchmark datasets. While performance degradation remains between our method and non-DP training, we achieved consistent reduction of the gap, in both the fine-tuning and PEFT settings. Our additional exposure risk analysis shows that ANADP provides privacy protection comparable to the standard DP-SGD training. We hope this work enables better deployment of privacy-preserving language models and encourages future research on adaptive DP for language model training."}, {"title": "Limitations", "content": "While ANADP offers consistent improvements, there are certain limitations that present opportunities for future work. First, although our method effectively identifies important parameters for downstream tasks and allocates noise accordingly, it does not explicitly distinguish whether these parameters are also privacy-sensitive. Identifying privacy-related parameters during the training process could be a crucial research problem. Moreover, developing an automated method to normalize noise would significantly streamline the application of ANADP."}, {"title": "Appendix A", "content": "Training Details. Following prior work in model privacy, we conduct our training using RoBERTa (on both the base and large version) (Liu et al., 2019). ROBERTa-base has 12 transformer layers, a hidden state size of 768, and a feedforward network (FFN) with an internal hidden size of 3072. ROBERTa-large is configured with 24 transformer layers, enhancing its complexity. In LoRA fine-tuning, we followed Yu et al. (2021a) where we incorporated bottleneck branches in both the attention layers and the feedforward layers. This approach differs slightly from the method used by Hu et al. (2021), who only added bottleneck branches to the query and values matrices within the attention layers. For the two types of PEFT methods, we choose the same rank 16 for all the experiments. For DP experiments, we use e=8, C=10, \u0431=1e-5 for SST-2, QNLI and 8=1e-6 for MNLI, QQP dataset. We run 50 epochs on all datasets and report the best validation accuracy. All experiments were conducted using NVIDIA A100 GPUs."}]}