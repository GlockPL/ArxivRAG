{"title": "ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software", "authors": ["Xiang Mei", "Pulkit Singh Singaria", "Jordi Del Castillo", "Haoran Xi", "Abdelouahab (Habs) Benchikh", "Tiffany Bao", "Ruoyu Wang", "Yan Shoshitaishvili", "Adam Doup\u00e9", "Hammond Pearce", "Brendan Dolan-Gavitt"], "abstract": "High-quality datasets of real-world vulnerabilities are enormously valuable for downstream research in software security, but existing datasets are typically small, require extensive manual effort to update, and are missing crucial features that such research needs. In this paper, we introduce ARVO: an Atlas of Reproducible Vulnerabilities in Open-source software. By sourcing vulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and implementing a reliable re-compilation system, we successfully reproduce more than 5,000 memory vulnerabilities across over 250 projects, each with a triggering input, the canonical developer-written patch for fixing the vulnerability, and the ability to automatically rebuild the project from source and run it at its vulnerable and patched revisions. Moreover, our dataset can be automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to grow over time. We provide a thorough characterization of the ARVO dataset, show that it can locate fixes more accurately than Google's own OSV reproduction effort, and demonstrate its value for future research through two case studies: firstly evaluating real-world LLM-based vulnerability repair, and secondly identifying over 300 falsely patched (still-active) zero-day vulnerabilities from projects improperly labeled by OSS-Fuzz.", "sections": [{"title": "INTRODUCTION", "content": "Vulnerabilities in software are both common and damaging: in 2023 alone, more than 28,000 vulnerabilities were tracked by the National Vulnerability Database (NVD), 4,648 of which were classified as Critical severity (using CVSS V3 scores). Studying the nature of this growing world of vulnerabilities in software is critical, but doing so requires a vulnerability dataset.\nHowever, many existing vulnerability datasets, such as Common Vulnerabilities and Exposures (CVE) and the NVD, are designed to alert users about vulnerabilities in software so that system maintainers can verify affected versions and apply patches for known vulnerabilities in currently deployed software. Due to this focus, these datasets are not effective as a research vulnerability dataset.\nA research vulnerability dataset must consist of real-world vulnerabilities. To be maximally useful, each vulnerability must contain metadata that includes: the input that triggers the vulnerability, the source-code patch that fixes the vulnerability, and the ability to compile the vulnerable and patched versions of the original source code. All of this must be reproducible into the future.\nUnfortunately, creating such a research vulnerability dataset manually is difficult. For example, despite over 3,600 hours of human work into reproducing publicly reported CVEs, Mu et al. [20] only succeeded in reproducing a total of 368 vulnerabilities.\nAnother complexity is that, when used as an evaluation benchmark, research vulnerability datasets tend to become \u201cstale\u201d over time as researchers (perhaps unintentionally) tune their systems to achieve good benchmark scores on the research vulnerability dataset without necessarily improving their real-world performance. Therefore, a continuously updated research vulnerability dataset is critical.\nIn this paper, we present ARVO, the 'Atlas of Reproducible Vulnerabilities.' ARVO is both a framework designed to address the shortage of research vulnerability dataset and a comprehensive bug dataset in its own right. Derived from Google's OSS-Fuzz project, ARVO aims to achieve a high level of reproducibility across a large number of real-world projects and vulnerabilities, providing a robust set of real-world vulnerabilities for a research vulnerability dataset. We focus on C/C++ projects due to their widespread use and the significant impact of bugs in these languages.\nThe ARVO dataset has the following key features: (1) large-scale: 5,001 vulnerabilities across 273 projects at the time of this writing; (2) recompilable: ARVO can rebuild the project at its vulnerable and patched versions for each vulnerability in the dataset; (3) triggering inputs: Each vulnerability has a proof-of-concept \"triggering\u201d input that can be used to test for the presence of the vulnerability; (4) precise fixes: For vulnerabilities in ARVO, we isolate and provide the precise developer patch that fixes the vulnerability; and (5) usable: We provide prebuilt container images for each vulnerability, allowing issues to be reproduced with a single command such as docker run -it -rm n132/arvo: 25402-vul arvo.\nThese features, as well as the fact that it can be continuously updated with new vulnerabilities with minimal manual effort, make the ARVO dataset well-suited as an on-going evaluation dataset for many downstream tasks in software security research, such as vulnerability discovery, fault localization, patch reduction, and automated program repair.\nARVO functions by ingesting bug reports from Google's OSS-Fuzz bug tracker [25], which provides findings from the continuous fuzzing of over 1,000 open-source projects. Unfortunately, the reports by themselves are not enough to reproduce each vulnerability: while they do provide the revision and triggering input found by the fuzzer, reliably building the target software and its dependencies at the vulnerable version, as well as locating the precise patch that fixes the issue, require significant additional effort.\nAlthough OSS-Fuzz does provide a tool\u00b9 for reproducing historical issues, it can only reproduce 13% of the issues we tested in OSS-Fuzz, particularly with older projects with many dependencies. As discussed in Section 4, ARVO achieves a much higher success rate (63%) by carefully tracking project dependencies and identifying the precise version needed, as well as mitigating the impact of missing assets (e.g., when a dependency or resource has been relocated or removed). With these reliable reproducing methods, ARVO located 5,001 patches over 5,651 vulnerabilities by bisection, and achieved 52.5% more cases than OSV-OSS-Fuzz [13], the current state-of-the-art approach. After rebuilding the project at the vulnerable version we also verified that the vulnerability can be reproduced and created public, pre-built Docker images for each issue, allowing it to be reproduced even if its dependencies succumb to bit rot.\nTo evaluate the accuracy of ARVO's patch identification, we conduct a comparison experiment in Section 5.3 between ARVO and Google's own OSS-Fuzz reproduction effort, which attempts to identify the commits that introduced and fixed each vulnerability in OSS-Fuzz; this data is automatically published to the OSV database [13]. For each issue, we check if the commit ARVO identifies matches the one listed by OSV, and manually investigate any discrepancies to decide which is correct, finding that ARVO achieves much better results than OSV for the cases where the two systems disagree over the overlapped cases.\nUsing ARVO, we uncovered several hundred cases of \"false positive fixes\", where issues marked as fixed by OSS-Fuzz could still be triggered on the most recent version of the project. This could allow malicious actors to harvest unfixed vulnerabilities, which the community does not realize are still extant, from OSS-Fuzz's public issue tracker.\nFinally, in Section 6 we offer several use cases to demonstrate how ARVO dataset can benefit research in software security, by using it to conduct evaluations of LLM-based vulnerability repair and to characterize real-world vulnerability fixes based on the developers' patches.\nIn summary, this paper makes the following contributions:\n(1) We identify the key challenges in improving reproducibility for research vulnerability datasets and describe our methods for addressing and mitigating these issues.\n(2) We design a system called ARVO that automatically identifies the correct patch commit from OSS-Fuzz projects and automatically builds a reproducible environment for the vulnerable software system.\n(3) We present the ARVO dataset, a reproducible, recompilable, and automatically updating dataset of over 5,000 real-world vulnerabilities in open source C/C++ projects.\n(4) We make ARVO itself-the framework, evaluation infrastructure, images, and metadata-open-source, so that other researchers can build on our work. This includes more than 10,000 Docker images that can be used to reproduce each vulnerability and can re-compile after any valid modification of the source code."}, {"title": "BACKGROUND", "content": "Before discussing ARVO, we must first cover existing research vulnerability datasets and the limitations of these techniques."}, {"title": "Fuzzing and OSS-Fuzz", "content": "Fuzzing is one of the most widespread techniques for finding vulnerabilities in software, particularly software written in memory unsafe languages such as C and C++ [16, 19, 26]. Since the release of American Fuzzy Lop (AFL) in 2013 [28], fuzzing has attracted considerable attention from academic researchers and industry, and has been used to find vulnerabilities in a wide range of critical software. The most widely used fuzzing technique, coverage-based greybox fuzzing, mutates inputs to a target program, runs the program on those inputs, and selects inputs that expose new coverage for further mutation [24].\nMeanwhile, open source software continues to gain prominence-as of November 2023 there were 284 million public repositories on GitHub [8]-and is part of the Internet's \u201ccritical infrastructure\" [6]. The vast scale of this open source ecosystem over the last decade has seen, with the boost of open-source software and version control systems, the realization and documentation of numerous security bugs. This has previously benefited the building of massive security bug datasets, such as OSV [13].\nOSV uses the Open Source Vulnerability Format [4] to describe bugs. OSV.dev [13] aggregates several bug datasets that expose data in the OSV format, and this includes more than 115,000 bugs. If we limit the scope to C/C++ projects and to reproducible bugs, OSS-Fuzz includes about 3,300 cases.\nCreated by Google in 2016, OSS-Fuzz [25] is an open-source project that performs continuous fuzzing to detect and report security vulnerabilities in over 1,000 open-source projects. Each project is expected to provide a fuzz harness that specifies API functions in the project to test. OSS-Fuzz then monitors the project repository, builds the software as new commits are made, fuzzes them with a variety of fuzzers and sanitizers (e.g., AddressSanitizer [11]), automatically reports crashes found by the fuzzers, and periodically checks whether the project has fixed the reported vulnerability. Google's OSS-Fuzz cluster has helped find and fix more than 10,000 vulnerabilities, as of August 2023 (the last reported data)."}, {"title": "Patch Locating", "content": "For known vulnerabilities, the corresponding source-code fixes, called patches, are vital to understand. Source code patches are used to detect the existence of patches without source code [7, 15, 29] and for hot-patch generation [1, 5]. Revision control software makes patches possible by recording all the historical changes. However, automatic identification of patches is an unsolved problem.\nThe most well-known vulnerability dataset of CVE and NVD does not not have patches as a required. Furthermore, for CVE entries with identified patches, there is no guarantee for accuracy and correctness. The purpose of the CVE and NVD datasets is to help alert system maintainers of vulnerabilities and identify vulnerable software versions, not identify the patch.\nSome automated methods such as CVEfixes [2] are designed to map each CVE vulnerability to its patch. These methods use keyword matching [22] and commit comment analysis [27]. However, these focused on the text document/code information that they can extract and analyze, and, therefore, have no guarantee of accuracy. Moreover not all developers will leave comments describing the bug especially when the bug does not have a CVE or the developer does not understand the details of the underlying vulnerability."}, {"title": "OSV (Open Source Vulnerabilities) and OSS-Fuzz", "content": "OSS-Fuzz essentially contains a mostly text-based dataset that provides, for each vulnerability, the fuzzer-generated input as Proof of Concept (PoC) to trigger the vulnerability, the security sanitizer report, and the software components revision to ease reproducing. Google's OSS-Fuzz fuzzing cluster compiles and runs the latest version of the software daily. For the found vulnerabilities, every 24 hours, OSS-Fuzz takes the known PoC as input to run the latest version of software to verify if the bug is fixed.\nWhile this method automates vulnerability discovery and can be used to build a research vulnerability dataset, the verification of the patch can have a 24-hour delay. Therefore, OSS-Fuzz vulnerability reports are coarse-grained and include a range of commits.\nOSV (Open Source Vulnerabilities) is a database that collects vulnerability reports from different software (called projects on OSV), including OSS-Fuzz, Linux, and Ubuntu. However, after checking all the ecosystems on OSV including more than 1,000 vulnerabilities, we found OSS-Fuzz is the only C/C++-focused ecosystem that provides patches for generic OSS projects.\nTo generate OSV reports for OSS-Fuzz and identify the patch, OSS-Fuzz has a sub-project called OSS-Fuzz-Vulns [12], which performs automatic bisection search and repository analysis. Based on the PoC obtained from fuzzing, OSS-Fuzz-Vulns can tell if a version might be vulnerable or not by walking the revisions and decided which version is affected and which is not. Based on this method and from automating the pipeline earlier (since 2021) they have around 3,300 cases\nSurprisingly, the data from OSS-Fuzz is often unreliable and inaccurate. For instance, consider OSV-2020-16762 which identifies a specific commit as the patch for a heap buffer overflow vulnerability. Yet, upon inspection, this commit does not alter the source code at all; instead, it simply adds a text file for GitHub Actions configuration, which cannot fix the underlying vulnerability\u00b3.\nIn Section 4 we evaluate OSS-Fuzz reproduction in detail, highlighting the limitations, and discuss our approach to improving the reproduction success rate from 13% to 63%."}, {"title": "REPRODUCIBILITY", "content": "A significant contribution of ARVO is the focus on the reproducibility of the ARVO dataset. Unlike prior work, ARVO allows not only for replaying the PoCs for the vulnerable and fixed version of the system, but also for the recompilation of each version of the software. To accomplish this goal, there are several challenges, which we highlight herein.\nIn this paper, we judge the reproducibility of research vulnerability datasets on two criteria: the resources needed for reproduction and the reproduction pipelines.\nReproducing Resources is all necessary metadata needed to reproduce a vulnerability, including vulnerability descriptions, source code of the related components, environment to reproduce, compile methods/scripts, an example of a vulnerable binary, the Proof of Concept (PoC) input that triggers the vulnerability, and the corresponding patch.\nReproducing Pipeline allows the ability to easily reproduce the vulnerabilities. We focus on only the pipeline reproducing success rate and the required maintenance to measure the pipeline. This is a challenges because of the complexity of resolving missing resources (which we'll discuss later in Section 3.2) and limited control on the upstream software."}, {"title": "Why Recompilation?", "content": "Our definition of reproducibility requires recompilation, which is not a common feature supported by most datasets. The lack of recompilation limits the applicability of the datasets to several research directions. For example, novel methods, such as white-box fuzzing [3] and program repair [14, 17], require a dataset that supports recompilation to perform the evaluation. Moreover, recompilation ensures that the resulting binary is reliable and reproducible, which avoids incorrect vulnerabilities in the dataset. However, such a reproducible, recompilable, and scalable research vulnerability dataset is still missing."}, {"title": "Challenges", "content": "Any large-scale research vulnerability dataset creation system must solve the following reproduction challenges, much of which stem from attempting to compile an old version of an open-source software system.\nMismatched Dependencies. While the source code is available in a revision control system, a key issue is the libraries and other dependencies that a specific version of the software depends on. Often, the exact version of the dependency is not directly specified in the revision control system. For instance, consider a system where the build process fetches the main branch of a dependency to build it. Clearly, this is not reproducible, as building a five-year-old version of this system will likely not compile against the current version of the dependency. Therefore, any reproducible and recompilable research vulnerability dataset creation system must handle dependencies, and Section 4.3 discusses how we solve this challenge.\nMissing Resources. A related issue to mismatched dependencies is missing resources, which is when the software system attempts to fetch the source of a dependency, however the dependency is no longer available. One example is that the website that hosted the dependency changed domain names. Another example is the PCRE library, which in 2021 switched from an FTP server hosting an SVN repository to a git repo hosted on GitHub. This issue becomes much more frequent as the number of dependencies of the software increases and as time increases. We discuss in Section 4.3 how ARVO handles this challenge.\nAutomated Pipeline. Prior work [20] spent 1,600 man-hours reproducing 202 out of 368 vulnerabilities. This underscores the need for an automated system that can continuously expand the dataset with new entries. Therefore, we design ARVO specifically to be automated and run with minor manual analysis."}, {"title": "Unsuitability of Prior Work", "content": "In light of the previously identified challenges, we now focus on current vulnerability datasets and demonstrate the need for ARVO. CVE and NVD only include a vulnerability description and sometimes a third-party URLs, which can be a report from another dataset, a commit of the patch, or a blog from the vulnerability discoverer. Therefore, there is not enough information to reproduction and requires manual effort of security professionals.\nOSS-Fuzz-Vuln (described in Section 2.3) aimed to solve the automation pipeline challenge by building a pipeline to fuzz open-source software. While this reduces manual analysis, significant manual effort is still needed to reproduce the vulnerability.\nThis effort is because the reproducing pipeline on OSS-Fuzz is neither reliable nor strong. Most OSS-Fuzz reports only include the two-component revisions: (1) the version that found the bug and (2) the version when the crash stopped. These versions do not correspond to patches, as they are created daily (and can include many commits in a busy open-source project).\nTherefore, locating the vulnerability patches requires recompiling old versions of the targets, and to derive this data for OSS-Fuzz-Vuln, OSS-Fuzz performed bisection over commits that could include the fix. However, OSS-Fuzz only found the patches for about 3,300 cases out of more than 10,000 reported bugs.\nIn addition, to measure reproducibility and recompilability, we selected 100 random cases (from the 10,000 reported bugs) and found only 13 cases that can reproduce the crash and the fix. Therefore, this motivates the need for a new system that can generate a research vulnerability dataset that is reproducible, recompilable, and scalable."}, {"title": "ARVO", "content": "We designed ARVO with the goal of producing a reproducible and scalable vulnerability dataset and solving the challenges mentioned in Section 3.2. In detail, we aim to achieve:\nReproducibility. Provide all the reproducing resources mentioned in Section 3.2 and a reliable pipeline to re-compile the (vulnerable/fixed) targets from the source code.\nScalability. The dataset should contain a large number of vulnerabilities and automatically incorporate new vulnerabilities as they are found, to allow the dataset to expand and grow easily over time.\nQuality and Diversity. Each vulnerability in the dataset should be validated to ensure it is actually a bug with security impact. The vulnerabilities should be distributed across a large number of different projects, to ensure that evaluations using the dataset are representative.\nEase of Use. The dataset should be easy for researchers and practitioners to use, without requiring them to have extensive security background or know how to build the projects in the dataset.\nIn this section, we will describe the methods used in ARVO and the improvement the methods made compared to prior work; in Section 5 we characterize ARVO and demonstrate that it achieves these goals. Overall, it is able to successfully reproduce 5,651 out of 8,934 vulnerabilities sourced from OSS-Fuzz (63.3%), and identifies the precise fix for 5,001 (88.5%) of the reproduced cases."}, {"title": "Overview", "content": "ARVO is an interactive framework to generate a research vulnerability dataset, designed to ingest source metadata from 'bug'/project databases and augment this information with relevant source code, build steps, and binaries. Because we hope to support downstream uses such as analysis of security patches, evaluating vulnerability discovery systems, and automated vulnerability repair, the ARVO dataset also needs to include environments for re-compiling the code of each project so that modifications to the source code can be straightforwardly tested. To enable easy access, ARVO provides an online Dockerized dataset as well as infrastructure to build the dataset from scratch.\nARVO consists of two major components, shown in Figure 1: (1) the reproducer and (2) the vulnerability patch locator, and ARVO outputs the ARVO dataset.\nThe reproducer takes the provided metadata from the upstream bug database(s), compiles the project binary for the specified (vulnerable) version, and verifies that the provided triggering input causes a crash. It also checks whether the vulnerability was actually fixed by the fix commit listed in the metadata by compiling the project at the fixed version checking that the program no longer crashes. If either of these steps fails, we consider the vulnerability unreproducible and exclude it from the dataset; we provide an analysis of the causes of such failures in Section 4.\nHowever, as previously discussed, the upstream metadata often does not provide the exact commit that fixes the issue, but rather a range of possible commits. ARVO's vulnerability patch locator searches this commit range to find the earliest commit that resolves the issue; because we prepared reproducible project build environments, we can bisect the commit history to identify the exact changes that fix the vulnerability."}, {"title": "Source Data", "content": "To obtain a large number of vulnerabilities and allow the dataset to grow over time, ARVO is designed to draw project and bug metadata from upstream sources (currently, OSS-Fuzz). We rely on some assumptions about the upstream data source (discussed in Section 3):\nVersion Information: To reproduce the issue and find the precise fix, we need version identifiers (e.g., git commit hashes) referencing the project's revision control system that identify the vulnerable and non-vulnerable version of the project and its dependencies. If these are not available, however, we could (with some loss of precision) fall back to relying on timestamps to locate the appropriate versions.\nBuild Environment: This refers to a virtualized, interactive environment able to compile and execute the target programs and their dependencies.\nCrash Information: At minimum, we need a triggering input and the command to execute the target program on that input. Additional information such as sanitizer output can also be used to validate that the crash we observe is the same one identified by the upstream source, but this is not strictly necessary.\nThe current implementation of ARVO uses OSS-Fuzz as its upstream source. To identify security-relevant issues with metadata we need, we searched the issue tracker according to the labels OSS-Fuzz automatically applies to each issue: Type=Bug-Security (the crash is likely to be security-relevant, based on the sanitizer report and call stack), label: Reproducible (the crash occurs deterministically whenever the triggering input is provided), and status: Verified (OSS-Fuzz verified that the target no longer crashes\u2074). Combining these query elements, we obtain 8,934 issues in over 300 projects after filtering obvious false positives (the vulnerable version is the same as the fixed version), which serve as the starting point for our dataset."}, {"title": "Reproducer", "content": "To reproduce an issue and locate its precise fix, ARVO must be able to build the project and its dependencies from source at different commits. However, this poses a number of challenges, particularly for older vulnerabilities where dependencies, resources, and toolchains may have been lost over time. Using the techniques described in this section, ARVO's Reproducer component can successfully reproduce 5,651 vulnerabilities out of 8,934 vulnerabilities (63.3%); this is a significant improvement over the 13% success rate achieved by OSS-Fuzz's provided reproducer.\nWe identify three key strategies that ARVO uses to improve the reproducibility of vulnerabilities: 1) revision control; 2) minimally intrusive build instrumentation; and 3) fixing missing resources. These strategies are implemented in the ARVO reproducer, as shown in Figure 2."}, {"title": "Revision Control", "content": "Successfully reproducing a vulnerability requires precise information about the build environment and versions of the main project and its dependencies. We found that the information provided by OSS-Fuzz is generally sufficient: the build environments (the OSS-Fuzz base_builder Docker container images with the compiler toolchain used to build the project) are publicly archived, and a publicly-accessible Google Cloud Storage bucket stores a srcmap.json file for each build with the commit hashes for the project and dependencies5."}, {"title": "Build Instrumentation", "content": "The build scripts used by OSS-Fuzz to compile the fuzz targets for each project are provided by the project developers in two parts: a Dockerfile (derived from base_builder) that downloads dependencies and external resources, and a build.sh script that actually compiles the fuzz targets. Because these build scripts can contain arbitrary commands, it is challenging to control the revisions of the project and its dependencies. The reproducer provided by OSS-Fuzz adjusts the main project to the correct commit, but it does not attempt to set dependencies to their corresponding versions. This leads to compatibility issues and build failures when dependencies have changed their APIs or build procedures. For instance, imagemagick relies on 15 separate components, each with frequently changing APIs and usage patterns, and attempting to reproduce a vulnerability in imagemagick without adjusting the dependencies to match the vulnerable version will likely result in a failed build.\nTo reproduce an issue and locate its precise fix, ARVO must be able to build the project and its dependencies from source at different commits. However, this poses a number of challenges, particularly for older vulnerabilities where dependencies, resources, and toolchains may have been lost over time. Using the techniques described in this section, ARVO's Reproducer component can successfully reproduce 5,651 vulnerabilities out of 8,934 vulnerabilities (63.3%); this is a significant improvement over the 13% success rate achieved by OSS-Fuzz's provided reproducer.\nWhen rolling back dependency versions, we attempt to be minimally intrusive and make our changes only at the download stage. Starting with the dependency names and commit hashes provided in srcmap.json, we locate the point where the dependency is fetched in the Dockerfile by looking for git, Mercurial, or SVN commands referencing the dependency's repository URL. We then add a command immediately after the download that rolls back the dependency to the correct commit. In some cases, where the provided version cannot be found (e.g., if the revision history for the project has been rewritten) we use the issue timestamp to identify the closest commit before the vulnerability was found. This approach minimizes the impact of our changes on the build process, reducing the likelihood of introducing new compatibility issues.\nBy contrast, the original OSS-Fuzz reproducer attempts to adjust the version of the main project by preparing the project source outside the build container and mounting it in the container before running the build script. This requires parsing the Dockerfile and attempting to reproduce any necessary post-checkout initialization steps carried out during docker build, which is challenging and often fails in practice. ARVO sidesteps these issues by ensuring that dependencies are at the correct versions as soon as they are fetched and allowing all other build steps to proceed as normal.\nWe found that correcting dependency versions is crucial for reproducing historical vulnerabilities. To demonstrate the impact of incorrect dependency versions, we performed an ablation test with 100 randomly selected issues that ARVO was able to reproduce. We then disabled the dependency revision control component and attempted to reproduce the issues; as seen in Table 1, the overall reproducing success rate decreased dramatically from 100% to 58%. We also note that vulnerabilities in projects with many dependencies are more likely to fail to reproduce without dependency revision control. When we disable the revision control, the successfully reproduced cases' have 0.45 dependencies on average while the failed cases have 8.48 dependencies on average, which clearly shows the importance of revision control. Thus, if we do not account for dependency versions, the resulting dataset may be biased towards simpler projects with fewer dependencies."}, {"title": "Broken Resource Fixing", "content": "Similar to \"bit rot\" in software, while reproducing old vulnerabilities, we encountered numerous dependencies where components were no longer accessible, particularly for projects from the 2017-2019 period. During this time, many projects migrated their repositories from Subversion to git, which breaks build scripts that reference the old repositories. Additionally, certain build scripts rely on tools and resources downloaded from the Internet, which may become unavailable over time. Because the failure of any step in the build process causes the entire process to fail, broken resources must be resolved to successfully reproduce the vulnerability.\nWe divide missing resources into two categories: core resources, which are necessary to compile the fuzz target, and non-core resources, which are not necessary for compilation but are required for other parts of the build process. Core resources include software dependencies such as libraries as well as tools used by the build process that may be necessary to compile key components. Non-core resources include documentation generation tools, seed corpora used during the fuzzing process, and other resources that are not directly related to the fuzz target."}, {"title": "Fix Locator", "content": "Pinpointing the patch that fixes a given vulnerability enables many different downstream uses of the vulnerability dataset, such as research into how developers fix vulnerabilities, benchmarking of localization and repair systems, etc. In this section we present ARVO's fix locator and how it addresses limitations in OSS-Fuzz's fix verification process.\nOSS-Fuzz. OSS-Fuzz builds each project once per day, and, if a crash is detected, it reports the issue to the project developers. The developers then identify the root cause and commit a fix for the issue, which can be time-consuming (vulnerabilities in our dataset averaged 68.89 days between the initial report and the fix). When OSS-Fuzz performs its daily build, it checks whether the most recent version at that time crashes on the triggering input; if it does not, the issue is marked as fixed. Particularly for highly active projects, the delay between the developers' fix and OSS-Fuzz's verification results in a range of possible candidate commits for the actual patch. Figure 4 illustrates the process through a concrete example: the vulnerability is first identified by OSS-Fuzz at 6f6caf; the developers commit a fix the next day at aa668b; finally, OSS-Fuzz verifies the fix at aa668b. Note that aa668b is not the actual fix, but rather a minor change to the ChangeLog file; to identify the actual fix, we must search over the 14 commits and 83 files that were changed between the initial report and the verification.\nARVO. Although bisection is conceptually simple, it is difficult to implement in practice due the difficulty of recompiling the project at each commit. ARVO's fix locator also benefits from the techniques used for vulnerability reproduction: because we can precisely control dependency versions and account for missing resources, we can recompile the project not just at its vulnerable version, but also at other commits around the same time. This allows us to perform bisection on the range provided by OSS-Fuzz to locate the true fix. ARVO identified precise patches on the commit level for vulnerabilities in 88.5% of the reproducible cases from the reproducer. We believe these developer-written patches for vulnerabilities may also be of interest in their own right for future research such as work on vulnerability repair.\nAs with reproducing the initial vulnerability, precise revision control for dependencies is crucial during bisection, because we cannot tell if a particular commit is before or after the fix was implemented unless we can build the project and run it on the triggering input. This means that as ARVO visits each commit during bisection, we must also identify the corresponding commit for each dependency to ensure that the build is compatible. Unlike the reproducer, the fix locator cannot rely on the dependency versions from srcmap.json, and so we instead use timestamps to find the appropriate commit.\nFigure 5 shows how revision control is applied for the Imagemagick issue previously discussed. For each revision of the main component, we used the timestamp to find the most recent version of the dependency at that time, improving the reliability of the build process."}, {"title": "Database Access", "content": "A goal of our dataset is that it should be easy to use, even for researchers who do not have a security background; we hope that this will allow researchers in other fields (e.g., machine learning) to use it as an evaluation target. Based on ARVO, we have uploaded docker images for each vulnerability to Docker Hub, allowing each issue to be reproduced and recompiled with a single command: docker run n132/arvo: <localId>-<vul|fix> arvo [compile].\nTo support more advanced uses of the dataset (e.g., rebuilding the project with other instrumentation), we make ARVO itself open-source so researchers can rebuild the ARVO dataset from scratch with their desired changes."}, {"title": "DATASET", "content": "This section presents the details of the ARVO dataset constructed using the methods described in Section 4."}, {"title": "Dataset Characteristics", "content": "Dataset Size and Growth. At the time of this writing", "dataset": "It does not contain a small number of projects", "18": ".", "finding": "In ARVO dataset we find that the average patch fixing a vulnerability affects 2.53 files (mean: 2.53", "median": 1, "std": 9.56}]}