{"title": "ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software", "authors": ["Xiang Mei", "Pulkit Singh Singaria", "Jordi Del Castillo", "Haoran Xi", "Abdelouahab (Habs) Benchikh", "Tiffany Bao", "Ruoyu Wang", "Yan Shoshitaishvili", "Adam Doup\u00e9", "Hammond Pearce", "Brendan Dolan-Gavitt"], "abstract": "High-quality datasets of real-world vulnerabilities are enormously valuable for downstream research in software security, but existing datasets are typically small, require extensive manual effort to update, and are missing crucial features that such research needs. In this paper, we introduce ARVO: an Atlas of Reproducible Vulnerabilities in Open-source software. By sourcing vulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and implementing a reliable re-compilation system, we successfully reproduce more than 5,000 memory vulnerabilities across over 250 projects, each with a triggering input, the canonical developer-written patch for fixing the vulnerability, and the ability to automatically rebuild the project from source and run it at its vulnerable and patched revisions. Moreover, our dataset can be automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to grow over time. We provide a thorough characterization of the ARVO dataset, show that it can locate fixes more accurately than Google's own OSV reproduction effort, and demonstrate its value for future research through two case studies: firstly evaluating real-world LLM-based vulnerability repair, and secondly identifying over 300 falsely patched (still-active) zero-day vulnerabilities from projects improperly labeled by OSS-Fuzz.", "sections": [{"title": "1 INTRODUCTION", "content": "Vulnerabilities in software are both common and damaging: in 2023 alone, more than 28,000 vulnerabilities were tracked by the National Vulnerability Database (NVD), 4,648 of which were classified as Critical severity (using CVSS V3 scores). Studying the nature of this growing world of vulnerabilities in software is critical, but doing so requires a vulnerability dataset.\nHowever, many existing vulnerability datasets, such as Common Vulnerabilities and Exposures (CVE) and the NVD, are designed to alert users about vulnerabilities in software so that system main- tainers can verify affected versions and apply patches for known vulnerabilities in currently deployed software. Due to this focus, these datasets are not effective as a research vulnerability dataset.\nA research vulnerability dataset must consist of real-world vul- nerabilities. To be maximally useful, each vulnerability must contain metadata that includes: the input that triggers the vulnerability, the source-code patch that fixes the vulnerability, and the ability to compile the vulnerable and patched versions of the original source code. All of this must be reproducible into the future.\nUnfortunately, creating such a research vulnerability dataset manually is difficult. For example, despite over 3,600 hours of human work into reproducing publicly reported CVEs, Mu et al. [20] only succeeded in reproducing a total of 368 vulnerabilities.\nAnother complexity is that, when used as an evaluation bench- mark, research vulnerability datasets tend to become \u201cstale\u201d over time as researchers (perhaps unintentionally) tune their systems to achieve good benchmark scores on the research vulnerability dataset without necessarily improving their real-world perfor- mance. Therefore, a continuously updated research vulnerability dataset is critical.\nIn this paper, we present ARVO, the 'Atlas of Reproducible Vul- nerabilities.' ARVO is both a framework designed to address the shortage of research vulnerability dataset and a comprehensive bug dataset in its own right. Derived from Google's OSS-Fuzz project, ARVO aims to achieve a high level of reproducibility across a large number of real-world projects and vulnerabilities, providing a ro- bust set of real-world vulnerabilities for a research vulnerability dataset. We focus on C/C++ projects due to their widespread use and the significant impact of bugs in these languages."}, {"title": "2 BACKGROUND", "content": "Before discussing ARVO, we must first cover existing research vulnerability datasets and the limitations of these techniques."}, {"title": "2.1 Fuzzing and OSS-Fuzz", "content": "Fuzzing is one of the most widespread techniques for finding vulnerabilities in software, particularly software written in memory unsafe languages such as C and C++ [16, 19, 26]. Since the release of American Fuzzy Lop (AFL) in 2013 [28], fuzzing has attracted considerable attention from academic researchers and industry, and has been used to find vulnerabilities in a wide range of critical software. The most widely used fuzzing technique, coverage-based greybox fuzzing, mutates inputs to a target program, runs the pro- gram on those inputs, and selects inputs that expose new coverage for further mutation [24].\nMeanwhile, open source software continues to gain prominence- as of November 2023 there were 284 million public repositories on GitHub [8]-and is part of the Internet's \u201ccritical infrastructure\u201d [6]. The vast scale of this open source ecosystem over the last decade has seen, with the boost of open-source software and version control systems, the realization and documentation of numerous security bugs. This has previously benefited the building of massive security bug datasets, such as OSV [13].\nOSV uses the Open Source Vulnerability Format [4] to describe bugs. OSV.dev [13] aggregates several bug datasets that expose data in the OSV format, and this includes more than 115,000 bugs. If we limit the scope to C/C++ projects and to reproducible bugs, OSS-Fuzz includes about 3,300 cases.\nCreated by Google in 2016, OSS-Fuzz [25] is an open-source project that performs continuous fuzzing to detect and report secu- rity vulnerabilities in over 1,000 open-source projects. Each project is expected to provide a fuzz harness that specifies API functions in the project to test. OSS-Fuzz then monitors the project repository, builds the software as new commits are made, fuzzes them with a variety of fuzzers and sanitizers (e.g., AddressSanitizer [11]), au- tomatically reports crashes found by the fuzzers, and periodically checks whether the project has fixed the reported vulnerability. Google's OSS-Fuzz cluster has helped find and fix more than 10,000 vulnerabilities, as of August 2023 (the last reported data)."}, {"title": "2.2 Patch Locating", "content": "For known vulnerabilities, the corresponding source-code fixes, called patches, are vital to understand. Source code patches are used to detect the existence of patches without source code [7, 15, 29] and for hot-patch generation [1, 5]. Revision control software makes patches possible by recording all the historical changes. However, automatic identification of patches is an unsolved problem.\nThe most well-known vulnerability dataset of CVE and NVD not not have patches as a required. Furthermore, for CVE entries with identified patches, there is no guarantee for accuracy and correctness. The purpose of the CVE and NVD datasets is to help alert system maintainers of vulnerabilities and identify vulnerable software versions, not identify the patch.\nSome automated methods such as CVEfixes [2] are designed to map each CVE vulnerability to its patch. These methods use key- word matching [22] and commit comment analysis [27]. However, these focused on the text document/code information that they can extract and analyze, and, therefore, have no guarantee of accuracy. Moreover not all developers will leave comments describing the bug especially when the bug does not have a CVE or the developer does not understand the details of the underlying vulnerability."}, {"title": "2.3 OSV (Open Source Vulnerabilities) and OSS-Fuzz", "content": "OSS-Fuzz essentially contains a mostly text-based dataset that pro- vides, for each vulnerability, the fuzzer-generated input as Proof of Concept (PoC) to trigger the vulnerability, the security sanitizer report, and the software components revision to ease reproducing. Google's OSS-Fuzz fuzzing cluster compiles and runs the latest version of the software daily. For the found vulnerabilities, every 24 hours, OSS-Fuzz takes the known PoC as input to run the latest version of software to verify if the bug is fixed.\nWhile this method automates vulnerability discovery and can be used to build a research vulnerability dataset, the verification of the patch can have a 24-hour delay. Therefore, OSS-Fuzz vulnerability reports are coarse-grained and include a range of commits.\nOSV (Open Source Vulnerabilities) is a database that collects vul- nerability reports from different software (called projects on OSV), including OSS-Fuzz, Linux, and Ubuntu. However, after checking all the ecosystems on OSV including more than 1,000 vulnerabili- ties, we found OSS-Fuzz is the only C/C++-focused ecosystem that provides patches for generic OSS projects.\nTo generate OSV reports for OSS-Fuzz and identify the patch, OSS-Fuzz has a sub-project called OSS-Fuzz-Vulns [12], which per- forms automatic bisection search and repository analysis. Based on the PoC obtained from fuzzing, OSS-Fuzz-Vulns can tell if a version might be vulnerable or not by walking the revisions and decided which version is affected and which is not. Based on this method and from automating the pipeline earlier (since 2021) they have around 3,300 cases\nSurprisingly, the data from OSS-Fuzz is often unreliable and in- accurate. For instance, consider OSV-2020-16762 which identifies a specific commit as the patch for a heap buffer overflow vulnera- bility. Yet, upon inspection, this commit does not alter the source code at all; instead, it simply adds a text file for GitHub Actions configuration, which cannot fix the underlying vulnerability.\nIn Section 4 we evaluate OSS-Fuzz reproduction in detail, high- lighting the limitations, and discuss our approach to improving the reproduction success rate from 13% to 63%."}, {"title": "3 REPRODUCIBILITY", "content": "A significant contribution of ARVO is the focus on the reproducibil- ity of the ARVO dataset. Unlike prior work, ARVO allows not only for replaying the PoCs for the vulnerable and fixed version of the system, but also for the recompilation of each version of the soft- ware. To accomplish this goal, there are several challenges, which we highlight herein.\nIn this paper, we judge the reproducibility of research vulnerabil- ity datasets on two criteria: the resources needed for reproduction and the reproduction pipelines.\nReproducing Resources is all necessary metadata needed to re- produce a vulnerability, including vulnerability descriptions, source code of the related components, environment to reproduce, compile methods/scripts, an example of a vulnerable binary, the Proof of Concept (PoC) input that triggers the vulnerability, and the corre- sponding patch.\nReproducing Pipeline allows the ability to easily reproduce the vulnerabilities. We focus on only the pipeline reproducing success rate and the required maintenance to measure the pipeline. This is a challenges because of the complexity of resolving missing resources (which we'll discuss later in Section 3.2) and limited control on the upstream software."}, {"title": "3.1 Why Recompilation?", "content": "Our definition of reproducibility requires recompilation, which is not a common feature supported by most datasets. The lack of recompilation limits the applicability of the datasets to several re- search directions. For example, novel methods, such as white-box fuzzing [3] and program repair [14, 17], require a dataset that sup- ports recompilation to perform the evaluation. Moreover, recompi- lation ensures that the resulting binary is reliable and reproducible, which avoids incorrect vulnerabilities in the dataset. However, such a reproducible, recompilable, and scalable research vulnerability dataset is still missing."}, {"title": "3.2 Challenges", "content": "Any large-scale research vulnerability dataset creation system must solve the following reproduction challenges, much of which stem from attempting to compile an old version of an open-source soft- ware system."}, {"title": "3.3 Unsuitability of Prior Work", "content": "In light of the previously identified challenges, we now focus on current vulnerability datasets and demonstrate the need for ARVO.\nCVE and NVD only include a vulnerability description and some- times a third-party URLs, which can be a report from another dataset, a commit of the patch, or a blog from the vulnerability discoverer. Therefore, there is not enough information to reproduc- tion and requires manual effort of security professionals.\nOSS-Fuzz-Vuln (described in Section 2.3) aimed to solve the automation pipeline challenge by building a pipeline to fuzz open- source software. While this reduces manual analysis, significant manual effort is still needed to reproduce the vulnerability.\nThis effort is because the reproducing pipeline on OSS-Fuzz is neither reliable nor strong. Most OSS-Fuzz reports only include the two-component revisions: (1) the version that found the bug and (2) the version when the crash stopped. These versions do not correspond to patches, as they are created daily (and can include many commits in a busy open-source project).\nTherefore, locating the vulnerability patches requires recom- piling old versions of the targets, and to derive this data for OSS- Fuzz-Vuln, OSS-Fuzz performed bisection over commits that could include the fix. However, OSS-Fuzz only found the patches for about 3,300 cases out of more than 10,000 reported bugs.\nIn addition, to measure reproducibility and recompilability, we selected 100 random cases (from the 10,000 reported bugs) and found only 13 cases that can reproduce the crash and the fix. There- fore, this motivates the need for a new system that can generate a research vulnerability dataset that is reproducible, recompilable, and scalable."}, {"title": "4 ARVO", "content": "We designed ARVO with the goal of producing a reproducible and scalable vulnerability dataset and solving the challenges mentioned in Section 3.2. In detail, we aim to achieve:\nReproducibility. Provide all the reproducing resources mentioned in Section 3.2 and a reliable pipeline to re-compile the (vulnerable/ fixed) targets from the source code.\nScalability. The dataset should contain a large number of vulnera- bilities and automatically incorporate new vulnerabilities as they are found, to allow the dataset to expand and grow easily over time.\nQuality and Diversity. Each vulnerability in the dataset should be validated to ensure it is actually a bug with security impact. The vulnerabilities should be distributed across a large number of different projects, to ensure that evaluations using the dataset are representative.\nEase of Use. The dataset should be easy for researchers and practi- tioners to use, without requiring them to have extensive security background or know how to build the projects in the dataset.\nIn this section, we will describe the methods used in ARVO and the improvement the methods made compared to prior work; in Section 5 we characterize ARVO and demonstrate that it achieves these goals. Overall, it is able to successfully reproduce 5,651 out of 8,934 vulnerabilities sourced from OSS-Fuzz (63.3%), and identifies the precise fix for 5,001 (88.5%) of the reproduced cases."}, {"title": "4.1 Overview", "content": "ARVO is an interactive framework to generate a research vulnera- bility dataset, designed to ingest source metadata from 'bug'/project databases and augment this information with relevant source code, build steps, and binaries. Because we hope to support downstream uses such as analysis of security patches, evaluating vulnerability discovery systems, and automated vulnerability repair, the ARVO dataset also needs to include environments for re-compiling the code of each project so that modifications to the source code can be straightforwardly tested. To enable easy access, ARVO provides an online Dockerized dataset as well as infrastructure to build the dataset from scratch.\nARVO consists of two major components, shown in Figure 1: (1) the reproducer and (2) the vulnerability patch locator, and ARVO outputs the ARVO dataset.\nThe reproducer takes the provided metadata from the upstream bug database(s), compiles the project binary for the specified (vul- nerable) version, and verifies that the provided triggering input causes a crash. It also checks whether the vulnerability was actually fixed by the fix commit listed in the metadata by compiling the project at the fixed version checking that the program no longer crashes."}, {"title": "4.2 Source Data", "content": "To obtain a large number of vulnerabilities and allow the dataset to grow over time, ARVO is designed to draw project and bug metadata from upstream sources (currently, OSS-Fuzz). We rely on some assumptions about the upstream data source (discussed in Section 3):\nVersion Information: To reproduce the issue and find the pre- cise fix, we need version identifiers (e.g., git commit hashes) referencing the project's revision control system that iden- tify the vulnerable and non-vulnerable version of the project and its dependencies. If these are not available, however, we could (with some loss of precision) fall back to relying on timestamps to locate the appropriate versions.\nBuild Environment: This refers to a virtualized, interactive environment able to compile and execute the target programs and their dependencies.\n\u25a0 Crash Information: At minimum, we need a triggering input and the command to execute the target program on that input. Additional information such as sanitizer output can also be used to validate that the crash we observe is the same one identified by the upstream source, but this is not strictly necessary.\nThe current implementation of ARVO uses OSS-Fuzz as its up- stream source. To identify security-relevant issues with metadata we need, we searched the issue tracker according to the labels OSS-Fuzz automatically applies to each issue: Type=Bug-Security (the crash is likely to be security-relevant, based on the sanitizer report and call stack), label: Reproducible (the crash occurs de- terministically whenever the triggering input is provided), and status: Verified (OSS-Fuzz verified that the target no longer crashes). Combining these query elements, we obtain 8,934 is- sues in over 300 projects after filtering obvious false positives (the vulnerable version is the same as the fixed version), which serve as the starting point for our dataset."}, {"title": "4.3 Reproducer", "content": "To reproduce an issue and locate its precise fix, ARVO must be able to build the project and its dependencies from source at different commits. However, this poses a number of challenges, particu- larly for older vulnerabilities where dependencies, resources, and toolchains may have been lost over time. Using the techniques described in this section, ARVO's Reproducer component can suc- cessfully reproduce 5,651 vulnerabilities out of 8,934 vulnerabilities (63.3%); this is a significant improvement over the 13% success rate achieved by OSS-Fuzz's provided reproducer.\nWe identify three key strategies that ARVO uses to improve the reproducibility of vulnerabilities: 1) revision control; 2) min- imally intrusive build instrumentation; and 3) fixing missing re- sources. These strategies are implemented in the ARVO reproducer, as shown in Figure 2."}, {"title": "Revision Control", "content": "Successfully reproducing a vulnerability requires precise informa- tion about the build environment and versions of the main project and its dependencies. We found that the information provided by OSS-Fuzz is generally sufficient: the build environments (the OSS-Fuzz base_builder Docker container images with the com- piler toolchain used to build the project) are publicly archived, and a publicly-accessible Google Cloud Storage bucket stores a srcmap.json file for each build with the commit hashes for the project and dependencies.\nBuild Instrumentation\nThe build scripts used by OSS-Fuzz to compile the fuzz targets for each project are provided by the project developers in two parts: a Dockerfile (derived from base_builder) that downloads dependencies and external resources, and a build.sh script that actually compiles the fuzz targets. Because these build scripts can contain arbitrary commands, it is challenging to control the revi- sions of the project and its dependencies. The reproducer provided by OSS-Fuzz adjusts the main project to the correct commit, but it does not attempt to set dependencies to their corresponding ver- sions. This leads to compatibility issues and build failures when dependencies have changed their APIs or build procedures. For in- stance, imagemagick relies on 15 separate components, each with frequently changing APIs and usage patterns, and attempting to reproduce a vulnerability in imagemagick without adjusting the dependencies to match the vulnerable version will likely result in a failed build.\nFigure 3 shows a simplified version of the workflow used to build OSS-Fuzz projects. First, the Dockerfile is used to download the resources needed before compiling (shown in blue on the left). This step not only runs commands such as git clone to download necessary resources but also executes some scripts such as git submodule init or custom initialization steps. Then, after all the components are initialized, in the red part, compile starts in the Docker container.\nWhen rolling back dependency versions, we attempt to be mini- mally intrusive and make our changes only at the download stage. Starting with the dependency names and commit hashes provided in srcmap.json, we locate the point where the dependency is fetched in the Dockerfile by looking for git, Mercurial, or SVN commands referencing the dependency's repository URL. We then add a com- mand immediately after the download that rolls back the depen- dency to the correct commit. In some cases, where the provided version cannot be found (e.g., if the revision history for the project has been rewritten) we use the issue timestamp to identify the closest commit before the vulnerability was found. This approach minimizes the impact of our changes on the build process, reducing the likelihood of introducing new compatibility issues."}, {"title": "Broken Resource Fixing", "content": "Similar to \"bit rot\" in software, while reproducing old vulnerabilities, we encountered numerous dependencies where components were no longer accessible, particularly for projects from the 2017-2019 period. During this time, many projects migrated their repositories from Subversion to git, which breaks build scripts that reference the old repositories. Additionally, certain build scripts rely on tools and resources downloaded from the Internet, which may become unavailable over time. Because the failure of any step in the build process causes the entire process to fail, broken resources must be resolved to successfully reproduce the vulnerability.\nWe divide missing resources into two categories: core resources, which are necessary to compile the fuzz target, and non-core re- sources, which are not necessary for compilation but are required for other parts of the build process. Core resources include soft- ware dependencies such as libraries as well as tools used by the build process that may be necessary to compile key components. Non-core resources include documentation generation tools, seed corpora used during the fuzzing process, and other resources that are not directly related to the fuzz target."}, {"title": "4.4 Fix Locator", "content": "Pinpointing the patch that fixes a given vulnerability enables many different downstream uses of the vulnerability dataset, such as research into how developers fix vulnerabilities, benchmarking of localization and repair systems, etc. In this section we present ARVO's fix locator and how it addresses limitations in OSS-Fuzz's fix verification process.\nOSS-Fuzz. OSS-Fuzz builds each project once per day, and, if a crash is detected, it reports the issue to the project developers. The developers then identify the root cause and commit a fix for the issue, which can be time-consuming (vulnerabilities in our dataset averaged 68.89 days between the initial report and the fix). When OSS-Fuzz performs its daily build, it checks whether the most recent version at that time crashes on the triggering input; if it does not, the issue is marked as fixed. Particularly for highly active projects, the delay between the developers' fix and OSS-Fuzz's verification results in a range of possible candidate commits for the actual patch. Figure 4 illustrates the process through a concrete example: the vulnerability is first identified by OSS-Fuzz at 6f6caf; the developers commit a fix the next day at aa668b; finally, OSS- Fuzz verifies the fix at aa668b. Note that aa668b is not the actual fix, but rather a minor change to the ChangeLog file; to identify the actual fix, we must search over the 14 commits and 83 files that were changed between the initial report and the verification."}, {"title": "4.5 Database Access", "content": "A goal of our dataset is that it should be easy to use, even for researchers who do not have a security background; we hope that this will allow researchers in other fields (e.g., machine learning) to use it as an evaluation target. Based on ARVO, we have uploaded docker images for each vulnerability to Docker Hub, allowing each issue to be reproduced and recompiled with a single command: docker run n132/arvo: <localId>- arvo [compile].\nTo support more advanced uses of the dataset (e.g., rebuilding the project with other instrumentation), we make ARVO itself open- source so researchers can rebuild the ARVO dataset from scratch with their desired changes."}, {"title": "5 DATASET", "content": "This section presents the details of the ARVO dataset constructed using the methods described in Section 4."}, {"title": "5.1 Dataset Characteristics", "content": "Dataset Size and Growth. At the time of this writing, out of 8,934 vulnerabilities initially obtained from OSS-Fuzz, ARVO reproduced 5,651 vulnerabilities across 273 projects. From these, we could pre- cisely locate the associated fix for 5,001 vulnerabilities.\nFigure 6 shows how these values grew over time against the proportion of OSS-Fuzz vulnerabilities that we could reproduce and fix. As can be seen, ARVO maintains a roughly constant rate of reproduction for cases after 2017. This means our mitigations for the missing resource challenge (discussed in Section 3.2) works.\nPrior to 2017 the OSS-Fuzz infrastructure changed more fre- quently. We hypothesize that this is why we have a lower success rate for those months, as we focus on a more generic approach suitable for the current OSS-Fuzz.\nThe growth of the ARVO dataset over time is continuous as OSS- Fuzz also grows in popularity, includes more projects, and findes more bugs. As we leverage their success, ARVO is likewise suitable for scaling into the future-OSS-Fuzz's continued fuzzing will keep the ARVO dataset growing and up-to-date.\nProject and Language Distribution. Table 2 shows the distribution of vulnerabilities among projects. This distribution is relatively even; the project with the most vulnerabilities represents 7.36% of the dataset, and the top 10 projects collectively account for only 31.13%. This indicates the comprehensive nature of the ARVO dataset: It does not contain a small number of projects, but instead a wide range of different C/C++ applications.\nPatch Statistics. Of the 5,001 vulnerabilities in the ARVO dataset for which we were able to identify the precise commit fixing the vulnerability, we first filter out duplicates (which can occur when a single patch fixes multiple vulnerabilities-this occurred in 1,246 cases) and then remove merge commits (as these contain many changes unrelated to the vulnerability-264 cases). The filtered set contains 3,491 patches.\nThe large size of this dataset allows us to collect some interesting statistics on the nature of vulnerability patches. Prior research has found that security-related fixes are typically small and self-contained [18]. Our data also supports this finding: In ARVO dataset we find that the average patch fixing a vulnerability affects 2.53 files (mean: 2.53, median: 1, std: 9.56); 2,216 patches (63.5%) affect just a single file. Looking at the number of lines added and removed by each patch, we find a median of 6 lines added and 2 lines removed; the means of both are significantly larger (131.0 added and 79.8 removed) due to a small number of outliers. 90% of the patches in our dataset have fewer than 60 lines added or removed."}, {"title": "5.2 Dataset Comparison", "content": "Table 3 presents the ARVO dataset against several other state-of- the-art bug databases. From this, we can see that ARVO dataset is the only dataset to achieve reproducibility on a large scale- indeed, we believe ARVO is unique in its combination of size and bug reproducability. We have found no other public dataset which achieves bug incident and patch reproducibility with complete support for project recompilation for all bug cases at the scale we provide. Primarily this is because while small datasets can achieve reproducability through manual effort, this does not scale to larger datasets. This is seen, for instance, in the CGC dataset's reproducible bugs, however the CGC dataset's scale and custom setup (using DARPA's custom DECREE operating system) limits its usage.\nMeanwhile, the ability to automatically update the dataset as its upstream data source reports new vulnerabilities (a feature shared with CVEFixes [2] and OSS-Fuzz-Vulns) means that ARVO will continue to grow over time, preventing any downstream users of the dataset from 'overfitting' to historical data. Moreover, larger datasets typically prioritize quantity over quality, which can com- promise their utility. This is particularly critical in binary analysis, where despite the abundance of resources associated with each vulnerability, substantial effort is required to reproduce them. By providing the assets for reproducability directly, ARVO will sub- stantially reduce this workload."}, {"title": "5.3 Comparison between OSS-Fuzz-Vulns and ARVO", "content": "From a pure size-based perspective, the ARVO dataset consists of a database comprising 5,001 successfully identified patches for vulnerabilities compared to OSV's OSS-Fuzz-Vulns subset at doc- umented fixes for 3,280 cases. ARVO also provides information for reproducibility; OSV does not. However, only 1,906 bug cases actually overlap between ARVO and OSV. To evaluate the quality of the located patches, we first split the common cases (1906) between ARVO and OSV into two groups: agree (86%) and disagree (14%) cases. We then manually verified 100 cases from each group. In agree cases, 84% were confirmed as true positives, while 56% of ARVO's results were true positives in disagree cases (OSV had only 15% true positives in disagree cases). Therefore, ARVO's overall success rate of locating patches is more than 80%.\nIn this comparison, we examined these overlapped cases to ex- plore if the ARVO locates the patching commit correctly. This was broadly the case, but in total there were 270 cases for which ARVO and OSV disagreed on the fix commit. We randomly selected 100 cases and analyzed them manually to compare the results and present the results in Table 4. We found that there were four distinct categories of outcomes: firstly, cases where OSV provided correct results; secondly, cases where ARVO provided the correct results; thirdly, instances where neither OSV nor ARVO gave correct re- sults (the bug patch was misidentified by both); and lastly, complex patches which require additional in-depth analysis to ascertain which patch fixes the bug more effectively. Another observation during this analysis was the identification of instances where the patches from OSV and ARVO were identical, with one being a merge commit having the other as its parent. In such cases, we deemed the original commit (parent commit) as the more accurate, given that merge commits typically consists of a large number of hunks with some not directly related to the fix.\nWe also explored the 1,636 cases in which ARVO and OSV iden- tified the same commit for the bug fixes. Here, we again randomly selected 100 cases to analyze their results (shown in Table 5). For the remaining 1,636 cases for which ARVO and OSV identified the same commit for the bug fixes, we randomly selected 100 cases to analyze their results. We found that there were three different categories of cases. Firstly, there is a distinction between cases where the commit message explicitly references the corresponding OSS-Fuzz issue (e.g., \"Fix OSS-Fuzz issue 20493\u201d), and no analysis is required; and cases where we had to manually analyze the patch and figure out if this patch fixes the bug or does it avoid the crash. Secondly, there is a distinction between patches which fix the bug and patches which do not actually address the bug at all, instead modifying files which do not affect the bug (e.g., Makefile, build.sh). These patches do not address the root cause of the bug and the bug may still be exploitable by attackers. Lastly, there were 4 cases where it was difficult to tell if the patch fixed the bug or not without doing a more in-depth analysis.\nTable 5 provides the best estimate of ARVO's patch localisation accuracy at approximately 84%, and Table 4 provides a lower bound at approximately 56%. Still, this lower bound comes from the less- frequent bugs where OSV and ARVO disagreed-i.e. these are the rarer, more complex cases.\nWe now explore three interesting cases in more detail:\nIssue 35566 is an unknown address crash in Qt5 as reported by libfuzzer. Comparing ARVO's fix commit with OSV's provides a challenge. We found that ARVO's fix commit was from the Qt5 project, but OSV's was in qtbase project. In fact, Qtbase is a depen- dency of Qt5 and is a submodule in Qt5 repository.\nARVO's fix commit involved a submodule update to update all of Qt5's submodules. In contrast, OSV's fix commit correctly fixes the bug, which identified that the actual bug was not in Qt5 but in Qtbase. The fuzzer discovered an input that triggered a crash within Qtbase instead of Qt5. Because the way ARVO's fix locator works, it identified submodule commit as the fix, because the submodule update commit also updates Qtbase from a vulnerable version to a fixed version, thus preventing the crash."}, {"title": "6 CASE STUDIES", "content": "This section presents two case studies showing how ARVO can be used for research purposes. The first explores an application of Large Language Models (LLMs) for bug repair, and the second discusses how we found hundreds of current zero-day bugs from OSS-Fuzz's bug reporting."}, {"title": "6.1 Evaluation of LLMs for Bug Repair", "content": "Recent research has found that large language models (LLMs) may be effective at automatically repairing security vulnerabilities. How- ever, existing work has only evaluated LLMs' capabilities at this task on a handful of vulnerabilities (e.g., Pearce et al. [21"}]}