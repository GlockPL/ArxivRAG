{"title": "The Text Classification Pipeline: Starting Shallow, going Deeper", "authors": ["Marco Siino", "Ilenia Tinnirello", "Marco La Cascia"], "abstract": "Text Classification (TC) stands as a cornerstone within the realm of Natural Language Processing (NLP), particularly when viewed through the lens of computer science and engineering. The past decade has seen deep learning revolutionize TC, propelling advancements in text retrieval, categorization, information extraction, and summarization. The scholarly literature is rich with datasets, models, and evaluation criteria, with English being the predominant language of focus, despite studies involving Arabic, Chinese, Hindi, and others. The efficacy of TC models relies heavily on their ability to capture intricate textual relationships and nonlinear correlations, necessitating a comprehensive examination of the entire TC pipeline.\nIn the NLP domain, a plethora of text representation techniques and model architectures have emerged, with Large Language Models (LLMs) and Generative Pretrained Transformers (GPTs) at the forefront. These models are adept at transforming extensive textual data into meaningful vector representations that encapsulate semantic information. The multidisciplinary nature of TC, encompassing data mining, linguistics, and information retrieval, highlights the importance of collaborative research to advance the field. This work seeks to integrate traditional and contemporary text mining methodologies, fostering a holistic understanding of TC.\nThis monograph provides an in-depth exploration of the TC pipeline, with a particular emphasis on evaluating the impact of each component on the overall performance of TC models. The pipeline includes state-of-the-art datasets, text preprocessing techniques, text representation methods, classification models, evaluation metrics, current results and future trends. Each chapter meticulously examines these stages, presenting technical innovations and significant recent findings. The work critically assesses various classification strategies, offering comparative analyses, examples, case studies, and experimental evaluations. These contributions extend beyond a typical survey, providing a detailed and insightful exploration of TC.", "sections": [{"title": "Introduction", "content": "In several Natural Language Processing (NLP) applications like news categorization, sentiment analysis, and subject labelling, Text Classification (TC) is a crucial and relevant task. The goal of TC is to tag or label textual components like sentences, questions, paragraphs, and documents. In this era of massive information dissemination, manually processing and categorizing huge amounts of text data takes a relevant quantity of effort and time. To name a few, text information can be found on social media, websites, chat rooms, emails, questions and answers from customer service representatives, insurance claims and user reviews. Furthermore, human factors such as skills and fatigue can have a relevant influence on the effectiveness of manual TC. It is preferable to automate the TC pipeline involving machine learning models to get objective outcomes. Furthermore, to reduce the problem of information overloading, the improvement of information retrieval effectiveness can help in finding the necessary information for a certain task. In Figure 1 is illustrated a flowchart of the steps involved in TC, under the light of traditional and most recent machine learning models. A critical first stage is the preprocessing of the text to provide as input to the model. Classical approaches usually employ AI methods to collect relevant features, which are then classified with machine learning techniques. Next, the text representation approach can severely impact the outcomes of the model. Involving a series of transformations used to directly map a source text to predicted labels, deep learning, as opposed to traditional models, incorporates feature engineering into the process of training of the model. Up until 2010, classical TC models were the most used and popular. Some of them are Logistic Regressor (LR), Na\u00efve Bayes (NB), Support Vector Machine (SVM) and K-Nearest Neighbour (KNN). These methods clearly outperform past rule-based techniques in consistence and accuracy (Mitra et al., 2007; Atmadja and Purwarianti, 2015). However, they still require feature engineering and they are time-consuming. Additionally, it is hard to understand the semantic of the words since they frequently neglect the context or natural sequential arrangement of textual material. In TC, deep learning algorithms gradually took the place of traditional techniques by the 2010s. Deep learning techniques for text mining automatically construct semantically pertinent representations without the need for humans to define rules and features. Consequently, the majority of TC activities are based upon deep neural networks.\nMost conventional machine learning models use a two-step procedure. First, the documents are stripped of a number of manually added features (or any other textual unit). In the following phase, a classifier receives these features so it can produce a prediction. The Bag of Words (BoW) feature and its extensions are frequently created by hand. Hidden Markov Models, NB, SVM, Random Forests (RF) and Gradient Boosting (GB), are some common classification algorithms employed in the second step. Numerous disadvantages exist with this two- step approach. For instance, using handcrafted features and expecting acceptable performance requires time-consuming feature engineering and analysis. Due to the strategy's heavy reliance on domain expertise for feature generation, it is also difficult to adapt it to new applications. Last but not least, because of the very specific features domain, these models cannot fully benefit from the vast volumes of training data available. To address the issues with the use of handcrafted features, the use of neural approaches has increased. The main component of these approaches is an embedding space, where text is encoded as a low-dimensional continuous features vector without the need for traditional features representation strategies. The Latent Semantic Analysis (LSA) proposed in Landauer and Dumais, 1997 is one of the earliest studies on embedding models. The proposed architecture is trained on 200K words and has fewer than 1 million parameters. In Bengio et al., 2000, the first neural language model was proposed. It consisted of an artificial neural network trained on over 10 million words. When progressively larger embedding models were constructed with significantly more training data, a paradigm change occurred. A number of Word2Vec models that Google creates in 2013 (Mikolov et al., 2013b) were trained using billions of words and quickly gained popularity for numerous NLP applications. As the basis for their contextual embedding model, the researchers from AI2\u00b9 and the University of Washington created a Bidirectional-Long Short Term Memory (BiLSTM) network using 93 million hyperparameters and a training performed on billions of words in 2017. A novel model named Embedding from Language Models (ELMO) (Peters et al., 2018) captures contextual information and performs significantly better than Word2Vec because. This subsequent development results in the construction of embedding models using Google's new neural architecture, the Transformer (Vaswani et al., 2017).\nTransformer is entirely attention-based, which significantly boosts the effectiveness of extensive model training on Tensor Processing Unit (TPU). In the same year, Google created the Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). BERT has 340M parameters and was trained on 3.3 billion words. More training data and larger models are proposed in the literature every day. The most recent OpenAI GPT model has more than 170 billion parameters Dale, 2021 and it is based on Transformers. Some academics contend that despite the enormous models' remarkable performance on different NLP tasks, they do not truly grasp language and are insufficient for many domains that are mission-critical (Jin et al., 2020; Marcus and Davis, 2019). Recently, there is a rise of interest toward neuro-symbolic hybrid models to solve significant flaws of neural models like interpretability, inability to use symbolic thinking and lack of grounding (Schlag et al., 2019; Gao et al., 2020).\nAlthough there are many excellent reviews and textbooks on TC techniques and applications, this work provides a thorough analysis of all the phases that go into creating a TC pipeline with several contributions, including novel and deep experiments to further investigate the impact on the performance of each stage of the pipeline. These contributions are usually reported at the end of each chapter as case studies. Even if specific languages are considered in the related works, from the standpoint of computer science, English is the language that is most frequently used and referred in the present literature regarding TC. Furthermore, most of the Large Language Models (LLMs) and pre-trained word embeddings are originally developed focusing on English, partially or totally neglecting the other languages. The rest of this work primarily uses the English as the reference language for many of the examples and cases presented and discussed.\nStarting with a discussion on some of the more contemporary tasks such as author profiling, topic classification, news classification, sentiment analysis we then present SOTA models and most recent and relevant findings. We also cover the most recent deep neural network architectures, which are divided into a number of types based on their functioning, including Transformers (LLMs and GPTs), Convolutional Neural Networks (CNNs), Capsule Nets and Recurrent Neural Networks (RNNs)."}, {"title": "Overview and contributions", "content": "This monograph is organized as follows: Chapter 2 presents the most common datasets used and available in the literature and the most used metrics. Then, we propose and discuss a dataset analysis and a data augmentation strategy to improve the performance of a classifier. In Chapter 3, the preprocessing technique to prepare raw text are presented and discussed. There, we further investigate and evaluate the impact of the most common techniques on SOTA models and datasets. In Chapter 4 the methods to represent text in a numerical way understandable by a computer are reported. In this chapter we also propose a strategy based on PCA, to visualize and analyse a word embedding space trained from scratch. In Chapter 5, traditional and modern classifiers commonly employed for TC are discussed, including some of our findings and results concerning a signal analysis through the layers of a shallow CNN, and a deeper discussion on modern LLMs and GPTs. In Chapter 6 generic and linguistic-specific metrics to evaluate the performance on TC tasks are discussed. In Chapter 7 the conclusions and the future perspectives are presented. The contributions and a summary for each chapter of this work are reported in what follows.\nSeveral works have investigated TC techniques from a general standpoint. We specifically mention the work in Li et al., 2020, which offers a thorough analysis of model architectures, spanning from traditional to the modern deep learning-based ones. The survey by Kowsari et al., 2019 offers a great examination of preprocessing procedures, including feature extraction and dimensionality reduction. However, despite including quantitative outcomes of conventional approaches, Minaee et al., 2021 mainly focuses on deep learning models. By providing a view of each stage required to design a TC model, this monograph seeks to enhance the landscape of TC from a general point of view. As a result, we give a thorough explanation of the key data preparation procedures used along with TC models. We provide model descriptions from traditional ones to deep learning-based ones from more recent years, in contrast to prior TC evaluations. The design of the classifier and feature extraction are highlighted for the traditional models.\nA specific overview for each chapter of this work is reported to conclude this section. Along with the background on the pipeline stage involved, the last part of each chapter is dedicated to case studies, sup- ported by experiments, models and/or methods proposed, quantitative and qualitative analysis."}, {"title": "Overview of Chapter 2: Challenges, datasets and dataset analysis and augmentation in TC", "content": "In the early history of machine learning, information retrieval systems primarily used TC algorithms. But as technology has developed over time, TC and document categorization have become widely employed in several fields, including law, engineering, social sciences, healthcare, psychology, and medicine. We highlight some domains that use TC algorithms in this section. Some TC tasks are discussed in this chapter, including three new datasets related to emerging author profiling tasks. The datasets available in the literature and related to these tasks and usually employed as benchmark, are also reported and presented in this chapter.\nThe contributions for this section are two. With the first, we present and discuss a strategy for a preliminary linguistic analysis of a dataset. Such an analysis can eventually drive subsequent choices in the development of the steps involved in the TC pipeline. With the second contribution, we introduce and discuss a novel data augmentation technique based on backtranslation. Thanks to this data augmentation strategy, model performance can be improved on several tasks."}, {"title": "Overview of Chapter 3: Text preprocessing", "content": "In this chapter we collect, report and discuss the text preprocessing techniques found in the literature and their possible and most recent variants, proposing a nomenclature standard based on acronyms. We also provide the reader with useful information for self-study and in-depth study of the techniques presented along with advices on how to operate educated choices to select the preprocessing technique (or combination of techniques) given a specific task, model, and dataset."}, {"title": "Overview and contributions", "content": "Our contributions are reported in the last section and concern several experiments. Specifically, we select the three most common techniques used in the literature to evaluate the impact of each of these techniques (alone or in combination) on the classification results of nine SOTA models (pre-trained deep, deep and non-deep) and on real world datasets. Then we evaluate how text preprocessing can affect the performance of modern pre-trained architectures based on attention (i.e., Transformers) compared to traditional ones. Finally, we determine if simple classifiers' performance are comparable to the ones obtained by Transformer-based models when text preprocessing is performed in accordance with the specific model and dataset used."}, {"title": "Overview of Chapter 4: Text representation", "content": "Before moving to the classification stage, it is necessary to convert unstructured data, especially free-running text data, into organized numerical data. To do this, a document representation model must be used to employ a subsequent classification system following the text preprocessing stage. Text representation models convert text data into a numerical vector space, which has a substantial impact on how well subsequent learning tasks can perform. In the history of NLP, word representation has always been a topic of interest. It is crucial to properly represent such text data, since it contains a wealth of information and may be applied broadly across a variety of applications. This chapter examines the expressive potential of several word representation models, ranging from the traditional to the contemporary SOTA word representation provided by large language models.\nThe chapter discusses numerous representation models that are frequently employed in the literature. Before discussing well-known rep- resentation learning and pre-trained language models, we first discuss various statistical models. Then we move to attention-based represen- tation and, in the last section of this chapter, is reported a case study about the analysis of a trained word embedding for a specific TC task. Thanks to a Principal Component Analysis (PCA) tool, it is shown and analysed the effect of a CNN training on a 3D visualization of a word embedding space. This way we are able to understand some implicit choices operated during the training of a deep learning model, to assign specific word vectors to certain keywords belonging to one of the two class labels used for the task."}, {"title": "Overview of Chapter 5: Text classification methods", "content": "In Chapter 5 are reported both the traditional methods for TC and the most modern ones based on deep learning. Models discussed in this chapter belong to three different groups. The non-deep learning deterministic models, the foundational deep learning models and the large pre-trained language models known as Transformers. The term \"earlier approaches\" refers to all techniques used before the advent of deep neural networks, when the prediction was based on manually created features. Neural networks with only a few hidden layers are also included in this category, and these are so-called \u201cshallow\u201d networks. These methods replace several rule-based ones, which they outperformed in terms of accuracy. The most recent deep learning models, which have an impact on all artificial intelligence domains, including TC, are also discussed. These techniques have become popular because they can simulate intricate features without requiring manual engineering, which reduces the need for subject expertise.\nIn the last section, we present and discuss real-world competitive models as case studies to address some SOTA task about TC. Finally, we present some approaches we used to perform a post-hoc analysis on a SOTA deep model to explore the results of the predictions provided. We perform a signal analysis of the CNN layer's output to understand the behaviour of the network, either during the training phase and during the inference phase. We propose a methodology to further in- vestigate the behaviour of a deep learning model, looking also at its predictions and at the outputs provided by the intermediate layers of the model. The analysis presented was conducted focusing on a fake news spreaders dataset to explore the behaviour of a shallow CNN. \u03a4\u03bf perform this exploration, we looked at the predictions provided after completing the training phase Siino et al., 2022a. This further step can be employed in the TC pipeline to improve the model performance and for a deeper understanding of its behaviours. Finally, we discuss Transformers (LLMs and GPTs) and the recent and emerging discipline of Prompt Engineering. We discuss several prompting techniques, and then we move to some ethical considerations on the use of generative \u0391\u0399."}, {"title": "Summary of Chapter 6: Evaluation Metrics", "content": "This chapter focuses on how to evaluate the performance of deep learn- ing models in the context of text classification tasks, introducing the most used metrics in the literature. We discussed various metrics such as accuracy, precision, recall, and F1 score, emphasizing the importance of selecting the right metric based on the specific goals. In addition, we explored the limitations of traditional evaluation metrics and high- lighted the necessity for more sophisticated approaches, particularly in scenarios involving imbalanced datasets. The use of confusion matrices and ROC-AUC scores were recommended to provide a more comprehen- sive evaluation of model performance, along with metrics as rouge and BLEU for tasks involving text generation and summarization. Moreover, we proposed the integration of human evaluation methods to supplement quantitative metrics, recognizing that the nuances of language often elude numerical representation."}, {"title": "Overview of Chapter 7: Conclusions and future perspectives", "content": "In the last chapter of this work, we report the final conclusions and future perspectives on the matter."}, {"title": "Tasks and datasets", "content": "The process of organizing texts, such as tweets, news articles, and customer reviews, into distinct categories can be broadly considered a form of Text Classification (TC). Common TC tasks include topic classification, news categorization, and sentiment analysis. Recent re- search has shown that by enabling text classifiers to process pairs of texts as inputs, various natural language understanding tasks\u2014such as natural language inference and extractive question answering\u2014can be effectively framed as TC problems. However, these tasks often do not operate within a finite and predefined set of labels, making them less typical of traditional TC. The initial section of this chapter introduces several popular TC tasks from the literature.\nThe availability of labelled datasets has been a significant driver in the rapid advancement of the TC field. The datasets presented in this chapter are frequently utilized as benchmarks in TC-related research. In this introduction part, we list the domain-specific properties of these datasets and provide an overview in the Table 2.1 that shows the task description, the overall sample count, the number of target classes, and articles presenting the corresponding dataset.\nThe TC tasks presented here are:\n\u2022 Author profiling\n\u2022 Topic classification\n\u2022 News classification\n\u2022 Sentiment analysis\nThe final section of this chapter introduces a methodology for analysing and evaluating datasets from a linguistic perspective. This preliminary analysis can guide subsequent steps in the classification pipeline. Additionally, we propose a data augmentation strategy based on backtranslation to automatically expose latent semantic information present in the text."}, {"title": "Research areas", "content": "One of the three main areas of automatic authorship identification, alongside authorship attribution and authorship verification, is author profiling. The development of this field began to take shape at the turn of the 20th century. Initially, the approach was applied to the writ- ings of Francis Bacon, William Shakespeare, and Christopher Marlowe by American self-taught physicist and meteorologist Thomas Corwin Mendenhall. Mendenhall analysed the word lengths of these authors to identify quantitative stylistic variations.\nAuthor profiling involves the analysis of a corpus of texts to deter- mine the author's identity or to identify distinct traits of the author based on stylistic and content-based factors. Commonly analysed factors include age and gender, but recent research has also explored additional aspects such as personality traits and occupation Wiegmann et al., 2020. Author profiling is valuable in various sectors, particularly forensics and marketing, where identifying specific traits of a text's author is crucial. The task of author profiling can vary depending on the application, the traits to be identified, the number of authors studied, and the volume of texts available for analysis. While traditionally focused on written works such as literary texts, the scope has expanded to include online texts with the advent of computers and the Internet.\nDespite significant advancements in the 21st century, author profiling remains a challenging and not fully resolved process. Below are some well-known author profiling datasets that have been featured in recent literature.\n\u2022 Fake News Spreaders (FNS).The FNS dataset is presented and discussed in Pardo et al., 2020 and available under request7. The dataset was used for the international shared task at PAN8.\nThe organizers of the task aim to determine whether it is feasible to differentiate between authors who have previously disseminated fake news and those who have not.\nThe dataset comprises tweets in both Spanish and English. Each author in the dataset is represented by one hundred tweets, along with a corresponding class label indicating whether the author has shared fake news in the past (labelled as 1) or not (labelled as\n\u2022 Hate Speech Spreaders (HSS). The HSS dataset is presented and discussed in Rangel et al., 2021b. As an initial step in curbing the spread of hate speech among online users, the task's organizers aim to identify potential Twitter users who disseminate hate speech.\nThe dataset includes tweets in both Spanish and English. Each author in the dataset is represented by two hundred tweets, along with a corresponding class label indicating whether the author has shared hate speech in the past (labelled as 1) or not (labelled as 0). For each language, the training set includes 100 authors per class, while the test set includes 50 authors per class. In total, the dataset comprises 600 authors, amounting to 120,000 tweets. The results of the participants in the Hate Speech Spreader (HSS) task are publicly available10.\n\u2022 Irony and Stereotype Spreaders (ISS). The ISS dataset is presented and discussed in Bueno et al., 2022b; Bevendorff et al., 2022b and available under request11. The dataset was used for the international shared task at PAN12. The task's organizers want to focus on irony. Especially when words are used subtly and figuratively to indicate the opposite of what is literally expressed. A more violent version of irony, sarcasm aims to mock or ridicule a target without necessarily restricting the possibilities of hurting them. The objective is profiling users whose tweets can be labelled as sarcastic.\nA group of 600 Twitter authors make up the dataset that the"}, {"title": "Topic classification", "content": "Topic classification, often referred to as topic analysis, aims to identify the main theme or themes of a text (for example, determining whether a product review pertains to \"ease of use\" or \"customer assistance\"). In topic analysis, the intricate textual theme is defined to ascertain the text's meaning. A crucial aspect of this method is topic labelling, which involves assigning themes to documents to streamline the topic analysis process. Below, we list several state-of-the-art (SOTA) datasets used in this domain.\n\u2022 Patronizing and Condescending Language (PCL). De- scribed in detail in P\u00e9rez-Almendros et al., 2022, the dataset for (PCL) is from the detecting PCL task hosted at SemEval- 2022. Such a task is an emerging one about detecting PCL P\u00e9rez- Almendros et al., 2020. PCL occurs when language implies su- periority over others, talks down to them, or portrays them or their circumstances in a kind but belittling manner, often evoking feelings of pity or compassion. PCL is typically involuntary and unconscious, often stemming from good intentions. To fulfil the task, a classifier must ascertain whether PCL is present in a given text. The dataset is available on GitHub14.\n\u2022 DBpedia. Wikipedia's most frequently used info boxes were used to create the DBpedia Lehmann et al., 2015, a sizable multilingual knowledge library. Every month, it releases a new edition of DBpedia, adding or removing classes and attributes. The most\n\u2022 Ohsumed. The Ohsumed15 has a MEDLINE database affiliation. There are 23 categories for cardiovascular diseases and 7,400 texts overall. All texts are classified into one or more classes and are abstracts of medical information.\n\u2022 ISTO Fake News dataset. The dataset16 contains two types of articles: fake and real news. This dataset was collected from real world sources; the truthful articles were obtained by crawling articles from Reuters.com (News website). As for the fake news articles, they were collected from different sources. The fake news articles were collected from unreliable websites that were flagged by Politifact (a fact-checking organization in the USA) and Wikipedia. The dataset contains different types of articles on different topics, however, the majority of articles focus on political and World news topics.\n\u2022 EUR-Lex. The EUR-Lex dataset Loza Menc\u00eda and F\u00fcrnkranz, 2008 consists of several document categories that are indexed in accordance with a number of orthogonal categorization systems to enable a variety of search functions. With 19,314 documents and 3,956 categories, the most widely used variant of the dataset is based on various parts of EU laws.\n\u2022 Yahoo! Answer. The Yahoo! Answer17 dataset Zhang et al., 2015 is about topic labelling with 10 different classes. Per class, there are 6,000 and 140,000 samples to test and train respectively. Three components, referred to as question titles, question contexts, and best responses, are included in every sentence.\n\u2022 Web Of Science (WOS). The WOS dataset Kowsari et al., 2017 is a set of information and meta-information about articles that is available via Web of Science, the most reputable global citation"}, {"title": "News classification", "content": "News classification involves the automated categorization of news ar- ticles into predefined tags based on their content, with the model's accuracy derived from training on labelled news records. News items can be categorized into various domains such as business, entertain- ment, politics, sports, technology, and more. A news classification system helps users efficiently find articles of interest, saving time and reducing information overload.\nNews content is one of the most critical sources of information. A news classification system enables users to access essential knowledge promptly. The task of categorizing news items by topic or user interest is crucial. By leveraging user preferences, identifying emerging news topics, or recommending relevant material, a news classification model assists individuals in obtaining real-time information tailored to their needs. Here, we delve into the details of several commonly used datasets in this domain.\n\u2022 Google News. The Google News dataset presented in Das et al., 2007 is made up by two datasets. The first one consists of a subset of clicks received on the Google News website over a certain time period, from the top 5000 users (top as sorted by the number of clicks). There are about 40,000 unique items that are part of this dataset and about 370,000 clicks. The second dataset is similar to the previous one (in fact a superset), and just contains more records: 500,000 users, 190,000 unique items and about 10,000,000 clicks. In order to have uniformity in comparisons, authors binarized the first dataset as follows: if the rating for an item, by a user, is larger than the average rating by this user (average computed over her set of ratings) they assign it a binary rating of 1, 0 otherwise.\n\u2022 Reuters news. The Reuters-21578 dataset 18 is often used for text categorization. It was gathered by the Reuters economic press release service in 1987. A version of Reuters-21578 with multiple classes containing 10,788 documents is called ModApte. 90 lessons, 7,769 training samples, and 3,019 test samples are included. R8, R52, RCV1, and RCV1-v2 are additional datasets generated from a portion of the Reuters dataset.\n\u2022 20 Newsgroup (20NG). The 20NG dataset19 consists of news- group documents that were posted on 20 various themes. For text categorization, text clustering, and other tasks, different variations of this dataset are employed. One of the most often used versions has 18,821 papers, evenly distributed among all topics.\n\u2022 AG News. The AG News dataset20 consists of news articles compiled by academic news search engine ComeToMyHead from more than 2,000 news sources. It makes advantage of each news story's title and description fields. A total of 120,000 training texts and 7,600 test texts are included in AG. Each sample consists of a brief sentence that has a four-class label.\n\u2022 Sogou. The SogouCS and SogouCA news sets are included in the Sogou21 dataset, which combines both of them. The name of the domains within the URL serve as the labels for each text. So, as the classification labels for the news, the domain names in their URLs are used. For illustration, the news at http://sports.sohu.com is classed under the sports category."}, {"title": "Sentiment analysis", "content": "Sentiment analysis, often referred to as opinion mining or emotion \u0391\u0399, involves the systematic identification, extraction, quantification, and study of affective states and subjective information using NLP, text analysis, computational linguistics, and biometrics. This technique is widely applied in marketing, customer service, and clinical medical settings. It is employed to analyse voice of the customer materials, including reviews and survey responses, as well as content from the internet and social media, and healthcare documents.\nThis category of tasks involves identifying the polarity and perspec- tive of users' opinions in text, such as tweets, movie reviews, or product reviews. Unlike traditional text classification (TC), which focuses on the objective content of the text, sentiment analysis aims to determine whether the text supports a particular viewpoint. It may also involve understanding the emotional states and subjective information conveyed in the text, often categorized by the emotions evoked. The task can be modelled as either a binary problem, classifying texts into negative and positive categories, or a multi-label task, grouping texts into multiple sentiment labels. Here, we present details of some of the most commonly used datasets in the literature, which serve as benchmarks for sentiment analysis.\n\u2022 Movie Review (MR). The MR dataset (Pang et al., 2002) is a set of film reviews that was created with the goal of identifying the sentiment attached to each user review and deciding whether it is positive or negative. There is a sentence for each review. There are 5,331 positive samples and 5,331 negative samples in the corpus.\n\u2022 Stanford Sentiment Treebank (SST). The SST dataset (Socher et al., 2013) extends MR. It has two categories: one with binary labels and the other with fine-grained (five-class) labels. Namely, SST-1 and SST-2, respectively. There are 8,544/1,101/2,210 sam- ples, in train/dev/test set respectively for a total of 11,855 movie reviews in SST-1. SST-2 is divided into train, dev and test sets, with respective sizes of 6,920, 872, and 1,821.\n\u2022 Multi-Perspective Question Answering (MPQA). The MPQA is an opinion dataset (Deng and Wiebe, 2015). It also has two class labels and an MPQA dataset of opinion polarity detecting sub-tasks. In total, 10,606 phrases from news stories from various news sources are included in MPQA. It should be"}, {"title": "Case Study: Dataset Analysis", "content": "The analysis example presented in this section was originally conducted in Siino et al., 2022a and is based on the Fake News Spreader (FNS) dataset. The discussion of the tools and methods described here can aid in the better development of subsequent stages in the classification pipeline.\nThe Profiling Fake News Spreaders Task (PFNSOT) dataset is a multilingual collection comprising Spanish and English tweets. For each language, the dataset includes 100 tweets per author and features 150 authors per class (i.e., FNS and non-FNS) in the training set, and 100 authors per class in the test set. We chose to use the PFNSOT dataset for two primary reasons: PAN's established tradition in organizing shared tasks, and the comparability of our extensive tests on several state-of- the-art (SOTA) models with the results of other task participants.\nWhile the task organizers encouraged the submission of multilingual models, submissions focusing on a single language were also accepted. As noted in the task overview, participant results indicated lower binary accuracy for the English language. To gain deeper insights into the dataset, we conducted both quantitative and qualitative investigations using established corpus linguistics methods, implemented in the well- known online corpus linguistics tool, Sketch Engine23 (Kilgarriff et al., 2014)."}, {"title": "Compare Corpora", "content": "This subsection provides a quantitative description of the Spanish and English datasets", "train_1": "their similarity measure is 1.41", "Spanish": 0.33, "English": 0.25}]}