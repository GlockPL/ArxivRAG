{"title": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control in Partially Observable Distribution Networks", "authors": ["Qiong Liu", "Ye Guo", "Tong Xu"], "abstract": "Inverter-based volt-var control is studied in this paper. One key issue in DRL-based approaches is the limited measurement deployment in active distribution networks, which leads to problems of a partially observable state and unknown reward. To address those problems, this paper proposes a robust DRL approach with a conservative critic and a surrogate reward. The conservative critic utilizes the quantile regression technology to estimate conservative state-action value function based on the partially observable state, which helps to train a robust policy; the surrogate rewards of power loss and voltage violation are designed that can be calculated from the limited measurements. The proposed approach optimizes the power loss of the whole network and the voltage profile of buses with measurable voltages while indirectly improving the voltage profile of other buses. Extensive simulations verify the effectiveness of the robust DRL approach in different limited measurement conditions, even when only the active power injection of the root bus and less than 10% of bus voltages are measurable.", "sections": [{"title": "1. Introduction", "content": "To achieve a carbon-neutral society, active distribu-tion networks (ADNs) will incorporate more renewable distributed generators (DGs) like PV and wind turbines. The generation output of these DGs is highly volatile and uncertain, which can cause problems such as voltage vi-olations and increased power loss. Meanwhile, most DGs are inverter-based (IB) devices that are capable of fast reactive control. This makes it increasingly attractive to use these IB devices in real-time volt-var control (VVC), which minimizes power loss and optimizes voltage profiles in ADNs [1, 2]. Such a voltage control paradigm is also known as \"IB-VVC\" [3].\nHowever, for ADNs with massive inverter-based energy resources, three challenges may emerge in VVC algorithm design: 1) IB-VVC needs to make real-time decisions to mitigate voltage fluctuations due to the high volatility and uncertainty of renewable energy [4]. 2) Current models of ADNs usually involve significant errors or even unknown parts [5]. 3) ADNs usually have limited measurement de-ployments [6].\nModel-based IB-VVC approaches have severe limita-tions in addressing those challenges. It is a nonlinear pro-gramming problem requiring time-consuming iterative steps to converge for off-the-shelf NLP solvers such as IPOPT solvers [5].\nThe performance of the model-based approach is vulner-able to parameter errors or unknown model [7]. In addition, when some load and generation values are unknown, model-based approaches are difficult to implement. One of the methods is to supplement those unknown data using pseudo-measurements, but those data are not real-time and may be full of noise [8]. State estimation can estimate the unknown data and filter out noise, which has been widely investigated [9]. However, it relies heavily on the accurate power flow model and may not work well for ADNs with inaccurate or unknown models. Sometimes, the pseudo-measurements may not be available for ADNs with weak measurement conditions.\nExisting literature has fully demonstrated the advantages of DRL approaches in addressing the former two challenges [10-18]. First, in the application stage, DRL achieves real-time control by performing a fast-forward calculation of policy networks. The time-consuming optimization process is shifted to the training stage [14, 15]. Second, DRL is a model-free method that learns to make near-optimal de-cisions by interacting with the ADN environments [18]. However, there is still insufficient research on robust DRL algorithms that are applicable to limited measurement con-ditions.\nDRL is a potential method to address the challenge of limited measurement deployments, but it has seldom been studied. The limited measurements lead to the problem of a partially observable state and unknown reward of the Markov decision process (MDP) in DRL. In cases where the reward is known, some DRL algorithms, such as SAC [19] and PPO [20], can be applied directly to the partially observable tasks with slight modifications [21]. However, the decision performance may be affected due to the missing real-time measurements [4]. For IB-VVC tasks, we can also use pseudo-measurements to complement the unknown state and introduce a physics-informed global graph attention network and a deep auto-encoder to filter the noise of the"}, {"title": "2. Preliminary", "content": "We considers an ADN with N + 1 buses, which can be represented by a graph G(N,E) with the collection of all buses N = 0, ..., N and the collection of all branches E = (i, j) \u2208 N \u00d7 N. Bus 0 denotes the graph root, and all tree branches are in the direction away from the root.\nIB-VVC optimizes power loss and voltage profiles by controlling the reactive power output of distributed gener-ators (DGs) and static Var compensators (SVCs). It can be formulated as an optimal power flow as follows [1, 24]:"}, {"title": "2.1. Inverter-based Volt-Var Control", "content": "1) The objective function of the optimization problem is minimizing the active power loss $P_{loss}$ of the networks:\n$\\min P_{loss} = \\sum_{i\\in N} P_{Gi} - \\sum_{i\\in N} P_{Di} + P_{0},$\t\t\t\t\t\t(1)\nwhere $P_{Gi}, P_{Di}$ are the active powers of DG and load at bus i. For the buses without DG, load, or SVC, the corresponding active and reactive powers are zero. $P_{0}$ is the active power injection of the root bus.\n2) The Dist-flow constraints are:\n$P_{j} = \\sum_{k:j \\to k}P_{jk} - \\sum_{i:i \\to j} P_{i} - \\frac{(P_{ij})^{2} + (Q_{ij})^{2}}{V_{i}^{2}}r_{ij},$\t\t\t(2)\n$Q_{j} = \\sum_{k:j \\to k}Q_{jk} - \\sum_{i:i \\to j} Q_{i} - \\frac{(P_{ij})^{2} + (Q_{ij})^{2}}{V_{i}^{2}}x_{ij},$\n$V_{j}^{2} = V_{i}^{2} - 2 (r_{ij} P_{ij} + x_{ij}Q_{ij}) + \\frac{r_{ij}^{2} + x_{ij}^{2}}{V_{i}^{2}}(P_{ij}^{2} + Q_{ij}^{2})$\nand\n$P_{j} = \\begin{cases}P_{G0} - P_{D0},& j = 0,\\P_{Gj} - P_{Dj},& j = 1, ..., N\\end{cases}$\t\t\t\t\t\t(3)\n$Q_{j} = \\begin{cases}Q_{G0} + Q_{C0},& j = 0,\\Q_{Gj} + Q_{Cj} - Q_{Dj},& j = 1, ..., N,\\end{cases}$\nwhere $\\forall j \\in N, P_{j}, Q_{j}$ denote the active and reactive power injections at bus j. $P_{jk}, Q_{jk}$ denotes the active and reactive power flow from bus j to k. $V_{i}$ is the voltage magnitude at bus i. $r_{ij}, x_{ij}$ are the resistance and reactance of branch ij. $i \\to j$ denotes a branch from bus i to bus j. $P_{Gj}, Q_{Gj}$ denote the active and reactive power generations of the DG at bus j. $P_{Dj}, Q_{Dj}$ denote the active and reactive power loads at bus j. $Q_{Cj}$ denotes the reactive power generation of the SVC at bus j. This paper sets the reference voltage bus as $V_{0} = 1 p.u.$ at bus 0.\n3) The voltage constraints are:\n$\\underline{V} \\leq V_{i} \\leq \\overline{V},$\t\t\t\t\t\t(4)\nwhere $\\underline{V_{i}}$ and $\\overline{V_{i}}$ are the upper and lower bounds of the voltage magnitude at bus i, respectively.\n4) The reactive power capability constraints of DGs and SVCs are:\n$\\underline{Q_{Gi}} \\leq Q_{Gi} \\leq \\overline{Q_{Gi}},$\t\t\t\t\t(5)\n$\\underline{Q_{Ci}} \\leq Q_{Ci} \\leq \\overline{Q_{Ci}},$\nwhere $\\overline{Q_{Gi}}, \\underline{Q_{Gi}}$ and $\\overline{Q_{Ci}}, \\underline{Q_{Ci}}$ the upper and lower limits of reactive power generation of the DG and SVC at bus i, respectively.\nThe variables of IB-VVC can be classified as follows: $P_{D}, Q_{D}, P_{G}$ are uncontrollable variables, $V, P_{0}, Q_{0}, I$ are dependence variables, $Q_{G}, Q_{C}$ are controllable (indepen-dent) variables [25]. Decision variables include controllable variables and dependence variables, respectively. Giving $P_{D}, Q_{D}, P_{G}$, IB-VVC minimizes the power loss and ensures the voltage is in its normal range by deciding controllable variables. The dependence variables are solved as well."}, {"title": "2.2. Preliminary of Deep Reinforcement Learning", "content": "The objective of the RL algorithm is to learn a policy $\\pi$ to make decisions that maximize the expected discounted accumulated reward from a sequence of MDP data T:\n$\\pi^{*} = arg \\max_{\\pi} E[R(T)],$\t\t\t\t\t\t(6)\nwhere the policy is\n$\\alpha \\sim \\pi (\\cdot | s),$\t\t\t\t\t\t(7)\nthe discounted accumulated reward, also named return, is\n$R = \\sum_{t=0}^{\\infty} \\gamma^{t} r_{t},$\t\t\t\t\t\t(8)\nthe sequence of MDP data $T = (S_{0}, A_{0}, S_{1}, A_{1}, ...)$ and, E is a mathematical operation of expect expected value.\nFor the sake of designing RL algorithms, the state-action value functions are introduced to evaluate the policy $\\pi$, which is the expected return of an RL policy by starting in the state-action pair:\n$Q^{\\pi} (s, a) = E_{\\alpha \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t}| s_{t} = s, a_{t} = a].$\t\t\t\t\t(9)\nConsidering the control capabilities of inverter-based devices are fast and independent among different decision steps, DRL only needs to maximize the immediate reward [23, 26, 27]. We can simplify the state-action value function as\n$Q^{\\pi} (s, a) = E_{\\alpha \\sim \\pi} [r| s, a].$\t\t\t\t\t\t(10)\nThen, the objective of RL is to find a policy n to maximize the state-action value function:\n$\\pi^{*} = arg \\max_{\\alpha \\pi} Q^{\\pi} (s, a).$\t\t\t\t\t\t(11)"}, {"title": "2.2.1. Quantile Regression", "content": "Quantile regression is a statistical technique that predicts multiple conditional quantiles of a response variable distri-bution rather than a single mean value of traditional mean re-gression [28]. This approach provides a more comprehensive view of the uncertainty and variability in predictions. The $\\tau^{th}$ quantile of the conditional distribution of the response variable Y given the predictor variable X can be described as"}, {"title": "3. Problem Formulation: Partially Observable Markov Decision Process", "content": "The proposed robust DRL approach is model-free and learns to make decisions from limited measurements of ADNs. We consider the following measurement conditions: 1) Voltage magnitude measurements are deployed only on a few buses that need to be optimized; 2) Active and reactive power injection measurements are deployed on the root bus; 3) Branch current magnitude measurements are deployed only on a few distribution branches; 4) The active and reactive power load and the active power outputs of DGs are deployed only on a few buses.\nIB-VVC of an ADN with a limited measurement deploy-ment is formulated as a partially observable Markov decision process (POMDP) (S, O, A, R, \u00ceR), then is solved by the DRL approach. S is the state space, O is observation space, A is action space and R is the reward function. Different from the traditional POMDP (S, O, A, R, P) [21, 29] that assume the real reward R is known, there is an additional term: \u201cthe surrogate reward R\", to represent the real reward when the real reward is unknown. In addition, the proposed POMDP also omits the transition probability function P because the IB-VVC is a single period optimization task, and DRL only needs to maximize the immediate reward and does not need to consider the future state [23, 26, 27]. This kind of formulation is also named the contextual bandit [30].\nAt each time step, the state of the ADN environment is s \u2208 S, the DRL agent only observes a partial states, \u2208 O, and then outputs an action a \u2208 A based on its policy \u03c0 : S \u2192 A. After executing the action, the ADN environment transits to the next state s' with the corresponding real reward r, whereas the DRL agent only observes a partially observable state s' and the surrogate reward \u00ee. The DRL agent collects the data tuple (so, a\u2081, r\u2081) into the data buffer for training the actor and critic networks.\""}, {"title": "3.1. State", "content": "The state reflects the working condition completely of the ADN, including the uncontrollable, controllable, and de-pendence variables. s = (V, P0, Q0, I, PD, QD, PG, QG, QC). In fact, the uncontrollable variables (PD, QD, PG) is enough for IB-VVC decision-making. We add the redundant mea-surements (V, P0, Q0, I, QG, QC) to reduce the learning difficulties of DRL [31]. Additionally, similar to the state estimation, the redundant information may represent the state robustly by eliminating the measurement error. Note that the complete state can only be obtained for an ADN with enough measurements."}, {"title": "3.2. Observation", "content": "For an ADN with limited measurement deployed, the ob-servation is the partially observable state so = (Vo, Po, Qo, Io, PDO, QDO, PG0, QG, QC). The subscript \",\" indicates the observable measurements. The vector of controllable vari-ables QG, Qc is a linear mapping of the action of the DRL agent and does not need to be measured."}, {"title": "3.3. Action", "content": "The action is related to the controllable variables, which is defined as a = (aQg, aQc). For the convenience of con-straining the controllable variables, the activation function of the output layer of the actor network is set as \"Tanh\". Then the action a is always in (-1, 1). The controllable variables QG, QC are obtained through linear mapping of a from (-1, 1) to their output capability intervals. We can represent it as \u0438 = 0.5(\u04e3 \u2013 \u0438)a + 0.5(\u016b + u), where u is the concatenation of QG, QC, and u, \u016b are their upper and bottom bounds. The action can also be considered as the normalized controllable variable u, which may reduce the learning difficulties of neural networks."}, {"title": "3.4. Reward", "content": "The reward is to evaluate the optimization performance of the action, which should be calculated based on the P0, PG, PD of the next state s'. It consists of two sub-terms: the reward of power loss and the reward of voltage violation. The reward of power loss rp is defined as:\n$r_{p} = -(P_{0} + \\sum_{i\\in N_{G}}PGi - \\sum_{i\\in N_{D}}PDi).$\t\t\t\t\t\t(14)\nThe reward of power loss can also be analytically calcu-lated by $r_{p} = \u2013 \\sum_{i j \\in E}r_{ij} \\frac{P_{ij}^{2} + Q_{ij}^{2}}{V_{i}^{2}}$, but it is seldom been used because it requires model parameters $r_{ij}$ that is difficult to be acquired.\nThe reward of voltage violation r, is defined as\n$r_{v} = \u2212 \\sum_{i\\in N} Cui [max (\\underline{V}_{i} \u2013 V, 0) + max (V \u2013 \\overline{V}_{i}, 0)],$\t\t(15)"}, {"title": "3.5. Surrogate Reward", "content": "For the unknown reward of power loss, we first propose the indirect reward and then propose the surrogate reward based on the indirect reward, which are calculated based on the $P_{0}, V$ of the next state s'. The indirect reward is the negative value of active power of the root node,\n$\\tilde{p} = -P_{0}.$\t\t\t\t\t\t(16)\nWe can also understand that the indirect reward ensures that power loss at unobservable nodes is also optimized from the perspective of model-based IB-VVC. For a given $P_{D}, Q_{D}, P_{G}$, when model-based IB-VVC decides a control-lable reactive power $Q_{G}, Q_{C}$ resulting in a minimum power loss, then the active power output of root bus $P_{0}$ is also its minimum value because $P_{0} = \u2212 \\sum_{i\\in N} P_{Gi} + \\sum_{i\\in N} P_{Di} + P_{loss}$. This means that minimizing the power loss is equal to minimizing the active power injection of the root bus $P_{0}$. For the case of the partially observable state, even $P_{Gi}, P_{Di}$ may not be measurable; it can be inferred from the partially ob-servable states [32]. For similar partially observable states, $P_{Gi}, P_{Di}$ tend to be similar. In those cases, if the DRL agent trials an action and obtains a smaller $P_{0}$ compared to other actions, it also indicates a smaller power loss.\nNoting that $P_{0}$ includes the information of both rp and $\\sum_{i\\in N}(P_{Di} \u2013 PG_{i})$ and rv usually makes up only a small portion of $P_{0}$. Approximating $P_{0}$ in DRL algorithms may be more challenging than approximating rp. It tends to result in a slightly larger approximation error of the critic network and degrade the VVC performance. To alleviate the problem, the surrogate reward is designed as\n$\\hat{r}_{p} = \\tilde{p}-\\tilde{p}.$\t\t\t\t\t\t(17)\nNote that $\\tilde{p} = -P_{0}$ are calculated based on the element of the next partially observable state s', while $\\tilde{p} = -P_{0}$ are calculated based on the element of the recent partially observable state so. For a given partially observable state so, \u0159, is determined, if the DRL agent outputs an action a that maximizes the surrogate reward $\\hat{r}_{p}$, it also corresponds to the maximize value of rp, and \u00f5p. In addition, the surrogate reward reflects the change of power loss directly, thus pro-viding an apparent reward signal for DRL agents."}, {"title": "4. Robust Deep Reinforcement Learning Method", "content": "The structure of the proposed robust DRL is shown in Fig. 2. In the training stage, a batch of historical data is sampled from the data buffer. The proposed robust DRL approach trains a conservative critic of voltage violation and a critic of power loss by using the sampling data. Then, a robust actor is trained to output the action that maximizes the accumulative value of the two critics. In the execution stage, the robust actor receives the partially observable state from the ADN environment and then outputs an action. After executing the action in the ADN environment, the surrogate reward can be calculated based on available measurements. The data tuple, consisting of the partially observable state, surrogate reward, and action, is then uploaded to the data buffer."}, {"title": "4.1. Conservative Critic", "content": "In the case of partially observable states, there exists multiple state s corresponding to the same partially observ-able state so. We can use so to predict s but with a degree of uncertainty [32].\nFor the case with partially observable states, the critic network is\n$Q^{\\pi} (s_{0}, a) = E_{\\alpha \\sim \\pi} [r| s, a],$\t\t\t\t\t\t(19)\nand the objective of DRL is finding the optimal action a*,\n$a^{*} = arg \\max_{a} Q(s_{0}, a).$\t\t\t\t\t\t(20)\nDue to the uncertainties of predicting so based on s, $arg \\max Q(s, a) \\neq arg \\max Q(s_{0}, a)$ and it increases the risk of voltage violations.\nThere are two methods to address the problem. One method is adding more measurements into the partial ob-servable states to improve the measurement condition, which has been studied in paper [7]. When additional mea-surements are not available or insufficient, another method proposed in this paper is estimating the uncertainties of reward to the partially observable state, then constructing a conservative critic to estimate the conservative state-action value. The robust policy can be trained with the guidance of the conservative critic network.\nInspired by paper [33], we use quantile regression tech-nology to estimate the uncertainties of state-action value with respect to the partially observable state. Then state-action value (10) is transformed to\n$Q_{r|(s,a)}(\\tau) = inf{r : F_{r|(s,a)} \\geq \\tau},$\t\t\t\t\t\t(21)\nwhere is the quantile, F is the cumulative distribution function, and $Q_{r|(s,a)}(\\tau)$ is the $\\tau^{th}$ conditional quantile value of r given (s, a). We set as a small value, like 0.2 in this paper, to estimate the conservative sate-action value. It can be estimated by solving the optimization problem:\n$Q_{r|(s,a)} (\\tau) = arg \\min_{Q} E [(1 - \\tau)1_{Q(s_{0},a)>r} (r \u2013 Q(s_{0}, a))\n+ \\tau1_{Q(s_{0},a)<r} (r - Q(s_{0}, a))].$\t\t\t\t\t(22)\nOther uncertainty estimation technologies, like the Bayesian neural network, can solve the problem. Here, we select quantile regression because it has a simple structure and stable performance."}, {"title": "4.2. A Practical Algorithm: Robust Soft Actor-Two-Critic", "content": "The proposed robust soft actor-two-critic algorithm combines the soft actor-critic (SAC) [19], two-critic DRL framework [34], conservative critic, and surrogate reward. SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework that provides a robust and sample-efficient learning perfor-mance. It is a state-of-the-art DRL algorithm that has been widely utilized in VVC tasks [5, 7, 22]. The two-critic DRL framework is a specific scheme for IB-VVC, which uses two critic networks to approximate the two objectives of IB-VVC separately [34]. It reduces the learning difficulties of each critic and avoids the mutual effect of the optimization objectives in learning critic networks, thus accelerating the learning process and improving the VVC performance further. In this paper, the two-critic DRL framework also provides a convenient way to design the conservative critic solely for voltage violations.\nFor the IB-VVC task, SAC only learns to maximize the entropy-regularized reward [26, 27],\n$\\pi^{*} = arg \\max_{\\pi} E [r + aH (\\pi (\\cdot | s_{0}))],$\t\t\t\t\t\t(23)\nwhere the entropy is $H (\\pi (\\cdot | s_{0})) = E_{\\alpha \\sim \\pi(\\cdot | s_{0})} [\u2212 log \\pi (\\cdot | s_{0})]$.\nHere, the partially observable state so is utilized rather than S.\nIntegrating the two-critic DRL framework and the surro-gate rewards, the critic of power loss $Q_{p}(s, a)$ and the critic of voltage violation $Q_{v}(s, a)$ are introduced:\n$Q(s_{0}, a) = E_{\\alpha \\sim \\pi} [\\tilde{p}| s_{0}, a],$\t\t\t\t\t\t(24)\n$Q(s_{0}, a) = E_{\\alpha \\sim \\pi} [\\tilde{r}| s_{0}, a].$\t\t\t\t\t\t(25)\nIn this paper, the objective of IB-VVC is to minimize the expected power loss and decrease the risk of voltage violation, so we only apply the conservative critic scheme to the critic of voltage violation. Then, equation (25) is transformed to\n$Q_{r|(s,a)} (\\tau) = inf{r : F_{p_{v}|(s,a)} \\geq \\tau},$\t\t\t\t\t\t(26)\nWe set as a small value to have a conservative critic, like 0.2 in this paper.\nThe two critics $Q_{p} (s_{0}, a)$ and $Q_{v} (s_{0}, a)$ are parameter-ized by two neural networks $Q_{\\phi_{p}}$ and $Q_{\\phi_{v}}$, with learnable parameters of $\\phi_{p}$ and $\\phi_{v}$. The critic of power loss $Q_{\\phi_{p}} (s_{0}, a)$ in (24) is learned by minimizing the MSE losses\n$L_{Q} (\\phi_{p}) = \\frac{1}{|B|}\\sum_{(s_{0},a,\\hat{r}_{p}) \\in B} (Q_{\\phi_{p}}(s_{0}, a) - \\hat{r}_{p})^{2}.$\t\t\t\t\t\t(27)\nThe critic of voltage violation $Q_{v}(s_{0}, a)$ is learned by min-imizing the quantile regression loss\n$L_{Q_{\\phi_{v}}}(\\phi_{v}) = [(\u03c4 \u2212 1)1_{Q_{\\phi_{v}}(s_{0},a)>\\hat{r}_{v}} (\\hat{r}_{v} \u2013 Q_{\\phi_{v}} (s_{0}, a))\n+ \u03c41_{Q_{\\phi_{v}}(s_{0},a) < \\hat{r}_{v}} (\\hat{r}_{v} - Q_{\\phi_{v}}(s_{0}, a))].$\t\t\t\t\t(28)"}, {"title": "4.3. The minimum measurement condition", "content": "The proposed robust DRL approach is applicable to dif-ferent measurement conditions. The minimal measurement condition is the measurement of the active power injection of the root bus Po and a few bus voltage magnitudes V that need to be optimized. Those measurements can be used as the observation for the proposed DRL algorithm and are also enough to calculate the surrogate reward. Under minimal conditions, DRL algorithms still can work well, which will be verified in the following simulation."}, {"title": "5. Simulation", "content": "We conducted numerical simulations on 33, 69, and 118-bus distribution networks to demonstrate the advantages of the proposed robust DRL approach. Those distribution network data were downloaded from Matpower [35] and then converted to Pandapower [36]. The configurations of DGs and SVCs were shown in Table 1. There were 2, 4, and 8 DGs, and 1, 1, and 2 SVCs in 33, 69, and 118-bus, respectively. The reactive power capacities of all DGs and SVCs were set as 3 and 2 MVar. All load power varies between 50% and 150% of their default values following uniform distributions. The active generations were a uniform distribution in the interval [0.5, 1.5] MW in the 33 and 69 bus systems and [1, 2] MW in the 118 bus system. The lower and upper bounds of the voltage magnitudes are set as 0.95 and 1.05 p.u., respectively.\nAs shown in Table 2, three measurement conditions were tested to verify the effectiveness of the proposed robust DRL. 1) 01: O1 is the minimal measurement condition that only the active power of the root bus and the voltage magnitudes of a few buses were measurable. Here, we set the voltages in buses with DGs and SVCs to be measurable. 2) 02: Measurements in Ol plus some branch current magnitude measurements. There were 5, 8, and 16 branch current magnitude measurements in 33, 69, and 118 bus distribution networks, respectively. 3) 03: Measurements in O2 plus the active and reactive power of loads in the buses with DGs and SVCs and the active power generations of DGs. The detailed configuration can be seen in our open source code [37]. Three corresponding complete states are also designed. In three complete states, all corresponding voltages, active and reactive power of loads, the active power generations of DGs, and branch currents are measurable."}, {"title": "5.1. Effectiveness of the Proposed Robust DRL", "content": "This section conducts the following simulation to demon-strate the effectiveness of the proposed robust DRL algo-rithm for ADNs with limited measurement deployments:\n1.  The standard DRL approach is under the ideal mea-surement conditions. All measurements including V, PO, QO, I, PD, QD, PG, QG, QC are available. This means that the approach utilizes the complete state and complete reward (CC).\n2.  The proposed robust DRL approach is under three limited measurement conditions. This means that the approach utilizes the partial observable state and sur-rogate reward (PS). The three simulations are donated as PS-1, PS-2, and PS-3, respectively.\n3.  Model-based optimization was formulated based on the AC power flow model and solved by recalling PandaPower [36] with the interior point solver. To be consistent with the DRL approaches, in model-based optimization, the voltage constraints were only added to the buses with voltage measurements. The result can be seen as optimal, which is a baseline for the performance of DRL algorithms. Note that the approach only works on the ideal measurements and ideal power flow model condition.\n4.  The without-control strategy (W) sets the reactive power generation of DGs and SVC at zero.\nAll of the simulation experiments are tested on 33, 69, and 118 bus distribution networks to show the scalability of the proposed method. Python was used to implement the algorithms, PyTorch was used for the DRL algorithms, and Pandapower [36] was used to calculate the balanced power flow for simulating the ADN environments. We trained the"}, {"title": "5.2. The Effect of Partially Observable State and Surrogate Reward", "content": "To demonstrate the impact of partial observable reward and surrogate reward, we tested the three measurement conditions mentioned above with both surrogate reward and complete reward. Therefore, we designed the four types of simulation settings: 1) complete state and complete re-ward (CC), 2) complete state and surrogate reward (CS), 3) partially observable state and complete reward (PC), and 4) partially observable state and surrogate reward (PS). In those DRL approaches, we calculate the reward for voltage violation using equation (18).\nComparing simulation results between CC and PC and between CS and PS in Fig 5 showed the effect of the partially observable state. It was apparent that the partially observable state degraded the VVC performance and slightly increased the power loss and voltage violation rate. We also observed that increasing measurement conditions from O1 to O3 of PC or PS improves VVC performance in power loss and voltage violation. Starting from the minimal measurement condition (01), VVC performance increased when addi-tional branch current magnitudes were added into the state. Further improvements were achieved by adding additional power injection of a few buses into the state. Generally, the improvement rate from Ol to O2 was larger than that of O2 to 03. There were only two exceptions when comparing the power loss of PC-O2 VS PC-O3, and PS-O2 VS PS-O3 in the 33 bus distribution system, and one exception when comparing the voltage violation of PC-O2 VS PC-O3 in the 118 bus distribution system.\nFor complete states, adding a few branch currents and additional power injection of a few buses also enhances VVC performance. The results were consistent among all experiments when comparing experiments O1, O2, and O3 between CC and CS. It verified that the redundant infor-mation reduces DRL's learning difficulties. Compared with paper [31] that only verified the rule that uses bus voltages as redundant information in supervised learning paradigms, this paper also verified the effectiveness of using branch currents as redundant information in DRL paradigms.\nComparing the simulation results of CC vs. CS and PC vs. PS in Figure 5 showed the effect of the surrogate reward of power loss. We found that the power loss error in CC and PC was slightly higher than the corresponding CS and PS. This verified that the proposed approach can optimize the power loss of the whole network. Reasonably, due to limited measurement deployments, the VVC performance experienced a slight degradation.\nThe simulation results in Figure 4 showed the effect of surrogate reward. We observed that the voltage violation of measurable buses was approaching zero. Although the voltage violation of all buses was slightly higher than that of measurable buses, it was still significantly lower than the voltage violation of the experiments without control. It verified that DRL, with the surrogate reward of voltage violation, optimized the voltage violation of measurable buses directly and of other buses indirectly."}, {"title": "5.3. The Effect of Conservative Critic", "content": "To demonstrate the effectiveness of the conservative critic, we tested the proposed robust DRL and the standard DRL under three limited measurement deployment condi-tions. The simulation results were shown in Figure 6. We observed that the conservative critic reduced the voltage violation value considerably but increased the power loss slightly. The results were consistent when comparing the experiments of DRL with and without the conservative critic on the three limited measurable conditions."}, {"title": "6. Conclusion", "content": "This paper proposes a robust DRL approach for IB-VVC in ADNs with limited measurement deployments. Unlike previous works that utilize pseudo-measurement to improve the measurement condition, the proposed approach solves the problem of limited measurement deployments directly. We first analyzed the problem of limited measurement de-ployments, which led to the problem of the partially"}]}