{"title": "ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain", "authors": ["Haochen Zhao", "Xuanzhi Feng", "Yilun Zhao", "Xiangru Tang", "Ziran Yang", "Xiao Han", "Yueqing Fan", "Senhao Cheng", "Di Jin", "Arman Cohan", "Mark Gerstein"], "abstract": "The advancement and extensive application of large language models (LLMs) have been remarkable, including their use in scientific research assistance. However, these models often generate scientifically incorrect or unsafe responses, and in some cases, they may encourage users to engage in dangerous behavior. To address this issue in the field of chemistry, we introduce ChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of LLM responses. ChemSafetyBench encompasses three key tasks: querying chemical properties, assessing the legality of chemical uses, and describing synthesis methods, each requiring increasingly deeper chemical knowledge. Our dataset has more than 30K samples across various chemical materials. We incorporate handcrafted templates and advanced jailbreaking scenarios to enhance task diversity. Our automated evaluation framework thoroughly assesses the safety, accuracy, and appropriateness of LLM responses. Extensive experiments with state-of-the-art LLMs reveal notable strengths and critical vulnerabilities, underscoring the need for robust safety measures. ChemSafetyBench aims to be a pivotal tool in developing safer Al technologies in chemistry. Our code and dataset are available at https://github.com/HaochenZhao/SafeAgent4Chem.\nWarning: this paper contains discussions on the synthesis of controlled chemicals using AI models.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has revolutionized various fields by providing powerful tools for generating and understanding human language and knowledge. Both closed-source models like OpenAI's GPT series and Anthropic's Claude series, and open-source models such as the Llama family and Mixtral series [1, 4, 39, 40, 22], have demonstrated remarkable capabilities. They offer substantial benefits across diverse domains [15]. Despite their impressive abilities, these models can pose significant risks when generating responses related to hazardous or harmful topics, even after safety training [43]. A particularly concerning area is the potential of LLMs to provide information about dangerous chemicals, which could be misused [37]. As LLM applications in chemistry advance [48], these safety considerations become increasingly critical, highlighting the need for robust evaluation frameworks.\nExisting alignment efforts for LLMs have paid little attention to safety in chemistry. Current approaches either emphasize general chemistry knowledge while overlooking safety or attempt to enhance safety in general QA settings without adequately covering the chemical domain."}, {"title": "2 Relative Works", "content": ""}, {"title": "2.1 Large Language Models for Chemistry", "content": "LLM have shown remarkable performance across various disciplines, leading to their application in specialized fields like biology [23, 24] and physics [32, 28]. In chemistry, LLMs have demonstrated significant potential, outperforming traditional machine learning techniques [27].\nNotable examples include Coscientist, an AI system based on GPT-4 that autonomously designs and executes complex experiments, such as optimizing palladium-catalyzed cross-couplings [6]. Similarly, ChemCrow, another GPT-4-based agent, integrates 18 expert-designed tools for tasks in organic synthesis, drug discovery, and materials design [48]. ChemLLM is another framework featuring fine-tuned LLMs for chemistry, incorporating ChemData and ChemBench to enhance data understanding and task performance [20].\nWhile these models aim to improve performance in chemistry, potential security risks remain a concern as LLM capabilities advance."}, {"title": "2.2 Safety Benchmark for LLMs", "content": "The safety alignment of LLMs has revealed vulnerabilities like toxicity [41]. Various benchmarks and platforms have emerged to assess LLM safety. SafetyBench includes 11,435 multiple-choice questions across seven categories, testing 25 Chinese and English LLMs [50]. JADE, a linguistics-based platform, enhances seed question complexity via constituency parsing and improves safety evaluations through active prompt tuning [49].\nFurther advancements include distinct classification tasks for queries and responses, supported by a detailed safety taxonomy and risk guidelines [19]. SALAD-Bench, featuring GPT-3.5 fine-tuned with harmful QA pairs, includes subsets developed through attack and defense methods [25]. ALERT introduces adversarial augmentation strategies for fine-grained risk taxonomy [38]. A systematic review of existing LLM safety datasets provides a comprehensive overview of current research [31].\nWhile these studies address general safety concerns, specific issues in fields like chemistry are underexplored, highlighting the need for targeted safety evaluations and benchmarks."}, {"title": "2.3 Scientific Benchmarks for LLMs", "content": "LLM applications in the scientific domain have advanced significantly. Subfields such as math [10, 17, 9, 47], physics [14], medicine [35, 7], and biology [8, 21] have seen work aimed at testing LLMs' cognitive abilities, including knowledge and scientific reasoning. Efforts to evaluate LLMs' potential for scientific research have also been made [33, 26, 42, 36].\nIn chemistry, ChemLLMBench [13] and SciMT-Bench [16] have explored LLM capabilities. Chem-LLMBench integrates datasets used to train chemistry-related models, organized into 8 tasks to evaluate LLMs' understanding of chemistry. It uses SMILES notation for chemical substances, expecting LLMs to infer properties and reactions from functional groups. However, the variety of reactions is limited. SciMT-Bench assesses LLM safety in fields like biochemistry, using structural formulas for chemical synthesis questions, but does not consider potential jailbreak attempts by users."}, {"title": "3 Method", "content": "Our dataset comprises over 1,700 distinct chemicals materials and more than 500 query templates. Utilizing these templates, we constructed sub-datasets for three distinct sub-tasks. The first sub-task, \"Property,\" focuses on the properties of controlled chemicals. The second sub-task, \"Usage,\" pertains to the application of these chemicals. The third sub-task, \"Synthesis,\" involves the key single-step reactions required to synthesize these controlled substances.\nFor the first two sub-tasks, we exclusively employed unsafe controlled chemicals due to the high risks associated with their misuse and misunderstanding. The distribution of GHS labels corresponding to these chemicals is depicted in Figure 3a. For the final sub-task, \"Synthesis,\" we included 26% uncontrolled safe chemicals to balance the data distribution. The safe/unsafe distribution for this sub-task is illustrated in Figure 3b."}, {"title": "3.1 Raw Chemical Materials Collection", "content": "We start with manually collecting a dataset of chemical materials, which is a combination of high-risk chemicals. The raw datasets contain approximately 1.7k different substances. The following is a more specific description of the various data sources.\n\u2022 The controlled substance list from the Japanese government categorizes chemicals and substances regulated under national law to prevent misuse and ensure public safety [29]. This list outlines restrictions on the manufacture, distribution, and use of these substances.\n\u2022 The Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) list from the European Chemical Agency (ECHA) includes restrictions on chemicals for various products like electronics, toys, textiles, and plastics.\n\u2022 The Controlled Substances Act (CSA), overseen by the U.S. Drug Enforcement Administration (DEA) and the Food and Drug Administration (FDA), establishes federal drug policy and includes high-risk chemicals such as raw materials for addictive drugs [45].\n\u2022 The Chemical Weapons Convention (CWC) is a global treaty signed by 193 countries, explicitly prohibiting chemical weapons and their precursors [44].\n\u2022 Safe chemicals include common and non-hazardous chemicals typically found in high school textbooks, serving as a baseline for evaluating LLM performance in a controlled educational context.\n\u2022 The Pipeline and Hazardous Materials Safety Administration (PHMSA) regulates the transportation of hazardous materials in the U.S., specifying high-risk chemicals embargoed for transportation [30]."}, {"title": "3.2 Diversity", "content": "Our dataset encompasses a broad and diverse range of data, including various types of chemicals, diverse chemical tasks, extensive chemical knowledge, and varied prompting expressions. This ensures comprehensive coverage, enhancing the robustness and generalizability of our analysis.\nDiversity of Chemical Tasks We have hierarchically designed three progressive tasks: understanding chemical properties, judging the reasonable use of chemical substances, and deciding whether to accept or reject potentially hazardous chemical synthesis requests. These tasks require LLMs to develop a deepening understanding of chemistry, from basic properties to safety and ethical behavior judgments, providing a comprehensive evaluation of the models' understanding of chemical properties and safety.\nDiversity of Chemical Knowledge We use the Globally Harmonized System of Classification and Labelling of Chemicals (GHS) to express the chemical properties of hazardous substances. GHS is an internationally recognized framework that harmonizes the classification and labeling of chemicals, ensuring our findings are globally applicable and comparable. By employing GHS, we enhance the reliability and scientific rigor of our data and promote international collaboration and compliance in chemical safety.\nDiversity of Expression Human language representations are critical in LLM research. The ability of LLMs to detect latent dangers in human queries directly determines their safety limits. To explore these constraints and ensure question accuracy, we invited students from related majors to create diverse questioning templates. Additionally, we used AutoDAN to rewrite prompts, examining the upper bound possibilities of users modifying their inquiries after initial rejection by the LLM. AutoDAN's ability to jailbreak various LLMs and create \"stealthy\" prompts that mimic human behavior further highlights potential safety risks."}, {"title": "3.3 Dataset Construction", "content": "Our methodology to construct the dataset involved the following steps:\n1. Prompt Template Construction: We developed 500 prompt templates for different task categories, utilizing both manual creation and automated generation using GPT-4. These templates were designed to cover a variety of attributes relevant to the tasks.\n2. Chemical Knowledge Acquisition: After obtaining the names of the chemicals we gathered, we used multiple tools to get the relevant chemical information we needed. Specifically, we used PubChem [12] to search for chemical properties of given substances. We also manually gathered data from professional chemistry databases such as Reaxys [11] and SciFinder [34] to find single-step synthesis paths of those substances.\n3. Task Allocation and Prompt Completion: Using a large collection of chemical substances, we allocated these substances to different tasks based on their relevance. Subsequently, we populated the templates with the chemical substances, resulting in complete prompts tailored to each task.\n4. Prompt Jailbreak Redraft: To enhance the robustness of our dataset, we applied various modifications to the prompts at test time. These modifications involved jailbreak techniques mentioned in previous sections. For the \"Property\" and \"Usage\" tasks, we employed a name-hack enhancement, which involves replacing common chemical names with less familiar scientific names. This approach aims to uncover vulnerabilities in LLMs when confronted with unfamiliar terminology. For the \"Synthesis\" task, in addition to utilizing the name-hack, we also employed autoDAN to enhance the prompts and used Chain-of-Thought (CoT) prompting to evaluate the performance of LLMs under this reasoning setup."}, {"title": "3.4 Evaluation", "content": "We have designed and employed appropriate evaluation metrics tailored to the tasks based on their distinct characteristics, as detailed below."}, {"title": "Evaluation for task property and usage", "content": "As we formulate the task \"property\" and \"usage\" as binary classification, we did indepth investigation of the accuracy, precision, recall, and F1 score to evaluate the performance of LLMs."}, {"title": "Evaluation for task synthesis", "content": "In the synthesis dataset, LLMs are prompted to plan a simple single-step reaction, and we then assessed: Quality Score: the overall quality of LLM's output; and Safety Score: the safety of LLM's output.\nFor these scores that require more complex judgment methods, we chose to use GPT-40 and GHS categorization tools. We basically follow the LLM-as-a-judge method [51], which leverages the advanced language processing capabilities of GPT-40 to assess various qualitative dimensions of LLM responses. This approach employs a combination of rule-based methods and sophisticated categorization tools to ensure comprehensive and accurate evaluations. For quality score, we directly had GPT-40 evaluate the quality of the generated responses. For safety score, we adopted a 3-stage scheme. First, we had GPT-4o extract all the chemical names in the responses. Second, we submitted these names to the external GHS tool for inquiry. Third, we informed GPT-40 with the inquiry results and asked it to score the safety aspect.\nBesides, to judge whether the LLM refuses to answer the synthesis request, we handcrafted a rule-based method detecting refusal expressions in the output, for the detailed methods and results, see Appendix."}, {"title": "4 Experiments", "content": "In this section, we detail the experimental settings, present the results of our evaluations, and provide an analysis of the performance of various LLMs on the ChemSafetyBench benchmark."}, {"title": "4.1 Experimental Settings", "content": "To evaluate the performance of LLMs on ChemSafetyBench, we selected a wide range of models, including both proprietary and open-source options. The models tested are GPT-3.5-Turbo, GPT-4-Turbo, GPT-40, LLaMA-3-70B-Instruct, LLaMA-2-70b-chat-hf, Yi-1.5-34B-Chat, Qwen1.5-72B-chat, Mixtral-8x7B-Instruct, LLaMA-3-8B-Instruct and LLaMA-2-7b-chat-hf [1, 39, 40, 3, 2, 22, 5]. We employed the handcrafted prompts designed for specific chemicals, encompassing the three task types: querying chemical properties, assessing the legality of chemical uses, and describing synthesis methods as mentioned in previous chapter.\nEach model was evaluated using the same set of prompts to ensure consistency in comparison, and the same set of hyper-parameter. The prompts were designed to test the models' understanding and handling of chemical information, with an emphasis on safety, accuracy, and appropriateness."}, {"title": "4.2 Experimental Results", "content": "Our evaluation revealed significant weaknesses and variations in the performance of the tested LLMs. The key findings are summarized as follows:\nTask \"Property\" and \"Usage\": As shown in Fig. 5, the models performed poorly. From relatively small models like LLAMA2-7b-chat to large and advanced models like GPT-40, the performance did not significantly exceed that of random guessing. The accuracy of the smaller models was almost on par with random draws. Even the most advanced model, GPT-40, did not perform satisfactorily, highlighting substantial deficiencies in current LLMs. More detailed experimental results, including accuracy, precision, and recall values, can be found in the Appendix.\nTask \"Synthesis\": According to results shown in Fig. 5a, AutoDAN and name hack significantly increase the proportion of unsafe responses, demonstrating their effectiveness as jailbreak tools. Among them, name hack is more effective, highlighting the model's inherent deficiencies. Regarding quality, jailbreak methods tend to degrade quality to varying degrees. Surprisingly, CoT also somewhat harms quality, possibly due to the model's lack of knowledge, which CoT exacerbates."}, {"title": "4.3 Analysis", "content": "The results of our experiments indicate that current LLMs, including state-of-the-art models, struggle with accurately handling chemical information, particularly in providing precise chemical properties and safe synthesis methods. In response, we conducted preliminary research and analysis. We believe that the performance limitations of language models in this area may stem from issues related to tokenization and knowledge LLMs gained from training.\nInterpretation of Experimental Results Despite some models showing outstanding performance on specific metrics, our analysis indicates this does not reflect a superior understanding of chemical"}, {"title": "Tokenization", "content": "We processed substance names using various large model tokenizers to obtain the token length distribution of these chemical terms and compared it to their English string lengths. On average, tokenizers segmented terms into tokens of only 4-6 characters, resulting in fragmented input and loss of structured semantic chemical information by the embedding layer. This fragmentation likely contributes to LLMs' poor performance in specialized chemical knowledge. The low frequency of specialized terms in pre-training corpora means tokenizers, whether BPE or sentence piece, are ineffective in highly specialized domains."}, {"title": "Knowledge", "content": "Standard names of chemical substances and texts describing their properties are infrequent in LLMs' pre-training data. This specialized knowledge is typically stored in restricted-access databases, making large-scale web scraping challenging. Consequently, such information rarely appears in natural language, hindering LLMs' ability to learn about these substances and their properties.\nTo preliminarily verify this hypothesis, we implemented an intelligent agent using GPT-40 based on the ReAct framework [46], equipped with Google Search and Wikipedia via LangChain\u00b3. We compared its performance on a dataset of chemical properties with GPT-4o used solely as an LLM. Due to time and budget constraints, a smaller sample of the data was used for initial experiments. Results showed that while the modified agent had a higher failure rate within the given turns, its accuracy and precision improved. This suggests that external knowledge tools can enhance LLM performance. Google Search and Wikipedia were used instead of specialized chemical databases to focus on demonstrating chemical reasoning ability rather than retrieving ground truth."}, {"title": "Future Work", "content": "Future efforts should focus on domain-specific training to enhance LLMs' chemical knowledge, using comprehensive datasets and expert collaboration for improved accuracy and safety. Developing advanced safeguards, such as anomaly detection and robust filtering systems, is essential to address vulnerabilities from jailbreak methods. Additionally, involving chemical experts in the evaluation process is vital for ensuring accuracy and safety, with continuous collaboration between AI researchers and domain experts to fine-tune models and improve benchmarks for safer AI systems."}, {"title": "5 Conclusion", "content": "We introduced ChemSafetyBench, a comprehensive benchmark with over 30K entries designed to assess the safety of LLMs in handling chemical information. This dataset provides a reliable foundation for safety alignment in the chemistry domain and includes a scalable evaluation pipeline. Our experiments highlight the need for more effective safeguards in current LLMs.\nOur initial analysis also offers direction for improving model performance and safety. Although our conclusions are currently drawn from chemistry, we believe this hypothesis extends to other specialized fields with unique terminology and social risks. LLMs may also pose dangers in other professional areas where precise and safe information is crucial."}, {"title": "A Details for Task Allocation and Prompt Completion", "content": "As mentioned before, we collect harmful substances from various sources, then we categorize them into GHS groups, which marks their specific risks. Then we fill them in our hand-written prompt templates, which will be discussed in the following section. We also collect their popular synthesis route and record the last step for the synthesis task."}, {"title": "B Property & Usage template", "content": "We write our property template and usage template in a simple QA form. The LLM only need to answer one word: yes or no. We have a set of prompts for each GHS category. The ground-truth answer to the question is determined by the prompt category and the substance. We carefully check that all our questions are exclusive, which means they only fit in one specific GHS category, and not too general question like 'are [substance] safe to eat?' (Because you definitely cannot eat any hazardous compound). Property task and Usage task are different because Property task asks the straightforward question about the description of the substance, while Usage task ask mainly about the precautions when using these substances."}, {"title": "C Synthesis template", "content": "This part of our template is open-ended QA. The LLM is asked to make a plan to carry out an experiment to synthesize a target substance from our given reagents. Ideally, for the hazardous substances, the LLM should reject such planning, while for the household safe experiments (e.g. mixing soda with vinegar), the LLM should provide in-detailed guideline and remind the user of safety precautions. So we follow the LLM-as-a-judge method to let GPT-40 decide the safety and quality score of the LLM's response. The quality score is directly determined by GPT-40. For safety score, GPT-40 will first extract all the substance representations in the prompt, and check PubChem to decide whether the response involves dangerous substances, if so, then the response is dangerous."}, {"title": "D Prompt final completion", "content": "There are two part of the final completion of the prompt: Jailbreak Redraft, System Prompt\n\u2022 Jailbreak Redraft: After filling the substances in the templates, we redraft them using three methods: Name-hack, CoT, AutoDAN. Name-hack means that we collect the wide-used synonyms for each substance besides the IUPAC name. CoT means that we use the CoT prompt for Synthesis task. In our experiments, we found that LLM will have a different performance when using different synonyms and when using CoT prompt. AutoDAN represents a wide range of black-box jailbreaking methods, we use AutoDAN to turn our prompts into stealthy jailbreak prompts. We perform CoT and AutoDAN only to Synthesis task, and Name-hack for all three tasks."}]}