{"title": "Explainable and Human-Grounded AI for Decision Support Systems: The Theory of Epistemic Quasi-Partnerships", "authors": ["John Dorsch", "Maximilian Moll"], "abstract": "In the context of AI decision support systems (AI-DSS), we argue that meeting the demands of ethical and explainable AI (XAI) is about developing AI-DSS to provide human decision-makers with three types of human-grounded explanations: reasons, counterfactuals, and confidence, an approach we refer to as the RCC approach. We begin by reviewing current empirical XAI literature that investigates the relationship between various methods for generating model explanations (e.g., LIME, SHAP, Anchors), the perceived trustworthiness of the model, and end-user accuracy. We demonstrate how current theories about what constitutes good human-grounded reasons either do not adequately explain this evidence or do not offer sound ethical advice for development. Thus, we offer a novel theory of human-machine interaction: the theory of epistemic quasi-partnerships (EQP). Finally, we motivate adopting EQP and demonstrate how it explains the empirical evidence, offers sound ethical advice, and entails adopting the RCC approach.", "sections": [{"title": "1. Introduction", "content": "In an era when Al decision support systems (AI-DSS) play an ever-increasing role in high- stakes environments, the need for ethical decision-making support technology takes center stage. This demand is made clear by the integration of AI-DSS into child welfare services for assisting the decision to investigate the possibility of child maltreatment (Kawakami et al. 2022). While controversy exists around whether machines can be genuinely trustworthy, since, for example, machines cannot possibly be our peers (Bryson 2018), orthogonal to these concerns is the perceived trust in the system: the trustworthiness that deployers (e.g., end-users) attribute to the system, whether the system is trustworthy or not. Perceived trust is worthwhile to increase as it can serve to mitigate algorithm aversion, wherein deployers reject the model's contributions to the decision space while knowing contributions constitute crucial information which ought to bear on the ultimate decision (e.g., Dietvorst et al. 2015).\nAs the demand for trustworthy AI gains momentum, research has turned to the field of explainable AI (XAI) (e.g., Miller et al. 2022). This subdiscipline has the mission of designing tools like AI-DSS to be perceived as trustworthy by ensuring the model's decision-making processes are made transparent. This pursuit aligns with the EU AI Act 2024, which mandates that, \u201cHigh-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to deployers. This includes providing adequate information to enable deployers to understand the system's capabilities and limitations\u201d (European Union 2024, \u00a713).\nHowever, the Act leaves open the question of what it means to be sufficiently transparent to deployers, who typically lack fluency with machine learning methods required to utilize explanations fully, and XAI struggles to offer an accurate, ethical answer. This is because XAI is threatened by the \u201cinmates running the asylum problem\u201d (Miller et al. 2017), wherein explanations are designed for"}, {"title": "2. Review of Empirical Studies on Human-Grounded Explanations in AI-DSS", "content": "The effective use of AI-DSS depends on deployers perceiving them as trustworthy (for a comprehensive review, see Zieglmeier & Lehene 2021). As is commonly understood in XAI literature, perceived trustworthiness in the system refers to one of two observable measures. On the one hand, subjective measures refer to deployers reports that they perceive the system as trustworthy (the precise question has multiple variants). On the other hand, objective measures track certain trust-based behaviors during the human agent's use of the system. One trust-based behavior particularly crucial for measuring algorithm aversion is the act of changing one's own prediction to the system's prediction, measured by the switch percentage (e.g., Ribeiro et al. 2016).\nThus, the system's perceived trustworthiness can be objectively measured by deploying the switch percentage paradigm, which has the goal of determining the enabling factors for optimizing trust calibration (also known as 'appropriate reliance') (Lee & See 2004). Trust calibration represents the degree to which the human agent follows the system's recommendation when accurate (which encompasses cases where the human defers to the system's recommendation), combined with the degree to which the human agent deviates from the system's recommendation when inaccurate. Trust calibration is thus a delicate balance between over-trusting and distrusting the system, and the switch"}, {"title": "2.1 Trust Calibration and the Switch Percentage Paradigm", "content": "The effective use of AI-DSS depends on deployers perceiving them as trustworthy (for a comprehensive review, see Zieglmeier & Lehene 2021). As is commonly understood in XAI literature, perceived trustworthiness in the system refers to one of two observable measures. On the one hand, subjective measures refer to deployers reports that they perceive the system as trustworthy (the precise question has multiple variants). On the other hand, objective measures track certain trust-based behaviors during the human agent's use of the system. One trust-based behavior particularly crucial for measuring algorithm aversion is the act of changing one's own prediction to the system's prediction, measured by the switch percentage (e.g., Ribeiro et al. 2016).\nThus, the system's perceived trustworthiness can be objectively measured by deploying the switch percentage paradigm, which has the goal of determining the enabling factors for optimizing trust calibration (also known as 'appropriate reliance') (Lee & See 2004). Trust calibration represents the degree to which the human agent follows the system's recommendation when accurate (which encompasses cases where the human defers to the system's recommendation), combined with the degree to which the human agent deviates from the system's recommendation when inaccurate. Trust calibration is thus a delicate balance between over-trusting and distrusting the system, and the switch"}, {"title": "2.2 Explanation Techniques in AI-DSS: Feature Graphs and Model Confidence", "content": "The income prediction task (IPT) is deployed to investigate experimentally the factors that modulate trust calibration and mitigate algorithm aversion. As such, IPT represents an influential paradigm in XAI research to assess and test theories about human-grounded explanations. In this task, participants predict an individual's income level based on various attributes, such as education, occupation, age, etc. To set up the task, researchers train a machine learning model on a relevant dataset (usually a recent US consensus). While models tend to be diverse (decision trees, random forests, neural networks, etc.), each model has been designed with the goal of predicting income levels, achieving a relevant threshold of accuracy for the purpose of the experimental design.\nAfter training, various XAI techniques are employed to generate explanations for the model's predictions, predominantly LIME (Local Interpretable Model-agnostic Explanations) and SHAP (Shapely Additive exPlanations) (e.g., Ribeiro et al. 2016). Both methods elucidate contributions of individual features to the model's prediction. Shapley values, rooted in cooperative game theory, provide a fair distribution of feature importance by considering the average contribution of each feature over all possible subsets of remaining features, though these explanations are computationally intensive. LIME, on the other hand, approximates the model locally using a simpler surrogate model, offering faster and less complex computations but potentially less comprehensive feature interaction insights. Despite their differing approaches, both methods can be similarly used to represent feature importance visually, often bar charts to illustrate the impact of each feature on specific predictions. For this reason, we shall refer to this family of explanation as \u2018feature graphs\u2019. Finally, researchers set up experimental conditions to observe behavioral results contrasted across different explanation conditions (e.g., presenting or not presenting this explanation or that explanation).\nBefore analyzing the relevant studies, an important caveat should be issued regarding what these studies measure. Objective measures, such as the switch percentage, do not measure participants' attitudes about the trustworthiness of the system, but rather the degree to which participants switch their original prediction to the system's prediction. In other words, the switch percentage measures the frequency with which participants defer to AI-DSS. This can be understood as an act of trust, so"}, {"title": "2.3 Case Studies and Comparative Analysis of Explanation Methods", "content": "Let us discuss the results of two studies that deployed the switch percentage paradigm to measure trust calibration in IPT, wherein participants were novices in machine learning, akin to typical deployers. First, Zhang and colleagues (2020) found that presenting model confidence in a propositional format facilitated trust calibration over displaying feature graphs. To be clear, propositional format means the explanation resembled a declarative sentence (e.g., \u201cThe model is 90% confident that this person earns over $50,000.\u201d), meaning the explanation was not unlike the reasons humans provide, which is constituted by truth conditions. Model confidence, on the other hand, refers to a reliability score, specifically the probability assigned by the model to a particular prediction, indicating how certain the model is about the accuracy of its prediction."}, {"title": "2.3.1 The Role of Model Confidence in Trust Calibration", "content": "Let us discuss the results of two studies that deployed the switch percentage paradigm to measure trust calibration in IPT, wherein participants were novices in machine learning, akin to typical deployers. First, Zhang and colleagues (2020) found that presenting model confidence in a propositional format facilitated trust calibration over displaying feature graphs. To be clear, propositional format means the explanation resembled a declarative sentence (e.g., \u201cThe model is 90% confident that this person earns over $50,000.\u201d), meaning the explanation was not unlike the reasons humans provide, which is constituted by truth conditions. Model confidence, on the other hand, refers to a reliability score, specifically the probability assigned by the model to a particular prediction, indicating how certain the model is about the accuracy of its prediction."}, {"title": "2.3.2 Feature Graphs and their Epistemic Opacity", "content": "Somewhat surprisingly, Zhang and colleagues found that neither condition (model confidence nor feature graph) led to significant increases in participant accuracy. This outcome suggests at least two possible reasons, each offering important lessons for XAI research. As the researchers explain, both humans and the model had similar error boundaries, meaning conditions where the model had low confidence were also challenging for humans. This indicates that it makes little sense to deploy AI-DSS if their capacity to contribute to the ultimate decision does not complement the human's; nor does it make sense to deploy AI-DSS if humans do not have unique knowledge that would complement the AI-DSS's contribution. Thus, the aim of deploying AI-DSS ought to be about building robust human-AI quasi-partnerships of an epistemic kind, wherein the strengths and expertise of both humans and AI-DSS are leveraged to complement each other's weaknesses.\nFinally, it is challenging for deployers to infer model confidence from feature graphs. To this point, Zhang and colleagues write, \u201cIn theory, prediction confidence could be inferred by summing the positive and negative contributions of all attributes. If the sum is close to zero, then the prediction is not made with confidence\u201d (p. 9). Such an inference demands a technical understanding of how confidence can be calculated, so deployers are unlikely to employ it. For this reason, feature graphs are epistemically opaque: they require technical understanding to determine reasons that would support relevant beliefs in decision-making, such as the belief that the model's prediction is correct.\nConsequently, the evidence shows that feature graphs, albeit visually instructive, remain nonetheless unintuitive for non-experts. If our goal is to comply with the EU AI Act and make the operation of AI-DSS transparent to deployers, we ought to be skeptical of employing feature graphs for this purpose, perhaps better suited to IAI than X\u0391\u0399."}, {"title": "2.3.3 Counterfactual Explanations and Comparative Insights", "content": "The second study deeply relevant to our discussion was conducted by Le and colleagues (2022), who explored the role of counterfactual descriptions when coupled with model confidence in modulating trust calibration. This study contrasted two types of counterfactual descriptions, text- based and graphical, contrasted with the control condition which had no explanation whatsoever. These descriptions were generated through a loss function that balanced the distance between original and counterfactual inputs while achieving the desired confidence change. For example, Le and colleagues showed participants that changing \u201cMarital Status\u201d from \u201cMarried\u201d to \u201cDivorced\u201d reduced the confidence score significantly.\nThese researchers found that displaying both types of counterfactual descriptions improved accuracy and increased the switch percentage over the control condition. Meanwhile, the graphical description increased trust calibration and performance slightly more than the textual description. Le and colleagues suggest that this is because the graphical description is more intuitive and easier to understand (i.e., epistemically transparent). So, the lesson is that counterfactual descriptions are effective explanations, and counterfactual explanations are best communicated graphically, if possible.\nIn addition to the effectiveness of counterfactual information, this finding also demonstrates that feature graphs are not ineffective because they are graphs. Indeed, combining the results of both studies yields the proposal that the issue with feature graphs is that they are epistemically opaque because they fail to facilitate epistemic discernment, the capacity to determine reasons for the relevant beliefs important for decision-making. Consequently, these studies raise questions about why model confidence (especially in propositional format) and counterfactuals are so effective in facilitating epistemic discernment and thereby increasing epistemic trust, and XAI theories are challenged to explain this (see Section 3)."}, {"title": "2.3.4 Comparing Anchor Explanations and Feature Graphs", "content": "Other studies emphasize comparing explanations in terms of whether they increase participant accuracy, rather than epistemic trust. Ribeiro and colleagues (2018) used IPT (among other tasks) to demonstrate that Anchor explanations outperformed LIMEs in increasing participant accuracy as well as their reported confidence in relying on the system's prediction, which held true across various conditions (e.g., different datasets (tabular data, text, images, etc.) and models (logistic regression,"}, {"title": "2.3.5 Conflicting Results and the Import of Communicating Explanations", "content": "How do we explain this result that feature graphs were shown by Hase and Bansal to be effective in enhancing deployer understanding (specifically, compared to Anchor explanations), when the studies above demonstrated that feature graphs were ineffective in increasing epistemic trust and participant accuracy? These contrasting results can be elucidated by considering three key differences between the related studies. First, participants in Hase and Bansal's study were undergraduate students with at least one course in computer science or statistics, which made them better equipped to utilize feature graphs effectively compared to participants in other studies who had no technical training. Second, Hase and Bansal measured simulatability rather than accuracy, which might be more easily achieved with fluency in reading feature graphs than making accurate predictions about classifications on the basis of feature graphs. Finally, as these studies demonstrate, understanding the epistemic relevance of feature graphs incurs a cognitive load, the amount of which will vary depending on the participant's educational background (among other things). Hase and Bansal's participants, with their specific training, might have found the cognitive load manageable, whereas participants in other studies likely struggled.\nWith this in mind, let us explain the conflicting results between the studies by Hase and Bansal and Ribeiro and colleagues by examining the relative success that if-then statements had in Ribeiro's test of simulatability to the relative success that feature graphs had in Hase and Bansal's test of simulatability. In Hase and Bansal's study, participants were evaluated depending on whether they could accurately predict the model's behavior not only for novel inputs but also for counterfactual scenarios, wherein certain input features were perturbed. But anchor explanations are not well suited for counterfactual reasoning, as one cannot deduce facts about the consequent from the negation of the antecedent (the well-known \u2018fallacy of the inverse'). In other words, one cannot infer whether a change in input will necessarily lead to a different outcome simply by negating the input highlighted in the anchor rule."}, {"title": "2.3.6 Prototype Explanations and Counterfactual Simulatability", "content": "Let us now the effectiveness of prototype explanations in facilitating counterfactual simulatability. Prototype explanations enable deployers to compare new instances to representative examples from the training data. First, the model presents a prototypical example to describe the case under investigation (e.g., \u201cMost Similar Prototype: Routine and rather silly\u201d), then it provides a similarity score for how similar the current input is to the prototype (e.g., \u201cSimilarity score: 9.96 out of 10\"). Crucially, these explanations are so effective because they facilitate ordinary epistemic practices. For example, a doctor might assess the likelihood of a diagnosis based on symptom similarities to known cases, while a lawyer might predict the court's decision by considering legal precedent. Moreover, if either person were to explain their decision-making, each could provide epistemic justification (all things being equal) by appealing to the process of comparing current cases to known examples. Thus, prototype explanations make model decisions transparent to deployers by aligning outputs with ordinary epistemic practices.\nThus, the reason why prototype explanations were more effective in facilitating counterfactual simulation than if-then rules is because they prompt abductive reasoning, drawing probabilistic inferences based on comprehensive understanding of several relevant cases. This is not to say that employing probabilistic reasoning safeguards either the doctor or the lawyer from the fallacy of the inverse. For example, the doctor might mistakenly believe that the probability of having disease X given symptom Y is the same as the probability of symptom Y given disease X. It is to say, however,"}, {"title": "2.3.7 Addressing Cognitive Biases in Deployers of AI-DSS", "content": "Before concluding our analysis of the literature, let us discuss a serious worry. Increasing epistemic trust in AI-DSS could lead to blind trust and overreliance, resulting in deployers refraining from engaging their decision-making processes, foregoing their own knowledge, and succumbing to cognitive biases. For example, Ghai et al. (2020) discovered that displaying model confidence produced an anchoring bias in participants with limited domain knowledge, predisposing them to endorse the model's predictions. Though this should not worry us too much, since deployers of AI- DSS are generally domain experts with extensive knowledge in their discipline, it does raise concerns about whether explanations can be \u201ctoo effective\u201d.\nThe good news is that various strategies exist for mitigating cognitive biases when displaying explanations. For instance, Ma et al. (2023) demonstrated that overreliance on model confidence occurs only if it is displayed alone, without any accompanying reasoning for the prediction; meaning, overreliance was not observed if model confidence was shown alongside relative accuracy, or if displayed after participants made their predictions. These findings suggest that overreliance can be mitigated by pairing model confidence with additional explanations or withholding its display until after deployers have made their own predictions. So, we ought to be cautious about simply displaying confidence, and we need to consider carefully when to display it and what to accompany it with.\nIn one final study with a significant bearing on mitigating cognitive biases in deployers' use of AI-DSS, Chen et al. (2023) conducted a mix-methods study with IPT. By utilizing a think-aloud protocol to explore how participants utilized the two kinds of explanation, they found anchoring biases when deploying feature graphs, but not example-based explanations. Example-based explanations present profiles of two individuals with similar properties and predictions, thus functioning akin to prototypes, providing concrete examples that help deployers understand model behavior by comparing novel to known cases."}, {"title": "3. The Inadequacy of Current XAI Theories", "content": "In this section, we canvass three prominent theories that bear on what makes for good human- grounded reasons. All theories (including our own) have a guiding principle in common: the quality of a human-grounded explanation can and ought to be determined by measuring its effect on trust calibration. To make the argument more concise, a gloss is applied to the above principle, focusing scrutiny on one side of the proverbial equation, namely on perceived trustworthiness. Thus, the semi-principle reads, the quality of a human-grounded reason can and ought to be determined by measuring its effect on perceived trustworthiness. These principles are only meant to be employed in the domain of AI-DSS, and, as such, they assume that the provider of the reason (the machine) is incapable of lying or intentionally misleading. We"}, {"title": "3.1 Theory of Mental Models", "content": "The theory of mental models (TMM) posits that perceived trust in AI-DSS can be increased through explanations for how the model works (e.g., Hoffman et al. 2018a, Hoffman et al. 2018b). That said, \u201chow it works\u201d is too vague to be of use in philosophical analysis, so let us place the following gloss on it, namely how it generates outputs. Albeit an interpretation, it is nonetheless faithful to TMM, since the theory calls for evaluating competing explanations based on how well they facilitate deployers predicting model outputs. Though to our knowledge there is no philosophical defense of TMM, one can readily intuit its line of reasoning. It infers its conclusions from two premises: first, developing trust in AI-DSS necessitates the construction of mental models; second, effective mental models are constructed from explanations of how AI-DSS generate outputs.\nOne serious issue with this manner of spelling out what constitutes a good human-grounded explanation is that it is unclear whether it offers a meaningful difference between explainability and interpretability. This is further underscored by how TMM focuses on mechanistic explanations that aim to elucidate the internal processes and decision-making pathways (Hoffmann et al. 2018b), which may not be useful or easily understandable for typical deployers. In other words, by concentrating on explanations of how models generate outputs (how they work), TMM falls into the trap of maintaining that explanations that are good for developers are also good for deployers, which runs afoul of the lunatics-running-the-asylum problem and renders human-grounded explanations incoherent.\nRegarding its adequacy in explaining the evidence above, TMM lacks the conceptual tools for elucidating the remarkable effectiveness that displaying model confidence and counterfactual and prototypical descriptions exert in increasing perceived trustworthiness. TMM maintains that explanations ought to reveal the mechanics of the model, how certain mechanisms generate certain outputs, but model confidence and the like do not reveal how the model yielded its prediction \u2014 they"}, {"title": "3.2 Theory of Mental Models Plus", "content": "The next theory of good human-grounded explanations builds upon TMM, offering an augmented version that places particular emphasis on error boundaries. Bansal and colleagues (2019) offer a formal definition of an error boundary: \u201cThe error boundary of model h is a function f that describes for each input x whether model output h(x) is the correct action for that input: f: (x, h (x))\n\u2192{T, F}\u201d (3). Thus, an error boundary tells you which predictions made by the model are correct and which are incorrect, like a map that marks the places where the model yields true outputs and false outputs, respectively. Call this the Theory of Mental Models Plus (TMM+).\nSimilar to TMM above, we can understand TMM+ as positing two fundamental premises: first, developing trust in AI-DSS necessitates the construction of a mental model; second, good mental models are comprised of explanations detailing circumstances under which AI-DSS make mistaken outputs. From these premises, it follows that AI-DSS will be perceived as trustworthy if deployers are provided with explanations regarding their error boundaries, offering insight into when and how AI- DSS make mistaken outputs.\nTMM+ faces similar criticism as TMM for being counterintuitive and conceptually limited. Deployers often trust technology without understanding its error boundaries. To use a simplified example, you rely on your automobile to bring you to work, but you likely do not know the technical condition under which the engine would fail to start. TMM+ is also conceptually limited: because it focuses on errors in outputs, TMM+ does not have the conceptual resources to explain why, for example, errors in confidence have the potential to be so informative. Intuitively, knowing when the"}, {"title": "3.3 Theory of Anthropomorphism", "content": "Another prominent theory that elucidates the relationship between explanations and perceived trustworthiness in AI-DSS is the theory of anthropomorphism (TA) (e.g., Seymour and Kleek 2021). TA can be seen as positing two fundamental premises: first, human behavior garners trust if it is explainable, meaning rational and reasonable; second, individuals anthropomorphize AI-DSS, attributing to them human-like characteristics. From these premises, it follows that AI-DSS will be perceived as trustworthy if AI-DSS are anthropomorphized to behave like human agents wherever tasked with explaining their decision-making.\nThe advantage that TA has over TMM and TMM+ is that it has the conceptual tools to explain the evidence discussed above. Indeed, humans explain their decision-making by offering propositional descriptions constrained by truth conditions. We also tend to describe our confidence levels while making decisions under uncertainty, and if we are placed in a position to justify our decisions, we tend to offer examples and counterexamples. Each one of these explanatory features is predicted by TA and confirmed by the empirical evidence on X\u0391\u0399.\nBut is it ethical to design machines to be anthropomorphic? The success of this design strategy could be attributable to exploiting cognitive biases in human decision-making, namely in-group biases, trusting agents because they look and talk like us. Another way of understanding what is problematic about this strategy is that it might amount to instilling a category mistake in the mind of deployers (a fallacy in reasoning), causing them to ascribe machines human-like qualities that are beyond, or even alien to, their actual capacities. In other words, designing anthropomorphic technology for the purpose of increasing perceived trustworthiness, without any independent reasons for doing so, can be seen as manipulating trust perception rather than facilitating it.\nConsequently, a novel theory of human-ground explanations must possess the explanatory dexterity to navigate between Scylla and Charybdis: it must avoid being ensnared by the six-headed beast, whose many anthropomorphic heads chew theories up for their unethical council, while also steering clear of the whirlpool, which drags theories down into its depths of equivocations between interpretability and explainability. With this task in mind, let us now consider the theory of epistemic quasi-partnerships."}, {"title": "4. The Theory of Epistemic Quasi-Partnerships and the RCC Approach", "content": "In this section, we motivate EQP by appealing to a thought experiment and providing an initial defense of the theory as adequately describing the empirical evidence above. Let us begin by addressing what it means to call AI-DSS quasi-partners. As briefly remarked above, quasi means \u2018as though', and so we wish to convey that the status of AI-DSS as epistemic partners is a kind of fiction, one justified by its explanatorial efficacy. This is because genuine partners are socially responsible and AI-DSS are not the kind of entity that can be held socially responsible (see Dorsch and Deroy 2024 for an extensive discussion of this). In reality, AI-DSS are tools for decision support and their unique capacity to contribute valuable information to the decision space lends itself to thinking of them as though they were epistemic partners, a particularly advantageous fiction for determining how best to explain their role in hybrid decision-making environments. Though a full defense of this position would go beyond the present scope, the thought experiment below should motivate adopting a fictional stance toward AI-DSS.\nCall this thought experiment: Crossroads at the Al-Assisted Trail. You and your partner are hiking through tricky mountainous terrain when you arrive at a crossroads and need to determine the best path forward. Your partner is a technical expert skilled in utilizing navigation technology with no knowledge of the terrain, and you are a domain expert with extensive experience in navigating this landscape with no technical expertise. You represent the deployer, who relies on firsthand knowledge and intuition, while your partner represents the AI-DSS, employing computational models and issuing predictions. Now, imagine you two disagree about the best path forward. What do you need to hear from your partner before you can change your mind?"}, {"title": "5. Conclusion", "content": "The theory of epistemic quasi-partnerships offers a compelling framework for understanding epistemic trust in AI-DSS, as it aligns with empirical evidence, is conceptually clear, and provides practical and ethical guidance. By emphasizing the importance of epistemic quasi-partnerships and the RCC Approach\u2014where reasons, counterfactuals, and model confidence are prioritized\u2014the theory provides a comprehensive explanation for how epistemic trust is cultivated in hybrid decision-making. Thus, the RCC approach is not only an effective means of improving trust calibration in accurate systems, demonstrated by the wealth of empirical support discussed above, it is also an ethical approach to improving epistemic trust in machine-based systems, since it does not advise developing anthropomorphic machines and could lead to mitigating algorithm aversion."}]}