{"title": "Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base", "authors": ["Zhiyu An", "Xianzhong Ding", "Yen-Chun Fu", "Cheng-Chung Chu", "Yan Li", "Wan Du"], "abstract": "This paper introduces Golden-Retriever, designed to efficiently navigate vast industrial knowledge bases, overcoming challenges in traditional LLM fine-tuning and RAG frameworks with domain-specific jargon and context interpretation. Golden-Retriever incorporates a reflection-based question augmentation step before document retrieval, which involves identifying jargon, clarifying its meaning based on context, and augmenting the question accordingly. Specifically, our method extracts and lists all jargon and abbreviations in the input question, determines the context against a predefined list, and queries a jargon dictionary for extended definitions and descriptions. This comprehensive augmentation ensures the RAG framework retrieves the most relevant documents by providing clear context and resolving ambiguities, significantly improving retrieval accuracy. Evaluations using three open-source LLMs on a domain-specific question-answer dataset demonstrate Golden-Retriever's superior performance, providing a robust solution for efficiently integrating and querying industrial knowledge bases.", "sections": [{"title": "Introduction", "content": "Technological companies maintain massive collections of proprietary documents generated over years, such as training materials, design documents, and research outputs. Engineers, especially new hires, are expected to quickly query these documents or assimilate the new knowledge in these documents. However, navigating in the large number of documents is challenging. These domain-specific documents normally have many abbreviations and jargons unique to their technical community, further complicating the problem.\nLarge Language Models (LLMs) offer excellent performance for general question-answering tasks (Petroni et al., 2019; Hu et al., 2021). To make a pre-trained LLM incorporate a company's domain-specific knowledge, we may fine-tune it over the company's proprietary documents. However, fine-tuning is computationally expensive, generalize poorly to new knowledge due to the Reversal Curse (Berglund et al., 2023), and limited in capacity, as it may overwrite old knowledge (Roberts et al., 2020; Zhai et al., 2024).\nRetrieval Augmented Generation (RAG) (Lewis et al., 2020) offers a flexible and scalable approach for utilizing large document collections. RAG consists of an embedding model, a document database, and a LLM. During offline preparation, RAG embeds document chunks into the document database that retains semantic information. When an user asks a question, RAG first retrieves relevant document chunks according to semantic similarity. Then, the retrieved chunks are incorporated into prompts for the LLM, which then generates an answer. The output of RAG is the answer generated by the LLM based on the document chunks. This allows dynamic updates of knowledge base for an LLM without retraining it.\nDespite its advantages, RAG also faces challenges to be used for domain-specific documents. First, since some jargons and abbreviations only appear in proprietary documents, RAG's LLM backbone may hallucinate and misinterpret them. Existing methods like Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) enhance the LLM's response post-retrieval. But when user's question contains ambiguous jargons, RAG fails to retrieve the most relevant documents, limiting the effectiveness of post-retrieval enhancements.\nTo disambiguate user's question before retrieval, another approach (Kochedykov et al., 2023) deconstructs vague questions into an Abstract Syntax Tree (AST) and synthesizes SQL queries, improving query fidelity. But their work is limited to SQL instead of natural language documents."}, {"title": "Related Work", "content": "Current RAG techniques often fall short of the ideal scenario for handling domain-specific queries in industrial knowledge bases.\nVanilla RAG (Lewis et al., 2020), for instance, struggles with accurately interpreting domain-specific jargons. When asked, \"What is the PUC architecture of Samsung or Hynix NAND chip?\", the system incorrectly interprets \"PUC\" as \"Process-Unit-Controller\" instead of the correct \"Peripheral Under Cell\". This misinterpretation highlights the problem of hallucination, where the model generates incorrect or nonsensical information based on ambiguous input. This issue is further illustrated in Figure 1, which shows that both Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) attempt to modify the response after the document retrieval step. However, if the initial retrieval is flawed due to misinterpreted jargons or lack of context, these post-processing techniques cannot fully rectify the inaccuracies.\nMoreover, Corrective-RAG and Self-RAG focus on refining the generated responses after retrieval, which is inherently limited if the retrieved documents themselves are not relevant. As depicted in Figure 1, these methods fail to address the root cause: the ambiguity in the user's question and the initial retrieval process. A related approach by (Kochedykov et al., 2023) aims to address vague questions by deconstructing them into an AST and synthesizing SQL queries accordingly. While this method improves query fidelity, it is limited to SQL queries and does not generalize to broader question-answering scenarios. Figure 1 illustrates this limitation, showing that while the method can disambiguate and structure queries more effectively, it is not applicable to general retrieval tasks where context and jargon interpretation are crucial."}, {"title": "Method", "content": "Golden-Retriever consists of offline and online parts. The offline part is a data pre-processing step that occurs before the deployment of the knowledge base chatbot, described in Section 3.1. The online part is an interactive process that takes place every time a user asks a question, detailed in Sections 3.2 through 3.6."}, {"title": "LLM-Driven Document Augmentation", "content": "The offline part of Golden-Retriever focuses on enhancing the document database to improve the relevance of retrieved documents. This process begins by collecting the company's original documents, such as slides, images with embedded text, and tables, to form the knowledge base. These documents are often varied in format and content, lacking a clear narrative, which can lead to low relevance scores when queried with RAG.\nTo address this, we use OCR to extract text from these documents and split it into smaller, manageable chunks for processing. For the Meta-Llama-3 model, these chunks are approximately 4,000 tokens each. Each chunk is then processed using an LLM to generate summaries from the perspective of a domain expert, leveraging the LLM's semantic understanding and in-context learning abilities. This augmented data is added to the document database, making it more likely to retrieve relevant documents when queried (Figure 3)."}, {"title": "Identify Jargons", "content": "The first step in the online process involves identifying jargons and abbreviations within the user's question. This step is essential because many domain-specific questions include specialized terms that require clarification to ensure accurate interpretation. To identify these terms, we utilize a prompt template designed to instruct the LLM to extract and list all jargons and abbreviations found in the input question. This process ensures that all potentially ambiguous terms are recognized, facilitating their resolution in later steps. The identified jargons and abbreviations are outputted in a structured format for further processing."}, {"title": "Identify Context", "content": "After identifying jargon, it is crucial to determine the context in which the question is asked, as the meaning of terms can vary significantly across different contexts. For instance, \"RAG\" could mean \"Retrieval Augmented Generation\" in the context of LLMs or \"Recombination-Activating Gene\" in genetics. To accurately interpret the context, we use a similar reflection step as in jargon identification. This involves designing a prompt template that takes the question as input. The prompt contains a list of pre-specified context names and their descriptions. The LLM uses this prompt to identify the context of the question. Few-shot examples with Chain-of-Thought (CoT) prompting are applied to enhance performance, guiding the LLM to respond in a specified data structure. The identified context is then stored and accessed by the main program for further processing.\nUsing simpler methods, such as transformer-based text classifiers like those used in (Kochedykov et al., 2023) to classify user intent, would require a dedicated training dataset. This is impractical for our application due to the extensive effort and resources needed to create such a dataset. Instead, we opt for an \"LLM as backend\" approach, which, despite incurring higher computational costs, does not require a dedicated training dataset and can be run efficiently on a local server. By identifying the context before"}, {"title": "Query Jargons", "content": "Once the jargon and context have been identified, the next step is to query a jargon dictionary for extended definitions, descriptions, and notes on the identified terms. This step is essential for providing the LLM with accurate interpretations of the jargon, ensuring that the augmented question is clear and unambiguous.\nThis process involves querying a SQL database with the list of jargon terms identified in Section 3.2. The jargon list is inserted into a SQL query template, which is then processed to retrieve the relevant information from the jargon dictionary. The retrieved information includes extended names, detailed descriptions, and any pertinent notes about the jargon. We choose not to use the LLM to generate SQL queries directly, as described in (Qin et al., 2023) and (Li et al., 2024). Generating SQL queries with LLMs can introduce uncertainties regarding query quality and safety, and can also increase inference costs. Instead, by using a code-based approach to synthesize the SQL query, we ensure that the queries are verifiably safe and reliable.\nThe detailed information obtained from this step is crucial for augmenting the user's original question. It allows for accurate context and jargon interpretation, which is fundamental for the RAG process to retrieve the most relevant documents and generate precise answers."}, {"title": "Augment Question", "content": "With the jargon definitions and context identified, the next step is to augment the user's original question to include this additional information. This augmentation ensures that the RAG process retrieves the most relevant documents by providing clear context and resolving any ambiguities in the question. This step involves integrating the original question with the context information and the detailed jargon definitions obtained from Sections 3.3 and 3.4. The augmented question explicitly states the context and clarifies any ambiguous terms, facilitating enhanced document retrieval.\nThe process is automated, with the code taking the original question and the results from the context and jargon identification steps and combining them into a structured template. The context infor-"}, {"title": "Query Miss Response", "content": "In some cases, the system may not find any relevant information for certain jargon terms in the dictionary. To handle such scenarios, Golden-Retriever has a fallback mechanism that synthesizes a response indicating that the database is unable to answer the question due to missing information. The system instructs the user to check the spelling of the jargon or contact the knowledge base manager to add new terms. This step ensures that the system maintains high fidelity and avoids generating incorrect or misleading responses. The unidentified jargon fits into a response template, instructing the user to check the spelling and contact the knowledge base manager to add the new term."}, {"title": "Evaluation", "content": "We conduct two experiments to evaluate our method's effectiveness. The first experiment tests our method's ability to answer domain-specific questions based on documents, and the second experiment tests LLM's ability to correctly identify abbreviations from questions."}, {"title": "Question-Answering Experiment", "content": "To evaluate our method's ability to answer domain-specific questions based on documents, we collected multiple-choice questions from training documents for new-hire engineers. The questions cover six different domains, with each domain having nine to ten questions. These questions are one to two sentences long and contain jargon or abbreviations, with choices ranging from two (True/False) to four (Multiple choice). \n\nThe questions and choices are presented to the LLM/chatbot along with instructions to select an answer. Responses are collected and graded by a human expert who records the number of correct answers for each quiz. Each quiz is repeated five times, and the average score is calculated for each method and LLM backbone."}, {"title": "Abbreviation Identification Experiment", "content": "To test if LLMs can robustly identify unknown abbreviations (Section 3.2), we generated random abbreviations and inserted them into question templates to create a synthetic dataset. For abbreviation generation, we computed the probability distribution of each letter being the first letter in all words in an English dictionary, then sequentially sampled the letters by that distribution to form abbreviations. We manually prepared question templates. \n\nThe synthetic questions are integrated with the prompt template, as shown in the \"Identify Jargon\" step in Figure 2. We prompt the LLM, record the responses, and check if they contain all abbreviations used in the questions. This experiment is conducted on the three aforementioned LLMs."}, {"title": "Result", "content": "We list the accuracy of each LLM in identifying all abbreviations in questions with varying numbers of abbreviations . The experiment shows that state-of-the-art models such as Llama3 and Mistral have high accuracy in identifying unknown abbreviations. We also observe different failure modes across the three LLMs, with detailed fail cases shown in Appendix C.2."}, {"title": "Conclusion", "content": "This paper presents Golden-Retriever, a novel agentic RAG system designed to efficiently navigate vast industrial knowledge bases and overcome the challenges of domain-specific jargon and context interpretation. Experiment on a dedicated question-answer dataset shows that Golden-Retriever significantly improves answer accuracy, demonstrating its superior performance compared with traditional RAG method."}, {"title": "Fine-tuning or Retrieval Augmented Generation?", "content": "Knowledge injection via fine-tuning has several significant drawbacks. For instance, when fine-tuned on a knowledge statement like \"A is B,\" the fine-tuned LLM can correctly answer \"What is A?\" but fails to answer \"What is B?\" with \"A\" for arbitrary A and B. This phenomenon is famously known as The Reversal Curse (Berglund et al., 2023). Although remedies such as generating reversed training data (Golovneva et al., 2024) have been proposed, they require higher training costs and do not guarantee that the tuned LLM will answer all possible forms of a query. Additionally, incorporating knowledge through fine-tuning necessitates a new fine-tuning job for each new piece of knowledge, which incurs computational costs and hinders efficient integration of new information. The amount of knowledge a model can effectively incorporate depends on the capacity of the fine-tuned model part (Roberts et al., 2020), while excessive fine-tuning may lead to catastrophic forgetting, where the model forgets previously learned knowledge (Zhai et al., 2024).\nIn contrast, RAG does not suffer from these drawbacks. The Reversal Curse, observed in fine-tuning methods, does not occur when knowledge statements are presented in-context, as part of the prompt. In RAG, the LLM learns knowledge statements in-context, significantly improving its reasoning capacity and enabling efficient instruction prompt tuning (Singhal et al., 2023). Furthermore, RAG does not require model retraining and can efficiently incorporate new knowledge corpora. These properties make RAG a superior choice for industrial knowledge bases."}]}