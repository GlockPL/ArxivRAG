{"title": "\"I understand why I got this grade\": Automatic Short Answer Grading with Feedback", "authors": ["Dishank Aggarwal", "Pushpak Bhattacharyya", "Bhaskaran Raman"], "abstract": "The demand for efficient and accurate assessment methods has intensified as education systems transition to digital platforms. Providing feedback is essential in educational settings and goes beyond simply conveying marks as it justifies the assigned marks. In this context, we present a significant advancement in automated grading by introducing Engineering Short Answer Feedback (EngSAF)\u2014a dataset of ~ 5.8k student answers accompanied by reference answers and questions for the Automatic Short Answer Grading (ASAG) task. The EngSAF dataset is meticulously curated to cover a diverse range of subjects, questions, and answer patterns from multiple engineering domains. We leverage state-of-the-art large language models' (LLMs) generative capabilities with our Label-Aware Synthetic Feedback Generation (LASFG) strategy to include feedback in our dataset. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison\u00b9. Additionally, we demonstrate the efficiency and effectiveness of the ASAG system through its deployment in a real- world end-semester exam at the Indian Institute of Technology Bombay (IITB), showcasing its practical viability and potential for broader implementation in educational institutions.", "sections": [{"title": "1 Introduction", "content": "Technology integration in education has resulted in transformative changes, redefining traditional pedagogical approaches and assessment methodologies. Effective education relies on feedback and explanations provided during assessments to ensure quality learning outcomes Shute [2008]. Grading questions in tests and examinations have proven to be a good measure to assess student's learning and understanding of a topic or a subject. An exam could include various question types, such as multiple choice, fill-in-the-blank, short answers, essays, etc. Among these question types, short answers and essays are more complicated to analyze than multiple-choice or fill-in-the-blank type questions due to flexibility and natural language in the response. Automating the grading process becomes crucial, especially in countries with extremely high student-to-teacher ratios, as it can significantly reduce instructor's workloads and improve the assessment process. Significant advancements have been"}, {"title": "1.1 Problem Statement", "content": "Given a question, a reference answer, and a student's answer, the aim is to provide content-focused elaborated feedback and assign a label indicating the degree of correctness in the student's answer compared to the reference answer. Here, we focus on questions where the answer type is a sentence or a short paragraph. This task involves evaluating the alignment between the student's and reference answers."}, {"title": "1.2 Motivation", "content": "The increasing demand for technology in education has led to a need for more efficient and effective methods of grading short-answer assessments. In the case of short answer grading, feedback with clear explanations goes beyond simply conveying a grade. It offers valuable insights into student's strengths, weaknesses, and areas of improvement. However, the effectiveness of feedback is largely unexplored due to the lack of public, content-centered, elaborated feedback datasets in different domains. These datasets are crucial for training and developing automated feedback systems that can provide personalized and nuanced feedback. The need for a more efficient and effective ASAG system incorporating feedback alongside grade has arisen due to these limitations, and this is where the Engineering Short Answer Feedback (EngSAF) dataset steps in, consisting of questions students answer from multiple engineering domains for the ASAG task.\nOur Contributions are:\n1. EngSAF dataset containing around 5.8K student responses to 119 questions from multiple engineering domains along with synthetically generated feedback explaining the assigned"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Automatic Short Answer Grading (ASAG)", "content": "ASAG is an essential area of research that has garnered significant attention in recent years. Several approaches have been proposed for traditional ASAG, ranging from rule-based methods to more sophisticated machine-learning techniques. One early approach for ASAG was based on keyword or pattern matching, where the presence or absence of certain keywords in the student's answer was used to determine its accuracy. Mitchell et al. [2002]; Sukkarieh et al. [2004]; Nielsen et al. [2009].\u03a4\u03bf overcome these limitations, researchers have developed more sophisticated methods that use natural language processing (NLP) techniques. One such method is based on Latent Semantic Analysis (LSA), which represents texts as high-dimensional vectors and compares them to the reference answers using cosine similarity LaVoie et al. [2020]. In a related study, the task of ASAG is addressed by incorporating features such as answer length, grammatical correctness, and semantic similarity in comparison to reference answers Sultan et al. [2016].\nMore recently, deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been applied to ASAG's task. These models are trained on large amounts of annotated data and can capture the semantic relationships between words in a student's"}, {"title": "2.2 Short Answer Grading Datasets", "content": "Several publicly accessible datasets have been curated, each designed to facilitate research and benchmarking in the ASAG task. In 2009, Mohler and Milhalcea published a short answer dataset on a data structures course that contained 630 records Mohler and Mihalcea [2009]. In 2011, they published an extended dataset on the same course that contained 2273 records Mohler et al. [2011], named the Texas Extended dataset. To complete the assignment of student-authored answer annotation, the Beetle corpus is being created Nielsen et al. [2008]. Another dataset that is publicly available on Kaggle and could be used for the task of ASAG is provided by Hewlett Foundation named ASAP-AES2 (Automated Assessment Prize Competition for Essay Scoring).\nHowever, all aforementioned datasets are conventional ASAG datasets solely consisting of grades or scores. Our extensive literature survey includes only one short answer grading dataset with content- focused elaborated feedback, i.e., Filighera et al. [2022], which introduces an inaugural dataset for short-answer feedback comprising bilingual responses in English and German. Human annotations were meticulously collected and refined to uphold the feedback's quality. However, this dataset is constrained by its limited number of student responses (~ 2k) in English and its exclusive focus on questions from a single domain, specifically computer network queries, thereby lacking diversity across different engineering domains. To address these limitations, we introduced EngSAF, which contains ~ 5.8k data points and questions from multiple engineering domains."}, {"title": "3 Engineering Short Answer Feedback (EngSAF) Dataset", "content": "This dataset contains 119 questions drawn from different undergraduate and graduate engineering courses, accompanied by approximately 5.8k student responses. An Instructor provided correct answer/ reference answer also accompanies each question. These questions and answers have been taken from actual quizzes/ exams from the Indian Institute of Technology, Bombay (IITB), covering a diverse range of 25 courses. The questions and responses span across multiple domains, including image processing, water quality management, and operating systems, to name a few. Technical Appendix contains the complete list of courses included in the dataset. Based on the instructor's assigned marks, each response has been categorized as \u201ccorrect,\u201d \u201cpartially correct\u201d or \u201cincorrect?\u201d"}, {"title": "3.1 Dataset Construction", "content": "EngSAF contains questions and student responses from different undergraduate and graduate engi- neering courses. The instruction provided a correct answer/ reference answer accompanying each question. The Instructor/ Teaching Assistant (TA) assigns marks to each student's answer from their respective course. The students' responses and grades are obtained from a reputed engineering university. The quality assessment of the proposed dataset is covered in subsection 3.5. If the"}, {"title": "3.2 Challenges and Requirements", "content": "\u2022 Diversity in student responses: Human language is inherently subjective, and different individuals may interpret the same content differently. This subjectivity extends to the grading process, making it challenging to provide universally applicable feedback.\n\u2022 Response Variability: Short answer responses can vary significantly regarding quality, coherence, and relevance. Some responses may be well-structured and articulate, while others may be incomplete or contain grammatical errors. Designing feedback that caters to this variability in response quality poses a considerable challenge.\n\u2022 Feedback Impact: Wrong feedback can negatively shape learners' perceptions of them- selves and their abilities. If the feedback consistently highlights their shortcomings or mistakes, they may internalize these negative perceptions and develop a fixed mindset about their capabilities. This can hinder their willingness to take risks, seek challenges, and persist in facing difficulties."}, {"title": "3.3 Label Aware Synthetic Feedback Generation (LASFG)", "content": "Leveraging the advanced language generative and reasoning capabilities of state-of-the-art Large Language Models (LLMs) like Gemini\u00b3 and ChatGPT, we enhance educational assessment with feedback, particularly in Automatic Short Answer Grading (ASAG). The approach involves utilizing Gemini's ability to comprehend input prompts consisting of a question, a student's answer, a reference answer, and the corresponding grading label provided by an instructor or teaching assistant to generate content-focused elaborated feedback. The generated feedback covers the reasoning or explanation of the Gold output label. Grammarly\u2074 then checks the synthetically generated feedback to remove any grammatical errors. Prompt details used for synthetic feedback generation can be found in the Technical Appendix 10.1. The quality estimation of the generated feedback is shown in Section 3.5"}, {"title": "3.4 Corpus Statistics", "content": "Following Dzikovska et al. [2013], we partitioned the data into training sets, comprising 70% of the dataset, as well as unseen answers (16%) and unseen questions (14%) for test sets as shown in table"}, {"title": "3.5 Quality Estimation", "content": "To show the reliability and credibility of our dataset and synthetically generated feedback, we randomly sampled 300 data points and equally distributed them across the output label. This sampled data is a good representation of the EngSAF dataset.\nEach synthetically generated feedback was scored by three human annotators over three aspects. Each aspect is scored on a scale (1-5), with a high score indicating a better response. Each feedback is analyzed on the following aspects.\n1. Fluency and Grammatical Correctness: This aspect tests whether the generated feedback is fluent in English and grammatically correct.\n2. Feedback Correctness/Accuracy: This aspect assesses the overall quality of the generated feedback regarding content, relevance, quality, and explanation for the assigned grade.\n3. Emotional Impact: This aspect assesses how feedback affects the learner's emotional state. Annotators are tasked with rating whether the feedback avoids triggering negative emotions or impacts by refraining from using words such as \"fail\" that may evoke feelings of discouragement or distress in the learner.\nThree human annotators evaluate the correctness of each output label for sampled data points, achieving an accuracy of 98% and pair-wise average Cohen's Kappa (k) score of 0.65 (sub- stantial agreement), showcasing the high reli- ability of the assigned output label."}, {"title": "4 Experiments", "content": "Building on the foundational work by Filighera et al. [2022] in their investigation, our primary objective is to conduct experiments to establish a baseline for our EngSAF dataset. We also seek to delve deeper into the impact of incorporating questions on the generated feedback and the assigned labels. Traditionally, in Automated Short Answer Grading (ASAG), assessments have focused solely on evaluating reference answers and student responses. However, Lv et al. [2021] challenge this convention by demonstrating that integrating questions into the evaluation process enhances the performance of traditional ASAG tasks. Therefore, our study aims to build upon these insights to explore the nuanced effects of incorporating questions in ASAG assessments, aiming to refine and improve the evaluation criteria and methodologies in this domain."}, {"title": "4.1 Experimental Setting", "content": "To establish baselines for EngSAF, we have conducted experiments in two settings."}, {"title": "5 Results", "content": "Table 5 shows a majority baseline, Llama-2 fine-tuning, Mistral-7b fine-tuning, and ChatGPT zero- shot experiments results. The majority baseline contains the most occurring label and feedback from the EngSAF train set. The most common label is \"correct response,\" and the most common feedback is \"Well done! You have answered the question correctly, covering all the required aspects.\". We conducted the ChatGPT experiment only on the unseen answers test set due to its zero-shot setting, as both unseen questions and answers are identical in the zero-shot setting."}, {"title": "5.1 Analysis", "content": "In this section, we delve into the insights drawn from the performance results of experiments on the EngSAF dataset for the ASAG task. The evaluation metrics used for labels are accuracy and macro-averaged F1. To evaluate the feedback, we measure the Rouge-2 Post [2018], SCAREBLEU 7, METEOR Banerjee and Lavie [2005] and BERTSCORE Zhang et al. [2019] scores."}, {"title": "6 Deployment", "content": "For the real-world deployment, our fine-tuned ASAG model was integrated into an end- semester exam on ET 623 (Learning Analytics Course) at IIT Bombay for the 2024 academic year. The setup included the students enrolled in the course who consented to participate in the experiment. The end-sem exam includes 2 short-answer questions, each accompanied by the instructor's correct/reference answer. We randomly sampled 25 student answers for each question and used our fine-tuned ASAG with Feedback model to predict the output label and feedback/ explanation for the predicted output."}, {"title": "7 Summary, Conclusion, and Future Work", "content": "This study presents a novel and extensive dataset for grading short answers across multiple engineering domains. Our dataset addresses the need for a standardized evaluation platform by covering diverse subjects and a wide range of short answers. We meticulously curated and annotated this dataset to ensure its quality and applicability. Using the Label Aware Synthetic Feedback Generation (LASFG) strategy we have synthetically incorporated feedback in the EngSAF dataset. Through experimentation, we benchmarked the dataset's performance using different LLM's fine-tuning and chatGPT zero shot experiments.\nIn conclusion, our research endeavors culminated in successfully creating and evaluating a cutting- edge multi-domain short answer grading dataset. The dataset's diverse content and meticulous annotations provide a solid platform for training and assessing grading models across various subject domains. Our benchmarking experiments showcased promising results, indicating that our dataset has the potential to significantly improve the quality and reliability of ASAG systems. Additionally, we deployed the ASAG system in a real-world setting for an IITB course exam, demonstrating its practical applicability. This work marks a crucial step forward in educational technology, equipping educators and researchers with a valuable resource to boost advancements in automated assessment techniques.\nWhile this research presents a novel dataset for effective short answer grading along with feedback, there are several possibilities for future investigation. One area of focus could be the refinement and expansion of our dataset to include more nuanced and complex short answers, enabling us to accommodate a broader range of grading scenarios. Baseline figures could be improved by expansion or augmentation of the dataset. External knowledge inclusion through knowledge graphs could be explored in this direction."}, {"title": "8 Limitations", "content": "Our study, titled \"I understand why I got this grade\": Automatic Short Answer Grading with Feedback\" has provided valuable insights into the multi-domain ASAG. However, it's important to acknowledge certain limitations. The synthetic nature of the generated feedback may introduce biases or might contain inconsistencies, factual inaccuracies, or overly generic content, that could impact the implications of the results. One key finding from table 3 is an imbalance in output labels. Specifically, Output label Zero (0) is less frequent compared to labels One (1) and Two (2). This imbalance could impact the performance of automatic grading models trained on this dataset. Despite our efforts to curate diverse short-answer responses across multiple engineering domains, the dataset's overall size might limit the complexity and depth of models that can be trained on it. Variations in data distribution, language patterns, and task requirements across different domains and contexts may impact the application of our results to real-world scenarios. While our dataset's strength lies in its coverage of multiple engineering domains, it's important to note that the range of domains might not be exhaustive. Researchers focusing on specific domains or aiming to enhance domain-specific performance may need to consider domain-specific or subject-specific fine-tuning or gather additional data."}, {"title": "9 Ethical Statement", "content": "The EngSAF dataset contains questions, reference answers, and student answers along with the output label assigned for each student answer. This dataset is taken from actual exams and quizzes conducted at the university. We are committed to protecting those student's privacy and anonymity. Hence, no student-specific information is disclosed within the dataset. The process of collecting and curating this dataset adhered to the ethical norms of informed consent. Prior to any contribution, instructors of each course were provided details concerning the data collection's intent, the utilization of their questions and responses, and any potential implications. While the ASAG models created with this dataset offer great potential in educational assessment, they must be used properly and ethically. As with any AI technology, its impact on education must be regularly monitored and evaluated. Ensuring an appropriate balance between the advantages derived from automation and safeguarding the educational experience and human judgement is of utmost importance."}, {"title": "10 Appendix", "content": ""}, {"title": "10.1 Prompt used for Synthetic Feedback Generation:", "content": "To effectively tackle the challenges mentioned above and meet the outlined requirements, the instruc- tion provided to the LLM should contain several key components:\n1. It should articulate the ASAG task specifying input and output.\n2. It should specify guidelines for generating concise feedback and emphasize excluding words or phrases that may evoke negative emotions in the learner.\n3. The instruction should outline the expected format for the input data to ensure compatibility and consistency in processing.\nPrompt Used:\nYou are an automatic short-answer feedback generator.\nGiven a question, a student answer, a reference/correct answer, and a correctness label (correct/partially correct/incorrect), your task is to provide constructive feedback or reasoning for the assigned label.\nEnsure the feedback does not reference the provided reference answer. Keep it concise (3-4 lines) and aim to guide the learner without invoking any negative emotions.\n<START> and <END> token shows the starting and ending of each given input.\n<START> {{Question}} <END>\n<START> {{Correct Answer}} <END>\n<START> {{Student Answer}} <END>\n<START> {{Output Label}} <END>"}, {"title": "10.2 Hyperparameters used", "content": "The hyperparameters used for fine-tuning the llama-2 and mistral-7b models are as follows:\nbf16 = True\nnumber_of_training_epochs = 4\nper_device_eval_batch_size = 8\nper_device_train_batch_size = 8\ngradient_accumulation_steps = 1\nlearning_rate = 2e-4\nwarmup_ratio = 0.03\nweight_decay=0.001\nlr_scheduler_type = cosine\nLoraConfig(\n lora_alpha=16,\n lora_dropout=0.1,\n r=64,\n bias=\"none\",\n task_type=\"CAUSAL_LM\",\n target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)"}, {"title": "10.3 Dataset Insight", "content": "To gain insights into the distribution of text length within our dataset, we generated plots in figure 2 depicting the distribution of text length for student answers, reference answers, questions, and feedback. Table 7 shows the average length and standard deviation of different text fields from the EngSAF dataset. We utilized NLTK's 8 word_tokenize function for finding the tokens present in the text."}, {"title": "11 EngSAF Dataset- Courses and Subjects", "content": "EngSAFE dataset contains questions from the following courses. Our code and dataset are available at https://github.com/dishankaggarwal/EngSAF\n1. Sustainability Assessment of Urban Systems\n2. Introduction to Philosophy\n3. Business Valuation, Mergers and Acquisitions\n4. Environmental Management\n5. Fundamentals of Environmental Chemistry\n6. Environmental Chemistry\n7. Municipal waste and biomedical waste management"}, {"title": "12 Zero-Shot Experiment Prompt", "content": "The following prompt is used for the zero-shot experiment using gpt-3.5-turbo-16k model.\nYou are an automatic short-answer feedback generator.\nGiven a question, a student answer, and a reference answer. Evaluate student an- swers against a reference answer for correctness, providing labels (correct/partially correct/incorrect) and constructive feedback in about 3-4 lines.\nEnsure feedback guides the learner without invoking negative emotions and does not reference the provided reference answer.\nFormat output with the label on the first line followed by feedback starting from the next line."}, {"title": "13 More Examples", "content": "Table 11 presents two additional questions from the EngSAF dataset. Each question is accompanied by a reference answer and multiple student answers, each labeled with their respective output label and feedback summary."}, {"title": "14 Oualitative Analysis", "content": "Table 8 shows an example of the fine-tuned ASAG model's output on a sample from the EngSAF test set used for qualitative analysis. The gold label classifies the student's answer as incorrect, emphasizing that the student did not address how corners benefit specific image analysis applications. The model classifies the answer as partially correct, acknowledging that the student captured the essence of what corners represent but failed to elaborate on their practical applications. The model's feedback correctly identifies the main issue with the student's answer: it lacks specific examples of how corners are useful in image analysis applications. The reference answer provides specific applications such as image segmentation, edge detection, and object classification. The feedback suggests that the student should include these specific applications to improve his answer. This guidance is important for a complete understanding and improvement. The feedback is educational, aiming to help the student understand better by encouraging the practical uses of the concept. This helps correct the current answer and aids the student's overall learning by showing the importance of practical applications in theoretical concepts. The distinction between an answer being incorrect or partially correct is quite subtle. Therefore, while the model predicted the student's response as partially correct, the feedback explained the assigned grade accurately."}, {"title": "15 Deployment details", "content": "For the real-world deployment, our fine-tuned ASAG model was integrated into an end-semester exam on ET 623 (Learning Analytics Course) at IIT Bombay for the 2024 academic year. The setup included the students enrolled in the course who consented to participate in the experiment. The end- sem exam includes 2 short-answer questions, each accompanied by the instructor's correct/reference answer. Table 10 contains the details of the question and reference answer. Each predicted feedback is analyzed by three Subject Matter Expert (SME) in terms of Feedback Correctness/ Accuracy and Emotional Impact as discussed in the Quality Estimate section of the main paper. Each aspect is scored on a scale (1-5), with a high score indicating a better response. Each annotator is a current PhD student and expert in the education technology domain, ensuring the reliability of evaluations during real-world deployment of the ASAG model in the learning analytics course.\nUpon evaluation, the evaluation scores greater than 4.5 (Out of 5) for both the Feedback Qual- ity/Correctness and the Emotional Impact aspect demonstrate the reliability and effectiveness of the ASAG model in real-world scenarios. Additionally, the model's predicted output label achieved an accuracy of 92.5%, further showcasing its performance and reliability. Further, they achieved a Fleiss' Kappa score of 0.83 (Almost perfect agreement) for the feedback quality/correctness aspect, providing valuable insights into the agreement among three annotators in evaluating performance."}, {"title": "16 Annotator Details", "content": "We've brought in three human evaluators to ensure the EngSAF dataset output labels and artificial feedback quality and reliability. Each evaluator is working towards a master's degree, is in their final year/ pre-final year, having a lot of expertise in evaluating short answers. Each annotator was provided with comprehensive guidelines and requirements to execute their assigned task proficiently. Each evaluator was fairly compensated (5$ per hour) for reviewing grades and the synthetically generated feedback. Each feedback from the randomly sampled 300 data points was scored independently,"}]}