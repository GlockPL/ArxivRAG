{"title": "Model-based Preference Optimization\nin Abstractive Summarization without Human Feedback", "authors": ["Jaepill Choi", "Kyubyung Chae", "Jiwoo Song", "Yohan Jo", "Taesup Kim"], "abstract": "In abstractive summarization, the challenge\nof producing concise and accurate summaries\narises from the vast amount of information con-\ntained in the source document. Consequently,\nalthough Large Language Models (LLMs) can\ngenerate fluent text, they often introduce in-\naccuracies by hallucinating content not found\nin the original source. While supervised fine-\ntuning methods that maximize likelihood con-\ntribute to this issue, they do not consistently\nenhance the faithfulness of the summaries.\nPreference-based optimization methods, such\nas Direct Preference Optimization (DPO), can\nfurther refine the model to align with human\npreferences. However, these methods still heav-\nily depend on costly human feedback. In this\nwork, we introduce a novel and straightforward\napproach called Model-based Preference Op-\ntimization (MPO) to fine-tune LLMs for im-\nproved summarization abilities without any hu-\nman feedback. By leveraging the model's in-\nherent summarization capabilities, we create\na preference dataset that is fully generated by\nthe model using different decoding strategies.\nOur experiments on standard summarization\ndatasets and various metrics demonstrate that\nour proposed MPO significantly enhances the\nquality of generated summaries without relying\non human feedback. The code is publicly avail-\nable at https://github.com/cjaep/MPO.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demon-\nstrated remarkable capabilities in generating flu-\nent and plausible text (Wang and Komatsuzaki,\n2021; Touvron et al., 2023a; Jiang et al., 2023).\nHowever, despite these advancements, LLMs of-\nten produce summaries that, while plausible, con-\ntain incorrect or contradictory information-a phe-\nnomenon known as hallucination (Maynez et al.,\n2020). The fundamental reason for this issue is\nthat LLMs are primarily trained to predict the\nmost likely next token based on maximum like-\nlihood, which is the most common objective for\npre-training language models (King et al., 2022).\nIn principle, reinforcement learning based objec-\ntives can circumvent these failures by choosing an\nappropriate reward function (Paulus et al., 2018;\nTian et al., 2024). Recently, reinforcement learn-\ning from human feedback (RLHF) has focused on\naligning language models with human preferences,\nthereby effectively enhancing the models' summa-\nrization abilities (B\u00f6hm et al., 2019; Pasunuru and\nBansal, 2018; Stiennon et al., 2020; Paulus et al.,\n2018; Ramamurthy et al., 2023).\nWhile RLHF and other preference-based opti-\nmization methods (Rafailov et al., 2023) effectively\nfine-tune models to align with human preferences,\nhuman feedback is not always reliable. For exam-\nple, even though the quality of text summaries de-\npends on various factors, Hosking et al. (2024)"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Problem Setup", "content": "Let V denote the vocabulary for both input and\noutput. We represent the input document as $x \\in X$\nand the output summary as $y = (y_0, ..., y_T) \\in V$.\nThe sequence y consists of T +1 elements, starting\nwith the beginning-of-sequence token $y_0$ and ends\nwith the end-of-sequence token $y_T$.\nA language model (LM) is an auto-regressive\nmodel of a sequence distribution $P(y|x)$, where\neach conditional probability is parameterized by\na neural network $p_\\theta$. We assume that the model\ncomputes the probability of the entire generated\ntext y using a common left-to-right decomposition.\nThus, the distribution can be expressed as a product\nof conditional probabilities:\n$P(y|x) = \\prod_{t=1}^{T} P_\\theta(y_t|y_{<t}, x)$."}, {"title": "2.2 LM for Summarization", "content": "Given an input document x, the optimal summary\ny from the set of valid strings Y is obtained using\na scoring function:\n$y^* = \\underset{y \\in Y}{argmax} p_\\theta(y|x)$.\nHowever, finding the optimal summary is not\ntractable. Therefore, the scoring function for the\noptimal string y varies according to decoding strate-\ngies to approximate the best possible output. There"}, {"title": "3 Proposed Method", "content": "In this section, we detail our process for encourag-\ning faithfulness in abstractive summarization. We\nfollow the typical pipelines of preference optimiza-\ntion (Rafailov et al., 2023; Ziegler et al., 2020; Sti-\\ennon et al., 2020; Ouyang et al., 2022). However,\nby leveraging the differences between determinis-\ntic and stochastic decoding strategies, our pipeline\ndoes not require any external knowledge (e.g., eval-\nuation metrics) or human feedback. This pipeline\nis depicted in Figure 2."}, {"title": "3.1 Supervised Fine-Tuning (SFT)", "content": "For the summarization task, we first fine-tune a pre-\ntrained language model using supervised learning\non training data (i.e., ground truth data), denoted as\n$D_{train} = {(x, y_{ref})}$. Based on this supervised fine-\ntuning (SFT) approach, the model is trained to gen-\nerate a single-sentence summary from a source doc-\nument. In this work, we utilize existing SFT models\nwith minimal modifications or apply SFT to pre-\ntrained language models using QLoRA (Dettmers\net al., 2023)."}, {"title": "3.2 Preference Optimization", "content": "For preference optimization, we employ Di-\nrect Preference Optimization (DPO, Rafailov\net al., 2023). DPO simplifies the process by elim-\ninating the need for an explicit reward function,\nmaking it preferable to RL-based algorithms, which\nincur significant computational costs by training\nmultiple language models and sampling from the\npolicy.\nGiven a dataset of preference pairs $D =\n{(x_i, y_i^w, y_i^l)}_{i=1}^N$, where $x_i$ represents source doc-\numents, $y_i^w$ are chosen responses, and $y_i^l$ are re-\njected responses, the probability of observing a\npreference pair is modeled using the Bradley-Terry\nmodel (Bradley and Terry, 1952):\n$p(y^w > y^l) = \\sigma(r(x, y) - r(x, y'))$,\nwhere $\\sigma$ is the sigmoid function, and $r(\\cdot,\\cdot)$ is a\nreward function.\nRafailov et al. (2023) demonstrated that models\ndirectly learn this policy from collected data with-\nout modeling the reward function. In other words,\nthe 2-stage policy can be simplified into 1-stage\npolicy. DPO loss can be expressed as:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) =$\n$\\quad - E_{(x,y^w,y^l)\\sim D} \\left[log \\sigma \\left(\\beta log \\frac{\\pi_\\theta(y^w | x)}{\\pi_{ref}(y^w | x)} - \\beta log \\frac{\\pi_\\theta(y^l | x)}{\\pi_{ref}(y^l | x)} \\right)\\right]$,\nwhere $\\pi_{ref}$ is the SFT model and $\\beta$ is a coefficient\nthat controls the trade-off between reward and di-\nvergence. By optimizing this objective, the model\naligns with the reward function while remaining\nclose to the pre-trained reference model, thus mini-\nmizing over-optimization (Tian et al., 2024)."}, {"title": "3.3 Constructing Preferences Pairs without Human Feedback", "content": "By exploiting the differences between determin-\nistic and stochastic strategies, we construct a\ndataset of preference pairs, denoted as $D_{valid} =\n{(x, y^w, y_{beam}, y_{temp})}_{i=1}^N$. This strategy is based on the\nobservation that deterministic decoding typically\nproduces more factual summaries (Wan et al.,\n2023). This significant difference in output quality\nsuggests that summaries generated through beam\nsearch decoding can be used as chosen samples,\nwhile those from temperature sampling can be des-\nignated as rejected samples. We then conduct pref-"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Dataset We used the TL;DR dataset and the eX-\ntreme Summarization (XSUM) dataset (Cachola\net al.", "divisions": "Faith-\nfulness, Relevance (with the source), and Similarity\n(with the target). For Faithfulness, we used Align-\nScore (Zha et al., 2023) and FactCC (Kryscinski\net al., 2020). To measure Relevance, we employed\nBARTScore (Yuan et al., 2021) and BS-FACT.\nLastly, to evaluate Similarity, we used ROUGE-\nL.\nImplementation Details For the SFT training,\nwe utilized QLoRA with a batch size of 2 and a\nlearning rate of 1e-4, training for one epoch in train-\ning split. After training, the SFT-trained QLORA\nwas merged with the pre-trained model. For prefer-\nence optimization, we set the DPO hyperparameter\n$\\beta$ to 0.5. The learning rate was set to 1e-4 with a\nbatch size of 4, and training was conducted for one\ne"}]}