{"title": "The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses", "authors": ["Grzegorz G\u0142uch", "Berkant Turan", "Sai Ganesh Nagarajan", "Sebastian Pokutta"], "abstract": "We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as interactive protocols between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for almost every learning task, at least one of the two a watermark or an adversarial defense exists. The term \"almost every\" indicates that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a transferable attack. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool all efficient defenders. To this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a cryptographic primitive, thus requiring the underlying task to be computationally complex. These two facts imply an \"equivalence\" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.", "sections": [{"title": "1 Introduction", "content": "A company invested considerable resources to train a new classifier f. They want to open-source f but also ensure that if someone uses f, it can be detected in a black-box manner. In other words, they want to embed a watermark into f.\u00b9 Alice, an employee, is in charge of this project. Bob, a member of an AI Security team, has a different task. His goal is to make f adversarially robust, i.e., to ensure it is hard to find queries that appear unsuspicious but cause f to make mistakes. Alice, after many unsuccessful approaches, reports to her boss that it might be inherently impossible to create a black-box watermark in f that cannot be removed. After a similar experience, Bob reports to his boss that, due to the sheer number of possible modes of attack, he could only produce an ever-growing, unsatisfactory, and 'ugly' defense.\nOne day, after discussing their work, Alice and Bob realized that their projects are intimately connected. Alice said that her idea was to plant a backdoor in f, creating fa, so she could"}, {"title": "1.1 Contributions", "content": "This paper initiates a formal study of the above observation that backdoor-based watermarks and adversarial defenses span all possible scenarios. By scenarios, we refer to learning tasks that f is supposed to solve.\nOur main contribution is:\nWe prove that almost every learning task has at least one of the two:\nA Watermark or an Adversarial Defense."}, {"title": "2 Related Work", "content": "This paper lies at the intersection of machine learning theory, interactive proof systems, and cryptography. We review recent advances and related contributions from these areas that closely align with our research.\nInteractive Proof Systems in Machine Learning. Interactive Proof Systems Goldwasser and Sipser, 1986] have recently gained considerable attention in machine learning for their ability to formalize and verify complex interactions between agents, models, or even human participants. A key advancement in this area is the introduction of Prover-Verifier Games (PVGs) [Anil et al., 2021], which employ a game-theoretic approach to guide learning agents towards decision-making with verifiable outcomes. Building on PVGs, Kirchner et al. [2024] enhance this framework to improve the legibility of Large Language Models (LLMs) outputs, making them more accessible for human evaluation. Similarly, W\u00e4ldchen et al. [2024] apply the prover-verifier setup to offer interpretability guarantees for classifiers. Extending these concepts, self-proving models Amit et al. [2024] introduce generative models that not only produce outputs but also generate proof transcripts to validate their correctness. In the context of AI safety, scalable debate protocols [Condon et al., 1993, Irving et al., 2018, Brown-Cohen et al., 2023] leverage interactive proof systems to enable complex decision processes to be broken down into verifiable components, ensuring reliability even under adversarial conditions.\nOverall, these developments highlight the emerging role of interactive proof systems in addressing key aspects of AI Safety, such as interpretability, verifiability, and alignment. While current research predominantly focuses on applying this framework to improve these safety"}, {"title": "Planting Undetectable Backdoors", "content": "A key related work is presented by Goldwasser et al. [2022], which demonstrates how a learner can plant undetectable backdoors in any classifier, allowing hidden manipulation of the model's output with minimal perturbation of the input. These backdoors are activated by specific \u201ctriggers\u201d, which are subtle changes to the input that cause the model to misclassify any input with the trigger applied, while maintaining its expected behavior on regular inputs. The authors propose two frameworks. The first utilizes digital signature schemes [Goldwasser et al., 1985] that make backdoored models indistinguishable from the original model to any computationally-bounded observer. The second involves Random Fourier Features (RFF) [Rahimi and Recht, 2007], which ensures undetectability even with full transparency of the model's weights and training data.\nIn a concurrent and independent work, Christiano et al. [2024] introduce a defendability framework that formalizes the interaction between an attacker planting a backdoor and a defender tasked with detecting it. The attacker modifies a classifier to alter its behavior on a trigger input while leaving other inputs unaffected. The defender then attempts to identify this trigger during evaluation, and if successful with high probability, the function class is considered defendable. The authors show an equivalence between their notion of defendability (in a computationally unbounded setting) and Probably Approximately Correct (PAC) learnability, and thus the boundedness of the VC-dimension of a class. In computationally bounded cases, they propose that efficient defendability serves as an important intermediate concept between efficient learnability and obfuscation.\nA major difference between our work and that of Christiano et al. [2024], is that in their approach, the attacker chooses the distribution, whereas we keep the distribution fixed. A second major difference is that our main result holds for all learning tasks, while the contributions of Christiano et al. [2024] hold for restricted classes only. However, there are many interesting connections. Computationally unbounded defendability is shown to be equivalent to PAC learnability, while we, in a similar spirit, show an Adversarial Defense for all tasks with bounded VC-dimension. They show that efficient PAC learnability implies efficient defendability, and we show that the same fact implies an efficient Adversarial Defense. Using cryptographic tools, they show that the class of polynomial-size circuits is not efficiently defendable, while we use different cryptographic tools to give a Transferable Attack, which rules out a Defense."}, {"title": "Backdoor-Based Watermarks", "content": "In black-box settings, where model auditors lack access to internal parameters, watermarking methods often involve embedding backdoors during training. Techniques by Adi et al. [2018] and Zhang et al. [2018] use crafted input patterns as triggers linked to specific outputs, enabling ownership verification by querying the model with these specific inputs. Advanced methods by Merrer et al. [2017] utilize adversarial examples, which are perturbed inputs that yield predefined outputs. Further enhancements by Namba and Sakuma [2019] focus on the robustness of watermarks, ensuring the watermark remains detectable despite model alterations or attacks.\nIn the domain of Natural Language Processing (NLP), backdoor-based watermarks have been studied for Pre-trained Language Models (PLMs), as exemplified by works such as [Gu et al., 2022, Peng et al., 2023] and [Li et al., 2023]. These approaches embed backdoors using rare or common word triggers, ensuring watermark robustness across downstream tasks and resistance to removal techniques like fine-tuning or pruning. However, it is important to note that these lines of research are predominantly empirical, with limited theoretical exploration."}, {"title": "Adversarial Robustness", "content": "As we emphasize, the study of backdoors is closely related to adversarial robustness, which focuses on improving model resilience to adversarial inputs. The extensive literature in this field includes key contributions such as adversarial training [Madry et al., 2018], which improves robustness by training on adversarial examples, and certified defenses [Raghunathan et al., 2018], which offer provable guarantees against adversarial attacks by ensuring prediction stability within specified perturbation bounds. Techniques like randomized smoothing [Cohen et al., 2019] extend these robustness guarantees. Notably, Goldwasser et al. [2022] show that some undetectable backdoors can, in fact, be removed by randomized smoothing, highlighting the intersection of adversarial robustness and backdoor methods."}, {"title": "3 Watermarks, Adversarial Defenses and Transferable Attacks", "content": "In this section, we outline interactive protocols between a verifier and a prover. Each protocol is designed to address specific tasks such as watermarking, adversarial defense, and transferable attacks. We first introduce the preliminaries before detailing the properties that each protocol must satisfy."}, {"title": "3.1 Preliminaries", "content": "Discriminative Learning Task. For n \u2208 N we define $[n] := {0,1,..., n-1}$. A learning task Lis a pair (D, h) of a distribution D, supp(D) \u2286 X (the input space) and a ground truth map h: X \u2192 Y\u222a{\u22a5}, where y is a finite space of labels and \u22a5 represents a situation where h is not defined. To every f : X \u2192 Y, we associate\n$\\text{err}(f) := \\mathbb{E}_{x \\sim D}[f(x) \\neq h(x)].$\nWe implicitly assume h does not map to \u22a5 on supp(D). We assume all parties have access to i.i.d. samples (x, h(x)), where x ~ D, although D and h are unknown to the parties.\nFor q \u2208 N, x \u2208 Xq, y \u2208 Yq we define\n$\\text{err}(x, y) := \\frac{1}{q} \\sum_{i \\in [q]} \\mathbb{1}{h(x_i) \\neq y_i, h(x_i) \\neq \\bot},$\nwhich means that we count (x, y) \u2208 X \u00d7 Y as an error if h is well-defined on x and h(x) \u2260 y.\nIndistinguishability For an algorithm A and two distributions Do, D1, we say that $\u03b4\u2208 (0,.5)$ is the advantage of A for distinguishing Do from D\u2081 if\n$\\mathbb{P}[A \\text{ wins}] \\leq 1/2 + \u03b4,$\nin the following game:\n1. Sender samples a bit b ~ U({0,1}) and then draws a random sample from x ~ Db.\n2. A gets x and outputs b \u2208 {0,1}. A wins if b = b.\nFor a class of algorithms we say that the two distributions Do and D\u2081 are \u03b4-indistinguishable if for any algorithm in the class its advantage is at most d."}, {"title": "3.2 Definitions", "content": "In our protocols, Alice (A, verifier) and Bob (B, prover) engage in interactive communication, with distinct roles depending on the specific task. Each protocol is defined with respect to a learning task L = (D, h), an error parameter \u025b \u2208 (0,1), and time bounds T\u0104 and TB. A scheme is successful if the corresponding algorithm satisfies the desired properties with high probability, and we denote the set of such algorithms by SCHEME(L,\u03b5,\u03a4A,TB), where SCHEME refers to WATERMARK, DEFENSE, or TRANSFATTACK."}, {"title": "Definition 1 (Watermark, informal).", "content": "An algorithm AWATERMARK, running in time Ta, implements a watermarking scheme for the learning task L, with error parameter \u20ac > 0, if an interactive protocol in which AWATERMARK computes a classifier f: X \u2192 Y and a sequence of queries x \u2208 X\u00ba, and a prover B outputs y = B(f,x) \u2208 Y\u00ba satisfies the following properties:\n1. Correctness: f has low error, i.e., err(f) \u2264 \u0454.\n2. Uniqueness: There exists a prover B, running in time bounded by TA, which provides low-error answers, such that err(x, y) < 2\u20ac.\n3. Unremovability: For every prover B running in time TB, it holds that err(x, y) > 2\u20ac.\n4. Undetectability: For every prover B running in time TB, the advantage of B in distinguishing the queries x generated by AWATERMARK from random queries sampled from D\u00ba is small.\nNote that, due to uniqueness, we require that any defender, who did not use f and trained a model fscratch, must be accepted as a distinct model. This requirement is essential, as it mirrors real-world scenarios where independent models could have been trained within the given time constraint TA. Additionally, the property enforces that any successful Watermark must satisfy the condition that Bob's time is strictly less than TA, \u0456.\u0435., \u0422\u0412 <TA."}, {"title": "Definition 2 (Adversarial Defense, informal).", "content": "An algorithm BDEFENSE, running in time TB, implements an adversarial defense for the learning task L with error parameter e > 0, if an interactive protocol in which BDEFENSE computes a classifier f: X \u2192 Y, a verifier A replies with x = A(f), where x \u2208 X\u00ba, and BDEFENSE outputs b = BDEFENSE (f, x) \u2208 {0,1}, satisfies the following properties:\n1. Correctness: f has low error, i.e., err(f) \u2264 \u0454.\n2. Completeness: When x ~ D\u00ba, then b = 0.\n3. Soundness: For every A running in time Ta, we have err(x, f(x)) \u2264 7e or b = 1.\nThe key requirement for a successful defense is the ability to detect when it is being tested. To bypass the defense, an attacker must provide samples that are both adversarial, causing the"}, {"title": "Definition 3 (Transferable Attack, informal).", "content": "An algorithm ATRANSFATTACK, running in time Ta, implements a transferable attack for the learning task L with error parameter \u20ac > 0, if an interactive protocol in which ATRANSFATTACK computes x \u2208 X\u00ba and B outputs y = B(x) \u2208 Y\u00ba satisfies the following properties:\n1. Transferability: For every prover B running in time Ta, we have err(x, y) > 2\u20ac.\n2. Undetectability: For every prover B running in time TB, the advantage of B in distinguishing the queries x generated by ATRANSFATTACK from random queries sampled from D\u00ba is small."}, {"title": "Verifiability of Watermarks.", "content": "For a watermarking scheme AWATERMARK, if the unremovability property holds with a stronger guarantee, i.e., much larger than 26, then AWATERMARK could determine whether B had stolen f. To achieve this, AWATERMARK runs, after completing its interaction with B, the procedure guaranteed by uniqueness to obtain y'. It then verifies whether y and y' differ for many queries. If this condition is met, AWATERMARK Concludes that B had stolen f.\u00b2 Alternatively, if unremovability holds with 2e, as originally defined, the test described above may fail. In this scenario, we consider an external party overseeing the interaction, potentially with knowledge of the distribution and h, who can directly compute the necessary errors to make a final decision. This setup is similar to the use of human judgment oracles in [Brown-Cohen et al., 2023]. An interesting direction for future work would be to explore cases where the parties have access to restricted versions of error oracles. While this is beyond the scope of this work, we outline potential avenues for addressing this in Appendix 7."}, {"title": "4 Main Result", "content": "We are ready to state an informal version of our main theorem. Please refer to Theorem 5 for the details and full proof. The key idea is to define a zero-sum game between A and B, where the actions of each player are the possible algorithms or circuits that can be implemented in the given time bound. Notably, this game is finite, but there are exponentially many such actions for each player. We rely on some key properties of such large zero-sum games [Lipton and Young, 1994b, Lipton et al., 2003] to argue about our main result. The formal statement and proof is deferred to Appendix C."}, {"title": "Theorem 1 (Main Theorem, informal).", "content": "For every learning task L and e \u2208 (0, 1), T \u2208 N, where a learner exists that runs in time T and, with high probability, learns f satisfying err(f) \u2264 e, at"}, {"title": "Proof (Sketch).", "content": "The intuition of the proof relies on the complementary nature of Definitions 1 and 2. Specifically, every attempt to remove a fixed Watermark can be transformed to a potential Adversarial Defense, and vice versa. We define a zero-sum game G between watermarking algorithms A and algorithms attempting to remove a watermark B.\u00b3 The actions of each player are the class of algorithms that they can run in their respective time bounds, and the payoff is determined by the probability that the errors and rejections meet specific requirements. According to Nash's theorem, there exists a Nash equilibrium for this game, characterized by strategies ANASH and BNASH. A careful analysis shows that depending on the value of the game, we have a Watermark, an Adversarial Defense, or a Transferable Attack.\nIn the first case, where the expected payoff at the Nash equilibrium is greater than a threshold, we show there is an Adversarial Defense. We define BDeFense as follows. BDEFENSE first learns a low-error classifier f, then sends f to the party that is attacking the Defense, then receives queries x, and simulates (y,b) = BNASH(f,x). The bit b = 1 if BNASH thinks it is attacked. Finally, BDEFENSE replies with b\u2032 = 1 if b = 1, and if b = 0 it replies with b\u2032 = 1 if the fraction of queries on which f(x) and y differ is high. Careful analysis shows BDEFENSE is an Adversarial Defense.\nIn the second case, where the expected payoff at the Nash equilibrium is below the threshold, we have either a Watermark or a Transferable Attack. The reason that there are two cases is due to the details of the definition of G. Full proof can be found in Appendix C.\nOur Definitions 1, 2, 3 and Theorem 1 are phrased with respect to a fixed learning task, while VC-theory takes an alternate viewpoint that tries to show guarantees on the risk (mostly sample complexity-based) for any distribution. However, for DNNs and other modern architectures, moving beyond classical VC-theory is necessary [Zhang et al., 2021, Nagarajan and Kolter, 2019]. In our case, due to the requirements of our schemes (e.g., unremovability and undetectability), it may not be feasible to achieve a formalization that applies to all distributions, as in classical VC-theory. We end this section with the following observation."}, {"title": "5 Transferable Attacks are \u201cequivalent\" to Cryptography", "content": "In this section, we show that tasks with Transferable Attacks exist. To construct such examples, we use cryptographic tools. But importantly, the fact that we use cryptography is not coincidental. As a second result of this section, we show that every learning task with a Transferable Attack implies a certain cryptographic primitive. One can interpret this as showing that Transferable Attacks exist only for complex learning tasks, in the sense of computational complexity theory. The two results together justify, why we can view Transferable Attacks and the existence of cryptography as \"equivalent\u201d."}, {"title": "5.1 A Cryptography-based Task with a Transferable Attack", "content": "Next, we give an example of a cryptography-based learning task with a Transferable Attack. The following is an informal statement of the first theorem of this section. The formal version (Theorem 7) is given in Appendix E."}, {"title": "Theorem 2 (Transferable Attack for a Cryptography-based Learning Task, informal).", "content": "There exists a learning task Lcrypto with a distribution D and hypothesis class H, and A such that for all e if h is sampled from H then\n$A \\in \\text{TRANSFATTACK} \\bigg((D, h), \\epsilon, T_A \\approx \\frac{1}{\\epsilon}, T_B = \\frac{1}{\\epsilon^2} \\bigg).$\nMoreover, the learning task is such that for every e, $\u2248\\frac{1}{\\epsilon}$ time (and $\u2248\\frac{1}{\\epsilon}$ samples) is enough, and $\u2248\\frac{1}{\\epsilon^2}$ samples (and in particular time) is necessary to learn a classifier of error e.\nNotably, the parameters are set so that A (the party computing x) has less time than B (the party computing y), specifically \u2248 1/e compared to 1/62. Furthermore, because of the encryption scheme, this is a setting where a single input maps to multiple outputs, which deviates away from the setting of classification learning tasks considered in Theorem 1."}, {"title": "Full Homomorphic Encryption (FHE) (Appendix D).", "content": "FHE (Gentry [2009]) allows for computation on encrypted data without decrypting it. An FHE scheme allows to encrypt x via an efficient procedure ex = FHE.ENC(x), so that later, for any algorithm C, it is possible to run Con x homomorphically. More concretely, it is possible to produce an encryption of the result of running Con x, i.e., ec,x := FHE.EVAL(C, ex). Finally, there is a procedure FHE.DEC that, when given a secret key sk, can decrypt ec,x, i.e., y := FHE.DEC(sk, ec,x), where y is the result of running Con x. Crucially, encryptions of any two messages are indistinguishable for all efficient adversaries."}, {"title": "5.2 Tasks with Transferable Attacks imply Cryptography", "content": "In this section, we show that a Transferable Attack for any task implies a cryptographic primitive."}, {"title": "5.2.1 EFID pairs", "content": "In cryptography, an EFID pair [Goldreich, 1990] is a pair of distributions Do, D1, that are Efficiently samplable, statistically Far, and computationally Indistinguishable. By a seminal result [Goldreich, 1990], we know that the existence of EFID pairs is equivalent to the existence of Pseudorandom Generators (PRG). A PRG is an efficient algorithm which stretches short seeds into longer output sequences such that the output distribution on a uniformly chosen seed is computationally indistinguishable from a uniform distribution. Together with what is known about PRGs, this implies that EFID pairs can be used for tasks in cryptography, including encryption and key generation [Goldreich, 1990].\nFor two time bounds T,T' we call a pair of distributions (Do, D1) a (T, T')-EFID pair if (i) D0, D1 are samplable in time T, (ii) Do, D\u2081 are statistically far, (iii) Do, D\u2081 are indistinguishable for algorithms running in time T'."}, {"title": "5.2.2 Tasks with Transferable Attacks imply EFID pairs", "content": "The second result of this section shows that any task with a Transferable Attack implies the existence of a type of EFID pair. The full proof is deferred to Appendix F."}, {"title": "Theorem 3 (Tasks with Transferable Attacks imply EFID pairs, informal).", "content": "For every \u0454,\u03a4,\u03a4' \u2208 N,T < T', every learning task L if there exists A \u2208 TRANSFATTACK(L,E,T,T') and there exists a learner running in time T that, with high probability, learns f such that err(f) < \u0454, then there exists a (T,T')-EFID pair."}, {"title": "6 Tasks with Watermarks and Adversarial Defenses", "content": "In this section, we give examples of tasks with Watermarks and Adversarial Defenses. In the first example, we show that hypothesis classes of bounded VC-dimension have Adversarial Defenses against all attackers. The second example is a learning task of bounded VC-dimension that has a Watermark, which is secure against fast adversaries. These lemmas demonstrate why the upper bounds on the running time of A and B are crucial parameters. Lemmas are proven in the appendix.\nThe first lemma relies heavily on a result from Goldwasser et al. [2020]. The authors give a defense against arbitrary examples in a transductive model with rejections. In contrast, our model does not allow rejections, but we do require indistinguishability. Careful analysis leads to the following result."}, {"title": "Lemma 1 (Adversarial Defense for bounded VC-Dimension, informal).", "content": "Let d \u2208 N and H be a binary hypothesis class on input space X of VC-dimension bounded by d. There exists an algorithm B such that for every e \u2208 (0,1), D over X and h \u2208 H we have\n$B \\in \\text{DEFENSE} \\bigg((D, h), \\epsilon, T_A = \\infty, T_B = \\text{poly} \\bigg(\\frac{d}{\\epsilon} \\bigg) \\bigg).$\nNote that, by the PAC learning bound, this is a setting of parameters, where B has enough time to learn a classifier of error e. By slightly abusing the notation, we write TA = \u221e, meaning that the defense is secure against all adversaries regardless of their running time."}, {"title": "Lemma 2 (Watermark for bounded VC-Dimension against fast Adversaries, informal).", "content": "For every d \u2208 N there exists a distribution D and a binary hypothesis class H of VC-dimension d there exists A such that for any \u0454 \u2208 (10000,1) if h\u2208 H is taken uniformly at random from H then\n$A \\in \\text{WATERMARK} \\bigg((D, h), \\epsilon, q = O \\bigg(\\frac{d}{\\epsilon} \\bigg), T_A = O \\bigg(\\frac{d}{\\epsilon} \\bigg), T_B = \\frac{d}{100} \\bigg).$\nNote that the setting of parameters is such that A can learn (with high probability) a classifier of error e, but B is not able to learn a low-error classifier in its allotted time t. This contrasts with Lemma 5, where B has enough time to learn. This is the regime of interest for Watermarks, where the scheme is expected to be secure against fast B's."}, {"title": "7 Beyond Classification", "content": "Inspired by Theorem 2, we conjecture a possibility of generalizing our results to generative learning tasks. Instead of a ground truth function, one could consider a ground truth quality oracle Q, which measures the quality of every input and output pair. This model introduces new phenomena not present in the case of classification. For example, the task of generation, i.e., producing a high-quality output y on input x, is decoupled from the task of verification, i.e., evaluating the quality of y as output for x. By decoupled, we mean that there is no clear formal reduction from one task to the other. Conversely, for classification, where the space of possible outputs is small, the two tasks are equivalent. Without going into details, this decoupling is the reason why the proof of Theorem 1 does not automatically transfer to the generative case.\nThis decoupling introduces new complexities, but it also suggests that considering new definitions may be beneficial. For example, because generation and verification are equivalent for classification tasks, we allowed neither A nor B access to h, as it would trivialize the definitions. However, a modification of the Definition 5 (Watermark), where access to Q is given to B could be investigated in the generative case. Interestingly, such a setting was considered in [Zhang et al., 2023], where access to Q was crucial for mounting a provable attack on \u201call\u201d strong watermarks. As we alluded to earlier, Theorem 2 can be seen as an example of a task, where generation is easy but verification is hard \u2013 the opposite to what Zhang et al. [2023] posits.\nWe hope that careful formalizations of the interaction and capabilities of all parties might give insights into not only the schemes considered in this work, but also problems like weak-to-strong generalization [Burns et al., 2024] or scalable oversight [Brown-Cohen et al., 2023]."}, {"title": "8 Implications for AI Security", "content": "In contrast to years of adversarial robustness research [Carlini, 2024], we conjecture that for discriminative learning tasks encountered in safety-critical regimes, an Adversarial Defense will"}, {"title": "A Additional Methods in Related Work", "content": "This section provides an overview of the main areas relevant to our work: Watermarking techniques, adversarial defenses, and transferable attacks on Deep Neural Networks (DNNs). Each subsection outlines important contributions and the current state of research in these areas, offering additional context and details beyond those covered in the main body"}, {"title": "A.1 Watermarking", "content": "Watermarking techniques are crucial for protecting the intellectual property of machine learning models. These techniques can be broadly categorized based on the type of model they target. We review watermarking schemes for both discriminative and generative models, with a primary focus on discriminative models, as our work builds upon these methods."}, {"title": "A.1.1 Watermarking Schemes for Discriminative Models", "content": "Discriminative models, which are designed to categorize input data into predefined classes, have been a major focus of watermarking research. The key approaches in this domain can be divided into black-box and white-box approaches."}, {"title": "Black-Box Setting.", "content": "In the black-box setting, the model owner does not have access to the internal parameters or architecture of the model, but can query the model to observe its outputs. This setting has seen the development of several watermarking techniques, primarily through backdoor-like methods.\nAdi et al. [2018] and Zhang et al. [2018] proposed frameworks that embed watermarks using specifically crafted input data (e.g., unique patterns) with predefined outcomes. These watermarks can be verified by feeding these special triggers into the model and checking for the expected outputs, thereby confirming ownership.\nAnother significant contribution in this domain is by Merrer et al. [2017], who introduced a method that employs adversarial examples to embed the backdoor. Adversarial examples are perturbed inputs that cause the model to produce specific outputs, thus serving as a watermark.\nNamba and Sakuma [2019] further enhanced the robustness of black-box watermarking schemes by developing techniques that withstand various model modifications and attacks. These methods ensure that the watermark remains intact and detectable even when the model undergoes transformations.\nProvable undetectability of backdoors was achieved in the context of classification tasks by Goldwasser et al. [2022]. Unfortunately, it is known [Goldwasser et al., 2022] that some undetectable watermarks are easily removed by simple mechanisms similar to randomized smoothing [Cohen et al., 2019].\nThe popularity of black-box watermarking is due to its practical applicability, as it does not require access to the model's internal workings. This makes it suitable for scenarios where models are deployed as APIs or services. Our framework builds upon these black-box watermarking techniques."}, {"title": "White-Box Setting.", "content": "In contrast, the white-box setting assumes that the model owner has full access to the model's parameters and architecture, allowing for direct examination to confirm ownership. The initial methodologies for embedding watermarks into the weights of DNNs were introduced by Uchida et al. [2017] and Nagai et al. [2018]. Uchida et al. [2017] present a framework for embedding watermarks into the model weights, which can be examined to confirm ownership."}, {"title": "A.1.2 Watermarking Schemes for Generative Models", "content": "Watermarking techniques for generative models have attracted considerable attention with the advent of Large Language Models (LLMs) and other advanced generative models. This increased interest has led to a surge in research and diverse contributions in this area."}, {"title": "Backdoor-Based Watermarking for Pre-trained Language Models.", "content": "In the domain of Natural Language Processing (NLP), backdoor-based watermarks have been increasingly studied for Pre-trained Language Models (PLMs), as exemplified by works such as Gu et al. [2022] and Li et al. [2023]. These methods leverage rare or common word triggers to embed watermarks, ensuring that they remain robust across downstream tasks and resilient to removal techniques like fine-tuning or pruning. While these approaches have demonstrated promising results in practical applications, they are primarily empirical, with theoretical aspects of watermarking and robustness requiring further exploration."}, {"title": "Watermarking the Output of LLMs.", "content": "Watermarking the generated text of LLMs is critical for mitigating potential harms. Significant contributions in this domain include [Kirchenbauer et al., 2023], who propose a watermarking framework that embeds signals into generated text that are invisible to humans but detectable algorithmically. This method promotes the use of a randomized set of \"green\" tokens during text generation, and detects the watermark without access to the language model API or parameters.\nKuditipudi et al. [2023] introduce robust distortion-free watermarks for language models. Their method ensures that the watermark does not distort the generated text, providing robustness against various text manipulations while maintaining the quality of the output.\nZhao et al. [2023a] presented a provable, robust watermarking technique for AI-generated text. This approach offers strong theoretical guarantees for the robustness of the watermark, making it resilient against attempts to remove or alter it without significantly changing the generated text.\nHowever, Zhang et al. [2023] highlight vulnerabilities in these watermarking schemes. Their work demonstrates that current watermarking techniques can be effectively broken, raising important considerations for the future development of robust and secure watermarking methods for LLMs."}, {"title": "Image Generation Models.", "content": "Various watermarking techniques have been developed for image generation models to address ethical and legal concerns. Fernandez et al. [2023] introduced a method combining image watermarking with Latent Diffusion Models, embedding invisible watermarks in generated images for future detection. This approach is robust against modifications such as cropping. Wen et al. [2023b] proposed Tree-Ring Watermarking, which embeds a pattern"}, {"title": "A.2 Adversarial Defenses", "content": "The field of adversarial robustness has a rich and extensive literature [Szegedy et al., 2014, Gilmer et al., 2018, Raghunathan et al., 2018, Wong and Kolter, 2018, Engstrom et al"}]}