{"title": "RESPONSIBLE AI IN OPEN ECOSYSTEMS: RECONCILING INNOVATION WITH RISK ASSESSMENT AND DISCLOSURE", "authors": ["Mahasweta Chakraborti", "Bert Joseph Prestoza", "Nicholas Vincent", "Seth Frey"], "abstract": "The rapid scaling of AI has spurred a growing emphasis on ethical considerations in both development and practice. This has led to the formulation of increasingly sophisticated model auditing and reporting requirements, as well as governance frameworks to mitigate potential risks to individuals and society. At this critical juncture, we review the practical challenges of promoting responsible AI and transparency in informal sectors like OSS that support vital infrastructure and see widespread use. We focus on how model performance evaluation may inform or inhibit probing of model limitations, biases, and other risks. Our controlled analysis of 7903 Hugging Face projects found that risk documentation is strongly associated with evaluation practices. Yet, submissions (N=789) from the platform's most popular competitive leaderboard showed less accountability among high performers. Our findings can inform AI providers and legal scholars in designing interventions and policies that preserve open-source innovation while incentivizing ethical uptake.", "sections": [{"title": "Introduction", "content": "In recent years, we have witnessed the adoption of artificial intelligence (AI) across various individual, collective, and public enterprises, including education, business, and research. This widespread implementation has been argued to boost productivity, enhance manufacturing, accelerate development, and facilitate the provisioning of critical services and infrastructure at unprecedented levels and reach (see e.g. Chapter 4 of the AI Index Report for an overview of specific claims along these lines [90]). The scalability offered by these technologies may revolutionize numerous industries and promote further investment in AI innovation for improved service delivery across diverse domains.\nNascent technologies often face skepticism and scrutiny before earning public trust [54]. Therefore, potential risks in AI need to be recognized and addressed at every stage of development, such as representative quality of training data [66], algorithmic designs [48], and learning objectives [117]. AI artifacts are also highly prone to inappropriate uses [39, 91]. As AI's market impact and reach in people's lives continues to expand, there have also been corresponding calls for ethical training and regulatory oversight for providers and deployers.\nModel evaluation is a standard component of the AI development and deployment cycle. In its most common form, evaluation involves testing models on held-out datasets to understand how well a model has learned. Such benchmarking is essential for assessing novelty against the state-of-the-art, deciding whether the model is suitable for widespread use, improving training, and informing design choices for further innovation. Today, competitive benchmarking against other models is a popular form of evaluation, and high performers garner legitimacy from users and investors, enjoy market visibility, and even steer development and consumption [43, 50, 103].\nWith rising stakes, responsible developers are increasingly expected to use evaluations not only to assess model capabilities but also to recognize its limitations. Holistic evaluation goes beyond simply reporting gross accuracy and encourages probing edge cases, measuring biases in predictions for specific domains and vulnerable subpopulations,"}, {"title": "Related Work", "content": "Here, we discuss background on Open Source AI and key areas of research that motivated our dataset documentation effort and accompanying quantitative study."}, {"title": "Ethics of AI, Tech Regulation, and Open Development", "content": "Fostering and standardizing ethical development is a nuanced challenge in open source, which, contrary to regulated for-profits and commercial entities, is historically informal and free-forming [21, 42, 38]. Developer motivations, ranging from altruism to popularity, may lead to varying governance structures [81, 35, 120]. Open sourcing has led to remarkable progress in science and technology, and notable AI projects started out in the same to inspire and support collective innovation. At the same time, there have been multiple instances of biased or improperly curated training data and artifacts that can compromise regular applications [23, 22, 79]. Further, open source projects have also been appropriated for nefarious uses [4, 2, 89, 93]. We review some milestones in ethical discussions and regulatory approaches relevant to open-source AI.\nWith the growing potential of AI uses and misuse, researchers and ethicists came together early on to specify and advocate documentation guidelines for every link in the AI development pipeline. Model cards, [94, 41, 13], data sheets [57, 19], and other factsheets are crucial sociotechnical governance tools that keep stakeholders informed and define the scope of AI consumer applications, ultimately contributing to more transparent and accountable development practices. Responsible documentation outlines permissible use cases (possibly extending to licensing [39]), cautioning against out-of-scope applications, and discloses other anticipated risks and practical challenges. Projects generally explain any anticipated limitations in implementing the AI solution, such as predictive biases across ethnicity or"}, {"title": "Evaluation Practices and Responsible AI", "content": "Evaluation and testing have been historically integral in informing innovation and consumer use of emergent technologies. As we work on making AI more capable, there are benefits to be seen from striving towards a 99% accurate solution from a 98% accurate solution. These gross accuracy metrics indeed serve to inform design decisions. However, model valuation through accuracy is only one of several aspects that concern AI use and governance [95, 53]. We review current approaches to AI evaluation, which inform how we analyze our collected data and interpret our findings.\nA bid for greater accuracy through standardization led to the rise of evaluation benchmarks. As benchmarks became more widely adopted, they evolved into popular, competitive leaderboards [115, 116, 105, 110, 26, 45, 78]. These initiatives draw attention and participation, and have been instrumental in the rapid progress of AI by serving as a recognition-based incentive for continuous, incremental innovation. However, strides towards predictive efficiency have also culminated in a uni-dimensional emphasis on metric-races and ranking.\nOver time, several researchers have observed practical limitations with bench-marking and leaderboards, including errors and other vulnerabilities that compromise their validity or even obfuscate limitations and risks [43, 59, 31, 108]. Despite their usefulness as a general estimation of model capabilities, held-out test accuracy from a benchmark is not a comprehensive measure of generalizability, adversarial robustness or risk tolerance. Therefore, the error rate on a benchmark may not always reflect the population error rate [67, 122, 50, 43, 14, 25] Leaderboards are often dominated by highly over-parameterized, complex, and energy-inefficient models, sometimes overfitted to benchmarks [50, 62, 14, 43, 25]. Because of the visibility enjoyed by top performing developers, rampant attempts have been observed to game leaderboards, such as multiple submissions and tweaks, to maximize rank [62]. Benchmarks also generally lack transparency and fail to account for model attributes crucial to design and informing use, such as compactness, fairness, inference speed, and energy footprint, to name a few. [50, 103]. Moreover, as rankings are generally based on models' aggregate performance (e.g., accuracy/F1, etc.) over a collection of tasks and datasets, submissions often conceal racial and gender biases [29, 88, 107, 24]. Researchers have also noted selective pressures arising from benchmarking with unintended consequences for innovation, such as the promotion of certain architectures and algorithms over others [43, 113].\nWith increasing recognition of the inadequacy of current evaluation practices, scholarship in AI safety has seen a notable push towards holistic approaches with expanding definitions, objectives and approaches beyond simply predictive accuracy [82, 28, 27, 92]. These include robustness against malicious or adversarial inputs [97], explainability of the model's decision-making and intermediate processes [84, 49], generalizability on out-of-sample data [17], and granular testing across demographic subgroups for biased behavior. There have also been notable strides in designing evaluations to measure model fairness [123, 96, 104].\nWith the legitimacy popular leaderboards enjoy [103], high performance may easily lead developers and users to assume the generalizability of their models and undermine the need for additional efforts to stress-test for runtime risks. This is especially dangerous and requires a reassessment of benchmarking as it is. Opportunistic overfitting can be mitigated by promoting dynamic benchmarks [97, 43, 75, 58], that are constantly updated to accommodate temporal/distributional drifts and emerging domains, newer tasks, and capabilities. Dehghani et al. and Hardt et al. further guide benchmark design to ensure judicious assessment while mitigating opportunistic submissions [43, 62, 25]. Other proposed measures include confidentiality of test/hold out sets and mitigation of data leakage [85, 44] and"}, {"title": "Empirical studies of open source and open weight AI", "content": "With rapid interest and growth of AI in open-source contributions, researchers have been exploring the potential of data-driven research and feasibility of repository mining on AI-centric development platforms and services [68, 70, 11, 10]. Software engineers have been particularly interested in modularity and artifact reuse from pre-trained model repositories or \"PTMs\" [69, 60, 112], and accompanying security risks and vulnerabilities [71, 74]. Several studies have explored model documentation [102, 60, 119]. Gong et al. focus on usage documentation across multiple platforms [60], while Liang et al. analyzed different sections across Hugging Face model cards and how comprehensive documentation may improve model popularity [83]. Castano et al. studied carbon footprint reporting [34]. Hugging Face's internal study found that among all model card components, respondents found the risks sections the longest and most challenging to complete [101]. Osborne analyzed licensing and collaboration patterns, finding positively skewered patterns in contribution, engagement, and model usage [99]."}, {"title": "Methods", "content": null}, {"title": "Research Questions", "content": "After careful perusal of repositories spanned by our review (Sec. 2.3), we base our exploration and empirical analysis on Hugging Face, the most popular of contemporary PTM repositories [60] with over 0.7 mil submissions at the time of the study. Hugging Face is increasingly appealing to AI developers, even over long-standing platforms like GitHub [11, 10]. This is especially true as projects scale, requiring greater storage and computing requirements. Hugging Face is a PaaS exclusively for AI/ML development, offering tooling, storage for large artifacts, and remote servers for training, testing, and hosting apps, all under a single roof. While some contemporary model directories provide official base model releases for specific libraries and frameworks (e.g., Nvidia CUDA, Tensorflow, or Pytorch model directories) or vetted research projects (e.g., ModelZoo), HF spans models from these categories alongside vast numbers of amateur submissions, community contributions as well as public and private institutions. Therefore, it provides a large enough representative sample to observe development and model adaptation practices as is and gauge developer ethics in the wild."}, {"title": "RQ1: How is documentation of risks, limits, and biases among projects related to model evaluation?", "content": "Evaluation and Risks are core components of standard model cards. Hugging Face's guided annotation template [52] encourages developers to select appropriate testing procedures and benchmarks to evaluate model performance and document the results. It also recommends that such evaluation should involve testing for potential usage limitations, vulnerabilities, and biases in the model to aid sociotechnical experts in comprehensive risk documentation. Therefore, proper motivation, understanding, and proficiency in evaluation are expected to inculcate cognizance of responsible development practices. We frame the research question as follows:\nRisks and Biases Documented ~ Project Covariates + Evaluation Reported"}, {"title": "RQ2: How is risk documentation of projects related to their accuracy?", "content": "We may expect developers of highly accurate models to be more proficient and well-rounded and, therefore, likelier to be able to also thoroughly probe and document risks, biases, and other limitations. Yet, we explain (see Sec. 2.2) how current trends may undermine the validity of benchmarking or even downplay the need for holistic evaluations above and beyond accuracy. RQ2 can be modeled as follows:\nRisks and Biases Documented ~ Project Covariates + Model Accuracy\nWe pursue RQ2 on submissions to Hugging Face's first edition of the Open LLM leaderboard, which ran from May 2023 to June 2024. It drew remarkable levels of participation across different project types. Importantly, it observed rigorous community monitoring for contamination [16] and other evaluation malpractices, as well as reproducibility checks to substantiate self-reported performances, thus strengthening the validity of measurements and analysis."}, {"title": "Data", "content": "Here, we describe some of the project-level covariates we consider in our empirical data analysis and explain their inclusion, i.e., how they motivate documentation habits and accountability among projects.\nOur review of prior studies and the HF Hub codebase and documentation revealed how the information we sought is distributed across the project landing page, repository and its metadata, index tags, and finally, the model card markdown files. HF Hub uses semantic tags to index models and facilitate search, which are often auto-detected or parsed from the YML component of model cards. To access repository records or model tags, we use the Hugging Face API. We focus our study on 700,072 repositories uploaded to Hugging Face as of 06/15/2024. We only include completely open repositories by filtering out 'gated' repositories whose file contents or commit history are private.\nProject use, Developer activity, and Community engagement: Git-based information, such as repository age and commit activity, were available for all open repositories, while usage/popularity metrics were available on every model's landing page. Controlling for time lets us account for documentation practices as a function of evolving development standards, conception of ethical practices, and regulatory oversight. We measure repository age as the time between project initiation (first commit) and data collection. For developer and community engagement around a model, we measure the total number of commits, pull requests, and all other discussions (including issues) on each repo. Developers seeking greater exposure and usage of their projects may practice better documentation [60]. Several prior studies used API calls or downloads to measure model popularity. At the time of the study, Hugging Face only displayed model download stats for the current month. We use total model likes from users as a cumulative measure of popularity. Unlike its counterparts like GitHub, Hugging Face does not offer an option to fork repositories directly but allows porting to build applications called spaces. We use the total number of spaces spinning off a repository to measure model circulation.\nModel application and usability: Since AI auditing and regulation through documentation are particularly applicable for service-ready models and AI applications [13, 77], we screen out incomplete projects and dumps and test our hypothesis on especially well integrated, ready to use projects. Based on a review of HF documentation and semantic categories listed in the API, we identify service-ready models through at least one of the following:\n\u2022 Model cards filled with detailed instructions, examples, and use cases: Detected using HF's Model card scanner\n\u2022 Verifiable integration into the HF ecosystem (can be used for training, tuning, or inference) : Integrated models are tagged with training or deployment options within the HF ecosystem, such as \"endpoints_compatible\", \"autotrain_compatible\" or have widgets enabled on their webpage for users to explore and interact with the model.\n\u2022 Model page carries a \"Use this model\" feature for deployment through a developer-provided space or supported third-party platforms.\nAll in all, 456,545 projects out of 700,072 repositories fulfilled this criteria.\nHF tracks information on the modalities and tasks performed in index tags for most service-ready models. These span six major types: Natural Language Processing, Computer Vision, Audio, Reinforcement Learning, Tabular Data, and Multimodal. Note that a particular AI application may qualify under multiple categories, e.g., a prompt-driven image generator may be placed under Natural Language Processing (interpreting human queries), Computer Vision (image generation), Multimodal (operates across multiple modalities, i.e., image and text), and Reinforcement Learning (learning from human feedback).\nDeveloper attributes: We scrape the model landing pages and linked developer profiles for information vital for a controlled study. Hugging Face supports single-user accounts or team accounts called \"organizations.\" The growing importance and evolving sophistication of documentation benefits from multiple contributors and distributed responsibilities. Hugging Face's official documentation designates model auditing responsibilities across well-defined roles, such as the manager, the sociotechnical expert, and the developer. Further, information management may also depend on the type of developer or provider. In particular, commercial entities anticipating regulatory purview may ideally conduct more thorough risk assessments to avert potential liabilities from failures and misuse. Team pages contain community sizes and the type of entity owning the account, such as a company releasing 'freemium' models, an educational institution (university or classroom), or a non-profit. For all developers, we also include the total number of models they contributed as a measure of experience.\nModel Scale: In the context of AI, scaling refers to enhancing learnability and performance by developing highly parameterized, data-intensive models. Between 2017 and 2022, parameterization in Google's language models grew from 110 million for BERT (base) [46] to 540 Billion for PaLM [5]. While promising enhanced capabilities, Large"}, {"title": "Exploratory Analysis", "content": "Before answering our quantitative RQs (covered in the next section), we begin with a preliminary analysis to understand general trends within our collected data.\nHugging Face initially gained recognition through an open-source implementation [118] of the seminal transformer architecture [114], primarily targeted toward NLP development. The first version of the Hugging Face client library was released in late December 20205 to facilitate remote, collaborative development and artifact storage, reuse, and sharing. By the end of 2020, the collective comprised 4,634 projects and 672 unique contributors. The platform has since expanded support to over 20 ML libraries, AI frameworks, and applications. At the time of data collection on 06/15/2024, the HF Hub held 700,072 projects across 178,030 developers."}, {"title": "Multivariate Hypothesis Testing for RQ1 and RQ2", "content": "Here, we address our core RQs introduced above. For both RQs, we consider models whose information on significant covariates was released on the HF Hub and available in structured, parseable form. Based on the literature review, trends in AI safety research, and our exploratory analysis, we consider five main categories of covariates: project scale, modality, domain, popularity and usage, and developer engagement. This left us with 7093 samples for RQ1, a subset of the entire HF directory representing service-ready, highly transparent models where data provenance and model specifics (number of parameters) are available through safe, robust file management. Around 23.19%, 7.86% and 2.04% had provided evaluation, risk assessments, and CO2 emission data, respectively. We frame our RQs as binary prediction modeling to determine if risk assessment and social impact accountability are significantly associated with 1. rates of performance evaluation and disclosure and 2. absolute mean performance on a set of very popular benchmarks used in the Open LLM Leaderboard. For both cases, we model the likelihood of risk assessment in model cards using binomial logit models, where evaluation practices (RQ1) or performance (RQ2) are the main regressors of interest, adjusting for crucial project-level covariates. We set the significance level of our analysis at 0.01.\nRQ2 is based on a subset of RQ1, which participates in the open LLM leaderboard and only comprises NLP models. In this particular subset of models with 100% evaluation reporting (through benchmark participation), we again find higher"}, {"title": "Discussion", "content": "Evaluation is core to responsible AI. It is essential to determining model capabilities, and also serves as empirical means to other aspects of responsible AI, such as understanding and acknowledging risks and limitations. Our analyses of OSS practitioners confirms that evaluation and risk assessment generally go hand in hand. However, we also observed that metric-centric arenas, such as competitive leaderboards, may see lesser acknowledgment of model risks among high performers.\nCertain other observations were consistent across both analyses. As one might anticipate, development at scale (data-intensive training or parameterization) positively correlates with compliance. Documentation of social impact is also closely associated with broader awareness (as expressed through estimation and reporting of CO2 footprint), and projects with more activity, contributions and larger teams tended to do a better job with risk reporting. On the other hand, prolific developers appear to pay less attention to assessing and documenting the limitations of their projects. Informed by these trends, we hereby present our recommendations for contributors, entrepreneurs, and AI hosting services. These include practices and interventions to encourage documentation overall, and to improve efficacy of evaluation protocols in informing both model strengths and weaknesses."}, {"title": "Recommendations", "content": "As one of the leading open-source AI hosting services, Hugging Face has taken steps to inculcate responsible documentation, monitor compliance\u00b9, and keep up with regulation [6]. Results from our empirical analyses suggest that risk documentation practices are more prevalent among large teams, while most contributions come from individual developers. Model card guidelines used by Hugging Face and other notable institutions23 are detailed to facilitate auditing, and usually set specific tasks across developers, sociotechnical experts and managers. Risk assessments involve multiple roles and can make compliance overwhelming for small teams. Streamlining, such as outlining priority requirements may make risk documentation more approachable.\nHF's open LLM leaderboard is a massive undertaking, supported by collaborative monitoring labor from community and moderators. It is notably more transparent than conventional leaderboards (See Sec. 2.2) and tracks model size, precision, libraries, and architectures of most submissions. Such considerations are expected to support explainability,"}, {"title": "Limitations", "content": "Our research formulation, analyses, and results are meant to explore the correlation between evaluation and risk documentation rather than establish a causal implication. We aimed to measure the prevalence of responsible development practices as operationalized through reporting compliance. With limited regulatory requirements or platform specifications on OSS as of now, we cannot conclusively determine whether the risk assessments provided are necessary or sufficient for any given model, i.e. while our quantitative analyses help to explain existing behaviors, it can be hard to translate these behaviors into impact. Growing consensus over AI safety standards and research establishing testing protocols can be expected to be adopted by platforms, and future work may explore their diffusion into practice, particularly how specific tests enable risk quantification and their efficacy."}, {"title": "Ethics Statement", "content": "For our data collection, we largely followed prior work and used the public Hugging Face API for model cards and open repository data. Beyond the API, we collected some limited public-facing numeric data from model landing pages, which are intended for public reference and sharing with no expectation of privacy (Table. 1). We did not add any features to our data collection code for specifically parsing personally-identifying information, nor was such information required for our analysis. The only identifiers used were public developer usernames, which are also part of the model path in the HF web indexing. Finally, we note that some public, open weight models carry \"Not Safe For Work\" and \"Not for all Audiences\" warnings from developers or the platform moderators7. We retain these labels in our dataset so that any researchers wishing to use this data can be fully informed about the potential for some model metadata to contain content inappropriate for some settings. Proprietary LLM-based language editing services Grammarly and Anthropic Claude were used to a limited extent to correct misspellings, grammar and consistency of composition, and the resulting manuscript was thoroughly verified and updated by all the authors over multiple iterations."}, {"title": "Conclusion", "content": "Through our focused study of a rising open source platform, we had the opportunity to observe a diverse range of AI/ML applications and development practices. Our analyses empirically probe open source AI trends in the backdrop of increasing concerns over their potential to transform or affect society, and consequent legal and ethical oversight. As we situate our investigation amidst the interests of these various stakeholders, we discover promising trends of concurrent compliance of evaluations and risk assessments. At the same time, our large sample study produces evidence supporting long standing observations and calls for fundamental reforms and greater rigor in AI evaluation.\nAs AI continues to grow, these lessons emphasize the importance of fostering a culture of responsible development and accountability across all sectors, not only commercial but also informal and non-profit undertakings. Platforms, developers, and stakeholders must work together to establish best practices and design balanced policies and standards that mutually support each other while also preserving the true spirit of innovation. This will be vital in ensuring that AI technologies are developed and deployed ethically, safely, and benefit humanity at large."}]}