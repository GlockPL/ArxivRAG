{"title": "Retrieval Replace Reduction:\nAn effective visual token reduction method via semantic match", "authors": ["Yingen Liu", "Fan Wu", "Ruihui Li", "Zhuo Tang", "Kenli Li"], "abstract": "Multimodal large language models (MLLMs)\nhave demonstrated strong performance across\nvarious tasks without requiring training from\nscratch. However, they face significant compu-\ntational and memory constraints, particularly\nwhen processing multimodal inputs that exceed\ncontext length, limiting their scalability. In this\npaper, we introduce a new approach, TRSM\n(Token Reduction via Semantic Match), which\neffectively reduces the number of visual tokens\nwithout compromising MLLM performance.\nInspired by how humans process multimodal\ntasks, TRSM leverages semantic information\nfrom one modality to match relevant seman-\ntics in another, reducing the number of visual\ntokens.Specifically, to retain task relevant vi-\nsual tokens, we use the text prompt as a query\nvector to retrieve the most similar vectors from\nthe visual prompt and merge them with the text\ntokens. Based on experimental results, when\napplied to LLaVA-1.5(Liu et al., 2023), our ap-\nproach compresses the visual tokens by 20%,\nachieving comparable performance across di-\nverse visual question-answering and reasoning\ntasks.", "sections": [{"title": "1 Introduction", "content": "Thanks to advanced model architectures and ex-\ntensive training data, large-scale language mod-\nels have achieved exceptional performance across\nvarious application domains. Traditionally, these\nmodels have focused on textual inputs, excelling\nin tasks like natural language understanding and\ngeneration within prominent NLP fields(Touvron\net al., 2023). However, real-world data includes\nnot only text but also diverse modalities such as\nimages, audio recordings, and point clouds.\nResearchers are now exploring the extension of\nthese impressive capabilities into multimodal do-\nmains, leading to the development of multimodal\nlarge-scale models such as GPT-4(OpenAI, 2024),\nGemini(Team, 2024), LLaVA(Liu et al., 2023), and\nMiniGPT-4(Zhu et al., 2023). These models avoid\nthe high computational costs associated with train-\ning from scratch by leveraging pre-training knowl-\nedge specific to each modality. Moreover, through\nthe establishment of strong representational map-\npings and alignments with modality-specific mod-\nels, these multimodal models can efficiently pro-\ncess inputs across multiple modalities, significantly\nbroadening their potential application areas.\nHowever, MLMMs face significant challenges\nduring deployment and inference, primarily related\nto computational resources and efficiency. Since\nthese models must process both visual and textual\ndata simultaneously, their context length increases\nsubstantially, leading to higher storage demands\nand greater computational performance require-\nments. How to efficiently deploy multimodal large\nlanguage models in resource constrain environment\nhas become a key issue that needs to be resolved.\nPrevious works often require the design of new\nnetwork architectures(Chu et al., 2023), which\nincrease significant computational costs. Addi-\ntionally, some approaches(Shang et al., 2024) are\nclosely tied to specific attention patterns or model\nstructures, making deployment in practical applica-\ntions challenging.\nFortunately, our preliminary exploration has\nyield intriguing observations about the empirical\nproperties of MLLMs. These findings pave the way\nfor the potential design of efficient inference.\nInspired by how humans complete multimodal\ntasks, we observe that for the same image, the ac-\ntual focus areas of the task vary depending on the\nprovided text prompt. For efficient multimodal\ntasks, only the regions where text and images mu-\ntually focus should participate in generation. This\napproach not only reduces computational and mem-\nory burdens but also helps avoid the hallucination\nphenomenon in large models caused by irrelevant\ninformation.\nBased on our observations, we introduce\nTRSM, inspired by Retrieval-Augmented Gener-\nation (RAG)(Borgeaud et al., 2022), which typ-\nically constructs a vector database from a large\ncorpus and retrieves the k most similar vectors to\na query. However, our method diverges from tra-\nditional RAG by not relying on an extensive exter-\nnal corpus. Instead, we create a vector database\nfrom input image vectors processed by the visual\nmodel, using the input prompt as the query. By\nretrieving the k most similar vectors from this im-\nage vector database, we achieve semantic matching\nthat enhances the understanding of retrieved doc-\numents, leading to more accurate responses. This\napproach optimizes the retention of task-relevant\nvisual tokens, providing a compact and efficient\nvisual context for MLLMs without compromising\nperformance across various multimodal applica-\ntions.\nAll in all, our approach offers significant ad-\nvantages distinct from previous works. First, it\neliminates the need to build complex network ar-\nchitectures, thus avoiding the additional costs as-\nsociated with training. Second, our method fea-\ntures a plug-and-play design, making it particularly\nsuitable for deployment in resource-constrained en-\nvironments. Finally, this approach draws on how\nhumans process multimodal tasks, providing good\ninterpretability, which enhances its flexibility and\nefficiency in practical applications."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Multimodal Large Language Models", "content": "The development of Large Language Models\n(LLMs) such as GPT-3(Brown et al., 2020),\nLLaMA(Touvron et al., 2023), and GPT-4(OpenAI,\n2024) has seen substantial progress in recent years.\nThese advancements have inspired the evolution\nof Multimodal Large Language Models (MLLMs),\nwhich extend the capabilities of LLMs to include\nimages. Notable examples of this progress are\nLLava(Liu et al., 2023), MiniGPT-4(Zhu et al.,\n2023), InstructBLIP(Dai et al., 2023),and Gem-\nini(Team, 2024) .\nThese MLLMs primarily utilize a visual en-\ncoder(Radford et al., 2021) to process and receive\nvisual input. They then align this visual data with\ntext through a vision-text projection and concate-\nnate the visual information with text tokens for gen-\neration by a pre-trained LLM. By integrating data\nfrom various modalities, MLLMs enhance contex-\ntual understanding, thereby improving the accuracy\nof information processing and generation.\nDespite these advancements, MLLMs face sig-\nnificant computational costs during inference and\ndeployment, highlighting the need for efficient\nmodel compression techniques."}, {"title": "2.2 Visual token pruning", "content": "The notorious quadratic complexity in Transform-\ners(Vaswani et al., 2023) is a well-known issue, as\nit poses a significant bottleneck when scaling in-\nput sequence lengths.This problem is particularly\npronounced in multimodal large language models,\nwhere image inputs are converted into numerous\ntokens, imposing a substantial computational and\ndeployment burden on the model.\nTo address this, recent research has focused\non pruning visual tokens to enhance inference\nefficiency.LLava-PruMerge (Shang et al., 2024) in-\ntroduces an innovative adaptive visual token reduc-\ntion strategy that leverages the sparsity observed\nin the visual encoder to selectively retain key vi-\nsual tokens. The informational content of these\nretained tokens is then improved through cluster-\ning and weighted averaging.MADTP(Cao et al.,\n2024) presents a Multi-modality Alignment Guid-\nance (MAG) module and a Dynamic Token Prun-\ning (DTP) module, which provide layer-wise and\ninstance-wise dynamic compression based on the\ncomplexity of input instances.\nIn our study, we propose a novel plug-and-play\ntoken reduction method that targets visual token\nredundancy, achieving comparable performance\nwith fewer than 20% of the original tokens."}, {"title": "3 Observations", "content": ""}, {"title": "3.1 The sparsity of visual context.", "content": "Multimodal large models utilize visual encoders to\nconvert image inputs into tokens that can be pro-\ncessed by pre-trained large models. These tokens\nare concatenated with the user's prompt and system\nprompt, forming a longer sequence for the model to\nprocess. For example, in the Llava-v1.5 model, an\noriginal image is projected into a fixed number of\n576 image tokens, while the user's prompt and sys-\ntem tokens typically do not exceed 40 tokens. This\ndemonstrates that visual tokens are the primary fac-\ntor in handling long sequences within multimodal\nlarge models.\nExperimental results indicate that the attention\nscore matrix in visual-text sequences exhibits sig-\nnificant sparsity, particularly within the visual se-\nquence portion. We hypothesize that not all image\ntokens from the visual encoder contribute positively\nto the final output of the MLMM. A considerable\nnumber of image tokens are likely redundant. The\nLLM itself has sufficient generalization capabili-\nties, meaning that even a small subset of image\ntokens can provide enough semantic information\nto aid the multimodal large model in the generation\nprocess. In summary, redundancy in the image con-\ntext can be extraneous for MLLMs and might lead\nto unnecessary computational expense."}, {"title": "3.2 The semantic match between modals.", "content": "During inference in MLMMs, we observed that\na single modality cannot accomplish the task in\nisolation. Multimodal tasks require the participa-\ntion of various modalities, each contributing similar\nsemantics, a phenomenon we term semantic match-\ning. For example, in multimodal visual dialogue\ntasks, it is necessary to match the semantics of both\nimages and text simultaneously.Analogously, when\nhandling visual question answering, one must first\nunderstand the question and then look for objects in\nthe image that are related or similar to the question.\nIrrelevant objects are disregarded. Once the related\nobjects are identified, a more detailed search among\nthese objects yields the answer. For instance, to\nanswer the color of a dog's eyes in a picture, one\nmust first identify all the animals in the image, not\nthe plants, and then determine the eye color, which\nmight be brown.\nTo complete tasks involving multiple modalities,\nit is essential to match the same semantics across\ndifferent modalities. This generally involves obtain-\ning the general semantics of the data from the first\nmodality and then searching for the corresponding\nsemantics in the data from the other modality. Once\nsemantic matching between the two modalities is\nachieved, the multimodal task can be completed\nmore efficiently."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Preliminaries", "content": "Multimodal large language models (MLLMs) rep-\nresent a significant advancement in artificial intel-\nligence by integrating visual encoders with pre-\ntrained large language models. This integration al-\nlows these models to jointly process and understand\ndiverse modalities, such as images and text, en-\nabling more comprehensive and contextually aware\nanalyses.\nFor a given image I, the visual encoder $f_{visual}$\ntransforms it into a series of token representations\n$X_1$:"}, {"title": null, "content": "$X_1 = f_{visual}(I) \\qquad(1)$\nHere, $X_1$ denotes a set of visual tokens that cap-\nture essential visual features and semantic informa-\ntion from the image. This process allows image\ncontent to be encoded in a format compatible with\nthe language model, facilitating the effective inte-\ngration of visual and textual data.\nTo ensure that the visual tokens are compatible\nwith the language model's processing framework, a\nvisual projection layer is employed. This layer con-\nverts visual tokens $X_1$ into language embedding\ntokens $Y_1, Y_2,..., Y_n$. The transformation is per-\nformed such that these language embedding tokens\nshare the same dimensional space d as the word\nembeddings used by the language model:"}, {"title": null, "content": "$y_j = f_{projection}(X_1) \\qquad(2)$\nwhere $f_{projection}$ denotes the projection function\nthat aligns the dimensionality of the visual tokens\nwith that of the language embeddings.\nThis alignment ensures that the visual informa-\ntion encoded in $Y_1, Y_2,..., Y_n$ can be processed\nseamlessly alongside the textual information by the\nlanguage model. Consequently, the MLMM can\neffectively integrate and leverage both visual and\ntextual representations, improving its ability to gen-\nerate accurate and contextually relevant outputs."}, {"title": "4.2 Semantic Matching Improves\nMulti-Modal Inference", "content": "Drawing inspiration from human cognitive pro-\ncesses, especially in tasks requiring the integration\nof visual and textual information, we propose the\nconcept of semantic matching. This mirrors the\nhuman tendency to focus on image regions that are\nmost relevant to the associated text, allowing for\nmore efficient interpretation and understanding of\nthe combined modalities.\nIn multimodal large language models (MLLMs),\nthe output generation is significantly shaped by\nregions of the image that align closely with the\ntext, where semantic matching is most prominent.\nMathematically, this relationship can be formulated\nas:"}, {"title": null, "content": "$Y_{key} = [Y_1, Y_2,\u00b7\u00b7\u00b7, Y_k] \\qquad(3)$\nHere, $Y_{key}$ denotes the top-k visual tokens\n$[Y_1, Y_2,..., y]$ that are deemed relevant to the text\ninput.\nSemantic matching offers two primary advan-\ntages in enhancing the generation process:\n(1) Reduction of Computational Load: By\nclassifying visual tokens into matching and non-\nmatching categories, semantic matching allows for\nthe removal of non-matching tokens. This reduces\nthe computational burden, as the model only pro-\ncesses and generates output based on relevant vi-\nsual information, thus optimizing performance and\nefficiency.\n(2) Mitigation of Illusion Phenomena: In\nMLMMs, the visual domain can often suffer from\n\"illusion phenomena\" where irrelevant or mislead-\ning visual information affects the model's output.\nSemantic matching helps to alleviate this issue by\nensuring that the multi-modal model focuses pri-\nmarily on image regions that are directly related to\nthe textual prompt. This results in more accurate\nand contextually relevant generation."}, {"title": "4.3 Token reduction via semantic retrieval", "content": "To accelerate computation and reduce the computa-\ntional burden during inference, an effective strategy\ninvolves removing non-matching tokens. The key\nto this approach is accurately identifying and select-\ning the matching tokens, as this directly impacts\nboth the model's inference efficiency and the qual-\nity of the final output.\nTo address this, we introduce a token reduction\nmethod aimed at identifying semantic match tokens\nand evit unmatch tokens.\nOur method is inspired by the Retrieval-\nAugmented Generation (RAG) framework\n(Borgeaud et al., 2022), which performs semantic\nmatching by retrieving similar vectors. In RAG,\na query vector is compared with vectors in a\ndatabase to retrieve the most relevant content for\ndownstream generation tasks. We extend this idea\nto the inference process of MLMMs to improve\ntheir semantic understanding and computational\nefficiency.\nSpecifically, in our approach, the vector $X_1$ pro-\ncessed through the visual encoder and projection\nlayer serves as the vector database, while the text\nvector $X_{prompt}$ acts as the query vector. During\nmultimodal inference, the text vector $X_{prompt}$ en-\ncapsulates the information from the text modality,\nand the visual vector $X_1$ encodes features from\nthe image modality. These vectors coexist in a\nshared semantic space, enabling cross-modal se-\nmantic alignment and seamless interaction between\nmodalities.\nIn the RAG framework, the query vector re-\ntrieves the top-k most similar vectors from the vec-"}, {"title": null, "content": "tor database, which are semantically aligned with\nthe query. Similarly, in our multimodal inference\nmethod, we employ this retrieval strategy, where\nthe top-k visual tokens are considered the most\nsemantically relevant to the text vector $X_{prompt}$.\nMathematically, this can be expressed as:"}, {"title": null, "content": "$Y_{topk} = {Y_i | S(Y_i, X_{prompt}) \u2265 \u03c4,\\forall y_i \u2208 X_1} \\qquad(4)$\nwhere $s(Y_i, X_{prompt})$ denotes the semantic simi-\nlarity score between a visual token $y_i$ and the text\nquery vector $X_{prompt}$, and $\\tau$ is a predefined thresh-\nold. The tokens that meet or exceed this threshold\nare retained as the most relevant for the subsequent\ninference steps.\nThis process effectively identifies the most rel-\nevant modality information based on semantic\nmatching, providing precise cues for multimodal\ninference, and significantly reducing the computa-\ntional burden by filtering out less relevant tokens.\nThe advantage of this method lies in its ability to\nfully exploit the latent correlations between differ-\nent modalities during multimodal data processing,\nthereby avoiding inference errors that may arise\nfrom relying on a single modality. By integrat-\ning the RAG retrieval mechanism into multimodal\ninference, we achieve more efficient cross-modal\ninformation fusion and reasoning, thereby enhanc-\ning the overall performance of the model. This\nsemantic matching mechanism offers a more in-\ntelligent approach for multimodal large language\nmodels, equipping them with stronger semantic un-\nderstanding and reasoning capabilities for complex\nscenarios.\nThis procedure is outlined in Algorithm 1.\nBy precisely identifying and selecting matching\ntokens, we can significantly reduce the model's\ncomputational complexity, enabling faster infer-\nence speeds while maintaining performance. The\nsuccessful application of this technique not only\nfacilitates the deployment of large models in\nresource-constrained environments but also pro-\nvides competitive solutions for real-time applica-\ntions."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Main Results", "content": "We first applied our method to the LLaVA-1.5\nmodel, a recently open-sourced multimodal large\nmodel. Notably, our approach requires no addi-\ntional training or fine-tuning, which distinguishes"}, {"title": null, "content": "it from most mainstream methods. Moving for-\nward, we plan to build and test this system across\nmore model architectures.\nWe evaluated the model on several visual ques-\ntion answering and visual reasoning benchmark\ndatasets, including GQA (Hudson and Manning,\n2019), TextVQA(Singh et al., 2019), POPE(Li\net al., 2023a), MME(Fu et al., 2024), MM-\nBench(Liu et al., 2024), and MMVet(Yu et al.,\n2023).\nWe further explored the model's performance\nunder different compression ratios, specifically se-\nlecting 0.5, 0.6, 0.7, and 0.8 for experimental anal-\nysis. By adjusting the proportion of retained visual\ntokens, we evaluated how the model's accuracy and\ninference efficiency changed at each compression\nlevel. This experiment helps us better understand\nthe impact of compression ratio on model perfor-\nmance and provides guidance for selecting the op-\ntimal compression ratio in practical applications.\nThe experimental results, shown in Table 1, illus-\ntrate the performance comparison across different\ncompression ratios.\nInterestingly, we observed that when removing\nsome visual tokens, the model's accuracy in sev-\neral tasks (such as POPE, MME, MMBench, and\nMMVet) was even higher than the original model.\nWe speculate that this is due to the simplification of\nvisual context, where irrelevant visual tokens are\nremoved, thereby reducing hallucination effects\nduring inference and improving the overall genera-\ntion quality."}, {"title": "5.2 Efficiency Analysis", "content": "To efficiently evaluate the computational perfor-\nmance of our method, we conducted a theo-\nretical analysis of factors such as latency and\nmemory usage using the Roofline tool based on\nLLMviewer(Yuan et al., 2024).\nUsing the LLava-1.5 7B model as an example,\nwe analyzed the multimodal large model inference\nprocess in typical scenarios. This model processes\nimages with a resolution of 336\u00d7336 pixels, which\nare converted into 576 visual tokens through the\nCLIP model, combined with a prompt input of\napproximately 40 text tokens.\nLLava-PruMerge achieved a compression ratio\nof about 25%, reducing the visual tokens to 144.\nIn contrast, our method, while maintaining similar\naccuracy on VQA tasks, achieved a 20% compres-\nsion ratio, reducing the visual tokens to 116.\nAs shown in Table 2, our method significantly\nimproved model inference speed and reduced mem-\nory consumption. Specifically, for generating the\nfirst token, prefill time was reduced to 32.4% of\nthe original, and the activation during inference\nwas reduced to 17%. This makes our method more\nsuitable for deploying large models in resource-\nconstrained environments."}, {"title": "5.3 Ablation Study", "content": "For retrieval-based systems, the choice of retrieval\nalgorithm directly impacts overall efficiency and in-\nference performance. Different retrieval algorithms\nvary in terms of processing speed, memory over-\nhead, and semantic matching accuracy, making it\ncritical to select the appropriate algorithm.\nIn this experiment, we conducted an ablation\nstudy on the retrieval algorithms used in TRSM to\nevaluate their effect on final accuracy. By testing\nvarious retrieval algorithms under the same condi-\ntions, we quantified their differences in processing\nefficiency and accuracy, allowing for a deeper anal-\nsis of their specific impact on model performance.\nThe results, as shown in Table 3, highlight the rela-\ntive performance of each retrieval algorithm during\nthe model inference process."}, {"title": "6 Conclusions", "content": "This paper introduced TRSM, an approach de-\nsigned to improve long visual context in multi-\nmodal language models,which get enlightments\nfrom human finish multimodal task and the basic\nconcept of sparsity in visual context.TRSM em-\nploys a retrieval-augmented generation strategy,\nwhere the top-k most similar vectors from the vi-\nsual tokens are retrieved based on their similarity\nto the text tokens. This approach enables seman-\ntic matching across modalities, and the generation\nprocess is completed by concatenating the seman-\ntically matched vectors from both modalities. Our\napproach, applied to LLaVA-1.5, demonstrated that\nby utilizing only 20% of visual tokens on average,\nthe pruned tokens can maintain comparable per-\nformance across a wide range of visual question-\nanswering and reasoning tasks.In the future, we"}]}