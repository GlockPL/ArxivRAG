{"title": "An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems.", "authors": ["A. Mantelero", "M.S. Esposito"], "abstract": "Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems.\n\nThe focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use.\n\nBased on empirical analysis of this evidence, this work presents a methodology and an assessment model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment models are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds.\n\nThe proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.", "sections": [{"title": "1. Introduction", "content": "The debate that has characterised the last few years on data and Artificial Intelligence (AI) represents an interesting arena in which to consider the theoretical evolution of the future approach in addressing the challenges posed by AI to human rights. This debate has been marked"}, {"title": "2. The debate on AI regulation", "content": "While data processing regulation has been focused for decades on the law, including the interplay between data use and human rights, in recent years the debate on AI and the use of data-intensive systems has rapidly changed its trajectory, from law to ethics. This is evident not only in the"}, {"title": "3. Framing the ethical and the human rights-based approaches", "content": "From the outset, the debate on data ethics has been characterised by an improper overlap between ethics and law, in particular with regard to human rights. In this sense, it has been suggested that ethical challenges should be addressed by \u201cfostering the development and applications of data science while ensuring the respect of human rights and of the values shaping open, pluralistic and tolerant information societies\u201d.14 We can summarise this approach as \u2018ethics first': ethics plays a central role in technology regulation because it is the root of any regulatory approach, the pre-legal humus that is more important than ever where existing rules do not address or only partially address technological challenges.\n\nAnother argument in favour of the central role of ethics comes out of what we might call the 'ethics after' approach. 15 In the concrete application of human rights we necessarily have to"}, {"title": "4. Defining an operational approach to human rights assessment in AI", "content": "In considering the impact of AI on human rights, the dominant approach in many documents is mainly centred on listing the rights and freedoms potentially impacted39 rather than operationalising this potential impact and proposing assessment models.\n\nMoreover, case-specific assessment is more effective in terms of risk prevention and mitigation than using risk presumptions based on an abstract classification of \u201chigh-risk sectors and high-risk uses or purposes\u201d,40 where sectors, uses and purposes are very broad categories which include different kind of applications \u2013 some of them continuously evolving \u2013 with a variety of potential impacts on rights and freedoms that cannot be clustered ex ante on the basis of risk thresholds, but require a case-by-case impact assessment.\n\nSimilarly, the adoption of a centralised technology assessment carried out by national ad hoc supervisory authorities\u2074\u00b9 can provide useful guidelines for technology development and can be used to fix red lines42 but must necessarily be complemented by a case-specific assessment of the impact of each application developed.\n\nFor these reasons, a case specific impact assessment remains the main tool to ensure accountability and the safeguarding of individual and collective rights and freedoms. In this regard, a solution to the problem could easily be drawn from the human rights impact assessment models already adopted in several fields.\n\nHowever, these models are usually designed for different contexts than those of AI applications.43 The latter are not necessarily large-scale projects involving entire regions with multiple social impacts. Although there are important data-intensive projects in the field of smart cities, regional"}, {"title": "4.1 A methodological approach for an evidence-based model", "content": "Having defined the importance of a human rights-oriented approach in AI data processing, there remains the methodological question of how to define the assessment benchmark.\n\nThree different approaches are possible: (i) a top-down theoretical approach; (ii) an inferential approach, and (iii) a bottom-up empirical approach. The first was used in the analysis conducted by Raso et al.52, in which various potentially affected rights are analysed on the basis of abstract scenarios grouped by sector-specific applications (risk assessments in criminal justice, credit scoring, healthcare diagnostics, online content moderation, recruitment and hiring systems, essay scoring in education).\n\nThe second approach was adopted by Fjeld et al.53, inferring values from existing ethics and right-based documents on AI regulation. This approach is close to the empirical approach, but is dominated by a quantitative dimension, focusing on the frequency of certain principles, and overlooking the heterogeneity of the documents. As the documents are often declarations by governmental and non-governmental bodies, they have the nature of guidelines and directives rather than concrete descriptions of the existing state-of-the-art: they are more focused on To-Be rather than on As-Is. This means that the prevalence of certain principles and values does not necessarily demonstrate a concrete and effective implementation of them.\n\nThe third approach, used in this work, adopts an evidence-based methodology grounded on empirical analysis of cases decided by DPAs and guidelines provided by these authorities . More specifically, the idea is to move from the reasoning adopted by decision-makers in scrutinising data-centred applications and use this experience to better understand which rights and freedoms are relevant in practice."}, {"title": "4.2 Human rights and data use in the DPAs' jurisprudence", "content": "Despite the authorities considered belonging to different legal and cultural traditions, the analysis of the documents did find common ground between them in their approach to human rights and freedoms.\n\nIt worth noting that, whereas the importance of these interests is clearly stated, in several cases the analysis of their relevance is not properly developed and in others emerges only indirectly in the DPAs' observations. In fact, DPAs often prefer to refer to principles such as proportionality,"}, {"title": "4.2.1 Respect for human dignity", "content": "A first core value underlying the DPA's decisions is human dignity, recognised as crucial in many legal systems and widely protected in European63 and international frameworks.64 Despite the difficulties in determining the precise meaning of this concept,65 it is a key notion in human rights law and a guiding principle that underpins and grounds all other principles in human rights, even in the context of data processing.67\n\nWe also found this broad notion of human dignity in the decisions of the DPAs, according to which human dignity encompasses various aspects of the individual sphere and is an important factor in many different contexts.\n\nFor instance, the DPAs recognise that negative outcomes for individual dignity may result from continuous and invasive monitoring, such as video surveillance or other monitoring"}, {"title": "4.2.2 Freedom from discrimination", "content": "According to DPAs, discriminatory practices\u201d may occur in many contexts and in relation to different types of personal data processing. Negative consequences may result, for example, from automated decision-making and profiling activities, which may perpetuate existing stereotypes and social segregation.80\n\nWith regard to AI-related applications, DPAs have focused on the risks of perpetuating discriminatory practices through automated profiling.81 Moreover, as the criteria and functioning of algorithms are often opaque, individuals might not know that they are being profiled or not understand the potential consequences. In this context, DPAs have considered, inter alia, the risk of bias that may arise from online behavioural advertising82 or IoT-based profiling.83 Likewise, DPAs have referred to the use of data-intensive systems in the context of police services and law enforcement, such as predictive policing.84\n\nAdverse discriminatory impacts may also result from the use of sensitive data to prevent or limit access to certain services or benefits. This is the case when sensitive information is requested by"}, {"title": "4.2.3 Physical, psychological, and social identity", "content": "Both the international and European legal frameworks consider personal identity in the broader context of individual privacy. The personal aspects of an individual's identity traditionally cover different dimensions \u2013 physical, psychological, and social identity \u2013 and a range of data (e.g. name, image, reputation, family and ethnic heritage, gender identification, sexual, political and religious orientation).\n\nThe notion of personal identity can thus encompass two different meanings: (i) the body of information that unequivocally identifies a person, distinguishing him or her from any other; (ii) information concerning the individual's projection in the social community.\n\nThe documents examined considered only the first meaning, with regard to data processing operations for personal identification.\n\nOne example is the use of biometric data to control access to certain areas (e.g. preventing outsiders from entering schools, tracking employees' whereabouts) or the use of genetic data. 93 The need to protect personal identity also emerges from the considerations expressed by the authorities in relation to the identification information collected in the social media environment94 and through RFID systems.95"}, {"title": "4.2.4 Physical, psychological and moral integrity and the intimate sphere", "content": "Personal integrity is protected at European and international level as an aspect of an individual's private life, and comprises the individual's physical, psychological and moral integrity. 97 In this sense, a natural person must be free from any interference, both in relation to the body and the mind. Respect for the intimate sphere of the data subject is also an important aspect of safeguarding individuals' integrity, referring to its moral dimension.98\n\nThe importance of the individual's physical integrity and intimate sphere is confirmed in DPA jurisprudence and opinions. Regarding the data subject's physical integrity, the DPAs mainly considered invasive data processing, such as that using implanted RFID devices (e.g. subcutaneous microchips) to collect and process personal information, including identification data, credit card number or health information.\u201d The DPAs limit their use to situations where they are strictly necessary and there are no less intrusive alternatives, giving data subject the right to ask for their removal at any time.\n\nWith regard to the individual's intimate sphere, DPAs have focused on monitoring tools, including video-surveillance in environments where privacy expectations are high, 100 as well as on the collection of biometric data, given their invasive nature,101 which could interfere with the data subject's intimate sphere in an excessive way.\n\nThe last group of cases concerns data processing operations carried out using IoT wearable devices or other devices used in close vicinity to the human body in daily life (e.g. smartphones"}, {"title": "4.2.5 Self-determination and personal autonomy", "content": "Individual self-determination and personal autonomy are widely recognised at both international and European level.103 Personal autonomy is protected as an aspect of individual private life104 and safeguards individuals against a wide range of external interference.105\n\nIndividual self-determination and personal autonomy necessarily entail the ability to freely take decisions and have them respected by others106. According to international and European human rights jurisprudence, individual personal autonomy also covers a further range of human behaviours, among which are the right to develop one's own personality and the right to establish and develop relationships with other people107, the right to pursue one's own aspirations and to control one's own information. 108\n\nIndividual self-determination and personal autonomy also represent foundational principles in data protection, which is why it is not surprising that DPAs often refer to them in a broad sense.109 These aspects emerge in both individual and relational contexts and involve freedom of choice, including freedom of movement and action, the free development of human personality and the right to informational self-determination.\n\nRegarding freedom of choice, which encompasses freedom of movement and action, the DPAs have paid particular attention to the possible adverse effects of continuous and invasive monitoring. For instance, they considered cases of data processing carried out using video surveillance systems in workplaces110 and schools111 or in public spaces112 (e.g. through the use of drones). The authorities also focus attention on the potentially negative outcomes arising from"}, {"title": "4.2.6 Freedom of expression and freedom of thought, conscience and religion", "content": "As in European and international legal systems132, the DPAs take freedom of expression to include various elements, such as the freedom to hold an opinion133, or to impart and receive information and ideas. 134\nAccording to the DPAs, data subjects' freedom of expression may be constrained, for example, by use of targeted Al systems in political campaigns,135 to influence voters and manipulate outcomes. Similarly, the DPAs pay attention to the interference with freedom that may occur when social networks automatically block access to a political group's page on the grounds of unverified complaints136. Moreover, the DPAs stress the need to safeguard the individual's freedom of expression in relation to fake news and online disinformation, underlining how misleading or false information may influence the public's political opinions. 137\n\nNegative consequences for freedom of expression and, in particular, for the freedom to receive information, may also arise from the publication of incorrect, obsolete information, 138 or unreal news by media. 139 In the same way, the DPAs consider individual freedom to receive information in assessing the legitimacy of data subjects' requests to remove or conceal information relating to them because they were unlawfully acquired.140\n\nFinally, the DPAs take into account the potential prejudice to the data subject's freedom of thought, conscience and religion141 that may derive from certain data processing operations. This is the case, for example, of the use of video surveillance systems in places of worship without security purposes or alternative measures, as this may condition and limit individuals' activity."}, {"title": "4.2.7 Freedom of assembly and association", "content": "Though only in a limited number of decisions, DPAs also consider the need to safeguard the data subject's freedom of assembly and association.143 They recognise that negative outcomes for the data subject's freedom of assembly may result for example from the gathering of identification"}, {"title": "4.2.8 The right to the confidentiality of communications", "content": "According to DPA jurisprudence, this right147 is relevant, for example, in cases of firms monitoring their employees' electronic communications (telephone conversations, e-mails, and social media148), where the consequences of any breach of confidentiality might affect not only the workers, but also others, such as the worker's family members and the company's customers. Here the authorities stress the need to balance the workers' right to secrecy of correspondence (and that of the other individual involved) with the legitimate rights and interests of the employer.\n\nFurthermore, the DPAs consider the right to confidential communications in relation to monitoring electronic communications and traffic data retention for national security purposes, where any interference with this fundamental right is allowed only if it is strictly necessary in the interests of national security.149"}, {"title": "5. A proposal for an HRIA model", "content": "The analysis described in Section 4 and the evidence provided by DPAs' decisions and practice show that the issues concerning the use of data-intensive applications are not circumscribed to the debated topic of bias and discrimination but have a broader impact on several human rights and freedoms. For this reason, a comprehensive HRIA model is needed.\n\nIt is worth noting that traditional HRIAs are often territory-based considering the impact of business activities in a given local area and community, whereas in the case of AI applications this link with a territorial context may be less significant.\n\nThere are two different scenarios: cases characterised by use of AI in territorial contexts with a high-impact on social dynamics (e.g. smart cities plans, regional smart mobility plans, predictive crime programmes) and those where Al solutions have a more limited impact as they are embedded in globally distributed products/services (e.g. AI virtual assistants, autonomous cars, recruiting AI-based software, etc.) and do not focus on a given socio-territorial community. While"}, {"title": "5.1 Planning and scoping", "content": "The first stage deals with definition of the HRIA target, identifying the main features of the product/service and the context in which it will be placed, in line with the context-dependent nature of the HRIA. Three are the main areas to consider at this stage:\n\n\u2022 description and analysis of the type of product/service, including data flows and data processing purposes\n\n\u2022 the human rights context (contextualisation on the basis of local jurisprudence and laws)\n\n\u2022 identification of relevant stakeholders.\n\nThe table below provides a non-exhaustive list of potential questions for HRIA planning and scoping. The extent and content of these questions will depend on the specific nature of the product/service and the scale and complexity of its development and deployment.152 This list is therefore likely to be further supplemented with project-specific questions."}, {"title": "5.2 Data collection and analysis", "content": "While the first stage is mainly desk research, the second focuses on gathering relevant empirical evidence to assess the product/service's impact on human rights and freedoms. In traditional HRIA this usually involves extensive fieldwork. But in the case of AI applications, data collection and analysis is restricted to large-scale projects such as those developed in the context of smart cities, where different services are developed and integrated. For the remaining cases, given the limited and targeted nature of each application, data collection is largely related to the product/service's features and feedback from stakeholders.\n\nBased on the information gathered in the previous stage (description and analysis of the type of product/service, human rights context, controls in place, and stakeholder engagement), we can proceed to a contextual assessment of the impact of data use on human rights, to understand which rights and freedoms may be affected, how this may occur, and which potential mitigation measures may be taken.\n\nSince in most cases the assessment is not based on measurable variables, the impact on rights and freedoms is necessarily the result of expert evaluation,154 where expert opinion relies on knowledge of case law, the literature, and the legal framework. This means that it is not possible to provide precise measurement of the expected impacts but only an assessment in terms of range of risk (i.e. low, medium, high, or very high).\n\nThe benchmark for this assessment is the evidence-based analysis using the methodology described in Section 4 and the results. Thus, different rights and freedoms may be relevant depending on the specific nature of the given application.\n\nExamination of any potentially adverse impact should begin with a general overview followed by a more granular analysis where the impact is envisaged."}, {"title": "6. Testing the HRIA", "content": "The next two sub-sections examine two possible applications of the proposed model, with two different scales of data use. The first case, an Internet-connected doll equipped with AI, shows how the impact of AI is not limited to adverse effects on discrimination, but has a wider range of consequences (privacy and data protection, education, freedom of thought and diversity, etc.), given the innovative nature of the application and its interaction with humans.\n\nThis highlights the way in which AI does not merely concern data and data quality but more broadly the transformation of human-machine interaction by data-intensive systems. This is even more evident in the case of the smart cities, where the interaction is replicated on large scale affecting a whole variety of human behaviours by individuals, groups and communities.\n\nThe first case study (an AI-powered doll) shows in detail how the HRIA methodology can be applied in a real-life scenario. In the second case (a smart city project) we do not repeat the exercise for all the various data-intensive components, because a full HRIA would require"}, {"title": "6.1 Testing HRIA on a small scale: the Hello Barbie case", "content": "Hello Barbie was an interactive doll produced by Mattel for the English-speaking market, equipped with speech recognition systems and AI-based learning features, operating as an IoT device. The doll was able to interact with users but did not interact with other IoT devices. 159\n\nThe design goal was to provide a two-way conversation between the doll and the children playing with it, including capabilities that make the doll able to learn from this interaction, e.g. tailoring responses to the child's play history and remembering past conversations to suggest new games and topics. 160 The doll is no longer marketed by Mattel due to several concerns about system and device security.161\n\nThis section discusses the hypothetical case, imagining how the proposed assessment model162 could have been used by manufactures and developers and the results that might have been achieved."}, {"title": "6.1.1 Planning and scoping", "content": "Starting with the questions listed in Tab. 1 above and information on the case examined, the planning and scoping phase would summarise the key product characteristics as follows:\n\na) A connected toy with four main features: (i) programmed with more than 8,000 lines of dialogue163 hosted in the cloud, enabling the doll to talk with the user about \u201cfriends, school, dreams and fashion\u201d;164 (ii) speech recognition technology165 activated by a push-and-hold button on the doll's belt buckle; (iii) equipped with a microphone, speaker and two tri-colour LEOs embedded in the doll's necklace, which light up when the device is active; (iv) a Wi-Fi connection to provide for two-way conversation.166\n\nb) The target-user is an English-speaking child (minor). Theoretically the product could be marketed worldwide in many countries, but the language barrier represents a limitation.\n\nc) The right-holders can be divided into three categories: direct users (minors), supervisory users (parents, who have partial remote control over the doll and the doll/user interaction) and third parties (e.g. friends of the user or re-users of the doll).\n\nd) Regarding data processing, the doll collects and stores voice-recording tracks based on dialogues between the doll and the user; this information may include personal data167 and sensitive information.168\n\ne) The main purpose of the data processing and AI is to create human-robot interaction (HRI) by using machine learning (ML) to build on the dialogue between the doll and its young users. There are also additional purposes: (i) educational; (ii) parental control and"}, {"title": "6.1.2 Initial risk analysis and assessment", "content": "The basic idea of the toy is an interactive doll, equipped with speech recognition and learning features, operating as an IoT device. The main component is a human-robot voice interaction feature based on AI and enabled by Internet connection and cloud services.\n\nThe rights potentially impacted are data protection and privacy, freedom of thought and diversity, and psychological and physical safety and health.179"}, {"title": "6.1.2.1 Data protection and the right to privacy", "content": "While these are two distinct rights, for the purpose of this case study we considered them together.180 Given the main product features, the impact analysis is based on following questions: 181\n\nDoes the device collect personal information? If yes, what kind of data is collected, and what are the main features of data processing? Can the data be shared with other entities/persons?\nCan the connected toy intrude into the users' private sphere?\nCan the connected toy be used for monitoring and surveillance purposes? If yes, is this monitoring continuous or can the user stop it?\nDo users belong to vulnerable categories (e.g. minors, elderly people, parents, etc.)?\nAre third parties involved in the data processing?\nAre transborder data flows part of the processing operations?\nTaking into account the product's nature, features and settings (i.e. companion toy, dialogue recording, personal information collection, potential data sharing by parents) the likelihood of prejudice can be considered very high (Tab. 3). The extent and largely unsupervised nature of the dialogue between the doll and the user, as well as the extent of data collection and retention make the probability high (Tab. 1). In addition, given its default features and settings, the exposure is very high (Tab. 2) since all the doll's users are potentially exposed to this risk.\n\nRegarding risk severity, the gravity of the prejudice (Tab. 4) is high, given the subjects involved (young children and minors), the processing of personal data in several main areas, including sensitive information,182 and the extent of data collection. In addition, unexpected findings may emerge in the dialogue between the user and the doll, as the harmless topics prevalent in the AI-processed sentences can lead young users to provide personal and sensitive information."}, {"title": "6.1.2.2 Freedom of thought, parental guidance and the best interest of the child", "content": "Based on the main features of the product, the following questions can be used for this analysis:\n\nIs the device able to transmit content to the user?\nWhich kind of relationships is the device able to create with the user?\nDoes the device share any value-oriented messages with the user?\n\n\u039f If yes, what kind of values are communicated?\n\n\u039f Are these values customisable by users (including parents) or on the basis of user interaction? If so, what range of alterative value sets is provided?\n\n\u039f Are these values the result of work by a design team characterised by diversity?\n\nHere the case study reveals the critical impact of AI on HRI owing to the potential content imparted through the device. This is even more critical in the context of toys where the interactive nature of AI-powered dolls changes the traditional interaction into a relational experience.183\n\nIn the model considered (Hello Barbie), AI creates a dialogue with the young user by selecting the most appropriate sentence from the more than 8,000 lines of dialogue available in its database. On the one hand, this enables the AI to express opinions which may also include value-laden messages, as in this sentence: \u201cIt's so cool that you want to be a mom someday\u201d.184 On the other, some value-based considerations are needed to address educational issues concerning \u201cinappropriate questions\u201d185 where the problem is not the AI reaction (Hello Barbie responds \u201cby asking a new question\u201d186), as previously, but the notion of appropriateness, which necessarily involves a value-oriented content classification by the AI system.\n\nAs these value-laden features of Al are inevitably defined during the design process, the composition of the design team, its awareness of cultural diversity and pluralism are key elements that impact on freedom of thought, in terms of default values proposed and the availability of alternative settings. In addition, the decision to provide only one option or several user-customisable options in the case of value-oriented content is another aspect of the design phase that can limit parents' freedom to ensure the moral and religious education of their children in accordance with their own beliefs."}, {"title": "6.1.2.3 Right to psychological and physical safety", "content": "Connected toys may raise concerns about a range of psychological and physical harms deriving from their use, including access to data and remote control of the toy.192 Based on the main features of the product examined, the following questions can be used for this analysis:\n\nCan the device put psychological or physical safety at risk?\nDoes the device have adequate data security and cybersecurity measures in place?\nCan third parties perpetrate malicious attacks that pose a risk to the psychological or physical safety of the user?\nAs regards the probability, considering the third-party origin of the prejudices and the limited interest in malicious attacks (no business interest, distributed and generic target), but also how easy it is to hack the toy, the probability (Tab. 1) of an adverse impact is medium. Exposure (Tab. 2) is low, given the prevalent use of the device in a supposedly safe environment, such as schools and home, where malicious access and control of the doll is difficult and adult monitoring is more frequent. The likelihood (Tab. 3) is therefore low.\n\nTaking into account the nature of the product examined, the young age of the user, and the potential safety and security risks,193 the gravity of prejudice (Tab. 4) can be considered medium. This is because malicious attacks can only be carried out by speech, and no images are collected. Nor can the toy \u2013 given its size and characteristics \u2013 directly cause physical harm to the user. The effort (Tab. 5) can be considered medium since parent-child dialogue and technical solutions can combat the potential prejudice. The severity (Tab. 6) is therefore medium.\n\nConsidering the likelihood as low and the severity of the prejudice as medium, the overall impact is medium (Tab. 8)."}, {"title": "6.1.2.4 Results of the initial assessment", "content": "The following table shows the results of the assessment carried out on the initial idea of the connected Al-equipped doll described above:"}, {"title": "6.1.3 Mitigation measures and re-assessment", "content": "Following the iterative assessment, we can imagine that after this initial evaluation of the general idea, further measures are introduced to mitigate the potential risks found. At this stage, the potential stakeholders (users, parents associations, educational bodies, data protection authorities etc.) can make a valuable contribution to better defining the risks and how to tackle them.\n\nWhile the role of the stakeholders cannot be directly assessed in this analysis, we can assume that their participation would have shown great concern for risks relating to communications privacy and security. This conclusion is supported by the available documentation on the reactions of parents and supervisory authorities.195\n\nAfter the first assessment and given the evidence on stakeholders' requests, the following mitigation measures and by-design solutions could have been adopted with respect to the initial prototype.\n\nA) Data protection and the right to privacy\n\nFirstly, the product must comply with the data protection regulation of the countries in which it is distributed.196 Given the product's design, we cannot exclude the processing of personal data."}, {"title": "6.2 HRIA in large-scale multi-factor scenarios: the Sidewalk case", "content": "Large-scale projects using data-intensive applications are characterised by a variety of potentially impacted areas concerning individual and groups. This produces a more complex and multi-factor scenario which cannot be fully assessed by the mere aggregation of the results of HRIAS conducted for each component of data-intensive applications.\n\nAn example is provided by data-driven smart cites, where the overall effect of an integrated model including different layers affecting a variety of human activities means that the cumulative impact is greater than the sum of the impacts of each application.\n\nIn such cases, a HRIA for data-intensive systems also needs to consider the cumulative effect of data use and the AI strategies adopted, as already happens in HRIA practice with large-scale scenario cases. This is all the more important in the field of AI where large-scale projects often feature a unique or dominant technology partner who benefits from a general overview of all the different processing activities (\u2018platformisation\u2019221).\n\nThe Sidewalk project in Toronto is an example of this \u2018platformisation' effect and a case study in the consequent impacts on rights and freedoms. This concluded smart city project was widely debated222 and raised several human rights-related issues common to other data-intensive projects. It also highlights how the universal nature of the benchmark framework proposed makes the assessment model suited to deployment in various jurisdictions, beyond European borders.\n\nThe case concerned a requalification project for the Quayside, a large urban area on Toronto's waterfront largely owned by Toronto Waterfront Revitalization Corporation. Based on an agreement between the City of Toronto and Toronto Waterfront,223 in 2017, through a competitive"}, {"title": "7. Conclusions", "content": "The recent turn in the debate on AI regulation from ethics to law, the wide application of AI and the new challenges it poses in a variety of fields of human activities are urging legislators to find a paradigm of reference to assess the impacts of AI and to guide its development. This cannot only be done at a general level, on the basis of guiding principles and provisions, but the paradigm must be embedded into the development and deployment of each application.\n\nWith a view to providing a global approach in this field, human rights and fundamental freedoms can offer this reference paradigm for a truly human-centred AI. However, this growing interest in a human rights-focused approach needs to be turned into effective tools that can guide Al developers and key AI users, such as municipalities, governments, and private companies.\n\nTo bridge this gap between theoretical thinking on the potential role of human rights in addressing and mitigating AI-related risks, this work has suggested an empirical evidence-based approach to developing a human rights impact assessment (HRIA) model for AI.\n\nUsing the results of an in-depth analysis of jurisprudence in the field of data processing in Europe, we have outlined how human rights and freedoms already play an important role in the assessment of data-intensive applications. However, there is the lack of a formal methodology to facilitate an ex-ante approach based on a human-oriented design of product/service development. Moreover, this empirical analysis has better clarified the interplay between human rights and data processing in data-intensive systems, facilitating the development of an evidence-based model that is easier to implement as it is based on existing case law rather than on an abstract theoretical evaluation of the potential impact of AI.\n\nThe core of our research is the proposed HRIA model for AI, which has been developed in line with the existing practices in HRIA, but in a way that better responds to the specific nature of AI applications, in terms of scale, impacted rights and freedoms, prior assessment of production design, and assessment of risk levels, as required by several proposals on Al regulation.\n\nThe result is a tool that can be easily used by entities involved in AI development from the outset in the design of new Al solutions and can follow the product/service throughout its lifecycle, providing specific, measurable and comparable evidence on potential impacts, their probability, extension, and severity, and facilitating comparison between alternative design options and an iterative approach to AI design, based on risk assessment and mitigation.\n\nIn this sense, the proposed model is no longer just an assessment tool but a human rights management tool, providing clear evidence for a human rights-oriented development of Al products and services and their risk management.\n\nIn addition, a more transparent and easy-to-understand impact assessment model facilitates a participatory approach to Al development by potential stakeholders, giving them clear and structured information about possible options and the effects of changes in Al design.\n\nFinally, the proposed model can also be used by supervisory authorities and auditing bodies to monitor risk management in relation to the impact of data use on individual rights and freedoms.\n\nBased on these results, several concluding remarks can be drawn. The first general one is that conducting a HRIA should be seen not as a burden or a mere obligation, but as an opportunity. Given the nature of AI products/services and their features and scale, the proposed assessment model can significantly help companies and other entities to develop effective human-centric AI in challenging contexts."}]}