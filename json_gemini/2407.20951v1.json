{"title": "An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems.", "authors": ["A. Mantelero", "M.S. Esposito"], "abstract": "Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems.\n The focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use.\n Based on empirical analysis of this evidence, this work presents a methodology and an assessment model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment models are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds.\n The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.", "sections": [{"title": "1. Introduction", "content": "The debate that has characterised the last few years on data and Artificial Intelligence (AI) represents an interesting arena in which to consider the theoretical evolution of the future approach in addressing the challenges posed by AI to human rights. This debate has been marked"}, {"title": "2. The debate on AI regulation", "content": "While data processing regulation has been focused for decades on the law, including the interplay between data use and human rights, in recent years the debate on AI and the use of data-intensive systems has rapidly changed its trajectory, from law to ethics. This is evident not only in the"}, {"title": "3. Framing the ethical and the human rights-based approaches", "content": "From the outset, the debate on data ethics has been characterised by an improper overlap between ethics and law, in particular with regard to human rights. In this sense, it has been suggested that ethical challenges should be addressed by \u201cfostering the development and applications of data science while ensuring the respect of human rights and of the values shaping open, pluralistic and tolerant information societies\u201d. We can summarise this approach as \u2018ethics first': ethics plays a central role in technology regulation because it is the root of any regulatory approach, the pre- legal humus that is more important than ever where existing rules do not address or only partially address technological challenges.\n Another argument in favour of the central role of ethics comes out of what we might call the 'ethics after' approach. In the concrete application of human rights we necessarily have to"}, {"title": "4. Defining an operational approach to human rights assessment in AI", "content": "In considering the impact of AI on human rights, the dominant approach in many documents is mainly centred on listing the rights and freedoms potentially impacted rather than operationalising this potential impact and proposing assessment models.\n Moreover, case-specific assessment is more effective in terms of risk prevention and mitigation than using risk presumptions based on an abstract classification of \u201chigh-risk sectors and high- risk uses or purposes\u201d, where sectors, uses and purposes are very broad categories which include different kind of applications \u2013 some of them continuously evolving \u2013 with a variety of potential impacts on rights and freedoms that cannot be clustered ex ante on the basis of risk thresholds, but require a case-by-case impact assessment.\n Similarly, the adoption of a centralised technology assessment carried out by national ad hoc supervisory authorities can provide useful guidelines for technology development and can be used to fix red lines but must necessarily be complemented by a case-specific assessment of the impact of each application developed.\n For these reasons, a case specific impact assessment remains the main tool to ensure accountability and the safeguarding of individual and collective rights and freedoms. In this regard, a solution to the problem could easily be drawn from the human rights impact assessment models already adopted in several fields.\n However, these models are usually designed for different contexts than those of AI applications. The latter are not necessarily large-scale projects involving entire regions with multiple social impacts. Although there are important data-intensive projects in the field of smart cities, regional"}, {"title": "4.1 A methodological approach for an evidence-based model", "content": "Having defined the importance of a human rights-oriented approach in AI data processing, there remains the methodological question of how to define the assessment benchmark.\n Three different approaches are possible: (i) a top-down theoretical approach; (ii) an inferential approach, and (iii) a bottom-up empirical approach. The first was used in the analysis conducted by Raso et al. , in which various potentially affected rights are analysed on the basis of abstract scenarios grouped by sector-specific applications (risk assessments in criminal justice, credit scoring, healthcare diagnostics, online content moderation, recruitment and hiring systems, essay scoring in education).\n The second approach was adopted by Fjeld et al. , inferring values from existing ethics and right- based documents on AI regulation. This approach is close to the empirical approach, but is dominated by a quantitative dimension, focusing on the frequency of certain principles, and overlooking the heterogeneity of the documents. As the documents are often declarations by governmental and non-governmental bodies, they have the nature of guidelines and directives rather than concrete descriptions of the existing state-of-the-art: they are more focused on To-Be rather than on As-Is. This means that the prevalence of certain principles and values does not necessarily demonstrate a concrete and effective implementation of them.\n The third approach, used in this work, adopts an evidence-based methodology grounded on empirical analysis of cases decided by DPAs and guidelines provided by these authorities . More specifically, the idea is to move from the reasoning adopted by decision-makers in scrutinising data-centred applications and use this experience to better understand which rights and freedoms are relevant in practice."}, {"title": "5. A proposal for an HRIA model", "content": "The analysis described in Section 4 and the evidence provided by DPAs' decisions and practice show that the issues concerning the use of data-intensive applications are not circumscribed to the debated topic of bias and discrimination but have a broader impact on several human rights and freedoms. For this reason, a comprehensive HRIA model is needed.\n It is worth noting that traditional HRIAs are often territory-based considering the impact of business activities in a given local area and community, whereas in the case of AI applications this link with a territorial context may be less significant.\n There are two different scenarios: cases characterised by use of AI in territorial contexts with a high-impact on social dynamics (e.g. smart cities plans, regional smart mobility plans, predictive crime programmes) and those where Al solutions have a more limited impact as they are embedded in globally distributed products/services (e.g. AI virtual assistants, autonomous cars, recruiting AI-based software, etc.) and do not focus on a given socio-territorial community. While"}, {"title": "5.1 Planning and scoping", "content": "The first stage deals with definition of the HRIA target, identifying the main features of the product/service and the context in which it will be placed, in line with the context-dependent nature of the HRIA. Three are the main areas to consider at this stage:\n \u2022 description and analysis of the type of product/service, including data flows and data processing purposes\n \u2022 the human rights context (contextualisation on the basis of local jurisprudence and laws)\n \u2022 identification of relevant stakeholders.\n The table below provides a non-exhaustive list of potential questions for HRIA planning and scoping. The extent and content of these questions will depend on the specific nature of the product/service and the scale and complexity of its development and deployment. This list is therefore likely to be further supplemented with project-specific questions."}, {"title": "5.2 Data collection and analysis", "content": "While the first stage is mainly desk research, the second focuses on gathering relevant empirical evidence to assess the product/service's impact on human rights and freedoms. In traditional HRIA this usually involves extensive fieldwork. But in the case of AI applications, data collection and analysis is restricted to large-scale projects such as those developed in the context of smart cities, where different services are developed and integrated. For the remaining cases, given the limited and targeted nature of each application, data collection is largely related to the product/service's features and feedback from stakeholders.\n Based on the information gathered in the previous stage (description and analysis of the type of product/service, human rights context, controls in place, and stakeholder engagement), we can proceed to a contextual assessment of the impact of data use on human rights, to understand which rights and freedoms may be affected, how this may occur, and which potential mitigation measures may be taken.\n Since in most cases the assessment is not based on measurable variables, the impact on rights and freedoms is necessarily the result of expert evaluation, where expert opinion relies on knowledge of case law, the literature, and the legal framework. This means that it is not possible to provide precise measurement of the expected impacts but only an assessment in terms of range of risk (i.e. low, medium, high, or very high).\n The benchmark for this assessment is the evidence-based analysis using the methodology described in Section 4 and the results. Thus, different rights and freedoms may be relevant depending on the specific nature of the given application.\n Examination of any potentially adverse impact should begin with a general overview followed by a more granular analysis where the impact is envisaged. In line with normal risk assessment"}, {"title": "6. Testing the HRIA", "content": "The next two sub-sections examine two possible applications of the proposed model, with two different scales of data use. The first case, an Internet-connected doll equipped with AI, shows how the impact of AI is not limited to adverse effects on discrimination, but has a wider range of consequences (privacy and data protection, education, freedom of thought and diversity, etc.), given the innovative nature of the application and its interaction with humans.\n This highlights the way in which AI does not merely concern data and data quality but more broadly the transformation of human-machine interaction by data-intensive systems. This is even more evident in the case of the smart cities, where the interaction is replicated on large scale affecting a whole variety of human behaviours by individuals, groups and communities.\n The first case study (an AI-powered doll) shows in detail how the HRIA methodology can be applied in a real-life scenario. In the second case (a smart city project) we do not repeat the exercise for all the various data-intensive components, because a full HRIA would require"}, {"title": "6.1 Testing HRIA on a small scale: the Hello Barbie case", "content": "Hello Barbie was an interactive doll produced by Mattel for the English-speaking market, equipped with speech recognition systems and AI-based learning features, operating as an IoT device. The doll was able to interact with users but did not interact with other IoT devices.\n The design goal was to provide a two-way conversation between the doll and the children playing with it, including capabilities that make the doll able to learn from this interaction, e.g. tailoring responses to the child's play history and remembering past conversations to suggest new games and topics. The doll is no longer marketed by Mattel due to several concerns about system and device security.\n This section discusses the hypothetical case, imagining how the proposed assessment model could have been used by manufactures and developers and the results that might have been achieved."}, {"title": "6.1.1 Planning and scoping", "content": "Starting with the questions listed in Tab. 1 above and information on the case examined, the planning and scoping phase would summarise the key product characteristics as follows:\n a) A connected toy with four main features: (i) programmed with more than 8,000 lines of dialogue hosted in the cloud, enabling the doll to talk with the user about \u201cfriends, school, dreams and fashion\u201d; (ii) speech recognition technology activated by a push- and-hold button on the doll's belt buckle; (iii) equipped with a microphone, speaker and two tri-colour LEOs embedded in the doll's necklace, which light up when the device is active; (iv) a Wi-Fi connection to provide for two-way conversation.\n b) The target-user is an English-speaking child (minor). Theoretically the product could be marketed worldwide in many countries, but the language barrier represents a limitation.\n c) The right-holders can be divided into three categories: direct users (minors), supervisory users (parents, who have partial remote control over the doll and the doll/user interaction) and third parties (e.g. friends of the user or re-users of the doll).\n d) Regarding data processing, the doll collects and stores voice-recording tracks based on dialogues between the doll and the user; this information may include personal data and sensitive information.\n e) The main purpose of the data processing and Al is to create human-robot interaction (HRI) by using machine learning (ML) to build on the dialogue between the doll and its young users. There are also additional purposes: (i) educational; (ii) parental control and"}, {"title": "6.1.2 Initial risk analysis and assessment", "content": "The basic idea of the toy is an interactive doll, equipped with speech recognition and learning features, operating as an IoT device. The main component is a human-robot voice interaction feature based on AI and enabled by Internet connection and cloud services.\n The rights potentially impacted are data protection and privacy, freedom of thought and diversity, and psychological and physical safety and health.\n 6.1.2.1 Data protection and the right to privacy\n While these are two distinct rights, for the purpose of this case study we considered them together. Given the main product features, the impact analysis is based on following questions:\n \u2022 Does the device collect personal information? If yes, what kind of data is collected, and what are the main features of data processing? Can the data be shared with other entities/persons?\n \u2022 Can the connected toy intrude into the users' private sphere?\n \u2022 Can the connected toy be used for monitoring and surveillance purposes? If yes, is this monitoring continuous or can the user stop it?\n \u2022 Do users belong to vulnerable categories (e.g. minors, elderly people, parents, etc.)?\n \u2022 Are third parties involved in the data processing?\n \u2022 Are transborder data flows part of the processing operations?\n Taking into account the product's nature, features and settings (i.e. companion toy, dialogue recording, personal information collection, potential data sharing by parents) the likelihood of prejudice can be considered very high (Tab. 3). The extent and largely unsupervised nature of the dialogue between the doll and the user, as well as the extent of data collection and retention make the probability high (Tab. 1). In addition, given its default features and settings, the exposure is very high (Tab. 2) since all the doll's users are potentially exposed to this risk.\n Regarding risk severity, the gravity of the prejudice (Tab. 4) is high, given the subjects involved (young children and minors), the processing of personal data in several main areas, including sensitive information, and the extent of data collection. In addition, unexpected findings may emerge in the dialogue between the user and the doll, as the harmless topics prevalent in the AI- processed sentences can lead young users to provide personal and sensitive information."}, {"title": "6.1.3 Mitigation measures and re-assessment", "content": "Following the iterative assessment, we can imagine that after this initial evaluation of the general idea, further measures are introduced to mitigate the potential risks found. At this stage, the potential stakeholders (users, parents associations, educational bodies, data protection authorities etc.) can make a valuable contribution to better defining the risks and how to tackle them.\n While the role of the stakeholders cannot be directly assessed in this analysis, we can assume that their participation would have shown great concern for risks relating to communications privacy and security. This conclusion is supported by the available documentation on the reactions of parents and supervisory authorities.\n After the first assessment and given the evidence on stakeholders' requests, the following mitigation measures and by-design solutions could have been adopted with respect to the initial prototype.\n A) Data protection and the right to privacy\n Firstly, the product must comply with the data protection regulation of the countries in which it is distributed. Given the product's design, we cannot exclude the processing of personal data."}, {"title": "7. Conclusions", "content": "The recent turn in the debate on AI regulation from ethics to law, the wide application of AI and the new challenges it poses in a variety of fields of human activities are urging legislators to find a paradigm of reference to assess the impacts of AI and to guide its development. This cannot only be done at a general level, on the basis of guiding principles and provisions, but the paradigm must be embedded into the development and deployment of each application.\n With a view to providing a global approach in this field, human rights and fundamental freedoms can offer this reference paradigm for a truly human-centred AI. However, this growing interest in a human rights-focused approach needs to be turned into effective tools that can guide Al developers and key AI users, such as municipalities, governments, and private companies.\n To bridge this gap between theoretical thinking on the potential role of human rights in addressing and mitigating AI-related risks, this work has suggested an empirical evidence-based approach to developing a human rights impact assessment (HRIA) model for AI.\n Using the results of an in-depth analysis of jurisprudence in the field of data processing in Europe, we have outlined how human rights and freedoms already play an important role in the assessment of data-intensive applications. However, there is the lack of a formal methodology to facilitate an ex-ante approach based on a human-oriented design of product/service development. Moreover, this empirical analysis has better clarified the interplay between human rights and data processing in data-intensive systems, facilitating the development of an evidence-based model that is easier to implement as it is based on existing case law rather than on an abstract theoretical evaluation of the potential impact of AI.\n The core of our research is the proposed HRIA model for AI, which has been developed in line with the existing practices in HRIA, but in a way that better responds to the specific nature of AI applications, in terms of scale, impacted rights and freedoms, prior assessment of production design, and assessment of risk levels, as required by several proposals on Al regulation.\n The result is a tool that can be easily used by entities involved in AI development from the outset in the design of new Al solutions and can follow the product/service throughout its lifecycle, providing specific, measurable and comparable evidence on potential impacts, their probability, extension, and severity, and facilitating comparison between alternative design options and an iterative approach to AI design, based on risk assessment and mitigation.\n In this sense, the proposed model is no longer just an assessment tool but a human rights management tool, providing clear evidence for a human rights-oriented development of Al products and services and their risk management.\n In addition, a more transparent and easy-to-understand impact assessment model facilitates a participatory approach to Al development by potential stakeholders, giving them clear and structured information about possible options and the effects of changes in Al design.\n Finally, the proposed model can also be used by supervisory authorities and auditing bodies to monitor risk management in relation to the impact of data use on individual rights and freedoms.\n Based on these results, several concluding remarks can be drawn. The first general one is that conducting a HRIA should be seen not as a burden or a mere obligation, but as an opportunity. Given the nature of AI products/services and their features and scale, the proposed assessment model can significantly help companies and other entities to develop effective human-centric AI in challenging contexts."}]}