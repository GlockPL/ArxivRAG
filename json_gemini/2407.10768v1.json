{"title": "MSEGRNN:ENHANCED SEGRNN MODEL WITH MAMBA FOR\nLONG-TERM TIME SERIES FORECASTING", "authors": ["Gaoxiang Zhao", "Xiaoqiang Wang"], "abstract": "The field of long-term time series forecasting demands handling extensive look-back windows and\nlong-range prediction steps, posing significant challenges for RNN-based methodologies. Among\nthese, SegRNN, a robust RNN-driven model, has gained considerable attention in LTSF analysis for\nachieving state-of-the-art results while maintaining a remarkably streamlined architecture. Concur-\nrently, the Mamba structure has demonstrated its advantages in small to medium-sized models due to\nits capability for information selection. This study introduces a variant of SegRNN that preprocesses\ninformation using a fine-tuned single-layer Mamba structure. Additionally, it incorporates implicit\nsegmentation and residual structures into the model's encoding section to further reduce the inherent\ndata iterative cycles of RNN architectures and implicitly integrate inter-channel correlations. This\nvariant, named MSegRNN, utilizes the Mamba structure to select useful information, resulting in a\ntransformed sequence. The linear-strategy-adapted derivative retains the superior memory efficiency\nof the original SegRNN while demonstrating enhanced performance. Empirical evaluations on\nreal-world LTSF datasets demonstrate the superior performance of our model, thereby contributing to\nthe advancement of LTSF methodologies.", "sections": [{"title": "Introduction", "content": "Long-term series forecasting is the process of predicting the future values of a sequence over an extended period.\nThis task primarily aims to forecast long-term trends, which hold significant importance in domains such as climate\nprediction [Aghelpour et al., 2019], decision support [Nooruldeen et al., 2022], and policy formulation [Zhang et al.,\n2022]. Long-term sequence data often exhibit higher levels of uncertainty, which can result in reduced prediction\naccuracy. Additionally, longer forecasting horizons require models to incorporate a broader historical context for\naccurate predictions, thereby increasing the complexity of modeling. Recent advancesments in long-term sequence\nanalysis have shifted towards deep learning methods, highlighting the crucial need to develop diverse and effective\ntime-series methodologies.\nLong-term time series analysis and forecasting comprise two critical components: (1) trend identification and analysis,\nwhich involves recognizing long-term directional changes in the data, This component is crucial for understanding the\nbehavior of time series, enabling the comprehension of past patterns and the prediction of potential future developments.\n(2) Uncertainty and noise management: Due to the extended time range of long-term time series data, they often exhibit\nincreased levels of uncertainty and noise. Effectively managing and mitigating these uncertainties and noises is vital for\nimproving the accuracy of predictions.\nIn the context of long-term time series analysis, it is essential to capture the sequential relationships between data.\nCommonly adopted feature representation methods in time series analysis include Convolutional Neural Networks"}, {"title": "Problem statement", "content": "The analysis of long-term time series involves addressing the challenge of predicting future values of multivariate time\nseries based on historical data. Typically, the problem of time series analysis can be formulated as follows:\n\\(Y = F(X) + \\xi\\) (1)\nIn which \\(X \\in R^{L\\times C}\\) represents multi-variable time sequences with historical context, F represents the trained neural\nnetwork structure, \\(Y \\in R^{H\\times C}\\) represents the future time sequences to be predicted, L indicates the historical lookback\nwindow, C indicates the number of variables, and H indicates the time step to be predicted. Long-term time series\nanalysis aims to expand the potential of predicting time steps, which poses significant challenges for training neural\nnetworks."}, {"title": "Related Work", "content": "A significant amount of work has been dedicated to advancing the development of deep learning in time series analysis.\nIn this section, we introduce literature relevant to our work. These methods can be broadly divided into the following\nfour categories:"}, {"title": "Transformer Models", "content": "Transformer models, originally designed for sequence-to-sequence tasks, have achieved significant achievements\naccross various domains. The core of applying Transformer to long-time series tasks is to use the attention mechanism\nto capture the long-term dependencies [Hao and Cao, 2020]. Extensive research has been devoted to advancing the\napplication of the Transformer architecture in long-time series analysis. Autoformer [Wu et al., 2021] decomposes the\nsequence into basic blocks within the model, thus gradually decomposing complex time series and enhancing learning\nability for complex patterns. Informer [Zhou et al., 2021] constructs a ProbSparse self-attention mechanism, adopting\nself-attention distillation and a generative decoder to address the high time complexity, large memory consumption, and\nencoder architecture limitations faced in long-time series analysis. Crossformer [Zhang and Yan, 2022] embeds time\nseries into two-dimensional vector through the Dimension-segment-wise structure, effectively captures dependencies\nacross time and dimensions through the Two-stage-attention structure. The segmentation technique used in Crossformer\nmethod is considered beneficial for long time series foresting."}, {"title": "MLP Models", "content": "Extensive research has been dedicated to the application of MLP methods in long-term time series forecasting. Dlinear\n[Zeng et al., 2023] decomposes time series through the application of moving average kernels and remainder components,\nyielding two feature variables. These variables are then mapped to the prediction results via linear layers. The Nlinear\n[Zeng et al., 2023] method adopts a regularization strategy in time series forecasting tasks to enhance performance.\nSpecifically, it subtracts the last value before inputting to the model, mapping through a linear layer, then adding it back\nfor the final prediction. The TiDE [Nie et al., 2022] method introduces a residual structure within the linear layer to\nencode past time series and covariates, subsequently decoding the encoded time series with future covariates to obtain\nprediction results. Long-term time series analysis methods based on MLP have demonstrated outstanding predictive\naccuracy while maintaining low time-space complexity, thus advancing the development of MLP in long-term time\nseries forecasting."}, {"title": "CNN Models", "content": "The core of time series analysis based on CNN architectures lies in utilizing convolutional kernels to extract temporal\nand channel features. The Disjoint-CNN [Foumani et al., 2021] approach, while maintaining computational efficiency,\nenhances prediction accuracy by decomposing one-dimensional convolutional kernels into disjoint temporal and\nspatial components. Nonetheless, CNN-based methodologies encounter challenges as the forecasting horizon expands.\nMICN [Wang et al., 2022] aims to fully exploit the latent information in time series by designing a multi-scale hybrid\ndecomposition module to decompose the complex patterns of input time series. Each module within this design\nleverages subsampling convolution and equidistant convolution to respectively extract local characteristics and global\ncorrelations. TimesNet [Wu et al., 2022] transforms one-dimensional time series into a set of two-dimensional tensors\nbased on multiple periodicities. It further introduces the TimesBlock to adaptively uncover these multi-periodic\npatterns and effectively extracts intricate temporal dynamics from the transformed tensors through parameter-efficient\ninitializations. These methodologies have attained remarkable results in the domain of long-term time series forecasting."}, {"title": "RNN Models", "content": "The RNN architecture was initially designed for processing sequence tasks and excel in short-term time series prediction.\nHowever, RNN-based methods face error accumulation issues in long-term time series tasks. SegRNN [Lin et al.,\n2023] aims to mitigate the accumulation of errors in the recurrent structure while preserving sequence information.\nSpecifically, in the encoding stage, the SegRNN method segments the data before encoding. In the decoding stage, it\nemploys a PMF approach for parallel decoding, significantly enhancing prediction accuracy and the spatiotemporal\nefficiency.\nNonetheless, the SegRNN method is not without its limitations: (1) SegRNN couldn't effectivly preporcess the data and\nextract the inforamtion, which may lead to a rapid accumulation of noise. (2)SegRNN adopts fixed segmentation of the\noriginal time series, which may result in partial information loss due to its rigidity. (3) SegRNN aims to minimize the"}, {"title": "Method", "content": "To address the challenges faced by current methods, we incorporate mamba structure, implicit segmentation and\nresidual networks into the SegRNN framework. This approach (1) first extracts the original data first to contrain noise,\n(2)segments the original data through linear mapping to reduce information loss caused by fixed segmentation, and (3)\nintroduces residual structures at the encoding layer to mitigate information loss within the recurrent structure.\nWe first discuss the overall sturcture that we used in Section 4.1, then we discuss the comparison between explicit\nsegmentation and implicit segmentation in Section 4.2, followed by a discussion on how to introduce residual structures\ninto the model during the encoding process of time series data in Section 4.3."}, {"title": "Model Structure", "content": "The Mamba architecture achieves long-term sequence memory and model selection capabilities by setting the SSM\nstructure parameters as trainable parameters dependent on the sequence content. We have adapted the Mamba\narchitecture to address time series preprocessing problems. The basic structure of our model is shown in Figure 1:"}, {"title": "Implicit Segmentation", "content": "The explicit segmentation approach employed by SegRNN pre-processes the data such that a given multivariate time\nseries, denoted as \\(X^{(i)} \\in R^{L\\times C}\\), where L represents the look-back length and C represents the number of channels in\nmultivariate time series. Upon selecting a segment length w, the time series can be transformed into segmented time\nseries \\(X^{(i)} \\in R^{n\\times w\\times C}\\), where n represents the number of segments:\n\\(n = \\frac{L}{w}\\) (2)\nSubsequently, the segmented model projects through a linear layer:\n\\(X_{prj}^{(i)} = W_{prj}X^{(i)}\\) (3)\nHowever, this segmentation strategy has certain limitations. Once the segment length is chosen, the cut-off points\nare also determined. This may disrupt the continuity of information in time series analysis, thus limiting the model's\ncapabilities. The implicit segmentation method aims to break this limitation. Specifically, given a time series analysis\nvector \\(X^{(i)} \\in R^{L\\times C}\\), we first expand its last dimension and then swap the first and second dimensions to obtain\n\\(X^{(i)} \\in R^{C\\times L\\times 1}\\) for subsequent processing. Then we utlize a linear layer to transform the last dimesntion of \\(X^{(i)}\\) to\nget \\(X^{(i)} \\in R^{C\\times L\\times n}\\) to represent the segmented matrix, subsequently, we interchange the second and third dimension,\nstill denoted as \\(X^{(i)}\\). Applying linear layer to the last dimension of \\(X^{(i)}\\) yields \\(X^{(i)} \\in R^{C\\times n\\times d}\\), which is consistent\nin dimension with the vector obtained via SegRNN methodology.\nNotably, our proposed method does not explicitly define the segmentation length. Instead, it achieves data segmentation\nand dimensionality transformation through dual linear projections. The incorporation of linear fully connected layers\nfacilitates a more continuous transformation of information. It is importanct to emphasize that this implicit segmentation\nstrategy is not confined to a singe approach but rather represents a general and straightforward technique for information\nenhancement."}, {"title": "Residual Structure for SegRNN", "content": "SegRNN aims to minimize the number of iterations in the RNN while preserving sequential information. Extensive\nexperiments show that increasing the number of iterations negatively impacts information retention. SegRNN uses\ndata segmentation to reduce the data's flow through the RNN structure. Specifically, for the vector obtained through\nsegmentation in SegRNN, \\(X^{(i)} \\in R^{C\\times n\\times d}\\), we transform it in the RNN structure with a hidden layer dimension d\nusing the following formula:\n\\(h_0 = 0\\)\n\\(h_1 = f(h_0, X_{[:, 1, :]}^{(i)}; W)\\)\n\\(h_2 = f (h_1, X_{[:, 2, :]}^{(i)}; W)\\)\n\\(:\\)\n\\(h_n = f(h_{n-1}, X_{[:, n, :]}^{(i)}; W)\\) (4)\nwhere \\(h_t \\in R^{1\\times C\\times d}\\) represents the hidden state at step t, \\(X_{[:, t, :]}^{(i)}\\) represents the input vector at step t, W denotes\nthe RNN's weight parameters, and f is the RNN's activation function.\nUsing segmentation techniques, SegRNN reduces the number of iterations from L to n. However, the number of\niterations still increases with the forecasting time steps. This unavoidably limits the model's ability due to the loss of\ninformation during iterations. Therefore, we introduce a residual structure in the model. For the segmented vector\n\\(X^{(i)} \\in R^{C\\times L\\times n}\\), we introduce a linear mapping from \\(X^{(i)}\\) to \\(h_n\\). Specifically, we first merge the last two dimensions\nand extend the first dimension, obtaining the three-dimensional matrix \\(X'^{(i)} \\in \\[][R^{1\\times C\\times (L\\times n)}\\). Then, we transform the\nlast dimension through a linear layer to get \\(h_n \\in R^{1\\times C\\times d}\\). Finally, we add \\(h_n\\) to the corresponding position in \\(h_n\\),\nobtaining the final output vector from the encoder.\nIntroducing the residual structure allows some information to bypass the recurrent structure and be directly mapped to\nthe encoder output. This approach minimizes information loss within the recurrent structure, providing a beneficial\nenhancement for time series data."}, {"title": "Experiments", "content": "Our experiments are structured as follows. In Section 5.1, we introduce the datasets used for long-term time series\nanalysis. Subsequently, in Section 5.2, we provide a detailed description of the experimental setup and baseline models\nfor comparison. Section 5.3 presents and analyzes the performance metrics. In Section 5.4, we conducted ablation\nstudies to investigate the effectiveness of the model. Finally in Section 5.5, we investigated the efficiency of the model,\nparticularly focusing on its memory consumption to demonstrate the lightweight characteristic of our model. All\nexperiments in this section were conducted on a single NVIDIA 4090 GPU."}, {"title": "Datasets", "content": "The evaluation is conducted across seven real-world datasets spanning various domains, including power transformer\ntemperature, weather, electricity load, and traffic, with channel ranging from 7 to 321 and frequencies from every 10\nminutes to hourly recordings. Detailed dataset information is provided in Table 1."}, {"title": "Experimental setup and Baselines", "content": "The unified configuration of the model is substantially aligned with the SegRNN approach. The look-back window is\nset to be 96, implicit Segmentation length is set to be 24 or 48 based on the size of datasets. A single layer of Mamba\nis utilized for preprocessing the data and a single GRU layer is used for sequence processing. The dimensionality of\nhidden layer with GPU structure is set to be 512, and the training epochs are set to be 30. Dropout rate, learning rate,\nand batch size vary with the data and the scale of the model.\nAs baselines, we have selected state-of-the-art and representative models in the long-term time series forecasting\ndomain, comprising the following categories: (i) RNNs: SegRNN; (ii) MLPs: Dlinear; (iii) CNNs: TimesNet; (iv)\nTransformers: iTransformer, Crossformer, PatchTST."}, {"title": "Main Result", "content": "We selected two evaluation metrics, MSE and MAE, with prediction steps of 96, 192, 336, and 720 time steps. The\nresulting forecasts are presented in Table 2. Note that all data, except for our method, originate from official code\nrepositories and original papers.\nOur method achieved top positions in 37 out of 48 metrics, while the SegRNN method achieved the top position in\n7 metrics. The iTransformer, Crossformer, and PatchTST methods each achieved top position in 6, 1 and 1 metrics,\nrespectively. This demonstrates the powerful capabilities of our model."}, {"title": "Ablation study", "content": "We further conducted an ablation study to verify the effectiveness of incorporating the Mamba structure, implicit\nsegmentation, and residual structures into our model. Using the weather dataset as an example, we performed\nexperiments excluding the Mamba structure and implicit segmentation structure respectivly. For clarity, in the table\nbelow, \"M\" denotes preprocessing using the Mamba structure, and \"LR\" represents the addition of linear segmentation\nand residual structure. The results are shown in Table 3:\nThe ablation study reveals that the model incorporating the Mamba structure, along with implicit segmentation and\nresidual structures performs the best overall. This is followed by the model that includes only implicit segmentation and\nresidual structures. Interestingly, merely adding the Mamba structure does not significantly enhance model performance.\nThis may be due to a synergistic effect between the two structures. Consequently, the ablation study confirms the\nsuperiority of our proposed structure."}, {"title": "Model efficiency", "content": "Since our Mamba structure has not undergone hardware-level optimization, this section focuses on comparing the\nmodel's memory usage to verify its lightweight characteristic. The lightweight characteristic is primarily due to the use\nof a small number of parameters in our Mamba structure experiments. Using the Weather dataset as an example, the\nmemory usage comparison with batch size 64 is shown in Figure 2.\nAs shown in the figure, the model's exhibits only marginal increases in memory usage compared to SegRNN, affirming\nits lightweight characteristics."}, {"title": "Conclusion", "content": "In this work, we present the MSegRNN method: a network that incorporates Mamba structure, implicit segmentation\nand residual structure into the SegRNN framework. These enhancements mitigate the information loss caused by fixed\nsegmentation in time series while enhance the information from the recurrent structure of the model. It is noteworthy\nthat our proposed implicit segmentation method is not limited to a specific long-term time series forecasting method.\nWe further conducted experiments on six real-world benchmark datasets for long time series. The results demonstrate\nthat our model yields significant imporvements in some real-world scenarios. To validate the efficiency of our method\nin terms of space complexity, we compared the memory consumption of MSegRNN against SegRNN on datasets of\nvarying scales. The results suggest that our method maintains high space efficiency, providing a solid foundation for the\nmodel's application.\nFuture work will explore how these structures enhance the effect of the model and identify the particular characteristics\nof time series where the MSegRNN method excels with corresponding empirical research."}]}