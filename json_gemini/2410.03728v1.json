{"title": "EXPLORING QUIC DYNAMICS: A LARGE-SCALE DATASET FOR ENCRYPTED TRAFFIC ANALYSIS", "authors": ["Barak Gahtan", "Robert J. Shahla", "Alex M. Bronstein", "Reuven Cohen"], "abstract": "QUIC, a new and increasingly used transport protocol, addresses and resolves the limitations of TCP by offering improved security, performance, and features such as stream multiplexing and connection migration. These features, however, also present challenges for network operators who need to monitor and analyze web traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising over 100,000 QUIC traces from more than 44,000 websites (URLs), collected over a four-month period. These traces provide the foundation for generating more than seven million images, with configurable parameters of window length, pixel resolution, normalization, and labels. These images enable an observer looking at the interactions between a client and a server to analyze and gain insights about QUIC encrypted connections. To illustrate the dataset's potential, we offer a use-case example of an observer estimating the number of HTTP/3 responses/requests pairs in a given QUIC, which can reveal server behavior, client-server interactions, and the load imposed by an observed connection. We formulate the problem as a discrete regression problem, train a machine learning (ML) model for it, and then evaluate it using the proposed dataset on an example use case 1.", "sections": [{"title": "1 Introduction", "content": "The rapid adoption of Quick UDP Internet Connections (QUIC) [13] as a transport protocol offers significant enhance- ments over traditional TCP, including improved security, performance, and features such as stream multiplexing and connection migration. These advancements, however, also introduce challenges for network monitoring and analysis, particularly in the context of encrypted traffic. Traditional methods of traffic analysis are less effective with QUIC due to its encryption, necessitating innovative approaches to follow and managing network performance as well as its effects on latency, error rates, and congestion control. Consequently, the development of a comprehensive and diverse dataset composed of QUIC traffic to various web servers is essential for thorough research. This paper introduces a large-scale dataset for QUIC traffic, representing a major step forward in understanding QUIC dynamics, especially given the limitations of traditional traffic monitoring techniques in the face of QUIC's encryption.\nThis paper proposes a dataset that considers the case of an observer listening to the channel between the QUIC client and server. The observer sees data packets being sent in both directions. The proposed dataset contains more than 100,000 QUIC traces collected from more than 44,000 websites during a four-month period, from various vantage points, using a page request workload. The proposed dataset offers significant value in both the networking and ML domains.\nTo demonstrate the dataset's potential, we present a use case of estimating the number of HTTP/3 [3] objects a QUIC connection carries. This information can be useful for various applications. The most important application is HTTP/3 load balancing. A load balancer can successfully balance the load it assigns to different machines if it is able to estimate the load imposed by each connection [22]. This is difficult with HTTP/3, because the load balancer does not know how"}, {"title": "2 Dataset Description", "content": null}, {"title": "2.1 Problem settings", "content": "We consider an observer who can see the QUIC encrypted packets transmitted from the client to the server and vice versa. For each packet, the observer knows its direction, length, and the observed time. With this information in hand, the QUIC traces can be converted into representative colored images, which are then suitable for training ML models. To convert the captured QUIC traces into time-series data, the sliding window technique [11] is used. This technique requires two parameters: the window length and the overlap between consecutive windows. Both parameters are configurable."}, {"title": "2.2 Trace collection and image generation", "content": "The process starts with HTTP/3 [3] GET requests that are generated to various web servers that support HTTP/3, each hosting multiple websites. Requests are issued for up to 26,000 different websites per web server. Headless Chrome [6] is used in incognito mode with the application cache disabled, and the websites are requested sequentially. displays the exact numbers for each web server and more detailed statistics broken down per web server for each class are provided in Appendix A.3. The generated network traffic traces are captured using Tshark [20] in packet capture (PCAP) format. These traces include only QUIC packets and cover the duration of the website request. For each PCAP file the corresponding SSL keys are stored to be used later to decrypt the traffic. The SSL keys are also provided in the dataset's materials.\nOnce we retain the time-series captured traces, the image datasets generation process can start for these traces. By Converting network traffic data\u2014such as packet arrival times, packet sizes, packet density, and packet directions\u2014into images the data is transformed into a format that is more compatible with DL models.\nThe use of images enhances pattern recognition abilities [9, 12, 23, 26, 27]. Images enable the capture of complex interactions between features like packet sizes and arrival times within a two-dimensional space (server-to-client and client-to-server). This spatial representation allows DL models to identify intricate patterns that might be missed by traditional statistical or time-series analysis methods. For instance, correlations between packet bursts and response delays may become more discernible when visualized as variations in pixel intensity within an image.\nshows an example of the construction steps for an image with a window length of 0.3 seconds from a trace. During step (a), some of the trace statistics are collected: the time when the observer sees this packet, the packet's length, and the packet's direction. For a 0.3-second window, each bin contains 9.375 milliseconds. Step (b) shows histograms with M = 32 time bins for the considered window. The upper one is for the packets sent by the server and the lower one is for packets sent by the client. The horizontal axis represents the time bins and the vertical axis represents the number of packets received during each bin. For example, in the 8-th time bin (boxed in orange), the server sent 10 packets and the client sent 19 packets. Step (c) shows the image constructed for the considered example. The image represents the packet length statistics and the number of packets.\nshows an example of the constructed image. The image is constructed on an M \u00d7 N equispaced grid. The horizontal dimension represents different time window locations, while the vertical dimension represents different packet lengths. Thus, each packet is binned into one of the M \u00d7 N bins according to its length and time. In the resulting image, the pixel at location (i, j) represents the normalized number of packets whose length falls within the j-th bin received during the temporal span of the i-th time bin. The pixel's RGB values represent the normalized number of packets (i.e., density) sent from the server to the client (red) and from the client to the server (green). The blue channel is unused. The time interval spanned by the i-th bin is [i\u2206t, (i + 1)\u2206t), where \u2206t = T/M and T denotes the window length. In our experiments, we used T = 0.1 and T = 0.3 seconds. To be counted in length bin j, the length of a packet should be in the range of [j\u2206l, (j + 1)\u2206l] with Al = L/N and L = 1,500 bytes denoting the maximum transmission unit (MTU). Histogram counts are normalized per channel window-wise using min-max normalization [21], $x_{nrm} = (x - X_{min})/(X_{max} - X_{min})$, where x and $x_{nrm}$ are the original and normalized packet counts, respectively, and $X_{min}$ and $X_{max}$ are the minimum and maximum values of the packet count for the specific direction in the considered window, respectively. The normalized value is multiplied by 255 to fit an 8-bit image format. If there is no traffic for a specific window, all pixels will contain the value zero. Note that the shortest QUIC packet is longer than what is represented by the first length bin. Therefore, the first row of the image grid consistently exhibits pixels with a value of zero.\nshows different densities for each channel. For example, during time bin i = 7, different shades of green are displayed. This indicates that the client sent packets of five different lengths, which fall into bins j = 2, 6, 12, 27, and 28. The five pixels are purely green, indicating that all the packets observed during bin i = 7 were sent by the client. The brightness of a pixel increases as its value approaches 255. Pixel (7, 12) is the brightest across the whole"}, {"title": "2.3 Dataset creation", "content": "For the creation of the image dataset in our example use case, several key parameters were defined: the sliding window length, normalization method, and pixel resolution. Specifically, we generated two image datasets with the following configurations: (1) two different sliding window lengths of T = 0.1 and T = 0.3 seconds; (2) images sized at M = N = 32, selected as a balance between resolution and computational cost. Using finer bins increased both the training and inference time with minimal accuracy improvement, while coarser bins negatively impacted model performance; and (3) normalization applied per window rather than per trace. For the baseline example, we utilized a 90% overlap between consecutive windows during training, with no overlap during evaluation. The resulting labeled image dataset originates from over 100,000 traces collected from more than 44,000 websites, generating over seven million images.\nLabeling the images: Each image in this dataset is labeled with the number of observed HTTP/3 responses; namely, the number of responses that have started to arrive within every time window. To this end, the SSL keys are used to decrypt the packets in a trace and reveal the packets' payloads. The HTTP/3 frames then are analyzed and HTTP/3 HEADERS frames are identified. Similarly, instead of labeling the images with the number of responses, the number of requests can be used as a label instead."}, {"title": "2.4 Training and test sets", "content": "The dataset can be split into two different settings: when the web servers are known to the observer and when they are not. In the former case, training and evaluation phases are done exclusively on the QUIC traces pertaining to the web"}, {"title": "2.5 Discussion:", "content": "Selecting the window length: The window length determines the temporal span each image represents, which directly impacts the data granularity. Shorter window lengths, such as 0.1 seconds, capture fine-grained temporal details of the network traffic, allowing for detailed analysis of short-term interactions between the client and server. This high granularity level is very useful for identifying subtle variations and transient behaviors in the traffic. Using shorter windows, however, also means generating a larger number of images per trace, leading to increased computational requirements. Conversely, longer window lengths, such as 0.3 seconds or more, offer a more aggregated view of the traffic, encapsulating longer sequences of packet interactions within each image. This approach reduces the number of images generated, thereby decreasing computational demands. Longer windows are beneficial for capturing broader trends and interactions over extended periods, which can be advantageous for understanding overall traffic patterns and behaviors. The trade-off between short and long window lengths is a potential loss of fine-grained details, which"}, {"title": "3 Estimating the Number of HTTP/3 Responses in a QUIC Connection", "content": "Estimating the number of HTTP/3 responses in a QUIC connection can assist a load balancer in making more informed decisions. By monitoring connections and estimating the number of responses within each connection, the load balancer can determine if a connection is considered heavy and adjust its decision on the selected server accordingly [22].\nTo evaluate the use of the proposed dataset, we formulate the problem of estimating the number of responses in a QUIC connection as a discrete regression problem. It is not a classic classification task, because the misclassification errors depend on the distance between the categories. For example, consider an image with 17 responses. Estimating this number as 16 is better than estimating it as 15 or 19. It is also not a standard regression task, as the target categories are discrete. To address this issue, we developed a dedicated loss function coupled with data augmentation that considers: (1) the imbalanced dataset, which is derived from real-world QUIC traces and (2) rewarding the model for correctly predicting classes that are closer to the actual label than those that are farther away. Appendix A.1 explains the discrete regression loss function in more detail.\nWe present a quantitative evaluation example of the proposed framework when the web servers are known to the observer, on a subset of the dataset, which was not present during training. A set of models were trained and evaluated exclusively on the QUIC traces pertaining to the web servers assumed at inference time. Two different models were trained with windows of T = 0.1 and T = 0.3 seconds. Classes with labels non-superior to 20 constitute 90% of the traces in the T = 0.3-second window dataset and 95% of the traces in the T = 0.1-second window dataset. Due to their scarceness, classes above 20 were excluded from the training and test sets.\nTo mitigate class imbalance, we developed a dedicated loss function and implemented a data augmentation technique. A grid search was performed to find the optimal values of \u03b1, \u03b2, and y of the loss function. The values considered were a \u2208 {0.3,0.5,0.7}, \u03b2\u2208 {0.4,0.6}, and \u03b3\u2208 {1,2,3}. The optimal combination was chosen based on the lowest validation loss seen during the training process. The optimal values for T = 0.3 seconds were found to be \u03b1 = 0.7, \u03b2 = 0.4, and y = 2, while for T = 0.1 seconds, y = 3 produced the best results with the same values of a and \u03b2. The training was performed with a batch size of 64 images using the Adam optimizer [15] with the ReduceLROnPlateau learning rate scheduler with a 30% reduction in the learning rate, during the training phase. To reduce the risk of overfitting, an early stopping technique was used, with a patience parameter of six epochs. The performance is measured on an AMD Ryzen Threadripper PRO 3955WX 16C CPU 3.9G running at 64MB cache, 64GB of CRUCIAL CT8G4DFRA32A RAM clocked at 3200MHz and an NVIDIA GTX-4090 GPU.\nThe results presented are for estimating the total number of HTTP/3 responses in a complete trace. The images were fed sequentially through the trained models whose predictions were summed and compared to the sums of the trace's true label."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we introduced VisQUIC, a labeled dataset of QUIC traffic traces designed to facilitate advanced network behavior analysis, and ML tasks on real world data. By transforming QUIC connection data into sequences of RGB images, we leveraged DL models to effectively predict and analyze network traffic. We detailed the key decisions made during the dataset creation process, such as the selection of window length and image size, and emphasized the trade-offs between data granularity and computational efficiency. Our experimental results highlighted the effectiveness of the proposed approach, achieving accurate predictions of HTTP/3 responses within QUIC connections using image-based models. With normalized images per window, our models attained up to 97% CAP accuracy in scenarios where the web server was known. Additionally, we estimated the total number of HTTP/3 responses associated with each QUIC connection across more than 12,000 traces with a high accuracy of 92.6%. These findings demonstrated the power of image-based data representation for capturing complex network traffic patterns and improving network performance analysis. This method not only enhanced the ability to monitor and manage encrypted traffic but also paved the way for future research in network security and optimization. By offering a detailed and high-resolution perspective of QUIC traffic, the VisQUIC dataset served as a valuable resource for developing scalable and robust network analysis tools, driving innovation in the field.\nLimitations: The dataset contains traces that are a result of web page requests done sequentially, one at a time. We use a page request workload because the number of web servers streaming video over QUIC is limited, leading to a dataset lacking diversity from the server perspective. We note that video streaming traffic patterns differ significantly from page requests, as they are heavily influenced by the streaming algorithms used by servers and not only the network conditions. Future work should study various bandwidths, using not only Chrome [8], but other browsers that support QUIC."}, {"title": "A Appendices", "content": null}, {"title": "A.1 Dedicated Loss Function", "content": "For showing a use of our proposed dataset, we formulated the problem of estimating the number of HTTP/3 responses in QUIC connection as a discrete regression problem. The proposed loss function is,\n$L = a FL + (1 \u2212 a) ((\u03b2 ORL + (1 \u2212 \u03b2)DBL)$.\nIt comprises an aggregate of three terms: (1) a focused loss (FL) term, intended to alleviate class imbalance by minimizing the relative loss for well-classified cases while emphasizing difficult-to-classify ones; (2) a distance-based loss (DBL) term penalizing the model according to the predicted class's distance from the true label; and (3) an ordinal regression loss (ORL) term that introduces higher penalties for misclassifications that disrupt the natural ordinal sequence of the dataset, where lower class values occur more frequently.\nThe FL term [17] builds on the weighted cross-entropy loss [7] by adding a focusing parameter, y, which adjusts the influence of each sample on the training process based on the classification confidence. This parameter, y, modifies the loss function by scaling the loss associated with each sample by (1 \u2013 pt), where pt is the predicted probability of the true class y. This scaling reduces the loss from easy examples (where pt is high), thereby increasing it for hard, misclassified examples, focusing training efforts on samples where improvement is needed. Accordingly, the term is:\n$FL(x, y) = E_{(x,y)} [-w(y) \u00b7 (1 \u2013 \u0177_y(x))^\u03b3 \u00b7 y \\log\\\u0177_y(x)]$,\nwhere x denotes the input sample, y is the one-hot encoded ground truth label, y(x) represents the model's output of class probabilities, \u0177y(x) denotes the predicted probability of the true class y, and w(y) is a weight inversely proportional to the class frequency of y in the training dataset. By assigning a higher weight to less frequent classes, the model places more emphasis on accurately classifying these classes during training. It is an effective strategy for dealing with class imbalance [2, 25, 17]. FL thus minimizes the relative loss for well-classified examples, while emphasizing difficult-to-classify ones.\nThe DBL term [28]\n$DBL = :E_{(x,y)} \\sum_i Yi(x)\u00b7 |i - y|$\nwith y denoting the ground truth class, is essentially a discrete regression loss that penalizes the model's output according to the predicted class's distance from the true label. The distance is computed as the absolute difference between the class indices and the target class.\nFinally, the ORL term [14, 10] is given by\n$ORL = E_{(x,y)} [-y \\log\\sigma(y) \u2013 (1 \u2013 y) \\log\\sigma(-y)]$,\nwith o denoting the sigmoid function saturating the input between 0 and 1. ORL uses a binary cross-entropy loss function, which compares the activation of each output neuron to a target that shows if the true class is greater than or equal to each class index, thus helping the model determine the order of the classes. Both DBL and ORL consider the relations between classes; they do so in different ways: DBL penalizes predictions based on the numerical distance, while ORL makes explicit use of the classes' order. It focuses on preserving the correct order among predictions rather than the numerical distance between them.\nThe parameters \u03b1, \u03b2, and y in the aggregated loss are used to balance the contributions of these three components to the combined loss. a is a parameter that controls the balance between the FL term and the ORL and DBL combination. A higher value of a gives more weight to the FL term, while a lower value gives more weight to the ORL and DBL combination. \u1e9e is a parameter that controls the balance between the ORL and DBL terms. A higher value of \u1e9e gives more weight to the ORL term, while a lower value gives more weight to the DBL term. y is a parameter used inside the FL component to adjust the focusing effect of the FL term. A higher y increases the effect of the focusing mechanism."}, {"title": "A.2 Data Augmentation:", "content": "Since the images are generated from QUIC traces that are formatted into a 32 \u00d7 32 pixel grid, each pixel corresponds to a unique feature of network traffic over a specific period. Any disruption in the temporal dependencies present in each image, such as through non-order-preserving modifications, may result in the loss of critical information, reducing the ML model's ability to estimate correctly. Thus, data augmentation is only applied to the minority classes (classes whose values are between 10 and 20), incorporating a minimal noise level [19]. We used noise with the standard deviation of \u03c3 = 2.55 corresponding to 1% of the pixel value, ensuring that the added noise does not drastically alter the image appearance or disrupt the temporal dependencies. The noise serves, however, to imitate minor variations, increasing the model robustness and generalization capabilities."}, {"title": "A.3 Extended Statistics", "content": null}, {"title": "A.4 Motivation", "content": "This dataset was created to study RTT estimation in the context of QUIC traffic over HTTP/3. To try to estimate RTT estimation, one needs to obtain information about response or request and to gather real QUIC traffic from various web servers, which currently is not researched a lot. QUIC is an encrypted protocol developed by Google, which is ran under Chrome browser."}, {"title": "A.5 Composition", "content": "Each instance is an image associated with a label of the number of responses within that image. Each image in this dataset is labeled with the number of observed HTTP/3 responses; namely, the number of responses that have started to arrive within every time window. To this end, the HTTP/3 frames are analyzed and HTTP/3 HEADERS frames are identified. Similarly, instead of labeling the images with the number of responses, the number of requests can be used as a label instead.\nTables 1, 2 and 3 contain statistics for all images with a label of 20 or less. The files also contain images with high-class labels, and from the captured traces, additional images and labels can be generated. We publish the whole traces dataset, which contains 100, 664 traces in PCAP format, and outline the traces counts in Table 1. The traces were collected from various locations. The label for each image presented in the dataset is the number of responses observed during that time window of the image. Further studies can generate more images and more labels.\nWhen using the traces or images in the dataset for training purposes, one should check if high-class labels of images are present on specific web servers before splitting the traces into sets of out-of-sample web servers. Some web servers lack images with high class values."}, {"title": "A.6 Collection Process", "content": "The process starts with HTTP/3 [3] GET requests that are generated to various web servers that support HTTP/3, each hosting multiple websites. Requests are issued for up to 3, 176 different websites per web server. Headless Chrome [6] is used in incognito mode with the application cache disabled, and the websites are requested sequentially. The generated network traffic traces are captured using Tshark [20] in packet capture (PCAP) format. These traces include only QUIC packets and cover the duration of the website request. Once we retain the time series captured traces, the image datasets generation process can start for these traces. The SSL keys are stored for each trace in a separate file (and are provided in the dataset). These keys are use to decrypt the relevant QUIC packets."}, {"title": "A.7 Preprocessing/cleaning/labeling", "content": "Images whose class labels were above 20 responses were not part of the training or evaluation for estimating the number of responses in a QUIC connection, due to their rarity in the data. Besides that, the raw data contains all of the data, unfiltered. Preprocessing was done by filtering out packets that are not QUIC packets. Furthermore, a large number of images were identical, and all of the duplicate images were removed. Before the filtering there was 21, 100, 925 images, and after there was 5,040, 459 images. We upload the full dataset including the duplicates. The raw data (captured traces) are saved and will also be available along with the images, upon the paper's acceptance using a link."}, {"title": "A.8 Other Uses", "content": "The dataset can be used for various communication tasks involving QUIC protocol, such as fingerprinting websites, estimating the number of requests in each connection, estimating the load on specific web servers, predicting server-client interactions over QUIC sessions, estimating RTT between a server and a client etc."}, {"title": "A.9 Maintenance", "content": "Both authors are maintaining the dataset on Github and on relevant links. The dataset can be updated and more labels can be added, for example, requests for each image in addition to responses. Moreover, the dataset can evolve to contain more images using different window lengths. Currently, the dataset contains two window lengths of 0.1 and 0.3 seconds. Errors may be submitted via the bugtracker on Github. More extensive augmentations may be accepted at the authors' discretion."}]}