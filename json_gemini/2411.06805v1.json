{"title": "Boosting the Potential of Large Language Models with an Intelligent Information Assistant", "authors": ["Yujia Zhou", "Zheng Liu", "Zhicheng Dou"], "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually incorrect in-\nformation, known as \"hallucination\". Initial retrieval-augmented generation (RAG)\nmethods like the \"Retrieve-Read\" framework was inadequate for complex reason-\ning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning\n(SFT) methods improved performance but required frequent retraining and risked\naltering foundational LLM capabilities. To cope with these challenges, we propose\nAssistant-based Retrieval-Augmented Generation (ASSISTRAG), integrating an\nintelligent information assistant within LLMs. This assistant manages memory and\nknowledge through tool usage, action execution, memory building, and plan speci-\nfication. Using a two-phase training approach-Curriculum Assistant Learning and\nReinforced Preference Optimization\u2014ASSISTRAG enhances information retrieval\nand decision-making. Experiments show ASSISTRAG significantly outperforms\nbenchmarks, especially benefiting less advanced LLMs, by providing superior\nreasoning capabilities and accurate responses.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) has significantly advanced the field of natural\nlanguage processing, demonstrating an impressive ability to mimic human-like language patterns [1].\nHowever, despite their extensive knowledge acquired during training, LLMs can occasionally generate\nfactually incorrect information, a phenomenon referred to as \u201challucination\" [2, 3]. To address this,\nthe integration of retrieval systems with LLMs has been suggested, allowing these models to tap into\nexternal databases to generate more reliable responses [4].\nInitially, retrieval-augmented generation (RAG) relied on a simple \"Retrieve-Read\" framework [5],\nwhich was adequate for basic question-answering but insufficient for complex, multi-step reasoning\ntasks. As language models advanced, various prompt-based RAG strategies emerged [6, 7], incorpo-\nrating pre-retrieval and post-retrieval prompts to refine the process. However, these strategies heavily\nrelied on the foundational capabilities of the language models. Consequently, the focus shifted to\nSupervised Fine-Tuning (SFT)-based RAG methods [8], which involve fine-tuning language models\nspecifically for RAG tasks to enhance their performance.\nWhile SFT-based methods have improved the quality of generated responses, they face two limitations\nthat hinder their practical application. Firstly, these fine-tuned models are not easily adaptable to"}, {"title": "Related Work", "content": "RAG represents a significant advancement in the domain of LLMs, particularly for tasks demanding\nextensive knowledge. This paradigm begins with a retrieval step, where the LLM accesses an external\ndatabase to gather relevant information before addressing queries. Traditionally, RAG follows a"}, {"title": "Methodology", "content": "In this section, we first define the task of RAG and then introduce our proposed framework, As-\nSISTRAG. ASSISTRAG enhances the capabilities of LLMs through the support of an intelligent\ninformation assistant. With abilities to use tools, execute actions, build memory, and plan, Assis-\nTRAG can provide precise memory and knowledge management services for LLMs."}, {"title": "Task Definition", "content": "Given a question q and a collection of documents $D = {d_i}_{|D|}$, the main LLM aims to generate\nan answer y based on both the question and the relevant documents. This can be formalized as\n$y = LLM_{main}([D_q, q])$, where $D_q$ represents the set of documents retrieved for the query q, and\n$[,]$ denotes the concatenation of the retrieved documents with the query. Expanding this concept,\nASSISTRAG employs an intelligent information assistant, $LLM_{Assist}$, to enhance the main LLM's\nresponses by providing relevant information, formalized as $y = LLM_{main}([LLM_{Assist}(q), q])$. In the\nfollowing sections, we will detail the capabilities of the ASSISTRAG framework, along with its\ntraining and inference procedures."}, {"title": "ASSISTRAG Overview", "content": "By incorporating an intelligent information assistant, ASSISTRAG aims to boost the potential of\nLLMs in handling complex reasoning tasks. As illustrated in Figure 2, this framework consists of\ntwo main components: a frozen main LLM tasked with generating answers based on the information\nprovided, and a trainable assistant LLM responsible for information management. This assistant\nLLM is designed with two tasks: Memory Management involves storing interactions with the main\nLLM and retrieving relevant past memories to assist in addressing similar questions. Knowledge\nManagement encompasses retrieving relevant information from external databases and processing it\nto support the main LLM in formulating responses to new questions.\nTo effectively accomplish these tasks, we have endowed the assistant with four key capabilities:\n\u2022 Tool Usage: Retrieving relevant information from internal memory and external knowledge bases.\n\u2022 Action Execution: Reasoning, analyzing information need, and extracting knowledge.\n\u2022 Memory Building: Recording essential knowledge and reasoning patterns from past interactions."}, {"title": "Memory Management", "content": "Effective memory management is crucial for enhancing the main LLM's performance by storing\nand retrieving historical interactions. This functionality comprises two key processes: capturing\nnew insights and retrieving previously stored information. This stage activates the following three\ncapabilities of AssistRAG:\n\u2022 Action I: Note Taking. This action $F_{NT}$ records critical information and the reasoning patterns\nbehind each historical interaction. Given the historical interactions of the main LLM, which include\nquestion q, reference r, and answer y, the assistant is tasked with memorizing the key reasoning\nprocess behind the answer into the memory slot $m_q$: $m_q \\leftarrow F_{NT}(q,r,y)$. The accumulation\nof memory slots for all prior questions forms the assistant's memory M, which is utilized for\nsubsequent memory retrieval.\n\u2022 Tool I: Memory Retriever. Given the question q and the assistant's memory M, the memory\nretriever retrieves historically relevant memories, represented as: $M_q \\leftarrow R_{memory} (q, M)$.\n\u2022 Plan I: Assessing the Usefulness of Retrieved Memory. If the question is entirely new, the\nretrieved memories may not only be unhelpful but also negatively impact the main LLM's response.\nTherefore, we implement this plan to determine whether the retrieved memory slots should be\nprovided to the main LLM. Using a prompt, the assistant evaluates whether the retrieved memories\nare beneficial for answering the current question. Only if the answer is affirmative will the retrieved\nmemories be supplied to the main LLM."}, {"title": "Knowledge Management", "content": "Effective knowledge management is essential for an intelligent information assistant, involving the\nefficient gathering of necessary knowledge to support the main LLM. This process includes analyzing\nthe information needs of the main LLM, retrieving relevant knowledge, and integrating it. This\nprocess involves the following four capabilities of AssistRAG:\n\u2022 Action II: Question Decomposition. This action $F_{QD}$ aims to break down the current question into\nmultiple sub-queries to facilitate the retrieval of knowledge across various aspects: $Q' \\leftarrow F_{QD}(q)$,\nwhere Q' represents a series of sub-queries derived from the question q.\n\u2022 Tool II: Knowledge Retriever. Utilizing a batch of sub-queries Q', the knowledge retriever sources\nrelevant documents from external knowledge bases D, denoted as: $D_{Q'} \\leftarrow R_{knowledge} (Q', D)$.\n\u2022 Action III: Knowledge Extraction. This action $F_{KE}$ involves extracting essential knowledge\nfrom a large number of retrieved documents. Given the question q and the retrieved documents\n$D_{Q'}$, the assistant is responsible for extracting the relevant knowledge $K_q$ from the search results:\n$K_q \\leftarrow F_{KE}(q, D_{Q'})$.\n\u2022 Plan II: Evaluating the Relevance of Extracted Knowledge. To ensure the accuracy and\nrelevance of the information provided to the main LLM, this plan determines whether the extracted\nknowledge should be included in the response generation process. Similarly, we prompt the\nassistant to assess whether the extracted knowledge is relevant to the current question."}, {"title": "ASSISTRAG Training", "content": "The training objectives of ASSISTRAG focus on two main goals: (1) enhancing the effectiveness\nof each action within the RAG process, and (2) ensuring that its outputs align with the main LLM's\nrequirements. To achieve these two goals, as depicted in Figure 3, we implement curriculum-based\nassistant learning and reinforced preference optimization to optimize the training of ASSISTRAG.\nSeveral studies have demonstrated that GPT-4 can achieve human-like annotation accuracy [32].\nBased on this consideration, we leverage it to collect training data for the three actions. The supervised\ntraining samples for each specific action are cataloged as $C_{QD}$, $C_{KE}$, and $C_{NT}$, preparing these for the\nassistant's subsequent training phase."}, {"title": "Curriculum Assistant Learning", "content": "Motivation. The tasks of question decomposition, knowledge extraction, and note-taking are\ninterconnected, each contributing towards navigating the reasoning path from a question to its answer."}, {"title": "Reinforced Preference Optimization", "content": "Motivation. Although ASSISTRAG effectively handles RAG tasks after assistant learning, its output\nmay sometimes not fully meet the downstream LLM's specific needs. To enhance integration, we\nimplement reinforced preference optimization, a technique that adjusts the assistant's output based\non feedback from the main LLM, ensuring tailored assistance that better meets its requirements.\nTraining Objective. To optimize the assistant for better alignment with the main LLM, we adopt\nDirect Preference Optimization (DPO) [33]. This approach involves generating two sets of references,\none from externally retrieved knowledge and the other generated by the assistant itself. The main\nLLM evaluates these sets, with a preference determined by comparing the F1 scores of its responses\nagainst correct answers. For reinforced preference optimization, we leverage the DPO algorithm's\noptimization objective, utilizing paired preference data $D_{dpo}$:\n$\\mathbb{E}_{(x,y_1,y_2)\\sim D_{dpo}} [\\log \\sigma (\\beta (\\log \\pi_{\\theta}(y_1|x) - \\log \\pi_{ref}(y_1|x)) - (\\log \\pi_{\\theta}(y_2|x) - \\log \\pi_{ref}(y_2|x)))] $ \nwhere $\\pi_{\\theta}(x,y) = \\frac{\\exp{h_{\\theta}(y|x)}}{\\sum_{y'} \\exp{h_{\\theta}(y'|x)}}$ is the reward implicitly defined by the language model $\\pi_{\\theta}$ and the\nreference model $\\pi_{ref}$. This reinforced training stage enhances the assistant's capability to deliver\nassistance that aligns more closely with the main LLM's preferences, enhancing overall efficacy."}, {"title": "ASSISTRAG Inference", "content": "Upon completing its training phase, ASSISTRAG initiates its inference process through three steps:\nInformation Retrieval and Integration. At this initial stage, ASSISTRAG first activates Action II\nto understand the main LLM's information needs. It then uses Tool I and Tool II to retrieve relevant\ninformation from internal memory and external knowledge bases, respectively. Subsequently, it\ninvokes Action III to extract essential knowledge from the retrieved documents.\nDecision Making. In this stage, ASSISTRAG decides whether to provide the retrieved memories and\nextracted knowledge to the main LLM. It activates Plan I and Plan II to evaluate the relevance and\nusefulness of the retrieved memories and knowledge for the current question. If the assistant deems\nthem helpful, they are supplied to the main LLM to aid in answer generation.\nAnswer Generation and Memory Updating. In the final phase, we prompt the main LLM to\ngenerate an answer based on the question, its internal knowledge, and the information provided by the\nassistant. Following this, ASSISTRAG activates Action I to utilize its note-taking feature, capturing\ncrucial reasoning steps from the interaction and incorporating them into its memory. This ensures the\nassistant's knowledge base remains up-to-date."}, {"title": "Experimental Setup", "content": "In this study, we evaluate the effectiveness of our proposed method through experiments on three\nintricate question-answering datasets: HotpotQA [34], 2WikiMultiHopQA [35], and Bamboogle [6].\nThese datasets, all derived from Wikipedia documents, provide a uniform corpus and retrieval"}, {"title": "Results and Analysis", "content": "The main results are presented in Table 1. Several key findings can be observed as follows:\nComparison among Different Reasoning Types. Applying our ASSISTRAG framework to Chat-\nGPT demonstrates a significant performance advantage over other models across all datasets. Specifi-"}, {"title": "Analysis", "content": "ASSISTRAG integrates\nmemory and knowledge management to sup-\nport the main LLM, encompassing three actions:\nnote-taking, question decomposition, and knowl-\nedge extraction. To evaluate their contribution,\nwe conduct ablation studies by removing each\naction or freezing the parameters of the assis-\ntant. Additionally, we assess the effects of not\nimplementing planning (w/o. Planning), cur-\nriculum learning (w/o. Curriculum), and rein-\nforced preference optimization (w/o. DPO) to\nexplore there contribution to the F1 score. Ta-\nble 2 illustrates that removing or freezing any of\nthe ASSISTRAG's actions results in decreased\nperformance, underscoring the value of the as-\nsistant learning in the RAG context. Notably,\nmaintaining these actions in a frozen state still\noutperforms completely removing them, highlighting their critical role in the RAG process. Concern-\ning training strategies, the absence of planning, curriculum learning, and preference optimization\nslightly diminishes performance, indicating that a structured progression from simple to complex\ntasks and aligning with downstream LLM preferences contribute to the assistant providing more\naccurate information to the downstream LLM, thereby enhancing the accuracy of LLM responses."}, {"title": "ASSISTRAG Training", "content": "The training objectives of ASSISTRAG focus on two main goals: (1) enhancing the effectiveness\nof each action within the RAG process, and (2) ensuring that its outputs align with the main LLM's\nrequirements. To achieve these two goals, as depicted in Figure 3, we implement curriculum-based\nassistant learning and reinforced preference optimization to optimize the training of ASSISTRAG.\nSeveral studies have demonstrated that GPT-4 can achieve human-like annotation accuracy [32].\nBased on this consideration, we leverage it to collect training data for the three actions. The supervised\ntraining samples for each specific action are cataloged as $C_{QD}$, $C_{KE}$, and $C_{NT}$, preparing these for the\nassistant's subsequent training phase."}, {"title": "Curriculum Assistant Learning", "content": "Motivation. The tasks of question decomposition, knowledge extraction, and note-taking are\ninterconnected, each contributing towards navigating the reasoning path from a question to its answer."}, {"title": "Reinforced Preference Optimization", "content": "Motivation. Although ASSISTRAG effectively handles RAG tasks after assistant learning, its output\nmay sometimes not fully meet the downstream LLM's specific needs. To enhance integration, we\nimplement reinforced preference optimization, a technique that adjusts the assistant's output based\non feedback from the main LLM, ensuring tailored assistance that better meets its requirements.\nTraining Objective. To optimize the assistant for better alignment with the main LLM, we adopt\nDirect Preference Optimization (DPO) [33]. This approach involves generating two sets of references,\none from externally retrieved knowledge and the other generated by the assistant itself. The main\nLLM evaluates these sets, with a preference determined by comparing the F1 scores of its responses\nagainst correct answers. For reinforced preference optimization, we leverage the DPO algorithm's\noptimization objective, utilizing paired preference data $D_{dpo}$:\n$\\mathbb{E}_{(x,y_1,y_2)\\sim D_{dpo}} [\\log \\sigma (\\beta (\\log \\pi_{\\theta}(y_1|x) - \\log \\pi_{ref}(y_1|x)) - (\\log \\pi_{\\theta}(y_2|x) - \\log \\pi_{ref}(y_2|x)))] $ \nwhere $\\pi_{\\theta}(x,y) = \\frac{\\exp{h_{\\theta}(y|x)}}{\\sum_{y'} \\exp{h_{\\theta}(y'|x)}}$ is the reward implicitly defined by the language model $\\pi_{\\theta}$ and the\nreference model $\\pi_{ref}$. This reinforced training stage enhances the assistant's capability to deliver\nassistance that aligns more closely with the main LLM's preferences, enhancing overall efficacy."}, {"title": "ASSISTRAG Inference", "content": "Upon completing its training phase, ASSISTRAG initiates its inference process through three steps:\nInformation Retrieval and Integration. At this initial stage, ASSISTRAG first activates Action II\nto understand the main LLM's information needs. It then uses Tool I and Tool II to retrieve relevant\ninformation from internal memory and external knowledge bases, respectively. Subsequently, it\ninvokes Action III to extract essential knowledge from the retrieved documents.\nDecision Making. In this stage, ASSISTRAG decides whether to provide the retrieved memories and\nextracted knowledge to the main LLM. It activates Plan I and Plan II to evaluate the relevance and\nusefulness of the retrieved memories and knowledge for the current question. If the assistant deems\nthem helpful, they are supplied to the main LLM to aid in answer generation.\nAnswer Generation and Memory Updating. In the final phase, we prompt the main LLM to\ngenerate an answer based on the question, its internal knowledge, and the information provided by the\nassistant. Following this, ASSISTRAG activates Action I to utilize its note-taking feature, capturing\ncrucial reasoning steps from the interaction and incorporating them into its memory. This ensures the\nassistant's knowledge base remains up-to-date."}, {"title": "Conclusion", "content": "In this study, we introduce ASSISTRAG to augment LLMs with an intelligent information assistant,\nsignificantly improving their ability to tackle tasks requiring complex reasoning. By implement-\ning a two-stage training methodology that integrates curriculum assistant learning with reinforced\npreference optimization, we enhance the assistant's skills in memory and knowledge management.\nExperiments demonstrate that AssisTRAG surpasses existing baselines with a notable margin.\nLooking ahead, we plan to further expand the assistant's skills to include long-text processing [40]\nand personalized support [41], thereby providing more effective assistance to the main LLM."}, {"title": "Implementation Details", "content": "To achieve the effective functioning of ASSISTRAG, we meticulously designed and executed the\ntraining and inference phases, ensuring optimal use of computational resources and robust fine-tuning\nprocesses."}, {"title": "Training Settings", "content": "The training of the assistant LLM involved a two-phase approach: Assistant Learning and Preference\nOptimization."}, {"title": "Assistant Learning Phase:", "content": "\u2022 Dataset Creation: We created a dataset comprising 50,000 training samples based on instruction-\nfollowing input-output pairs. These pairs were categorized across three distinct task types: question\ndecomposition, note-taking, and knowledge extraction.\n\u2022 Model Architecture: The assistant LLM is based on ChatGLM3-6B [37], a state-of-the-art\nlanguage model known for its robust performance in various NLP tasks.\n\u2022 Training Procedure: The assistant LLM was fully fine-tuned across all parameters over 2 epochs,\nwith a batch size of 32 and a peak learning rate of 2e-5. This phase focused on enhancing the\nmodel's ability to decompose complex queries, take notes, and extract relevant knowledge."}, {"title": "Preference Optimization Phase:", "content": "\u2022 Optimization Technique: We employed a DPO (Distributed Preference Optimization) trainer to\nrefine the assistant's feedback mechanisms.\n\u2022 Fine-tuning with LoRA: Low-Rank Adaptation (LoRA) was utilized for fine-tuning, which helps\nin adjusting a subset of model parameters efficiently, reducing the computational load.\n\u2022 Learning Rate and Duration: The learning rate was set to le-5, with the training duration\nextending to 2 epochs.\nTraining Resources: The entire training process was conducted using 8 A800 GPUs, providing\nsubstantial computational power to handle the intensive training tasks efficiently."}, {"title": "Inference Settings", "content": "During the inference phase, the ASSISTRAG system was fine-tuned to operate seamlessly with the\nmain LLM, ensuring effective retrieval and processing of information."}, {"title": "Data Annotation and Samples", "content": "The training data annotation process was critical in ensuring the quality and relevance of the dataset.\nWe utilized GPT4-turbo for annotating the dataset, ensuring high accuracy and consistency across the\ntraining samples. The dataset included approximately 50k training samples, labeled to cover a wide\nrange of instruction-following tasks."}, {"title": "Error Analysis", "content": "We have conducted a comprehensive error analysis on the performance of AssistRAG. To facilitate\nthis analysis, we selected 50 erroneous examples from the HotpotQA dataset and calculated the\nproportion of each error type:\n1. Insufficient Knowledge Retrieval: Instances where the retrieved knowledge does not\ncontain the answer.\n2. Knowledge Extraction Errors: Cases where the answer is present in the retrieved knowl-\nedge, but the assistant fails to extract this information.\n3. Answer Reasoning Mistakes: Situations where the assistant extracts the correct information\nbut the main LLM produces an incorrect answer.\n4. Other: Including errors such as non-exact match answers.\nFrom Table 8, our findings indicate that more than half of the errors stem from insufficient knowl-\nedge retrieval, which is likely linked to the performance of the retriever and the manner in which\nquestions are reformulated. Additionally, a significant portion of errors are due to reasoning mistakes,\nhighlighting the importance of the main LLM's reasoning capabilities. Given that HotpotQA involves\nmulti-hop question-answering tasks, these findings underscore the high demands placed on reasoning\nabilities."}, {"title": "Limitations", "content": "Despite the advancements offered by ASSISTRAG, several limitations warrant consideration. Firstly,\nrelying on an intelligent information assistant introduces additional computational complexity and\nlatency. The two-phase training approach and its operation during inference require substantial\ncomputational resources, which may limit the practical application of ASSISTRAG in environments\nwith restricted processing capabilities or where real-time responses are critical."}]}