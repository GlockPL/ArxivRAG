{"title": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors", "authors": ["Jakub Macina", "Nico Daheim", "Ido Hakimi", "Manu Kapur", "Iryna Gurevych", "Mrinmaya Sachan"], "abstract": "Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MATH-TUTORBENCH, an open-source benchmark for holistic tutoring model evaluation. MATHTUTORBENCH contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MATHTUTORBENCH and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) present an opportunity to transform education by offering ubiquitous access to individualized tutoring (Jurenka et al., 2024). While these models excel at generating correct answers (Wei et al., 2022; Achiam et al., 2023), experienced teachers help students think for themselves and do not just give away answers to make learning effortless (Sharma et al., 2024). Teaching involves a combination of skills including subject expertise, the ability to diagnose and correct student mistakes, and the application of sound pedagogical techniques. For example, teachers need to know when to withhold answers from students, use Socratic questioning (Anghileri, 2006), or how to engage them cognitively in problem solving (Chi and Wylie, 2014; Kapur, 2016). Therefore, a crucial element of building LLM tutors is their evaluation; it is critical to understand whether their guidance is helpful to prevent harm, and to guide progress in future model development.\nYet, current evaluation practices do not meet these criteria. On the one hand, automatic metrics usually evaluate tutoring models by measuring the word overlap between a ground-truth response and a generated response (Tack et al., 2023), or focus exclusively on question-answering performance (Chevalier et al., 2024). This is fast but arguably fails to capture the intricacies of tutoring. Although human evaluation might be a way to capture these nuances by defining suitable criteria to capture them (Tack and Piech, 2022; Maurya et al., 2024), it is expensive. Importantly, it can only create a snapshot of current performance and cannot"}, {"title": "2 Related Work", "content": "be used to evaluate or compare to future models. In this work, we fill this gap by releasing MATHTUTORBENCH, a collection of datasets and metrics to holistically evaluate dialog tutoring models for math tutoring. Teaching is a complex and multifaceted task that extends beyond subject mastery (Nye et al., 2014; Tack et al., 2023; Wang et al., 2024b). Therefore, MATHTUTORBENCH is divided into three categories: math expertise which evaluates the subject-matter expertise of the tutor, student understanding which evaluates the tutor's ability to verify, locate and correct student solutions, and teacher response generation which evaluates the scaffolding abilities of the tutor. Math expertise and student understanding are evaluated based on standard metrics, and we propose a novel metric for evaluating teacher response generation. In particular, we train a small and quick-to-run reward model by contrasting effective and less effective tutor utterances in terms of structured scaffolding guidance with questions and hints instead of giving away the answer (Anghileri, 2006). The reward model is then used to score tutor model generations. We show that this metric is reliable by showing that it can distinguish utterances from expert teachers from those stemming from novice teachers (Wang et al., 2024b) with high accuracy.\nWe evaluate various open- and closed-weight state-of-the-art LLMs and specialized tutoring models on MATHTUTORBENCH. Our results show that there is a trade-off between subject expertise and pedagogical abilities that is dependent on the degree of specialization of a tutoring LLM. Specializing an LLM for pedagogy comes at the cost of solving ability and, conversely, a high solving accuracy often means that the LLM lacks pedagogy. Still, more specialized tutoring models tend to retain their teaching abilities even further into a dialog with a student, while general models quickly become worse. With this, our work contributes to accelerating the development of tutoring LLMs by providing a holistic benchmark that can be evaluated quickly and fairly using automatic metrics. We release our code and data publicly to promote open research on tutoring LLMs."}, {"title": "2.1 LLM-Based Dialog Tutoring", "content": "A good tutor should scaffold student learning in a structured way rather than just provide correct answers. Current approaches to dialog tutoring using LLMs try to achieve this by differnent means: prompt-based elicitation of pedagogical behavior, finetuning models on pedagogical conversations, and alignment with pedagogical preferences.\nFirst, most existing works use LLMs with a carefully chosen prompt which enumerates desired pedagogical behavior. Bridge (Wang et al., 2024b) analyzes the teacher behavior and proposes to structure the prompt into a sequence of decisions, similar to real teachers, first to infer the error type and then to determine the pedagogical intent. Other works mostly directly write extensive prompts (Sonkar et al., 2023; Kargupta et al., 2024). However, defining such prompts is tedious, sensitive to small changes and difficult to test (Jurenka et al., 2024).\nSecond, several approaches finetune models on real or mostly synthetically generated data. SocraticLM (Liu et al., 2024) uses a GPT-4 judge to evaluate the quality of teacher guidance using correctness and Socratic principles. A similar approach is to role-model teacher and student conversations based on textbook data (Chevalier et al., 2024; Wang et al., 2024a). MathDial (Macina et al., 2023a) is one of the few works that use teachers' utterances when interacting with students to finetune models. However, it is expensive to collect such data on a larger scale. Therefore, LearnLM (Team et al., 2024) uses an empirically validated mixture of synthetic and teacher-created datasets. However, for capturing high-quality tutoring, teacher-created data is essential and therefore upweighted in their final data mix. Finally, LLMs can be aligned for pedagogical preferences during post-training (Team et al., 2024), because these are usually tacit. However, no datasets are openly available or they rely on larger models such as GPT-4 as a judge which limits its generalizability.\nOur benchmark contributes an important missing ingredient in the development of LLM-based tutors  the ability to quickly evaluate and compare models on key pedagogical aspects."}, {"title": "2.2 Automatic & Human Evaluation", "content": "Several works rely on automatic NLG metrics such as BLEU (Papineni et al., 2002) or BERTScore (Zhang et al., 2020) for evaluation which require human-annotated ground truths. However, since tutoring has the goal of helping students learn (Macina et al., 2023b), it is very open-ended and there is no single best pedagogical approach at each turn (Jurenka et al., 2024). This results in noisy and unreliable scores from auto-"}, {"title": "3 Background", "content": "MATHTUTORBENCH addresses the limitations of existing automatic metrics by focusing specifically on tutoring, is simple to run, replicable, and could serve as a proxy for deciding which models to focus on in human studies. Moreover, our benchmark only uses data collected from real teachers."}, {"title": "3.1 Next Teacher Utterance Generation", "content": "We focus on educational dialogues between a student and a teacher, where a student is trying to solve a multi-step problem \\(p \\in V^*\\). The problem has a single numerical solution \\(a\\) and a sequence of solution steps \\(s = (s_1,...,s_v)\\), where each \\(s_n \\in V^*\\) and \\(s_v\\) contains the final answer \\(a\\). A student solution consists of steps \\(\\hat{s} = (\\hat{s}_1, ..., \\hat{s}_M)\\) and the first step with a mistake is \\(e \\in \\{0,1, . . ., M\\}\\), where \\(e = 0\\) means no mistake.\nThe goal of dialog tutoring is to continue an existing teacher-student dialog \\(H := (u_1, \u2026 , u_{T-1})\\) consisting of \\(T - 1\\) turns \\(u_t \\in V^*\\) with a new turn \\(u_T \\in V^*\\) that simulates the teacher and guides the student towards solving a problem. This is usually done by learning a model \\(p_\\theta(u_T | H, K, i_t)\\) with parameters \\(\\theta \\in \\mathbb{R}^d\\), which is optionally conditioned on background knowledge \\(K\\) (in our case only the problem \\(p\\)) and a teacher intent \\(i_T\\), and"}, {"title": "4 MathTutorBench", "content": "We introduce MATHTUTORBENCH, a benchmark that evaluates the tutoring capabilities of tutoring models. MATHTUTORBENCH consists of three high-level skills that a good human teacher needs to have (Bommasani et al., 2021): Expertise, Student Understanding and Pedagogical Abilities. These skills are tested by seven different tasks, each consisting of a dataset, prompt, and metric. All tasks in MATHTUTORBENCH are related to math tutoring. The problems are mostly sourced from GSM8k (Cobbe et al., 2021). Table 1 summarizes the datasets and tasks. The prompts used for the tasks in the benchmark are shown in Appendix B."}, {"title": "4.1 Tasks", "content": "This section explains each task and complements Table 1 with the rationale for including it.\n1. Problem Solving. We include a math word problem solving task that measures the accuracy of the final numeric answer generated with chain-of-thought (Wei et al., 2022) compared to the answer a. Even though this type of evaluation is popular, saturated, and contaminated, in MATHTUTORBENCH it serves as an indicator of a balance between expertise and pedagogical abilities.\n2. Socratic Questioning. Socratic questioning is related to the problem decomposition to smaller and more manageable parts. This task is to evaluate whether a model generates for each correct step \\(s_n\\) at least one corresponding guidance question \\(q_n\\) towards the correct answer, which could be posed to the student instead of simply providing the answer (Shridhar et al., 2022; Liu et al., 2024).\n3. Student Solution Correctness. This task evaluates a teacher's ability to verify the correctness of a student's answer. Framed as a balanced binary classification task based on student solution chain \\(\\hat{s}\\), this dimension ensures that the model can objectively discern whether a student's reply is correct or incorrect, a crucial prerequisite for providing accurate feedback and identification of misconceptions (Wang et al., 2024b).\n4. Student Mistake Location. Mistake location is a critical component of effective tutoring, focusing on a teacher's ability to accurately identify the exact location of the first mistake in a student's responses (Daheim et al., 2024). This task assesses whether a tutoring model can pinpoint where a student's reasoning has gone wrong, enabling timely and precise feedback. By detecting steps with mistakes, the model can help students understand their misconceptions and steer the conversation to mitigate them, thus fostering a more productive learning experience (Kapur, 2016; Wang et al., 2024b).\n5. Student Mistake Correction. This task mea-"}, {"title": "4.3 Scaffolding Score", "content": "Evaluating pedagogical abilities in tutoring is inherently challenging due to the open-ended nature of the involved tasks. Unlike more structured domains like factual question answering, pedagogy requires assessing the quality of responses such as questioning guidance to the root cause of a mistake, and actionability of productive scaffolding. In other words, we need an efficient and lightweight"}, {"title": "4.3.1 Criteria-based Scoring", "content": "The most straightforward approach is to train individual critic models for each pedagogical task using labeled data. For an evaluation taxonomy with n total evaluation criteria, for each criterion i we train a binary classifier \\(C_i(y)\\) that outputs a binary prediction of whether the criteria is present or not in response y. To combine these into a final score for a response, we aggregate them as \\(\\sum_{i=1}^n C_i(y)\\), which represents a discrete score of the total number of predicted desired criteria for the response. For example, MRBench (Maurya et al., 2024) is a small dataset annotated with 8 criteria such as the presence of guidance, actionability, and telling the answer. However, the scale of the required data and sparse features pose significant challenges."}, {"title": "4.3.2 Pairwise Ranking of Teacher Responses", "content": "Since labeled data for each criterion can be scarce, we here explore a more unified strategy. Instead of training a separate model for each criterion, where each annotation criterion has inherent subjectivity, we relax the objective and train a single critic model that aggregates multiple criteria into a pairwise comparison. We train a reward model using binary ranking loss by following Ouyang et al. (2022):\n\\(L_{rank} = - log \\sigma(r_\\theta(x, y_c) - r_\\theta(x, y_r) - m)\\) (1)"}, {"title": "4.3.3 Pairwise Preference Data Pipeline", "content": "To create pairwise preference data, we follow our pedagogical criteria from Section 3.2. For example, a response is preferred if it is a Socratic question \\(q_t\\) or it has dialog intent \\(i_t\\) which probes student understanding. Contrary, a response is chosen as dispreferred if it contains part(s) of the reference solution s or has a lower number of desired criteria. To formalize this, for a given dialog history H and a taxonomy with n criteria, we define a score for each response y:\n\\(f(y) = \\sum_{i=1}^n 1(y \\text{ has desired criterion } i)\\) (2)\nwhere 1(\u00b7) is the indicator function that equals 1 if holds and O otherwise. The condition within the indicator function is determined by: a) human criteria annotations (for MRBench (Maurya et al., 2024)), b) dialog intent annotations \\(i_t\\) of the used pedagogical strategy (for MathDial (Macina et al., 2023a)), and c) subquestion annotation \\(q_t\\) (for GSM8k (Cobbe et al., 2021)). For each pair of responses \\((y_i, y_j)\\), we construct a dataset of preference-label pairs \\(D = \\{(y_i, y_j) | f(y_i) > f(y_j)\\}\\), where the margin is defined as \\(m_(y_i, y_j) = f(y_i) \u2013 f(y_j)\\). The dataset captures the relative preference between responses based on the number of desired criteria they exhibit. The description of the datasets used for training and testing is found in Table 4."}, {"title": "5 Experiments", "content": "MathTutorBench includes an evaluation of three groups of models: general LLMs, LLM tutors, and math reasoners. General LLMs such as open-weight Llama3.1 70B and 8B, newer Llama3.2 3B model, and closed source gpt-40-mini. We use specialized tutoring models, namely closed-sourced LearnLM-1.5-Pro and recent open-source tutoring models Qwen2.5-7B-SocraticLM (Liu et al., 2024) and Llemma-7B-32K-MathMix (ScienceTutor) (Chevalier et al., 2024). To measure the importance of specially finetuned tutoring models, we evaluate the Qwen2.5-Math-7B-Instruct, which is optimized for math reasoning and was used for finetuning the specialized tutor model SocraticLM."}, {"title": "5.2 Scaffolding Score - Test Set and Metrics", "content": "The goal of the Scaffolding score is to estimate the pedagogical quality of the teacher response generation. To validate it, we build a test set containing 482 examples based on Bridge (Wang et al., 2024b) which contains student dialogs with novice teachers. The test set has no instance or problem overlap with our training data. In Bridge, novice teacher responses are improved by expert teachers following an expert-defined decision-making process. The process first identifies the type of the error and then determines the pedagogical strategy and intent. For example, while novice teachers tend to explicitly correct student mistakes by giving away correct answers to students, expert teachers use various scaffolding nudges such as the Socratic method, use hints, or ask for further elaboration of the problematic part. We use the following formula to compute the accuracy of pairwise ranking between expert teacher and novice teacher:\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}(y_{expert, i} > y_{novice, i}). (3)"}, {"title": "5.3 Scaffolding Score - Models and Baselines", "content": "We use LLM-as-a-judge prompting as a baseline, similar to Jurenka et al. (2024). For this, we use Llama-3.1-70B-Instruct, GPT-40-mini, and the specialized judge model Prompetheus-7b-v2.0 (Kim et al., 2024). Moreover, we pick well-performing existing preference-tuned reward models with high scores from the RewardBench (Lambert et al., 2024) on a variety of chat comparisons, namely, Internlm2-7b-reward and Skywork-Reward-Llama-3.1-8B-v0.2. To finetune single criteria-based binary classifiers we use ModernBERTbase (Warner et al., 2024) with a classification head. Finally, we use Qwen2.5-0.5B-Instruct and Qwen2.5-1.5B-Instruct for finetuning on preference data, which are small enough to run fast as a part of the benchmark."}, {"title": "6 Results", "content": "In this section, we showcase our core findings on MATHTUTORBENCH and demonstrate the robustness and quality of the scaffolding reward model."}, {"title": "6.1 Comparing SotA LLMs (Table 2)", "content": "Math expertise does not translate directly to student understanding and pedagogy. Our evaluations reveal a striking imbalance in current language models. While these models exhibit impres-"}, {"title": "6.2 Scaffolding Score - Results", "content": "Figure 3 shows a comparison between various models evaluated on the task of scoring expert teacher responses higher than novice teacher responses, see Equation 3. LLM-as-a-judge models are sensitive to prompts and positional bias, so we randomize the order. We report simple and extended prompts with detailing pedagogical guidelines (Figure 6 and 7) but their accuracy is lower than 0.7. Performance of reward models from RewardBench (Lambert et al., 2024) on the pedagogical preferences is only slightly higher than random. We also train a combination of criteria-based ModernBERT binary classifiers aggregated into a summed final score, how-"}, {"title": "7 Conclusion", "content": "In this work we propose MATHTUTORBENCH, a holistic benchmark for quick and cost-effective as-sessment of the educational capabilities of LLM"}, {"title": "Limitations", "content": "Our work focuses on high school math tutoring and limits the insights of the benchmark to multi-step math problems. Despite a limited number of available conversation dataset in other domains, we plan to extend the benchmark to further STEM domains to generalize its applicability and reach.\nThe conversational data in the benchmark does not contain conversations longer than 10 turns and thus can miss to evaluate very long educational conversations with long-term dependencies which might be present in online tutoring classes.\nWe study 1:1 conversational tutoring between teacher and student in this work. Specifically, we focus on a teacher using hints and nudges to aid student learning and provide engaging learning opportunities for students. However, there are additional functions of a teacher that we decided not to model, for example building rapport or trust with less engaged students.\nThe benchmark does not contain all possible dimensions for educational evaluation. For example, it is missing a safety evaluation of potentially harmful tutor responses. It is an extensive research area and not the goal of this work. However, as the benchmark is open-source we plan to extend it to include more safety evaluations."}, {"title": "Ethics Statement", "content": "Intended usage The goal of the benchmark is to evaluate new and existing dialog tutoring models on the skills related to math expertise, student understanding, and pedagogical capabilities. We will release the code and the dataset under CC-BY-4.0 license. This follows the licences of all the datasets which we are using in the benchmark."}]}