{"title": "Improving the Scaling Laws of Synthetic Data with Deliberate Practice", "authors": ["Reyhane Askari-Hemmat", "Mohammad Pezeshki", "Elvis Dohmatob", "Florian Bordes", "Pietro Astolfi", "Melissa Hall", "Jakob Verbeek", "Michal Drozdzal", "Adriana Romero-Soriano"], "abstract": "Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4\u00d7 fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8\u00d7 fewer samples with a 30% reduction in iterations, all while achieving superior performance compared to prior work.", "sections": [{"title": "Introduction", "content": "A key principle underlying learning in human is deliberate practice (DP)\u2014progress is made not by repeating what is already known but by continuously engaging with tasks that stretch the limits of one's abilities (Ericsson et al., 1993). For example, when learning to play the guitar, simply practicing songs that one has mastered does little to improve skill. Instead, targeted practice on challenging tasks and refining learning through feedback, leads to real progress. This principle highlights that effective learning requires exposure to informative and difficult examples rather than passive repetition.\nIn contrast, most machine learning models are trained on pre-collected data that remain static throughout training, limiting their ability to dynamically adapt to their own weaknesses. One promising source of data for visual recognition tasks is large-scale pre-trained text-to-image models (Rombach et al., 2022). They provide an essentially infinite source of synthetic training data, presenting an alternative to real-world datasets, which are often expensive or infeasible to curate (Hemmat et al., 2023; Shin et al., 2023; Zhang et al., 2024). With the great promise of text-to-image models, a natural question arises: what is the potential of learning using only synthetic data? Empirical studies show that increasing the volume of synthetic training data often leads to diminishing returns, with performance gains following a power law stagnation (Fan et al., 2024; Tian et al., 2024a). Instead, pruning to remove uninformative examples has proven effective in improving the effectiveness of training with real or synthetic data (Sorscher et al., 2022; Kolossov et al., 2024; Feng et al., 2024).\nInspired by human learning principles and recent advances in generative image models, we propose the Deliberate Practice (DP) for Synthetic Data Generation framework. Unlike static approaches that generate all synthetic training data upfront (Fan et al., 2024; Shin et al., 2023; Hemmat et al., 2023), our framework incorporates a dynamic loop between a diffusion model and a downstream learner throughout the training. More concretely, rather than generating an entire dataset at once and irrespective of the learner and then pruning it to remove uninformative samples, we propose DP to efficiently generate data directly from the"}, {"title": "Problem Formulation", "content": "Problem Setup. Standard supervised learning relies on a large real labeled training set. Here, however, we assume no real training data is available, and instead, we must rely on a generative model to synthesize training examples.\nFormally, let \\( \\mathcal{Y} \\) denote the set of class labels. Our goal is to train a classifier \\( f_{\\theta} : \\mathcal{X} \\rightarrow \\mathcal{Y} \\), parameterized by \\( \\theta \\), which maps inputs \\( x \\in \\mathcal{X} \\) (e.g., images) to labels \\( y \\in \\mathcal{Y} \\). We are given a predefined label set \\( \\mathcal{Y} \\), a fixed (small) validation set \\( \\mathcal{D}_{val} = \\{(x_i, y_i)\\}_{i=1}^{n_{val}} \\) consisting of real data for evaluation, and a generative model \\( g_{\\theta_g} \\) capable of sampling synthetic data conditioned on a label, i.e., \\( x \\sim g_{\\theta_g}(y) \\). However, no real training data is available, i.e., \\( \\mathcal{D}_{tr} = \\emptyset \\). The objective is to train \\( f_{\\theta} \\) using as few generated examples as possible while maximizing generalization to real data as measured by performance on \\( \\mathcal{D}_{val} \\). The key challenge is to generate minimal yet effective training data, requiring a principled mechanism to select/generate informative examples.\nThe Need for Informative Examples. Not all synthetic samples contribute equally to learning. Prior work shows that simply increasing the synthetic dataset size leads to diminishing returns, as many generated samples are redundant or too easy (Fan et al., 2024). Instead, training should focus on examples that maximize learning efficiency.\nGiven a measure of informativeness for a synthetic sample \\( x \\), one approach is to generate a large dataset and prune uninformative examples. Formally, let \\( \\mathcal{D}_{pool} = \\{(x_i, y_i)\\}_{i=1}^N \\) be a large set of \\( N \\) generated samples. We define a pruned dataset as \\( \\mathcal{D}' := \\{(x_i, y_i) | i \\in [N], q_i = 1\\} \\), where \\( q_i \\in \\{0, 1\\} \\) is a selection variable determining whether a data point \\( (x_i, y_i) \\in \\mathcal{D}_{pool} \\) is retained. The subset size is constrained by \\( m = \\sum_{i=1}^N q_i \\). The quantity \\( N/m \\) is referred to as the over-sampling ratio.\nLet \\( P \\) and \\( Q \\) denote the distributions of the original and pruned datasets, respectively. The pruning process operates as an importance sampling scheme:\n\\[dQ = \\pi dP,\\]\nwhere \\( \\pi \\) is a normalized weighting function that retains the informative samples. The generate-then-prune approach ensures that only informative examples are kept, it is computationally inefficient, as many generated samples are discarded. This motivates the need to devise mechanisms to directly sample the informative examples.\nApproximate Sampling of Informative Examples. Suppose that \\( \\mathcal{D}_{pool} \\) is generated using a diffusion model with induced probability \\( P \\). The generative process is governed by a reverse SDE (Song and Ermon, 2019):\n\\[dx = [v(x, t) - g(t)^2\\nabla\\log p_t(x)] dt + g(t) dW(t),\\]\nwhere \\( W(t) \\) is a Wiener process, modeling stochastic noise, \\( v(x,t) \\) is a drift term, \\( g(t) \\) is a coefficient controlling the noise level at time \\( t \\), and \\( \\log p_t(x) \\) is the score function.\nInstead of sampling from \\( P \\), we aim to sample directly from \\( Q \\) as in Eq. (1). By Girsanov's theorem (Oksendal, 2013), modifying the probability measure from \\( P \\) to \\( Q \\) introduces a correction term in the reverse SDE:\n\\[dx = [v(x, t) - g(t)^2(\\nabla\\log p_t(x) + \\nabla \\log \\pi(x, t))] dt + g(t) dW(t).\\]\nThe term \\( \\nabla \\log \\pi(x, t) \\) effectively modifies the score function and biases the sampling distribution according to the weighting function \\( \\pi(x, t) \\). This modification allows approximating direct sampling from the pruned distribution \\( Q \\), eliminating the need to first sample uniformly from \\( P \\) and later prune the data."}, {"title": "Efficient Entropy-Guided Sampling with DDIM.", "content": "We leverage denoising diffusion implicit models (DDIMs) (Song et al., 2020) for efficient sampling. At each step \\( t \\), the reverse update for generating a conditional sample is:\n\\[x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} z_{\\theta,t} + \\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\epsilon_{\\theta}(x_t, y) + \\sigma_t \\epsilon,\\]\nwhere \\( \\epsilon_t \\) is random noise and \\( \\alpha_t \\) and \\( \\bar{\\alpha}_{t-1} \\) are time-dependent coefficients. The term \\( z_{\\theta,t} \\) approximates the final denoised sample:\n\\[z_{\\theta,t} = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t, y)}{\\sqrt{\\bar{\\alpha}_t}},\\]\nin which \\( \\epsilon_{\\theta}(x_t, y) \\) approximates the conditional score function using a pretrained denoising network (Ho and Salimans, 2022):\n\\[\\epsilon_0(x, y) \\approx (1 + \\lambda)\\tilde{\\epsilon}_\\theta(x, y) - \\lambda\\tilde{\\epsilon}_\\theta(x)\\]\nwhere \\( \\lambda \\) is called the classifier-free guidance coefficient which controls the strength of conditional sampling on the label.\nAn efficient way of sampling from a modified diffusion mode as described in Eq. 3 was proposed by Hemmat et al. (2023), where the weighting function is derived from the entropy of the downstream learner, such that,\n\\[\\log \\pi \\propto H(f_{\\phi}(x_0)) = - \\sum_{y \\in \\mathcal{Y}} f_{\\phi}(y | x_0) \\log f_{\\phi}(y | x_0).\\]\nTo compute the entropy as in Eq. 6, we need the denoised sample \\( x_0 \\). The term \\( z_{\\theta,t} \\) can be used to cheaply approximate entropy mid-generation. This allows direct sampling of high-entropy examples by modifying the score function:\n\\[\\epsilon_{\\theta}^{(t)}(x_t, y) = \\epsilon_{\\theta}^{(t)}(x_t, y) + w\\nabla_{x_t} H(f_{\\phi}(z_{\\theta,t})),\\]\nwhere \\( w \\) controls the contribution of the entropy-guidance.\nIn Hemmat et al. (2023), real data is used to pre-train the learner, enabling an accurate estimation of \\( \\nabla_{x_t}H(f_{\\phi}(z_{\\theta,t})) \\). However, when real data is unavailable, alternative approaches are needed to assess sample informativeness. In the next section, we propose to leverage the learner itself during training to evaluate entropy and determine the informativeness of generated samples dynamically."}, {"title": "The deliberate Practice Framework for Synthetic Data Generation", "content": "In this section, we describe our Deliberate Practice framework, in which we efficiently train the learner with synthetic data in absence of any real data. In particular, we move to a setup where we dynamically expand the dataset throughout the training. Our framework is summarized in Algorithm 1."}, {"title": "Training on informative examples improves the scaling laws", "content": "Before presenting empirical results, we first analyze how selecting informative examples affects the scaling of synthetic data. We study a high-dimensional linear classifier trained with uniform vs. selective sampling"}, {"title": "Theoretical Analysis under an Idealized Setup.", "content": "Consider a simple generative model for training data:\n\\[x \\sim \\mathcal{N}(0, \\Sigma), \\quad y = \\text{sign}(w_o^\\top x),\\]\nwhere \\( w_o \\in \\mathbb{R}^d \\) is the ground-truth labeling function. This gives a distribution \\( P \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\).\nWe study the impact of uniform sampling versus selective sampling of informative examples on generalization. To formalize this, we assume a pool of \\( n \\) i.i.d. training pairs:\n\\[X \\in \\mathbb{R}^{n \\times d}, \\quad Y \\in \\mathbb{R}^n.\\]\nA linear classifier \\( w \\) is trained using the following loss:\n\\[\\hat{w} = \\underset{w}{\\text{arg min}} \\sum_{i=1}^n q_i l(w^\\top x_i, y_i) + \\frac{\\lambda}{2} ||w||^2,\\]\nwhere \\( l(z,y) = (z - y)^2/2 \\) is the squared loss, \\( \\lambda > 0 \\) is a regularization parameter, and \\( q_i := q(x_i^\\top w_s) \\) is a selection strategy that determines whether an example is included in training based on its projection in a given direction \\( w_s \\in \\mathbb{R}^d \\), and an arbitrary measurable binary function \\( q : \\mathbb{R} \\rightarrow \\{0, 1\\} \\) which encodes the selection strategy.\nThe selection/pruning ratio is given by:\n\\[p = \\mathbb{E}[q(x^\\top w_s)] \\text{ for } x \\sim \\mathcal{N}(0, \\Sigma).\\]\nThe resulting classifier has a closed-form solution:\n\\[\\hat{w} = \\frac{1}{\\lambda} R \\Sigma X^\\top DY, \\quad R := (\\frac{1}{\\lambda} X^\\top DX + \\lambda I_d)^{-1},\\]\nwhere \\( D \\in \\mathbb{R}^{n \\times n} \\) is a diagonal matrix with \\( D_{ii} = q_i \\).\nOur objective is to analyze the asymptotic test error of \\( \\hat{w} \\):\n\\[E_{\\text{test}}(\\hat{w}) = \\mathbb{P}(\\text{sign}(x^\\top\\hat{w}) \\neq y),\\]\nwhere \\( (x, y) \\) is a test example,"}, {"title": "Asymptotic Behavior of the Test Error.", "content": "We leverage random matrix theory (RMT) techniques (Couillet and Liao, 2022; Liao and Mahoney, 2021; Firdoussi et al., 2024) to characterize the test error in Eq. (13). Our analysis is based on the spectral density of the resolvent matrix \\( R \\) in Eq. (12), allowing us to compute the first two moments of \\( yx^\\top\\hat{w} \\) for a test sample \\( x \\) and derive an expression for the test error. For simplicity, we assume an isotropic setup where \\( \\Sigma = I_d \\) and defer the general case to Appendix A.\nWe shall work in the following so-called high-dimensional proportionate scaling regime\n\\[d, n \\rightarrow \\infty, \\quad d/n \\rightarrow \\phi,\\]\nin which the input-dimension \\( d \\) and the sample size \\( n \\) diverge to infinity at the same rate. The scalar \\( \\phi \\in (0, \\infty) \\) captures the effective dimensionality or over-parametrization rate of the problem."}, {"title": "The deliberate Practice Framework for Synthetic Data Generation", "content": "In this section, we describe our Deliberate Practice framework, in which we efficiently train the learner with synthetic data in absence of any real data. In particular, we move to a setup where we dynamically expand the dataset throughout the training. Our framework is summarized in Algorithm 1."}, {"title": "Training on informative examples improves the scaling laws", "content": "Before presenting empirical results, we first analyze how selecting informative examples affects the scaling of synthetic data. We study a high-dimensional linear classifier trained with uniform vs. selective sampling"}, {"title": "Connection Between Pruning and DP", "content": "In Section 2, we discussed how DP approximates direct sampling from a pruned distribution. Here, we validate this experimentally on ImageNet-100 using two setups:\nOversampling then Pruning: Generate a large pool and select high-entropy samples.\nDirect entropy-guided generation: Generate only informative samples (a special case of DP with a single step of data addition).\nWe start with 130k generated samples (regular vanilla sampling), train for 17k iterations, then add a one-time additional 130k samples, increasing the total data size to 260k and training for an additional 33k iterations.\nIn setup 1, we vary the pool size, ranging from no pruning (130k pool) up to an oversampling ratio of 18 (2.4M pool), selecting the top 130k high-entropy samples. In setup 2, we generate exactly 130k entropy-guided samples, varying the entropy-gauidance coefficient.\nFigure 5 (a, b) shows that both methods improve performance up to a point, after which excessive selection of high-entropy samples leads to degradation\u2014likely due to selecting high-entropy but harmful outliers. This aligns with our theoretical predictions in Figure 5 (c).\nRegarding computational costs, generating a single image with entropy-guidance on an Nvidia H100 takes 1.82x longer than standard vanilla sampling. However, achieving similar performance through oversampling requires significantly more data, leading to a linear increase in cost. As a result, DP is 5x more efficient while also providing higher absolute improvements compared to pruning-based selection. See Figure 5 for details and Figure 11 for some visualizations."}, {"title": "The evolution of hard examples over time", "content": "\"Does the sample hardness change as training progresses?\"\nTo answer this question, Figure 6 (left) tracks the error on examples that were misclassified at the time they were added. As expected, once introduced, the model gradually learns to classify them correctly. However, an interesting trend emerges: even before these examples were added, their error was lower than at the moment of inclusion. This suggests that the notion of hardness is dynamic\u2014what is considered challenging at one point may become easier over time. Conversely, examples that were once easy might later become difficult due to shifts in the learned decision boundaries. This highlights a key limitation of static pruning approaches and underscores the importance of dynamically adapting the selection of informative examples throughout training, as done in Deliberate Practice (DP). See Figure 12 for some visualization of generations through training."}, {"title": "Related Work", "content": "Synthetic data for training neural networks. Synthetic data has become a powerful tool for training machine learning models across various domains. For instance, text-to-image diffusion models have been successfully used for visual representation learning (Astolfi et al., 2023; Li et al., 2025; Tian et al., 2024a,b; Sar\u0131y\u0131ld\u0131z et al., 2023). However, limitations of synthetic data are highlighted by Fan et al. (2024), emphasizing the importance of generating more challenging and informative examples. Addressing distribution shifts between synthetic and real data, Hemmat et al. (2023) and Yuan et al. (2023) propose synthesizing training data that matches real data distributions or conditioning on real examples to reduce this gap. Expanding small-scale datasets has also been studied, see e.g. Zhang et al. (2024). Another related line of work involves using VLMs and LLMs to generate descriptions for augmenting datasets (Dunlap et al., 2023).\nSynthetic data is increasingly used to train (LLMs). For example, LLaMA3 (Grattafiori et al., 2024) employs AI-generated data for fine-tuning. Similarly, self-play approaches, e.g., Yuan et al. (2024), align with our framework by generating increasingly difficult examples for training.\nContinual learning and active learning. Our work is also closely related to principles from active learning (Bang et al., 2024; Evans et al., 2023) and continual learning, which prioritize iterative model updates with tailored data. These methods highlight the importance of selecting informative samples based on the model's current state. Sorscher et al. (2022) showed that pruning static datasets using metrics like margin scores can improve scaling laws by retaining the most informative examples, albeit in a non-adaptive manner.\nChallenges and risks of synthetic data. The challenges of training models on synthetic data, have gained significant attention. Dohmatob et al. (2024a,b) studied \u201cmodel collapse\u201d, a phenomenon where iterative training on synthetic data degrades performance. They emphasize that data verification mechanisms can mitigate this risk and enable scaling with synthetic data. Similarly, our framework by generating informative examples through a dynamic loop, improves sample efficiency."}, {"title": "Conclusion", "content": "We introduced Deliberate Practice for Synthetic Data Generation, a framework that improves scaling laws by dynamically generating challenging and informative training examples. Unlike traditional methods that rely on static datasets, our approach approximates generating data directly from a pruned distribution, reducing inefficiencies and ensuring models continuously training on informative samples. We provided theoretical insights into the benefits of training on pruned distributions and empirically demonstrated that our method significantly improves performance while requiring fewer training iterations. Our results on ImageNet-100 and ImageNet-1K show that Deliberate Practice achieves superior accuracy with far less data and compute, outperforming previous state-of-the-art. Our work highlights the potential of structured synthetic data generation in advancing efficient and adaptive learning."}, {"title": "Some Important Examples of Pruning Strategies", "content": "Keep Hard Examples (KH). Consider the case where the pruning strategy is given by \\( q_i = q_{KH}(x_i^\\top w_s) \\) for all \\( i \\), where\n\\[q_{KH}(t) := 1[|t| \\leq \\xi] = \\begin{cases} 1, & \\text{ if } |t| \\leq \\xi, \\\\ 0, & \\text{ else,} \\end{cases}\\]\nfor some \\( \\xi \\geq 0 \\). Define \\( \\alpha := \\xi / ||w_s|| \\). We have explicit formula for the constants \\( \\beta \\) and \\( \\tilde{\\beta} \\) appearing in Theorem 1. Viz,\nLemma 1. With \\( \\tau := \\rho/\\sqrt{1 - \\rho^2} \\), \\( \\epsilon_1 := 2\\Phi(\\alpha/\\sqrt{1 - \\rho^2}) - 1 \\), and \\( \\epsilon_2 := 2\\Phi(\\tau \\alpha) - 1 \\), it holds that\n\\[\\beta(q_{KH}) = 2(\\rho \\varphi(0) \\epsilon_1 - \\varphi(\\alpha) \\epsilon_2), \\quad \\tilde{\\beta}(q_{KH}) = 2\\varphi(0) \\sqrt{1 - \\rho^2} \\cdot \\epsilon_1.\\]\nExample 2: Keep Easy Examples (KE). Here, the pruning strategy is \\( q_i = q_{KE}(x_i^\\top w_s) \\), where\n\\[q_{KE}(t) := 1[t] > \\xi] = \\begin{cases} 0, & \\text{ if } |t| \\leq \\xi, \\\\ 1, & \\text{ else.} \\end{cases}\\]\nLemma 2. With \\( \\tau := \\rho/ \\sqrt{1 - \\rho^2} \\), \\( \\epsilon_1 := 2(1 - \\Phi(\\alpha/\\sqrt{1 - \\rho^2})) \\), \\( \\epsilon_2 := 2\\Phi(\\tau \\alpha) - 1 \\), it holds that\n\\[\\beta(q_{KE}) = 2(\\rho \\varphi(0) \\epsilon_1 + \\varphi(\\alpha) \\epsilon_2), \\quad \\tilde{\\beta}(q_{KE}) = 2\\varphi(0) \\sqrt{1 - \\rho^2} \\cdot \\epsilon_1.\\]\nExample 3: Interpolation between Keep Hard and Keep Easy Strategies. Consider the following pruning strategy proposed in Kolossov et al. (2024)\n\\[q(t) \\propto \\sigma(t)^w (1 - \\sigma(t))^w,\\]\nfor some tuning parameter \\( w \\). Here, \\( \\sigma \\) is the sigmoid function. We can associate \\( q(x^\\top w_s) \\) with the probability the auxiliary classifier \\( x \\rightarrow \\text{sign}(x^\\top w_s) \\) assigns to an example \\( x_i \\). Thus, positive values of \\( w \\) correspond to keeping examples considered uncertain (i.e hard) by this classifier, while negative values correspond to examples considered easy."}, {"title": "Main Ingredients of Proofs", "content": "Deterministic Equivalent for the Resolvent Matrix \\( R \\)\nDefinition 1 (Deterministic Equivalents). Given a sequence of random \\( N \\times N \\) matrices \\( (R_N)_n \\), a deterministic equivalent thereof is a sequence of deterministic \\( N \\times N \\) matrices \\( (\\tilde{R}_N)_N \\) such that\n\\[\\text{tr } A_N (R_N - \\tilde{R}_N) \\overset{p}{\\rightarrow} 0,\\]\nfor all sequences of \\( N \\times N \\) matrices \\( (A_N)_N \\) with bounded Frobenious norm.\nLet \\( \\Pi \\) (resp. \\( \\Pi_{\\perp} = I_d - \\Pi \\)) be the projection onto the span (resp. orthogonal complement of the span) of \\( w_s \\). Define the following auxiliary vectors and scalars\n\\[\\nu = \\frac{\\Sigma^{1/2}w_s}{||w_s||}, \\quad \\nu_1 = \\Pi \\nu, \\quad \\nu_{\\perp} = \\Pi_\\perp \\nu.\\]\nNote that \\( \\nu_{\\perp} \\) is \\( (d - 1) \\)-dimensional and \\( ||\\nu_{\\perp}|| = \\sqrt{||\\nu||^2 - \\nu_1^2} \\).\nHenceforth we make the replacement \\( z = -\\lambda < 0 \\), so that the resolvent matrix \\( R \\) now writes\n\\[R = R(z) := (\\frac{1}{\\lambda} X^\\top DX/n - zI_d)^{-1}.\\]"}, {"title": "Some Important Examples of Pruning Strategies", "content": "Keep Hard Examples (KH). Consider the case where the pruning strategy is given by \\( q_i = q_{KH}(x_i^\\top w_s) \\) for all \\( i \\), where\n\\[q_{KH}(t) := 1[|t| \\leq \\xi] = \\begin{cases} 1, & \\text{ if } |t| \\leq \\xi, \\\\ 0, & \\text{ else,} \\end{cases}\\]\nfor some \\( \\xi \\geq 0 \\). Define \\( \\alpha := \\xi / ||w_s|| \\). We have explicit formula for the constants \\( \\beta \\) and \\( \\tilde{\\beta} \\) appearing in Theorem 1. Viz,\nLemma 1. With \\( \\tau := \\rho/\\sqrt{1 - \\rho^2} \\), \\( \\epsilon_1 := 2\\Phi(\\alpha/\\sqrt{1 - \\rho^2}) - 1 \\), and \\( \\epsilon_2 := 2\\Phi(\\tau \\alpha) - 1 \\), it holds that\n\\[\\beta(q_{KH}) = 2(\\rho \\varphi(0) \\epsilon_1 - \\varphi(\\alpha) \\epsilon_2), \\quad \\tilde{\\beta}(q_{KH}) = 2\\varphi(0) \\sqrt{1 - \\rho^2} \\cdot \\epsilon_1.\\]\nExample 2: Keep Easy Examples (KE). Here, the pruning strategy is \\( q_i = q_{KE}(x_i^\\top w_s) \\), where\n\\[q_{KE}(t) := 1[t] > \\xi] = \\begin{cases} 0, & \\text{ if } |t| \\leq \\xi, \\\\ 1, & \\text{ else.} \\end{cases}\\]\nLemma 2. With \\( \\tau := \\rho/ \\sqrt{1 - \\rho^2} \\), \\( \\epsilon_1 := 2(1 - \\Phi(\\alpha/\\sqrt{1 - \\rho^2})) \\), \\( \\epsilon_2 := 2\\Phi(\\tau \\alpha) - 1 \\), it holds that\n\\[\\beta(q_{KE}) = 2(\\rho \\varphi(0) \\epsilon_1 + \\varphi(\\alpha) \\epsilon_2), \\quad \\tilde{\\beta}(q_{KE}) = 2\\varphi(0) \\sqrt{1 - \\rho^2} \\cdot \\epsilon_1.\\]\nExample 3: Interpolation between Keep Hard and Keep Easy Strategies. Consider the following pruning strategy proposed in Kolossov et al. (2024)\n\\[q(t) \\propto \\sigma(t)^w (1 - \\sigma(t))^w,\\]\nfor some tuning parameter \\( w \\). Here, \\( \\sigma \\) is the sigmoid function. We can associate \\( q(x^\\top w_s) \\) with the probability the auxiliary classifier \\( x \\rightarrow \\text{sign}(x^\\top w_s) \\) assigns to an example \\( x_i \\). Thus, positive values of \\( w \\) correspond to keeping examples considered uncertain (i.e hard) by this classifier, while negative values correspond to examples considered easy."}, {"title": "Main Ingredients of Proofs", "content": "Deterministic Equivalent for the Resolvent Matrix \\( R \\)\nDefinition 1 (Deterministic Equivalents). Given a sequence of random \\( N \\times N \\) matrices \\( (R_N)_n \\), a deterministic equivalent thereof is a sequence of deterministic \\( N \\times N \\) matrices \\( (\\tilde{R}_N)_N \\) such that\n\\[\\text{tr } A_N (R_N - \\tilde{R}_N) \\overset{p}{\\rightarrow} 0,\\]\nfor all sequences of \\( N \\times N \\) matrices \\( (A_N)_N \\) with bounded Frobenious norm.\nLet \\( \\Pi \\) (resp. \\( \\Pi_{\\perp} = I_d - \\Pi \\)) be the projection onto the span (resp. orthogonal complement of the span) of \\( w_s \\). Define the following auxiliary vectors and scalars\n\\[\\nu = \\frac{\\Sigma^{1/2}w_s}{||w_s||}, \\quad \\nu_1 = \\Pi \\nu, \\quad \\nu_{\\perp} = \\Pi_\\perp \\nu.\\]\nNote that \\( \\nu_{\\perp} \\) is \\( (d - 1) \\)-dimensional and \\( ||\\nu_{\\perp}|| = \\sqrt{||\\nu||^2 - \\nu_1^2} \\).\nHenceforth we make the replacement \\( z = -\\lambda < 0 \\), so that the resolvent matrix \\( R \\) now writes\n\\[R = R(z) := (\\frac{1}{\\lambda} X^\\top DX/n - zI_d)^{-1}.\\]"}, {"title": "Test Error Representation (\u201cScaling Laws\u201d)", "content": "We are now ready to state our main theoretical results, which is a generalization of Theorem 1.\nRemark 1. For simplicity of presentation, all our theoretical results only consider symmetric pruning strategies for which \\( q(-t) = q(t) \\). This includes the \"keep hard\" and \"keep easy\" pruning strategies considered in (Sorscher et al., 2022).\nProposition 2. For a random test point \\( (x, y) \\sim P \\) independent of training data, it holds that \\( yx^\\top\\hat{w} \\sim \\mathcal{N}(m, \\nu - m^2) \\) in the limit (14), where\n\\[m := \\frac{m_0}{1 + \\delta}, \\qquad \\nu := \\frac{v_0}{(1 + \\delta)^2},\\]\n\\[m_0 := \\frac{\\mu}{\\pi ||w_0||} \\mathbb{R} \\Sigma \\Sigma' c \\, \\qquad \\nu_0 := \\frac{2}{\\pi} \\text{tr } \\Sigma \\Sigma' + \\frac{\\tilde{c}^2}{\\pi (1 + \\delta)^n}, \\qquad \\tilde{c} := \\mathbb{E}_{(x,y) \\sim P'} [q(x^\\top w_s)yx], \\qquad \\Sigma' := \\mathbb{E} [R \\Sigma \\tilde{R}].\\]\nConsequently, the limiting test error of \\( \\hat{w} \\) is given by\n\\[E_{\\text{test}}(\\hat{w}) \\rightarrow \\Phi \\left( -\\frac{m_0}{\\sqrt{v_0 - m_0^2}} \\right).\\]"}, {"title": "Isotropic Case", "content": "Consider the specific case where the covariance matrix is \\( \\Sigma = I_d \\). It is not hard to see that we must have \\( \\mathfrak{m}(z) = \\mathfrak{m}(z) = \\mathfrak{d}(z)/\\phi \\). Let us now compute \\( \\mathfrak{m}(z) \\).\nLemma 3. For every \\( z = -x < 0 \\), \\( \\mathfrak{m}(z) \\) is given by formula (16).\nProof. Indeed, observe that in the isotropic case the equation (39) reduces to \\( p - \\phi - t(z) = \\phi z/(t(z) - z) \\), or equivalently\n\\[0 = \\phi z + (t(z) - p + \\phi)(t(z) - z) = t(z)^2 - (p - \\phi + z)t(z) + \\phi z.\\]"}, {"title": "Test Error Representation (\""}]}