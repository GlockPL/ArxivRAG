{"title": "FairDgcl: Fairness-aware Recommendation with Dynamic Graph Contrastive Learning", "authors": ["Wei Chen", "Meng Yuan", "Zhao Zhang", "Ruobing Xie", "Fuzhen Zhuang", "Deqing Wang", "Rui Liu"], "abstract": "As trustworthy Al continues to advance, the fairness issue in recommendations has received increasing attention. A recommender system is considered unfair when it produces unequal outcomes for different user groups based on user-sensitive attributes (e.g., age, gender). Some researchers have proposed data augmentation-based methods aiming at alleviating user-level unfairness by altering the skewed distribution of training data among various user groups. Despite yielding promising results, they often rely on fairness-related assumptions that may not align with reality, potentially reducing the data quality and negatively affecting model effectiveness. To tackle this issue, in this paper, we study how to implement high-quality data augmentation to improve recommendation fairness. Specifically, we propose FairDgcl, a dynamic graph adversarial contrastive learning framework aiming at improving fairness in recommender system. First, FairDgcl develops an adversarial contrastive network with a view generator and a view discriminator to learn generating fair augmentation strategies in an adversarial style. Then, we propose two dynamic, learnable models to generate contrastive views within contrastive learning framework, which automatically fine-tune the augmentation strategies. Meanwhile, we theoretically show that FairDgcl can simultaneously generate enhanced representations that possess both fairness and accuracy. Lastly, comprehensive experiments conducted on four real-world datasets demonstrate the effectiveness of the proposed FairDgcl.", "sections": [{"title": "1 INTRODUCTION", "content": "RECOMMENDER systems play a vital role in numerous online applications such as e-commerce platforms and entertainment apps, significantly enriching the user experience [1], [2], [3], [4]. Recently, Graph Neural Networks (GNNs) [5], [6], [7] have significantly boosted recommender systems performance by efficiently leveraging user-item interactions. However, since recommender systems are applied to human-centered applications, excessive pursuit of recommendation accuracy may harm the fairness experience of some users [8], [9]. A recommender system is unfair when it yields unequal outcomes for different user groups based on user-sensitive attributes. (e.g., age, gender). For instance, equally skilled men are favored over women in job recommender [10], and active users tend to receive better recommendations than inactive ones [11].\nA recent study [12] uses GNN models to measure the distribution differences between the two user groups in both training data and recommendation outcomes. The results reveal substantial disparities in recommendation outcomes among user groups, often surpassing those observed in the training data. This implies that the GNN models might not only inherit unfairness but also amplify it from the data. Consequently, the main concern with the unfairness issue stems from the biases inherent in the training data. Based on this premise, research efforts [12], [13], [14], [15] have been devoted to improving recommendation fairness from the perspective of data augmentation. Usually, they first generate some augmented interaction data based on some hypothetical labels\u00b9 to balance the uneven interactions between different user groups, and then selectively mix them into the original data for joint training (see Figure 1). Despite achieving some promising results relying on these synthetic data, their applicability may be limited due to users' different personal preferences in real-world recommendation scenarios, potentially reducing data quality and\nInstead of relying on additional assumed labels for generating augmented data, contrastive learning [16] (CL) employs the input data itself as the supervision signal. It constructs an augmented data pair to teach the model to compare their similarity, and has shown competitive performance in various machine learning tasks [17], [18], [19], [20]. Impressed by the exceptional performance of CL paradigm, recent studies [21], [22], [23] have introduced graph contrastive learning (GCL) into recommender systems. Its key is to perform data augmentation to generate different views and contrastive learning tasks. While GCL can indeed solve the problem of low-quality augmented data to some extent, it still faces two main challenges when tackling fairness issues in recommender systems:\n(1) Fairness-oriented Data Augmentation. Most current GCL methods heavily depend on unsupervised data augmentation techniques, like randomly removing nodes or edges [17], [22]. However, relying solely on these methods may not be sufficient to guarantee fair recommendation results. An intuitive way to promote fairness in this process is by setting fairness constraints [24], [25], such as removing interactions that contain sensitive information. Nonetheless, when dealing with graph data, i) the relationship between nodes and edges can be highly complex, with interactions among them being multi-dimensional and non-linear. Simply removing or altering specific interactions may not provide a comprehensive solution to the unfairness issue. ii) More importantly, discrimination may not manifest overtly. It could be concealed within the inherent feature of nodes or arise as a consequence of how nodes interact with one another. This inherent ambiguity significantly complicates the identification and quantification of these biased graph elements. Therefore, it is highly desirable to explore efficient fairness-oriented data-augmentation methods in GCL.\n(2) Dynamic Data Augmentation. In fairness-oriented recommendation, an important problem lies in striking a balance between recommendation accuracy and fairness [25], [26]. This balance is delicate, as efforts to enhance one aspect may adversely impact the other. As previously noted, within the same model, post-training recommendation unfairness often surpasses pre-training data's inherent unfairness. This suggests that while aiming for greater accuracy, the model may inadvertently introduce more unfairness, leading to a dynamic balance between accuracy and fairness during the process. Although previous methods have shown promise with single-round data augmentation\u00b2, maintaining this balance consistently is still difficult. Thus, it is essential to implement a dynamic, multi-round data augmentation approach, which can offer a more nuanced way of continuously recalibrating the balance between accuracy and fairness. However, the discrete nature of graph data, such as binary adjacency matrix values, complicates maintaining balance during dynamic augmentation.\nIn light of these challenges, we propose a novel Dynamic graph contrastive learning framework to improve recommendation Fair-ness (FairDgcl), as shown in Figure 1. Specifically, we develop a graph adversarial contrastive"}, {"title": "2 RELATED WORK", "content": "In this section, we will briefly review the fairness-aware recommendation and data augmentation, and discuss the relationship between this study and previous works."}, {"title": "2.1 Fairness-aware Recommendation", "content": "With the development of trustworthy AI [27], [28], [29], ensuring algorithmic fairness is crucial, particularly in humancentric recommender systems. In the field of recommendation, fairness demands can be categorized into useroriented [25], [30], [31] and item-oriented [32], [33], [34], depending on the stakeholder being considered. User-side fairness focuses on ensuring that all users get equitable chances at recommendations, regardless of their sensitive attributes. Item-side fairness is concerned with ensuring that all items, regardless of their popularity, are fairly recommended to users. Furthermore, user-oriented fairness can be categorized into two categories: i) individual fairness, which advocates for equal treatment of every user, ii) group fairness, which aims for equal recommendation opportunities across user groups defined on sensitive attributes. In this paper, we explore fairness-aware recommendations based on the concept of group fairness [35], [36]. Many fair training methods have been developed to achieve group fairness in this domain. For example, Yao and Huang [37] propose four metrics to optimize the fairness of recommender systems, achieved by integrating fairness-oriented constraints into the learning objective. Wu et al. [38] leverage adversarial learning to efficiently filter sensitive attributes within user data, while preserving essential information, thereby\nachieving fair representation learning. Li et al. [39] develop a notion of counterfactual fairness rooted in causal theory, and construct a framework for a mixed-structure adversarial model to actualize it. Li et al. [8] introduce a reranking algorithm that ensures the fairness of recommendation systems by effectively solving a 0-1 programming problem. Yang et al. [40] address fairness issues arising from distributional discrepancies in recommender systems using distributionally robust optimization techniques. Although these models have achieved numerous successes, they primarily focus on the algorithm itself. Machine learning problems typically involve two core components: data and algorithm. In this paper, our aim is to enhance recommendation fairness from a data-centric perspective, which is significantly different from prevalent model-based research."}, {"title": "2.2 Data Augmentation", "content": "With the success of data augmentation techniques in natural language processing (NLP) [41], [42] and computer vision (CV) [43], [44] tasks, data augmentation techniques applied in recommender system has also received extensive attention. In recommendation scenarios, GCL is an effective data augmentation strategy used to alleviate the data sparsity problem. The key idea of this approach is to create diverse enhanced views and subsequently strive to optimize the similarity of representations across these views. Furthermore, some studies [20], [45] design learnable data augmentation methods that automatically improve node representation in bipartite graphs within the GCL framework. However, it should be noted that the methods previously mentioned do not address fairness concerns, and the exploration of fairness-aware data augmentation has been limited in only a few studies. For example, Spinelli et al. [24] argue that nodes with similar sensitive attributes tend to form connections, leading to biased predictions. Thus, they propose an algorithm to selectively remove edges in graphs, reducing biases and improving fairness in predictive tasks. Chen et al. [25] introduce a graph editing methodology inspired by counterfactual thinking [46], [47], aimed at generating a fair graph and learning fair node representations. More recently, Ying et al. [15] provide an attribute-aware counterfactual data augmentation strategy tailored for minority users, designed to reduce disparities across different user groups. Additionally, Chen et al. [12] propose a fairness-aware augmented framework, based on the assumption that users in one group share similar item preferences with users in the other group, aiming to generate synthetic interaction data to balance group interactions for different user groups. While achieving improved results relying on these augmented data, their reliability in real scenarios is questionable due to varying user preferences, potentially degrading data quality and impairing model effectiveness. Hence, we will explore high-quality data augmentation strategies to improve fairness in recommender systems. In this work, we consider contrastive learning as a data augmentation technique, utilizing the input data itself as the supervision signal and constructing an augmented data to guide model learning."}, {"title": "3 PRELIMINARIES", "content": "In this section, we present preliminary knowledge. We first introduce typical neural graph collaborative filtering methods and then describe the notation of user-oriented fairness."}, {"title": "3.1 Neural Graph Collaborative Filtering", "content": "Collaborative filtering (CF) is one of the core components of recommendation. Its purpose is to model users' potential preferences through observed user-item interactions. A typical recommender system comprises two sets of entities: a user set U and an item set V. In the context of graph collaborative filtering, the interactive matrix is transformed into a bipartite graph denoted as $G =< U\\cup V,E >$, where E represents the corresponding edges.\nIn line with most prior studies [20], [25], [45], we adopt LightGCN [5] as the backbone GNN, leveraging its message-passing mechanism to aggregate and summarize node embeddings for both the user u and item v. Subsequently, we calculate the predicted user u's preference for item v as the inner product between the respective embeddings, defined as $y (u, v) = h_u^Th_v$. During the training process, we adopt the Bayesian Personalization Ranking (BPR) loss to learn the embeddings for each user and item:\n$L_{BPR}(u, v^+, v^-) = -log\\sigma(y (u, v^+) \u2212 y (u, v^-))$, (1)\nwhere $(u,v^+)$ represents the positive interactions and $(u, v^-)$ represent random negative interactions, $\\sigma$ denotes the sigmoid function. The BPR loss aims to boost the prediction scores of observed user-item interactions above those of unobserved interactions."}, {"title": "3.2 User-oriented Fairness", "content": "In recommendation, user-oriented fairness aims to provide a balanced and personalized recommendation experience by minimizing disparities among different users. More specifically, user fairness can be categorized into two main aspects based on target audiences: individual fairness and group fairness. Individual fairness treats similar users equally, while group fairness aims to reduce service disparities among user groups for satisfactory recommendations. This work primary emphasis lies in addressing user-level group fairness while considering two distinct user groups. Initially, we divide users U into two groups based on their sensitive attributes, namely $U_{s=0}$ and $U_{s=1}$. Subsequently, we utilize the group fairness metric to improve fairness [9], [12], [25], a widely accepted measure designed to minimize performance disparities between different user groups. Formally, the group fairness is expressed as follows:\n$\\Phi = \\frac{1}{|U_{s=0}|} \\sum_{u \\in U_{s=0}} F_u - \\frac{1}{|U_{s=1}|} \\sum_{u \\in U_{s=1}} F_u$, (2)\nwhere $F_u$ is the recommendation performance for user u (e.g., recall, ndcg). In practice, optimizing Eq. (2) in recommendation models is challenging due to the nondifferentiable nature of conventional ranking-based metrics such as recall and ndcg. To tackle this problem, prior research has proposed two main approaches. The first approach focuses on creating surrogate loss functions to make"}, {"title": "4 METHODOLOGY", "content": "In this section, we first present the overview of the proposed FairDgcl model (in Figure 2), and then bring forward the details of its major modules. Finally, we introduce the optimization processes."}, {"title": "4.1 Model Overview", "content": "As shown in Figure 2, FairDgcl consists of two main modules, namely view generator and view discriminator. Specifically, the view generator learns fair augmentation strategies and generates fair representations. Then, the view discriminator is designed to evaluate whether the augmented views are fair enough. The view generator and the view discriminator are trained in an adversarial style to generate highquality views. These augmented views are used to train fair and effective user and item representations. Table 1 outlines the primary symbols and their meanings in this paper."}, {"title": "4.2 View Generator", "content": "Given a graph $G =< U\\cup V,E >$, where U (V) represents user (item) nodes, and E denotes corresponding edges. The view generator is designed to generate two augmented views. As previously discussed, fairness-aware data augmentation requires considering the dynamic balance between accuracy and fairness. Accordingly, we propose to use two learnable models as view generators to generate adaptive views for GCL. It should be noted that it is crucial to generate diverse and discriminative views with GCL. If two views"}, {"title": "4.2.1 Recognition Model View Generator", "content": "GNNs employ message passing to capture node representations. However, in real-world situations, these interactions can introduce unfairness issues due to potential biases. For example, if user-item interactions are influenced by sensitive attributes, the model may wrongly interpret them as inherent preferences, leading to biased outputs. Hence, we propose a parameterized recognition model [48], [49] designed to reduce unfairness by directly removing interactions associated with sensitive attributes. This can reduce\nthe model's reliance on sensitive attributes and better reflect the users' real interests.\nTechnically, the recognition model computes a realvalued fairness weight $w_e$ and determines a binary sampling probability $p_e\\in \\{0,1\\}$ for every edge $e \\in E$ in l-th layer. It's worth mentioning that the edge e will be retained if $p_e = 1$ and discarded otherwise. Formally, the weight $w_e^l$ is computed as follows:\n$w_e^l = MLP ([h_u^l || h_v^l])$, (3)\nwhere $e = (u, v)$ represents an edge, $w_e^l$ signifies edge fairness importance, MLP stands for multi-layer perception, and $||$ represents concatenation. $h_u^l$ and $h_v^l$ are user and item representations in the l-th layer. Note that a higher $w_e^l$ suggests that the edge e is more likely to be critical and should be preserved. To enable end-to-end optimization and ensure differentiability in the edge dropping procedure, we transform the discrete variable $p_e$ into a continuous variable in (0, 1). It can be achieved using the Gumbel-Max reparameterization trick [50], where we define it as:\n$p_e = sigmoid ((log(\\eta) \u2013 log(1 \u2013 \\eta) + w_e^l)/\\tau)$, (4)\nwhere $\\eta$ is a priori constant offset, and temperature hyperparameter $\\tau$ is used to control the approximation. As $\\tau$ approaches 0, $p_e$ converges to a binary value. Finally, we denote the augmented interactive graph as $G_1$."}, {"title": "4.2.2 Generative model as View Generator", "content": "In contrast to the recognition approach in the previous section, this part focuses on improving node representation by utilizing Variational Graph Auto-Encoder (VGAE) [51] from the reconstruction respective. VGAE seamlessly combines variational auto-encoder principles with graph generation techniques, offering an efficient and flexible framework to enhance node representations. VGAE consists of two main components: the encoder and the decoder. Encoders typically utilize GCNs to extract node feature representations and project them into a latent space. Specifically, it transforms the node feature vector X and adjacency matrix A into two vectors: a mean vector $\\mu$ and a standard deviation vector $\\sigma$, denoted as $\\mu, log(\\sigma^2) = Encoder(X, A)$. The decoder's role is to rebuild the graph based on features, often by predicting edge connections between nodes using the inner product of their latent representations, denoted as $p(X|Z) = sigmoid (XX^T)$. Here, Z represents the node representation drawn from the latent space, and sigmoid function used to transform the inner product into a probability. This process ensures a smooth and concise logic flow.\nIn Figure 2, we employ a multi-layer GCN as the encoder to capture graph embeddings and calculate both the mean and standard deviation of these embeddings. The decoder, implemented as an MLP, takes these mean and standard deviation values, along with Gaussian noise, to generate a new graph. Lastly, VGAE is end-to-end trained by maximizing the evidence lower bound (ELBO):\n$L_{VGAE} = E_{q(Z|X,A)} [logp(A|Z)] \u2013 KL[q(Z|X, A)||p(Z)]$, (5)\nwhere the first term represents the reconstruction probability, indicating how likely the decoder can reconstruct"}, {"title": "4.3 View Discriminator", "content": "The view discriminator is a user-level attribute classifier for recognizing the generated views. Specifically, the discriminator takes an augmentation view embedding as input and judges whether the view still contains sensitive attributes. Considering the presence of two different generative models, ideally, we should train two discriminators independently. However, this approach weakens the connections between the two generated views and also increases the model's parameters\u00b3. Thus, we merge the two augmented views into one augmented view and use a unified discriminator to simultaneously optimize both augmented representations. More precisely, we employ a parameter-sharing graph encoder f to transform each of the augmented views into a unified embedding space, denoted as $H_1 = f(G_1)$ and $H_2 = f(G_2)$. Subsequently, an MLP model serves as the discriminative network to predict the sensitive information based on the fused user representation, that is:\n$\\hat{s} = MLP (mean (H_u^{(1)}, H_u^{(2)}))$, (6)\nwhere mean denotes mean pooling, MLP is short for multilayer perception. $H_u^{(1)}$ and $H_u^{(2)}$ represent the user embeddings in augmented view $G_1$ and $G_2$, respectively. $\\hat{s} \\in [0, 1]$ is the predicted score of sensitive attribute.\nTo train the discriminator, we define the attribute label of user i as $s_i$, and the classification loss is defined as follows:\n$L_{VD} = -\\frac{1}{|U|} \\sum_{i \\in U}[s_i log \\hat{s_i} + (1 \u2212 s_i) log (1 \u2013 \\hat{s_i})]$, (7)\nWe'll theoretically show that minimizing $-L_{VD}$ encourages view generator to produce fair augmented views.\nAssume the discriminator loss for each sample is bounded, then we show that minimizing $-L_{VD}$ is equivalent to optimizing an upper bound on the group fairness $\\Phi$ in Eq. (2).\nLet $\\hat{s}$ be the predicted score of attribute information. We assume that the discriminator loss for each sample is bounded, meaning there exists a constant $\\delta$ so that $|s_i log \\hat{s_i} + (1 \u2212 s_i) log (1 \u2013 \\hat{s_i})| \u2264 \u03b4$. Together with the concavity of log() function and Jensen's inequality, for any $\\hat{s_i} \\in [0, 1]$ with $|log(\\hat{s_i})| \u2264 \u03b4$, we have:\n$log(\\hat{s_i}) \u2265 -\\frac{\\delta}{1 - e^{-(\\delta - 1)}}$. (8)\nThen, we suppose the proportion of users in $U_{s=0}$ is $r_0$, and the proportion in $U_{s = 1}$ is $r_1$, with $r = max(r_0, r_1)$. Next, we derive group fairness $\\Phi = \\frac{1}{|U_{s=0}|} E_{i\\sim U_{s=0}} [F_i] - \\frac{1}{|U_{s=1}|} E_{j\\sim U_{s=1}} [F_j]$, where $F_i$ is the recommendation quality (e.g.,"}, {"title": "4.4 Learning informative View Generator", "content": "In theory, optimizing Eq. (7) can indeed help the view generator to reduce data unfairness and generate fair node embedding. However, if not properly controlled, it might adopt extreme strategies. For instance, the generators could consistently generate node representations with all features set to zero [9]. While this would eliminate unfairness entirely, it would also remove valuable information from the original data, making the generated data essentially useless. To ensure that the view generator model retains its informativeness, we have also introduced two important loss functions to preserve the most essential information from the original graph.\nTask-Guide BPR Loss. While view generator may generate fair embeddings from different perspectives, there may not be optimization signals to align these generated views with the main task. Thus, we employ the BPR loss, as shown in Eq. (1), to individually optimize each view generator. For the first recognition model:\n$L_{BPR} (u, v^+, v^-) = \u2212 log\u03c3 (\\hat{y}^{(1)}(u,v^+) \u2212 \\hat{y}^{(1)}(u, v^-))$, (10)\nwhere $(u,v^+)$ indicates the positive interaction in $G_1$, $(u, v^-)$ is a randomly chosen negative interaction, $\\hat{y}^{(1)}$ denotes the interaction score from the first recognition model. Then, the $L_{BPR}$ in the second generative model can be derived in a similar manner.\nIn line with existing self-supervised learning paradigms [20], [52], we pull close the representations for the same node in two different views, and push away the embeddings for different entities in the two views. Based on these two augmented embeddings, the\ncontrastive loss is formally defined by:\n$L_{NCE} = \\sum_{i \\in U \\cup V} - log \\frac{exp (cos (H_1 (i), H_2(i)) /\u03c4)}{\\sum_{j\u2260i} exp (cos (H_1 (i), H_2(j)/\u03c4)}$, (11)\nwhere cos() represents the cosine similarity function and $\\tau$ is the temperature hyper-parameter. $H(i) \\in R^d$ denotes the embedding vector in the i-th row. In practice, this contrastive loss $L_{NCE}$ not only ensures the alignment of two views but also enhances the view generator's capability to produce more informative representation.\nHere, we'll show that minimizing contrastive loss can help view generator create more informative views.\nGiven the original graph G and its generative views $G_1$ and $G_2$, along with their corresponding embeddings $H_1$ and $H_2$. Contrastive learning objective is a lower bound of mutual information between G and generative views $G_1, G_2$.\nFirst, for two random variables X and Y, we define the mutual information I(X, Y) as:\n$I(X; Y) = \\sum_{X\\in X,Y\\in Y}p(x, y) log \\frac{p(x, y)}{p(x)p(y)}$, (12)\nwhere $p(x, y)$ is the joint distribution of X and Y, and p(x) and p(y) are marginal distributions. Then, we consider a general InfoNCE contrastive loss, it can be expressed as:\n$L_{NCE} = -E_{p(x,y)} log \\frac{e^{g(x,y)}}{\\sum_{y^-\\neq y}e^{g(x,y^-)}}$, (13)\nwhere (x, y) denotes positive sample pair, and (x, y-) is negative sample pair. g() is a scoring function designed to assign higher scores to positive samples. Following previous works [9], [53], combining jensen's inequality and logarithms and inequalities, we can derive:\n$I(X; Y) \u2265 log (1+e^{-L_{NCE}})$. (14)\nThis shows that minimizing the contrast loss is equivalent to maximizing the lower bound of mutual information of X and Y. Thus, in this work, we can state:\n$-L_{NCE} \u2264 I (H_1;H_2)$. (15)\nIn information theory [54], the data processing inequality states that for a Markov chain consisting of a sequence of random variables $X \u2192 Y \u2192 Z$, it must adhere to the inequality $I(X;Y) > I(X;Z)$. This inequality ensures a logical and concise flow of information. In our proposed FairDgcl, we can observe that G, $G_1$ and $G_2$ follow the relationship $G_1 \u2192 G\u2192 G_2$. Considering that $G_1$ and $G_2$ are conditionally independent when G is observed, this scenario is Markov equivalent to the chain $G_1 \u2192 G \u2192 G_2$. Therefore, we have $I (G_2;G) > I (G_1;G_2)$. In the given context, we also observe that the mutual information between multiple views and the original graph is not less than the mutual information between a single view and the original graph [53]. From this, we can deduce the following inequality: $I(G;G_1, G_2) \u2265 I(G;G_2)$. Finally, by combining the"}, {"title": "4.5 Overall Optimization", "content": "The overall training process of our proposed model comprises three components: the main task's BPR loss function, the loss function for the view generator, and the loss function for the view discriminator. To be specific, the view generator consists of two contrastive views, with its loss function $L_{VG}$ can be expressed as follows:\n$L_{VG} = L_{VGAE} + L_{BPR} + L_{BPR} + \u03b1L_{NCE}$, (17)\nwhere \u03b1 is hyper-parameters that control the strength of contrastive section. Finally, by combining $L_{VD}$ and $L_{BPR}$, these three components adhere to the mini-max adversarial optimization as follows:\n$min_{\u03b8_g,\u03b8_f}max_{\u03b8_d} L = min_{\u03b8_g,\u03b8_f}max_{\u03b8_d} L_{BPR} + L_{VG} - \u03b2L_{VD}$, (18)\nwhere $\u03b8_f$ is the parameter set of graph encoder5. $\u03b8_g$ and $\u03b8_d$ denote the parameter set of view generator and view discriminator, respectively. \u03b2 regulates the impact of view discriminator. The parameters of $\u03b8_g, \u03b8_f$, and \u03b1 are optimized together using a min-max optimization procedure. During each training step, we begin by minimizing L through updating $\u03b8_g$ and $\u03b8_f$, with $\u03b8_d$ held constant. Subsequently, we maximize L by updating \u03b1 while keeping $\u03b8_g$ and $\u03b8_f$ fixed. The overall training details are presented in Algorithm 1.\nComparison with Existing Methods. Learning fair user representation in recommender system has become a vital research focus, with methodologies ranging from model optimization to the use of auxiliary interaction data. Our"}, {"title": "5 EXPERIMENTS", "content": "In this section, we present empirical results to illustrate the effectiveness of our proposed FairDgcl framework. The experiments are designed to address the following four research questions:\n\u2022 RQ1: How does the performance of our proposed model compare to that of various baselines?\n\u2022 RQ2: How do the key components of the proposed FairDgcl framework affect the overall performance?\n\u2022 RQ3: How sensitive is the performance of FairDgcl to variations in its hyper-parameters?\n\u2022 RQ4: Do noticeable variations exist in the representation of distinct user demographics within our model?"}, {"title": "5.1 Experimental Settings", "content": "We conduct experiments on four benchmark datasets: Movielens-1M6 (ML-1M), Movielens-100K7 (ML-100K), Last.FM8, and IJCAI-20159, which are widely used in fairness-aware recommendations [12], [25], [38]. The dataset statistics are summarized in Table 2. ML-1M and ML-100K [55], are two popular movie datasets for recommender systems, which encompass comprehensive records of useritem interactions and user profile information. Last.FM [56] is a music dataset collected from Last.fm music website that contains users' ratings to artists. IJCAI-2015 [57] is sourced from Tmall.com and focuses on e-commerce activities. It"}, {"title": "5.1.3 Evaluation Protocols", "content": "As our primary focus on trade-off between recommendation fairness and accuracy, we need to evaluate two aspects and report the trade-off results. We use the all-rank evaluation protocol, which tests and ranks both the positive items and all non-interacted items for each test user in the test set. Firstly, for recommendation accuracy evaluation, we adopt commonly-used Recall@K and Normalized Discounted Cumulative Gain (NDCG)@K as metrics. Larger values of Recall and NDCG indicate better recommendation accuracy performance. Then, we use group fairness I (in Eq. (2)) to evaluate fairness. \u03a6R (\u03a6N) refers to the difference in terms of recall (ndcg) performance in different user groups. It's important to highlight that smaller values for PR and \u03a6N indicate better fairness results, Following [12], we set the value of K to {10, 20, 30}."}, {"title": "5.1.4 Implement Details", "content": "In this experiment, we use the PyTorch framework to implement FairDgcl. For parameter inference, we employ the Adam [58] optimizer with a learning rate of 1e-3. The default hidden dimensionality is set to 64, and the training epoch is set to 200. The hyper-parameters are determined based on grid search. In more detail, the number of graph neural iterations is tuned from {1,2,3,4}, and the batch size are tuned in the ranges of [512, 1024, 2048, 4096]. In the model training phase, \u03b1 and \u03b2 are respectively chosen from {0.0001, 0.001,0.01, 0.1, 1}. We fine-tune the parameters of all baseline methods and report the best results."}, {"title": "5.2 Experimental Results (RQ1)", "content": "We compare FairDgcl with different baselines on four realworld datasets to evaluate its effectiveness. To ensure a fair comparison, we try to maintain consistent recommendation accuracy across all baselines. The results are presented in Table 3 and our observations can be summarized as follows:\nOverall, our proposed FairDgcl model consistently outperforms others in terms of both group fairness scores and recommendation results across the majority of validation metrics. This not only validates the model's exceptional performance in ensuring fair recommendations but also highlights its remarkable recommendation quality, further confirming its effectiveness. These improvements can be attributed to two key factors. Firstly, FairDgcl employs an adversarial learning framework, comprising a view generator and a view discriminator, to automatically generate fair view through a min-max game, thereby improving fairness. Secondly, the introduction of contrastive learning paradigm with informative contrastive views enhances the model's informativeness. These strategies collectively contribute to our model achieving superior fairness and accuracy.\nAmong the fairness-aware baseline models, Graphair stands out as the top performer, just confirming the importance of dynamic data augmentation in fairness-aware recommender systems. However, in most cases, Graphair slightly falls short of our model's performance. This discrepancy may be attributed to Graphair's focus solely on edge deletion augmentation, which results in a somewhat coarser method design, overlooking the augmentation of other perspectives and consequently leading to suboptimal performance. In contrast, our model employs two different augmentation approaches based on the contrastive learning framework, effectively enhancing model's performance. As for the CAMUS model, it typically yields improved fairness outcomes, but its recommendation accuracy tends to be unstable. The same holds true for FairDrop, as they both employ heuristic strategies to improve interaction samples. This entails a laborious trial-and-error process to determine the optimal augmentation level for various datasets, making it challenging to achieve consistent improvements.\nWhen compared to the base model on the ML-100K dataset, we do notice a slight drop in recommendation accuracy (i.e., K=20 and K=30). One possible reason is that there are relatively sparse interactions within this dataset, which may lead model to unintentionally remove some valuable interactions during the adversarial learning process. Nevertheless, when considering the gains in fairness performance, the reduction in accuracy remains minimal."}, {"title": "5.3 Ablation Study (RQ2)", "content": "To investigate the specific contributions of different components, we conduct a more in-depth analysis of FairDgcl and perform ablation studies. It should be noted that if the module \"RM\u201d or \u201cGM\u201d are removed, FairDgcl will degrade to Graphair. The ablation results are presented in Figure 3. First, we can observe that removing specific modules consistently results in a decrease in the model's fairness and accuracy across most scenarios. This emphasizes the crucial role played by various model components in determining the overall performance results. More specifically, within the majority of datasets, the decrease in fairness exhibits a more significant impact compared to the decrease in accuracy. This is unsurprising, given that these modules are primarily crafted with a focus on enhancing fairness.\nThen, when delving deeper into the comparison between FairDgcl and its variants on ML-1M, a notable observation emerges: the accuracy performance exhibits inconsistency when compared to other datasets. The decrease in accuracy is likely due to a greater variance in gender distribution when compared to other datasets. Eliminating the fairnessrelated module could potentially strengthen the model's focus on serving the primary user groups. This could lead to better prediction accuracy for these users, ultimately contributing to an overall improvement in accuracy."}, {"title": "5.4 Parameter Sensitivity (RQ3)", "content": "In this section", "hyper-parameters": "model depth"}]}