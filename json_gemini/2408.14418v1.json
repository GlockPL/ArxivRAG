{"title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues", "authors": ["Kuluhan Binici", "Abhinav Ramesh Kashyap", "Viktor Schlegel", "Andy T. Liu", "Vijay Prakash Dwivedi", "Thanh-Tung Nguyen", "Xiaoxue Gao", "Nancy F. Chen", "Stefan Winkler"], "abstract": "Automatic Speech Recognition (ASR) systems are used to transcribe speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization. This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions. Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts. To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs). Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings. Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems. This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization.", "sections": [{"title": "1 Introduction", "content": "Automatic Speech Recognition (ASR) (Yu and Deng 2016) is the task of transcribing speech signals into text, enabling a wide range of applications from voice-activated assistants to automated customer service systems. ASR systems significantly aid various downstream tasks such as dialogue summarization (Zhong et al. 2022), where the goal is to distill key information from spoken interactions. However, errors introduced by ASR systems can degrade the performance of these summarization tasks (Li et al. 2014; Guo et al. 2024), which limits their application in high-stake domains where correctness of the summaries is important. The synthesis of Electronic Medical Records (EMRs) from doctor-patient dialogues (Krishna et al. 2021) is among such summarization tasks where accuracy is critical. Here, ASR errors can lead to inaccuracies in transcribing clinical terms, medication, or procedure names, resulting in erroneous medical notes (Hodgson and Coiera 2016), which can lead to misdiagnosis, incorrect treatment plans, and potentially harmful patient outcomes.\nOne way to improve dialogue summarization is to improve the ASR systems. However, this requires large amounts of supervised data (Radford et al. 2022), which is often unavailable in the healthcare domain due to privacy and ethical concerns surrounding the recording of doctor-patient conversations. Consequently, practical clinical summarization systems often treat ASR systems as black boxes, mandating that overall improvements must arise either from post-processing ASR outputs as a separate step, or making down-stream methods robust to ASR errors. Recent works propose post-processing ASR outputs using LLMs to correct erroneous ASR transcripts (Radhakrishnan et al. 2023; Bai et al. 2024). Nonetheless, based on prior studies, using prompting techniques to clear noise only proves effective only when using LLMs of large sizes, typically those that exceed 100B parameters (Yang et al. 2023). Moreover, LLMs are prone to hallucinations (Maynez et al. 2020), which can introduce irrelevant symptoms or medication names into the dialogue transcript, potentially degrading summarization quality while attempting to clear ASR noise.\nAlternatively, the summarization robustness to ASR errors can be improved through data augmentation techniques (Fabbri et al. 2021). However, the conventional augmentation approach of exposing the summarization model to erroneous ASR dialogues during the training phase is not feasible either, again due to the limited availability of medical dialogue audio preventing the generation of ASR dialogue transcripts (Nanayakkara et al. 2022). Heuristic approaches for augmentation, such as randomly applying a set of corruption operations, are not ideal either, as they fail to accurately mimic ASR errors both qualitatively and quantitatively, causing the augmented training data distribution to diverge from the real test distribution (Wang et al. 2020).\nRecognizing these limitations, we propose the use of LLMs to generate synthetic dialogues mimicking real ASR transcriptions with their characteristic errors, as a means for data augmentation. To circumvent hallucinations, we rely on the signal from downstream tasks during fine-tuning on these generated transcripts. As such, if new domain-specific entities are mistakenly hallucinated during the augmentation process, the fine-tuning phase ensures that the model learns to disregard these irrelevant entities, increasing the feasibility of this approach as a result. To accommodate for the scarcity of medical audio recordings, we leverage the in-context learning (Brown et al. 2020) capabilities to specialize LLMs for the task of ASR transcript generation based on a few descriptive examples. Specifically, using Primock57 dataset (Papadopoulos Korfiatis et al. 2022), which includes audio recordings of clinical visits alongside their corresponding human-transcribed text, we first produce noisy transcriptions using ASR models. These noisy ASR dialogue transcripts are then paired with their clean human-transcribed versions to form the few-shot examples needed for effective in-context learning.\nWhile in-context learning enables the qualitative approximation of ASR errors such as phonetic confusions, ensuring that synthetic errors are quantitatively similar remains a challenge (Everson et al. 2024). To address this, we first analyze the error profile of ASR models by measuring their word-error-rates and the distribution of error types, including insertions, deletions, and substitutions. Subsequently, we introduce a novel description syntax that instructs the LLMs on where to make realistic errors and what types of errors to introduce. By tagging the inputs with appropriate error tags based on the measured noise profiles of the target ASR models, we ensure that the synthetic errors generated are both qualitatively and quantitatively similar to real-world ASR errors. This methodology allows us to create synthetic noisy dialogues that accurately reflect ASR error patterns, which can be used for effective data augmentation in training robust summarization models.\nOur experimental evaluation reveals that large language models (LLMs) are effective noise modelers, capable of producing errors similar to those found in transcripts produced by ASR. This is evident from the qunatitative and qualitative similarities between the word error profiles of the synthetic dialogues we generate and actual ASR dialogue transcripts. Such similarity also reflects on the downstream summarization performance, mirroring the performance drop caused by ASR errors. Lastly, when we utilize these synthetic noisy dialogues to augment the training set of the summarization models, the performance on the noisy test set improves by up to 16%, indicating enhanced robustness against ASR errors. In summary, our contributions leading to the robustness are:\n\u2022 We utilize the in-context learning ability of LLMs to create synthetic errors that closely resemble those present in ASR transcriptions. This method is used as a data augmentation strategy to improve the strength of dialogue summarization models in situations where audio recordings are limited.\n\u2022 We introduce an error tagging syntax designed to accurately match the error patterns of real ASR transcriptions in the generated synthetic dialogues. This syntax provides detailed control over the type and amount of errors added, ensuring that the synthetic data closely aligns with real-world ASR errors."}, {"title": "2 Related Work", "content": "Dialogue Summarization Summarizing human conversations or speech has been a longstanding challenge that has received significant attention over the years (Hori and Furui 2003; Liu and Hakkani-T\u00fcr 2011). An important subset of these endeavors involves ASR-based summarization, where automatic speech recognition systems are used to transcribe spoken dialogues before summarization (Zhong et al. 2022). Recently, this challenge has extended to clinical applications, including the summarization of patient-doctor conversations (Krishna et al. 2021), utilizing LLMs as tools for text comprehension (Le-Duc et al. 2024).\nASR Error Correction ASR models are prone to producing erroneous dialogue transcripts, especially in challenging environments where the conversation is disrupted by noise. These errors can significantly degrade the quality of downstream tasks like summarization using ASR transcriptions. Recent advancements in LLMs have shown promise in addressing these ASR errors. For instance, Yang et al. (2023) investigated prompting techniques in correcting ASR errors. Radhakrishnan et al. (2023) introduced Whispering LLaMA, a cross-modal generative error correction framework that fuses acoustic information with linguistic representations. Bai et al. (2024) proposed a seed-based method that enhances ASR outputs by integrating linguistic context during correction, effectively reducing error propagation in downstream tasks. Hu et al. (2024) introduced a method that leverages the inherent noise in audio signals to generate robust language embeddings. To provide a standard benchmark for evaluating ASR error correction methods, Chen et al. (2024) introduced the Hyporadise dataset. These techniques are unsuitable for the medical domain because fine-tuning denoising models is often impractical due to the lack of public medical dialogue audio recordings (Nanayakkara et al. 2022). Additionally, prompting without fine-tuning is only effective for large models (Yang et al. 2023), which might be impractical, due to privacy concerns when using public APIs or resource constraints when hosting locally.\nData Augmentation Data augmentation (DA) is crucial for enhancing model performance by diversifying training examples. Traditional DA methods in NLP, such as paraphrasing (Sharma et al. 2023), back-translation (Sugiyama and Yoshinaga 2019), and noise injection (Wang et al. 2018), have been effective in some scenarios, but may not fully capture the complexity of real-world variations. Recent LLM advancements have revolutionized DA by using models like GPT-3 and GPT-4 to generate high-quality synthetic data. For instance, Chintagunta et al. (2021) use in-context learning with LLMs to create synthetic medical dialogue summaries, while Dialogic (Li et al. 2022) employs LLMs to generate annotated dialogues with automatic verification and revision. DA is also used to improve robustness against adversarial examples, as used byLiu et al. (2020), who apply adversarial training to make sentiment analysis models more resilient to noise and variations. Our method differs by addressing the challenges of augmenting medical dialogue data where real ASR transcripts are scarce. Instead of relying on heuristic corruptions that don't mimic ASR errors ac-"}, {"title": "3 MEDSAGE", "content": "We use LLMs to generate realistic ASR noise. On a high level, we pair human-transcribed sentences with their ASR counterparts as in-context examples to an LLM, which we instruct to generate noisy sentences (Section 3.1). These examples convey the qualitative aspects of the ASR errors such as phonetic confusions. To also ensure that the errors in the generated synthetic dialogues quantitatively match that of the ASR transcriptions, we propose a controlled generation strategy (Section 3.2). Our strategy involves introducing a tagging syntax that instructs the LLM on the specific word locations to corrupt with specific types of errors. This enables the generation of synthetic dialogues with any arbitrary error distribution that mirrors the characteristics observed in real ASR transcriptions. An overview of our MEDSAGE pipeline is given in Figure 1."}, {"title": "3.1 Error Generation using In-context Learning", "content": "To generate synthetic noisy dialogues, we first use a specialised system prompt to instruct the LLMs about the error generation task. Later we form the in-context examples by pairing sentences from human-transcribed clean dialogues with their ASR-generated counterparts. These examples are subsequently provided to the LLMs to convey the characteristics of ASR errors that we are trying to mimic. An example query that illustrates our structure when prompting the noise-generating LLMs is displayed in the following colored box. As seen from the example, the clean sentences are given as inputs and the responses are expected to contain erroneous versions as if they were obtained through ASR models. The curly braces surrounding certain substrings are a part of our error tagging system, which is detailed later in Section 3.2, and they denote which words or substrings are transcribed wrongly by the ASR models and how. For instance, in the example provided, the word \"Tylenol\" is wrongly transcribed as \u201ctie-and-all\", highlighting the common issue of ASR systems confusing tokens with similar pronunciation. Lastly, the input at the end contains the actual clean sentence that we aim to corrupt. The words to be corrupted are also indicated to the LLM through the use of curly braces.\""}, {"title": "3.2 Controlled Generation", "content": "For data augmentation to be effective in improving performance on noisy test data, the generated synthetic data must closely follow the error patterns found in real-world ASR outputs. However, solely relying on in-context learning with a few examples often fails to capture the nuanced distribution of errors produced by ASR models. To overcome this limitation, we propose a controlled noise injection mechanism. This is to, ensuring the synthetic data aligns with the error profile of the target ASR model. This approach relies on a detailed analysis of the types and frequencies of errors in ASR transcripts. Using these insights, we then guide the noise generation process by conditioning the LLM with an error tagging syntax.\nASR Error Profiling: To quantitatively represent the error profile of ASR models, we focus on three primary error types: insertion, deletion, and substitution, which collectively contribute to the calculation of Word Error Rate (WER). To estimate the distribution of these errors, we first transcribe a set of medical conversation dialogues using the ASR models. The transcriptions are then aligned with human-annotated ground truth using the Wagner-Fisher algorithm (Wagner and Fischer 1974). This alignment allows us to and quantify the occurrences of each specific error type produced by the ASR model.\nWe estimate the probability distribution of errors as follows. Let $c_i$ denote the event that the word at index i is corrupted. The probability $p(c_i)$ of a word being corrupted is defined as the WER of the ASR model i.e., $p(c_i) = WER$. Given that a word is corrupted, the probability of a specific error type $e_t$ can be expressed as $p(e_t|c_i)$, where $e_t \\in \\{insertion, deletion, substitution\\}$. These conditional probabilities represent the distribution of different error types observed in the ASR model's output (See Section 5.2). We use this error distribution to formulate our tagging system.\nTagging System We develop a tagging system to instruct the LLM on the specific word-level corruptions to perform to be able to have more control on the WER of generated noise and the distribution of error types. Specifically, we employ the following tags to indicate the error-type that the model should simulate at the word level.\n\u2022 Words enclosed in curly brackets {} should be replaced with phonetically similar words. For instance, {wheezy} might be replaced with {weesy}, and {Tylenol} could be changed to {tie-and-all}.\n\u2022 The tag  indicates where a new word should be inserted. These new words should be general in nature and should not introduce new domain-specific terminology such as drug names or symptoms.\n\u2022 We do not specify any tags for deletion; instead, words that need to be deleted are simply removed from the text.\nThis meaning of the tagging system is conveyed to the the model both through the system prompt and in-context examples. To decorate the in-context examples with error tags accordingly, we align the clean and noisy examples pairs using the Wagner-Fisher algorithm and determine the locations of word errors along with their types. During inference, these tags are randomly applied on the ground-truth transcripts based on the estimated error distribution of the target ASR model. The probability of tagging a word at index i with a specific error type is thus given by:\n$p(e_t|c_i).P(c_i)$"}, {"title": "Algorithm 1: Generating Synthetic Noisy Dialogues Using LLMs", "content": "1: Input: Clean transcripts $T_{cln}$, in-context example pairs $E_{in}$, LLM\n2: Output: Synthetic noisy dialogues $T_{syn}$\n4: #Create in-context examples\n5: for each pair $(t_{cln}, t_{ASR})$ in $E_{in}$ do\n6:Compute word-locations and types of errors $e_i$, $e_t$\n7:$(t_{cln}, t_{ASR})'$ insert_tags$(t_{cln}, t_{ASR}, e_i, e_t)$\n8: Replace $(t_{cln}, t_{ASR})$ with $(t_{cln}, t_{ASR})'$ in $E_{in}$\n9: end for\n11: Initialize $T_{syn} \\leftarrow \\emptyset$\n13: #Prompt LLM with in-context examples\n14: for each input dialogue $t_{cln}$ in $T_{cln}$ do\n15: Sample error indexes $e_i \\sim p(c_i)$\n16: Sample error types $e_t \\sim p(e_t | C_i = e_i)$\n17: $t'_{cln}$ insert_tags$(t_{cln}, e_i, e_t)$\n18: $t_{syn} \\leftarrow LLM(prompt, t'_{cln})$\n19: $T_{syn} \\leftarrow T_{syn} \\cup t_{syn}$\n20: end for"}, {"title": "4 Experiment Settings", "content": "This section presents our experimental evaluation of the proposed method for generating synthetic noisy dialogue transcripts to improve the robustness of summarization models against ASR errors.\nASR models: We utilized the Whisper tiny, Whisper large (Radford et al. 2023), and Wav2vec2-base (Baevski et al. 2020) ASR models to generate transcriptions of medical dialogues.\nLarge Language Models: We use Llama-3-8B (Dubey et al. 2024) and Mistral-7B (Jiang et al. 2023) both for generating synthetic dialogues and summarization, while Gemma-7B (Team et al. 2024) is only used for summarization.\nDatasets: For testing our approach, we use the Primock57 dataset, which comprises audio recordings of 57 enacted doctor-patient dialogues and their text transcripts written by human annotators. For experiments involving fine-tuning we use the NoteChat-1000 dataset (Wang et al. 2024) for training, which includes 1000 synthetic doctor-patient dialogue transcripts generated by multiple LLMs in a cooperative roleplay setting, conditioned on clinical notes.\nEvaluation Metrics: We utilized three types of evaluation metrics to assess the performance of the summarization models. For lexical similarity, we used the ROUGE metrics (Lin 2004), specifically focusing on ROUGE-L, which measures the longest common subsequence overlap between generated and reference summaries. To capture the semantic similarity, we employed BERTScore (Zhang et al. 2020), which uses embeddings from pre-trained BERT models to compare the contextual meaning of the texts. Additionally, recognizing the importance of accurately identifying medical terminology in summaries, we assessed the overlap of domain-specific named entities (DSEs) using the F1 score,"}, {"title": "5 Preliminary Experiments", "content": "In this section, we present preliminary experiments that lay the foundation for our work. First, we conduct a motivating study that verifies how errors introduced by ASR systems negatively impact the performance of a downstream medical dialogue summarization task. Subsequently, we perform an analysis that suggests different ASR models exhibit distinct error profiles."}, {"title": "5.1 ASR noise harms medical report quality", "content": "Our findings, as displayed in Table 1, reveal that ASR noise can significantly degrade the quality of the generated summaries, especially when using small ASR models. Specifically, using Wav2vec2-base generated transcripts instead of human annotated ones lead to a noticeable reduction in the domain-specific entity F1 score of 23% (22.11 \u2192 16.99). While using larger models like Whisper-large exhibits comparable downstream task performance as using ground truth dialogues, deployment area of such large-scale models is limited due to the computational resource requirements they demand. Such a difference in the DSE overlap measure indicates a loss in accurately capturing critical medical entities. The ROUGE-L scores also decline, reflecting reduced textual overlap and coherence between the generated and reference summaries. Moreover, the BERT Scores drop, suggesting a decrease in the semantic similarity to the reference summaries. These results underscore the challenges of downstream tasks, such as dialogue summarisation, face, arising from erroneous ASR transcripts.\nWe also explored applying few-shot denoising on ASR transcribed dialogues, which is denoted as \u201c+ Denoising\u201d. However, the results show that this method does not recover the summarization performance, highlighting the limitations of traditional post-processing based noise reduction techniques in this context."}, {"title": "5.2 Different ASR models exhibit different error profiles", "content": "We analyzed the errors made by the ASR models on Primock57 audio samples. Whisper-large transcription results in 25% WER. Both the Whisper-tiny and Wav2vec2-base models had similar WER scores, with the former achieving 44% and the latter 45%. Moreover, the breakdown of error types associated with each ASR model is displayed in Figure 2. The differing noise profiles suggest that to accurately mimic the properties of ASR transcriptions, the synthetic dialogue generation process must be controllable and adjustable with respect to the error profile of the target ASR model."}, {"title": "6 Main Experiments", "content": "Building on the insights from our preliminary experiments, we present our main experimental findings in this section."}, {"title": "6.1 Data augmentation using synthetic dialogues improves robustness against ASR errors", "content": "To assess the impact of MED SAGE, we first investigate the effectiveness of LLM-generated synthetic dialogues in building robustness against ASR errors through data augmentation. In this experiment, we begin by augmenting the training set of the note chat-1000 dataset using synthetic dialogues. Later, we fine-tune summarization models through LoRa (Low-Rank Adaptation) adapters (Hu et al. 2021) on augmented datasets that include a mix of clean ground-truth dialogues and our synthetic noisy dialogues. Then we compare the summarization performance of these fine-tuned models against three baselines on the ASR-transcribed Primock57 audio recordings, which comprise our test set. The three baselines we use for evaluation are:\n\u2022 Zero-shot: Pre-trained LLMs are prompted to replicate medical notes written by doctors based on input dialogues.\n\u2022 FT on clean: LLMs are fine-tuned only on clean transcripts.\n\u2022 Denoising: The ASR transcripts are cleaned by LLama-3-8B model before summarization.\nAs shown in Table 2, the results indicate that the inclusion of synthetic noisy dialogues in the training set considerably improves the robustness of the summarization models, resulting in up to 16.4% improvement in F1. In contrast, FT on clean baseline only marginally improves or even degrades performance depending on the summarization model architecture. This suggests that while fine-tuning on clean dialogues alone can benefit summarization models by adapting them to the medical domain, it does not consistently enhance their ability to handle ASR noise. Moreover, denoising consistently harmed summarization performance across all experiments. This can be attributed to the two aforementioned key factors: (1) denoising models below a certain parameter size (such as LLama-8B) are ineffective at cleaning noise, and (2) using prompting techniques without fine-tuning introduces a risk of hallucinations, potentially injecting irrelevant medical entities into the dialogue."}, {"title": "6.2 LLM-produced synthetic errors are realistic", "content": "To ensure that the improvements observed indeed stem from the accurate simulation of ASR noise, we need to establish whether the synthetic dialogues generated by our MEDSAGE method exhibit error characteristics similar to those found in actual ASR transcriptions. This assessment involves two main comparisons.\nQualitative Comparison To illustrate that synthetic dialogues produced by our MEDSAGE method properly mimics the errors caracteristics that real ASR transcriptions exhibit, we present randomly selected snippets from doctor-patient conversations in Figure 4. The commonalities between the nature of errors present in both utterances, such as phonetic confusions underscore the accuracy of the noise injections. For instance, the words \"white spots\" are confused with \"whish spits\". This qualitative evidence supports our quantitative findings, demonstrating that LLM-generated synthetic noise can replicate the nuanced flaws typically seen in ASR outputs."}, {"title": "6.3 Error tagging system enables controllable generation", "content": "Lastly, we validate that our method is capable of adjusting the injected noise rate through the use of corruption tags, as detailed in the methodology section. To this end, we insert corruption tags at various rates to simulate increasing amounts of noise. We then record the resulting transcription quality and summarization performance for each noise level.\nAs depicted in Figure 5, we observe a clear trend: as the noise rate increases, the quality of the generated summaries decreases. This is evidenced by a gradual drop in the summarization quality quantified by the scores on various metrics. This analysis demonstrates the controllability of our noise generation method, which is essential for adapting the synthetic noise generation to errors made by various different ASR models with ease."}, {"title": "7 Conclusion", "content": "This study highlights the critical role of Automated Speech Recognition (ASR) technology in transcribing medical dialogues and the consequent impact of ASR errors on downstream summarization tasks. Recognizing the challenges posed by limited availability of supervised data, we explored a novel approach to improve summarization models through"}, {"title": "Appendix A: Computing Infrastructure", "content": "The specifications of the system used to run computational experiments are:\n\u2022 Ubuntu 22.04.4 LTS\n\u2022 NVIDIA A100 80GB GPUS\n\u2022 Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz"}, {"title": "Appendix B: Additional Experiment Details & Results", "content": "Here we present additional qualitative and quantitative experimental results that extend beyond those given in the main paper.\nHuggingface Models Used in Experiments\nFor the experiments resulting in Table 2 of the main paper, we fine-tuned the following model checkpoints retrieved from Hugging Face:\n\u2022 meta-llama/Meta-Llama-3-8B-Instruct\n\u2022 mistralai/Mistral-7B-Instruct-v0.2\n\u2022 unsloth/gemma-7b-it\nFor the remaining experiments, we used AWQ quantized checkpoints of the same models:\n\u2022 solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\n\u2022 TheBloke/Mistral-7B-Instruct-v0.2-AWQ\nFor inference, we primarily used the Text Generation Inference (TGI) framework\u00b9. For fine-tuning, we utilized Unsloth\u00b2.\nExtended Evaluation Using Other Metrics\nThe experimental evaluation in the main paper primarily reports F1 score over domain-specific entities, Rouge-L, and Bert score. Here, we extend this evaluation by presenting results using additional metrics such as Precision, Recall, Rouge-1, and Rouge-2.\nDenoising Models can Hallucinate\nAs outlined in the main paper, one significant challenge associated with using LLMs for ASR error correction, without fine-tuning, is their inclination to hallucinate, or generate content that is not present in the original input. This issue can severely degrade the quality of medical reports that are generated based on dialogues that are pre-processed by denoising models."}]}