{"title": "Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components", "authors": ["Peiyong Wang", "Casey R. Myers", "Lloyd C. L. Hollenberg", "Udaya Parampalli"], "abstract": "Artificial Intelligence (AI), with its multiplier effect and wide applications in multiple areas, could potentially be an important application of quantum computing. Since modern AI systems are often built on neural networks, the design of quantum neural networks becomes a key challenge in integrating quantum computing into AI. To provide a more fine-grained characterisation of the impact of quantum components on the performance of neural networks, we propose a framework where classical neural network layers are gradually replaced by quantum layers that have the same type of input and output while keeping the flow of information between layers unchanged, different from most current research in quantum neural network, which favours an end-to-end quantum model. We start with a simple three-layer classical neural network without any normalisation layers or activation functions, and gradually change the classical layers to the corresponding quantum versions. We conduct numerical experiments on image classification datasets such as the MNIST, FashionMNIST and CIFAR-10 datasets to demonstrate the change of performance brought by the systematic introduction of quantum components. Through this framework, our research sheds new light on the design of future quantum neural network models where it could be more favourable to search for methods and frameworks that harness the advantages from both the classical and quantum worlds.", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML), or more generally, Artificial Intelligence (AI), aims to develop AI agents and systems that could simulate or even surpass human intelligence. With the promise of quantum computing and quantum advantage in many other fields, such as quantum chemistry and quantum optimisation, there has been rising interest on the application of quantum computing to the research and development of AI and machine learning. However, as of now, with the current state of quantum computing, it is still a contentious issue whether it is truly useful to introduce quantum computing to AI. Some research, originating mainly from the quantum computing community, argues that quantum computing has advantages in machine learning and AI, such as speed-up for both statistical machine learning algorithms  and modern deep neural networks , as well as memory advantages in sequence learning tasks . There are also some doubts about the current research paradigm of combining quantum computing and AI to harness quantum advantage for both runtime acceleration and/or performance boost, ranging from whether the conventional notion of quantum advantage is the right goal for quantum machine learning , to results showing that quantum machine learning models rarely outperform the corresponding off-the-shelf classical machine learning models . Most of the time, the quantum machine learning models studied in , such as quantum neural networks (QNN), perform poorly compared to the classical multilayer perceptron and simple convolutional neural network.\nUnlike quantum kernel machines , which replace the classical kernel function with kernel functions calculated via the evaluation of quantum circuits, the correspondence between classical and quantum neural networks is less straightforward. Classical neural networks (NN) have a clear hierarchical layered structure. Different layers often deal with different levels of features. For example, in a convolutional neural network for image classification, layers close to input data learn low-level features such as shapes and edges, while layers close to the output layer learn high-level features related to semantic concepts in the images . However, most quantum neural networks in today's research lack such hierarchical structure, especially for those that follow a sandwich \"Data Encoding \u2192 Quantum Process \u2192 Results Readout\" structure. From the perspective of classical neural network architectures, no matter how many \"quantum layers\" are there in the middle of the sandwich, it is still a single layer since all those quantum layers could be represented with a single linear map. This leads to an inherent disadvantage when directly comparing most of the quantum neural network models in current research with the corresponding off-the-shelf classical neural networks."}, {"title": "2 Methods", "content": "In this paper, we explore these issues by focussing on the transition from a fully classical model to a classical-quantum hybrid model, HybridNet, in which the layers are realised via (simulated) quantum circuits, but the information flow between layers remains classical. As our main contribution, we propose such a gradual transition strategy for benchmarking the effectiveness of quantum neural network layers for particular tasks. We proposed a novel trainable quanvolution  kernel, FlippedQuanv3x3, based on the flipped model for quantum machine learning . We also adopt the data reuploading circuit , combined with the Hamiltonian embedding of classical data , as introduced in , DataReUploadingLinear, to mimic the effect of a classical linear (dense) layer.\nThis paper is organised as follows. In Section 2, we introduce the framework of the gradual transition strategy with a simple classical neural network as an example. We also introduce the details for the implementations of FlippedQuanv3x3 and DataReUploadingLinear in Section 2. In Section 3, we investigate the performance of our hybrid model by numerical experiments on three famous image classification datasets: MNIST, FashionMNIST and CIFAR-10, and analyse the results. We discuss the general implications of our results and possible future directions in Section 4."}, {"title": "2.1 Let the Quantum Creep In", "content": "Most of the quantum machine learning models, especially quantum neural networks, are end-to-end quantum. These QNN architectures can be written in the form of a parameterised unitary\n$$f(x; \\theta) = \\langle x|O(\\theta) |x\\rangle,$$\nwhere $|x\\rangle$ is the quantum encoding of classical input data $x$, $O(\\theta)$ is an observable parameterised by $\\theta$, formulated by a unitary quantum circuit parameterised by $\\theta$ absorbed into some cost-function related observable $O$. This can also be formulated in the form of a quantum channel (such as the quantum convolutional neural network ):\n$$f(x; \\theta) = Tr[O\\rho(x; \\theta)],$$\nwhere\n$$\\rho(x; \\theta) = \\sum_i K_i(\\theta) \\rho_x K_i(\\theta)^\\dagger,$$\n$ \\rho_x$ is the quantum encoding of classical input data $x$ and $\\sum_i K_i(\\theta)^\\dagger K_i(\\theta) = I$ are the Kraus operator representation of the quantum channel parameterised by $\\theta$.\nBoth of these QNN architectures lack the hierarchical layered structure that commonly exists in classical neural networks, giving these models a major disadvantage compared to off-the-shelf classical neural network models. Hence, it would be hard to determine whether the lower performance of QNN models compared to classical NN models is due to the absence of a layered structure, or the intrinsic ineffectiveness of the quantum model.\nTo address this problem, we adopt an approach in which we gradually replace the layers of classical neural networks with quantum layers that mimic the behaviour"}, {"title": "2.2 Flipped Quanvolution", "content": "The flipped model was first proposed in :\n$$f_\\theta(x) = Tr[\\rho(\\theta)O(x)],$$\nwhere $\\theta$ is the set of trainable parameters and $x$ is the input data. It exchanges the position of the input data $x$ and trainable parameters $\\theta$ compared to the common form in Eqn. 1, shifting the data to the observable side and the parameters to the initial state side. If the parameterised initial state is a pure state, i.e. $\\rho(\\theta) = |\\varphi(\\theta)\\rangle \\langle \\varphi(\\theta)|$, then $f_\\theta(x)$ becomes\n$$f_\\theta(x) = \\langle \\varphi(\\theta)| O(x) |\\varphi(\\theta) \\rangle.$$\nFor the two-qubit case, if we let\n$$|\\varphi(\\theta)\\rangle = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\end{bmatrix},$$,\nwhere $a, b, c, d \\in \\mathbb{C}$, and\n$$O(x) = \\frac{1}{\\sqrt{2}} (xI + M_x^T),$$,"}, {"title": "2.2.1 Circuit Implementation", "content": "To prepare the parameterised two-qubit quantum state $|\\varphi(\\theta)\\rangle$, we adopt the SU(N) unitary for convenience in implementation:\n$$SU(N)(\\theta) = exp\\Big(-i\\sum_{i=1}^m \\theta_i G_i\\Big),$$,\nwhere $m = 4^n - 1$, $N = 2^n$, $n$ is the number of qubits in the circuit, and $G_i \\in {I, X, Y, Z}^{\\otimes n}\\setminus{I^{\\otimes n}}$, $\\theta = {\\theta_1, \\cdots, \\theta_{4^n-1}}$. For $n = 2$ ($N = 4$), $\\theta$ is a 15-dimensional vector. $|\\varphi(\\theta)\\rangle$ is obtained via applying the SU(4) unitary to the $|00\\rangle$ state:\n$$|\\varphi(\\theta)\\rangle = SU(4)(\\theta) |00\\rangle.$$"}, {"title": "2.3 Data Reuploading with Quantum Hamiltonian Embedding", "content": "The replacement for the classical linear (dense) layer, DataReUploadingLinear, is designed following the method proposed in , which involves a data reuploading circuit with quantum Hamiltonian embeddings, as shown in Fig 4. Specifically, since the shape of the output feature map before the linear layer, classical or quantum, is fixed to 16 \u00d7 28 \u00d7 28, as shown in Fig. 1, the flattened feature map, which has dimension 12544, can be padded to 16384 = 4\u2077 = (2\u2077)\u00b2 with zeros. Then the padded feature map is reshaped into a matrix $M$ with dimension $2\u2077 \u00d7 2\u2077$. As in Eqn. 7 a Hermitian matrix, $H_M$, could be constructed with the real-valued square matrix $M$ by\n$$H_M = \\frac{M + M^T}{2}$$\nThen the data embedding unitary can be written as\n$$W(M;t) = e^{\\frac{-iH_Mt}{L}}$$\nHowever, unlike in , where $t$ is a trainable parameter, we fix $t$ to $\\frac{\\pi}{4}$ for convenience, where $L$ is the number of layers (or repetitions) of the data reuploading circuit. In the simulation, we fix $L$ to 10.\nThe parameterised segments of the DataReUploadingLinear are SU(2\u2077) unitaries, parameterised by different parameters $w_i$, $i \\in {1,2,3,\\cdots, L}$. Then, the state before the measurement can be written as\n$$|\\psi(M; \\Omega)\\rangle = \\prod_{i=1}^L [SU(2^7) (w_i)W(M; \\frac{\\pi}{4})] |+\\rangle^{\\otimes 7},$$,\nwhere $ \\Omega = {w_1,w_2,\\cdots,w_L}$ and $L = 10$.\nThe purpose of the linear layer, both the quantum DataReUploadingLinear and the classical one, is to project the flattened feature map from the previous convolution (or quanvolution) layer to a 10-dimensional space for classification. In DataReUploadingLinear, we measure the ten projection operators, $P_i = |i\\rangle \\langle i|$, $i \\in {0,1,2,3,...,9}$. Since the maximum value of $i$ is 9, $P_i$ could be constructed as a 4-qubit operator, $|0000\\rangle \\langle 0000|, |0001\\rangle \\langle 0001|, \\cdots ,|1000\\rangle \\langle 1000|, |1001\\rangle \\langle 1001|$. We denote the output vector of the last layer as $p$:\n$$p=\\begin{bmatrix} p_0 \\\\ p_1 \\\\ \\vdots \\\\ p_8 \\\\ p_9 \\end{bmatrix}$$\nThen for each element $p_i$ in $p$, we have\n$$p_i = \\langle \\psi(M; \\Omega)| (P_i \\otimes I^{23}) |\\psi(M; \\Omega) \\rangle.$$\n$p$ is the \"classification score\" of the input image. The index (0, 1, \u2026, 8, 9) of the largest element in $p$ is chosen as the predicted label."}, {"title": "2.4 Loss Function", "content": "We adopt the commonly used cross-entropy loss as our minimisation target during training. Denote $\\hat{y}_i$ as the true probability of the input image for label $i$, which is 1 for the true label and 0 for the rest; and $y^{pred}_i$ as the classification score of label $i$ (in our case it equals to $p_i$), then the Cross Entropy function can be written as\n$$CrossEntropy = - \\sum_{i=0}^9 \\hat{y}_i log_2 y^{pred}_i$$\nfor 10-class classification. Since the value of $y^{pred}_i$ could be 0 or a negative number (in the classical case), it is common practice to use $Softmax(y^{pred})$ to replace $y^{pred}_i$ in the cross-entropy loss :\n$$Softmax(y^{pred}_i) = \\frac{exp(y^{pred}_i)}{\\sum_k exp(y^{pred}_k)}$$"}, {"title": "3 Experiments and Results", "content": "In this paper, we trained all three different replacement levels on three different datasets. Each training combination (replacement level \u00d7 dataset) is repeated five times with different parameter initialisations. The hyper-parameters are the same throughout all training combinations, and listed in Table 1. We used linear algebra functionalities in PyTorch  for circuit simulation, as well as the Adam optimiser implementation in PyTorch, torch.optim.Adam, for training. The numerical experiments are conducted on a single NVIDIA H100 GPU."}, {"title": "3.1 Datasets", "content": "We trained and tested our models on three different datasets: MNIST , FashionMNIST  and CIFAR-10 . All three data sets are obtained via Torchvision . Both MNIST and FashionMNIST consist of 70000 28 \u00d7 28 black-and-white images, split into a train set with 60000 images and a test set of 10000 images. The MNIST dataset consists of handwritten digits (0 to 9, see Fig. 5) extracted from two NIST databases.\nThe FashionMNIST is a dataset of Zalando's article images (see Fig. 6). It was intended as a direct drop-in replacement of the MNIST dataset. Since the classes of"}, {"title": "3.2 Results and Analysis", "content": "After training models with different levels of replacement five times with different parameter initialisations with all three datasets, we gather the averaged performance results of the last iteration in Table 4. Detailed graphs on how performance metrics change during the optimisation process for both training and test data sets can be found in the Appendix A.\nIn general, the HybridModel with replacement_level = 2 achieves the best average performance in all three datasets. The average performance of all three different replacement levels degrades when the complexity of the data increases, from MNIST to FashionMNIST and then to CIFAR-10. Such performance degradation is likely due to the lack of expressivity of the network structure itself (normally in order to reach high performance on the CIFAR-10 dataset, one would need to follow the structure of neural networks like ResNet-18 ). It is also should be noted that operations such as Batch Normalisation and Layer Normalisation also play an important part in the performance of a neural network). Compared to previous results from the quantum machine learning literature on similar sets of data, such as in , our classical-quantum hybrid neural network achieves a higher performance compared to an end-to-end quantum neural network (96.9% versus 89.7% test accuracy on MNIST, and 86.6% versus 79.6% on FashionMNIST).\nWe also notice that, simply replacing the classical Conv2d layer with FlippedQuanv3x3 layer does not bring a significant performance change on all three datasets. This can be explained through the analysis in Section 2.2, which shows that mathematically the FlippedQuanv3x3 layer has little difference from the Conv2d layer. Major performance boost occurs when the classical Linear layer is replaced with the quantum DataReUploadingLinear. In , it was shown that the transformation applied to input data $(H_M)$ by the DataReUploadingLinear is nonlinear:\n$$W(H_M;t=\\frac{\\pi}{4}) = I - \\frac{iH_M}{2!\\times2^1L} + \\frac{(iH_M)^2}{2!\\times2^2L^2} + \\frac{(iH_M)^3}{3!\\times2^33} + \\cdots,$$\nwhere we set $t = \\frac{\\pi}{4}$, $H_M = \\frac{M+M^T}{2}$, $M$ is the padded, reshaped feature map from the previous layer, and $L$ is the number of layers in the data reuploading circuit in the DataReUploadingLinear layer. We can see that the nonlinearity provided by Eqn. 26 is much stronger than the classical linear layer, which is just the affine transformation:\n$$Linear(x) = W^Tx + b,$$"}, {"title": "4 Discussion", "content": "In this paper, we proposed a scheme for developing quantum neural network models and comparing their performance with their classical counterparts by gradually swapping classical components of the classical layers and keeping the information-passing structure, instead of designing an end-to-end quantum neural network model. During the development of classical deep learning models, the information-passing structure is as important as the design of individual layers, if not more important. For example,"}]}