{"title": "Deep Learning for Generalised Planning with Background Knowledge", "authors": ["Dillon Z. Chen", "Rostislav Hor\u010d\u00edk", "Gustav \u0160\u00edr"], "abstract": "Automated planning is a form of declarative problem solv-ing which has recently drawn attention from the machine learning (ML) community. ML has been applied to planning either as a way to test 'reasoning capabilities' of architec-tures, or more pragmatically in an attempt to scale up solvers with learned domain knowledge. In practice, planning prob-lems are easy to solve but hard to optimise. However, ML approaches still struggle to solve many problems that are of-ten easy for both humans and classical planners. In this pa-per, we thus propose a new ML approach that allows users to specify background knowledge (BK) through Datalog rules to guide both the learning and planning processes in an inte-grated fashion. By incorporating BK, our approach bypasses the need to relearn how to solve problems from scratch and instead focuses the learning on plan quality optimisation. Ex-periments with BK demonstrate that our method successfully scales and learns to plan efficiently with high quality solu-tions from small training data generated in under 5 seconds.", "sections": [{"title": "Introduction", "content": "Learning for planning is drawing increasing interest due to advances in deep learning architectures. Most current ap-proaches use a neural model to learn a heuristic to com-pute a greedy policy (St\u00e5hlberg, Bonet, and Geffner 2022) or navigate a search algorithm (Chen, Thi\u00e9baux, and Tre-vizan 2024). However, such approaches cannot incorporate domain knowledge into their architecture and instead must learn all the domain mechanics from the domain definition and training data from scratch. It is usually the case that users already have knowledge about solving the domain, and are more interested in solution quality. We thus propose an approach which allows users to provide the solving knowl-edge to the learner in order to focus on optimising solution quality, rather than relearning how to solve the planning task.\nThis paper proposes incorporating domain background knowledge (BK), as common in the field of inductive logic programming (Cropper et al. 2022), into the machine learn-ing (ML) model for planning. We represent a generalised policy as an ML model which returns the best action to take in a given problem and state. The BK we propose to provide the ML model is a generalised nondeterministic pol-icy \\( \\sigma \\) which returns a set of actions to take in a given prob-lem and state, thus restricting the hypothesis space of the"}, {"title": "Preliminaries", "content": "This section details the necessary preliminaries for under-standing the rest of the paper. The first two subsections intro-duce the formalism and representation of classical planning tasks and domains. More specifically, planning tasks will be represented in their 'lifted' form with first-order logic. The final subsection introduces Datalog, a declarative program-ming language based on first-order logic."}, {"title": "Planning Task", "content": "A planning task (Geffner and Bonet 2013) is a state transition model \\( P = (S, A, s_0, G) \\) where \\( S \\) is a set of states, \\( A \\) is a set of actions, \\( s_0 \\in S \\) is the initial state, and \\( G \\subseteq S \\) is a set of goal states. An action \\( a \\in A \\) is a function \\( a: S \\rightarrow S \\cup \\{\\perp\\} \\) that maps a state \\( s \\) to a successor \\( a(s) \\in S \\) if \\( a \\) is applicable in \\( s \\), otherwise \\( a(s) = \\perp \\). We assume that all actions have a unit cost. A plan for a planning task \\( P \\) is a sequence of applicable actions that transforms the initial state to a goal state when applied in order. Formally, a plan is of the form \\( a_1,...,a_n \\) such that \\( s_i = a_i(s_{i-1}) \\) for all \\( i \\in [n] := \\{1, ..., n\\} \\) and \\( s_n \\in G \\), and we call \\( s_0,..., s_n \\) the trace of the plan. A plan is called optimal if it is shortest among all plans. A planning task is solvable if it has at least one plan, and unsolvable otherwise. A state \\( s \\) is a dead-end if the new planning task \\( P_s = (S, A, s, G) \\) is unsolvable."}, {"title": "Planning Domain", "content": "In practice, planning tasks are repre-sented in a lifted form (Lauer et al. 2021) given by a tuple \\( (\\mathcal{P}, \\mathcal{O}, \\mathcal{A}, s_0, g) \\) and set of variables \\( \\mathcal{V} \\), where \\( \\mathcal{P} \\) denotes a fi-nite set of first-order predicates, \\( \\mathcal{O} \\) a set of objects, \\( \\mathcal{A} \\) a set of action schemata, \\( s_0 \\) the initial state, and \\( g \\) now the goal con-dition. A predicate \\( p \\in \\mathcal{P} \\) is a symbol with a corresponding arity denoting how many parameters it has. An atomic for-mula over \\( \\mathcal{V} \\cup \\mathcal{O} \\) is an expression of the form \\( p(X_1,..., X_n) \\) where \\( p \\in \\mathcal{P} \\) and \\( X_i \\in \\mathcal{V} \\cup \\mathcal{O} \\). An atomic formula where \\( X_i \\in \\mathcal{O} \\) for \\( i \\in [n] \\) is called a (ground) atom. A substitution \\( \\nu \\) is a map \\( \\nu: \\mathcal{V} \\cup \\mathcal{O} \\rightarrow \\mathcal{O} \\) such that \\( \\nu(o) = o \\) for all \\( o \\in \\mathcal{O} \\). The set of all substitutions is denoted \\( Sub \\). A substitution \\( \\nu \\) and atomic formula \\( a = p(X_1,..., X_n) \\) induces a ground atom \\( \\nu(a) = p(\\nu(X_1), ..., \\nu(X_n)) \\). States in a lifted plan-ning task are represented as sets of ground atoms, and \\( s_0 \\) is the initial state. The goal condition \\( g \\) is also a set of ground atoms and a state \\( s \\) is a goal state if \\( g \\subseteq s \\).\nAn action schema \\( a \\in \\mathcal{A} \\) in a lifted planning task is a tuple \\( (V(a), pre(a), add(a), del(a)) \\) where \\( V(a) \\) is a set of variables, and the preconditions \\( pre(a) \\), add effects \\( add(a) \\), and delete effects \\( del(a) \\) are sets of atomic formulas over \\( V(a) \\cup \\mathcal{O} \\). A ground action is an action schema with all its variables substituted with objects, noting that the precondi-tions, add and delete effects of ground actions are sets of ground atoms. A ground action \\( a \\) is applicable in a state \\( s \\) if \\( pre(a) \\subseteq s \\), in which case we define its successor \\( a(s) = (s \\setminus del(a)) \\cup add(a) \\), and \\( a(s) = \\perp \\) otherwise. For the remainder of the paper, we assume planning tasks are represented in a lifted form. A planning domain is a tu-ple \\( \\mathcal{D} = (\\mathcal{P}, \\mathcal{A}) \\) and a planning task belongs to a domain if it shares the same predicates and schemata as \\( \\mathcal{D} \\)."}, {"title": "Datalog", "content": "We outline necessary definitions and results on Datalog and refer to (Dantsin et al. 2001) for details. A lit-eral \\( \\lambda \\) is either an atomic formula \\( a \\), or its negation \\( \\neg a \\). The"}, {"title": "Background Knowledge Policies", "content": "In this section, we formalise how Datalog programs can be used as policies for planning domains and outline formal properties that make them suitable BK."}, {"title": "Datalog Policies for Planning", "content": "We begin by defining a general notion of a nondeterministic (ND) policy for deterministic planning problems. Next, we define how a Datalog program can induce such a policy for planning problems. These Datalog programs then form the core of the BK policies utilised by the learning and planning algorithms."}, {"title": "Non-deterministic Policy", "content": "A (ND) policy for a problem \\( P = (S, A, s_0, G) \\) is a function of the form \\( \\sigma: S \\rightarrow 2^A \\). A \\( \\sigma \\)-trajectory from a state \\( s_1 \\in S \\) is a finite sequence of states \\( s_1,..., s_n \\) such that for all \\( i = 1, ..., n-\\), there exists an action \\( a \\in \\sigma(s_i) \\) such that \\( s_{i+1} = a(s_i) \\)."}, {"title": "Datalog Program Induced Policy", "content": "Let \\( P = (\\mathcal{P}, \\mathcal{O}, \\mathcal{A}, s_0, g) \\) be a planning task in its lifted form, and \\( \\mathcal{F} \\) a Datalog program. Then \\( \\mathcal{F} \\) induces a ND policy for"}, {"title": "Applicable Actions", "content": "The baseline BK policy we can provide for any planning domain is the ND policy that returns all applicable actions for each state. The Datalog program would be given by"}, {"title": "LRNN Datalog Program", "content": "Lifted Relational Neural Networks (LRNN) (\u0160ourek et al. 2018) do not have fixed computation structures, as in usual deep learning architectures, and define them declaratively via logic programming. They bring more expressiveness for learning with structured data, while subsuming existing neu-ral architectures like convolutional, recurrent, or graph neu-ral networks (\u0160ourek, \u017delezn\u00fd, and Ku\u017eelka 2021). In this section, we introduce the general LRNN principle, and then discuss how to instantiate a LRNN given a BK policy and a planning domain. Figure 2 summarises the LRNN concept."}, {"title": "General LRNN Architecture", "content": "LRNNs can essentially be viewed as differentiable Datalog programs \\( \\mathcal{F} \\), endowing the contained rules with tuples of learnable parameters. An input to an LRNN is a set of ground atoms \\( s \\) with each \\( f \\in s \\) associated (optionally) with a feature vector \\( \\vec{f} \\). The output is then the canonical model \\( M_{\\mathcal{F}}(s) \\) of \\( \\mathcal{F} \\) with an embedding vector associated with each derived ground atom in \\( M_{\\mathcal{F}}(s) \\).\nSpecifically, an LRNN assigns every rule \\( r \\in \\mathcal{F} \\) a matrix \\( H^r \\) associated with its \\( head(r) \\), and matrices \\( B^{\\lambda} \\) associated with each of its body atoms \\( \\lambda \\in body(r) \\). Given an input \\( s \\), it then recursively computes vectors \\( \\vec{f} \\) for each ground atom \\( f \\in M_{\\mathcal{F}}(s) \\setminus s \\) based on the derivation of \\( f \\) by \\( \\mathcal{F} \\). The computation of \\( \\vec{f} \\) is done in two steps.\nFirstly, we consider all instances of a single ruler that can derive \\( f \\). For each \\( r \\in \\mathcal{F} \\) and derivable \\( f \\), we define a restricted set of substitutions\n\\( Sub_{r, f} = \\{ \\nu \\in Sub \\mid \\nu(head(r)) = f \\land M_{\\mathcal{F}}(s) \\models \\nu(body(r)) \\} \\)\nand compute a multiset of \u201cmessages\u201d by\n\\( M_{r, f} = \\{ \\{ \\varphi_1(\\sum_{\\lambda \\in body(r)} B^{\\lambda} \\cdot \\nu(\\lambda)) \\mid \\nu \\in Sub_{r, f} \\} \\} \\)\nwhere \\( \\varphi_1 \\) is an activation function like the sigmoid, tanh, or ReLU applied component-wise. Secondly, we define the set of rules that can generate \\( f \\) by \\( \\mathcal{F}_f = \\{ r \\in \\mathcal{F} \\mid Sub_{r, f} \\neq \\emptyset \\} \\) and combine the message multisets \\( M_{r, f} \\) for all \\( r \\in \\mathcal{F}_f \\) by\n\\( \\vec{f} = \\varphi_2(\\bigtriangleup_{r \\in \\mathcal{F}_f} agg(M_{r, f})) \\)\nwhere \\( \\varphi_2 \\) is another activation function, and \\( agg \\) is an ag-gregation function such as a component-wise summation, mean, or maximum."}, {"title": "LRNNS for Planning", "content": "Given a BK Datalog policy \\( \\mathcal{F} \\) for a planning domain \\( \\mathcal{D} \\), we extend it with a simple \u201cmessage passing\u201d scheme (Gilmer et al. 2017), instantiating an ex-tended Datalog program \\( \\mathcal{F}' \\). Firstly, for each n-ary predicate \\( p \\in \\mathcal{P} \\) in the planning problem, we introduce an extra rule\n\\( n-ary(X_1,..., X_n) \\leftarrow p(X_1,..., X_n) \\)\nmapping all atoms of the same arity to atoms with the same predicate for simplicity. This then allows to define very generic rules representing message passing between the do-main objects. To follow its standard binary form, we further"}, {"title": "Experiments", "content": "We perform experiments on 4 classical plan-ning domains: Blocksworld, Ferry, Rover, and Satellite. We modify Rover to remove the path finding component. We take the training and test tasks of the domains from the International Planning Competition 2023 Learning Track (IPC23LT) (Seipp and Segovia-Aguas 2023). Other domains from the IPC23LT are omitted as they were either too easy by exhibiting fast optimal algorithms, or too difficult by hav-ing no polynomial time satisficing algorithms.\nWe generate training data labels by expanding the state space of training tasks with less than 10000 states. The data has the form of (s, a, y) tuples where s is a state with the goal condition encoded, a is an applicable action in s, and y \\( \\in \\{0, 1\\} \\) indicates whether the action is optimal or not. At most 25 training tasks are selected for each domain in this way. The data is used to train the LRNNs to compute a policy from BK that aims to minimise plan length. The bar plot in the left of Figure 3 shows the sizes of train and test tasks in terms of the number of objects, noting that the training tasks are significantly smaller than the testing tasks."}, {"title": "Results", "content": "Figure 4 displays the average plan length improvement (PLI) of the LRNN models over the baseline BK policies for each"}, {"title": "Conclusion and Future Work", "content": "We proposed a new learning for planning paradigm aimed at improving solution quality in planning domains that are easy to solve but hard to optimise. Our approach employs \"background knowledge\" in the form of declarative Datalog rules, representing a satisficing strategy for a planning do-main, along with a general message passing scheme. These rules are then parameterised and trained from data in an end-to-end differentiable manner, resulting in plan quality im-provements over satisficing policies in the experiments. Our new approach opens up several avenues of future work for computing high quality plans more efficiently, such as by in-corporating our generalised policies into new anytime plan-ning and heuristic or local search algorithms."}, {"title": "Related Work", "content": "Our work represents one of the many emerging research di-rections in scaling up planning, moving beyond the tradi-tional paradigm of directly inputting a task into a domain-independent planner and hoping it solves the task. This sec-tion outlines related work concerning different paradigms for scaling up planning: learning for planning, generalised planning, and planning incorporating domain knowledge. We note that our work spans all three of these areas."}, {"title": "Learning for Planning", "content": "Learning for Planning is attract-ing the most attention recently due to the success of ML across various other research fields. Recent deep learning works involve learning action policies (Toyer et al. 2020; Silver et al. 2024; Rossetti et al. 2024), heuristics for guid-ing search or greedy policies (Shen, Trevizan, and Thi\u00e9baux 2020; Karia and Srivastava 2021; St\u00e5hlberg, Bonet, and Geffner 2022, 2023; Chen, Thi\u00e9baux, and Trevizan 2024; Agostinelli, Panta, and Khandelwal 2024), and quantifying expressiveness of architectures (Hor\u010d\u00edk and \u0160\u00edr 2024). Tra-ditional symbolic or classical machine learning have also"}, {"title": "Generalised Planning", "content": "Generalised Planning (GP) entails computing programs that characterise the solutions of plan-ning tasks in a domain (Srivastava, Immerman, and Zil-berstein 2008). Policies, as described in this work, consti-tute one such program. Other characterisations include finite state controllers (Bonet, Palacios, and Geffner 2009, 2010; Hu and Giacomo 2011, 2013; Aguas, Jim\u00e9nez, and Jons-son 2018) and programs with branching and loops (Aguas, Jim\u00e9nez, and Jonsson 2021; Aguas et al. 2022). GP has also been represented as Qualitative Numeric Planning (QNP) tasks (Srivastava, Immerman, and Zilberstein 2008) and has been shown to be theoretically equivalent to fully observ-able nondeterministic planning (FOND) (Bonet and Geffner 2020). The connection between GP and FOND has also been exploited to synthesise generalised policies (Bonet and Geffner 2018; Illanes and McIlraith 2019). For comprehen-sive surveys on GP, we refer to articles by Celorrio, Aguas, and Jonsson (2019) and Srivastava (2023)."}, {"title": "Planning Incorporating Domain Knowledge", "content": "Incorpo-rating domain knowledge into planning primarily involves deciding the language to formally represent such knowledge. Hierarchical Task Networks (Bercher, Alford, and H\u00f6ller 2019) in the SHOP planner (Nau et al. 1999, 2003), and temporal logics in TLPlan (Bacchus and Kabanza 2000)"}]}