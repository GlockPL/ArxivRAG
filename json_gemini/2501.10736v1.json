{"title": "Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention", "authors": ["Shanwen Wang", "Changrui Chen", "Xin Sun", "Danfeng Hong", "Jungong Han"], "abstract": "Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning techniques on sensing images semantic segmentation [1], [2] provide promising solutions for disaster prevention [3], land-use surveillance [4], environment protection [5], and urban planning [6]. The increasing number of Earth-observation satellites makes a large amount of raw remote sensing (RS) images be continuously captured, which amplifies the advantage of deep learning methods [7]. However, it is a time-consuming and laborious task to manually label a large amount of RS data into a large number of categories with the complexity of labeling rules [8]. Therefore, semi-supervised learning draws the attention of the RS communities,\nbecause it only uses a small number of labeled samples and a large amount of unlabeled [9].\nSeveral semi-supervised semantic segmentation methods have been conducted for natural images, which are mainly divided into teacher-student consistency [10], feature perturbation consistency [11], and self-training pseudo-labeling [12]. Self-training pseudo-labeling methods often experience performance degradation due to incorrect pseudo-labels generated in the early stages, which can lead to error propagation. Feature perturbation consistency approaches rely heavily on selecting appropriate perturbations and balanced labeled data, making them particularly sensitive to data imbalance. Among these, the teacher-student consistency framework stands out as the most stable and is especially well-suited for tasks that demand high model complexity [13]. Recent works [9], [14] have introduced semi-supervised segmentation into the field of RS image analysis, meanwhile, encountered some domain-specific problems [15], e.g., rich multi-scale information and high inter-class similarities. Such inherent domain gaps between natural and RS images pose great challenges for semi-supervised RS image segmentation.\n1) Rich Multi-scale Information [16]: Objects on the RS image exhibit a wide range of scales, from expansive objects (e.g., Building) to small entities (e.g., Car). Moreover, objects from the same category, captured by satellites and drones with various resolutions, provide quite different features and context information.\n2) High Inter-class Similarities [17]: Objects from different categories appear to have high visual similarity and are usually intertwined on the RS images. For example, as shown in. Fig. 1(c), the boundary regions of Low vegetation, Tree, Car, and Building are misclassified with Unimatch [18].\nTo address the above challenges, we propose a new semi-supervised semantic segmentation model for RS images based"}, {"title": "II. RELATED WORK", "content": "In recent years, the developments of deep neural networks have encouraged the emergence of a series of works on semi-supervised semantic segmentation of RS images. This section gives a short description of recent developments in related fields."}, {"title": "A. RS semantic segmentation", "content": "Semantic segmentation is to segment an image into regional blocks with certain semantic meanings [19], and recognize the specific semantic category of each regional block respectively. It commonly labels the image pixel by pixel [20]\u2013[23]. Early semantic segmentation models including FCN [24], SegNet [25], and U-Net [26], achieved remarkable performance in traditional semantic segmentation tasks. With the success of Transformers in vision in recent years, more and more semantic segmentation models are adopting the Transformers architecture [27]. Complex backgrounds as well as resolution variations are the main challenges in semantic segmentation of RS images. In RS images, large intra-class and small inter-class variations between objects make image feature characterization difficult. Workman et al. [28] argued that samples with high-resolution labels can be used to guide the training process in supervised learning using low-resolution labels. Region aggregation methods have also been used to improve the resolution of images. Quan et al. [29] utilized multi-scale edge features obtained by Differential Difference of Gaussian (DoG) methods to improve the results of edge extraction. Li et al. [30] proposed a progressive recurrent neural network to remove RS images destriping. Zhong et al. [31] proposed a transformer-based noise identification network model to help mitigate the over-segmentation phenomenon. However, semantic segmentation algorithms rely heavily on a large amount of training data, and it is difficult for these frameworks to achieve good results when labeled data is limited."}, {"title": "B. Semi-supervised Semantic Segmentation", "content": "Semi-supervised semantic segmentation algorithms focus on how to better utilize large amounts of unlabeled data than supervised algorithms. Most semi-supervised semantic segmentation models use a basic convolutional neural network as the backbone, and implement semi-supervised algorithms in three different strategies.\nThe first strategy is pseudo-labeling for self-training, which involves generating pseudo-labels for unlabeled images based on a previously trained model on labeled data. It trains the model with newly generated images with pseudo-labels [32]\u2013[34]. The second strategy is to optimize the model by optimizing the consistent regularized loss function of feature perturbations [27], [35]\u2013[37]. Such methods utilize data-enhancement techniques to apply perturbations directly to the input image. They force the model to predict the same labels for both original and enhanced images. Some feature-based perturbation methods add internal perturbations to the segmentation network, resulting in a modified feature [38]. UniMatch [18] took into account the nature of the semantic segmentation task and merged appropriate data augmentation into FixMatch [11], which has evolved into a concise baseline of semi-supervised semantic segmentation algorithms. The last strategy"}, {"title": "C. Semi-Supervised Semantic Segmentation in RS Images", "content": "Semi-supervised remote sensing semantic segmentation has drawn more and more attention to make better use of the large amount of unlabeled RS image data [40]\u2013[43]. Zhang et al. [14] proposed a new self-training mechanism for generating pseudo-labels with feature-level relationships between neighbor pixels to normalize the predictions of the adaptive model. Fang et al. [40] proposed an approximate rank-order clustering (AROC) model. This model is applied to cluster deep features, thereby generating pseudo-labels for abundant unlabeled samples. Li et al. [44] proposed a new self-supervised edge perception learning framework to mine the potential information hidden at the edge of the target. Huang et al. [9] proposed a decoupled weighted learning (DWL) framework for semi-supervised RS image segmentation. The introduced decoupled learning module separates the prediction of labeled and unlabeled data during the training process, in order to reduce the negative impact of erroneously pseudo-labeled unlabeled data on the training procedure.\nThese techniques and models improve the performance of semantic segmentation of RS images in the case of a small number of labeled images. This work proposes new solutions through the viewpoint of inherent scale diversity and similarity among intra-class and inter-class to address the above challenges."}, {"title": "III. METHODS", "content": "This section is organized as follows: Section III-A describes the main optimization objectives, Section III-B introduces the principles of the multi-scale uncertainty consistency module, and Section III-C presents the basic principles of the cross-teacher-student attention module. The overall structure of our approach is shown in Fig. 2."}, {"title": "A. Main Optimization Objectives", "content": "We define $D^{L} = \\{(x_{i}, y_{i})\\}_{i=1}^{N_{L}}$ as labeled data and $D^{U} = \\{(x_{i})\\}_{i=1}^{N_{U}}$ as unlabeled. Here $x_{i} \\in R^{H \\times W \\times 3}$ denotes the labeled image, $Y_{i} \\in R^{H \\times W \\times K}$ is the ground truth of K classes, while $x_{i} \\in R^{H \\times W \\times 3}$ denotes the unlabeled image, $N_{L}$ and $N_{U}$ are the amount of labeled and unlabeled images. H and W specify the height and width of the image. It is worth noting that in general $N_{U}$ is much larger than $N_{L}$.\nFor unlabeled images, weak augmentation (WA) and strong augmentation (SA) are performed to train the student and teacher networks respectively. The main loss function is:\n$L = L_{S} + L_{U} = \\frac{1}{N_{L}} \\sum_{i=0}^{N_{L}} L_{C E}(P_{i}, Y_{i}) + L_{U},$ (1)\nwhere $L_{S}$ represents the loss of supervised learning with labeled data, and $L_{U}$ represents the loss with unlabeled data,"}, {"title": "B. Multi-Scale Uncertainty Consistency Module", "content": "Basic models of teacher-student architectures update the teacher's weights $\\theta_{t}$ by the exponential moving average (EMA) with the student's weights $\\theta_{t}^{s}$. It integrates the information from different training steps. The teacher's weight $\\theta_{t}$ is updated at training step t as $\\theta_{t} = \\alpha \\theta_{t-1} + (1 - \\alpha) \\theta_{t}^{s}$, where $\\alpha$ is the EMA decay. This basic teacher-student architecture is widely used in semi-supervised networks. However, it is critical to take into account the inherent difference between RS and natural images. Objects in RS images exhibit a wide range of scales, spanning from large buildings to small vehicles, and the same object can appear at varying sizes under different resolutions. Therefore, semi-supervised learning in RS images is difficult to extract effective features. To solve this problem, we propose a Multi-Scale Uncertainty Consistency (MSUC) module for the student to progressively learn multi-scale features from reliable targets, as shown in Fig. 3. Given a batch of training images, the teacher model not only generates pseudo-labels for target prediction, but also estimates the uncertainty of each target at multiple feature levels. It optimizes the student model through multilevel consistency loss. This will motivate the model to learn different layers and sizes of features from RS images."}, {"title": "Uncertainty Estimation", "content": "We use Monte-Carlo Dropout [45] to estimate uncertainty. Monte-Carlo method can not only calculate the uncertainty between different classes but also evaluate the uncertainty in the multiple predictions of a neural network. The basic principle of Monte-Carlo Dropout is to make predictions of T times for the same sample with the same model. These T predictions are different, and their variance is calculated to compute the model uncertainty. In detail, we perform T random forward passes for the teacher model, each of which uses the random dropout and noise. Thus, for each pixel in the image, we obtain a set of softmax probability vectors: $\\{p_{t}^{k}\\}_{t=1}^{T}$. We choose the prediction entropy as a measure of the approximate uncertainty because it has a fixed range. Formally, the prediction entropy can be summarized as:\n$U_{k} = -\\frac{1}{T} \\sum_{t=1}^{T} p_{t}^{k} log(p_{t}^{k}),$ (3)\n$u = -\\sum_{k=1}^{K} U_{k},$ (4)\nwhere $p_{t}^{k}$ is the result of pixels of the class k in the tth prediction procedure. In this way, the uncertainty of multiple predictions for the same class can be calculated. According to Eq. 3 and Eq. 4, the value u increases when the network provides completely opposite predictions many times. Moreover, Eq. 3 and Eq. 4 can also take into account the uncertainty between different classes. If the neural network gives similar predictions across multiple classes, the uncertainty value u will also increase."}, {"title": "Consistency Loss Functions for Multiscale Uncertainty", "content": "To formally describe MSUC, we denote the encoded features from the four stages of the encoder as $V_{i} \\in R^{C_{i} \\times H_{i} \\times W_{i}}, i \\in \\{1, 2, 3, 4\\}$, where $C_{i}, H_{i}$, and $W_{i}$ denote the number of channels, the height, and the width of the feature maps from the ith stage, as shown in Fig. 3. More specifically, $V_{1}$ is the output of the first stage with the lowest abstraction but the highest spatial resolution, and $V_{4}$ is the output of the fourth stage with the highest abstraction but the lowest spatial resolution. The final consistency loss function was computed by aligning the encoded visual features at each stage of the student model and the teacher model.\n$L_{M S U C} = \\sum_{i=1}^{4} \\frac{\\sum_{m} \\mathbb{I}(U_{i m} < H) L_{\\delta}(V_{i m} - V_{i m})}{\\sum_{m} \\mathbb{I}(U_{i m} < H)},$ (5)\nwhere $V_{i}^{t}$ and $V_{i}^{s}$ are the outputs of the teacher and student encoders at the pixel m of the $i^{th}$ stage, $U_{im}$ is the uncertainty at the pixel m of the $i^{th}$ stage, $\\mathbb{I}$ is the indicator function, and H is a threshold. $L_{\\delta}$ is the Huber loss function, which is formulated as follows:\n$L_{\\delta}(V_{i m} - V_{i m}) = \\begin{cases} \\frac{(V_{i m} - V_{i m}^{t})^{2}}{2} & \\text{if } |V_{i m} - V_{i m}^{t}| < \\delta \\\n\\delta \\|V_{i m} - V_{i m}^{t}| - \\frac{\\delta^{2}}{2} & \\text{otherwise} \\end{cases}$ (6)\nwhere $\\delta$ is the soft threshold for Huber loss, which is set to 1.0 in this paper."}, {"title": "C. Cross-Teacher-Student Attention", "content": "We propose the Cross-Teacher-Student Attention (CTSA) model, as illustrated in Fig. 4, to promote the ability of semi-supervised segmentation method for objects with high inter-class similarities.\nConsistency regularization methods are based on the assumption of smoothness [46]. This means that a robust model should produce similar predictions for points and their variants with noise. In other words, a model trained with consistency methods should not be affected by different perturbations added to the data. However such semi-supervised frameworks ignore the inherent domain gap between natural and RS images. One RS image contains much richer content than one natural image, and objects of different categories are commonly intertwined. Therefore perturbations will make the RS image contain more confusing information. The features learned by the neural network from different perturbed RS images may lead to some outliers as pseudo-annotations, misleading the training of the model on unannotated data.\nTo address this problem, we use the features extracted by the student and teacher networks via different perturbations to reconstruct the features of the unlabeled data. We use the encoder result of the student as query and the encoder result of the teacher as key and value. The teacher guides the reconstruction of the encoder of the student, as shown in Fig. 4. Specifically, we calculate the similarity between each channel of the unlabeled features of the student and teacher networks. The channels with higher similarity are more important in reconstructing the unlabeled features.\nMore importantly, the new features constructed through CTSA effectively integrate the advantages of the teacher and student network encoders. Specifically, the SA data fed into the student encoder provides the original features which are diverse and rich. Meanwhile, the teacher encoder, receiving WA data, reconstructs another set of smooth and stable features. The decoder is trained using features generated from strongly supervised data along with features reconstructed by CTSA, resulting in better optimization outcomes. Particularly in the early stages of semi-supervised learning, where training instability and low-quality pseudo-labels often hinder efficient convergence, the teacher network encoder provides more easily learnable features for the student network decoder. This improves the model's stability, mitigates training fluctuations, accelerates convergence, and enhances training efficiency.\nWe further clarify the CTSA formulation as follows. Given the student and teacher characteristics $V_{s}$ and $V_{t}$ after the fourth stage of the encoder, we set $V_{t}$ as query, $V_{s}$ as key and value, in a multi-head manner:\n$F_{V_{t}} = flatten(V_{t}), F_{V_{s}} = flatten(V_{s}),$ (7)\n$q = F_{V_{t}}w_{q}, k = F_{V_{s}}w_{k}, v = F_{V_{s}}w_{v},$ (8)\nwhere $w_{q}, w_{k}, w_{v} \\in R^{C \\times 2C}$ are the transformer weights, $F_{V_{t}}, F_{V_{s}} \\in R^{C \\times d}$, d is the number of patches and C is the channel dimension. The CTSA is defined as:\n$V_{out} = \\phi[ \\psi(q^{T}k)v^{T}w_{out} ],$ (9)\nwhere $\\phi$ denotes instance normalization and $\\psi$ is softmax function. $V_{out}$ is fed into the decoder to get the final predicted value of the CTSA module as described below:\n$p_{u} = Decoder(V_{out}).$ (10)\nThe loss function for the CTSA module is calculated as Eq. 20, where $y_{i}$ is the generated pseudo-label from unlabeled data.\n$L_{C T S A} = \\frac{1}{N_{U}} \\sum_{i=0}^{N_{L}} L_{C E}(p_{i}^{u}, Y_{i}),$ (11)"}, {"title": "IV. EXPERIMENT", "content": "In this section, we conduct experiments on semi-supervised semantic segmentation of RS images to evaluate the proposed MUCA model. We first perform ablation studies on MUCA to validate the effectiveness of the proposed modules. Then, we conducted comparative experiments to compare MUCA with several SOTA models. Our code is released at https://github.com/wangshanwen001/RS-MUCA."}, {"title": "A. RS Dataset", "content": "LoveDA: The LoveDA RS dataset [47] consists of 5987 images and 166,768 annotated objects from three different cities. Each image is 1024 \u00d7 1024 with a spatial resolution of 0.3 meters. The dataset has seven segmentable classes, such as Building, Road, Water, Barren, Forest, Agriculture, and Background. Due to the memory limitations of the GPU, each image in the LoveDA dataset is resized and cropped to 512x 512, producing a total of 16,764 cropped images for subsequent deep learning training. During training, we divide the dataset into training set, validation set, and test set by 6:2:2.\nISPRS-Potsdam: The ISPRS-Potsdam RS dataset is provided to facilitate research on RS images semantic segmentation [48]. This dataset has a resolution of 0.05 meters and consists of 38 super-large satellite RS images of 6000 \u00d7 6000 size. There are six segmentable classes in the dataset including Impervious surfaces, Building, Low vegetation, Tree, Car, and Background. To facilitate the training procedure, we crop the original image into 512 \u00d7 512 and the number of cropped images is 5472. The dataset is divided into training, validation and test sets by 6:2:2."}, {"title": "B. Data Augmentation and Experiment Details", "content": "We first augmentation the labeled images via geometric transformations (i.e., image scaling, horizontal flipping, vertical flipping and length and width warping) and Gaussian blurring. The weak augmentation used for unlabeled images is geometric augmentation, and strong augmentation is done by methods such as CutMix [49].\nThe experiments are conducted on A6000 (48G) with Python-3.8.10, Pytorch-1.13.0, and Cuda v11.7. The optimization algorithm is stochastic gradient descent with an initial learning rate 0.007. The learning rate is updated on each epoch, the weight decay is set to 0.0001, and the minimum learning rate is set to 0.00007. Then, we train the semi-supervised segmentation model using 1%, 5% and 10% of the labeled images and the remaining unlabeled images."}, {"title": "C. Evaluation metrics", "content": "Most of the popular semantic segmentation metrics are employed to comprehensively evaluate the performance, e.g., intersection and concurrency ratio (IoU), F1-Score (F1), and Cohen's Kappa as show below:\n$IoU = \\frac{T P}{T P+F N+F P},$\n(12)\n$Recall = \\frac{T P}{T P+F N},$\n(13)\n$Precision = \\frac{T P}{T P+F P},$\n(14)\n$F 1 = \\frac{2 \\times Recall \\times Precision}{Recall + Precision},$\n(15)\n$OA = \\frac{T P+T N}{T P+T N+F P+F N},$\n(16)\n$P R E = \\frac{(T P+F N)(T P+F P)+(T N+F N)(T N+F P)}{(T P+T N+F P+F N)^{2}},$\n(17)\n$\\Kappa = \\frac{O A-P R E}{1-P R E},$\n(18)\nwhere TP is the number of correctly predicted positive pixels, TN is the number of correctly predicted negative pixels, FP is the number of incorrectly predicted positive pixels, and FN is the number of incorrectly predicted negative pixels. In our experiments we average IoU and F1 across all classes, so the final experimental metrics also include mean IoU (mIoU) and mean F1 (mF1). Then, mIoU and mF1 is calculated as follows:\n$m I o U = \\frac{\\sum_{i=1}^{K} I o U_{i}}{K},$\n(19)\n$m F 1 = \\frac{\\sum_{i=1}^{K} F 1_{i}}{K},$\n(20)\nwhere IoU; means the IoU of the $i^{th}$ class and K is the number of classes."}, {"title": "D. Ablation study", "content": "Ablation of components: We ablate each component step by step to investigate their performance. It is worth noting that we also evaluate the multi-scale consistency loss function of MSUC module without uncertainty, which is denoted as NoUC. Differing from the MSUC module, NoUC only uses standard consistency regularization to compute the loss for each hierarchical feature map. The ablation studies are conducted on the ISPRS-Potsdam dataset with labeled data ratio of 5%. The baseline is the supervised model with 5% labeled"}, {"title": "E. Comparison with SOTA methods on RS Datasets", "content": "This section conducts experiments on ISPRS-Potsdam and LoveDA datasets, compared to the SOTA methods, including Mean teacher [10], CutMix [49], CCT [38], CPS [39], LSST [50], Fixmatch [11], Unimatch [18], DWL [9], and Allspark [12]. Specifically, we show the results for labeled data ratios of 1%, 5% and 10% with SegFormer-B2 as the segmentation model to verify the effectiveness, respectively. We also execute the experiments of OnlySup and FullySup for reference, where OnlySup only uses the labeled data for training and FullySup uses all data. The experimental results are presented in Tables"}, {"title": "F. Model generalizability experiments", "content": "The proposed model is non-intrusive and can be easily integrated into existing semantic segmentation networks without changing the network structure itself. Therefore, we conduct experiments by integrating our model into several popular networks including U-Net, PSPNet, DeepLabv3+, and SegFormer-B2 on ISPRS-Potsdam dataset. These experiments are designed to evaluate the general applicability of the MUCA on segmentation models. The first three of these models are classical CNN semantic segmentation models, and the last one is based on the Transformer architecture. We comparative analyze the performance improvement of MUCA with Onlysup and NoUC on these models. NoUC, as clearly defined in the Ablation Study section, refers to a simplified method that does not perform uncertainty estimation or incorporate the CTSA module. Instead, it relies solely on standard consistency regularization to calculate the loss of feature maps at each layer. The model general applicability experiments were performed on 5% labeled training data and the results are shown in Table VII.\nIt can be seen that semi-supervised model MUCA achieves significant improvements with 5% labeled training data. It enhances the performance of popular semantic segmentation models including U-Net, PSPNet, DeepLabv3+, and SegFormer-B2. This shows that our MUCA has the ability to improve the performance of classical CNN-baed semantic segmentation models and novel Transformer architecture models."}, {"title": "G. Visualization Comparison", "content": "We conducted visual comparison experiments to see the advantages of our approach more intuitively and clearly. Fig. 6 shows the visual comparison of several semi-supervised semantic segmentation methods on the ISPRS-Potsdam dataset.\nThe Fixmatch and Unimatch make errors in segmenting regions for the Low vegetation, Tree, Impervious surfaces and Building classes. Allspark successfully recognizes part of the region where Low vegetation classes are mixed with Tree classes, but the segmentation results are significantly enlarged for building classes. In contrast, our method shows significant advantages in accurately recognizing and segmenting Building (blue), Car (yellow), and Tree (green). Especially, we can see that methods including Fixmatch and Unimatch show large-scale segmentation errors when Car, Low vegetation and Tree are mixed. In addition, we overlap our segmentation results with the original image in the last column to clearly show our advantage. This overlapped image demonstrates that our model achieves excellent visual results, particularly in the Tree and Car categories."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "Our study addresses the challenges associated with semi-supervised RS image semantic segmentation by proposing the Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model. The MUCA model includes two special modules, i.e., Multi-scale Uncertainty Consistency (MSUC) and Cross-Teacher-Student Attention (CTSA). The goal of MSUC is to learn rich multi-scale information, meanwhile, CTSA distinguishes the high inter-class similarities through the cross-network attention mechanism. The new features constructed by CTSA enable the student network decoder to benefit from the dual enhancement of both WA and SA, achieving better optimization results and demonstrating greater stability during the training phase.\nCompared to SOTA algorithms, our method achieved the best results for the metrics mIoU, mF1, and Kappa, however, did not achieve the best IoU for all the categories. This phenomenon is understandable and explainable. On the one hand, this paper does not address other problems in the semi-supervised domain such as long-tailed distribution among classes. On the other hand, different algorithms have different focuses which may lead to a particularly good result for one class and a poor result for others. This study highlights the more realistic scenario where MUCA demonstrates the best overall performance across all classes. Finally, we hope that the model proposed in this paper can serve as a simple and powerful baseline in the field of semi-supervised semantic segmentation of RS images. We also aim to inspire more valuable research for future work."}]}