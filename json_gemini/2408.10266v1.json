{"title": "Diffusion Model for Planning: A Systematic Literature Review", "authors": ["Toshihide Ubukata", "Jialong Li", "Kenji Tei"], "abstract": "Diffusion models, which leverage stochastic pro- cesses to capture complex data distributions effectively, have shown their performance as generative models, achieving no- table success in image-related tasks through iterative denoising processes. Recently, diffusion models have been further applied and show their strong abilities in planning tasks, leading to a significant growth in related publications since 2023. To help researchers better understand the field and promote the devel- opment of the field, we conduct a systematic literature review of recent advancements in the application of diffusion models for planning. Specifically, this paper categorizes and discusses the current literature from the following perspectives: (i) relevant datasets and benchmarks used for evaluating diffusion model-based planning; (ii) fundamental studies that address aspects such as sampling efficiency; (iii) skill-centric and condition-guided planning for enhancing adaptability; (iv) safety and uncertainty managing mechanism for enhancing safety and robustness; and (v) domain-specific application such as autonomous driving. Finally, given the above literature review, we further discuss the challenges and future directions in this field.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of Generative Artificial intelligence (GenAI) models, Diffusion models [1], [2] have recently emerged as a powerful class of generative models. These models utilize stochastic processes to transform random noise into high-quality data through iterative denoising. Diffusion models have shown great capabilities in image-related tasks, such as generation [3]\u2013[5], restoration and enhancement [6], and editing [7].\nThe fundamental principle of diffusion models involves introducing noise to training data and learning to reverse this process through iterative denoising, effectively capturing complex data distributions. Technically, diffusion models use stochastic differential equations to progressively refine noisy inputs into coherent outputs, enabling accurate modeling of the underlying data distribution. This iterative refinement process allows diffusion models to explore a wide solution space, generating high-quality, diverse outputs.\nThanks to the above specific characteristics of diffusion models, their application extends beyond image processing and has recently been applied in planning tasks, especially in high-dimensional scenarios like motion planning. Recent studies have increasingly reported advantages of diffusion models, such as robustness, highlighting their potential as planners.\nThis trend is also evidenced by the increasing number of re-lated publications, particularly since 2023, as shown in Figure 1. In this context, we believe that planning based on diffusion models is a promising research area. However, due to their technical diversity and wide applicability, comprehensively understanding diffusion model-based planning is a challenge.\nTo this end, we conduct a Systematic Literature Review (SLR) to comprehensively examine the applications of dif-fusion models in planning. Specifically, we reviewed 41 relevant and filtered papers and categorized them based on their research focus (e.g., safety, efficiency). Additionally, we compared the relationships and differences in ideas within the same category of papers. We hope this paper can provide a comprehensive overview, making it easier for researchers and practitioners to understand the state of the art of diffusion models in the field of planning.\nThe structure of this paper is as follows: Section II de-tails the methodology used for the searching and filtering literature. Section III delves into the various datasets em-ployed to evaluate diffusion models, categorizing them based on specific tasks such as motion planning, path planning, and reinforcement learning. Section IV covers the foundation of the diffusion model in planning, motion and path plan-ning, efficiency improvements, preference customization, and hierarchical structures against traditional planning methods. Section V explores skill-centric and condition-guided planning approaches. Section VI addresses the enhancement of safety and robustness in planning, including safety mechanisms and strategies for managing uncertainty. Section VII discusses specific applications, such as 3D planning and optimization, instructional video procedure planning, autonomous driving,"}, {"title": "II. LITERATURE SEARCH METHODOLOGY", "content": "To ensure a comprehensive survey of the current state of the art in diffusion models for planning, we employed a literature search strategy.\nWe conducted our literature survey on IEEE Xplore, ACM Digital Library, Dblp, and ArXiv. We focused on papers published from 2020, following the release of the first paper on Diffusion Models [2], until May 31, 2024. The primary keywords used in our search were:\n\u2022 Diffusion: \"diffusion\", \"diffuser\", \"DDPM\"\n\u2022 Planning: \"plan\", \"replan\" + {\"-er\", \"-ing\"}\nOur search was specifically targeted at papers related to diffusion-based planning in the reinforcement learning dia-gram, explicitly excluding those that focused on large lan-guage models (LLMs) or natural language processing (NLP). Initially, we identified 47 papers. After applying the above filtering criteria, we narrowed this down to 41 relevant papers.\nThe excluded literature is as follows:\n\u2022 An Agent-Based Model of COVID-19 Diffusion to Plan and Evaluate Intervention Policies [8]: An agent-based model demonstrating effective reduction of symp-tomatic COVID-19 cases through targeted measures and genetic algorithms. (Note: This is the 2021 paper shown in Fig 1.)\n\u2022 PLANNER: Generating Diversified Paragraphs via Latent Language Diffusion Model [9]: Text generation using autoregressive models and latent language diffu-sion, relevant to LLMs and NLP.\n\u2022 Professional Basketball Player Behavior Synthesis via Planning with Diffusion [10]: Sports analytics planning using a diffusion probabilistic model, not within our scope.\n\u2022 Denoising Heat-inspired Diffusion with Insulators for Collision-Free Motion Planning [11]: Motion planning using collision-avoiding diffusion kernels and visual input for obstacle detection, not relevant to our review.\n\u2022 First measurements of radon-220 diffusion in mice tumors, towards treatment planning in diffusing alpha-emitters radiation therapy [12]: First estimates of radon-220 diffusion lengths in mice tumors (0.25-0.6 mm), crucial for treatment planning in diffusing alpha-emitters radiation therapy.\n\u2022 Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs [13]: The RPG framework enhances text-to-image dif-fusion models using multimodal LLMs, enabling precise image generation and editing without additional training."}, {"title": "III. DATASET AND BENCHMARK", "content": "The evaluation of diffusion models for planning encom-passes a variety of datasets tailored to distinct tasks, ranging from motion planning and maze solving to robotic manipu-lation and reinforcement learning. This section summarizes the dataset used for diffusion based planning proposals. Each dataset presents unique challenges that highlight the models' capabilities and limitations.\nA. Motion and Path Planning\nD4RL Benchmark Suite [54] benchmark suite is providing standardized datasets and environments, mainly for use in the field of offline reinforcement learning. It includes tasks such as Maze2D, locomotion tasks like HalfCheetah, Hopper, and Walker2D, and complex multi-step manipulation tasks in kitchen environments.\nGym-MuJoCo [55] provides simulated environments for continuous control tasks using a physics engine, including HalfCheetah, Hopper, and Walker2D. These environments involve locomotion tasks and are used to evaluate the perfor-mance of models in dynamic movement and control settings.\nAntMaze AntMaze is a benchmark environment that is part of the MuJoCo suite [55]. It presents challenges in path planning and navigation within complex mazes. This dataset is used to test models for their path-finding and navigation capabilities in constrained environments.\nFranka Kitchen [56], Real-world Kitchen, and Adroit [57] datasets involve complex manipulation tasks in both simulated and real-world environments. Kitchen tasks in-clude multi-step actions such as opening microwaves, moving objects, opening ovens, and grasping cloths. Adroit tasks involve dexterous manipulation with robotic hands. These environments are used to evaluate models like XSkill [30] and Generative Skill Chaining [31], testing the models' abilities to handle complex sequences, precise manipulations, skill transfer, and long-horizon task planning capabilities in realistic settings.\nRLBench [58] provides a variety of robotic tasks such as opening and closing boxes, which are used to evaluate adaptive replanning capabilities in dynamic environments. Models like Adaptive Online Replanning [29] use these tasks to demonstrate flexibility and real-time adjustment abilities.\nBlock Stacking Tasks involve stacking blocks to achieve specified configurations and are used in evaluating models. These tasks assess the models' precision in manipulation and planning under physical constraints.\nB. 3D Planning\nDatasets such as PROX [59] and ScanNet [60] are used for 3D scene understanding, human pose generation, grasp planning, and robot arm motion planning. These datasets test the integration of scene-conditioned generation, optimization, and goal-oriented planning in 3D environments.\nC. Instructional Video\nInstructional video datasets like CrossTask [61] and COIN [62] focus on generating action sequences from start to goal visual observations. Models like PDPP [49] and ActionDif-fusion [51] use these datasets to evaluate their ability to plan and execute complex procedures based on visual and language instructions.\nD. Autonomous Driving\nThe nuPlan [63] dataset is a benchmark for closed-loop planning in autonomous driving. It assesses models like Diffusion-ES [52] for optimizing non-differentiable reward functions and following complex instructions in real-time decision-making and trajectory optimization for autonomous vehicles.\nE. Safety-Critical Datasets\nSafety-critical datasets introduce constraints in experiments using datasets like Maze2D and Gym-MuJoCo to assess the models' ability to ensure safety and feasibility in trajectory planning. These datasets are used for evaluating models like SafeDiffuser [39] and Cold Diffusion on the Replay Buffer [40]. The main purpose is to test the safety and robustness of the models in generating reliable and feasible plans in various environments.\nSummary - Dataset and Benchmark\nThe datasets used for evaluating diffusion-based planners vary, including motion planning, maze solving, and robotic tasks. Specifically, datasets like D4RL, Gym-MuJoCo, and AntMaze, are used to evaluate the models' capabilities in control, navigation, and long-horizon planning. Complex manipulation challenges are addressed with Kitchen and Adroit datasets, while RLBench and block stacking tasks focus on adaptive replanning and precision. The Franka Kitchen datasets test skill transfer in realistic environments. For 3D planning, datasets like PROX and ScanNet eval-uate scene understanding and optimization. Additionally, instructional video datasets and the nuPlan dataset for au-tonomous driving assess the models' ability to follow pro-cedural instructions and make real-time decisions. Safety-critical datasets underscore the importance of robustness and reliability in trajectory planning."}, {"title": "IV. FUNDAMENTAL AND GENERAL PLANNING", "content": "A. Foundation of Diffusion Model for Planning\nThis section introduces the foundational concepts that have studied the current landscape of planning with diffusion mod-els.\nDiffuser [14] stands as a pioneering study in this field. This model is positioned within model-based reinforcement learning, integrating trajectory optimization directly into the model learning process through diffusion models.\nIt introduces a diffusion probabilistic model designed for trajectory planning, which predicts all timesteps concurrently. This approach is significant for its innovative use of iterative denoising processes, allowing for flexible conditioning and planning. Additionally, the employment of auxiliary guides to adjust sampling strategies enhances the model's ability to satisfy high returns or constraints. The Diffuser's contributions lie in its exploration of diffusion-based planning methods, demonstrating robust strategies like classifier-guided sampling and image inpainting to address long-horizon decision-making and adaptability during test time.\nOn the other hand, Diffusion-QL [15] focuses on offline re-inforcement learning, presenting a conditional diffusion model for policy representation. This model leverages the expres-siveness of diffusion processes to capture complex, multi-modal action distributions, which are essential for offline RL tasks. By combining behavior cloning with Q-learning guidance during the diffusion model training, Diffusion-QL effectively generates high-value actions through an iterative denoising process guided by Q-values. This method showcases its superiority by outperforming previous techniques in various benchmark scenarios, highlighting the potential of diffusion models to enhance policy representation and action quality in offline settings.\nThe evolution of these technologies underscores a common reliance on diffusion processes to facilitate flexible and ef-ficient planning and decision-making. Both approaches em-phasize iterative denoising as a core technical idea, enabling models to handle complex prediction and optimization tasks effectively. While Diffuser [14] focuses on trajectory-level planning within a model-based RL framework, Diffusion-QL [15] extends the utility of diffusion models to policy representation in offline RL.\nB. Motion and Path Planning\nTargeting motion and path planning, this section introduces diverse approaches leveraging diffusion models, comparing their contributions, proposed methodologies.\nEnsemble-of-costs-guided Diffusion for Motion Planning (EDMP) [16] focuses on motion planning for robotic manip-ulation, integrating classical and deep-learning approaches. It demonstrates generalization to diverse and out-of-distribution scenes, effectively generating multimodal trajectories. Its con-cept is the utilization of a diffusion-based network to learn pri-ors over kinematically valid trajectories, incorporating scene-specific costs through an ensemble of cost functions to enhance trajectory diversity and success rates.\nIn contrast, Intermittent Diffusion Based Path Planning [18] caters to heterogeneous mobile sensors navigating cluttered environments. This method introduces random perturbations to gradient flow dynamics, aiding sensors in escaping local minima. A decentralized approach leverages local information for collision avoidance, bypassing the offline planning phase and ensuring smooth navigation via a projection strategy.\nContrastive Diffuser [19] shifts focus to reinforcement learning for long-term planning. By employing contrastive learning, it refines the base distribution of diffusion-based RL methods. The return contrast mechanism directs generated trajectories towards high-return states, enhancing performance across multiple benchmarks. This method exemplifies the efficacy of combining diffusion models with reinforcement learning principles.\nMotion Planning Diffusion [17] innovates in robot motion planning by integrating trajectory generative models with optimization-based planning. The approach involves sampling from the posterior distribution using diffusion models, accel-erating motion planning and improving solution quality. The temporal U-Net plays a pivotal role in encoding the diffusion model over trajectories.\nFor legged robots, DiPPeR (Diffusion-based 2D Path Plan-ner) [21] emphasizes scalability and speed. This framework employs an image-conditioned diffusion planner and a robust training pipeline using CNNs, achieving faster trajectory gen-eration compared to traditional and data-driven methods. Its efficacy is validated through deployment on real-world robotic platforms like Boston Dynamics' Spot and Unitree's Go1.\nDiffusion Policy [20] advances robot visuomotor control, utilizing a conditional denoising diffusion process. Key tech-nical contributions include receding horizon control, visual conditioning, and a time-series diffusion transformer. These innovations enable robust execution across multiple manipu-lation tasks, outperforming several baseline methods.\nBuilding on previous research into A* and RRT* heuristics [65], the study by Wu et al. [27] presents a hierarchical framework that integrates diffusion models with reinforcement learning for evasive planning. This dual-layer architecture fea-tures a high-level diffusion model for global path planning and low-level reinforcement learning algorithms for local evasive maneuvers. Task-oriented costmaps enhance the framework by improving explainability and predictability through the consideration of detection risks and dynamic adjustments.\nFinally, DiPPeST (Diffusion-based Path Planner for Synthe-sizing Trajectories) [64] extends the capabilities of diffusion-based planning to quadrupedal robots. The proposal uses RGB input for global path generation and real-time refinement, achieving efficient navigation and obstacle avoidance without additional training. The integration with a visual framework underscores its practical applicability in real-world scenarios.\nC. Efficiency\nThis section explores improving computational efficiency and decision-making effectiveness in diffusion model-based planning.\nLatent Diffuser [22] addresses offline reinforcement learn-ing by proposing a framework for continuous latent action space representation. This approach leverages latent, score-based diffusion models, showcasing the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pre-trained diffusion model. The introduction of a sequence-level exact sampling method ensures competi-tive performance in both low-dimensional locomotion control and higher-dimensional tasks, thereby advancing the state of efficient planning.\nEquivariant Diffuser for Generating Interactions (EDGI) [23] further pushes the boundaries in model-based reinforce-ment learning. By incorporating spatial, temporal, and per-mutation symmetries through an SE(3) \u00d7 Z \u00d7 Sn-equivariant diffusion model, EDGI enhances sample efficiency and gener-alization. This method also introduces flexible soft symmetry breaking during test time for task-specific adaptations, thereby improving planning efficiency and the ability to generalize across different symmetry groups.\nIn the realm of multi-task decision-making and long-term planning, Diffused Task-Agnostic Milestone Planner (DTAMP) [24] utilizes diffusion-based generative sequence models, DTAMP effectively handles long-horizon tasks and vision-based control. It achieves state-of-the-art performance on D4RL and CALVIN benchmarks by planning milestones in a latent space and using goal-conditioned imitation learning. The implementation of classifier-free diffusion guidance ensures efficient path planning, highlighting its robustness in multi-task environments.\nDiffuserLite [25] introduces a real-time diffusion planning approach using its plan refinement process (PRP), which generates coarse-to-fine-grained trajectories to minimize re-dundant information. Its ability to function as a flexible plugin with other diffusion planning algorithms marks a leap toward real-time diffusion planning.\nFinally, Hierarchical Diffuser [26] introduces a framework that combines hierarchical and diffusion-based planning to improve efficiency in long-horizon tasks. By employing a high-level diffuser for sparse subgoal generation and a low-level diffuser for detailed subgoal achievement, this approach enhances computational efficiency and generalization capabil-ities. The introduction of Sparse Diffuser and Sparse Diffuser with Dense Actions (SD-DA) variants further improves return prediction and task performance.\nD. Preference Customization\nFor human preference alignment in reinforcement learning, AlignDiff [28] introduces a framework combining reinforce-ment learning from human feedback (RLHF) with diffusion models for zero-shot behavior customization, addressing chal-lenges in aligning agent behaviors with abstract and muta-ble human preferences. Key contributions are using RLHF to quantify multi-perspective human feedback datasets and developing metrics to evaluate agents' preference matching, switching, and covering capabilities, demonstrating superior performance. The proposal involves a transformer-based at-tribute strength model for quantifying behavior strengths and an attribute-conditioned diffusion model acting as a plan-ner, guided by the attribute strength model during inference. Baseline methods include goal-conditioned behavior cloning and so on. It is evaluated using diverse locomotion tasks in simulation environments like Hopper, Walker, and Humanoid from benchmarks such as MuJoCo and DMControl.\nE. Replanning\nUnder unforeseen and changing conditions, replanning is crucial for enhancing adaptability and robustness in dynamic environments. Replanning with Diffusion Models (RDM) [29] introduces an approach to determine optimal replanning times, enhancing planning efficiency and effectiveness in stochastic and long-horizon tasks. A key contribution is the introduction of a principled method that assesses the likelihood of the cur-rent plan's executability. This method decides when to replan to ensure trajectories remain consistent with the original goal state. The approach includes multiple replanning strategies: replanning from scratch, replanning from previous context, and replanning with future context. This highlights the robustness and broad applicability in complex scenarios, representing an advancement in adaptive replanning for diffusion models.\nSummary - Fundamental and General Planning\nDiffusion models enhance various aspects of reinforcement learning, including state and policy representation and trajectory optimization. Foundational approaches integrate trajectory optimization directly into model-based RL and employ conditional diffusion models for policy representa-tion in offline RL, demonstrating superior performance in generating high-value actions through iterative denoising processes. For motion and path planning, diffusion models improve precision and efficiency in complex tasks, such as robotic manipulation and mobile sensor navigation. Efficiency enhancements are seen in methods utilizing continuous latent action spaces and equivariant diffusion models for better generalization. Additionally, combining reinforcement learning from human feedback with diffu-sion models enables zero-shot behavior customization, and adaptive replanning strategies ensure robust planning in dynamic environments. Overall, diffusion models advance RL by improving flexibility, efficiency, and performance in diverse applications."}, {"title": "V. SKILL LEARNING AND CONDITIONAL PLANNING", "content": "In this section, we delves into the state of the art in skill discovery, task planning, and trajectory generation through the lens of diffusion models and conditional generative ap-proaches. These methodologies offer solutions to complex problems, ranging from cross-embodiment skill transfer to adaptive planning in dynamic environments.\nThe papers introduced in this section are summarized in Table II, highlighting their target problems and the application of diffusion models within these fields.\nA. Skill Discovery and Planning\nSkill Discovery and Planning section focus on extracting and leveraging skills from both human and robot demonstra-tions, enabling robots to understand and replicate complex tasks through techniques like imitation learning, hierarchical planning, and generative models. This allows for efficient task execution based on skill abstraction and diffusion models.\nXSkill [30], positioned as a cross-embodiment skill dis-covery framework, leverages imitation learning to bridge the gap between human and robot demonstrations. By developing a self-supervised algorithm to extract skill prototypes from unstructured videos, this work introduces a skill-conditioned diffusion policy that translates human demonstrations into robot actions. Furthermore, it incorporates a skill alignment transformer to align and compose learned skills for executing unseen tasks based on human prompts. This study is pivotal in addressing the challenge of transferring skills across different embodiments, ensuring robots can perform tasks demonstrated by humans despite inherent physical differences.\nGenerative Skill Chaining (GSC) [31] tackles the complex-ities of long-horizon task and motion planning by integrating generative modeling with skill chaining. Their main proposal lies in its probabilistic framework that captures the joint distribution of skill preconditions, parameters, and effects. This approach allows for efficient generation of skill sequences through forward and backward diffusion processes, ensuring both initial state and final goal conditions are met. Addition-ally, GSC incorporates constraint-based planning to enhance the feasibility of generated plans, making it a scalable solution for complex, multi-step tasks that require dynamic adaptation.\nSkillDiffuser [32] offers an interpretable hierarchical plan-ning framework, combining skill abstractions with diffusion-based task execution. It proposes an end-to-end approach where discrete, interpretable skills are learned and used to condition a diffusion model for trajectory planning. By uti-lizing a classifier-free guidance model, it generates state tra-jectories that align with high-level skill abstractions derived from visual and language inputs. This framework excels in providing interpretable visualizations of skills, ensuring robust and adaptable performance across multiple tasks, including those with ambiguous language instructions.\nThe development paths of these technologies illustrate distinct but complementary approaches; XSkill [30] empha-sizes cross-embodiment skill transfer through self-supervised learning and skill-conditioned policies. GSC [31] focuses on long-horizon planning with a probabilistic approach to skill chaining, leveraging forward and backward diffusion processes. SkillDiffuser [32] integrates hierarchical planning with interpretable skill abstractions, utilizing diffusion models to generate coherent state trajectories.\nB. Conditional Planning\nThis section emphasizes generating task-specific trajectories and policies by conditioning on various contextual inputs, such as task requirements and environmental constraints. This ap-proach uses diffusion models and other generative techniques to create feasible and optimal plans from offline data, thereby enhancing decision-making and adaptability in diverse and dynamic scenarios.\nMetaDiffuser [33] stands out in the realm of offline meta-reinforcement learning by introducing a context-conditioned diffusion model for task-oriented trajectory generation. Its dual-guided module promotes both dynamic consistency and high returns, demonstrating superior generalization across di-verse unseen tasks without environment interaction during the learning process. The proposal to integrate a context encoder with a conditional diffusion model allows for the generation of trajectories tailored to specific tasks, enhancing dynamic feasibility and high-return outcomes.\nIn contrast, Decision Diffuser [34] positions conditional generative modeling as a viable alternative to traditional reinforcement learning methods. This approach leverages a return-conditional diffusion model to simplify policy gener-ation by avoiding value function estimation, thus bypassing the complexities of dynamic programming. The flexibility of this model in handling various constraints and skill composi-tions during inference is particularly notable, highlighting its broader applicability in decision-making tasks.\nAdaptDiffuser [35] further extends the diffusion model-based planning methods by incorporating an evolutionary approach. It generates and utilizes synthetic data guided by reward gradients to fine-tune the diffusion model, enhancing adaptability and performance on both seen and unseen tasks. This framework's innovative use of gradient-based guidance during the generation process ensures that the data produced aligns closely with task-specific requirements, providing sub-stantial performance improvements over prior methodologies.\nLastly, Constraint-Guided Diffusion Policies (CDG) [36] ad-dresses UAV trajectory planning through an imitation learning-based approach. By dividing the trajectory planning problem into sub-problems of collision avoidance and dynamic feasi-bility, CGD employs block-coordinate descent to iteratively optimize trajectory parameters. This method demonstrates remarkable adaptability to new constraints, ensuring dynami-cally feasible and collision-free UAV trajectories in real-time scenarios.\nComparatively, MetaDiffuser [33] and AdaptDiffuser [35] emphasize enhancing generalization and adaptability through innovative uses of synthetic data and dual-guided modules. De-cision Diffuser [34] approach simplifies policy generation and showcases flexibility in decision-making contexts. Meanwhile, CDG [36] focuses on real-time adaptability and constraint handling in UAV planning.\nSummary - Skill Learning and Conditional Planning\nIn this section, we explored advancements in skill learn-ing and conditional planning using diffusion models. These innovations focus on various aspects such as cross-embodiment skill discovery, long-horizon task and motion planning, hierarchical planning, offline meta-reinforcement learning, and UAV trajectory planning. The methodolo-gies presented leverage the power of diffusion models to enhance the flexibility, scalability, and generalization capabilities of robotic systems. By integrating skill learn-ing with conditional planning, these approaches address complex planning problems, ensuring robust and adaptive performance across diverse environments and tasks."}, {"title": "VI. SAFETY AND ROBUSTNESS IN PLANNING", "content": "This section explores the safety and robustness of diffusion model planning, building on existing research in offline rein-forcement learning robustness, such as the work by Panaganti et al. [66]. First, we discuss methods the safety mechanisms for diffusion-based planning, ensuring the generation of safe plans. Next, we address approaches that manage uncertainty and partial observations, improving the adaptability and ac-curacy of diffusion models in dynamic environments. The papers introduced in this section are shown in Table III, which explains their target problems, usage of diffusion models.\nA. Safety Mechanisms\nIn the domain of safety mechanisms, the integration of DDPMs with control barrier functions (CBFs) has been a development [37]. The framework developed for safety-critical optimal control employs three distinct DDPMs for dynami-cally consistent trajectory generation, value estimation, and safety classification. This method introduces a guided sam-pling procedure for generating optimal and safe trajectories, leveraging conditional sampling schemes that combine value function and safety classifier guides. The primary contribution lies in combining performance and safety through modular components that can be retrained for new environments, promising scalability and efficiency enhancements.\nAnother contribution focuses on refining the reliability of plans generated by diffusion models [38]. The introduction of the \"restoration gap\" metric evaluates the quality of gen-erated plans, coupled with a gap predictor for refinement. This approach ensures more feasible and reliable plans by guiding the reduction of the restoration gap and mitigating adversarial guidance through attribution map regularization. This method demonstrates its effectiveness across different benchmarks, emphasizing its potential for real-world, safety-critical applications.\nBy incorporating finite-time diffusion invariance and em-bedding control barrier functions into the diffusion process, SafeDiffuser [39] ensures safety constraints are consistently met. The development of different variants like Robust-Safe, Relaxed-Safe, and Time-Varying-Safe Diffusers highlights its adaptability to various safety-critical tasks, demonstrating ro-bustness and effectiveness in diverse environments such as maze planning and robot locomotion.\nFurther advancing planning reliability, Cold Diffusion on the Replay Buffer (CDRB) [40] utilizes cold diffusion to optimize the planning process via an agent's replay buffer of previously visited states. This method improves obstacle avoidance during planning tasks, utilizing k-means clustering to manage buffer size efficiently, thereby enhancing computational efficiency without compromising performance.\nIn terms of satisfying temporal and symbolic constraints, LTLDOG [41] modifies the inference steps of the reverse pro-cess based on finite linear temporal logic (LTLf) instructions. By introducing a satisfaction value function on LTLf and employing differentiable evaluation and regressor-guidance neural networks, LTLDoG ensures the adherence to static and temporal safety constraints during trajectory generation. This framework demonstrates its utility in robot navigation and manipulation tasks, underscoring its potential for complex and dynamic environments.\nIn summary, while DDPMs and their integration with CBFs and guided sampling address dynamic consistency and safety [37], restoration gap metrics [38] and Cold Diffusion [40] enhance plan feasibility and reliability. SafeDiffuser [39] en-sures safety through control barrier functions, and LTLDoG [41] focuses on temporal and symbolic constraint satisfaction. Together, these contributions provide a robust foundation for developing safe, reliable, and efficient planning mechanisms across various applications.\nB. Managing Uncertainty\nUnderstanding and managing uncertainty and partial ob-servations are crucial in advancing plans' robustness and adaptability.\nPlanning as In-Painting [42] introduces a language-conditioned diffusion model for task planning in partially observable environments. This framework utilizes the diffusion model to jointly model state trajectory and goal estimation, facilitating on-the-fly planning by dynamically updating plans based on newly observed information. Its effectiveness has been demonstrated in various embodied tasks, such as vision-language navigation and object manipulation, underscoring its robustness in handling environmental uncertainties. Future research aims to improve real-world applicability through bet-ter handling of uncertainties, integration of complex language instructions, and expansion to 3D spaces.\nAnother advancement is DYffusion [43], a dynamics-informed diffusion model designed for spatiotemporal fore-casting. This model incorporates temporal dynamics directly into the diffusion process, utilizing a time-conditioned interpo-lator and forecaster network to enhance forecasting efficiency and accuracy. Future directions include exploring advanced stochastic methods, developing faster inference strategies, and integrating new neural architectures for improved perfor-mance.\nPlanCP [44] introduces conformal prediction for uncertainty-aware planning with diffusion dynamics models. This work enhances the robustness and accuracy of trajectory predictions by incorporating uncertainty estimates, demonstrated across various planning and offline reinforcement learning tasks. This method combines conformal prediction with diffusion models to reduce uncertainty while maintaining performance.\nDiMSam [45] utilizes diffusion models as samplers for task and motion planning (TAMP) under partial observability. This approach leverages deep generative diffusion models to learn constraint-satisfying samplers for TAMP, defining constraints on a latent embedding of object states to handle unseen objects. By integrating these samplers within a classical TAMP solver, it achieves long-horizon constraint-based reasoning, demonstrated on articulated object manipulation tasks. Future work will explore advanced stochastic methods for the forward process, use more advanced ODE solvers, and apply the model to real-world problems where input and output spaces differ.\nIn summary, the advancements in diffusion models for plan-ning under uncertainty reflect a diverse array of approaches; The Planning as In-Painting [42] focuses on dynamic task planning, DYffusion [43] enhances spatiotemporal forecasting through temporal dynamics, PlanCP [44] integrates conformal prediction for uncertainty-aware planning, and DiMSam [45] uses diffusion models as samplers for TAMP. These studies improve the robustness, accuracy, and applicability of planning models in uncertain and complex environments from various perspectives.\nSummary - Safety and Robustness in Planning\nThe section on robustness in planning highlights the ad-vancements in integrating safety mechanisms and manag-ing uncertainty in diffusion-based planning models. The discussed frameworks emphasize enhancing safety-critical applications through control barrier functions and denoising diffusion probabilistic models, ensuring optimal and safe trajectory generation. Techniques like the CDRB [40] and the use of restoration gap metrics refine the quality and feasibility of generated plans. Additionally, approaches for managing uncertainty, such as the Planning as In-Painting [42] framework and the integration of conformal prediction with diffusion dynamics models, demonstrate improve-ments in planning reliability under uncertain conditions. Overall, these studies enahnce the diffusion models to deliver robust, reliable, and safe planning solutions across diverse and complex environments."}, {"title": "VII. DOMAIN-SPECIFIC PLANNING", "content": "The application of diffusion models is widespread in fields such as robotics, autonomous driving, and instructional content analysis. As summarized in Table IV, this section explores domain-specific planning and provides detailed insights into how diffusion models are employed to solve specific planning problems.\nA. 3D Planning and Optimization\nPlanning in three-dimensional environments is vital for advancements in robotics, autonomous systems, and complex scene understanding, offering solutions that bridge the gap between theoretical models and practical applications.\nScene Diffuser [46", "47": "focuses on vi-sual imitation learning and robot manipulation. DP3 integrates 3D visual representations with diffusion policies, enabling effective visuomotor policy learning for a wide range of robotic tasks. By employing a compact 3D visual representa-tion extracted from sparse point clouds and an efficient MLP encoder, DP3 can handle 72 simulation tasks with minimal demonstrations, achieving high success rates in real robot tasks. This approach highlights its generalization capabilities across various dimensions, including space, viewpoint, and instance variability.\nA distinct but related approach involves using 3D diffusion models as priors for RGB-based one-shot view planning to optimize object reconstruction tasks [48", "46": "and 3D Diffusion Policy (DP3) [47", "48": "while distinct in its application, shares the underlying prin-ciple of leveraging 3D diffusion models to enhance planning efficiency and effectiveness.\nB. Procedure Planning in Instructional Videos\nThis section explains the advancements in leveraging diffu-sion models to enhance the generation of action plans from instructional content.\nPDPP [49", "50": "advances procedural planning by integrating task-oriented attention mechanisms to manage decision spaces more effectively. This model enhances its functionality by leveraging joint visual-text embeddings from pre-trained vision-language models, which improve task classification ac-curacy. The fusion of task-aware diffusion with text-enhanced visual representation learning forms a crucial component of this model's methodology.\nMeanwhile, ActionDiffusion [51", "49": "distinguishes itself with its inno-vative projected diffusion method and its strategy of removing intermediate supervision, which simplifies the training process. Masked Diffusion model [50", "51": "excels through its emphasis on temporal dependencies, employing an action-aware noise mask that enhances the generation of coherent and contextually appropriate action plans.\nC. Autonomous Driving\nDiffusion-ES [52"}]}