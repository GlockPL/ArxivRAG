{"title": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced Multi-Agent Reinforcement Learning Approach", "authors": ["Xin Tang", "Qian Chen", "Wenjie Weng", "Binhan Liao", "Jiacheng Wang", "Xianbin Cao", "Xiaohuan Li"], "abstract": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment capabilities, prompting the development of UAVs for various application scenarios within the Internet of Things (IoT). The unique capabilities of UAVs give rise to increasingly critical and complex tasks in uncertain and potentially harsh environments. The substantial amount of data generated from these applications necessitates processing and analysis through deep neural networks (DNNs). However, UAVs encounter challenges due to their limited computing resources when managing DNN models. This paper presents a joint approach that combines multiple-agent reinforcement learning (MARL) and generative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed at reducing latency from task capture to result output. To address these challenges, we first consider the task size of the target area to be inspected and the shortest flying path as optimization constraints, employing a greedy algorithm to resolve the subproblem with a focus on minimizing the UAV's flying path and the overall system cost. In the second stage, we introduce a novel DNN task assignment algorithm, termed GDM-MADDPG, which utilizes the reverse denoising process of GDM to replace the actor network in multi-agent deep deterministic policy gradient (MADDPG). This approach generates specific DNN task assignment actions based on agents' observations in a dynamic environment. Simulation results indicate that our algorithm performs favorably compared to benchmarks in terms of path planning, Age of Information (Aol), energy consumption, and task load balancing.", "sections": [{"title": "I. Introduction", "content": "Advancements in Unmanned Aerial Vehicles (UAVs) technology have led to their increasing use across various civil and military domains, including aerial object detection, swift rescue operations in disaster-stricken areas, emergency scenarios, and extensive agriculture. Unlike traditional Internet of Things (IoT) devices, UAVs offer not only lower costs but also superior mobility and enhanced data collection capabilities. While UAVs are well-suited for the critical and complex tasks mentioned, managing the substantial data they generate remains a challenge. With the rise of artificial intelligence (AI), deep neural networks (DNNs) have been proposed to handle the extensive data produced. Despite their efficiency in processing data, DNNs require significant resources due to their intricate structure and high performance demands. General methodologies segmented DNN models by layers, treating each layer as the primary computational unit, which often led to inefficiencies in collaborative inference due to the substantial computational demands of certain layers.\nGiven the size and cost constraints of UAVs, their power supply is often limited, and their computing capacity is relatively low. Generally, there are two primary approaches to tackle this issue. One approach involves deploying lightweight models on UAVs, which reduces computational demands and"}, {"title": "B. Motivation and challenges", "content": "Although current research on collaborative DNN in- ference within mulit-UAV offers a promising approach for processing DNN-based tasks, significant issues still need to be resolved [9]\u2013[15], [17], [18].\nThe primary challenge is that conventional scene data collection and monitoring tasks rely on staged task processing. For example, in [10], [24], the methods involve an initial phase of task planning, followed by data collection, and concluding with online or offline processing of related tasks, such as target detection and positioning. This sequential approach is evidently inadequate for meeting the real-time requirements of sudden events or emergency scenarios. Moreover, the limited computational power, battery life, and other onboard resources of UAVs further constrain their ability to perform real-time tasks effectively.\nSecondly, when DNN tasks need to be offloaded, many studies explore the transfer of intermediate data to ground nodes [13], [15], [18]. However, non-line-of-sight communica- tion and the high relative mobility between UAVs and ground nodes especially dynamic nodes-introduce significant un- certainty into the offloading process. Additionally, existing methods for offloading computationally intensive tasks, such as heuristic-based, decomposition-based [10], and game-based approaches [20], [25], typically require numerous iterations to converge to a satisfactory and stable assignment decision. This iterative decision-making process is often impractical for latency-sensitive tasks and energy-constrained systems.\nThirdly, non-uniformities in the processing pipeline of DNN-based tasks are apparent. To elucidate this, we conduct a pilot study analyzing layer-wise execution latency, interme- diate output data size, and computational demands for Yolov5 [26], AlexNet [27], and VGG16 [28]. The results, shown in Fig. 2, demonstrate most of the computational cost is concentrated in the first half of the DNN, with considerable variability in latency, output data size, and computation across different layers of these DNNs. This underscores the need for uniform task division. Arbitrary division can lead to significant imbalances in task assignment among UAVs, which may adversely affect the operational efficiency and survival time of the multi-UAV, thereby hindering its overall effectiveness in multi-UAV operations [12]. To delineate the differences between the proposed DNN task assignment strategy and existing studies in this field, a comparison of various works is presented in Table I. Performance metrics meeting the specified criteria are indicated with a \u2713."}, {"title": "C. Summary of contributions", "content": "In this paper, our approach presents several distinctions from prior research. Specifically, we develop a collaborative inference model system for mulit-UAV network. The model processes image classification requests by distributing and assigning the inference computations to the most suitable UAVs to minimize collaborative inference latency. This ap- proach also considers each UAV's memory footprint, comput- ing resources, energy consumption, and the number of layers"}, {"title": "B. MARL for task offloading", "content": "MARL enables each agent to optimize its policy through observations of both the environment and the policies of other agents [33], [34]. Many existing MARL-based approaches significantly enhance task offloading efficiency within dy- namic networks [19], [35]. The authors in [36] propose a dedicated actor neural network for coordination and a scalable training algorithm for UAV-aided MEC task offloading. In [37], the authors present a DRL-based joint secure offloading and resource allocation scheme for vehicular edge computing networks. Task processing latency is a critical metric for evaluating the performance of task offloading strategies. The studies have given limited attention to this aspect. In [38], the authors propose an adversarial MARL scheme for secure computational offloading and resource allocation in a multi-UAV-assisted MEC system. The authors in [39] present a mobility-aware service offloading and migration scheme with Lyapunov optimization and a MADDPG algorithm. In [40], a distributed task offloading strategy based on MARL is introduced to optimize task allocation efficiently. The authors in [41] focus on optimizing task offloading and UAV trajectory under energy and queue latency constraints. However, most of these studies concentrate on optimizing network performance metrics, including task completion latency and energy con- sumption. Our research emphasizes DNN inference services, where the Aol for DNN tasks as a critical performance metric."}, {"title": "C. GAI for UAV networks", "content": "GAI has the potential to effectively overcome the lim- itations of conventional AI and can be applied to optimize UAV networks, particularly in improving transmission rates, communication capacities, and energy efficiency [42]-[44]. In [45], the authors present a radio propagation model with a conditional generative adversarial network. The authors in [46] introduce a two-stage generative neural network to predict link states and generate path losses, latencys, and angles of arrival and departure in millimeter wave communication for UAVs. The authors in [47] explore the long-term optimization of joint task offloading and resource allocation in a multi-access edge computing network for UAVs. In [48], the authors present a hierarchical aerial computing system where UAVs collect tasks from ground devices and offload them to HAPs. Different from the above schemes, we contribute to the existing literature by presenting a novel application of GDM in the context of DNN task assignment decision. Our approach incorpo- rates a path planning, GDM-based MARL decision-making framework specifically tailored for multi-UAV networks. This methodology aims to minimize the cost associated with DNN task completion while ensuring the system's reliability."}, {"title": "III. System model", "content": "In this section, we first present the considered network architecture, then detailly introduce the DNN task model, communication model, mobile model, AoI model, and energy consumption model, respectively."}, {"title": "A. Network architecture", "content": "The network architecture is primarily composed of an HAP layer and a UAV swarm layer, as illustrated in Fig. 3. The HAP layer consists of a tethered platform with extensive computing and communication capabilities, such as an airship. This platform exchanges information with UAVs via wireless links to facilitate model training [49] and path planning (see Section V). The UAV swarm comprises a leader UAV and multiple wingman UAVs. All UAVs are isomorphic, with the total number of UAVs being $n = \\{1,2,\\ldots, N\\}$, and each is equipped with trained DNN models. The leader UAV receives path planning results transmitted by the airship and operates according to the predetermined route. It is also responsible for collecting and processing images and point cloud data of the target area, including tasks such as target detection and mapping. Thus, it functions as both a task producer and executor, as well as a decision-maker for task assignment. The wingman UAVs primarily execute the tasks offloaded by the leader UAV, with the assignment of DNN tasks between wingman UAVs being streamlined. The leader UAV evaluates the task size and the remaining resource status of the wingman UAVs to determine whether to offload tasks. Therefore, to prevent task overload on the UAVs, extend the operational lifespan of the UAV swarm, and enhance the timeliness of task processing and network stability, efficient task assignment decisions for the UAV swarm are particularly crucial."}, {"title": "B. DNN Task Model", "content": "The area where the task is performed in this paper consists of q target area center coordinate points, denoted as $q = 1, 2, ..., W$, and the sequence of the UAV inspection target points is represented as $q(q) = [q(1),...,q(W)]$. The task data size in the target area is $l_q = \\{1, ..., L_w\\}$. Assume that the number of DNN tasks in each synchronization cycle is $i = \\{1,2,..., I\\}$, the DNN type is $\\kappa = \\{1,...,K\\}$ (such as Yolov5 [26], VGG16 [28], etc.). The DNN model typically comprises l layers, denoted as $l = \\{1, 2, ..., L\\}$. Each layer has a cache capacity $m_{i,l}$ and a computing requirement $c_{i,l}$,"}, {"title": "C. Communication Mode", "content": "DNN task assignment is performed within the UAV swarm, and during the task execution, the UAV swarm main- tains a fixed formation flying. At time slot t, the coordinates of each UAV are designated as $P_n = [x_n, y_n, z_n]$, and the distance between the two UAVs is defined as follows\n$d_{n,n+1} = \\sqrt{ ||P_n - P_{n+1}||^2}$.\nThe UAV-to-UAV data link utilizes line-of-sight commu- nication, and its channel model is described by the Close-in (CI) free space reference model [35], [43]. The path loss is expressed as follows\n$PLC1(d_{r,n+1}, f) = PLFS,ref(f)+10n_{CI}log_{10} (d_{ren+1}) + \\xi_{0,CI}$,\nwhere $PLFS,ref (f)$ represents the free space path loss per unit length, $n_{CI}$ is the path loss index, and $\\xi_{0,CI}$ is the shadow fading parameter, which is typically assumed to follow a zero- mean Gaussian distribution."}, {"title": "D. Mobile Mode", "content": "Let $\\Upsilon_{q}$ denote the binary decision variable for the in- spection target point $9_{q(q)}$; specifically, when the target point $9_{q(q)}$ is inspected by the UAV, $\\Upsilon_{q}$ is equal to 1. Otherwise, is equal to 0. After the UAV swarm collects data from the target point $9_{q(q)}$, it must process the data before reaching the next target point $9_{q(q+1)}$. The Euclidean distance between the two target points is denoted as $D_{q(q),q(q+1)}$. The total moving distance of the UAV to complete the inspection task is the sum of the distances of each segment along its flying path, as illustrated below\n$D_{total} = \\sum_{q(1)}^{q(q)} D_{9(9), 9(q+1)}$.\nAssuming that the UAV flies at a constant speed $v$, the time taken by the UAV swarm to travel from the current target point to the next target point is expressed as follows\n$t_{next} = D_{q(q),q(q+1)}/\\nu$.\nThe total flying time for the entire process of the UAV executing the task is represented as follows\n$t_{fly} = D_{total}/\\nu$."}, {"title": "E. Aol Model", "content": "Aol defined in this paper aims to measure the duration from data collection to the completion of processing, incorpo- rating waiting latency, transmission latency, and computation latency to reflect the real-time performance of DNN task execution. We assume that each UAV has a maximum memory capacity $m_{threshold}$, available energy $e_{threshold}$, and comput- ing capacity $f_n$. The Aol is optimized by minimizing the UAV's moving distance and refining the DNN task assignment strategy. The waiting latency for the data collected by the leader UAV to be processed by the DNN model is denoted as\n$t_{await}^{i,n}= T_{i} - \\Upsilon_i$,\nwhere $T_i$ represents the moment when the collected raw data is obtained on the leader UAV or when the subtask $n$ is generated on a wingman UAV. $\\Upsilon_i$ signifies the moment when the collected raw data is processed on the leader UAV or when"}, {"title": "F. Energy Consumption Model", "content": "The energy consumption of UAVs in performing DNN tasks primarily consists of computing energy consumption, transmission energy consumption, and flying energy consump- tion.\n1) The computing energy consumption of the UAV is denoted as\n$e_{n}^{comp} = \\sum_{i=1}^{I}\\sum_{l=P_{i,n'-1}+1}^{P_{i,n'}}{k_0c_{i,l}f_{n}}$,\nwhere $k_0$ represents the energy efficiency parameter and $c_{i,l}$ denotes the computing requirement or complexity of the $l$-th layer in the $i$-th task.\n2) The transmission energy consumption is approximated as the product of the transmission power $P_n$ and the transmis- sion time $t^{trans}_{i,n}$ of the intermediate result of the DNN task as follows\n$e^{trans}_{i,n} = P_n t^{trans}_{i,n}$.\n3) As one of the energy consumptions that must be considered, the flying energy consumption of the UAV is adopted in [19]. The flying energy consumption is approximated as the product $e^{fly}$ of the propulsion power $P^{fly}$ of the UAV and the total flying time during the task execution as follows\n$e^{fly}= P^{fly}t_{fly}$,\n$P^{fly} = P_1 \\sqrt{ 1 + \\frac{3v^2}{V_{tip}}} + P_2  \\sqrt{ 1 + \\frac{1}{2} \\sqrt{ \\frac{v^2}{2\\nu^{2}}}}."}, {"title": "IV. Problem formulation", "content": "In this paper, AoI and load balancing are comprehensively considered in the DNN task assignment of UAV swarms. Generally, completing tasks solely by UAVs is a common method to enhance AoI. However, this approach often leads to a significant increase in the energy consumption of individual UAVs, thereby reducing the overall survival time of the UAV swarm. To optimize load balancing in task assignment, frequent DNN task transfers within the UAV swarm may also extend task execution time and potentially deteriorate the timeliness of the collected data. Therefore, during the task execution process, it is essential to establish a reasonable DNN task partitioning and an assignment scheme that effectively balances task completion rate $\\eta = [l_{completed}/l_k]$, AoI, and load balancing to improve the stability of the UAV swarm.\nTo this end, this paper defines a utility function $U$ for DNN task assignment, which includes three components: the utility $u_1$, representing the contribution of a single UAV to a specific task; the utility $u_2$, which reflects the task completion rate and emphasizes the importance of both completing the task and improving the AoI; and the UAV load balancing utility $u_3$, indicating the variance of the remaining energy of the UAVs. This last component is crucial for maintaining balanced energy consumption across the UAV swarm.\n$U = \\delta u_1 + \\epsilon u_2 + \\theta u_3$,\n$u_1 = e^{comp} + e^{trans}$,\n$u_2 =\\begin{cases}\\frac{\\alpha\\eta_k}{\\beta Aol} i, \\text{task is completed} \\\n(\\frac{\\Gamma(\\I_{completed})}{\\I_{k}}) - \\beta\\tau_{max}, \\text{ otherwise}\\end{cases}$,\n$\\begin{aligned} u_3 = \\sum_{n=1}^{N} \\frac{\\{e_{threshold} - e_{n} - \\frac{e_{thresthold} - e_{n}}{N}\\}^2}{N} \\end{aligned}$,\nwhere $\\alpha, \\beta, \\gamma, \\delta,\\epsilon,\\theta$ are weight coefficients, $l_{completed}$ indicates the number of layers of the K-th DNN model that have been completed.\nConsidering the constraints related to the number of UAVs, cache capacity, energy consumption, and latency, we"}, {"title": "V. Path planning method based on greedy algorithm", "content": "The greedy algorithm is employed to optimize the UAV's path due to its low computational cost and fast convergence [3]. A stochastic component is introduced to the greedy algorithm, which considers the task size within the target area, thereby providing an approximate solution for the globally optimal UAV flight path."}, {"title": "A. Fitness Function", "content": "This paper employs a greedy algorithm to minimize the total flying distance of the UAV while jointly considering the task size of the target area to address the UAV path planning problem. Prior to executing DNN task assignment, the UAV must plan an optimal flying path to reduce task completion time and energy consumption. Given that the task sizes of each target area vary, we denote the task size of the q-th target point as $I_{Lq}$, and let $v_q$ represent the average rate at which the UAV swarm processes tasks. Consequently, the time required to process the task size at the q-th target point is denoted as\n$t_q = L_q/\\Upsilon_q$.\nTo enhance overall task processing efficiency and Aol, the UAV swarm is required to complete the task after inspecting the target point and before reaching the next target point, thus establishing the condition $t_q < t_{next}$. Additionally, we define $\\Delta t = t_{next} - t_q$ as the difference between the flying time and the task processing time between two task target points.\nAccordingly, the fitness function integrates the UAV flying distance and the quantity of tasks processed by the DNN model, represented as\n$F = \\nu D_{total} + \\rho \\Delta t$,"}, {"title": "B. Path Planning Method", "content": "The greedy algorithm is a straightforward, lightweight heuristic solution method that is easy to implement. Its funda- mental principle involves making the best choice at each step based on the current state, with the aim of achieving a global optimal solution through a series of local optimal solutions [3]. However, this approach can lead to suboptimal outcomes, as selecting local optimal solutions does not guarantee a global optimal solution. To address this limitation, this paper incor- porates randomness into the design of the greedy algorithm to mitigate the risk of converging on a local optimal solution.\nSpecifically, k candidate target points are randomly se- lected from the set of uninspected target points, denoted as W. If $\\lvert W\\rvert \\le k$ indicates that all uninspected target points are chosen as candidate target points k, then we define this"}, {"title": "VI. DNN task assignment algorithm based on GDM-MADDPG", "content": "GDM demonstrates robust generative capabilities and is adept at navigating complex dynamic decision optimization scenarios. Notably, it can identify optimal solutions even in the absence of a dedicated dataset. This section focuses on the design methodology and algorithmic framework of the GDM-MADDPG method. It also encompasses the design and modeling of both the diffusion reverse denoising process and MDP."}, {"title": "A. Design process of GDM-MADDPG algorithm", "content": "Task assignment decision generation differs from tra- ditional applications of GDM, primarily because decision optimization often lacks a large, dedicated dataset for offline training. In this paper, the DNN task assignment decision pro- cess involves the GDM predicting noise distribution through iterative refinement, followed by training its reverse denoising process, specifically denoising Gaussian noise to yield optimal action decisions. This training enables the agent to execute the generated actions within the environment. The agent then fine-tunes its network parameters based on received rewards, thereby enhancing its action decision generation capabilities. This approach transforms the challenge of limited datasets into an opportunity for dynamic online learning and agent self- optimization. Building on these principles, the design process encompasses the following steps\nAgent Environment Design: The types of tasks and data sizes at various target points are heterogeneous and subject to dynamic changes. GDM is crafted to generate optimal assignment actions, including split point selection and task selection, based on the observations of a given agent.\nAction Space Design: The action space is established by applying a series of denoising steps on Gaussian noise via GDM, with the action vector's dimension corresponding to the number of tasks.\nUtility Function Design: The objective is to derive action strategies for DNN task assignment through GDM, specifically targeting optimal solutions related to aspects such as Aol, load balancing, and energy consumption.\nModel Training Design: This step focuses on training the action network to enable the agent to produce specific actions based on observations in the designated environment, all with the aim of maximizing the utility function."}, {"title": "B. Reverse denoising process of GDM", "content": "The inverse denoising process of GDM serves as a reverse denoising technique that reconstructs original data from noisy observations. In this context, the denoising network, once fully trained, is capable of generating optimal decisions for DNN task assignment in any dynamic environment. Both the selection of assignment modes and the design of utility functions typically adapt to the dynamic variations present in the environment. This capability to respond to changing conditions and generate appropriate actions is invaluable in decision optimization. Consequently, the inverse denoising process of the GDM can leverage information such as the UAV observation space O, task data size $l_q$, and task type $\\kappa$ as conditions within the denoising network, thereby facilitating adaptive decision generation."}, {"title": "V. Experiment setup and performance evaluation", "content": "To assess the efficacy of the proposed DNN task assign- ment algorithm, we conduct numerical experiments and ana- lyze its performance. This analysis includes a comparison with several benchmark algorithms to demonstrate its effectiveness and advantages."}, {"title": "A. Experiment setup", "content": "The hardware configuration consists of an Intel i7- 13700K CPU and an NVIDIA RTX 4090 with 24 GB of RAM. The software environment includes pytorch GPU ver- sion 1.11.0 and Python version 3.8. The parameters for DNN are specified as follows: for the Actor network, the number of neurons in the first and second hidden layers are set to 256, respectively, while the number of neurons in the third hidden layer is determined by the dimensionality of possible actions. For the Critic network, the neuron counts for the three hidden layers are set to 256, respectively. During training, the learning rates for the Actor and Critic networks are set to$10^{-3}$. The mini-batch size during training is 512. The discount factor is set to m = 0.9, and the initial value and decay rate for exploration are defined as $\u03f5_0$ = 0.9 and $\u03b2 = 10^{-4}$, respectively.\nIn our simulation, we examine the performance of col- laborative DNN task assignment across different data sizes. To simplify calculations, we assume a constant transmission model, where the data transmission rate remains uniform during a complete collaborative task assignment. The experi- mental parameters and their corresponding values are detailed in Table II [51]."}, {"title": "2) Simulation scenario", "content": "We perform simulations under the following scenario: one airship positioned at an altitude of 10 km, accompanied by nine UAVs at an altitude of 3 km, covering an area of 12\u00d712 km\u00b2. The airship is located at the center of this area, while the leader UAV and wingman UAVs are uniformly distributed throughout it. The airship obtains the mission directives from the ground control center, formulates the optimal flight sequence for the designated mission targets, and transmits the planned route information to the lead aircraft. The lead aircraft, along with several wingmen UAV, then executes a coordinated flight along the predetermined route. It is important to note that all UAVs fall within the coverage area of the airship and are interconnected and capable of communicating with one another. Additionally, considering overall accuracy in detecting target amounts, the Yolov5 model outperforms all seven attention-optimized Yolov5 variants as well as other state-of-the-art object detection models [4]. In this study, we employ Yolov5 as the DNN inference task model and conduct comparative testing experiments that focus on Aol, load balancing, and system stability to evaluate the effectiveness of the proposed method."}, {"title": "3) Benchmark algorithms", "content": "To demonstrate the advantages of the proposed method, we examine the following benchmark algorithms. To ensure a fair comparison, the structures and parameters of the deep neural networks (DNNs) are standardized. MADDPG is a policy-based reinforcement learning method, primarily utiliz- ing a centralized training and distributed execution framework. It extends single-agent reinforcement learning algorithms from the actor-critic framework to accommodate multi-agent sce- narios [41]. MADDPG with path planning integrates the optimal path planning approach presented in this paper with"}, {"title": "B. Performance evaluation", "content": "As illustrated in Fig. 5, the cost associated with the UAV's optimal pathfinding increases as the number of target points rises. Notably, the path planning algorithm proposed in this study consistently exhibits lower costs compared to the greedy algorithm across all evaluated scenarios. Specifically, for target point counts of 10, 20, 30, 40, and 50, the total cost produced by the proposed algorithm is reduced by approxi- mately 19.3%, 21.1%, 23.0%, 25.6%, and 27.1% respectively, in comparison to the greedy algorithm. This enhancement can be attributed to the expanding solution space that accompanies an increase in the number of target points. As the candidate set of target points grows, the proposed algorithm is afforded greater opportunities to explore solutions that yield lower total costs, ultimately facilitating the identification of the optimal path with the minimal total cost. Thus, it is evident that the algorithm developed in this study is particularly adept at addressing path planning challenges that involve a larger number of target points.\nThis experiment simulated a UAV inspection area consist- ing of eight target points, each defined by a distinct task size, as illustrated in Fig. 6. The experimental results are illustrated in Fig. 6a depicts the UAV path planning method based on the greedy algorithm we proposed. By comprehensively con- sidering both task size and flying path, this method generates the global optimal UAV flying path and target point sequence, prioritizing target points with larger task sizes while ensuring coverage of all target points. Conversely, Fig. 6b presents a greedy algorithm that does not incorporate a random selection strategy. This approach, which selects the best option at each step based solely on the current state, is prone to local optima. As a result, it can lead to a significant increase in flying path length, reducing the survival time of the UAV swarm and hindering its ability to conduct long-term operations under energy constraints."}, {"title": "2)Training Procedures", "content": "We conduct ten independent training runs for each al- gorithm. As illustrated in Fig. 7, although the proposed DM- MADDPG converges at a slower rate than the two algorithms, DM-MADDPG demonstrates superior performance compared to the other solutions. The total reward attained by DM- MADDPG increases throughout the training process, even- tually converging to approximately 5000 after around 600 training episodes. This enhancement is attributed to the gener- ative capabilities of GDM, which markedly improve action sample efficiency by progressively reducing noise through multiple denoising steps. Finally, the low performance of the two algorithm, further highlighting the effectiveness of the proposed GDM-MADDPG method.\nBy conducting a series of systematic experiments, we aim to identify the optimal values for two key parameters that influence the performance of MADDPG algorithm: 1) learning rate and 2) batch size. The experimental results, illustrated in Fig. 8a, clarify the convergence behavior of the reward function under varying learning rates. A learning rate of 0.01 is excessively large, preventing convergence even after 1000 training iterations. Similarly, learning rates of 0.0001 is not facilitate convergence. Notably, the 0.001 learning rate promotes faster convergence. The suitable learning rate of 0.001 is employed to assess the effects of different batch size on training performance. As illustrated in Fig. 8b, a batch size of 512 is recognized as the most advantageous option.\nThe convergence of GDM-MADDPG across various DNN model tasks is illustrated in Fig. 9. Despite the differ- ences in structural complexity among the DNN models, the proposed method successfully accommodates these tasks and achieves convergence. The AlexNet model, characterized by fewer layers and lower computational requirements, demon- strates favorable convergence performance. In contrast, the VGG16 model, with its greater number of layers and higher computational demands, experiences increased processing de- lays, resulting in lower rewards and reduced convergence effi- cacy for our method. Although the convergence performance of our method on the YOLOv5 model is slightly inferior to that of AlexNet, YOLOv5, as a widely adopted real-time object detection algorithm, has a model complexity that lies between the two aforementioned models. Consequently, subsequent experiments employ YOLOv5 as the experimental task model to validate the superior performance of this method in terms of AOI and task completion rate."}, {"title": "3)Benchmark algorithms:", "content": "This study compares the Aol of different methods across varying task sizes, as illustrated in Fig. 10. The Aol per- formance of the MADDPG-based method is significantly enhanced when combined with the path planning approach. Furthermore, the GDM-MADDPG method proposed in this paper, which builds upon MADDPG and incorporates the ad- vantages of GDM in handling complex data distributions and generating high-quality task assignment decisions, improves the timeliness and generalization ability of task assignment decision generation. This results in the generation of the most suitable task assignment decisions for the current state, thereby reducing the transmission time during serial task execution."}, {"title": "VIII. Conclusion", "content": "In this paper, we addressed the challenge of joint flying path planning, task assignment, and load balancing in UAV networks as a two-stage optimization problem. Our primary objective is to minimize the DNN-based task Aol while maximizing the survival time of the UAV swarm. In the first stage, we consider the task size of the area to be inspected and the shortest flying path as optimization constraints. We then employ a heuristic algorithm to optimize the UAV's flying path, focusing on reducing energy consumption and minimizing flying duration. In the second stage, we introduce a novel GDM-MADDPG algorithm to determine optimal DNN task assignment decisions. Through numerical simulations, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions."}]}