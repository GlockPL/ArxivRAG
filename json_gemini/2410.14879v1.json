{"title": "Vital Insight: Assisting Experts' Sensemaking Process of Multi-modal Personal Tracking Data Using Visualization and LLM", "authors": ["JIACHEN LI", "JUSTIN STEINBERG", "XIWEN LI", "AKSHAT CHOUBE", "BINGSHENG YAO", "DAKUO WANG", "ELIZABETH MYNATT", "VARUN MISHRA"], "abstract": "Researchers have long recognized the socio-technical gaps in personal tracking research, where machines can never fully model the complexity of human behavior, making it only able to produce basic rule-based outputs or \"black-box\" results that lack clear explanations. Real-world deployments rely on experts for this complex translation from sparse data to meaningful insights. In this study, we consider this translation process from data to insights by experts as \"sensemaking\" and explore how HCI researchers can support it through Vital Insight, an evidence-based 'sensemaking' system that combines direct representation and indirect inference through visualization and Large Language Models. We evaluate Vital Insight in user testing sessions with 14 experts in multi-modal tracking, synthesize design implications, and develop an expert sensemaking model where they iteratively move between direct data representations and AI-supported inferences to explore, retrieve, question, and validate insights.", "sections": [{"title": "1 INTRODUCTION", "content": "The ubiquitous presence of sensor-rich smartphones and wearables has prompted researchers to use the data from these devices to model human behavior and track daily living activities for a variety of outcomes including health tracking, context-aware applications, and digital health interventions [9, 38, 46, 59, 71, 78]. However, longstanding socio-technical gaps persist in collecting data and generating valuable insights, due to challenges such as limited information, noisy data, ambiguous contexts, and variations in individual behavior [1, 11, 16, 17, 32, 58]. Prior works have identified that it's extremely challenging for sensing systems and analytic algorithms to 'perfectly' capture and explain the complexity of human behaviors [60, 75, 93, 94, 98, 103]. To address these gaps, experts often put in manual effort to transform real-world data into meaningful and usable insights [8, 10, 50].\nThis manual effort of transferring data to insights by experts could be considered as a complex sensemaking process  an iterative procedure where individuals alternate between data (e.g. various sensing modalities), and develop an appropriate understanding of a specific situation (e.g. human behaviors) [34]. This sensemaking process employed by experts generates a deeper understanding of human behavior that exceeds the capabilities of machines. For instance, phone and wearable devices often fail to capture sufficient information about human behaviors, limiting machines to basic, rule-based insights like \"at home\", or 'black-box' machine learning results like \"walking\". Experts, however, can iteratively switch between different modalities to synthesize inferences with well-reasoned explanations. For example, they can validate \"walking\u201d by comparing accelerometer data from both the phone and watch, contextualize \"walking\" with GPS data indicating a location in a park, notice patterns of periodic \"walking\" and \"stationary\u201d events, and even infer \"walking the dog\" when realized the alignment with regular routines reported in the survey. It's even more challenging for machines to derive high-level \u2018insights' other than just 'results' from raw data, such as \u201cis it a normal day"}, {"title": "2 BACKGROUND", "content": "The concept of sensemaking emerged in the late 1970s [28, 77] and became more prominent in the 1990s, particularly in research related to organizations, education, and decision-making [27, 40, 54]. Since then, several research fields have adopted and expanded upon this notion, developing models tailored to explain the sensemaking process within their specific domains [5, 54, 57, 80, 86, 104]. Despite variations in definitions, the overarching idea remains that sensemaking is an iterative process involving information search and developing an understanding of a situation to take appropriate action. Russel et al. did multiple case studies to design the cost structure of the sensemaking process and presented a schema-based iterative model [86]. Pirolli and Card studied the sensemaking process in the context of intelligence analysts and proposed a model having information foraging and sensemaking loops [80]. Similarly, Klein et al. [55] presented a data frame model where sensemaking involves a cyclic process of aligning data with frames (explanatory structures) and adapting the frame to data.\nSome past works have also investigated sensemaking within the context of passive sensing for personal health. Mamykina et al. [66], situated self-tracking sensemaking behaviors of adolescents with Diabetes and their caretakers with the existing literature and proposed a theoretical framework for these behaviors. Co\u015fkun and Karahano\u011flu [23] did a systematic literature review of sensemaking practices in HCI literature and defined four sensemaking modes self-trackers go through (i.e., self-calibration, data augmentation, data handling, and realization). Researchers have also studied personal health sensemaking in collaborative online platforms [37, 82] and information delivery [14]. Although researchers have investigated sensemaking in self-tracking, sensemaking of participants' data by experts remains under-explored. Sensemaking by experts differs from that of participants, as it generally involves more comprehensive analysis and often lacks the contextual information that participants possess. In this paper, we fill this gap by understanding experts' sensemaking process through interviews and the Vital Insight tool."}, {"title": "2.2 Tools to assist the sensemaking of personal tracking data", "content": "Some past research has acknowledged the challenges of making sense of personal tracking data and has developed various tools to assist with different tasks. Among those tools aimed at experts, many focus on specific tasks, such as activity annotation on accelerometer data [43, 81], multi-modal sensor data [6, 29, 68], video analysis [30, 83, 91], and spatial-temporal events [85]. The majority of these tools are built around visualization, where the system serves as an interactive interface to better display data and assist in the annotation process. Beyond activity annotation, some tools take a broader approach to understanding multi-modal data. For instance, Lifestream integrates different data modalities and features into one dashboard to facilitate the exploration and evaluation of personal data streams with users [47]. Chung et al. used the EventFlow tool, which aggregates workflow information to highlight patterns and variabilities, helping different stakeholders understand the daily routines of older adults [20]. Some works also explored other modalities, such as conversational interfaces in helping the understanding of data [3, 44, 52, 89].\nMost of these tools mentioned above focus on better data representation as a means to help experts and users understand and reflect on the data. However, some tools recognize that simply presenting data is not enough for meaningful understanding, and leverage AI to provide automatic assistance. For example, machine learning models have been widely used to attach labels to sensing data and have proven successful in tasks such as activity recognition, stress detection and mental health [73, 74, 101], sleep analysis [56] and more. However, most of these tools are limited to ground truth labels and can only perform basic recognition tasks, failing to assist in more complex levels of understanding of"}, {"title": "3 INTERVIEW STUDY", "content": "In this section, we describe our initial interviews with experts about the challenges they face in deriving insights from personal tracking data, highlighting the needs and opportunities in assisting experts in their sensemaking process."}, {"title": "3.1 Method", "content": "We conducted an IRB-approved qualitative interviews with 14 experts in multi-modal sensing data with a $15 compensation. Through these hour-long semi-structured online interviews, we wanted to better understand the experts' unique research processes while inferring complex real-world personal tracking data. After obtaining verbal consent, we recorded the audio of the interviews and used the transcripts to synthesize insights. Two members of the research team carefully reviewed the transcripts and notes, and independently coded specific segments of the data, employing an inductive open coding approach [21]. The coders then exchanged codebooks and discussed to harmonize and merge the codes by consensus, and conducted a thematic analysis under the principles of Grounded Theory [13, 39]. We recruited a total of 14 experts in multi-modal sensing data and personal tracking. Each expert completed a demographic survey and reported their level of expertise. They had diverse backgrounds in gender and race, with an average of 6.64 years of experience in personal tracking data. We reached data saturation with 14 participants and stopped further recruitment [2, 42]. Detailed demographic information is provided in Table 1."}, {"title": "3.2 Results", "content": "In this section, we present our findings from the preliminary interviews, highlighting the various challenging sensemaking scenarios experts encounter in real-world deployments, along with needs and opportunities for assistance."}, {"title": "3.2.1 Challenging sensemaking scenarios in current practices", "content": "The first significant challenging scenario is contextual-izing the data given the highly personalized nature of an individual's life. The experts we interviewed mentioned encountering people living in unconventional settings such as boats or RVs, or with conditions like using a cane, making it difficult to create a unified approach to interpret their behaviors. However, in many cases, researchers do not leverage critical information about a person's lifestyle during data analysis. Our participants highlighted a concerning issue where \"none of that (user information) gets posted to the data afterward\" and is \"lost track of\".\nAnother challenge stems from unpredictable human decisions, which create a vast array of possible events. For instance, one expert recounted an incident where a participant accidentally dislodged a motion sensor and stored it in a drawer without notifying their research team. It took a while for the team to notice the \"little spurts every so often\", and they could not deduce what had happened until talking with their participants. Due to the inherent limitations of passive sensing systems, experts often have to speculate about the events behind the data. However, such speculations require the experts to rely on their own experiences and imagination, which could be restrictive and even introduce biases in the inference. Thus, P9 expressed a desire for creative interpretations that could offer insights beyond their initial understanding as useful inspiration. This highlights a challenging sensemaking process of inferring unpredictable variety of real-world events through experts' limited ability to hypothesize what may have occurred.\nThe example of the fallen motion sensor reveals a prevalent scenario while interpreting personal tracking data: capturing and explaining anomalies. Nearly all experts point out that things can easily go awry in a longitudinal study, where identifying the cause of an anomaly is challenging due to the lack of a straightforward correlation between issues and their causes. There are inherent data limitations that experts must consider, for example, GPS location from the phone is only valid if the participant was carrying their phone with them (P3). Similarly, discrepancies in step counts between a phone and a watch can arise from varying levels of device compliance, instead of being an 'actual' anomaly in data collection. Successfully identifying and rationalizing anomalies demands a deep understanding of both system limitations and human behavior, a skill that typically only experts possess. However, even for experts, the task remains cognitively challenging as it often requires cross-referencing multiple data sources to determine whether an anomaly truly indicates a problem and, if so, why it occurred.\nFurthermore, when cross-referencing data, our participants mentioned additional challenges with processing massive amounts of data while achieving the desired level of granularity. P9 highlighted this issue,\n\u201cWe just have so much data, and it'd be pretty overwhelming to just kinda put that all in there.\u201d (P9)\nIn longitudinal studies that span months or even years, it becomes impractical for experts to examine every piece of data to get the desired granularity required for some applications like labels for machine learning (P12/13). Another way to obtain interpretation of data is to source directly from participants. However, discussions with experts reveal another significant challenge: understanding and utilizing self-report data. Self-report data is often limited, and experts must be cautious in treating self-report data as absolute truth, especially when dealing with older adults or those with cognitive impairments (P4). Even when experts manage to obtain some level of ground truth, they still face the challenge of integrating this information with other data modalities, mirroring the process of contextualizing raw sensor data. This gap makes it difficult for experts to generate and validate their interpretations."}, {"title": "3.2.2 Needs and Opportunities in assisting experts", "content": "A key insight emerged from our interviews: experts' sensemaking is inherently iterative. P6 mentioned that in real-world practice while making sense of data, they are \"constantly developing and improving.\u201d, \u201ca never-ending process\" that oscillates between \u201cwhat we want to measure\u201d and \u201cwhat is available for us.\u201d To better understand this iterative process, we identified two distinct approaches: one starting from the data and moving towards the insights, and the other beginning with the goals and working towards the data.\nThe first approach of examine the sensing data to identify any emerging patterns or insights allows experts to uncover unexpected trends and behaviors that might not have initially considered. For instance, P8 remarked, \"it wasn't until we got into the project that we realized maybe there's other aspects we need to be looking at.\" P6 likened the process to a \"fishing expedition,\u201d explaining that\n\"more people are always coming up with new and interesting ways of extrapolating info out of this data.\"(P6)\nStarting with the data enables experts to discover novel, previously undocumented insights and generate ideas that could lead to a deeper understanding of the data or behaviors of the observed participants.\nThe second approach prioritizes identifying goals such as activities of interest from the outset. Experts initially concentrate on specific activities or behaviors that are crucial for their study or intervention. They then examine the available sensors and data to ascertain how to obtain relevant information about the identified activities. This method is more targeted, starting with a precise idea of what needs to be observed and then devising the best strategies to collect and interpret the required data. As P12 noted, \u201cwhich activities you look at depends on your research goal\".\nAlthough these two paths differ, both are integral in the sensemaking process. Almost all experts confirmed that they frequently use these approaches in tandem to enhance their understanding and adjust their investigative focus as new insights emerge. P12 highlighted the complementary nature of these approaches, stating,\n\"I think both approaches have their place. If you already have a hypothesis and know what is important, the first approach (goal to data) is more relevant. But if you're less certain and are in the process of generating hypotheses, the second approach (data to insights) would be more appropriate.\" (P12)\nNow, recognizing the iterative nature of the sensemaking process and the two distinct paths it can take, we must consider how we can support this process effectively. We have identified two primary needs based on these paths: direct\""}, {"title": "4 VITAL INSIGHT: CONNECT DATA WITH INSIGHTS", "content": "In this section, we present the proposed system design of Vital Insight."}, {"title": "4.1 Data types and system infrastructure", "content": "The input for Vital Insight could be a variety of data types commonly used in real-world deployments with passive sensing devices such as phones and wearables. These data types include time-series data (e.g., physiological signals and mobility patterns), discrete data (e.g., phone unlock/lock states), and self-reported data (e.g., survey responses). Vital Insight provides a combination of visual data representations and text-based descriptions. We built the system using React, with a MongoDB backend hosted on a local server to ensure fast data retrieval and robust security protections. We have made the system open-source and publicly accessible on GitHub at [anonymized link]."}, {"title": "4.2 Interface design", "content": "Vital Insight has two major components: a visualization that provides direct representation of sensor data, and different granularity of summaries and inferences generated by LLM that provides indirect inference. Our system interface has five panels with eight components: time selection, main visualization, user profile, daily summary (user check-in, LLM-generated daily summary and questions), LLM-anomaly, and LLM-hourly summary (Fig. 1). To generate the interface, our system uses data from a real study with older adults, which we discuss in greater detail in Sec. 4.3.\nThe Time Selection panel on the top left allows users to select the start and end date/time to zoom in/out, and iteratively switch between a holistic view and details on demand, which is in line with best practices for visual information seeking in data visualization [90, 100]. The User Profile panel on the top right consists of two main sections: basic demographic details and information related to the participant's regular routines (e.g., exercise, recreation, chores) (Fig. 3 left), designed to provide experts with a holistic understanding of the participant's lifestyle and interests.\nThe Main Visualization panel serves as the central display to represent the sensing data (Fig. 1 left). The visualization component features five categories of information gathered from the phone, smartwatch, and chatbot: location, health, phone usage, activity, and events. We used a small multiple-style time series plot arrangement to display each category of information along a unified timeline. Small multiples are a visualization technique where various attributes are plotted along a consistent x-axis scale, facilitating multifaceted exploration [7], pattern recognition [61], and general data exploration [96]. This method is particularly effective for tasks that involve direct visual comparisons of time series data, aiding in comparing, exploring, and analyzing trends [4, 12, 45, 48, 69]. The Location view shows place labels, e.g., home and church. We generate this data by cross-referencing the raw GPS coordinates from the phone app with participants' self-reported frequently visited locations and addresses. The system marks each location with a unique background color across all views with a label at the top, aiming to provide contextual information when experts examine the data. The Health view presents heart rate and respiration data from the smartwatch using scatter plots in red and blue. Two black trendlines overlayed on the graph represent the participant's average heart rate and respiration over a one-month period. Exclamation marks (!) next to the scatter plot indicate detected anomalies. We present more information about the anomaly detection module later. The Phone Usage view includes Wi-Fi (connected/disconnected, purple line chart), phone battery levels (0-100, green line chart), and phone interaction (unlock/lock events, solid/dashed grey rectangle overlaid). The Activity view reports step counts and basic activity recognition. A bar chart presents the step counts, with the height of the bar representing the number of steps taken during the time indicated by the width. The activity view presents basic activities from iPhone's standard activity recognition API, using different colored rectangles ('stationary' in gray, 'walking' in green, 'automotive' in purple), indicating the activities and their respective time spans. Finally, the Event view includes significant events throughout the day, including phone calls (green) and interactions with the Alexa chatbot (orange), and their respective durations.\nDaily Summary panel, provides a high-level overview of the participant's day. It includes three tabs: LLM Summary, User Check-in, and Questions (Fig. 4 left). The LLM Summary tab presents an LLM-generated summary of the entire day in paragraphs and bullet points for easy reading, thus providing experts the ability to quickly understand a person's day. The User Check-in tab displays the real conversation between the chatbot and the participant (orange 'Chatbot' event in visualization). The Questions tab presents a list of questions and missing information that the system identifies as potentially helpful for data interpretation. This information can serve as an inspiration for experts to reflect on potential actions, particularly when engaging with different stakeholders, such as speaking directly with participants. We present detailed rationale for the LLM-generated content in Sec. 4.2.\nThe Anomaly panel provides insights about unusual data points. We have currently implemented this functionality for two data modalities: heart rate and respiration (Fig. 3 right). For each anomaly, the panel displays the specific time range, description, and a possible reason, all generated by the LLM. Users can select the specific anomaly time to view the detailed information. This panel is designed to support the scenario of capturing and explaining anomalies highlighted in interviews.\nThe next panel is the Hourly Summary, which provides LLM-generated interpretations for each hour of data (Fig. 4 right). Users can select a specific time (e.g. 4-5 AM) to view the corresponding interpretation. Similar to the daily summary, this panel offers three tabs for each hour: Summary, Inference, and Questions. The Summary tab provides an objective summary of the sensor data with minimal inference, while the Inference tab offers more creative insights derived from the data. The Questions tab, similar to the one in the Daily Summary panel, presents questions that the LLM identifies as useful for interpreting that hour's data. This panel offers different levels of inferences on various potential events in detail.\nAdditionally, we implemented several interactive functions to improve the system's usability. To enhance the visual alignment of different modalities, we added a vertical tooltip that appears across all modalities when the user hovers over the visualization and displays the time and value. This helps experts easily compare and integrate different sensing"}, {"title": "4.3 LLM contents", "content": "The Vital Insight system has four main LLM functionalities: the hourly summary, daily summary, question, and anomaly, all developed based on the OpenAI GPT-4 model through zero-shot Natural Language String method(Fig. 5) [41, 53, 64]. We provide detailed prompts in the Supplementary Materials.\nThere are four primary forms of data input: Sensor Data, User Profile, User Check-in, and Daily Features. For Sensor Data, we process the data into a semantic format. Discrete data, such as WiFi and phone battery levels, are directly converted into sentences with values and timestamps (e.g. \"The battery level of the person's phone is [BATTERY] at [TIME].\"). For time-series data like heart rate, encoding each data point individually would overwhelm our system. Instead, since these data are typically sampled at a fixed rate, we group them and send them in an array format like \"The person's respiration from [TIME] to [TIME] (10-second interval) is [VALUE1, VALUE2, ... ].\", and sort this narrative in chronological order. Another data input is the User Profile, where demographic and routine information is sent to the system. The next data input is User Check-in, where we process conversation in a time(From [TIME] to [TIME]) and utterance ([ROLE]:[UTTERANCE]) format. The last part is Daily Features, which are commonly used statistical features from sensor data such as travel distance and device compliance. These four components form the data sources.\nThe foundation of the system is the Hourly Summary function, which processes the most granular level of data and provides an hourly summary. The prompts consist of the following components: Goal, Data Interpretation Guidance, Data, User Profile, User Check-in, Historical Summary from GPT, and a JSON Formatter. The Goal outlines the task's objective and explains the purpose of each input component. Data Interpretation Guidance offers expert advice on analyzing the data. Data includes one hour of sensing data in the semantic format along with the User Profile and User Check-in. The Historical Summary from GPT retrieves outputs from the previous hour's LLM-generated summary as additional contextual information. The JSON Formatter specifies the desired JSON output format. The hourly function outputs three pieces of information: Summary, Inference, and Further Questions as described in Sec. 4.2."}, {"title": "4.4 Alignment with interview insights", "content": "We designed Vital Insight based on insights from the initial interviews. In this section, we briefly describe how the features align with those insights. From the interviews, we identified four key sensemaking scenarios that guided the feature design. To address the scenario of contextualizing data based on personalized schedules and self-reported data, we incorporated user check-in and profile modules to provide direct information about the person. Additionally, important contexts such as location labels are integrated into the visualization to aid experts in contextualizing the data. The LLM incorporates all contextual information at each step to enhance its inferences. To generating insights on various possible events, the LLM provides daily and hourly summaries to help experts understand events at different levels of granularity. We designed the anomaly panels to specifically address the scenario of capturing and explaining anomalies with a quick glance. Lastly, the question panel offers inspiration for actions and interactions with different stakeholders, providing valuable guidance for experts on specific aspects of the data to inspect further. Additionally, the design of Vital Insight, incorporating both visualization and LLM inference, aims to assist with both direct representation and indirect inference."}, {"title": "5 USER TESTING", "content": "After designing Vital Insight, we conducted user testing with the same 14 experts to evaluate the usability of the system. In this section, we detail the protocol of the user testing sessions, along with the quantitative and qualitative findings from the sessions. Further, we explain the various patterns in experts' sensemaking processes we observed during the sessions, drawing insights from both observations and discussions with the experts."}, {"title": "5.1 Method", "content": "To generate the dashboard used in the user testing session, we used real-world data from a deployment with older adults, where we collected: 1) pre-study surveys, 2) passive mobile and wearable sensing data, and 3) voice assistant check-ins. Participants completed surveys on demographics and regular routines before deployment. We provided the participants with a Garmin smartwatch and installed the study app on their phone. Using a system successfully deployed in previous studies, our app directly syncs with the Garmin device and collects their physical activity, location, call logs, app usage, Wi-Fi/Bluetooth connections, IBI, heart rate, accelerometer, step counts and more [15, 59, 70, 72]. Additionally, participants can initiate a conversation with an LLM-powered chatbot on the study provided Amazon Alexa to check in on their day (prompts attached in Supplementary material). The system runs in"}, {"title": "5.1.2 Study Design", "content": "At the beginning of the session, we provided experts with a brief tutorial on the Vital Insight system. We then gave them a link to the system and asked them to share their screen as they completed a series of tasks with the information on the system. We used the same day of data for all the user testing sessions for consistency. We designed these tasks to specifically address the four sensemaking scenarios we identified during the initial interviews. For the first task, we asked experts to use all available information on the system and write a short, general summary of the person's day. We kept the task broad, allowing experts to determine their own version of what the person's day entailed. Next, experts were asked to dive deeper into specific activities throughout the day in greater detail. We then focused on anomalies, asked experts to identify potential anomalies in the data, and guided them to the corresponding panels to share their thoughts on the results from the system. Finally, we discussed interaction with stakeholders, directing experts to the question section and discussing possible actions they might take based on the information provided. After completing these tasks, we had an open-ended discussion to gather their general thoughts on the system and provided them with a survey. In the survey, we first asked experts to evaluate the different modules of the system. They rated each module based on its importance, trustworthiness, utilization, and clarity. Next, they assessed how well the system aligns with the five design implications we identified. Finally, they completed a NASA TLX survey to evaluate the overall work load of the system.\nSimilar to the initial interviews, we conducted hour-long user testing sessions online with the same 14 experts, and gave a $15 gift card as compensation. We also gathered information on their research familiarity with older adult populations since the user testing data was from that demographic. Our participants generally have a strong research familiarity with older adult populations, averaging 4.14 on a scale of 1 (Unfamiliar) to 5 (Familiar) (Table. 1). We recorded all user testing sessions, capturing the experts' interactions with the system. For the qualitative analysis, we obtained transcripts of the sessions and the summaries that experts wrote during the first task. We also took observation notes during each user testing session. We then followed an open-coding approach to analyze all the materials, the same method as our initial interview study (Sec. 3.1). The analysis focused on two main themes: the usability of the system and the sensemaking process of different experts. We also conducted a quantitative analysis of the survey results."}, {"title": "5.2 System Evaluation Results", "content": "In this section, we first present the quantitative results from the survey, followed by an explanation of general feedback on the system based on the qualitative interviews."}, {"title": "5.2.1 Survey results", "content": "First, we report the results of the survey. Regarding the impact of different modules on sensemaking. The results indicate that Visualization was clearly the most crucial component, while Hourly Summary was the least, with the other three modules (Daily Summary, User Profile and User Check-in) being similarly important. In terms of trust in different modules (scored 1=Distrust to 5=Trust), experts trusted User Check-ins (Mean 4.38, SD=0.45) and Visualization (4.29, SD=0.51) the most, followed by Daily Summary (3.88, SD=0.75), Hourly Summary (3.71, SD=0.67), and Anomaly (3.71, SD=1.00). The average trustworthiness score of the system as a whole was 4.08 (SD=0.67). This suggests that the experts generally considered the system to be trustworthy, with the indirect inference components by LLM rated slightly less than the direct data representations, which is understandable. The lower trust in the Anomaly module may also be due to differing definitions of what constitutes an anomaly, and we discuss this in Section 5.2.2.\nNext, we examined the usage and clarity of each data view in the visualization. Among all seven data views in the visualization, experts used an average number of 6.07 (SD=1.14). Heart rate, steps, and activity were used by all experts (14), followed by respiration (12), WiFi and battery (11), and phone unlock usage (10). Regarding the clarity of each modality (scored 1 = Very unclear to 5 = Very clear), the average score across all seven modalities was 4.33. The clarity ratings from most to least clear were battery (4.79, SD=0.43), WiFi (4.64, SD=0.50), phone unlock usage (4.57, SD=0.51), heart rate (4.29, SD=1.27), respiration (4.14, SD=1.23), steps (4, SD=1.11), and activity (3.86, SD=1.17). These findings show that most data views were generally clear, and experts were actively using these information during their sensemaking process.\nWhile the survey results were largely positive, the NASA-TLX results indicated a somewhat high usability load, scoring 44.29 out of 100. Experts rated their success in completing tasks relatively high, but also high imentally demanding. Experts noted that the time allotted for the task was too short for them to complete the tasks effectively, especially given the learning curve associated with the information-rich interface."}, {"title": "5.2.2 General usability and qualitative feedback", "content": "Experts generally liked the system, with P4 expressing, \u201cI wish I had this for my study.\" While we covered and evaluated many aspects of the system in the surveys, we wanted to explore the qualitative feedback in greater detail to provide more context, and identify aspects not captured in the survey.\nExperts loved the visualization aspect of the system. For example, P3 specifically mentioned that they appreciated how the steps from both the phone and watch were plotted together. However, experts still had some suggestions for improvement, such as adding icons for each modality to enhance easy recognition, providing an easier zoom-in option like a sliding window or drag feature, adding hovering interactions for anomalies, and making the date more salient. Further, experts found the User Profile module very helpful (\u201cThe user profile, that's very powerful(P1)", "extracting key information from the user check-in\" panel and displaying it on the visualization according to the time. Additionally, P4 and others suggested displaying timestamps along with some semantics for the conversation with chatbot in the user check-in module, other than only as an event on the visualization.\nDuring the user testing sessions, many experts appreciated the rich details in both the visualizations and LLM-generated summaries. P13 praised the different levels of detail provided, stating,\n\u201cI notice the summary is on different levels. It's also very helpful. When I want to look at details, I can look at the hourly summary and anomaly. And if I wanna get a big picture, I can look at a daily summary.": "nMany experts also liked the bullet points in the summaries, noting that they provided a good overview. However, experts suggested that it could be even more effective if there were an additional high-level summary beyond the bullet"}, {"title": "5.3 Experts' sensemaking process", "content": "From our evaluations, it is evident that experts employ a variety of approaches for sensemaking, with notable common-alities as well as differences. We observed distinct patterns in the general sensemaking process, including which data view experts typically start. Most experts started their analysis with the visualization (P4/5/6/8/10/11/12/"}]}