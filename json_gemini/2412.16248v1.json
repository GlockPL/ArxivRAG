{"title": "Optimizing Low-Speed Autonomous Driving: A Reinforcement Learning Approach to Route Stability and Maximum Speed", "authors": ["Benny Bao-Sheng Li", "Elena Wu", "Hins Shao-Xuan Yang", "Nicky Yao-Jin Liang"], "abstract": "Autonomous driving has garnered significant attention in recent years, especially in optimizing vehicle performance under varying conditions. This paper addresses the challenge of maintaining maximum speed stability in low-speed autonomous driving while following a predefined route. Leveraging reinforcement learning (RL), we propose a novel approach to optimize driving policies that enable the vehicle to achieve near-maximum speed without compromising on safety or route accuracy, even in low-speed scenarios.\n\nOur method uses RL to dynamically adjust the vehicle's behavior based on real-time conditions, learning from the environment to make decisions that balance speed and stability. The proposed framework includes state representation, action selection, and reward function design that are specifically tailored for low-speed navigation. Extensive simulations demonstrate the model's ability to achieve significant improvements in speed and route-following accuracy compared to traditional control methods.\n\nThese findings suggest that RL can be an effective solution for enhancing the efficiency of autonomous vehicles in low-speed conditions, paving the way for smoother and more reliable autonomous driving experiences.", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement Learning (RL) has become a powerful approach for addressing complex decision-making challenges in autonomous systems, particularly in low-speed scenarios. Unlike high-speed driving, low-speed environments demand high precision, safety, and stability [7] due to dynamic obstacles and confined spaces. This paper explores several applications of RL in low-speed contexts, demonstrating its potential to enhance performance in various tasks.\n\n1. Autonomous Parking Systems: RL optimizes vehicle parking maneuvers in tight spaces, allowing vehicles to learn adjustments in steering and speed for efficient parking while avoiding obstacles.\n\n2. Low-Speed Car Following: In congested traffic, RL enables vehicles to dynamically adjust their speed based on the lead vehicle's behavior, improving safety and comfort.\n\n3. Urban Low-Speed Navigation: RL facilitates autonomous navigation in urban environments, allowing vehicles to adapt to changing conditions and outperform traditional control methods.\n\n4. Delivery and Shuttle Vehicles: RL enhances the navigation of autonomous delivery robots and shuttles in complex environments, optimizing efficiency and safety at low speeds.\n\n5. Obstacle Traversal in Off-Road Vehicles: RL improves control strategies for off-road vehicles traversing uneven terrain, maintaining stability while avoiding damage.\n\n6. Low-Speed Drone Navigation: For drones operating in confined spaces, RL enhances obstacle avoidance, ensuring safe navigation where GPS signals are weak.\n\n7. Service Robot Navigation: In indoor settings, RL optimizes the movement of service robots through dynamic environments, ensuring safety around humans.\n\nThese applications highlight RL's ability to meet the unique challenges of low-speed navigation. This paper focuses on using RL to maximize speed stability in low-speed autonomous driving while following predefined routes, presenting a novel approach to optimize both speed and safety in constrained environments."}, {"title": "BACKGROUNDS", "content": "One of the critical challenges in autonomous driving is ensuring that vehicles can follow predefined routes at maximum speed while maintaining stability, especially in low-speed conditions. This problem becomes more complex in real-world scenarios where safety, precision, and adaptability are paramount.\n\nTo test and validate our proposed RL-based approach, we conduct experiments using the AWS DeepRacer platform. AWS DeepRacer offers a simulated environment where autonomous models can be trained and tested in real-time, providing a practical and scalable solution for RL experimentation. Specifically, we leverage the AWS DeepRacer Student League [8] to rigorously evaluate our algorithm's performance. Our focus is on optimizing for the shortest path, maximum speed, and stability, aiming to demonstrate that our RL approach can outperform traditional control methods in low-speed, complex environments.\n\nThrough these experiments, we aim to provide a robust validation of our approach, showcasing its potential application in real-world autonomous driving systems."}, {"title": "METHODS", "content": "The primary objective of our algorithm is to ensure that the vehicle consistently operates at its maximum allowable speed of 1 m/s, regardless of the driving conditions. We already know that the speed limit is constrained to 1 m/s, so the challenge is to develop a control mechanism that allows the vehicle to sustain this top speed in all scenarios, such as navigating curves, straight paths, and while avoiding obstacles. The algorithm dynamically adjusts the vehicle's acceleration and deceleration to keep it at this speed limit, ensuring that any deviations due to environmental factors are promptly corrected. This speed control is critical for optimizing performance while adhering to safety requirements.\n\nQuadratic and Exponential Rewards in Robotics [5]: Nonlinear reward functions such as quadratic or exponential penalties are commonly used in robotics for tasks like navigation and speed control. These functions help to penalize large deviations more severely, which can be crucial for maintaining stability at high or low speeds, nonlinear reward functions have been applied successfully in robotics.\n\nQuadratic and exponential reward functions are commonly used in reinforcement learning (RL) to optimize speed control in robotic and autonomous systems. These nonlinear rewards help balance achieving maximum speed while ensuring safety and stability.\n\nAn exponential reward function penalizes deviations from the target speed even more aggressively, which can be useful in environments where maintaining speed is crucial for performance.\n\n$R(s,a) = e^{-a|vtarget-Vactual|}$\n\nWhere:\n\n\u2022 a is a scaling factor controlling how steep the penalty is.\n\n\u2022 Utarget is the desired speed (e.g., 1 m/s).\n\n\u2022 Vactual is the actual speed."}, {"title": "Progress-Based Rewards in Autonomous Driving [9]", "content": "In autonomous driving, progress-based rewards are often used to incentivize the vehicle to follow a specific route or track as efficiently as possible. The idea is that the agent (vehicle) receives a reward based on how much progress it makes toward completing a lap, reaching a goal, or following a path. Incorporating a ratio of progress made to distance traveled is a way to ensure that the agent does not take unnecessarily long or inefficient routes.\n\nProgress-Based Reward Using Incremental Distance [3]:\n\n$Rt = \\frac{AProgress}{AL}$\n\nWhere:\n\n\u2022 Rt is the reward at time step t.\n\n\u2022 AProgress = Progress - Progresst-1 is the change in progress along the route between consecutive time steps.\n\n\u2022 \u2206L = Lt-Lt-1 is the incremental distance traveled between the two time steps.\n\nThis design of the reward function [10] encourages the agent to maximize progress while minimizing unnecessary distance traveled, promoting more efficient movement.\n\nThe formula Rt = AProgress provides a straightforward and interpretable metric for evaluating traversal efficiency in autonomous systems. However, numerical stability issues may arise under specific conditions, which can affect the reliability and robustness of the metric.\n\nKey Challenges in Numerical Stability\n\n1. Small-Denominator Problem:\n\n\u2022 When the path increment \u2206L approaches zero, the value of Rt can become exceedingly large or undefined. This situation commonly occurs in scenarios involving sharp turns, low-speed maneuvers, or near-stationary conditions.\n\n\u2022 Such instability not only distorts the evaluation metric but may also propagate errors into the learning process in reinforcement learning frameworks.\n\n2. Sensitivity to Noise:\n\n\u2022 Measurement errors or noise in position data can significantly affect Progress and AL , particularly when these increments are small. Even minor inaccuracies can lead to disproportionate changes in the metric.\n\n3. Discontinuous Behavior:\n\n\u2022 In complex trajectories with frequent stops and starts, the metric can exhibit abrupt changes, leading to challenges in reward function continuity during training.\n\nTo address the instability caused by small path incremental (AL \u2192 0), a regularization terme > 0 is introduced to stabilize the metric. The modified formula is expressed as:\n\n$Rt = \\frac{AProgress}{AL + \u20ac}$\n\nwhere e is a small positive constant. This adjustment ensures that the denominator never approaches zero, preventing large or undefined values for Rt. Regularization improves the numerical stability of the metric while maintaining its interpretability. The Parameter e is critical for achieving a balance between stability and sensitivity, which can be fixing value or the cure:\n\n1. Small \u20ac:\n\n\u2022 Retains high sensitivity to variations in AL.\n\n\u2022 Risk of instability when AL is extremely small.\n\n2. Large \u0454:\n\n\u2022 Improves stability but reduces the metric's responsiveness to changes in AL.\n\nAn empirical or adaptive approach can be used to tune e depending on the application scenario:\n\n$\u03b5= \u03b1\u00b7 mean(AL)$\n\nwhere a is a scaling factor that controls the sensitivity of the regularization term. By dynamically adjusting e, the metric can maintain stability while remaining responsive to variations in path increments."}, {"title": "Context-Aware Regularization:", "content": "For the racing training it can be implemented dynamically based on trajectory characteristics:\n\n\u2022 Adjust e based on environmental factors, such as the path curvature [1] or speed profile.\n\n2. Time-Decaying Regularization:\n\n\u2022 e can be designed to decay over time. This allows the model to focus on finer details as training progresses. The time-decaying regularization is expressed as:\n\n$Et = \u20ac0.e^{-\u1e9et}$\n\nwhere 60 is the initial regularization value, \u03b2 > 0 is the decay rate, and t represents the training step. By reducing e over time, the metric transitions from emphasizing stability in the early stages to precision in the later stages of training.\n\nFor this improvement it can benefit the training result:\n\n1. Prevention of Instability:\n\n\u2022 By ensuring that the denominator never approaches zero, the metric avoids producing exceedingly large or undefined values.\n\n\u2022 This is particularly useful in scenarios involving near-zero path increments, such as during sharp turns or low-speed maneuvers.\n\n2. Enhanced Robustness:\n\n\u2022 The addition of e makes the metric less sensitive to small variations in \u2206L, improving reliability under noisy conditions.\n\n3. Continuity in Training:\n\n\u2022 Regularization reduces abrupt changes in the reward function, facilitating smoother gradient updates during the learning process.\n\nFinal Experimental Validation to validate the effectiveness of smoothing via regularization, a comparative study can be conducted using:\n\n\u2022 Unregularized Formula:\n\n$Rt = \\frac{AProgress}{AL}$\n\n\u2022 Regularized Formula:\n\n$Rt = \\frac{AProgress}{AL + \u20ac}$\n\nMetrics such as training stability, reward consistency, and trajectory optimization efficiency can be evaluated to demonstrate the benefits of regularization."}, {"title": "Smooth Steering Control in Autonomous Driving [2]", "content": "A common approach to penalize excessive steering changes is by incorporating a term in the reward function that penalizes large deltas (changes) in the steering angle between two consecutive time steps. The penalty can either be proportional to the magnitude of the change or quadratic to heavily penalize larger changes. Here's an example of a steering delta penalty.\n\n$Rsteer -kosteer$\n\nWhere:\n\n\u2022 Rsteer is the steering penalty applied to the overall reward.\n\n\u2022 Aesteer = Osteer,t - Osteer,t-1 is the difference in steering angle between two consecutive time steps t and t - 1.\n\n\u2022 k [10] is a scaling factor that determines how much to penalize larger steering changes.\n\nThis reward component is designed to penalize excessive steering angle changes, encouraging smoother and more stable driving behavior.\n\nKey Characteristics of the Formula:\n\n1. Linear Penalization:\n\n\u2022 The penalty is directly proportional to the absolute value of the steering change. Larger steering adjustments result in higher penalties.\n\n2. Encouragement of Smooth Steering:\n\n\u2022 By penalizing frequent or abrupt steering changes, the formula incentivizes the agent to follow trajectories with minimal oscillations, promoting smoother control dynamics.\n\n3. Parameter Sensitivity k :\n\n\u2022 A higher value of k imposes a stricter penalty for steering changes, discouraging large steering adjustments. However, this can lead to overly cautious behavior, where the agent avoids necessary steering actions, particularly in scenarios requiring sharp turns.\n\n\u2022 A lower value of k reduces the impact of the penalty, allowing the agent more flexibility in steering adjustments. While this promotes responsiveness, it can potentially result in unnecessary oscillations or instability in the trajectory.\n\nThe advantages of this formula in the racing training:\n\n1. Improved Vehicle Stability:\n\n\u2022 Penalizing sharp steering changes helps to maintain vehicle stability, particularly at higher speeds where abrupt steering can lead to unsafe maneuvers.\n\n2. Energy Efficiency:\n\n\u2022 Smoother steering reduces unnecessary energy consumption, which is especially relevant in autonomous electric vehicles.\n\n3. Trajectory Smoothness:\n\n\u2022 The reward promotes smooth trajectories, improving both passenger comfort and adherence to the desired path.\n\nExtend the formula and implement into the training and evaluate results:\n\nIn scenarios requiring sharp turns, the steering penalty may discourage necessary adjustments due to the increased cost associated with large steering angle changes. This can lead to suboptimal trajectory planning, particularly in environments with high curvature."}, {"title": "Dynamic Curvature Weighting:", "content": "To mitigate this issue, the penalty can be weighted [6] by the curvature [4] of the path:\n\n$Rsteer = -k Asteer. (1 \u2013 Wcurve)$\n\nwhere:\n\n\u2022 Asteer | represents the absolute change in steering angle.\n\n\u2022 k [10] is the proportionality constant controlling the penalty magnitude.\n\n\u2022 Wcurve is a weighting factor proportional to the curvature of the path. It increases in regions of high curvature, reducing the penalty to encourage necessary adjustments.\n\nWcurve is important factor in the path, so it can use it as dynamic weight improve the formula:\n\n\u2022 Implement a curvature-based weighting function wcurve = min( curvature, 1), where y is a scaling factor. This ensures the penalty adapts smoothly to varying curvature without introducing instability.\n\n\u2022 For example:\n\n$Wcurve = \\frac{curvature}{curvature + \u03b3}$\n\n\u2022 For dynamically changing curvature, an adaptive y can be used:\n\n$\u03b3 = a. mean(curvature)$\n\nwhere:\n\n- a > 1 is a scaling factor to adjust y dynamically based on the curvature distribution.\n\n- mean(curvature) represents the average curvature over a given segment of the path.\n\nThis approach ensures that y adapts in real time, allowing the weighting function to remain sensitive to path complexity while maintaining numerical stability. By dynamically adjusting y, the steering penalty can balance responsiveness and smoothness across varying curvature scenarios.\n\n2. Path-Aware Adjustments:\n\n\u2022 Include path features, such as lane boundaries or obstacle positions, to refine the steering penalty in challenging scenarios.\n\n3. Experimental Validation:\n\n\u2022 Test the effect of curvature-weighted penalties on complex trajectories and evaluate the trade-off between trajectory smoothness and successful navigation through sharp turns.\n\nLow-Speed Scenarios. At low speeds, steering changes have a reduced impact on stability and vehicle dynamics. Penalizing these changes equally across all speeds can lead to overly cautious behavior, especially in scenarios where quick adjustments are required to correct minor deviations. To adapt the penalty, a speed-dependent scaling factor can be introduced:\n\n$Rsteer = -k steer. (1 - Wcurve). Uscale$\n\nbecause for the low speed environment the speed already set to 1 m/s so Uscale set as 1, then the low speed scenarios formula is the same as sharp turns.\n\nSummary of Steering Penalty Comparison The generated plot compares the steering penalty rewards under two different scenarios: smooth steering and abrupt steering. The penalty rewards were calculated using the formula Rsteer = -k |Asteer|.\n\nKey Observations:\n\n1. Smooth Steering:\n\n\u2022 The penalty rewards remain relatively low and consistent across timesteps."}, {"title": "Integration into Composite Reward Functions", "content": "Implications: The comparison highlights the importance of incorporating curvature-based weighting (Wcurve) in steering penalties. While the unweighted penalty ensures stability on straight paths, the weighted penalty offers better flexibility in scenarios requiring sharp turns or dynamic maneuvers. The balance between these two approaches can be adjusted by tuning the curvature scaling factor (\u03b3).\n\nTo optimize autonomous driving performance, steering penalties can be integrated with a velocity-based reward to form a composite reward function. This ensures that the agent is encouraged to maintain an optimal balance between speed and trajectory smoothness while adapting to path curvature.\n\nComposite Reward Function ,The composite reward function can be expressed as:\n\n$Rtotal = Wprogress:Rprogress+wsteer:Rsteer+Wvelocity\u00b7Rvelocity$\n\nwhere:\n\n\u2022 Rprogress: Encourages progress along the track.\n\n$Rprogress = \\frac{\u25b2Progress}{AL + \u20ac}$\n\n\u2022 Rsteer: Discourages abrupt steering changes:\n\n$Rsteer = -k. Asteer. (1 - Wcurve)$\n\n\u2022 Rvelocity: Encourages the agent to maintain the target velocity:\n\n$Rvelocity = e^{-a-vactual-Utarget}$\n\n\u2022 Wprogress, Wsteer, Wvelocity: Weighting factors that balance the contributions of each reward component.\n\nBenefits:\n\n1. Trajectory Smoothness: Rsteer reduces oscillations and ensures stability, with the weighted formulation allowing flexibility in high-curvature regions.\n\n2. Speed Optimization: Rvelocity incentivizes the agent to maintain an optimal speed while balancing safety.\n\n3. Progress Maximization: Rprogress ensures that the agent prioritizes forward motion and avoids unnecessary detours.\n\nImplementation: The weights Wprogress, Wsteer, Wvelocity can be dynamically adjusted based on the track segment:\n\n\u2022 For straight segments, increase Wvelocity to prioritize speed.\n\n\u2022 For curved segments, increase wsteer to emphasize trajectory smoothness.\n\nThis integration allows the agent to balance trajectory smoothness, speed, and progress efficiently across diverse driving scenarios."}, {"title": "EXPERIMENT", "content": "For experimental validation, we employed the AWS DeepRacer Student League platform, which provides a simulated environment for autonomous driving tasks. The training parameters are summarized in Table 1.\n\nExperimental Setup: The vehicle operates within a continuous action space, where the speed is constrained between 0.5 m/s and 1 m/s, and steering angles range from -30\u00b0 to 30\u00b0. The composite reward function described earlier was utilized to balance progress, velocity optimization, and trajectory smoothness.\n\nTraining Strategy:\n\n\u2022 Duration: The model was trained for a maximum of 10 hours to ensure convergence.\n\n\u2022 Scenarios: Training covered various track conditions, including straight paths, high-curvature segments, and mixed terrain.\n\n\u2022 Hyperparameter Tuning: We tuned reward function weights (Wprogress, Wsteer, Wvelocity) to optimize performance for different scenarios.\n\nResults: The trained model exhibited robust performance across diverse driving scenarios. Key findings include:\n\n1. The weighted composite reward function significantly improved trajectory smoothness on high-curvature tracks compared to the unweighted version.\n\n2. Velocity optimization resulted in efficient completion times while maintaining stability.\n\n3. The integration of progress, velocity, and steering penalties enabled the vehicle to adapt effectively to both straight and curved paths.\n\nThese results demonstrate the effectiveness of the composite reward function and the training methodology in achieving robust and efficient autonomous driving performance.\n\nParticipation in the AWS DeepRacer Student League took place from March to October, spanning several months of intense competition. Through consistent performance and strategic optimization, the team successfully advanced to the final round. In the highly competitive final, the model emerged victorious, securing first place.\n\nAs an integral member of the team, Elena Wu, currently pursuing her university studies in the United States, played a crucial role in the success. Her dedication, expertise, and strategic contributions were instrumental in navigating the challenges of the competition. This year, Elena achieved a remarkable milestone by winning the first-place title in the AWS DeepRacer Student League Grand Finale, showcasing her exceptional skills and commitment to excellence."}]}