{"title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model Via Self-Improvement", "authors": ["An Yang", "Beichen Zhang", "Binyuan Hui", "Bofei Gao", "Bowen Yu", "Chengpeng Li", "Dayiheng Liu", "Jianhong Tu", "Jingren Zhou", "Junyang Lin", "Keming Lu", "Mingfeng Xue", "Runji Lin", "Tianyu Liu", "Xingzhang Ren", "Zhenru Zhang", "Qwen Team", "Alibaba Group"], "abstract": "In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance.\nQwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems. The flagship model, Qwen2.5-Math-72B-Instruct, significantly outperforms both open-source models and leading closed-source models (e.g., GPT-40, Gemini Math-Specialized 1.5 Pro). Particularly in the challenging AMC 2023, with the assistance of RM, Qwen2.5-Math-72B-Instruct successfully solves almost all the problems. Qwen2.5-Math-7B-Instruct surpasses Qwen2-Math-Instruct 72B in performance. Under CoT and TIR settings, it achieves MATH scores of 83.6 and 85.3, respectively. Even our smallest 1.5B model, achieving a MATH score of around 80 when utilizing the Python Interpreter, outperforms the majority of current models in this domain. We hope that Qwen2.5-Math can contribute to the community for solving complex mathematical problems.\nThe base models, instruct models, and reward model of the Qwen2.5-Math series are available on Hugging Face 1 and ModelScope2, and the evaluation scripts on GitHub\u00b3. We have also developed a demo that supports the TIR mode in Qwen-Agent, which allows running code locally to experience Tool-Integrated Reasoning capabilities of Qwen2.5-Math.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past year, we have devoted considerable effort to researching and enhancing the reasoning capabilities of large language models, with a particular emphasis on their ability to solve arithmetic and mathematical problems. In this report, we introduce a series of math-specific large language models, Qwen2.5-Math, Qwen2.5-Math-RM, and Qwen2.5-Math-Instruct-1.5B/7B/72B. To provide a comprehensive understanding of the technical developments behind Qwen2.5-Math, we also offer a detailed overview of its predecessor, Qwen2-Math (Qwen, 2024).\nWe introduce a series of self-improvement techniques to develop Qwen2.5-Math models on top of the Qwen2-Math. Self-improvement techniques take advantage of supervision from large language models themselves (Cao et al., 2024). Specifically, we apply self-improvement from three aspects during the training of Qwen2.5-Math. In pre-training, we employ Qwen2-Math-Instruct to synthesize math queries and corresponding responses on a large scale to enrich the pre-training corpus of Qwen2.5-Math. In post-training, we train a reward model on massive sampling from previous models and apply it to the iterative evolution of data in supervised fine-tuning. The better mathematical models trained from this enhancement lead to a more robust reward model, Qwen2.5-Math-RM. Then, we use this reward model in reinforcement learning and best-of-N sampling during inference. Synthetic data and judgment play a significant role in the enhancement of Qwen2.5-Math compared with its predecessor.\nSpecifically, the overall pipelines for developing Qwen2-Math and Qwen2.5-Math are illustrated in Figure 2. First, the Qwen2-Math base models are trained on a high-quality mathematical pre-training dataset called the Qwen Math Corpus v1, which contains approximately 700 billion tokens. Second, we train a math-specific reward model Qwen2-Math-RM, derived from Qwen2-Math-72B, to create the Qwen2-Math-Instruct models. This reward model is used to construct Supervised Fine-Tuning (SFT) data through Rejection Sampling (Yuan et al., 2023). Moreover, the reward model plays a key role in the reinforcement learning stage, where we employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) following SFT. Third, leveraging the Qwen2-Math-72B-Instruct model, we synthesize additional high-quality mathematical pre-training data, which serves as the foundation for Qwen Math Corpus v2. This updated corpus contains over 1 trillion tokens and is used to pre-train the Qwen2.5-Math models. Lastly, similar to the process used for the Qwen2-Math-Instruct models, we construct the Qwen2.5-Math-RM and Qwen2.5-Math-Instruct models. An important distinction in this stage is the inclusion of both English and Chinese Chain-of-Thought (CoT) reasoning data, as"}, {"title": "2 QWEN2.5-MATH PRE-TRAINING", "content": "In mathematical pre-training, our primary focus is on constructing a high-quality dataset rich in mathematical content. This dataset encompasses a wide variety of sources, including math-related web texts, code snippets, encyclopedias, exam questions, and synthetic mathematical data generated by Qwen2 (Yang et al., 2024). The process of assembling this pre-training dataset involves several key steps: data recall, deduplication, filtering, data synthesis, and optimization of the data mixture. The final curated dataset, which forms the foundation of our pre-training, is termed the Qwen Math Corpus v1. The Qwen2-Math base models, initialized with Qwen2-1.5B/7B/72B, undergo continuous pre-training using the Qwen Math Corpus v1."}, {"title": "3 QWEN2.5-MATH POST-TRAINING", "content": "After completing extensive mathematical pre-training, we proceed with post-training to further augment the mathematical logical reasoning capabilities of Qwen-Math, specifically focusing on Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). Our investigation is particularly focused on two key challenges: (1) How to automatically generate a substantial volume of high-quality and reliable CoT and TIR annotations, and (2) How to effectively leverage these annotations for both Supervised Fine-Tuning and Reinforcement Learning."}, {"title": "3.1 SUPERVISED FINE-TUNING", "content": "We aim for Qwen-Math to excel in two core capabilities: solving math problems through step-by-step natural language reasoning (Wei et al., 2022), and leveraging external tools (e.g., a Python interpreter) to address complex mathematical or algorithmic reasoning tasks (Yue et al., 2023). We have constructed dedicated datasets for both Chain-of-Though (CoT) and Tool-integrated Reasoning (TIR) and combined these datasets to train the model jointly. All models are trained for 3 epochs with a sequence length of 4,096 tokens. For the 72B model, we use a batch size of 256 and a learning rate of 5 \u00d7 10-6. For the 1.5B and 7B models, we set the batch size to 128 and the learning rate to 2 \u00d7 10-5. During training, the learning rate gradually decays to a final value of 7 \u00d7 10-7."}, {"title": "3.1.1 CHAIN-OF-THOUGHT DATA SYNTHESIS", "content": "Query Construction. The chain-of-thought dataset comprises a wide-ranging collection of 580K English and 500K Chinese mathematical problems, including both annotated and synthesized items. The annotated problems are derived from well-established sources such as the training set of GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and NuminaMath (LI et al., 2024). In an effort to bolster the Chinese reasoning capabilities of Qwen2.5-Math, we have further enriched the dataset with additional Chinese mathematical problems from exclusive K-12 problem collections. The synthesized problems are evolved from the annotated ones using the MuggleMath approach (Li et al., 2024b). To maintain a balanced distribution across varying levels of problem complexity, we utilize a difficulty-scoring model to categorize our problem set effectively.\nResponse Construction. We adopt an iterative approach that leverages rejection sampling, guided by reward modeling and annotated answers, to incrementally enhance the quality of responses (Yuan et al., 2023). At each iteration, the current best model is deployed to generate multiple reasoning pathways for the given problems, expanding the pool of candidate solutions. For problems with annotated answers, we select the top-k reasoning paths with correct final answers from the pool. For synthesized problems lacking definitive answers, we implement a weighted majority voting mechanism to deduce the most plausible correct reasoning paths. From these, we choose the top-k pathways that receive the highest reward scores. In the development of Qwen2.5-Math, an additional iteration is conducted using the Qwen2-Math-Instruct models to polish the quality of responses further. The final CoT training set encompasses 2000K English samples and 500K Chinese samples."}, {"title": "3.1.2 TOOL-INTEGRATED REASONING DATA SYNTHESIS", "content": "It is important to recognize that while CoT prompting plays a crucial role in enhancing the reasoning skills of large language models, it faces challenges in achieving computational accuracy and in handling complex mathematical or algorithmic problems, such as finding the roots of quadratic equations or computing the eigenvalues of matrices (Yue et al., 2023). To overcome these limitations and improve the model's proficiency in precise calculations, symbolic manipulation, and algorithmic reasoning, we have developed a dataset that incorporates a tool-integrated reasoning format. This innovative format enables the model to leverage a Python interpreter as an auxiliary resource in reasoning tasks.\nQuery Construction. The tool-integrated reasoning dataset consists of 190K annotated problems and 205K synthesized problems. The annotated problems are sourced from the training sets of established benchmarks, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), CollegeMath (Tang et al., 2024a), and NuminaMath (LI et al., 2024). The synthesized problems are generated by employing techniques from MuggleMath (Li et al., 2024b) and DotaMath (Li et al., 2024a) designed to facilitate query evolution within the GSM8K and MATH training sets. Additionally, we have selected 75K annotated problems for translation into Chinese using the Qwen2-72B model (Yang et al., 2024), aimed at enhancing the model's reasoning capabilities in Chinese.\nResponse Construction. For the annotated problems, we utilize an online Rejection Fine-Tuning (RFT) (Yuan et al., 2023; Singh et al., 2024) approach to iteratively generate tool-integrated reasoning paths whose final answers align with the reference answers. In each RFT iteration, we carry out multiple nucleus samplings with the currently best model at various temperatures, increasing the sample size for particularly challenging problems. After each iteration, to enhance data diversity, we apply a deduplication process to the responses, and the resulting cleaned dataset is then used to"}, {"title": "3.2 REWARD MODEL TRAINING", "content": "To provide supervisory signals beyond merely the final answer during both the selection of supervised fine-tuning data and the subsequent stages of reinforcement learning training, we have developed a mathematical reward model for Qwen2-Math and Qwen2.5-Math, referred to as Qwen2-Math-RM and Qwen2.5-Math-RM, respectively. These reward models are specifically designed to guide the model throughout the training process by offering more granular feedback on the quality of reasoning and intermediate steps, ultimately facilitating more robust model improvements."}, {"title": "3.2.1 DATA SYNTHESIS", "content": "In the development of Qwen2-Math-RM, we utilize 206K English mathematical problems, each paired with 6 candidate responses sampled from an intermediate version of Qwen2-Math. For Qwen2.5-Math-RM, we further enhance its support for both the Chinese language and TIR mode, training it with a more diverse set of 361K English and 257K Chinese mathematical problems, with each problem accompanied by 6 responses sampled from Qwen2.5-Math. This expansion ensures that Qwen2.5-Math-RM is well-equipped to provide supervisory feedback across a broader range of problem types and languages.\nTo establish the preference signals among the responses, we check the final answers of the responses to determine their correctness. Responses with the correct answers are labeled as positive, while those with incorrect answers are labeled as negative, thereby naturally creating a ranking relationship among the responses. We then filter out any cases where all responses are either entirely correct or entirely incorrect. However, to avoid the potential drawback of retaining only overly simplistic data, we enrich the dataset with responses from various intermediate versions and models of different sizes. This strategy ensures a more balanced distribution of query difficulty and maintains an even ratio of positive to negative responses."}, {"title": "3.2.2 TRAINING STRATEGY", "content": "We initialize the reward model from the supervised fine-tuning model. In terms of architecture, we replace the language modeling head originally used for next-token prediction with a scalar-value head, consisting of two linear layers. As previously mentioned, each query in the reward model's training dataset is paired with 6 responses, comprising both positive and negative candidates. If there are $k$ positive responses, then the remaining $6 - k$ are negative. Following Ouyang et al. (2022), the loss function for the reward model can therefore be formulated as follows:\n$L_{rm}(\\theta) = \\frac{1}{kx (6-k)} \\mathbb{E}_{(x,y_{pos},y_{neg})\\sim D} [log (\\sigma (r_{\\theta} (x, y_{pos}) \u2013 r_{\\theta}(x, y_{neg})))].$ \nHere, $r_{\\theta}(x, y)$ denotes the output of the reward model, where $x$ represents the problem and $y$ is the corresponding response. Rather than breaking these into multiple individual pairs and computing the loss in a pairwise fashion, we adopt a listwise approach to compute the ranking loss directly over valid pairs. This method enhances both training efficiency and effectiveness."}, {"title": "3.3 REINFORCEMENT LEARNING", "content": "Query Selection. The queries for reinforcement learning training are selected from the reward model's training set. We leverage supervised fine-tuning models with varying sizes to resample 8 responses for each query, with each response classified as either correct or incorrect by comparing it to the gold-standard answer. In the reinforcement learning stage, our primary goal is to ensure that the model consistently produces correct answers for queries where a correct response is possible. Therefore, we only retain queries for which 2 to 5 out of the 8 responses are correct. Queries with fewer than 2 correct answers are excluded as they indicate that the current Math model lacks the"}, {"title": "Group Relative Policy Optimization (GRPO)", "content": "As introduced by Shao et al. (2024), GRPO is a reinforcement learning method specifically designed for large language models, obviating the need for additional value function approximation as in PPO. GRPO uses the average rewards of a group of sampled outputs as a baseline to calculate the advantages of each output. The objective of GRPO is defined as Eq. 2:\n$J_{GRPO}(\\theta) = \\mathbb{E}_{[q~P(Q), \\{O_i\\}_{i=1}^{G}~\\pi_{gold}(O\\vert q)]}\\frac{1}{G}\\sum_{i=1}^{G} [\\frac{1}{O_i}\\sum_{t=1}^{O_i} {min[A_{i,t}, clip(\\frac{\\pi_{\\theta}}{\\pi_{\\theta^{old}}}, 1 - \\epsilon, 1 + \\epsilon) \\hat A_{i,t}] - \\beta D_{KL}[\\pi_{\\theta}\\vert\\vert\\Pi_{ref}]}],$\nwhere $\\pi^\\theta_{old} = \\pi(\\theta_{i,t} |q, O_{i,<t}), G$ is the number of responses in a group. $\\pi_{ref}, \\pi_{\\theta}, and \\pi_{\\theta}^{old}$ are reference, training, and sampling models, respectively. q and $\\{O_i\\}_{i=1}^G$ are questions and generated responses set in training. The advantage of each responses $\\hat A_i$ is calculated by $\\hat A_i = \\frac{r_i-mean(r_i)}{std(r_i)}$.\nThen this sequence-level advantage is applied to each token in the response as $A_{i,t}$."}, {"title": "Reward Shaping", "content": "We combine the rewards from both a rule-based verifier and the reward model to shape the overall reward signal. The rule-based verifier extracts potential answers from each response and compares them against the gold-standard answer.\nGiven that the output of the reward model is denoted as $r_m \\in \\mathbb{R}$, and the sparse reward from the rule-based verifier as $r_v \\in \\{0,1\\}$, the overall reward is calculated as follows:\n$r = \\sigma(\\alpha\\cdot r_m) + (r_v - 1),$ \nwhere $\\alpha$ is set as 0.5 in all of our experiments.\nThis shaping mechanism ensures that correct responses consistently receive higher overall rewards compared to incorrect ones. Within each of the correct and incorrect groups, the responses are ranked based on the scores from the reward models. ecially in hard samples."}, {"title": "Implementations", "content": "Our experiments are implemented based on the open-source RLHF framework ChatLearn5. The core implementation of our rule-based verifier is similar to the one used in our evaluation. All policy models in different parameter sizes are trained with the same reward model. We sample 32 responses for each query. Considering a pair of queries and responses as a sample, the number of samples in one episode is 4,096 and 2,048 for training 7B and 72B, respectively. All models are trained with a 512 global batch size. The learning rates are 1 \u00d7 10-5 and 5 \u00d7 10-6 for 7B and 72B, respectively. And the KL coefficient for all training is 1 \u00d7 10\u22123. We mask all output tokens the Python executor provides in reinforcement learning of tool-integrated reasoning."}, {"title": "4 DECONTAMINATION", "content": "Decontamination is critical to ensuring unbiased model performance evaluation. Following prior work (Yang et al., 2024), we exclude potentially contaminated training samples using 13-gram matching. To improve the accuracy of this matching process, we perform text normalization, removing irrelevant punctuation and symbols. To further reduce false negatives, particularly for common mathematical expressions, we introduce an additional criterion: the ratio of the longest common subsequence must exceed 0.6 for a sample to be considered contaminated. For pre-training data, we filter potentially contaminated samples against datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b). When dealing with post-training data, including SFT data, RM training data, and the RL query set, we exclude any potentially contaminated problems or solutions across all reported evaluation datasets. These evaluation datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022b), Gaokao 2023 En (Liao et al., 2024), Olympiad Bench (He et al., 2024), College Math (Tang et al., 2024b),"}, {"title": "5 EVALUATION", "content": "We evaluate our Qwen2-Math and Qwen2.5-Math base models on three widely used English math benchmarks GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and MMLU-STEM (Hendrycks et al., 2021a). In addition, we also evaluate three Chinese math benchmarks CMATH (Wei et al., 2023), GaoKao Math Cloze (Zhong et al., 2024), and GaoKao Math QA (Zhong et al., 2024). All evaluations are tested with few-shot chain-of-thought prompting. The prompts of these benchmarks are shown in Appendix B. For general models, we report the results on LLama-3.1-8B/70B/405B (AI@Meta, 2024) and Qwen2-1.5B/7B/72B (Yang et al., 2024). For specific models, DeepSeekMath-Base-7B (Shao et al., 2024), DeepSeek-Coder-V2-Lite-Base (Zhu et al., 2024), and Intermln2-Math-Base-20B (Ying et al., 2024) are used as baselines.\nThe results are shown in Table 2. We can see that the smallest model of the Qwen2.5-Math series, Qwen2.5-Math-1.5B, outperforms all specific baselines on GSM8K, MATH, CMATH, GaoKao Math Cloze, and Gaokao Math QA. Furthermore, the medium-size model, Qwen2.5-Math-7B, obtains 91.6 and 55.4 scores on GSM8K and MATH, which outperforms Qwen2-72B with 89.5 and 51.1, and Llama-3.1-405B with 89.0 and 53.8. Our flagship Qwen2.5-Math-72B achieves new SOTA on MATH, CMATH, Gaokao Math Cloze, and Gaokao Math QA, which obtains 66.8 on MATH. Compared to Qwen2-Math-1.5B/7B/72B, Qwen2.5-Math-1.5B/7B/72B have achieved significant improvements on all benchmarks. For example, Qwen2.5-Math-1.5B/7B/72B obtains 5.4, 5.0, 6.3 scores improvement on MATH, and 3.4, 12.2, 19.8 scores improvement on Gaokao Math QA, which demonstrates the effectiveness of our Qwen Math corpus v2."}, {"title": "5.2 INSTRUCTION MODELS", "content": "We evaluate Qwen2-Math-Instruct on mathematical benchmarks in both English and Chinese. In addition to the widely-used benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), we also involve more exams that are more challenging to fully inspect the capabilities of Qwen2-Math-Instruct and Qwen2.5-Math-Instruct, such as OlympiadBench (He et al., 2024),"}, {"title": "6 CONCLUSION", "content": "In this report, we introduce Qwen2.5-Math, which features several key technical highlights: (1) extensive use of synthesized mathematical data from Qwen2-Math during the pre-training phase, (2) iterative generation of fine-tuning data and reinforcement training guided by the reward model during the post-training and inference phase and (3) support for bilingual (English and Chinese) queries,\nalong with chain-of-thought and tool-integrated reasoning capabilities. As a result, Qwen2.5-Math represents the most advanced open-source math model series to date. The Qwen2.5-Math-1.5B-Instruct model already surpasses most previous 70B math models, while the Qwen2.5-Math-7B-Instruct matches the performance of Qwen2-Math-72B-Instruct. Our flagship model, Qwen2.5-Math-7B-Instruct, outperforms Qwen2-Math-72B-Instruct with an average score increase of 4.4 points across 7 datasets. We hope that the advances we've made with specialized models like Qwen2.5-Math will continue to strengthen the overall capabilities of the Qwen model and bring us closer to achieving artificial general intelligence."}, {"title": "A.1 NUMBER THEORY", "content": "What is the smallest positive integer t such that there exist integers $x_1, x_2,...,x_t$ with\n$x_1^3 + x_2^3 +...+ x_t^3 = 2002^{2002}$?\nTo determine the smallest positive integer t such that there exist integers $X1, X2,...,xt$ satisfying the equation\n$x_1^3 + x_2^3 + \u2026 + x_t^3 = 2002^{2002}$,\nwe start by analyzing the properties of cubes and their sums. We need to find the smallest t for which the sum of t cubes equals $2002^{2002}$.\nFirst, we note that 2002 can be factored into prime factors:\n$2002 = 2 \u00d7 7 \u00d7 11 \u00d7 13$.\nThus,\n$2002^{2002} = (2 \u00d7 7 \u00d7 11 \u00d7 13)^{2002} = 2^{2002} \u00d7 7^{2002} \u00d7 11^{2002} \u00d7 13^{2002}$.\nNext, we consider the properties of cubes modulo 9. Cubes modulo 9 can only be congruent to 0, 1,\nor-1. This is because:\n$0^3 = 0 \\pmod{9}$,\n$1^3 = 1 \\pmod{9}$,\n$2^3 = 8 = -1 \\pmod{9}$,\n$3^3 = 27 = 0 \\pmod{9}$,\n$4^3 = 64 = 1 \\pmod{9}$,\n$5^3 = 125 = \u22121 \\pmod{9}$,\n$6^3 = 216 = 0 \\pmod{9}$,\n$7^3 = 343 = 1 \\pmod{9}$,\n$8^3 = 512 = -1 \\pmod{9}$.\nNow, we need to determine $2002^{2002} \\pmod{9}$. First, we find 2002 mod 9:\n$2002\u00f79 = 222 remainder 4 \\implies 2002 \u2261 4 \\pmod{9}$.\nThus,\n$2002^{2002} \u2261 4^{2002} \\pmod{9}$.\nNext, we use Euler's theorem, which states that $a^{\\phi(n)} \\equiv 1 \\pmod{n}$ for gcd(a, n) = 1. Here, $\\phi(9) = 6$, so:\n$4^6 \u2261 1 \\pmod{9}$."}, {"title": "", "content": "We can reduce the exponent 2002 mod 6:\n$2002 \\div 6 = 333 remainder 4 \\implies 2002 \u2261 4 \\pmod{6}$.\nThus,\n$4^{2002} \u2261 4^4 \\pmod{9}$.\n$4^4 \u2261 256$.\nFinally, we find 256 mod 9:\n$256 \\div 9 = 28 remainder 4 \\implies 256 \u2261 4 \\pmod{9}$.\nTherefore,\n$2002^{2002} \u2261 4 \\pmod{9}$.\nWe need to find the smallest t such that the sum of t cubes is congruent to 4 modulo 9. The possible sums of cubes modulo 9 are:\n$0 \u2261 0 \\pmod{9}$,\n$1 \u2261 1 \\pmod{9}$,\n$-1 \u2261 8 \\pmod{9}$,\n$1 + 1 \u2261 2 \\pmod{9}$,\n$1 + (-1) \u2261 0 \\pmod{9}$,\n$-1 + (-1) \u2261 -2 = 7 \\pmod{9}$,\n$1 + 1 + 1 \u2261 3 \\pmod{9}$,\n$1 + 1 + (-1) \u2261 1 \\pmod{9}$,\n$1 + (-1) + (-1) \u2261 -1 = 8 \\pmod{9}$,\n$-1 + (-1) + (-1) \u2261 -3 = 6 \\pmod{9}$,\n$1 + 1 + 1 + 1 \u2261 4 \\pmod{9}$.\nWe see that the sum of four cubes can be congruent to 4 modulo 9. Therefore, the smallest t is 4.\nThus, the smallest positive integer t such that there exist integers $X1, X2, ..., Xt$ with\n$x_1^3 + x_2^3 + \u2026 + x_t^3 = 2002^{2002}$ is\n4\nFor a positive integer n we denote by s(n) the sum of the digits of n. Let $P(x) = x^n + a_{n\u22121}x^{n\u22121} + ... + a_1x + a_0$ be a polynomial, where n \u2265 2 and $a_i$ is a positive integer for all $0 \u2264 i \u2264 n \u2212 1$. Could it be the case that, for all positive integers k, s(k) and s(P(k)) have the same parity?\nTo determine whether it is possible for s(k) and s(P(k)) to have the same parity for all positive integers k, we need to analyze the behavior of the sum of the digits function s and the polynomial P(x)."}, {"title": "", "content": "First", "2": "n$P(x) \u2261 x^n + a_{n\u22121"}, "x^{n\u22121} + \u00b7\u00b7\u00b7 + a_1x + a_0 \\pmod{2}$.\nSince ar are positive integers, a\u017c mod 2 is either 0 or 1. Let $b_i = a_i \\pmod{2}$. Then we can rewrite the polynomial modulo 2 as:\n$P(x) \u2261 x^n + b_{n\u22121}x^{n\u22121} + \u00b7\u00b7\u00b7 + b_1x + b_0 \\pmod{2}$.\nNow, we need to check the parity of P(k) for k \u2261 0 mod 2 and k \u2261 1 mod 2.\n1. **Case 1: k \u2261 0 mod 2**\n$P(0) \u2261 b_0 \\pmod{2}$.\nFor s(P(0)) to have the same parity as s(0), bo must be 0. Therefore, ao must be even.\n2. **Case 2: k \u2261 1 mod 2**\n$P(1) \u2261 1^n + b_{n\u22121}\u00b71^{n\u22121} + \u00b7\u00b7\u00b7 + b_1 \u00b7 1 + b_0 = 1 + b_{n\u22121} + \u00b7\u00b7\u00b7 + b_1 + b_0 \\pmod{2}$.\nFor s(P(1)) to have the same parity as s(1), $1 + b_{n\u22121} + \u00b7\u00b7\u00b7 + b_1 + b_0$ must be odd. Since 1 is odd, the sum $b_{n-1} + \u00b7\u00b7\u00b7 + b_1 + b_0$ must be even.\nFrom the above cases, we have: ao must be even. The sum $b_{n-1} + \u00b7\u00b7\u00b7 + b_1 + b_0$ must be even.\nHowever, if ao is even, then bo = 0. This means the sum $b_{n\u22121} + \u00b7\u00b7\u00b7 + b_1$ must be even. But since bi are either 0 or 1, the sum of $b_{n\u22121} + \u00b7\u00b7\u00b7 + b_1$ being even implies that there must be an even number of 1's among $b_{n-1},..., b_1$.\nNow, consider the polynomial P(x) = x + 1. For this polynomial: ao = 1 (odd), which contradicts our requirement that ao must be even. - The sum $b_{n\u22121} + \u00b7\u00b7\u00b7 + b_1 + b_0 = 1 (odd), which also contradicts our requirement that the sum must be even.\nTherefore, it is impossible for s(k) and s"]}