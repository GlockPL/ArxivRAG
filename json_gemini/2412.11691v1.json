{"title": "Multilingual and Explainable Text Detoxification with Parallel Corpora", "authors": ["Daryna Dementieva", "Nikolay Babakov", "Amit Ronen", "Abinew Ali Ayele", "Naquee Rizwan", "Florian Schneider", "Xintong Wang", "Seid Muhie Yimam", "Daniil Moskovskiy", "Elisei Stakovskii", "Eran Kaufman", "Ashraf Elnagar", "Animesh Mukherjee", "Alexander Panchenko"], "abstract": "Even with various regulations in place across countries and social media platforms (Government of India, 2021; European Parliament and Council of the European Union, 2022), digital abusive speech remains a significant issue. One potential approach to address this challenge is automatic text detoxification, a text style transfer (TST) approach that transforms toxic language into a more neutral or non-toxic form. To date, the availability of parallel corpora for the text detoxification task (Logacheva et al., 2022; Atwell et al., 2022; Dementieva et al., 2024a) has proven to be crucial for state-of-the-art approaches. With this work, we extend parallel text detoxification corpus to new languages-German, Chinese, Arabic, Hindi, and Amharic-testing in the extensive multilingual setup TST baselines. Next, we conduct the first of its kind an automated, explainable analysis of the descriptive features of both toxic and non-toxic sentences, diving deeply into the nuances, similarities, and differences of toxicity and detoxification across 9 languages. Finally, based on the obtained insights, we experiment with a novel text detoxification method inspired by the Chain-of-Thoughts reasoning approach, enhancing the prompting process through clustering on relevant descriptive attributes.", "sections": [{"title": "1 Introduction", "content": "The issue of managing toxic speech remains a crucial aspect of human communication and digital violence prevention (Shi et al., 2020), including the mitigation of toxic responses generated by Large Language Models (LLMs) (Yao et al., 2023). The typical approach to dealing with abusive speech on social platforms involves message blocking (Cobbe, 2021). To address this, numerous toxic and hate speech detection models have been developed for different languages, i.e. English (Mathew et al., 2021), Spanish (Molero et al.,"}, {"title": "2 Related Work", "content": "Modern Text Style Transfer Text style transfer (TST) methods can generally be categorized"}, {"title": "3 New ParaDetox Annotation", "content": "We manually collected new data following the main quality criteria (Logacheva et al., 2022): (i) new paraphrases should be non-toxic; (ii) maximal content preservation; (iii) fluency on par with the original text. These data cover five languages\u2014German, Hindi, Amharic, Arabic, and Chinese\u2014chosen based on the native languages of the authors. Annotation and quality control were conducted either by the authors themselves or by hired assistants fluent in the respective languages.\nDefinition of Toxicity We adopt the definition introduced by Dementieva et al. (2024a) only addressing vulgar or profane language (Costa-juss\u00e0 et al., 2022; Logacheva et al., 2022) while the overall message can be either toxic or neutral, but it should not involve deep insults or hate towards individuals or groups of people.\nData Preprocessing For all languages, we maintain the length of samples as sentences of around"}, {"title": "3.1 German", "content": "German ParaDetox was collected with several annotators with manual quality verification:"}, {"title": "3.1.1 Input Data Preparation", "content": "The German language source data is based on three datasets containing toxic, offensive, or hate speech comments on social media about primarily political"}, {"title": "3.1.2 Annotation Process", "content": "To create the final parallel detoxified German dataset, we hired two native German annotators. Annotator A is a female born in 1994 who holds a Master of Arts degree in Social Sciences, and Annotator B is a male born in 1992 who holds a Master of Science degree in Computer Science. The data was distributed so that each sample was transcribed by only one of the annotators."}, {"title": "3.2 Hindi", "content": "Hindi dataset was collected manually by native-speakers gaining data from multiple sources:"}, {"title": "3.2.1 Input Data Preparation", "content": "We used the HASOC dataset created at FIRE 2019 (Mandl et al., 2019) as source for Hindi language. Contents in this dataset are relevant within Indian subcontinent which are collected from various social media platforms prevalent in India. For curation, posts containing OFFENSIVE and PROFANE contents in train and test splits were used. 1 455 PROFANE posts (1 237 train + 218 test) and 873 OFFENSIVE posts (676 train + 197 test) were chosen to prepare detoxifiable toxic data for our task. On a total of 2328 samples, we first performed deduplication via exact string matching."}, {"title": "3.2.2 Annotation Process", "content": "Annotation Setup Out of 2328 samples, 1007 samples were marked as detoxifiable. Annotators were guided to re-write toxic pairs in a non-toxic manner, keeping the meaning of the original post unchanged.\nAnnotators One male NLP researcher working in the field of hate/toxic speech and another female"}, {"title": "3.3 Amharic", "content": "We compiled new Amharic ParaDetox datasets with the following annotation details:"}, {"title": "3.3.1 Input Data Preparation", "content": "The input toxicity data is entirely sourced from the two previous studies, namely (Ayele et al., 2023) and (Ayele et al., 2022). We extracted a subset of these datasets labeled as offensive."}, {"title": "3.3.2 Annotation Process", "content": "Annotation Setup We customized the Potato-Portable Text Annotation Tools and utilized it for the annotation of Amharic ParaDetox dataset. Annotators were provided annotation guidelines, took hands-on practical training, completed independent training tasks before the main annotation task.\nWe began with a pilot annotation of 125 items by three native Amharic speakers and reviewed the quality in a group meeting with experts to clarify the task. Next, we annotated 2995 tweets, each by a single annotator. Each tweet was classified as either detoxifiable or non-detoxifiable. Detoxifiable tweets were then rewritten in a detoxified manner.\nAnnotators Two annotators (one male and one female) were evolved in the main annotation, where both of them are university lecturers and have basic knowledge of NLP tasks."}, {"title": "3.4 Arabic", "content": "Here are details of Arabic ParaDetox collection:"}, {"title": "3.4.1 Input Data Preparation", "content": "The Arabic ParaDetox dataset was created by combining parts of several existing datasets along with the Arabic-translated version of the Jigsaw dataset (Jigsaw, 2017). It includes the Levantine Twitter Dataset for Hate Speech and Abusive Language (L-HSAB) (Mulki et al., 2019), which focuses on Levantine dialects, and the Tunisian Hate and Abusive Speech (T-HSAB) dataset (Haddad et al., 2019), which targets Tunisian dialects. It also"}, {"title": "3.4.2 Annotation Process", "content": "Annotators Detoxification was performed by three PhD-level annotators (two male, one female), all native Arabic speakers with strong computational linguistics backgrounds. Each text sample was transcribed by two annotators, and majority voting determined whether a sentence could be detoxified and if the resulting detoxification was appropriate."}, {"title": "3.5 Chinese", "content": "We collected new Chinese ParaDetox datasets with the following annotation details:"}, {"title": "3.5.1 Input Data Preparation", "content": "Input Toxicity Data The Chinese ParaDetox dataset is derived from TOXICN (Lu et al., 2023), a recently released Chinese toxic language dataset. TOXICN was compiled from social media platforms and comprises 12011 comments addressing several sensitive topics, including gender, race, region, and LGBTQ issues. From this dataset, we extracted a subset based on multiple criteria: the number of toxic words, the ratio of toxic words"}, {"title": "3.5.2 Annotation Process", "content": "Annotation Setup For data annotation and verification, we employed a specifically designed three-task pipeline: Task 1: Determine if the sentences are toxic. Annotators were required to choose one of three options: the given sentence is neutral, toxic but can be rewritten, or toxic and cannot be rewritten. The last option was included based on the observation that some toxic texts are impossible to rewrite in a non-toxic manner. Task 2: Rewrite sentences in a non-toxic style. Annotators were instructed to create detoxified versions of the toxic sentences identified in Task 1 preserving the main content of the original sentences and rewriting the toxic words. Task 3: Cross-check verification. The detoxified sentences were assigned to different annotators for verification to ensure the quality."}, {"title": "3.6 Final Dataset", "content": "The full picture of newly collected and available for now parallel detoxification data in 9 languages is presented in Table 2. In the final stage, experts and native speakers thoroughly reviewed the entire dataset to ensure it met the task's specific requirements and criteria. Using both existing (Logacheva et al., 2022; Dementieva et al., 2024a) and newly collected data, we selected 1000 samples per language which were then split into 400 training and 600 test instances. These datasets and their respective divisions were subsequently utilized for further described analysis and experiments."}, {"title": "4 Explaining ParaDetox with LLM", "content": "Although Large Language Models (LLMs) still have room for improvement in text classification tasks, specifically, for hate and toxic speech (Roy et al., 2023), they have shown significant success in generating explanations (Singh et al., 2024). Given the resource-intensive nature of manually annotating descriptive aspects for each sample across multiple languages, we utilized GPT-4 to assist in generating explanations. We ensured the quality of these explanations by validating them with native speakers, while also conducting an in-depth analysis of parallel text detoxification data."}, {"title": "4.1 Approach", "content": "For all our experiments, we employ GPT-4 (OpenAI, 2022) (May, 2024) leveraging the Chain-of-Thought reasoning method (Qiao et al., 2023) and the CO-STAR framework (Kwon and Gopalan, 2021) specifically designed for reasoning about toxicity and stereotypical biases in data to enhance the detoxification prompt design. All 1000 pairs per nine languages were used for this analysis. The full texts of all prompts are available in Appendix A.\nWe compare toxic and detoxified parts to validate the detoxification process and identify cross-lingual similarities and differences in toxicity. For both parts, we extract descriptive features-toxicity level, tone, language type, implied sentiment, and negative connotation\u2014using the following prompt (Appendix A.1, output example in Table 12):\nSentence: {sentence};\nToxicity Level: Specify here (Low/Medium/High);\nTone: the overall tone of the sentence-choose from keywords;\nLanguage: Language style-choose from keywords;\nImplied Sentiment: the overall sentiment-choose from keywords;\nContext: Brief description of how context contributes to toxicity;\nNegative Connotations: List specific negative word-s/phrases here.\nWe first prompted the model for open-ended descriptions for each feature, then selected the top 30 keywords from the explanations to refine the prompt, minimizing hallucinations. The core prompt was in English, with the target sentence in the respective language. Experts and native speakers reviewed all 1000 samples per language for each feature and toxic keyword. All experts observed GPT-4's tendency to overreact to certain"}, {"title": "4.2 Toxicity Descriptive Features Analysis", "content": "The overall view on top descriptive features for all languages as well as toxicity level per language are provided in Figure 3. The full list of top descriptive feature per language are provided in Appendix E.\nAcross all languages, we observe a reduction from high toxicity to medium or low levels, confirming that the paraphrases have been effectively detoxified. The original texts are predominantly aggressive, derogatory, vulgar, and insulting, often conveying hostile, negative, and disdainful sentiments. In contrast, the neutral paraphrases tend to shift towards informal, colloquial, or even neutral language, though they may still retain some negative or critical undertones."}, {"title": "4.3 Toxic Keywords Analysis", "content": "We extracted the most frequent toxic collocations from the toxic texts, as shown in Figure 4.\nWe found both similarities and differences in the typical rude and obscene language across languages. While some toxic words-like, f*ck, idiot, as* are present almost in all target languages, we can also see cultural specifics. In Ukrainian, Russian, and Chinese, derogatory comparisons involving homosexual individuals are considered insults, while in Hindi and Amharic, referring to someone using animal names is more prevalent. In Germany,"}, {"title": "4.4 Text Detoxification Analysis", "content": "Then, we analyzed the way how detoxification was performed (see Table 3). We sought lemmas that reflect various editorial actions\u2014delete, remove, rephrase, replace, insert, add\u2014using the following prompt template: Answer shortly, how this text: {toxic text} was rephrased into this: {detoxified text}. Additionally, we computed the Levenshtein distance between toxic and non-toxic parts (Appendix D).\nAcross all languages, adding new content is rare. Detoxification mainly involves removing or rephrasing toxic elements. In German, Arabic, Hindi, Ukrainian, and Amharic, removal and rephrasing occur equally, while Spanish favors removal and Chinese/Russian rely more on rephrasing. Consequently, localized edits with fluent substitutions generally suffice for effective detoxification."}, {"title": "4.5 Chain-of-Thoughts Text Detoxification", "content": "Finally, we developed a new chain-of-thought reasoning approach to improve text detoxification with LLMs by guiding detoxification with explanations and close examples.\nOur descriptive analysis suggests that the most effective detoxification approach varies according to descriptive features, toxicity expression and the target language itself. Depending on these factors, the detoxification strategy should be chosen accordingly. While it is challenging to come up with the clear human-readable instruction, the detoxification can be explained via examples. Thus, based on the extracted descriptive features, we performed"}, {"title": "5 Automatic Evaluation Setup", "content": "We adopt the evaluation pipeline from Logacheva et al. (2022) to our multilingual setup. Direct links to the datasets/models instances are in Appendix B.\nStyle Transfer Accuracy (STA) We subsampled 5000 samples-2500 toxic and 2500 neutral-from toxicity classification corpora for each language (see in Table 2) that were not used for ParaDetox data collection. We fine-tuned XLM-R-large (Conneau et al., 2020) instance for the binary toxicity classification task."}, {"title": "Joint score (J)", "content": "is the aggregation of the three above metrics:\n$J = \\sum_{i=1}^n STA(y_i) \\cdot SIM(x_i, Y_i) \\cdot ChrF1(x_i, Y_i)$,\nwhere $STA(y_i)$, $SIM(X_i, Y_i)$, $ChrF1(x_i, Y_i) \\in [0, 1]$ for each text detoxification output $Y_i$."}, {"title": "6 Baselines", "content": "For comparison, we considered several unsupervised and supervised text detoxification approaches together with a baseline prompt construction. Details of the hyperparameters and model choices for each method can be found in Appendix C.\nDuplicate Trivial baseline: the output sentence is a copy-paste of the input sentence. This baseline has 1.0 (or 100%) SIM score by definition.\nDelete Removal of offensive terms using a manually compiled list of vulgar words. We collected and compiled together the lists of such toxic keywords for all target languages based on openly available sources (see Table 5).\nBacktranslation As for a more sophisticated unsupervised baseline, we performed translation of non-English texts into English with NLLB (Costa-juss\u00e0 et al., 2022) to then perform detoxification with the fine-tuned on English ParaDetox BART (Logacheva et al., 2022). The detoxification results were translated back to the target languages.\ncondBERT We adapted one of the MLM-based unsupervised methods from Dale et al. (2021). We used mBERT (Devlin et al., 2019) as a base model. The model runs MLM to generate list of substitutes selecting non-toxic ones.\nFine-tuned LM on Translated Data We also tried to obtain synthetic parallel corpora by translating selected 400 English ParaDetox samples to our target languages. We utilized mBART for machine translation model (Liu et al., 2020) for the translation step. We tuned the mBART for text generation (Tang et al., 2020) on the obtained data."}, {"title": "7 Results", "content": "We conducted a multilingual text detoxification across all languages on the test sets, with the results presented in Table 4 and detailed metrics per language in Appendix G. Surprisingly, the Delete method outperformed other unsupervised approaches for three languages\u2014Chinese, Arabic, and Amharic. This may be due to the nature of these languages (Table 3), where detoxification relies heavily on paraphrasing. Since the proposed methods still struggled with appropriate paraphrasing, Delete, which removes toxic content without rephrasing, performed best. However, for other languages, where rephrasing is also key, LM-based solutions excelled, likely due to better representation of the languages in the pre-training data.\nWhile for the majority of languages mBART fine-tuned on human-curated data outperformed the model fine-tuned on translated data, this results is not consistent. As described previously, some obscene terms are similar across languages and can be translated from English, offering sufficient information about toxicity for the target language. However, in the case of German, Hindi, Ukrainian,"}, {"title": "8 Conclusion", "content": "This work addressed the multilingual and explainability aspects of the text detoxification task. We introduced manually curated parallel detoxification datasets for new languages\u2014German, Chinese, Arabic, Hindi, and Amharic and the detailed data collection process. Next, we used LLMs as explainability tools on nine languages to analyze key descriptive features of toxic and non-toxic texts, identify top toxic collocations, and determine the primary actions required for detoxification per different toxicity expressions. Building on these insights, we developed a new Chain-of-Thoughts LLM prompting text detoxification method that incorporates detoxification cluster information about the input text. This approach reduced model's hallucinations, improved precision in edits, incorporated cultural specifics, and outperformed all baselines."}, {"title": "Limitations", "content": "Firstly, while the work aims to extend data to new languages, there remains significant room for improvement in incorporating as many languages as possible. The selection of languages in this study was based on the native languages of the authors, but broader involvement of other language stakeholders could enhance the dataset.\nSecondly, this work focuses solely on multilingual detoxification without exploring monolingual or cross-lingual tasks. Further research could be conducted to identify the most effective detoxification model for each language using the created data. Additionally, cross-lingual approaches could explore how detoxification knowledge transfers between languages, opening new avenues for research. Preliminary cross-lingual transfer experiments have been conducted for English and Russian (Dementieva et al., 2023), but the new dataset now includes more languages for further exploration.\nFor the CoT approach, we focused on human-readable cluster explanations in English; however, this approximation was not thoroughly explored for other languages. Our method currently relies on example-based explanations, and further research into human-readable cluster descriptions remains open for future work.\nLastly, the primary experiments in this study were conducted using GPT-4, a closed-source model from OpenAI. While GPT-4 continues to perform exceptionally well in various NLP benchmarks, demonstrating stable generation of coherent explanations, we recognize the importance of supporting open-source initiatives. Therefore, we acknowledge the necessity of ablation study with opensource LLMs."}, {"title": "Ethics Statement", "content": "We explore the task of text detoxification with no intent to violate the freedom of speech, but rather to help mitigate digital violence, create safer online environments for children, and promote the development of secure AI models. The ideal implementation of detoxification models on communication platforms would be as suggestions, rather than forced corrections. A user-friendly interface for these suggestions should be considered by stakeholders.\nAdditionally, detoxifying LLMs, not just human content, is a relevant topic. Already several ap-"}, {"title": "J", "content": "is the aggregation of the three above metrics:\n$J = \\sum_{i=1}^n STA(y_i) \\cdot SIM(x_i, Y_i) \\cdot ChrF1(x_i, Y_i)$,\nwhere $STA(y_i)$, $SIM(X_i, Y_i)$, $ChrF1(x_i, Y_i) \\in [0, 1]$ for each text detoxification output $Y_i$."}, {"title": "C.1 Delete", "content": "The resources used for the multilingual toxicity lexicon compilation are listed in Table 5. The full list is available online for public usage and reproducibility."}, {"title": "C.2 Backtranslation", "content": "For the translation step, we used the NLLB instance.11 For English sentences detoxification, we utilized previously released BART-detox English instance. 12"}, {"title": "C.3 condBERT", "content": "We re-used of the condBERT pipeline introduced in (Dale et al., 2021) 13 with mBERT-base14 model and the hyperparameters for the masked language modelling task via MaskedTokenPredictorBert class with parameters max_len= 250 and contrast_penalty= 0.0."}, {"title": "C.4 MBART", "content": "Previous experiments in Dementieva et al. (2024a) showed quite poor performance of BloomZ-7b (Muennighoff et al., 2023) for the text detoxification. To choose the model for supervised fine-tuning for new multilingual text detoxification, we compared in this case two multilingual text generation models-mT0-large (Muennighoff et al., 2023)15 and mBART-large (Tang et al., 2020)16. The results comparison"}, {"title": "D Toxic and Detoxified Sentences Lengths Comparison", "content": "Additionally to the toxic keywords and edits types analysis, we also provide the lengths comparison of toxic and non-toxic parallel pairs (Figure 6) and the Levenshtein distances between them (Figure 7). The lengths and distances calculation are based on the tokenization performed with textdetox/xlmr-large-toxicity-classifier used for STA calculation. Here, we again observe language-specific differences. For instance, in Chinese, detoxified versions are longer than their toxic counterparts, while in Amharic the length disparity is substantial. Even though toxic phrases are removed, the size of the replacement phrases can vary depending on both the language and the nature of the toxicity."}]}