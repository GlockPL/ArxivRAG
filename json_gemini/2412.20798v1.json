{"title": "A Tale of Two Imperatives: Privacy and Explainability", "authors": ["Supriya Manna", "Niladri Sett"], "abstract": "Deep learning's preponderance across scientific domains has reshaped high-stakes decision-making, making it essential to follow rigorous operational frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation (RTE). This paper examines the complexities of combining these two requirements. For RTP, we focus on 'Differentially privacy' (DP), which is considered the current gold standard for privacy-preserving machine learning due to its strong quantitative guarantee of privacy. For RTE, we focus on post-hoc explainers: they are the go-to option for model auditing as they operate independently of model training. We formally investigate (DP) models and various commonly-used post-hoc explainers: how to evaluate these explainers subject to RTP, and analyze the intrinsic interactions between DP models and these explainers. Furthermore, our work throws light on how RTP and RTE can be effectively combined in high-stakes applications. Our study concludes by outlining an industrial software pipeline, with the example of a wildly used use-case, that respects both RTP and RTE requirements.", "sections": [{"title": "1 Introduction", "content": "Deep learning has achieved massive success over the past decade in several domains and high stakes Dong et al. (2021). It has been a cornerstone ever since in almost all aspects of scientific discoveries. However, it is to be noted that deep learning, although predominant in today's scientific understandings, comes with its inherent shortcomings Talaei Khoei et al. (2023); Saeed and Omlin (2023); Boulemtafes et al. (2020). In this paper, we shall investigate the intricacies of two such pivotal shortcomings: Privacy and Explainability. We shall thereafter substantiate our findings with a famous use case from previous studies.\nFirstly, the deep models, although achieve superior performance across domains, are inherently prone to sensitive data leakage Boulemtafes et al. (2020). It has been extensively shown that the models can leak information about the data with which it has been trained. Even, especially in the biomedical domain, it has been shown that simple 'linkage' attacks can exploit anonymized electronic health records Sweeney (2015). Due to the inherently vulnerable nature of deep models, it is not only prone to severe privacy attacks including membership inference (MIA) Hu et al. (2022), model stealing Oliynyk et al. (2023), model inversion Fredrikson et al. (2015); Wang et al. (2021); Veale et al. (2018) etc but also impose a threat on the applicability of deep learning in real-world applications.\nSecondly, the deep models are hard to explain Saeed and Omlin (2023). Neural nets are nonlinear systems, often try to learn the (approximate) distribution of the training data."}, {"title": "2 Preliminaries", "content": "Privacy-preserving machine learning is the building block for RTP in modern day ML systems. Researchers have developed numerous strategies for privacy-preserving machine learning, including homomorphic encryption, PATE, and differential privacy (DP) Boulemtafes et al. (2020); Papernot et al. (2018); Abadi et al. (2016). Among these, DP has emerged as the gold standard due to its strong worst-case guarantee against inference attacks Blanco-Justicia et al. (2022); Dwork et al. (2014). In this work, we focus on DP models with varying privacy guarantees.\nA randomized mechanism F satisfies (\u03b5, \u03b4)-DP if, for any two neighboring datasets D and D' differing in at most one record, and for all measurable subsets S of the output space,\nPr[F(D) \u2208 S] < ePr[F(D') \u2208 S] + \u03b4.\nHere, & represents the probability of the mechanism failing to provide privacy guarantees. Specifically, it accounts for the small chance that the added noise does not sufficiently obscure the presence or absence of an individual in the dataset. Smaller values of d are preferred.\nDP comes with a few intersting properties such as sequential composition, parallel composition and, post-processing. In our study, post-processing is most relevant which let the user perform arbitrary operations on the output of a DP mechanism Dwork et al. (2014).\nTo make a model differentially private (DP), it undergoes privacy-preserving training, with DP-SGD being the most prominent method Ponomareva et al. (2023). It introduces Gradient clipping ensuring bounded contributions of individual data points and noise to the gradients during each update to ensure differential privacy Abadi et al. (2016). Gaussian noise is commonly used due to its well-analyzed privacy guarantees. Alternatively, laplacian noise is employed in some contexts where bounded sensitivity is easier to calculate Dwork et al. (2014). For DP-SGD, gaussian noise is predominantly used whereas, Laplacian noise is popular in Local Differential Privacy (LDP) Dwork et al. (2014); Ponomareva et al. (2023).\nLocal Differential Privacy (LDP) provides privacy guarantees at the data source. A mechanism F satisfies e-LDP if, for any two inputs x and x' and all outputs y,\nPr[F(x) = y] < e&Pr[F(x') = y].\nIn this work, we denote the non-private model as M. We obtain its private counterpart M' by retraining with DP-SGD."}, {"title": "3 Two Side of the Coin: Privacy and Explainability", "content": "Explainable machine learning is crucial for Right-to-Explainability (RTE). As mentioned earlier, this paper focuses on local, post-hoc explainers (denoted as I) due to their \u201cplug-and-play\u201d nature, which makes them highly practical, especially for industrial applications Bhatt et al. (2020b). Broadly, these explainers can be classified into two categories: Perturbation-based methods and Gradient-based methods. Both types of methods take a model (g) and a datapoint (x) as input and output a feature attribution score(s) against g(x). A a feature attribution score (FAS) (or a feature attribution vector) consists of scores assigned to individual features of the given input representing their importance towards the classification the given model comes up with. A positve attribution score implies a feature has positively contributed towards the classification and negative attribution score shows that a feature does not positively contribute towards the classification. Given an explainer and a datapoint (x), we denote the FAS which explains the prediction of M and M\u2032 as s and s\u2032 respectively; given M(x) = M\u2032(x).\nIn our use case, s (and s\u2032) are typically computed on a per-pixel or per-element basis, and they generally match the dimensions of the input or the corresponding intermediate layer. In the next section, we shall discuss the non-private setup and potential security breaches involved."}, {"title": "3.1 The Privacy Aspect", "content": "The non-private setup consists of an adversary A, having access to a data-point x, a non-private model M, the output vector V obtained from M for x, an explainer I generating a feature attribution score s against M(x).\nFrom the point of A, we identify two distinct ways of attacking and leaking the information from the trained model2:\n1. For a given data-point x, assuming A has access to the output vector V from the trained model M, A can perform an MIA on the model Hu et al. (2022).\n2. Assuming A has access to the feature attribution score s of x generated by I subject to M(x), A can leverage s to execute an MIA on M Shokri et al. (2021), especially when I is faithful.\nHowever, as M\u2032 , is the DP counterpart of M, it inherits the post-processing property Dwork et al. (2014) which makes any mechanism including obtaining V, s, applied over M\u2032 to produce results which are also DP, so we can mitigate both of the breaches."}, {"title": "3.2 The Explainability Aspect", "content": "As discussed earlier, in this study we are exclusively considering local post-hoc explainers Lundberg et al. (2020); Huber et al. (2021); Lundberg and Lee (2017). The quality these local explanations are judged across several parameters Hedstr\u00a8om et al. (2023): Faithfulness"}, {"title": "3.3 Pitfalls of the Evaluation Metrics", "content": "Firstly, in the perturbation-based metrics such as PF, I/D-AUC, IROF, IF etc while applying operations such as flipping the pixels, inserting and/or deleting features, performing meaningful perturbation on the input space to generate synthetic inputs as a part of their evaluation process, do not crosscheck whether the generated input is out-of-distribution with respect to the trained model Hase et al. (2021); Chang et al. (2018). Secondly, metrics such as OPs are often excessively similar to the mechanics of the explainer itself. Instead of evaluating faithfulness, these methods primarily compute the similarity between the evaluation metric and explanation techniques, assuming the evaluation metric itself to be the ground truth Ju et al. (2021). Li et al. Li et al. (2023) has acknowledged the same in their benchmark M\u00b9 that LIME Ribeiro et al. (2016) and OPs are methodologically similar thus, the evaluation maybe skewed. Thirdly, previous studies have extensively shown that test-time input ablation is often prone not only to generating out-of-distribution (OOD) synthetic input Hase et al. (2021); Haug et al. (2021); Chang et al. (2018); Janzing et al. (2020) but also are socially misaligned Jacovi and Goldberg (2021). Thus, metrics for example I/D-AUC, IROF etc are suspected to be severely misleading. Quantitative metrics such as SF are also substantially constrained and do not provide a universal overview of faithfulness. Lastly, all these evaluations are based on naive assumptions (e.g.: erasure Jacovi and Goldberg (2020)), derived from a set of seemingly valid observations. Therefore, these quantitative metrics are not necessarily axiomatically valid. Even axiomatic necessary tests such as Model Parameter Randomization test Adebayo et al. (2018) happen to have a set of empirical confounders Yona and Greenfeld (2021); Kokhlikyan et al. (2021); Bora et al. (2024). To the best of our knowledge, there has not been any universally valid necessary and sufficient approach for faithfulness evaluation Lyu et al. (2024).\nFurthermore, the lack of a unified faithfulness evaluation method and the absence of ground truth for explainers lead to the well-known disagreement problem Evans et al. (2022); Neely et al. (2021); Roy et al. (2022); Krishna et al. (2022). Even if explainers are faithful, their inherent mechanisms for calculating feature importance can lead to different explanations. For instance, Han et al. Han et al. (2022) demonstrated that while a subset of"}, {"title": "3.4 Aspiration for the Alternatives", "content": "Acknowledging these inherent limitations in current evaluation methods, we propose two key remarks:\n1. Remark 1. Expert oversight should determine whether an explanation is suitable for high-stakes applications.\n2. Remark 2. Explanations should align with local constraints and contexts, even when (so-called) faithfulness cannot be measured reliably.\nTo address the first requirement, in our use case we ensure that concerned physicians first receive X-ray images along with predictions and explanations. Results are only communicated to patients after the physician has completed a formal review and certified both the prediction and the explanation.\nFor the second remark, we are introducing the localization assumption (LA) and quantifying the same with a class of measures we collectively named the Privacy Invariance Score (PIS) for explanations."}, {"title": "4 Localization Assumption & Privacy Invariance Score", "content": "Right-to-Privacy (RTP) Thomson (1975) and Right-to-Explanation (RTE) Vredenburgh (2022) are two inalienable aspects of modern-day ML software. We have already mentioned that previous works have shown potential security breaches leveraging the explanation in section 3.1. So, RTE can hamper RTP but is the converse true? If yes, How can we quantify the same? We first propose the desideratum for explanations in this setting and then run extensive experiments to quantitatively judge the explainers."}, {"title": "4.1 Description of the setting", "content": "It is shown that adversaries can leverage sensitive information from explanations but due to post-processing of DP, any DP model will always output private explanation Dwork et al. (2014). We want to check the extent to which the explanations from a DP model can be used as a proxy for that of the non-private model here. Formally, for a non-private model M, its private counterpart M'; consider a set of post-hoc explainers E used for auditing"}, {"title": "4.2 Localization Assumption", "content": "As discussed earlier, different works evaluated explainers in various ways. These ad-hoc norms are often unique to each paper and inconsistent Jacovi and Goldberg (2020). However, practitioners have proposed some necessary axiomatic desiderata Lyu et al. (2024) such as the well-established Implementation Invariance (II) criterion proposed by Sundararajan et al. Sundararajan et al. (2017).\nAccording to II if I is faithful and M(x) = M'(x) \u2200x \u2208 X, i.e. M, M' are functionally equivalent (FE) then s = s' \u2200x. Commonly used explainers like Integrated Gradient Sundararajan et al. (2017), SmoothGrad Smilkov et al. (2017), DeepLift Li et al. (2021), layerwise relevance propagation (LRP) Montavon et al. (2019) etc had been extensively accessed based on II Sundararajan et al. (2017).\nFollowing the line of prior explainability research, Jacovi et al. Jacovi and Goldberg (2020) formally proposed The Model Assumption (MA):\n\u201cTwo models will make the same predictions if and only if they use the same reasoning process.\u201d\nNevertheless, in our setting, as the accuracy of M' is generally less Ji et al. (2014), and we cannot say M and M' are FE Sundararajan et al. (2017). Furthermore, as X in practice can be arbitrarily large (theoretically could be countably infinite), quantifying whether an explainer (I) is sufficiently trustworthy or not is often impractical. Also, finding a counterexample that violates the condition implying I is not faithful is computationally expensive. Hence, we modify MA and propose the localization assumption for evaluating the explainers' quality in our setting.\nFirst of all, in both the II and MA we do not advocate comparing any arbitrary models trained on the same data, having the same X to compare. For example, for a finite X, a trained neural network and a decision tree could have the same prediction \u2200x \u2208 X. That doesn't mean their reasoning is similar as the algorithms themselves are different. However, the private model M', in our setting, has identical architecture to that of M. The fundamental goal of differential privacy is assumed to be masking individual contributions of the training set rather than completely changing its overall reasoning. However, as noise is induced in the gradient during the training, the parameters are expected not to remain entirely the same in the private model. Therefore, we assume that their reasoning should primarily be similar, if not the same. Formally, our adapted version of the MA is:\nAssumption 1 The Localization Assumption (LA). For a given tuple (M, M', x,I) having M(x) = M'(x), sim(s, s') >= \u03b8. Where \u03b8 is a predefined similarity threshold.3"}, {"title": "4.3 The Notion of Similarity", "content": "As previously discussed in PIS, we aim to measure the similarity between pair of explanations ; it is essential to account for two key factors here: the context of comparison and human understanding of that comparison. In this framework, given (M, M', x,I) and M(x) = M'(x), we seek to investigate two primary aspects:\n\u2022 To what extent do s and s' agree?\n\u2022 Wherever they agree, what is the degree of that agreement?\nFirstly, explanations generally contain both positive and negative attributions\u2074. For a classification, the positive attributions contribute in the favor of the classification while the negative ones don't. Keeping this in mind, in order to address the first question, we start by calculating the number of places in both explanations where they yield different types of attribution scores; we denote this as the disagreement score (DS, measured in %). As both s and s' comprise attribution scores (per pixel or per element) that either positively contribute to the classification or do not, DS serves as a (crude) measure for assessing how"}, {"title": "4.4 Other notions for Similarity", "content": "Can cosine similarity be an alternative?: Mostly No, as in such high dimensions the large random vectors are almost always almost orthogonal if they are not (condensely) clustered Mohammadi and Petridis (2022); Wyner (1967). Moreover, unlike the correlation and disagreement cosine is not that much nuanced as this doesn't compare the Condorcet pairs but imposes a vague sense of how much the vectors are 'aligned' defined over their inner product space in such high dimension which doesn't go with our objective."}, {"title": "5 Experimental Setup", "content": "Our dataset comprises 2,000 Pneumonia cases sourced from the Chest X-ray dataset by Sharma et al. Sharma (2020), and 2,000 TB cases randomly sampled from the NIAID TB Portal Program dataset National Institute of Allergy and Infectious Diseases (n.d.). To create the 'Normal' subset, we include an equal split of 1,000 unaffected Pneumonia samples from the uneffected class in the aforsaid Chest X-ray dataset Sharma (2020) and 1,000 unaffected TB samples from Rahman et al. Rahman et al. (2020), totaling 2,000 normal cases. For evaluation, the test set contains 200 images for each class.\nThis paper, unlike prior studies, is not focused on the empirical studies of private models in healthcare Zerka et al. (2020); Naresh et al. (2023); Khalid et al. (2023). Instead, we concentrate on the applicability of commonly used explainers in a privacy preserving environment. Our aim is to investigate whether RTE and RTP can be achieved simultaniously. As a result, rather than a large dataset where extensive experimentation with DP under a closely controlled environment is computationally expensive and cumbersome Subramani et al. (2021), we focus on first making a dataset of an appropriate size where we have considered the commonly available and utilized disease class from the previous studies: Pneumonia, Tuberculosis (TB). However, after our primary experiment, we devise the same setup for another benchmark dataset: CIFAR-10 but we arrived at similar conclusion. Details can be found in Appendix B."}, {"title": "5.2 Choosing the privacy budget aka '\u03b5'", "content": "The determination of privacy requirements and expectations is a critical aspect of our experimental setup. However, selecting e comes with a notable dichotomy:\nThe theoretical school advocates for stringent privacy guarantees, proposing \u20ac < 1. This conservative approach provides robust worst-case guarantees against inference attacks. For instance, in the case of, \u0454 \u2264 0.1 ensures that no membership inference attack (MIA) can outperform random guessing by more than a marginal 2.5% (i.e., 52.5% success rate).\nConversely, industrial implementations often employ substantially larger values, typically \u20ac > 7. According to the theoretical school such choices, while pragmatic, significantly increase vulnerability. With such choices of e, system becomes severely vulnerable with the worst case MIA attack succedding rate exceeding 99%.\nThis stark contrast stems from divergent underlying assumptions Lowy et al. (2024):\n\u2022 The theoretical model assumes an adversary with near-complete knowledge of the training set, lacking information on only a single data point.\n\u2022 It also presupposes uniform privacy guarantees across all data points.\nVery recently, Lowy et al. (2024) has shown why often these assumptions are not relevant in practical and industrial settings and DP with large \u0454 (even e \u2265 7) can sufficiently defend"}, {"title": "5.3 Description of the Explainers", "content": "We primarily selected gradient-based explainers which are predominantly used in computer vision tasks Buhrmester et al. (2021) \u2014 specifically, Saliency, SmoothGrad, Integrated Gradients , Grad-Shap and, Grad-CAM. These gradient-based explainers rely on the sensi-tivity the model shows to the features subject to the output it obtains. In other words, these explainers leverage the gradient of the output (or any selected class of interest) w.r.t. the input features. For example, Saliency computes the partial derivatives of the output with respect to the input given, Integrated Gradients computes the average gradient while the input varies along a linear path from a baseline. SmoothGrad involves adding noise to the input image and generating multiple saliency maps, each corresponding to a slightly different noisy version of the input image; the saliency maps are then averaged to produce a 'smoothed' saliency map. Grad-CAM also involves taking gradients of the target output with respect to the given layer. From perturbation-based explainers, we included only SHAP Lundberg and Lee (2017), specifically Grad-Shap, as core perturbation methods are less widely adopted for computer vision. Grad-Shap can be viewed as an approximation of Integrated Gradients by computing the expectations of gradients for different baselines\u2078. Other perturbation-based methods like LIME Ribeiro et al. (2016) assign weights to super-pixels rather than individual pixels as a result, we cannot compare the same with other methods we chose which output"}, {"title": "5.4 Description of the Networks", "content": "Disease detection using chest X-ray is a well-regarded case study. In previous works Al-qaness et al. (2024), the choice of CNNs is significantly prominent and in this work also we have chosen the famous CNNs that had been used in previous research. Particularly, We selected ResNet-34 He et al. (2016), EfficientNet-v2\u00b9\u2070 Tan and Le (2021), and DenseNet-121 Huang et al. (2017) for our analysis due to their competitive performance. Importantly, we ensured that the DP counterparts of these models were functionally comparable across a wide range of values for e, to the best of our ability. However, previous studies have shown that DP doesn't make the model learn all classes in a comparable fashion Suriyakumar et al. (2021); Fioretto et al. (2022) as a result, we report the accuracy (acc) as a measure of Perf Com\u0442\u0440. and agreement on hard prediction to measure Alignment on a class-wise basis, allowing for a more nuanced comparison.\nWe train the non-private and private models fixing all the hyperparameters except for the number of epochs, as private models need more computation to learn due to the heavy regularization DP introduces in the training Ponomareva et al. (2023). However, to make a fair comparison we have fixed the number of all hyperparameters in all the private models with different \u20ac\u00b9\u00b9.\nHowever, while training we discovered an issue in the vanilla architecture of the aforemen-tioned networks: they utilize batch normalization (BatchNorm). Nevertheless, BatchNorm normalizes a sample based on the statistics of the batch it is in. This means the same sample can get different normalized values depending on the other samples in the batch. For differential privacy, each sample's privacy needs to be independently preserved. Since BatchNorm depends on other samples, it violates this principle and leaks information about other samples. Yousefpour et al. Yousefpour et al. (2022) advises replacing BatchNorm layers with privacy-friendly options like Group Normalization, Layer Normalization, Instance Normalization etc. From the engineering perspective, we have to select one such replacement that scales with sufficiently large datasets without hampering the privacy bounds. Based on previous empirical evidence Subramani et al. (2021), we replace the BatchNorm layers with GroupNorm layers in all non-private models along with their private counterparts, as GroupNorm does not alter the base architecture drastically, scales well and adheres to the privacy principle strictly. We employ the DP-SGD algorithm for the private training Abadi et al. (2016). The non-private models' accuracies are reported in Table 1 and in Figure 1, 2, 3 for private counterparts."}, {"title": "6 Key Observation", "content": "In this section, we present our experimental results to find out whether the post-hoc explain-ers agree with the Localization Assumption (LA), defined in Section 4.2. Integrated Gradients and Grad-Shap showed more than 45% DS everywhere which directly violates our DS threshold of 15%. As we consider DS a sanity check for LA, and these two explainers fail this test, we exclude them from further discussions. Across models, around 30 \u2013 40% Grad-Cam explanations of test samples violated the DS threshold, we have considered the"}, {"title": "7 Why Explainers Performed so Poorly?", "content": "As we have observed in the previous section, the explanations from the private and non-private models are largely uncorrelated. The private models have been trained with DP-SGD. In DP-SGD, we clip the gradient each time and infuse calibrated noise into it while backpropagating to make the model DP. To understand the divergent behavior of explainers for private and non-private models, we begin by examining how the parameters of models are altered after DP training in private models. Specifically, we conduct a comparative analysis of the"}, {"title": "7.1 Did the models perceive the same information?", "content": "In its forward pass, a neural network hierarchically transforms the given input into increasingly abstract (and complex) representations. A Representation (also referred to as a feature representation or simply (intermediate) feature) is the set of activations stored layerwise in the network during the forward pass for a given input. We begin our investigation by examining how these representations differ in the private and non-private model, as this reveals how these two different models process and transform the input data across layers. Understanding the alignment of these representations is critical for analyzing how DP training has altered the model's parameters and how these changes influence the model's overall reasoning.\nHowever, since the training processes for DP model is fundamentally different and designed to achieve distinct goals, we first conduct a formal assessment to determine whether any statistical dependence exists between their layerwise representations. This step ensures that the representations are comparable before further analysis. We denote the representation at layer l as \u03c3\u03b9.\nStatistical tests for activations are often non-trivial and subject to a few constraints such as invariance to permutation of the neurons, orthogonal transformation Klabunde et al. (2024) etc. Consequently, we use the Hilbert-Schmidt Independence Criterion (HSIC) Gretton et al. (2007) which operates over a Reproducing Kernel Hilbert Space (RKHS) and can detect dependencies across complex, high-dimensional variables while respecting the aforesaid constraints. We employ the HSIC unconditional independence test using the two-parameter y approximation scheme with a p-value cut-off of 0.05 Gretton et al. (2012).\nFrom our findings, we were able to reject the null hypothesis Ho for almost all layers\u00b9\u00b3. In other words, across models layerwise representations are not independent, for all e. Since the representations are sufficiently comparable, we'll now compute their similarity layerwise. However, before a full-fledged comparison, we present a short note on representational similarity (RS)."}, {"title": "7.1.1 ON REPRESENTATIONAL SIMILARITY", "content": "Representational similarity measures compare neural networks by computing similarity between activations of a fixed set of inputs (in our case, test set) at a given pair of layers. There's been an extensive line of work on this domain however, each of them can be"}, {"title": "7.2 Do the models show similar sensitivity?", "content": "In this section, we investigate how sensitive the model's output is to the layerwise representa-tions generated during the forward pass. Formally, for a given layer I and a class of interest (here, hard prediction) \u0398, we compute the gradient: \u2207\u03c3\u0398. This gradient is particularly important for two reasons: first, it quantifies the influence of an infinitesimal perturbation in the representations at layer I on the final output. Second, as previously discussed in section 5.3, gradient-based explainers also leverage \u2207\u03c3\u0398 to generate explanations\u00b9\u2074. Thus,"}, {"title": "8 Is there any alternative way to have private explanations?", "content": "We now know that the DP models cannot accommodate widely used post-hoc explainers primarily due to their own behaviour. As a result, we cannot move forward with DP models to obtain private explanations. However, just for private explanations, we don't need that. We can achieve the same locally as well.\nIn this approach, rather than training the model with DP-SGD, we use the non-private model as usual and take their explanations. We add calibrated noise to the explanation to make it Local Differential Private (LDP)  Formally, given a function f : D \u2192 Rd, the Laplace mechanism A is defined as:\nA(D) = f(D) + Lap(0|t)d\nWhere A satisfies e-differential privacy for t = Af, and Af is the sensitivity\u00b9\u2078 of f Dwork et al. (2014).\nHere, Noise is directly applied to the heatmap obtained from the explainer. Following the guidelines by Fan (2018), for an image with c channels, each of which spans k possible pixel intensities, the sensitivity of the query is (k \u2212 1)nc, where n is the maximum number of pixels that can differ in two adjacent images. Given this query sensitivity, e-differential privacy can be provided by independently applying the Laplace mechanism to each pixel in each channel, using a scaling parameter of (k \u2212 1)nc/e. However, such a large amount of noise, especially for smaller e, would completely obfuscate the semantics of the image. To reduce the query sensitivity, the authors propose dividing the image into b \u00d7 b grids of pixels, where each grid is averaged to create a uniform value. Treating each grid as a single unit to be obfuscated reduces the required scaling parameter to (k \u2212 1)nc/b\u00b2e. Therefore, in the case of explanations (or likewise any heatmap with 1 channel), the query sensitivity is 255n/b\u00b2.\nAny valid set of n and b chosen by the user ensures e-differential privacy, but tuning these parameters offers a trade-off between privacy and utility. Therefore, this framework allows us to generate explanations that are both useful and private.\nHere, a natural question arises: what constitutes a good value for n? Previous works have tackle this problem empirically as this primarily depends on the use-case. We, therefore,"}, {"title": "8.1 What Could Be the Notion of PIS and LA Here?", "content": "Well, after LDP-fying with b \u00d7 b grids, we cannot compare explanations pixel-to-pixel. Here, our motivation is to compare how much LDP-fying degrades the quality of the explanation, to measure that we have to change the similarity measure accordingly. That is why here, unlike global DP, we do not measure DS or correlation but following the postulate of LA, we expect that the most competent explanation should be least affected and thus should be most similar and can be a close proxy to the actual explanation. For the notion of PIS, we measure the similarity of the newly made explanation image with Structural Similarity Index (SSIM) Wang et al. (2004). SSIM is suitable here because it captures perceptual similarities rather than exact pixel values, which is crucial for understanding qualitative degradation. While pixel-by-pixel comparison may not be sensitive to perceptual changes, SSIM evaluates structural components\u2014luminance, contrast, and structure\u2014that reflect images are visually processed (by the observer). In this context, SSIM effectively measures the similarity by"}, {"title": "9 The Novel Software", "content": "Based on our extensive findings, we propose a comprehensive software architecture that ensures both privacy preservation and model explainability. The system is designed with a scrutiny check and privacy-preserving components, orchestrated to work seamlessly while maintaining both RTP and RTE guarantees.\nSystem Overview and Security Measures: At the entry point, all incoming images pass through an autoencoder (AE) that acts as the first line of security Neloy and Turgeon"}, {"title": "10 Related Work", "content": "Privacy-preserving machine learning (PPML). PPML is a critical component for RTP and includes a variety of techniques designed to safeguard individuals' data while enabling meaningful machine learning analyses. Key methods in PPML include Differential Privacy (DP) Dwork et al. (2014); Abadi et al. (2016), Federated Learning (FL) McMahan et al. (2017); Kaissis et al. (2020); Kone\u010dn\u00fd et al. (2016), Homomorphic Encryption (HE) Brand and Pradel (2023), and Secure Multi-Party Computation (SMPC) Zhou et al. (2024) to"}, {"title": "11 Limitation of the Work", "content": "In this work, we primarily restricted our study to DP models, their activation patterns and their intrinsic behaviour. We have shown that an immediate consequence of the same is the non-functionality of famous gradient-based post-hoc explainers with the DP model. There can be different notions and setups for privacy-preserving machine learning and other classes of explainers are also there we didn't consider them for this research. Also, the (un)fairness of DP models Fioretto et al. (2022) is not considered under the scope of this research."}, {"title": "12 Conclusion and Future Studies", "content": "We started our research with a simple question of whether we can achieve RTP and RTE together. We kept an on eye the pitfalls of commonly used evaluation metrics for explainability and proposed our desiderata and found that no commonly used gradient-based explainers are useful for private models. We investigate the activations inside a DP model and how the model is sensitive towards those; we discovered that the intrinsic behaviour of DP models is the key reason behind this. Our 'mechanistic' Saphra and Wiegreffe (2024) insights of the private models across different privacy guarantees highlight how DP training alters the internal representations and their sensitivities. It gives a fresh perspective on interpreting DP models and their behaviour from a nuanced angle. Lastly, we make use of LDP to achieve private explanations and conclude our study by outlining the pipeline for the industrial software for our use case that respects both RTP and RTE.\nIn future studies, we aim to go deeper into the sensitivity landscape of DP models by investigating second-order and higher-order derivatives of the model's output w.r.t. representations. While our current analysis focused on first-order gradients, examining higher-order derivatives could potentially uncover richer structural patterns in the sensitivity space that aren't immediately apparent through first-order analysis. This exploration is particularly intriguing as higher-order derivatives might capture more nuanced interactions between features and reveal how privacy constraints affect these complex relationships. Specifically, we want to analyze how the Hessian and higher-order tensors of DP models differ from their non-private counterparts, which could provide insights into more complicated behavioural patterns. Understanding these higher-order sensitivities could not only enhance our mechanistic interpretation of DP models but also potentially lead to more sophisticated XAI method(s) that capture intricate model behaviours subject"}]}