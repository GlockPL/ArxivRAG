{"title": "ON MEMORY CONSTRUCTION AND RETRIEVAL FOR PERSONALIZED CONVERSATIONAL AGENTS", "authors": ["Zhuoshi Pan", "Qianhui Wu", "Huiqiang Jiang", "Xufang Luo", "Hao Cheng", "Dongsheng Li", "Yuqing Yang", "Chin-Yew Lin", "H. Vicky Zhao", "Lili Qiu", "Jianfeng Gao"], "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as LLMLingua-2, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SECOM, a method that constructs the memory bank at segment level by introducing a conversation Segmentation model that partitions long-term conversations into topically coherent segments, while applying Compression based denoising on memory units to enhance memory retrieval. Experimental results show that SECOM exhibits a significant performance advantage over baselines on long-term conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have developed rapidly in recent years and have been widely used in conversational agents. In contrast to traditional dialogue systems, which typically focus on short conversations within specific domains (Dinan et al., 2019), LLM-powered conversational agents engage in significantly more interaction turns across a broader range of topics in open-domain conversations (Kim et al., 2023; Zhou et al., 2023). Such long-term, open-domain conversations over multiple sessions present significant challenges, as they require the system to retain past events and user preferences to deliver coherent and personalized responses (Chen et al., 2024).\nSome methods maintain context by concatenating all historical utterances or their summarized versions (LangChain Team, 2023a; Wang et al., 2023). However, these strategies can result in excessively long contexts that include irrelevant information, which may not be relevant to the user's current request. As noted by Maharana et al. (2024), LLMs struggle with understanding lengthy conversations and grasping long-range temporal and causal dynamics, particularly when the dialogues contain irrelevant information (Jiang et al., 2023c). Some other works focus on retrieving query-related conversation history to enhance response generation (Yuan et al., 2023; Alonso et al., 2024; Kim et al., 2024; Maharana et al., 2024). These approaches typically construct memory bank from the conversation history at either the turn-level (Yuan et al., 2023) or session-level (Wang et al., 2023). Xu et al. (2022), Chen et al. (2024), Li et al. (2024) and Zhong et al. (2024) further leverage summarization techniques to build memory units, which are then retrieved as context for response generation."}, {"title": "SECOM", "content": "Building on these works, a key question arises: Which level of memory granularity-turn-level, session-level, or their summarized forms-yields the highest effectiveness? Moreover, is there a novel memory structure that could outperform these three formats?\nIn this paper, we first systematically investigate the impact of different memory granularities on conversational agents within the paradigm of retrieval augmented response generation (Lewis et al., 2020; Ye et al., 2024). Our findings indicate that turn-level, session-level, and summarization-based methods all exhibit limitations in terms of the accuracy of the retrieval module as well as the semantics of the retrieved content, which ultimately lead to sub-optimal responses.\nLong conversations are naturally composed of coherent discourse units. To capture this structure, we introduce a conversation segmentation model that partitions long-term conversations into topically coherent segments, constructing the memory bank at the segment level. During response generation, we directly concatenate the retrieved segment-level memory units as the context as in Yuan et al. (2023); Kim et al. (2024), bypassing summarization to avoid the information loss that often occurs when converting dialogues into summaries (Maharana et al., 2024).\nFurthermore, inspired by the notion that natural language tends to be inherently redundant (Shannon, 1951; Jiang et al., 2023b; Pan et al., 2024), we hypothesize that such redundancy can act as noise for retrieval systems, complicating the extraction of key information (Grangier et al., 2003; Ma et al., 2021). Therefore, we propose removing such redundancy from memory units prior to retrieval by leveraging prompt compression methods such as LLMLingua-2 (Pan et al., 2024).\nOur contributions can be summarized as follows:\n\u2022 We systematically investigate the effects of memory granularity on retrieval augmented response generation in conversational agents. Our findings reveal that turn-level, session-level, and summarization-based approaches each face challenges in ensuring precise retrieval and providing a complete, relevant, and coherent context for generating accurate responses.\n\u2022 We contend that the inherent redundancy in natural language can act as noise for retrieval systems. We demonstrate that prompt compression technique, LLMLingua-2, can serve as an effective denoising method to enhance memory retrieval performance.\n\u2022 We present SECOM, a system that constructs memory bank at segment level by introducing a conversation Segmentation model, while applying COMpression based denoising on memory units to enhance memory retrieval. The experimental results show that SECOM outperforms baselines on two long-term conversation benchmark LOCOMO and Long-MT-Bench+. Further analysis and ablation studies confirm the contributions of the segment-level memory units and the compression-based denoising technique within our framework."}, {"title": "PRELIMINARY", "content": "Let $H = {c_i}_{i=1}^{C}$ represent the available conversation history between a user and an agent, which consists of C sessions. $c_i = {t_j}_{j=1}^{T_i}$ denotes the i-th session that is composed of $T_i$ sequential user-agent interaction turns, with each turn $t_{ij} = (u_j,r_j)$ consisting of a user request $u_j$ and the corresponding response from the agent $r_j$. Denote the base retrieval system as $f_R$ and the response generation model as $f_{LLM}$. The research framework here can be defined as: (1) Memory construction: construct a memory bank M using conversation history H; For a turn-level memory bank, each memory unit $m \\in M$ corresponds to an interaction turn t, with $|M| = \\sum_{i=1}^{C} T_i$. For a session-level memory bank, each memory unit m corresponds to a session c, with M = C. (2) Memory retrieval: given a target user request u* and context budget N, retrieve N memory units ${m_n \\in M}_{n=1}^{N} \\leftarrow f_R(u^*, M, N)$ that are relevant to user request u*; (3) Response generation: take the retrieved N memory units in time order as the context and query the response generation model for response $r^* = f_{LLM}(u^*, {m_n}_{n=1}^{N})$.\nIn the remainder of this section, we first elaborate the proposed conversation segmentation model that splits each session $c_i$ into $K_i$ topical segments ${s_k}_{k=1}^{K_i}$ in Section 2.2, with which we construct a session-level memory bank with each memory unit m corresponding to a segment s and $|M| = \\sum_{i=1}^{C} K_i$. In Section 2.3, we describe how to denoise memory units to enhance the accuracy of memory retrieval."}, {"title": "CONVERSATION SEGMENTATION", "content": "Zero-shot Segmentation Given a conversation session c, the conversation segmentation model $f_1$ aims to identify a set of segment indices $I = {(p_k, q_k)}_{k=1}^{K}$, where K denotes the total number of segments within the session c, $p_k$ and $q_k$ represent the indexes of the first and last interaction turns for the k-th segment $s_k$, with $p_k \\le q_k, p_{k+1} = q_k + 1$. This can be formulated as:\n$f_1(c) = {I_k}_{k=1}^{K}$, where $s_k = {t_{p_k}, t_{p_{k+1}},..., t_{q_k}}$\nHowever, building a segmentation model for open-domain conversation is challenging, primarily due to the difficulty of acquiring large amounts of annotated data. As noted by Jiang et al. (2023d), the ambiguous nature of segmentation points complicates data collection, making the task difficult even for human annotators. Consequently, we employ GPT-4 as the conversation segmentation model $f_1$ to leverage its powerful text understanding ability across various domains. To provide clearer context and facilitate reasoning, we enhance session data c by adding turn indices and role identifiers to each interaction $t_j$ as: \u201cTurn j: \\n[user]: $u_j$\\n[agent]: $r_j$\u201d. We empirically demonstrated that segmentation can also be accomplished with more light-weight models, such as Mistral-7B and even RoBERTa scale models, making our approach applicable in resource-constrained environments.\nSegmentation with Reflection on Limited Annotated Data When a small amount of conversation data with segment annotations is available, we leverage this annotated data to inject segmentation knowledge into LLMs and better align the LLM-based segmentation model with human preferences. Inspired by the prefix-tuning technique (Li & Liang, 2021) and reflection mechanism (Shinn et al., 2023; Renze & Guven, 2024), we treat the segmentation prompt as the \"prefix\" and iteratively optimize it through LLM self-reflection, ultimately obtaining a segmentation guidance G."}, {"title": "COMPRESSION BASED MEMORY DENOISING", "content": "Given a target user request u* and context budget N, the memory retrieval system $f_R$ retrieves N memory units ${m_n \\in M}_{n=1}^{N}$ from the memory bank M as the context in response to the user request u*. With the consideration that the inherent redundancy in natural language can act as noise for the retrieval system (Grangier et al., 2003; Ma et al., 2021), we denoise memory units by removing such redundancy via a prompt compression model $f_{Comp}$ before retrieval:\n${m_n \\in M}_{n=1}^{N} \\leftarrow f_R(u^*, f_{Comp}(M), N)$.\nSpecifically, we use LLMLingua-2 (Pan et al., 2024) as the denoising function $f_{Comp}$ here."}, {"title": "EXPERIMENTS", "content": "Implementation Details We use GPT-35-Turbo* for response generation in our main experiment. We also adopt Mistral-7B-Instruct-v0.3\u2020 (Jiang et al., 2023a) for robustness evaluation across different LLMs. We employ zero-shot segmentation for QA benchmarks and further incorporate the reflection mechanism for segmentation benchmarks to leverage the available annotated data. To make our method applicable in resource-constrained environments, we conduct additional experiments by using Mistral-7B-Instruct-v0.3 and a RoBERTa based model fine-tuned on SuperDialseg (Jiang et al., 2023d). Details for the conversation segmentation such as the prompt and hyper-parameters are described in Appendix A.1. We use LLMLingua-2 (Pan et al., 2024) with a compression rate of 75% and xlm-roberta-large (Conneau et al., 2020) as the base model to denoise memory units. Following Alonso et al. (2024), we apply MPNet (multi-qa-mpnet-base-dot-v1) (Song et al., 2020) with FAISS (Johnson et al., 2019) and BM25 (Amati, 2009) for memory retrieval.\nDatasets & Evaluation Metrics We evaluate SECOM and other baseline methods for long-term conversations on the following benchmarks:\n(i) LOCOMO (Maharana et al., 2024), which is the longest conversation dataset to date, with an average of 300 turns with 9K tokens per sample. For the test set, we prompt GPT-4 to generate QA pairs for each session as in Alonso et al. (2024). We also conduct evaluation on the recently released official QA pairs in Appendix A.5.\n(ii) Long-MT-Bench+, which is reconstructed from MT-Bench+ (Lu et al., 2023), where human experts are invited to expand the original questions and create long-range questions as test user requests. Since each conversation only contains an average of 13.3 dialogue turns, following Yuan et al. (2023), we merge five consecutive sessions into one long-term conversation. We also use these human-written questions as few-shot examples to prompt GPT-4 to generate a long-range test question for each dialogue topic as the test set."}, {"title": "CONCLUSION", "content": "In this paper, we systematically investigate the impact of memory granularity on retrieval-augmented response generation for long-term conversational agents. Our findings reveal the limitations of turn-level and session-level memory granularities, as well as summarization-based methods. To overcome these challenges, we introduced SECOM, a novel memory management system that constructs memory bank at the segment-level and employs compression-based denoising techniques to enhance retrieval performance. The experimental results underscore the effectiveness of SECOM in handling long-term conversations. Further analysis and ablation studies confirm the contributions of the segment-level memory units and the compression-based denoising technique within our framework."}, {"title": "DETAILS OF CONVERSATION SEGMENTATION MODEL", "content": "We use GPT-4-0125 as the backbone LLM for segmentation. The zero-shot segmentation prompt is provided in Figure 6. It instructs the segmentation model to generate all segmentation indices at once, avoiding the iterative segmentation process used in LumberChunker (Duarte et al., 2024), which can lead to unacceptable latency. We specify that the output should be in JSONL format to facilitate subsequent processing. To generate segmentation guidance, we select the top 100 poorly segmented samples with the largest Window Diff metric from the training set. The segmentation guidance consists of two parts: (1) Segmentation Rubric: Criteria items on how to make better segmentation. (2) Representative Examples: The most representative examples that include the ground-truth segmentation, the model's prediction, and the reflection on the model's errors. The number of rubric items is set to 10. To meet this requirement, we divide the top 100 poorly segmented samples into 10 mini-batches and prompt the LLM-based segmentation model to reflect on each batch individually. The segmentation model is also asked to select the most representative example in each batch, which is done concurrently with rubric generation. Figure 8 presents the prompt used to generate rubric. The generated rubric is shown at Fig. 9 and Fig. 10 on TIAGE and SuperDialSeg, respectively. After the segmentation guidance is learned, we utilize the prompt shown in Figure 7 as a few-shot segmentation prompt. For simplicity and fair comparison, we do not use any rubric for conversation segmentation in LOCOMO and Long-MT-Bench+."}, {"title": "ADDITIONAL COST ANALYSIS", "content": "Table 5 compares the overall costs involved in memory construction, memory retrieval, and response generation across different methods. The results demonstrate that our method significantly enhances performance compared to the baseline while only slightly increasing computational overhead, and it outperforms the MemoChat method in both efficiency and effectiveness."}, {"title": "THE ANALOGY BETWEEN THE REFLECTION AUGMENTATION AND PREFIX-TUNING", "content": "When a small amount of conversation data with segment annotations is available, we explore how to leverage this data to transfer segmentation knowledge and better align the LLM-based segmentation model with human preferences. Inspired by the prefix-tuning technique (Li & Liang, 2021) and reflection mechanism (Shinn et al., 2023; Renze & Guven, 2024), we treat the segmentation prompt as the \"prefix\" and iteratively optimize it through LLM self-reflection, ultimately obtaining a segmentation guidance G."}, {"title": "DETAILS OF DATASET CONSTRUCTION", "content": "(i) LOCOMO (Maharana et al., 2024): this dataset contains the longest conversations to date, with an average of more than 9K tokens per sample. Since LOCOMO does not release the corresponding question-answer pairs when we conduct our experiment, we prompt GPT-4 to generate QA pairs for each session as in Alonso et al. (2024). We also conduct evaluation on the recently released official QA pairs in Appendix A.5.\n(ii) Long-MT-Bench+: Long-MT-Bench+ is reconstructed from the MT-Bench+ (Lu et al., 2023) dataset. In MT-Bench+, human experts are invited to expand the original questions and create long-range questions as test samples. However, there are two drawbacks when using this dataset to evaluate the memory mechanism of conversational agents: (1) the number of QA pairs is relatively small, with only 54 human-written long-range questions; and (2) the conversation length is not sufficiently long, with each conversation containing an average of 13.3 dialogue turns and a maximum of 16 turns. In contrast, the conversation in LOCOMO has an average of 300 turns and 9K tokens. To address (1), we use these human-written questions as few-shot examples and ask GPT-4 to generate a long-range test question for each dialogue topic. For (2), following (Yuan et al., 2023), we merge five consecutive sessions into one, forming longer dialogues that are more suitable for evaluating memory in long-term conversation. We refer to the reconstructed dataset as Long-MT-Bench+ and present its statistics in Table 7."}, {"title": "DETAILS OF RETRIEVAL PERFORMANCE MEASUREMENT", "content": "We measure the retrieval performance in terms of the discounted cumulative gain (DCG) metric (J\u00e4rvelin & Kek\u00e4l\u00e4inen, 2002):\n$DCG = \\sum_{i=1}^{p} \\frac{rel_i}{log_2(i + 1)},$\nwhere $rel_i$ denotes the relevance score of the retrieved user-agent turn ranked at position i, and p represents the total number of retrieved turns. Note that in the Long-MT-Bench+ dataset, answering a single question often requires referring to several consecutive turns. Therefore, we distribute the relevance score evenly across these relevant turns and set the relevance score of irrelevant turns to zero. For instance, assume that the ground truth reference turn set for question q is $R(q) = {r_{k+j}}_{j=1}^{N}$, which is provided by the dataset. In this case, the relevance score for each turn is set as follows:\n$rel_i = \\begin{cases} 1 & i < k+1 \\ \\frac{1}{N} & k+1 < i < k+N. \\ 0 & i>k+N \\\\end{cases}$"}, {"title": "ADDITIONAL EXPERIMENTS ON COQA AND PERSONA-CHAT", "content": "To further validate SeCom's robustness and versatility across a broader range of dialogue types, we conduct additional experiments on other benchmarks, Persona-Chat (Zhang et al., 2018) and CoQA (Reddy et al., 2019).\nGiven the relatively short context length of individual samples in these datasets, we adopt an approach similar to Long-MT-Bench+ by aggregating multiple adjacent samples into a single instance. For CoQA, each sample is supplemented with the text passages of its 10 surrounding samples. Since CoQA answers are derived from text passages rather than dialogue turns, we replace the turn-level"}]}