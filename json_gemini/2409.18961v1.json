{"title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation", "authors": ["Dylan Li", "Gyungin Shin"], "abstract": "Unsupervised instance segmentation aims to segment distinct object instances in an image without relying on human-labeled data. This field has recently seen significant advancements, partly due to the strong local correspondences afforded by rich visual feature representations from self-supervised models (e.g., DINO). Recent state-of-the-art approaches use self-supervised features to represent images as graphs and solve a generalized eigenvalue system (i.e., normalized-cut) to generate foreground masks. While effective, this strategy is limited by its attendant computational demands, leading to slow inference speeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages self-supervised visual features to obtain initial groupings of patches and applies a strategic merging to these segments, aided by a sophisticated background-based mask pruning technique. ProMerge not only yields competitive results but also offers a significant reduction in inference time compared to state-of-the-art normalized-cut-based approaches. Furthermore, when training an object detector using our mask predictions as pseudo-labels, the resulting detector surpasses the current leading unsupervised model on various challenging instance segmentation benchmarks.", "sections": [{"title": "Introduction", "content": "Instance segmentation identifies and delineates each distinct object within an image, providing both its class and precise pixel-wise location. This capability is crucial for a wide range of applications, from autonomous driving systems [8] that must navigate complex environments to medical imaging technologies that require accurate tumor segmentation [1,20]. However, the cost of manually annotating dense masks for training data is prohibitively high, especially for domains such as medical imaging that require deep expertise.\nTo overcome the challenges with dense annotations, multiple endeavors have attempted to tackle category-agnostic instance segmentation in an unsupervised manner."}, {"title": "Related work", "content": "Our work is connected to three themes in the literature, including self-supervised visual representation learning, unsupervised single object detection/segmentation and unsupervised instance segmentation."}, {"title": "Self-supervised visual representation learning", "content": "Self-supervised learning in computer vision has advanced significantly by adopting the principle of learning from the intrinsic structure of data, drawing inspiration from how language models such as GPT [2] and BERT [11] achieve semantic understanding from text. One strategy in self-supervised learning involves leveraging pretext tasks, such as those employed by Masked Autoencoders [16], wherein models learn by predicting the obscured parts of an image. Another set of strategies, embodied by SwAV [4], MoCo [6,7,17], and DINO [5,9,25], uses data augmentations to generate varied perspectives of images and aligns feature representations of these perspectives. Among these self-supervised paradigms for training encoders, DINO in particular encodes detailed segmentation information, a capability diminished in models trained with supervised labels [5,30]. In this work, by leveraging DINO's inherent grouping ability, we propose a simple yet effective approach to instance segmentation without explicit labels."}, {"title": "Unsupervised single object detection and segmentation", "content": "Unsupervised object detection and segmentation aim to localize a single, dominant object in an image in an unsupervised manner with a bounding box or a segmentation mask, respectively.\nLOST [31] extracts features from a self-supervised network and isolates the foreground by first identifying the seed patch with the lowest count of positive correlations with other patches. A seed expansion strategy is then used to include additional patches that correlate positively with the original seed patches.\nAnother line of work uses normalized-cut-based methods [24,30,36] to distinguish the foreground from the background. This class of methods uses the eigendecomposition of the Laplacian matrix derived from a feature affinity matrix constructed with self-supervised features. The resulting eigenvectors, processed with traditional clustering or thresholding methods, can be translated into meaningful segmentations in pixel space."}, {"title": "Unsupervised instance segmentation", "content": "Unsupervised instance segmentation aims to identify multiple objects in an image without human labels, introducing a more complex challenge than the aforementioned single object detection and segmentation. An early attempt [32] generates a single salient mask per image, and trains the Mask R-CNN detector [18] using the generated masks as pseudo-labels. However, the single mask generation approach does not provide sufficient mask instances per image, resulting in suboptimal detection performance.\nRecent approaches [34, 35], on the other hand, propose methods for generating multiple instance masks per image, that are used to train a general object detector. MaskCut [35] has demonstrated promising outcomes by iteratively applying a normalized-cut-based single object segmentation technique (i.e., TokenCut [36]) to images and training a robust object detector with pseudo-labels generated through the MaskCut algorithm. Despite its effectiveness, MaskCut uses repeated eigenvalue system resolutions for the normalized cut, incurring significant computational demands. Additionally, MaskCut limits itself to three segmentations per image. In contrast, ProMerge eschews computationally intensive eigenvalue calculations in favor of using raw feature affinities and does not impose a restriction on the number of segmentations per image. By doing so, it offers a more computationally efficient alternative for the instance segmentation task while achieving higher precision and recall."}, {"title": "Method", "content": "In this section, we first describe the problem scenario (Sec. 3.1) and introduce ProMerge, a simple yet effective prompt-and-merge method to tackle unsupervised instance segmentation (Sec. 3.2). We then describe a background-based mask pruning strategy to extract foreground masks from a set of prompted masks that increases the effectiveness of prompting and merging (Sec. 3.3). We finally describe a pseudo-label training scheme in which an object detector is trained with pseudo-labels generated by ProMerge (Sec. 3.4)."}, {"title": "Problem formulation", "content": "We address the challenging task of instance segmentation in an unsupervised manner. Given an image $I \\in \\mathbb{R}^{3\\times H\\times W}$, where $H$ and $W$ denote the height and width of the image, we aim to produce a set of $N$ instance binary masks"}, {"title": "ProMerge: Prompt and Merge", "content": "Point-prompting visual features. Our approach begins with generating preliminary mask proposals. We use visual features, denoted as $F = \\{f_{ij} \\in \\mathbb{R}^c| i = 1,..., h, j = 1,...,w\\}$, that are obtained by feeding an image into an image encoder (e.g., patch tokens for Vision Transformers (ViT) [12]). Here, $c$, $h$, and $w$ represent the channel, height, and width dimensions of the features. To generate initial masks from the visual features (i.e., patch tokens), we consider the technique of point-prompting, wherein a 2D grid of $K$ equally spaced patch tokens is selected as seeds for mask generation. This subset of the selected tokens $P = \\{p_l \\in \\mathbb{R}^C|l = 1, ..., K\\}$, which we call the set of prompt tokens, is individually compared with all of the patch tokens in the image. By comparing each prompt token in a one-to-all manner via a similarity measure, we generate an affinity matrix $A_l \\in [-1,1]^{h\\times w}$ for each prompt token $p_l$. In this paper, we use key features from the last attention layer of a self-supervised image encoder (i.e., DINO) as patch tokens [35] and compute the cosine similarity between them. That is,\n$A_l = (A_{l;ij}) = \\frac{p_l \\cdot f_{ij}}{||p_l||_2||f_{ij}||_2}$ (1)\nwhere $||.||_2$ denotes L2 norm. We then apply a bipartition threshold, $\\tau_b$, to the affinity matrix to obtain a binary mask $M_l$.\nMerging prompted masks. Given the prompted masks above, we consider an iterative clustering method, wherein masks, sorted by area in descending order, are sequentially merged with a set of larger masks processed at the previous iterations. The processed masks serve as bases for merging smaller masks that are introduced in later iterations.\nIn comparing a new, smaller mask with a previously processed, larger mask, we consider two straightforward conditions. First, we use the Intersection-over-Area (IoA) metric to determine if the smaller mask should be merged with the larger mask. If the ratio of the intersection area between two masks to the"}, {"title": "Background-based mask pruning", "content": "While the prompt-and-merge framework thus far is intuitive, we observe that it suffers from poor performance in isolation, due to the noisy background masks among the prompted masks. We introduce a mask pruning strategy based on background prediction between the prompting and merging steps. We rely on a two-step process that (i) groups the initial $K$ masks into the foreground or background, and aggregates the background masks via pixel-wise voting to produce a single, fine background mask for the image and (ii) uses the predicted background mask to filter out noisy foreground masks from the initial mask proposals. Each step in the mask pruning strategy is detailed below.\nBackground aggregation. Recall that after prompting the visual features with $K$ equally spaced points in a 2D grid, we obtain $K$ binary masks. We then classify each of the masks as either a foreground or background candidate. We employ a simple heuristic: a background mask is likely to contain a majority of pixels along the edge for at least two of the edges of the image. Specifically, we consider a mask as background if more than one of its sides contains a number of positive pixels that exceeds half the length of that side. Then, we create a single representative background mask for the image by applying a pixel-wise voting scheme to the background candidates. Formally, given the set of background candidate masks $B = \\{M_l^{B}|l=1, ..., L^{B}\\}$, a pixel value at $(i, j)$ of the aggregated background mask $M^{bg}$ is determined with:\n$M^{bg}_{ij} = \\frac{\\sum_{l=1}^{|B|} [M_{l;ij}^{B} > 0.5]}{|B|}$ (2)\nwhere the $[.]$ operator is the indicator function, which returns 1 if the condition within the operator is satisfied, and 0 otherwise. The condition in the operator"}, {"title": "ProMerge+: Training an object detector with pseudo-labels from ProMerge", "content": "Following [35], we train an object detector on ProMerge predictions generated from inference on images in a large-scale image dataset (i.e., ImageNet2012 [10]). The purpose of this pseudo-label training is two-fold: firstly, to obtain an object detector with better performance by learning from the noisy pseudo-labels; and secondly, to assess the detector's ability to generalize across different data distributions by training on images from one dataset (e.g., ImageNet2012) and evaluating on another (e.g., SA-1B [21]). The trained detector, ProMerge+, surpasses performance and zero-shot transfer capabilities of the current leading model."}, {"title": "Experiments", "content": "In this section, we first provide implementation details (Sec. 4.1) and compare our method with the state-of-the-art-methods (Sec. 4.2). Then, we provide extensive ablation study to analyze our approach (Sec. 4.3)."}, {"title": "Implementation details", "content": "Our implementation is based on the PyTorch [26] and Detectron2 [37] libraries, and A100 GPUs are used for our experiments unless otherwise stated.\nDatasets. We evaluate our ProMerge on six benchmarks including COCO2017 [23], COCO-20K [33], LVIS [15], KITTI [13], subsets of Objects365 [28] and SA-1B [21] (44K and 11K images, respectively). Among these, SA-1B is the most challenging benchmark due to the densely-annotated fine-grained masks, with an average of 101 segmentation masks per image. To train ProMerge+, we use unlabeled ImageNet2012 training images [10] (1.2M images) and evaluate the resulting model on the six benchmarks in a zero-shot manner (i.e., the model is not trained with images sharing the same data distribution as evaluation data). For the ablation study, we use COCO2017 following [35].\nEvaluation metrics. We evaluate our methods based on average precision (AP) and recall (AR), the standard metrics for the instance segmentation task.\nInference of ProMerge. We follow the previous work [35] for our inference setting. Specifically, we use DINO [5] with the ViT-B/8 architecture [12] as an image encoder and input images are resized to 480\u00d7480 pixels before being fed into the encoder. We apply Conditional Random Field (CRF) [22] to an output mask for post-processing. Additionally, we split connected components of initial masks before merging, which we find beneficial in terms of both AP and AR (shown in Sec. 4.3).\nPseudo-label training. When we train an object detector with pseudo-masks generated by ProMerge, we use the Cascade Mask R-CNN model [3] with the ResNet50 backbone [19] initialized with DINO features [5]. We train the detector for 160K iterations using the SGD optimizer with a batch size of 16, a momentum of 0.9, and a learning rate of 0.005, which is decreased by a factor of 5 during training."}, {"title": "Main results", "content": "In this section, we compare our methods to the state-of-the-art methods with both standard evaluation metrics and inference speed.\nComparison to state-of-the-art methods. We first compare ProMerge to training-free methods including TokenCut [36] and MaskCut [35] algorithms. The overall higher recall of ProMerge is attributed to its flexibility in not requiring a"}, {"title": "Ablation study", "content": "Here, we conduct an extensive ablation study to analyze the effect of each component in ProMerge. Specifically, we explore the effects of pixel-wise voting for background aggregation, foreground filtering methods, different merging conditions, and varied hyperparameter choices.\nEffect of each component. We identify major components that affect the performance of our approach: (i) prompting and merging; (ii) background-based mask pruning (denoted as Mask Pruning); and (iii) connected component splitting (denoted as CC Splitting). Notably, a naive approach that relies solely on prompting and merging suffers from poor performance, with 0.4 $AP^{mk}_{100}$ and 1.0 $AR^{mk}_{100}$, which are worse than a prompting-only approach. However, adding background-based mask pruning greatly boosts both APmk and ARmk by 1.7% and 3.7%. CC-splitting further increases $AP^{mk}_{100}$ by 0.3% and $AR^{mk}_{100}$ by 2.8%. These results demonstrate that though the core of prompt-and-merge is straightforward, the competitive performance of our approach is facilitated by incorporating sophisticated components such as background mask pruning.\nEffect of pixel-wise voting. In the background-based mask pruning process, we use a pixel-wise voting strategy to aggregate background candidate masks to produce a single, representative background mask for a given image. We consider the case where we substitute voting with a simpler non-voting mechanism, and compare it with the voting based mechanism in ProMerge. In the non-voting experiment, we sum up all background candidate masks. If a pixel at location (i, j) has at least one mask with a value of one (i.e., positive pixel), it is regarded"}]}