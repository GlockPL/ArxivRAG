{"title": "Large Language Models are Zero-Shot Recognizers for Activities of Daily Living", "authors": ["GABRIELE CIVITARESE", "MICHELE FIORI", "PRIYANKAR CHOUDHARY", "CLAUDIO BETTINI"], "abstract": "The sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADLs recognition is typically based on deep learning methods requiring large datasets to be trained. Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADLs recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADLs recognition system. ADL-LLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADLs recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.", "sections": [{"title": "1 Introduction", "content": "Sensor-based Human Activity Recognition (HAR) is a well-known research topic that has been extensively studied by several research groups [14]. The HAR task involves determining the most likely activity performed by a subject through the intelligent analysis of sensor data collected from one or multiple devices (e.g., wearables, environmental, radio-based). The adopted sensing technologies influence the granularity at which activities can be recognized. For instance, by using inertial sensors in mobile/wearable devices, it is possible to detect physical activities like running, walking, and standing still. On the other hand, to recognize high-level Activities of Daily Living (ADLs) like cooking, taking medicines, or watering plants in smart-home environments, environmental sensors are particularly useful, also considering that subjects at home are less inclined to carry or wear devices all the time.\nAmong the many applications of sensor-based ADL recognition in the areas of energy management, safety, well-being, and healthcare, the long-term monitoring of the ADLs performed by elderly subjects in their homes gained significant importance [38]. The continuous advances in the medical domain significantly extended the world population's life expectancy [13]. While this is surely an outstanding achievement, aging also brings several challenges to society. Among the aging-related issues, cognitive decline is one of the greatest health threats of old age, with nearly 50% of adults over the age of 85 afflicted with the Alzheimer's disease [10]. Detecting the early symptoms of cognitive decline is crucial to implement primary or secondary preventive therapeutic strategies [27]. One of the main markers used by clinicians to assess the cognitive status of a subject is represented by ADLs, whose impairment (i.e., difficulty or inability to perform them independently) is correlated to a rapid functional decline and a higher likelihood of progression to dementia [29]. However, the subjects at risk usually have only a few sporadic visits with medical experts (e.g., neurologists, neuropsychologists, and geriatricians). Such visits often fail to accurately diagnose cognitive decline, especially when it may be revealed by subtle behavioral changes. In-deed, besides a few instrumental tests, and neuro-psychological evaluations at the time of visit, the clinicians mainly rely on possibly inaccurate reports provided by the subjects themselves or their caregivers. It is hence of great value to implement smart telemedicine strategies to continuously and unobtrusively monitor the subjects at risk while keeping them in the comfort of their homes.\nIn the last decades, several research groups proposed the use of unobtrusive sensing infrastruc-tures in smart homes to continuously recognize ADLs in smart homes, to identify digital markers that may indicate the onset of cognitive decline [21, 31, 39]. For the sake of privacy, most of the existing solutions do not consider cameras or microphones (that may not be accepted in home environments), but they are usually based on environmental sensors monitoring the interaction of the subject with the home infrastructure (e.g., motion sensors, door sensors, pressure sensors).\nHowever, the majority of the proposed approaches to automatically detect ADLs from sensor data require training deep learning models [14], thus necessitating the collection of large datasets in the home environment (possibly labeled with the ground truth reporting what activity is being performed). Labeled data collection in these settings is privacy-intrusive, costly, and prohibitive [7]. Another well-known line of research in this domain considers the adoption of purely knowledge-based approaches [15]. The rationale behind these methods is that common-sense knowledge about human activities (e.g., a person usually prepares their meals in the kitchen, turns on the stove, and uses cooking instruments) can be modeled with a set of rules. Knowledge-based ADLs recognition is performed by mapping sensor data windows to semantic concepts and performing rule-based reasoning. The advantage of this strategy is that it eliminates the problem of collecting labeled data in smart environments. However, the design and implementation of knowledge models require the expertise of domain experts and knowledge engineers, and their capability of handling uncertainty and generalizing is typically limited [16].\nThe recent success of Large Language Models (LLMs) paved the way to LLM-based human activity recognition. Preliminary results in the state-of-the-art indicate that LLMs implicitly encode the knowledge about human activities, and hence they can potentially be used to cope with labeled data scarcity [30, 34]. Given these promising results, we hypothesize that LLMs could be also leveraged to process data from environmental sensors in smart-home environments to reveal the ADLs performed by the monitored subjects, without the need to collect any labeled data. We expect that LLMs would generalize across individuals and environments without requiring any patient- or environment-specific training data, thus offering a scalable approach. Moreover, an LLM-based approach would be more flexible in terms of the set of recognizable ADLs (that is usually fixed in supervised approaches).\nIn this work, we propose ADL-LLM, a method leveraging pre-trained LLMs to recognize ADLs in smart-home environments. At its core, ADL-LLM transforms raw sensor data into natural language sentences describing the high-level events observed in the home. ADL-LLM uses a specifically"}, {"title": "2 Related Work", "content": "The labeled data scarcity problem in Human Activity Recognition (HAR) is one of the major challenges limiting large-scale real deployments, and several research groups have been working on this problem for several years. Three categories of approaches have been proposed to mitigate labeled data scarcity: data-driven, knowledge-based, and neuro-symbolic."}, {"title": "2.1 Data Scarcity in Human Activity Recognition", "content": "The majority of the existing approaches to mitigate labeled data scarcity rely on deep learning models. The most common strategies are semi-supervised learning [1], transfer learning [18], and self-supervised learning [23]. Semi-supervised approaches use small labeled datasets to initialize the model, which is incrementally updated using pseudo-labels inferred on the unlabeled data stream [1]. Examples of semi-supervised methods proposed for HAR are self-learning, co-learning, active learning, and label propagation. On the other hand, transfer learning methods leverage models that are pre-trained on a source domain with a significant amount of labeled data, and then adapted in a target domain with labeled data scarcity [40, 41]. Self-supervised learning is a specific transfer learning approach, where the model is pre-trained on massive unlabeled datasets to obtain a reliable feature extractor [23, 24, 28]. The pre-trained feature extractor is then fine-tuned only using a limited amount of labeled data on the target domain. Existing approaches for self-supervised learning focuses on data from wearable/mobile devices, since this domain offers offers large datasets (e.g., Biobank [48], Capture-24 [12]) that can be used for model pre-training. To the best of our knowledge, we are not aware of self-supervised learning approaches for smart-home ADLs recognition, due to the fact that in this domain there are only a few small datasets, and learning a comprehensive representation that accommodates the diverse layouts of different homes is still an open challenge.\nADL-LLM can be considered as a sort of zero-shot transfer learning approach, where a model pre-trained on language is used to recognize activities from a textual representation of sensor data."}, {"title": "2.1.2 Knowledge-Based Methods", "content": "Several research groups investigated whether common-sense knowledge about human activities could be leveraged to completely avoid labeled data collection. These approaches proposed the use of logic formalisms (e.g., ontologies) to model the relationships between sensor events and activities [15, 16]. However, building such logic models requires skills in the formalism model (e.g., OWL2 in the case of ontologies) and significant domain expertise. This task is usually performed by small groups of persons. Building a comprehensive knowledge"}, {"title": "2.1.3 Neuro-Symbolic Methods", "content": "Neuro-Symbolic AI (NeSy) methods combine the best from data-driven and knowledge-based worlds. Indeed, the goal of NeSy models is to combine symbolic reasoning and machine learning to mitigate data scarcity and improve the model's interpretabil-ity [25]. Considering the HAR domain, logic formalisms have been used to refine the output of the data-driven model [8] or to infuse knowledge inside deep learning models [6, 47]. In these methods knowledge constraints allow the model to capture high-level patterns without learning them from data, thus mitigating labeled data scarcity. However, NeSy methods have been only proposed for wearable-based HAR and their applicability to smart home ADLs recognition has not been explored yet.\nFor certain aspects, the capabilities of LLMs can be considered as Neuro-Symbolic since they can implicitly mimic reasoning [19]. Hence, ADL-LLM could be considered as a NeSy model whose data-driven component attempts to mimic symbolic reasoning."}, {"title": "2.2 Using LLMs for Human Activity Recognition", "content": "In the following, we present relevant research works using LLMs in the HAR domain, distinguishing supervised and zero-shot techniques. Supervised methods combines LLMs and labeled HAR datasets for specific tasks (e.g., prediction, classification), while zero-shot techniques directly utilize pre-trained LLMs to identify activities on unlabeled sensor data."}, {"title": "2.2.1 LLM-based approaches for supervised activity recognition", "content": "Several research groups studied how to use LLMs for the sensor-based HAR domain. The most common approach is to fine-tune LLMs with labeled sensor datasets, in order to specialize them on a specific domain [34, 35]. Other works adopted multi-modal models to align sensor data embeddings and text embeddings leveraging contrastive learning to improve the recognition rate [50]. In Neuro-Symbolic HAR, some works used LLMs to infuse common-sense knowledge about human activities into a deep learning model [5]. LLMs has also been leveraged to transform sensor data windows first into sentences in natural language, and then into embeddings to improve the recognition rate and generalization capabilities of supervised models [43]. Finally, since LLMs are pre-trained to predict the next word in a sentence, some research groups fine-tuned them also to predict future sensor events [22, 42]."}, {"title": "2.2.2 Zero-shot approaches for human activity recognition using LLMs", "content": "Similarly to ADL-LLM, other works aim at recognizing human activities using LLMs without any training phase. For instance, some works show that LLMs can reason on raw inertial sensor data from mobile/wearable devies to distinguish between a limited set of low-level physical activities [26, 30]. Another line of research leverages pre-trained model generating 3D motion sequences from text [49] to generate synthetic labeled data from wearable/mobile devices [32]. Considering smart environments, ChatGPT has been used for zero-shot activity recognition [46]. However, this work assumes the availability of several sensors capturing the interaction of the subject with many household items (e.g., specific food ingredients), that is unrealistic in real-world deployments. In this work, we consider a more realistic setting considering the sensors that are usually deployed in smart-homes for ADLs recognition (e.g., motion sensors, magnetic sensors, plug sensors). Finally, another work uses LLMs to provide"}, {"title": "3 The ADL-LLM framework", "content": "In this section, we present ADL-LLM: our LLM-based method for ADLs recognition in smart home environments."}, {"title": "3.1 Problem Formulation", "content": "We consider a classic problem formulation for ADL recognition. A smart home is equipped with a set S = {S1, S2, . . ., Sm} of environmental sensors. Examples are magnetic sensors revealing open-close events (e.g., doors, cabinets, etc.), motion/presence sensors revealing movement/presence in specific areas, and smart plugs revealing the use of appliances. There is a set of target activities of interest A = {A1, A2, . . ., An }, usually suggested by clinicians. As in most works in this domain, we consider a smart home environment inhabited by a single subject or with a sensing infrastructure able to properly associate a sensor's activation with the subject that activated it. Based on the interaction of the subject with the environment, the smart home generates a continuous stream of sensor activations and de-activations. This stream is partitioned in fixed-time windows of \u03c4 seconds with a given overlapping factor o. Given a window wt, the goal of ADL-LLM is to determine the activity A \u2208 A that the subject was more likely performing in the interval [t, t + \u03c4]."}, {"title": "3.2 Overall architecture of ADL-LLM", "content": "We assume that ADL-LLM runs on a trusted platform. In the case of the medical application, it may run on a telemedicine platform as one of the components of a complex system that continuously monitors subjects at risk of cognitive decline. The smart home generates a continuous stream of raw sensor data that is pre-processed by the SENSOR STATE GENERATION module to obtain a stream of sensor states. Intuitively, a sensor state represents the timespan in which a sensor is active (e.g., \u201cthe TV in the living room is ON from 12:34 to 13:02\u201d). Since LLMs are specialized in natural language text, the main idea of ADL-LLM is to generate a natural language representation for each window, so that an LLM can \u201creason\u201d on it leveraging its implicit common-sense knowledge. The LLM receives as input a SYSTEM PROMPT (i.e., instructions about the task it has to perform) and a USER PROMPT (i.e., the textual representation of the window enriched with additional instructions for the LLM by the USER PROMPT CONSTRUCTION module). The output of the LLM is finally processed by the ACTIVITY LABEL EXTRACTION module to obtain the most likely activity performed by the subject.\nADL-LLM works in two different modalities:\n\u2022 Zero-Shot, where no labeled data is available\n\u2022 Few-Shot, where a small labeled dataset is available"}, {"title": "3.3 The main components of ADL-LLM", "content": "In the following, we describe the main modules of ADL-LLM, i.e., those used both in the zero-shot and the few-shot modalities."}, {"title": "3.3.1 Sensor States Generation", "content": "The smart home generates a continuous stream of binary sensor events \\(Stream = (..., E_{t-1}^{S_m}, E_{t}^{S_m}, E_{t+1}^{S_m}, ...)\\). Each \\(E_t^S\\) represents the activation or the deactivation of a sensor S at a specific time t. For instance, consider a magnetic sensor on the fridge. This sensor generates an ON event when activated (when the subject opens the fridge) and an OFF event when deactivated (when the subject closes the fridge). Other sensors share a similar behavior, like motion and pressure sensors. For sensors that generate continuous values, as the smart plug that continuously detect the power consumption of specific appliances, we use a threshold on the detected energy consumption to infer when the appliance is turned ON and OFF. For sensors that provide almost continuous values but are not directly associated with activation and deactivation events, like the temperature sensor, we discretize the values in different relevant ranges (e.g., for"}, {"title": "3.3.2 Segmentation", "content": "ADL-LLM employs a fixed-time segmentation of the stream of sensor states, considering windows of \u03c4 seconds and an overlap factor o. Intuitively, each window is associated with all the sensor states whose interval intersects with the window interval. More formally, a window wt over the interval [t,t + \u03c4] is associated with the sensor states st(\u03c3i, ts, te) such that ts \u2264 t + \u03c4 and te \u2265 t.\nNote that a window may be associated with multiple states even derived from different sensors. Based on the intervals we distinguish three categories that will be useful for text generation:\n\u2022 Inner states: this is the case of ts > t and te \u2264 t + \u03c4. This represents the simple case of the state occurring completely within the window.\n\u2022 Already active states: this is the case of ts < t. These states are already active before the start of the window.\n\u2022 Persistent states: this is the case of te > t. These states persist after the end of the window.\nNote that a state may be both Already active and Persistent when ts < t and te > t."}, {"title": "3.3.3 Window2Text", "content": "The WINDOW2TEXT module converts a sensor state window wt into its corresponding textual representation T(wt). Intuitively T(wt) describes what happened in the home from the subject's perspective.\nIn order to do this, the sensor states associated with a window are mapped to a description in natural language of what the subject may have done to lead to those states."}, {"title": "3.3.4 System prompt", "content": "The system prompt is the set of instructions defining the general task assigned to the LLM and is independent of the specific window being considered. Our prompt first asks the LLM to act as an activity recognition system. Then, it provides information about the rooms in the smart home, the main interactions with the environment that can be captured by the available sensors, and the list of activities of interest. Since this information depends on the specific"}, {"title": "3.3.5 User Prompt Construction", "content": "The user prompt contains the information about the specific window wt for which the LLM has to predict the most likely activity. The user prompt contains:\n\u2022 The textual representation T(wt) of the window obtained with WINDOW2TEXT (described in Section 3.3.3).\n\u2022 Instructions about selecting only one activity among the target ones.\n\u2022 The sentence \u201cReason step by step\u201d, used to instruct the LLM to perform intermediate \u201creasoning\u201d steps before providing the predicted ADL. This technique is well-known as Chain Of Thought (COT), it usually leads to increasing the correctness of the LLM's output [45], and proved to be useful even in our system."}, {"title": "3.3.6 LLM-based ADLs Recognition and Activity Label Extraction", "content": "The system prompt and the user prompt are provided as input to the LLM for each window wt, and the LLM provides a textual description as output, reporting some facts supporting the selection of the predicted activity. Finally, the ACTIVITY LABEL EXTRACTION module uses regular expressions to extract only the activity label from the LLM's output. In rare cases, despite the instructions, the LLM provides an activity label that does not syntactically match one the candidate activities in A. For instance, instead of snacking, in a few cases the LLM returned as output preparing late night snack or preparing midnight snack. In these cases, the ACTIVITY Label EXTRACTION module uses the cosine similarity to compare the text embeddings of the labels. The label with the highest similarity to the predicted output is selected as the final prediction."}, {"title": "3.4 Few-shot ADL-LLM components", "content": "When a small dataset of labeled sensor data is available, ADL-LLM can be improved by implementing few-shot prompting. This is achieved by extending the zero-shot architecture with two components described in the following."}, {"title": "3.4.1 Building a Pool of Examples", "content": "Suppose that a small labeled dataset D is available. Let D be a set of pairs (wt, A) where wt is a window of raw sensor data, while A is the corresponding activity"}, {"title": "3.4.2 Semantic-based Example Selection", "content": "Given a textual representation T(wt) of a window wt currently processed by ADL-LLM, the corresponding embedding \u03c6wt = \u03a6(T(wt)) is computed, where \u03a6 is a function (e.g., a pre-trained LLM) mapping sentences into a latent space representation (i.e., their embedding). Then, for each pair (T(w), A) \u2208 E, we compute the pairwise cosine similarity between \u03c6wt and \u03c6w = \u03a6(T(w)). We enrich the system prompt with the k pairs (T(w), A) \u2208 E that are associated with the highest pairwise similarity between \u03c6wt and \u03c6w. The pseudo-code for SEMANTIC-BASED Example SELECTION is summarized in Algorithm 1."}, {"title": "4 Experimental Evaluation", "content": "We evaluated ADL-LLM using two public datasets: MARBLE [4] and UCI ADL [36]. We chose these datasets because they include sensor data from a wide variety of sensors (e.g., magnetic, plug, motion) that we are currently considering on a project involving data collection in the homes of subjects with Mild Cognitive Impairment (MCI). While the majority of works in the literature utilize the well-known CASAS datasets [17], which primarily include data from motion sensors,"}, {"title": "4.1 Datasets", "content": "The MARBLE dataset [4] includes data collected from 12 subjects in both single and multi-inhabitant scenarios. The dataset was collected using the following environmental sensors: magnetic sensors to capture the opening and closing of various drawers and cabinets, pressure sensors to detect when a person was sitting at the dining table or on the sofa in the living room, and smart plugs to monitor the usage of the stove and television. Additionally, the dataset includes data from an Android application on the user's smartphone, which automatically detects incoming and outgoing phone calls. Finally, the dataset incorporates data from a smartwatch worn by the user, providing micro-localization information (based on an infrastructure using Bluetooth Low Energy (BLE) beacons) at the room-level.\nWe performed our experiments only considering the single-inhabitant scenarios. Moreover, while the dataset also included inertial sensor data from the smartwatch (e.g., continuous acceleration on three axis), we ignore these type of data as it can not be directly transformed into a prompt in natural language.\nThe dataset includes the following 13 activities: answering phone, clearing table, preparing hot meal, preparing cold meal, eating, entering home, leaving home, making phone call, setting up table, taking medicines, using PC, washing dishes, and watching TV.\nIn our experiments we do not consider the washing dishes activity since it is mainly characterized by inertial sensors in smartwatch data (not used by ADL-LLM). Moreover, we merged the making phone call and answering phone call activities into a single activity called phone call."}, {"title": "4.1.2 UCI ADL", "content": "The UCI ADL dataset [36] includes data about the ADLs performed by two subjects in their own homes. The two homes are named Home A and Home B in the dataset. The overall data collection duration is 35 days: 14 days for Home A and 21 days for Home B.\nDifferent environmental sensors are considered: PIR sensors (installed on doors, bathroom sink, stove, and shower areas) to detect the subject's movements, pressure sensors to detect when the subject was sitting on the couch and the bed, magnetic sensors to detect when the subject was opening and closing cupboards and cabinets, and plug sensors sensors to detect the usage of microwave and toaster.\nHome A and Home B sensing infrastructure have some differences. Compared to Home A, Home B has a slightly more limited sensing infrastructure since it is not equipped with the PIR sensor over the stove, the plug sensor to detect the usage of the toaster, and the magnetic sensor to detect interactions with the bathroom cabinet. Nonetheless, with respect to Home A, Home B includes additional PIR sensors installed on the kitchen, bedroom and living room doors.\nThe dataset covers the following activities: preparing breakfast, preparing lunch, preparing dinner, snacking, personal care (labeled as grooming in the dataset), showering, leaving,relaxing on couch (labeled as spare time/TV in the dataset), sleeping, and toileting. Note that preparing dinner occurs only in the dataset collected in Home B.\nIn our experiments, we ignored the toileting activity since the real semantic of this activity and of the flush sensor was not clear in the dataset (and it was not reported in the dataset's documentation)."}, {"title": "4.2 Supervised baselines", "content": "In order to show the effectiveness of ADL-LLM, we compare it with three state-of-the-art supervised approaches for ADLs recognition.\n\u2022 BiLSTM [33]: Also known as Bidirectional LSTM. It represents a standard recurrent neural network based on LSTM, processing windows in both forward and backward directions.\n\u2022 CascEnsLSTM [33]: Also known as Cascade Ensemble LSTM. A more complex recurrent network that is composed of two heads (an LSTM and a Bi-LSTM) processing the same window in parallel. The output of the two heads is concatenated and fed to a final LSTM layer.\n\u2022 DeepConvLSTM [37]: Also known as Deep Convolutional and LSTM Recurrent Neural Network. It combines convolutional and LSTM layers to effectively capture both spatial and temporal features.\nThese baselines are representative because they reflect the most effective architectures used in the literature for supervised ADLs recognition. We did not include methods specifically designed for labeled data scarcity scenarios because we are not aware of state-of-the-art approaches specifically designed for this setting. As we already mention in Section 2, methods like self-supervised learning or neuro-symbolic approaches have been proposed only for mobile/wearable-based HAR. Smart-Home ADLs recognition introduces different challenges, since there are no large dataset available for pre-training, and different homes may have significantly different sensor layouts."}, {"title": "4.3 Experimental Setup", "content": "We implemented a prototype of ADL-LLM in Python 3.10.12 language using the langchain 0.1.17 library. As LLM for ADLs recognition, we used the OpenAI \u2018gpt-3.5-turbo-0125\u2019 model (setting temperature to 0), while we used the OpenAI \u2018text-embedding-3-small' model to compute embeddings from the textual representation of windows for few-shot prompting. Based on what is suggested in the literature, we used 16-second windows with 80% overlap for the MARBLE dataset [4] and 60 seconds windows with 80% overlap for the UCI ADL dataset [36]."}, {"title": "4.3.2 Dataset splitting strategy", "content": "Since our major contribution is a zero-shot approach, in principle we could have used the entire datasets as test set. However, we also need a training set for two reasons: to train the supervised baselines and for few-shot ADL-LLM. For this reason, we divided each dataset into 30% for training set and 70% for testing set. This splitting reflects a labeled data scarcity scenario, where only a reduced labeled dataset is available. The training set is not used in any way by zero-shot ADL-LLM, while it is used to train the supervised baselines and to create the pool of examples in few-shot ADL-LLM."}, {"title": "4.3.3 Simulating data scarcity scenarios", "content": "In order to evaluate the effectiveness of few-shot ADL-LLM we consider different labeled data availability scenarios to populate the pool: 100%, 50%, 25%, 10%, and 5% of the training set. In these settings, the supervised baselines use the available labeled data for training. We expect that, in data scarcity scenarios, few-shot ADL-LLM outperforms the supervised baselines."}, {"title": "4.4 Results", "content": "Tables 1 and 2 show, for both datasets, the recognition rate of zero-shot ADL-LLM (not using any labeled data) with the supervised baselines trained using the 100% of the training set."}, {"title": "4.4.1 Zero-shot ADL-LLM", "content": "We observe that, for all the datasets, zero-shot ADL-LLM reaches recognition rates that are very close to the best supervised baseline, that is DeepConvLSTM. Indeed, on the UCI ADL dataset, ADL-LLM is only 3% behind DeepConvLSTM in terms of F1 score, while only 2% on the MARBLE dataset. We observe that Home A of the UCI ADL dataset represents the easiest benchmark while Home B, having a reduced sensing infrastructure (like we explained in Section 4.1), makes ADLs recognition more challenging.\nAn interesting insight is that sometimes ADL-LLM is capable of recognizing ADLs that are not captured by supervised baselines. For instance, this happens in the Home A of the UCI ADL dataset, where ADL-LLM is capable of recognizing snacking (even if with a low F1 score), while the supervised baselines fail. Similar phenomena occur in Home B for preparing dinner and in MARBLE for entering home and leaving home. This is due to the fact that these activities are poorly represented in their respective datasets, and a supervised classifier can not learn them properly during the training phase (considering that the training set is 30% of the whole dataset). On the other hand, ADL-LLM leverages the common-sense knowledge implicitly encoded in the LLM, and it is capable of recognizing these activities without any type of training."}, {"title": "4.4.2 Few-shot ADL-LLM", "content": "In the following, we show the results of few-shot ADL-LLM. We performed the experiments by testing different values of k (i.e., the number of examples), specifically 3, 5, 7. Figures 8 and 9 show the overall recognition rate of few-shot prompting compared to DeepConvLSTM (i.e., the best performing supervised baseline) at different data scarcity scenarios. For both datasets, we empirically determined that the best value for k is 7.\nOur results show that, even when only the 5% of training data is used to populate the pool of examples, few-shot ADL-LLM outperforms zero-shot ADL-LLM, thus showing the effectiveness of adding examples in the prompt. Except for Home A of the UCI ADL dataset, few-shot ADL-LLM outperforms the supervised baseline, whose recognition rates significantly decrease as data scarcity increase. As we mentioned before, Home A (Figure 8a) represents a too simple benchmark and even a small dataset leads to excellent recognition rates, and ADL-LLM slightly outperforms the baseline only when considering the 5% of the training set. These results confirm that, even when some labeled data are available, ADL-LLM is a better choice than standard data-driven approaches."}, {"title": "5 Discussion", "content": "In this work, we segmented sensor data using fixed-size windows, that is a standard approach in the literature. However, while this approach makes sense to train machine learning classifiers and find low-level patterns, this may be less suitable for LLMs. Indeed, fixed-size segments capture only a small portion of the activity, and they often include transitions between different activities.\nWe will explore dynamic segmentation strategies based on change-point detection [2]. Indeed, dynamic segmentation can be leveraged to detect significant changes in sensor data distribution that likely associated with changes in the performed activities [3]. We believe that dynamic segmentation would allow LLMs to process more meaningful information to reliably perform ADLs recognition."}, {"title": "5.1 Segmentation", "content": "For the sake of this work, we constrained the LLM in predicting the most likely activity among a set of candidates. This is the most common approach in the ADLs recognition literature. However, this is a limitation, since in real life deployments we can not foresee in advance all the possible activities carried out by the subeject. Moreover, the answers from the model may \u201coverfit\u201d on the specific activity sets, especially when there is a one-to-one mapping between an activity in the set and sensor events.\nWe will explore whether the flexibility of pre-trained LLMs can also be effective in open-world HAR, where there is no fixed set of activities to consider. One of the main challenges will be that the LLM may provide different labels for the same activity (e.g., eating may also be recognized as dining, snacking, having lunch, etcetera). Hence, we believe that in such settings it will be necessary to semantically cluster the predicted labels."}, {"title": "5.2 Open-world HAR", "content": "In our experiments, we adopted third-parties LLM models due to their effectiveness and to simplify the implementation. However, outsourcing information about personal activities to such untrusted third-parties raises several concerns about privacy. Indeed, personal data about human activities may reveal private habits and health issues [9]. Moreoever, since we use the LLM for each sensor data time window, ADL-LLM may be too costly when using third parties LLMs.\nOur vision is that open source LLMs should be used in real-world implementations of ADL-LLM. Since the deployment of such models may be prohibitive in smart-home gateways, we believe that they should run on high-performance computing machines in a trusted domain (e.g., in a telemedicine infrastructure considering a medical application). When this is not possible, compressed versions (e.g., quantized versions and/or with reduced number of parameters) may be even deployed in the home domain (e.g., in a home gateway powered with a GPU). However, since these models are currently less effective, we expect a significant drop in the recognition rate.\nWe evaluated the impact of smaller open source LLMs on ADL-LLM performing preliminary experiments by deploying a LLaMa2 model on our infrastructure. Specifically, we used the llama-2-13b-chat-hf model through HuggingFace 4. We ran these experiments on a Ubuntu server with a partitioned NVIDIA A100 GPU, with 42GB of RAM, and a AMD EPYC 8-core CPU. zero shot ADL-LLM using LLaMa2 and gpt3.5 on the MARBLE dataset.\nAs expected, gpt-3.5-turbo-0125 shows superior performance than llama-2-13b-chat-hf. Specifi-cally, LLaMa struggles even more in recognizing the activities lacking distinct sensor inputs like preparing a cold meal, setting up table, and clearing table. Additionally, llama-2-13b-chat-hf was not accurate in providing the output activity in the required format. As we described in Section 3.3.4,"}, {"title": "5.3 Cloud vs Local LLM processing", "content": "the system prompt specifies to enclose the most likely activity within the curly braces. While gpt-3.5-turbo-0125 always complied"}, {"title": "Large Language Models are Zero-Shot Recognizers for Activities of Daily Living", "authors": ["GABRIELE CIVITARESE", "MICHELE FIORI", "PRIYANKAR CHOUDHARY", "CLAUDIO BETTINI"], "abstract": "The sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADLs recognition is typically based on deep learning methods requiring large datasets to be trained. Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADLs recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADLs recognition system. ADL-LLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADLs recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.", "sections": [{"title": "1 Introduction", "content": "Sensor-based Human Activity Recognition (HAR) is a well-known research topic that has been extensively studied by several research groups [14]. The HAR task involves determining the most likely activity performed by a subject through the intelligent analysis of sensor data collected from one or multiple devices (e.g., wearables, environmental, radio-based). The adopted sensing technologies influence the granularity at which activities can be recognized. For instance, by using inertial sensors in mobile/wearable devices, it is possible to detect physical activities like running, walking, and standing still. On the other hand, to recognize high-level Activities of Daily Living (ADLs) like cooking, taking medicines, or watering plants in smart-home environments, environmental sensors are particularly useful, also considering that subjects at home are less inclined to carry or wear devices all the time.\nAmong the many applications of sensor-based ADL recognition in the areas of energy management, safety, well-being, and healthcare, the long-term monitoring of the ADLs performed by elderly subjects in their homes gained significant importance [38]. The continuous advances in the medical domain significantly extended the world population's life expectancy [13]. While this is surely an outstanding achievement, aging also brings several challenges to society. Among the aging-related issues, cognitive decline is one of the greatest health threats of old age, with nearly 50% of adults over the age of 85 afflicted with the Alzheimer's disease [10]. Detecting the early symptoms of cognitive decline is crucial to implement primary or secondary preventive therapeutic strategies [27]. One of the main markers used by clinicians to assess the cognitive status of a subject is represented by ADLs, whose impairment (i.e., difficulty or inability to perform them independently) is correlated to a rapid functional decline and a higher likelihood of progression to dementia [29]. However, the subjects at risk usually have only a few sporadic visits with medical experts (e.g., neurologists, neuropsychologists, and geriatricians). Such visits often fail to accurately diagnose cognitive decline, especially when it may be revealed by subtle behavioral changes. In-deed, besides a few instrumental tests, and neuro-psychological evaluations at the time of visit, the clinicians mainly rely on possibly inaccurate reports provided by the subjects themselves or their caregivers. It is hence of great value to implement smart telemedicine strategies to continuously and unobtrusively monitor the subjects at risk while keeping them in the comfort of their homes.\nIn the last decades, several research groups proposed the use of unobtrusive sensing infrastruc-tures in smart homes to continuously recognize ADLs in smart homes, to identify digital markers that may indicate the onset of cognitive decline [21, 31, 39]. For the sake of privacy, most of the existing solutions do not consider cameras or microphones (that may not be accepted in home environments), but they are usually based on environmental sensors monitoring the interaction of the subject with the home infrastructure (e.g., motion sensors, door sensors, pressure sensors).\nHowever, the majority of the proposed approaches to automatically detect ADLs from sensor data require training deep learning models [14], thus necessitating the collection of large datasets in the home environment (possibly labeled with the ground truth reporting what activity is being performed). Labeled data collection in these settings is privacy-intrusive, costly, and prohibitive [7]. Another well-known line of research in this domain considers the adoption of purely knowledge-based approaches [15]. The rationale behind these methods is that common-sense knowledge about human activities (e.g., a person usually prepares their meals in the kitchen, turns on the stove, and uses cooking instruments) can be modeled with a set of rules. Knowledge-based ADLs recognition is performed by mapping sensor data windows to semantic concepts and performing rule-based reasoning. The advantage of this strategy is that it eliminates the problem of collecting labeled data in smart environments. However, the design and implementation of knowledge models require the expertise of domain experts and knowledge engineers, and their capability of handling uncertainty and generalizing is typically limited [16].\nThe recent success of Large Language Models (LLMs) paved the way to LLM-based human activity recognition. Preliminary results in the state-of-the-art indicate that LLMs implicitly encode the knowledge about human activities, and hence they can potentially be used to cope with labeled data scarcity [30, 34]. Given these promising results, we hypothesize that LLMs could be also leveraged to process data from environmental sensors in smart-home environments to reveal the ADLs performed by the monitored subjects, without the need to collect any labeled data. We expect that LLMs would generalize across individuals and environments without requiring any patient- or environment-specific training data, thus offering a scalable approach. Moreover, an LLM-based approach would be more flexible in terms of the set of recognizable ADLs (that is usually fixed in supervised approaches).\nIn this work, we propose ADL-LLM, a method leveraging pre-trained LLMs to recognize ADLs in smart-home environments. At its core, ADL-LLM transforms raw sensor data into natural language sentences describing the high-level events observed in the home. ADL-LLM uses a specifically"}, {"title": "2 Related Work", "content": "The labeled data scarcity problem in Human Activity Recognition (HAR) is one of the major challenges limiting large-scale real deployments, and several research groups have been working on this problem for several years. Three categories of approaches have been proposed to mitigate labeled data scarcity: data-driven, knowledge-based, and neuro-symbolic."}, {"title": "2.1 Data Scarcity in Human Activity Recognition", "content": "The majority of the existing approaches to mitigate labeled data scarcity rely on deep learning models. The most common strategies are semi-supervised learning [1], transfer learning [18], and self-supervised learning [23]. Semi-supervised approaches use small labeled datasets to initialize the model, which is incrementally updated using pseudo-labels inferred on the unlabeled data stream [1]. Examples of semi-supervised methods proposed for HAR are self-learning, co-learning, active learning, and label propagation. On the other hand, transfer learning methods leverage models that are pre-trained on a source domain with a significant amount of labeled data, and then adapted in a target domain with labeled data scarcity [40, 41]. Self-supervised learning is a specific transfer learning approach, where the model is pre-trained on massive unlabeled datasets to obtain a reliable feature extractor [23, 24, 28]. The pre-trained feature extractor is then fine-tuned only using a limited amount of labeled data on the target domain. Existing approaches for self-supervised learning focuses on data from wearable/mobile devices, since this domain offers offers large datasets (e.g., Biobank [48], Capture-24 [12]) that can be used for model pre-training. To the best of our knowledge, we are not aware of self-supervised learning approaches for smart-home ADLs recognition, due to the fact that in this domain there are only a few small datasets, and learning a comprehensive representation that accommodates the diverse layouts of different homes is still an open challenge.\nADL-LLM can be considered as a sort of zero-shot transfer learning approach, where a model pre-trained on language is used to recognize activities from a textual representation of sensor data."}, {"title": "2.1.2 Knowledge-Based Methods", "content": "Several research groups investigated whether common-sense knowledge about human activities could be leveraged to completely avoid labeled data collection. These approaches proposed the use of logic formalisms (e.g., ontologies) to model the relationships between sensor events and activities [15, 16]. However, building such logic models requires skills in the formalism model (e.g., OWL2 in the case of ontologies) and significant domain expertise. This task is usually performed by small groups of persons. Building a comprehensive knowledge"}, {"title": "2.1.3 Neuro-Symbolic Methods", "content": "Neuro-Symbolic AI (NeSy) methods combine the best from data-driven and knowledge-based worlds. Indeed, the goal of NeSy models is to combine symbolic reasoning and machine learning to mitigate data scarcity and improve the model's interpretabil-ity [25]. Considering the HAR domain, logic formalisms have been used to refine the output of the data-driven model [8] or to infuse knowledge inside deep learning models [6, 47]. In these methods knowledge constraints allow the model to capture high-level patterns without learning them from data, thus mitigating labeled data scarcity. However, NeSy methods have been only proposed for wearable-based HAR and their applicability to smart home ADLs recognition has not been explored yet.\nFor certain aspects, the capabilities of LLMs can be considered as Neuro-Symbolic since they can implicitly mimic reasoning [19]. Hence, ADL-LLM could be considered as a NeSy model whose data-driven component attempts to mimic symbolic reasoning."}, {"title": "2.2 Using LLMs for Human Activity Recognition", "content": "In the following, we present relevant research works using LLMs in the HAR domain, distinguishing supervised and zero-shot techniques. Supervised methods combines LLMs and labeled HAR datasets for specific tasks (e.g., prediction, classification), while zero-shot techniques directly utilize pre-trained LLMs to identify activities on unlabeled sensor data."}, {"title": "2.2.1 LLM-based approaches for supervised activity recognition", "content": "Several research groups studied how to use LLMs for the sensor-based HAR domain. The most common approach is to fine-tune LLMs with labeled sensor datasets, in order to specialize them on a specific domain [34, 35]. Other works adopted multi-modal models to align sensor data embeddings and text embeddings leveraging contrastive learning to improve the recognition rate [50]. In Neuro-Symbolic HAR, some works used LLMs to infuse common-sense knowledge about human activities into a deep learning model [5]. LLMs has also been leveraged to transform sensor data windows first into sentences in natural language, and then into embeddings to improve the recognition rate and generalization capabilities of supervised models [43]. Finally, since LLMs are pre-trained to predict the next word in a sentence, some research groups fine-tuned them also to predict future sensor events [22, 42]."}, {"title": "2.2.2 Zero-shot approaches for human activity recognition using LLMs", "content": "Similarly to ADL-LLM, other works aim at recognizing human activities using LLMs without any training phase. For instance, some works show that LLMs can reason on raw inertial sensor data from mobile/wearable devies to distinguish between a limited set of low-level physical activities [26, 30]. Another line of research leverages pre-trained model generating 3D motion sequences from text [49] to generate synthetic labeled data from wearable/mobile devices [32]. Considering smart environments, ChatGPT has been used for zero-shot activity recognition [46]. However, this work assumes the availability of several sensors capturing the interaction of the subject with many household items (e.g., specific food ingredients), that is unrealistic in real-world deployments. In this work, we consider a more realistic setting considering the sensors that are usually deployed in smart-homes for ADLs recognition (e.g., motion sensors, magnetic sensors, plug sensors). Finally, another work uses LLMs to provide"}, {"title": "3 The ADL-LLM framework", "content": "In this section, we present ADL-LLM: our LLM-based method for ADLs recognition in smart home environments."}, {"title": "3.1 Problem Formulation", "content": "We consider a classic problem formulation for ADL recognition. A smart home is equipped with a set S = {S1, S2, . . ., Sm} of environmental sensors. Examples are magnetic sensors revealing open-close events (e.g., doors, cabinets, etc.), motion/presence sensors revealing movement/presence in specific areas, and smart plugs revealing the use of appliances. There is a set of target activities of interest A = {A1, A2, . . ., An }, usually suggested by clinicians. As in most works in this domain, we consider a smart home environment inhabited by a single subject or with a sensing infrastructure able to properly associate a sensor's activation with the subject that activated it. Based on the interaction of the subject with the environment, the smart home generates a continuous stream of sensor activations and de-activations. This stream is partitioned in fixed-time windows of \\(\u03c4\\) seconds with a given overlapping factor o. Given a window wt, the goal of ADL-LLM is to determine the activity A \u2208 A that the subject was more likely performing in the interval [t, t + \\(\u03c4\\)]."}, {"title": "3.2 Overall architecture of ADL-LLM", "content": "We assume that ADL-LLM runs on a trusted platform. In the case of the medical application, it may run on a telemedicine platform as one of the components of a complex system that continuously monitors subjects at risk of cognitive decline. The smart home generates a continuous stream of raw sensor data that is pre-processed by the SENSOR STATE GENERATION module to obtain a stream of sensor states. Intuitively, a sensor state represents the timespan in which a sensor is active (e.g., \u201cthe TV in the living room is ON from 12:34 to 13:02\u201d). Since LLMs are specialized in natural language text, the main idea of ADL-LLM is to generate a natural language representation for each window, so that an LLM can \u201creason\u201d on it leveraging its implicit common-sense knowledge. The LLM receives as input a SYSTEM PROMPT (i.e., instructions about the task it has to perform) and a USER PROMPT (i.e., the textual representation of the window enriched with additional instructions for the LLM by the USER PROMPT CONSTRUCTION module). The output of the LLM is finally processed by the ACTIVITY LABEL EXTRACTION module to obtain the most likely activity performed by the subject.\nADL-LLM works in two different modalities:\n\u2022 Zero-Shot, where no labeled data is available\n\u2022 Few-Shot, where a small labeled dataset is available"}, {"title": "3.3 The main components of ADL-LLM", "content": "In the following, we describe the main modules of ADL-LLM, i.e., those used both in the zero-shot and the few-shot modalities."}, {"title": "3.3.1 Sensor States Generation", "content": "The smart home generates a continuous stream of binary sensor events \\(Stream = (..., E_{t-1}^{S_m}, E_{t}^{S_m}, E_{t+1}^{S_m}, ...)\\). Each \\(E_t^S\\) represents the activation or the deactivation of a sensor S at a specific time t. For instance, consider a magnetic sensor on the fridge. This sensor generates an ON event when activated (when the subject opens the fridge) and an OFF event when deactivated (when the subject closes the fridge). Other sensors share a similar behavior, like motion and pressure sensors. For sensors that generate continuous values, as the smart plug that continuously detect the power consumption of specific appliances, we use a threshold on the detected energy consumption to infer when the appliance is turned ON and OFF. For sensors that provide almost continuous values but are not directly associated with activation and deactivation events, like the temperature sensor, we discretize the values in different relevant ranges (e.g., for"}, {"title": "3.3.2 Segmentation", "content": "ADL-LLM employs a fixed-time segmentation of the stream of sensor states, considering windows of \\(\u03c4\\) seconds and an overlap factor o. Intuitively, each window is associated with all the sensor states whose interval intersects with the window interval. More formally, a window wt over the interval [t,t + \\(\u03c4\\)] is associated with the sensor states st(\\(\\sigma\\)i, ts, te) such that ts \u2264 t + \\(\u03c4\\) and te \u2265 t.\nNote that a window may be associated with multiple states even derived from different sensors. Based on the intervals we distinguish three categories that will be useful for text generation:\n\u2022 Inner states: this is the case of ts > t and te \u2264 t + \\(\u03c4\\). This represents the simple case of the state occurring completely within the window.\n\u2022 Already active states: this is the case of ts < t. These states are already active before the start of the window.\n\u2022 Persistent states: this is the case of te > t. These states persist after the end of the window.\nNote that a state may be both Already active and Persistent when ts < t and te > t."}, {"title": "3.3.3 Window2Text", "content": "The WINDOW2TEXT module converts a sensor state window wt into its corresponding textual representation T(wt). Intuitively T(wt) describes what happened in the home from the subject's perspective.\nIn order to do this, the sensor states associated with a window are mapped to a description in natural language of what the subject may have done to lead to those states."}, {"title": "3.3.4 System prompt", "content": "The system prompt is the set of instructions defining the general task assigned to the LLM and is independent of the specific window being considered. Our prompt first asks the LLM to act as an activity recognition system. Then, it provides information about the rooms in the smart home, the main interactions with the environment that can be captured by the available sensors, and the list of activities of interest. Since this information depends on the specific"}, {"title": "3.3.5 User Prompt Construction", "content": "The user prompt contains the information about the specific window wt for which the LLM has to predict the most likely activity. The user prompt contains:\n\u2022 The textual representation T(wt) of the window obtained with WINDOW2TEXT (described in Section 3.3.3).\n\u2022 Instructions about selecting only one activity among the target ones.\n\u2022 The sentence \u201cReason step by step\u201d, used to instruct the LLM to perform intermediate \u201creasoning\u201d steps before providing the predicted ADL. This technique is well-known as Chain Of Thought (COT), it usually leads to increasing the correctness of the LLM's output [45], and proved to be useful even in our system."}, {"title": "3.3.6 LLM-based ADLs Recognition and Activity Label Extraction", "content": "The system prompt and the user prompt are provided as input to the LLM for each window wt, and the LLM provides a textual description as output, reporting some facts supporting the selection of the predicted activity. Finally, the ACTIVITY LABEL EXTRACTION module uses regular expressions to extract only the activity label from the LLM's output. In rare cases, despite the instructions, the LLM provides an activity label that does not syntactically match one the candidate activities in A. For instance, instead of snacking, in a few cases the LLM returned as output preparing late night snack or preparing midnight snack. In these cases, the ACTIVITY LABEL EXTRACTION module uses the cosine similarity to compare the text embeddings of the labels. The label with the highest similarity to the predicted output is selected as the final prediction."}, {"title": "3.4 Few-shot ADL-LLM components", "content": "When a small dataset of labeled sensor data is available, ADL-LLM can be improved by implementing few-shot prompting. This is achieved by extending the zero-shot architecture with two components described in the following."}, {"title": "3.4.1 Building a Pool of Examples", "content": "Suppose that a small labeled dataset D is available. Let D be a set of pairs (wt, A) where wt is a window of raw sensor data, while A is the corresponding activity"}, {"title": "3.4.2 Semantic-based Example Selection", "content": "Given a textual representation T(wt) of a window wt currently processed by ADL-LLM, the corresponding embedding \\(\\phi_{w_t} = \\Phi(T(w_t))\\) is computed, where \\(\\Phi\\) is a function (e.g., a pre-trained LLM) mapping sentences into a latent space representation (i.e., their embedding). Then, for each pair (T(w), A) \u2208 E, we compute the pairwise cosine similarity between \\(\\phi_{w_t}\\) and \\(\\phi_w = \\Phi(T(w))\\). We enrich the system prompt with the k pairs (T(w), A) \u2208 E that are associated with the highest pairwise similarity between \\(\\phi_{w_t}\\) and \\(\\phi_w\\). The pseudo-code for SEMANTIC-BASED Example SELECTION is summarized in Algorithm 1."}, {"title": "4 Experimental Evaluation", "content": "We evaluated ADL-LLM using two public datasets: MARBLE [4] and UCI ADL [36]. We chose these datasets because they include sensor data from a wide variety of sensors (e.g., magnetic, plug, motion) that we are currently considering on a project involving data collection in the homes of subjects with Mild Cognitive Impairment (MCI). While the majority of works in the literature utilize the well-known CASAS datasets [17], which primarily include data from motion sensors,"}, {"title": "4.1 Datasets", "content": "The MARBLE dataset [4] includes data collected from 12 subjects in both single and multi-inhabitant scenarios. The dataset was collected using the following environmental sensors: magnetic sensors to capture the opening and closing of various drawers and cabinets, pressure sensors to detect when a person was sitting at the dining table or on the sofa in the living room, and smart plugs to monitor the usage of the stove and television. Additionally, the dataset includes data from an Android application on the user's smartphone, which automatically detects incoming and outgoing phone calls. Finally, the dataset incorporates data from a smartwatch worn by the user, providing micro-localization information (based on an infrastructure using Bluetooth Low Energy (BLE) beacons) at the room-level.\nWe performed our experiments only considering the single-inhabitant scenarios. Moreover, while the dataset also included inertial sensor data from the smartwatch (e.g., continuous acceleration on three axis), we ignore these type of data as it can not be directly transformed into a prompt in natural language.\nThe dataset includes the following 13 activities: answering phone, clearing table, preparing hot meal, preparing cold meal, eating, entering home, leaving home, making phone call, setting up table, taking medicines, using PC, washing dishes, and watching TV.\nIn our experiments we do not consider the washing dishes activity since it is mainly characterized by inertial sensors in smartwatch data (not used by ADL-LLM). Moreover, we merged the making phone call and answering phone call activities into a single activity called phone call."}, {"title": "4.1.2 UCI ADL", "content": "The UCI ADL dataset [36] includes data about the ADLs performed by two subjects in their own homes. The two homes are named Home A and Home B in the dataset. The overall data collection duration is 35 days: 14 days for Home A and 21 days for Home B.\nDifferent environmental sensors are considered: PIR sensors (installed on doors, bathroom sink, stove, and shower areas) to detect the subject's movements, pressure sensors to detect when the subject was sitting on the couch and the bed, magnetic sensors to detect when the subject was opening and closing cupboards and cabinets, and plug sensors sensors to detect the usage of microwave and toaster.\nHome A and Home B sensing infrastructure have some differences. Compared to Home A, Home B has a slightly more limited sensing infrastructure since it is not equipped with the PIR sensor over the stove, the plug sensor to detect the usage of the toaster, and the magnetic sensor to detect interactions with the bathroom cabinet. Nonetheless, with respect to Home A, Home B includes additional PIR sensors installed on the kitchen, bedroom and living room doors.\nThe dataset covers the following activities: preparing breakfast, preparing lunch, preparing dinner, snacking, personal care (labeled as grooming in the dataset), showering, leaving,relaxing on couch (labeled as spare time/TV in the dataset), sleeping, and toileting. Note that preparing dinner occurs only in the dataset collected in Home B.\nIn our experiments, we ignored the toileting activity since the real semantic of this activity and of the flush sensor was not clear in the dataset (and it was not reported in the dataset's documentation)."}, {"title": "4.2 Supervised baselines", "content": "In order to show the effectiveness of ADL-LLM, we compare it with three state-of-the-art supervised approaches for ADLs recognition.\n\u2022 BiLSTM [33]: Also known as Bidirectional LSTM. It represents a standard recurrent neural network based on LSTM, processing windows in both forward and backward directions.\n\u2022 CascEnsLSTM [33]: Also known as Cascade Ensemble LSTM. A more complex recurrent network that is composed of two heads (an LSTM and a Bi-LSTM) processing the same window in parallel. The output of the two heads is concatenated and fed to a final LSTM layer.\n\u2022 DeepConvLSTM [37]: Also known as Deep Convolutional and LSTM Recurrent Neural Network. It combines convolutional and LSTM layers to effectively capture both spatial and temporal features.\nThese baselines are representative because they reflect the most effective architectures used in the literature for supervised ADLs recognition. We did not include methods specifically designed for labeled data scarcity scenarios because we are not aware of state-of-the-art approaches specifically designed for this setting. As we already mention in Section 2, methods like self-supervised learning or neuro-symbolic approaches have been proposed only for mobile/wearable-based HAR. Smart-Home ADLs recognition introduces different challenges, since there are no large dataset available for pre-training, and different homes may have significantly different sensor layouts."}, {"title": "4.3 Experimental Setup", "content": "We implemented a prototype of ADL-LLM in Python 3.10.12 language using the langchain 0.1.17 library. As LLM for ADLs recognition, we used the OpenAI \u2018gpt-3.5-turbo-0125\u2019 model (setting temperature to 0), while we used the OpenAI \u2018text-embedding-3-small' model to compute embeddings from the textual representation of windows for few-shot prompting. Based on what is suggested in the literature, we used 16-second windows with 80% overlap for the MARBLE dataset [4] and 60 seconds windows with 80% overlap for the UCI ADL dataset [36]."}, {"title": "4.3.2 Dataset splitting strategy", "content": "Since our major contribution is a zero-shot approach, in principle we could have used the entire datasets as test set. However, we also need a training set for two reasons: to train the supervised baselines and for few-shot ADL-LLM. For this reason, we divided each dataset into 30% for training set and 70% for testing set. This splitting reflects a labeled data scarcity scenario, where only a reduced labeled dataset is available. The training set is not used in any way by zero-shot ADL-LLM, while it is used to train the supervised baselines and to create the pool of examples in few-shot ADL-LLM."}, {"title": "4.3.3 Simulating data scarcity scenarios", "content": "In order to evaluate the effectiveness of few-shot ADL-LLM we consider different labeled data availability scenarios to populate the pool: 100%, 50%, 25%, 10%, and 5% of the training set. In these settings, the supervised baselines use the available labeled data for training. We expect that, in data scarcity scenarios, few-shot ADL-LLM outperforms the supervised baselines."}, {"title": "4.4 Results", "content": "Tables 1 and 2 show, for both datasets, the recognition rate of zero-shot ADL-LLM (not using any labeled data) with the supervised baselines trained using the 100% of the training set."}, {"title": "4.4.1 Zero-shot ADL-LLM", "content": "We observe that, for all the datasets, zero-shot ADL-LLM reaches recognition rates that are very close to the best supervised baseline, that is DeepConvLSTM. Indeed, on the UCI ADL dataset, ADL-LLM is only 3% behind DeepConvLSTM in terms of F1 score, while only 2% on the MARBLE dataset. We observe that Home A of the UCI ADL dataset represents the easiest benchmark while Home B, having a reduced sensing infrastructure (like we explained in Section 4.1), makes ADLs recognition more challenging.\nAn interesting insight is that sometimes ADL-LLM is capable of recognizing ADLs that are not captured by supervised baselines. For instance, this happens in the Home A of the UCI ADL dataset, where ADL-LLM is capable of recognizing snacking (even if with a low F1 score), while the supervised baselines fail. Similar phenomena occur in Home B for preparing dinner and in MARBLE for entering home and leaving home. This is due to the fact that these activities are poorly represented in their respective datasets, and a supervised classifier can not learn them properly during the training phase (considering that the training set is 30% of the whole dataset). On the other hand, ADL-LLM leverages the common-sense knowledge implicitly encoded in the LLM, and it is capable of recognizing these activities without any type of training."}, {"title": "4.4.2 Few-shot ADL-LLM", "content": "In the following, we show the results of few-shot ADL-LLM. We performed the experiments by testing different values of k (i.e., the number of examples), specifically 3, 5, 7. Figures 8 and 9 show the overall recognition rate of few-shot prompting compared to DeepConvLSTM (i.e., the best performing supervised baseline) at different data scarcity scenarios. For both datasets, we empirically determined that the best value for k is 7.\nOur results show that, even when only the 5% of training data is used to populate the pool of examples, few-shot ADL-LLM outperforms zero-shot ADL-LLM, thus showing the effectiveness of adding examples in the prompt. Except for Home A of the UCI ADL dataset, few-shot ADL-LLM outperforms the supervised baseline, whose recognition rates significantly decrease as data scarcity increase. As we mentioned before, Home A (Figure 8a) represents a too simple benchmark and even a small dataset leads to excellent recognition rates, and ADL-LLM slightly outperforms the baseline only when considering the 5% of the training set. These results confirm that, even when some labeled data are available, ADL-LLM is a better choice than standard data-driven approaches."}, {"title": "5 Discussion", "content": "In this work, we segmented sensor data using fixed-size windows, that is a standard approach in the literature. However, while this approach makes sense to train machine learning classifiers and find low-level patterns, this may be less suitable for LLMs. Indeed, fixed-size segments capture only a small portion of the activity, and they often include transitions between different activities.\nWe will explore dynamic segmentation strategies based on change-point detection [2]. Indeed, dynamic segmentation can be leveraged to detect significant changes in sensor data distribution that likely associated with changes in the performed activities [3]. We believe that dynamic segmentation would allow LLMs to process more meaningful information to reliably perform ADLs recognition."}, {"title": "5.1 Segmentation", "content": "For the sake of this work, we constrained the LLM in predicting the most likely activity among a set of candidates. This is the most common approach in the ADLs recognition literature. However, this is a limitation, since in real life deployments we can not foresee in advance all the possible activities carried out by the subeject. Moreover, the answers from the model may \u201coverfit\u201d on the specific activity sets, especially when there is a one-to-one mapping between an activity in the set and sensor events.\nWe will explore whether the flexibility of pre-trained LLMs can also be effective in open-world HAR, where there is no fixed set of activities to consider. One of the main challenges will be that the LLM may provide different labels for the same activity (e.g., eating may also be recognized as dining, snacking, having lunch, etcetera). Hence, we believe that in such settings it will be necessary to semantically cluster the predicted labels."}, {"title": "5.2 Open-world HAR", "content": "In our experiments, we adopted third-parties LLM models due to their effectiveness and to simplify the implementation. However, outsourcing information about personal activities to such untrusted third-parties raises several concerns about privacy. Indeed, personal data about human activities may reveal private habits and health issues [9]. Moreoever, since we use the LLM for each sensor data time window, ADL-LLM may be too costly when using third parties LLMs.\nOur vision is that open source LLMs should be used in real-world implementations of ADL-LLM. Since the deployment of such models may be prohibitive in smart-home gateways, we believe that they should run on high-performance computing machines in a trusted domain (e.g., in a telemedicine infrastructure considering a medical application). When this is not possible, compressed versions (e.g., quantized versions and/or with reduced number of parameters) may be even deployed in the home domain (e.g., in a home gateway powered with a GPU). However, since these models are currently less effective, we expect a significant drop in the recognition rate.\nWe evaluated the impact of smaller open source LLMs on ADL-LLM performing preliminary experiments by deploying a LLaMa2 model on our infrastructure. Specifically, we used the llama-2-13b-chat-hf model through HuggingFace 4. We ran these experiments on a Ubuntu server with a partitioned NVIDIA A100 GPU, with 42GB of RAM, and a AMD EPYC 8-core CPU. zero shot ADL-LLM using LLaMa2 and gpt3.5 on the MARBLE dataset.\nAs expected, gpt-3.5-turbo-0125 shows superior performance than llama-2-13b-chat-hf. Specifi-cally, LLaMa struggles even more in recognizing the activities lacking distinct sensor inputs like preparing a cold meal, setting up table, and clearing table. Additionally, llama-2-13b-chat-hf was not accurate in providing the output activity in the required format. As we described in Section 3.3.4,"}, {"title": "5.3 Cloud vs Local LLM processing", "content": "strategies and personalization strategies (e.g., fine-tuning) for these smaller models."}, {"title": "5.4 Impact of LLMs evolution", "content": "While we executed all of our experiments running gpt-"}]}]}