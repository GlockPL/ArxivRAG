{"title": "A Concept-Centric Approach\nto Multi-Modality Learning", "authors": ["Yuchong Geng", "Ao Tang"], "abstract": "In an effort to create a more efficient AI system, we introduce a new multi-modality learning framework that leverages a modality-agnostic concept space possessing abstract knowledge and a set of modality-specific projection models tailored to process distinct modality inputs and map them onto the concept space. Decoupled from specific modalities and their associated projection models, the concept space focuses on learning abstract knowledge that is universally applicable across modalities. Subsequently, the knowledge embedded into the concept space streamlines the learning processes of modality-specific projection models. We evaluate our framework on two popular tasks: Image-Text Matching and Visual Question Answering. Our framework achieves performance on par with benchmark models while demonstrating more efficient learning curves.", "sections": [{"title": "1 Introduction", "content": "Humans are capable of learning knowledge at a remarkable speed even during younger ages, which is in drastic contrast to most learning frameworks that require substantial resources to achieve human-like intelligence on specific tasks. Moreover, despite the exciting advancements from Large Language Models with multi-modality adaptations, there is hot debate over whether these models have achieved general intelligence or if they merely function via lossy compression of training corpora. We believe a concept-centric approach to multi-modality learning could be the key to not only bridging the efficiency gap but also marching towards a more natural learning process that mimics human learning.\nAt the center of our framework is a concept space that carries universal knowledge applicable to diverse modalities. Recent inspiring works on Concept Learning often focus on linking concepts to specific neurons (Liu et al., 2023b) and encoded embedding vectors (Kalibhat et al., 2023; Wang et al., 2023) of a model or injecting specific concepts as neurons into a model's structure (Sheth &\nKahou, 2023; Koh et al., 2020). Compared to these works, our proposed framework takes a systematic approach by organizing modality-agnostic abstract concepts in an interpretable knowledge space and establishing connections to different modalities by projecting modality-specific inputs onto the same space.\nWhile it is common in multi-modality learning to create a shared representation space for multiple modalities (Radford et al., 2021; Li et al., 2022; Ramesh et al., 2022) or even utilize projections to align features from different modalities (Liu et al., 2023a), our shared concept space differentiates itself by possessing abstract knowledge which facilitates efficient learning and effortless incorporation of new modalities into the framework, as demonstrated in our experiments. We believe the proposed framework is a step closer to matching the capabilities of human learning, where we excel in creating a cohesive comprehension of concepts and seamlessly connecting multiple modalities, such as vision and language, to the learned knowledge."}, {"title": "2 Related Work", "content": "Multi-Modality Learning. Vision and language modalities remain at the forefront of multi-modality learning research, with some works exploring alternative modalities like audio (Akbari et al., 2021;\nShi et al., 2022). Within the vision-language area, CLIP by Radford et al. (2021) employs two modality-specific encoders to learn a joint representation through image-text matching. A subsequent work (Ramesh et al., 2022) introduces a text-to-image generation framework, using a text encoder and an image decoder for high-quality image generation from textual descriptions. Transformer-based architectures (Vaswani et al., 2017) have been widely explored for cross-modality information exchange and learning (Singh et al., 2022a; Bao et al., 2022; Kim et al., 2021a).\nBeyond combining and relating modalities, research has delved into diverse areas such as multi-modality few-shot learning (Alayrac et al., 2022; Li et al., 2021) and visual-textual pattern mining"}, {"title": "3 Method", "content": "Our proposed multi-modality learning framework consists of a modality-agnostic concept embedding space that models underlying relationships between concepts via entailment probabilities and a set of modality-specific projection models that extract representation from single-modality inputs and project them onto the domain where the concept space is in, i.e., the knowledge space.\nLearning abstract knowledge in the concept space ensures generality, which makes its domain a good landing place for extracted representations from different modalities. Decoupled from the concept space and each other, modality-specific projection models can be tailored for adaptation to their unique inputs, while modality-specific knowledge stays connected after the projection.\nWe describe the design of the concept space in Sec. 3.1 and projection models in Sec. 3.2. Further implementation details can be found in Sec. 4.1."}, {"title": "3.1 Learning Concept Space", "content": "Davis et al. describe a knowledge representation as a surrogate that both carries the thing that exists in the real world and serves as a medium for pragmatically efficient computation (1993). Building upon their definition of a knowledge representation, we adopt an embedding space proposed by Li et al. (2018) to organize learned representations of abstract concepts. Like mental entities of specific knowledge in our brains, where we can relate concepts to each other, abstract entities in this concept space should be capable of interacting with each other, allowing reasoning inferences. In the proposed framework, we focus on entailment relations between concepts depicted by entailment probabilities to allow interactions between concepts. Contrary to latent spaces or learned ML model parameters, probing into the learned knowledge of this concept space can be easily achieved by querying the entailment probabilities of concept pairs of interest. Furthermore, our experiments demonstrate the efficiency of learning and referencing this concept space, facilitated by its compact parameter size, which qualifies it as a medium for pragmatically efficient computation.\nDefining Concept Space. We first define a knowledge space $\\mathcal{K} \\subset \\mathbb{R}^{d}$ as a $d$-dimensional embedding space. Let $\\mathcal{Y}$ be a set for modality-agnostic concepts. Each concept $y \\in \\mathcal{Y}$ is represented in $\\mathcal{K}$"}, {"title": "3.2 Learning Projection Models", "content": "Defining Projection Models. Decoupled from the abstract concept space, each modality-specific projection model can be viewed as a mapping function $f^{*} : \\mathcal{X}^{*} \\rightarrow \\mathcal{K}$ that generates a box representation in $\\mathcal{K}$ for each input from its modality-specific sample space $\\mathcal{X}$ of an unspecified modality denoted by *. This projection onto $\\mathcal{K}$ allows interactions between specific objects from $\\mathcal{X}^{*}$ and abstract concepts in $\\mathcal{C}$. Specifically, given a modality-specific input $x \\in \\mathcal{X}^{*}$, its representation in $\\mathcal{K}$ can be obtained by $f^{*}(x^{*}; \\theta) = \\Omega \\subset \\mathcal{K}$ follows the same definition of $\\Omega_{y} \\in \\mathcal{C}$. With this representation made available, the probability that an object is associated with a concept $c$ can be naturally described by an entailment probability of $P(y|x) = P(\\Omega_{y}|\\Omega)$.\nAdapting to Concept Space. Given $f^{*'}$s corresponding modality training set $\\mathcal{D}^{*}$, not only should the projection produced for an input $x$ entail a single concept $y$, but it should also entail all other concepts related to $x$. In other words, the projection $\\Omega$ for $x$ should lie at the intersection of a set of concepts that can describe $x$. Namely, the most optimal projection for $x$ should maximize the entailment probability of $P(\\Omega_{y \\in \\mathcal{Y}_{j}}|x)$.\nTo drive projection models to produce this most optimal projection, we use a combination of a binary cross-entropy loss on attribute concepts $\\mathcal{Y}_{attr} \\subset \\mathcal{V}$:\n$\\ell_{attr}(y, \\Omega_{*}) = \\frac{1}{\\mid \\mathcal{Y}_{attr} \\mid} \\sum_{y \\in \\mathcal{Y}_{attr}} \\left[ -\\mathbb{I}(y \\in \\mathcal{y}) \\cdot w \\cdot \\log P(\\Omega_{y} | \\Omega_{*}) \\right]$ \n$\\quad + \\mathbb{I}(y \\notin \\mathcal{y}) \\cdot [-\\log (1 - P(\\Omega_{y} | \\Omega_{*}))]$", "equations": ["\\ell_{attr}(y, \\Omega_{*}) = \\frac{1}{\\mid \\mathcal{Y}_{attr} \\mid} \\sum_{y \\in \\mathcal{Y}_{attr}} \\left[ -\\mathbb{I}(y \\in \\mathcal{y}) \\cdot w \\cdot \\log P(\\Omega_{y} | \\Omega_{*}) \\right] \\quad + \\mathbb{I}(y \\notin \\mathcal{y}) \\cdot [-\\log (1 - P(\\Omega_{y} | \\Omega_{*}))]"]}, {"title": "3.3 Cross Modality Joint Training", "content": "To allow probabilistic analysis for cross-modality tasks, we introduce a joint training stage that encourages different projection models to produce projections that overlap with each other's for the same object. This joint training stage is lightweight since modality-specific projection models have already been trained and adapted to a unified concept space. It requires very modest resources, with convergence occurring within a few hundred training steps, as indicated in Fig. 5 of Appendix. Subsequently, this design with demonstrated efficiency allows the effortless incorporation of new projection models into our proposed framework, mirroring humans' ability to learn and link knowledge across modalities in a fast and efficient manner. Specifically, consider a system with two modalities, A and B, as an example. The training dataset would be denoted as $\\mathcal{D}_{A \\cup B} = \\left\\{ (x_{A}, x_{B}, y_{i}) \\right\\}_{i=1}^{N}$, and the training objective for this joint training stage is defined as:\n$\\mathcal{L}_{joint} (\\theta_{A}, \\theta_{B}; \\mathcal{D}_{A \\cup B}) = \\frac{1}{2 \\mid \\mathcal{D}_{A \\cup B} \\mid} \\sum_{(x_{A},x_{B},y) \\in \\mathcal{D}_{A \\cup B}} [ P(f_{A}(x_{A};\\theta_{A}) | f_{B}(x_{B};\\theta_{B})) + P(f_{B}(x_{B};\\theta_{B}) | f_{A}(x_{A};\\theta_{A})) ]$\nThe overall training objective becomes a combination of modality-specific projection losses and this joint training loss. Optionally, optimization can also include parameters from $\\mathcal{C}$, so that the abstract knowledge learned in the concept space is adjusted based on modality-specific information. Then the objective becomes $\\mathcal{L}_{joint} = \\mathcal{L}_{joint} + \\beta \\mathcal{L}_{c}$ where $\\mathcal{L}_{c}$ denotes the KL divergence loss of the concept space."}, {"title": "3.4 Adapting to Downstream Tasks", "content": "With an abstract concept space and decoupled projection models, our proposed learning framework naturally accommodates various downstream tasks involving single or multiple modalities. Regardless of the specific downstream tasks, their inference process consists of two stages: creating projections and relating them to learned knowledge. This approach more closely resembles human learning than traditional black-box models. In our daily interactions with objects, we process external stimuli like vision by creating abstract mental entities for objects we see. We then comprehend these mental entities using our understanding of the world, or, in other words, our concept space (G\u00e4rdenfors,\n2014). In Section 4, we use an Image-Text Matching task involving multi-modality and a Visual Question Answering task with a single-modality-focused approach to illustrate the functionality of the proposed framework."}, {"title": "4 Implementation and Experiments", "content": "We base our evaluation on three datasets: CLEVR (Johnson et al., 2017a), COCO (Lin et al., 2014), and GQA (Hudson & Manning, 2019) where their concepts are formed from original and supplemental annotations. Both attribute and categorical concepts are present in COCO and GQA whereas CLEVR only contains attribute concepts. More details on the datasets and preprocessing steps can be found in Appendix B. Our experiments follow the same train and validation splits as the original datasets. The proposed framework is pretrained on the train sets and tested on the validation sets."}, {"title": "4.1 Pretraining", "content": "Concept Space. To ensure that each concept box always has a valid set of lower boundaries smaller than its upper boundaries, we use two vectors, $(\\mathbf{w}_{min,y},\\mathbf{w}_{\\Delta,y}) = \\Omega_{y}$, instead of $(\\mathbf{w}_{min}, \\mathbf{w}_{max})$ to represent a box in our actual experiments, where $\\mathbf{w}_{\\Delta} \\in \\mathcal{K}_{>0}$ is restricted to non-negative values. A box's upper boundaries can be obtained by $\\mathbf{w}_{max} = \\mathbf{w}_{min} + \\mathbf{w}_{\\Delta}$. We set the dimension of $\\mathcal{K}$ to 50, based on empirical experiments. Initial values for $\\mathcal{C}$ are sampled from two uniform distributions. As for the negative sampling method, in CLEVR, the only negative concept pairs come from combinations of concepts residing in the same-attribute families, such as (red, blue). For COCO and GQA, negative samples are randomly selected from all concepts. The concept space is trained for two epochs for each dataset with a batch size of 256 using an AdamW optimizer (Loshchilov & Hutter, 2017) with a learning rate of $10^{-3}$. The training of this concept space can be completed quickly as there are only thousands of parameters for a moderately-sized concept space.\nProjection Models. In adapting our framework to the datasets featuring vision and natural language modalities, we incorporate a vision projection model $f_{vision}$ based on a Vision Transformer encoder (Dosovitskiy et al., 2020) and a natural language projection model $f_{NL}$ based on a BERT encoder (Devlin et al., 2018). Both models utilize their encoders' outputs on [CLS] tokens to generate projection boxes in $\\mathcal{K}$. The outputs $e$ with a dimension of 768 are divided into two equal chunks, $h_{min}$ and $h_{\\Delta}$, each with a dimension of 384. These chunks are then input into two fully connected layers to produce $\\mathbf{w}_{min}$ and $\\mathbf{w}_{\\Delta}$ for their respective projection boxes. To ensure $\\mathbf{w}_{\\Delta}$ is always a non-negative vector, an additional ReLU layer is applied. The complete projection process for inputs from the vision modality is outlined in Algorithm 1."}, {"title": "4.2 Image-Text Matching", "content": "Image-text matching is a binary classification task on whether a natural language sentence describes an image. Our framework can naturally adopt a common approach involving creating representations for sentences and images in a shared latent space. In contrast to those works, however, our latent space is a knowledge-embedded concept space that supports efficient probing. Specifically, given an image-text pair ($x_{vision}, x_{NL}$), their representations in the learned concept space $\\mathcal{C}$ are generated by $f_{vision}(x_{vision}) = \\Omega_{vision}$ and $f_{NL}(x_{NL}) = \\Omega_{NL}$. The probability that $(x_{vision}, x_{NL})$ is a positive pair can be determined by the cross entailment probability of $P(matched (x_{vision}, x_{NL})) = [P(\\Omega_{vision} | \\Omega_{NL}) + P(\\Omega_{NL} | \\Omega_{vision})]$. This inference process is demonstrated in Fig. 7 in Appendix.\nIn our experiments, we employ two methods to create negative image-text pairs: swapping whole description sentences and swapping attributes. Specifically, for the first method, we replace 50% of images' description sentences using random sampling. For example, an original description sentence of a CLEVR object might be changed from \"There is a large, metal, red cube\" to \"There is a rubber, small, yellow sphere.\" On the other hand, swapping attributes involves changing only a subset of attributes that describe an object, creating a more challenging image-text matching task. For instance, the same description sentence would be changed to \"There is a small, metal, red cube.\""}, {"title": "4.3 Visual Question Answering", "content": "Visual Question Answering (VQA) evaluates an AI system's ability to reason about images by answering questions related to those images in a natural language format. For this task, we focus on the CLEVR dataset, whose questions are designed to include attribute identification, counting, comparison, spatial relations, and logical operations. Recently, several works (Johnson et al., 2017b; Yi et al., 2018; Mao et al., 2019; Li et al., 2020a; Mei et al., 2022) have focused on a neural-symbolic reasoning approach, using chains of symbolic programs to predict answers to these questions. Our framework's adaptation to VQA involves using a similar set of symbolic programs, but these programs operate on the knowledge space $\\mathcal{K}$ containing interpretable concepts in $\\mathcal{C}$ instead of the high-dimensional latent spaces used by previous works.\nProblem Formulation. Given an image-question pair $\\left\\{ \\mathcal{X}_{vision}, q_{i} \\right\\}$ where $\\mathcal{X}_{vision}$ is an original CLEVR image as shown in Fig. 6 and $q_{i}$ is a natural language question such as \"Are there more cubes than yellow things?\", an AI system needs to generate an answer $o_{i}$ in the natural language format such as \"Yes\".\nSymbolic Programs. We design our symbolic programs as deterministic functions operating on $\\mathcal{K}$. Precisely, we follow the same program definitions as proposed by Johnson et al. (2017a).\nProgram Generator. An LSTM model $\\pi$ is used to process questions into sequences of programs: $\\Omega_{i} = \\pi(q_{i})$. We follow the same pretraining procedure used in (Johnson et al., 2017b) to train this program generator. However, as there is no fine-tuning stage in our adaptation, the parameters in $\\pi$ are frozen once pretraining is finished.\nObject Detection and Projection. Similar to our pretraining process, we use $f_{detection}$ to obtain a set of single-object images $\\mathbf{x}_{vision}$ from $\\mathcal{X}_{vision}$ which are then fed into $f_{vision}$ so their projections can be obtained. Additionally, each single object's coordinates predicted by $f_{detection}$ are attached to its projection box so questions involving spatial relations can be inferred.\nInference Process. A correctly predicted program sequence $\\Omega_{i}$ starts with a Scene function that returns all objects in an image and ends with a program that outputs the answer $o_{i}$. Intermediate programs takes output from previous programs as inputs, which is a reoccurring process until the last function. Our concept space $\\mathcal{C}$ is mainly involved in attribute identification which follows the same rule as used when evaluating projection models' performance in Sec. 4.1. The complete inference process is also demonstrated in Fig. 8 in Appendix."}, {"title": "5 Ablation Study", "content": "We discover that using a pretrained concept space with learned abstract knowledge helps modality-specific projection models converge faster compared to the ones without the access. Specifically, we cut our framework's access to the pretrained concept space $\\mathcal{C}$. Instead, the framework is only provided with a freshly initialized concept space $\\mathcal{C}'$ and the loss function during pretraining of the vision-modality projection model is changed to $\\mathcal{L}'_{vision} = \\mathcal{L}_{vision} + \\mathcal{L}_{concept}$. Fig. 3 shows that the original framework's projection models can converge faster than the ablated version. Based on this evidence, we conclude that the abstract knowledge shared by the pretrained concept space streamlines the learning process of modality-specific projection models."}, {"title": "6 Discussion", "content": "Addressing Bias. Hidden bias learned from datasets often hinders the trustworthiness of ML systems. For example, NLP models often tend to associate the word \"monarch\" more with the word \"male\" than \"female,\" reflected, for instance, in higher similarity scores for embeddings of \"monarch\" and \"male.\" Our proposed framework facilitates effective probing into the model's learned knowledge and offers the capacity to rectify such biases. Further demonstrations of probing into the learned concept space can be found at Appendix A.3. In the same monarch example, as training targets for concept space are simply probability distributions, bias can be easily addressed by ensuring the ground truth concept relations reflect the same entailment probability between the concept pairs of \"monarch-male\" and \"monarch-female,\" which could be easily achieved from user interference.\nScalability of the Concept Space. In our experiments, the concept space is organized to reflect ground truth entailment probabilities observed in the training sets. We believe that our approach of replicating entailment probabilities from training sets can be extended to datasets with a broader array of concepts. Previous works (Vilnis et al., 2018; Li et al., 2018; Lai & Hockenmaier, 2017) have demonstrated that similar embedding spaces can accurately learn entailment probabilities for concept pairs in WordNet (WordNet). Scaling up the number of concepts introduces a challenge in generating the ground truth of entailment probabilities. We think the rich textual data available today offers a viable avenue for extracting concept relations, including entailment relations, as shown in the work by He & Peng (2020). To further verify the scalability of the concept space, we used the proposed method and fiited an concept space to the full WordNet noun entries, contributing to 10765 concepts. Measured by the KL divergence metric, the WordNet's concept space achieves a $D_{KL}$ of 0.1308 against ground truth. For comparison, GQA's concept space is measured at 0.1172.\nCall for Concept-Focused Datasets. In our development, we discovered a lack of high-quality datasets focused on annotating concepts in real-life images. Even with our preprocessing steps, the attribute/concept annotations in COCO and GQA are significantly noisy, partially reflected in the reduced performance of both our framework and others. We believe that potential datasets with"}, {"title": "Future Works", "content": "Although our results are encouraging, we believe there is room for improvement. The current framework supports a moderate number of concepts defined by entailment relations. We envision future iterations expanding this capability to support more concepts with diverse relations. The results of the Image-Text Matching Task inspire us to explore the potential adaptation of the proposed framework to the Text-to-Image Generation task (Ramesh et al., 2022). The concept space embedded with interpretable knowledge could contribute to achieving a safer and bias-free generative process."}, {"title": "Conclusion", "content": "In this work, we introduce a novel multi-modality framework that centers around a concept space embedded with modality-agnostic knowledge. Our experiments show this concept-centric framework demonstrates more efficient learning curves compared to traditional architectures while maintain comparable performances on downstream tasks."}, {"title": "A Concept Space Details", "content": null}, {"title": "A.1 Preliminary", "content": "A smoothing function for the concept space is defined as:\n$m_{soft}^{i}(w) = \\frac{softplus(w^{2})}{softplus(G^{i}_{max} - G^{i}_{min})}$\nwhere the denominator is a normalization term with $G^{i}_{max}$, $G^{i}_{min}$ being the global maximum and minimum values at i dimension. In short, this smoothing function is introduced so a valid joint probability can be calculated even if two concepts/boxes are disjoint and we refer readers to Li et al. (2018) for its complete proof."}, {"title": "A.2 Concept Space Training Objective", "content": "We define a KL-divergence measure between a predicted conditional probability distribution $q(y_1 | y_2)$ and a target $p(y_1 | y_2)$ as:\n$D_{KL}(P(y_1|y_2)||Q(y_1|y_2)) = E_{(y_1,y_2)\\sim P} [\\log \\frac{P(y_1 | y_2)}{Q(y_1 | y_2)}]$", "equations": ["D_{KL}(P(y_1|y_2)||Q(y_1|y_2)) = E_{(y_1,y_2)\\sim P} [\\log \\frac{P(y_1 | y_2)}{Q(y_1 | y_2)}]"]}, {"title": "A.3 Probing into Concept Space", "content": "Figure 4 shows an example of probing into learned knowledge of the concept space exposed to CLEVR. Benefited from such efficient probing mechanism, this concept space offers more interpretability compared to traditional latent spaces or model parameters of previous learning frameworks."}, {"title": "B Evaluation Datasets and Preprocessing", "content": "We base our evaluations on three datasets:\nCLEVR dataset comprises synthesized images paired with intricate questions testing a system's visual reasoning capabilities. We choose CLEVR for evaluation because it provides a highly controlled mini-world, where concepts are easily drawn from visual objects, and relationships between concepts are clearly defined. Each CLEVR image displays a scene with a random number of objects, each described by color, shape, material, and size, which produces 15 unique values, such as blue, cube, forming attribute concepts related to specific objects.\nCOCO dataset exposes our framework to a knowledge world resembling the real world better than computer-generated images from CLEVR. We use attribute annotations proposed by Patterson &\nHays to establish attribute concepts such as soft, cooked, and parked (2016). The original COCO classes are used as category concepts. We focus our evaluation on the top 35 frequent attributes and their associated categories to gain meaningful insights, resulting in 64 concepts.\nGQA dataset is similar to COCO, providing a controlled sandbox mimicking the real world. We use the original attribute and category labels in GQA as concepts and filter out rare attributes and classes, resulting in the same amount of concepts as in COCO. Example attribute and category concepts include happy, old, gray, and boy.\nSince each image in these datasets contains multiple objects, a preprocessing step is essential to isolate single objects. This isolation allows focused learning on targeted objects, reducing ambiguity. This process mirrors human learning, where attention naturally centers on a novel object while ignoring the surrounding environment G\u00e4rdenfors (2014).\nBoth COCO and GQA datasets already include object segmentation data. For the CLEVR dataset, we employ a MASK R-CNN model (He et al., 2017), denoted as $f_{detection}$, trained on a small amount of annotated data as an object detection model to generate segmentation. Visual object inputs are created by cropping original images to include only the objects of interest, as illustrated in Fig. 6.\nIn addition to object isolation, we generate a descriptive sentence for each object, introducing natural language as a new modality in the dataset. Each sentence of an object has the structure \"There is"}, {"title": "C Projection Models Details", "content": null}, {"title": "C.1 Architecture", "content": "ViT-based vision-modality projection models use a vision transformer (ViT-Base) pretrained on\nImageNet-21k Dosovitskiy et al. (2020) as the backbone. The baseline MLP model is comprised of three fully-connected layers used as ViT's classification head, with each middle layer containing 128 neurons.\nResNet-based vision-modality projection models use a ResNet model (ResNet-50) pretrained on\nImageNet-21k He et al. (2015) as the backbone. Because of ResNet's large feature vectors, the linear layer used to project feature vectors onto the concept space is expanded to a three-layer MLP, featuring two intermediate layers comprising 512 and 256 neurons, respectively. The baseline MLP model is comprised of three fully-connected layers installed after ResNet's layers, with each middle layer containing 128 neurons.\nBERT-based nlp-modality projection models use a pretrained BERT encoder (BERT-base) Devlin et al. (2018) as the backbone."}, {"title": "C.2 Training Details", "content": "Vision modality projection models are trained for 10 epochs with a batch size of 256 with an exception\nof CLEVR whose models are only trained for 1 epoch. An AdamW optimizer with a learning rate of\n$10^{-4}$ is used. Learning rate schedulers are used to achieve warm-up for first epoch and then a process of $10^{-1}$ linear decrease over the remaining epochs.\nNatural-language modality projection models are trained for 1 epoch using the same setup and hyper-parameters as used by the vision ones.\nThresholds for attribute identification are selected based on performances from training splits.\nThresholds producing the best f1 score on training sets are used in tests."}, {"title": "D Image-Text Matching Experiment Details", "content": null}, {"title": "D.1 Our Framework", "content": "We follow the cross-modality joint training method and train our vision and natural language projection models for only 1 epoch with a batch size of 256 and a learning rate of $10^{-4}$."}, {"title": "E Computation Resources", "content": "We run our experiments on a virtual machine (VM) hosted by Microsoft's Azure. This VM has four\nNVIDIA A100 PCIe GPUs with 320 GB of total memory."}]}