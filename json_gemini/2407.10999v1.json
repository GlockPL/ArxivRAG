{"title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot", "authors": ["Kaiqi Zhang", "Shuai Yuan", "Honghan Zhao"], "abstract": "With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text generation tasks such as summarization and article creation is very difficult. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks.", "sections": [{"title": "1 Introduction", "content": "Automatically evaluating an outputted span of text from a model is difficult because of its uncertainty in text format and diversity of tasks. It is different from the other simple tasks like classification, which can simply evaluate outputs of models by splitting datasets. Automatic evaluation of a span of text usually uses a model-based (e.g., Zheng et al. (2024); Jiang et al. (2023); Wang et al. (2023b)) or statistics-based (e.g., Fu et al. (2023); Papineni et al. (2002)); Lin (2004)) method to evaluate. And it considers various standards (correctness, helpfulness and creativity, etc.) of the text.\nSince the birth of ChatGPT (Ouyang et al. (2022)) at the end of 2022, NLP research and development has officially entered the era of LLM. Although the R&D of LLM is rapid, there is still a lack of available automatic evaluation methods for LLM. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult.\nIn this paper, we propose a model-based evaluation method: TALEC. TALEC focuses on evaluation in specific application scenarios. Besides, all the experiments and benchmark in this paper are related to our real application and are in the automobile field. TALEC allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL, Brown et al. (2020)) to teach judge model these in-house criteria. Our criteria can be viewed in Table 1. We also propose an engineering approach to adjust and iterate the shots, which is splitting the dataset to \"train\", \"eval\" and \"test\" dataset. The \"train\" dataset is to find typical cases. Then we will provide these typical cases for the context as shots. The remaining two datasets is to help to adjust and iterate the shots and Verify the final result.\nIn addition, we find some problems when using shots which is written manually. Moreover, too many shots will also cause forgetting some former information and may exceeding context length limit. To solve this, We come up with a prompt paradigm and try combining zero-shot and few-shot to make the judge model focus on more information."}, {"title": "2 Related Works", "content": "The way of evaluating deep learning model has changed dramatically since the birth of ChatGPT. Before the this, various methods to automatically evaluate have been proposed. BLEU (Papineni et al. (2002)) and ROUGE (Lin (2004)) calculate the similarity between output text and reference text. But restricted to LLMs' uncertainty in output text format and diversity of tasks, it is difficult to offer a good and proper reference text. GPTScore (Fu et al. (2023)) uses perplexity (PPL, Jelinek et al. (1977)) to evaluate output text based on former context, but recent research shows that there is no correlation between PPL and LLMs' long-text understanding ability (Hu et al. (2024)). MMLU (Hendrycks et al. (2020)), GPQA (Rein et al. (2023)), C-Eval (Huang et al. (2024)) and some benchmark in SueprCLUE (Xu et al. (2023)) use multiple choice questions to evaluate. This method is simple and efficient, but it only focuses on model's knowledge and reasoning abilities, lacking of other abilities like instruction-following.\nAfter the birth of ChatGPT, automatic evaluation methods become more explainable and pay more attention to the abilities of multiple dimensions of the model. MT-Bench (Zheng et al. (2024)) uses GPT-4 to compare responses from tow different models and pay special attention to multi-turn dialogue ability of model. Besides, There are a lot of methods use fine-tuned model or aligned model to evaluate (Jiang et al. (2023); Wang et al. (2023b), etc.). Some methods focus on special abilities of LLM, such as LLM-EVAL (Lin and Chen (2023)) is a unified multidimensional automatic evaluation method for open-domain conversations with LLMs. TruthfulQA (Lin et al. (2021)) focuses on hallucination of LLM. IFEval (Zhou et al. (2023)) designs some tasks to measure the instruction-following ability of LLM. HumanEval (Chen et al. (2021)) and MBPP (Austin et al. (2021)) is benchmark to measure coding ability of LLM and they use pass@k as the final score. MATH (Hendrycks et al. (2021)) and GSM8K (Cobbe et al. (2021)) pay more attention to mathematical ability of LLM. And some methods use unique approach to evaluate, like BotChat (Duan et al. (2023)), which uses a approach similar to the Turing test to evaluate.\nBut all of them only use some general criteria (correctness, helpfulness and creativity, etc.) and have low correlation with human, making it unavailable in specific application domains. So we now formally introduce our method: TALEC."}, {"title": "3 Our Method: TALEC", "content": "TALEC is grounded in customizable, challenging, and adaptable evaluation criteria, distinguishing itself from conventional automatic methods. It allows users to flexibly set their own evaluation criteria, which maybe more difficult than some general criteria because the criteria may have to meet not only general standards but also specific needs of customers and business security requirements at the same time. In addition, it is hard to set the exact scale of the criteria, making it more difficult sometimes. Even for a human evaluator, a series of practices and quality inspections is needed.\nIn this paper, we do experiments on four distinct tasks and ten customized labels. The tasks include:\nSentiment Analysis. Given a comment, determine the type of sentiment based on the textual information and provide the reason for the judgment.\nKnowledge QA. Given a knowledge question in the automobile field, provide a detailed answer to this question.\nSearch QA. Given a piece of reference information from search engines and a related question, answer the question based on the reference information.\nTitle Generation. Given an article and some complex requirements, generate main title and sub-\ntitle that is based on the article and completely meets the requirements.\nLabels and their descriptions can be viewed in Table 1. The labels can be divided into two categories: acceptable labels(score=1) and unacceptable labels(score=0). If any unacceptable label appears, the final sore will be 0. If unacceptable label does not appear but any acceptable label appears, the final sore will be 1. If no labels appears, the final sore will be 2 (full score)."}, {"title": "3.2 Assumption of TALEC", "content": "Many automatic evaluation only rely on the ability of the model itself, without any knowledge injection and teaching based on concrete problems and examples. Actually, even for a human evaluator, a series of practices with concrete examples and quality inspections is needed to learn our evaluation criteria. Therefore, the point is treating judge model like a human evaluator. What we need to do is teaching the judge model repeatedly and patiently with concrete examples. In order to do this, we simulate this practice-quality inspection cycle process by splitting the dataset to \"train\", \"eval\", \"test\" dataset and adding manual adjustment. The details of this simulation will be displayed in Section 3.3."}, {"title": "3.3 TALEC", "content": "We introduce TALEC, a novel automatic evaluation framework that leverages SOTA model like GPT-4 to evaluate an outputted span of text from a model. TALEC mainly uses ICL to teach judge model the customized evaluation criteria. The overall process of TALEC can be viewed in Figure 1. We will introduce several key points of TALEC below.\nEngineering Approach to Adjust and Iterate the Shots. We split the dataset to \"train\", \"eval\", \"test\" dataset. The \"train\" dataset is to find typical cases. Then we will provide these typical cases for the context as shots. The \"eval\" dataset is to help manual optimization. The overall process can be listed as: (1). Find a some typical cases by feeling, then write the reasons why the cases are wrong, and regard them as the first version of shots. (2). Use this version of shots to run and gather statistics on \"eval\" dataset. (3). adjust the shots (add/delete/modify) based on the results on \"eval\" dataset. (4). Repeat the above process until you get a better results on \"eval\" dataset. After this process, use the final version of shots to run and gather statistics on \"test\" dataset, to verify the effectiveness of the shots.\nCriteria Division. In our customized criteria, there are 10 labels per task. Each label has several positive and negative shots. So too many shots may cause exceeding context length limit. Criteria division will solve this problem. It is to divide the overall criteria to label granularity. For example, we assume there are 10 labels. Normally, we will let GPT-4 determine whether these 10 labels exist at one time. But criteria division is to let GPT-4 judge 10 times, and only evaluate one label each time. In addition, we also find that despite a 10-fold increase in cost, criteria division result in greatly improvement in judging on almost all the labels. We will discuss the improvement in Section 4.1.\nPrompt Paradigm. ICL is a good way to teach judge model the in-house criteria. However, it will cause some problems. When injecting manually-written shots into the context of a model, the model will not only try to understand the shots but also\nimitate the writing style of shots. This imitation may make the model ignore some key information and drop its Chain of Thought (CoT, Wang et al. (2023a)) ability. The prompt paradigm can be listed as: (1). Repeat the description of a label before judge. (2). Try not to use transitive (or) progressive words such as \"and\", \"but\", \"however\" in the first half of the judge reason. (3). Try to keep the positive and negative shots consistent in formatting, especially in the first half. We will verify the effectiveness of our prompt paradigm in section 5.\nCombine Zero-shot with Few-shot. This approach is to compensate for the model's omission of key information. As Figure 1 shows, the model will make two completely independent judgments, one is zero-shot judge and another one is few-shot judge. Then we will connect system Prompt, Shots, answer outputted by zero-shot, answer outputted by few-shot to get a new context and use this context to judge again. Then we will get the final result. The prompt template can be viewed in Figure 4. Ablation experiment results of this approach is shown in Section 4.2."}, {"title": "4 Ablation Experiment", "content": "We verify the effectiveness of the approaches mentioned above. Note that we use different variants of GPT-4 (gpt-4-0613, gpt-4-32k-0613 and gpt-4-0125-preview) to suit different contexts length. The baseline of these experiments is called Standard Prompt Paradigm, which uses engineering approach to adjust and iterate the shots and applies criteria division and our prompt paradigm. However, Standard Prompt Paradigm does not combine zero-shot with few-shot, it only uses a conventional ICL approach.\nDuring a series of experiments, we find that \"Incorrect Answer/Unrelated Matching Results\" label in Knowledge QA task is very special. A uniformly formatted few-shot would instead negatively affect the label, which distinguishes itself from the others. We guess it is because the errors in the answer of the Knowledge QA task may be evenly distributed throughout the answer. Shots is useless in this scenario because it is difficult to help localize the error information. Furthermore, uniformly formatted shots will further limit model's ability to find error information."}, {"title": "4.1 Criteria Division", "content": "Our evaluation methodology employs large language models, with a constraint on their maximum context length. Generally, this approach suffices for most tasks. However, in instances where lengthy prompts and responses are required, such as in the context of few-shot article generation, a single instance can extend to 1500-2000 tokens, thereby posing a limitation on the context length. Furthermore, when evaluations involve intricate tasks with multiple dimensions, the accuracy of the assessment may be negatively affected.\nTo address these challenges, we strategically decompose the customized evaluation criteria into distinct components, primarily by segmenting the evaluation dimensions. This approach enables the model to concentrate more on each criterion, thereby streamlining the evaluation process and enhancing the outcomes. By associating each problem label with its corresponding few-shot examples and inputting them into the model, we bypass the\nneed for a single evaluation of all labels. Additionally, this method effectively alleviates the problem of insufficient context.\nWe have attempted to compare the following two experimental setups:\nStandard Prompt Paradigm(Division). Individual evaluation dimensions are fragmented into distinct criteria, which are separately fed into the Judge model. The model's output is aggregated multiple times for each criterion before calculating the overall score.\nNon-division. The complete set of evaluation criteria, along with their associated shots, is simultaneously fed into the model, enabling it to produce the final score directly without sequential processing."}, {"title": "4.2 Combine Zero-shot with Few-shot", "content": "We said above that the imitation to text format in shots may make the judge model ignore some key information. So we try injecting zero-shot judge to avoid the impact of shots. The process can be viewed in Figure 1 and the prompt can be viewed in Figure 4.\nWe compare two experimental setups to verify the effectiveness of this approach:\nStandard Prompt Paradigm(Single-turn wo Zero-shot). Use the shots obtained from our engineering approach and inject the shots into context to judge. This approach only judges one case in a single-turn.\nMulti-turn with Zero-shot. As shown in Figure 1, the model will make two completely independent judgments, one is zero-shot judge and another one is few-shot judge. Then we will connect system Prompt, Shots, answer outputted by zero-shot, answer outputted by few-shot to get a new context and use this context to judge again. Then we will get the final result."}, {"title": "4.3 SFT vs ICL", "content": "Previous automated evaluation methods have opted to incorporate knowledge through Supervised Fine-Tuning. However, state-of-the-art models such as GPT-4, due to their proprietary nature, cannot be subjected to SFT to further enhance their performance. Therefore, methods utilizing SFT are compelled to resort to relatively weaker open-source models as a foundation, which may potentially impact the evaluation results. Consequently, we have chosen to introduce knowledge using In-Context Learning.\nWe validate the differences in knowledge introduction using SFT and ICL methods respectively, by fine-tuning Qwen-72B-Chat model (Bai et al. (2023)) and comparing the effects. We have established three experimental setups using three different models:\nStandard Prompt Paradigm(GPT4 + ICL). Using GPT-4 as the evaluation model with in-context learning.\nQwen-72B-Chat + ICL. Using Qwen-72B-Chat as the evaluation model with in-context learning.\nQwen-72B-Chat + SFT. Fine-tuning Qwen-72B-Chat as the evaluation model. We construct a dataset composed of 179 manually annotated high-quality data specific to the aforementioned four tasks, 100 open-source evaluation data obtained from the TigerScore dataset, and 300 open-source general data. We have trained Qwen-72B-Chat for one epoch on this dataset.\nThe results are shown in Table 6. It can be observed that GPT-4 with in-context learning outperforms the method based on Qwen-72B-Chat across all tasks. Meanwhile, Qwen-72B-Chat integrated with in-context learning exhibits comparable performance to Qwen-72B-Chat combined with SFT on two tasks, surpasses the latter on one task, and underperforms on the other task. This suggests that in-context learning can achieve results similar to SFT on LLM, and to a certain extent, can serve as a substitute for SFT. Furthermore, the in-context learning approach can be applied to state-of-the-art proprietary LLM with superior performance, thereby yielding enhanced results."}, {"title": "5 Prompt Engineering", "content": "In the system prompt, we explicitly offer descriptions of the evaluation criteria to help model better\nunderstand the criteria. However, we noticed that the model occasionally failed to recall the previously provided descriptions because of very long context caused by too many shots.\nFor instance, the requirements for some generation tasks include word count specifications. However, due to the token-based tokenizer structure of LLLMs, they can't accurately count the number of words. Consequently, we employed a rule-based approach to judge this aspect, ensuring more precise assessments without relying on the model's limitations. To clarify this, we clarified in the prompt that the model should disregard word count requirements. Surprisingly, the model continued to consider it in its evaluations.\nTo mitigate this issue, we introduce a novel approach where the model is prompted to recurrently summarize and reiterate the interpretation from the system prompt before delivering its evaluation, as depicted in Figure 2.\nTo validate the efficacy of this method, we executed two experiments employing distinct strategies:\nStandard Prompt Paradigm(Repeat descriptions). We required the model to reiterate the descriptions prior to evaluation, maintaining a consistent format for the shots.\nNon-repetition. We adopted a more informal format for the shot composition, eliminating the need for the model to reiterate the descriptions.Experimental results indicate that repeating the descriptions of evaluation criteria prior to each label"}, {"title": "5.2 Standardize the Format of Examples", "content": "It is universally recognized that large models possess Chain of Thought (CoT) capabilities. The prudent use of CoT enables the model to provide a detailed explanation before delivering the final answer, thereby substantially improving the accuracy of responses. However, in our approach, the incorporation of 'shots' has instigated a problem. As models mimic the structure of the shots in their output, an informal arrangement of shots could potentially cause the model to prematurely conclude the answer without a comprehensive explanation.\nIn the previously mentioned example, it seems that the model's Chain of Thought(CoT) capability is activated by initially offering an explanation. However, the model actually determines the output label at the outset due to the varied formats employed in the construction of positive and negative examples, particularly the inclusion of adversative phrases preceding the negative examples. The explanation is subsequently appended following the decision on the label, leading to an inversion of cause and effect.\nHence, we propose that in the realm of prompt engineering, the utilization of a consistent format for both positive and negative examples is crucial. This should be accompanied by a reduction in the use of adversative expressions. The objective is to postpone the revelation of the answer, thereby enabling the model to provide a comprehensive explanation prior to presenting the ultimate response at the conclusion."}, {"title": "6 Comparison with Other Method", "content": "We list Spearman correlation of 3 typical method and our method here: GPTScore (0.1888), Tiger-Score (0.3373), MT-Bench (0.6/0.85) and TALEC (0.8962/0.875). It is difficult to make a completely fair side-by-side comparison with other methods due to the differences in the score system, evaluation question types, and evaluation criteria. However, the high alignment of TALEC compared to other methods can also indirectly indicate its validity and usability."}, {"title": "6.2 Comparison with Human Annotation", "content": "Compared to automated evaluation, human assessment possesses a greater capacity to encompass the intricate and adaptable evaluation criteria and scales inherent in our business operations. Nevertheless, the financial implications of employing human evaluators are substantial, with the primary expenditure being personnel training costs. This is particularly true for specialized domains where the evaluator requirements are notably stringent. Furthermore, the efficiency of human assessment significantly lags behind that of automated evaluation, thereby considerably impeding the iterative development of large language models.\nAdditionally, human annotation, while useful, is not always dependable. In the initial phases of our experiment, we utilized manual evaluation to gather enough data for the development of a more effective assessment system. This process involved a dual-review system, blind reviews, comprehensive quality checks for contentious cases, random spot checks for non-contentious cases, and a concluding round of spot checks. Only through the application of these multiple strategies and checks were we able to ensure the data's accuracy. However, an analysis of the manual annotation results prior to inspection revealed a significant lack of alignment among different annotators in the absence of rigorous quality control, as demonstrated in Table 9. This discrepancy can be partially attributed to the complexity of the custom business evaluation criteria. Despite these challenges, automated evaluation has proven to be highly effective in such a demanding context."}, {"title": "7 Conclusion", "content": "We propose a method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. We try many approach to improve the judge abilities of model, such as criteria division and combining zero-shot with few-shot. We also come up with an engineering approach to adjust and iterate shots. which splits the dataset and simulates the practice-quality inspection cycle process. In addition, we find that when injecting manually-written shots into the context of a model, the model will not only try to understand the shots but also imitate the writing style of shots. This imitation may make the model ignore some key information and drop its CoT ability. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. In the end, we compare TALEC with other methods and humans, verifying the availability of TALEC."}, {"title": "8 Limitations", "content": "Although TALEC outperforms than many other methods, it still makes some really stupid mistakes sometimes. And TALEC relies heavily on manual annotation in the early stage, making it hard to start. Some other methods also focus on special abilities like hallucination and contextual memory, but TALEC can't evaluate these abilities so far."}]}