{"title": "High-Throughput Phenotyping of Clinical Text Using Large Language Models", "authors": ["Daniel B. Hier", "S. Ilyas Munzir", "Anne Stahlfeld", "Tayo Obafemi-Ajayi", "Michael D. Carrithers"], "abstract": "High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automating high-throughput phenotyping of clinical text.", "sections": [{"title": "I. INTRODUCTION", "content": "MANUAL phenotyping of electronic health records is laborious [1], [2]. Precision medicine has intensified the need for high-throughput methods to support the acquisition and processing of vast volumes of unstructured medical-related data [3], [4]. High-throughput phenotyping remains challenging due to the complexity of the task and the volume of physician notes [5]\u2013[7]. Natural language processing (NLP) methods for identifying signs (concept recognition) in clinical text have evolved from rule-based and dictionary-based systems [8], [9], to machine learning and statistical models [10], [11], to deep learning methods such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) [12]-[15], and more recently to transformer architectures [16]-[21]. Barriers to the wider application of NLP methods to high-throughput phenotyping include limited accuracy, the need for large quantities of manually annotated data for training, and an inability to generalize an application from one domain of interest to another [2], [6], [7], [22].\nThe emergence of large language models (LLMs) offers an opportunity to address previously unsolvable NLP problems, including the high-throughput phenotyping of physician notes [23]\u2013[25]. LLMs belong to a class of foundation models which are inherently strong learners of heterogeneous data due to their large capacity, unified input modeling of different modalities, and improved multi-modal learning techniques [26]. LLMs have a superior ability to extract, summarize, translate, and generate textual information with only a few or even no prompt/fine-tuning samples [26]. Thus, they have been shown to perform well in high-throughput phenotyping [27] as they offer scalability and generalizability and require minimal additional training. Recent work in [28], [29] demonstrates their usefulness in processing large volumes of text in electronic health records (EHR) [28], [29]. They are also able to derive phenotypes from other text sources, such as PubMed abstracts and clinical summaries [27].\nA goal of the precision medicine initiative is to use patient phenotypes to guide treatments and improve outcomes [30]. Patient phenotypes must be computable before being entered into precision medicine machine learning models. Patient phenotypes are recorded in EHRs as unstructured text. The most widely used standard for recording phenotypes is Human Phenotype Ontology (HPO) [1], [31], [32]. In a medical setting, the patient reports symptoms to the physician, who then examines the patient to obtain clinical signs. Together, these signs and symptoms constitute the phenotype of the patient. For brevity, we will collectively refer to these signs and symptoms as simply signs. The usual signs of the disease are the phenotypes of the disease. Online Mendelian Inheritance in Man (OMIM) provides a view of genetic heterogeneity of similar phenotypes across the genome for related diseases. This is referred to as phenotypic series i.e. a collection of phenotypes (diseases or traits) that are caused by mutations in different genes but have similar clinical features. For example, in OMIM, dystonia phenotypic series include several types of dystonia with different genetic causes such as DYT6, DYT11, and DYT25 [33], [34]. Each type of dystonia in this phenotypic series has similar clinical features, such as involuntary muscle contractions and abnormal postures but differ in their genetic causes based on mutations in different genes (e.g., TOR1A, THAP1, SGCE, GNAL). The phenotypic series in OMIM helps to categorize and understand the genetic heterogeneity of multiple diseases aiding in diagnosis and treatment strategies. Patient phenotyping involves multiple steps, including sign identification (finding the sign in the text), sign categorization (assigning signs to high-level categories), sign normalization (mapping signs to terms in an ontology), and sign binarization"}, {"title": "II. DATA", "content": "The neurological disease and phenotypic data was retrieved from the OMIM database via their API (api.omim.org). Disease phenotypes are described in the clinical synopsis and the clinical features sections for each disease within the OMIM database. The clinical synopsis is a list of signs, symptoms, mode of inheritance, and age of onset. The clinical features section summarizes the published literature that underpins the phenotype of each disease. The OMIM API has separate calls for clinical features and clinical synopsis. The text from OMIM served as a surrogate for text from physician notes. Diseases in OMIM with similar phenotypes are grouped in a phenotypic series. OMIM currently has 582 Phenotypic Series, each with anidentifier beginning with PS. We evaluated 16 phenotypic series that spanned across 405 neurogenetic diseases (see Table I)."}, {"title": "III. METHODS", "content": "The high-throughput phenotyping pipeline extracts phenotypic terms in clinical text through the following steps (Fig. 1). Parameter details for the OMIM API and Open AI API calls are available on the project GitHub site.\nText Extraction and Preprocessing. Given a list of diseases and MIM numbers for each phenotypic series, the model reads and extracts clinical summaries from the OMIM API. (Parameter details are described on the project GitHub site.) White spaces and tabs were converted to a single white space. Commas, hyphens, semicolons, single quotes, double quotes, forward slashes, and backslashes were converted to a single white space. Periods were retained to identify sentence boundaries. The Clinical Features and Description sections from OMIM were normalized for further analysis.\nSign Identification. The aim is to identify neurological signs in the preprocessed text using the OpenAI API using the LLM. Text was passed to the OpenAI API with instructions to find all the signs within the text (Box 1).\nSign Categorization. The OpenAI API was given a list of identified signs with instructions to categorize them into one of 30 high-level categories (Box 2).\nSign Normalization. Signs were normalized by mapping them to the HPO using two approaches: the deep learning method combined spaCy (Explosion AI, Berlin) with Gensim BioWordVec embeddings. Vectors were generated for each sign and compared to vectorized HPO terms using cosine similarity. The HPO term with the highest cosine similarity to the retrieved sign was assigned as the best match. The LLM method involved passing a list of signs to GPT-4 or GPT-3.5-Turbo with instructions to map the sign to a term and ID in the HPO (Box 3).\nCategory Binarization. Categories were binarized as either '0' (no signs found in that category) or '1' (one or more signs found in that category). If a disease had two or more terms in a phenotype category, such as 'areflexia' and 'decreased reflexes' in the hyporeflexia category, the category was scored as '1', indicating one or more signs. The high-level categories are shown in Box 2.\nDisease Vectorization. For each disease, a vector was assembled from the 30 binary phenotype categories so that the disease vector had 30 elements, each with a value of '0' or '1'. Of the 405 diseases evaluated, 283 had adequate clinical summaries in OMIM to allow high-throughput phenotyping and the creation of a disease vector. The disease vectors were stored as a data frame.\nVisualization of Disease Heterogeneity within a Phenotypic Series. We created a separate heatmap for each phenotypic series to visualize similarities and differences between diseases within a phenotypic series (Seaborn library [40]). Each row in the heatmap was a disease in the phenotypic series. Each column was one of the 30 binarized phenotype categories (red for 'present', blue for 'absent'.)\nVisualization of Distances between the Centroids of Phenotypic Series. The 30 phenotype categories were reduced to two dimensions by principal component analysis (PCA). PCA calculated the coordinates of each disease in the phenotypic series, which were then shown as markers on a scatter plot (Fig. 7). Using the enhanced explainability plot approach described in [41], we calculated centroids for each phenotypic series, as showns as an 'X' on the scatter plot. For interpretability, we limited centroids to five phenotypic series per scatter plot. Distances between phenotypic series centroids represented relative similarities between different series as computed by PCA using the phenotype vectors.\nPerformance Metrics. The disease processing rate, the sign identification rate, sign categorization rate, and sign normalization rate were based on 405 diseases, a corpus of 175,724 words, and 16 phenotypic series (Table I).\nThe metrics for sign identification, sign categorization, and sign normalization were calculated based on a validation dataset of 40 diseases selected from the Dystonia, Parkinson, Hereditary Spastic Paraparesis, and Charcot-Marie-Tooth phenotypic series. In the validation dataset, GPT-4 found 609 signs, and GPT-3.5-Turbo found 358 signs. Sign identification was assessed by comparing signs identified by GPT-3.5-Turbo and GPT-4 with those identified by two manual annotators using the Prodigy annotation tool (Explosion AI, Berlin). Annotation methods have previously been described [42]. Manual annotators received instructions similar to those given to GPT-4 and GPT-3.5-Turbo. Two sets of signs were created for each of the 40 diseases in the validation dataset. The first set of signs were those found by GPT-3.5-Turbo or GPT-4. The second set was those found by the two manual annotators. Measures of agreement between the manual annotators and the LLMs included the mean Jaccard Index for each pair of sets, the highest cosine text similarity index (based on the spaCy similarity method and the Gensim BioWordVec embeddings) averaged across all available signs from each set, and the number of weak matches' (similarity of less than 0.80) in each set. For sign identification, a sign from the GPT-sign set with at least 0.80 similarity to a sign in the manual-annotator-sign set was rated as a true positive'. A sign in the GPT-sign set with less than 0.80 similarity to any sign in the manual-annotator-sign set was rated as a false positive'. A sign in the manual-annotator-sign set with no counterpart in the GPT-sign-set with at least 0.80 similarity was rated as a false negative'.\nTo evaluate sign categorization, a neurology domain expert manually reviewed GPT-4 and GPT-3.5-Turbo sign categorization for diseases for each classifiable sign in the validation dataset and rated them as correct' (true positive) or incorrect' (false negative). Signs considered uncategorizable by the domain expert were rated true negative', whereas unclassifiable signs assigned a category by GPT-4 were rated false positives'. The expert also manually reviewed sign normalization using the SOTA NLP method, GPT-4, and GPT-3.5-Turbo.\nFor sign normalization to be rated as correct,' both the normalized HPO term and the HPO ID had to be accurate. Signs that were considered unnormalizable by the domain expert were rated true negatives'. Unnormalizable signs normalized by GPT-4 were rated false positive.' For normalizable signs, matching was rated as true positive' or 'false negative.' Accuracy, precision, and recall were calculated using standard methods [43]."}, {"title": "IV. RESULTS", "content": "We performed high-throughput neurological phenotyping on 405 disease variants from 16 OMIM phenotypic series (Table I). Sign identification, sign categorization, and sign normalization were performed by GPT-3.5-Turbo or GPT-4 in three sequential submissions to the OpenAI API. GPT-3.5-Turbo and GPT-4 processing rates were 14.2 sec per disease and 16.4 sec per disease, respectively. Although higher throughput might be possible with a faster CPU, more than 90% of the time expended was due to the four API calls.\nThe GPT-4 model outperformed the GPT-3.5-Turbo model on several performance metrics (Table II). GPT-4 produced usable data for 283 diseases, whereas GPT-3.5-Turbo produced usable data for 207 diseases. GPT-4 identified more signs (5,595 compared to 4,227) and more unique signs (2,705 compared to 2,567) than GPT-3.5-Turbo. The Jaccard Index, a stringent measure of concordance requiring exact matches between the large language models and the manual annotators, was higher for GPT-4 (0.31) than GPT-3.5-Turbo (0.16).\nA more relaxed measure of concordance, the maximum similarity index (based on cosine similarity from spaCy and BioWordVec embeddings from Gensim), showed high maximal mean similarities for signs compared to manual annotators (93.1 for GPT-3.5-Turbo and 94.2 for GPT-4). 'Weak' matches (maximum similarity less than 0.80) were lower with GPT-4 than with GPT-3.5-Turbo. Compared to manual annotators, precision, recall, and F1 for sign identification were higher with GPT-4 than with GPT-3.5.\nThe OpenAI API interface assigned each sign to one of 30 high-level categories. A significant simplification of the feature space was achieved by categorization of signs, as illustrated by comparing the word clouds for CMT signs (Fig.5 with CMT categories (Fig. 5). The ability of GPT-3.5-Turbo and GPT-4 to correctly assign signs to high-level categories was manually checked by a neurology expert for signs in the disease validation set. The accuracy of the GPT-4 was higher than that of the GPT-3.5-Turbo on sign categorization (94.0% compared to 58.4%). Sign categorization allowed us to create heatmaps for each phenotypic series where rows were diseases and columns were phenotype categories as illustrated by Figs. 2 to Fig. 4. Distances between phenotypic series centroids can be plotted using PCA for dimension reduction. Fig. 7 shows an example of five phenotypic series centroids.\nSign normalization was evaluated for the disease validation set. The SOTA NLP method performed best at 90.6% accuracy, followed by GPT-4 at 57.9% accuracy, and GPT-3.5-Turbo at 44.8% accuracy."}, {"title": "V. DISCUSSION", "content": "We have developed a high-throughput pipeline that processes clinical text and identifies signs of disease. To support high-throughput, ease of use, and processing speed, the pipeline uses application programming interfaces (APIs) [44]. We used an API to retrieve summary text from OMIM and another API to allow GPT-4 to identify, categorize, and normalize signs. Clinical summaries from the OMIM database were utilized as our use case since the text is easily retrievable, rich with phenotypes, and not regulated as protected health information. However, these methods can be applied to text from other sources, including electronic health records, PubMed abstracts, full-text articles, and other clinical summaries. This work involved 'deep phenotyping,' which can be distinguished from other work on 'surface phenotyping.' In deep phenotyping, the granular signs of disease are identified and mapped to an ontology [1]. Surface phenotyping is a less exacting process that assigns disease codes such as International Classification of Diseases (ICD) to physician notes or other clinical text [7], [15], [45], [46].\nRecognizing (identifying signs) and normalizing (mapping signs to an ontology) are challenging tasks for traditional NLP methods [2], [22], [46]\u2013[49]. Progress has been made toward improving the recognition and normalization of medical concepts using transformers combined with specialized biomedical word embeddings [20], [21]. Large pre-trained language models provide a new approach to deep phenotyping (concept identification and normalization) that does not require additional training or a large corpus of manual annotations [27], [50]\u2013[52]. Our pipeline for high-throughput phenotyping performed three phenotyping operations: sign identification, sign categorization, and sign normalization. In general, GPT-4 performed these operations with high accuracy and outperformed GPT-3.5-Turbo (Tables III, and IV).\nSimilarly, Groza et al. [27] evaluated GPT models for phenotype concept recognition using the ChatGPT interface. Their study demonstrated that GPT-4 outpaced state-of-the-art methods in mention-level F1 scores of 0.7. Our work extends that of Groza et al. by demonstrating the utility of the GPT API to facilitate high-throughput phenotyping. In previous work, we have shown that GPT-4 can identify phenotypes in physician notes [28], [29], which is important for precision medicine [53], [54].\nGPT-4 exhibited some weaknesses in sign normalization, achieving an accuracy of only 57.9%. This task has been noted by others as particularly challenging for GPT-4 [27]. In comparison, a state-of-the-art (SOTA) NLP model that combined BioWordVec from Gensim with the spaCy NLP similarity method demonstrated significantly higher accuracy at 90.6%. While GPT-4 excelled at identifying plausible HPO terms for each input term, it was notably less accurate in providing the correct HPO IDs. In some instances, it even produced implausible HPO IDs. This discrepancy likely stems from GPT-4's design, which relies heavily on pre-training to infer HPO IDs rather than employing a direct lookup capability. Currently, GPT-4 does not have an inherent mechanism to verify or retrieve accurate HPO IDs from a database.\nMoreover, an inherent limitation of GPT models like GPT-4 is their non-deterministic nature. The choice of HPO ID for sign normalization can vary between different runs, even when the same input is provided [27]. This variability introduces inconsistencies that can be problematic in clinical applications where precision and reliability are paramount. Despite these limitations, GPT-4's ability to generate plausible HPO terms highlights its potential for improvement with enhancements such as integrating a lookup mechanism for HPO IDs or using hybrid models that combine the strengths of GPT-4 with deterministic systems like BioWordVec and spaCy.\nWe used GPT-4 to categorize the signs into 30 high-level categories. These high-level categories were chosen for their relevance to neurological phenotypes [55]. Although HPO has 28 high-level categories under Phenotypic abnormality [56], these categories are too broad to be useful in analyzing the phenotypes of neurological diseases. This categorization process significantly reduced the number of phenotypic terms needed to describe the diseases (compare Fig. 5 to Fig. 6). By assigning each phenotypic term to one of 30 high-level categories, we gained the ability to represent each disease in a phenotypic series as a row on a heatmap (Figs. 2 to 4). Heatmaps have also been used to visualize Orphadata disease phenotypes [55].\nOnce the phenotypic terms are acquired, a disease phenotype can be represented as a vector. Various methods to calculate similarity between these disease vectors are available [35], [37], [57]\u2013[60]. We used Principal Component Analysis (PCA) to reduce the dimensionality of these vectors to two dimensions (x and y), enabling us to visualize each disease as a marker on a scatter plot. To visualize distances between the phenotypic series, we represented each series as a centroid of its component diseases. Although Fig. 7 is representative, these methods can be applied to display phenotypic distances between any combination of diseases or phenotypic series.\nLarge language models, including GPT-4, show promise for high-throughput phenotyping of clinical text though some issues identified in this work warrant further investigation. The level of accuracy required by LLMs for clinical decision-making remains uncertain [61]. It is important to recognize that human annotators do not always agree perfectly [42], and even expert physicians are susceptible to diagnostic errors [62]. There is ongoing debate over whether health informatics tasks, such as phenotyping, are better suited to large general-purpose models or smaller, specially trained language models [63]. Concerns have been raised about the foundational weaknesses of large language models in healthcare, stemming from their limited training on EHR data [64]. Additionally, LLMs often struggle to process EHR data in tabular form (e.g., long tables of biochemical results found in EHRs) [65]. Groza et al. [27] have highlighted the stochastic nature of LLM outputs. If these models are to be used routinely in healthcare, issues of trust, privacy, equity, fairness, and confidentiality must be satisfactorily addressed [66], [67]. Furthermore, the problem of 'hallucinations' and 'confabulations' by LLMs remains unresolved [68].\nFuture work will fully assess the robustness and error-handling capabilities of our pipeline. While we tested GPT-3.5-Turbo and GPT-4, we did not compare their performance with other proprietary or open-source models. Scalability, cost analysis and stability studies are also required. Note that the neurological disease and phenotypic data utilized in this work were not protected health information, but privacy concerns must still be addressed.\nNonetheless, the case for applying large language models to high-throughput phenotyping is compelling [25], [27], [50]\u2013[52], [69]\u2013[71]. These models are fast, accurate, and ready to run 'out of the box.' Unlike traditional neural network models, they do not rely on an extensive corpus of manual annotations. These models should be generalizable to a variety of diseases without additional training. Current limitations in sign normalization can be addressed using techniques from augmented retrieval generation [72], by additional pre-training, or by creating small specialized models specifically for sign normalization. Large language models such as GPT-4 are expected to become the dominant method for high-throughput clinical text phenotyping."}]}