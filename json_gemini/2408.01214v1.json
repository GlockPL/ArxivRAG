{"title": "High-Throughput Phenotyping of Clinical Text Using Large Language Models", "authors": ["Daniel B. Hier", "S. Ilyas Munzir", "Anne Stahlfeld", "Tayo Obafemi-Ajayi", "Michael D. Carrithers"], "abstract": "High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automating high-throughput phenotyping of clinical text.", "sections": [{"title": "I. INTRODUCTION", "content": "MANUAL phenotyping of electronic health records is laborious [1], [2]. Precision medicine has intensified the need for high-throughput methods to support the acquisition and processing of vast volumes of unstructured medical-related data [3], [4]. High-throughput phenotyping remains challenging due to the complexity of the task and the volume of physician notes [5]\u2013[7]. Natural language processing (NLP) methods for identifying signs (concept recognition) in clinical text have evolved from rule-based and dictionary-based systems [8], [9], to machine learning and statistical models [10], [11], to deep learning methods such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) [12]-[15], and more recently to transformer architectures [16]-[21]. Barriers to the wider application of NLP methods to high-throughput phenotyping include limited accuracy, the need for large quantities of manually annotated data for training, and an inability to generalize an application from one domain of interest to another [2], [6], [7], [22].\n\nThe emergence of large language models (LLMs) offers an opportunity to address previously unsolvable NLP problems, including the high-throughput phenotyping of physician notes [23]\u2013[25]. LLMs belong to a class of foundation models which are inherently strong learners of heterogeneous data due to their large capacity, unified input modeling of different modalities, and improved multi-modal learning techniques [26]. LLMs have a superior ability to extract, summarize, translate, and generate textual information with only a few or even no prompt/fine-tuning samples [26]. Thus, they have been shown to perform well in high-throughput phenotyping [27] as they offer scalability and generalizability and require minimal additional training. Recent work in [28], [29] demonstrates their usefulness in processing large volumes of text in electronic health records (EHR) [28], [29]. They are also able to derive phenotypes from other text sources, such as PubMed abstracts and clinical summaries [27].\n\nA goal of the precision medicine initiative is to use patient phenotypes to guide treatments and improve outcomes [30]. Patient phenotypes must be computable before being entered into precision medicine machine learning models. Patient phenotypes are recorded in EHRs as unstructured text. The most widely used standard for recording phenotypes is Human Phenotype Ontology (HPO) [1], [31], [32]. In a medical setting, the patient reports symptoms to the physician, who then examines the patient to obtain clinical signs. Together, these signs and symptoms constitute the phenotype of the patient. For brevity, we will collectively refer to these signs and symptoms as simply signs. The usual signs of the disease are the phenotypes of the disease. Online Mendelian Inheritance in Man (OMIM) provides a view of genetic heterogeneity of similar phenotypes across the genome for related diseases. This is referred to as phenotypic series i.e. a collection of phenotypes (diseases or traits) that are caused by mutations in different genes but have similar clinical features. For example, in OMIM, dystonia phenotypic series include several types of dystonia with different genetic causes such as DYT6, DYT11, and DYT25 [33], [34]. Each type of dystonia in this phenotypic series has similar clinical features, such as involuntary muscle contractions and abnormal postures but differ in their genetic causes based on mutations in different genes (e.g., TOR1A, THAP1, SGCE, GNAL). The phenotypic series in OMIM helps to categorize and understand the genetic heterogeneity of multiple diseases aiding in diagnosis and treatment strategies. Patient phenotyping involves multiple steps, including sign identification (finding the sign in the text), sign categorization (assigning signs to high-level categories), sign normalization (mapping signs to terms in an ontology), and sign binarization"}, {"title": "II. DATA", "content": "The neurological disease and phenotypic data was retrieved from the OMIM database via their API (api.omim.org). Disease phenotypes are described in the clinical synopsis and the clinical features sections for each disease within the OMIM database. The clinical synopsis is a list of signs, symptoms, mode of inheritance, and age of onset. The clinical features section summarizes the published literature that underpins the phenotype of each disease. The OMIM API has separate calls for clinical features and clinical synopsis. The text from OMIM served as a surrogate for text from physician notes. Diseases in OMIM with similar phenotypes are grouped in a phenotypic series. OMIM currently has 582 Phenotypic Series, each with anidentifier beginning with PS. We evaluated 16 phenotypic series that spanned across 405 neurogenetic diseases (see Table I)."}, {"title": "III. METHODS", "content": "The high-throughput phenotyping pipeline extracts phenotypic terms in clinical text through the following steps (Fig. 1). Parameter details for the OMIM API and Open AI API calls are available on the project GitHub site.\n\nSign categorization plays a crucial role in model interpretability and performance. For example, HPO currently has 17,957 terms descended from the root term phenotypic abnormality. Given this complexity, feature reduction is necessary to make phenotypes interpretable [35]-[38]. One approach (which we adopt in this work) to feature reduction of phenotypes is to 'roll-up' many granular terms into a small number of high-level categories [39].\n\nThe goal of this project is to perform automated high-throughput phenotyping on OMIM clinical summaries as a surrogate for the phenotyping of physician notes. This use case has advantages in that the text is easily available via an application programming interface (API), is rich in phenotypes and not protected by health privacy laws. Moreover, the task is similar to the phenotyping of physician notes [29]. The operational requirements of high-throughput phenotyping applied to healthcare include high processing speed, high accuracy, scalability to large data volumes, generalizability to various diseases, maintenance of privacy, and resilience to imperfect data inputs. The functional requirements of such a system include the ability to identify signs in free text, assign signs to high-level categories, map (normalize) signs to concepts in a designated ontology, and vectorize signs for use in computational models. This work evaluates the capabilities of two LLMs (GPT-4 and GPT-3.5-Turbo) in identifying, categorizing, and normalizing signs in clinical narratives. As a proof of concept, we vectorized phenotype data obtained from OMIM to visualize neurological disease variability within a phenotypic series with heatmaps as well as distances between different phenotypic series with dimension-reduced scatter plots."}, {"title": "IV. RESULTS", "content": "We performed high-throughput neurological phenotyping on 405 disease variants from 16 OMIM phenotypic series (Table I). Sign identification, sign categorization, and sign normalization were performed by GPT-3.5-Turbo or GPT-4 in three sequential submissions to the OpenAI API. GPT-3.5-Turbo and GPT-4 processing rates were 14.2 sec per disease and 16.4 sec per disease, respectively. Although higher throughput might be possible with a faster CPU, more than 90% of the time expended was due to the four API calls.\n\nThe GPT-4 model outperformed the GPT-3.5-Turbo model on several performance metrics (Table II). GPT-4 produced usable data for 283 diseases, whereas GPT-3.5-Turbo produced usable data for 207 diseases. GPT-4 identified more signs (5,595 compared to 4,227) and more unique signs (2,705 compared to 2,567) than GPT-3.5-Turbo. The Jaccard Index, a stringent measure of concordance requiring exact matches between the large language models and the manual annotators, was higher for GPT-4 (0.31) than GPT-3.5-Turbo (0.16).\n\nA more relaxed measure of concordance, the maximum"}, {"title": "V. DISCUSSION", "content": "We have developed a high-throughput pipeline that processes clinical text and identifies signs of disease. To support high-throughput, ease of use, and processing speed, the pipeline uses application programming interfaces (APIs) [44]. We used an API to retrieve summary text from OMIM and another API to allow GPT-4 to identify, categorize, and normalize signs. Clinical summaries from the OMIM database were utilized as our use case since the text is easily retrievable, rich with phenotypes, and not regulated as protected health information. However, these methods can be applied to text from other sources, including electronic health records, PubMed abstracts, full-text articles, and other clinical summaries. This work involved 'deep phenotyping,' which can be distinguished from other work on 'surface phenotyping.' In deep phenotyping, the granular signs of disease are identified and mapped to an ontology [1]. Surface phenotyping is a less exacting process that assigns disease codes such as International Classification of Diseases (ICD) to physician notes or other clinical text [7], [15], [45], [46].\n\nRecognizing (identifying signs) and normalizing (mapping signs to an ontology) are challenging tasks for traditional NLP methods [2], [22], [46]-[49]. Progress has been made toward improving the recognition and normalization of medical concepts using transformers combined with specialized biomedical word embeddings [20], [21]. Large pre-trained language models provide a new approach to deep phenotyping (concept identification and normalization) that does not require additional training or a large corpus of manual annotations"}]}