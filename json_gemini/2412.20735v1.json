{"title": "HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving", "authors": ["Yang Li", "Dong Du", "Linfeng Song", "Chen Li", "Weikang Wang", "Tao Yang", "Haitao Mi"], "abstract": "We introduce HUNYUANPROVER, an language model finetuned from the HUN-\n  YUAN 7B for interactive automatic theorem proving with LEAN4. To alleviate\n  the data sparsity issue, we design a scalable framework to iterative synthesize\n  data with low cost. Besides, guided tree search algorithms are designed to enable\n  effective \"system 2 thinking\" of the prover. HUNYUANPROVER achieves state-\n  of-the-art (SOTA) performances on major benchmarks. Specifically, it achieves\n  a pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA re-\n  sults. It proves 4 IMO statements (IMO_1960_P2, IMO_1962_P2, IMO_1964_P2\n  and IMO_1983_P6) in miniF2F-test. To benefit the community, we will open-\n  source a dataset of 30k synthesized instances, where each instance contains the\n  original question in natural language, the converted statement by autoformaliza-\n  tion, and the proof by HunyuanProver.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have profoundly impacted mathematical\nreasoning and theorem proving in artificial intelligence. Despite notable progress in natural lan-\nguage domains, language models still encounter substantial challenges in formal theorem proving\n(e.g., using LEAN (Moura & Ullrich, 2021) or Isabelle (PAULSON, 1994)) probably due to the\nmassive search space of Olympiad-level theorem proving and limited data in such scenario. As\nthe results, even the most advanced models like GPT-40 Hurst et al. (2024) struggle with complex\nformal-statement proving (Xin et al., 2024a). A formal theorem-proving model should be capable of\nunderstanding both the syntax and semantics of a formal system, enabling it to generate valid next\nsteps within the system. More critically, it must integrate this capability with its abstract mathemat-\nical reasoning skills to perform effective and efficient deductions.\nWe propose HUNYUANPROVER, a framework designed to address the aforementioned challenges\nin automatic theorem proving. HunyuanProver takes two core modules: a scalable prover-data\ngenerator and guided tree-search algorithms. The prover-data generator only takes open-source data\nto train the initial autoformalizer and prover. The autoformalizer then converts a large amount of\nexisting math questions into the format of the target prover (e.g., LEAN4). The prover is then\niteratively improved on such data where new proof data is generated at each iteration to train the\nprover. At testing time, tree-search algorithms and multiple critic models are designed to perform\n\"slow thinking\" that is empirically essential for solving complex theorem-proving tasks.\nEvaluations show that HUNYUANPROVER yields an accuracy of 68.4% on the miniF2F benchmark.\nIn addition, we also observe several key findings: 1) using explicitly trained critic for tree-search\nguidance is helpful; 2) the scale of finetuning data for theorem proving is critical, thus designing\nefficient data scaling framework is important; 3) data curation and selection is important as well\nwhen there is sufficient amount of training data.\nOur key contributions includes:\n\u2022 We introduce a scalable pipeline that utilizes open-source prover data and massive math problems\nin natural language to generate a large number of new training data for automatic theorem proving."}, {"title": "2 Scalable Data Generation for Prover Improving", "content": "One major bottleneck for automated theorem proving is the lack of training data. For example, as\none of the largest open-source LEAN4 datasets, mathlib4 (Moura & Ullrich, 2021) only contains\naround 50k theorems(with tactics) for training. This is far from sufficient to train a stronger prover\ngiven the difficulty of automatic theorem proving. On the other hand, massive high-quality math\nproblems in natural language have been released in recent years (Yu et al., 2024; Luo et al., 2023).\nOur efforts fall into two critical aspects to scale the training data for automatic theorem proving: aut-\noformalization (Section 2.1) that maps a natural language problem (Section 2.2) into LEAN format\nstatement, and tactic generation for iterative theorem proving."}, {"title": "2.1 Autoformalization Data Generation", "content": "For autoformalization, we start with 130k high-quality natural language to LEAN statement pairs,\nwhich includes 50k from lean workbook (Ying et al., 2024) and 80k from MMA (Jiang et al., 2024).\nWe first translate the natural language part of the 130k data into Chinese to double the dataset size,\nand with such data we train a autoformalization model, it can translate both English and Chinese\nproblems to lean4 format statement.\nIn the next step, 30 million internal math problems in natural language are converted into formal\nstatement by the autoformalization model. For each natural language problem, we sample 8 outputs\nwith different temperatures. We obtain a dataset D\u00ba of 20 million LEAN statements after filtering\nout these do not conform with LEAN grammar or do not satisfy other rules adopted by previous\npractices (Ying et al., 2024).\nIn addition to using internal data, we also utilized open-source natural language mathematical data\nNuminaMath CoT(LI et al., 2024)."}, {"title": "2.2 Prover Improving via Iterative Tactic-Level Proving Data Generation", "content": "We design a iterative framework that takes a LEAN engine F and statement dataset D\u00ba from aut-\noformalization to generate new tactic data for prover improving. Specifically, in iteration t, we\nleverage best-first search (Section 3.1) algorithm with the prover from the previous iteration \u03c0\u03c4-1\non all unsolved statements so far in D. We collect the proving trajectories (e.g., t) for newly solved\nstatements (e.g., q) if there is any:\n$D_t = \\{(q, \\tau) \\mid q \\in D^0 - D_{t-1}, \\tau \\sim BFS(q), \\tau \\neq null\\} \\cup D_{t-1}$ (1)\nThen the prover is updated using rejection finetuning (RFT) with the proving trajectories in Dt after\nfiltering out easy statements solved in early iterations. The initial prover \u03c0\u03bf is trained on public data\nincluding mathlib4. After more than 10 rounds of iteration, more than 20 million tactic-level data is\nobtained.\nEnhancing Diversity As indicated by previous work (Xin et al., 2024c), prover diversity is im-\nportant due to the massive search space of theorem proving. We further develop two methods to\nenhance the prover diversity within the iterative data-generation framework. For the first method,\nwe design rules to convert the last state of an unfinished proving trajectory into a new statement.\nIn this way, more diverse proving data is obtained for prover training. For the second method, we\ncollect data from proving more challenging statements, including those Olympiad-level algebraic\ninequalities Wei et al. (2024) and lean workbook Ying et al. (2024)."}, {"title": "3 Guided Tree Search", "content": "As described in Section 2, our task involves iterative interaction with LEAN as the environment,\nwhere for each iteration the policy predicts a new tactic given from a state in the proving process.\nWe abstract this process as tree search where a state si in the proving process corresponds to a tree\nnode ni with the input statement q being the root no. An edge that points from ni to nj represents\napplying a tactic on node (state) ni to yield node (state) nj. To handle this problem, we design two\nmajor tree search algorithms as described in Section 3.1. We also design several critics, as shown in\nSection 3.2, to guide these algorithms."}, {"title": "3.1 Tree Search Algorithms", "content": "We explore two search algorithms: best-first search (BFS) and Monte-Carlo tree search (MCTS) for\nthe tactic-level iterative statement proving process.\nBest-First Search We first study BFS due to its simplicity and effectiveness. As demoed in the\nleft part of Figure 1, BFS can be viewed as an iterative process of selection and expansion. The\nsearch process continues until either the statement q is successfully proved or the maximal number\nof iteration T is reached.\nIn the selection step, the node \u00een with the highest critic score is selected (without return) from the set\nof active nodes N:\n$\\hat{n} = \\arg \\max_{n \\in N} CRITIC(n)$ (2)\nwhere CRITIC represents the critic function. In the expansion step, the policy \u03c0 samples a set (C) of\nK candidate tactics under n:\n$\\hat{C} = \\{\\hat{c_i} | \\hat{c_i} \\sim \\pi(q, n), i \\in [0, ..., K]\\}$ (3)\nEach tactic in C is then executed against the LEAN engine to yield a new tree node if the tactic is\nvalid under state n. After removing those that are identical to previously explored tree nodes, the\nremaining are merged into the active node set N. We check two nodes are identical simply by string\nmatching.\n\u03b7MCTS Thought being simple and effective in general, BFS has several limitations on handling\ncomplex search problems. One limitation is that each node is only visited once with a fixed amount\nof expansion budget. Besides, BFS only takes the critic score as guidance, thus it can suffer from\nany bias and misjudgment inherent in the critic model.\nWe additionally adapt \u03b7MCTS (Tian et al., 2024) to handle such limitations. As shown in Fig-\nure 1, the original \u03b7MCTS algorithm takes 4 steps of selection, expansion, simulation and back-\npropagation in each iteration. Here we remove the step of simulation, leaving it for future work. As"}, {"title": "3.2 Critics for Search Guidance", "content": "Critic modeling is a central component in tree search as it provides the guidance. We design three\ntypes of critic models to guide the search process.\nPolicy Confidence (PC) We first leverage policy confidence as guidance for a cold start of guided\nsearch due to limited tree-search data for training critic models at the beginning. Particularly, for\ntactic c under state n in the proving process of statement q, we define its policy confidence $f^\\pi(c)$ as\ntoken-level average log probability:\n$f^\\pi(c) = \\frac{1}{|c|} \\sum_{j=1}^{|c|} log p(c_j | q, n, c_{<j})$ (7)\nwhere |c| is the number of tokens in c.\nProcess Reward Model (PRM) The process reward model (PRM), denoted as $v_\\theta^\\pi(q, n)$ (parame-\nterized by $\u03b8$), represents the possibility of tree node n for proving statement q when starting from it\nand following policy \u03c0 thereafter.\nTo train a parameterized PRM $v_\\theta$ using statement set $D = [q^1,...]$, we first generate a search\ntree for each statement qk by following policy \u03c0 under the guidance of the critic from the previous\niteration. Next, for each node $n_i^k$ in the search tree, a score is assigned to reflect its quality. Ideally,\nhuman experts are hired for rating the node quality, but the cost is dramatic. Following Wang et al.\n(2024c), we simply assign a score of +1 or -1 to each node by indicating whether it can reach the final\nstate that indicates successful proving of qk. This approximation has been proven effective in the\nexperiments of Wang et al. (2024c). As the results, we obtain a PRM dataset $D_\\pi = [(q^k, n_i^k, l), ... ]$\nwith l indicating the score for node n.\nWith dataset $D_\\pi$, the PRM is then optimized by minimizing the mean squared error for each node:\n$\\theta = -\\mathbb{E}_{(q,n,l)\\sim D_\\pi} (v_\\theta(q, n) - l)^2$ (8)\nSimilar to Wang et al. (2024c), $v_\\theta$ is a LLM with an MLP layer on top to output a scalar for each\ntoken. We use the scalar prediction at the last token of each state as the value."}, {"title": "Distance Critic (DC)", "content": "PRM only captures the possibility of proving the input statement q starting\nfrom a tree node n, therefore we further design a distance-based critic model to predict the estimated\nremaining number of steps to prove q from n.\nOne naive solution can be training the critic model to directly predict the distance, while it is likely\nto suffer from data sparsity issue. When the state is very complex, it is very difficult to predict an\naccurate number of the remaining steps. We train the critic model to predict not only the exact num-\nber of remaining steps but also the range in which this number falls, represented using a balanced\nbinary tree structure. This approach helps mitigate the data sparsity issue by enabling the model to\nmake predictions in a hierarchical, coarse-to-fine manner. Specifically, the critic model is trained to\nidentify a path on the balanced binary tree, progressively narrowing down to the exact number of\nremaining steps."}, {"title": "4 Experiments", "content": "We evaluate theorem-proving performance on the following benchmarks to pinpoint\nthe effectiveness of each proposed module:\n\u2022 MiniF2F (Zheng et al., 2022) examines LLM's automatic formal problem-solving skills targeted\nat high-school-level exercises and competitions, such as AMC, AIME, and IMO, with a particular\nfocus on algebra and number theory. The benchmark comprises 244 validation problems and 244\ntest problems."}, {"title": "Inference", "content": "We use the LEAN engine from LeanDojo (Yang et al., 2024) as the engine to conduct\nboth tactic data generation and benchmark evaluation. Whole and per-step timeout for tactic exe-\ncuting in LEAN are set to 3600 seconds and 60 seconds, respectively. At most 800 search steps\nare conducted for both BFS and MCTS. For each search step, K = 8 tactics are sampled from the\ngiven LEAN state under temperatures 0.7, 0.8, 1.0 and 1.1, where two tactic are sampled under each\ntemperature. These temperature values are empirically decided."}, {"title": "Finetuning Hyperparameters", "content": "Our prover is obtained by fine-tuning a HUNYUAN 7B model on\nour self-generated tactic data. During fine-tuning, at most 4 epochs are conducted and checkpoint\nis selected based on miniF2F valid set. The maximum sequence length, learning rate, minimal\nlearning rate and batch size are set to 4096, 2 \u00d7 10\u22125, 1 \u00d7 10\u22126 and 256, respectively. Cosine\nlearning schedule is used."}, {"title": "Comparing Systems", "content": "We compare HunyuanProver with former state-of-the-art systems. Among\nthem, Lean-STaR (Lin et al., 2024) and InternLM2.5-StepProver (Wu et al., 2024) are interactive\nstep-level proving methods, while on the other hand, DeepSeek-Prover-V1.5 (Xin et al., 2024b) is a\nwhole-proof generation method."}, {"title": "4.2 Results and Analysis", "content": "Table 1 shows the comparison between HunyuanProver with other existing state-of-the-art systems.\nAmong these systems, our HunyuanProver with distance-tritic as guidance shows the best accu-\nracy on miniF2F, advancing the previous SOTA system (InternLM2.5-StepProver+BFS+CG) by\n2.5% points using less search budget. Though InternLM2.5-StepProver+BFS+CG is slightly better\nthan DeepSeek-Prover-V1.5-RL+MCTS, the later does not take any critic for guidance. We believe\nMCTS can be more beneficial to the task of automatic theorem proving, and this potential has not\nbeen fully revealed. In addition, we can also probe the importance of critic guidance by incorporat-\ning DCG on HunyuanProver, which shows 3.6%-point gain."}, {"title": "Effectiveness of Iterative Tactic Data Generation", "content": "Figure 3 visualizes the changes regarding\nminiF2F-test accuracy and the amount of training data during the prover improving process based on\niterative tactic data generation. We initially see performance boosts in early iterations until version\nv8, and then minor improvements are observed by further increase the number of training tokens\nfrom roughly 2.75B (v8) to 4.25B (v12). After v12, we remove some data generated from early\niterations(before v8), Most of the removed training data are relatively easy statements. We can see\nthe performance boost is achieved by removing these easy data. This highlights the importance of\ndata selection in the iterative improving process."}, {"title": "Effectiveness of Different Critics and Tree Search Methods", "content": "As shown in Table 2, we conduct an\nablation study on different search and critic combinations under three different versions of Hunyuan-"}, {"title": "5 Conclusions", "content": "In this report, we present HunyuanProver, a system that enhances automatic theorem-proving ca-\npabilities using LEAN through iterative tactic data generation and guided tree search. The iterative\ntactic data generation method expands the training dataset by nearly 40-fold, resulting in a signif-"}]}