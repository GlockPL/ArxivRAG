[{"title": "Memory-efficient Training of LLMs with Larger Mini-batches", "authors": ["Dang Nguyen", "Wenhan Yang", "Rathul Anand", "Yu Yang", "Baharan Mirzasoleiman"], "abstract": "Training with larger mini-batches improves the performance and convergence rate\nof training machine learning models. However, training with large mini-batches\nbecomes prohibitive for Large Language Models (LLMs) with billions of param-\neters, due to the large GPU memory requirement. To address this problem, we\npropose finding small mini-batches that simulate the dynamics of training with\nlarger mini-batches. Specifically, we formulate selecting smaller mini-batches of\nexamples that closely capture gradients of large mini-batches as a submodular\nmaximization problem. Nevertheless, the very large dimensionality of the gradi-\nents makes the problem very challenging to solve. To address this, we leverage\nideas from zeroth-order optimization and neural network pruning to find lower-\ndimensional gradient estimates that allow finding high-quality subsets effectively\nwith a limited amount of memory. We prove the superior convergence rate of\ntraining on the small mini-batches found by our method and empirically show its\neffectiveness. Our method can effectively reduce the memory requirement by 2x\nand speed up training by 1.3x, as we confirm for fine-tuning Phi-2 on MathIn-\nstruct. Our method can be easily stacked with LoRA and other memory-efficient\nmethods to further reduce the memory requirements of training LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in a variety of tasks, ranging\nfrom machine translation to conversational AI. However, pre-training and fine-tuning LLMs with\nbillions of parameters requires a large amount of compute and GPU memory not only to store the\nparameters but also to compute gradients and optimizer states, such as momentum and variance in\nAdam, which is often used for training LLMs. For example, fine-tuning a small language model,\nsuch as Phi-2 with 2.7B parameters, with a batch size of 128 requires at least 44 GB of GPU memory.\nThe large memory requirement makes it prohibitive to train such models with larger batch sizes,\nwhich can effectively improve the convergence of training and performance of the models. This\nraises a key question: can we train LLMs with larger batch sizes, using a limited amount of GPU\nmemory?\nTo address this problem, many memory-efficient techniques have been recently proposed, mainly\nto enable efficient fine-tuning of pre-trained language models. At a high level, such methods try\nto find a smaller set of parameters [Adelman et al., 2021], or find low-rank [Hu et al., 2021, Zhao\net al., 2024] or quantized [Dettmers et al., 2022] weights or optimizer states to train the model in\na memory-efficient manner. There have also been efforts to adapt gradient-free optimization for\ntraining LLMs [Malladi et al., 2023]. However, most memory-efficient techniques cannot achieve\na performance comparable to that of training the full model parameters or considerably increase the\ntraining time of LLMs."}, {"title": "2 Related Work", "content": "Memory-efficient training of LLMs. To address the large memory requirements of training LLMs,\nseveral methods have been recently proposed. LoRA [Hu et al., 2021] freezes the pre-trained model\nweights and trains two low-rank adaptor weight matrices to adapt the weights of each layer. Despite\ngaining memory efficiency, LoRA suffers from a performance drop compared to training with full-\nrank matrices. To improve upon this, several variations of LoRA [Liu et al., 2024, Renduchintala\net al., 2023, Xia et al., 2024b] have been proposed, which aim to preserve the training dynamics of\nfull-parameter training. However, these changes also lead to increased computational costs. GaLore\n[Zhao et al., 2024] reduces the memory cost of optimizer states during pre-training and fine-tuning\nby calculating the gradients and projecting them into a low-rank space.\nSeveral methods approximate backpropagation by sparsifying gradients [Frantar and Alistarh, 2023],\nsubsampling the computational graph [Adelman et al., 2021], gradient check-pointing [Chen et al.,\n2016], and quantization of weights and optimizer states [Dettmers et al., 2022]. However, these ap-\nproaches can incur large approximation errors and cause performance drops. Zeroth-order gradient\napproximation has also been used for memory-efficient training. MeZO [Malladi et al., 2023] builds\non the traditional zeroth-order methods to estimate gradients using two forward passes, significantly\nreducing the memory cost of LLM fine-tuning to the same level as LLM inference. However, train-\ning with MeZo cannot reach a comparable performance as training the full model parameters.\nOur method can be easily stacked with existing memory-efficient methods to improve convergence\nand further reduce memory requirements.\nData selection for training LLMs. Data selection for training LLMs has garnered significant at-\ntention due to its potential to enhance model performance while reducing computational costs. For\npre-training, training on examples with middle Perplexity rankings is shown beneficial [Marion\net al., 2023]. Clustering based on embeddings of a pretrained model and sampling from the clusters\nto drop redundancies has been also investigated [Tirumala et al., 2024].\nFor fine-tuning, training on manually crafted high-quality instruction/response pairs has shown\nhighly beneficial [Zhou et al., 2023a]. Building on this observation, data selection using LLMs\nsuch as chat-GPT or training on textbooks is proposed [Chen et al., 2024, Eldan and Li, 2023, Li\net al., 2023a,c], and metrics such as diversity [Bukharin and Zhao, 2023, Du et al., 2023], and diffi-\nculty [Bhatt et al., 2024, Marion et al., 2023, Zhou et al., 2023b] are shown relevant. Using influence\nfunctions to select a subset of the fine-tuning data that is most beneficial for a given validation set\nhas been also explored [Xia et al., 2024a]. Existing methods select data in a one-shot manner before\nfine-tuning, and either require access to another open LLM or a large preprocessing time to fine-"}, {"title": "3 Problem Formulation and Background", "content": "We consider the task of supervised fine-tuning a large language model $f$ on data $D$, containing a\nset of $I$ prompts and ground-truth responses. Formally, denoting the tokens for the $i$th prompt by\n$X_i = [X_{i,1}, X_{i,2},\u00b7\u00b7\u00b7]$ and the tokens in the corresponding response by $Y_i = [Y_{i,1}, Y_{i,2},\u2026\u2026\u2026, Y_{i,T\u03b5}]$, we\nwish to minimize the following loss function:\n$\n\\theta^* \\triangleq \\arg \\min_\\theta L_D(\\theta), \\text{ s.t. } L_D(\\theta) = \\sum_{i=1}^I \\sum_{t=1}^{T} \\log[Pr(Y_{i,t+1}|x_i, Y_{i,1\u2026\u2026.t}, \\theta)].\n$\n(1)\nThe standard approach to minimize the above loss function is using mini-batch gradient methods\nsuch as mini-batch SGD, which iteratively samples random mini-batches $M_t$ of $b$ examples scaled\nby the learning rate $\u03b7$, and updates the model parameters in the negative direction of the gradient of\nthe mini-batch.\n$\n\\theta_{t+1} = \\theta_t - \\eta g_t, \\text{ s.t. } g_t = \\frac{1}{|M_t|} \\sum_{i\\in M_t} \\nabla L_i(\\theta),\n$\n(2)\nwhere $L_i(\u03b8)$ is the loss of the $i$th prompt. Other stochastic optimizers like Adam [Kingma and\nBa, 2014] adapt the learning rate across dimensions by scaling the gradient updates by square roots\nof exponential moving averages of squared past gradients. In doing so, they reduce the learning\nrate across sharp dimensions and increase the learning rate across flatter dimensions to improve\nconvergence. In general, training with larger mini-batches effectively improves the convergence rate\nand performance of the trained models.\nFor random mini-batches that are unbiased estimates of the full gradient, i.e., $E_{i\\in I}[\u2207L_i(0)] =$\n$\u2207L(0)$, as long as the mini-batch size $|M_t|$ = $b$ is not too large, the convergence rate of mini-batch\nSGD directly scales with a factor of $1/b$. Formally, for a non-convex $L$-gradient Lipschitz function,\nmini-batch SGD with a small enough step size will visit an $\u03b5$-stationary point with a high probability\nat least once in the following number of iterations [Ghadimi and Lan, 2013]:\n$\n\\mathcal{O}\\Big(\\frac{L(L(\\theta_0) - L^*)}{\\epsilon^2}(1 + \\frac{\\sigma^2}{b\\epsilon^2})\\Big)\n$\n(3)\nwhere $E_{i\\in I}[(\u2207L_i(0) - \u2207L(0))^2] < \u03c3^2$ is the variance of the individual gradients and b is the mini-\nbatch size. Effectively, the variance of the mini-batches scales with the size of the mini-batches\nand directly improves the convergence rate. The effect of the mini-batch size on other stochastic\noptimizers is similar. Nevertheless, for LLMs with billions of parameters, training with large mini-\nbatches requires very large GPU memory and becomes prohibitively expensive. Hence, in practice\none needs to train with a small mini-batch size.\nTo improve the convergence and performance of training LLMs, our goal is to find small mini-\nbatches that closely capture the training dynamics with larger mini-batches. Formally, we wish to\nfind a set of smaller mini-batches {$S_1, S_2, \u2026\u2026 $} of size $|S_t|$ = $r < b = |M_t|$ that are unbiased and\nhave a similar variance to that of larger mini-batches of size $b$. That is,\n$\ng_t = \\frac{1}{|S_t|} \\sum_{i \\in S_t} \\nabla L_i(\\theta) = g_t, \\quad \\text{Vars}_{M_t}[g_t] = \\text{Var}_{S_t}[g] = \\frac{\\sigma^2}{b},\n$\n(4)\nIf such mini-batches can be found efficiently, they can (1) directly improve the convergence rate of\nmini-batch SGD and subsequently improve the performance; (2) reduce the memory requirement\nand speed up training by reducing the time required for backpropagating over the large mini-batch.\nDue to the high cost of pre-training, we mainly focus on fine-tuning LLMs in this work, but our\nproblem formulation and approach applies to pre-training LLMs, without further modification."}, {"title": "4 Method", "content": "In this section, we discuss finding smaller mini-batches of examples that can simulate the dynamics\nof training with large mini-batches."}, {"title": "4.1 Finding the Small Mini-batches", "content": "The key observation is that since larger random mini-batches Mt are unbiased and have a small\nvariance, if we can find small mini-batches St such that each St closely matches the gradient of\nthe large mini-batches Mt, then the small mini-batches will also be unbiased and will have a small\nvariance of \u03c3\u00b2/b. Formally, we wish to solve the following optimization problem at every iteration\nof training with mini-batch stochastic optimization methods, such as mini-batch SGD and Adam:\n$\nS_t \\in \\underset{S_t \\subset M_t, |S_t| \\leq r}{\\arg \\min} ||g_t - g_t||.\n$\n(5)\nIn doing so, training on St instead of Mt yields the benefits of training with larger mini-batches,\nincluding improved convergence and performance. Improved mini-batch selection has recently been\nshown to boost the convergence of training vision classifiers [Yang et al., 2023]. Nevertheless,\nsolving Problem (5) for LLMs requires addressing several new challenges, as we will discuss below.\nProblem (5) is NP-hard. However, a near-optimal subset can be found efficiently by maximiz-\ning the following monotone submodular\u00b9 facility location function [Mirzasoleiman et al., 2020a,b,\nPooladzandi et al., 2022]:\n$\nS_t^* \\in \\underset{S_t \\subset M_t, |S_t| \\leq r}{\\arg \\max} \\sum_{i \\in M_t} \\max_{s \\in S_t}[C - ||\\nabla L_i(\\theta) - \\nabla L_s(\\theta) ||],\n$\n(6)\nwith a large constant $C$ and assigning a weight $\u03b3_s = \\sum_{i \\in M_t} \\mathbb{I}(\\arg \\min_{s \\in S_t} ||\u2207L_i(\u03b8) \u2212\u2207L_s(\u03b8) || =$\n$s)$ to every example $s \u2208 S_t$. The weighted gradient of the subset $g_t = \\sum_{s \u2208 S_t} \u03b3_s\u2207L_s$ then closely\ncaptures the gradient of the larger mini-batch $g_t$.\nA monotone submodular function can be efficiently maximized by applying the greedy algorithm\n[Wolsey, 1982]. The greedy algorithm commences with an empty set $S = \\emptyset$, and at each step $i$, it\nselects an element $e \u2208 M_t$ that maximizes the marginal utility $F(e|S) = F(S \u222a {e}) \u2212 F(S_i)$.\nFormally, $S = S_{i-1} \u222a {\\arg \\max_{e \\in M_t} F(e/S_{i-1})}$. The time complexity of the greedy algorithm is\n$O(brq)$ where identify $r = |S_t|$, $b = |M_t|$ and $q$ is the complexity of calculating $F$. The greedy\nalgorithm can be further expedited through lazy evaluation [Minoux, 2005].\nChallenges of finding subsets for LLMs. The facility location formulation in Eq. (6) allows find-\ning a near-optimal subset St that closely replicates the gradient of Mt. Nevertheless, to calculate\nthe value of the function F, it requires calculating pairwise distances between gradients of all ex-\namples in the large mini-batch Mt, which in turn requires calculating the gradients of all examples\nin Mt. However, this does not yield any memory efficiency or speedup. In addition, even if one\ncalculates the full gradients, their very large dimensionality prevents calculating meaningful sim-\nilarities. Indeed, pairwise similarities in such high-dimensional gradient vectors become vacuous,\nand hence a high-quality subset cannot be extracted. In summary, there are two main challenges\nin finding smaller mini-batches that closely capture the gradient of large mini-batches: (1) finding\nlower-dimensional gradient estimates without having to explicitly calculate the full gradient of ex-\namples in Mt, and (2) calculating the lower-dimensional gradients efficiently with a limited about\nof memory. Next, we will discuss addressing each of these challenges in detail."}, {"title": "4.2 Finding Lower-dimensional Gradient Approximations", "content": "In this section, we address the challenge of finding lower-dimensional gradient estimates efficiently.\nFirst, we propose a memory-efficient zeroth-order approach to obtain lower-dimensional gradient\nestimates. Then, we discuss how we can further reduce the dimensionality of the gradient estimates\nto enable finding a high-quality subset efficiently with a limited amount of memory."}, {"title": "(1) Obtaining Memory-efficient Zeroth-order Lower-dimensional Gradient Estimates", "content": "First, we discuss calculating memory-efficient and lower-dimensional gradient estimates. To do so,\nwe use Simultaneous Perturbation Stochastic Approximation (SPSA) [Spall, 1992] to get memory-\nefficient gradient estimates. SPSA estimates the gradient as:\n$\n\\widehat{\\nabla L_i(\\theta)} = \\frac{L_i(\\theta + \\epsilon z) - L_i(\\theta - \\epsilon z)}{2\\epsilon}z \\approx zz^\\top\\nabla L_i(\\theta),\n$\nwhere $z \u2208 R^d$ with $z \u223c N(0, I_d)$, and d is the number of model parameters and $\u03b5$ is the perturbation\nscale. The $n$-SPSA gradient estimate averages $\\widehat{\\nabla L_i(\\theta)}$ over $n$ randomly sampled $z$.\nSPSA requires two forward passes through the model to compute the gradient estimate for each\nexample. As $\u03b5$ \u2192 0, the SPSA estimate provides a rank-1 reconstruction of the gradient. During\ntraining, $n$ is a hyperparameter and can follow a schedule [Bollapragada et al., 2018, Cai et al.,\n2022]. Nevertheless, for language models $n = 1$ is shown to be the most effective [Malladi et al.,\n2023]. The vanilla calculation of $\\widehat{\\nabla L_i}$ costs twice the memory of inference, as it needs to store\n$z \u2208 R^d$. However, [Malladi et al., 2023] proposed a memory-efficient way (MeZo) to calculate\n$\\widehat{\\nabla L_i}$, by first sampling a random seed $s$, and then for each of $z$'s uses in Eq. (7), the random number\ngenerator is reset by $s$ and resample the relevant entry of $z$. Using this in-place implementation,\ncalculating $\\widehat{\\nabla L_i}$ has a memory footprint equivalent to the inference memory cost.\nTraining with MeZo cannot reach a comparable performance as training the full model parameters.\nNevertheless, it is accurate enough to find the subsets effectively.\nUsing MeZo to get the last-layer gradients. While MeZo enables calculating full gradients in\na memory-efficient manner, full gradients are too high-dimensional to allow finding the subsets\neffectively. Hence, we need to lower the dimensionality of the gradients before they can be used to\nfind a subset.\nTo do so, we only calculate the last layer gradients, which capture most of the gradient variation\n[Katharopoulos and Fleuret, 2018]. To get the last layer gradient with MeZo, our key idea is to only\nsample random perturbations for parameters corresponding to the last layer (V projection) in the\nperturbation vector $z$, and use zero for the other entries. That is,\n$\n\\widehat{\\nabla L_i(\\theta_L)} = \\frac{L_i(\\theta + \\epsilon z_L) - L_i(\\theta - \\epsilon z_L)}{2\\epsilon}z_L,\n$\n(8)\nwhere $z_L \u2208 R^{dL}$ with $z = [0^{d-d_L}, z_L]$ and $z_L \u223c N(0, I_{d_L})$, and $d_L$ is the dimensionality of the\nflattened last layer weight matrix.\nThe above zeroth-order last-layer gradient estimates can be calculated very efficiently in just one\nforward pass. To do so, we first make a forward pass to get the activations $X_{L\u22121}$ of the penultimate\nlayer of the network. Then, we only perturb the last-layer parameters twice to calculate $\\widehat{\\nabla L_i}$\nbased\non the pre-calculated $X_{L\u22121}$. The time of getting the lower dimensional last-layer gradients\nwill\nbe dominated by the time of computing $X_{L\u22121}$, and the cost of the second step is negligible. For\ncalculating the loss of the perturbed last-layer parameters in a memory-efficient manner, we use a\nfixed seed to generate the same perturbations $z^L$ multiple times, as is done in MeZo. Hence, the\nmemory overhead is also negligible.\nWhile the dimensionality of the last-layer gradients is considerably lower than the full gradients, they\nare still too high-dimensional to allow finding high-quality subsets. For example, the dimensionality\nof the last layer (last V projection) of Phi-2 model is 2560 \u00d7 2560 = 6,553, 600 when training the\nfull parameters and 2560 \u00d7 128 = 327,680 when using LoRA. This makes calculating the pair-\nwise gradient similarity between examples prohibitive and ineffective in practice. Indeed, in such a\nhigh-dimensional space, small noisy dimensions dominate the similarity calculation and make them\nvacuous. This prevents selecting high-quality subsets."}, {"title": "(2) Further Reduce the Dimensionality by Sparsifying the Last-layer Gradients", "content": "To further reduce the dimensionality of the gradient estimates, we sparsify the last-layer gradients to\nobtain lower-dimensional gradients in a memory-efficient way. Our goal is to find a small number\nof dimensions in the last (V projection) layer which yields a similar loss reduction to that of the full\ngradient, at one parameter update step. As shown below, the gradient norm indicates, to the first"}, {"title": "4.3 Method: Select Small Mini-batches (SSM)", "content": "We summarize our method here. At every iteration of training, we sample a large mini-batch of ex-\namples. Then, we use MeZo to find memory-efficient estimates of last-layer gradients for examples\nin the large mini-batch. This can be done efficiently with just one forward pass. We further reduce\nthe dimensionality of the gradient estimates by selecting the top $k$ dimensions with the highest mag-\nnitude. We calculate the pairwise similarity between the lower-dimensional gradient estimates using\n$l_1$ distance. $l_1$ distance preserves the distances better in high-dimensional spaces and is consistently\nmore preferable than Euclidean distance [Aggarwal et al., 2001]. Finally, we find a small mini-batch\nfrom every large random batch during the training by applying the greedy algorithm to maximize\nthe submodular facility location function in Eq. (6). We do not assign weights ($\u03b3$) to gradients of\nexamples in the subset, to effectively reduce the redundancy in the data and balance different groups\nof examples.\nThe following theorem shows that as long as the smaller mini-batches closely mimic the gradient\nof the large batches, training with SSM imitates the training dynamics with large batches. Thus, it\nimproves convergence and performance, compared to training with small random mini-batches.\nTheorem 4.1. As long as $E[||gt - g_t||] \u2264 ||\u2207L(0_t)||$ and learning rate is small enough, minimizing\na non-convex $L$-gradient Lipschitz function with mini-batches of size $r < b$ found by SSM from\nlarger mini-batches of size $b$ will visit an $\u03b5$-stationary point with a high probability at least once in"}, {"title": "5 Experiments", "content": "In this section, we evaluate the performance of our method, SSM, for fine-tuning LLMs. We com-\npare the performance, memory requirement, and wall-clock training time of training with small and\nlarge random mini-batches, with that of our method. We also do an ablation study showing the\neffectiveness of different design choices of SSM."}, {"title": "5.1 Settings", "content": "Datasets. We fine-tune the MathInstruct [Yue et al., 2023] dataset for the mathematical reasoning\ntask. This dataset consists of about 260K instruction tuning examples, which are curated from 13\nopen-source math datasets. MathInstruct has a broad coverage of mathematical fields and a wide\nrange of difficulty levels. Fine-tuning LLMs on MathInstruct has shown state-of-the-art performance\non a variety of standard math evaluation benchmarks.\nModels. We utilize the Phi-2 model [Li et al., 2023b], a 2.7 billion-parameter language model by\nMicrosoft Research. Phi-2 achieves high performance on various benchmarks despite its smaller\nsize. Utilizing high-quality, curated training data and advanced scaling techniques, Phi-2 outper-\nforms much larger models on multi-step reasoning tasks.\nTraining details. Following the setup used in [Yue et al., 2023], we adopt a training regime with a\nlearning rate of 2e-5 and a cosine scheduler with a 3% warm-up period, i.e. the learning rate linearly\nincreases from 0 to 2e-5 over the first 3% of training steps, then follows a cosine decay to 0 at the\nend of training. We set a maximum sequence length of 512. For all experiments on MathInstruct,\nwe standardize the number of gradient steps to correspond to 1K, unless explicitly specified. We\nuse LoRA with a rank of 128, alpha of 512, and dropout rate of 0.05. For Phi-2, we apply LORA\nfor all attention matrices (QKV_proj) and two fully connected layers. All experiments are run on 4\nNVIDIA A40 GPUs. We ran each experiment three times.\nEvaluation datasets. We adopt the framework established in [Yue et al., 2023], leveraging a vari-\nety of popular datasets across both in-domain and out-of-domain datasets. The in-domain datasets\ninclude GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], and NumGLUE [Mishra\net al., 2022]. For the out-of-domain datasets, we include SVAMP [Patel et al., 2021], Mathematics\n[Davies et al., 2021], and SimulEq [Koncel-Kedziorski et al., 2016]. These datasets collectively\ncover a wide range of mathematical areas such as algebra, probability, number theory, calculus, and\ngeometry. Furthermore, some questions in these datasets require the application of commonsense,\nreading comprehension, and multi-step reasoning. All questions are formatted as open-ended.\nEvaluation metric. We use the standard evaluation metric for open-formed questions, exact match,\nwhich measures the model's accuracy by comparing its generated answers against the correct solu-\ntions. For an answer to be considered correct, it must match the reference solution precisely. We\nevaluate under the 0-shot setting with a maximum sequence length of 2048 tokens for decoding. The\ndefault prompt is Program-of-Thought (PoT), falling back to Chain-of-Thought (CoT) prompting if\nthe former does not work [Yue et al., 2023]."}, {"title": "5.2 Main results", "content": "SSM achieves a superior performance with limited memory. Tab 1 shows the in-distribution and\nout-of-distribution accuracies when LoRA fine-tuning Phi-2 on the MathInstruct dataset for 1K and\n2K iterations. We see that larger mini-batches (bs=128) generally outperform training with smaller\nmini-batches (bs=64), given the same number of training iterations. This is consistent with our the-\noretical results. By closely capturing the gradients of larger mini-batches, SSM effectively outper-\nforms training with random mini-batches of the same size. Moreover, we see that training with more\niterations also improves the performance on both in-domain and out-domain evaluation datasets as\nindicated by an increase in the performance of all methods from iteration 1K to 2K. Notably, SSM\nimproves the performance of training with both smaller and larger mini-batches throughout the train-\ning process as depicted in Fig 1. Averaging over all datasets, SSM improves the performance over\ntraining with smaller mini-batches by 3.9% at iteration 1K and 1.2% at iteration 2K.\nSSM speeds up training and improves convergence. Fig. 2 compares the wall-clock time and\nthe average performance of LoRA fine-tuned Phi-2 with different mini-batch sizes. For 1K train-\ning steps, when SSM selects smaller mini-batches of 64 examples from 128, it speeds up training\nby 1.3x, compared to training with a mini-batch size of 128, while halving the memory consump-\ntion. On the other hand, while using the same memory budget, SSM yields a roughly 4% higher\nperformance compared to models trained with a smaller mini-batch size of 64."}, {"title": "5.3 Ablation studies", "content": "The importance of weighted sampling. Tab 2 highlights the importance of weighted sampling in\nSSM. Weighted sampling ensures samples from different data sources are equally represented in the\nlarger mini-batch, improving the accuracy significantly by 2.5% on average."}, {"title": "6 Conclusion", "content": "In this work, we proposed a memory-efficient approach to train Large Language Models (LLMs).\nOur method efficiently finds small mini-batches of examples that closely capture gradients of large\nrandom mini-batches. In doing so, it simulates training with large mini-batches using a small amount\nof memory. We formulated the problem as a submodular maximization problem based on pairwise\ngradient similarities between examples in the large mini-batch. To address the challenges of finding\nsubsets based on the very high-dimensional gradients of LLMs, we leveraged ideas from zeroth-\norder optimization and neural network pruning to find lower-dimensional gradient estimation that\nallows finding high-quality subsets effectively with a limited amount of memory. We proved the\nsuperior convergence rate of training on the small mini-batches found by our method and empirically\nshowed its effectiveness for fine-tuning Phi-2 on MathInstruct to solve challenging math problems."}, {"title": "A Proof of Theorem 4.1", "content": "We build on the analysis of [Ghadimi and Lan", "2023": "and characterize the effect\nof the small mini-batch gradient errors on the convergence.\nFor a $L$-gradient Lipschitz function $L$ we have:\n$\n|L(\\theta_1) - L(\\theta_2) - \\langle\\nabla L(\\theta_2)", "fol-\nlows": "n$\n\\theta_{t+1"}, "leftarrow \\theta_t - \\eta_t(\\nabla L(\\theta_t) + \\zeta_t), \\text{ s.t. } \\zeta_t = \\tilde{\\zeta_t} + \\xi_t\n$\n(12)\nwhere $\\tilde{\\zeta_t} = ||\\nabla L(\\theta_t) - g_t||$ is the error of large random mini-batch in capturing the full gradient,\nand $\\xi_t = ||g_t - g_t^s ||$ is the error of small mini-batch $S_t$ in capturing the gradient of $M_t$.\nFollowing [Ghadimi and Lan, 2013"], "have": "n$\nL(\\theta_{t+1"}, {"obtain": "n$\n\\sum_{t=1}^{N} (\\eta_t - \\frac{L}{2} \\eta_t^2) ||\\nabla L(\\theta_t)||^2 \\leq L(\\theta_0) - L(\\theta_{N+1}) - \\sum_{t=1}^{N} (\\eta_t - L \\eta_t^2) \\langle \\nabla L(\\theta_t), \\zeta_t \\rangle + \\frac{L}{2} \\sum_{t=1}^{N} \\eta_t^2 ||\\zeta_t||^2\n$\n(17)\n$\n< L(\\theta_0) - L^* - \\sum_{t=1}^{N} (\\eta_t - L \\eta_t^2) \\langle \\nabla L(\\theta_t), \\tilde{\\zeta_t} + \\xi_t \\rangle + \\frac{L}{2} \\sum_{t=1}^{N} \\eta_t^2 ||\\zeta_t||^2\n$\n(18)\n$\n= L(\\theta_0) - L^* - \\sum_{t=1}^{N} (\\eta_t - L \\eta_t^2) \\langle \\nabla L(\\theta_t), \\tilde{\\zeta_t} + \\xi_t \\rangle + \\sum_{t=1}^{N} \\eta_t^2 ||\\tilde{\\zeta_t} + \\xi_t||^2\n$\n(19)\n$\nL"}]