[{"title": "Memory-efficient Training of LLMs with Larger Mini-batches", "authors": ["Dang Nguyen", "Wenhan Yang", "Rathul Anand", "Yu Yang", "Baharan Mirzasoleiman"], "abstract": "Training with larger mini-batches improves the performance and convergence rate of training machine learning models. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs) with billions of parameters, due to the large GPU memory requirement. To address this problem, we propose finding small mini-batches that simulate the dynamics of training with larger mini-batches. Specifically, we formulate selecting smaller mini-batches of examples that closely capture gradients of large mini-batches as a submodular maximization problem. Nevertheless, the very large dimensionality of the gradients makes the problem very challenging to solve. To address this, we leverage ideas from zeroth-order optimization and neural network pruning to find lower-dimensional gradient estimates that allow finding high-quality subsets effectively with a limited amount of memory. We prove the superior convergence rate of training on the small mini-batches found by our method and empirically show its effectiveness. Our method can effectively reduce the memory requirement by 2x and speed up training by 1.3x, as we confirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked with LoRA and other memory-efficient methods to further reduce the memory requirements of training LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in a variety of tasks, ranging from machine translation to conversational AI. However, pre-training and fine-tuning LLMs with billions of parameters requires a large amount of compute and GPU memory not only to store the parameters but also to compute gradients and optimizer states, such as momentum and variance in Adam, which is often used for training LLMs. For example, fine-tuning a small language model, such as Phi-2 with 2.7B parameters, with a batch size of 128 requires at least 44 GB of GPU memory. The large memory requirement makes it prohibitive to train such models with larger batch sizes, which can effectively improve the convergence of training and performance of the models. This raises a key question: can we train LLMs with larger batch sizes, using a limited amount of GPU memory?\nTo address this problem, many memory-efficient techniques have been recently proposed, mainly to enable efficient fine-tuning of pre-trained language models. At a high level, such methods try to find a smaller set of parameters [Adelman et al., 2021], or find low-rank [Hu et al., 2021, Zhao et al., 2024] or quantized [Dettmers et al., 2022] weights or optimizer states to train the model in a memory-efficient manner. There have also been efforts to adapt gradient-free optimization for training LLMs [Malladi et al., 2023]. However, most memory-efficient techniques cannot achieve a performance comparable to that of training the full model parameters or considerably increase the training time of LLMs."}, {"title": "2 Related Work", "content": "Memory-efficient training of LLMs. To address the large memory requirements of training LLMs, several methods have been recently proposed. LoRA [Hu et al., 2021] freezes the pre-trained model weights and trains two low-rank adaptor weight matrices to adapt the weights of each layer. Despite gaining memory efficiency, LoRA suffers from a performance drop compared to training with full-rank matrices. To improve upon this, several variations of LoRA [Liu et al., 2024, Renduchintala et al., 2023, Xia et al., 2024b] have been proposed, which aim to preserve the training dynamics of full-parameter training. However, these changes also lead to increased computational costs. GaLore [Zhao et al., 2024] reduces the memory cost of optimizer states during pre-training and fine-tuning by calculating the gradients and projecting them into a low-rank space.\nSeveral methods approximate backpropagation by sparsifying gradients [Frantar and Alistarh, 2023], subsampling the computational graph [Adelman et al., 2021], gradient check-pointing [Chen et al., 2016], and quantization of weights and optimizer states [Dettmers et al., 2022]. However, these approaches can incur large approximation errors and cause performance drops. Zeroth-order gradient approximation has also been used for memory-efficient training. MeZO [Malladi et al., 2023] builds on the traditional zeroth-order methods to estimate gradients using two forward passes, significantly reducing the memory cost of LLM fine-tuning to the same level as LLM inference. However, training with MeZo cannot reach a comparable performance as training the full model parameters.\nOur method can be easily stacked with existing memory-efficient methods to improve convergence and further reduce memory requirements.\nData selection for training LLMs. Data selection for training LLMs has garnered significant attention due to its potential to enhance model performance while reducing computational costs. For pre-training, training on examples with middle Perplexity rankings is shown beneficial [Marion et al., 2023]. Clustering based on embeddings of a pretrained model and sampling from the clusters to drop redundancies has been also investigated [Tirumala et al., 2024].\nFor fine-tuning, training on manually crafted high-quality instruction/response pairs has shown highly beneficial [Zhou et al., 2023a]. Building on this observation, data selection using LLMs such as chat-GPT or training on textbooks is proposed [Chen et al., 2024, Eldan and Li, 2023, Li et al., 2023a,c], and metrics such as diversity [Bukharin and Zhao, 2023, Du et al., 2023], and difficulty [Bhatt et al., 2024, Marion et al., 2023, Zhou et al., 2023b] are shown relevant. Using influence functions to select a subset of the fine-tuning data that is most beneficial for a given validation set has been also explored [Xia et al., 2024a]. Existing methods select data in a one-shot manner before fine-tuning, and either require access to another open LLM or a large preprocessing time to fine-"}, {"title": "3 Problem Formulation and Background", "content": "We consider the task of supervised fine-tuning a large language model f on data D, containing a set of I prompts and ground-truth responses. Formally, denoting the tokens for the ith prompt by Xi = [Xi,1, Xi,2,\u00b7\u00b7\u00b7] and the tokens in the corresponding response by Yi = [Yi,1, Yi,2,\u2026\u2026\u2026, Yi,T\u025b], we wish to minimize the following loss function:\n\\begin{equation} \n\\theta^* = \\underset{\\theta}{\\arg \\min} \\mathcal{L}_{\\mathcal{D}}(\\theta), \\text{ s.t. } \\mathcal{L}_{\\mathcal{D}}(\\theta) = \\sum_{i=1}^{I}\\sum_{t=1}^{T} \\log[Pr(Y_{i,t+1}|x_i, Y_{i,1\\dots t}, \\theta)]. \n\\end{equation}\n\nThe standard approach to minimize the above loss function is using mini-batch gradient methods such as mini-batch SGD, which iteratively samples random mini-batches Mt of b examples scaled by the learning rate \u03b7, and updates the model parameters in the negative direction of the gradient of the mini-batch.\n\\begin{equation} \n\\theta_{t+1} = \\theta_t - \\eta g_t, \\text{ s.t. } g_t = \\frac{1}{\\|M_t\\|} \\sum_{i \\in M_t} \\nabla \\mathcal{L}_i(\\theta), \n\\end{equation}\nwhere \\mathcal{L}_i(\\theta) is the loss of the ith prompt. Other stochastic optimizers like Adam [Kingma and Ba, 2014] adapt the learning rate across dimensions by scaling the gradient updates by square roots of exponential moving averages of squared past gradients. In doing so, they reduce the learning rate across sharp dimensions and increase the learning rate across flatter dimensions to improve convergence. In general, training with larger mini-batches effectively improves the convergence rate and performance of the trained models.\nFor random mini-batches that are unbiased estimates of the full gradient, i.e., \\mathbb{E}_{i \\in I}[\\nabla \\mathcal{L}_i(\\theta)] = \\nabla \\mathcal{L}(\\theta), as long as the mini-batch size |Mt| = b is not too large, the convergence rate of mini-batch SGD directly scales with a factor of 1/b. Formally, for a non-convex L-gradient Lipschitz function, mini-batch SGD with a small enough step size will visit an \u03f5-stationary point with a high probability at least once in the following number of iterations [Ghadimi and Lan, 2013]:\n\\begin{equation} \n\\mathcal{O} \\left( \\frac{L (\\mathcal{L}(\\theta_0) - \\mathcal{L}^*)}{\\epsilon^2} + \\frac{\\sigma^2}{b \\epsilon^2} \\right) \n\\end{equation}\nwhere \\mathbb{E}_{i \\in I} [(\\nabla \\mathcal{L}_i(\\theta) - \\nabla \\mathcal{L}(\\theta))^2] < \\sigma^2 is the variance of the individual gradients and b is the mini-batch size. Effectively, the variance of the mini-batches scales with the size of the mini-batches and directly improves the convergence rate. The effect of the mini-batch size on other stochastic optimizers is similar. Nevertheless, for LLMs with billions of parameters, training with large mini-batches requires very large GPU memory and becomes prohibitively expensive. Hence, in practice one needs to train with a small mini-batch size.\nTo improve the convergence and performance of training LLMs, our goal is to find small mini-batches that closely capture the training dynamics with larger mini-batches. Formally, we wish to find a set of smaller mini-batches {S1, S2, \u2026\u2026 } of size |St| = r < b = |Mt| that are unbiased and have a similar variance to that of larger mini-batches of size b. That is,\n\\begin{equation} \ng_i = \\frac{1}{\\|S_t\\|} \\sum_{i \\in S_t} \\nabla \\mathcal{L}_i(\\theta) = g_t, \\quad \\text{Vars} [g_i] = \\text{Var}_{M_t} [g_t] = \\frac{\\sigma^2}{b}, \n\\end{equation}\nIf such mini-batches can be found efficiently, they can (1) directly improve the convergence rate of mini-batch SGD and subsequently improve the performance; (2) reduce the memory requirement and speed up training by reducing the time required for backpropagating over the large mini-batch.\nDue to the high cost of pre-training, we mainly focus on fine-tuning LLMs in this work, but our problem formulation and approach applies to pre-training LLMs, without further modification."}, {"title": "4 Method", "content": "In this section, we discuss finding smaller mini-batches of examples that can simulate the dynamics of training with large mini-batches."}, {"title": "4.1 Finding the Small Mini-batches", "content": "The key observation is that since larger random mini-batches Mt are unbiased and have a small variance, if we can find small mini-batches St such that each St closely matches the gradient of the large mini-batches Mt, then the small mini-batches will also be unbiased and will have a small variance of \u03c3\u00b2/b. Formally, we wish to solve the following optimization problem at every iteration of training with mini-batch stochastic optimization methods, such as mini-batch SGD and Adam:\n\\begin{equation} \nS_t \\in \\underset{S_t \\subset M_t, |S_t| \\le r}{\\arg \\min} ||g_t - g_i||. \n\\end{equation}\nIn doing so, training on St instead of Mt yields the benefits of training with larger mini-batches, including improved convergence and performance. Improved mini-batch selection has recently been shown to boost the convergence of training vision classifiers [Yang et al., 2023]. Nevertheless, solving Problem (5) for LLMs requires addressing several new challenges, as we will discuss below.\nProblem (5) is NP-hard. However, a near-optimal subset can be found efficiently by maximiz- ing the following monotone submodular\u00b9 facility location function [Mirzasoleiman et al., 2020a,b, Pooladzandi et al., 2022]:\n\\begin{equation} \nS_t^* \\in \\underset{S_t \\subset M_t, |S_t| \\le r}{\\arg \\max} \\sum_{i \\in M_t} \\max[C - ||\\nabla \\mathcal{L}_i(\\theta) - \\nabla \\mathcal{L}_s(\\theta) ||], \n\\end{equation}\nwith a large constant C and assigning a weight ys = \u2211i\u2208Mt \u2161(argmin s\u2208St  ||\u2207Li(0) \u2212\u2207Ls(0) || = s) to every example s \u2208 St. The weighted gradient of the subset g\u2081 = \u2211ses, Ys\u2207Ls then closely captures the gradient of the larger mini-batch gt.\nA monotone submodular function can be efficiently maximized by applying the greedy algorithm [Wolsey, 1982]. The greedy algorithm commences with an empty set S = \u00d8, and at each step i, it selects an element e \u2208 Mt that maximizes the marginal utility F(e|S) = F(S\u222a {e}) \u2013 F(Si). Formally, S = S-1\u222a {arg maxe M, F(e/S-1)}. The time complexity of the greedy algorithm is O(brq) where identify r = |St|, b = |Mt| and q is the complexity of calculating F. The greedy algorithm can be further expedited through lazy evaluation [Minoux, 2005].\nChallenges of finding subsets for LLMs. The facility location formulation in Eq. (6) allows find- ing a near-optimal subset St that closely replicates the gradient of Mt. Nevertheless, to calculate the value of the function F, it requires calculating pairwise distances between gradients of all ex- amples in the large mini-batch Mt, which in turn requires calculating the gradients of all examples in Mt. However, this does not yield any memory efficiency or speedup. In addition, even if one calculates the full gradients, their very large dimensionality prevents calculating meaningful sim- ilarities. Indeed, pairwise similarities in such high-dimensional gradient vectors become vacuous, and hence a high-quality subset cannot be extracted. In summary, there are two main challenges in finding smaller mini-batches that closely capture the gradient of large mini-batches: (1) finding lower-dimensional gradient estimates without having to explicitly calculate the full gradient of ex- amples in Mt, and (2) calculating the lower-dimensional gradients efficiently with a limited about of memory. Next, we will discuss addressing each of these challenges in detail."}, {"title": "4.2 Finding Lower-dimensional Gradient Approximations", "content": "In this section, we address the challenge of finding lower-dimensional gradient estimates efficiently. First, we propose a memory-efficient zeroth-order approach to obtain lower-dimensional gradient estimates. Then, we discuss how we can further reduce the dimensionality of the gradient estimates to enable finding a high-quality subset efficiently with a limited amount of memory."}, {"title": "(1) Obtaining Memory-efficient Zeroth-order Lower-dimensional Gradient Estimates", "content": "First, we discuss calculating memory-efficient and lower-dimensional gradient estimates. To do so, we use Simultaneous Perturbation Stochastic Approximation (SPSA) [Spall, 1992] to get memory-efficient gradient estimates. SPSA estimates the gradient as:\n\\begin{equation} \n\\widehat{\\nabla \\mathcal{L}_i(\\theta)} = \\frac{\\mathcal{L}_i(\\theta + \\epsilon z) - \\mathcal{L}_i(\\theta - \\epsilon z)}{2 \\epsilon} z \\approx z z^\\top \\nabla \\mathcal{L}_i(\\theta), \n\\end{equation}\nwhere z \u2208 Rd with z ~ \\mathcal{N}(0, Id), and d is the number of model parameters and \u03f5 is the perturbation scale. The n-SPSA gradient estimate averages \\widehat{\\nabla \\mathcal{L}_i(\\theta)} over n randomly sampled z.\nSPSA requires two forward passes through the model to compute the gradient estimate for each example. As \u03f5 \u2192 0, the SPSA estimate provides a rank-1 reconstruction of the gradient. During training, n is a hyperparameter and can follow a schedule [Bollapragada et al., 2018, Cai et al., 2022]. Nevertheless, for language models n = 1 is shown to be the most effective [Malladi et al., 2023]. The vanilla calculation of \\widehat{\\nabla \\mathcal{L}_i} costs twice the memory of inference, as it needs to store z \u2208 Rd. However, [Malladi et al., 2023] proposed a memory-efficient way (MeZo) to calculate \\widehat{\\nabla \\mathcal{L}_i}, by first sampling a random seed s, and then for each of z's uses in Eq. (7), the random number generator is reset by s and resample the relevant entry of z. Using this in-place implementation, calculating \\widehat{\\nabla \\mathcal{L}_i} has a memory footprint equivalent to the inference memory cost.\nTraining with MeZo cannot reach a comparable performance as training the full model parameters. Nevertheless, it is accurate enough to find the subsets effectively.\nUsing MeZo to get the last-layer gradients. While MeZo enables calculating full gradients in a memory-efficient manner, full gradients are too high-dimensional to allow finding the subsets effectively. Hence, we need to lower the dimensionality of the gradients before they can be used to find a subset.\nTo do so, we only calculate the last layer gradients, which capture most of the gradient variation [Katharopoulos and Fleuret, 2018]. To get the last layer gradient with MeZo, our key idea is to only sample random perturbations for parameters corresponding to the last layer (V projection) in the perturbation vector z, and use zero for the other entries. That is,\n\\begin{equation} \n\\widehat{\\nabla \\mathcal{L}_i(\\theta_L)} = \\frac{\\mathcal{L}_i(\\theta + \\epsilon z_L) - \\mathcal{L}_i(\\theta - \\epsilon z_L)}{2 \\epsilon} z_L, \n\\end{equation}\nwhere zL \u2208 RdL with z = [0d-dL ,zL] and zL ~ \\mathcal{N}(0, IdL), and dL is the dimensionality of the flattened last layer weight matrix.\nThe above zeroth-order last-layer gradient estimates can be calculated very efficiently in just one forward pass. To do so, we first make a forward pass to get the activations XL\u22121 of the penultimate layer of the network. Then, we only perturb the last-layer parameters twice to calculate \\widehat{\\nabla \\mathcal{L}_i} based on the pre-calculated XL\u22121. The time of getting the lower dimensional last-layer gradients will be dominated by the time of computing XL\u22121, and the cost of the second step is negligible. For calculating the loss of the perturbed last-layer parameters in a memory-efficient manner, we use a fixed seed to generate the same perturbations zL multiple times, as is done in MeZo. Hence, the memory overhead is also negligible.\nWhile the dimensionality of the last-layer gradients is considerably lower than the full gradients, they are still too high-dimensional to allow finding high-quality subsets. For example, the dimensionality of the last layer (last V projection) of Phi-2 model is 2560 \u00d7 2560 = 6, 553, 600 when training the full parameters and 2560 \u00d7 128 = 327,680 when using LoRA. This makes calculating the pair- wise gradient similarity between examples prohibitive and ineffective in practice. Indeed, in such a high-dimensional space, small noisy dimensions dominate the similarity calculation and make them vacuous. This prevents selecting high-quality subsets."}, {"title": "(2) Further Reduce the Dimensionality by Sparsifying the Last-layer Gradients", "content": "To further reduce the dimensionality of the gradient estimates, we sparsify the last-layer gradients to obtain lower-dimensional gradients in a memory-efficient way. Our goal is to find a small number of dimensions in the last (V projection) layer which yields a similar loss reduction to that of the full gradient, at one parameter update step. As shown below, the gradient norm indicates, to the first"}, {"title": "4.3 Method: Select Small Mini-batches (SSM)", "content": "We summarize our method here. At every iteration of training, we sample a large mini-batch of examples. Then, we use MeZo to find memory-efficient estimates of last-layer gradients for examples in the large mini-batch. This can be done efficiently with just one forward pass. We further reduce the dimensionality of the gradient estimates by selecting the top k dimensions with the highest mag- nitude. We calculate the pairwise similarity between the lower-dimensional gradient estimates using l1 distance. l\u2081 distance preserves the distances better in high-dimensional spaces and is consistently more preferable than Euclidean distance [Aggarwal et al., 2001]. Finally, we find a small mini-batch from every large random batch during the training by applying the greedy algorithm to maximize the submodular facility location function in Eq. (6). We do not assign weights (\u03b3) to gradients of examples in the subset, to effectively reduce the redundancy in the data and balance different groups of examples.\nThe following theorem shows that as long as the smaller mini-batches closely mimic the gradient of the large batches, training with SSM imitates the training dynamics with large batches. Thus, it improves convergence and performance, compared to training with small random mini-batches.\nTheorem 4.1. As long as \\mathbb{E}[||gt - gi||] \\le ||\\nabla \\mathcal{L}(\\theta_t)|| and learning rate is small enough, minimizing a non-convex L-gradient Lipschitz function with mini-batches of size r < b found by SSM from larger mini-batches of size b will visit an \u03f5-stationary point with a high probability at least once in"}, {"title": "5 Experiments", "content": "In this section, we evaluate the performance of our method, SSM, for fine-tuning LLMs. We com- pare the performance, memory requirement, and wall-clock training time of training with small and large random mini-batches, with that of our method. We also do an ablation study showing the effectiveness of different design choices of SSM."}, {"title": "5.1 Settings", "content": "Datasets. We fine-tune the MathInstruct [Yue et al., 2023] dataset for the mathematical reasoning task. This dataset consists of about 260K instruction tuning examples, which are curated from 13 open-source math datasets. MathInstruct has a broad coverage of mathematical fields and a wide range of difficulty levels. Fine-tuning LLMs on MathInstruct has shown state-of-the-art performance on a variety of standard math evaluation benchmarks.\nModels. We utilize the Phi-2 model [Li et al., 2023b], a 2.7 billion-parameter language model by Microsoft Research. Phi-2 achieves high performance on various benchmarks despite its smaller size. Utilizing high-quality, curated training data and advanced scaling techniques, Phi-2 outper- forms much larger models on multi-step reasoning tasks.\nTraining details. Following the setup used in [Yue et al., 2023], we adopt a training regime with a learning rate of 2e-5 and a cosine scheduler with a 3% warm-up period, i.e. the learning rate linearly increases from 0 to 2e-5 over the first 3% of training steps, then follows a cosine decay to 0 at the end of training. We set a maximum sequence length of 512. For all experiments on MathInstruct, we standardize the number of gradient steps to correspond to 1K, unless explicitly specified. We use LoRA with a rank of 128, alpha of 512, and dropout rate of 0.05. For Phi-2, we apply LORA for all attention matrices (QKV_proj) and two fully connected layers. All experiments are run on 4 NVIDIA A40 GPUs. We ran each experiment three times.\nEvaluation datasets. We adopt the framework established in [Yue et al., 2023], leveraging a vari- ety of popular datasets across both in-domain and out-of-domain datasets. The in-domain datasets include GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], and NumGLUE [Mishra et al., 2022]. For the out-of-domain datasets, we include SVAMP [Patel et al., 2021], Mathematics [Davies et al., 2021], and SimulEq [Koncel-Kedziorski et al., 2016]. These datasets collectively cover a wide range of mathematical areas such as algebra, probability, number theory, calculus, and geometry. Furthermore, some questions in these datasets require the application of commonsense, reading comprehension, and multi-step reasoning. All questions are formatted as open-ended.\nEvaluation metric. We use the standard evaluation metric for open-formed questions, exact match, which measures the model's accuracy by comparing its generated answers against the correct solu- tions. For an answer to be considered correct, it must match the reference solution precisely. We evaluate under the 0-shot setting with a maximum sequence length of 2048 tokens for decoding. The default prompt is Program-of-Thought (PoT), falling back to Chain-of-Thought (CoT) prompting if the former does not work [Yue et al., 2023]."}, {"title": "5.2 Main results", "content": "SSM achieves a superior performance with limited memory. Tab 1 shows the in-distribution and out-of-distribution accuracies when LoRA fine-tuning Phi-2 on the MathInstruct dataset for 1K and 2K iterations. We see that larger mini-batches (bs=128) generally outperform training with smaller mini-batches (bs=64), given the same number of training iterations. This is consistent with our the- oretical results. By closely capturing the gradients of larger mini-batches, SSM effectively outper- forms training with random mini-batches of the same size. Moreover, we see that training with more iterations also improves the performance on both in-domain and out-domain evaluation datasets as indicated by an increase in the performance of all methods from iteration 1K to 2K. Notably, SSM improves the performance of training with both smaller and larger mini-batches throughout the train- ing process as depicted in Fig 1. Averaging over all datasets, SSM improves the performance over training with smaller mini-batches by 3.9% at iteration 1K and 1.2% at iteration 2K.\nSSM speeds up training and improves convergence. Fig. 2 compares the wall-clock time and the average performance of LoRA fine-tuned Phi-2 with different mini-batch sizes. For 1K train- ing steps, when SSM selects smaller mini-batches of 64 examples from 128, it speeds up training by 1.3x, compared to training with a mini-batch size of 128, while halving the memory consump- tion. On the other hand, while using the same memory budget, SSM yields a roughly 4% higher performance compared to models trained with a smaller mini-batch size of 64."}, {"title": "5.3 Ablation studies", "content": "The importance of weighted sampling. Tab 2 highlights the importance of weighted sampling in SSM. Weighted sampling ensures samples from different data sources are equally represented in the larger mini-batch, improving the accuracy significantly by 2.5% on average."}, {"title": "6 Conclusion", "content": "In this work, we proposed a memory-efficient approach to train Large Language Models (LLMs). Our method efficiently finds small mini-batches of examples that closely capture gradients of large random mini-batches. In doing so, it simulates training with large mini-batches using a small amount of memory. We formulated the problem as a submodular maximization problem based on pairwise gradient similarities between examples in the large mini-batch. To address the challenges of finding subsets based on the very high-dimensional gradients of LLMs, we leveraged ideas from zeroth- order optimization and neural network pruning to find lower-dimensional gradient estimation that allows finding high-quality subsets effectively with a limited amount of memory. We proved the superior convergence rate of training on the small mini-batches found by our method and empirically showed its effectiveness for fine-tuning Phi-2 on MathInstruct to solve challenging math problems."}, {"title": "A Proof of Theorem 4.1", "content": "We build on the analysis of [Ghadimi and Lan", "2023": "and characterize the effect of the small mini-batch gradient errors on the convergence.\nFor a L-gradient Lipschitz function L we have:\n\\begin{equation"}, "n|\\mathcal{L}(\\theta_1) - \\mathcal{L}(\\theta_2) - \\langle \\nabla \\mathcal{L}(\\theta_2), \\theta_1 - \\theta_2 \\rangle | \\le \\frac{L}{2} ||\\theta_1 - \\theta_2||^2. \n\\end{equation}\nThe gradient descent updates when training on mini-batches found by SSM can be written as fol- lows:\n\\begin{equation} \n\\theta_{t+1} \\leftarrow \\theta_t - \\eta_t (\\nabla \\mathcal{L}(\\theta_t) + \\zeta_t), \\text{ s.t. } \\zeta_t = \\tilde{\\zeta}_t + \\xi_t \n\\end{equation}\nwhere \\tilde{\\zeta}_t = ||\\nabla \\mathcal{L}(\\theta_t) \u2013 gt|| is the error of large random mini-batch in capturing the full gradient, and \\xi_t = ||gt - g_i|| is the error of small mini-batch St in capturing the gradient of Mt.\nFollowing [Ghadimi and Lan, 2013"], "have": "n\\begin{equation"}, {"obtain": "n\\begin{equation} \n\\sum_{t=1}^N (\\eta_t - \\frac{L \\eta_t^2}{2}) \\mathbb{E}_{\\Psi_N} [||\\nabla \\mathcal{L}(\\theta_t)||^2] \\le \\mathcal{L}(\\theta_0) - \\mathcal{L}(\\theta_{N+1}) - \\sum_{t=1}^N (\\eta_t - \\frac{L \\eta_t^2}{2}) \\mathbb{E}_{\\Psi_t} [\\langle \\nabla \\mathcal{L}(\\theta_t), \\zeta_t \\rangle] - \\frac{L}{2}\\sum_{t=1}^N \\eta_t^2  \\mathbb{E}_{\\Psi_t} [||\\zeta_t||^2] \n\\end{equation}\n\\begin{equation} \n\\le \\mathcal{L}(\\theta_0) - \\mathcal{L}^* - \\sum_{t=1}^N (\\eta_t - \\frac{L \\eta_t^2}{2}) \\mathbb{E}_{\\Psi_t} [\\langle \\nabla \\mathcal{L}(\\theta_t), \\zeta_t \\rangle] - \\frac{L}{2}\\sum_{t=1}^N \\eta_t^2  \\mathbb{E}_{\\Psi_t} [||\\zeta_t||^2] \n\\end{equation}\n\\begin{equation} \n= \\mathcal{L}(\\theta_0) - \\mathcal{L}^* - \\sum_{t=1}^N (\\eta_t - \\frac{L \\eta_t^2}{2})  (\\mathbb{E}_{\\Psi_t} [\\langle \\nabla \\mathcal{L}(\\theta_t), \\tilde{\\zeta}_t + \\xi_t \\rangle]) + \\frac{L}{2}\\sum_{t=1}^N \\eta_t^2  [||\\tilde{\\zeta}_t + \\xi_t ||^2] \n\\end{equation}\n\\begin{equation} \n= \\mathcal{L}(\\theta_0) - \\mathcal{L}^* - \\sum_{t=1}^N (\\eta_t - \\frac{L \\eta_t^2}{2})  \\mathbb{E}_{\\Psi_t} [\\langle \\nabla \\mathcal{L}(\\theta_t), \\zeta_t \\rangle] + \\sum_{t=1}^N  \\eta_t^2 ||\\tilde{\\zeta}_t + \\xi_t ||^2,  \n\\end{equation}\n\\begin{equation} \n+ \\sum_{t=1}^N \\eta_t^2  (||\\tilde{\\zeta}_t|| + ||\\xi_t|| + 2 \\langle \\tilde{\\zeta}_t, \\xi_t \\rangle]), \n\\end{"}]