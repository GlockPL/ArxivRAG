{"title": "Design Proteins Using Large Language Models: Enhancements and Comparative Analyses", "authors": ["Kamyar Zeinalipour", "Neda Jamshidi", "Monica Bianchini", "Marco Maggini", "Marco Gori"], "abstract": "Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B, Llama-2-7B, Llama-3-8B, and gemma-7B, to produce valid protein sequences. All of these models are publicly available. Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of natural language processing (NLP) has achieved remarkable progress, particularly through the development and utilization of large pre-trained language models. These sophisticated models represent a significant leap forward, primarily due to their ability to understand and generate human-like text based on training from extensive datasets. Typically, these models are trained using unsupervised learning techniques, where they learn to predict the next word or token in a sequence by examining the tokens that precede it. This method has propelled them to the forefront of various NLP applications, including chatbots (Wei et al., 2024), text summarization (Zhang et al., 2024; Tang et al., 2023), and advanced information extraction tasks (Dagdelen et al., 2024). Among the intriguing avenues explored with these models is their application in the field of bioinformatics, specifically in protein generation (Madani et al., 2020). Indeed, the protein alphabet is composed of twenty common amino acids, each represented by a single character. Regarding their primary structure, proteins, which are vital biological molecules, are made up of chains of amino acids, thus forming sequences of letters and drawing a parallel to the structure of natural languages. As in natural languages, protein sequences have directionality and are typically composed of reused modular elements that exhibit slight variations. Moreover, common protein motifs and domains, which are the basic building blocks of proteins, are similar to words, phrases, and sentences in human language. This similarity suggests that language models, which excel in handling sequential data, could effectively generate amino acid chains, or proteins.\nThe primary objective of our research lies in advancing the understanding and application of medium-sized language models, particularly those in the 7 billion to 8 billion parameter range, including Mistral-7B, Llama-2-7B, Llama-3-8B, and gemma-7B, for the generation of high-quality protein sequences. Our hypothesis, backed by preliminary studies, suggests that these models, even when trained with considerably small datasets, can produce accurate and viable protein sequences effectively."}, {"title": "2 Related Works", "content": "The integration of natural language processing (NLP) techniques, traditionally applied to human languages, into bioinformatics, has transformative potential, particularly in the analysis of biological sequences such as DNA, RNA, and proteins. These biological data, sharing similarities with linguistic texts in their structured and functional building blocks, are highly amenable to computational methodologies. The impactful success seen in NLP through transformer-based models has led to breakthroughs in specialized models geared toward understanding the complexities of these biological sequences. By utilizing extensive databases such as UniProt (Consortium, 2019), ENSEMBL (Cunningham et al., 2022), and GenBank (Benson et al., 2012), these models harness rich data to enhance both predictive and analytical capabilities in bioinformatics.\nThe realm of protein sequences has seen notable advancements through the adoption of both supervised and unsupervised learning models. Language models have been increasingly leveraged and employed in the domain of protein design (Ferruz and H\u00f6cker, 2022). Supervised learning approaches refine models by training them with labeled data, which is invaluable for accurately predicting protein stability or identifying structural similarities among sequences (Bepler and Berger, 2021; Alley et al., 2019). On the other hand, the introduction of transformer technology has been pivotal in popularizing unsupervised learning methods (Vaswani et al., 2017). These methods involve the strategic corruption of input sequences which are then used to train models to predict and reconstruct the NATURAL sequence. Leading models such as ESM (Rives et al., 2021), ProtTrans (Elnaggar et al., 2021), and ProteinBERT (Brandes et al., 2022) demonstrate this approach, offering powerful embeddings that prove critical in supporting a wide array of downstream biochemical tasks (Yang et al., 2024; Rao et al., 2019). These tasks include, but are not limited to, analyzing protein-protein interactions, predicting molecular functions, and identifying potential sites for drug binding. In addition to these developments, the adoption of autoregressive models widely recognized for their ability to generate coherent, long-form text in classical NLP settings has been successfully applied to the domain of protein sequencing. Prototypes like ProGen (Nijkamp et al., 2023; Madani et al., 2020), ProtGPT2 (Ferruz et al., 2022) and ProLLaMA (Lv et al., 2024) capitalize on this capability, employing autoregressive algorithms to effectively predict the future elements of protein sequences from given contexts. This predictive ability is critical for sophisticated applications such as protein design, where the generation of novel and function-"}, {"title": "3 Methodology", "content": "In this section, we delineate the methodologies employed to adapt pre-trained LLMs for the generation of protein sequences. Our approach involved refining the tokenizer based on the Byte-Pair Encoding (BPE) methodology, followed by fine-tuning the entire pre-trained model using a designated dataset of protein sequences. Subsequently, this fine-tuned model was utilized to generate new protein sequences. It's important to note that base models such as LLMs, while powerful, are not inherently capable of designing novel proteins. Their success in this domain is achieved through a specialized fine-tuning process, which involves not only adapting the model to a specific task using a smaller, task-specific dataset but also modifying the tokenizer. This is because the tokens that LLMS were initially trained on are natural language tokens, whereas our domain requires a different set of tokens. Therefore, we also need to train the tokenizer to handle this new domain effectively.\nVerification of these sequences was carried out by generating their respective PDB structures using DeepMind's AlphaFold (Jumper et al., 2021). We assessed the quality of these structures using various metrics such as pLDDT, RMSD, TM-Score and REU. The performance of the models \u2014 namely Mistral-7B (Jiang et al., 2023), Llama-2-7B (Touvron et al., 2023), Llama-3-8B, and gemma-7B (Team et al., 2024) was then compared with previous studies that employed language models for protein sequence generation. We have also evaluated the potential fitness of our generated sequences in comparison to natural and random sequences in the context of PLDDT, Rosetta-Relax scores, RMSD and TM-Scores, thereby providing a comparative analysis. Subsequently, we will provide a detailed description of all these steps, focusing on the training of the LLMs and their validation."}, {"title": "3.1 From LLMs to Proteins", "content": "Large language models, such as transformers, are sophisticated algorithms trained on extensive textual datasets. These models utilize their predictive capability primarily to determine the subsequent token based on the preceding ones. Given their training on a vast amount of text data, LLMs are highly adaptable and can be finely tuned for specialized tasks, including summarizing specific document types like legal texts. An interesting application of these models is in the domain of protein generation. Proteins, being amino acid sequences, differ significantly from standard text data. This difference necessitates the retraining of tokenizers to achieve more accurate tokenization for proteins, enhancing the model ability to recognize and predict relevant patterns in amino acid sequences. Following the retraining, these adapted tokenizers are used to refine the parameters of pre-trained LLMs. This fine-tuning process tailors the LLMs to predict protein sequences effectively by generating valid protein structures. In subsequent sections, we will elaborate on the methodologies applied for tokenizer retraining, describe the various LLMs utilized, and discuss their specific fine-tuning.\nTokenizer retraining In situations where the corpus significantly diverges from that utilized during the initial training of a language model, it becomes imperative to retrain the model from scratch. This process necessitates adjusting the tokenizer to accommodate the nuances of the new dataset. A tokenizer serves the critical function of converting textual data into numerical representations suitable for computational processing by language models. For the retraining of our tokenizer, we employed the Byte-Pair Encoding (BPE) method. BPE is a hybrid between a character-level and word-level tokenizer. It starts with a base vocabulary of individual characters and iteratively merges the most frequently adjacent pairs of characters or character sequences. Through this methodology, BPE effectively manages the vocabulary size, allowing for efficient handling of unknown words by breaking them down into recognizable subwords. This is particularly beneficial in managing morphologically rich languages or corpora with specialized jargon. In our adaptation process, we retained the original vocabulary size of the tokenizer used in prior models to maintain consistency and optimize integration with the pre-trained configurations. This approach ensures that the retrained models sustain"}, {"title": "Fine-Tune Pre-trained LLMs", "content": "In this research, our objective was to assess the capabilities of various pre-trained language models in the specialized task of protein generation. To this end, we fine-tuned four distinct models: Mistral-7B, Llama-2-7B, Llama-3-8B, and gemma-7B. Each model is based on the transformer architecture, which is renowned for its effectiveness in handling sequence-to-sequence tasks and operates under a causal framework conducive to generative tasks. The four models were specifically chosen to represent a bandwidth of computational capacities predominantly ranging between 7 billion and 8 billion parameters, enabling a focused analysis on how parameter scale influences model performance in biological sequence generation. Mistral-7B, developed by MistralAI, contains precisely 7 billion parameters. In contrast, both Llama-2-7B and the newer Llama-3-8B are products from Meta, featuring 7 billion and 8 billion parameters, respectively. The latter represents an advanced iteration within the LLama series, potentially offering enhancements in learning efficiency and output refinement. Finally, gemma-7B from Google, also with 7 billion parameters, extends our model diversity, providing an additional perspective from another leading tech giant's approach to language model development.\nBy employing these models, we aim to conduct a thorough comparative analysis, examining not just the quantitative outcomes in terms of accuracy and efficiency in protein generation, but also qualitative aspects such as the fidelity and usability of generated sequences. Given the similar parameter size, any observed differences in performance can be more directly attributed to architectural nuances and training methodologies between the models. This study not only advances our understanding of the capabilities of high-capacity language models in biosciences but also guides future developments in computational biology and the deployment of AI-driven tools for scientific discovery.\nFirstly, we observe that each of these language models employs variants of the cross-entropy loss function. Throughout the fine-tuning process, the objective is to minimize this loss, which effectively maximizes the probability of predicting subsequent tokens accurately, based on the context provided by previous tokens. This optimization directly enhances the model ability to generate coherent and contextually appropriate text."}, {"title": "3.2 Evaluation", "content": "In this section, we describe each evaluation method implemented in our study following the generation of proteins. Initially, protein sequences generated using tuned LLMs were structurally modeled using AlphaFold2, which provided three-dimensional structures along with per-residue confidence scores (pLDDT). Subsequently, the topological similarity of these structures to known protein configurations was assessed using the TM-Score computed by FoldSeek Additionally, Rosetta -Relax was employed to analyze the energetic profiles of the modeled proteins, enhancing our understanding of their stability and viability. For intra-dataset structural comparisons, RMSD calculations were conducted using PyMOL. Detailed descriptions and analyses of these metrics are provided in the following sections.\nAlphafold2 (pLDDT) In the initial phase of the evaluation, we utilized AlphaFold2 to predict the structures of the generated proteins and compute their predicted Local Distance Difference Test (pLDDT) scores. AlphaFold2, developed by DeepMind, represents a significant advancement in protein structure prediction by leveraging sophisticated deep learning methodologies. It predicts protein structures from amino acid"}, {"title": "4 Experimental results", "content": "In this section, we delineate the experiments conducted in this study, presenting an evaluation of the results garnered from the protein sequences we generated. Additionally, we discuss the regeneration of proteins utilizing language-based models specifically designed for protein generation tasks, including ProGen in four distinct sizes, ProtGPT2, and ProLLaMA.\nInitially, we explore the dataset utilized in our experiments, which is notably smaller than those used in other models, followed by a detailed exposition of our training setup. Finally, we present a comprehensive analysis of the evaluation results employing various metrics such as pLDDT, RMSD, TM-Score and REU."}, {"title": "4.1 Dataset", "content": "In this study, the UniRef50 dataset, originating from the UniProt databases, has been utilized. The UniProt Reference Cluster (UniRef) databases systematically organize clustered sets of protein sequences from UniProtKB and selected UniParc records, aiming to reduce redundancy and provide comprehensive coverage of sequence space. This is achieved through varying levels of sequence identity across three datasets, facilitating faster similarity searches among proteins.\nSpecific attention was given to the Homo sapiens subset within UniRef50, which initially comprised over 60,000 protein sequences. Given the constraints of computational resources and the criteria of our intended language models, a sequence length"}, {"title": "4.2 Training Setup", "content": "The training methodology employed in this study involved training Language Models (LMs) specifically tailored for protein generation utilizing four Nvidia A6000 GPUs. The training configuration utilized a sequence length of 512, with a maximum training step limit of 2000 and a batch size of 1, coupled with a gradient accumulation step size of 16 for enhanced training efficiency. The learning rate was set at 5e-5, and a cosine learning rate scheduler was employed to adaptively adjust the learning rate. Furthermore, a weight decay of 0.01 and a num warm-up step value of 150 were applied to stabilize the training process. The utilization of the bfloat16 data format contributed to faster computation due to reduced precision, enhancing overall training performance. We employed DeepSpeed (Rasley et al., 2020), a deep learning optimization library developed by Microsoft, to facilitate efficient training and optimization of the models. and also we applied FlashAttention 2 (Dao, 2023).\nFour distinct LLMs models, namely Mistral-7B, Llama-2-7B, Llama-3-8B, and gemma-7B were trained using this meticulously tuned training configuration. The selection of appropriate hyperparameters and the utilization of multiple GPUs facilitated efficient and timely training of these models. The strategic incorporation of the cosine learning rate scheduler and weight decay mechanism bolstered the models' convergence and performance during training, ultimately leading to the successful generation of protein sequences."}, {"title": "4.3 Results Evaluation", "content": "In this section, we randomly selected 250 proteins, each with a length between 70 to 140 amino acids, from each of the under-investigation models for structure prediction and subsequent evaluation. In order to initiate the protein generation process, we input a special token, known as the beginning-of-sequence (BOS) token. Once this token is fed into the model, it begins to generate protein sequences, leveraging the patterns and knowledge it has acquired during its training phase. These proteins were submitted to AlphaFold2, which generated 3D structural models with corresponding PLDDT scores for each protein. We proceeded to randomly select 20 3D structural proteins from each of the under-investigated models for a more in-depth analysis. The chosen proteins were then subjected to further evaluations, including the calculation of Intra RMSD, Inter RMSD, TM-Score, and REU with selected proteins. This multi-faceted approach to evaluation has allowed us to thoroughly assess the performance of our models and the quality of our 3D protein structure predictions.\nTo evaluate the pLDDT score for each protein, AlphaFold2 generates five 3D structural models with corresponding pLDDT scores. We then calculated the mean of the five pLDDT scores to obtain a representative pLDDT score for each protein. We present the evaluation results using all the metrics discussed in Section 4. Notably, P-Mistral consistently outperforms all other models across various metrics.\nThe most significant difference between the trained models and randomly generated proteins We procedurally generated a set of proteins in a random manner, with each of these proteins being composed of a sequence of 20 amino acids, is observed in the pLDDT metric, as depicted in Figure 4. Our models,P-Llama2 and P-Llama3, exhibit a distribution similar to the NATURAL data. Additionally, we observed a significant disparity between randomly generated proteins and other models when evaluating the TM-score metric, as illustrated in Figure 5. Other metrics, such as Inter and Intra RMSD, are shown in Figures 8 and 6.\nFurthermore, for the REU metric, we identified an optimal range between -100 and -300. The randomly generated proteins fall significantly outside this interval, whereas the models we introduced"}, {"title": "Given a sequence of tokens, the cross-entropy loss predicts the probability of each subsequent token based on the previous context, i.e., given $x_1, x_2, ..., x_n$ in training data, the model is able to predict each subsequent token $x_{t+1}$ based on previous tokens $x_1,..., x_t$. The formula for the loss across an entire sequence of length N is:", "content": "$L = -\\sum_{t=1}^N \\log(p_{model}(x_{t+1} | x_1, x_2, ..., x_t))$"}, {"title": "5 Conclusion", "content": "In this study, we introduced four novel models designed to generate high-quality protein sequences by leveraging pre-trained language models. This research is motivated by the growing demand for efficient and accurate tools that can assist in understanding and engineering protein structures, which are pivotal in numerous biological and medical applications. Our approach involved a meticulous design and training phase, followed by rigorous testing and validation processes to assess the performance of each model.\nTo provide a thorough evaluation, we conducted comprehensive experiments comparing our models with a range of existing models that also utilize language models for protein sequence generation. Comparative analyses were performed, which were grounded on diverse criteria, including sequence quality, diversity, and fidelity to biological functions. These analyses also incorporated several structural assessment metrics such as PLDDT (predicted Local Distance Difference), TM-Score (to assess structural similarity), RMSD (Root Mean Square Deviation), and REU (Rosetta Energy Unit). Our findings revealed that some of our proposed models, particularly P-Mistral, exhibited superior performance compared to existing models, even surpassing those trained on considerably larger datasets. This remarkable performance underscores the potential of our models to offer significant advancements in the field of protein sequence generation.\nWe are committed to the principles of open science and reproducibility. Consequently, we will make all four models publicly available to the research community. This accessibility will empower other"}]}