{"title": "LEARNING HOW HARD TO THINK:\nINPUT-ADAPTIVE ALLOCATION OF LM COMPUTATION", "authors": ["Mehul Damani", "Idan Shenfeld", "Andi Peng", "Andreea Bobu", "Jacob Andreas"], "abstract": "Computationally intensive decoding procedures\u2014including search, reranking,\nand self-critique can improve the quality of language model (LM) outputs in\nproblems spanning code generation, numerical reasoning, and dialog. Existing\nwork typically applies the same decoding procedure for every input to an LM. But\nnot all inputs require the same amount of computation to process. Can we allo-\ncate decoding computation adaptively, using more resources to answer questions\nwhose answers will be harder to compute? We present an approach that predicts\nthe distribution of rewards given an input and computation budget, then allocates\nadditional computation to inputs for which it is predicted to be most useful. We\napply this approach in two decoding procedures: first, an adaptive best-of-k pro-\ncedure that dynamically selects the number of samples to generate as input to a\nreranker; second, a routing procedure that dynamically responds to a query using\na decoding procedure that is expensive but accurate, or one that is cheaper but\nless capable. Across a suite of programming, mathematics, and dialog tasks, we\nshow that accurate computation-allocation procedures can be learned, and reduce\ncomputation by up to 50% at no cost to response quality, or improve quality by up\nto 10% at a fixed computational budget.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent improvements in language models\n(LMs) have dramatically enhanced their ability\nto tackle complex tasks in mathematics, cod-\ning, and reasoning. However, as with natural\nand artificial intelligent agents of all kinds (Sil-\nver et al., 2016), LMs cannot solve all problems\non the first try: they benefit from the ability\nto perform search (Yao et al., 2024), sampling\n(Brown et al., 2024), or more sophisticated de-\ncoding procedures like chain-of-thought (Wei\net al., 2022) and self-critique (Wang et al.,\n2023).\nImportantly, computationally intensive prob-\nlem domains may exhibit considerable varia-\ntion in the difficulty of individual problem in-\nstances: not all problems are equally hard to\nsolve. For example, even a novice program-\nmer can likely write code to to test if an integer\nis even. Balancing a binary tree might require\nmultiple attempts, and finding a polynomial-\ntime set cover algorithm is likely impossible, and not be worth attempting at all. Maximally efficient\nuse of computational resources thus requires identifying, a priori, the inputs for which additional\ncomputation will improve outputs. In LMs, recent work has shown that significant gains from adap-\ntive choice of decoding procedures are theoretically achievable (Snell et al., 2024). However, past"}, {"title": "2 PRELIMINARIES", "content": "Suppose we have an LM-based agent that will interact with a large number of users in parallel. At\nany moment, each of a set of users has issued a query xi, for which we wish to produce a response\nYi. We have acccess to both a language model (LMs) p(y | x), capable of generating candidate\nresponses, as well as a reward model r(x, y) capable of assessing the quality of candidate responses.\nIn the absence of any computational constraints, we might wish to find the best response to every\nuser query by optimizing arg maxy r(xi, y) independently for each query xi.\nIn practice, however, we generally do not have the ability to perform exhaustive search over can-\ndidate responses y. Instead, we use a decoding procedure f(x, b) that (stochastically) generates a\nresponse y subject to some constrained computation budget b. (In general, increasing the budget\nto a query should increase the quality of the response.) Many such decoding procedures are in wide\nuse; our experiments will focus on two of the most widely used.\nIn best-of-k, we generate a finite number of samples, then rerank them:\nf(x,b) = \\underset{y_i\\in \\{Y_1,...,Y_b\\}~p(x)}{\\text{argmax}} r(x,y_i).\nIn routing, we have access to a strong but expensive decoding scheme ps (which could involve\nsearch, reasoning, or even just a larger model). For every query we can either use this decoding\nscheme or to fall back to a weak but cheaper decoding scheme pW (e.g. ordinary decoding or a\nsmaller model). Then:\nf(x,b) = \\begin{cases} y \\sim p_W(.x) & \\text{if } b = b_W \\\\ y \\sim p_s (x) & \\text{if } b = b_s \\end{cases}"}, {"title": "3 METHOD", "content": "In Eq. (4), we would like to allocate computation bi to queries xi to maximize expected reward,\nwhich we will denote q(xi, bi) = Ey\u3047~f(xi,bi)[r(xi, Yi)]. But there is a problem: without querying\nthe LM, we cannot know the value of q(xi, bi) for some new xi. To make matters worse, Monte\nCarlo estimation of q(xi, bi) (as in Snell et al., 2024) may in general require more computation (e.g.\nmore LM samples) than we eventually wish to allocate to xi! To efficiently allocate computation,\nwe need some way to predict q(xi, bi) from xi alone, and then use these predictions to solve Eq. (4).\nWhen estimating problem difficulty and allocating decoding budget, it will be useful to reason about\nthe benefits of incremental changes in compute budgets. Formally, let us define the marginal re-\nward \u2206ij = q(xi, j) \u2013 q(xi, j \u2212 1) (with \u2206(\u00b7, 0) = 0). Under this definition, q(xi, bi) = \u03a3j=1 \u2206\u03ad\u03bb;\nintuitively, Aij represents the expected gain from allocating one more \u201cunit\u201d of computation to xi,\ngiven that we have already allocated j 1 units. We may then re-write Eq. (4) as:\n\\underset{\\text{i,j}}{\\text{max}} \\sum_{i=1}^n \\sum_{j=0}^{Bmax} C_{ij}\\Delta_{ij} \\text{ s.t. } \\sum_{i,j} C_{ij} \\leq B.n; C_{ij} \\leq C_{i,j-1} \\forall i, j"}, {"title": "3.1 ESTIMATING PROBLEM DIFFICULTY BY PREDICTING MARGINAL REWARDS", "content": "Given a training set of queries xi, we collect empirical estimates of Aij by decoding from f(x, b)\nat values of b up to some Bmax. We then train a model \u25b2(xi; 0) that predicts a vector of marginal\nrewards for all budgets j simultaneously by optimizing the mean squared error:\n\\text{arg} \\underset{\\theta}{\\text{min}} \\sum_{x_i, \\Delta_i} || \\Delta_i - \\Delta(x_i;\\theta)||^2"}, {"title": "3.2 ALLOCATING COMPUTATION", "content": "Eq. (5) is an integer linear program, but in fact has a very special form that allows it to be solved\nin O(Bmaxn) time using a greedy algorithm that incrementally \u201cturns on\u201d Cij for which Aij is\nlargest. (Formally, the feasible sets of cij form a matroid, Edmonds, 1971, so this greedy procedure\nis guaranteed to find an optimum.) We exploit this property in two ways:\nOnline allocation: If queries {x;} are known a priori, we simply replace each Aij in Eq. (5) with\nthe corresponding estimate from A(xi; 0), solve for cij, then finally set each b\u2081 = maxj Cij.\nOne drawback of this procedure is that responses must be processed in a batch. However, in some\ncases it is also possible to effectively set budgets independently for each xi:\nOffline allocation: If allocations must be made without access to a full batch of samples, we con-\nstruct a fixed allocation policy as follows:\n(1) Hold out a subset of the data used for training the reward estimator \u2206, then use it to label queries\nin this held-out set (e.g. based on the first-sample prediction \u25b3(xi)1). Divide these queries into a\nfixed set of bins according to their predicted marginal rewards.\n(2) Solve the allocation problem for this held-out as in Eq. (5), with the additional constraint that all\nqueries in a bin receive the same budget allocation. For each bin, store the assigned budget.\n(3) During deployment, compute a reward prediction for each xi, map it to a bin, and return the\nbudget associated with that bin.\nDuring deployment, all queries can be processed independently (at the risk of a budget violation if\nthe distribution of queries is significantly different from those used to compute the allocation policy)."}, {"title": "3.3 INTERESTING SPECIAL CASES", "content": "Binary Reward + Best-of-k: In domains such as coding and math, rewards are often binary, indi-\ncating success or failure. For instance, an outcome reward model can be used to assess correctness\nin math while unit tests indicate correctness in coding. In such settings, the probability that a single\nsample from the model succeeds may be used to analytically compute all marginal rewards.\nLet A = Ey~p(\u00b7\\x) [r(x, y)] denote the probability of obtaining a successful result from a single\nsample (for r(x, y) \u2208 {0,1}). Then the reward estimate q(x, b) is simply the probability of getting\nat least one success in b attempts: q(x, b) = 1 \u2212 (1 \u2212 1)6. Then \u2206(xi, bi) = di(1 \u2013 di)bi.\nIn this case, rather than training the reward predictor \u25b2 using a squared loss as in Eq. (6), we obtain\nempirical estimates of A\u2081 at training, then minimize the cross-entropy:\n\\sum_{X_i,\\lambda_i} [\\lambda_i log(\\Lambda(x_i;\\theta)) + (1 - \\lambda_i) (log(1 - \\Lambda(x_i; \\theta))]\\text{\nwith an appropriately parameterized A(x; \u03b8)."}, {"title": "4 EXPERIMENTS", "content": "We apply our method to several adaptive decoding procedures: best-of-k and routing (to either\na large model or a sophisticated search algorithm). We evaluate improvements over standard de-\ncoding and procedures that allocate computation uniformly across problem instances, as well as\nperformance relative to the theoretical upper bound. In addition, we provide intrinsic evaluations of\nthe accuracy and calibration of reward predictors A."}, {"title": "4.1 ADAPTIVE BEST-OF-K", "content": "We use best-of-k reranking with a reward model in three settings: Math, Code and Chat.\nMethods: We evaluate the following methods:\n1. Online Ada-BoK (ours): The online variant of our method that solves a joint optimization\nproblem, as detailed in Section 3.2.\n2. Offline Ada-BoK (ours): The offline variant that solves the allocation problem on a held-\nout dataset, as detailed in Section 3.2. This method is only used in Math and Code domains.\n3. Best-of-k: This baseline allocates the same number of samples k = B to every query.\n4. Oracle: Oracle is a non-implementable method that uses the ground truth marginal rewards\nto solve the allocation problem. This method solves the allocation problem by plugging in\nthe true marginal rewards. It is unrealizable in practice, but provides an upper bound on\nthe reward that could be obtained if the learned marginal reward predictor \u25b2 were perfect.\nEvaluation Metrics: Our main evaluation metric for Math and Code is the expected success rate\nunder an oracle verification procedure:\nExpected Success Rate = \\frac{1}{n} \\sum_{i=1}^n E_{y_i\\sim f(x_i,b_i)} [1\\{y_i \\text{ is correct}\\}\\text{\nwhere n is the number of queries. Similarly, for Chat, it is the expected reward:\nExpected Reward = \\frac{1}{N} \\sum_{i=1}^N E_{y_i\\sim f(x_i,b_i)} [r(x, Y_i)]\\text{\nTo estimate these in practice, we sample a large number of generations Bmax for each query and\nthen use bootstrapping to approximate the expectation for different bi .\nThe compute budget B is the average number of samples that may be drawn per query. Best-of-k\nwith k = B uses the same number of total samples as our adaptive method. We plot the performance\nof different methods for different compute budgets, where budgets are specified in terms of B.\nNote that, in the binary reward-based Math and Code domains, it may be efficient to set b\u2081 = 0 for\nsome queries, as many problems have a 0% success rate. In these cases, a default response such as I\ndon't know may be returned, allowing the sample budget to be allocated more effectively elsewhere.\nFor chat experiments, we require all bi \u2265 1."}, {"title": "CODE", "content": "Setup: For the coding experiments, we adopt a subset of TACO, a dataset focused on algorith-\nmic code generation with problems sourced from various programming contest platforms (Li et al.,\n2023). We use a custom off-the-shelf Starcoder-15B model, which was released by the creators of\nthe TACO dataset after fine-tuning on the dataset. We use the official open-source evaluation frame-\nwork released with TACO. A generation is classified as a success if it passes all test cases. Finally,"}, {"title": "MATH", "content": "Setup: We use a subset of the Numina-COT Dataset (Li et al., 2024), which contains Math prob-\nlems obtained from diverse sources. We use Mistral AI's Mathstral-7B model, which is specialized\nfor mathematical tasks, and is based on Mistral 7B (Jiang et al., 2023). We use this model directly\noff-the-shelf and do not perform any fine-tuning. We use an oracle verifier to select the best answer\nout of k. This implies that if the model generates at least one correct answer out of k, it will be\nsuccessful. The verifier uses a 2-stage pipeline that employs the evaluation framework of Hendrycks\net al. in the first stage and a custom LLM-verifier in the second stage (Hendrycks et al., 2021). This\npipeline is detailed in Appendix A. We used the LoRa variant to learn the marginal rewards.\nResults: Figure 3 illustrates the success rate across different compute budgets. Our online and of-\nfline methods consistently outperform best-of-k for all compute budgets. While the improvements\nare marginal in the low-budget regime (B < 8), adaptive computation shines in the moderate-to-"}, {"title": "CHAT", "content": "Setup: For chat, we use a subset of the LMSYS-Chat dataset, which contains real-world conver-\nsations with 25 LLMs (Zheng et al., 2024). We use the popular Gemma-7b-it model, which in an\ninstruction-tuned model for chat (Team et al., 2024). We use reward as our primary evaluation met-\nric. We use an off-the-shelf reward model called NCSOFT/Llama-3-OffsetBias-RM-8B, which\nwas ranked amongst the top 10 on RewardBench at the time of writing. We used the MLP variant to\nlearn the marginal rewards. Finally, we conduct evaluation on 2 different subsets of the dataset-\n1. Full: The vanilla experiment uses the the entire test set.\n2. Tranches: The tranches experiment that uses a subset of 20% from the full test set. To\ncreate this subset, we first generated multiple responses for each query in the test set and\nlabeled them using the reward model. We then calculated the reward variance for each\nquery and selected only those queries that fall in the lowest 10% or highest 10% of variance.\nIn essence, the tranches test set is composed of queries with the lowest and highest variance\nin rewards, representing the two extremes.\nThe main goal of the tranches experiment is to evaluate our method when the distribution of queries\ndiffers from typical datasets, which are often collected in somewhat controlled settings. Such\ndatasets may not fully capture the true diversity of user queries that are encountered by general\nchatbots. While understanding the true distribution of user queries is beyond the scope of this work,\nthe tranches experiment simulates a more extreme distribution to provide performance insights.\nResults: Figure 4 present the results for the two subsets. We focus on the small compute budget\nregime as chat requires much lesser search and we empirically observed rewards saturating quickly.\nIn the case of the full variant, the gains are relatively modest. The Oracle curve shows a 15-25%\nreduction in compute budget while maintaining the same average reward. Although our adaptive\nmethod consistently outperforms best-of-k everywhere, the reductions in compute budget are rela-\ntively small, ranging from 0-10%. This indicates that while adaptive allocation can provide some\nbenefit, an equitable allocation performs almost at par. For the tranches variant, the results are no-\ntably different. Our adaptive method achieves substantial gains, reducing the budget by 25-40%\nwhile matching the rewards of best-of-k."}, {"title": "4.2 ROUTING", "content": "Through this set of experiments, we aim to evaluate the effectiveness of our learned preference\npredictor in routing queries between a weak decoding procedure pW and a strong procedure ps.\nSettings: We present results for two different (pW, pS) pairs which are based on:\n1. Model Size: In this setting, pW and ps are models from the same family but with different\nmodel sizes. We use the instruction-tuned Gemma-2b-it and Gemma-7b-it models as pw\nand ps. We use the full variant of the LMSYS dataset described in Section 4.1 .\n2. Value-Augmented Sampling: In this setting, pW and ps have the same base LLM model.\nDuring decoding, a value function is used to guide search, improving performance but with\nsignificant computational overhead. In this experiment we use Llama-2 7B for both the\nLLM and the value function. In each decoding step, we compute the value of 10 possible\ntokens, increasing the cost of decoding by a factor of 10. We use the harmless subset of the\npopular Anthropic HH dataset (Bai et al., 2022).\nMethods: We present results for the following methods:\n1. Online Routing (ours): \u2206 is used to predict the preference probabilities for a set of\nqueries. These predictions are then routed using the online allocation procedure.\n2. Random: A simple baseline that randomly routes a fixed fraction of queries to \u03c0\u03c2. Any\ntarget B may be obtained by changing this fraction.\n3. Oracle: As above, a non-realizable skyline that uses ground-truth information about the\nreward distribution of pW (\u00b7 | xi) and ps (\u00b7 | xi) for routing.\nSetup: We use expected reward, defined in Eq 10, as our main evaluation metric. For reward\nprediction in experiments with variable model size, we use the smaller pW as the base LLM. That\nis, we train using the hidden states of pW or perform LoRA fine-tuning of pW. This ensures that"}, {"title": "4.3 ANALYSIS", "content": "HOW DO THE LEARNED MARGINAL REWARD PREDICTORS PERFORM?\nSo far, we have seen that our learned marginal reward predictors can be used effectively in adaptive\ncompute allocation. However, we also evaluate their performance independently of the adaptive\ncompute allocation. To this end, we introduce three evaluation metrics:\n1. Avg. (Average Loss): The empirical loss when the prediction for every query is the average\nmarginal reward, i.e., \\Delta = \\frac{1}{n} \\sum_{i=1}^n \\Delta_i. In this case, the model predicts the same marginal\nreward for each input. If the representations of the language model do not carry any mean-\ningful information, we expect the performance to approximately equal this average loss.\n2. Opt.* (Oracle Loss): The loss that a perfect predictor would achieve. Since we use soft\nlabels rather than binary (0-1) labels, the minimum possible loss is a positive value.\n3. Acc. (Accuracy): The accuracy of predictions if the median of A values is used as a\nthreshold. That is, if \u2206i > median, the label for that query is 1, and otherwise 0. The\naccuracy of a random predictor would 50%.\n\nHOW DOES ALLOCATION VARY WITH PREDICTED DIFFICULTY?\nHaving established the performance benefits of adaptive allocation, we now explore what the dis-\ntribution of compute budget for different marginal reward predictions looks like. To investigate\nthis, we focus on the Code and Math domains, where allocation is performed using A, the success\nprobability of a query. We stratify the predicted success probabilities into three evenly-sized bins\naccording to their predicted success probability.\nFigure 6 illustrates how the compute allocation changes across bins as the budget increases. At\nthe lowest budget, the majority of the allocation goes to queries predicted to be easy or medium in\ndifficulty. At the highest compute budget, by contrast, most of the compute is allocated to the hard\nbin. This shift can be understood intuitively: easy-to-moderate queries typically require only a few\nsamples to solve, beyond which the marginal gain of additional samples decays rapidly. In contrast,\nfor queries with low success probabilities, the marginal gain remains high even with a large number\nof samples and decays extremely slowly. Finally, the distinct allocation patterns between Math and\nCode domains highlight how the underlying difficulty distributions of datasets significantly impact\nbudget allocation strategies."}, {"title": "5 RELATED WORK", "content": "Decoding Procedures in LLMs. There has been extensive research into utilizing different decod-\ning schemes to enhance LLM capabilities, usually by expending inference-time compute. One of\nthe most straightforward approaches is best-of-k sampling, where k different model responses are\ngenerated per user query, and the final model output is selected using a reward model (Gao et al.,\n2023; Beirami et al., 2024), majority voting (Wang et al., 2023), or verifiers (Li et al., 2022). An-\nother line of work has shown that allowing LLMs to generate intermediate \"reasoning\u201d steps, or\n\"chain-of-thoughts\u201d (CoTs) can significantly improve their final answer across various tasks (Wei\net al., 2022; Nye et al., 2021; Zhou et al., 2023). However, while this type of test-time search can\nboost performance, it incurs additional computational costs due to the need to decode (potentially\nmany more) reasoning tokens. LLM decoding can also be framed as sampling from a tree of possi-\nble sequences, inspiring research that uses inference-time compute to search through this tree more\neffectively (Yao et al., 2024; Liu et al., 2024; Han et al., 2024).\nAdaptive Computation Time in NN. Several works explored the idea of learning how to adapt\ninference-time compute in neural networks (Graves, 2016; Dehghani et al., 2019; Banino et al.,\n2021). These works focus on networks with recurring components, where the decision is the number\nof times to pass the input through these components. Our work, however, is architecture-agnostic\nand focuses on adaptively selecting a decoding procedure. Recently, Snell et al. (2024) demonstrated\nthat allocating inference-time compute optimally can outperform simply using a larger model. Here,\nwe show how to realize these improvements in practice using learned difficulty predictors."}, {"title": "6 CONCLUSION", "content": "We have introduced an approach for adaptively scaling test-time computation by predicting which\nqueries would benefit the most from additional computation. We first showed that it is possible\nto learn lightweight models that predict marginal rewards on top of pre-trained LMs-indicating\nthat LMs encode usable information about the reward distribution of their responses to inputs. We\nthen presented an allocation algorithm that adaptively allocates computation to queries. Results in\nprogramming, mathematics, and chat show our approach gives significant reductions in computation\nat a target level of output quality, or improvements in quality given a fixed computation budget.\nLimitations: In this work, we assumed that we have access to a verifier in the Math experiments.\nHowever, such verifiers will generally not be available, and using reward models specializing in\nMath might be the more practical implementation of our method.\nFuture Work: The gap between our current performance and the oracle indicates that there is room\nfor improvement in marginal reward prediction. This could be addressed by exploring more ad-\nvanced prediction models or by developing techniques that allocate additional inference-time com-\nputation specifically to obtain better marginal reward estimates."}, {"title": "A EXPERIMENT DETAILS", "content": ""}, {"title": "\u0391.1 \u039c\u0391\u03a4\u0397", "content": "Dataset: We use a subset of the Numina-COT Dataset (Li et al., 2024), which contains math prob-\nlems from various sources. We filtered out multiple choice questions from the dataset. Our final\ndataset contained 15K training samples, 2K validation samples and 3K test samples.\nModel: We use Mistral AI's Mathstral-7B model, which is specialized for mathematical tasks,\nand is based on Mistral 7B (Jiang et al., 2023). We use this model directly off-the-shelf and do not\nperform any fine-tuning.\nVerification Pipeline: We used a 2 stage pipeline to verify the correctness of an answer. The first\nstage used the evaluation framework of Hendrycks et al. (2021). This framework expects the LM to\nenclose its answer within boxed {} and then runs an automated evaluation of the extracted answer\nagainst the ground truth answer. However, we found that the automated evaluation had a high false\nnegative rate for various reasons:\n1. Answer not enclosed within boxed {}: For some queries, the model had the right answer\nbut did not enclose it inside boxed {}. This led to the responses being classified as incorrect.\n2. Errors in Checking: We found that due to errors in checking, many correct responses\nwere marked incorrect. Some examples of false negatives are:\n1. Model Response: frac{25}{2} ; Ground Truth Answer: 12.5\n2. Model Response: x=2 ; Ground Truth Answer: 2\n3. Model Response: -0.5 ; Ground Truth Answer: -frac{1}{2}\n4. Model Response: 11.00; Ground Truth Answer: 11\nThese errors were particularly harmful for the training of our marginal reward predictors. because\nqueries predicted to be (and empirically) easy were being marked incorrect. Furthermore, our online\nallocation exacerbated this issue, by assigning very few samples to such queries.\nTo address this issue, we introduced a second stage in the verification pipeline that used a LM as\nan evaluator. Because stage 1 does not have false positives, we only used stage 2 for responses that\nwere marked incorrect in stage 1. We used Llama-3.1-8B-Instruct with temperature set to 0.1 as our\nevaluation LM, and prompted it as follows:"}, {"title": "A.2 CODE", "content": "Dataset: We use a subset of TACO, a dataset focused on algorithmic code generation with problems\nsourced from various programming contest platforms such as CodeChef, CodeForces and Hacker-\nRank. Our primary reason for selecting TACO was the availability of test cases for most problems,\nwhich is required to train a predictor of success probability. We filtered out problems that did not\nhave a single unit test. We also filtered out problems that were sourced from geeksforgeeks and\naizu, as the official evaluation framework did not support those. Our final dataset consisted of 10K\ntraining samples, 1K validation samples and 1K test samples. Note that these three splits in our\nexperiments were extracted from official TACO training set. This is because the TACO test set has\na very different distribution of problem difficulties, while our method aims to produce accurate in-\ndistribution allocation of computation. We believe producing distributionally robust predictors is an\nimportant topic for future work.\nModel: We used the Starcoder-15B model, which was finetuned and open-sourced by the authors\nof Taco.\nTraining: We generate 100 responses for every query (temperature=0.7) and label them using the\nunit-test verifier to obtain the mean success probability \u5165\u2081. The estimated \u5165\u2081 are then used as targets\nto train (x; 0), the marginal reward predictor.\nEvaluation: We used the official evaluation framework released by the creators of the TACO dataset.\nA response was considered a success if it passed all available test cases. Our adaptive allocation\nalgorithms are assigned a maximum budget of Bmax = 100 samples per query (that is the allocation\nfor any query was capped at 100 samples)."}, {"title": "A.3 CHAT", "content": "Dataset: We use a subset of the LMSYS-Chat dataset, which contains one million real-world con-\nversations with 25 LLMs. We filter the dataset to only select samples in English and with less than\n10 turns. We also filter out samples which were labelled redacted, as these were often artificially\nmodified. Our final dataset consisted of 50K training samples and 5K test samples.\nModel: We used the Gemma-2B-it model. We did not perform any fine-tuning.\nTraining: We generate 8 responses (temperature=0.7) for every query and label them using\nNCSOFT/Llama-3-OffsetBias-RM-8B, which was ranked amongst the top 10 on RewardBench\nat the time of writing. We then use bootstrapping to approximate A. The approximated A\u2081 are\nthen used as targets to train (x; 0), the marginal reward predictor.\nEvaluation: We evaluate using the same reward model as above. Our adaptive allocation algorithm\nwas assigned a maximum budget of Bmax = 8 samples per query."}, {"title": "A.4 ROUTING: MODEL SIZE", "content": "Dataset: We used the same LLMSYS-Chat dataset that we used for the best-of-k chat experiments.\nModel: Our weak model was Gemma-2B-it. Our strong model was Gemma-7B-it. We did not\nperform any fine-tuning on either of these models.\nTraining: We generate 8 responses (at a temperature of 0.7) for every query with both the models.\nWe labelled them using NCSOFT/Llama-3-OffsetBias-RM-8B as the reward model. Supervision\nfor the reward model was then computed using a Monte Carlo estimate of:\n(p^s > p^W|x) = E_{y1~ps,y2~pw} [\u03c3(r(x,y1) \u2013 r(x, y2))]\\text{"}, {"title": "A.5 ROUTING: VAS", "content": "Dataset: We use the harmless subset of the popular Anthropic-HH dataset. We sampled 8K samples\nrandomly from the train set, and 400 samples from the test set.\nModel: Our weak decoding procedure was a fine-tuned Llama-7B model. Our strong decoding\nprocedure used an additional value function that was also based on the Llama-7B model.\nTraining: We generated 4 responses per query. The preference probability was then computed using\na Monte Carlo estimate of:\np(p^s > p^W|x) = E_{y1~ps,y2~pw} [\u03c3(r(x,y1) \u2013 r(x, y2))]\\text{\nWe labelled the responses using OpenAssistant/reward-model-deberta-v3-large-v2 as the reward\nmodel.\nEvaluation: Our adaptive allocation used the predictions of the marginal reward predictor to route\nthe top Bth percentile of queries to ps. We evaluated using the same reward model mentioned\nabove."}]}