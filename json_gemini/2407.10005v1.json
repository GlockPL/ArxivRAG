{"title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond", "authors": ["Yingcong Li", "Ankit Singh Rawat", "Samet Oymak"], "abstract": "Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.", "sections": [{"title": "1 Introduction", "content": "Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and effective alternative to traditional fine-tuning techniques. It enables successful prediction across a wide range of tasks simply through a forward pass, eliminating the need for task-specific model weight updates. Since its introduction, ICL capability has become an important feature of LLM with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via advanced prompting techniques, such as chain-of-thought [Wei et al., 2022]. While ICL already exhibits considerable benefits with a small number of demonstrations, i.e., few-shot data, there is a growing interest in extending its benefits to the many-shot settings, potentially realizing even more pronounced benefits [Agarwal et al., 2024].\nICL ability also presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference [M\u00fcller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models [Schlag et al., 2021, Von Oswald et al., 2023, Ahn et al., 2023] implement preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations.\nTo this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing n examples $(x_i, y_i = x_i^T\\beta + \\xi_i)_{i=1}^n \\subset \\mathbb{R}^d \\times \\mathbb{R}$ and a test instance or query $x_{n+1} \\in \\mathbb{R}^d$ to the model, with d being the feature dimension, $\\beta \\in \\mathbb{R}^d$ being the task weight vector, and $\\xi_i$ denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict $\\hat{y}_{n+1}$ an estimate for $y_{n+1} = x_{n+1}^T\\beta + \\xi_{n+1}$. We aim to provide answers to the following questions by exploring the loss landscape of ICL:\n(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?\n(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires $O(d)$ samples. How can we reconcile this discrepancy between classical learning and ICL?\n(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections $W_k, W_q \\in \\mathbb{R}^{d\\times d}$. What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA [Hu et al., 2022] for adaptation?\nIn this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions:\n(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popu-lar state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Section 4 and Figure 4.\n(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3), where the in-context examples are $\\alpha$-correlated with either the query or the task vector. For"}, {"title": "2 Problem Setup and Preliminaries", "content": "We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., x and X) represent vectors and matrices, respectively. The symbol $\\odot$ is defined as the element-wise (Hadamard) product, and * denotes the convolution operator. $1_d$ and $0_d$ denote the d-dimensional all-ones and all-zeros vectors, respectively; and $I_d$ denotes the identity matrix of dimension $d \\times d$. Additionally, let $\\text{tr}(W)$ denote the trace of the square matrix W.\nAs mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows.\nLinear data distribution. Let $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ be a (feature, label) pair generated by a d-dimensional linear model parameterized by $\\beta \\in \\mathbb{R}^d$, i.e., $y = x^T\\beta + \\xi$, where x and $\\beta$ are feature and task vectors, and $\\xi$ is the label noise. Given demonstrations $(x_i, y_i)_{i=1}^n$ sampled from a single $\\beta$, define the input in-context prompt\n$Z = [Z_1 \\dots Z_n \\ Z_{n+1}] = \\begin{bmatrix}\nX_1 & \\dots & X_n & X_{n+1} \\\\\nY_1 & \\dots & Y_n & 0\\end{bmatrix} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$\nHere, we set $Z_i = \\begin{bmatrix} x_i \\\\ y_i\\end{bmatrix}$ for $i \\leq n$ and the last/query token $Z_{n+1} = \\begin{bmatrix} x_{n+1} \\\\ 0\\end{bmatrix}$. Then, given Z, the goal of the model is to predict the correct label $y_{n+1}$ corresponding to $x_{n+1}$. For cleaner notation, when it is clear from context, we drop the subscript $n + 1$ and set $x = x_{n+1}, Z = Z_{n+1}$. Different from the previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024] where $(x_i, y_i)_{i=1}^{n+1}$ and $\\beta$ are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between $(x_i, y_i)_{i=1}^{n+1}$ and $\\beta$.\nModel architectures. To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work [Von Oswald et al., 2023, Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on single-layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt $Z \\in \\mathbb{R}^{(n+1) \\times (d+1)}$ in (1), which can be treated as a sequence of (d + 1)-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by\n$ATT(Z) = (Z W_qW_k Z^T) Z W_v \\qquad(2a)$\n$SSM(Z) = ((Z W_q) \\odot ((Z_0 W_k \\odot Z_0 W_v) * f)) v \\qquad(2b)$\nwhere $W_k, W_q, W_v \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ denote the key, query and value weight matrices, respectively. In (2b), the parameter $f \\in \\mathbb{R}^{n+1}$ is a 1-D convolutional filter that mixes tokens. The Hadamard product is the gating mechanism [Dauphin et al., 2017] between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For $f$ only, we use indexing $f = [f_0 \\dots f_n]^T \\in \\mathbb{R}^{n+1}$ and given any vector a, denote convolution output $(a * f)_i = \\sum_{j=1}^i f_{i-j}a_j$. Note that our notation slightly differs from the original H3 model [Fu et al., 2023] in two ways:\n1. SSMs provide efficient parameterization of $f$ which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model $s_i = As_{i-1} + Bu_i$ and $y_i = Cs_i$ with parameters $(A \\in \\mathbb{R}^{d \\times d}, B \\in \\mathbb{R}^{d \\times 1}, C \\in \\mathbb{R}^{1 \\times d})$ from which the filter $f$ is obtained via the impulse response $f_i = CA^{i-1}B$ for $i \\geq 0$. Here d is the state dimension and, in practice, A is chosen to be diagonal. Observe that, setting d = 1 and A = $\\rho$, C = B = 1, SSM reduces to the exponential smoothing"}, {"title": "2.1 In-context Linear Estimation", "content": "We will next study the algorithms that can be implemented by the single-layer attention and state-space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-preconditioned gradient descent (PGD) and sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., f) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM.\nBackground: 1-step gradient descent. Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given n samples $(x_i, y_i)_{i=1}^n$, define\n$X = [x_1 \\dots x_n] \\in \\mathbb{R}^{n \\times d}$ and $y = [y_1\\dots y_n] \\in \\mathbb{R}^n$.\nStarting from $\\beta_0 = 0_d$ and letting $\\eta = 1/2$ be the step size, a single-step GD preconditioned with weights W returns prediction\n$\\hat{y} = x^TWX^Ty := g_{PGD}(Z), \\qquad(3)$\nand a single-step sample-weighted GD given weights $w \\in \\mathbb{R}^n$ and $W \\in \\mathbb{R}^{d \\times d}$ returns prediction\n$\\hat{y} = x^TWX^T(w \\odot y) := g_{WPGD}(Z), \\qquad(4)$\nwhere Z is defined in (1) consisting of X, y and x. Our goal is to find the optimal W, as well as w in (4) that minimize the population risks defined as follows.\n$\\min_{W} L_{PGD}(W) \\ \\text{where} \\ L_{PGD}(W) = \\mathbb{E} [(y - g_{PGD}(Z))^2], \\qquad(5a)$\n$\\min_{W,w} L_{WPGD}(W) \\ \\text{where} \\ L_{WPGD}(W) = \\mathbb{E} [(y - g_{WPGD}(Z))^2]. \\qquad(5b)$\nHere, the expectation is over the randomness in $(x_i, \\xi_i)_{i=1}^{n+1}$ and $\\beta$, and we use $W$ to represent the set of corresponding trainable parameters. The search spaces for w and W are $\\mathbb{R}^n$ and $\\mathbb{R}^{d \\times d}$, respectively.\nAs per (2), given input prompt $Z \\in \\mathbb{R}^{(n+1) \\times (d+1)}$, either of the underlying models outputs a (n+1)-length sequence. Note that the label for the query $x = x_{n+1}$ is excluded from the prompt Z. Similar to Ahn et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let $Z_0 = [Z_1 \\dots Z_n \\ 0]^T$ be the features post-causal masking at time/index n + 1. Given weights $W_k, W_q, W_v$ and the filter $f$ for SSM, predictions at the query token $z = \\begin{bmatrix} x \\\\ 0\\end{bmatrix}$ take the following forms following sequence-to-sequence mappings in (2):\n$g_{ATT}(Z) = (z W_q W_k Z_0^T) Z_0 W_v,$\n$g_{SSM}(Z) = ((z W_q) \\odot ((Z_0 W_k \\odot Z_0 W_v) * f)_{n+1}) v,$\nwhere $v \\in \\mathbb{R}^{d+1}$ is the linear prediction head and $((Z_0 W_k \\odot Z_0 W_v) * f)_{n+1}$ returns the last row of the convolution output. Note that SSM can implement the mask by setting $f_0 = 0$. Now consider the meta"}, {"title": "3 Main Results", "content": "In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., $W_k, W_q \\in \\mathbb{R}^{(d+1) \\times r}$, as well as LoRA-tuning when adapting the model under distribution shift.\n3.1 Analysis of Linear Data Models\nWe first consider the standard independent data setting. We will then examine correlated designs.\nIndependent data model. Let $\\Sigma_x$ and $\\Sigma_\\beta$ be the covariance matrices of the input feature and task vectors, respectively, and $\\sigma \\geq 0$ be the noise level. We assume\n$\\beta \\sim N(0, \\Sigma_\\beta), \\ x_i \\sim N(0, \\Sigma_x), \\ \\xi_i \\sim N(0, \\sigma^2), \\ 1 \\leq i \\leq n + 1 \\qquad(7)$\nand the label is obtained via $y_i = x_i^T\\beta + \\xi_i$. Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models.\nTheorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices $\\Sigma_x, \\Sigma_\\beta$ are full rank. Recap the objective from (5a) and let $W^* := \\arg \\min_W L_{PGD}(W)$, and $L_* = L_{PGD}(W^*)$. Additionally, let $\\Sigma = \\Sigma_x^{1/2}\\Sigma_\\beta\\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Then $W^*$ and $L_*$ satisfy\n$W^* = \\Sigma_x^{-1/2} W_* \\Sigma_x^{-1/2} \\qquad \\text{and} \\qquad L_* = M - n \\text{tr} (W_*), \\qquad(8)$\nwhere we define $W_* = (n + 1 + M\\Sigma^{-1})^{-1}$.\nCorollary 1 Consider noiseless i.i.d. linear data where $\\Sigma_x = \\Sigma_\\beta = I_d$ and $\\sigma = 0$. Then, the objective in (5a) returns\n$W = \\frac{1}{n+d+1}I_d \\qquad \\text{and} \\qquad L_* = d - \\frac{nd}{n+d+1}.$\nNote that Theorem 1 is consistent with prior work [Ahn et al., 2023, Theorem 1] when specialized to isotropic task covariance, i.e., $\\Sigma_\\beta = I_d$. However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: What is the optimization landscape with correlated in-context samples? Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies.\nRetrieval augmented generation. To provide a statistical model of the practical RAG approaches, given the query vector $x_{n+1} = x$, we propose to draw ICL demonstrations that are similar to x with"}, {"title": "3.2 Low-rank Parameterization and LORA", "content": "In this section, we investigate training low-rank models, which assume $W_k, W_q \\in \\mathbb{R}^{(d+1) \\times r}$ where r is the rank restriction. Equivalently, we consider objective (5a) under condition $\\text{rank}(W) = r$.\nLemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce rank $(W) \\leq r$ and $W = W^T$. Let $\\Sigma = \\Sigma_x^{1/2} \\Sigma_\\beta \\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Denoting $\\lambda_i$ to be the i'th largest eigenvalue of $\\Sigma$, we have that\n$\\min_{\\text{rank}(W) \\leq r,W=W^T} L(W) = M - \\sum_{i=1}^r \\frac{n \\lambda_i}{(n + 1)\\lambda_i + M} \\qquad(13)$\nNote that $\\text{tr}(\\Sigma) = \\sum_{i=1}^d \\lambda_i$. Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk $L_* = \\sum_{i=1}^d \\frac{n \\lambda_i}{(n + 1)\\lambda_i + M}$.\nImpact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of $\\Sigma$, $\\Sigma^{new}$, $(\\lambda_i)_{i=1}^d, (\\lambda_i^{new})_{i=1}^d$. Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss $L(W_{lora}) := L(W + W_{lora})$ in (5a) with fixed W. Suppose $\\text{tr}(\\Sigma) = \\text{tr}(\\Sigma^{new}) = M$, $\\sigma = 0$ and W is jointly diagonalizable with $\\Sigma$, $\\Sigma^{new}$, then LoRA's risk is upper-bounded by\n$\\min_{\\text{rank}(W_{lora})\\leq r} L(W_{lora}) \\leq \\min_{I \\subseteq [d], \\ |I| \\leq r} \\sum_{i \\in I^c} \\frac{\\lambda_i + M}{n + 1 + M/\\lambda_i} + \\sum_{i \\in I} \\frac{\\lambda_i^{new} + M}{n + 1 + M/ \\lambda_i^{new}} \\qquad(14)$\nNote that, the right hand side is provided assuming the optimal LoRA-updated model $W_{lora}$ is also jointly diagonalizable with covariances $\\Sigma$, $\\Sigma^{new}$, and W."}, {"title": "4 Experiments", "content": "We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption.\nExperimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter $\\beta$ is randomly generated for each training sequence. In all experiments, we set the dimension d = 20. Depending on the in-context length (n), different models are trained to make in-context predictions. We train"}, {"title": "5 Related Work", "content": "There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al., 2023b, Rae et al., 2021] in large language models (LLMs) due to its success in continuously enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Towards this, Xie et al. [2022] explain ICL by language model's ability to perform implicit Bayesian inference where, under specific assumptions on the pre-training data distribution, the model infers a shared latent concept among the in-context examples and leverages the concept to make a prediction. M\u00fcller et al. [2021], Hollmann et al. [2022], M\u00fcller et al. [2023] introduce prior-data fitted network (PFN) to approximate Bayesian inference on synthetic datasets and use it to perform downstream tasks such as tabular dataset classification. On the other hand, Olsson et al. [2022] posit induction heads as the key mechanism enabling ICL in Transformers. Park et al. [2024] study how various distributional properties of training data aid in the emergence of ICL in Transformers.\nIn the previous work, Garg et al. [2022] explored ICL ability of Transformers. In particular, they considered in-context prompts where each in-context example is labeled by a target function from a given function class, including linear models. A number of works have studied this and related settings to develop a theoretical understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins et al., 2024, Lin and Lee, 2024, Li et al., 2024, Bai et al., 2024, Aky\u00fcrek et al., 2023, Zhang et al., 2023, Du et al., 2023]. Aky\u00fcrek et al. [2023] focus on linear regression and provide a construction of Transformer weights that can enable a single step of GD based on in-context examples. They further show that Transformers trained on in-context prompts exhibit behaviors similar to the models recovered via explicit learning algorithm on the in-context examples in a prompt. Along the similar line, Von Oswald et al. [2023] provide a construction of weights in linear attention-only Transformers that can emulate GD steps on in-context examples for a linear regression task. Interestingly, they find similarity between their constructed networks and the networks resulting from training on in-context"}, {"title": "6 Discussion", "content": "In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We have established a general connection between ICL and gradient methods that accounts for correlated data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization"}, {"title": "A Equivalence among Gradient Descent, Attention, and State-Space Models", "content": "In this section, we present the proofs related to Section 2. Recap that given data\n$X = [x_1 \\dots x_n] \\in \\mathbb{R}^{n \\times d}$,\n$\\xi = [\\xi_1 \\dots \\xi_n] \\in \\mathbb{R}^n$,\n$y = [y_1 \\dots y_n] = X\\beta + \\xi \\in \\mathbb{R}^n$,\n$Z = [Z_1 \\dots Z_n \\ Z_{n+1}] = \\begin{bmatrix}\nX_1 & \\dots & X_n & X_{n+1} \\\\\nY_1 & \\dots & Y_n & 0\\end{bmatrix} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$\nand corresponding prediction functions\n$g_{PGD}(Z) = x^TWX^Ty, \\qquad(15a)$\n$g_{WPGD}(Z) = x^TWX^T(w \\odot y), \\qquad(15b)$\n$g_{ATT}(Z) = (z W_qW_k Z_0^T) Z_0 W_v, \\qquad(15c)$\n$g_{SSM}(Z) = ((z W_q) \\odot ((Z_0 W_k \\odot Z_0 W_v) * f)_{n+1}) v, \\qquad(15d)$\nwe have objectives\n$\\min_{W} L_{PGD}(W) \\ \\text{where} \\ L_{PGD}(W) = \\mathbb{E} [(y - g_{PGD}(Z))^2], \\qquad(16a)$\n$\\min_{W,w} L_{WPGD}(W) \\ \\text{where} \\ L_{WPGD}(W) = \\mathbb{E} [(y - g_{WPGD}(Z))^2], \\qquad(16b)$\n$\\min_{W_k,W_q,W_v,v} L_{ATT}(W) \\ \\text{where} \\ L_{ATT}(W) = \\mathbb{E} [(y - g_{ATT}(Z))^2], \\qquad(16c)$\n$\\min_{W_k,W_q,W_v,v,f} L_{SSM}(W) \\ \\text{where} \\ L_{SSM}(W) = \\mathbb{E} [(y - g_{SSM}(Z))^2]. \\qquad(16d)$\nHere, the expectation is over the randomness in $(x_i, \\xi_i)_{i=1}^{n+1}$ and $\\beta$, and the search space for W is $\\mathbb{R}^{d \\times d}$, for w is $\\mathbb{R}^n$, for $W_k, W_q, W_v$ is $\\mathbb{R}^{(d+1) \\times (d+1)}$, for v is $\\mathbb{R}^{d+1}$, and for f is $\\mathbb{R}^{n+1}$.\nA.1 Proof of Proposition 1\nConsider the problem setting as discussed in Section 2, Proposition 1 can be proven by the following two lemmas."}, {"title": "B Analysis of General Data Distribution", "content": "In this section, we provide the proofs in Section 3, which focuses on solving Objective (5a). For the sake of clean notation, let $L(W) := L_{PGD}(W)$ and $g := g_{PGD}$ in this section."}]}