{"title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond", "authors": ["Yingcong Li", "Ankit Singh Rawat", "Samet Oymak"], "abstract": "Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.", "sections": [{"title": "1 Introduction", "content": "Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and effective alternative to traditional fine-tuning techniques. It enables successful prediction across a wide range of tasks simply through a forward pass, eliminating the need for task-specific model weight updates. Since its introduction, ICL capability has become an important feature of LLM with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via advanced prompting techniques, such as chain-of-thought [Wei et al., 2022]. While ICL already exhibits considerable benefits with a small number of demonstrations, i.e., few-shot data, there is a growing interest in extending its benefits to the many-shot settings, potentially realizing even more pronounced benefits [Agarwal et al., 2024].\nICL ability also presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference [M\u00fcller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models [Schlag et al., 2021, Von Oswald et al., 2023, Ahn et al., 2023] implement preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations.\nTo this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing $n$ examples $(x_i, y_i = x_i^T\\beta + \\xi_i)_{i=1} \\subset \\mathbb{R}^d \\times \\mathbb{R}$ and a test instance or query $x_{n+1} \\in \\mathbb{R}^d$ to the model, with $d$ being the feature dimension, $\\beta \\in \\mathbb{R}^d$ being the task weight vector, and $\\xi_i$ denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict $\\hat{y}_{n+1}$ an estimate for $y_{n+1} = x_{n+1}^T\\beta + \\xi_{n+1}$. We aim to provide answers to the following questions by exploring the loss landscape of ICL:\n(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?\n(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires $\\mathcal{O}(d)$ samples. How can we reconcile this discrepancy between classical learning and ICL?\n(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections $W_k, W_q \\in \\mathbb{R}^{d \\times d}$. What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA [Hu et al., 2022] for adaptation?\nIn this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions:\n(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popular state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Section 4 and Figure 4.\n(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3), where the in-context examples are $\\alpha$-correlated with either the query or the task vector. For"}, {"title": "2 Problem Setup and Preliminaries", "content": "We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., $\\mathbf{x}$ and $\\mathbf{X}$) represent vectors and matrices, respectively. The symbol $\\odot$ is defined as the element-wise (Hadamard) product, and $\\ast$ denotes the convolution operator. $\\mathbf{1}_d$ and $\\mathbf{0}_d$ denote the $d$-dimensional all-ones and all-zeros vectors, respectively; and $I_d$ denotes the identity matrix of dimension $d \\times d$. Additionally, let $\\text{tr}(\\mathbf{W})$ denote the trace of the square matrix $\\mathbf{W}$.\nAs mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows.\nLinear data distribution. Let $(\\mathbf{x}, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ be a (feature, label) pair generated by a $d$-dimensional linear model parameterized by $\\beta \\in \\mathbb{R}^d$, i.e., $y = \\mathbf{x}^T\\beta + \\xi$, where $\\mathbf{x}$ and $\\beta$ are feature and task vectors, and $\\xi$ is the label noise. Given demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^n$ sampled from a single $\\beta$, define the input in-context prompt\n$Z = [\\mathbf{Z}_1 \\ldots \\mathbf{Z}_n \\mathbf{Z}_{n+1}] = \\begin{bmatrix} \\mathbf{x}_1 & \\ldots & \\mathbf{x}_n & \\mathbf{x}_{n+1} \\\\ y_1 & \\ldots & y_n & 0 \\end{bmatrix}^T \\in \\mathbb{R}^{(n+1) \\times (d+1)}$\nHere, we set $\\mathbf{Z}_i = \\begin{bmatrix} \\mathbf{x}_i \\\\ y_i \\end{bmatrix}$ for $i \\leq n$ and the last/query token $\\mathbf{Z}_{n+1} = \\begin{bmatrix} \\mathbf{x}_{n+1} \\\\ 0 \\end{bmatrix}$. Then, given $\\mathbf{Z}$, the goal of the model is to predict the correct label $y_{n+1}$ corresponding to $\\mathbf{x}_{n+1}$. For cleaner notation, when it is clear from context, we drop the subscript $n + 1$ and set $\\mathbf{x} = \\mathbf{x}_{n+1}, \\mathbf{Z} = \\mathbf{Z}_{n+1}$. Different from the previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024] where $(\\mathbf{x}_i, y_i)_{i=1}^n$ and $\\beta$ are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between $(\\mathbf{x}_i, y_i)_{i=1}^n$ and $\\beta$.\nModel architectures. To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work [Von Oswald et al., 2023, Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on single-layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt $\\mathbf{Z} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$ in (1), which can be treated as a sequence of $(d + 1)$-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by\n$\\text{ATT}(\\mathbf{Z}) = (\\mathbf{Z}\\mathbf{W}_q\\mathbf{W}_k^T\\mathbf{Z}^T)\\mathbf{Z}\\mathbf{W}_v \\quad (2a)$\n$\\text{SSM}(\\mathbf{Z}) = ((\\mathbf{Z}\\mathbf{W}_q) \\odot ((\\mathbf{Z}\\mathbf{W}_k \\odot \\mathbf{Z}\\mathbf{W}_v) \\ast f)) \\quad (2b)$\nwhere $\\mathbf{W}_k, \\mathbf{W}_q, \\mathbf{W}_v \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ denote the key, query and value weight matrices, respectively. In (2b), the parameter $f \\in \\mathbb{R}^{n+1}$ is a 1-D convolutional filter that mixes tokens. The Hadamard product is the gating mechanism [Dauphin et al., 2017] between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For $f$ only, we use indexing $f = [f_0 \\ldots f_n] \\in \\mathbb{R}^{n+1}$ and given any vector $\\mathbf{a}$, denote convolution output $(\\mathbf{a} \\ast f)_i = \\sum_{j=1}^i f_{i-j}a_j$. Note that our notation slightly differs from the original H3 model [Fu et al., 2023] in two ways:\n1. SSMs provide efficient parameterization of $f$ which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model $s_i = A s_{i-1} + B u_i$ and $y_i = C s_i$ with parameters $(A \\in \\mathbb{R}^{d \\times d}, B \\in \\mathbb{R}^{d \\times 1}, C \\in \\mathbb{R}^{1 \\times d})$ from which the filter $f$ is obtained via the impulse response $f_i = C A^i B$ for $i \\geq 0$. Here $d$ is the state dimension and, in practice, $A$ is chosen to be diagonal. Observe that, setting $d = 1$ and $A = \\rho, C = B = 1$, SSM reduces to the exponential smoothing"}, {"title": "2.1 In-context Linear Estimation", "content": "We will next study the algorithms that can be implemented by the single-layer attention and state-space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-preconditioned gradient descent (PGD) and sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., $f$) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM.\nBackground: 1-step gradient descent. Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given $n$ samples $(x_i, y_i)_{i=1}^n$, define\n$\\mathbf{X} = [\\mathbf{x}_1 \\ldots \\mathbf{x}_n] \\in \\mathbb{R}^{n \\times d} \\text{ and } \\mathbf{y} = [y_1 \\ldots y_n] \\in \\mathbb{R}^n$.\nStarting from $\\beta_0 = \\mathbf{0}_d$ and letting $\\eta = 1/2$ be the step size, a single-step GD preconditioned with weights $\\mathbf{W}$ returns prediction\n$\\hat{y} = \\mathbf{x}^T\\mathbf{W}\\mathbf{X}^T\\mathbf{y} := \\text{g}_{PGD}(\\mathbf{Z}), \\quad (3)$\nand a single-step sample-weighted GD given weights $\\omega \\in \\mathbb{R}^n$ and $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ returns prediction\n$\\hat{y} = \\mathbf{x}^T\\mathbf{W}\\mathbf{X}^T (\\omega \\odot \\mathbf{y}) := \\text{g}_{WPGD}(\\mathbf{Z}), \\quad (4)$\nwhere $\\mathbf{Z}$ is defined in (1) consisting of $\\mathbf{X}, \\mathbf{y}$ and $\\mathbf{x}$. Our goal is to find the optimal $\\mathbf{W}$, as well as $\\omega$ in (4) that minimize the population risks defined as follows.\n$\\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W}) \\text{ where } \\mathcal{L}_{PGD}(\\mathbf{W}) = \\mathbb{E} [(\\hat{y} - \\text{g}_{PGD}(\\mathbf{Z}))^2], \\quad (5a)$\n$\\min_{\\mathbf{W},\\omega} \\mathcal{L}_{WPGD}(\\mathbf{W}) \\text{ where } \\mathcal{L}_{WPGD}(\\mathbf{W}) = \\mathbb{E} [(\\hat{y} - \\text{g}_{WPGD}(\\mathbf{Z}))^2]. \\quad (5b)$\nHere, the expectation is over the randomness in $(\\mathbf{x}_i, \\xi_i)_{i=1}^{n+1}$ and $\\beta$, and we use $\\mathbf{W}$ to represent the set of corresponding trainable parameters. The search spaces for $\\omega$ and $\\mathbf{W}$ are $\\mathbb{R}^n$ and $\\mathbb{R}^{d \\times d}$, respectively.\nAs per (2), given input prompt $\\mathbf{Z} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$, either of the underlying models outputs a $(n+1)$-length sequence. Note that the label for the query $\\mathbf{x} = \\mathbf{x}_{n+1}$ is excluded from the prompt $\\mathbf{Z}$. Similar to Ahn et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let $\\mathbf{Z}_0 = [\\mathbf{Z}_1 \\ldots \\mathbf{Z}_n \\mathbf{0}]^T$ be the features post-causal masking at time/index $n + 1$. Given weights $\\mathbf{W}_k, \\mathbf{W}_q, \\mathbf{W}_v$, and the filter $f$ for SSM, predictions at the query token $\\mathbf{z} = \\begin{bmatrix} \\mathbf{x} \\\\ 0 \\end{bmatrix}$ take the following forms following sequence-to-sequence mappings in (2):\n$\\text{g}_{ATT}(\\mathbf{Z}) = (\\mathbf{z}^T \\mathbf{W}_q\\mathbf{W}_k^T \\mathbf{Z}_0^T \\mathbf{Z}_0 \\mathbf{W}_v) v, \\quad (6)$\n$\\text{g}_{SSM}(\\mathbf{Z}) = ((\\mathbf{z}^T\\mathbf{W}_q) \\odot ((\\mathbf{Z}_0 \\mathbf{W}_k \\odot \\mathbf{Z}_0 \\mathbf{W}_v) \\ast f)_{n+1})v, \\quad (6)$\nwhere $v \\in \\mathbb{R}^{d+1}$ is the linear prediction head and $((\\mathbf{Z}_0 \\mathbf{W}_k \\odot \\mathbf{Z}_0 \\mathbf{W}_v) \\ast f)_{n+1}$ returns the last row of the convolution output. Note that SSM can implement the mask by setting $f_0 = 0$. Now consider the meta"}, {"title": "3 Main Results", "content": "In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., $W_k, W_q \\in \\mathbb{R}^{(d+1) \\times r}$, as well as LoRA-tuning when adapting the model under distribution shift.\n3.1 Analysis of Linear Data Models\nWe first consider the standard independent data setting. We will then examine correlated designs.\nIndependent data model. Let $\\Sigma_x$ and $\\Sigma_\\beta$ be the covariance matrices of the input feature and task vectors, respectively, and $\\sigma \\geq 0$ be the noise level. We assume\n$\\beta \\sim \\mathcal{N}(0, \\Sigma_\\beta), \\quad \\mathbf{x}_i \\sim \\mathcal{N}(0, \\Sigma_x), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2), \\quad 1 \\leq i \\leq n + 1 \\quad (7)$\nand the label is obtained via $y_i = \\mathbf{x}_i^T\\beta + \\xi_i$. Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models.\nTheorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices $\\Sigma_x, \\Sigma_\\beta$ are full rank. Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}^* = \\mathcal{L}_{PGD}(\\mathbf{W})$. Additionally, let $\\Sigma = \\Sigma_x^{1/2} \\Sigma_\\beta \\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Then $\\mathbf{W}^*$ and $\\mathcal{L}_e$ satisfy\n$\\mathbf{W}^* = \\Sigma_x^{1/2} \\mathbf{W}_* \\Sigma_x^{-1/2} \\quad \\text{and} \\quad \\mathcal{L}_* = M - n \\text{tr} (\\tilde{\\mathbf{W}}),$ $\\quad (8)$\nwhere we define $\\mathbf{W}_* = ((\\frac{n}{(n + 1)}I_d + M\\Sigma^{-1})^{-1}$.\nCorollary 1 Consider noiseless i.i.d. linear data where $\\Sigma_x = \\Sigma_\\beta = I_d$ and $\\sigma = 0$. Then, the objective in (5a) returns\n$\\mathbf{W}^* = \\frac{1}{n+d+1}I_d \\quad \\text{and} \\quad \\mathcal{L}* = d - \\frac{nd}{n+d+1}$.\nSee Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work [Ahn et al., 2023, Theorem 1] when specialized to isotropic task covariance, i.e., $\\Sigma_\\beta = I_d$. However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: What is the optimization landscape with correlated in-context samples? Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies.\nRetrieval augmented generation. To provide a statistical model of the practical RAG approaches, given the query vector $\\mathbf{x}_{n+1} = \\mathbf{x}$, we propose to draw ICL demonstrations that are similar to $\\mathbf{x}$ with"}, {"title": "3.1 Analysis of Linear Data Models", "content": "the same shared task vector $\\beta$. Modeling feature similarity through the cosine angle, RAG should sample the ICL examples $\\mathbf{x}_i, i \\leq n$, from the original feature distribution conditioned on the event $\\cos(\\mathbf{x}_i, \\mathbf{x}) \\geq \\alpha$ where $\\alpha$ is the similarity threshold. As an approximate proxy, under the Gaussian distribution model, we assume that $\\beta \\sim \\mathcal{N}(0,I_d), \\mathbf{x} \\sim \\mathcal{N}(0, I_d)$ and that RAG samples $\\alpha$-correlated demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^n$ as follows:\n$\\mathbf{x}_i | \\mathbf{x} \\sim \\mathcal{N}(\\alpha \\mathbf{x}, (1 - \\alpha^2)I_d), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2) \\text{ and } y_i = \\mathbf{x}_i^T \\beta + \\xi_i, \\quad 1 \\leq i \\leq n. \\quad (9)$\nNote that the above normalization ensures that the marginal feature distribution remains $\\mathcal{N}(0, I_d)$. The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis by assuming $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ and $d/n = \\mathcal{O}(1)$ where $\\mathcal{O}(\\cdot)$ denotes proportionality, our derivation leads to the following result:\nTheorem 2 Consider linear model as defined in (9). Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}_* = \\mathcal{L}_{PGD}(\\mathbf{W}^*)$. Additionally, let $\\kappa = \\alpha^2d + 1$ and suppose $\\alpha = \\mathcal{O}(1/\\sqrt{d}), d/n = \\mathcal{O}(1)$ and $d$ is sufficiently large. Then $\\mathbf{W}^*$ and $\\mathcal{L}_*$ have approximate forms\n$\\mathbf{W}^* \\approx \\frac{1}{\\kappa n + d + \\sigma^2} I_d \\quad \\text{and} \\quad \\mathcal{L}_* \\approx d + \\sigma^2 - \\frac{\\kappa nd}{\\kappa n + d + \\sigma^2}. \\quad (10)$\nHere, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, $\\alpha^2d + 1$ is the dominant multiplier ahead of $n$ in both equations. Thus, we deduce that, RAG model follows the same error bound as the independent data model, however, its sample size is amplified by a factor of $\\alpha^2d + 1$. $\\alpha = 0$ reduces to the result of Corollary 1 whereas we need to set $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ for constant amplification. When $\\alpha = 1$, RAG achieves the approximate risk $\\mathcal{L}_* \\approx \\frac{d}{\\frac{n}{(a^2d+1)}} + \\sigma^2$, where the constant bias is due to the higher order moments (e.g., the 4'th and 6'th moments) of the standard Gaussian distribution. As $d$ increases, the normalized loss $\\mathcal{L}_*/d \\rightarrow 0$. The full analysis of its optimal solution $\\mathbf{W}^*$ and loss $\\mathcal{L}_*$ are deferred Theorem 4 in Appendix B.3.\nTask-feature alignment. We also consider another dependent data setting where task and feature vectors are assumed to be correlated. This dataset model has the following motivation: In general, an LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of the prompt), the LLM output becomes more deterministic and there are much fewer token candidates. For instance, if the task is \u201cCountry\u201d, \u201cFrance\u201d is a viable output compared to \u201cHelium\u201d and vice versa when the task is \u201cChemistry\u201d. Formally speaking, this can be formalized as the input $\\mathbf{x}$ having a diverse distribution whereas it becomes more predictable conditioned on $\\beta$. Therefore, it can be captured through a linear model by making the conditional covariance of $\\mathbf{x} \\vert \\beta$ to be approximately low-rank. This formalism can be viewed as a spectral alignment between input and task, which is also well-established in deep learning both empirically and theoretically [Li et al., 2020, Arora et al., 2019, Canatar et al., 2021, Cao et al., 2019]. Here, we consider such a setting where the shared task vector is sampled as standard Gaussian distribution $\\beta \\sim \\mathcal{N}(0, I_d)$ and letting $\\kappa = \\alpha^2d + 1$, we sample the $\\alpha$-correlated ICL demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^{n+1}$ as follows:\n$\\mathbf{x}_i \\vert \\beta \\sim \\mathcal{N}(\\alpha \\beta, I_d), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2) \\text{ and } y_i = \\kappa^{-1/2} \\mathbf{x}_i^T\\beta + \\xi_i, \\quad 1 \\leq i \\leq n + 1. \\quad (11)$\nAbove, $\\kappa^{-1/2}$ is a normalization factor to ensure that label variance remains invariant to $\\alpha$. To keep the exposition cleaner, we defer the full analysis of its optimal solution $\\mathbf{W}^*$ and loss $\\mathcal{L}_*$ to Theorem 5 in Appendix B.4. Similar to the RAG setting, by assuming $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ and $d/n = \\mathcal{O}(1)$, we obtain the following results for the optimal parameter and risk.\nTheorem 3 Consider linear model as defined in (11). Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}_* = \\mathcal{L}_{PGD}(\\mathbf{W}^*)$. Additionally, given $\\kappa = \\alpha^2d + 1$ and suppose $\\alpha = \\mathcal{O}(1/\\sqrt{d}), d/n = \\mathcal{O}(1)$ and $d$ is sufficiently large. Then $\\mathbf{W}^*$ and $\\mathcal{L}_*$ have approximate forms\n$\\mathbf{W}^* \\approx \\frac{1}{\\kappa n + (d + \\sigma^2)/\\kappa} I_d \\quad \\text{and} \\quad \\mathcal{L}_* \\approx d + \\sigma^2 - \\frac{\\kappa nd}{\\kappa n + (d + \\sigma^2)/\\kappa}. \\quad (12)$\nSimilar to (10), (12) contains $\\kappa = \\alpha^2 + 1$ multiplier ahead of $n$, which reduces the in-context sample complexity and setting $\\alpha = 0$ reduces to the results of Corollary 1."}, {"title": "3.2 Low-rank Parameterization and LORA", "content": "In this section, we investigate training low-rank models, which assume $\\mathbf{W}_k, \\mathbf{W}_q \\in \\mathbb{R}^{(d+1) \\times r}$ where $r$ is the rank restriction. Equivalently, we consider objective (5a) under condition $\\text{rank}(\\mathbf{W}) = r$.\nLemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce $\\text{rank}(\\mathbf{W}) \\leq r$ and $\\mathbf{W} = \\mathbf{W}^T$. Let $\\tilde{\\Sigma} = \\Sigma_x^{1/2} \\Sigma_\\beta \\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Denoting $\\lambda_i$ to be the $i$'th largest eigenvalue of $\\tilde{\\Sigma}$, we have that\n$\\min_{\\substack{\\mathbf{rank}(\\mathbf{W}) \\leq r, \\mathbf{W} = \\mathbf{W}^T}} \\mathcal{L}(\\mathbf{W}) = M - \\sum_{i=1}^d \\frac{n \\lambda_i}{(n + 1) \\lambda_i + M}. \\quad (13)$\nNote that $\\text{tr} (\\Sigma) = \\sum_{i=1}^d \\lambda_i$. Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk $\\mathcal{L}_e = \\sum_{i=1}^d \\frac{\\lambda_i}{n+1 + M/\\lambda_i}$. See Appendix C.1 for more details.\nImpact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of $\\Sigma, \\Sigma^{new}, \\{\\lambda_i\\}_{i=1}^d, \\{\\lambda_i^{new}\\}_{i=1}^d$. Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss $\\mathcal{L}(\\mathbf{W}_{lora}) := \\mathcal{L}(\\mathbf{W} + \\mathbf{W}_{lora})$ in (5a) with fixed $\\mathbf{W}$. Suppose $\\text{tr} (\\Sigma) = \\text{tr} (\\Sigma^{new}) = M, \\sigma = 0$ and $\\mathbf{W}$ is jointly diagonalizable with $\\Sigma, \\Sigma^{new}$, then LoRA's risk is upper-bounded by\n$\\min_{\\text{rank}(\\mathbf{W}_{lora}) \\leq r} \\mathcal{L}(\\mathbf{W}_{lora}) \\leq \\min_{\\mathcal{I} \\subseteq [d], \\vert \\mathcal{I} \\vert \\leq r} \\Biggl[ \\sum_{i \\notin \\mathcal{I}} \\frac{\\lambda_i + M}{n + 1 + M/\\lambda_i} + \\sum_{i \\in \\mathcal{I}} \\frac{\\lambda_i^{new} + M}{n + 1 + M/\\lambda_i^{new}} \\Biggr] . \\quad (14)$\nNote that, the right hand side is provided assuming the optimal LoRA-updated model $\\mathbf{W}_{lora}$ is also jointly diagonalizable with covariances $\\Sigma, \\Sigma^{new}$, and $\\mathbf{W}$."}, {"title": "4 Experiments", "content": "We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption.\nExperimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter $\\beta$ is randomly generated for each training sequence. In all experiments, we set the dimension $d = 20$. Depending on the in-context length (n), different models are trained to make in-context predictions. We train"}, {"title": "5 Related Work", "content": "There is growing interest in understanding the mechanisms behind ICL [Brown et al."}, {"title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond", "authors": ["Yingcong Li", "Ankit Singh Rawat", "Samet Oymak"], "abstract": "Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.", "sections": [{"title": "1 Introduction", "content": "Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and effective alternative to traditional fine-tuning techniques. It enables successful prediction across a wide range of tasks simply through a forward pass, eliminating the need for task-specific model weight updates. Since its introduction, ICL capability has become an important feature of LLM with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via advanced prompting techniques, such as chain-of-thought [Wei et al., 2022]. While ICL already exhibits considerable benefits with a small number of demonstrations, i.e., few-shot data, there is a growing interest in extending its benefits to the many-shot settings, potentially realizing even more pronounced benefits [Agarwal et al., 2024].\nICL ability also presents an important research avenue to develop stronger theoretical and mechanistic understanding of large language models. To this aim, there has been significant recent interest in demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference [M\u00fcller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned with understanding the optimization landscape of ICL, which is also crucial for understanding the generalization properties of the model. A notable result in this direction is the observation that linear attention models [Schlag et al., 2021, Von Oswald et al., 2023, Ahn et al., 2023] implement preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While this line of works provide a fresh perspective to ICL, the existing studies do not address many questions arising from real-life applications nor provide guiding principles for various ICL setups motivated by practical considerations.\nTo this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an in-context prompt containing $n$ examples $(x_i, y_i = x_i^T\\beta + \\xi_i)_{i=1} \\subset \\mathbb{R}^d \\times \\mathbb{R}$ and a test instance or query $x_{n+1} \\in \\mathbb{R}^d$ to the model, with $d$ being the feature dimension, $\\beta \\in \\mathbb{R}^d$ being the task weight vector, and $\\xi_i$ denoting the noise in individual labels. Given the in-context prompt, the model is tasked to predict $\\hat{y}_{n+1}$ an estimate for $y_{n+1} = x_{n+1}^T\\beta + \\xi_{n+1}$. We aim to provide answers to the following questions by exploring the loss landscape of ICL:\n(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative sequence models implement richer algorithms beyond PGD?\n(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear estimation typically requires $\\mathcal{O}(d)$ samples. How can we reconcile this discrepancy between classical learning and ICL?\n(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and query projections $W_k, W_q \\in \\mathbb{R}^{d \\times d}$. What happens when they are low-rank? What happens when there is distribution shift between training and test in-context prompts and we use LoRA [Hu et al., 2022] for adaptation?\nIn this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL with 1-layer models and make the following contributions:\n(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popular state-space model (SSM). We prove that under correlated design, both models implement 1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the advantage of implementing sample-weighting which allows it to outperform linear attention in temporally-heterogeneous problem settings in Section 4 and Figure 4.\n(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments are zero. Through this, we can assess the impact of distributional alignment on the sample complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3), where the in-context examples are $\\alpha$-correlated with either the query or the task vector. For"}, {"title": "2 Problem Setup and Preliminaries", "content": "We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., $\\mathbf{x}$ and $\\mathbf{X}$) represent vectors and matrices, respectively. The symbol $\\odot$ is defined as the element-wise (Hadamard) product, and $\\ast$ denotes the convolution operator. $\\mathbf{1}_d$ and $\\mathbf{0}_d$ denote the $d$-dimensional all-ones and all-zeros vectors, respectively; and $I_d$ denotes the identity matrix of dimension $d \\times d$. Additionally, let $\\text{tr}(\\mathbf{W})$ denote the trace of the square matrix $\\mathbf{W}$.\nAs mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts containing in-context data following a linear model. We construct the input in-context prompt similar to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows.\nLinear data distribution. Let $(\\mathbf{x}, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ be a (feature, label) pair generated by a $d$-dimensional linear model parameterized by $\\beta \\in \\mathbb{R}^d$, i.e., $y = \\mathbf{x}^T\\beta + \\xi$, where $\\mathbf{x}$ and $\\beta$ are feature and task vectors, and $\\xi$ is the label noise. Given demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^n$ sampled from a single $\\beta$, define the input in-context prompt\n$Z = [\\mathbf{Z}_1 \\ldots \\mathbf{Z}_n \\mathbf{Z}_{n+1}] = \\begin{bmatrix} \\mathbf{x}_1 & \\ldots & \\mathbf{x}_n & \\mathbf{x}_{n+1} \\\\ y_1 & \\ldots & y_n & 0 \\end{bmatrix}^T \\in \\mathbb{R}^{(n+1) \\times (d+1)}$\nHere, we set $\\mathbf{Z}_i = \\begin{bmatrix} \\mathbf{x}_i \\\\ y_i \\end{bmatrix}$ for $i \\leq n$ and the last/query token $\\mathbf{Z}_{n+1} = \\begin{bmatrix} \\mathbf{x}_{n+1} \\\\ 0 \\end{bmatrix}$. Then, given $\\mathbf{Z}$, the goal of the model is to predict the correct label $y_{n+1}$ corresponding to $\\mathbf{x}_{n+1}$. For cleaner notation, when it is clear from context, we drop the subscript $n + 1$ and set $\\mathbf{x} = \\mathbf{x}_{n+1}, \\mathbf{Z} = \\mathbf{Z}_{n+1}$. Different from the previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024] where $(\\mathbf{x}_i, y_i)_{i=1}^n$ and $\\beta$ are assumed to be independent, our analysis focuses on a more general linear setting that captures the dependency between $(\\mathbf{x}_i, y_i)_{i=1}^n$ and $\\beta$.\nModel architectures. To start with, we first review the architectures of both Transformer and state-space model (SSM). Similar to the previous work [Von Oswald et al., 2023, Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on single-layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the Transformer. Given the input prompt $\\mathbf{Z} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$, which can be treated as a sequence of $(d + 1)$-dimensional tokens, the single-layer linear attention ATT and H3-like single-layer SSM SSM are denoted by\n$\\text{ATT}(\\mathbf{Z}) = (\\mathbf{Z}\\mathbf{W}_q\\mathbf{W}_k^T\\mathbf{Z}^T)\\mathbf{Z}\\mathbf{W}_v \\quad (2a)$\n$\\text{SSM}(\\mathbf{Z}) = ((\\mathbf{Z}\\mathbf{W}_q) \\odot ((\\mathbf{Z}\\mathbf{W}_k \\odot \\mathbf{Z}\\mathbf{W}_v) \\ast f)) \\quad (2b)$\nwhere $\\mathbf{W}_k, \\mathbf{W}_q, \\mathbf{W}_v \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ denote the key, query and value weight matrices, respectively. In (2b), the parameter $f \\in \\mathbb{R}^{n+1}$ is a 1-D convolutional filter that mixes tokens. The Hadamard product is the gating mechanism [Dauphin et al., 2017] between key and query channels, which is crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For $f$ only, we use indexing $f = [f_0 \\ldots f_n] \\in \\mathbb{R}^{n+1}$ and given any vector $\\mathbf{a}$, denote convolution output $(\\mathbf{a} \\ast f)_i = \\sum_{j=1}^i f_{i-j}a_j$. Note that our notation slightly differs from the original H3 model [Fu et al., 2023] in two ways:\n1. SSMs provide efficient parameterization of $f$ which would otherwise grow with sequence length. In essence, H3 utilizes a linear state-space model $s_i = A s_{i-1} + B u_i$ and $y_i = C s_i$ with parameters $(A \\in \\mathbb{R}^{d \\times d}, B \\in \\mathbb{R}^{d \\times 1}, C \\in \\mathbb{R}^{1 \\times d})$ from which the filter $f$ is obtained via the impulse response $f_i = C A^i B$ for $i \\geq 0$. Here $d$ is the state dimension and, in practice, $A$ is chosen to be diagonal. Observe that, setting $d = 1$ and $A = \\rho, C = B = 1$, SSM reduces to the exponential smoothing"}, {"title": "2.1 In-context Linear Estimation", "content": "We will next study the algorithms that can be implemented by the single-layer attention and state-space models. Through this, we will show that training ATT and SSM with linear ICL data is equivalent to the prediction obtained from one step of optimally-preconditioned gradient descent (PGD) and sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that under mild assumption, the optimal sample weighting for SSM (e.g., $f$) is an all-ones vector and therefore, establishing the equivalence among PGD, ATT, and SSM.\nBackground: 1-step gradient descent. Consider minimizing squared loss and solving linear regression using one step of PGD and WPGD. Given $n$ samples $(x_i, y_i)_{i=1}^n$, define\n$\\mathbf{X} = [\\mathbf{x}_1 \\ldots \\mathbf{x}_n] \\in \\mathbb{R}^{n \\times d} \\text{ and } \\mathbf{y} = [y_1 \\ldots y_n] \\in \\mathbb{R}^n$.\nStarting from $\\beta_0 = \\mathbf{0}_d$ and letting $\\eta = 1/2$ be the step size, a single-step GD preconditioned with weights $\\mathbf{W}$ returns prediction\n$\\hat{y} = \\mathbf{x}^T\\mathbf{W}\\mathbf{X}^T\\mathbf{y} := \\text{g}_{PGD}(\\mathbf{Z}), \\quad (3)$\nand a single-step sample-weighted GD given weights $\\omega \\in \\mathbb{R}^n$ and $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ returns prediction\n$\\hat{y} = \\mathbf{x}^T\\mathbf{W}\\mathbf{X}^T (\\omega \\odot \\mathbf{y}) := \\text{g}_{WPGD}(\\mathbf{Z}), \\quad (4)$\nwhere $\\mathbf{Z}$ is defined in (1) consisting of $\\mathbf{X}, \\mathbf{y}$ and $\\mathbf{x}$. Our goal is to find the optimal $\\mathbf{W}$, as well as $\\omega$ in (4) that minimize the population risks defined as follows.\n$\\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W}) \\text{ where } \\mathcal{L}_{PGD}(\\mathbf{W}) = \\mathbb{E} [(\\hat{y} - \\text{g}_{PGD}(\\mathbf{Z}))^2], \\quad (5a)$\n$\\min_{\\mathbf{W},\\omega} \\mathcal{L}_{WPGD}(\\mathbf{W}) \\text{ where } \\mathcal{L}_{WPGD}(\\mathbf{W}) = \\mathbb{E} [(\\hat{y} - \\text{g}_{WPGD}(\\mathbf{Z}))^2]. \\quad (5b)$\nHere, the expectation is over the randomness in $(\\mathbf{x}_i, \\xi_i)_{i=1}^{n+1}$ and $\\beta$, and we use $\\mathbf{W}$ to represent the set of corresponding trainable parameters. The search spaces for $\\omega$ and $\\mathbf{W}$ are $\\mathbb{R}^n$ and $\\mathbb{R}^{d \\times d}$, respectively.\nAs per (2), given input prompt $\\mathbf{Z} \\in \\mathbb{R}^{(n+1) \\times (d+1)}$, either of the underlying models outputs a $(n+1)$-length sequence. Note that the label for the query $\\mathbf{x} = \\mathbf{x}_{n+1}$ is excluded from the prompt $\\mathbf{Z}$. Similar to Ahn et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure inputs cannot attend to their own labels and training can be parallelized. Let $\\mathbf{Z}_0 = [\\mathbf{Z}_1 \\ldots \\mathbf{Z}_n \\mathbf{0}]^T$ be the features post-causal masking at time/index $n + 1$. Given weights $\\mathbf{W}_k, \\mathbf{W}_q, \\mathbf{W}_v$, and the filter $f$ for SSM, predictions at the query token $\\mathbf{z} = \\begin{bmatrix} \\mathbf{x} \\\\ 0 \\end{bmatrix}$ take the following forms following sequence-to-sequence mappings in (2):\n$\\text{g}_{ATT}(\\mathbf{Z}) = (\\mathbf{z}^T \\mathbf{W}_q\\mathbf{W}_k^T \\mathbf{Z}_0^T \\mathbf{Z}_0 \\mathbf{W}_v) v, \\quad (6)$\n$\\text{g}_{SSM}(\\mathbf{Z}) = ((\\mathbf{z}^T\\mathbf{W}_q) \\odot ((\\mathbf{Z}_0 \\mathbf{W}_k \\odot \\mathbf{Z}_0 \\mathbf{W}_v) \\ast f)_{n+1})v, \\quad (6)$\nwhere $v \\in \\mathbb{R}^{d+1}$ is the linear prediction head and $((\\mathbf{Z}_0 \\mathbf{W}_k \\odot \\mathbf{Z}_0 \\mathbf{W}_v) \\ast f)_{n+1}$ returns the last row of the convolution output. Note that SSM can implement the mask by setting $f_0 = 0$. Now consider the meta"}, {"title": "3 Main Results", "content": "In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD in (5a). To this end, we consider multiple problem settings, including distinct data distributions and low-rank training. The latter refers to the scenario where the key and query matrices have rank restrictions, e.g., $W_k, W_q \\in \\mathbb{R}^{(d+1) \\times r}$, as well as LoRA-tuning when adapting the model under distribution shift.\n3.1 Analysis of Linear Data Models\nWe first consider the standard independent data setting. We will then examine correlated designs.\nIndependent data model. Let $\\Sigma_x$ and $\\Sigma_\\beta$ be the covariance matrices of the input feature and task vectors, respectively, and $\\sigma \\geq 0$ be the noise level. We assume\n$\\beta \\sim \\mathcal{N}(0, \\Sigma_\\beta), \\quad \\mathbf{x}_i \\sim \\mathcal{N}(0, \\Sigma_x), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2), \\quad 1 \\leq i \\leq n + 1 \\quad (7)$\nand the label is obtained via $y_i = \\mathbf{x}_i^T\\beta + \\xi_i$. Our following result characterizes the optimal solution of (5a). Note that the data generated from (7) satisfies the conditions in Proposition 1. Therefore, the same results can be applied to both linear-attention and H3 models.\nTheorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices $\\Sigma_x, \\Sigma_\\beta$ are full rank. Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}^* = \\mathcal{L}_{PGD}(\\mathbf{W})$. Additionally, let $\\Sigma = \\Sigma_x^{1/2} \\Sigma_\\beta \\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Then $\\mathbf{W}^*$ and $\\mathcal{L}_e$ satisfy\n$\\mathbf{W}^* = \\Sigma_x^{1/2} \\mathbf{W}_* \\Sigma_x^{-1/2} \\quad \\text{and} \\quad \\mathcal{L}_* = M - n \\text{tr} (\\tilde{\\mathbf{W}}),$ $\\quad (8)$\nwhere we define $\\mathbf{W}_* = ((\\frac{n}{(n + 1)}I_d + M\\Sigma^{-1})^{-1}$.\nCorollary 1 Consider noiseless i.i.d. linear data where $\\Sigma_x = \\Sigma_\\beta = I_d$ and $\\sigma = 0$. Then, the objective in (5a) returns\n$\\mathbf{W}^* = \\frac{1}{n+d+1}I_d \\quad \\text{and} \\quad \\mathcal{L}* = d - \\frac{nd}{n+d+1}$.\nSee Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work [Ahn et al., 2023, Theorem 1] when specialized to isotropic task covariance, i.e., $\\Sigma_\\beta = I_d$. However, their result is limited as the features and task are assumed to be independent. This prompts us to ask: What is the optimization landscape with correlated in-context samples? Toward this, we consider the following RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and Proposition 1 applies.\nRetrieval augmented generation. To provide a statistical model of the practical RAG approaches, given the query vector $\\mathbf{x}_{n+1} = \\mathbf{x}$, we propose to draw ICL demonstrations that are similar to $\\mathbf{x}$ with"}, {"title": "3.1 Analysis of Linear Data Models", "content": "the same shared task vector $\\beta$. Modeling feature similarity through the cosine angle, RAG should sample the ICL examples $\\mathbf{x}_i, i \\leq n$, from the original feature distribution conditioned on the event $\\cos(\\mathbf{x}_i, \\mathbf{x}) \\geq \\alpha$ where $\\alpha$ is the similarity threshold. As an approximate proxy, under the Gaussian distribution model, we assume that $\\beta \\sim \\mathcal{N}(0,I_d), \\mathbf{x} \\sim \\mathcal{N}(0, I_d)$ and that RAG samples $\\alpha$-correlated demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^n$ as follows:\n$\\mathbf{x}_i | \\mathbf{x} \\sim \\mathcal{N}(\\alpha \\mathbf{x}, (1 - \\alpha^2)I_d), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2) \\text{ and } y_i = \\mathbf{x}_i^T \\beta + \\xi_i, \\quad 1 \\leq i \\leq n. \\quad (9)$\nNote that the above normalization ensures that the marginal feature distribution remains $\\mathcal{N}(0, I_d)$. The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis by assuming $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ and $d/n = \\mathcal{O}(1)$ where $\\mathcal{O}(\\cdot)$ denotes proportionality, our derivation leads to the following result:\nTheorem 2 Consider linear model as defined in (9). Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}_* = \\mathcal{L}_{PGD}(\\mathbf{W}^*)$. Additionally, let $\\kappa = \\alpha^2d + 1$ and suppose $\\alpha = \\mathcal{O}(1/\\sqrt{d}), d/n = \\mathcal{O}(1)$ and $d$ is sufficiently large. Then $\\mathbf{W}^*$ and $\\mathcal{L}_*$ have approximate forms\n$\\mathbf{W}^* \\approx \\frac{1}{\\kappa n + d + \\sigma^2} I_d \\quad \\text{and} \\quad \\mathcal{L}_* \\approx d + \\sigma^2 - \\frac{\\kappa nd}{\\kappa n + d + \\sigma^2}. \\quad (10)$\nHere, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, $\\alpha^2d + 1$ is the dominant multiplier ahead of $n$ in both equations. Thus, we deduce that, RAG model follows the same error bound as the independent data model, however, its sample size is amplified by a factor of $\\alpha^2d + 1$. $\\alpha = 0$ reduces to the result of Corollary 1 whereas we need to set $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ for constant amplification. When $\\alpha = 1$, RAG achieves the approximate risk $\\mathcal{L}_* \\approx \\frac{d}{\\frac{n}{(a^2d+1)}} + \\sigma^2$, where the constant bias is due to the higher order moments (e.g., the 4'th and 6'th moments) of the standard Gaussian distribution. As $d$ increases, the normalized loss $\\mathcal{L}_*/d \\rightarrow 0$. The full analysis of its optimal solution $\\mathbf{W}^*$ and loss $\\mathcal{L}_*$ are deferred Theorem 4 in Appendix B.3.\nTask-feature alignment. We also consider another dependent data setting where task and feature vectors are assumed to be correlated. This dataset model has the following motivation: In general, an LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of the prompt), the LLM output becomes more deterministic and there are much fewer token candidates. For instance, if the task is \u201cCountry\u201d, \u201cFrance\u201d is a viable output compared to \u201cHelium\u201d and vice versa when the task is \u201cChemistry\u201d. Formally speaking, this can be formalized as the input $\\mathbf{x}$ having a diverse distribution whereas it becomes more predictable conditioned on $\\beta$. Therefore, it can be captured through a linear model by making the conditional covariance of $\\mathbf{x} \\vert \\beta$ to be approximately low-rank. This formalism can be viewed as a spectral alignment between input and task, which is also well-established in deep learning both empirically and theoretically [Li et al., 2020, Arora et al., 2019, Canatar et al., 2021, Cao et al., 2019]. Here, we consider such a setting where the shared task vector is sampled as standard Gaussian distribution $\\beta \\sim \\mathcal{N}(0, I_d)$ and letting $\\kappa = \\alpha^2d + 1$, we sample the $\\alpha$-correlated ICL demonstrations $(\\mathbf{x}_i, y_i)_{i=1}^{n+1}$ as follows:\n$\\mathbf{x}_i \\vert \\beta \\sim \\mathcal{N}(\\alpha \\beta, I_d), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma^2) \\text{ and } y_i = \\kappa^{-1/2} \\mathbf{x}_i^T\\beta + \\xi_i, \\quad 1 \\leq i \\leq n + 1. \\quad (11)$\nAbove, $\\kappa^{-1/2}$ is a normalization factor to ensure that label variance remains invariant to $\\alpha$. To keep the exposition cleaner, we defer the full analysis of its optimal solution $\\mathbf{W}^*$ and loss $\\mathcal{L}_*$ to Theorem 5 in Appendix B.4. Similar to the RAG setting, by assuming $\\alpha = \\mathcal{O}(1/\\sqrt{d})$ and $d/n = \\mathcal{O}(1)$, we obtain the following results for the optimal parameter and risk.\nTheorem 3 Consider linear model as defined in (11). Recap the objective from (5a) and let $\\mathbf{W}^* := \\arg \\min_{\\mathbf{W}} \\mathcal{L}_{PGD}(\\mathbf{W})$, and $\\mathcal{L}_* = \\mathcal{L}_{PGD}(\\mathbf{W}^*)$. Additionally, given $\\kappa = \\alpha^2d + 1$ and suppose $\\alpha = \\mathcal{O}(1/\\sqrt{d}), d/n = \\mathcal{O}(1)$ and $d$ is sufficiently large. Then $\\mathbf{W}^*$ and $\\mathcal{L}_*$ have approximate forms\n$\\mathbf{W}^* \\approx \\frac{1}{\\kappa n + (d + \\sigma^2)/\\kappa} I_d \\quad \\text{and} \\quad \\mathcal{L}_* \\approx d + \\sigma^2 - \\frac{\\kappa nd}{\\kappa n + (d + \\sigma^2)/\\kappa}. \\quad (12)$\nSimilar to (10), (12) contains $\\kappa = \\alpha^2 + 1$ multiplier ahead of $n$, which reduces the in-context sample complexity and setting $\\alpha = 0$ reduces to the results of Corollary 1."}, {"title": "3.2 Low-rank Parameterization and LORA", "content": "In this section, we investigate training low-rank models, which assume $\\mathbf{W}_k, \\mathbf{W}_q \\in \\mathbb{R}^{(d+1) \\times r}$ where $r$ is the rank restriction. Equivalently, we consider objective (5a) under condition $\\text{rank}(\\mathbf{W}) = r$.\nLemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a) and enforce $\\text{rank}(\\mathbf{W}) \\leq r$ and $\\mathbf{W} = \\mathbf{W}^T$. Let $\\tilde{\\Sigma} = \\Sigma_x^{1/2} \\Sigma_\\beta \\Sigma_x^{1/2}$ and $M = \\text{tr}(\\Sigma) + \\sigma^2$. Denoting $\\lambda_i$ to be the $i$'th largest eigenvalue of $\\tilde{\\Sigma}$, we have that\n$\\min_{\\substack{\\mathbf{rank}(\\mathbf{W}) \\leq r, \\mathbf{W} = \\mathbf{W}^T}} \\mathcal{L}(\\mathbf{W}) = M - \\sum_{i=1}^d \\frac{n \\lambda_i}{(n + 1) \\lambda_i + M}. \\quad (13)$\nNote that $\\text{tr} (\\Sigma) = \\sum_{i=1}^d \\lambda_i$. Removing the rank constraint and considering noiseless data setting, this reduces to the following optimal risk $\\mathcal{L}_e = \\sum_{i=1}^d \\frac{\\lambda_i}{n+1 + M/\\lambda_i}$. See Appendix C.1 for more details.\nImpact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of $\\Sigma, \\Sigma^{new}, \\{\\lambda_i\\}_{i=1}^d, \\{\\lambda_i^{new}\\}_{i=1}^d$. Consider adapting LoRA matrix to the combined key and value weights in attention, which reflects minimizing the population loss $\\mathcal{L}(\\mathbf{W}_{lora}) := \\mathcal{L}(\\mathbf{W} + \\mathbf{W}_{lora})$ in (5a) with fixed $\\mathbf{W}$. Suppose $\\text{tr} (\\Sigma) = \\text{tr} (\\Sigma^{new}) = M, \\sigma = 0$ and $\\mathbf{W}$ is jointly diagonalizable with $\\Sigma, \\Sigma^{new}$, then LoRA's risk is upper-bounded by\n$\\min_{\\text{rank}(\\mathbf{W}_{lora}) \\leq r} \\mathcal{L}(\\mathbf{W}_{lora}) \\leq \\min_{\\mathcal{I} \\subseteq [d], \\vert \\mathcal{I} \\vert \\leq r} \\Biggl[ \\sum_{i \\notin \\mathcal{I}} \\frac{\\lambda_i + M}{n + 1 + M/\\lambda_i} + \\sum_{i \\in \\mathcal{I}} \\frac{\\lambda_i^{new} + M}{n + 1 + M/\\lambda_i^{new}} \\Biggr] . \\quad (14)$\nNote that, the right hand side is provided assuming the optimal LoRA-updated model $\\mathbf{W}_{lora}$ is also jointly diagonalizable with covariances $\\Sigma, \\Sigma^{new}$, and $\\mathbf{W}$."}, {"title": "4 Experiments", "content": "We now conduct synthetic experiments to support our theoretical findings and further explore the behavior of different models of interest under different conditions. The experiments are designed to investigate various scenarios, including independent data, retrieval-augmented generation (RAG), task-feature alignment, low-rank parameterization, and LoRA adaption.\nExperimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL. As described in Section 2, we consider meta-learning setting where task parameter $\\beta$ is randomly generated for each training sequence. In all experiments, we set the dimension $d = 20$. Depending on the in-context length (n), different models are trained to make in-context predictions. We train"}, {"title": "5 Related Work", "content": "There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al., 2023b, Rae et al., 2021] in large language models (LLMs) due to its success in continuously enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Towards this, Xie et al. [2022] explain ICL by language model\u2019s ability to perform implicit Bayesian inference where, under specific assumptions on the pre-training data distribution, the model infers a shared latent concept among the in-context examples and leverages the concept to make a prediction. M\u00fcller et al. []"}]}]}