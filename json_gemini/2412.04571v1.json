{"title": "Dissociating Artificial Intelligence from Artificial Consciousness", "authors": ["Graham Findlay", "William Marshall", "Larissa Albantakis", "Isaac David", "William GP Mayner", "Christof Koch", "Giulio Tononi"], "abstract": "Developments in machine learning and computing power suggest that artificial general intelligence is within reach. This raises the question of artificial consciousness: if a computer were to be functionally equivalent to a human, being able to do all we do, would it experience sights, sounds, and thoughts, as we do when we are conscious? Answering this question in a principled manner can only be done on the basis of a theory of consciousness that is grounded in phenomenology and that states the necessary and sufficient conditions for any system, evolved or engineered, to support subjective experience. Here we employ Integrated Information Theory (IIT), which provides principled tools to determine whether a system is conscious, to what degree, and the content of its experience. We consider pairs of systems constituted of simple Boolean units, one of which a basic stored-program computer\u2014simulates the other with full functional equivalence. By applying the principles of IIT, we demonstrate that (i) two systems can be functionally equivalent without being phenomenally equivalent, and (ii) that this conclusion is not dependent on the simulated system's function. We further demonstrate that, according to IIT, it is possible for a digital computer to simulate our behavior, possibly even by simulating the neurons in our brain, without replicating our experience. This contrasts sharply with computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is progressing rapidly, fueled by theoretical and hardware developments in machine learning [1-3]. Though large language models such as ChatGPT and vision-language-action models (implementing robotically embodied AI) still lack several human capabilities, they perform spectacularly on a wide array of tasks, demonstrating abilities that were once the stuff of science fiction [4-6]. Consequently, a majority of AI researchers expect that artificial general intelligence is within reach, leading to systems that perform at least as well as humans in most and, eventually, in all cognitive domains [7]. Such prospects raise the following question: would machines with human-like intelligence also have human-like consciousness? More precisely, would they have our subjective experiences \"what it is like\" to perceive a scene, endure a pain, or entertain a thought in one's mind [8]? Because of the torrential growth of the size and power of AI systems, these questions are shifting from purely hypothetical to practically significant: how we interact with AI, and what rights we accord to them, will hinge on whether we consider them as conscious entities who share our subjective experiences, including pleasure and pain [9-12].\nWhen Alan Turing proposed what is now known as the Turing test in 1950, he emphasized that his \"imitation game\" was a test of intelligence, not of consciousness [13]. Nevertheless, many have since embraced computational functionalism: \"the thesis that performing computations of the right kind is necessary and sufficient for consciousness\" [14, 15]. So far, there is little agreement on what constitutes a computation \u201cof the right kind.\u201d Most functionalists seem to agree that if computers were to replicate our cognitive functions in every respect, they would also enjoy our subjective experiences. Others propose stricter definitions of functional equivalence: to be conscious like us, computers should simulate not only our cognitive functions but the detailed causal interactions that occur in our brain-say at the level of individual neurons [16].\nBut is functional equivalence really sufficient for phenomenal equivalence? To address this question, it is critical to rely on a general theory of consciousness, one that states the necessary and sufficient conditions for any system, evolved or engineered, to support subjective experience. In this paper, we address the issue of computer consciousness on the basis of Integrated Information Theory (IIT) [17-19], which offers a principled explanation and a mathematical framework to determine whether and how a system is conscious.\nCrucially, IIT does not start from neural correlates of consciousness, such as neural activity patterns [20-22], nor from cognitive functions often associated with consciousness, such as broadcasting or monitoring information, or adjusting an inferential model of the world and our actions [23-27]. Instead, IIT starts with the essential properties of consciousness itself those that are irrefutably true of every conceivable experience."}, {"title": "Theory", "content": "This section provides a high-level overview of IIT and highlights aspects of the theory that are relevant to the results presented here. For a more detailed and complete exposition of IIT and its justification, see [17, 19].\nCausal models. The starting point of IIT's analysis is a causal model of a physical system that captures all the interactions within it. Here, \"physical\" is meant operationally, indicating that the individual units can be manipulated and observed. The causal model is constituted of micro units, each having two internal states as well as inputs and outputs. The units are micro units in the sense that no finer detail about them or their function is included in the model. Given a causal model, any set of micro units can be considered as a candidate system. Any units not included in a candidate system are referred to as that system's background conditions. The causal model of a candidate system is fully characterized by its transition probability matrix (TPM), causally conditioned over its background conditions [33].\nHere we consider models with synchronously updating units implementing Boolean logic, which can be thought of as standing for transistors driven by a system-wide clock. While our idealized units ignore most physical details, they are sufficient to make our points about functional and phenomenal equivalence. Practically, the spatiotemporal grain of transistors and their interconnections is unambiguous, well-demarcated from other grain sizes, and is thought to capture the fundamental causal interactions within engineered silicon circuits [34, 35].\nIdentifying complexes. According to IIT, a substrate can support consciousness it is a complex if and only if it fulfills the five postulates [17-19]. To determine whether a system of units is a complex, one evaluates its system integrated information ($\\varphi_S$) - the extent to which the system's intrinsic information (capturing the postulates of intrinsicality and information) [36] is affected by its minimum partition into causally independent parts (capturing the integration postulate). To satisfy the exclusion postulate, a candidate system must specify a maximum of integrated information ($\\varphi$) compared to all competing candidate systems with overlapping units and grains. This can be determined by evaluating $\\varphi_S$ for every possible set of units within the causal model. Moreover, every system is evaluated at multiple grains, by exhaustively grouping subsets of its micro units into macro units, and mapping states of the constituent micro units to states of the resulting macro units. This procedure, called macroing, is done because a system's intrinsic causal powers ($\\varphi_S$) may be higher at a coarser than at a finer grain, depending on its internal organization [33, 37, 38].\nOnce the maximum of integrated information within a causal model has been identified, all units of that complex are excluded from participating in other complexes. The search for complexes is then repeated recursively over the remaining units of the system until all non-overlapping complexes are identified."}, {"title": "Unfolding the cause-effect structure of a complex", "content": "Finally, composition is evaluated by assessing how a complex's causal power is structured. Briefly, distinctions capture the causal powers of subsets of units, while relations between distinctions characterize how their powers overlap. Distinctions and relations are both associated with measures of their irreducibility ($\\varphi_d$ and $\\varphi_r$, respectively). Together, the totality of a system's distinctions and relations composes its cause-effect structure (also called a \u0424-structure). The cause-effect structure fully characterizes what a complex in a state specifies about itself through the congruent causes and effects of its various subsets. The process of identifying all of a complex's distinctions and relations, and thereby obtaining its cause-effect structure, is referred to as unfolding. According to IIT, the cause-effect structure of a complex in a state accounts for the quality of its experience in full, with no additional ingredients needed (\"quality is structure,\" [39]). The quantity of consciousness associated with a cause-effect structure is measured by its structure integrated information ($\\Phi$)-the total irreducibility of its distinctions and relations ($\\Sigma\\varphi_d + \\Sigma\\varphi_r$). If the state of a complex changes, its cause-effect structure may change as well, and therefore its quality of consciousness."}, {"title": "Results", "content": "Below, we apply IIT's causal powers analysis to a target system constituted of four micro units (PQRS) and to a functionally-equivalent four-bit computer, constituted of 117 micro units, able to simulate PQRS indefinitely. We first demonstrate that functional equivalence does not imply equivalence of cause-effect structures at the grain of micro units. We then demonstrate that there is no function-relevant macroing of the computer compliant with IIT's postulates that replicates the cause-effect structure of the target system. Finally, we extend the four-bit computer to be Turing-complete and demonstrate that the previous results do not depend on the complexity of the function being implemented.\nA target system (PQRS) and the cause-effect structure it specifies. Fig. 1A shows PQRS, the target system to be simulated, comprising a set of four binary units whose dynamics are defined by a truth table or, more generally, a transition probability matrix (Fig. 1B-C). For simplicity, we consider PQRS's causal model in isolation, without background conditions. The system PQRS in state 0101 (also written as pQrS) satisfies intrinsicality, information, integration, and exclusion, and is thus a complex, with $\\varphi_s = 1.51$ intrinsic bits (Fig. 1D), also called ibits [36]. Its cause-effect structure, unfolded according to [17], is composed of 13 distinctions and 8184 relations and has $\\Phi = 391.25$ ibits (Fig. 1E-F). The system will transition through a repeating cycle of nine states, with varying complexes, cause-effect structures, and values (Supplementary Fig. 9). This minimal system, though it contains > 8,000 relations, was chosen to reduce the computational requirements of analyzing the cause-effect structures specified by the computer simulating it, without loss of generality, as shown below.\nA computer constituted of 117 units that can simulate PQRS indefinitely. Fig. 2A shows a basic four-bit computer capable of simulating PQRS. This simple digital computer with a Harvard-like, stored-program architecture has recognizable components: a clock, frequency dividers, program memory, an instruction register, data registers, and a multiplexer that combines instructions with data to serve as a minimal processor. The data registers store the state of the system until instructed to update; the program memory stores the function of the units the computer is simulating; the multiplexer's processing units combine this information about state and function to compute the next state of the target system; the clock synchronizes these processes and tells the registers when to update their state. The clock and program are read-only components of the computer; they output to the data registers and multiplexer but receive no feedback connections. For a detailed explanation of the computer's operation, and for a comparison to common computer architectures, see the \"Step-by-step guide to the computer's operation\" supplement.\nWhen initialized appropriately, this computer simulates the behavior of PQRS. The initialization procedure is simple: First, the states of the program units are set in such a way that they encode PQRS's state transition rules (Fig. 2B). Next, the states of the units P', Q', R', and S' are set to reflect the initial state of PQRS (Fig. 2C). Finally, the state of the clock is set as shown in Fig. 2A. The processing units may be initialized to any state. After eight updates, the states of P', Q', R', and S' reflect a single state transition of P, Q, R, and S (Fig. 2D), and the two systems will continue to produce homomorphic sequences of states indefinitely. This is true regardless of the initial states of PQRS and P'Q'R'S', hence the two systems-the 4-unit simulandum and the 117-unit simulans-are functionally equivalent (modulo eight updates).\nThe computer fragments into multiple complexes, none of which specifies a cause-effect structure identical to that of PQRS. Does the computer, when programmed to be functionally equivalent to pQrS, also replicate its cause-effect structure? By applying IIT's causal powers analysis to the computer as a whole, we find that, unlike pQrS, the computer has $\\varphi_s = 0$ ibits (Fig. 3A, grey). This is because the computer contains modules that are connected to the rest of the system in a purely feedforward manner. This violates the requirement for integration, which can be assessed by partitioning the set of units that constitute a system into separate parts. In an integrated system ($\\varphi_s > 0$ ibits), every part will both make a difference to- and take a difference from the rest of the system [40].\nMost subsets of units also have $\\varphi_s = 0$ ibits. In fact, the complete IIT analysis reveals that the computer contains 24 individual complexes, each one constituted of between one and four units, each with $\\Phi < 6$ ibits (Fig. 3A, blue; \"Micro-grain analysis of the weakly connected computer\" supplement). Thus, the entire computer is never a single, large complex, no matter the state, but merely an aggregate of smaller complexes, each of which has its own tiny cause-effect structure. Importantly, each of the 24 complexes specify cause-effect structures that differ from the one specified by pQrS"}, {"title": "The computer's lack of integration is not merely due to lack of feedback", "content": "The computer's lack of integration is not merely due to lack of feedback. It is possible to modify the computer by introducing feedback connections that alter its TPM, but do not impair its ability to simulate pQrS (Fig. 3C). This version of the computer is strongly connected in the graph-theoretical sense, yet as a whole remains reducible, with $\\varphi_s = 0$ ibits. In fact, the same subsystems that were maximally irreducible without feedback connections (Fig. 3A-B) remain maximal even with such feedback in place (see \"Micro-grain analysis of the strongly connected computer\" supplement). This is because, despite the presence of feedback connections, the computer has many fault lines, such as sparse connectivity and bottlenecks, along which it segregates into smaller, non-overlapping and maximally integrated pieces [40]. In other words, the computer with or without feedback is an aggregate of about two dozen tiny complexes, rather than a single, larger complex in its own right."}, {"title": "The computer's failure to replicate the cause-effect structure of PQRS cannot be rescued by treating its units or states at macro grains", "content": "The computer's failure to replicate the cause-effect structure of PQRS cannot be rescued by treating its units or states at macro grains. In IIT, the units that constitute a complex, called intrinsic units, [33] are those that maximize the complex's existence, as measured by $\\varphi_s$. In principle, a complex's intrinsic units are established by evaluating the system at all possible grains, exhaustively grouping subsets of its micro units into macro units, and mapping states of the constituent micro units to states of the resulting macro units (Fig. 4A) [33, 37, 38, 41]. This procedure, called macroing, is done because a system's intrinsic causal powers may be higher at a coarser than at a finer grain, depending on its internal organization (Fig. 4B). In such cases, a system's intrinsic units are macro units, rather than micro units.\nA complex and its intrinsic units must comply with IIT's postulates of physical existence [17]. Thus, like complexes, macro units can only qualify as units if their cause-effect power is irreducible to that of independent subsets (satisfying integration). Otherwise, one could literally build something (a macro unit) out of nothing (non-interacting micro units;"}, {"title": "Fig. 4C)", "content": "Fig. 4C). And, just as complexes must be definite, which translates into maximally irreducible cause-effect power (satisfying exclusion), so must macro units. Otherwise, one could build something (a macro unit) out of nearly nothing (weakly interacting sets of micro units; Fig. 4D). Several other constraints on what constitutes a valid macro unit follow from the postulates. For example, intrinsic units should not overlap (satisfying exclusion) [33].\nFrom the extrinsic perspective of a computer engineer, certain macroing schemes may provide a natural way to construct a high-level characterization of the computer. For example, it is useful to abstract away low-level digital logic into high-level components with precise input-output behavior, such as multiplexers, adders, bit shifters, and registers. It is also helpful to chunk together multiple updates of the state of such components into interpretable computational steps.\nFig. 5 shows one of many ways in which one might consider macroing the computer from an extrinsic, computational-functionalist perspective. Each line of the program, together with its corresponding unit in the instruction register, is grouped into a macro unit (a through \u03c0). The state of each"}, {"title": "Dissociation between function and cause-effect structure", "content": "macro unit might be defined as the state of its constituent instruction register unit, abstracting away all the sub-macro interactions within. One might also group the multiplexer units together (w) and define the macro state as the state of the multiplexer's output. Finally, one might consider each data register as a macro unit (\u03c1, \u03c3, \u03c2, \u03c5) whose state is defined by the state of its output (P', Q', R', S'). This way of macroing results in a clear mapping between macro units \u03c1\u03c3\u03c2\u03c5 and PQRS that captures the functional equivalence of the two systems.\nHowever, the macroing shown in Fig. 5 is inconsistent with IIT's postulates of physical existence. For example, the requirement that macro units be integrated (Fig. 4C-D) means that w is not a valid unit, as it is fully reducible (for further examples, see \"Applying IIT's postulates to candidate macro units within the computer\" supplement). In fact, as shown in the \"Macro grain analysis of the strongly connected computer\" supplement, there is no macroing of the computer reflecting its function as a simulans that can replicate the cause-effect structure of PQRS. On the basis of IIT, this means that functional equivalence is not accompanied by phenomenal equivalence."}, {"title": "While one example is enough to make the case, we conjecture that this result will generalize to many standard computer architectures", "content": "While one example is enough to make the case, we conjecture that this result will generalize to many standard computer architectures. The underlying reason is that IIT is concerned with the complete set of causal powers intrinsic to a system, no more (i.e., no powers extrinsic to a mechanism or system may be borrowed, and no intrinsic powers may be multiplied) and no less (i.e., one cannot selectively pick the units or interactions within the system that are of interest to an external observer, and ignore the rest). For the very general computer architecture explored below, macroing of micro units into functionally relevant macro units necessarily involves either omission of, or causal overlap over, a set of shared constituents essential to each unit's function. These two facts together make it extremely unlikely that any similar computer could precisely replicate a target system's cause-effect structure (see \"How the computer compares to modern computer architectures\" and \u201cWhy the computer is unlikely to support a complex with high structure integrated information (\u03a6)\u201d). Regardless, the point is that, in general, functional equivalence does not denote cause-effect structure equivalence, and therefore does not imply phenomenal equivalence on the basis of IIT.\nCause-effect structures specified by the computer can be dissociated from those of target systems it is simulating. The above example shows that while a simple computer can be functionally equivalent to a system with a radically different substrate\u2014the complex PQRS\u2014it cannot specify an equivalent cause-effect structure. Owing to its architecture, the computer fragments into many small complexes, each of which specifies a trivial cause-effect structure, rather than the single, much larger cause-effect structure specified by PQRS. We now show that the computer fragments into the same small complexes that specify trivial cause-effect structures regardless of the target system it is simulating.\nConsider another four-unit system, WXYZ, an elementary cellular automaton that happens to implement Wolfram's Rule 110 (Supplementary Fig. 11) [42]. Cellular automata of any size can be made to implement Rule 110, which completely specifies that system's causal model and TPM. Furthermore, systems of arbitrary, finite size that implement Rule 110 are Turing complete [42, 43]. If placed into state 1101, WXyZ fragments into two complexes, whose cause-effect structures are markedly different from that specified by pQrS (Fig. 6, upper quadrants). The general-purpose four-bit computer described here can simulate WXyZ just as"}, {"title": "Figure 4C).Inductive extension to large computers simulating arbitrarily complex systems", "content": "Figure 4C).Inductive extension to large computers simulating arbitrarily complex systems. (A) The computer can be incrementally extended to simulate systems of arbitrary size (left), including biological brains or the Rule 110 cellular automaton (middle), which is Turing-complete (right). By transitivity, the programmable computer is Turing-complete, too, if extended to arbitrary sizes. (B) By induction on the size of the simulated system, it can be shown that the cause-effect structures specified by subsystems within the computer will not change qualitatively at the micro grain and can only grow linearly with system size (see \"Analysis of the ring motif\" supplement), whereas the cause-effect structures of the simulated target systems may grow double-exponentially in size and richness, resulting in an increasing phenomenal dissociation between the computer and the systems it simulates."}, {"title": "Discussion", "content": "IIT starts from the essential properties of experience and formulates them in causal, operational terms. This leads to the conclusion that to support consciousness, a system must have cause-effect power that is intrinsic, specific, irreducible, definite (maximally irreducible), and structured. The particular cause-effect structure of distinctions and relations specified by the system accounts for the quality of a particular experience \"what it is like to be\" that system in its current state [8]. The amount of structure integrated information ($\\Phi$) of the cause-effect structure corresponds to the quantity of consciousness."}, {"title": "Here we applied the mathematical framework of IIT to the causal model of a simple target system", "content": "Here we applied the mathematical framework of IIT to the causal model of a simple target system, constituted of four units implementing Boolean functions, as well as to an explicit model of a simple, stored-program, four-bit computer, constituted of 117 units and programmed to be functionally equivalent to the target. Although the computer replicated the input-output functions of the target, neither the computer as a whole, nor subsets of its units, could replicate the cause-effect structure of the target. Furthermore, the latter could not be replicated by macroing the units of the computer according to the postulates of IIT. Finally, these results held for a Turing-complete version of the computer which, in principle, can simulate arbitrarily large systems.\nThe target system employed here was extremely small and specified a single cause-effect structure that was comparatively simple (for the state shown in Fig. 1, composed of 13 distinctions and 8184 relations, with $\\Phi = 391.25$ ibits). The functionally equivalent computer, considered at the micro grain, fragmented into 24 small complexes, each of which specified trivial cause-effect structures (with a maximum of 4 distinctions and 4 relations). The substrate of consciousness in the brain can be assumed to be a complex constituted of a much larger number of units that specifies a cause-effect structure of extraordinary richness and high $\\Phi$ [46, 47]. Based on the principles of IIT and the results obtained here, we conjecture that computers with architectures sufficiently similar to the one shown here will fragment into many small complexes that specify trivial cause-effect structures (see \"How the computer compares to modern computer architectures\" and \u201cWhy the computer is unlikely to support a complex with high structure integrated information (\u03a6)\u201d). If they were to support some non-trivial cause-effect structures, it would not be in virtue of their computational power, functional sophistication, or which simulations they perform. We argue that if standard computers implementing general AI were to replicate our behaviors and cognitive functions even if they did so by simulating, neuron by neuron, the functioning of our brain-they would generally have negligible consciousness and would not replicate our experiences.\nThe issue of computer consciousness has been considered before, but not on the basis of a comprehensive, quantitative theory of consciousness. Some have argued that computers are unlikely to be conscious, motivated by various intuitions. For example, consciousness might depend on some critical biological attribute, such as homeostasis, emotion, or embodiment [48-52]. However, it is not clear why such attributes would be critical for consciousness, nor why they would be beyond the reach of computers [53, 54]. Other have suggested that computers may lack some critical \"physical-chemical\" ingredient, though it is not clear which or why [55, 56].\nOn the other side of the fence, a common assumption is that consciousness should be identified with some function or computation, whether language, attention, self-monitoring,", "recurrent processing,": "r", "processing": 23}, {"title": "mental states", "content": "mental states, if seen as intermediate computational states between critical inputs and outputs, can in principle be implemented by both brains and computers. However, there is strong evidence that consciousness can be dissociated from a variety of cognitive functions [21]. We can be conscious even when we are not doing, thinking, or saying anything, as in \"pure presence\" meditation [62, 63]; in dreaming sleep, when we are not interacting with the environment [64]; under certain psychedelics [65], or under ketamine anesthesia [66].\nMoreover, some have argued that computation is an implausible foundation for consciousness precisely because it is substrate-independent and observer-dependent [67-69]. Turing completeness can be achieved using radically different substrates [70-73], whether simple [43, 74] or complex, feedforward [75] or recurrent [76, 77]. Also, any program could in principle be mapped by an observer onto almost any thermodynamically open substrate through an appropriate encoding of inputs and outputs [67, 68] (but see [78, 79]). Why would one assume that a simulation of what something does should be equivalent to what something is? After all, we do not expect that running a simulation of a rainstorm will make a computer wet [55, 80] or that a simulation of a black hole will bend space-time around the computer. Above all, why should certain functions or computations (but not others) be associated with experience, and why would they feel the way they do (and not some other way)?\nOn the basis of IIT and in sharp contrast to computational functionalism, we argue that what matters for consciousness, its presence, quality, and quantity, is a system's intrinsic causal structure\u2014what the system is rather than its extrinsic functions\u2014what the system does [17, 19, 81]. Because internal organization cannot generally be inferred from overall system dynamics or input-output behavior [17, 82, 83], functions are not reliable proxies for the properties of consciousness. In short, consciousness is about being, not doing.\nOf course, the conclusions derived in this paper can only be considered valid to the extent that IIT itself can be considered valid. Future experiments may show that, contrary to the predictions of IIT, our own consciousness is not associated with a maximum of intrinsic cause-effect power, or that the quality of specific experiences (say, the experience of visual space) is not consistent with the properties of the associated cause-effect structures, [17, 18, 47, 84]. In that case IIT would be invalidated, and with it the present results concerning machine consciousness. So far, however, IIT provides a rational basis for making inferences about the presence of consciousness in biological or artificial systems other than ourselves. IIT is grounded in the essential properties of our own consciousness the only case in which we have immediate, irrefutable proof that subjective experience exists. Its predictions can be assessed on our own brain, so far with consistent results [18]. It can explain disparate facts in a coherent manner-for example, why certain brain substrates are important for consciousness while others are not [18, 21], why consciousness vanishes during sleep despite ongoing neural activity [85], whether a stimulus will be perceived or remain unconscious during psychophysical manipulations [29], why the stream of consciousness may split under some concurrent tasks [28], why specific contents of consciousness such as the visual field, temporal flow or mental objects may feel the way"}]}