{"title": "Explainable Diagnosis Prediction through Neuro-Symbolic Integration", "authors": ["Qiuhao Lu, Ph.D.", "Rui Li, Ph.D.", "Elham Sagheb, M.S.", "Andrew Wen, M.S.", "Jinlian Wang, Ph.D.", "Liwei Wang, M.D., Ph.D.", "Jungwei W. Fan, Ph.D.", "Hongfang Liu, Ph.D."], "abstract": "Diagnosis prediction is a critical task in healthcare, where timely and accurate identification of medical conditions can significantly impact patient outcomes. Traditional machine learning and deep learning models have achieved notable success in this domain but often lack interpretability which is a crucial requirement in clinical settings. In this study, we explore the use of neuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop explainable models for diagnosis prediction. Essentially, we design and implement LNN-based models that integrate domain-specific knowledge through logical rules with learnable thresholds. Our models, particularly Mmulti-pathway and Mcomprehensive, demonstrate superior performance over traditional models such as Logistic Regression, SVM, and Random Forest, achieving higher accuracy (up to 80.52%) and AUROC scores (up to 0.8457) in the case study of diabetes prediction. The learned weights and thresholds within the LNN models provide direct insights into feature contributions, enhancing interpretability without compromising predictive power. These findings highlight the potential of neuro-symbolic approaches in bridging the gap between accuracy and explainability in healthcare AI applications. By offering transparent and adaptable diagnostic models, our work contributes to the advancement of precision medicine and supports the development of equitable healthcare solutions. Future research will focus on extending these methods to larger and more diverse datasets to further validate their applicability across different medical conditions and populations.", "sections": [{"title": "Introduction", "content": "Diagnosis prediction is a critical yet challenging task in the healthcare domain, where accurate and timely identification of medical conditions can significantly impact patient outcomes. Over the past few decades, various computational methods have been explored to tackle this problem, evolving from traditional machine learning techniques to more advanced deep learning models and, more recently, large language models (LLMs). Despite their success in specific applications, these methods often lack explainability, which is especially critical in the healthcare domain.\nEarly efforts in computational diagnosis prediction primarily used symbolic methods, relying on predefined rules and logic to draw conclusions from data1,2,3. These methods are highly interpretable, as their reasoning process is transparent. However, their performance is often limited by the rigidity of human-defined rules, which struggle to capture the complexity and variability of real-world medical data.\nTraditional machine learning models, such as decision trees, support vector machines (SVMs), and logistic regression, have improved the ability to analyze medical data by learning patterns from training data4,5,6,7. These models offer better generalization than symbolic methods and are capable of handling more complex datasets. However, they still require substantial feature engineering, where domain experts must manually select and transform relevant features from raw data. This process is time-consuming and can introduce biases. Moreover, while some models like decision trees are interpretable, others, such as SVMs or ensemble methods (e.g., random forests), can become opaque as the model complexity increases.\nDeep learning models, particularly neural networks, often surpass traditional ML models in terms of accuracy. Convo- lutional neural networks (CNNs) have been particularly successful in image-based diagnosis, while recurrent neural networks (RNNs) and transformers have shown promise in sequential data, such as electronic health records (EHRs)8,9,10. However, deep learning models are often criticized for their lack of interpretability or explainability. The \u201cblack-box\" nature has raised concerns about the transparency of decision-making processes, which is especially critical in clinical settings where understanding the rationale behind a diagnosis is crucial. Additionally, deep learning models require"}, {"title": null, "content": "large volumes of labeled data for training, which can be challenging to obtain in the healthcare domain.\nMore recently, large language models (LLMs) have been applied to diagnosis prediction, leveraging their ability to understand and generate human-like text11,12,13,14. LLMs can process large amounts of textual data, such as clinical notes, and identify complex patterns that might be missed by other models. While LLMs represent a significant advancement in handling unstructured data and can provide contextual insights that are difficult for traditional models to capture, they also inherit many of the challenges associated with deep learning models. LLMs are even more complex and opaque, making it challenging to interpret their predictions. Their massive scale requires substantial computational resources, which can limit their practical application in many healthcare settings. Furthermore, LLMs are not well-suited for working with numerical features or tabular data, which are often crucial in medical contexts, such as lab results or vital signs.\nNeuro-symbolic AI (NeSy) is an emerging field that integrates the interpretability and structured reasoning of symbolic methods with the powerful learning capabilities of neural networks. Symbolic reasoning enables the incorporation of domain knowledge and logical rules, providing a level of transparency and explanation that pure neural networks often lack. Meanwhile, neural networks contribute to the model's ability to learn from data and handle complex patterns, making neuro-symbolic AI a compelling candidate for tasks like diagnosis prediction. This hybrid approach is particularly well-suited for healthcare applications where explainability is critical.\nIn this work, we specifically focus on Logical Neural Networks (LNNs)15. LNNs are a neuro-symbolic AI framework designed to learn interpretable models by expressing rules in first-order logic (FOL), a powerful language with clear semantics and a rich set of operators. Unlike traditional neuro-symbolic AI methods that rely on non-learnable t- norms 16,17, LNNs incorporate learnable parameters that allow the network to adjust its logical rules based on data, thereby enhancing its ability to model complex medical information. This makes LNNs not only accurate but also transparent and adaptable, ensuring that the model's decisions are explainable and suitable for clinical settings.\nThis study represents one of the first explorations into the use of neuro-symbolic methods for diagnosis prediction, a natural fit given the need for both accuracy and explainability. By integrating domain-specific logical rules with neural networks, the approach offers a fresh perspective on the potential of explainable AI in healthcare. The LNN-based models not only outperform traditional machine learning models across multiple metrics but also provide transparent decision-making processes that align with clinical reasoning. In particular, the LNN-based models $M_{multi-pathway}$ and $M_{comprehensive}$ achieve the highest accuracy of 80.52% and AUC scores of 0.8457 and 0.8399, respectively, surpassing the best traditional model, Random Forest, which has an accuracy of 76.95% and an AUC of 0.8342. Moreover, $M_{comprehensive}$ shows the highest precision of 87.88%, while $M_{multi-pathway}$ achieves the highest F1-score of 68.75% among all models tested. This superior performance highlights the effectiveness of integrating logical rules with neural networks, enabling the models to capture complex relationships between risk factors while maintaining interpretability.\nIn summary, this research contributes to the ongoing discourse on AI in healthcare by demonstrating how neuro-symbolic methods can bridge the gap between performance and explainability. The adaptability of LNNs, with their learned weights and thresholds, creates opportunities for personalized diagnostics and supports the development of precision medicine. While further investigation is necessary to generalize these findings across diverse medical conditions and populations, the insights gained from this study will help build more equitable and transparent AI diagnosis systems in the future."}, {"title": "Related Work", "content": "Recent years have witnessed a growing interest in developing machine learning methods for diagnosis prediction. For instance, Huang et al. develop a decision-tree-based model that integrates clinical and genetic features with a gender-specific rule to identify the risk of diabetic nephropathy among type 2 diabetes patients\u00b2. Ma et al. develop Dipole, a neural method that utilizes bidirectional recurrent neural networks (BiRNNs) to capture and remember information from both past and future patient visits within Electronic Health Records (EHRs). It introduces three attention mechanisms to model the relationships between different visits, enhancing both prediction accuracy and interpretability of the results. Experimental evaluations on real-world EHR datasets demonstrate that their method outperforms existing diagnosis prediction approaches. Gao et al. introduce Dr.Knows, an innovative approach designed"}, {"title": "Methods", "content": "Task Overview The task of diagnosis prediction involves determining whether a patient has diabetes based on a set of numerical features. Formally, given a dataset of patients, each instance is represented by a feature vector $X = [X_1,X_2,...,x_n]$, where n is the number of features, and each feature $x_i$ corresponds to a specific medical measurement (e.g., number of pregnancies, glucose level, BMI). The objective is to predict a binary label $Y \\in {0, 1}$, where Y = 1 indicates the presence of diabetes and Y = 0 indicates its absence. This task is treated as a binary classification problem, where the model is trained to learn a mapping from the input features X to the target label Y.\nDataset In our experiments, we use the Pima Indian Diabetes dataset18, a widely used benchmark dataset drawn from a population with a high incidence of diabetes mellitus. The Pima Indians, a Native American group from Mexico and Arizona, have been the focus of extensive research due to their elevated diabetes prevalence, considered representative of broader global health trends19,20. This dataset is particularly important for global health research and addressing health disparities among underrepresented minority groups. It contains records from 768 Pima Indian women aged 21 and older, with 258 testing positive for diabetes and 500 testing negative. We randomly split the dataset into 80% for training and 20% for testing.\nThe detailed statistics of the dataset are shown in Table 1, which provides a summary of the key features used in the dataset. The features include the number of times pregnant (Preg), plasma glucose concentration at 2 hours in an oral glucose tolerance test (Gluc), diastolic blood pressure (BP), triceps skin fold thickness (Skin), 2-hour serum insulin levels (Insulin), body mass index (BMI), diabetes pedigree function (DPF), and age (Age). Additionally, the dataset includes a binary outcome feature (Outcome) indicating whether a person is diabetic (1) or non-diabetic (0). Each feature's data type, along with the minimum, median, and maximum values, are presented to give an overview of the distribution and range of the dataset.\nLogical Neural Networks Neural networks are powerful tools for learning from data, but they are typically limited in how they handle logic. While classical logic involves operators like conjunction (\u0245), disjunction (V), and negation (\u00ac),"}, {"title": null, "content": "these operators are not naturally differentiable. This means that classical Boolean logic is difficult to use with neural networks, which rely on gradient-based methods like backpropagation for learning.\nTo overcome this challenge, neuro-symbolic AI introduces differentiable versions of logical operators, allowing neural networks to learn from logical rules. For instance, conjunction (A) and disjunction (V) can be replaced with differentiable functions known as t-norms and t-conorms, respectively16. One common t-norm is the product t-norm17, which is defined as:\n$x\u2227y = Product(x, y) = x. y$\nwhere x, y \u2208 [0, 1] represent real-valued inputs. This approach works well because it matches classical Boolean logic at the extremes (i.e., when x, y are either 0 or 1), but it also allows for smooth changes when x, y take values between 0 and 1.\nLogical Neural Networks (LNNs) take this idea further by introducing learnable parameters into the logical operators, such as conjunction (\\), to better adapt to the data15. The LNN conjunction operator (LNN-^) is defined as:\nmax(0, min(1, \u03b2 \u2013 w\u2081(1 \u2212 x) \u2013 w\u2082(1 \u2013 y)))\nwhere \u03b2, w1, W2 are learnable parameters, and x, y \u2208 [0, 1] are inputs. The output of LNN-A is clamped between 0 and 1 to maintain consistency with logical semantics, ensuring that it behaves similarly to classical logic when x and y are close to 0 or 1. LNN negation operates as a pass-through function, defined as LNN-\u00ac(x) = 1 x. The LNN disjunction operator is derived from the conjunction operator and is defined as:\nLNN- V (x, y) = 1 \u2013 LNN- ^ (1 \u2212 x, 1 \u2013 y)\nAdditionally, LNNs impose certain constraints to maintain the crispness of first-order logic (FOL) semantics. These constraints ensure that the learned conjunction behaves properly across all inputs. For example, the constraints in LNN-A are given by:\n\u03b2 \u2013 (1 \u2013 \u03b1)(\u03c9\u2081 + w2) \u2265 \u03b1\n\u03b2 \u03b1\u03c91 < 1 - \u03b1\n\u03b2 - \u03b1\u03c92 \u2264 1-\u03b1\nwhere a \u2208 [1, 1] is a hyperparameter that controls the strength of the constraint, and w1, W2 \u2265 0. These conditions ensure that the LNN conjunction behaves similarly to classical logic, while still allowing it to learn from data. We direct readers to the foundational work on LNNs for a more comprehensive understanding of their theory15 and various applications21, 22, 23.\nLNN-based Diagnosis Prediction In this section, we describe the application of LNNs to the task of diagnosis prediction. The task is framed as binary classification that involves predicting whether a patient has a particular medical condition (i.e., diabetes) based on numeric features such as blood pressure, age, gender, etc.\nDiagnosis prediction often relies on decision-making rules involving thresholds on patient features. These rules can be expressed in first-order logic (FOL) as a combination of predicates in the form $f_k > \u03b8_k$, where $f_k$ is a patient feature and $\u03b8_k$ is a threshold. For example, a simple rule for diagnosing hypertension could be $f_{blood pressure} > \u03b8_1 \\land f_{age} > \u03b8_2$. However, such rules are static and require manual tuning.\nTo enable data-driven learning and flexibility, these logical rules are reformulated into the LNN framework, where conjunctions (A) and disjunctions (V) are replaced by their differentiable counterparts: LNN-^ and LNN-V. These operators allow the model to learn from real-valued inputs in [0, 1], which are derived from patient features. For each feature, the LNN learns a threshold using a smooth transition logic function $T_L(f, \u03b8) = f \u00b7 \u03c3(f \u2013 \u03b8)$, where o is the sigmoid function and @ is a learnable threshold ensuring the comparison remains within the [0, 1] range21.\nFor example, the LNN-based hypertension rule becomes:\n$p(diagnosis=hypertension | patient) = LNN- \u2227 (T_L(f_{blood pressure}, \u03b8_1), T_L(f_{age}, \u03b8_2))$"}, {"title": null, "content": "This reformulation ensures the rule is continuous and differentiable, making it suitable for learning via gradient-based optimization. The model can combine multiple rules using LNN-V to capture more complex diagnostic scenarios.\nThe task of diagnosis prediction is framed as a binary classification problem where the model predicts whether a patient has the diagnosis (positive class) or not (negative class). To train the LNN, we minimize the binary cross-entropy loss, defined as:\n$L(y, \u0177) = \\frac{1}{N} \\sum_{i=1}^{N} [y_i log(\u0177_i) + (1 \u2212 y_i) log(1 \u2013 \u0177_i)]$\nwhere $y_i$ is the true label for patient i, and \u0177r is the predicted probability of the diagnosis.\nIn general, patient features are passed through the trained LNN model, which applies the learned diagnostic rules to output a probability score for the diagnosis. The LNN's ability to combine learned thresholds and logical rules ensures an interpretable yet flexible approach to diagnosis prediction.\nRule Models for Diabetes Diagnosis Prediction For the task of diabetes prediction, we have developed a set of rule-based models that capture key patient features associated with diabetes risk. These rules are expressed using logical operators, and for simplicity, we omit the use of the logical neural network symbol LNN- and the smooth transition logic function TL(f, 0):\n$M_{glucose-bmi} = f_{gluc}(x) > \u03b8_1 \\land f_{BMI}(x) > \u03b8_2$\nThis rule links high glucose levels with obesity, both of which are strong indicators of diabetes.\n$M_{family-insulin} f_{DPF}(x) > \u03b8_1 \\land f_{insulin}(x) > \u03b8_2 \u2227 f_{age}(x) > \u03b8_3$\nCombining family history, insulin levels, and age provides a more comprehensive assessment of diabetes risk.\n$M_{balanced} = [f_{DPF}(x) > \u03b8_1 \\lor f_{age}(x) > \u03b8_2] \\lor [f_{preg}(x) > \u03b8_3 \\land f_{gluc}(x) > \u03b8_4 \\land f_{BP}(x) > \u03b8_5] \\lor [f_{skin}(x) > \u03b8_6 \\land f_{insulin}(x) > \u03b8_7 \\land f_{BMI}(x) > \u03b8_8]$\nThis model balances various combinations of risk factors, ensuring that multiple pathways contribute to the risk assessment.\n$M_{multi-pathway} = [f_{DPF}(x) > \u03b8_1 \\land f_{age}(x) > \u03b8_2] \\lor [f_{gluc}(x) > \u03b8_3 \\land f_{insulin}(x) > \u03b8_4 \\land f_{BMI}(x) > \u03b8_5 \\land f_{skin}(x) > \u03b8_6 \\land f_{BP}(x) > \u03b8_7 \\land f_{preg}(x) > \u03b8_8]$\nThis rule offers a comprehensive assessment, considering both family history and age, or a combination of insulin, glucose, BMI, and other physiological factors.\n$M_{comprehensive} = [f_{gluc}(x) > \u03b8_1 \\land f_{insulin}(x) > \u03b8_2 \\land f_{BMI}(x) > \u03b8_3 \\land f_{preg}(x) > \u03b8_4] \\lor [f_{DPF}(x) > \u03b8_5 \\land f_{insulin}(x) > \u03b8_6]$\nThis model operates through two pathways: one assessing metabolic factors and another linking family history with insulin levels. This structure allows the model to capture risk from both immediate physiological conditions and hereditary background.\nBy structuring these models with logical combinations of clinically significant features, we aim to enhance both the interpretability and predictive power of diabetes diagnosis models. Each model captures different aspects of diabetes risk, from immediate physiological conditions to long-term hereditary factors, providing a diverse and comprehensive approach to risk assessment."}, {"title": "Evaluation", "content": "We evaluate a set of baseline models, including Logistic Regression, SVM, Random Forest, K-Nearest Neighbors (KNN), and Naive Bayes, using Accuracy, Precision (P), Recall (R), F1-score (F1), and Area Under the Receiver Operating Characteristic Curve (AUC) as metrics. These traditional models are specifically chosen for their interpretability, as each provides a degree of transparency in its decision-making process, which is critical for understanding diagnosis predictions. Their interpretability makes them well-suited for comparison with the five LNN- based models, which integrate diverse features and rules. This evaluation setup allows for a comprehensive comparison between interpretable traditional methods and neuro-symbolic approaches, promoting a deeper understanding of the decision-making mechanisms underlying diagnosis predictions."}, {"title": "Results", "content": "Table 2 shows the performance comparison of both traditional models and LNN-based models for diabetes diagnosis prediction.\nTraditional Models The traditional models-including Logistic Regression, SVM, Random Forest, KNN, and Naive Bayes-exhibit varying degrees of performance. The Random Forest model achieves the highest accuracy (0.7695), F1-score (0.6380), and AUC (0.8342) among the baseline models, indicating a strong balance between precision and recall. This performance can be attributed to Random Forest's ability to handle nonlinear relationships and interactions between features. SVM and Logistic Regression also demonstrate competitive performance, with AUCs of 0.8315 and 0.8262, respectively. Naive Bayes, while achieving the highest recall (0.6011) among the baselines, has a lower precision (0.6645), leading to an F1-score of 0.6281. KNN shows the lowest performance across all metrics among the baseline models, suggesting its limited ability in this task.\nLNN-based Models The LNN-based models generally outperform the baseline models across multiple metrics. These models leverage logical combinations of key features related to diabetes risk factors, capturing more complex relationships in the data.\n\u2022 $M_{glucose-bmi}$: While it achieves moderate accuracy (0.7338) and AUC (0.8035) among the LNN models, it has a low recall (0.3636) and F1-score (0.4938). This is expected since the model only considers two features, potentially overlooking other important risk factors.\n\u2022 $M_{family-insulin}$: The model performs poorly across all metrics. The very low recall (0.0364) indicates that the model fails to identify most positive cases. This may be due to the strictness of the conjunction of three conditions, which significantly narrows the criteria for predicting diabetes, thereby reducing sensitivity.\n\u2022 $M_{balanced}$: This model shows improved performance, with an accuracy of 0.7922 and an F1-score of 0.6522. The use of disjunctions allows the model to capture patients who meet any one of several sets of criteria,"}, {"title": null, "content": "enhancing its ability to identify positive cases, as reflected in its higher recall (0.5455) compared to $M_{glucose-bmi}$ and $M_{family-insulin$ \u2022\n\u2022 $M_{multi-pathway}$: This model achieves the highest F1-score (0.6875) and AUROC (0.8457) among all models, indicating a strong and balanced performance. By integrating both hereditary factors and a comprehensive set of physiological metrics, $M_{multi-pathway}$ effectively captures a wider range of diabetes risk profiles.\n\u2022 $M_{comprehensive}$: The model achieves the highest precision (0.8788) among all models and shares the highest accuracy (0.8052) with $M_{multi-pathway}$. The high precision indicates that the model is highly effective at correctly identifying patients without false positives. However, its recall (0.5273) is slightly lower than that of $M_{multi-pathway}$, suggesting it may miss some positive cases due to the specificity of its rules.\n\u2022 Impact of Model Complexity: The models that consider multiple pathways ($M_{balanced}$, $M_{multi-pathway}$, and $M_{comprehensive}$) outperform simpler models ($M_{glucose-bmi}$ and $M_{family-insulin}$). This suggests that capturing the heterogeneous nature of diabetes risk factors leads to better predictive performance.\n\u2022 Role of Logical Operators: The use of disjunctions (V) in $M_{balanced}$, $M_{multi-pathway}$, and $M_{comprehensive}$ allows these models to capture patients who meet any of several sets of criteria. This flexibility increases the models' recall by identifying more true positive cases.\n\u2022 Underperformance of $M_{family-insulin}$: The poor performance of this model highlights the limitations of relying on a narrow set of features connected strictly by conjunctions (A). The strict criteria reduce the model's ability to identify positive cases, leading to low recall.\n\u2022 Comparative Advantage over Baselines: The LNN-based models outperform the baseline models not only in the numeric metrics but also in maintaining interpretability and explainability. While Random Forest and SVM provide strong performance among baselines, they lack the transparency of LNN-based models, which is critical in healthcare applications for ethical and practical reasons."}, {"title": null, "content": "\u2022 Interpretability Figure 1 shows the rule models $M_{multi-pathway}$ and $M_{comprehensive}$ for diabetes prediction. Both models share the same architecture-a logical disjunction (V) of two conjunctions (^)\u2014indicating that diabetes can be predicted through multiple pathways. The key difference lies in the selection of features within each conjunction.\nIn $M_{multi-pathway}$, the left conjunction includes glucose (G), insulin (I), BMI (B), skin thickness (S), blood pressure (T), and pregnancies (P), all of which are clinical measures associated with metabolic health. The right conjunction incorporates Diabetes Pedigree Function (D) and age (A), highlighting genetic predisposition and demographic factors. In contrast, $M_{comprehensive}$ focuses on a subset of these features, emphasizing the combined effect of glucose, insulin, BMI, and pregnancies in the left conjunction, and insulin and DPF in the right.\nAnalyzing the weights and thresholds, we observe that features like glucose and BMI have significant weights in both models, aligning with medical knowledge that elevated glucose levels and high BMI are strong indicators of diabetes risk. Notably, the learned thresholds for these features are often below the median values in the dataset (e.g., glucose thresholds below the median of 117 as mentioned in Table 1), suggesting that even modest elevations in these features can contribute to a positive prediction, compensating for the strictness of the conjunction. Conversely, in $M_{comprehensive}$, the left conjunction includes fewer features, and the thresholds are higher, suggesting more significant deviations from normal values for the condition to be satisfied, which balances the less stringent conjunction (fewer features need to meet their thresholds). This insight illustrates how the models balance feature selection and threshold settings to capture subtle but clinically relevant patterns, enhancing their interpretability and potential utility in early detection of diabetes.\nIn summary, the integration of domain-specific logical rules in the LNN-based models enhances their ability to predict diabetes accurately while maintaining interpretability. The models that consider multiple pathways and balance various risk factors, such as $M_{multi-pathway}$ and $M_{comprehensive}$, demonstrate the highest effectiveness. These findings underscore the potential of neuro-symbolic approaches in developing predictive models that are both accurate and interpretable, offering valuable tools for clinical decision-making."}, {"title": "Discussion and Conclusion", "content": "In this study, we explore the integration of neuro-symbolic AI methods, specifically Logical Neural Networks (LNNs), for explainable diagnosis prediction, with a focus on diabetes. The results demonstrate that neuro-symbolic approaches offer a promising balance between accuracy and explainability, addressing the critical need for transparent models in healthcare. By incorporating logical rules based on domain-specific knowledge and learning thresholds from data, the LNN-based models are able to outperform traditional machine learning models, such as logistic regression, SVM, and random forest. Specifically, models like $M_{multi-pathway}$ and $M_{comprehensive}$ balance multiple pathways of risk factors, capturing complex interdependencies between physiological and hereditary factors associated with diabetes. These models demonstrate high performance while maintaining interpretability, making them natural fits for real-world applications in healthcare.\nA significant advantage of our approach lies in the interpretability of the learned weights and thresholds within the LNN-based rule models. These parameters provide direct insight into how different features contribute to the diagnosis prediction. For example, the learned thresholds for features like glucose levels and BMI are consistent with clinical knowledge, highlighting their importance in diabetes risk assessment. This transparency is crucial for clinical settings, as it allows healthcare professionals to understand, trust, and even adjust the model's decision-making process.\nMoreover, the adaptability of neuro-symbolic approaches has important implications for healthcare equity. Different patient cohorts may exhibit varying risk factors and thresholds due to genetic, environmental, or socioeconomic differences. By learning these thresholds directly from data, LNNs can tailor diagnostic models to specific populations, enhancing the accuracy and fairness of predictions across diverse groups. This capability supports the development and implementation of precision medicine, where treatments and diagnostics are customized to individual patient characteristics, potentially improving outcomes for underrepresented and underserved populations.\nDespite these promising findings, there are limitations to our study. The Pima Indian Diabetes dataset, while widely used, represents a single disease and a relatively small cohort. The generalizability of our neuro-symbolic approach"}, {"title": null, "content": "to other diseases or more diverse populations requires further exploration. Moreover, while LNNs have shown their effectiveness in modeling diabetes risk factors, the complexity of certain rules, such as those relying on conjunctions, may limit the sensitivity of the model under specific conditions, as seen with $M_{family-insulin}$. Additionally, the design of the rules in LNNs relies heavily on domain expertise, which introduces potential biases and limits scalability when applied to other medical conditions where expert knowledge may be less established or more varied.\nIn future work, we plan to extend the application of neuro-symbolic methods to more comprehensive and larger healthcare datasets, covering a wider range of medical conditions. This would allow us to further assess the scalability and flexibility of neuro-symbolic approaches in diagnosis prediction across diverse clinical contexts. Additionally, we aim to investigate the potential of these methods in enhancing diagnostic accuracy for underrepresented groups, which is crucial for the advancement of precision medicine.\nIn conclusion, this study highlights the potential of neuro-symbolic methods, particularly Logical Neural Networks, in advancing explainable and accurate diagnosis prediction models. By combining the strengths of symbolic reasoning and neural networks, LNNs offer a viable solution to the challenges of interpretability and performance in healthcare AI applications. The learned weights and thresholds not only improve model transparency but also provide opportunities to address healthcare disparities through personalized diagnostics. As we continue to explore and refine these methods, neuro-symbolic AI holds promise for contributing to more equitable, reliable, and effective healthcare solutions in the future."}]}