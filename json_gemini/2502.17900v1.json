{"title": "Knowledge-enhanced Multimodal ECG Representation Learning with\nArbitrary-Lead Inputs", "authors": ["Che Liu", "Cheng Ouyang", "Zhongwei Wan", "Haozhe Wang", "Wenjia Bai", "Rossella Arcucci"], "abstract": "Recent advances in multimodal ECG representation learning center on aligning ECG signals with paired free-text reports. However, suboptimal alignment persists due to the complexity of medical language and the reliance on a full 12-lead setup, which is often unavailable in under-resourced settings. To tackle these issues, we propose K-MERL, a knowledge-enhanced multimodal ECG representation learning framework. K-MERL leverages large language models to extract structured knowledge from free-text reports and employs a lead-aware ECG encoder with dynamic lead masking to accommodate arbitrary lead inputs. Evaluations on six external ECG datasets show that K-MERL achieves state-of-the-art performance in zero-shot classification and linear probing tasks, while delivering an average 16% AUC improvement over existing methods in partial-lead zero-shot classification.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in deep learning have enabled automated classification of cardiovascular disease (CVD) using electrocardiograms (ECGs), one of the most crucial diagnostic tools. However, most methods are supervised, requiring large amounts of annotated data, which is costly and demands prohibitively extensive expert effort in annotation (Liu et al., 2023a; Huang and Yen, 2022). \u03a4\u03bf address this challenge, self-supervised multimodal learning has recently emerged as an effective approach for learning representative ECG features from accompanied free-text clinical reports (Li et al., 2023; Pham et al., 2024; Liu et al., 2024). \u03a4\u03bf this end, MERL (Liu et al., 2024) recently introduced the first comprehensive benchmark using the largest dataset MIMIC-ECG (Gow et al.) for pretraining, and six datasets (Wagner et al., 2020; Liu et al., 2018; Zheng et al., 2022, 2020) for evaluating downstream task performance, including zero-shot classification and linear probing.\nDespite outperforming signal-only self-supervised approaches, multi-modal approaches, including MERL (Liu et al., 2024), still have notable drawbacks: They directly align ECG signals with reports, introducing unnecessary noise due to the free-text nature of the reports, and failing to fully exploit the rich cardiac knowledge contained within the text. Additionally, they encode ECG in a lead-agnostic manner, overlooking the unique spatial and temporal characteristics of the individual 12 ECG leads. Moreover, they require all 12 leads to be available as input, limiting their ability to generalize across different lead combinations. This raises important practical concerns since full 12-lead ECG data is not always available in clinical environments due to factors such as patient mobility issues, the need for rapid assessments in emergencies, and limited resource in pre-hospital care environments (Bray et al., 2021; Swor et al., 2006; Quinn et al., 2020; Nonogi et al., 2008; Kotelnik et al., 2021; Zhang and Frick, 2019; Nonogi et al., 2008).\nTo overcome the challenges listed above, we make the following contributions: (1) We propose a framework dubbed Knowledge-enhanced ECG Multimodal Representation Learning (K-MERL), which extracts cardiac-related entities from free-text ECG reports, converting unstructured reports into structured knowledge to enhance self-supervised ECG multimodal learning. To the best of our knowledge, this is the first work to leverage structured cardiac entities extracted from clinical reports to improve ECG multimodal learning. (2) To effectively capture and leverage the lead-specific spatial and temporal characteristics of 12-lead ECGs, we explore various tokenization and positional embedding techniques. In particular, we design lead-specific tokenization and lead-specific spatial positional embeddings, enabling the framework to capture the distinctiveness of each lead. (3) To enable our framework to handle arbitrary combinations of input leads, we introduce a dynamic lead masking strategy. In addition, we propose an independent segment masking strategy to further capture lead-specific temporal patterns. (4) Our K-MERL framework demonstrates superior performance in zero-shot classification and linear probing on multiple downstream datasets in various lead combinations, from a single lead to all 12 leads."}, {"title": "2 Method", "content": "To this end, we first utilize a general-purpose open-source large language model (LLM), such as Llama3.1 (AI@Meta, 2024), without domain-specific fine-tuning, to extract cardiac-related entities from free-text ECG reports. This makes our approach adaptable and well-positioned to benefit from future advancements in LLMs. Additionally, we design a lead-aware ECG encoder with lead and segment masking strategies, allowing the model to handle arbitrary lead inputs while capturing lead-specific spatial-temporal patterns.\nOur overall framework is illustrated in Fig 1(b), shown together with the previous state-of-the-art MERL that is based on naive cross-modal contrastive learning (Liu et al., 2024), in Fig 1(a). While both approaches utilize contrastive learning with an ECG signal encoder FE processing signal inputs and a text encoder Fr processing reports, our method introduces substantial innovations, including lead-specific processing, dynamic masking strategies, and the extraction of cardiac-related entities from free-text reports, significantly enhancing ECG multimodal learning.\nIn the following sections, we introduce the model framework and lead-specific processing in Sec 2.2, followed by the proposed masking strategies in Sec 2.3. We then describe the pipeline for extracting cardiac-related entities as structured knowledge from ECG reports in Sec 2.4. Finally, in Sec 2.5, we explain the knowledge-enhanced ECG multimodal learning process, a synergy of the aforementioned components."}, {"title": "2.2 Lead-specific Processing", "content": "To begin with, we define the symbols used in our framework: Given a training dataset X consisting of N ECG-report pairs, we represent each pair as (e,t), where $e \\in E$ denotes the raw 12-lead ECG signals for lead $l \\in \\{1, 2, 3, ..., 12\\}$ of the i-th subject $(i = 1, 2, 3, . . ., N)$, and $t_{l} \\in T$ represents the associated free-text report. We then perform lead-specific processing, as illustrated in Fig 2.\nConsider an input ECG signal $e_{l}^{i}$ with 12 leads and a signal length denoted by S. We split the time-series signal into M non-overlapping segments, each segment of length $\\frac{S}{M}$, and perform tokenization for them. In this way, each lead ECG is projected into a sequence of tokens:\n$e_{l}^{i} [p_{1}], e_{l}^{i} [p_{2}], e_{l}^{i} [p_{3}],...,e_{l}^{i} [p_{M}]$\nwhere $e_{l}^{i} [p_{m}]$ corresponds to the ECG token for the m-th segment for lead l. For 12 leads, the total number of tokens is 12 \u00d7 M. Unlike MERL (Liu et al., 2024), which generates a single token for a 12-lead ECG temporal segment, we produce tokens separately for each individual lead to capture the lead-specific nature.\nWe apply a learnable linear projection $W \\in \\mathbb{R}^{p \\times d}$ to each token $e_{l}^{i} [p_{m}]$. Then, we introduce learnable lead embeddings $[lead_{1},..., lead_{12}]$, where $lead_{l} \\in \\mathbb{R}^{d}$, to capture the characteristics of each lead. The resulting input sequence can be written as:\n$lead_{1} + W e_{l}^{i}[p_{1}], . . ., lead_{1} + W e_{l}^{i}[p_{M}],..., \\newline lead_{12} + W e_{l}^{i}[p_{1}], . . ., lead_{12} + W e_{l}^{i}[p_{M}].$\nIn line with lead-specific spatial positional embedding, we also incorporate learnable lead-agnostic temporal embeddings to retain the temporal information of ECG signals. These embeddings are denoted as $[temp_{1},..., temp_{M}]$, where $temp_{m} \\in \\mathbb{R}^{d}$. It is worth noting that these positional embeddings are shared across leads, enabling the model to recognize temporal properties across leads, as all leads originate from the same source and share the same temporal domain properties. The resulting input sequence can be written as:\n$temp_{1} + lead_{1} + W e_{l}^{i}[p_{1}], . . ., \\newline temp_{M} + lead_{1} + W e_{l}^{i}[p_{M}], \\newline . . ., \\newline temp_{1} + lead_{12} + W e_{l}^{i}[p_{1}], \\newline temp_{M} + lead_{12} + W e_{l}^{i}[p_{M}].$"}, {"title": "2.3 Lead and Segment Masking", "content": "Using a fixed number of masked leads limits the model's flexibility in handling arbitrary lead inputs. To address this, we propose Dynamic Lead Masking (DLM), enabling the model to handle varying lead combinations (Fig. 2 a). For an ECG signal $e_{l}^{i}$ with 12 leads, we first randomly sample a number from $\\{9, 10, 11\\}$, which determines how many leads will be masked. Then, we randomly select a set of unmasked lead indices, denoted as l, and mask the remaining leads. This approach ensures the model is exposed to diverse combinations of unmasked and masked leads during pretraining. The resulting ECG signal with the selected unmasked leads is denoted as $e_{l}^{i}$.\nTo better capture the temporal patterns of each ECG lead, we introduce Lead-independent Segment Masking (LSM) (Fig. 2 a). Applying masking across all tokens from an ECG signal could lead to imbalances, where some leads have more masked tokens than others. To avoid this, LSM applies masking separately to each lead, ensuring an equal number of masked tokens per lead. For each unmasked lead signal $e_{l}^{i}$, we randomly select masked token indices $H^{l}$ based on a masking proportion of 0.25. The model then processes only the unmasked tokens, denoted as $\\{e_{l}^{i}[p_{h}]\\}_{h \\notin H^{l}}$.\nIn the experiments we ablate DLM or LSM to verify their effectiveness, as shown in Tab 2d and Fig 7."}, {"title": "2.4 Mining Cardiac-related Entities from\nReport", "content": "In this section, we introduce the structured knowledge extraction process for handling free-text ECG reports. The pipeline is illustrated in Fig 3. Since each ECG report provides descriptions of cardiac-related entities, as shown in the leftmost part of Fig. 3, our goal is to extract all positive cardiac-related entities mentioned in the report as structured knowledge to enhance the supervision signals for ECG multimodal learning.\nUnlike existing biomedical multimodal learning approaches from the radiology domain, which rely on knowledge graphs to extract structured knowledge from reports (Zhang et al., 2023; Wu et al., 2023), we directly query an LLM with the following prompt: 'Please extract all positive\nEntities from the given\nECG report. Output format is [Entity1,\nEntity2, ...]'. There are two main reasons\nfor this approach. First, there is no off-the-shelf knowledge graph (KG) specifically focused on ECG, making it impractical to use KG-based methods for extracting structured knowledge. Second, since we are only extracting existing terms from the free-text report, we can easily verify that the extracted cardiac-related entities are present and positive, ensuring no non-existent terms are generated by the LLM. Moreover, (Zhang et al., 2023) has already demonstrated that a general-purpose LLM can effectively extract existing medical terms from free-text reports independently of any external knowledge database. To ensure accuracy, after each extraction operation, we query the LLM with: \u2018Please verify the extracted cardiac-related entities as existing and positive in the given report. Output format is YES or NO', and only retain the cardiac-related entities with a 'YES' response. After this stage, we obtain a total of 341 unique cardiac-related entities in the whole dataset..\nAfter extracting all cardiac-related entities from whole dataset, we observe that many names share the same semantics but are expressed differently, as shown in the second part of Fig 3. This variation arises because different clinical protocols generate ECG reports in different styles, even though they describe the same cardiac-related entities. To address this, we query the LLM with: 'Please merge the cardiac-related\nentities that have the same semantics\nbut different expressions. Here are <all\nEntities>. Output format\nJSON, where the key is the original\nand the value is the merged name.'\nthis stage, we obtain a total of 252 unique\nentities in the whole dataset..\nSince cardiac-related entities are organized in a clear hierarchical structure (Arnaout et al., 2016; Okshina et al., 2019), for example, as shown in the rightmost part of Fig 3, 'anterior myocardial infarction' and 'inferior infarction' are subtypes of superclass 'Myocardial Infarction' (Brieger et al., 2000), we query the LLM the following prompt: \u2018Please detect all the superclasses present in <all\nRelated Output format\nJSON, where the key is the superclass and the values are the related entities that belong to\nsuperclass.\u2019\nthis stage, we identify 25 superclasses of"}, {"title": "2.5 Knowledge-enhanced ECG Multimodal\nLearning", "content": "In this framework, as shown in Fig 1 (b), two distinct encoders for ECG signals and text reports, symbolized as FE and Fr, transform the sample pair $(e_{i}, t_{i})$ into the latent embedding space, represented as $(z_{e,i}, z_{t,i})$.\nfeature level is then denoted as\n$X = \\{(z_{e,1}, z_{t,1}), (z_{e,2}, z_{t,2}), ..., (z_{e,N}, z_{t,N})\\}$,\n$z_{e,i} = F_{e}(e_{i})$ and $z_{t,i} = F_{r}(t_{i})$. Af-\ntwo non-linear projectors for ECG and\nembeddings, denoted as $P_{e}$ and $P_{t}$, trans-\n$z_{e,i}$ and $z_{t,i}$ into the same dimensionality\n$d$, with $\\hat{z}_{e,i} = P_{e}(AvgPool(z_{e,i}))$ and $\\hat{z}_{t,i} =\nP_{t}(AvgPool(z_{t,i}))$. Next, we compute the cosine\n$s_{e2t}^{i,j} = \\frac{\\hat{z}_{e,i} \\cdot \\hat{z}_{t,j}}{\\|\\hat{z}_{e,i}\\| \\cdot \\|\\hat{z}_{t,j}\\|}$, representing the\nreport similarities, and formulate the ECG-\ncontrastive loss $L_{contrast}$.\n$L_{e2t}^{i} =\n- log\n\\frac{exp(s_{e2t}^{i,j}/\\eta)}{\\sum_{k=1}^{L} 1_{[k \\neq i]} exp(s_{e2t}^{i,k}/\\eta)},$\n$L_{contrast} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{L} L_{e2t}^{i,j}.$\n(1)\ntemperature hyper-parameter, denoted as $\\eta$,\n0.07 in our study. L refers to the batch size\nstep, which is a subset of N.\nknowledge from extracted cardiac-related\na cardiac query network, de-\nThis network consists of four trans-\nconcatenated with a linear classifier\neach ECG\u2019s corresponding cardiac\nGiven the set of cardiac-related en-\ncompute a corresponding set of cardiac\nusing the text encoder, denoted as\n$\\hat{Q} = \\{\\hat{q}_{1}, \\hat{q}_{2}, ..., \\hat{q}_{Q}\\}$, where each query vector\n$\\,\\hat{q}_{i} = F_{T}(q_{i})$. These query vectors\nfor the cardiac query net-\nduring pre-training, the ECG features"}, {"title": "3 Experiments", "content": "We pre-train K-MERL using the\ncomprising\nECG signal recorded at 500Hz over a\nduration, along with its correspond-\nFor fair comparison with the MERL\nwe adhere to their\nprotocol, available in the official\npreprocessing, we ob-\nfor model pre-training.\nwe inherit the\nwe use a\nas the text encoder. The key dif-\napproach are the proposed lead-\ntemporal positional\ncardiac-related enti-\nUtilize Llama3.1-\nwith ablations of\nPre-training con-\nBoth zero-shot clas-\nusing full and partial\nmultiple public datasets cover-\nWe adhere to the\nprovided by MERL\nimplemented on\nThe PTBXL\nincludes 21,837 ECG\n18,885 patients, sampled at 500 Hz\nIt provides four subsets for multi-\n5 categories), Sub-\n19 categories), and\nwith varying sample sizes.\nCPSC2018"}, {"title": "3.3 State-of-the-art on Zero-shot\nClassification", "content": "We first evaluate K-MERL on zero-shot classification using 12-lead input across all downstream datasets. The results for each dataset, along with the average AUC score across six datasets, are shown in Fig 4. Our framework significantly outperforms MERL with both backbone architectures, demonstrating the superiority of K-MERL when using the original disease names as text prompts.\nAdditionally, since we extract cardiac-related entities from reports during pre-training, there may be overlap with categories in downstream tasks. This could provide our model with prior knowledge of certain categories, leading to an unfair comparison with MERL (Liu et al., 2024). To address this, we use Med-CPT (Jin et al., 2023), the text encoder, to extract embeddings for all 277 cardiac-related entities and for all category names in the downstream datasets. We compute the similarity between these embeddings, and if the similarity exceeds 0.95, we consider them overlapped. We identify 35 out of 277 extracted cardiac-related entities that overlap with downstream categories, as listed in Tab 5. We label these as 'Seen Classes,' while the remaining downstream categories are labeled as 'Unseen Classes.'\nThe average F1 score are depicted in Fig 5(b). K-MERL outperforms MERL in both seen and unseen categories. Notably, both K-MERL and MERL exhibit performance drops on unseen classes compared to seen classes, demonstrating that we successfully detected an overlap of approximately 12.7% between the extracted cardiac-related entities from MIMIC-ECG and downstream categories, effectively separating the tasks into 'seen' and 'unseen' groups. The results show that K-MERL performs well not only on categories present during pre-training but also on unseen categories, demonstrating its generalizability. Since the original MERL (Liu et al., 2024) framework relies on manual prompt engineering (PE) at inference time to enhance performance, we also evaluate MERL with customized prompts, as detailed in Sec. D, to provide a comprehensive comparison. Notably, our method outperforms MERL with PE while being entirely independent of prompt engineering."}, {"title": "3.4 Performance of Linear Probing", "content": "As shown in Tab 1, K-MERL consistently outperforms multimodal methods, including MERL (Liu et al., 2024) with both ResNet and ViT backbones, as well as all eSSL methods across datasets and data ratios. This highlights K-MERL's robust performance and the quality of its learned ECG features, which not only improve multimodal tasks but also significantly enhance single-modality tasks."}, {"title": "3.5 Performance with Partial Leads Input", "content": "As shown in Fig 6 (a) and (b), K-MERL consistently outperforms MERL across all lead combinations from 1 to 12 in both zero-shot classification and linear probing. Impressively, K-MERL with just a single lead surpasses MERL's performance using all 12 leads. Additionally, K-MERL shows a stable performance trend as the number of leads increases, unlike MERL, which exhibits fluctuations in Fig 6 (a). This demonstrates the effectiveness of our dynamic lead masking strategy, lead-specific processing, and spatial-temporal positional embeddings, contributing to K-MERL's superior results."}, {"title": "4 Analysis", "content": "This section provides extensive ablation studies on the key components of K-MERL and reports zero-shot classification results for single-lead and 12-lead inputs across all downstream datasets. Due to the page limit, we show more ablation studies in Sec F\nTab 2a shows the effect of removing and $L_{CQ}$ during pre-training. Removing $L_{CQ}$, which excludes structured knowledge from cardiac-related entities, leads to a significant performance drop. While removing also reduces performance, the impact is less severe. This indicates that both losses are necessary, with cardiac-related entities alignment providing a larger benefit for pre-training."}, {"title": "Tokenization Size", "content": "In Fig 7 (a), we ablate the token size p and find the optimal length to be 100. Larger token sizes (e.g., 200) have a more negative impact than smaller sizes (e.g., 25), likely due to convert multiple segments to one token, which introduces ambiguity. Across all token sizes, K-MERL consistently outperforms MERL (Liu et al., 2024), demonstrating the robustness and effectiveness of our method."}, {"title": "Lead-specific Processing", "content": "In Tab 2b, we ablate the effects of lead-specific tokenization, lead-specific spatial positional embedding, and lead-agnostic temporal embedding. he results show each component enhances K-MERL's performance, with the full combination yielding the best results. The results demonstrate that lead-specific processing is crucial for enabling the ECG multimodal model to recognize lead uniqueness."}, {"title": "Masking Strategy and Ratio", "content": "Tab 2d shows the results of various masking strategies, where all approaches enhance K-MERL's performance. Removing dynamic lead masking and using a fixed number of masked leads degrades performance, highlighting its importance. Similarly, omitting lead masking during pre-training causes a sharp drop in zero-shot classification, indicating its role in capturing lead-specific features. Fig 7 (b) explores mask ratios and lead masking. An optimal configuration is identified with a mask ratio of 25% and a minimum of 9 masked leads. Increasing the mask ratio beyond this or using more than 9 leads as the minimum for masking leads to a decrease in performance."}, {"title": "Cardiac-related Entities Processing", "content": "As shown in Tab 2c, both subtype aggregation and merging duplicate entity names improve K-MERL's performance. However, the best results are achieved when both procedures are applied together, indicating they complement each other."}, {"title": "Text Encoder", "content": "Tab. 2e shows Med-CPT (Jin et al., 2023) outperforms BioClinicalBERT (Alsentzer et al., 2019) and Med-KEBERT (Zhang et al., 2023), due to contrastive pretraining on a large medical corpus, suggesting contrastive pretraining improves text encoder performance for this task."}, {"title": "5 Conclusion", "content": "We present K-MERL, a knowledge-enhanced ECG multimodal learning framework capable of processing arbitrary lead inputs. First, we mine cardiac-related entities as structured knowledge from ECG free-text reports using a general LLM, without relying on external domain-specific resources. Next, we align ECG features with these cardiac-related entities to integrate this knowledge into the ECG multimodal learning. Additionally, we introduce lead-specific processing and lead&segment masking strategies to capture the spatial-temporal patterns unique to each ECG lead, enabling the model to handle varying lead inputs. Our experiments on six downstream ECG classification tasks, along with extensive ablation studies, demonstrate K-MERL's superior zero-shot and linear probing performance compared to existing ECG multimodal and self-supervised learning methods."}, {"title": "Limitation", "content": "While K-MERL demonstrates promising results in handling arbitrary lead inputs and integrating knowledge from ECG reports, there are some limitations to consider. The framework's reliance on LLMs for mining cardiac-related entities, though effective, may be limited by the model's ability to capture highly specialized domain knowledge. Additionally, while our experiments show strong zero-shot and linear probing performance, further evaluation is needed to assess K-MERL's effectiveness in real-world clinical settings, where data quality and noise levels can be more challenging. Future work will focus on enhancing the robustness of knowledge extraction and developing more adaptive strategies for handling diverse ECG data sources."}, {"title": "E Scalability", "content": "We scale our ECG encoder using ViT-Tiny, ViT-Small, ViT-Middle, and ViT-Base, as shown in Fig. 9. K-MERL consistently improves as model size increases, demonstrating its scalability for ECG multimodal learning."}, {"title": "F Additional Ablation Studies", "content": "Tab 6a, 6b, and 6c present the results of additional ablation studies. (1) Tab 6a shows the impact of various LLMs on processing cardiac-related entities, with Llama3.1-70B-Instruct achieving the best performance across both 1-lead and 12-lead settings. The performance increases with larger LLMs, suggesting that larger models improve cardiac-related entities extraction. (2) Tab 6b explores the effects of different numbers of transformer layers in the Cardiac Query Network FCQ, showing that performance improves as the number of layers increases and saturates at 4 layers. (3) Tab 6c examines the effect of the number of attention heads in FCQ, with 4 heads providing the best performance."}]}