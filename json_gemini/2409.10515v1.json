{"title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems", "authors": ["Hitesh Tulsiani", "David M. Chan", "Shalini Ghosh", "Garima Lalwani", "Prabhat Pandey", "Ankish Bansal", "Sri Garimella", "Ariya Rastrow", "Bj\u00f6rn Hoffmeister"], "abstract": "Dialog systems, such as voice assistants, are expected to engage with users in complex, evolving conversations. Unfortunately, traditional automatic speech recognition (ASR) systems deployed in such applications are usually trained to recognize each turn independently and lack the ability to adapt to the conversational context or incorporate user feedback. In this work, we introduce a general framework for ASR in dialog systems that can go beyond learning from single-turn utterances and learn over time how to adapt to both explicit supervision and implicit user feedback present in multi-turn conversations. We accomplish that by leveraging advances in student-teacher learning and context-aware dialog processing, and designing contrastive self-supervision approaches with Ohm, a new online hard-negative mining approach. We show that leveraging our new framework compared to traditional training leads to relative WER reductions of close to 10% in real-world dialog systems, and up to 26% on public synthetic data.", "sections": [{"title": "1. Introduction", "content": "Automatic speech recognition (ASR) for dialog systems has traditionally been a focused field, where the primary goal is to produce a text transcript for an utterance given the acoustic signal corresponding to that utterance. While such systems have been largely successful, particularly in the domain of dialog systems and voice assistants (leading to word error rates below 2% on the Librispeech benchmark), in real-world applications such single-utterance systems have been shown to struggle with a long-tailed distribution of rare words, proper nouns, etc., leading to decreased user satisfaction with such systems.\nSuch a struggle with long-tailed distributions has led to several promising directions of research aimed at specializing large-scale general models to handle rare words. These approaches generally center around fine-tuning where models are tuned on rare words as they are discovered, \"ASR model personalization\" where model parameters are locally adapted with user-specific context, or \u201cContextual biasing\" where model inputs include additional user-specific context as part of the input to the model. While these approaches have shown promising results, they often require additional compute during training or"}, {"title": "Terminology", "content": "We refer to our system as \"self-learning\" to convey the system's ability to iteratively improve its performance by learning from dialogue contexts and user feedback. This goes beyond traditional SSL (self-supervised learning) techniques by integrating an \"interactive\" (though offline) component where the system learns from its environment rather than solely relying on pre-existing unlabeled data."}, {"title": "2. Background & Related Work", "content": "Methods for modeling context for automatic speech recognition (ASR) systems can be generally categorized into two main categories: supervised methods, which rely on additional data and labels to infer context which is useful for speech recognition, and unsupervised methods, which learn context cues directly from the utterance, and any associated prior/future utterances. In this section, we discuss our proposed approach in context with prior approaches for context-aware ASR."}, {"title": "2.1. Supervised Context Modeling", "content": "As discussed in section 1, supervised context modeling for au-tomatic speech recognition largely falls into three categories:\n\u2022 Fine-tuning: where models are fine-tuned on specific datasets to increase global context awareness.\n\u2022 Model Personalization: where model parameters are updated on a per-user basis using a small set of user-specific samples.\n\u2022 Contextual biasing: where models take additional context as input during the training and inference stages.\nEach of these approaches has benefits and drawbacks. Perhaps the most common approach for context modeling is fine-tuning, which includes context by training on specialized datasets. Such an approach can be quite effective, as it turns a long-tail distribution problem into an in-domain problem. However, it requires the collection of explicit data for the target problem, and the scope of the context that a model can learn is limited to the collected data. Further, this data collection process is often expensive \u2013 thus fine-tuning is often employed largely as an augmentation to an existing pre-trained model to fix specific errors, rather than as a good method for improving context awareness in general.\nWhile fine-tuning adjusts the model globally to incorporate context (such as rare words), recently some approaches have been explored that focus on adjusting the model parameters locally to account for context. show that small personalized models can be effective at incorporating information from user contexts, and show that small model adapters consisting of only a few"}, {"title": "2.2. Unsupervised Learning From Dialogue Contexts", "content": "Instead of learning context explicitly, unsupervised learning of context clues is a largely under-explored area in automatic speech recognition. Recent work has started to explore how we can learn contextual information from audio context alone. take in several utterances at once, and use this joint context to perform automatic speech recognition on the final target utterance (demonstrating up to 15% improvements in WER). Unfortunately, these methods require previous utterances to be available at test time and suffer when no previous context is available. Using only the target utterance, show that other unrelated utterances within a batch can be used to filter noise from automated speech recognition models, however, they do not show that such methods help beyond global and local noise removal.\nInstead of using audio, apply BERT to the partial ASR transcript generated so far and use those BERT embeddings to inform the generation of the next token (effectively fusing the language model with the speech model). Similarly, both and show that taking in related text context from past utterances can improve ASR performance. These approaches, while interesting, focus primarily on text embeddings of prior context, and do not show that such ASR performance can persist in a context-free scenario (as is often the case in on-device learning) or discuss the inclusion of future context (available at train, but not inference time).\nThe approach of unsupervised learning from dialog contexts is closely inspired by who introduce a family of methods (CLC) for learning from both past and future dialog contexts, using contrastive learning between the latent representations of past/future dialogues and the latent representation of the target utterance. The motivation behind this work is that audio that shares similar dialog contexts should have more similar latent representations, and thus, is more likely to contain relevant acoustic information. While our current approach borrows the PF-CLC objective from as an additional pre-training objective on top of our fully supervised and self-supervised fine-tuning process, we also found that alone, PF-CLC led to only minor improvements in overall performance due to the small per-gpu batch sizes used during training (in our case, each GPU has a maximum batch size of 16). Thus, to improve the performance of PF-CLC in our real-world training scenario, we introduce a novel scheme for online hard-negative mining, allowing for improved efficiency when applying the CLC losses during fine-tuning. Further, does not study in detail how to embed contexts during training, the impact of training on both past and future contexts, or if this context training persists under distillation."}, {"title": "2.3. Model Distillation", "content": "Model distillation has long been an effective tool when used to improve the performance of models during inference time. Not only are student models often more efficient than teacher models, but surprisingly, such models are often more effective on downstream test data. These trends have held in ASR as well, where all show that large teacher ASR models can be distilled to resource-efficient but performant student models.\nBeyond model compression, however, model distillation has more recently also been used effectively to bridge streaming models and non-streaming models, as both and have shown that using distillation between model architectures can lead to overcoming funda-mental architecture limitations at inference time. Recently, showed that ASR can be improved by dis-tilling in language models such as BERT, however, they did so using a vector-based representation, unlike our proposed ap-proach that leverages model distillation and self-supervision. Closest to our framework, Masumura"}, {"title": "3. Self-Learning for Dialogue ASR", "content": "An overview of our approach is given in Figure 1, and consists of two key components: a context-aware teacher model, leveraging both explicit context signals and implicit user feedback in the dialogue, and a single-utterance student model distilled from the context-aware teacher."}, {"title": "3.1. Teacher Model", "content": "Following best practices, our teacher model is composed of a Conformer-based transducer network - a nonstreaming model which can attend to all frames in an utterance. In addition, our teacher model also leverages both past and future contexts as explained in the following sections."}, {"title": "3.1.1. LEARNING FROM EXPLICIT CONTEXT", "content": "In this work, we leverage several explicit context sources drawn from the dialogues themselves. The first is the audio context, formed by the sequence of user input queries in a given dialogue (preceding and succeeding audio context is represented as XP and XF respectively in Figure 2). The second is text context, the ASR one-best hypothesis (indi-cated as \u0176P and \u0176F) corresponding to the sequence of user input queries in a dialogue along with the response generated by assistant encoded in text form (indicated as AP and AF).\nTraditionally, a transducer-based system, at each time step, outputs a probability distribution over its vocabulary (word-pieces) conditioned on the acoustic observations X = X1, X2, ..., X\u03c4 and previously observed word-piece tokens Y1, Y2, ..., Yu\u22121, which could be expressed as \u0420(\u0443u|X,Y1,Y2,...,Yu\u22121). To model the explicit long-term con-text in the teacher model, we extend the above equation by further conditioning on the set of context signals, Z = {\u0176P, \u0176F, AP, AF, XP, XF }, to get P(yu|X,Y1,Y2,\u2026\u2026\u2026,Yu\u22121,Z).\nAudio context modeling Like we ex-plore two methods for adding audio context from dialogues:\nFeature Concatenation: In feature concatenation, we concatenate features of past and future utterances along with the seed utterance and pass it through the audio encoder. Encoder outputs are then segmented to extract embeddings corresponding to seed utterance and are combined with prediction network output to compute transducer loss. The presence of a self-attention network in the audio encoder allows us to learn the dependency on context streams.\nAudio Embeddings: For audio embeddings, past and future context is encoded via a separate encoder (called \"context encoder\"). We use either a HuBERT pre-trained conformer encoder or the audio encoder of the transducer network as the context encoder. These audio embeddings are passed through a multi-headed self-attentive pooling layer and concatenated in time dimension with keys and values of self-attention module (MHSA) in the audio encoder (represented in Figure 2. Thus the inputs to MHSA (query - q, key - k, value - v) can be represented as q = X;k = [XP,X,XF];v = [XP,X,XF]. This allows us to attend to the contextual signal on a per-query basis. Note here that the output and input of the MHSA module still have the same number of time frames. This ensures that no other component in the model needs to be modified. Another distinct advantage of re-purposing MHSA in this manner as opposed to introducing a separate cross-attention layer (to attend to context) is that it allows us to easily extend conventional single utterance models to be context aware.\nText context modeling In addition to audio context, following and we explore two variants for encoding text context from prior utterances in a dialog:\nBERT Embeddings: In the BERT embedding case, we leverage a pre-trained text embedding model with 5M parameters based on BERT to generate a summary vector for the past/future text representations.\nLearned Embeddings: In the learned embedding case, text context is tokenized using a sentence piece model and each token is represented as a one-hot vector over vocabulary size. This is then converted to an em-"}, {"title": "3.1.2. LEARNING FROM IMPLICIT CONTEXT", "content": "In a multi-turn interaction with a voice assistant, the user may repeat or rephrase their query (to correct the system) following an unexpected response by the assistant. To empirically establish that such user reformulations (implicit interactions) are correlated with ASR, we conducted a simple experiment. We prepared two datasets: (i) Natural sampling: data is uniformly sampled to form our test set; and (ii) Reformulation sampling: we sample utterances that cause the user to repeat or rephrase their query. We then evaluate both our existing teacher and student models on these datasets. In this experiment, we observed that both the teacher and student models have significantly higher word error rates (11% and 15% respectively) on the reformulation sampling dataset compared to uniform sampling. This observation, combined with the fact that approximately 15% of analyzed interactions had user reformulations, shows that user-provided implicit feedback can correct ASR errors. Please note that such feedback is not directly solicited through the dialogue but inferred from user corrections and follow-up queries, hence we refer to it as \"implicit\".\nThus, while it is possible to learn to leverage context signals from the explicit context, it is also important to learn from implicit signals in the data. Recently, showed that implicit context present in the dialogues can be used to further augment the training process through contrastive learning. Drawing on their work, in this work, we leverage the past-future CLC objective from to incorporate implicit context in addition to the explicit context discussed in the previous section. In this PF-CLC approach, the positive pairs contain past/future/current utterances from the same utterance, while negative pairs are formed by past/future pairs from other utterances in the batches, further encouraging the model to organize the latent space semantically in addition to phonetically during pre-training."}, {"title": "3.1.3. ONLINE HARD NEGATIVE MINING (OHM)", "content": "When training our self-learning based system, we found a significant correlation between the local GPU batch size and the performance of the pre-training. We hypothesize that this correlation is caused by the PF-CLC learning introduced in as with smaller local batch sizes, there are fewer \"hard negatives\" in each batch, leading to reduced efficiency when training with CLC-based losses. Unfortunately, scaling the local GPU batch size can be practically difficult, without introducing complex optimizers and training procedures for model sharding. This presents a challenging issue: how can we train contrastive models under restricted local batch sizes?\nWhile several technical methods have been developed for contrastive learning with small batch sizes including which leverage tools from gradient checkpoint and model parallelism to improve the \"effective\" batch size, such methods have significant compute bottlenecks, and still rely on some form of all-reduce to compute the global contrastive loss. These all-reduces are technically complex, and on many GPU clusters can lead to significant communication overhead when machines must communicate with non-local devices.\nInstead, of such a complex all-reduce based approach, we target the root of the problem by aiming to build more effective local batches (i.e. batches that will induce high contrastive loss by leveraging hard negative mining, introduced by . Traditionally, such methods for hard-negative mining rely on a pre-labeling step, where batches are pre-constructed in an offline-scan, and then consumed during training. Unfortunately, such a pre-labeling approach does not scale well as the size of the training data increases. To remedy this, we introduce Ohm, a simple online hard-negative mining procedure that can run in line with traditional streaming data pipelines. An overview of the Ohm approach is given in Algorithm 1.\nOhm consists of several key stages each augmenting a data pipeline. In the first stage, samples are collected into an initial buffer B using a stateful map. When the size of the initial buffer exceeds the update window size, then a parametric clustering method C is fit on the samples from B. Note that this process happens per-device, and only with the samples yielded to each device, leading to non-blocking, and non-communicative behavior. Future version of Ohm could, however, communicate the C model, leading to all-reduce like behavior with reduced overhead. Once C had been trained, C is used in a streaming fashion to assign labels to each sample. Finally, reservoir sampling is used to sample batches of samples from each cluster group as they become available. This leads to batches which are generally closer semantically, even in the presence of a poor clustering algorithm C. Since the representations that we are using change, we periodically update C (every 10,000 steps) using the buffer B. Ohm thus solves a practical problem: training on GPUs with less VRAM precludes the use of larger (more effective) batch sizes, and leveraging Ohm claws back some of that performance loss."}, {"title": "3.1.4. REFORMULATION UP-SAMPLING", "content": "In addition to contrastive learning we further over-sample interactions containing reformulations during training of both the transducer and re-scorer. This approach can help to emphasize loss from reformulation samples, without introducing additional overhead. In our experiments, we em-pirically find an over-sampling ratio of 1:5 (1 reformulation to every 5 standard samples) to be effective (See Table 1)."}, {"title": "4. Experimental Design", "content": "In this section, we discuss the details used when evaluating our approach on both real-world conversational data, and open semi-synthetic data."}, {"title": "4.1. Datasets", "content": "All our transducer models are first pre-trained on 200k+ hours of de-identified ASR data (PRETRAIN) using trans-ducer loss without incorporating any contextual information. We then fine-tune and evaluate our approach on one of the two sources of data below: a closed-source real-world dataset from a conversational assistant, and the recently introduced OD3 dataset ."}, {"title": "4.1.1. CLOSED SOURCE DATA", "content": "This dataset consists of de-identified dialogues constructed from real-world interactions with a voice assistant. These dialogues are each constructed around a seed utterance, which is human transcribed. Given the seed utterance, the method pulls in all related conversations occurring 90 seconds before and after it. This step is iteratively applied, amassing additional conversational exchanges as they appear. If this approach yields over five utterances, the interval for gathering conversations is cut down to 45 seconds, and the process is repeated. This shortening of the interval persists until the number of utterances falls below five or the interval narrows to a 15-second minimum. These restrictions are enforced to ensure that utterances in a dialog are a semantically coherent interaction around the same request.\nMining dialogues with reformulations To train and eval-uate our system, we additionally detect a subset of dialogues consisting of reformulations. To detect reformulations, we use a text similarity-based approach. To be precise, we use cosine similarity and edit distance between the ASR hypothesis of the seed and context utterances (generated during the user's interaction with the assistant). The dialog is said to contain a reformulation if any {seed, context} pair has a similarity greater than the threshold.\nWe train our context encoder teacher models on 10M de-identified dialogues. Additionally, we upsample dialogues containing user reformulations, by 20%, during training of the transducer i.e. one in every five dialogues has reformulation. For evaluation, we only select dialogues containing reformulations and ensure that all utterances in the dialog are human-transcribed. By focusing on dialogues where the user reformulated his query, we ensure that the selected dialog has significant ASR errors (as discussed in section subsubsection 3.1.2) and where contextual signals are expected to be meaningfully related to user queries. We create two datasets for evaluation (1) ALL: All transcribed utterances across all validation dialogues (60K utterances) and (2) REF: A subset of ALL containing only utterances that"}, {"title": "4.1.2. OD3", "content": "Following we further evaluate our models on the open directed dialogue dataset (OD3). The OD3 dataset is a semi-synthetic dataset, where human-generated task-oriented dialogues from several popular data sets are augmented with LLM-generated conversational errors and computer-generated TTS audio. OD3 contains 620K turns of audio (approximately 1,172 hours)."}, {"title": "4.2. Model Details", "content": "For our experiments, we use transducer architecture, with Conformer as the audio encoder. We experiment with two different teacher architectures: (i) 200M parameters: 17 conformer blocks and attention size of 1024 (ii) 1B parameters: 18 conformer blocks and attention size of 1536. Each conformer block is composed of four modules - multi-head attention and convolutional modules are sandwiched between two feed-forward modules. The convolutional module has a kernel size of 30 frames. Before conformer blocks, we use a pre-processing block consisting of two convolution layers, which takes in features at a 30ms frame rate, and has a kernel size of 5 and stride of 3.\nOur student model has 1B parameters and consists of 18 conformer blocks with an attention size of 1536. However, for student models we restrict the attention module to only attend to past frames - this ensures user-perceived latency is minimal. For the prediction network, we use a two-layer LSTM network with 1024 hidden dimensions and a vocabulary of 4,000 tokens."}, {"title": "4.3. Training details", "content": "Our teacher model is pre-trained using the PRETRAIN dataset for 500K iterations, using a per-gpu batch size ranging from 32 to 1, depending on the length of the sequence (sequences are batched to maximally use GPU memory) across either 64 P100 GPUs (for 200M model) or 64 A100 GPUs (for 1B model). We pre-train using an Adam optimizer - we linearly increase the learning rate for 5000 steps and thereafter decrease it proportionally to the inverse square root of the step (as per schedule described in , and use magnitude-based gradient clipping with a value of 10.\nWe then fine-tune our teacher models for 150k steps, using an Adam optimizer with gradient clipping, featuring a learning rate decay schedule that starts at 1e-8, holds at 1e-5, and decays to le-6, with the clipping norm set to 0.3, and a schedule policy that adjusts the learning rate at 20K, 80K, and 600K training steps. In addition, we apply dynamic L2 regularization to the Multi-head self-attention layers of the conformer using a PiecewiseConstantDecay scheduler that increases the regularization factor at training steps 15K and 30K (calculated as 2 \u00d7 number of conformer blocks \u00d7 warmup steps), with the regularization values set to le-6, 5e-6, and le-5 at these intervals.\nFor models trained with contrastive learning, we use a set of 32 learned clusters for hard-negative mining, with a buffer size of 4096 for the online reservoir sampling. We leverage the BIRCH clustering algorithm, over the embeddings of XP. In the future, we intend to explore addi-tional clustering algorithms and leverage better distance func-tions for the Ohm mining approach. For the hyper-parameters of the PF-CLC loss, we follow the parameters in. Our student model is pre-trained using PRETRAIN dataset for 400K iterations with 64 A100 GPUs and a per-gpu batch size ranging from 128 to 1. For model distillation, we use both SSRD and PRETRAIN data, sampled at differing ratios, and standard ASR transducer loss."}, {"title": "5. Results and Discussion", "content": "As discussed in section 4, we evaluate our system on both closed-source data and the OD3 dataset. In general, we use both standard word error rate (WER, \u2193) and relative word error rate improvement (WERR, \u2191) to evaluate our system."}, {"title": "5.1. Teacher Performance", "content": "Our overall results for the teacher model on the ALL and REF are given in Table 2. We can see that our system, combining the feature-concatenation audio context (subsection 3.1), learned text context (subsection 3.1), and CLC/Ohm losses (subsubsection 3.1.2), outperforms the baseline model by"}, {"title": "Combining Context Types:", "content": "We get the best performance when both audio and text contexts are combined (compared to adding two modalities individually). Interestingly, the 200M model with context is significantly better than the 1B model without contextual signals, highlighting the efficacy of our proposed approach in modeling implicit context signals. On OD3, we can see that adding both context types leads to a 6.47% performance improvement, tracking similarly to the performance improvements seen in the ALL dataset."}, {"title": "Causal vs. Non-Causal Context:", "content": "In Table 5, we ablate the types of context that we show to the model. We observe that injecting non-causal (\u201cfuture\u201d) context during training provides relative WERR of 7.39% as opposed to 5.08% on the REF dataset (as well as improvements on the ALL dataset), indicating that future context is significantly more important when correcting user reformulations. This is likely due to the fact that user reformulation is a \u201cfuture signal\u201d i.e. it follows the utterance that caused the error."}, {"title": "Implicit Context Learning:", "content": "We can see that learning from the implicit context in the model is important for under-standing and correcting dialog errors. As shown in Table 6, Adding CLC and Ohm to the baseline model leads to signifi-cant improvement in the overall performance, particularly on the REF dataset (so much that it enables a 200M parameter model to outperform a 1B parameter model without such losses). On the OD3 dataset (Table 3), the performance is even more distinct, with learning from implicit context leading to up to a 26.6% relative improvement over a baseline non-context model. In addition, zero shot comparison with other open source benchmark models is shown in Table 4."}, {"title": "5.2. Distilling knowledge to student model", "content": "Table 7 shows the performance of our model when distilled to a context-free student model. We can see that in all"}, {"title": "5.3. Tail-Distribution Performance", "content": "While overall WER is an important measure, many times, a strong indicator of user satisfaction is performance on a wide range of queries on different topics (Such as home automa-tion, calling/messaging and shopping). In Table 8, we present WERR and SERR (Sentence Error Rate Improvement) when the WER is computed on each topic independently, and then averaged instead of being averaged over all utterances (independent of domain, i.e. Table 8 makes the assumption that all domains are equally likely). From this, we can see that while our non-context models perform well on the most common utterances, the contrastive models lead to signif-icant improvements in less-common domains in our ALL dataset, including queries categorized into shopping (82.86% WERR), calling/messaging tasks (73.7% WERR), and music request tasks (36.8% WERR), all of which often need contextual disambiguation. On the other hand, while still in the long tail of the dataset, our approach performs worse than the baseline on home automation tasks (-22.68% WERR), one of the less diverse tasks that requires less contextual disambiguation. In such cases, our model may be relying more on the context, than the target utterance: leading to de-creased performance. It remains interesting for future work to explore how we can dynamically trade off between context clues (for challenging utterances), and non-context learning"}, {"title": "6. Conclusion", "content": "This work introduces a framework that improves ASR in dialog systems through a dual-model approach to contextual learning: a context-aware teacher model that improves learning through explicit and implicit context signals, and a distilled student model that maintains efficiency during infer-ence without context reliance. We achieve significant WER reductions, up to 9.58% on real-world datasets and 26.6% on the OD3 dataset, with the student model maintaining up to 33% of the reduction without context across the distillation process. The enhancements observed, particularly for rare words and diverse user queries, indicate a path toward more robust and satisfying conversational experiences, notably, the pronounced gains for tail queries suggests that our approach can significantly improve performance on less common tasks. Future directions for this work involve exploring the dynamic adjustment of the relative importance of context versus the target utterance based on their predicted utility, error correction and safety . This could potentially unlock even greater improvements in ASR performance, paving the way for more intelligent and adaptable conversational AI systems."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal conse-quences of our work. Automatic speech recognition technol-ogy enhances accessibility, education, healthcare, legal pro-cesses, customer service, workplace productivity, language preservation, global connectivity, media accessibility, and safety across various societal sectors. While such impact is largely positive, it is important to recognize the impact of self-learning systems for automatic speech recognition on greater discussions in privacy and security, which are well discussed in related work ."}, {"title": "A. Insertions/Deletions/Substitutions in CLC/Ohm", "content": "We can further break down the performance in Table 6 in terms of insertions, deletions and substitutions, which is given in Table A.1. We can see that adding CLC loss significantly improves the rate of deletion compared to baseline models. Unfortunately, this comes at the cost of improvement in substitution and insertion. CLC, instead of doing the best job of disambiguating generated tokens, focuses on recall as opposed to precision. Ohm improves the disambiguation, as at the cost of deletions: more tokens are dropped, but the tokens that are preserved are more accurate."}]}