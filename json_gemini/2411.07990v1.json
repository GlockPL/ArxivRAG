{"title": "Derivational Morphology Reveals Analogical Generalization in Large Language Models", "authors": ["Valentin Hofmann", "Leonie Weissweiler", "David Mortensen", "Hinrich Sch\u00fctze", "Janet Pierrehumbert"], "abstract": "What mechanisms underlie linguistic generalization in large language models (LLMs)? This question has attracted considerable attention, with most studies analyzing the extent to which the language skills of LLMs resemble rules. As of yet, it is not known whether linguistic generalization in LLMs could equally well be explained as the result of analogical processes, which can be formalized as similarity operations on stored exemplars. A key shortcoming of prior research is its focus on linguistic phenomena with a high degree of regularity, for which rule-based and analogical approaches make the same predictions. Here, we instead examine derivational morphology, specifically English adjective nominalization, which displays notable variability. We introduce a new method for investigating linguistic generalization in LLMs: we fit cognitive models that instantiate rule-based and analogical learning to the LLM training data and compare their predictions with those of the LLM, allowing us to draw direct conclusions regarding underlying mechanisms. We compare the performance of GPT-J on a set of nonce adjectives with that of a high-performing rule-based model and a competitive analogical model. As expected, both models explain the predictions of GPT-J equally well for adjective classes with regular nominalization patterns. However, for adjective classes with variable nominalization patterns, the analogical model provides a much better match. Furthermore, GPT-J's behavior is sensitive to the individual word frequencies, even for regular complex forms a behavior that is consistent with an analogical account of regular forms but not a rule-based one. These findings refute the hypothesis that GPT-J's linguistic generalization on adjective nominalization involves rules, suggesting similarity operations on stored exemplars as the underlying mechanism. Finally, we highlight a difference between linguistic generalization in humans and LLMs: while humans generalize based on types, LLMs generalize based on tokens, which we show has detrimental effects on their predictions. Overall, our study suggests that analogical processes play a bigger role in the linguistic generalization of LLMs than previously thought.", "sections": [{"title": "Introduction", "content": "In the recent past, large language models (LLMs) such as Chinchilla [1], Gemini [2], GPT-4 [3],\nLLaMA [4], Mistral [5], OLMO [6], and PaLM [7] have reached an unprecedented level of linguistic\ncapability. While some have likened the language skills of LLMs to those of humans [8, 9], others\nhave highlighted the persistent linguistic inadequacies of LLMs [10-13]. Crucially, regardless of\nexactly how human-like or non-human-like the language skills of LLMs are, it is well established\nthat they go beyond simply copying from the training data [14-19].\nWhat are the mechanisms underlying this kind of linguistic generalization in LLMs? Prior studies\nhave approached this question by investigating the extent to which the language skills of LLMs\nresemble abstract, symbolic rules [20, 21]. For example, the consistency with which LLMs predict the\ncorrect agreement for unseen subject-verb pairs has been interpreted as evidence that they implicitly\ninfer a set of symbolic rules from the training data [18]. Comparatively speaking, much less attention\nhas been devoted to the question of whether the language skills of LLMs could be the result of\nanalogical processes operating on stored exemplars. Within cognitive science, this alternative type of\ngeneralization is argued to be a central learning mechanism and a foundation for the ability of humans\nto form abstract conjectures, a hallmark of human intelligence [22\u201325]. Within linguistics specifically,\nanalogical models of word formation have proved successful in capturing detailed patterns of variation\nin multiple languages [26-32].\nWhile LLMs store a considerable\namount of their training data in their\nmodel weights [33-35, 19], thus im-\nplicitly providing a reservoir of stored\nexemplars that might support analogi-\ncal reasoning, it is still unclear in what\nways these stored data are in fact im-\nplicated in the LLMs' language skills.\nAs a third possibility, it could be the\ncase that LLMs learn rules for regu-\nlar linguistic phenomena while han-\ndling irregular linguistic phenomena\nby means of analogy over stored ex-\nemplars, in line with dual-mechanism\napproaches [36-38, 31].\nHere, we present the first in-depth\nanalysis of the role of analogical lin-\nguistic generalization in LLMs. Our\nwork is motivated by a key shortcom-\ning of the existing literature: prior\nstudies, most of which assume rule-\nbased generalization in LLMs [e.g.,\n18], have focused on syntactic phe-\nnomena such as subject-verb agree-\nment, which display a high degree of regularity. Crucially, in such cases, both rule-based and\nanalogical, exemplar-based approaches make the exact same predictions [39\u201341]; in other words,\nrule-like behavior of LLMs on regular linguistic phenomena does not represent any evidence for\nrule-based generalization. This very insight was at the heart of the pioneering research that first\napplied neural networks in the context of language learning, which argued that \u201clawful behavior and\njudgments may be produced by a mechanism in which there is no explicit representation of the rule\" [42]. In fact, neural network models of language depend in important ways on similarity relations\namongst input examples [43-47], suggesting that analogy might play a major role for the language\nskills of LLMs. However, this hypothesis has not been systematically tested yet.\nIn this study, we focus on a domain of language that is known to exhibit more variability than syntax,\nmaking it better suited for distinguishing rule-based from analogical generalization: derivational\nmorphology [48-52]. Specifically, we analyze how LLMs learn English adjective nominalization\nwith -ity and -ness [53\u201356], focusing on adjectives that themselves contain a derivational suffix (e.g.,\navailable, selfish, hyperactive). Such cases of affix stacking are an ideal testbed for our purposes\nsince the adjective class (i.e., the adjective-final suffix) provides a controlled way to vary the regularity\nof the nominalization process: while some adjective classes are nominalized in a very regular way,\nexhibiting a clear preference for either -ity (e.g., adjectives ending in -able such as available) or\n-ness (e.g., adjectives ending in -ish such as selfish), others exhibit a substantial degree of variability\n(e.g., adjectives ending in -ive such as hyperactive). Furthermore, English adjective nominalization\nwith -ity and -ness has been shown to be fully explainable as a result of analogical generalization in\nhumans [57], suggesting that LLMs might employ the same mechanism. In general, probabilistic\nmodels [58], and particularly exemplar-based analogy models [59, 60], have recently proven very"}, {"title": "Results", "content": "Generalization to Nonce Words\nWe compare the linguistic generalization behavior of GPT-J with that of two high-performing\ncognitive models (Materials and Methods, Cognitive Models): the Minimal Generalization Learner\n[MGL; 73, 74] and the Generalized Context Model [GCM; 22, 23, 75]. The MGL is a rule-based\nmodel that we have selected because it undertakes to capture detailed patterns of variation that earlier,\nsimpler models did not capture. The GCM is an exemplar-based analogy model that was previously\napplied to variability in inflectional morphology [28, 31]. The comparison will give us some evidence\nabout the underlying mechanism; if, for example, GPT-J agrees consistently with the MGL over the\nGCM, this would suggest that it uses a similar (i.e., rule-based) mechanism under the hood.\nThe MGL and GCM models can be fit to either word types or word tokens. The inventory of word\ntypes corresponds to the list of words in a mental lexicon; only the existence of a word in the language\nis taken into account, and not its frequency in the training data. In an inventory of word tokens, each\noccurrence of a word in the training data is treated as a separate instance, with the result that more\nfrequent words have more instances than less frequent words. We consider both settings, because the\ncontrast between behaviors governed by type frequencies and those governed by token frequencies is\na major theme in cognitive research on the lexicon. We focus on English adjective nominalization and\nexamine four adjective classes (i.e., sets of adjectives ending in the same suffix), two of which clearly\nprefer -ity or -ness, and two of which are less regular while still showing an overall tendency towards\none of the two suffixes (Table 2). We train the cognitive models on all adjective-derivative pairs\nthat meet the following three criteria: (i) the adjective belongs to one of the four adjective classes in\nquestion; (ii) the derivative ends in -ity or -ness; (iii) both the adjective and the derivative occur in\nthe Pile. For evaluation, we use UniPseudo [76] to generate 50 nonce adjectives for each of the four\nadjective classes (Materials and Methods, Nonce Adjectives). We check that both the generated nonce"}, {"title": "Predictions for Seen Words", "content": "According to cognitive theories, analogies are based on remembered examples. If the mechanism\nunderlying GPT-J's behavior is analogical, it must implicitly remember a large number of examples.\nAs the first step in evaluating this inference, we ask how well GPT-J's behavior matches the statistics\nof its training data. Accurately matching the training data, derivative by derivative, would imply that\nthe distributed representations in GPT-J encode information about individual derivatives.\nWe extend the four adjective classes examined so far and include six other adjective classes that\ncan be nominalized with either -ity or -ness: -al, -ar, -ed, -ic, -ing, and -less. We can divide the ten\nadjective classes into four groups with similar degrees of competition between -ity and -ness (see\nSupporting Information for details):\n\u2022 -ed, -ing, -ish, -less (R-NESS): this group exhibits the highest degree of regularity and almost\nalways takes -ness.\n\u2022 -able, -al, -ar, -ic (R-ITY): this group also exhibits a high degree of regularity (although\nsomewhat lower than in the case of R-NESS), with a strong tendency toward -ity.\n\u2022 -ous (V-NESS): this adjective class exhibits a high degree of variability, with a slight tendency\ntoward -ness.\n\u2022 -ive (V-ITY): this adjective class also exhibits a high degree of variability, with a slight\ntendency toward -ity.\nWe ask whether GPT-J treats adjectives from these four groups differently, and whether differences\nbetween the more regular and more variable ones correspond to differences in the training data. We\ndraw upon the Pile and extract all derivatives ending in -ity and -ness whose bases belong to one of\nthe 10 adjective classes. To decrease noise, we only extract derivatives whose bases also occur in the\nPile and apply several filtering heuristics, such as excluding words with non-alphabetic characters\n(see Supporting Information). To include all productively formed derivatives, we do not impose a\nfrequency threshold on the derivatives.\nThe overall setup of probing GPT-J is identical to the comparison with the cognitive models: we\nmeasure the probability that GPT-J assigns to the two derivatives resulting from adding -ity and -ness"}, {"title": "Frequency Effects and Analogical Pressure", "content": "To test whether at least part of GPT-J's behavior on adjective nominalization can be explained by rules,\nwe analyze the extent (on a log probability scale) to which GPT-J prefers the observed nominalized\nform over the alternative, non-observed nominalized form. We consider only cases for which just\none outcome of nominalization is attested in the Pile. The score for the unattested alternative thus\nrepresents the latent competition from the new form that might be created by rule or analogy. The\ndifference between the two scores can be viewed as reflecting GPT-J's confidence in its decision to\nuse a form that it has encountered during training. A large difference indicates high confidence, while\na small difference reflects low confidence. For each adjective class, we create two sets: one in which\nthe attested derivative has a low frequency in the Pile, $f \\in (0, 10]$, and one in which the attested\nderivative has a high frequency in the Pile, $f \\in [100, \\infty)$.\nIf an adjective class is handled by a rule, the difference in frequency between the two sets should\nnot affect GPT-J's confidence to predict the attested derivative. This is because rule-based theories\nabstract away from individual words; once a rule has been acquired, regular complex forms are\nassumed to be generated on the fly, much like complex sentence structures are, rather than being\nstored in memory. Rule-based theories predict that only memorized exceptions to rules will exhibit\nfrequency effects. Many researchers might suggest the following default rule for nominalization\n(taking NOM to be the underlying morpheme that may be spelled out as -ness or -ity):\n$\\text{NOM} \\rightarrow \\text{-ness} \\qquad(1)$\nUnder this assumption, all forms in -ity would be memorized exceptions. Statistical theories of\nrule-formation, such as MGL, would induce the following narrower rule for R-NESS, which has"}, {"title": "Human Use of Word Types Versus Tokens", "content": "We have established that GPT-J relies on token-level analogical generalization. In contrast, previous\nstudies have concluded that humans generalize over word types [87-89]: their propensity to generalize\na word formation pattern depends on the number of distinct word types in the individual's mental\nlexicon that support the pattern (referred to as the size of the lexical gang). This points to a difference\nbetween the morphological processing in humans and LLMs. We will now investigate this difference\nin greater detail, by comparing the predictions of GPT-J to judgments made by humans."}, {"title": "Judgments of Nonce Words", "content": "First, we make a direct comparison to GPT-J's behavior for nonce words. 22 native English speaker\nvolunteer annotators indicated their preference for the -ity versus the -ness derivative of each nonce\nadjective in our study (Supporting Information). Because GPT-J is not a state-of-the-art model, we\nalso introduce an additional comparison, by asking whether a more recent model is more human-like"}, {"title": "Familiarity of Complex Words", "content": "Our results on nominalizations indicate that GPT-J and GPT-4 do not have a mental lexicon in the\nsense that humans do, in that they lack the ability to step back from word tokens and generalize over\nword types. Here, we present a brief demonstration that this observation pertains to morphologically\ncomplex words more generally, and not just to nominalizations. For this demonstration, we draw\non the Hoosier Lexicon, a dataset of 19,320 English words that includes word frequencies and\nfamiliarity ratings on a seven-point Likert Scale [98]. An important finding of the original study was\na dissociation between word frequency and rated familiarity; one might expect the two to be highly\ncorrelated, however some infrequent words are judged as much more familiar than their frequency\nwould suggest. Needle et al. [99] identify morphological structure as an important factor contributing\nto this dissociation. A word like precancellation, with a recognizable prefix, stem, and affix seems\nfamiliar even though it is rare, on the strength of the familiarity of its parts.\nWe analyze the $n = 2, 835$ words in the Hoosier lexicon that have a frequency of less than 10,000\nin the Pile (corresponding to a frequency of roughly 1 in 50,000,000 words or less). Leveraging\nthe CELEX dictionary [54] and methodology from prior work [100, 101], we use affix-stripping to\nidentify the $n = 1,005$ words that exemplify derivational morphology by virtue of being parsable\nas a simpler word plus any combination of affixes. $n = 1,830$ words cannot be parsed in this way,\nand we consider them to be simplex words (see Supporting Information). For human judgments, we\ntake the familiarity ratings reported by Nusbaum et al. [98]. We estimate the 'familiarity' that GPT-J\nassigns to a word as the log probability that it assigns in the context of neutral prompts (see Materials\nand Methods, Vocabulary Test for details). Comparing log probabilities to human familiarity ratings\nis justified because the probabilities assigned to words by language models are known to correlate\nwith psycholinguistic measures of lexical access [e.g., reading times; 102], which for humans are\nimpacted by familiarity to a larger extent than frequency [103]."}, {"title": "Discussion", "content": "This paper provides the first empirical evidence for analogical linguistic generalization in LLMs.\nWe found that an analogical cognitive model best explains how GPT-J nominalizes unseen nonce\nadjectives whose adjective class exhibits a high degree of variability. While the analogical and the"}, {"title": "Materials and Methods", "content": "Cognitive Models\nMGL [73, 74] works by inferring abstract rules from the lexicon. It starts by iterating over pairs\nof words and forming initial generalizations based on shared phonological features, which are\nthen iteratively merged, yielding increasingly abstract rules. Each rule is associated with a value\nsignifying its accuracy in the lexicon, potentially adjusted by its overall support in the lexicon. To\nmake predictions for a new input, the rule with the highest accuracy that matches the phonological\nproperties of the input is selected. GCM [22, 23, 75] does not infer abstract rules but instead stores\nall forms from the training data in an inventory. To make predictions for a new input, the input is\ncompared to all instances that exhibit a specific type of output (e.g., to all bases that have a derivative\nwith -ity). The type of output with the highest total similarity to the input is selected.\nWe use the implementation of MGL made available by Albright and Hayes [74], using default\nhyperparameters. For GCM, our implementation exactly follows prior studies in linguistics using the\nmodel [e.g., 74, 28, 31].\nNonce Adjectives\nTo create the nonce adjectives for the four adjective classes, we draw upon UniPseudo [76]. UniPseudo\nuses an algorithm based on Markov chains of orthographic n-grams that it applies to a specifiable\nlist of input words, generating a list of pseudowords. Importantly, when all input words end in a\ncertain sequence of characters, the generated pseudowords also end in that sequence of characters.\nWe leverage this property of UniPseudo to generate 50 nonce adjectives for each adjective class based\non a curated list of adjectives drawn from CELEX [112], MorphoLex [113], and MorphoQuantics\n[114]. For pseudoword length, we use the two most frequent lengths as measured on the extracted\nadjectives for each class and generate 25 pseudowords for each length. We use the bigram algorithm.\nSee Supporting Information for the full list of pseudowords."}, {"title": "GPT-J", "content": "We describe the method we use to probe GPT-J more formally. Let b be a base (e.g., sensitive)\nand s be a suffix (e.g., -ity). We denote with d(b, s) the derivative resulting from adding s to b and\napplying all required morpho-orthographic changes (e.g., deletion of base-final e). For instance, for\nb = sensitive and s = -ity, we have d(b, s) = sensitivity. To measure the probability that GPT-J\nassigns to d(b, s) as a derivative of b, we use various prompts t(b). While some of the prompts\nask GPT-J to nominalize b (e.g., t(b) = Turn the given adjective into a noun. b \u2192), others are less\nexplicit (e.g., t(b) = b \u2192). See Supporting Information for the full set of prompts.\nGiven a filled prompt t(b), we pass it through GPT-J and measure the probability that GPT-J assigns\nto the two derivatives d(b, -ity) and d(b, -ness) as continuations of t(b).\nWe use the GPT-J implementation available on Hugging Face [115]. GPT-J has a total of\n6,053,381,344 parameters. All experiments are performed on a stack of eight GeForce GTX 1080 Ti\nGPUs (11GB)."}, {"title": "GPT-4", "content": "Since the OpenAI API does not provide access to the token-level probabilities, we cannot use the\nsame method as for GPT-J. Instead, we leverage GPT-4 instruction-following capabilities and directly\nask it which of the two derivatives it prefers for a given nonce adjective."}, {"title": "Vocabulary Test", "content": "To measure the probability that GPT-J assigns to a word w, we use various prompts t(w) (e.g.,\nt(w) = The following is a word: w). See Supporting Information for the full set of prompts.\nGiven a filled prompt t(w), we pass it through GPT-J and measure the probability that GPT-J assigns\nto the word."}, {"title": "Supporting Information", "content": "Derivative Statistics\nWe analyze the statistics of -ity and\n-ness derivatives in the Pile (Table 5).\nWe first focus on type frequency, i.e.,\nthe number of different derivatives\ncontained in the Pile. For most classes,\nthere is a clear preference for either\n-ity or -ness, the only two exceptions\nbeing adjectives ending in -ive and\n-ous. For adjectives ending in the\nGermanic suffixes -ed, -ing, -ish, and\n-less, there is a particularly strong\npreference for -ness, although a few\nderivatives in -ity can be found in the\ndata. These statistics are similar to the\nresults of a recent analysis based on\ndictionary data [57], indicating that\nthe Pile provides a realistic picture of\nthe variation between -ity and -ness in\npresent-day English.\nNext, we turn to token frequency, i.e.,\nthe number of times individual deriva-\ntives occur in the Pile. We notice that the trends for type frequency are largely reflected by token\nfrequency: in the case of adjective classes for which -ity derivatives have a higher type frequency\nthan -ness derivatives, -ity derivatives also tend to have a higher average token frequency than -ness\nderivatives (and vice versa). The only exception is -ous, where -ity has a lower type frequency\nbut a higher average token frequency than -ness. This is due to a particularly large number of -ity\nderivatives in the high token frequency range: excluding the top 5% of derivatives with the highest\ntoken frequency, the average token frequency is higher for -ness (73.0) than -ity (15.1), in line with\nthe type frequency trend for -ous.\nFinally, we examine a measure that linguistic scholarship has suggested to be particularly relevant for\nproductivity [116, 54], specifically the number of hapaxes (i.e., derivatives occurring only once in\nthe Pile). Here, the trends for individual adjective classes are similar to type frequency and token\nfrequency, with the potential exception of -ive, where the preponderance of -ity compared to -ness is\nslightly less pronounced.\nNonce Adjectives\nTable 7 lists all nonce adjectives used in the paper.\nAdjective Annotation\nWe collected human judgments from volunteers using the SoSciSurvey platform. Native speakers of\nEnglish were recruited using snowball sampling and asked if they were willing to participate in a\nsurvey about derivational morphology. They were unaware of the exact goals of the study but knew\nthat their data would be used for research purposes. In total, 22 participants took part in the survey.\nAll participants held at least an undergraduate college degree. The collection of adjective annotations\nwas approved by the institutional review board of the Allen Institute for AI.\nEach participant coded half of the nonce words (i.e., 100 nonce words), resulting in 11 annotations\nper nonce word. Participants were first shown an introductory message explaining the task as shown\nin Figure 7a. They were then given one of two survey versions, each with 100 nonce words that\ncycled through the four suffixes to avoid repetition. To reduce the total time necessary for completing\nthe survey, participants were immediately shown the next question upon clicking one word. An\nexample of a question is shown in Figure 7b."}, {"title": "Annotator Variation", "content": "Figure 5b plots for each tested adjective class\nthe ratio of bases for which participants overall\npreferred -ness over -ity, i.e., more participants\nselected the -ness rather than the -ity derivative.\nThere is a clear preference for -ity in the case\nof -able and a clear preference for -ness in the\ncase of -ish. For the two suffixes with a larger\ndegree of competition, -ive shows the expected\npattern, with participants preferring -ity over\n-ness for the majority of bases, but -ous shows\na preference for -ity, which is different from its\ngreater association with -ness in the Pile. This\ncan also be seen from the ratio of participants\npreferring -ness over -ity for individual bases\n(see Figure 6a), which is on average smaller\nthan 50% for -able (17.7%), -ive (39.8%), and\n-ous (47.5%), and greater than 50% only for -ish\n(95.1%). Figure 6a also shows a high degree of\nvariation between individual bases of a certain\nadjective class: e.g., for -ous, there are bases for which participants clearly preferred -ity (e.g., 81.8%\npreferred -ity for indaminous), but there is also a base for which participants exclusively selected\n-ness (100% preferred -ness for rebelorous).\nThe human annotations vary also by participant:\nparticipants differed strongly in terms of how\noften they selected -ity or -ness for each adjec-\ntive class (see Figure 6b). For example, 13 par-\nticipants preferred -ity for -ous bases, but nine\nparticipants preferred -ness. The high degree of\nvariation is also reflected by an only small inter-\nannotator agreement (IAA) of 0.335, measured\nusing Fleiss' \u03ba. However, measuring IAA on\nall bases hides the fact that IAA is substantially\nhigher for -ish (0.899) and -able (0.587) than for\n-ive (0.096) and -ous (0.054), measured using\nGwet's AC1 [117]. We also notice that there\nis a correlation between the responses given by\nindividual participants for bases of different ad-\njective classes, especially between -able and -ive\n(0.417), and -ive and -ous (0.415), measured us-\ning Pearson's r. In other words, there is a moderate tendency for participants to prefer -ity or -ness\nfor both -able and -ive bases (and similarly for -ive and -ous bases)."}, {"title": "Prompts", "content": "We want to test which of two derivatives the one ending in -ness or the one ending in -ity \u2014 is\npreferred by a language model. To do so, we need to measure the probability that the language\nmodel assigns to the two competing forms. For example, we need to measure the probability that the\nlanguage model assigns to sensitiveness, and the probability that it assigns to sensitivity. Language\nmodels such as GPT-J and GPT-4 always assign probabilities to tokens given a sequence of preceding\ntokens. Therefore, in order to measure the probability that a language model assigns to a specific\nderivative, we need to decide on what tokens to use as the preceding context. This is commonly\nreferred to as prompting, and the sequence of preceding tokens that is fed into the language model as\nprompt [118]. Properties of the prompt (e.g., the exact wording of a request) can substantially affect\nthe language model predictions [77], which is why it has become common practice to examine several\ndifferent prompts when analyzing the behavior of language models. Here, we use the following 12\nprompts to measure the probabilities that GPT-J assigns to the derivatives:"}]}