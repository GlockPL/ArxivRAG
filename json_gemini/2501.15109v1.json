{"title": "Clear Preferences Leave Traces: Reference Model-Guided Sampling for Preference Learning", "authors": ["Nirav Diwan", "Tolga Ergen", "Dongsub Shim", "Honglak Lee"], "abstract": "Direct Preference Optimization (DPO) has emerged as a de- facto approach for aligning language models with human preferences. Recent work has shown DPO's effectiveness re- lies on training data quality. In particular, clear quality dif- ferences between preferred and rejected responses enhance learning performance. Current methods for identifying and obtaining such high-quality samples demand additional re- sources or external models. We discover that reference model probability space naturally detects high-quality training sam- ples. Using this insight, we present a sampling strategy that achieves consistent improvements (+0.1 to +0.4) on MT- Bench while using less than half (30-50%) of the training data. We observe substantial improvements (+0.4 to +0.98) for technical tasks (coding, math, and reasoning) across mul- tiple models and hyperparameter settings.", "sections": [{"title": "Introduction", "content": "Preference learning aims to align Large Language Models (LLMs) with human preferences. It is applied after pre- training and supervised fine-tuning (SFT) to teach models to generate responses that better align with human expecta- tions while preserving knowledge acquired in earlier train- ing stages. This approach has demonstrated practical impact in improving user experience (Bai et al. 2024), implement- ing safety filters (Liu, Sun, and Zheng 2024; Huang et al. 2024), and moderating content (Ma et al. 2023).\nDirect Preference Optimization (DPO) (Rafailov et al. 2024) is a a supervised off-policy method that has recently emerged as a leading approach to preference learning. Un- like on-policy methods, DPO directly optimizes the policy using paired preference data, where each pair contains an aligned (or preferred) and misaligned (or rejected) response. By training the model to assign higher probabilities to pre- ferred responses, DPO effectively shapes the model's output distribution without requiring a separate reward model. This simplicity, combined with strong empirical results, has made DPO increasingly popular for language model alignment."}, {"title": "Methodology", "content": ""}, {"title": "Background", "content": "Direct Preference Optimization (DPO) (Rafailov et al. 2024) is a supervised off-policy method used to optimize models based on user preferences without relying on a sep- arate reward model. Instead, DPO directly aligns the pol- icy with preference data by leveraging a ranking-based ap- proach.\nThe objective of DPO is to use the reference policy, typ- ically a SFT model, to guide the optimization process. The preference optimization is defined using pairs of responses, where one response is preferred over the other. By focusing on these response pairs, the goal is to minimize the DPO loss function defined as:\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} [\\log \\sigma (\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} )]$\nwhere $\\pi_{\\theta}$ is the policy model, $\\pi_{ref}$ is the reference model, $\\sigma$ is the sigmoid function, and $(x, y_w, y_l)$, $\\beta$ is the hyper-parameter controlling the deviation from the base reference policy, are preference pairs comprising a prompt x, a pre- ferred response $y_w$, and a rejected response $y_l$, drawn from the dataset D."}, {"title": "Preference Clarity", "content": "It quantifies the degree of quality dif- ference between two responses in a preference pair. Given responses $r_1$ and $r_2$, we calculate preference clarity using ground truth quality scores $s_1$ and $s_2$ assigned to each re- sponse. These scores can be obtained either through human annotation or using LLM-as-a-judge frameworks with mod- els like GPT-4. Formally, for a preference pair $(r_1, r_2)$, the preference clarity is measured as:\n$clarity(r_1, r_2) = s_1 - s_2$         (1)\nwhere $s_1, s_2$ are the ground truth quality scores."}, {"title": "Reference-model based sampling", "content": "Method. Our sampling strategy leverages normalized ref- erence policy probabilities to identify clear preference signals. For input x with responses $y_w$ (preferred) and $y_l$ (rejected), we compute the difference between length- normalized reference probabilities. We sample pairs where this difference exceeds threshold $\\delta$:\n$| \\frac{\\log (\\pi_{ref} (y_w|x))}{|y_w|} - \\frac{\\log (\\pi_{ref}(y_l|x))}{|y_l|} | > \\delta$         (2)\nwhere $\\pi_{ref}(y|x)$ is the reference probability and $|y|$ is the sequence length. This normalization enables comparison be- tween responses of different lengths, and focuses training on pairs with larger probability gaps. This simple approach prioritizes training examples with clearer preference signals while downweighting pairs where reference model estimates that the responses are similar.\nReference Model as Quality Detector. Our analysis re- veals that the sampling strategy effectively identifies high- quality preference pairs. Using LLAMA-3-8B as the ref- erence model, we compute probability differences between preferred and rejected responses in the Ultrafeedback dataset (Figure 1). Samples selected at higher thresholds ($\\delta$) in Equation 2 consistently demonstrate greater preference clar- ity. This suggests reference models naturally detect strong training examples without requiring additional annotation."}, {"title": "Experiments", "content": "We use a simple evaluation framework for our sampling method. First, we use an SFT model as our reference model. We compute the probabilities for all responses in our prefer- ence pairs. We then create different versions of our training dataset by sampling based on probability gaps. Each version uses a different threshold $\\delta$. We align models using DPO on both the full dataset and our sampled versions. Finally, we evaluate and compare their performances on standard bench- marks. This setup directly tests whether selecting clearer preference pairs improves model alignment.\nDataset. We use the Ultrafeedback dataset (Cui et al. 2023) for all our experiments. The dataset provides a bi- narized preference version containing 64k samples, where each response pair is accompanied by preference scores."}, {"title": "Results and Discussion", "content": "Performance\nWe present the results in table 1. The policy models trained using our proposed reference model-based sampling consis- tently outperform those trained on the full dataset. Notably, the improvements range from moderate to large (+0.11 to +0.40), with most notable gains achieved using reduced training data. For LLAMA3, we observe large improve- ments (+0.40) while using less than a third of the origi- nal dataset. Similarly, LLAMA 3.1 shows moderate gains (+0.14) using less than half the data. The pattern differs slightly for Mistral 7B, where the largest performance im- provement occurs with 70% of the original dataset and a smaller sampling threshold.\nHyperparameters\nThe performance of our approach depends on both the sam- pling threshold ($\\delta$) and preference optimization parameters.\nSampling Threshold ($\\delta$): No single sampling threshold $\\delta$ works optimally across all models. While higher thresholds identify clearer preference pairs, they also reduce the train- ing data size, which can limit performance gains. For in- stance, Mistral 7B achieves optimal performance at d = 0.5, with larger thresholds showing no slight improvement over the full dataset. We note that our discrete sampling thresh- olds ($\\delta \\in \\{0,0.5, 1, 2\\}$) may not exhaustively cover the op- timal values for each model, suggesting potential for fur- ther optimization. However, we typically notice the largest increase in benchmark performance when 50-70 % of the original dataset is retained using the reference-based sam- pling.\nPreference Optimization ($\\beta$): In contrast, we believe a fixed $\\beta$ = 0.01 performs well regardless of the sampling threshold. We notice small increases (+0.1) in MT-Bench for LLAMA-3.1-8B when we train it using large $\\beta$ = 0.1. However, we do not observe an increase in performance for LLAMA-3-8B. Instead, the performance remains the"}, {"title": "Aspect-wise increases", "content": "We also analyze task-specific performance improvements on MT-Bench. Technical tasks (coding, math & reasoning) show large gains compared to general knowledge and chat tasks (writing, roleplay, extraction, knowledge). LLAMA3- 8B demonstrates the most dramatic improvements, with MT- Bench scores for technical tasks increasing by nearly 1.0 points. While Mistral and LLAMA-3.1 show more modest gains, their improvements on technical tasks remain sub- stantial (+0.4). The improvements in general knowledge and chat tasks, while consistent across models, are less pro- nounced (+0.2 to +0.4). This disparity might be attributed to benchmark saturation - models trained on the full dataset already achieve high scores on non-technical tasks, leaving limited room for improvement. In contrast, technical tasks present more headroom for measurable gains, suggesting our sampling method is effective at identifying and lever- aging strong preference signals for technical tasks."}, {"title": "Limitations & Future Work", "content": "There are several promising directions to build upon our work. First, testing our approach on LLMs of varying sizes and architectures would help verify the conditions under which this property holds. Experiments with models trained on different SFT datasets could further establish the gen- erality of our findings. While MT-Bench serves as a stan- dard benchmark for measuring alignment and instruction- following capabilities, it has limitations in consistency, re- liability and biases (Zheng et al. 2023). Alternative bench- marks like Arena-hard (Li et al. 2024) could provide addi- tional validation, though computational costs currently re-"}, {"title": "Related Work", "content": "Preference Learning Methods. Preference alignment has emerged as a crucial step in developing LLMs for production environments. Reinforcement Learning from Human Feed- back (RLHF) (Ouyang et al. 2022; Christiano et al. 2017) pioneered this approach, using on-policy learning to align models with human preferences. Recent work has shifted to- ward off-policy methods, with Direct Preference Optimiza- tion (DPO) (Rafailov et al. 2024) gaining widespread adop- tion due to its simplicity and effectiveness. Several variants of DPO have been proposed, introducing additional regular- ization terms (Liu et al. 2023; Pal et al. 2024), new hyperpa- rameters (Meng, Xia, and Chen 2024), or modified training objectives (Ethayarajh et al. 2024).\nQuality-Focused Approaches. The challenge of obtain- ing high-quality preference pairs has been addressed through three main approaches. The first focuses on improving data collection, using either human experts or LLMs to curate training samples (Hu et al. 2024; Huang et al. 2023; Jiang et al. 2024). The second develops more robust training mechanisms through modified objectives (Wu et al. 2024; Chowdhury, Kini, and Natarajan 2024). These approaches have seen limited adoption compared to standard DPO. The third approach involves collecting large datasets and filter- ing out noisy samples (Morimura et al. 2024; Kim et al. 2024). While data collection improvements require manual oversight, and robust training methods have shown limited practical success, the filtering approach offers a promising direction - especially for collecting large scale data and then narrowing down the high quality samples, preventing re- iteration and expensive use of resources.\nData Quality in Alignment. Recent work has highlighted how preference data quality impacts alignment success (Ivi- son et al. 2024). Models demonstrate enhanced learning from clearly distinguished preference pairs, motivating re- search into methods for identifying and generating high- quality training data. For instance, FilterDPO (Morimura et al. 2024) proposes an on-policy approach that selects training samples by comparing policy-generated responses with preferred responses based on reward differences. While effective, such methods require additional computation or reward modeling. Our work presents a complementary ap- proach that identifies strong preference pairs using only ref- erence model probabilities, eliminating the need for addi- tional resources or inference steps."}]}