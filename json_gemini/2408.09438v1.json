{"title": "Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition", "authors": ["Qifei Li", "Yingming Gao", "Yuhua Wen", "Cong Wang", "Ya Li"], "abstract": "To address the limitation in multimodal emotion recognition (MER) performance arising from inter-modal information fusion, we propose a novel MER framework based on multitask learning where fusion occurs after alignment, called Foal-Net. The framework is designed to enhance the effectiveness of modality fusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL) and cross-modal emotion label matching (MEM). First, AVEL achieves alignment of emotional information in audio-video representations through contrastive learning. Then, a modal fusion network integrates the aligned features. Meanwhile, MEM assesses whether the emotions of the current sample pair are the same, providing assistance for modal information fusion and guiding the model to focus more on emotional information. The experimental results conducted on IEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and emotion alignment is necessary before modal fusion. The code is open-source\u00b9.", "sections": [{"title": "1. Introduction", "content": "Emotion recognition is an important part of human-computer interaction (HCI). In order to improve the interaction experience, it is necessary to fully utilize the information between different modalities to improve the recognition performance of the system [1]. Hence, efficient modal fusion methods is one of the current research hotspots.\nIn the MER task, there are many challenges that need to be investigated. This challenges include the need for improved feature representation, developing reasonable model structure to fit the feature representation, exploring effective modal fusion methods to realize inter-modal information complementary and improve the performance of MER, and addressing or alleviating the problem of model performance degradation caused by missing modal information.\nThe commonly used modalities for emotion recognition include audio, video, and text. In recent years, most of researchers have extracted deep representations of pre-trained models to represent modal information [2, 3, 4, 5], such as HuBERT [6], WavLM [7], BERT [8], ROBERT [9], Resnet-FER2013 [10] and MAE [11]. Compared with the conventional features, such as Mel Frequency Cepstrum Coefficient, one-hot representation and Face Action Unit, the deep representations have better performance and generalization. For modeling the modal information, Wang et al. [12] proposed a modality-sensitive MER framework to exploring complementary features. Mitra"}, {"title": "2. Proposed Method", "content": "In this section, we will provide a detailed introduction to the proposed AVEL, MEM auxiliary tasks, and the modal fusion module used."}, {"title": "2.1. Audio-Video Emotion Aligning", "content": "Inspired by ALBEF [17], in order to achieve better modal fusion for complementary emotional information between modalities in the later stage, we first align the emotional information between the audio and video modalities. In other words, the goal of AVEL is to enhance the similarity between sample pairs with the same emotional category through contrastive learning, while reducing the similarity between sample pairs with different emotional categories.\nThe input audio-video sample pairs are $\\{X_a^i, X_v^i\\}$, where $i \\in [0, N]$, $N$ is batch size and $a, v$ represent audio, video respectively. First, Foal-Net leverages WavLM and CLIP to extract audio embeddings $Z_a \\in \\mathbb{R}^{N \\times T \\times D_a}$ and video embeddings $Z_v \\in \\mathbb{R}^{N \\times F \\times D_v}$, where $T, F$ represent the number of frames in the audio and video, respectively, $D_a, D_v$ are the dimension of audio and video embeddings. Then, we conduct average pooling along with time dimension of $Z_a$ and $Z_v$ to obtain $Z_a \\in \\mathbb{R}^{N \\times D_a}$ and $Z_v \\in \\mathbb{R}^{N \\times D_v}$. The $Z_a$ and $Z_v$ are fed into projection blocks, which are designed to map their inputs to feature vectors of the same dimension, enabling the calculation of inter-modal similarity matrices. These operations are measured as:\n$E_a = MLP_a(Z_a); E_v = MLP_v(Z_v)$ (1)\n$C_{a2v} = \\epsilon \\times (E_a \\cdot E_v^T); C_{v2a} = \\epsilon \\times (E_v \\cdot E_a^T)$ (2)\nwhere the $MLP_a(\\cdot)$ and $MLP_v(\\cdot)$ are projection modules, which consist of two linear layers. $E_a \\in \\mathbb{R}^{N \\times D}$ and $E_v \\in \\mathbb{R}^{N \\times D}$ denote the scaled audio and video feature vectors, the $\\epsilon$ is temperature hyper-parameter. The $C_{a2v} \\in \\mathbb{R}^{N \\times N}$ and $C_{v2a} \\in \\mathbb{R}^{N \\times N}$ represent inter-modal similarity matrices.\nThe labels $\\check{C} \\in \\mathbb{R}^{N \\times N}$ for the contrastive loss, as shown in Figure 2, are set such that if the true labels are the same for different sample pairs within the same batch, the corresponding ground truth values are set to 1. Otherwise, these ground truth values are set to 0. Finally, the inter-modal emotion alignment"}, {"title": "2.2. Audio-Video Emotion Matching", "content": "The research by Sun et al. [22] indicates that during the training process, for some simple samples, the model can identify emotions using only the information from a single modality. This may result in insufficient fusion of modal information.\nHence, we propose the MEM auxiliary task to alleviate this issue. MEM is a binary classification task designed to enable the model to fully leverage information from both modalities to determine whether the emotional labels for the current input sample pair are consistent. We adopt the hard negative technique to generate the negative samples required for binary classification. Hard negative refers to identifying samples in other modalities that have inconsistent emotional information with the current modality sample but the highest similarity. First, we will set the value of $C_{a2v}$ and $C_{v2a}$ to negative infinity at the corresponding position where $\\check{C}$ has a value of 1. Then, the steps for finding the most similar negative sample from other modalities for the current sample are as follows:\n$id_{v2v}^i = \\mathop{\\mathrm{argmax}}(\\mathrm{softmax}(C_{i2v})); i \\in [0, N]$ (4)\n$id_{v2a}^i = \\mathop{\\mathrm{argmax}}(\\mathrm{softmax}(C_{i2a})); i \\in [0, N]$ (5)\n$Z_{neg}^a = Z_v[id_{v2a}^i]; Z_{neg}^v = Z_a[id_{a2v}^i]$ (6)\nwhere $id_{a2v}^i$ and $Z_{neg}^v$ respectively represent the index and CLIP embeddings of samples in the video modality. They are selected to serve as negative samples for the audio modality. Similarly, $id_{v2a}^i$ and $Z_{neg}^a$ represent the index and WavLM embeddings of samples from the audio modality, serving as negative samples for the video modality. Then, they are fed into fusion network for MEM. The operations are as follows:\n$M_a^p = \\delta(f_a(Z_a^i, Z_v^i)); M_a^n = \\delta(f_a(Z_a^i, Z_{neg}^v))$ (7)\n$M_v^p = \\delta(f_v(Z_v^i, Z_a^i)); M_v^n = \\delta(f_v(Z_v^i, Z_{neg}^a))$ (8)\n$\\mathcal{L}_m = \\frac{1}{k\\in{a,v}}CE(cat(M_k^p, M_k^n), M_e)$ (9)\nwhere $f_a(\\cdot)$ and $f_v(\\cdot)$ denote fusion networks for audio and video modalities respectively. The $\\delta$ means average pooling along with time dimension. The $CE$ represents Cross Entropy loss. $M_e$ and $M_k^i (k \\in \\{a, v\\})$ denote the output of paired positive and unpaired negative samples from the fusion network, respectively. $M_e \\in \\mathbb{R}^{2N \\times 1}$ represents the labels used for MEM, where the first half takes the value of 1, and the second half takes the value of 0.\nFinally, the loss function of Foal-Net is represented as Equation 10, where $\\mathcal{L}_{ce}$ is the emotion classification loss, and the value of $\\lambda$ is 0.01.\n$\\mathcal{L}_{Total} = \\mathcal{L}_{ce} + \\mathcal{L}_a + \\mathcal{L}_m$ (10)"}, {"title": "2.3. Modal Fusion Module", "content": "The fusion module we used is based on multi-head cross-attention [15]. Here, we use the representation of the speech modality as the query to illustrate the calculation method of the cross-attention mechanism. The calculation process of multi-head cross-attention is as follows:\n$F_{out} = LN(MH(Q^a, K^v, V^v) + F^a)$ (11)\n$MH(Q^a, K^v, V^v) = C(head_1, \\ldots, head_h)$ (12)\n$head_h = Attention(Q_h^a, K_h^v, V_h^v)$ (13)\n$Attention(Q, K, V) = softmax(\\frac{Q(K)^T}{\\sqrt{d_k}})(V)$ (14)\n$Q^a = F^a W_h^Q + b^Q$ (15)\n$K^v = F^v W_h^K + b^K$\n$V^v = F^v W_h^V + b^V$ (16)\nwhere $F^a$ and $F^v$ denotes the embeddings from pretrained audio and video models respectively. $LN$, $MH$ and $C$ mean Layer Normalization, Multi-Head and Concatenate. The $h$ represents the $h_{th}$ cross-attention. When calculating $F_{out}$, the method remains consistent with $F_{out}$, but the query is now based on the representation of the video modality. In this paper, we utilize two multi-head cross-attention layers for modal fusion, with each layer consisting of four heads. The $F_{out}$ and $F_{out}$ will undergo an average pooling layer along with time dimension, and then be concatenated together for MER."}, {"title": "3. Experiments and Results", "content": "Dataset The IEMOCAP [23] multimodal corpus is one of the most well-known databases for emotion recognition, encompassing data from three modalities: audio, video, and text. In total, it comprises 5 sessions, each including one male and one female speaker. To be consistent and compare with previous studies, we conduct experiments with 5,531 audio utterances of four emotion categories happy (happy & excited, 1,636), angry (1,103), sad (1,084) and neutral (1,708). We perform five-fold cross-validation using a leave-one-session-out strategy on the corpus to evaluate the effectiveness of our proposed method.\nThe weighted accuracy (WA) and unweighted accuracy (UA) are used as metrics in line with previous methods.\nExperimental Details The feature dimensions of the WavLM\u00b2 and CLIP\u00b3 image encoders are 1024 and 768, respectively. The Projection module has 512 neurons, with a dropout rate of 0.5. During training, the batch size is set to 64, the learning rate is a constant 1e-4, and the optimizer is AdamW. The dropout rate for cross-attention is 0.1. The input for the audio modality consists of 6 seconds of speech with a sampling rate"}, {"title": "3.2. Performance Comparison with Previous Methods", "content": "The effectiveness of the method we proposed can be underscored by contrasting it with the latest significant findings derived from the IEMOCAP corpus, as show in Table 1. It demonstrates that the best UA (80.10%) and WA (79.45%) are achieved by Foal-Net. Research on facial expression recognition in the IEMOCAP corpus is relatively scarce, primarily due to two reasons. Firstly, the video frames are relatively blurry, making it challenging to extract facial expression features. Secondly, the issue of speakers appearing in the same frame in the videos complicates the data processing. However, based on the experimental results, the performance of the audio-video combination is not inferior to that of the audio-text combination. This suggests that besides facial information, other details in the images also contribute to emotion recognition. Meanwhile, it highlights the outstanding performance of the Foal-Net."}, {"title": "3.3. Experiments and Analysis", "content": "We conducted a series of ablation experiments to validate the effectiveness of the features we used, the proposed auxiliary tasks, and the final model. The baseline is the Foal-Net without two auxiliary tasks. To validate the effectiveness of global image encoding for emotion recognition compared to local facial information encoding, we extracted deep representations from three pretrained models (EmoNet [32], Resnet-FER2013 [10], SENet-FER2013 [10]) for facial expression recognition and utilized them for emotion recognition. As shown in Table 2, their performance is significantly lower than that of the CLIP model's features. This indicates that, in addition to facial expressions, body movements and other information in the images also contribute to emotion recognition. Furthermore, another reason for the superior performance of CLIP features is the inclusion of semantic information.\nIn addition, we observe that after incorporating the AVEL task, UA and WA improved by 2.2% and 1.36%, respectively, compared to the baseline. This indicates the crucial necessity of cross-modal emotion alignment before modal fusion. On the contrary, the performance improvement is not very significant when introducing the MEM task alone. This is because the use of hard negative for finding negative samples heavily relies on the similarity matrix from the AVEL task. When the similarity matrix is not optimized, it cannot bring positive benefits to the MEM task. This also indirectly confirms the necessity of alignment before fusion. When we introduce both the AVEL and MEM tasks simultaneously, the model's performance is further improved compared to using AVEL or MEM alone. This demonstrates that with the assistance of AVEL, MEM can facilitate the fusion of modal information and enhance the model's capability for emotion recognition."}, {"title": "4. Conclusions", "content": "In this paper, we propose a novel framework called Foal-Net for MER, which includes two auxiliary tasks: AVEL and MEM. Our research on the AVEL task has demonstrated the effectiveness and necessity of initially aligning emotional information across modalities before integrating inter-modality information. The MEM task can guide modality fusion and make fusion module focus more on emotional information during the fusion process with the assistance of the AVEL task. Under the influence of both tasks, Foal-Net achieves SOTA performance. Moreover, we show that the embeddings of CLIP outperform facial expression features in MER. In the future work, we will optimize the MEM task as its performance relies on AVEL and validate the universality of our method by substituting the video modality with the text modality."}]}