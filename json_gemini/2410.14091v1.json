{"title": "Towards Effective Planning Strategies for Dynamic\nOpinion Networks", "authors": ["Bharath Muppasani", "Protik Nag", "Vignesh Narayanan", "Biplav Srivastava", "Michael N. Huhns"], "abstract": "In this study, we investigate the under-explored intervention planning aimed at\ndisseminating accurate information within dynamic opinion networks by leveraging\nlearning strategies. Intervention planning involves identifying key nodes (search)\nand exerting control (e.g., disseminating accurate/official information through the\nnodes) to mitigate the influence of misinformation. However, as network size\nincreases, the problem becomes computationally intractable. To address this,\nwe first introduce a novel ranking algorithm (search) to identify key nodes for\ndisseminating accurate information, which facilitates the training of neural network\n(NN) classifiers for scalable and generalized solutions. Second, we address the\ncomplexity of label generation (through search) by developing a Reinforcement\nLearning (RL)-based dynamic planning framework. We investigate NN-based RL\\uplanners tailored for dynamic opinion networks governed by two propagation\nmodels for the framework. Each model incorporates both binary and continuous\nopinion and trust representations. Our experimental results demonstrate that\nour ranking algorithm-based classifiers provide plans that enhance infection rate\ncontrol, especially with increased action budgets. Moreover, reward strategies\nfocusing on key metrics, such as the number of susceptible nodes and infection\nrates, outperform those prioritizing faster blocking strategies. Additionally, our\nfindings reveal that Graph Convolutional Networks (GCNs)-based planners\nfacilitate scalable centralized plans that achieve lower infection rates (higher\ncontrol) across various network scenarios (e.g., Watts-Strogatz topology, varying\naction budgets, varying initial infected nodes, and varying degree of infected\nnodes).", "sections": [{"title": "Introduction", "content": "The spread of information across social networks profoundly impacts public opinion, collective\nbehaviors, and societal outcomes [1]. Especially during crises such as disease outbreaks or disasters,\nthere is often too much information coming from different sources. Sometimes, the resultant flood of\ninformation is unreliable or misleading, or spreads too quickly, which can have serious effects on\nsociety and health [6]. Online platforms such as Facebook, Twitter, and WeChat, while essential for\ncommunication, significantly contribute to the swift spread of misinformation. This has led to public\nconfusion and panic in events ranging from the Fukushima disaster to the COVID-19 pandemic,\ndemonstrating the need for effective information management on these platforms. Furthermore, the\nintentional dissemination of fabricated news, especially noted during significant political events such\nas U.S. presidential elections, underscores the influence of misinformation on democratic processes\nand highlights the urgent requirement for interventions to mitigate these impacts [2, 9]."}, {"title": "Problem Formulation", "content": "In this section, we discuss our approach to modeling the social network environment, the dynamics\nof information propagation, and our strategy for containing misinformation."}, {"title": "Environment Description", "content": "A social network is formally represented as a directed graph G = (V, E), where V denotes the set of\nnodes (agents), and E denotes the set of edges (relationships or connections) between agents [12].\nThe graph structure we consider for our study is undirected, indicating that relationships between\nagents are bi-directional. Each node within the graph represents an individual agent, and each agent\nholds a specific opinion on a given topic. An edge between any two nodes signifies a direct connection\nor relationship between those agents, facilitating the exchange of opinions.\nOpinion values are quantified within the range [-1, 1], representing different levels of sentiment,\nand the weight assigned to each edge quantifies the mutual trust level between connected agents,\nscaled within the interval [0, 1]. In our simulations, we explore three distinct cases of opinion and\ntrust values. While existing literature works have explored only binary opinion and trust models,\ncomputational social science often models the opinion and trust values as continuous variables.\nInvestigation of planning strategies in continuous models remains under-explored. Case-1 involves\nbinary opinion values with binary trust, simplifying the network dynamics into discrete states. In\nCase-2, we use floating-point opinion values while maintaining binary trust, allowing for a more\ngranular assessment of opinions while still simplifying trust dynamics. Finally, Case-3 features both\nfloating-point opinion values and floating-point trust, representing more realistic opinion and trust\nrelationships within the network capturing continuous variations."}, {"title": "Propagation Model", "content": "In the analysis of opinion networks, it is essential to understand how opinions form and evolve,\nguided by the dynamics of trust among agents. In our analysis, the evolution and propagation of\nopinions within opinion networks are modeled using a linear adjustment mechanism (discrete linear\nmaps), as described by the following transition function\n$x_i(t + 1) = x_i(t) + \\mu_{ik}(X_k(t) - x_i(t)), t = 0, 1, . . . .$\nEquation 1 models the dynamics of opinion evolution, where the opinion of agent i at time t + 1\ndepends on its current opinion and the influence exerted by a connected agent k, who is actively\nsharing some information with agent i, moderated by the trust factor pik. This model adapts\ndifferently across various experimental setups as detailed below.\nIn Case-1 and Case-2, where mutual trust values are discrete {0, 1}, the application of Equation\n1 results in immediate shifts in opinion. For example, if an agent i with a current opinion value of\n0.5 on some topic is influenced by a connected neighboring agent k with an opinion value of -1 on\nthe same topic, agent i's opinion immediately shifts to -1 in the next timestep, reflecting a discrete\ntransition. Conversely, in Case-3, which involves a continuous range of opinion and trust values,\nchanges are more gradual. Here, if agent i holds an opinion of 0.5 and is influenced by a neighbor k\nwith an opinion of -1 and a moderate trust factor, the opinion of agent i incrementally moves closer\nto -1 in subsequent timesteps. This reflects a gradual shift towards a consensus opinion, depending\non the magnitude of the trust level between agents i and k.\nTo further enhance our understanding of opinion dynamics in networks with continuous trust relation-\nships, we have also used the DeGroot propagation model [4] in Case-3. The propagation of opinions\nin this model is governed by the following equation:\n$X_i(t + 1) = \\sum_{k=1}^{n} \\mu_{ik}x_k(t), t = 0,1,....$\nEquation 2 describes the opinion of agent i at time t + 1 as a weighted average of the opinions of\nall the neighboring agents at time t, where the weights pik represent the trust agent i has in agent k.\nOften, in the DeGroot model, the summation in 2 is a convex sum, i.e., the trust values add to one so\nnthat we have $\u2211_{k=1}^{n} \\mu_{ik} = 1$ for each i = 1, . . ., n. This normalization allows the DeGroot model to\nexhibit stable asymptotic behaviour.\nAt each timestep, the following processes occur: Nodes with opinion values lower than -0.95\nare identified as sources of misinformation and transmit the misinformation to their immediate\nneighbors (referred to as 'candidate nodes') according to one of the propagation model detailed\nin Equations 1 and 2. Concurrently, an intervention strategy is applied where a subset of these\nneighbors-constrained by an action budget-is selected to receive credible information from a\ntrusted source. This source is characterized by an opinion value of 1 and we vary trust parameter\namong 1, 0.8, and 0.75. The process includes a blocking mechanism where a node that exceeds a\npositive opinion threshold of 0.95 is considered 'blocked', ceasing to interact with the misinformation\nspread or disseminate positive influence further. Our primary objective is to devise a\nlearning mechanism that efficiently identifies and selects key nodes within the network to disseminate\naccurate information at each time step."}, {"title": "Methods", "content": "In this section, we will explain our methodologies, presenting an overview of the network architectures\nemployed, including the GCN and ResNet frameworks. Additional details about the neural network\narchitecture utilized for our experiments can be found in Appendix A.2. We detail our proposed\nranking algorithm utilized in the SL process. Additionally, we elaborate on the implementation of\nthe Deep Q-Network with experience replay for RL. Furthermore, we provide an explanation of the\nvarious reward functions designed for our RL setup."}, {"title": "Ranking Algorithm based Supervised Learning", "content": "In this section, we propose a ranking algorithm based SL model to classify the key nodes at each\ntime step to disseminate accurate information. Our SL method utilizes a GCN architecture.\nRanking Algorithm: We pose the ranking algorithm as a search problem where the objective is to\nfind the optimal set of nodes that when blocked minimizes the overall infection rate. The network is\nrepresented as a graph G, where nodes can be infected, blocked, or possess opinion values within\nthe range [-0.95, 0.95]. Initially, a simulation network S is created by setting the opinion values of\ninfected nodes to -1 and removing blocked nodes. Let M denote the number of nodes in S that are\nneither infected nor blocked. Given an action budget K, we select K nodes from M in $\\binom{M}{K}$ possible\nways, forming the set C of all possible combinations. For each subset c \u2208 C, we temporarily block\nthe nodes in c by setting their opinion values to 1 and simulate the spread of misinformation within\nS. The resulting infection rates for each subset c are stored in the set R. We identify the subset\nc* \u2208 C that yields the minimal infection rate, denoted as r*. This subset c* is our target set. We then\nconstruct a target matrix T \u2208 $R^{N\u00d71}$, where N is the total number of nodes in the original network\nG. All entries of T are initialized to 0, and for each node i \u2208 c*, the i-th entry of T is set to 1. This\ntarget matrix T is subsequently used to train the GCN-based model. A pseudocode for this ranking\nalgorithm is presented in Algorithm 1 in Appendix A.5.\nOverall Training Procedure: The training of our GCN-based model leverages the labels defined in\nthe target matrix T \u2208 $R^{N\u00d71}$. This matrix is compared with the model's output matrix O\u2208$R^{N\u00d71}$,\nwhich estimates the blocking probability of each node. We evaluate training efficacy using the binary\ncross-entropy loss between T and O, which quantifies prediction errors. Model weight adjustments\nare implemented via standard backpropagation [20] based on this loss.\nEach training iteration consists of several episodes, starting with the generation of a random graph\nstate G containing initially infected nodes. The GCN then processes this state to output matrix O\nusing the graph's features and structure. Labels are generated, as detailed above using the ranking\nalgorithm, generating the target matrix T, and the binary cross-entropy loss between O and T is\ncalculated for backpropagation. The environment updates by blocking predicted nodes, allowing\ninfection spread, and adjusting node attributes. The process repeats until misinformation spread\nis halted, with each episode refining the graph's state for subsequent iterations. While the ranking\nalgorithm employs a brute force approach to identify optimal nodes, which becomes increasingly\ncomplex in the case of continuous opinion models, its integration within the SL framework to generate\nsynthetic labels for the network represents the novel aspect of our methodology."}, {"title": "Reinforcement Learning-based Centralized Dynamic Planners", "content": "In SL, the process of generating labels can be costly and impractical as network size increases. This is\nevident while considering mitigating misinformation propagation in large networks, where identifying\nthe optimal set of nodes for blocking requires a combinatorial search that is computationally infeasible.\nThus, RL emerges as a viable alternative.\nWe employ the Deep Q-Network (DQN) [26] framework using random exploration combined with\nexperience replay. Unlike the classical DQN, where the network outputs a Q function corresponding\nto each possible action, we have modified our approach to develop a Deep Value Network (DVN)\nas the number of available actions in each time-step in our problem setup need not be fixed. In this\nmodified version, the network outputs the value for a given input state. Consequently, the output\nlayer consists of a single neuron instead of one neuron per action. The agent's experiences at each\ntime step are stored in a replay memory for the neural network parameter updates. The loss function\nfor training is given by\n$L(s_t, s_{t+1}|0) = (r_t + V_0-(s_{t+1}) \u2013 V_0(s_t))^2,$\nwhere st represents the current state, st+1 denotes the subsequent state after action at is taken, and\nrt is the reward received for taking at in st. The specific reward functions used in this study are\ndiscussed later in the section."}, {"title": "Reward Functions for RL setup", "content": "The reward function is designed to encourage policies that effectively mitigate the spread of misinfor-\nmation. Specifically, the reward functions modeled for our study are: (1) R\u2081 = -(Ainfection rate),\nwhere Ainfection rate is defined as the change in infection rate resulting from taking action at. Specif-\nically, Ainfection rate = infection rate at st+1 infection rate at st, with st+1 being the state after\naction at is applied at state st. This reward structure encourages the model to reduce the rate at which\nmisinformation spreads by penalizing increases in the infection rate. (2) R\u2081 = -(# candidate nodes),\ntargets the immediate neighbors of infected nodes that are susceptible to becoming infected in\nthe next timestep, thereby promoting strategies that minimize the potential for misinformation\nto spread. (3) R2 = -(#candidate nodes) \u2013 (Ainfection rate), merges these concepts, balanc-\ning the need to control both the number of susceptible nodes and the overall infection rate. (4)\nR3 = $ \\frac{1}{\\text{Total time steps}}$, rewards quicker resolutions, providing higher rewards for strategies that\ncontain misinformation rapidly and evaluating the effectiveness only at the end of each episode.\nFurthermore, (5) R4 = -(infection rate), directly penalizes the current infection rate, thus favoring\nactions that achieve lower overall infection rates. In addition to these individual rewards, our setup\nalso includes a combined reward that incorporates elements of both R3 and R\u2081. Throughout the\nsimulation, the agent continually receives rewards based on the number of candidate nodes, fostering\nstrategies that limit the expansion of the infection network. As the simulation concludes, the agent\nreceives an episodic reward calculated as (6) R5 = -(#candidate nodes)$*\\frac{\\text{# time steps}}{\\text{Total time steps}}$, thereby\nreinforcing the importance of quick and efficient resolution of misinformation spread."}, {"title": "Network Architectures", "content": "In our experiments, we utilized a GCN to model node features within a network. Each node\nwas characterized by three key features: opinion value, connectivity degree, and proximity to a\nmisinformed node. These features were represented in a matrix F \u2208 $R^{N\u00d73}$, where N denotes the\ntotal number of nodes. The feature matrix is dynamic and evolves to reflect changes in the network.\nIt includes the opinion value, the connectivity degree, which identifies nodes potentially susceptible\nto misinformation while excluding those already blocked or misinformed, and the proximity to a\nmisinformed node, which is calculated as the shortest path to the nearest infected node, assigning a\ndistance of infinity to unreachable nodes.\nWe have also considered using Residual Network (ResNet) Architecture. The ResNet model im-\nplemented in our study is a variant of the conventional ResNet architecture. The core component\nof our ResNet model is the ResidualBlock, which allows for the training of deeper networks by\naddressing the vanishing gradient problem through skip connections. Each ResidualBlock consists\nof two sequences of convolutional layers (Conv2d), batch normalization (BatchNorm2d), and sig-\nmoid activations. Complete details about the model architectures used in our study are provided in\nAppendix A.2."}, {"title": "Experiments", "content": "In this section, we present the details about training data generation and configurations chosen for\nour SL and RL methodologies. We also explain the test data used for evaluating the trained models."}, {"title": "Training Setup", "content": "In the SL setup, we experimented with three distinct graph structures: Watts-Strogatz, nearest\nneighbors k = 3 and a rewiring probability p = 0.4, Erdos-Renyi, with branching factor of 4,\nand Tree graphs, with branching factors randomly selected from the range [1, 4]. Each graph type\nfacilitated training models to evaluate the influence of various structural dynamics on performance.\nWe used the GCN model for the SL method. Due to consistent performance of the trained models\non the different graph topologies, we chose the small-world topology to present all the subsequent\nanalysis and summarize the results in Appendix A.6.1. We also trained centralized RL planners\nusing both ResNet and GCN network architectures. Each trained configuration is represented as\nmodel-n-x-y, where model \u2208 {ResNet, GCN}, n \u2208 {10,25,50} represents the network length, x \u2208\n{1,2,3} represents the number of initial infected nodes and y \u2208 {1,2,3} represents the action budget."}, {"title": "Test Data Generation", "content": "The datasets used in related works [14] are just network structures, while we did not find any real-time\nopinion propagation data. To evaluate our intervention strategies, we generate two synthetic datasets\nusing the Watts-Strogatz model with the training dataset's configurations. This approach allows us\nto simulate complex networks and control the structure, connectivity, and initial infected nodes to\nassess our models effectively."}, {"title": "Results and Discussion", "content": "In this section, we evaluate the models, using the infection rate metric, trained using our ranking-\nbased SL and RL algorithms with various reward functions. We discuss the efficiency of these\nmodels using Dataset v2, particularly on a network of 50 nodes with a connectivity degree of 4,\nas it represents the most complex test dataset we generated. Similar evaluation results for other\ndatasets can be found in the Appendix A.6. The details of the hardware used for our experiments\nare provided in the Appendix A.4. The code and the datasets generated in our study are available in\nthe following GitHub Repository. Our empirical investigation yielded insightful results regarding\nthe performance of our trained models under various training conditions. With a comprehensive\nexperimental evaluations, we seek to verify the following research questions while meeting the\ndesired objective.\nObjective and Research Questions: 01: Identify the optimal combination of initially infected nodes\nand action budget parameters for training models to effectively control the spread of misinformation.\nRQ1: Among the reward functions considered, which is the most effective one? RQ2: For reward\nfunctions that focus on the time of blocking, does adding any other factor lead to better results? If\nyes, which factor? RQ3: Do reward functions that look at global graph information perform better\nthan those considering local, neighboring information? RQ4: Does GCN offer better scalability and\nperformance when compared with ResNet.\nO1: What is the best combination of initially infected nodes and action budget parameters for training\nthe models to control the misinformation spread\nTo examine this we focused our analysis on the Mean Squared Error (MSE) loss plots obtained\nduring the training phase. Figure 7 in Appendix A.6 illustrates the comparison of training loss across\nvarious network parameter settings for all considered reward types in Case-1, employing a ResNet\nmodel trained on a network of 50 nodes. The trend in loss convergence across episodes was found\nto be consistent for both the ResNet and GCN models across all cases examined. The analysis\nrevealed that reward functions exhibiting lower and more stable loss values correlate with improved\nmodel learning performance. Our findings highlight that increasing the number of initially infected\nnodes typically elevates the stabilization point of MSE loss, indicating a more challenging learning\nenvironment. Additionally, a higher action budget contributes to increased MSE variability, reflecting\nthe added complexity and generally poorer performance during training. Based on this analysis, we\nfind ResNet-n-1-1 and GCN-n-1-1, n \u2208 {10,25,50}, to be the best training configurations.\nRQ1: Among the reward functions considered, which is the most effective one? Answer: R4\nTo determine the best reward function for controlling the misinformation spread we compare the\naverage infection rates using different reward functions. Table 1 presents the average infection rate\nvalues across different cases considered for Dataset v2 with a degree of connectivity 4, featuring a\nnetwork of 50 nodes, detailing the average infection rates. It compares the performance of the ResNet\nmodel, trained on a network of 50 nodes, with the GCN model, trained on a network of 10 nodes,\nusing the reinforcement learning training algorithm across the different reward types, and the GCN\nmodel trained using SL on a network of 25 nodes. Results on the additional datasets are provided in\nAppendix A.6. It can be observed that reward function R4 consistently provides lower infection rate\nvalues across different cases."}, {"title": "Conclusions", "content": "This paper introduces scalable and innovative intervention strategies for containing the spread of\nmisinformation within dynamic opinion networks. Our significant contributions include analysis\nusing continuous opinion models, a novel ranking algorithm for identifying key nodes, and the\nutilization of GCNs to optimize intervention strategies. Additionally, we design and study various\nreward functions for reinforcement learning, enhancing our approach to misinformation mitigation.\nDespite significant progress, our work has limitations. In the field of computational social science,\noften more complex agent models are being investigated. While we have made significant efforts to\nextend the understanding of planning strategies, especially in continuous opinion networks, exploring\ncomplex agent traits such as stubbornness and the representation of directed trust, and implementing\ntopic-dependency in a multi-topic network along with distributed planners instead of centralized\\uplanners as in our work is a compelling future direction.\nBroader Societal Impact: This work provides methods that can be used to exert control on information\nspread. When used responsibly by authorized information providers, which the authors support, it\nwill help reduce prevalent infodemics in social media. But it may also be misused by an adversary\nto wean control from an authorized party (e.g., information owner) and counter efforts to tackle\nmisinformation. Overall, the authors believe more research efforts are needed to control opinion\nnetworks in pursuit of long-term societal benefits."}]}