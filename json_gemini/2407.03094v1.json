{"title": "Conformal Prediction for Causal Effects of Continuous Treatments", "authors": ["Maresa Schr\u00f6der", "Dennis Frauen", "Jonas Schweisthal", "Konstantin He\u00df", "Valentyn Melnychuk", "Stefan Feuerriegel"], "abstract": "Uncertainty quantification of causal effects is crucial for safety-critical applica- tions such as personalized medicine. A powerful approach for this is conformal prediction, which has several practical benefits due to model-agnostic finite-sample guarantees. Yet, existing methods for conformal prediction of causal effects are limited to binary/discrete treatments and make highly restrictive assumptions such as known propensity scores. In this work, we provide a novel conformal predic- tion method for potential outcomes of continuous treatments. We account for the additional uncertainty introduced through propensity estimation so that our conformal prediction intervals are valid even if the propensity score is unknown. Our contributions are three-fold: (1) We derive finite-sample prediction intervals for potential outcomes of continuous treatments. (2) We provide an algorithm for calculating the derived intervals. (3) We demonstrate the effectiveness of the conformal prediction intervals in experiments on synthetic and real-world datasets. To the best of our knowledge, we are the first to propose conformal prediction for continuous treatments when the propensity score is unknown and must be estimated from data.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) for estimating causal quantities such as causal effects and the potential outcomes of treatments is nowadays widely used in real-world applications such as personalized medicine [11]. However, existing methods from causal ML typically focus on point estimates [e.g., 31, 33], which means that the uncertainty in the predictions is neglected and hinders the use of causal ML in safety-critical applications [11, 25]. As the following example shows, uncertainty quantification (UQ) of causal quantities is crucial for reliable decision-making.\nMotivating example: Let us consider a doctor who seeks to determine the dosage of chemotherapy in cancer care. This requires estimating the tumor size in response to the dosage for a specific"}, {"title": "2 Related Work", "content": "UQ for causal effects: Existing methods for UQ of causal quantities are often based on Bayesian methods [e.g., 2, 19, 20, 21]. However, Bayesian methods require the specification of a prior distribution based on domain knowledge and are thus neither robust to model misspecification nor generalizable to model-agnostic machine learning models. A common ad hoc method for computing uncertainty intervals is Monte Carlo (MC) dropout [12]. However, MC dropout yields approximations of the posterior distribution, which are not faithful [27].\nConformal prediction: CP [28, 32, 43] has recently received large attention for finite-sample UQ. For a prediction model $ on dataset $(X_i, Y_i)_{i=1,...,n}$ and a new test sample $X_{n+1}$, CP aims to construct a prediction interval $C(X_{n+1})$ such that $P(Y_{n+1} \\in C(X_{n+1})) \\geq 1 - \\alpha$ for some significance level $\\alpha$.\nWe refer to [4] for an in-depth overview. Due to its strong finite-sample mathematical guarantees, CP is widely used for traditional, predictive ML with widespread applications such as in medical settings [48] or drug discovery [3, 9].\nSeveral extensions have been developed for CP. One literature stream focuses on CP with marginal coverage under distribution shifts between training and test data [e.g., 8, 10, 13, 14, 15, 16, 17, 29,"}, {"title": "3 Problem formulation", "content": "Notation: We denote random variables by cap- ital letters $X$ with realizations $x$. Let $P_X$ be the probability distribution over $X$. We omit the subscript whenever it is obvious from the context. For discrete $X$, we denote the proba- bility mass function by $P(x) = P(X = x)$ and the conditional probability mass functions by $P(y | x) = P(Y = y | X = x)$ for a discrete random variable $Y$. For continuous $X$, $p(x)$ is the probability density function w.r.t. the Lebesgue measure.\nSetting: Let the data $(X_i, A_i, Y_i)_{i=1,...,n}$ consisting of observed confounders $X \\in \\mathcal{X}$, a continuous treatment $A \\in \\mathcal{A}$, and an outcome $Y \\in \\mathcal{Y}$ be drawn exchangeably from the joint distribution $P$. Furthermore, let $\\pi(a | x)$ define the generalized propensity score for treatment $A = a$ given $X = x$.\nThroughout this work, we build upon the potential outcomes framework [37]. We denote the potential outcomes after a hard intervention $a^*$ by $Y(a^*)$ and after a soft intervention $A^*(x) \\sim \\pi(a | x) = P_{A^*|X=x}$ by $Y(A^*(x))$. We make three standard identifiability assumptions for causal effect estimation: positivity, consistency, and unconfoundedness [e.g., 1, 23]. Finally, we consider an arbitrary machine learning model $\\phi$ to predict the potential outcomes. Hence, we define the outcome prediction function as $\\phi : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, $\\phi(X, A) \\rightarrow Y$.\nOur objective: In this work, we aim to derive conformal prediction intervals $C(X_{n+1}, \\diamond)$ for the prediction of a potential outcome $Y_{n+1}(\\diamond)$ of a new data point under either hard, $\\diamond = a^*$, or soft intervention, $\\diamond = A^*(X_{n+1}) \\sim \\pi(a | X_{n+1})$. The derived intervals are called valid for any new exchangeable sample $X_{n+1}$ with non-exchangeable intervention $\\diamond$, i.e.,\n$P(Y_{n+1}(\\diamond) \\in C(X_{n+1},\\diamond)) \\geq 1 - \\alpha, \\ \\diamond \\in \\{a^*, A^*(X_{n+1})\\}.$   (1)\nfor some significance level $\\alpha \\in (0,1)$. Of note, our CP method can later be used with an arbitrary machine learning model $\\phi$ for predicting the potential outcomes.\nIn CP, the interval $C$ is constructed based on so-called non-conformity scores [43], which capture the performance of the prediction model $\\phi$. For example, a common choice for the non-conformity score is the residual of the fitted model $s(X, A, Y) = |Y - \\phi(X, A)|$, which we will use throughout our work. For ease of notation, we define $S_i := s(X_i, A_i, Y_i)$.\nWhy is CP for causal quantities non-trivial? There are two main reasons. First, coverage guarantees of CP intervals essentially rely on the exchangeability of the non-conformity scores. However, intervening on treatment $A$ shifts the propensity function and, therefore, induces a shift in the covariates $(X, A)$ ($\\rightarrow$ Challenge $a$). Formally, we have a propensity shift in which the intervention"}, {"title": "4 CP intervals for potential outcomes of continuous treatments", "content": "Recall that intervening on test data samples breaks the exchangeability assumption necessary for the validity, i.e., the guaranteed coverage of at least $(1 - \\alpha)$, of CP inter- vals. Therefore, we now construct CP intervals where we account for a (potentially unknown) covariate shift in the data induced by the intervention.\nScenarios: In our derivation, we distinguish two different scenarios(see Fig. 3):\n\\textbf{\u2460 Known propensity score} (see Section 4.1): If the propensity score in the observational data is known, it means that the treatment policy is known. Then, we aim to update the policy by increasing/decreasing the treatment by a value $\\Delta a$, i.e., $A^*(X) = A + \\Delta A$.\nExample: Imagine a doctor is about to prescribe a medication to a new patient. Instead of prescribing the same dosage as he would have prescribed to a similar patient in the past, the doctor is interested in the potential health outcome of the patient when increasing (or decreasing) the original dosage by amount $\\Delta A$.\n\u2461 Unknown propensity score (see Section 4.2): In observational data, the propensity score is typically unknown. Therefore, we are interested in the effect of hard interventions, i.e., $a^*$. Here, we face additional uncertainty from the propensity score estimation ($\\rightarrow$ Challenge $b$).\nExample: In our running example, a patient comes to a new doctor who has never prescribed the respective medication and thus will base the decision on observational data (electronic health records) from other physicians, yet the observational data was collected under a different, unknown treatment policy. Therefore, the prescribed intervention (and thus dosage) cannot be expressed in terms of the policy in the observational data.\nIn our derivations, we make use of the following two mathematical tools. First, we define the propensity shift. Formally, it is the shift between the observational and interventional distributions, $P$ and $P'$, in terms of the tilting of the propensity function by a non-negative function $f$. Hence, we have\n$\\pi'(a | x) = \\frac{f(a, x)}{E_P[f(A, X)]} \\pi(a | x).$   (2)\nfor some $f$ with $E_P[f(X, A)] > 0$ and $a \\in \\mathcal{A}, x \\in \\mathcal{X}$.\nSecond, our CP method will build upon ideas from so-called split conformal prediction [32, 43], yet with crucial differences. In our methods, the calibration step differs from the standard procedure in that we conditionally calibrate the non-conformity scores depending on the tilting function $f$ to achieve marginal coverage for the interventional \u2013 and thus shifted - data."}, {"title": "4.1 Scenario 1: Known propensity score", "content": "We first consider scenario \u2460 with known propensity scores. Here, existing CP intervals are not directly applicable due to the shift from old to new propensity ($\\rightarrow$ Challenge $a$). For our derivation, we need the following lemma building upon and generalizing the intuition presented above.\nLemma 1 ([16]). Let the (potentially unknown) covariate shift of interest be contained in the finite-dimensional function class $\\mathcal{F}$. Define the distribution-shift-calibrated $(1 - \\alpha)$-quantile of the non-conformity scores as\n$\\hat{\\theta}_S(X_{n+1}) := \\underset{g\\in \\mathcal{F}}{\\text{arg min}} \\frac{1}{n+1} \\left( \\sum_{i=1}^n l_\\alpha(g(X_i), S_i) + l_\\alpha(g(X_{n+1}), S) \\right)$   (5)\nfor an imputed guess $S$ of the $(n + 1)$-th non-conformity score $S_{n+1}$. The prediction interval\n$C(X_{n+1}) := \\{y \\ | \\ S_{n+1}(y) \\leq \\hat{\\theta}_{S_{n+1}(y)}(X_{n+1})\\}$   (6)\nfor the true $S_{n+1}$ given a realization of $Y_{n+1} = y$ satisfies the desired coverage guarantee under all distribution shifts $f \\in \\mathcal{F}$, i.e.,\n$P_f(Y_{n+1} \\in C(X_{n+1})) \\geq 1 - \\alpha.$   (7)\nBuilding upon Lemma 1, we derive our first main result in Theorem 1. We define the finite- dimensional function class of interest as $\\mathcal{F} := \\left\\{ \\theta \\frac{\\pi(a+\\Delta A | x)}{\\pi(a | x)} \\ | \\ \\theta \\in \\mathbb{R}^+\\right\\}$. It is easy to verify that all $f \\in \\mathcal{F}$ represent the desired propensity shift to $\\pi'(a | x) = \\frac{\\pi(a + \\Delta A | x)}{\\pi(a | x)}$ as defined in Eq. (2).\nTheorem 1 (Conformal prediction intervals for known baseline policy). Consider a new dat- apoint with $X_{n+1} = x_{n+1}, A_{n+1} = a_{n+1}$, and $A^*(X_{n+1}) = a^* = a_{n+1} + \\Delta A$. Let $\\eta = \\{\\eta_1, ..., \\eta_{n+1}\\} \\in \\mathbb{R}^{n+1}$ be the optimal solution to\n$\\underset{\\eta_i, i=1,...,n+1 \\geq 0}{\\text{max}} \\sum_{i=1}^{n+1} \\eta_i$ \\ \\ \\ s.t. \\ \\ \\ $S_i - \\eta_i \\frac{\\pi(a^* | X_i)}{\\pi(a_i | X_i)} \\geq 0, \\forall i = 1, ..., n+1$ \\ \\ and \\ \\ $-\\alpha \\leq \\eta_i \\leq 1 - \\alpha, \\forall i = 1, ..., n + 1,$   (8)\nfor an imputed unknown $S_{n+1} = S$. Furthermore, let $S^*$ be defined as the maximum $S$ s.t. $\\eta_{n+1} < 1 - \\alpha$. Then, the prediction interval\n$C(x_{n+1}, a^*) := \\{y \\ | \\ S_{n+1}(y) \\leq S^*\\}$   (9)\nsatisfies the desired coverage guarantee\n$P(Y(A^*(X_{n+1})) \\in C(X_{n+1}, A^*(X_{n+1}))) \\geq 1 - \\alpha.$   (10)\nProof. We provide a full proof in Supplement C.2. Here, we briefly outline the underlying idea of the proof. First, we show that the function class $\\mathcal{F}$ indeed satisfies Eq. (2) for the intervention $A^*(X) = A + \\Delta A$, and we then rewrite Eq. (5) as a convex optimization problem. Next, we exploit the strong duality property, we optimize over the corresponding dual problem to receive a dual prediction set with equal coverage probability. Finally, we derive $S^*$ from the dual prediction set to construct $C_{n+1}$ and prove the overall coverage guarantee."}, {"title": "4.2 Scenario 2: Unknown treatment policy", "content": "If the underlying treatment policy is unknown, the only possible intervention is a hard intervention $a^*$. As described above, measuring the induced propensity shift is non-trivial due to two reasons: (i) The propensity model needs to be estimated, which introduces additional uncertainty affecting the validity of the intervals ($\\rightarrow$ Challenge $b$). (ii) The density function corresponding to a hard intervention is given by the Dirac delta function\n$\\delta_{a^*}(a) := \\begin{cases}0 & \\text{for } a \\neq a^*, \\\\\\infty & \\text{for } a = a^*, \\end{cases}$   (11)\nwhich hinders a direct adaptation of Theorem 1 due to the inherent discontinuity of the improper function. Hence, to proceed, we make the following assumption on the propensity estimator.\nAssumption 1. The estimation error of the propensity function $\\hat{\\pi}(a | x)$ is bounded in the sense that, for all $i = 1,..., n + 1$, there exists $M > 0$ such that\n$c_a := \\frac{\\pi(a_i | X_i)}{\\hat{\\pi}(a_i | X_i)} \\in [\\frac{1}{M}, M].$   (12)\nUnder Assumption 1, the distribution shift induced by the intervention is then defined as\n$\\hat{\\pi}'(a | x) = \\frac{\\delta_{a^*}(a)}{\\hat{\\pi}(a | x)} = c_a \\frac{\\delta_{a^*}(a) \\hat{\\pi}(a | x)}{\\hat{\\pi}(a | x)} = \\frac{f(a, x)}{E_P[f(A, X)]} \\hat{\\pi}(a | x),$   (13)\nfor a suitable function $f$. We further formulate $\\delta_{a^*}(a)$ in terms of a Gaussian function as\n$\\delta_{a^*}(a) = \\underset{\\sigma \\rightarrow 0}{\\text{lim}} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\text{exp}(-\\frac{(a - a^*)^2}{2\\sigma^2}).$   (14)\nThis motivates the following lemma. Therein, we specify the class $\\mathcal{F}$ of tilting functions $f$ that represent the distribution shift induced by the hard intervention $a^*$.\nLemma 2. For $\\sigma > 0$, we define\n$f(a,x) := \\frac{c_a \\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma^2})}{\\sqrt{2\\pi} \\sigma \\hat{\\pi}(a | x)}$   (15)\nwith $E_P[f(A, X)] = 1$. Furthermore, we define the finite-dimensional function class $\\mathcal{F}$\n$\\mathcal{F} := \\left\\{  \\frac{c_a}{\\sqrt{2\\pi} \\sigma \\hat{\\pi}(a | x)} \\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma^2}) \\ | \\  0 < \\sigma, \\  \\frac{1}{M} \\leq c_a \\leq M \\right\\}.$   (16)\nThen, $f(a, x) \\in \\mathcal{F}$ for all $c_a \\in [\\frac{1}{M}, M]$ and $\\sigma \\rightarrow 0$. As a result, the distribution shift\n$\\hat{\\pi}'(a | x) = \\underset{\\sigma \\rightarrow 0}{\\text{lim}} \\frac{c_a \\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma^2})}{\\sqrt{2\\pi} \\sigma \\hat{\\pi}(a | x)} \\hat{\\pi}(a | x)$   (17)\ncan be represented in terms of Eq. (2) through functions $f \\in \\mathcal{F}$.\nFollowing the motivation in scenario 1, we thus aim to estimate the $(1 - \\alpha)$-quantile of the non- conformity scores under the distribution shift in Lemma 1. We can reformulate this problem as\n$\\underset{\\sigma > 0, \\frac{1}{M} \\leq c_a \\leq M}{\\text{min}} \\frac{1}{n+1} \\sum_{i=1}^{n+1} (1 - \\alpha)u_i + \\alpha v_i \\ \\ \\ s.t. \\ \\ \\ S_i - c_a \\frac{\\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma^2})}{\\sqrt{2\\pi} \\sigma \\hat{\\pi}(a_i | X_i)} - u_i + v_i = 0, \\forall i = 1..., n + 1,$   (Ps)\n$u_i, v_i \\geq 0, \\forall i = 1..., n + 1,$\nfor the imputed score $S_{n+1} = S$. As the score is unknown, computing the CP interval would require solving (Ps) for all $S \\in \\mathbb{R}$, yet which is computationally infeasible. As a remedy, we"}, {"title": "4.3 Algorithm", "content": "We now use our Theorem 3 to present an algorithm for computing CP intervals of potential outcomes from continuous treatment variables under unknown propensities in Alg. 2. We present a similar algorithm for scenario with known propensities in Supplement B.\nWe make the following comments: In our algorithm, an optimization solver is used to calculate $\\eta_{n+1}$ according to Theorem 3. The specific choice of the solver is left to the user. In our experiments in Section 5, we perform the optimization via interior point methods. Further, the overall goal of our algorithm is to find the optimal imputed non-conformity score $S^*$ such that the coverage guarantees hold. This search can be implemented through suitable iterative search algorithms."}, {"title": "5 Experiments", "content": "Algorithm 1: Algorithm for computing CP intervals of potential outcomes of continuous interventions under unknown propensities.\nInput: Observational data $(X_i, A_i, Y_i)_{i\\in \\{1,..., n\\}}$, new sample $X_{n+1}$ and intervention $a^*$, significance level $\\alpha$, prediction model $\\phi$, propensity estimator $\\hat{\\pi}$, assumed error bound $M$, error tolerance $\\epsilon$, optimization solver\nOutput: CP interval $C_{n+1}$ for a new test sample\nSup $\\leftarrow$ max$\\{\\text{max}_{i=1,...,n} S_i, 1\\}$; Slow $\\leftarrow$ min$\\{\\text{min}_{i=1,...,n} S_i, -1\\}$;\n/* Calculate $\\eta^{Sup}, \\eta^{Slow}$ */\n1 $\\eta^{Sup} \\leftarrow$ solver$(\\phi, \\hat{\\pi}, (X_i, A_i, Y_i)_{i\\in \\{1,...,n\\}}, X_{n+1}, a^*, \\alpha, M, \\text{Sup})$; /* Calculate $\\eta^{Slow} */$\n2 $\\eta^{Slow} \\leftarrow$ solver$(\\phi, \\hat{\\pi}, (X_i, A_i, Y_i)_{i\\in \\{1,...,n\\}}, X_{n+1}, a^*, \\alpha, M, \\text{Slow})$; /* Iterative search for S* */\nwhile $\\eta^{Sup}_{n+1} < 1 - \\alpha$ do\n3 $\\text{Sup} \\leftarrow 2 \\text{Sup};$ /* Calculate \\eta */$\n4 $\\eta^{Sup} \\leftarrow$ solver$(\\phi, \\hat{\\pi}, (X_i, A_i, Y_i)_{i\\in \\{1,...,n\\}}, X_{n+1}, a^*, \\alpha, M, \\text{Sup})$; /* Calculate \\eta */\nwhile $\\eta^{Slow}_{n+1} >= 1 - \\alpha$ do\n5 $\\text{Slow} \\leftarrow 0.5 \\text{Slow};$ /* Calculate \\eta */$\n6 $\\eta^{Slow} \\leftarrow$ solver$(\\phi, \\hat{\\pi}, (X_i, A_i, Y_i)_{i\\in \\{1,...,n\\}}, X_{n+1}, a^*, \\alpha, M, \\text{Slow})$; /* Calculate \\eta */$\n7 $\\text{S}^* \\leftarrow \\frac{\\text{Sup} + \\text{Slow}}{2};$\nwhile $\\text{Sup} - \\text{Slow} > \\epsilon$ do\n8 $\\eta^{S^*} \\leftarrow$ solver$(\\phi, \\hat{\\pi}, (X_i, A_i, Y_i)_{i\\in \\{1,...,n\\}}, X_{n+1}, a^*, \\alpha, M, \\text{S}^*);$ /* Calculate \\eta */$\nif $\\eta^{S^*}_{n+1} < 1 - \\alpha$ then\n9 $\\text{Slow} \\leftarrow \\frac{\\text{Sup} + \\text{Slow}}{2};$\nelse\n10 $\\text{Sup} \\leftarrow \\frac{\\text{Sup} + \\text{Slow}}{2};$\n$\\text{S}^* \\leftarrow \\frac{\\text{Sup} + \\text{Slow}}{2};$\n/* Compute C(Xn+1, a*) */\n11 return $C(X_{n+1}, a^*) = \\{y \\ | \\ S_{n+1}(y) < \\text{S}^* \\}$.\nBaselines: As we have discussed above, there are no baselines that directly compute finite-sample prediction intervals for potential outcomes of continuous treat- ments. The only comparable method is MC dropout [12]. Yet, we again emphasize that MC dropout is an ad hoc method with poor approximations of the pos- terior, which is known to give in- tervals that are not faithful [27].\nImplementation: All methods are implemented with as a multi-layer perception (MLP) and an MC dropout regulariza- tion of rate 0.1. Crucially, we use the identical MLP for both our CP method and MC dropout. Hence, all performance gains must be attributed to the cover- age guarantees of our conformal method. In the MC dropout base- line, the uncertainty intervals are computed via Monte Carlo sam- pling. In scenario 2 with un- known propensity scores, we per- form the conditional density estimation through conditional normalizing flows [41]. Details about our implementation and training are in Supplement E.\nPerformance metrics: We evaluate the methods in terms of whether the prediction intervals are faithful [e.g., 19]. That is, we compute whether the empirical coverage of the prediction intervals surpasses the threshold of $1 - \\alpha$ for different significance levels $\\alpha \\in \\{0.05, 0.1, 0.2\\}.\nSynthetic datasets: We follow common practice in causal ML and evaluate our methods using synthetic datasets [e.g., 1, 22]. The reason is the fundamental problem of causal inference, because of which counterfactual outcomes are never observable in real-world datasets. Synthetic datasets enable us to access counterfactual outcomes and, thus, to benchmark methods in terms of whether the computed intervals are faithful.\nWe consider two synthetic datasets with different propensity scores and outcome functions. Dataset 1 uses a step-wise propensity function and a concave outcome function. Dataset 2 is more complex and uses a Gaussian propensity function and oscillating outcome functions. Both datasets contain a single discrete confounder, a continuous treatment, and a continuous outcome. By choosing low-dimensional datasets, we later render it possible to plot the treatment-response curves so that one can inspect the prediction intervals visually. (We later also show that our method scales to high-dimensional settings as part of the real-world dataset.) Details about the data-generating processes are in Supplement E.\nReal-world dataset: We demonstrate the applicability of our CP method to datasets from medicine by leveraging the MIMIC-III dataset [30]. MIMIC-III contains de-identified health records from patients admitted to critical care units at large tertiary care hospitals. Our goal is to predict patient outcomes in terms of blood pressure when treated with a different duration of mechanical ventilation. We use 8 confounders from medical practice (e.g., respiratory rate, hematocrit). Overall, we consider 14,719 patients, which we randomly split into train (60%), validation (10%), calibration (20%), and test (10%). Further details are in Supplement E."}, {"title": "6 Discussion", "content": "Conclusion: We presented a novel conformal prediction method for potential outcomes of con- tinuous treatments with finite-sample guarantees. Our method is applicable to potential outcomes and extends naturally to treatment effects. A key strength of our method is that the intervals are valid under distribution shifts introduced by the treatment assignment, even if the propensity score is unknown and has to be estimated.\nLimitations: As with any other method, our UQ method has limitations that offer opportunities for future research. Our method relies on the quality of the propensity estimator. Although we incorporate estimation errors in the construction of our intervals, poorly estimated propensities could potentially lead to wide prediction intervals. We acknowledge that our prediction intervals are conservative for the hard intervention and for segments of the output space with limited calibration data, implying that a representative calibration dataset is essential for the performance of our method. As for all CP methods, the use of sample splitting may reduce data efficiency.\nBroader impact: Our method makes a significant step toward UQ for potential outcomes and, therefore, toward reliable decision-making. To this end, we provided strong theoretical and empirical evidence that our prediction intervals are valid. To this end, our method fills an important demand for using causal ML in medical practice [11] and other safety-critical applications with limited data."}, {"title": "A Additional theoretical results", "content": "A.1 Calculating prediction intervals for further causal quantities and differences\nWe presented a method for calculating conformal prediction intervals for potential outcomes of continuous treatments. In the following, we show how the intervals can be combined to yield valid prediction intervals for further causal quantities, such as the individual treatment effect (ITE) $Y_i$ of treatment $a$:\n$\\gamma_i(a) := Y_i(a) - Y_i(0).$   (21)\nLemma 4. Let $S_a^+$ and $S_0^+$ denote the optimal imputed non-conformity scores $S_{n+1}$ for treatment a and no treatment at significance level $1 - \\frac{\\alpha}{2}$ for $a \\in (0, 1)$, respectively. Furthermore, let\n$\\mathcal{C}^+ := \\phi(x_i, a) + S_a^* - \\phi(x_i, 0) + S_0^*,$\n(22)\n$\\mathcal{C}^- := \\phi(x_i, a) - S_a^* - \\phi(x_i, 0) - S_0^*.$   (23)\nThen the interval $C_\\gamma(X_i, a) := [\\mathcal{C}^-, \\mathcal{C}^+]$ contains the ITE $\\gamma_i$ with probability $1 - \\alpha$.\nProof. Let $\\epsilon_i(a)$ be the estimation error of the potential outcome, i.e.\n$\\epsilon_i(a) := Y_i(a) - \\phi(x_i, a).$   (24)\nWe can rewrite the coverage guarantee of the original conformal prediction intervals for the potential outcome $Y_i(a)$ as\n$P(Y_i(a) \\in C(X_i, a)) = P(|\\epsilon_i(a)| \\leq S_a^*) \\geq 1 - \\frac{\\alpha}{2}.$   (25)\nNow observe that\n$P(\\gamma_i(a) \\in C_\\gamma(x_i, a)) = P((Y_i(a) - Y_i(0)) \\in C_\\gamma(x_i, a))$   (26)\n$=P((Y_i(a) \\geq \\mathcal{C}^- + Y_i(0)) \\wedge (Y_i(a) \\leq \\mathcal{C}^+ + Y_i(0)))$   (27)\n$=P((\\epsilon_i(a) \\geq \\epsilon_i(0) - (S_a^* + S_0^*)) \\wedge (\\epsilon_i(a) \\leq \\epsilon_i(0) + (S_a^* + S_0^*)))$   (28)\n$=P(|\\epsilon_i(a) - \\epsilon_i(0)| \\leq S_a^* + S_0^*)$   (29)\n$\\geq P(|\\epsilon_i(a)| + |\\epsilon_i(0)| \\leq S_a^* + S_0^*).$   (30)\nThus, it follows directly that\n$P(\\gamma_i(a) \\in C_\\gamma(X_i, a)) \\geq 1 - \\alpha.$   (31)\nA.2 Alternative scenario 2: Fixing an approximation of $\\delta_{a^*}(a)$\nIn Section 4.2, we formulated the unknown propensity shift in terms of\n$\\delta_{a^*}(a) = \\underset{\\sigma \\rightarrow 0}{\\text{lim}} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma^2})$   (32)\nand minimized over $\\sigma$ and $c_a$ in Theorem 3 to construct the CP intervals. However, in certain applications, it might be beneficial to control the spread of the approximation of $\\delta_{a^*}(a)$ through fixing $\\sigma$ to a small value $\\sigma_0$ and performing the soft intervention $\\hat{\\pi}(a | x) = \\frac{c_a}{\\sqrt{2\\pi} \\sigma_0 \\hat{\\pi}(a | x)} \\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma_0^2})$ . In this case, the resulting optimization problem is a convex problem similar to Theorem 1. We present the alternative optimization problem below.\nTheorem 3 (Alternative for Theorem3: Conformal prediction intervals for unknown propen- sity scores). Let a new datapoint be given with $X_{n+1} = x_{n+1}$ and $A_{n+1} = a_{n+1}$. Let $\\eta = {\\eta_1, ..., \\eta_{n+1}\\} \\in \\mathbb{R}^{n+1}$ be the optimal solution to\n$\\underset{\\frac{1}{M} \\leq c_a \\leq M}{\\text{min}} \\sum_{i=1}^{n+1} \\eta_i$ \\ \\ \\ s.t. \\ \\ \\ $S_i - c_a \\frac{\\text{ exp}(-\\frac{(a - a^*)^2}{2\\sigma_0^2})}{\\sqrt{2\\pi} \\sigma_0 \\hat{\\pi}(a_i | X_i)} - \\eta_i < 0, \\ \\ \\ S_{n+1} + \\eta_{n+1} \\geq \\frac{c_a}{\\sqrt{2\\pi} \\sigma_0 \\hat{\\pi}(a_{n+1} | X_{n+1})},$   (33)\n$ -\\alpha \\leq \\eta_i \\leq 1 - \\alpha, \\ \\forall i = 1, ..., n + 1,$\nfor an imputed unknown $S_{n+1} = S$. Furthermore, let $S^*$ be defined as the maximum $S$ s.t. $\\eta_{n+1} < 1 - \\alpha$. Then, the prediction interval\n$C(x_{n+1}, a^*) := \\{y \\ | \\ S_{n+1}(y) \\leq S^*\\}$   (34)\nsatisfies the desired coverage guarantee\n$P(Y(a^* \\in C(X_{n+1}, a^*) \\geq 1 - \\alpha,$   (35)\nwhere with a slight abuse of notation $Y(a^*)$ denotes the potential outcome under the soft intervention $\\tilde{\\pi}$ above."}, {"title": "B Algorithm", "content": "In our main paper", "2": "Algorithm for computing CP intervals of potential outcomes of continuous inter- ventions under known propensities.\nInput: Observational data $(X_i, A_i, Y_i)_{i\\"}]}