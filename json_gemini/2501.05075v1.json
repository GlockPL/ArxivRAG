{"title": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model", "authors": ["Shuo Tong", "Han Liu", "Runyuan Guo", "Xueqiong Tian", "Wenqing Wang", "Ding Liu", "Youmin Zhang"], "abstract": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, existing DDSSs' limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot learning capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's potential for capturing temporal relationships within series of process data and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning (PEFT) through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream soft sensing tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLM's pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Finally, using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern industrial processes, accurate and stable measurement of key quality variables plays a critical role in real-time process monitoring and optimization, product quality control, and energy conservation. However, with the continuous expansion of industrial scale, processes are becoming increasingly complex and the number of process variables is growing. Some critical variables cannot be effectively monitored in real-time using physical hardware due to uncertainties in measurement environments, high maintenance costs of detection instruments, and high latency in offline measurements [1], [2]. To address this issue, soft sensing technology has emerged. This technology establishes predictive models based on easily measurable process variables (auxiliary variables) and mathematical relationships with difficult-to-measure quality variables (primary variables), providing rapid quality variable information for production processes. Due to the advantages of low cost and high performance, soft sensors is widely studied and applied in various industrial fields [3].\nCurrently, there are two main modeling approaches for soft sensing: mechanism-driven models and data-driven models [4], [5]. Due to the complexity of industrial processes, it is often challenging to obtain sufficient prior mechanistic knowledge for accurate mechanism-driven modeling [6]. In contrast, data-driven models do not require prior expert knowledge and can construct accurate prediction models solely based on extensive historical data. In recent years, fueled by the robust feature representation and nonlinear fitting capabilities of deep learning, data-driven soft sensor (DDSS) based on deep learning have emerged as the mainstream approach in soft sensing modeling, achieving state-of-the-art (SOTA) performance [7]. The popular paradigms in DDSS model include long short-term memory (LSTM) [2], autoencoder and its variants [1], [5], and convolutional neural network (CNN) [6].\nDespite the significant achievements of deep learning-based soft sensing methods to date, the current approaches still face three major challenges that have yet to be overcome: Limited universality. The soft sensing pipeline is a serial, multi-stage, multi-task modeling process [4], [8], [9], [10]. Apart from final soft sensor modeling stages, tasks like anomaly detection and missing value imputation are crucial for model quality [11], [12]. However, the fragmented nature of these multi-stage tasks requires data analysts to employ customized deep learning methods for each stage, involving separate modeling, parameter tuning, and optimization, which significantly increases the development cost and complexity of the models.\nThus, current soft sensing modeling typically relies on simple statistical techniques for data preprocessing or neglects the anomaly handling steps [3], [10]. As discussed in Section II-C, such inaccurate modeling approaches pose significant risks to the reliability and safety of soft sensors. Limited input modality. DDSS are restricted to using only structured data from industrial processes as input [1], [2], [3]. The restricted contextual information prevents the models from conducting a comprehensive analysis from multiple perspectives and levels, hindering the enhancement of robustness and representational capacity. Moreover, structured data lacks semantic-level descriptions and rich contextual or background information, resulting in data-driven models' inability to interpret high-level abstract concepts [3]. Poor few-shot learning capability. Due to the uncertainty and the high measurement costs associated with certain key variables, the number of samples available for engineering applications is often limited [13], [14]. This presents a challenge for traditional DDSS. Existing soft sensing approaches for few-shot tasks are typically categorized into three main types: gray-based methods [15], feature extraction-based methods [16], and the virtual sample generation-based methods [14]. However, all three approaches require complex feature engineering or model design specifically, which is time-consuming and requires domain expertise.\nTraditional DDSS struggle to effectively address these challenges. Recently, large-scale pre-trained language models (LLMs), owing to their vast number of parameters and extensive training data, have demonstrated exceptional performance across a wide range of downstream tasks in both natural language processing (NLP) and computer vision (CV) [17]. This success is largely attributed to the emergent capabilities (unexpected new abilities) of LLM, such as robust generalization, cross-modal knowledge transfer, multimodal capabilities, and knowledge representation [18], [19]. These features make the development of multimodal artificial general intelligence (AGI) a viable possibility. This paper aims to introduce LLM to explore whether the emergent capabilities can address the challenges faced by DDSS. The specific details are as follows:\nUniversality: Pre-trained LLMs are currently among the most popular used foundation models (also known as large models). Foundation models (FMs) are powerful general-purpose models trained on vast amounts of data to generate general-purpose representations [20]. These representations are subsequently fine-tuned to facilitate model training across various downstream tasks. Compared to traditional training paradigms that require the specialized design of algorithms for each task, this new universal paradigm reduces the cost and complexity associated with developing task-specific models. As a result, universal FMs have emerged across multiple domains such as NLP [21], CV [22], time-series forecasting [23], and medicine [24].\nAs illustrated in Fig. 1, we establish a universal soft sensing foundation model (SSFM) adapted for structured data through fine-tuning a pre-trained LLM. This highly compatible SSFM is coupled with various task-specific adapters (TSAs) in downstream tasks to provide robust computational, learning, and problem-solving capabilities at the foundational level, enabling rapid data processing initiation. This transformation facilitates the shift from artisanal soft sensing models to a factory-style foundational model paradigm, meeting diverse operational needs in industrial processes efficiently.\nCross-modal knowledge transfer and multimodal: The underlying architecture of FMs, transformer, can serialize modalities such as text, images, videos, and audio into a unified set of tokens for training, enabling multimodal universality and cross-modal knowledge transfer [25]. For example, Voice2Series [26] aligns time series data with acoustic model (AM) formats and leverages large-scale pre-trained AM for time series classification tasks. Bao et al. introduced a universal vision-language pretrained Model (VLMo) [27], fine-tuning visual models to train language experts. ST-LLM [28] utilizes LLM with a novel attention freeze strategy to achieve optimal performance in traffic prediction.\nThe emergence of such cross-modal capabilities lies in the essential ability of transformers to capture dependencies between different tokens across long sequences, as well as the weight-sharing mechanism [26], [19]. Therefore, the presence of contextual correlations between sequences is key to determining whether this modality can be successfully handled using LLMs. Given that process variable data exhibits strong temporal relationships, which align with this criterion, we have specifically designed an auxiliary variable series encoder (AVS Encoder) for industrial process auxiliary variables (AVs). AVS Encoder tokenizes and embeds AVs data to adapt it to the input requirements of LLM, leveraging LLM's capability in representation learning to rapidly adapt to process variable data. In addition, we aim to introduce new input modalities for DDSS through the cross-modal capabilities of the SSFM, enriching the input representations. To this end, we have designed two text-based knowledge-embedded soft sensors (TKESS), namely the prompt-driven soft sensor (LLM-PSS) and the prompt and data mixed embedding-driven soft sensor (LLM-PDSS). Introducing knowledge into soft sensors in the form of natural language has three advantages: (1) Enriching the input representations of soft sensor, the introduction of knowledge through prompt-driven learning further enhances the comprehensiveness and performance of the model, and provides new research directions in the field of soft sensing. (2) Compared to data-driven models, natural language is more in line with human habits and preferences, making LLM-PSS more user-friendly and easy to access for non-researcher users. (3) Compared to data-driven models, explanations in natural language are more in line with human cognitive habits and logical reasoning, enhancing the intuitiveness and information content of model interpretations.\nPowerful knowledge representation capability: LLMs are trained through self-supervised learning on vast amounts of data without external labels. This enables LLMs to extract richer and higher-level knowledge representations from the data [17]. Harnessing these learned knowledge representations, LLMs demonstrate powerful reasoning and pattern recognition abilities, primarily manifested in two aspects: (1) Knowledge transfer capability, where LLMs effectively transfer pre-trained knowledge to downstream tasks within the same modality or across modalities [26], [27], [28]. (2) Few-shot learning capability, where LLMs have been proven to excel even in scenarios with limited or zero labeled examples [29]. In this paper, we leverage these two knowledge representation capabilities of LLMs to further enhance the limited predictive accuracy of traditional DDSS and to tackle the challenge of insufficient in-domain data for small sample scenarios.\nIn summary, based on pre-trained LLM, we introduce for the first time a universal two-stage adaptation fine-tuning strategy termed LLM-TKESS (LLM for text-based knowledge-embedded soft sensing), for general soft sensing modeling tasks, aiming to unleash the potential of LLM in addressing key challenges in traditional soft sensing tasks. The main contributions of this paper can be summarized as follows:\n1) The AVS Encoder was proposed by us, designed with a novel tokenization and embedding method based on the horizontal temporal and vertical spatial characteristics of AVs. This aims to bridge the modality gap between natural language modalities and process variable data.\n2) We introduce a two-stage universal soft sensing modeling framework, LLM-TKESS. Without distorting the inherent representations of LLM, we first conduct autoregressive PEFT to train the SSFM aligned with AVs tokens. Subsequently, we design TSAs tailored for different downstream tasks and seamlessly integrate them with SSFM in a lightweight manner.\n3) In the downstream tasks, two different TKESS, LLM-PSS and LLM-PDSS, were introduced by us based on prompt learning, representing our initial attempt to incorporate natural language modalities into soft sensor.\n4) Through extensive experiments, we have demonstrated that the powerful representation learning capabilities of LLMs effectively enhance the prediction accuracy of soft sensor, achieving SOTA performance. Additionally, we validate the robust few-shot learning capabilities by leveraging the innate knowledge of LLMs without the need for additional complex operations."}, {"title": "II. PRELIMINARIES", "content": "LLMs exhibit various evolutions and types, but fundamentally, they employ the transformer architecture as their underlying framework. One of the core components of the transformer is the multi-head self-attention (MSA) mechanism. MSA simultaneously computes multiple self-attention mechanisms, capturing rich long-range dependencies within sequences across various subspaces. This endows LLMs with significant contextual comprehension, enabling them to adeptly manage diverse multimodal tasks. The specific pipeline of the MAS is shown in the gray area in Fig. 3(a). Suppose the t-th input sequence is denoted by $X_t \\in \\mathbb{R}^{n \\times d}$, where n represents the sequence length and d symbolizes the embedding dimension. Assumed a total of h attention heads, for every individual attention head j, the input transformation into $Q_j \\in \\mathbb{R}^{n \\times d_k}$ (query), $K_j \\in \\mathbb{R}^{n \\times d_k}$ (key), and $V_j \\in \\mathbb{R}^{n \\times d_k}$ (value) via the respective linear mapping matrices $W_i^q \\in \\mathbb{R}^{d \\times d_k}$, $W_i^k \\in \\mathbb{R}^{d \\times d_k}$ and $W_i^v \\in \\mathbb{R}^{d \\times d_k}$.\n$Q_j = X_tW_i^q, K_j = X_tW_i^k, V_i = X_tW_i^v$ (1)\nThe dot product of the query $Q_j$ and key $K_j$ is transformed into attention weights via the softmax function and subsequently, these weights are used in a weighted summation with the value $V_j$ to calculate the output $head_j$ of self-attention for each head.\n$head_j = Attention (Q_j, K_j, V_j) = Softmax(\\frac{Q_jK_j^T}{\\sqrt{d_k}})V_j$ (2)\nSubsequently, the outputs of self-attention from all heads are aggregated through a concatenation operation. The aggregated result then undergoes a linear transformation via matrix $W_o \\in \\mathbb{R}^{d \\times d}$, culminating in the final output of MSA:\nMulti-head (Q, K, V) = Concat (head_1, head_2,..., head_h) W_o (3)\nLLMs construct the network architecture through the stacking of multiple layers of transformers."}, {"title": "C. Data Preprocessing in Soft Sensing Tasks", "content": "Possessing comprehensive and accurate data forms a crucial precondition for the accurate modeling of DDSS [30]. However, due to system failures, sensor malfunctions, or other factors, the collected industrial data often contains a significant amount of anomalies and missing values, which severely impair the generalization ability of DDSS, greatly compromising their safety and hindering their deployment and application [13], [15], [16]. Therefore, it necessitates relevant preprocessing operations on the raw data, specifically for detecting anomalies and imputing missing values to enhance data quality and consistency.\nCurrently, to streamline the modeling process, most soft sensing methods conventionally opt for statistical models to execute rudimentary preprocessing of collected data. For instance, as depicted in Fig. 2, numerous methods remove outliers using box plots or the 3\u03c3 rule [3], [10]. Nevertheless, statistical techniques fail to account for temporal information and contextual relations within the data, merely capable of detecting outliers that fall outside the range of data distribution (green box). They are ineffectual at accurately detecting anomalies such as spikes anomaly from instantaneous electromagnetic interference from industrial sensors, noise anomaly ensuing from environmental noise, and scale anomaly due to calibration errors (red box). However, these three types of anomalies are frequently encountered in industrial data collection. In addition, while traditional imputation methods might fulfill single missing data by calculating the mean [8], statistical methods struggle to accurately align and impute data when confronted with localized missing sections (yellow box).\nBased on deep learning models, the detection of anomalies and data imputation in industrial data can achieve better performance [15], [16], [31], but this requires the design and separate training of different complex deep learning models specifically for data preprocessing and soft sensing in various industrial data scenarios, which significantly increases the workload and the difficulty of modeling.\nHence, establishing a robust and accurate general soft sensing framework applicable across stages is crucial for the precise and efficient deployment of soft sensors. Given LLM's robust versatility and generalization capabilities across multiple tasks, we utilize fine-tuned LLM as the SSFM. By incorporating minimal-parameter adapters for anomaly or missing value handling, we achieve precise treatment of various types of anomalies and gaps without the need for additional deep learning modeling."}, {"title": "D. Data Preparation for Time-Lags", "content": "Constructing inputs that carry sequential information is one of the conditions that catalyze the emergent capabilities of LLMs. In complex industrial processes, the variable data measured by sensors exhibit strong dynamic correlations over time [3]. Constructing multivariable input data with time lags allows LLM to consider not only the vertical correlations among AVs but also capture the horizontal semantic dependencies over time. This maximizes the utilization of the inherent semantic information within the data and significantly leverages the long-distance knowledge representation inherent in pre-trained LLM. For the time-lagged input data at time t, X(t) \u2208 $\\mathbb{R}^{n \\times m}$ can be represented as:\nX(t) = [x(t), x(t \u2212 1), x(t \u2212 2), \u2026\u2026\u2026, x (t \u2212 n + 1)]\n= $\\begin{bmatrix}\nx_1(t), & x_1(t - 1), & \\ldots, & x_1(t - n + 1) \\\\\nx_2(t), & x_2(t - 1), & \\ldots, & x_2(t - n + 1) \\\\\n\\vdots & \\vdots & \\ldots & \\vdots \\\\\nx_m(t), & x_m(t - 1), & \\ldots, & x_m(t - n + 1)\n\\end{bmatrix}$ (5)\nwhere n denotes the size of the historical time series window, and m represents the dimension of the AVs. Multivariate sequence samples X (t) are used as model inputs during both stages of LLM-TKESS."}, {"title": "III. METHODOLOGY", "content": "As depicted in Fig. 3, the proposed LLM-TKESS consists of two primary stages: the self-supervised fine-tuning alignment stage and the SSFM-TSA downstream task adaptation stage. In the first stage, the proposed AVS Encoder is used to converts the time series of normalized multivariate into token embeddings feature representations suitable for pre-trained LLM input. These token embeddings are then fed into the pre-trained LLM along with a learnable positional encoding. Through autoregressive training, the pre-trained LLM is fine-tuned to align industrial sensor data with the LLM's knowledge representation, resulting in the soft sensing foundation model (SSFM). In the second stage, SSFM serves as the base model for various downstream tasks. Depending on the specific soft sensing tasks at hand, a task-specific adapter (TSA) is selected and integrated with SSFM. While maintaining all SSFM parameters frozen, various downstream tasks are trained by fine-tuning only a small set of adapter parameters. Finally, capitalizing on the LLM's multimodal capabilities, two novel TKESS are introduced, expanding the possibilities within the soft sensing modeling domain."}, {"title": "B. Proposed AVS Encoder", "content": "The key to enabling LLMs achieve cross-modal knowledge transfer lies in constructing a downstream task token embedding that is compatible with the LLM. During the pretraining process, LLM's tokenizer divides natural language sequences into discrete tokens at the word, subword, or character level. However, in industrial modeling, the inputs of soft sensor time-lags auxiliary variable (AVs)\u2014and the outputs primary variable are time series. Traditional discrete tokenizers struggle to represent time series accurately and finely [32]. Based on this, considering the multivariate dependency characteristics of AVs, we propose a new auxiliary variable series encoder (AVS Encoder) to perform tokenizer and high-dimensional embedding on the multivariate series samples obtained in Section II-D, to better adapt to the knowledge representation of LLMs.\nFig. 4 illustrates the specific structure of the AVS Encoder. Firstly, to address the strong coupling and non-linear relationships among complex AVs, we design an auxiliary variable tokenizer suitable for times series data to achieve the tokenization of input samples. Specifically, dividing each individual time step as a unit, the input sample X (t) is divided into n tokens, denoted as $T_1, T_2 \\dots T_n$, to better capture the longitudinal non-linear relationships among AVs. Each token contains all auxiliary variable values at that moment. The p token $T_p \\in \\mathbb{R}^m$ (where p = 1...n ) can be represented as follows:\n$T_p = [x_1 (p), x_2 (p),...x_m (P)]$ (6)\nThis tokenization approach yields a number of tokens equivalent to the time window, with the tokens also exhibiting horizontal temporal relationships. This approach better captures the complex cross-channel information interaction and related patterns among AVs, thereby further enhancing the model's performance. Subsequently, through the operation of embedding fusion, all tokens are embedded into a continuous high-dimensional vector space to adapt to the inputs of LLMs. To more effectively capture the semantic information of input tokens, we employ two branches: one utilizing 1D convolution to extract local feature dependencies, and the other using linear probing to capture global deep features. Finally, the extracted features from these two branches are fused to obtain the token embeddings E \u2208 $\\mathbb{R}^{n \\times d}$, where d represents the embedding dimension."}, {"title": "C. SSFM Autoregressive Fine-Tuning Alignment Phase", "content": "During the pre-training phase of LLMs, self-supervision is typically employed to train these models on a substantial corpus of unlabeled textual data. However, soft sensing tasks in industrial settings involve non-language data, which limits the adaptability of LLMs to downstream industrial tasks. Therefore, it is crucial to bridge the gap between the upstream and downstream domains using a data-efficient approach. One of the most common self-supervised pre-training methods for LLMs is autoregressive language modeling, where the objective is to predict the next token in a sequence. Consequently, LLMs exhibit inherent autoregressive properties [33]. Research has demonstrated that employing an autoregressive training strategy similar to the one used during the pretraining phase can effectively enhance performance in downstream tasks [34], [35]. Based on this, we adopted a similar self-supervised autoregressive training approach for the channel-mix tokens obtained in Section III-B during the first phase of training the LLM-TKESS. This approach aims to leverage the intrinsic knowledge of LLM to better acquaint them with the characteristics of industrial data. To ensure that the data-independent representation learning capability of the LLM is not compromised during this process, we employed parameter efficient fine-tuning (PEFT) [33]. This technique involves freezing the majority of model parameters and selectively fine-tuning only a minimal subset of parameters, thereby aligning the model with downstream tasks and yielding the SSFM.\nThe diagram illustrating the autoregressive fine-tuning alignment stage of the SSFM is depicted in Fig. 3(a). This stage integrates the AVS Encoder, positional encoding, the pre-trained LLM, and an output linear layer. Initially, an input sequence X (t) \u2208 $\\mathbb{R}^{n \\times m}$, comprising n temporal steps and m AVs, is processed through the AVS Encoder to generate n token embeddings E \u2208 $\\mathbb{R}^{n \\times d}$. These embeddings are then summed with the positional encoding $E_p \\in \\mathbb{R}^{n \\times d}$ and input into the pre-trained LLM containing L + K layers of transformers. Within the pre-trained LLM, while ensuring that all other parameters remain frozen, we first fine-tune all layer normalizations across the layers using standard practice [36]. Additionally, for the first K layers, parameters of the multi-head attention mechanism are frozen to preserve the LLM's generalization capabilities. Conversely, for the subsequent L layers, we integrate low-rank adaptation (LoRA) [37] to each layer, injecting a trainable rank-factorization matrix with a smaller number of parameters into the query and key of the transformer layer of LLM. This adaptation helps to learn the context information between tokens more effectively without undermining the model's inherent expressive power [37].\nAs illustrated in Fig. 5(a), for the LLM's pre-trained weight matrices $W_q \\in \\mathbb{R}^{d \\times d_k}$ or $W_k \\in \\mathbb{R}^{d \\times d_k}$, LoRA introduces two low-rank matrices A \u2208 $\\mathbb{R}^{r \\times d_k}$ and B \u2208 $\\mathbb{R}^{d \\times r}$ to perform parallel computations with $W_q$ and $W_k$. Taking the weight matrix $W_q$ as an example, with $H_{in} \\in \\mathbb{R}^{n \\times d}$ representing the input, LoRA introduces learned weight correction terms $\\frac{\\alpha}{r}BA$W during the fine-tuning process to encapsulate task-specific knowledge. This results in the output as follows:\nH_{LORA} = H_{in}W_q + H_{in}\\frac{\\alpha}{r}BA = H_{in}W_q + \\frac{\\alpha}{r}H_{in}BA (7)\nWhere \u03b1 is the scaling factor, during the parameter initialization phase, A is initialized using a random gaussian distribution while B is initialized to a zero matrix. After passing through L + K layers, the input embeddings yield the LLM's output $H_{L+K} \\in \\mathbb{R}^{n \\times d}$. Subsequently, we utilize linear probing to map $H_{L+K}$ to an autoregressive output predicting the next token value. It is noteworthy that in contrast to employing log-likelihood as the loss function during the pre-training phase, we opt for mean squared error (MSE) as the loss function in the initial fine-tuning stage to ensure a continuous and precise representation of the predictive results. Assuming the output of the linear layer is represented by $SSFM (\u00b7)$ and the training dataset comprises z input samples, the loss function can be articulated as follows:\n$L_{SSFM} = \\frac{1}{nz} \\sum_{i=1}^n\\sum_{i=1}^z ||SSFM (x_1,...,x_{i-1}) - x_i ||^2$ (8)"}, {"title": "D. Adaptation Stage for Downstream Tasks in SSFM-TSA", "content": "In the second stage, we utilize SSFM as the universal base model for downstream task adaptation, fixing all its model parameters. By simply adding plug-and-play task-specific adapters (TSAs) [38] to SSFM, we can effectively model various tasks in soft sensing. Compared to using fine-tuning methods at this stage, where separate entirely new models are trained for each downstream task, this paradigm of base FMs + adapters training ensures high parameter sharing. It requires attaching very few trainable parameters specifically for each downstream task, significantly reducing modeling costs while enhancing modeling efficiency and flexibility. This approach is particularly suitable for industrial environments with high uncertainty, temporal variability, and complex process dynamics.\nThe training pipeline for SSFM-TSA is depicted in Fig. 3(b). Similar to the first stage, the input processing remains consistent, where after obtaining token embeddings and positional embeddings, the two are summed and fed into the SSFM-TSA. Within the SSFM-TSA model, we introduce two serial TSAs with a minimal number of trainable parameters into each transformer layer. Following standard practices [38], these adapters are positioned respectively after the multi-head attention and the feed-forward layers. This insertion strategy ensures that while the model retains its original characteristics, it can flexibly adapt to the requirements of specific tasks. Except for TSAs, all other parameters in SSFM are frozen. TSA consists of three components: down project layer, a non-linear activation function GELU (gaussian error linear unit), and up project layer, forming a small trainable module with a bottleneck structure (as shown in Fig. 5(b)). Taking the TSA post-multi-head attention as an example, the down project first maps the high-dimensional hidden state vector Hattn to a lower-dimensional space. This is followed by a transformation of the dimensionality reduction representation through GELU, and finally, the up project layer projects the low-dimensional representation back to the high-dimensional space of the original input. Furthermore, the TSAs employs a skip-connection to better preserve the original input information. This process can be represented by (9).\n$H_{TSA} = W_{up} (GELU (W_{down}H_{attn} + b_{down})) + b_{up} + H_{attn}$ (9)\nWhere HTSA denotes the output features of the TSA, with $W_{down}$ and $W_{up}$ representing the weight matrices of the dimensionality reduction and dimensionality increase layers, respectively. Meanwhile, $b_{down}$ and $b_{up}$ correspond to the bias vectors of these layers. GELU (\u00b7) refers to the GELU activation function. Throughout the training process, all parameters within the SSFM are frozen, and we adapt to downstream tasks solely through fine-tuning the TSA and the task-specific linear layer. Upon completing the training, the SSFM-TSA possesses the capability to tackle multiple downstream soft-sensing modeling tasks. These tasks encompass the preprocessing stage of soft sensor modeling, including missing value imputation, and anomaly detection, as well as DDSS tasks during the soft sensor phase and tasks based on text knowledge embedding. Each task is equipped with its own adapter. During the inference stage, handling different tasks is achieved by assembling SSFM with respective TSAs, thereby implementing a universal industrial soft sensing modeling approach."}, {"title": "E. Anomaly Detection", "content": "Industrial data anomaly detection is the first step in soft sensing modeling. By detecting and handling anomalies errors in the data can be removed or corrected, thereby enhancing the reliability of data analysis, modeling, and decision-making processes. However, anomaly labels in data collected from industrial environments are often inaccessible. Relying on domain experts for manual annotation of labels is not only labor-intensive but also inefficient and prone to errors. Therefore, we adopt a semi-supervised synthetic anomaly method to detect anomalies in sequences based on the similarity of sequence reconstruction. First, assuming sequence of the s-th auxiliary variable is represented as Vs, if there are a total of u + 1 samples, it can be expressed as follows:\n$V_s = [x_s (0), x_s(1), x_s(2), \\dots, x_s (u)]$ (10)\nWe treat all samples in V, as (pseudo-)positive labels. Additionally, we inject a specific type of anomaly (spikes anomaly, noise anomaly, scale anomaly) into random positions within Vs, treating these samples as (pseudo-)negative labels. The synthesis formula for spikes anomaly can be represented by follow:\n$A_{Spike} = m \\cdot s,  m \\sim Bernoulli(p), s \\sim N (0,\u03c3\u00b2)$ (11)\nWhere p denotes the probability of a bernoulli variable m. When m = 1, spikes noise is injected. In addition, we extract noise from N (0, \u03c3\u00b2) to serve as noise anomaly and generate scale anomaly by scaling sequence segments through a scaling factor. Next, we divided the dataset into training, validation, and test sets. During the training phase, our objective was to reconstruct the sequence $X_s (t)$ of the s-th auxiliary variable with minimal training epochs to obtain $X^{recon}_s (t)$, using MSE to optimize the training objective for anomaly detection adapter. The anomaly detection loss function LAT can be represented by (12) and (13):\n$X_s (t) = [x_s (t), x_s (t \u2212 1), \\dots, x_s (t \u2212 n + 1)]$ (12)\n$L_{AT} = MSE (X_s (t), X^{recon}_s (t))$ (13)\nFinally, we proceed to reconstruct the auxiliary variable series on the test set and obtain the reconstruction errors between $X_s (t)$ and $X^{recon}_s (t)$. When the reconstruction error of a certain sample exceeds a predefined threshold, the sample is identified as an anomalous instance. This can be followed by utilizing missing value imputation to replace the identified anomaly."}, {"title": "F. Missing Value Imputation", "content": "After annotating the anomalies, we integrate a missing value imputation adapter with the SSFM to train a downstream model for missing value imputation. Specifically, we first randomly select values from the input sequence X (t) to mask, thereby obtaining $X^{mask} (t)$. $X^{mask} (t)$ replaces X (t) as input into SSFM-TSA, and the reconstruction result $X^{recon} (t)$ is derived through the final linear probing. X (m) denotes the masked values in the input sequence, while $X^{recon} (m)$ represents the reconstructed values at the masked locations. Ultimately, by minimizing the MSE, we compel $X^{recon} (m)$ to increasingly converge with X (m), culminating in the training of the missing value imputation adapter. The loss function $L_{MDI}$ can be articulated as follow:\n$L_{MDI} = MSE (X (m), X^{mask} (m))$ (14)"}, {"title": "G. LLM for Data-Driven Soft Sensor (LMM-DSS)", "content": "DDSS analyze the relationship between input and output process data of controlled objects to establish a black-box model, which possesses strong nonlinear processing capabilities and online correction abilities.\nIn this paper, we explore the potential of leveraging pre-trained LLM for addressing the classical approach of DDSS. Initially, we utilize the AVs time series X (t) described in Section II-D as the input for SSFM-TSA. After passing through the last transformer layer of the SSFM, we obtain the hidden_state $H^{L+K} \\in \\mathbb{R}^{n \\times d}$. Subsequently, $H^{L+K}$ is input into a linear layer adapted for the soft sensor task. The output dimension of this linear layer is 1, aimed at predicting the current time t primary variable value $y^{predict}$, which can be represented by follow:\n$y^{predict} = H^{L+K}W + b$ (15)\nwhere W \u2208 $\\mathbb{R}^{d \\times 1}$ and b \u2208 $\\mathbb{R}^1$ represent the weight matrix and bias vector of the linear layer, respectively. We also employ the MSE as the loss metric to compel the predicted values $y^{predict}$ to approximate the true measured values $y^{true}$.\n$L_{DSS} =  \\frac{1}{nc} \\sum_{t=1}^c || Y_t^{prediction} - Y_t^{true} ||^2$ (16)"}, {"title": "H. The Proposed LLM-PSS", "content": "Prompt learning represents a straightforward yet effective methodology for activating downstream tasks of pre-trained LLMs [39]. Prompt Learning transfigures the original input into a textual string prompt via templates, fed into the LLM to elicit the final output. Based on this, we designed the LLM-PSS, using this intuitive approach to demonstrate the capability of pre-trained LLMs in text and prompt-based soft sensing modeling, and to preliminarily explore the feasibility of soft sensing modeling through text.\nThe pipeline of LLM-PSS is illustrated in Fig. 6(a). Initially, we transform the input sequence of AVs into text-based prompt inputs through templates. We selected the four most crucial aspects for constructing the text prompt template for the soft sensing dataset: background, time, AVs, and instruction. The background provides an introduction to the core principles of the industrial domain; Time annotates the current time information; The AVs section is populated with the names of all AVs, from V1_name to Vm_name; and the instruction offers a language description to guide the LLM in generating specific types of outputs. After obtaining the text prompt input, it is fed into the standard pre-trained tokenizer (HF) and embedder (HF) provided by HuggingFace (HF) to obtain the text embeddings. Subsequently, the text input is passed into the pre-trained LLM for fine-tuning, similar to the procedure in Section III-C. The current time's output ypredict is then obtained through a linear layer, with the loss function for this training process being the same as in Section III-G."}, {"title": "I. The Proposed LLM-PDSS", "content": "Through the"}]}