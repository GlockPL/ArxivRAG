{"title": "Comprehensive Online Training and Deployment for Spiking Neural Networks", "authors": ["Zecheng Hao", "Yifan Huang", "Zijie Xu", "Zhaofei Yu", "Tiejun Huang"], "abstract": "Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence (AI) due to their brain-inspired and energy-efficient properties. In the current supervised learning domain of SNNs, compared to vanilla Spatial-Temporal Back-propagation (STBP) training, online training can effectively overcome the risk of GPU memory explosion and has received widespread academic attention. However, the current proposed online training methods cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Efficient Multi-Precision Firing (EM-PF) model, which is a family of advanced spiking models based on floating-point spikes and binary synaptic weights. We point out that EM-PF model can effectively separate temporal gradients and achieve full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that EM-PF model can be flexibly combined with various techniques including random back-propagation, parallel computation and channel attention mechanism, to achieve state-of-the-art performance with extremely low computational overhead in the field of online learning.", "sections": [{"title": "1 Introduction", "content": "Spiking Neural Networks (SNNs), as the third-generation neural network towards brain-inspired intelligence [Maass, 1997], have gained widespread attention from researchers in Artificial Intelligence (AI) community. SNNs utilize spiking neurons as the basic computing unit to transmit discrete spike firing sequences to the postsynaptic layer. Due to the fact that spiking neurons only emit spikes when the membrane potential exceeds the firing threshold, compared to the activation values in traditional Artificial Neural Networks (ANNs), spike sequences have sparse and event-driven properties, which can demonstrate superior computational efficiency and power consumption ratio on neuromorphic hardware [Merolla et al., 2014, Davies et al., 2018, Pei et al., 2019].\nSpatial-Temporal Back-propagation (STBP) is the most significant training algorithm in the supervised learning domain of SNNs currently [Wu et al., 2018]. By introducing the concepts of temporal dimension and surrogate gradient, STBP simultaneously tackles the Markov property and non-differentiable problem of SNNs existed in the forward propagation and firing process. However, although STBP training has significantly improved the learning performance and universal property of SNNs [Wang et al., 2023, Qiu et al., 2024, Shi et al., 2024], as its back-propagation chains are inseparable due to the temporal dependencies, its GPU memory will inevitably boost linearly with the number of time-steps. This phenomenon greatly increases the training burden and hinders the"}, {"title": "2 Related Works", "content": "Recurrent learning algorithms for SNNs. Considering the similarity in computational mechanisms between SNNs and Recurrent Neural Networks (RNNs), Wu et al. [2018] and Neftci et al. [2019] transferred the Back-propagation Through Time (BPTT) method from RNNs to the supervised learning field of SNNs and utilized surrogate functions to tackle the non-differentiable problem existed in the spike firing process, which is called the STBP training algorithm. On this basis, Li et al. [2021], Guo et al. [2022b] and Wang et al. [2023] respectively attempted to start from the perspective of regulating the distribution about the backward gradient and membrane potential, introducing progressive surrogate functions and penalty terms. Deng et al. [2022] proposed a target learning function which comprehensively considers the SNN output distribution within each"}, {"title": "3 Preliminaries", "content": "Leaky Integrate and Fire (LIF) model. The current mainstream spiking model used in SNN community is LIF model, which involves three calculation processes, including charging, firing and resetting. As shown in Eq.(1), at each time-step, LIF model will receive the input current $I_{LIF}[t]$ and refer to the previous residual potential $v_{LIF}^{l-1}[t-1]$, then accumulate the corresponding membrane potential $m_{LIF}[t]$. When $m_{LIF}[t]$ has exceeded the firing threshold $\\theta^{l}$, a binary spike $S_{LIF}[t]$ will be transmitted to the post-synaptic layer and $m_{LIF}[t]$ will be reset. Here $W^{l, float}$ denotes the synaptic weight with floating-point precision and $\\lambda^{l}$ represents the membrane leakage parameter.\n$m_{LIF}[t] = \\lambda^{l} v_{LIF}^{l-1}[t-1] + I_{LIF}[t], v_{LIF}^{l-1}[t] = m_{LIF}[t] - S_{LIF}[t],$\n$I_{LIF}[t] = W^{l, float}S_{LIF}^{l-1}[t], S_{LIF}^{l}[t] =\\begin{cases}1, m_{LIF}[t] \\geq \\theta^{l}\\\\ 0, otherwise\\end{cases}$\nOmLIF [t]\n(1)\nSTBP Training. To effectively train LIF model, the back-propagation procedure of SNNs usually chooses to expand along both spatial and temporal dimensions, as shown in Fig.1(a). We use L to denote the target loss function. As shown in Eq.(2), $ame[t]$ depends on both $\\frac{\\partial m_{LIF} [t]}{\\partial S_{LIF}[t]}$ and $\\frac{\\partial m_{LIF} [t]}{\\partial m_{LIF}[t+1]}$ simultaneously, while the non-differentiable problem of $\\frac{\\partial S_{LIF}[t]}{\\partial m_{LIF}[t]}$ will be tackled through calculating approximate surrogate functions. Although STBP training enables SNN to achieve relatively superior performance, it inevitably causes severe memory overhead during the training process, which will"}, {"title": "4 Methods", "content": "4.1 Overcoming the back-propagation discrepancy of online training\nFrom Eq.(2), one can find that the backward gradient of STBP training can also be rewritten as\n$\\frac{\\partial L}{\\partial m_{LIF}[t]} = \\sum_{i=t}^{T} \\frac{\\partial L}{\\partial S_{LIF}[i]} \\prod_{j=t+1}^{i} \\frac{\\partial m_{LIF}[j]}{\\partial S_{LIF}[j]} =  \\prod_{j=t+1}^{T} \\frac{\\partial m_{LIF}[j]}{\\partial m_{LIF}[j-1]} \\cdot \\frac{\\partial L}{\\partial S_{LIF}[T]}$. On this basis, we point out the concept of Separable Backward Gradient:\nDefinition 4.1. When $\\frac{\\partial S_{LIF}[t]}{\\partial m_{LIF}[t]}$ if the surrogate function of $S_{LIF}[t]$ w.r.t. $m_{LIF}[t]$ is constant, we will have $\\epsilon^{l}[i,t]$, here $\\epsilon'[i,t] = = \\prod_{j=t+1}^{i} \\frac{\\partial m_{LIF}[j]}{\\partial m_{LIF}[j-1]}$ denotes the temporal gradient contribution weight of the i-th step w.r.t. the t-th step, which is a constant value. Therefore, we can further have $\\frac{\\partial m_{LIF}[t]}{\\partial Online} = \\frac{\\partial m_{LIF}[t]}{\\partial STBP}$, the gradient at this point is called Separable Backward Gradient.\nWhen the precondition of Definition 4.1 holds true, the back-propagation chain can be consid- ered separable in the temporal dimension and the backward gradient of online training can be seamlessly transformed from that of STBP training. Unfortunately, vanilla STBP training gener- ally requires surrogate gradient functions which are related to the membrane potential value (e.g. $\\frac{\\partial S_{LIF}[t]}{\\partial m_{LIF}[t]} = max (0 - m_{LIF}[t] - \\theta, 0)$), to provide richer information for binary spikes with limited representation capabilities. Therefore, current online training cannot fully overcome the discrepancy between forward and backward propagation, which also limits its learning precision.\nTo tackle this problem, we propose the EM-PF model, which is an advanced spiking model suitable for online training. As shown in Eq.(3) and Fig.1(c), compared to vanilla LIF model, EM-PF model"}, {"title": "4.2 EM-PF model with membrane potential batch-normalization", "content": "In EM-PF model, the distribution of $m^{l}[t]$ plays a crucial role: on the one hand, it affects the distribution of input current in the post-synaptic layer at the current and subsequent time-steps; on the"}, {"title": "4.3 Strengthening the performance of online learning and deployment", "content": "As illustrated in Fig.1(a)-(b) and Tab. 1, compared to STBP training, vanilla online training only saves GPU memory during the training phase, without providing any advantages in terms of computation time or memory overhead during the inference phase, which impedes effective online deployment for trained models. In comparison, we point out that online training based on EM-PF model can achieve full-stage computational optimization:"}, {"title": "4.4 Enhancing online training through channel attention mechanism", "content": "Channel Attention mechanism [Hu et al., 2018, Wang et al., 2020, Guo et al., 2022a] is usually inserted after the convolutional layers to optimize the network performance. We transfer the idea of ECA [Wang et al., 2020] to the online training based on EM-PF model, then propose Spiking Efficient Channel Attention (SECA) mechanism, as shown in Eq.(9).\n$SECA(I^{l}[t]) = Sigmoid(Conv1d (Sign(GAP(I^{l}[t])))) I^{l}[t], I^{l}[t] \\in R^{B \\times C \\times H \\times W}.$\nHere the input current $I^{l}[t] \\in R^{B \\times C \\times H \\times W}$ will be compressed to $R^{B \\times C \\times 1 \\times 1}$ through the Global Average Pooling (GAP) layer, then $Conv1d(\\cdot)$ and $Sigmoid(\\cdot)$ will be used to capture and activate the attention scores among different channels, ultimately merging with the shortcut path. Considering that the EM-PF model can convey enough information representation at each time-step, we enable the spike sequence to share the weight of SECA in the temporal dimension. Due to its extremely low parameter quantity (only 1 Conv1d layer with 1 \u00d7 1 \u00d7 K parameters), SECA can further enhance the learning ability of SNNs under the condition of hardly affecting its online deployment.\nIn addition, as shown in Eq.(10) and Fig.3(c)-(d), we further propose two variants for SECA:\n$SECA-I(I^{l}[t]) : SECA(I^{l}[t]), SECA-II(I^{l} [t]) : SECA(I^{l} [t]+BN2d(BConv2d(I^{l}[t]))).$"}, {"title": "5 Experiments", "content": "To validate the superiority of our proposed scheme compared to vanilla online learning framework, we investigate the learning performance of EM-PF model on various datasets with different data-scale and data-type, including CIFAR-10(100) [Krizhevsky et al., 2009], ImageNet-200(1k) [Deng et al., 2009] and DVS-CIFAR10 [Li et al., 2017]. We comprehensively consider previous methods based on STBP and online training as our comparative works for ResNet [He et al., 2016, Hu et al., 2024b] and VGG [Simonyan and Zisserman, 2014] backbones. Training and implementation details have been provided in Appendix."}, {"title": "5.1 Comparison with previous SoTA works", "content": "CIFAR-10 & CIFAR-100. As shown in Tab.2, compared to tradition STBP and online learning framework, our scheme is based on floating-point spikes and binary synaptic weights, which saves approximately 15\u00d7 parameter memory, enabling effective online deployment for SNN models. In addition, we achieve higher learning precision within the same or fewer time-steps. For example, our method outperforms GLIF [Yao et al., 2022] and SLTT [Meng et al., 2023] with accuracies of 3.49% and 5.53% respectively on CIFAR-100, ResNet-18.\nImageNet-200 & ImageNet-1k. For large-scale datasets, our EM-PF model has also demonstrated significant advantages. For instance, we respectively achieve accuracies of 60.68% and 68.07% on"}, {"title": "5.2 Validation study for accelerating computation", "content": "As shown in Fig.4, we investigate the effects of random back-propagation and parallel computation on accelerating computation during the training and inference phases, respectively. According to Fig.4(a), directly adopting random back-propagation can increase the training speed by about 80%, while further combining parallel computation can increase the speed to over 1.9\u00d7. In the inference phase, when we choose the residual block shown in Fig.3(b), which means that 50% of the neurons will use parallel computing mode, the inference speed can be improved by about 15%. In the extreme case (100% parallel computation), the inference speed can be further improved to about 1.3\u00d7."}, {"title": "5.3 Performance analysis for SECA", "content": "As shown in Tab.3, we explore the network performance before and after inserting SECA modules. One can note that SECA hardly introduces additional parameter memory and can provide extra precision improvement for binary synaptic layers. According to Fig.4, by combining random back-propagation and parallel computation, the online training speed based on SECA modules can even reach 1.3\u00d7 than that of vanilla online training. In addition, introducing parallel computation in the inference phase can also alleviate the problem of relatively slow inference speed in SNN models based on SECA to some extent."}, {"title": "6 Conclusions", "content": "In this paper, we systematically analyze the deficiencies of traditional online training, then propose a novel online learning framework based on floating-point spikes and binary synaptic weights, which effectively tackles the performance degradation problem caused by temporal dependent gradients and can achieve comprehensive model learning and deployment by flexibly combining various optimization techniques. Experimental results have verified that our proposed scheme can break through the limitations of previous methods and provide further inspiration for the future development of online learning."}, {"title": "A Appendix", "content": "A.1 Proof of Theorem 4.2\nTheorem 4.2 In the following two cases, the back-propagation of EM-PF model satisfies the condition of Separable Backward Gradient and \u2200i > t, el [i,t] = 0:\n(i) Il[1] \u2265 \u03b8l [1] \u2013 \u03bbl[1]v[0]; \u2200t \u2265 2, Il[t] \u2265 \u03b8l[t] \u2013 \u03bbl[t]0[t \u2212 \u2265 \u2013 1].\n(ii) \u221at \u2208 [1,T], Il[t] <0[t] - [t]v[t \u2013 1].\nProof. For case (i), EM-PF model will emit a spike at each time-step, which means that \u2200t \u2208 [1,T], = 1. Combining with the definition of el [i, t], we will have:\nds [t]\ndm [t]\ni\nds[i]\n\u03f5[i, t] =   = 1. \n\n\n(S1)\nFor case (ii), EM-PF model will keep silent at each time-step, which means that \u2200t \u2208 [1,T], = 0. Combining with Eq.(S1), we will obviously conclude that \u2200i > t, el [i, t] = 0.\nA.2 Experimental configuration\nFor experimental cases in Tabs.2-3, we choose Stochastic Gradient Descent [Bottou, 2012] as our optimizer and Cosine Annealing [Loshchilov and Hutter, 2017] as our scheduler. The initial learning rate and weight decay are set to 0.01 and 5 \u00d7 10-4, respectively. We consider various data augmentation techniques, including Auto-Augment [Cubuk et al., 2019], Cutout [DeVries and Taylor, 2017], and Mixup [Zhang et al., 2017]. For ResNet backbone, we generally choose vanilla parallel computation block (version I plus version III), as shown in Fig.3(b). For VGG structure, we utilize the version of learnable parameters for ImageNet-200 and the version of membrane potential batch-normalization for DVS-CIFAR10. For experimental cases based on SECA, we all choose the EM-PF model with membrane potential batch-normalization (version II). More detailed experimental configuration has been provided in Tab.S1.\nA.3 Overall algorithm pseudo-code"}]}