{"title": "Improving Multi-Label Contrastive Learning by Leveraging Label Distribution", "authors": ["Ning Chen", "Shen-Huan Lyu", "Tian-Shuang Wu", "Yanyan Wang", "Bin Tang"], "abstract": "In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label information. Previous studies selected positive and negative samples based on the overlap between labels and used them for label-wise loss balancing. However, these methods suffer from a complex selection process and fail to account for the varying importance of different labels. To address these problems, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, when selecting positive and negative samples, we only need to consider whether there is an intersection between labels. To model the relationships between labels, we introduce two methods to recover label distributions from logical labels, based on Radial Basis Function (RBF) and contrastive loss, respectively. We evaluate our method on nine widely used multi-label datasets, including image and vector datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics.", "sections": [{"title": "1 Introduction", "content": "Multi-label learning aims to construct a model that can effectively assign a set (multiple) of labels associated with an instance to that instance. For example, in the case of a scenic image composed of labels such as \u201cperson\u201d, \u201cwaves\u201d, \u201cbeach\u201d, and \"sky\", the model needs to learn these labels associated with the scenic image. Currently, multi-label learning has a wide range of applications, such as text classification, image annotation , gene function prediction, musical instrument classification in music, and video annotation , among others.\n The remarkable research in contrastive learning has injected a strong impetus into the study of multi-label learning. Indeed, multi-label learning and contrastive learning share a common goal: to extract more meaningful feature representations from data. Based on this, researchers in multi-label learning have turned their attention to contrastive learning. However, these studies focus on contrastive learning in self-supervised scenarios, while multi-label learning is typically conducted in supervised settings. Therefore, exploring supervised contrastive learning for multi-label learning has become an urgent and crucial research direction. A promising development is that the research by Khosla et al. has extended unsupervised contrastive learning to supervised contrastive learning, achieving impressive results on the ImageNet dataset. Additionally, they proposed two supervised contrastive loss functions, which provide significant guidance for subsequent research in multi-label learning.\n The core idea of contrastive learning is to learn feature representations by comparing positive and negative samples, with a key aspect being the definition and selection of these positive and negative samples. Building on the work of Khosla et al., Zhang and Wu further addressed the challenge of defining positive and negative samples in the context of multi-label learning. They proposed three solutions based on the degree of label overlap: 1) ALL: A sample qualifies as a positive sample if its labels completely overlap with those of the query sample; otherwise, it qualifies as a negative sample. 2) ANY: A sample is a positive sample if its labels include any of the labels of the query sample. 3) MulSupCon : Building on the ANY method, this method separately considers the labels of samples to balance the loss weights among different labels. Building on the work of Zhang and Zhou, Huang et al. further proposes five types of relationships between sample labels and anchor labels, dynamically reweighting the loss based on these relationships.\n However, these multi-label contrastive learning assume that all labels are of equal importance, which clearly contradicts the realities of the real world. For example, the Figure 1 shows that although some buildings exist, their importance clearly differs compared to other labels. To account for the varying importance of labels in contrastive learning, we have in-troduce label distribution. Unlike logical labels, which can only indicate whether a label belongs to a sample, label distribution further represents the degree of relevance or impor-tance of a label to an instance and we can find this in Figure 1. This distribution can more finely characterize the ambiguity and uncertainty between instances and labels. Moreover, research by demonstrates that label distribution contributes to better gen-eralization performance of models. However, it is a fact that the multi-label datasets we"}, {"title": "2 Related Work", "content": "The core idea of contrastive learning is to learn low-dimensional representations of data by contrasting positive and negative samples, so that similar samples are close to each other in the feature space, while dissimilar samples are far apart. , in their work SimCLR, emphasize that data augmentation is a key aspect of contrastive learning, a suggestion that we adopt in our work. He et al. propose MoCo, which tackles the problem of the number of negative samples being tied to and insufficient for the batch size and maintains the consistency of negative samples through momentum encoding, a solution"}, {"title": "3 The Proposed Method", "content": "To fully utilize label information in contrastive learning, we propose a novel model called MulSupConLD. Unlike previous multi-label contrastive learning, such as MulSupCon which directly use logical labels to balance the loss between labels, thereby overlooking the crucial point that the degree of importance among labels related to instances varies, we incorporate label distribution into contrastive learning. Label dis-tribution inherently reflects the varying degrees of importance among labels, rather than simply categorizing them as relevant or irrelevant, which implies that the model can learn richer information. Given that label distribution is often difficult to obtain and is absent in the datasets we use as well as in the data of comparative methods, we draw"}, {"title": "3.1 Multi-Label Supervised Contrastive Loss", "content": "MulSupCon gives two preliminary ideas to define positive samples: 1) ALL: Samples whose logical labels completely match those of sample $i$ qualify as positive samples. 2) ANY: Samples that contain any of the labels in the logical labels of sample $i$ qualify as positive samples. Clearly, the mathematical formula for the set of positive samples $P(i)$ of sample $i$ for ALL is:\n$P(i) = \\{p|p \\in A(i), y_p = y_i\\}$\nand for ANY is:\n$P(i) = \\{p|p \\in A(i), y_p \\cap y_i \\neq \\emptyset\\}$,\nwhere $A(i)$ denotes indices of all samples including $B$ and $Q$. Accordingly, MulSup-Con  balance the loss weights between labels based on the ALL and ANY strategies, and the loss function for each anchor $i$ is:\n$L_i =  -\\frac{1}{\\sum_{y \\in y_i} (i)} \\sum_{p \\in P(i)}  \\log \\frac{\\exp (z_i \\cdot z_p/\\tau)}{\\sum_{a \\in A(i)} \\exp(z_i \\cdot z_a/\\tau)}$,\nwhere $\\tau$ is a temperature parameter. So the loss function of one batch is:\n$L = \\frac{1}{|\\gamma|} \\sum_i L_i$"}, {"title": "3.2 MulSupConLD", "content": "Multi-Label contrastive learning, such as MulSupCon , logical labels are directly incorporated into the model's training process. One advantage of this approach is that logical labels are typically straightforward to obtain, as multi-label datasets often provide them. However, logical labels can only indicate whether a label is relevant to an instance or not, and they fail to convey the relative importance among labels. Consequently, using logical labels to balance class loss makes it challenging to effectively enhance the model's performance. Therefore, we consider introducing label distributions into contrastive learning and leveraging these distributions to balance the loss across different classes.\n However, label distributions are not available in the datasets we use nor in those of the comparative methods, we need to recover the corresponding label distributions from the logical labels. Inspired by DLDL , we propose two contrastive learning-based methods for recovering label distributions from logical labels: one uses RBF and the other directly utilizing the contrastive loss function. We evaluate the performance of both methods in the experimental Section 4. Since both of our methods build on contrastive learning, we first discuss the loss function of our contrastive learning approach and define positive samples. We use ALL to define positive samples and the loss of the query $i$ is:\n$L_{con,i} =  \\sum_{p \\in P(i)} \\log \\frac{\\exp (sim(z_i,z_p)/\\tau)}{\\sum_{a \\in A(i)} \\exp (sim(z_i \\cdot z_a)/\\tau)}$\nwhere $P(i)$ is the set of positive samples, $A(i)$ includes samples from the current batch $B$ and samples from the queue $Q$, $sim$ denotes cosine similarity and $\\tau$ is a temperature parameter . Unlike DLDL , which uses the $k$-nearest neighbors of sample $i$ in RBF as similar samples, after defining our contrastive loss function, we can directly utilize the positive sample set $P$ and employ the encoded vectors $z$ from the contrastive loss function within the RBF. Therefore, our loss function of RBF is:\n$C^{(1)}_{i,p} = \\exp(-\\|z_i - z_p\\|_2^2/2\\sigma^2)$,\nwhere $\\sigma$ is the hyper-parameter of the RBF kernel, and $p$ comes from the positive sample set $P(i)$ of sample $i$. We set $\\sigma$ to 0.01, following DLDL. Contrastive learning brings the features of samples similar to the anchor closer together in the feature space, while pushing apart those that are dissimilar. Consequently, the label distributions of similar samples are also expected to be similar. Then, we can establish constraints for both:\n$\\mathcal{G}^{(1)}_i = \\sum_{p \\in P(i)} C^{(1)}_{i,p} ||d_i - d_p||_2^2$.\nTo reduce the influence of the model assigning a certain degree of description to labels that are irrelevant to the instance, we have imposed constraints on the label distribution. The formula for this constraint is as follows:\n$\\mathcal{H}_i = ||Q_u^i - Q_u^{i,d}||_2$,"}, {"title": "4 Experiments", "content": "Initially, we will utilize MulSupConLD to pretrain models on various datasets. Subsequently, we will employ the fully connected layer, which generates label distributions from the pre-trained models, along with the Binary Cross Entropy (BCE) Loss to further train on the training set, thereby adapting the models for multi-label classification tasks."}, {"title": "4.1 Performance Comparison", "content": "In this section, we compare the performance differences between our two proposed methods, MulSupConRLD and MulSupConCLD, for implementing MulSupConLD and other multi-label classification methods on both image datasets and vector datasets. The evaluation results for the vector datasets partially come from C-GMVAE, except for MulSupCon, whose results come from our replication on the same datasets. We also evaluate MulSupCon on image datasets, and we can find these results from Table 2 to Table 4. From the experimental results, our methods demonstrate superior performance on both vector and image datasets.\n We believe the reasons for outperforming other comparative methods are twofold. On one hand, we use label distributions to balance the loss across different classes. On the other hand, we incorporate sufficiently effective supervisory information during the recovery of label distributions, enabling the model to assign description degrees to labels that are most"}, {"title": "4.2 Ablation Study", "content": "To validate the necessity of label distribution recovery modules into our method, we conduct ablation experiments on vector dataset Scene and image dataset PASCAL. Previous performance analysis shows that MulSupConCLD outperforms MulSupConRLD in overall performance. Therefore, we conduct ablation experiments on MulSupConCLD and break it down into the following components:\n1) MulSupConCLD: In this variant, we remove the constraints on the label distribution."}, {"title": "4.3 Parameter Sensitivity Analysis", "content": "In this section, we analyze the impact of the hyperparameters $\\alpha$ and $\\beta$ on the model's performance under the MulSupConCLD method.\n We choose MulSupConCLD because experimental performance analysis shows that it generally delivers better performance. We select both hyperparameters $\\alpha$ and $\\beta$ from the range {0.001, 0.01, 0.1, 1}. We fix hyperparameters $\\alpha$ and $\\beta$ separately and investigate their impact on the performance of MulSupConCLD on both the vector dataset Scene and"}, {"title": "5 Conclusion", "content": "We observe that current multi-label contrastive learning struggles to learn an effective model because using logical labels to balance classes losses overlooks the varying impor-tance among labels. To address this problem, we propose MulSupConLD, a framework that improves multi-label contrastive learning by incorporating label distributions. In our method MulSupConLD, we recover label distributions from logical labels and utilize them to balance classes losses. Through the combined effects of contrastive learning and label"}]}