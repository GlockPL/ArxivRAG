{"title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models", "authors": ["SAYEM MOHAMMAD IMTIAZ", "ASTHA SINGH", "FRAOL BATOLE", "HRIDESH RAJAN"], "abstract": "Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity, harmful responses, and factual inaccuracies. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, drawing inspiration from fault localization via program slicing, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. Furthermore, dynamic selection allows for a more nuanced and precise model repair compared to a fixed selection strategy. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.", "sections": [{"title": "1 Introduction", "content": "The recent advancement in large language model (LLM) capabilities marks a transformative moment in natural language processing (NLP). Owing to the effectiveness of transformer in scaling efficiently to the large corpus, LLMs now excel in tasks such as question-answering, text summarization,"}, {"title": "2 Background", "content": "LLMs have revolutionized NLP and SE tasks due to their ability to capture complex patterns and generate human-like text and code. In this section, we provide an overview of the GPT (Generative Pre-trained Transformer) architecture, which serves as a foundation for many modern LLMs [Zhao et al. 2023], including the models used in our study.\nThe GPT architecture, introduced by OpenAI [Radford et al. 2019], is based on the transformer model [Vaswani et al. 2017]. It consists of multiple layers of transformer blocks, which are the fundamental units of computation in the model. These blocks are also the primary focus of our slicing technique in IRepair. Each block contains two main components:\n\u2022 Multi-Head Attention: This mechanism allows the model to focus on different parts of the input sequence simultaneously. Multi-head attention splits the input into multiple \u2018heads,' each learning to attend to different input aspects. The attention function is defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere Q, K, and V are query, key, and value matrices, respectively, obtained from linear projections of the input sequence. dk is the dimension of the key vectors. The attention output is then passed through a feed-forward network (FFN).\n\u2022 Feed-Forward Neural Network (FFN): This component processes the output of the attention mechanism. It is a neural network that operates on each position in the input sequence independently. The FFN typically consists of two linear transformations with a non-linear activation function in between:\n$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$\nwhere W1, W2, b1, and b2 are learnable parameters.\nEach transformer block applies layer normalization and residual connections around these components."}, {"title": "3 Approach", "content": "Figure 1 presents a high-level overview of the approach used in IRepair, which aims to repair data-driven errors in large language models. The approach consists of two primary components: computing the relevant model slice and then repairing the identified slice selectively. In the first stage, we apply the concept of program slicing to language models to identify the slice that requires repair. In the second stage, we repair the identified slice selectively, focusing on the most error-prone sections of the model while minimizing any impact on its general performance. The following sections will explore the detailed steps involved in IRepair."}, {"title": "3.1 Problem Formulation", "content": "Let \u03c0\u03b8 : X \u2192 Y denote a pre-trained language model that maps input texts from X and a set of parameters, \u03b8, to corresponding output texts in Y. Consider a bad demonstration dataset, denoted as DE = (XE, YE), where the response YE to a prompt XE is undesirable. The goal is to ensure that the model \u03c0\u03b8 does not produce responses similar to YE when given prompts similar to XE. Additionally, consider a curated or refined dataset, denoted as DR = (XE, YR), which demonstrates the desirable response YR for those error-evoking prompts XE. In practice, these refined responses"}, {"title": "3.2 Slicing Intent", "content": "Drawing upon the concept of relevant program slicing [Weiser 1984], in this step, we aim to slice off the most error-prone sections of the model for a selective repair. Such a focused repair approach is critical for LLMs, as updating all parameters using limited repair data may lead to overfitting and knowledge degradation [Ding et al. 2022]. By selectively slicing the model based on bad data, our aim is to address the root cause of errors in the model while preserving the model's overall knowledge."}, {"title": "3.2.1 Challenges in slicing LLM", "content": "Existing slicing techniques for deep learning models are primarily designed for networks using ReLU activations [Zhang et al. 2020, 2022]. These techniques rely on activation status or their magnitudes to identify relevant parts of the model. However, transformer-based language models employ an attention mechanism with linear transformations, making these methods inapplicable. In contrast to ReLU activations, the magnitude of a linear transformation in attention doesn't directly correspond to its importance due to subsequent matrix multiplications.\nAs an illustration, consider the following simplified example of unmasked attention scores for a sequence with three tokens, T1, T2 and T3 and an embedding dimension, dk = 1:"}, {"title": "3.2.2 Our approach", "content": "To address these challenges, we propose a gradient-based approach for determining the relevance of model parameters to the slicing criterion, which does not rely on the activation of neurons. Specifically, we identify the relevant transformer block, referred to as intent, of the model by assessing the sensitivity of the blocks to the slicing criterion. The approach works by treating a sample of bad data, (DE = (XE, YE)), as criteria for slicing the intent that requires fixing. Figure 1 shows the overview of our proposed algorithm for slicing the LLM, which involves"}, {"title": "3.3 Repairing Intent", "content": "In this step, the identified slice or intent, Oslice, which is primarily responsible for undesirable generation, is addressed through two optimization objectives, as shown in Figure 1. Specifically, our loss function includes an NLL term as repair loss and a KL term to preserve the model's normal utility, as shown in Equation 3.\n$Loss =a \\cdot NLL (P_{\\theta_{slice}} (\\cdot | X^R)) + KL (P_{\\theta_{slice}} (\\cdot | X^N) || P_{\\theta_{ref}} (\\cdot | X^N))$"}, {"title": "3.3.1 Repair Loss", "content": "We use the negative log-likelihood (NLL) as the repair loss for our technique. This loss aims to maximize the log-likelihood of the curated responses (YR) for fault-evoking prompts (XE). NLL is a commonly employed loss function for repairing models via continued pre-training or supervised fine-tuning [Gehman et al. 2020; Geva et al. 2022; Lee et al. 2024; Wang et al. 2022]. However, unlike existing techniques, we only optimize the sliced parameters (\u03b8slice) of the patient model, \u03c0\u03b8. This selective approach allows for more focused and aggressive updates of the error-prone parameters, potentially leading to more effective repairs. Additionally, updating a smaller portion of the total parameters reduces general performance degradation, as most of the model retains its original parameters. The relative importance of this term is regulated by the a coefficient."}, {"title": "3.3.2 KL Loss", "content": "KL loss is used to preserve the general performance of the model during the repair process. Specifically, this term aims to minimize the divergence between a reference distribution (the output of the reference model, \u03c0\u03b8ref) and the target distribution (output of \u03c0\u03b8 on the pre-training corpus DN). This term essentially encourages the model to maintain similar generation capabilities to the reference model on unrelated aspects."}, {"title": "3.3.3 Dynamic Slicing", "content": "Finally, we employ a dynamic slicing mechanism that selects the most error-prone block of the model during the course of training for an adaptive repair. This design decision is motivated by three key factors:\nError concentration. First, our threshold-free slicing technique selects only the most relevant or error-prone block of the model based on the criteria. However, a single block may not be solely"}, {"title": "3.3.4 Algorithm Overview", "content": "The Repair method in Algorithm 2 outlines the procedure for repairing the intent using our proposed optimization objectives. The method takes as input the affected or to-be-repaired model \u03c0\u03b8, a reference model \u03c0\u03b8\u2081ef, which is the same model as initial \u03c0\u04e9 and is used to maintain similar performance on unrelated aspects post-correction, and references to bad examples (DE), good examples (DR), and normal examples (DN). Additionally, a hyper-parameter a is provided, which is used as a measure of the strength of the repair loss.\nThe repair process begins by sampling a batch of good examples in Line 3 during each training iteration. It then constructs a batch of corresponding bad examples to use as slicing criteria for the current iteration (Line 4). Additionally, a random batch from the normal examples is obtained to compute a KL term (Line 5). Next, the Slice method is invoked to extract the most error-inducing block of the current model, \u03c0\u03b8 (Line 6). The NLL loss for the sliced parameters, \u018fslice, is computed using the good batch in Line 7. Similarly, a KL term is calculated for both the currently repaired model and the reference model using the batch of normal examples (Line 8). In Line 9, the combined loss is obtained, with the NLL term regulated by a user-defined coefficient (a). After computing the loss, gradients with respect to the sliced parameters are computed in Line 10 and updated in Line 11. The repair process continues until convergence or early stopping is triggered and the repaired model is returned."}, {"title": "4 Evaluation", "content": "In this section, we introduce our evaluation setup, outline our research questions, and discuss the experimental results in detail. As previously mentioned, we evaluate our technique within a model detoxification framework, where our goal is to repair toxic models using the principles outlined in IRepair. To this end, we examine our framework across three research questions:\n\u2022 RQ1: How effectively can IRepair repair or detoxify the model? This research question evaluates IRepair's effectiveness in eliminating toxicity from models and compares it against several state-of-the-art baseline techniques.\n\u2022 RQ2: What is the computational overhead of IRepair? This research question measures the computational overhead of IRepair by assessing total floating point operations (FLOPS),"}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Model. We evaluate IRepair across three models from the GPT family, namely GPT-2 Large (812M parameters), GPT-2 XL (1.61B parameters), and GPT-Neo (1.3B parameters). The GPT-2 models, developed by OpenAI, were trained on 8 million web pages from the WebText dataset [Radford et al. 2019]. The GPT-Neo model, developed by EleutherAl, was trained on the PILE dataset [gpt 2024a]. We load these pre-trained models from the official Hugging Face repositories of OpenAI and EleutherAI [gpt 2024b,c].\n4.1.2 Dataset. To evaluate IRepair, we used the detoxification dataset developed by Lee et al. [Lee et al. 2024], consisting of 24,576 toxic and non-toxic pairs generated by GPT-2 models [Radford et al. 2019] from prompts sampled from the WikiText-2 train split [Merity et al. 2017]. The dataset is balanced with an equal number of toxic and non-toxic examples for detoxification. In our experimental setup, we treat the toxic continuations as a bad demonstration dataset, DE, and the non-toxic continuations as a good dataset, DR. The evaluation data includes a test set and a small development sample, which consists of 50 challenge prompts from REALTOXICITYPROMPTS [Gehman et al. 2020] and a subset of the WikiText-2 test split, totaling around 32.7K tokens. This development set is used for tuning hyperparameters through repeated runs with various combinations.\nAdditionally, for each model, we construct an individual normal dataset, DN, a curated pre-training subset that preserves diversity while minimizing targeted error symptoms. We leverage the notion of 'unconditional generation' to construct this dataset [Wang et al. 2022]. Specifically, starting with the special start-of-sequence token (GPT models use \u2018<|endoftext|>' as the start-of-sequence token [Wang et al. 2022]), we generate approximately 15,000 texts for each model using different random seeds. Following prior work, we employ nucleus sampling with a temperature of 1 and p = 0.9 during generation [Wang et al. 2022]. The unconditionally generated text corpus is considered a good representative of the model's training corpus [Wang et al. 2022]. Then, we score each generation using perspective API and remove the ones with toxicity scores higher than 0.5 [Gehman et al. 2020], removing approximately 1.2% of the initial dataset. By minimizing the KL divergence in these examples, we aim to preserve the model's ability to generate random text similarly to its pre-repair state, thereby reducing the impact on its general performance.\n4.1.3 Baseline. We compare the performance of two variants of IRepair: the standard IRepair, which does not enforce a KL constraint, and IRepair + KL, which does, against several representative state-of-the-art baselines within domain-adaptive training, as introduced below:\nDomain-Adaptive Pretraining (DAPT). DAPT is a framework introduced by Gururangan et al. [Gururangan et al. 2020], which involves continuing the pretraining of a model on domain-specific texts. Gehman et al. [Gehman et al. 2020] applied the DAPT framework to further train GPT-2 models on nontoxic texts to detoxify them. In our setup, we evaluate the effectiveness of DAPT in detoxifying models and compare it with IRepair.\nDirect Preference Optimization (DPO). DPO is a cutting-edge algorithm designed to replace RLHF (Reinforcement Learning from Human Feedback [Ouyang et al. 2022]) due to its complex and unstable training process. It directly steers the model towards desirable generations over undesirable ones [Rafailov et al. 2024]. The DPO algorithm has been shown to effectively eliminate"}, {"title": "4.1.4 Metric", "content": "Following the prior works [Gehman et al. 2020; Geva et al. 2022; Lee et al. 2024], we use the following two metrics to evaluate the toxicity and general performance of the model after detoxification.\nToxicity. Gehman et al. developed a dataset called REALTOXICITYPROMPTS to evaluate toxicity in LLMs [Gehman et al. 2020]. This dataset consists of sentence-level prompts that are provided to LLMs to generate continuations, which are likely to elicit toxic responses from the models. They also created a challenge subset of this dataset, which includes 1,199 prompts that consistently caused all models in their experiments to generate toxic responses [Gehman et al. 2020]. This challenge subset has been used in previous studies to evaluate the effectiveness of detoxification methods [Geva et al. 2022; Lee et al. 2024]. Similarly, we leverage this subset to assess the detoxification quality of our repaired models. Additionally, following prior works [Gehman et al. 2020; Geva et al. 2022; Lee et al. 2024], we use the widely adopted toxicity detection tool, PERSPECTIVE API [pap 2024], to assign a toxicity score to each generation. The score ranges from 0 to 1, with higher scores indicating more toxic responses. The toxicity score for the test data is calculated as the average toxicity score across the entire test set, following prior works [Lee et al. 2024].\nPPL. Perplexity (PPL) is a widely used metric to evaluate the generation quality of language models. It has been employed to assess degradation in model generation after the repair process [Gehman et al. 2020; Geva et al. 2022; Lee et al. 2024; Wang et al. 2022]. PPL measures how uncertain or \"perplexed\" the model is in predicting the next word. Higher PPL indicates that the model is worse at predicting the next word, meaning its generation quality is lower. When evaluated on a test corpus, it reflects how well the model's generated text aligns with the test data. Following prior works on GPT models [Geva et al. 2022; Lee et al. 2024], we compute the PPL of the model before and after repair using two benchmarks. PPL (WikiText2) is computed on the test split of WikiText-2 [Merity et al. 2017], which contains 4,358 rows and approximately 241K words. PPL (Lambada) is evaluated on the LAMBADA benchmark [Paperno et al. 2016], which assesses the text-understanding capabilities of language models. To calculate PPL, we split the test corpus into 1024-token segments, compute the NLL for target tokens in each segment, and weigh the NLL"}, {"title": "4.1.5 Training Details", "content": "As discussed in \u00a7 4.1.1, we used pre-trained models from the official Hugging Face repositories of OpenAI and EleutherAI [gpt 2024b,c]. We leveraged Python's deep learning library PyTorch [Paszke et al. 2017] for further training these models across all techniques. All hyperparameters in our study were fine-tuned using a small development dataset produced by Lee et al. [Lee et al. 2024]. For DPO, DAPT, and DAPT + KL, we used the implementation provided by Lee et al. [Lee et al. 2024] as a reference.\nSimilarly, we fine-tuned the hyperparameters for all techniques using the development set. Specifically, the final tuned learning rates for standard IRepair, IRepair + KL, DPO, DAPT, and DAPT+KL are 2e-5, 5e-5, 1e-6, 1e-6, and 5e\u00af6, respectively. Through trial and error, we found that a higher learning rate tends to achieve better repair quality at the expense of general performance and vice versa. Since IRepair only modifies a small portion of the model, it can accommodate a larger learning rate with less adverse impact on general performance compared to other indiscriminate techniques. Similarly, DAPT+KL allows a slightly higher learning rate than DPO and DAPT as it explicitly aims to maintain general performance during repair. We also set the value of a to 0.5 for both IRepair and DAPT+KL after tuning.\nFor training models using all techniques, we used the memory-efficient RMSProp optimizer with 150 warmup steps and a linear learning rate scheduler. A batch size of four was used for the techniques, with a validation split of eight batches, each with a batch size of eight. Models were trained with a validation loss patience of 30 iterations. All models were trained on an NVIDIA A100 GPU with 40GB of memory. We conducted all the training using the same random seed to ensure reproducibility and enable a fairer comparison."}, {"title": "4.2 RQ1: How Effectively can IRepair Repair the Model?", "content": "In this research question, we evaluate the repair effectiveness of IRepair and compare it against several baselines. Table 1 provides a comparative overview of IRepair's performance across all models. The results show that both variants of IRepair consistently outperform all other techniques"}, {"title": "4.3 RQ2: What is the Computational Overhead of IRepair?", "content": "In this research question, we evaluate the computational overhead of IRepair and compare it against baseline techniques. Table 2 presents the overhead of various techniques across four metrics. Among the two IRepair variants, the overhead of standard IRepair is more amenable to the other three baseline techniques across all metrics. For example, overall, it ranks first in memory consumption and second in GPU time despite incurring higher TFLOPs and requiring more iterations to converge.\nThe additional compute units (TFLOPs) consumed by IRepair variants are due to the extra forward pass and a higher number of iterations required for convergence. IRepair + KL involves four forward passes: one for a toxic batch of data to assess the sensitivity (DE), one for a non-toxic batch (DR), and for normal data, one pass through the model under repair (\u03c0\u03b8) and another through the reference model (\u03c0\u03b8ref). In contrast, standard IRepair does not compute the KL term, thereby eliminating two forward passes for normal data, which results in significantly lower TFLOPs overall\u201453% less than IRepair + KL. DPO also requires four forward passes; however, it converges in fewer iterations, leading to lower total TFLOPs."}, {"title": "4.4 RQ3: Does the Dynamic Selection Employed by IRepair Offer Any Advantage?", "content": "In the final research question, we investigate the effectiveness of dynamic slicing in delivering focused model repair. As described in \u00a7 4.1.3, we evaluated two additional variants of both the standard IRepair and IRepair + KL: IRepair + Min and IRepair + Fixed. The IRepair + Min variant selects the transformer block with the lowest error concentration, as opposed to the highest in the regular IRepair. This baseline allows us to assess the impact of selection on repair efficacy. Similarly, IRepair + Fixed disables dynamic slicing and instead pre-selects the block with the highest error concentration for repair. This variant enables us to assess the impact of dynamic selection on repair effectiveness. The impact on model performance in this RQ is evaluated solely on the WikiText2 benchmark.\nTable 3 presents the comparative results of these variants against the regular IRepair. It demonstrates that both regular IRepair variants significantly outperform their Min and Fixed counterparts. For instance, standard IRepair and IRepair + KL reduce toxicity by 80.5% and 88.6% more than their"}, {"title": "5 Related Work", "content": "Software engineering research has proposed various techniques to repair bugs in deep neural networks (DNNs) that arise during training or within the network structure itself [Ma et al. 2018; Wardat et al. 2022; Zhang and Chan 2019; Zhang et al. 2021]. Examples include Zhang et al. 's method for monitoring DNN training and suggesting corrective actions for anomalies [Zhang et al. 2021], and Wardat et al.'s work on identifying and fixing structural bugs in DNNs [Wardat et al. 2022]. However, these techniques primarily address issues stemming from the DNN itself. In contrast, data-driven errors in LLMs, such as toxicity or hallucinations, can stem from biases and inconsistencies within the training data itself [Ji et al. 2023], requiring solutions beyond structural or training bug fixes.\nOn the other hand, machine learning research offers several strategies to mitigate such errors, primarily operating in three stages [Korbak et al. 2023; Pan et al. 2023; Wang et al. 2022]: runtime methods [Dathathri et al. 2020; Leong et al. 2023; Liu et al. 2021; Niu et al. 2024; Schick et al. 2021; Wang et al. 2022; Weng et al. 2023; Xu et al. 2022; Yang et al. 2022], the pre-training stage [Huang et al. 2023; Korbak et al. 2023; Liu et al. 2024], and alignment stage [Gehman et al. 2020; Lee et al. 2024; Liu et al. 2023; Rafailov et al. 2024; Wang et al. 2022]. These approaches are often complementary, each with its own scope and applicability [Huang et al. 2023]. Runtime methods aim to control model outputs through non-invasive techniques such as context augmentation, vocabulary modification, word filtering, and output post-processing [Dathathri et al. 2020; Huang et al. 2023; Wang et al. 2022]. These methods provide flexible runtime control, though they face certain constraints - they do not modify the underlying model architecture or weights when such modifications would be beneficial [Wang et al. 2022]. The additional computational overhead also presents challenges in latency-sensitive applications [Wang et al. 2022]. Prompt-based approaches for steering instruction-following models fall into this category as well, relying on prompt engineering techniques to mitigate undesirable responses [Ganguli et al. 2023; Xie et al. 2023].\nIn contrast to runtime methods, pretraining-based approaches suggest removing problematic data from the training corpus or modifying model architecture [Korbak et al. 2023; Liu et al. 2024]. While effective, this can be prohibitively expensive, making them suitable for new model developments [Wang et al. 2022]. DAT methods that operate during the alignment stage, like ours, offer a preemptive approach to error mitigation. This is particularly suitable in scenarios where these limitations are especially impactful and require a more invasive repair [Lee et al. 2024; Wang et al. 2022].\nDAT methods aim to continue pretraining or fine-tuning models on domain-specific texts [Gehman et al. 2020; Lee et al. 2024]. For instance, Gehman et al. [Gehman et al. 2020] applied a framework called Domain-Adaptive Pretraining (DAPT) [Gururangan et al. 2020] to further pretrain GPT-2 on curated non-toxic data, reducing its toxicity. Similarly, Wang et al. [Wang et al. 2022] used the DAPT framework with self-generated data to detoxify models. However, their technique is model-dependent and relies on self-generated data, which may not be applicable in all error scenarios,"}, {"title": "6 Threats To Validity and Limitations", "content": "An internal threat to the study is the quality of the detoxification and evaluation datasets. To address this, we utilize both the training dataset and evaluation setup from a recent reputable work on LLM detoxification [Lee et al. 2024]. Additionally, we employ the implementations provided in the same study to address concerns about the construct validity of baseline techniques. Another internal threat arises from the reliability of the evaluation metrics. To mitigate this concern, we measure repair or toxicity scores using the widely used Perspective API [pap 2024] and evaluate model quality post-repair using perplexity, as employed in many prior works [Gehman et al. 2020; Lee et al. 2024; Wang et al. 2022]. Similarly, our evaluation metrics for measuring computational overheads are based on well-established metrics in the literature [Kaplan et al. 2020; Lee et al. 2019]. An external threat is the relevance of the models used. To address this, we have selected three models from the GPT and GPT-Neo families with billions of parameters, all of which have previously been employed for evaluating detoxification techniques [Gehman et al. 2020; Korbak et al. 2023; Lee et al. 2024; Leong et al. 2023; Xu et al. 2022; Yang et al. 2022].\nWhile the case study performed in this paper shows that IRepair can effectively address the data-driven errors in large language models (LLMs), it is demonstrated within the context of detoxification."}, {"title": "7 Conclusion", "content": "In this paper, we introduce IRepair, an intent-aware technique for selectively repairing data-driven errors in LLMs through dynamic model slicing. While domain-adaptive training with curated data has shown promise, it tends to optimize model parameters indiscriminately, which can limit repair efficacy and increase the risk of negatively affecting general model performance by altering unrelated parameters. To address these limitations, IRepair identifies the relevant portions of the model responsible for the errors, allowing for more targeted repair and making it an intent-aware approach. Our method employs a gradient-based technique to select the most relevant parts of the model by analyzing sensitivity to slicing criteria. Unlike existing slicing routines, our technique is specifically designed to address transformer-related challenges and to avoid the need for expensive tuning of selection thresholds. In a case study focused on model detoxification, IRepair demonstrated its effectiveness in addressing the root causes of toxicity while minimizing the impact on general performance, outperforming state-of-the-art baselines. Our empirical results also suggest that errors can be highly concentrated in very limited regions of the model, highlighting the need for selective repair. We further demonstrate that a dynamic selection-based repair strategy is essential for effectively addressing errors dispersed throughout the model."}]}