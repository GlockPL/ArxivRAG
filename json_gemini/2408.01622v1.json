{"title": "Positive-Unlabeled Constraint Learning (PUCL) for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations", "authors": ["Baiyu Peng", "Aude Billard"], "abstract": "Planning for a wide range of real-world robotic tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. This paper presents a novel Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a continuous arbitrary constraint function from demonstration, without requiring prior knowledge of the true constraint parameterization or environmental model as existing works. Within our framework, we treat all data in demonstrations as positive (feasible) data, and learn a control policy to generate potentially infeasible trajectories, which serve as unlabeled data. In each iteration, we first update the policy and then a two-step positive-unlabeled learning procedure is applied, where it first identifies reliable infeasible data using a distance metric, and secondly learns a binary feasibility classifier (i.e., constraint function) from the feasible demonstrations and reliable infeasible data. The proposed framework is flexible to learn complex-shaped constraint boundary and will not mistakenly classify demonstrations as infeasible as previous methods. The effectiveness of the proposed method is verified in three robotic tasks, using a networked policy or a dynamical system policy. It successfully infers and transfers the continuous nonlinear constraints and outperforms other baseline methods in terms of constraint accuracy and policy safety.", "sections": [{"title": "I. INTRODUCTION", "content": "PLANNING for many robotics and automation tasks requires explicit knowledge of constraints, which define what states or trajectories are allowed or must be avoided [1], [2]. Sometimes these constraints are initially unknown or hard to specify mathematically, especially when they are nonlinear, possess unknown shape of boundary or are inherent to user's preference and experience. For example, human users may determine an implicit minimum distance of the robot to obstacles based on the material of the obstacle (glass or metal), the shape and density of obstacles and their own personal preference. The robot should be able to infer such a constraint as well as the desired minimum distance to achieve the task goal and meet human's requirements. This means an explicit constraint should be inferred somehow, e.g., from existing human demonstration sets.\nConstraint inference from demonstrations has drawn more and more attention since 2018 [3]. Various methods have been developed to learn various types of constraints. In this work, we focus on the constraints of not visiting some undesired states(-actions) throughout the trajectory (discussed in detail in section II-A). These constraints define what the user does not want to happen and are ubiquitous in practice, e.g., not crashing into an unknown obstacle or not surpassing an unknown maximum velocity."}, {"title": "II. PRELIMINARIES AND PROBLEM STATEMENTS", "content": "For a finite-horizon Markov decision process (MDP) [16], [17], states and actions are denoted by $s \\in S$ and $a \\epsilon A$. $\\gamma$ denotes the discount factor, and the potentially unknown real-valued reward is denoted by $r(s, a)$. A trajectory $\\tau = \\{$s_1, a_1,..., s_T, a_T$\\}$ contains a sequence of state-action pairs in one episode. For the trajectory $\\tau$, the total discounted reward (return) is defined as $r(\\tau) = \\sum_{t=1}^T \\gamma^t r(s_t, a_t)$. A policy, the mapping between states and actions, is denoted by $a = \\pi(s)$.\nIn this study, we focus on learning the Markovian state-action constraints, i.e., avoiding visiting some forbidden states (and actions) throughout the entire trajectory. Mathematically, we denote the true constraint set by $C^* = \\{(s,a) \\in S \\times A | (s, a) \\text{ is truly infeasible}\\}$. Thus, the true constraint is written as $(s, a) \\notin C^*$. This definition implicitly assumes that the studied constraint is (1) time-independent: the infeasible set remains unchanged for every timestep of every trajectory; (2) deterministic: a state-action pair is either truly feasible or truly infeasible, though the prediction model could output a probabilistic prediction of feasibility. (3) defined only on the current state (and potentially action), independent of previous states and actions. In addition to the unknown constraints, some known constraints could also be present in the task, e.g., goal state constraints and system dynamic constraints. For simplicity of explanation, all demonstrations and trajectories are assumed to satisfy the known constraints (trajectories violating known constraints are discarded and not used for inferring unknown constraint)."}, {"title": "B. Problem Formulation", "content": "As in similar works [3], [8], we require that the reward function $r(s,a)$ is known in advance. This assumption is reasonable in many robotics applications, where the reward might be shortest path, shortest time to reach the goal. We emphasize that the unknown constraints are not indicated in the known reward function, i.e., the agent does not receive any penalty for violating the unknown constraints. Subsequently, the real task is written as a constrained optimization problem:\nmax\n$\\qquad r(\\tau)$\n$\\text{s.t.} \\qquad (s_i, a_i) \\notin C^*, \\forall (s_i, a_i) \\in \\tau$\nwhere $r(\\tau)$ represents the reward function of trajectory $\\tau$, (1b) represents the unknown constraint to be learned.\nThe demonstration set $D = \\{\\tau_i\\}_{i=1}^{N_{Ma}}$ comprises trajectories demonstrated by a demonstrator, who may not necessarily be an expert but still to some extent solves the task. More concretely, we make the following assumption of the demonstrations optimality and feasibility:\nAny demonstration from $D = \\{\\tau_i\\}_{i=1}^{N_{Ma}}$ is feasible and $\\delta$-sub-optimal solution to the problem 1:\n(feasible): $\\tau_i$ satisfies (1b) and all known constraints.\n($\\delta$-sub-optimal): $(1 - \\delta)r(\\tau^*) \\leq r(\\tau_i)$, where $\\tau^*$ is the feasible optimal solution to the problem 1. $\\delta \\in [0,1)$ is a coefficient of sub-optimality. In practice it is treated as a hyper-parameter and specified according to the user's confidence of demonstration optimality.\nDefinition 1 (Constraint Learning). The task of learning constraint from demonstration is to recover the unknown true constraint set $C^*$ in Problem 1 from the demonstration set $D$, with the known reward function $r(\\tau)$.\nNote that in the rest of the paper we only consider the state constraints and omit actions, but the framework can be easily extended to state-action constraints by augmenting the state with the action to form an augmented state.\nTo represent and learn an arbitrary constraint set in continuous space, we approximate it with a constraint network $\\zeta_\\theta(s) \\in (0,1)$, which induces a constraint set $C_\\theta = \\{s \\in S | \\zeta_\\theta(s) \\leq d\\}$, where $\\theta$ is the network weights and $d = 0.5$ is the threshold."}, {"title": "III. METHOD", "content": "As in many IRL-based and MBI-based works [3], [8], we infer the underlying constraint by contrasting the (safe) demonstration with some potentially unsafe trajectories generated by a certain policy. The training follows an iterative framework to incrementally learn the constraint. In each iteration, we first sample a set of high-reward trajectories $P = \\{\\tau_i\\}_{i=1}^{M_p}$ by performing the current policy $\\pi_\\phi$ (discussed later in III-D). Each sampled trajectory consists of sequential state-action pairs $\\tau = \\{s_j, a_j\\}_{j=1}^N$. Similar to [3], we speculate that the trajectory which wins higher reward than demonstration is potentially unsafe and cheat by visiting some forbidden states. However, it remains unclear which specific state(s) within the trajectory are unsafe. In other words, trajectory $\\tau$ visits both truly feasible states and truly infeasible states but both remain unlabeled. In contrast, it is certain that all states on the demonstrated trajectories are labeled feasible by Assumption 1. Our goal is to classify each single state as feasible or infeasible by learning from a batch of fully labeled feasible samples and another batch of unlabeled mixed samples.\nThis insight inspires us to formulate constraint inference as a positive-unlabeled learning problem. PU learning is a machine learning technique that aims to learn a classifier from a positive dataset and an unlabeled dataset containing both positive and negative samples [14]. Within our framework, positive samples correspond to feasible states. Thus, the data from demonstrations serve as positive data, while the policy offers unlabeled data containing some infeasible samples. We can thus utilize PU learning methods to uncover the infeasible states from the two datasets.\nPU learning methods for binary classification mainly divide into three categories: two-step methods, biased learning and class-prior-based methods [14]. The latter two methods both rely on an important assumption, Sampled Completely At Random (SCAR), which requires the demonstrations and policy to have a similar distribution in the truly feasible region. Our former paper [15] already explored the class-prior-based methods. However, the required SCAR assumption is usually unrealistic since the state distribution largely depends on the specific task and is hard to control. Violation of this assumption leads to performance degradation. In contrast, the two-step methods are based on a separability assumption, which requires the two classes can be strictly separated. Since we limit our discussion to deterministic constraint and a state is either truly feasible or infeasible, the separability assumption naturally holds. Therefore, we develop our methods based on the two-step framework [18], [19]."}, {"title": "B. Method: Positive-Unlabeled Constraint Learning", "content": "Our two-step PU learning methods consists of the following steps: (1) identifying reliable infeasible states from unlabeled trajectory set P, and (2) using supervised learning techniques to learn a classifier from the feasible demonstrations D and reliable infeasible state set (denoted as R). In step 1, the identification of R starts from the intuition that the states in the unlabeled set P that are very different from all states in the demonstration set D are highly likely to be truly infeasible. We propose a kNN-like approach, which is straightforward to comprehend and implement, to identify the reliable infeasible set R. The unlabeled states are first ranked according to their distance to the k nearest feasible states from demonstration set. The unlabeled states whose distance is higher than a threshold are selected as reliable infeasible states. Mathematically, the ranking score for each unlabeled data $s \\in \\tau_i \\in P$ is calculated by the average distance:\n$S_D(s) = \\frac{1}{k} \\sum_{s' \\in kNN(s,D)} dis(s,s')$\nwhere $kNN(s,D)$ indicates the k-nearest neighbours of $s$ in the set D, and $dis(s, s')$ is a distance metric selected based on the specific task. In this work, this distance is always measured using Euclidean distance. Additionally, considering the range of each dimension of the state could be very different, each dimension is preferred to be normalized before computing distance and nearest neighbours. The reliable infeasible set R consists of all unlabeled data whose score exceeds a threshold $d_r$.\n$R = \\{s \\in P | S_D(s) \\geq d_r\\}$\nIn practice, since D contains consecutive state series from trajectories, a smaller $k$ around 1-5 is preferred. For $d_r$, it indicates user's belief over the size of true infeasible area. A lower $d_r$ tends to identify a larger R, leading to a more conservative constraint network. Conversely, a higher $d_r$ tends to find a smaller R and a more radical constraint function. In the extreme case of $d_r = 0$, nearly any state not visited by the demonstrations is classified as infeasible. We admit selecting $d_r$ can be tricky, but such a parameter is generally inevitable since learning constraints is known to be an ill-posed problem with infinite valid solutions [8].\nHowever, in the case where the demonstration closely adheres to the true constraint boundary with a distance shorter than $d_r$, kNN-like metric may fail to learn an accurate boundary, as the reliable infeasible data are all at least $d_r$ far from the demonstrations. To adapt to this case, we additionally expand R by adding the state(s) from each trajectory in P that is closest to the original R, since these states are likely to between the current boudnary and the true boudnary.\n$R \\leftarrow R \\cup \\{s | \\exists \\tau_i \\in P, s = arg \\underset{s' \\in \\tau_i}{min} S_R(s')\\}$\nwhere $S_R(s)$ is the score function sharing similar definition as $S_P(s)$ in (2) but with a different set P.\nIn step 2, we directly train a neural network binary classifier $\\zeta_\\theta(s)$ from the reliable infeasible data R, the memory buffer M (discussed later in III-C) and the (feasible) demonstrations data D with a standard cross entropy loss:\n$L(\\theta) = \\frac{1}{N+M}[\\sum_{s_i \\sim D}log \\zeta_\\theta(s_i) - \\sum_{s_j \\sim R \\cup M} log(1 - \\zeta_\\theta(s_j))]$\nAfter training, the learned constraint set is expressed as $C_\\theta = \\{s \\in \\zeta_\\theta(s) \\leq 0.5\\}$"}, {"title": "C. Iterative Learning Framework with Policy Filter and Memory Buffer", "content": "Last subsection discussed about learning constraint from a given unsafe set P as well as demonstrations D. This and next subsection will elaborate on the generation of unsafe set P. Existing methods generate P either in an iterative manner [4], [8], [9] or in a one-batch manner [3], [7], [13]. The former iteratively updates policy with current constraint function and generate new trajectories P to update the constraint. Since the policy is always updated with respect to up-to-date constraint network, the new trajectories generated in each iteration are mostly distributed around the boundary of the current constraint network, which will improve the accuracy of the constraint boundary. Therefore, this manner is efficient to learn the constraint boundary and can automatically adjust the distribution of the generated trajectories.\nIn contrast, the one-batch manner generates a bulk of trajectories P uniformly [3] or greedily [7] at the start of training. The following training only takes place on only existing P, with no new data generated. This manner is more stable but relatively inefficient and poses a higher demand on the diversity and the coverage of the generated trajectories.\nThis work follows the iterative framework, and introduce a policy filter and a memory buffer both from our previous work [15]. Fig. 2 gives a sketch of the whole iterative structure.\nPolicy filter: In each iteration of policy updating, the policy is updated for fixed steps with the current constraint network $\\zeta(s;\\theta)$. To prevent bad updates from bad policies, a policy filter (6) is introduced to select trajectories with relatively high reward than the demonstration, which are believed to truly violate the unknown constraint according to Assumption 1.\n$P \\leftarrow \\{\\tau_i \\in P | r(\\tau_i) \\geq (1 - \\delta)r(\\tau_i^P)\\}$\nwhere $\\delta$ is a coefficient defined in Assumption 1, $\\tau_i^P$ is the demonstration that starts from the same state as $\\tau_i$.\nMemory buffer: Most iterative-framework-based papers generate new trajectories in each iteration and discard those from previous iterations [4], [8], [9]. Our previous work [15] identified that such a training manner tends to forget the infeasible region learned in early iterations. Thus, we introduce a memory replay buffer and record all the reliable infeasible data R identified in each iteration into the memory buffer M. In the following iterations of learning constraints, both the current R and memory buffer M serve as infeasible data for training (see (5))."}, {"title": "D. Represent and Learn Policy via Constrained RL or Dynamical System Modulation", "content": "In the iterative framework discussed in the last subsection, we need to maintain a policy to generates high-reward trajectories while satisfying the currently learned constraints. We consider and compare two approaches: 1) constrained RL, which is effective at learning constrained policies in a high-dimensional environment with complex reward function but suffers from a slow and unstable training process [20], and 2) dynamical system modulation (DSM), which modulates a nominal policy with a rotation matrix to satisfy safety constraints. It features minimal training time and provides a controller with stability guarantee. This method is applicable and very suitable for robotic tasks where the control command is the velocity of each state (i.e., joint angle or end-effector pose).\nConstrained reinforcement learning: a networked policy is trained with a modern constrained RL algorithm PID-Lagrangian [20], which offers the advantage of automatically and more stably adjusting the penalty weight. As shown in (7), it reshapes the original reward by incorporating the constraint as a penalty term into the original reward function to avoid the infeasible states. Here, $w_p$ is a penalty weight adjusted by PID-Lagrangian updating rules [20]\u2013[22], and $c(s)$ is the constraint indicator function.\nUsing this reshaped $r'(s, a)$, a constrained optimal policy can be straightforwardly learned with the standard RL algorithm PPO [23].\n$r'(s,a) = r(s, a) - w_p c(s)$, where\n$\\qquad c(s) =  \\begin{cases}\n0 & \\text{if } \\zeta(s) > d \\text{ (feasible)} \\\\\n1 & \\text{if } \\zeta(s) \\leq d \\text{ (infeasible)}\n\\end{cases}$\nDynamical System Modulation: For the task where the control input is the velocity or acceleration of each state dimension, DSM can be applied to obtain a safe control policy by modulating nominal policy with the constraint network. The nominal dynamical system policy is represented as (8), where $s_g$ is the goal state and matrix $A(s)$ is a Gaussian Mixture Regression model learned from the known reward function following [24].\n$\\pi(s) = s = A(s)(s - s_g)$\nGiven a learned constraint network $\\zeta(s;\\theta)$ whose output ranges from 0 to 1, one can obtain a provably safe policy $\\pi(s;\\zeta)$ by modulating the nominal policy following (9) [25], where n(s) is the normal vector to the constraint boundary, $e_i(s)$ are the tangent vectors, r(s) is the vector towards a reference point, $\\Lambda_i(s)$ modify the velocity along different vectors, and $\\Gamma(s)$ specifies the relative distance of current state s to the constraint boundary.\n$\\pi(s) = M(s) \\Lambda(s) (s - s_g)$, where\n$M(s) = E(s) D(s) E^{-1}(s)$,\n$E(s) = [r(s), e_1(s), e_2(s), ... e_{n-1}(s)]$,\nn(s) = $ \\frac{\\frac{d \\zeta(s)}{ds}}{\\frac{d \\zeta(s)}{ds}} \\ , n(s) \\perp e_1(s) \\perp e_2(s) \\perp ... e_{n-1}(s)$\n$D(s) = diag[\\Lambda_1(s), \\Lambda_2(s), \\Lambda_3(s) ..., \\Lambda_n(s)]$,\n$\\Lambda_1(s) = 1 - \\Gamma(s), \\Lambda_{2,3,...,n}(s) = 1 + \\frac{\\Gamma(s)}{2}$\n$\\Gamma(s) = 1 + 10 * (\\zeta(s) - 0.5)^{0.2}$\nFinally, the pseudo-code for the algorithm is given in Algorithm 1."}, {"title": "IV. EXPERIMENT", "content": "Environment and constraint: to examine the performance of the proposed method, we apply it to learn two position constraints of the end-effector's trajectory of a Panda robot arm, one in 2D and another in 3D spaces. In both tasks, the agent is initialized randomly and rewarded to reach a goal state. The state space consists of 2D or 3D positions of the end-effector and the control variables are their corresponding velocities. In the 2D reaching task, the true constraint is avoiding an irregular obstacle composed of 2 ellipses as shown in Fig. 3. In the 3D reaching task, the true constraint is avoiding tall cylinder(s).\nWe emphasize that learning these two constraints is nontrivial and challenging in our problem setting for several reasons. Firstly, we assume no prior knowledge of the obstacles' size, shape, and location, nor of the true constraints, while some existing methods learn constraints of obstacles with known shape (i.e., a box or a cylinder) [13]. Secondly, there is no requirement for a differentiable environmental model (if using constrained RL to learn policy). Thirdly, the constraints are nonlinear, not just (the union of) boxes or hyperplanes, which contrasts with previous IRL-based constraint learning works that only learn a linear plain constraint such as x > -3 [8], [9].\nThe expert demonstration sets for two tasks consists of 4 or 10 safe trajectories, respectively. All demonstrations are generated by an entropy-regularized suboptimal RL agent, trained assuming full knowledge of the true constraint. The reward function in both tasks are the negative of the Cartesian path length to reach the goal, which is a natural choice for a goal-reaching task.\nBaselines and implementations: Our baselines are chosen to study the following two questions:\n1) How effective is our PUCL method compared with other constraint learning algorithms with similar problem setting?\n2) In our PUCL framework, what are the differences between using constrained RL and dynamical system modulation to obtain the policy?\nTo answer question 1, we compare the constraint learning performance of the four methods, all using constrained RL as the policy: 1) PUCL (the proposed method); 2) maximum-likelihood constraint learning (MECL) [8], a popular IRL-based method that can learn a continuous arbitrary constraint function; 3) binary classifier (BC) [8], a method directly trains an NN feasibility classifier using loss (5) but with all data from Pas infeasible data; 4) generative-PUCL (GPUCL), an alternative PUCL approach using a generative model to identify reliable infeasible data. It first learns a Gaussian Mixture Model (GMM) of the expert distribution, and then identifies reliable infeasible data as the data from P with the lowest likelihood of being generated by the GMM. GPUCL is also first formulated in this paper but do not discussed in detail due to weaker performance. The results of comparison of the above four methods are presented in section IV-B.\nTo answer question 2, we conduct another experiment implementing our PUCL approach using 1) a constrained RL policy or 2) a DSM policy (see section III-D). The two frameworks are named CRL-PUCL and DSM-PUCL, respectively, and the results of comparison are presented in section IV-C.\nA list of important hyperparameters of our algorithm is given in Table I. The hidden activation functions in all networks are Leaky ReLU. The output activation function of the constraint network is the sigmoid function, whereas that of the policy and value network is tanh. Other baselines all adopt the same network architecture. Please refer to our code repo (released later) for other parameters."}, {"title": "B. Results of Different Constraint Learning Methods", "content": "The IoU index and the unsafe rate in the two tasks are presented in Fig. 4. All the results are the average of 10 independent runs, and the shaded area represents the variance. Each group of 10 runs uses the same demonstrations and hyper-parameters, but differ in the initialization parameters for the networks.\nThe proposed method PUCL exhibits superior performance compared to the baseline across both environments and metrics. It achieves a nearly zero unsafe rate at the convergence of the training in both environments. MECL suffers from performance decrease in the late training stage of the 2D task, this is partly caused by the constraint forgetting problem [15]. GPUCL adopts a similar framework to PUCL but performs worse than PUCL, possibly due to the difficulty of fitting a good expert distribution with a very limited numbers of demonstrations.\nDespite the superiority of PUCL, one may question why the IoU metric of PUCL is still below 0.8 (2D task) or 0.6 (3D task), relatively far from 1. This is due to the fundamental difficulty of learning the nonlinear constraint without knowing the true parameterization of the constraint and with a limited number of demonstrations. There would be regions in the state space whose feasibility cannot be inferred and decided from the given demonstrations, thus restricting the learning accuracy.\nIn provide readers with an intuitive understanding, we include visualizations of the 2D constraint learned with PUCL and MECL in Fig. 3. Comparing the learned constraints (red region) with the true constraints (white ellipses) confirms the effectiveness of our method in acquiring a model of a nonlinear constraint. Although the learned constraint area is partially incorrect due to limited demonstrations, it effectively captures the essence of the true constraint and is sufficient for training a safe policy. The baseline method MECL learns a constraint that is more inaccurate, and even some states visited by the demonstrations are classified as infeasible."}, {"title": "C. Results of Different Policy Representation and Learning Methods", "content": "As introduced in section III-D, we considered two policy representing and learning approaches, namely CRL (Constrained Reinforcement Learning) and DSM (Dynamical System Modulation). We test the two variants in the 3D task and summarize the metrics at convergence and training time of CRL-PUCL and DSM-PUCL in Table II. Here, the recall rate is computed as (True infeasible)/(True infeasible + False feasible), and precision is (True infeasible)/(True infeasible + False infeasible). Training was conducted on a PC with a 12th Intel i9-12900K \u00d7 24 and an NVIDIA GeForce RTX 3070. All results are the average of 15 independent runs.\nDSM-PUCL and CRL-PUCL achieve similar classification performance in IoU, recall rate and precision, and DSM-PUCL generally exhibits lower variance. For the learned policy, while both methods have an unsafe rate below 1%, the rate of DSM-PUCL is much lower than CRL-PUCL. This is because DSM-PUCL modulates the policy in a principled way and features a safety guarantee [25]. DSM-PUCL also requires less training time than CRL-PUCL. Based on these comparison, we conclude that both methods are effective for learning constraints. For robotic tasks where the control variable is the velocity/acceleration of each state dimension, DSM may be more favourable than CRL in terms of training time and policy performance, while CRL is more suitable for more general task with more complex environment dynamics and rewards."}, {"title": "D. Constraint Transfer to Variant of the Same Task", "content": "One remarkable advantage of learning constraint is that the learned constraint can be transferred to similar task with the same constraint but potentially different goals and rewards. To demonstrate this, we consider the 3D tasks of robot avoiding four tall cylinders forming a convex shape (see Fig. 1). We first applied DSM-PUCL to learn this constraint from 19 demonstrated trajectories (Fig. 1 shows some of the demonstrations.) Then the learned constraint network is transferred to a relevant task with a shifted goal state s' and a set of different starting points. The new nominal policy heading to s', is directly modulated with the transferred constraint, still following DSM in section III-D. This modulated policy, referred to as the transferred policy, is illustrated in Fig. 1, where it also avoids these cups despite the change of goal states. It is worth noting that the transferred constraint can also be utilized by other policy learning/optimization methods such as constrained RL. We adopted DSM in this section only due to its straightforward and rapid modulation process."}, {"title": "V. CONCLUSIONS AND LIMITATIONS", "content": "This paper proposed the Positive-Unlabeled Constraint Learning method to infer an arbitrary (potentially nonlinear) constraint function from demonstration. The proposed method treats the demonstration as positive data and the higher-reward-winning policy as unlabeled data. It first identifies the reliable infeasible data, and trains a feasibility classifier as constraint function. The benefits of the proposed method were demonstrated learn a 2D or 3D position constraint, using either a constrained RL policy or a DSM policy. It managed to recover and transfer a continuous nonlinear constraint and outperformed other baseline methods in terms of accuracy and safety.\nAlthough PUCL is effective, it only learns a decent but still inaccurate constraint given limited demonstrations and true constraint might be violated in some cases. In the future, we will explore integrating active learning technique to further refine the learned constraint model."}]}