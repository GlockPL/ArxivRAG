{"title": "An overview of domain-specific foundation model: key technologies, applications and challenges", "authors": ["Haolong CHEN", "Hanzhi CHEN", "Zijian ZHAO", "Kaifeng HAN", "Guangxu ZHU", "Yichen ZHAO", "Ying DU", "Wei Xu", "Qingjiang SHI"], "abstract": "The impressive performance of ChatGPT and other foundation-model-based products in human language understanding has prompted both academia and industry to explore how these models can be tailored for specific industries and application scenarios. This process, known as the customization of domain-specific foundation models, addresses the limitations of general-purpose models, which may not fully capture the unique patterns and requirements of domain-specific data. Despite its importance, there is a notable lack of comprehensive overview papers on building domain-specific foundation models, while numerous resources exist for general-purpose models. To bridge this gap, this article provides a timely and thorough overview of the methodology for customizing domain-specific foundation models. It introduces basic concepts, outlines the general architecture, and surveys key methods for constructing domain-specific models. Furthermore, the article discusses various domains that can benefit from these specialized models and highlights the challenges ahead. Through this overview, we aim to offer valuable guidance and reference for researchers and practitioners from diverse fields to develop their own customized foundation models.", "sections": [{"title": "1 Introduction", "content": "ChatGPT has redefined people's understanding of artificial intelligence with its outstanding performance. As its core technology, the Large Language Model (LLM) has become an essential tool for researchers and practitioners across various fields to improve their workflows. General-purpose foundation models are usually trained on large public datasets, enabling them to learn and address a wide range of common problems. However, these datasets cannot fully encompass all the specialized knowledge and technical details of certain specific domains. As a result, although general-purpose foundation models possess broad general knowledge, they lack the necessary depth to meet the complex needs of some specific fields [1]. Therefore, constructing domain-specific foundation models tailored to the needs of particular industries has become particularly important. Domain-specific foundation models, also known as industry-specific foundation models, are developed using data and applications specific to a particular field. Compared to general-purpose foundation models, they are trained with a large amount of domain-specific data, enabling them to more accurately understand and generate domain-specific professional content.\nWith the widespreading of ChatGPT-like products, the scope of the \"foundation model\" is gradually expanding. It is thus necessary to first make a clear definition of the foundation model discussed in this article so as to lay the foundation for the subsequent discussion on the customization of domain-specific"}, {"title": "2 Preliminaries of multi-modality foundation models", "content": "In this section, we will elaborate on the preliminary technological basis for customizing domain-specific foundation models. We begin with the architecture of foundation models, detailing all functional modules. Then, from four perspectives - feature extraction, modality alignment, scaling laws, and emergent phenomena - we will explain the foundational technologies that enable each module to contribute to the high performance of foundation models."}, {"title": "2.1 Architecture of foundation models", "content": "According to state-of-the-art research on foundation models, it is widely considered that multi-modality foundation models can encompass all functionalities and structures of single-modality foundation models. Essentially, a single-modality foundation model implements only a subset of the functionalities of a multi-modality foundation model."}, {"title": "2.2 Feature extraction", "content": "Feature extraction concerns extracting representative features from raw data. In the field of machine learning, particularly deep learning, feature extraction is a crucial step. As raw data often contains a significant amount of redundant and noisy information, feature extraction, which maps the data into a more information-dense feature space, enables the models to understand data structure and patterns more effectively.\nIn deep learning, neural networks can perform end-to-end feature extraction from raw data. However, this manner often requires large amounts of data and computational resources to ensure good performance and generalization. In each layer of neural networks, it transfers the output of previous layer to a new vector space. This structure allows for a flexible definition of the output dimensions of each layer without explicitly specifying the transformation. Leveraging these favorable characteristics, autoencoder learns an effective representation of the data by minimizing the reconstruction error between the input vector and the reconstructed vector. Autoencoder first compresses the input data into a low-dimensional feature vector and then projects them back into the original data space by a decoder, as illustrated in Figure 2.\nAn important design principle behind the modality encoder and modality decoder metioned before, is pairing of these components as autoencoders for training. As a variant of autoencoders, variational autoencoder (VAE) aims to both minimize the reconstruction error and maximum the likelihood of the input data to learn the distribution of the compressed vectors. For example, in the image modality, VQGAN [48] is constructed in this manner. It has also been widely applied in subsequent generative models [49,50]."}, {"title": "2.3 Modality alignment", "content": "In the workflow of a single-modality foundation model, the architecture excludes input and output projectors, reflecting the absence of a requirement for cross-modality data processing. In contrast, multi-modality foundation models must accommodate the processing of various data modalities, including both the primary and ancillary modalities. To achieve data conversion between modalities through input projectors and output projectors, the key is to apply modality alignment. The goal of modality alignment is to process the feature vectors of different modalities into a common feature space with the same dimension by using a loss function to characterize the correlation between feature vectors. Ideally, modality alignment should ensure that raw data of different modalities carrying the same semantic information are"}, {"title": "2.4 Scaling law", "content": "Scaling law refers to the mathematical pattern about how the performance of a system changes as the scale of the system increases. In the field of AI, especially in the research and application of foundation models, scaling law describes a series of rules and phenomena about how model performance changes as the model scale expands, including the number of parameters, dataset size, computational resources. It uses quantitative analysis methods to reveal the intrinsic mechanism of the performance improvement of foundation models.\n[54] discussed how the inductive biases of different models affect the relationship between model scale expansion and performance. They found that model architecture is indeed one of the key factors affecting the benefits of model expansion. They also pointed out that although the standard transformer architecture may not always achieve the best performance, it does exhibit the best scalability. In the fields of computer vision [55] and natural language process [56], models based on the transformer architecture have shown an exponential relationship between model scale and model performance.\nBesides, [57] examined the impact of the number of downstream tasks and model scale on the performance of instruction fine-tuning. They fine-tuned the models on a wide variety of tasks by a multi-task"}, {"title": "2.5 Emergent phenomenon", "content": "The scaling law reveals that the scaling up of model size can lead to incremental improvements in model performance. On the contrary, the emergent phenomenon refers to the new properties exhibited by the model after reaching a critical point of scale expansion, one of which is a significant improvement in model performance [60].\nThe emergent phenomenon essentially reveals the source of the superior performance of foundation models. In the field of deep learning, especially in the domain of LLM, the emergent phenomenon has been widely observed. For example, models like LLaMA have demonstrated exceptional comprehension, generation capabilities, and even a certain degree of logical reasoning ability, which small language models cannot achieve. As the model scale increases, the model can have more parameters and a more complex structure, allowing it to capture the complex features and patterns in the data. Foundation models often exhibit strong generalization capabilities, which is mainly because their abundant parameters can store rich knowledge, enabling them to make accurate inferences and predictions on unseen data. It can provide them adaptability and universality to different tasks, and even allows the model to truly learn the underlying principles and reasoning methods in the data. [60] suggested that the emergence point of the emergent phenomenon in LLMs can be influenced by the task and the prompting method used. Specifically, the use of a chain-of-thought prompting approach can significantly enhance the LLMs' ability to handle complex reasoning tasks [61], thereby bringing forward the emergence point of the emergent phenomenon."}, {"title": "3 Key technologies for building domain-specific foundation models", "content": "This section delves into the technical pathways toward customizing domain-specific foundation models. We will provide a detailed explanation of how to flexibly select and combine the appropriate modules from five key components - modality encoders, input projectors, backbone calculators, output projectors, and modality decoders based on the specific requirements of different domains. Additionally, we will analyze concrete cases to help readers better understand and apply the methodologies discussed in this section.\nWe can categorize the customization of domain-specific foundation models into three levels, ranging from low to high level of customization (i.e., from high to low reliance on general-purpose foundation models or pre-trained modules):\n1.  Domain-specific enhancement based on general-purpose foundation models.\n2.  Customization of the foundation model based on pre-trained modules.\n3.  Construction of the foundation model without pre-trained modules."}, {"title": "3.1 Domain-specific enhancement based on general-purpose foundation models", "content": "General-purpose foundation models offer comprehensive capabilities, making them suitable for a wide array of task scenarios. When such a general-purpose foundation model can fully handle the required data modalities, it renders unnecessary any modifications to its underlying architecture for model developers. Instead, they can focus on implementing domain-specific enhancements.\nDepending on whether the domain-specific enhancement requires altering the parameters of the foundation model, we can further divide this into two types: plug-and-play domain-specific enhancement and fine-tuning-based domain-specific enhancement."}, {"title": "3.1.1 Plug-and-play domain-specific enhancement", "content": "The generality, generalization capabilities, and reasoning abilities of general-purpose foundation models enable them to serve as the foundation for domain-specific models. To achieve plug-and-play domain enhancement without modifying the parameters of the foundation model, two approaches can be utilized: leveraging existing knowledge or embedding new knowledge. The first approach aims to leverage domain knowledge already stored within the general-purpose foundation model, as illustrated in Figure 5 (a). The second approach, embedding new knowledge, involves equipping the foundation model with the ability to handle domain tasks by introducing domain-specific knowledge. This can be further divided into two methods: embedding knowledge through prompts and embedding knowledge via an external knowledge base. These methods are depicted in Figure 5 (b) and (c) respectively. The following sections will provide a detailed explanation of these techniques."}, {"title": "1. Invoking existing knowledge for domain enhancement", "content": "During the training process, a general-purpose foundation model may have already enclosed domain knowledge. Prompt tuning improves prompts to better invoke the model's inherent domain knowledge, where \"tuning\" refers to the optimization of prompts. Specifically, it involves inserting a carefully crafted prompt into the input data as context to improve the generated output. These carefully crafted prompts can be natural language descriptions, examples, rules, or other text or embedding vectors that guide the model to understand the task requirements. When generating outputs, the model will consider these carefully crafted prompts to produce task-relevant results. Prompt tuning can be categorized into hard prompts and soft prompts:"}, {"title": "(a) Hard prompts", "content": "Hard prompt methods are common techniques in natural language processing (NLP). They guide the language model's output using interpretable and reusable handcrafted words and tokens. Hard prompts are typically manually designed and tailored for specific tasks, making them difficult to modify. PET (Pattern Exploiting Training) [62] is a classic hard prompt learning method that models questions as cloze tasks and optimizes the final output words. This method trains the model on a small amount of supervised data and performs ensembling predictions on unsupervised data to guide the model."}, {"title": "(b) Soft prompts", "content": "Designing hard prompts requires experimental exploration and expertise, and manually designed prompts may not align well with model's data processing methods. To simplify this process and enhance the flexibility of prompt tuning, researchers proposed soft prompt-based tuning methods. Prefix Tuning [63] is a form of soft prompt tuning that adapts to specific downstream tasks by adding learnable prefix vectors (soft prompts) to the beginning of the input sequence. These prefix vectors, as part of the input, guide the model's output to meet task requirements. The advantage of prefix tuning is that it only updates these prefix vectors rather than the model's parameters, significantly reducing computational and storage resource demands while retaining the rich knowledge learned by the pre-trained model."}, {"title": "2. Knowledge embedding for domain enhancement", "content": "When the existing knowledge of a general-purpose foundation model is insufficient to solve domain tasks, introducing new knowledge by embedding additional background information can achieve higher quality output. This method is known as knowledge embedding for domain enhancement."}, {"title": "(a) Knowledge embedding by prompts", "content": "Prompts, serving as a direct interface between the user and the large language model, can be used to incorporate domain knowledge. However, the method of knowledge embedding through prompts has a significant limitation: the amount of embedded domain knowledge is restricted by the maximum prompt length of the model. The limitation on the model's ability to process longer text inputs stems from three core issues of the Transformer architecture:\n*   Limitations of positional encoding: Transformer models typically generate fixed-length positional encodings using sine and cosine functions, where each position in the sequence is uniquely encoded. However, when the sequence length exceeds the maximum length used during training, the model cannot effectively handle the additional text because it cannot generate valid encodings for the new positions.\n*   Resource consumption of the attention mechanism: The attention mechanism is the core of Transformer models, allowing the model to compute attention weights for each element in the sequence. However, as the sequence length increases, the computational complexity and memory requirements of this mechanism grow quadratically, leading to significant resource consumption.\n*   Long-distance dependency issue: When handling long sequences, the Transformer needs to span a large number of input tokens, which often results in problems such as gradient vanishing or exploding. This makes it difficult for the model to capture dependencies between elements that are far apart in the sequence.\nTo address the above issues, Lossless Long Text technology has emerged. It aims to enhance the model's ability to process long texts that exceed its input length limitations, allowing users to input a large amount of domain knowledge directly through prompts into the large language model as contextual information for domain enhancement. Lossless Long Text technology expands the long text input capability of large language models in two directions: extrapolation and interpolation:\ni.  Extrapolation: Extrapolation involves extending the model's context window to handle new texts that exceed the length of the training data. This typically involves improving the positional encoding mechanism so the model can understand and process longer sequences. Longformer [78] extends the ability to handle long texts effectively by combining local and global attention mechanisms; BigBird [79] uses sparse attention mechanisms and reversible layers to extrapolate the model's long-sequence processing capabilities; LongRoPE [65] improves positional encoding by introducing rotational transformations in self-attention, allowing the model to handle long-distance dependencies and support inputs up to two million tokens without impacting computational efficiency.\nii. Interpolation: Interpolation refers to enhancing the model's ability to process long texts within its existing sequence length capacity by adjusting and optimizing the attention mechanism. This typically involves improvements to the attention mechanism so the model can more effectively handle long-distance information. The BERT model [7] enhances text understanding through pre-training with a bidirectional Transformer. XLNet [80]"}, {"title": "(b) Knowledge embedding by external knowledge base", "content": "In practical applications, users may not be able to provide sufficient domain knowledge to enhance a general-purpose foundation model. To address this issue, model deployers can augment the general-purpose foundation model with a dedicated domain knowledge base. This method allows the general-purpose foundation model to reference this external knowledge base when generating answers or performing tasks, thereby obtaining the necessary domain information and context to provide more accurate and targeted responses or solutions. Retrieval-Augmented Generation (RAG) [1,67,81] technology was developed to achieve this purpose. RAG technology aims to enhance the language model's generation capabilities by leveraging an external document base without retraining the model. It is particularly suitable for tasks requiring a customizable dynamic knowledge base, such as question answering, text summarization, and fact-checking. The core of RAG technology is the integration of a retrieval component that can quickly find information relevant to the current task in a large document database during the generation process. Once the relevant documents are retrieved, this information is used as additional contextual information to aid the generation process. The advantage of RAG technology is that it combines the generation capabilities of large language models with the knowledge provided by external retrieval systems, without requiring domain knowledge themselves. Furthermore, because the external knowledge base can be replaced as needed, RAG technology offers high flexibility and adaptability.\nIn RAG, the two core challenges are how to retrieve useful information from the database and how to organize that information to augment the generation of foundation models. For the retrieval task, the earliest and most naive approach uses sparse retrieval that directly matches data based on raw data like BM25 [82]. Inspired by the field of information retrieval, dense retrieval like DPR [83] has been proposed, which projects raw data into a high-dimensional space, potentially helping to capture semantic similarity better. However, a significant challenge is that even if the retrieved data is highly related to the original input in the semantic space, we cannot guarantee that it will help the model generation, due to issues like polysemy. To address this problem, some researchers have introduced other technologies like knowledge graphs to aid the retrieval process [68]. [84] showed that adding noise can help enhance the model performance compared to traditional dense retrieval methods. Additionally, the organization of the retrieved information can also directly influence the output quality. For example, providing a ranking of the retrieved data [69] can improve model performance.\nWhile the aforementioned technologies were initially proposed to enhance large language models in specific domains, their applications are not limited to language models. With the development of foundation model domains, these technologies are expected to be extended to foundation models of other modalities."}, {"title": "3.1.2 Domain-specific enhancement based on fine-tuning", "content": "When plug-and-play domain enhancement techniques are difficult to implement or require embedding too much domain knowledge into the general-purpose foundation model, or when deep modifications to the general-purpose foundation model are necessary, we can turn to the strategy of domain enhancement based on fine-tuning. This strategy aims to customize the required domain-specific foundation model while preserving the pre-trained knowledge of the general-purpose foundation model as much as possible by specific domain enhancement [85].\nFine-tuning techniques can be divided into three main types: adapter-based fine-tuning, low-rank matrix decomposition-based fine-tuning, and full-parameter fine-tuning. In following sections, this paper will elaborate on these techniques, sorted from low to high resource requirements and complexity needed for fine-tuning."}, {"title": "1. Adapter-based fine-tuning", "content": "Adapter-based fine-tuning [70] is a method that inserts small trainable adapter modules into pre-trained models, aiming to efficiently adapt the model to specific downstream tasks. During fine-tuning, only parameters of adapter modules are updated, while the original parameters of the pre-trained model remain unchanged, reducing computational resources and storage requirements while retaining the rich knowledge learned by the model during pre-training. AdapterFusion [71] is an extension of adapter-based fine-tuning that allows the model to learn multiple tasks or adapt to various data distributions simultaneously by fusing multiple adapter modules, with each adapter module focusing on capturing task-specific features. Infused Adapter by Inhibiting and Amplifying Inner Activations (IA3) [72] scales the activation layers by injecting learned vectors into the attention and feed-forward modules of the Transformer architecture. Since these learned vectors are only trainable parameters during fine-tuning, IA3 significantly reduces the number of trainable parameters compared to traditional adapter-based fine-tuning, thereby reducing training costs and improving training efficiency. Additionally, IA3 does not introduce inference latency because its adapter weights can be merged with the foundation model while maintaining the flexibility and adaptability of the model, allowing customized fine-tuning for different tasks and datasets. [73] proposes a method called Model Reprogramming that converts both the input and output domains of a foundation model by adding two adapter layers, enabling more flexible adaptation functionality."}, {"title": "2. Low-rank matrix decomposition based fine-tuning", "content": "Fine-tuning based on low-rank matrix decomposition reduces the number of parameters that need to be updated by decomposing the weight matrices in the pre-trained model into the product of low-rank matrices. Low-rank matrix decomposition can capture the most important information in the weight matrices while keeping the original pre-trained parameters unchanged during fine-tuning, updating only low-rank decomposition matrices, thus reducing the computational and storage requirements during fine-tuning. This method improves fine-tuning efficiency while maintaining or approaching the performance of full-parameter fine-tuning."}, {"title": "(a) Low-Rank Adaptation (LoRA)", "content": "Low-Rank Adaptation (LoRA) [74] achieves efficient fine-tuning performance by decomposing model parameters into the product of low-rank matrices using singular value decomposition (SVD). The principle of LoRA can be represented by the following equation:\n$W_{new}=W_{old} + \\Delta W,$\nwhere $W_{old}$ is the original weight matrix, and $\\Delta W$ is the low-rank update matrix, which can be constructed by selecting the singular vectors corresponding to smaller singular values of $W_{old}$. The singular value decomposition of $W_{old}$ is given by:\n$\\Delta W = UE \\text{V}^\\text{T}.$\nHere, $U$ and $V$ are matrices obtained from the SVD of $W_{old}$, and $\\Sigma$ is a diagonal matrix containing the most important singular values of $W_{old}$. By updating only the parameters in $U$, $\\Sigma$, and $V$, LORA efficiently fine-tunes the model."}, {"title": "One limitation of LoRA", "content": "is that it typically applies the same low-rank structure to all layers, ignoring the varying importance of different layers and parameters for downstream tasks. Adaptive Low-Rank Adaptation (AdaLoRA) [86] is an improved method based on LoRA, which adaptively determines which layer parameters need to be updated. By employing adaptive learning rates and task-specific parameter adjustment strategies, AdaLoRA enables the model to automatically adjust the intensity and scope of fine-tuning according to the specific requirements of the task.\nResearchers have also found that LoRA's continuous pre-training performance is unsatisfactory on some large-scale datasets. Thus, Layerwise Importance Sampled AdamW (LISA) [87] strategy is proposed, where the weight norm distribution of different layers exhibits uncommon skewness. LISA adopts importance sampling strategy by randomly activating different layers in the foundation model for optimization. Specifically, LISA consistently updates the bottom embedding layers and the top linear head while randomly updating a small number of intermediate self-attention layers. This method, with memory consumption comparable to LORA, outperforms LoRA and even full-parameter fine-tuning in various downstream fine-tuning tasks."}, {"title": "(b) Low-Rank Hadamard Product (LoHa)", "content": "Low-Rank Hadamard Product (LoHa) [75] updates the model's weights by introducing the Hadamard product of low-rank matrices. The principle of LoHa can be represented by the following equation:\n$W_{new} = W_{old} \\oplus \\Delta W,$\nwhere $W_{old}$ is the original weight matrix, and $\\Delta W$ is the update matrix approximated by a low-rank matrix. The update matrix $\\Delta W$ can be further decomposed into the Hadamard product of two low-rank matrices:\n$\\Delta W = L_1 \\circ L_2.$\nHere, $L_1$ and $L_2$ are two low-rank matrices that adjust elements of the original weight matrix by learning key information extracted from the input data. By updating only parameters in $L_1$ and $L_2$, LoHa achieves efficient fine-tuning of the model while maintaining its adaptability to new tasks."}, {"title": "(c) Low-Rank Kronecker Product (LoKr)", "content": "Low-Rank Kronecker Product (LoKr) [76] is another parameter-efficient fine-tuning method that emerged after LoHa. LoKr utilizes the properties of the Kronecker product to expand the dimensions of the weight matrix while keeping the increase in the number of parameters within a manageable range. The Kronecker product allows the model to learn complex interactions across different dimensions, which is particularly useful for capturing high-order relationships in the input data. The updating process of LoKr can be represented as:\n$W_{new} = W_{old} + \\Delta W,$\nwhere $\\Delta W$ is the Kronecker product of two low-rank matrices $L_1$ and $L_2$:\n$\\Delta W = L_1 \\otimes L_2.$\nIn this formula, $L_1$ and $L_2$ are two low-rank matrices, and they compute a large matrix that is updated during the fine-tuning process through Kronecker product. LoKr is particularly suitable for tasks that require increasing the model's dimensions to capture more complex relationships, while maintaining similar parameter efficiency to LoHa. However, LoKr may require more complex mathematical operations to handle the Kronecker product, and in some cases, its computational cost may be higher than that of LoHa."}, {"title": "3. Full fine-tuning", "content": "Full fine-tuning is not constrained by pre-training tasks or data distributions, making it flexible to adapt to various downstream tasks. It allows the model to be directly optimized end-to-end on the data of the final task without the need for additional adapter modules. However, because it requires updating all parameters in the model, full fine-tuning demands significant computational resources and longer training times. Moreover, foundation models have a vast number of"}, {"title": "3.2 Customization of the foundation model based on pre-trained modules", "content": "Foundation models may contain millions or even billions of parameters, and through transfer learning, it's possible to reduce parts of the model that need training, thereby significantly lowering training costs. This approach is known as the customization of the foundation model based on pre-trained modules. The essence of transfer learning lies in utilizing the knowledge embedded in model parameters during the pre-training process when constructing a new model. As mentioned earlier, in the architecture of foundation models, there are typically five main modules: modality encoders, backbone calculators, modality decoders, input projectors, and output projectors. Among them, the modality encoder, backbone calculator, and modality decoder carry a large amount of knowledge as they are directly involved in the encoding, processing, and decoding of data. In contrast, the input projector and output projector themselves carry less model knowledge. In some cases, they may not even have explicit models responsible for these functions, or these modules are only trained when building new foundation models. Therefore, when customizing foundation models, we generally do not choose to transfer the input projector and output projector.\nNext, this paper will detail how to customize foundation models based on the pre-trained modality encoder, backbone calculator, and modality decoder. Through this approach, we can effectively leverage the knowledge of pre-trained models while reducing the computational resources demand, making the model more suitable for specific tasks and environments."}, {"title": "3.2.1 Transfer learning based on pre-trained modality encoders", "content": "General-purpose foundation models often adapt to the distribution characteristics of a vast dataset during training, internalizing ample domain knowledge in their model parameters. They are thus well-suited as feature extraction modules for customized foundation models. By utilizing the pre-trained model's pre-existing feature extraction module as the modality encoder for domain data, downstream task modules can be seamlessly integrated after this module to fulfill task requirements. The modality encoder encapsulates knowledge about crucial features of the data. There are two main approaches to transfer learning for obtaining modality encoders:\n*   Transfer within the same modality across different data domains: This approach involves transferring the knowledge of the modality encoder trained on the source data domain to the target data domain. It typically entails aligning the feature distribution of the source and target domain data. Fine-tuning the pre-trained modality encoder from the source domain on the target domain enables it to adapt to the characteristics of the new data. Specifically, this can be achieved by adjusting or adding layers to the encoder. Additionally, domain adaptation techniques such as domain adversarial training or domain-invariant feature extraction techniques can be employed to reduce the distribution discrepancy between the source and target domains.\n*   Transfer across different modalities: When it comes to multi-modality foundation models, there are often cases that lacking of corresponding pre-trained encoders for certain modalities. In such situations, a cross-modality transfer strategy can be utilized, adapting encoders of other modalities to process new data modalities. For instance, the authors of ImageBind treat depth and thermal imaging data as a form of single-channel images, thereby using image encoders to extract features for thermal data. Initializing the model with weights pre-trained on image datasets can lead to faster convergence compared to random initialization and can enhance generalization to some extent."}, {"title": "3.2.2 Transfer learning based on pre-trained backbone calculator", "content": "For multi-modality foundation models, the backbone calculator is the core computational component responsible for processing encoded feature vectors and performing tasks such as classification and generation. The backbone calculator of customized foundation models can be transferred from pre-trained models to leverage the complex feature processing and task execution capabilities learned from large-scale datasets. This approach avoids training the backbone calculator from scratch but still requires"}, {"title": "constructing appropriate previous modules to encode the data into feature vectors that can be processed by the backbone calculator", "content": "For example, NEXT-GPT [26] converts raw data from various modalities into language modality feature vectors before feeding them into a pre-trained LLM, allowing the model to process inputted tokens according to task requirements. There are two main approaches to transferring pre-trained backbone calculators:\n*   Transfer of a single pre-trained backbone calculator: Utilizing a general-purpose foundation model (e.g., LLaMA) as the backbone calculator to process data from the central modality. When transferring a pre-trained backbone calculator to a specific domain application, it typically requires fine-tuning to adapt to specific data characteristics and task requirements of that domain. This step can be performed on a limited domain dataset, fine-tuning the model parameters to optimize its processing capabilities for the domain-specific data.\n*   Modular combination of multiple pre-trained backbone calculators: Modular combination is a flexible design method in deep learning architectures that allow integrating multiple specialized pre-trained models into a unified framework based on task requirements. The Mixture of Experts (MOE) model [88] can serve as an effective mechanism to further optimize this modular combination. The MoE model introduces multiple expert networks and uses a gating mechanism and mixing strategy to dynamically select and combine outputs of these experts, enabling specialized processing for different tasks or data subsets.\nThe primary function of the gating mechanism is to determine how input data should be distributed among different experts. It generates a weight or score for each expert based on the characteristics of the input data, reflecting each expert's capability or suitability for processing the current input. The output of the gating mechanism is typically used to guide the mixing strategy, indicating the importance of each expert for the current input. The mixing strategy then combines the outputs of multiple experts according to specific rules to generate the final model output. This strategy can be simple, such as averaging or weighted averaging, or more complex, involving probability distributions of model outputs or other advanced methods.\nFor example, for complex tasks requiring both image recognition and language understanding, one expert network might excel at identifying edges in images, while another expert network might be adept at understanding semantic relationships in natural language. The gating mechanism of the MoE model can automatically adjust the participation level of each expert network based on the characteristics of the input data and the task requirement, which allows the model to flexibly invoke the most appropriate expert network when dealing with mixed visual and language inputs, achieving optimal performance. Moreover, MoE models are highly scalable. They can adapt to new task requirements or data types by adding new expert networks and updating the gating mechanism, making them suitable for constructing flexible customized foundation models."}, {"title": "3.2.3 Transfer learning based on pre-trained modality decoders", "content": "Modality decoders play a crucial role in multi-modality foundation pre-trained models. They are responsible for converting processed feature vectors back into the form of the original data. In generative tasks, such as converting text to images or audio to text, modality decoders need not only to accurately decode the feature vectors to reconstruct understandable original data but also to exhibit a certain level of creativity. Some pre-trained modality decoders can also understand and process multi-modality feature inputs. For instance, CoDi-2 can utilize both text and audio as conditions to control image generation. By transferring such pre-trained decoders, there is no need to train complex decoder structures from scratch, allowing them to be directly applied to image generation tasks.\nHere are methods to effectively utilize pre-trained modality decoders for transfer learning:\n1.  Fine-tuning pre-trained modality decoders: Similar to modality encoders, modality decoders can be fine-tuned on a specific task's dataset to adapt to new task requirements. This process usually involves adjusting the last few layers of the decoder or adding new layers to better capture the data characteristics of the specific domain.\n2.  Transferring cross-modality generative modality decoders: In cross-modality generation tasks, pre-trained modality decoders can be directly used to generate data in the target modality. The conditional information is first encoded into feature vectors through a conditional encoder, and then combined with feature vectors of the original data to achieve conditional generation. The"}, {"title": "3.3 Construction of the foundation model without pre-trained modules", "content": "When it is not possible to construct a foundation model through transferring from pre-trained models", "modules": "the modality encoder, the backbone calculator, and the modality decoder. For example, in LLaMA-2 [11", "input raw text -> input text feature vectors -> output text feature vectors -> output raw text\". Additionally, Bai et al. [13": "introduces the concept of visual sentences, and proposes a large visual model (LVM) that can autoregressively generate the required image based on visual sentences. It realizes in-context learning within the pure image modality, which enables the model to directly infer tasks from image modality prompts and generate corresponding results. This not only explores the potential of pure visual input but also provides a new perspective for constructing domain-specific foundation models - the central modality does not have to be limited to language but any modality widely used in a specific domain.\nMulti-modality foundation models require the additional input projectors and output projectors to achieve modality alignment. For instance, CoDi-2 [21", "25": "to process input data, aligning all corresponding modalities to the image modality. Then, the feature vectors of the image modality are transformed into the feature space of the language modality through a multi-layer perceptron (MLP). In detail, it uses the pre-trained autoregressive Transformer of the LLM LLaMA-2-7b-chat-hf as the backbone calculator's foundation. The image and audio features processed by the backbone calculator are converted back to the image domain via two MLPs. And they are then used as control vectors input of a Diffusion-based generative model to obtain the final image and text results. The training losses include text generation loss, modality conversion loss, and data generation loss. So that the multi-modality feature processing capability of the backbone calculator and the modality conversion capability of the two MLPs can be trained at the same time by an end-to-end way. The model's modality alignment is reflected in two terms. On one hand, the model aligns the feature vectors of multiple modalities to the image modality through the pre-trained modality encoders of ImageBind. On"}]}