{"title": "Learning to Model Graph Structural Information on\nMLPs via Graph Structure Self-Contrasting", "authors": ["Lirong Wu", "Haitao Lin", "Guojiang Zhao", "Cheng Tan", "Stan Z. Li"], "abstract": "Recent years have witnessed great success in han-\ndling graph-related tasks with Graph Neural Networks (GNNs).\nHowever, most existing GNNs are based on message passing to\nperform feature aggregation and transformation, where the struc-\ntural information is explicitly involved in the forward propagation\nby coupling with node features through graph convolution at each\nlayer. As a result, subtle feature noise or structure perturbation\nmay cause severe error propagation, resulting in extremely poor\nrobustness. In this paper, we rethink the roles played by graph\nstructural information in graph data training and identify that\nmessage passing is not the only path to modeling structural infor-\nmation. Inspired by this, we propose a simple but effective Graph\nStructure Self-Contrasting (GSSC) framework that learns graph\nstructural information without message passing. The proposed\nframework is based purely on Multi-Layer Perceptrons (MLPs),\nwhere the structural information is only implicitly incorporated\nas prior knowledge to guide the computation of supervision\nsignals, substituting the explicit message propagation as in GNNs.\nSpecifically, it first applies structural sparsification to remove\npotentially uninformative or noisy edges in the neighborhood,\nand then performs structural self-contrasting in the sparsified\nneighborhood to learn robust node representations. Finally,\nstructural sparsification and self-contrasting are formulated as\na bi-level optimization problem and solved in a unified frame-\nwork. Extensive experiments have qualitatively and quantitatively\ndemonstrated that the GSSC framework can produce truly en-\ncouraging performance with better generalization and robustness\nthan other leading competitors. Codes are publicly available at:\nhttps://github.com/LirongWu/GSSC.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, the emerging Graph Neural Networks (GNNs)\nhave demonstrated their powerful capability in handling graph-\nrelated tasks [1-6]. Despite their great success, most existing\nGNNs are based on message passing, which consists of two\nkey computations: (1) AGGREGATE: aggregating messages\nfrom its neighborhood, and (2) UPDATE: updating node\nrepresentation from its representation in the previous layer\nand the aggregated messages. Due to the explicit coupling\nof node features and graph structural information in the\nAGGREGATE operation, subtle feature noise or structure per-\nturbation may cause severe error propagation during message\npassing, resulting in extremely poor robustness [7-10]. There\nhas been some pioneering work [11, 12] delving into the\nnecessity of message passing for modeling graph structural\ninformation. These methods adopt a pure MLP architecture\nthat is free from feature-structure coupling, where structural\ninformation is only implicitly used to guide the computation\nof downstream supervision. For example, Graph-MLP [11]\ndesigns a neighborhood contrastive loss to bridge the gap\nbetween GNNs and MLPs by implicitly utilizing the adja-\ncency information. Besides, LinkDist [12] directly distills self-\nknowledge from connected node pairs into MLPs without the\nneed for aggregating messages. Despite their great progress,\nthese MLP-based models still cannot match the state-of-the-art GNNs in terms of classification performance due to the\nunderutilization of graph structural information. Therefore,\n\u201chow to make better use of graph structural information\nwithout message passing\" is still a challenging problem.\nThe quality of graph structural information plays a very\nkey crucial in various graph learning algorithms, making\ngraph sparsification techniques, such as DropEdge [13], STR-\nSparse [8] and GAUG [14], etc., emerge as a common means\nto improve performance. The goal of graph sparsification\nis to filter out noisy structural information, i.e., finding an\ninformative subgraph from the input graph by removing noisy\nedges. However, most existing graph sparsification methods\nare tailored for general GNNs, and they jointly optimize\ngraph sparsification and GNN training by back-propagation of\ndownstream supervision in an end-to-end manner. However,\nsuch an optimization may work for GNNs but is hard to\nbe extended directly to MLP-based models, where structural\ninformation has been used to guide the computation of down-\nstream supervision, and it is not feasible to use downstream\nsupervision in turn to optimize the structural sparsification.\nThe differences between the two cases of using graph\nsparsification techniques for GNNs and MLP-based models\nare illustrated in Fig. 1, where the key point is whether graph\nstructural information is used explicitly or implicitly. For GNN\nmodels, the structural information G is explicitly involved in\nthe forward propagation, coupled with node features through\ngraph convolution at each layer, and thus we can directly\nobtain the gradients, i.e., the derivative of supervision loss\nw.r.t the sparsification parameters. As a result, the parameters\nof GNN and graph sparsification can be jointly optimized by\ndownstream supervision. In contrast, MLP-based models only\nimplicitly utilize structural information G to guide the compu-\ntation of downstream supervision signals (denoted as a dashed\nline), and thus the derivative of supervision loss w.r.t the\nsparsification parameters is not available, which prevents the\nsparsification network from being directly optimized through\ndownstream supervision (denoted as a red cross). As an\nalternative, this paper proposes a homophily-oriented objective\nto guide the optimization of the sparsification network."}, {"title": "II. RELATED WORK", "content": "Recent years have witnessed the great success of GNNs in\ngraph learning [15-19]. There are two categories of GNNs:\nspectral GNNs and spatial GNNs. The spectral-based GNNs\ndefine convolution kernels in the spectral domain based on\nthe graph signal processing theory. For example, ChebyNet\n[20] uses the polynomial of the Laplacian matrix as the\nconvolution kernel to perform message passing, and GCN is\nits first-order approximation. The spatial-based GNNs focus\non the design of aggregation functions directly. For example,\nGraphSAGE [21] employs a generalized induction framework\nto efficiently generate node embeddings for previously unseen\ndata by aggregating known node features. GAT [22], on the\nother hand, adopts the attention mechanism to calculate the\ncoefficients of neighbors for better information aggregation.\nWe refer interested readers to the recent survey [2] for more\nGNN variants, such as SGC [23], APPNP [24] and DAGNN\n[1]. However, the above GNNs all share the de facto design\nthat structural information is explicitly utilized for message\npassing to aggregate node features from the neighborhood.\nRecently, there are some recent attempts to train graph data\nby combining contrastive learning and knowledge distillation\n[25, 26] with MLPs. For example, Graph-MLP [11] designs a\nneighborhood contrastive loss to bridge the gap between GNNS\nand MLPs by implicitly utilizing the adjacency information.\nInstead, LinkDist [12] directly distills self-knowledge from\nconnected node pairs into MLPs without the need to aggregate\nmessages. Despite their great progress, they still cannot match\nthe state-of-the-art GNN models in terms of classification\nperformance, more importantly, they ignore the potential noise\nin the graph structure. Another MLP-based model is Graph\nMLP-Mixer [27], which generalizes ViT/MLP-Mixer to graph\ndata and have achieved promising results.\nGraph transformers (GTs) is another research area closely\nrelated to graph representation learning. For example, [28] is\nthe first work that generalizes Transformer to graphs, which\nuses Laplacian position encoding to preserve local structural\ninformation. GraphiT [29] utilizes relative position encoding to\nenhance attention mechanism. Recently, GRIT [30] proposes\na novel structural encoding called relative random walk prob-\nabilities (RRWP) to enhance local structure expressive power.\nBesides, Graph Transformers (SGFormer) [31] simplifies and\nempowers Transformers, which requires none of positional\nencodings, feature/graph pre-processing, or augmented loss.\nFor more GT architectures, please refer to recent survey [32]."}, {"title": "B. Graph Sparsification", "content": "The robust learning on graphs includes adversarial training\n[9, 33, 34], label denoising [35], structure learning [36-38],\netc., among which the closest one to ours is graph sparsifica-\ntion, which aims to find a small subgraph from the input graph\nthat best preserve some desired properties. Existing graph\nsparsification techniques can be divided into two categories:\nunsupervised and supervised. The unsupervised sparsifica-\ntion techniques, such as Spectral Sparsifier (SS) [39] and\nRank Degree (RD) [40], mainly deal with simple graphs by\nsome pre-defined graph metrics, e.g., node degree distribution,\nclustering coefficient, etc. Besides, DropEdge [13] randomly\nremoves a fraction of edges according to the pre-defined\nprobability before each training epoch to get sparsified graphs.\nIn contrast, supervised sparsification directly parameterizes\nthe sparsification process and optimizes it end-to-end along\nwith GNN parameters under downstream supervision. For\nexample, Learning Discrete Structure (LDS) [41] works under\na transductive setting and learns Bernoulli variables associated\nwith individual edges. Besides, GAUG [14] first optimizes the\ngraph structure learning and GNN parameters in an end-to-end\nmanner and then directly removes some low-importance edges\nduring training. Moreover, NeuralSparse [8] proposes a general"}, {"title": "III. METHODOLOGY", "content": "Used Notations. Given a graph G = (V,E), where V is the\nset of N nodes with features X = [X\u2081, X\u2082,\u2026, XN] \u2208RN\u00d7d\nand & denotes the edge set. Each node v\u1d62 \u2208 V is associated\nwith a d-dimensional features vector x\u1d62, and each edge e\u1d62,\u2c7c E\nE denotes a connection between node v\u1d62 and v\u2c7c. The graph\nstructure can also be denoted by an adjacency matrix A \u2208\n[0,1]N\u00d7N with A\u1d62,\u2c7c = 1 if e\u1d62,\u2c7c \u2208 E and A\u1d62,\u2c7c = 0 if e\u1d62,\u2c7c & E.\nNode classification is a typical node-level task where only a\nsubset of node V\u2081 with corresponding labels Y\u0141 are known,\nand we denote the labeled set as D\u2081 = (V\u0141, Y\u0141) and unlabeled\nset as D\u1d64 = (V\u1d64, Y\u1d64), where V\u1d64 = V\\V\u0141. The task of node\nclassification aims to learn a mapping p(Y | X, A) on labeled\ndata D\u0141, so that it can be used to infer the label Y \u2208 Y\u1d64."}, {"title": "A. Theoretical Justification", "content": "From the perspective of statistical learning, the key of node\nclassification is to learn a mapping p(Y | X, A) based on\nnode features X and adjacency matrix A. However, instead\nof directly working with the original graph, we would like\nto leverage sparsified subgraphs to remove task-irrelevant\ninformation and learn more robust representations. In other\nwords, we are interested in the variant as follows\n$$p(Y | X, A) = \\sum_{g\u2208S_g} p(Y | X, g)p(g | X, A),$$(1)\nwhere g\u2208 Sg is a sparsified subgraph of original graph G.\nIn practice, the distribution space size of Sg is 2|E|, and it\nis intractable to enumerate all possible g as well as estimate\nthe exact values of p(Y | X, g) and p(g | X, A). Therefore,\nwe turn to approximate the distributions by two tractable\nparameterized functions q\u03b8(\u00b7) and q\u03c6(\u00b7), as follows\n$$p(Y | X, A) = \\sum_{g\u2208S_g} q_\u03b8(Y | X, g)q_\u03c6(g | X, A),$$(2)\nwhere q\u03b8(\u00b7) and q\u03c6(\u00b7) are approximation functions for p(Y |\nX,g) and p(g | X, A). Moreover, to make the above graph\nsparsification process differentiable, we employ the commonly\nused reparameterization tricks [49] to transform the discrete\ncombinatorial optimization problem to a continuous proba-\nbilistic generative model, as follows\n$$\\sum_{g\u2208S_g} q_\u03b8(Y | X, g)q_\u03c6(g | X, A) \\propto \\sum_{g'\u223cq_\u03c6(g|X,A)} q_\u03b8(Y | X, g'),$$(3)\nwhere g' is a subgraph sampled from q\u03c6(g | X, A).\nTo optimize Eq. (3), a bi-level optimization framework\nis adopted to alternate between learning q\u03c6(g | X, A) and\nq\u03b8(Y | X, g'). In addition to the downstream supervision for\nlearning q\u03b8(Y | X, g'), another objective function H(\u00b7) is\nrequired to optimize q\u03c6(g | X, A). Intuitively, H(\u00b7) should\nbe an unsupervised metric to evaluate the quality of the\nsparsified graph g'. Graph homophily, as an important graph\nproperty, may be a desirable option for H(\u00b7).\nIntroduction on Graph Homophily: The graph homophily is\ndefined as the fraction of inter-class edges in a graph,\n$$r = \\frac{|{(i, j) : l_{i,j} \u2208 E \u2227 Y_i = Y_j}|}{|E|}$$(4)\nThe homophily ratio of the sparsified subgraph g' may be a\ndesirable option for H(\u00b7) for the following three reasons: (1)\nMost common graphs adhere to the principle of homophily,\ni.e., \u201cbirds of a feather flock together\", which suggests\nthat connected nodes often belong to the same class, e.g.,\nfriends in social networks often share the same interests or\nhobbies. (2) It has been shown in previous work [50] that\ncommon GNN classifiers, such as GCN and GAT, usually\nperform better on datasets with higher homophily ratios. (3)\nWhen downstream supervision is not accessible, the prior\nknowledge of graph homophily can serve as strong guidance\nfor searching the suitable sparsified subgraph.\nProblem Statement: In this paper, the three important issues\non framework design can be summarized as:\n\u2022 Implementing sparsification network q\u03c6(g | X, A) that\ntakes node features X and adjacency matrix A as inputs\nto generate a sparsified subgraph g'. (Sec. III-B)\n\u2022 Implementing self-contrasting network q\u03b8(Y | X, g') that\ntakes node features X and the sparsified subgraph g' as\ninputs to make predictions Y. (Sec. III-C)"}, {"title": "B. Structural Sparsification Network", "content": "The sparsification network aims to generate a discrete spar-\nsified subgraph g' for graph G as shown in Fig. 2, serving as\nthe approximation function q\u03c6(g | X, A). Therefore, we need\nto answer three questions about the sparsification network:\n(1) How to model the sparsification distribution? (2) How\nto differentiably sample discrete sparsified subgraphs from\nthe learned sparsification distribution? (3) How to optimize\nthe sparsification distribution and subgraph sampling process?\nNext, we first answer Question (1)(2) and defer the discussion\nof optimization strategy until Sec. III-D.\n1) Sparsification Distribution: To model the sparsification\ndistribution, we introduce a set of latent variables A =\n{\u03bb\u1d62,\u2c7c|e\u1d62,\u2c7c \u2208 E}, where \u03bb\u1d62,\u2c7c \u2208 [0,1] denotes the sparsifica-\ntion probability between node v\u1d62 and v\u2c7c. We can estimate\n\u03bb\u1d62,\u2c7c directly with distribution q\u03c6 (x\u1d62,\u2c7c | h\u1d62,\u2c7c) prameterized by\nh\u1d62,\u2c7c \u2208 RF. However, directly fitting each q\u03c6 (i,j | Hi,j)\nlocally involves solving |E|F parameters, which increases the\nover-fitting risk given the limited labels in the graph. Thus,\nwe consider the amortization inference [51], which avoids\nthe optimization of parameter p\u1d62,\u2c7c for each local probability\ndistribution q\u03c6 (x\u1d62,\u2c7c | h\u1d62,\u2c7c) and instead fits a shared neural\nnetwork to model parameterized posterior. Specifically, we\nfirst transform the input to a low-dimensional hidden space,\ndone by multiplying the features of input nodes with a shared\nparameter matrix W \u2208 RF\u00d7d, that is, z\u1d62 = Wx\u1d62. Then, we\nparameterize the sparsification distribution \u03bb\u1d62,\u2c7c as follows:\n$$q_\u03c6 (\u03bb_{i,j} | X, A) = \u03c3 (z_i z_j^\\top),$$\nwhere \u03c3(\u00b7) is an element-wise sigmoid function.\n2) Subgraph Sampling: To sample discrete sparsified\nsubgraphs from the learned sparsification distribution and\nmake the sampling process differentiable, we adopt Gumbel-\nSoftmax sampling [49], which can be formulated as follows\n$$g_{i,j} = \\frac{1}{1 + exp(\\frac{log M_{i,j} + G}{\u03c4})} + \\frac{1}{2} \\qquad where i \u2208 V, j \u2208 N_i$$(6)\nwhere M\u1d62,\u2c7c = (1 \u2212 \u03b1)q\u03c6 (\u03bb\u1d62,\u2c7c | X, A) + \u03b1A\u1d62,\u2c7c is defined\nas the learned structural sparsification strategy. In addition,\n\u03b1\u2208 [0,1] is the fusion factor, which aims to prevent the\nsampled sparsified subgraph g' from deviating too much from\nthe original graph. Besides, \u03c4 is the distribution temperature,\nand G ~ Gumbel(0, 1) is a gumbel random variate.\nNext, we will discuss in detail how to model the STR-\nContrast network q\u03b8 (Y | X, g') based on the node features\nX and the sampled sparsified subgraph g'. Without loss of\ngenerality, we can denote the edge set of the sparsified graph\ng' as E' to distinguish E of the original graph G."}, {"title": "C. Structural Self-Contrasting Network", "content": "Not involving any explicit message passing, the structural\nself-contrasting network treats the structural information im-\nplicitly as prior to guide the computation of supervision\nsignals. In this section, we introduce the structural self-\ncontrasting network from the following three aspects: (1)\nbackbone architecture design, including an MLP and two\nprediction heads; (2) objective function design, how to self-\ncontrast between the target node v\u1d62 and its neighboring node\nv\u2c7c \u2208 V\u1d62; (3) optimization difficulty and strategy, including\nhow to properly sample negative samples and support batch-\nstyle training. A high-level overview of the proposed GSSC\nframework is shown in Fig. 3.\n1) Architecture: The structural self-contrasting network is\nbased on a pure MLP architecture, with each layer composed\nof a linear transformation, an activation function, a batch\nnormalization, and a dropout function, formulated as:\n$$H^{(l)} = Dropout \\left(BN \\left(\u03c3\\left(H^{(l-1)}W^{(l-1)}\\right)\\right)\\right), \\quad H^{(0)} = X$$(7)\nwhere 1 < l < L, \u03c3 = ReLu(\u00b7) denotes an activation\nfunction, BN(\u00b7) is the batch normalization, and Dropout(\u00b7)\nis the dropout function. W(0) \u2208 Rd\u00d7F and W(l) \u2208 RF\u00d7F\n(1 \u2264 l < L \u2212 1) are layer-specific weight matrices with the\nhidden dimension F. Furtheromore, we define two additional\nprediction heads: y\u1d62 = fw(h\u207d\u1d38\u207e\u1d62) \u2208 RC and z\u1d62 = gy(h\u207d\u1d38\u207e\u1d62) \u2208\nRC, where C is the number of categories.\n2) Structural Smoothness Constraint: The structural\nsmoothness assumption indicates that connected nodes should\nbe similar, while disconnected nodes should be far away. With\nsuch motivation, we propose a structural smoothness constraint\nthat enables the model to learn the graph connectivity and dis-\nconnectivity without explicit message passing. Given a target\nnode v\u1d62, we first generate an augmented node v\u1d62\u2192\u2c7c between"}, {"title": "IV. EXPERIMENTS", "content": "The experiments aim to answer the following six questions:\n(Q1) How effective is GSSC compared to those GNNs, graph\nsparsification, and MLP-based models?\n(Q2) Is GSSC robust to label noise and structure noise?\n(Q3) How effective is the homophily-oriented objective?\n(Q4) Is STR-Sparse applicable to other GNNs and MLP-based\nmodels? How does STR-Contrast perform with other\nexisting graph sparsification methods?\n(Q5) How efficient is the model in terms of inference time?\n(Q6) How do hyperparameters affect performance?"}, {"title": "D. Optimization Strategy", "content": "node v\u1d62 and its neighboring node v\u2c7c \u2208 N\u1d62 by learnable\ninterpolation, with its node representation g\u1d62\u2192\u2c7c defined as\n$$g_{i\u2192j} = B_{i,j} g_y (h^{(L)}_i) + (1 \u2212 B_{i,j}) g_y (h^{(L)}_j),$$(8)\nwhere B\u1d62,\u2c7c = sigmoid(a [h\u207d\u1d38\u207e\u1d62 ||h\u207d\u1d38\u207e\u2c7c])\nwhere B\u1d62,\u2c7c is defined as learnable interpolation coefficients\nwith the shared weight a. Then, we take the generated\naugmented node v\u1d62\u2192\u2c7c as a positive sample and other non-\nneighboring nodes as negative samples and define the con-\nstraint between nodes v\u1d62, v\u2c7c as follows\n$$l_{i,j} = D (y_i, g_{i\u2192j}) - log \\sum_{e_{i,k} \u2208 E'} e^{D (y_i, z_k)},$$(9)\nwhere y\u1d62 = fw(h\u1d62)), zk = gy(h), and D: RC \u00d7\nRC \u2192 R is a discriminator that maps two representations\nto an agreement score and taken as Mean Square Error\n(MSE) in our implementation by default. The motivations\nwhy we adopt learnable interpolation to augment nodes is\nbased on the following judgment: compared with those non-\nneighboring nodes, the number of neighboring nodes is much\nsmaller, which makes the model overemphasize the differences\nbetween the target and non-neighboring nodes, resulting in\nimprecise class boundaries. We have demonstrated the benefits\nof node augmentation and negative samples in Table. I.\nThe total structural smoothness constraints over the edge set\nE of the sparsified subgraph g' can be defined as\n$$L_{smooth} = \\sum_{i=1}^{N} \\sum_{e_{i,j} \u2208 E'} (g'_{i,j}) \\bigg(D (y_i, g_{i\u2192j}) - log \\sum_{e_{i,k} \u2208 E'} e^{D (y_i, z_k)}\\bigg).$$(10)\n3) Optimization Difficulty and Strategy: Directly opti-\nmizing Eq. (10) is computationally expensive for two tricky\noptimization difficulties: (1) it treats all non-neighboring nodes\nas negative samples, which suffers from both data redundancy\nand huge computational burden; and (2) it performs the\nsummation over the entire set of nodes, i.e, requiring a large\nmemory space for keeping the entire graph. To address these\nproblems, we adopt the edge sampling strategy [52] for batch-\nstyle training. More specifically, we first sample a mini-batch\nof edges from the entire edge set E to construct a mini-batch\nEb \u2208 Eg. Then we randomly sample negative nodes from a\npre-defined negative sample distribution Pk(v) for each edge\ne\u1d62\u2c7c \u2208 Es instead of enumerating all non-neighboring nodes as\nnegative samples. Finally, we can rewrite Eq. (10) as follows\n$$L_{smooth} = \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{e_{i,j} \u2208 E_b} \\bigg(D(y_i, g_{i\u2192j}) + D (y_j, g_{j\u2192i})\n\u2212 E_{v_k\u223cP_k(v)} \\big(log e^{D(y_i, z_k)} + log e^{D (y_j, z_k)}\\big)\\bigg),$$(11)\nwhere B is the batch size, and v\u2096 is a random sample drawn\nfrom the pre-defined negative sample distribution Pk(v\u1d62) =\n\\frac{d_i}{\\sum_{v \u2208 V} d_v} for each node v\u1d62, where d\u1d62 is the degree of node v\u1d62.\nSimilarly, we can formulate the cross-entropy loss on the\nlabeled node set V\u0141 as a classification loss, as follows\n$$L_{cla} = \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{i\u2208V_L \u2229 V' } \\bigg(CE(Y_i, y_i) + \\sum_{e_{i,j} \u2208 E_b}CE(Y_i, y_j)\\bigg),$$(12)\nwhere V'\u2081 = {v\u1d62, v\u2c7c|e\u1d62,\u2c7c \u2208 Eb} is all the sampled nodes in\nEb, y\u1d62 = softmax(y\u0302\u1d62) \u2208 RC, and z\u0302\u2c7c = softmax(z\u0302\u2c7c) \u2208 RC.\nBesides, CE(\u00b7) is the cross-entropy loss, and Y\u1d62 is the ground-\ntruth label of node v\u1d62. The total training loss is defined as:\n$$L_{total} (g' (\u03c8), \u03b8) = L_{smooth} + L_{cla}.$$(13)\nNote that Eq. (13) is defined for node classification and needs\nsome minor modifications to be extended to graph-level tasks.\nSince there are no node labels in graph-level tasks, it requires\nreplacing Lcla in Eq. (13) with another loss Lgraph. Therefore,\nwe aggregate the node embeddings in a graph into a graph\nembedding by average pooling, and then compute the loss\nbetween it and the graph label, e.g., MSE for a regression task,\nor cross-entropy for a classification task. The total training loss\nfor graph-level tasks is Ltotal (g'(\u03c8), \u03b8) = Lsmooth + Lgraph."}, {"title": "E. Discussion and Comparison", "content": "1) Discuss on Structural Self-contrasting Network: Dif-\nferent from existing GNNs, such as GCN and GAT, that guide\nthe feature aggregation among neighbors through powerful"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel Graph Structural Self-\nContrasting (GSSC) framework to train graph data without ex-\nplicit message passing. The GSSC framework is based purely\non MLPs, where structural information is only implicitly\nincorporated to guide the computation of supervision signals.\nWe formulate structural sparsification as a probabilistic gener-"}, {"title": "D. Optimization Strategy", "content": "Algorithm 1 AGD for bi-level optimization\n1: Initialize sparsification network parameter \u03c8(0) and neigh-\nborhood self-contrasting network parameter \u03b8(0).\n2: for n \u2208 {1, ..., N} do\n3: Generate sparsified subgraph g'(\u03c8(n-1)) by Eq. (5-6).\n4: Compute Ltotal (g'(\u03c8(n-1)), \u03b8(n-1)) by Eq. (13);\n5: Lower-level maximization: Fix parameter \u03c8(n-1),\nand update parameter \u03b8(n) by Eq. (17).\n6: Compute H (g'(\u03c8(n-1)), \u03b8(n)) by Eq. (16);\n7: Upper-level minimization: Fix parameter \u03b8(n), and\nupdate parameter \u03c8(n) by Eq. (18).\n8: end for\n9: return Predicted labels Y\u1d64 and optimized paramter \u03b8(N)\nmessage passing, the structural self-contrasting network is\nbased purely on a multilayer perceptron where structural\ninformation is only implicitly used as prior in the computation\nof supervision signals, but does not explicitly involve in the\nforward propagation. Another research topic that is close to us\nis graph contrastive learning [54, 55], but we differ from them\nin the three aspects: (1) learning objective, graph contrastive\nlearning aims to learn transferable knowledge from abundant\nunlabeled data in an unsupervised setting and then generalize\nthe learned knowledge to downstream tasks. Instead, the\nstructural self-contrasting network works in a semi-supervised\nsetting, i.e., the partial label information is available during\ntraining. (2) augmentation, graph contrastive learning usually\nrequires multiple types of sophisticated augmentation to obtain\ndifferent views for contrasting [56\u201358]. However, we augment\nnodes only by simple linear interpolation. (3) We remove\nthose noisy edges by the sparsification network, which can\nbe considered as an \u201cadaptive\u201d edge-wise self-contrasting.\n2) Discuss on the structural sparsification network:\nDifferent from existing graph sparsification methods that\nare either attention-based or gradient-based, our STR-Sparse\nformulates graph sparsification learning as a two-stage pro-\ncess based on a probabilistic generative model, where (1)\nin the first stage, learning the sparsification distribution and\nmodeling the sparsification strategy; (2) in the second stage,\nsampling sparsified subgraphs from the learned sparsification\nstrategy through Gumbel-Softmax sampling, which transforms\nthe problem from a discrete combinatorial optimization to a\ncontinuous subgraph generation problem."}, {"title": "F. Evaluation on Complexity Analysis (Q5)", "content": "The time complexity of the GSSC framework mainly comes\nfrom two main parts: (1) STR-Sparse O(|V|dF+|E|F) and (2)\nSTR-Contrast O(|V|dF + |E'|F), with a total complexity of\nO(|V|dF + (|E|+|E'|)F), where |E| and |E'| are the number\nof edges in the original and sparsified graph. Due to graph\nsparsification, we have |E'| < |E|, so the total complexity\nis linear w.r.t the number of nodes |V| and edges |E|, which\nis in the same order of magnitude as GCN. However, with\nthe removal of neighborhood-fetching latency, the inference\ntime complexity can be reduced from O(|V|dF + |E|F) of\nGCN to O(|V|dF). The inference time (ms) averaged over\n30 runs is reported in Fig. 6, where all methods use L = 2\nlayers and hidden dimension F = 16. Besides, all baselines\nare implemented based on the standard implementation in the\nDGL library [78] using the PyTorch 1.6.0 library on NVIDIA\nv100 GPU. In a fair comparison, we observe that GSSC\nachieves the fastest inference speed on all datasets."}, {"title": "G. Hyperparameter Sensitivity Analysis (Q6)", "content": "We evaluate the hyperparameter sensitivity with respect to\ntwo key hyperparameters: fusion factor a and batch size B,\nand the results are reported in Fig. 7 and Fig. 8, respectively.\nIn practice, we can determine a and B by selecting the model\nwith the highest accuracy on the validation set.\n1) Fusion Factor: As can be observed in Fig. 7, fusion\nfactor a is crucial for the proposed framework. If we set\na = 1, i.e., removing the structural sparsification, the perfor-\nmance is usually the poorest compared with other settings. In\npractice, we find that setting a to a small value, e.g., a = 0.1\nusually produces pretty good performance. However, a too-\nsmall a may cause the sparsified subgraph to deviate too much\nfrom the original graph on some datasets, resulting in poor\nperformance. For example, on the Coauthor-CS dataset, the\nmodel achieves much better performance when setting a as\n0.3 than 0.1. In our experiments, we have only tested with the\nsmallest a = 0.1, and further performance improvements are\nexpected by finer-grained hyperparameter search.\n2) Batch Size: Fig. 8 shows the performance of GSSC\ntrained with batch size B \u2208 {256, 512, 1024, 2048, 4096},\nfrom which we observe that B is a dataset-specific hyperpa-\nrameter. For simple graphs with few nodes and edges, such\nas Cora and Citeseer, a small batch size, B = 256 or 512,\ncan yield fairly good performance. However, for large-scale\ngraphs, such as Coauthor-CS and Coauthor-Phy, the perfor-\nmance improves with the increase of batch size B."}]}