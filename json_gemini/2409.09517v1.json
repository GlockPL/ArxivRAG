{"title": "Deep Learning Under Siege: Identifying Security Vulnerabilities and Risk Mitigation Strategies", "authors": ["Jamal Al-Karaki", "Muhammad Al-Zafar Khan", "Mostafa Mohamad", "Dababrata Chowdhury"], "abstract": "With the rise in the wholesale adoption of Deep Learning (DL) models in nearly all aspects of society, a unique set of challenges is imposed. Primarily centered around the architectures of these models, these risks pose a significant challenge, and addressing these challenges is key to their successful implementation and usage in the future. In this research, we present the security challenges associated with the current DL models deployed into production, as well as anticipate the challenges of future DL technologies based on the advancements in computing, AI, and hardware technologies. In addition, we propose risk mitigation techniques to inhibit these challenges and provide metrical evaluations to measure the effectiveness of these metrics.", "sections": [{"title": "1 Introduction", "content": "Deep Learning (DL), built upon ideas from mathematical optimization, is the art of mimicry of the human brain's ability to learn via information processing through iterative improvement in a multitude of layers. The more training examples the model has to work with that is, the more data the DL model ingests the more robust and predictable the model becomes. Due to the Universal Approximation Theorem [1], given a set of inputs, $x = (x_1, x_2,...,x_n)$ and outputs, $y = (Y_1, Y_2, ..., Y_m)$, for $m, n \\in Z^+$, with a sufficient amount of complexity (this arises due to having many neurons, connections, and nonlinear activations), DL models which have baked-in Artificial Neural Networks (ANNs) can learn to find the function $f : x \\rightarrow y$, where f is sufficiently a minimum; see [2,3]."}, {"title": "2 Security Challenges in Deep Learning Architectures", "content": "As a constantly evolving, disruptive, and pervasive technological tool, the rapid adoption of these models in fields like Natural Language Processing (NLP) most commonly associated with Large Language Models (LLMs) like ChatGPT [4] and Gemini [5] \u2013 application domains in healthcare [6,7,8], finance [9,10], and so on, and their incorporation into other Machine Learning (ML) subfields for example, in the field of Reinforcement Learning (RL), whereby Deep Neural Networks (DNNs)-powered algorithms learned the optimal state and action value functions to beat the world champion in Go [11,12], to protein folding [13], to computational linear algebra [14]. There are a plethora of security challenges that pose detrimental risks to the model developer, the organization, and the model adopters.\nWhile offering unprecedented capabilities and immense possibilities only limited by one's imagination, DL architectures are opaque, oftentimes called \"black boxes\" because human experts do not understand why certain decisions are taken by the model and are therefore susceptible to a range of security threats. Of particular prominence is the conscientious crafting of deceptive inputs in order to create incorrect outputs, and this, more so than others, underscores a critical susceptibility to security challenges. For example, in critical domains like autonomous vehicular control, if the model underwent a hostile attack, it could cause the vehicle to lose control and crash, seriously harming or even killing the occupants.\nAs DL models become more widely adopted in commercial and industrial applications, the need for having a reliable understanding of the various risks and challenges is ever more important. In addition, understanding how to attenuate these risks is even more crucial. The challenges associated with DL can be directly directed to its architectures.\nFurther, with the widespread adoption of Cloud as a Service (CaaS) offerings, new challenges involve data privacy. Traditional methods like Federated Learning [15], Differential Privacy [16], and Homomorphic Encryption [17] each have their own challenges.\nIn this paper, we aim to shed light on the following research questions:\n1. Is it possible to develop an all-embracing list of security challenges in the current DL architectures, and for future DL and AI architectures, can one provide a list of anticipated challenges?\n2. Given a security challenge, can one provide a mitigation strategy against these risks?\n3. Can one quantify the effectiveness of these strategies?\nWhile answering these questions may provide obvious, having such outlines in a clear, concise, and consolidated form does not exist, to the best of our knowledge. There are several case studies conducted by consulting firms that try to identify these risks and associated mitigation strategies, but these are confusing, more concentrated on AI holistically with less emphasis on DL, and contain a plethora of information that is unuseful. Thus, in this study, we aim to address this major hurdle."}, {"title": "1.1 Main Contributions", "content": "In addressing the research questions, our foremost contributions of this research are as follows:\n1. An exhaustive list of the current state-of-the-art challenges in DL security, together with discussions of how they work.\n2. A new naming convention (\u03b1, \u03b2, ...) that categorizes these attacks and places them into relevant buckets depending on their genus and type.\n3. A comprehensive coverage of inhibitory strategies to mitigate the associated risk of each DL security challenge.\n4. The proposal of metrics on how to metricsize and quantify each of the attack vectors.\n5. A demonstration of hypothetical scenarios on the mechanics of numerically using the metrics.\nThis paper is organized as follows:\nIn Sec. 2, we discuss congruous research and their findings thereof.\nIn Sec. 3, we discuss the various challenges of DL model architectures, propose solutions to prevent them, and mitigate the associated risks. In addition, we provide metrics to ascertain the effectiveness of each of these proposed risk mitigation approaches. Lastly, we provide one sample calculation to demonstrate the mechanics of the metric.\nIn Sec. 4, we provide a recapitulation and reflection of our findings and propose future research directions."}, {"title": "2 Related Work", "content": "Below, we provide an abridged review of relevant literature pieces that helped influence our findings. This is not an exhaustive list.\nIn [18], introduce a scheme of privacy-preserving collaborative training of DL models, a primitive form of Federated Learning. The key findings of this paper that relate to our research is that in order to successfully address the challenges associated with privacy during the training of DL models, one needs to consider: Selective parameter sharing instead of sharing each other's training data small subsets of the model weights and biases should be shared, and the exploitation of the parallelized nature of DL optimization schemes, such as the variants of gradient descent - train models asynchronously and autonomously, and then conglomerate the model parameters which has the additional benefit of amplifying the efficiency of training and model scalability.\nIn [19], the authors highlight various challenges in the form of a review piece associated with DL algorithms. In particular, the authors identified the following difficulties: Model theft and model reverse engineering, inference of sensitive training data (Federated Learning was specifically developed to combat this challenge), recovery of recognizable images in image-related learning tasks, and vulnerability to adversarial training examples, and highlight privacy-preserving techniques. The novelty of this research is the review of antipathetic attacks under physical conditions, which demonstrates practical utility, and relates the research to the real world. Similarly, in [20], the authors focus on the potential harm caused by false predictions and misclassifications of DL models and the graveness of protecting delicate information during the training and learning phases.\nIn [21], the authors focus on how to ensure that DL models function optimally despite nefarious attacks, explore defenses against these attacks, and address threats associated with data privacy during model training. The novelty of these efforts is realized in the systematic characterization of security attack threats in the categories based on their timings: Poisoning attacks, which occur during training and corrupt the training data, and defense attacks, which occur during the inference phase of the model cycle, and impede the classification process. Analogously in [22], the authors systematize DL security and privacy and propose an overarching threat detection model that helps identify model vulnerabilities. The novelty associated with this piece of work is the formal exploration of the trade-off between model accuracy, model complexity, and sturdiness against adversarial exploit.\nIn [23], the author's study presents a taxonomy of privacy attacks in order to categorize different types of attacks based on adversarial knowledge and the assets being targeted. Further, the research identifies the causes of privacy leaks within ML models and systems and provides a review of the most common defenses proposed to mitigate these privacy attacks.\nIn [24], the research focuses on DL safety associated with adversarial attacks in applications to 2D and 3D Computer Vision. By doing a comprehensive review of 170 research papers, they extend the idea of adversarial attacks beyond traditional imperceptible perturbations to be more diverse and complex than previously thought.\nIn [25], the authors analyze 321 AI privacy incidents in order to provide a taxonomy of AI privacy risks. It was found that AI technologies, driven by DL models, can create new privacy risks, such as exposure risks from deepfake pornography, worsen existing privacy risks, such as surveillance risks due to the collection of training data, and can change the landscape of privacy concerns.\nWhile we may include many other works, many of them are restatements of the research carried out in the aforementioned papers and simply discussing them here would be futile. We would also like to point out that our discussion of parallel research is by no means exhaustive."}, {"title": "3 Challenges and Proposed Solutions", "content": "In the subsections that follow, we discuss some of the major end-to-end challenges of DL architectures and how to mitigate them."}, {"title": "3.1 Attack Vectors and Challenges", "content": "Using the methodical characterization provided in the literature, we have categorized the security challenges of DL architectures into the following groupings: Model Theft (a), Antipathetic Attacks (\u03b2), Personal Data Privacy and Concealment (\u03b3), Data Toxification (\u03b4), Explainability (\u20ac), Security Updates (\u03b6), Deployment Difficulties (\u03b7), Novel Threats (\u03b8). Within these categories, we discuss the various types of security issues that could potentially arise.\n1. Extraction Attacks: a. Intruders attempt to re- and deconstruct models by querying them and analyzing the outputs to steal their architecture or model parameters.\n2. Larceny Attacks: a. The confidentiality of proprietary models is breached when intruders attempt the unauthorized wholesale replication of the model for iniquitous intent.\n3. Inversion Attacks: \u03b2. Sensitive information on training data is extricated by manipulating the model's predicted values.\n4. Elusory Attacks: \u03b2. Input data is engineered to deceive the DL model, causing it to make erroneous predictions.\n5. Poisoning Attacks: \u03b2. Introducing malicious training data in order to corrupt the model's training.\n6. Data Dissipative Loss: \u03b3. Training data is involuntarily leaked during training and/or inference.\n7. Membership Attacks: \u03b3. Intruders try various trial-and-error methods to determine whether a datapoint was part of the training data, thereby revealing other important information.\n8. Sub-rosa Attacks: \u03b4. Model developers may have parti pris \u2013 ulterior motives and hidden agendas - and embed hidden malicious actors in the DL model, having specific triggers in the training data to activate them.\n9. DL Models are Blackboxes: \u20ac. DL models are shrouded in opacity, making it difficult and oftentimes impossible to comprehend their decision-making processes and identify vulnerabilities.\n10. Interpretability Dearth: \u20ac. Even simple feed-forward network assemblies have a multitude of connectivity between nodes, thus making it extremely difficult, if not impossible, to understand what's going on and if there are any malicious actors deployed within them.\n11. Model Update Threats: \u03b6. These are vulnerabilities introduced during model updates. These can degrade the model's performance or outright compromise the model in its entirety.\n12. Model Patching Threats: \u03b6. These are vulnerabilities introduced when small updates (patches) are introduced. Analogous to 11, they can have comparable effects.\n13. Secure Deployment: \u03b7. These are security risks, such as tampering or unwarranted access, that arise during the model deployment into the production phase.\n14. GenAI and Deepfakes: \u03b8. GenAI and the associated underlying transformer-based models are constantly evolving. One of the major architectural threats is pretraining. If the previous model in which the current model is trained on has errors or interceptions, these could be inherited by the new model. Another major challenge is designing robust model architectures that can establish whether content is AI-generated and identify the source of deepfakes. This leads to the design of tamper-proof DL architectural systems for tracking content creation and modification.\n15. Quantum AI Threats: \u03b8. Quantum Computing, and Quantum Machine Learning, while they hold the potential to perform computations like never before by leveraging the Quantum Mechanical properties of Superposition and Entanglement, run the risk of receiving both classical and quantum attacks. One of the major challenges is developing architectures that are efficient in executing post-quantum cryptographic protocols for AI applications. Another major challenge is that quantum DL architectures can lose information along the way due to decoherence of the hardware with the environment.\n16. Knowledge Injection Attacks in Neurosymbolic AI Architectures: \u03b8. These occur via the manipulating the symbolic knowledge base to influence system behavior. One of the major challenges is that for hybrid architectures, how does one verify the integrity of the symbolic knowledge? In addition, in order to enhance reasoning capabilities, how does one maintain privacy in order to preserve diverse knowledge.\n17. Challenges Associated with AGI Architectures: \u03b8. We acknowledge that Artificial General Intelligence (AGI) is currently not a realized technology. However, we envisage that from a model architecture point of view, a major challenge would be to develop robust methods for specifying and enforcing ethical constraints in alignment with human values and intentions. In addition, due to the autonomous thought process of AGI, many experts have raised concerns about the self-modification of code and the associated security issues. These include constructing AGI model architectures that are self-tamper proof, effective architectural containment measures while not inhibiting self-determination, and developing architectural protocols for safe AGI-to-AGI interactions.\n18. Challenges Associated with ASI Architectures: \u03b8. The ability to supersede human intelligence with Artificial Superintelligence (ASI) seems even more far-fetched than the hopes of AGI. However, with the rapid AI revolution we are experiencing, it proves prudent to plan for such eventualities. One challenge is the development of ASI architectures that have built-in fail-safe mechanisms that remain effective against superintelligent adversaries. Secondly, another major challenge is the creation of architectural security measures that cannot be trivially bypassed by ASI and the design of architectures that preserve human life from asymmetric intelligence scenarios.\nIt is noteworthy to point out that for novel DL architectures, a common challenge is the preservation and security of information."}, {"title": "3.2 Some Preventative Strategies and Risk Mitigation Policies", "content": "In this subsection, we propose a set of inhibitory procedures and policies to reduce the risks associated with the security challenges posed by DL model architectures as follows:\n1. Incorporation of Adversarial Training Examples: In order to make the DL model robust, include adversarial examples in the training data so that the model is exposed to a plethora of datapoints that span both amicable and adversarial types. This increases the robustness of the overall model and makes it strong against attacks. Mathematically, we can quantify the adversarial robustness using\n$R = \\arg \\max_{x_i \\in X} f(x_{adv}),$ (1)\nwhere $f(x_{adv})$ is the model prediction for adversarial example $x_{adv}$. The equation simply does a count of the number of correct classifications.\n2. Noise Filtering of Raw Data: During model preprocessing, apply noise filters to sieve out adversarial actors that may be hidden within the data. Mathematically, we borrow a tool from signal processing; we can quantify the quality of the filtered data using\n$DQ_{filtered} = 1 - \\frac{N_{after}}{N_{before}}$ (2)\nwhere $N_{before}$ is the number of datapoints before filtering, and $N_{after}$ is the number of datapoints that remains after filtering.\n3. Data Autoclaving: Sanitize your raw data by cleaning it in order to remove anomalous examples. We quantify the effectiveness of cleaning via the anomaly detection rate\n$A = \\frac{N_A}{m}$ (3)\nwhere $n_a$ is the total number of anomalies detected, and m is the total number of datapoints.\n4. Model Access Control: Have restricted model access control policies and security clearance protocols in place whereby only certain user classes can have access to the model and the training data. Mathematically, the effectiveness of the model access control can be calculated using\n$A/C = \\frac{N_{unauthorized}}{N}$ (4)\nwhere $N_{unauthorized}$ is the number of unauthorized access attempts that were blocked, and N is the total number of access attempts.\n5. Protection from using Output to Surmise Input: Use privacy-preserving techniques, such as differential privacy, so that the model output does not reveal information about the model's input. In order to mathematically quantify privacy preservation, we set a privacy budget threshold, E, and calculate the total privacy losses. If the total privacy loss is less than the threshold, then the privacy-preserving technique was effective. Mathematically, we require the privacy loss, P, to be at most\n$P < E.$ (5)\n6. Model Surveillance: Monitor model queries and the number of queries per user to detect anomalous model queries. Analogous to our proposal for quantifying the effectiveness of data autoclaving, we measure the query anomaly detection rate as\n$Q = \\frac{q_A}{q}$ (6)\nwhere $q_a$ is the total number of anomalous queries detected, and q is the total number of queries."}, {"title": "4 Conclusion", "content": "We have considered the various security challenges that DL models, and thereby their architectures, can possess, together with strategies and policies to mitigate these risks and metrical measures that can quantify the effectiveness of these strategies. This work was built upon characterizations provided in the literature, and we meticulously filtered them down to address individual security risks posed. It is our belief that our characterization is unique, and can greatly assist in the planning and strategizing by policymakers. As a future consideration, it would prove a worthwhile task to collect data, or generate synthetic data, and apply ML and DL models to predict the various risk types that is, \"using DL to detect DL security threats\"; this is a venture that will be considered in future works. In doing so, this will help strategists and risk managers to design effective master plans to overcome this faux pas."}]}