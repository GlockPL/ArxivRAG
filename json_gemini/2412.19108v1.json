{"title": "Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection", "authors": ["Xiaoyu Huang", "Weidong Chen", "Bo Hu", "Zhendong Mao"], "abstract": "Multivariate time series (MTS) anomaly detection is a critical task that involves identifying abnormal patterns or events in data that consist of multiple interrelated time series. In order to better model the complex interdependence between entities and the various inherent characteristics of each entity, the graph neural network (GNN) based methods are widely adopted by existing methods. In each layer of GNN, node features aggregate information from their neighboring nodes to update their information. In doing so, from shallow layer to deep layer in GNN, original individual node features continue to be weakened and more structural information, i.e., from short-distance neighborhood to long-distance neighborhood, continues to be enhanced. However, research to date has largely ignored the understanding of how hierarchical graph information is represented and their characteristics that can benefit anomaly detection. Existing methods simply leverage the output from the last layer of GNN for anomaly estimation while neglecting the essential information contained in the intermediate GNN layers. To address such limitations, in this paper, we propose a Graph Mixture of Experts (Graph-MoE) network for multivariate time series anomaly detection, which incorporates the mixture of experts (MoE) module to adaptively represent and integrate hierarchical multi-layer graph information into entity representations. It is worth noting that our Graph-MoE can be integrated into any GNN-based MTS anomaly detection method in a plug-and-play manner. In addition, the memory-augmented routers are proposed in this paper to capture the correlation temporal information in terms of the global historical features of MTS to adaptively weigh the obtained entity representations to achieve successful anomaly estimation. Extensive experiments on five challenging datasets prove the superiority of our approach and each proposed module.", "sections": [{"title": "Introduction", "content": "Detecting anomalies in multivariate time-series data is crucial for ensuring security and preventing financial loss in industrial applications, where devices like servers and engines are monitored using multivariate time-series entities, organizations need efficient systems to identify potential issues quickly (Ren et al. 2019). However, anomalies are typically rare in both real-world application scenarios and existing public datasets, making data labeling both challenging and costly. Thus, we focus on unsupervised multivariate time series anomaly detection in this paper.\nAn effective unsupervised strategy is modeling the dataset into a distribution. As machine learning advances (Guo et al. 2024; Gao, Chen, and Xu 2023; Gao, Zhang, and Xu 2020; Hu et al. 2023; Xu et al. 2024b,c,a; Ma et al. 2024, 2021, 2022), many existing methods (Rasul et al. 2021b,a) have been explored alongside this strategy to achieve density estimation of the distribution. However, a key challenge with this approach is that many existing methods ignore the interdependencies between time series, which are critical for accurately modeling the data distribution and performing efficient density estimates for anomaly detection. Graph neural network (GNN) based methods have emerged as a promising solution for enhancing anomaly detection in time-series data by modeling the dependencies between entities. Deng and Hooi (2021) propose to combine structure learning with GNNs to detect anomalies in MTS. MTAD-GAT (Zhao et al. 2020) utilizes graph attention networks to model spatial and temporal dependencies. GReLeN (Zhang, Zhang, and Tsung 2022) learns a probabilistic relation graph for multivariate time series and utilizes hidden variables to capture spatial dependencies. Cai et al. (2021) present a method based on graph neural networks for anomaly detection in dynamic graphs.\nAlthough previous GNN-based methods have achieved promising results, they simply leverage the output from the last layer of GNNs to model the relations between entities. However, as stated in previous analysis works (Chen, Wang, and Li 2021; Chen and Wang 2022; Xu et al. 2018; He et al. 2024; Wang et al. 2024a; Tian, Chen, and Song 2021; Yang et al. 2024b,a) of GNNs, a common limitation of existing method in MTS anomaly detection is that GNNs are essentially \"homogeneous\" across the whole graph, i.e., forcing all nodes to share the same aggregation mechanism, regardless of the differences in their node features or neighborhoods. That might be suboptimal in graph-based relationship modeling, e.g., when some nodes may require information aggregated over longer ranges while others prefer shorter-range local information. As illustrated in the upper example in Fig. 1, the average number of neighborhood nodes within 1-hop, 2-hop, and 3-hop are 2.3, 12.5, and 18.9, respectively, which shows that the network needs to go deep into multi-layer GNNs to establish more global relations. A single-layer GNN can only establish the simplest local relations. Thus, the intermediate layers of GNNs capture information from different multi-hop neighborhoods, i.e., from short-distance to long-distance neighborhoods, which is vital for comprehensively understanding the complex relations and dependencies in MTS data.\nIn this paper, we propose a Graph Mixture of Experts (Graph-MoE) network, an unsupervised method for multivariate time series (MTS) anomaly detection, to address the aforementioned challenges. Unlike previous GNN-based methods that solely leverage the output from the last layer of GNN to model the interdependences between entities, our Graph-MoE comprehensively utilizes all intermediate information of multi-layer GNNs, that is, it considers the hierarchical node (entity) representations in domains of variable-distance neighborhoods. Specifically, the proposed Graph-MoE incorporates the mixture of experts (MoE) framework to adaptively represent and integrate hierarchical information of different GNN layers into entity representations. In each GNN layer, a specific expert network is implemented with the global-guided entity attention block for the intra-level aggregation of node representations. Moreover, we propose a memory-augmented router, which uses a memory unit to hold the global historical features of the MTS to mine inter-series correlation patterns between time series of the current state, predicting the importance coefficients of experts and facilitating the inter-level aggregation of entity representations. It is worth noting that our Graph-MoE can be integrated into any GNN-based MTS anomaly detection method in a plug-and-play manner. Extensive experiments on five challenging datasets demonstrate the superiority of our approach and each proposed module. In short, our main contributions are summarized as follows:"}, {"title": "Related Work", "content": "Time Series Anomaly Detection\nTime series anomaly detection refers to the identification and discovery of abnormal patterns or behaviors in time series data. However, due to the complexity and volume of time series data, obtaining a well-labeled dataset is challenging (Choi et al. 2021), unsupervised methods are often used to detect temporal anomalies. A typical method is to use auto-encoder (AE) (Kingma and Welling 2013). However, the robustness and generalization capabilities of AEs are limited, which led to the development of VAEs (Su et al. 2019). Additionally, there are graph-based methods in unsupervised learning algorithms. MTGNN (Wu et al. 2020) learns and models relationships between time series variables using a Graph Convolutional Network (GCN). Graph WaveNet (Wu et al. 2019) models the spatio-temporal graph structure through graph convolution to detect anomalies. GANF (Dai and Chen 2022) and MTGFlow (Zhou et al. 2023) leverage graph structures combined with normalizing flow techniques to perform density estimation for anomaly detection in series.\nMixture of Experts\nThe Mixture of Experts (MoE) is originally developed as an ensemble method that assigns input data to different \"expert\" submodels, each processing a specific part or feature of the input. To adapt MoE for multi-task learning, Ma et al. (2018) introduce weight-sharing among multiple tasks. Shazeer et al. (2017) introduce the Sparsely-Gated MoE, consisting of thousands of feed-forward sub-networks. Further improvements in the stability and efficiency of sparse MoE models are made by (Lewis et al. 2021). Lepikhin et al. (2021) utilize MoE to create V-MoE, a vision transformer with sparse activation. Liu et al. (2024b) reveal the Mixture of Multimodal Experts (MoME), enhancing Multimodal Relation Extraction (MRE) through a mixture of hierarchical visual context learners.\nMemory-augmented Networks\nGrowing interest has been attracted to memory-augmented models for improving long-term dependency modeling. LongMem (Wang et al. 2024b) enhances language models with a memory network to store and retrieve long-term context for better language modeling. In recommendation systems, MA-GNN (Ma et al. 2020) uses a graph neural network for short-term context and a shared memory network for long-term dependencies, while DMAN (Tan et al. 2021) segments behavior sequences and uses dynamic memory blocks to combine short-term and long-term user interests for more accurate recommendations. These models highlight the importance of memory mechanisms."}, {"title": "Methodology", "content": "Preliminaries\nWe present a brief introduction to our baseline in this section to provide a better understanding of our proposed method.\nNormalizing flow. It is a widely-used method for unsupervised density estimation (Gudovskiy, Ishizaka, and Kozuka 2022; Liu, Tan, and Zhou 2022), which employs an invertible affine transformation to convert an original distribution into a target distribution. When direct density estimation based on the original data distribution $X$ is not feasible, alternatives to direct density estimation are to estimate the density of the target distribution $Z$. We consider the case of a source sample $x \\in \\mathbb{R}^D$ drawn from the distribution $X$ and a target distribution sample $z \\in \\mathbb{R}^D$ drawn from the distribution $Z$. $z = f(x)$ is a one-to-one mapping between $X$ and $Z$. Utilizing the change of variable formula, we can derive that\n$\\displaystyle P_X(x) = P_Z(z) \\left| \\det \\frac{\\partial z}{\\partial x^T} \\right|,\\qquad(1)$ \nTaking advantage of the invertibility of mapping functions and the tractability of Jacobian determinants $\\left| \\det \\frac{\\partial z}{\\partial x^T} \\right|$ in flow models, the aim is to achieve $z = z$, where $z = f(x)$. When additional conditions $C$ are incorporated, density estimation can be greatly improved. The conditional normalizing flow models represent the mapping as $z = f_{\\theta}(x|C)$. The parameters $\\theta$ of the $f_{\\theta}(x)$ are optimized by maximum likelihood estimation (MLE):\n$\\displaystyle \\theta^* = \\arg \\max_{\\theta} \\left( \\log(P_Z(f_{\\theta}(x|C))) + \\log\\left(\\left| \\det \\frac{\\partial f_{\\theta}(x|C)}{\\partial x^T} \\right| \\right) \\right),\\qquad(2)$\nNotation and Problem Definition. Multivariate Time Series (MTS) are defined as $x = (x_1,x_2,...,x_k)$ where $x_i \\in \\mathbb{R}^L$. For each entity, $k$ signifies the number of entities, while $L$ signifies the number of observations of each entity. We use the z-score to normalize the time series of different entities\n$\\displaystyle x_i = \\frac{X_i - \\text{mean}(x_i)}{\\text{std}(x_i)},\\qquad(3)$\nwhere $\\text{mean}(x_i)$ and $\\text{std}(x_i)$ represent the mean and standard deviation of the entity $i$ along the time dimension, respectively. A sliding window with size $T$ and stride size $S$ is used to sample the normalized MTS to preserve the temporal correlations of the original series. The training sample $x$ can be obtained by adjusting $T$ and $S$, where $c$ is the number of samples. For simplicity, $x$ stands for $x_{cS:cS+T}$.\nGraph Mixture of Experts\nOur model, depicted in Fig. 2, comprises independent expert networks dedicated to the intra-level aggregation of global-guided entity representations.\nFeature Extraction. Following previous work (Dai and Chen 2022; Zhou et al. 2023) for a fair comparison, we model the temporal variations of each entity by using the RNN model, i.e., LSTM (Hochreiter and Schmidhuber 1997) and GRU (Cho et al. 2014).\n$\\displaystyle H_t = \\text{RNN}(x_t, H_{t-1}),\\qquad(4)$\nwhere $H_t$ is the hidden RNN features for a window sequence of entity $k, x_e$ at time $t \\in [cS : cS + T)$. The combination of entities $H_t \\text{ for }\\forall k$ forms the initial input $H$ of our proposed Graph-MoE network.\nGraph Construction. Due to the mutual and evolving dependence among entities, we utilize self-attention to learn a dynamic graph structure by treating entities at a certain time window of multivariate time series as graph nodes. For each input window sequence $x_e$, two linear transformations are performed, and then, the pairwise relationship $e_{ij}$ at time window $c$ between nodes $i$ and $j$ is determined as:\n$\\displaystyle e_{ij}^c = (\\phi_1(x_i))^T(\\phi_2(x_j)),\\qquad(5)$\nwhere $\\phi_1$ and $\\phi_2$ are two linear transformations. To quantify the relationship between node $i$ and $j$, the attention score $a_{ij}^c$ of the matrix $A^c$ is calculated by\n$\\displaystyle a_{ij}^c = \\frac{\\exp(e_{ij}^c)}{\\sum_{k=1}^{K} \\exp(e_{ik}^c)},\\qquad(6)$\nThe attention matrix $A^c$ inherently represents the mutual dependencies among the entities, which is treated as the adjacency matrix of the constructed graph, consequently. With time series inputs continually changing, $A^c$ also evolves to capture the dynamic interdependence between them.\nGraph Updating. To derive the temporal embedding $H^l$ for the $l$-th GNN layer of all entities at $t$ with incorporating the temporal features $H_t^{l-1}$ of the last graph layer, a graph convolution operation is performed through the learned graph $A^c$. Inspired by previous work GANF (Dai and Chen 2022) and MTGFlow (Zhou et al. 2023), we also find that the history information of the node itself helps enhance temporal relationships of time series. Hence, the $l$-th GNN layer of spatio-temporal condition at time $t$ is defined as:\n$\\displaystyle H^l = \\text{ReLU}(A^cH_t^{l-1}W_1 + H_{t-1}W_2)W_3,\\qquad(7)$\nwhere $W_1$ and $W_2$ are graph convolution and history information weights, respectively. $W_3$ is used to improve the expression ability of condition representations. Superscript $l$ indicates the output of the $l$-th GNN layer. The spatio-temporal condition $H^l$ for window $c$ of the $l$-th GNN layer is the concatenation of $H^l$ along the time axis.\nMixture of Experts. A mixture of experts (MoE) is proposed and introduced to explore whether distinct global temporal features may favor graph information encoded at different levels. A mixture of experts dynamically combines the outputs of sub-models known as \u201cexperts\u201d via a weighting function known as the \"router\". The proposed mixture of experts is formulated as follows:\n$\\displaystyle C = \\sum_{l=1}^L R_l \\cdot f_l(H^l) = \\sum_{l=1}^L R_l \\cdot \\hat{H}^l,\\qquad(8)$\nwhere $f_l(\\cdot)$ denotes the $l$-th expert function, $\\hat{H}^l$ is the expert-aligned entity features of the $l$-th graph layer, and $R = [R_1; R_2; ...; R_L]$ is the routing weights for the $L$ experts. For each entity $x_i$, since each GNN layer gathers information from directly connected entities, stacking multiple GNN layers enables the learning of neighborhood relationships between entities over long distances. Thus, each layer of our Graph-MoE has a specific expert network that independently captures features at different levels that focus on the different range of neighborhoods.\nOur MoE takes the current temporal embedding $H_t$ obtained from the RNN encoder as queries and the spatio-temporal condition $H^l$ extracted from the $l$-th GNN layer as key-value pairs to aggregate node features within each layer. Formally, the cross-attention block operates as follows:\n$\\displaystyle \\hat{H}^l = \\text{LN}(\\text{Attention}(H_t, H^l))|_{Q:H_t, {K,V}:H^l},\\qquad(9)$\nwhere LN is the layer normalization operation. To enhance the capabilities of intra-layer feature representations, we employ the FFN and the ReLU activation function in the expert network. The expert-aligned entity features $\\hat{H}^l$ are:\n$\\displaystyle \\hat{H}^l = \\text{FFN}_l(H^l) = \\phi_l(\\text{ReLU}(\\phi_l(H^l))),\\qquad(10)$\nwhere $\\phi_1$ and $\\phi_2$ refer to the multi-layer perceptron (MLP). Then we respectively concatenate the outputs obtained from the attention mechanism, forming hierarchical global-guided temporal features $H$:\n$\\displaystyle \\hat{H} = [\\hat{H}^1, \\hat{H}^2, \\ldots, \\hat{H}^L].\\qquad(11)$\nFor current time step $t$, $\\hat{H}$ is specified as $\\hat{H}_t$, which is then used for memory updates and dynamic routers generation.\nMemory-augmented Routers\nThe memory-augmented router is designed to extract features from the current temporal slice while also capturing the characteristics of the entire historical data. These extracted features are then utilized by the module to predict the importance coefficients of experts of different time series.\nTo exploit the characteristics of memory, inspired by (Liu et al. 2022; Chen et al. 2020), we propose memory-augmented routers to enhance the weighted combination of GNNs' multi-layer outputs. In doing so, the memory module uses a matrix to store the global historical temporal features. In the process of updating, the matrix is revised step-by-step incorporating the output from previous steps. Then, at time step $t$, the matrix from the previous step, $M_{t-1}$, is functionalized as the query, and its concatenations with the previous output serve as the key and value to feed the multi-head attention module. The entire process is formulated as follows:\n$\\displaystyle Y = [M_{t-1}; \\hat{H}_t],\\qquad(12)$\n$\\displaystyle Z = \\text{Attention}(M_{t-1}, Y)|_{Q:M_{t-1}, {K,V}:Y},$\nwhere $[M_{t-1}; \\hat{H}_t]$ is the row-wise concatenation of $M_{t-1}$ and $\\hat{H}_t$. Considering that the memory module is performed in a recurrent manner along with the estimation process, it potentially suffers from gradient vanishing and exploding. We therefore introduce residual connections and a gate mechanism. The former is formulated as\n$\\displaystyle M_t = \\phi_M(Z + M_{t-1}) + Z + M_{t-1},\\qquad(13)$\nwhere $M$ refers to the multi-layer perceptron (MLP). The detailed structure of the gate mechanism in the memory module is shown in Fig. 3, where the forget and input gates are applied to balance the inputs from $M_{t-1}$ and $\\hat{H}_t$, respectively. The forget and input gates are formalized as\n$\\displaystyle G_f = \\hat{H}_tW^f + \\tanh(M_{t-1}) \\cdot U^f,\\qquad(14)$\n$\\displaystyle G_i = \\hat{H}_tW^i + \\tanh(M_{t-1}) \\cdot U^i,\\qquad(15)$\nwhere $W^f$ and $W^i$ are trainable weights for $\\hat{H}_t$ in each gate; and similarly, $U^f$ and $U^i$ are the trainable weights for $M_{t-1}$ in each gate. The final output of the gate mechanism is formalized as follows:\n$\\displaystyle M_t = \\sigma(G_f) \\odot M_{t-1} + \\sigma(G_i) \\odot \\tanh(M_t),\\qquad(16)$\nwhere $\\odot$ refers to the Hadamard product, $\\sigma$ is the sigmoid activation, and $M_t$ is the output of the memory module at step $t$, which contains both the global historical temporal information and the current temporal features. We then update the memory by replacing $M_{t-1}$ with $M_t$ in the next step. The output $\\mathcal{O}_R(M_t)$, derived from the memory module, is then fed into the expert router, which is a linear transformation $\\mathcal{O}_R$ of the input followed by a softmax layer:\n$\\displaystyle R = \\text{Softmax}(\\phi_R(M_t)),\\qquad(17)$\nwhere the output $R$ preserves the predicted expert weights through the memory-augmented router. Finally, we leverage Eq. (8) to generate the spatio-temporal condition $C$."}, {"title": "Experiment", "content": "Experiment Setup\nDatasets. Following GANF (Dai and Chen 2022) and MT-GFlow (Zhou et al. 2023), we conduct experiments on five widely-used public datasets for MTS anomaly detection. The details are illustrated as follows: SWaT (Mathur and Tippenhauer 2016) consists of 51 sensors from an industrial water treatment plant, including 41 attacks over four days. WADI (Ahmed, Palleti, and Mathur 2017) is a collection of 123 sensor and actuator data from the WADI testbed, at the frequency of one second. PSM (Abdulaal, Liu, and Lancewicki 2021) is a collection of multiple application server nodes at eBay with 25 features, providing ground truths created by experts over 8 weeks. MSL (Hundman et al. 2018) is a collection of the sensor and actuator data of the Mars rover with 55 dimensions. SMD (Su et al. 2019) is a 5-week-long dataset that is collected from a large Internet company with 38 dimensions.\nEvaluation Metrics. Following previous studies (Dai and Chen 2022; Zhou et al. 2023), Graph-MoE is designed to detect anomalies at the window level, where a window is labeled as abnormal if any time point within it is identified as anomalous. The performance of the model is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC).\nImplementation Details. We set the window size as 60 and the stride size as 10. During the training stage, we utilize an Adam optimizer. For the SWaT dataset, we use a single flow block with a batch size of 640 and a learning rate of 0.005. The batch size of the WADI dataset is 320, and the learning rate is 0.0035. Other datasets employ two flow blocks with a batch size of 256 and a learning rate of 0.002. We follow the dataset setting of previous work (Zhou et al. 2023; Dai and Chen 2022) and split the original testing dataset by 60% for training, 20% for validation, and 20% for testing in SWaT. For other datasets, the training split contains 60% data, and the test split contains 40% data. All our experiments are conducted on GPU, running for 80 epochs.\nBaselines. To demonstrate the superiority of our method, we compare it with current competitive approaches, including the state-of-the-art methods that encompass both semi-supervised and unsupervised approaches to anomaly detection. Notably, semi-supervised methods include DeepSAD (Ruff et al. 2020) and DROCC (Goyal et al. 2020). Besides, unsupervised methods include DeepSVDD (Ruff et al. 2018), ALOCC (Sabokrou et al. 2020), USAD (Audibert et al. 2020), DAGMM (Zong et al. 2018), GANF (Dai and Chen 2022) and MTGFlow (Zhou et al. 2023).\nMain Comparison\nAs shown in Table 1, we compare Graph-MoE with the aforementioned baselines and report the average performance and standard deviations on AUROC scores through five random repetitions. The results show that our Graph-MoE reaches an exceptionally high AUROC score compared to the other eight baselines. Note that the standard deviation observed in the SMD arises from the fact that it encompasses 28 distinct sub-datasets. For each sub-dataset, we evaluate the performance and subsequently calculate the average of all these individual results. From the results of Table 1, we have the following observations.\nEffectiveness on the number of Graph-MoE layers. In our proposed method Graph-MoE, the most critical parameter is the number of layers. We explore the influence of different layers of Graph-MoE on anomaly detection performance, and the results are shown in Table 2. We can observe that the AUROC fraction increases gradually with the increase in the number of layers and reaches a peak at the third-layer Graph-MoE. However, as the number of layers increases to 4, the performance declines.\nThe reason is that when the number of layers is set to 1 and 2, the model can only aggregate a restricted amount of neighborhood information, which is insufficient for capturing complex long-distance relationships. Moreover, an excessive number of layers can lead to the occurrence of over-smoothing, which reduces the distinctiveness between node features, diminishes diversity and ultimately weakens the model's representation ability. The results show that a reasonable selection of layers can significantly improve the detection ability of the model.\nDiscussion on each proposed module. To further investigate the contribution of each component in our method, we conduct a set of ablation studies. The results are shown in Table 3. In this table, \"x\" indicates that the module is not used, while \"/\" indicates that the module is utilized. The components MoE and MAR represent the mixture of experts and memory-augmented routers, respectively. Initially, without the inclusion of MAR and MoE, there are modest performance improvements of 0.8% and 0.3% on SWaT and WADI over MTGFlow, respectively, which suggests that the base graph structure alone is somewhat effective in capturing spatio-temporal information. However, when MAR and MoE are introduced separately, both contribute to further performance gains, with MoE having a more pronounced impact. The results could be attributed to MoE's ability to better integrate expert knowledge from different GNN layers, enhancing the model's representational capacity. When MAR and MoE are both introduced into our model, their synergistic effect leads to substantial performance improvements of 2.8% on the SWaT dataset and 2.5% on the WADI dataset, which underscores the effectiveness of combining MAR's global historical feature extraction with MoE's integration of multi-level information in addressing complex time series anomaly detection challenges.\nDiscussion on the plug-and-play properties. To evaluate the plug-and-play capability of our model, we select three representative graph-based anomaly detection methods, namely GANF (Dai and Chen 2022), MTGFlow (Zhou et al. 2023), and USD (Liu et al. 2024a). As shown in Table 4, we apply Graph-MoE upon these three baselines. Specifically, after incorporating Graph-MoE, GANF's performance increases from 79.8 to 82.6, representing a gain of 3.5 percentage points, the largest improvement observed. MT-GFlow's performance rises from 84.8 to 87.2, with a 2.8% increase. Similarly, USD's performance improves from 90.2 to 92.3, an increase of 2.3 percentage points. This consistent performance enhancement proves that Graph-MoE effectively strengthens anomaly detection capabilities across different graph-based models, demonstrating the strong plug-and-play capabilities of our proposed Graph-MoE.\nQualitative Results\nMoreover, we visualize the distributions of data on the SWAT dataset to examine the effectiveness of our method. To investigate the anomaly discrimination ability of Graph-MoE, we present the normalized anomaly score for GANF, MTGFlow and Graph-MoE in Fig. 4. As it is displayed, for normal series, anomaly scores of Graph-MoE are more centered at 0 than those of GANF and MTGFlow, and the overlap areas of normal and abnormal scores are also smaller in Graph-MoE, reducing the false positive ratio. This larger score discrepancy corroborates that Graph-MoE has superior detection performance. Fig. 5 shows the point plot of Graph-MoE anomalies predicted outputs over time. The X-axis displays the progression of time, while the Y-axis indicates the log-likelihood of the anomaly. Abnormal series are highlighted with a red background. This fluctuation in log likelihoods confirms that our proposed Graph-MoE has capabilities for detecting anomalies by identifying low-density regions within the modeled distribution."}, {"title": "Conclusion", "content": "In this work, we proposed Graph-MoE, an unsupervised-learning based method for anomaly detection in multivariate time series data. Unlike previous methods, we employ a multi-layer, multi-expert learning architecture to dynamically model the complex interdependencies between entities and the various inherent characteristics of each entity at different levels, capturing both local and global information and enhancing feature representations of each entity. We also propose Memory-augmented Routers, which leverage the memory mechanism to store the patterns of global historical temporal features to predict the importance coefficients of experts, therefore facilitating effective inter-level aggregation. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance."}, {"title": "Appendix", "content": "Performance evaluation via AUROC curves\nIn order to convey an intuitive assessment of performance, AUROC curves are depicted in Fig. 1. A comparative analysis reveals that, in contrast to Graph-MoE, both DeepSVDD (?) and DROCC (?) adopt a hyperspherical projection of all training samples, which inherently limits their capability to precisely delineate the decision boundary separating normal from anomalous instances. The semi-supervised learning strategy utilized in DeepSAD (?), leverages more informative training procedures that effectively mitigate the challenges posed by high levels of anomaly contamination in the data series. MTGFlow (?) leverages the flexibility of dynamic graphs to estimate the abnormal density of entities, but its detection effectiveness is constrained due to the neglect of information from intermediate layers. Graph-MoE outperforms the above baseline methods. Because it fully leverages the multi-level information of GNNs, rather than relying solely on the output from the final layer. By introducing the Mixture of Experts (MoE) module and Memory-Augmented Routers (MAR), our model can adaptively integrate multi-level graph information and global historical features, thereby capturing complex entity relationships with greater precision.\nJustification for our approach\nIn the layer-by-layer processing of our Graph-MoE, each layer enhances the representation of nodes by integrating information from their immediate neighbors. As evidenced in Fig. 2, by leveraging three-layer Graph-MoE, our model effectively captures and measures relationships across different domains, particularly between short-distance and long-distance neighborhoods, by incorporating information from a broader range of nodes. These multi-level integration enhances the model's capacity to learn complex relationships. However, extending beyond three-layer results in diminishing returns due to the over-smoothing effect, which reduces the distinctiveness of node features. Consequently, employing a three-layer graph structure enables the node representations to be more comprehensive in our method."}]}