{"title": "Understanding Implosion in Text-to-Image Generative Models", "authors": ["Wenxin Ding", "Cathy Y. Li", "Shawn Shan", "Ben Y. Zhao", "Haitao Zheng"], "abstract": "Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce \"model implosion,\" where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models.\nIn this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of \u201csupervised graph alignment\" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.", "sections": [{"title": "1 INTRODUCTION", "content": "Large-scale text-to-image generative models like Stable Diffusion, Midjourney, DALLE, and Adobe Firefly have made tremendous impact on various artistic and creative industries. Each of these models is trained on hundreds of millions, if not billions, of images and corresponding text captions. Given prior understanding of poisoning attacks on deep neural networks, many believe that employing such massive training datasets makes these generative models naturally robust to poisoning attacks.\nSurprisingly, recent results have shown that these large diffusion models to be quite vulnerable to poisoning attacks targeting the connections between textual prompts and image visual features. Multiple projects [24, 40, 49, 54] have demonstrated the use of poisoning attacks to successfully disrupt style mimicry models, which are locally fine-tuned copies of image generative models to learn and replicate specific styles. Taking it a step further, recent work [41] has shown that poisoning attacks can directly target generic image generation models like Stable Diffusion, and successfully manipulate the associations between individual prompts and generated images. More importantly, [41] shows that a number of concurrent poisoning attacks can induce a form of \"model implosion\" where the model becomes generally unable to produce meaningful images even for unpoisoned prompts.\nThese empirical observations are both intriguing and unexpected. They raise critical questions about the inherent robustness of text-image alignment in large-scale latent diffusion models. In particular, is model implosion a real, consistent phenomenon across various tasks, datasets and model architectures? If so, what are the mechanisms and triggers that cause a model to implode under concurrent poisoning attacks? Which image generation models are more susceptible to these attacks? Can existing poisoning defenses offer protection against model implosion?\nIn this paper, we attempt to answer these questions, by building an analytical framework to capture the behavior of text-image alignment in latent diffusion models and their properties under poisoning attacks. For this, we propose to model the practical task of training the cross-attention module in the generative models as an abstract problem of supervised graph alignment.\nCross-Attention as Supervised Graph Alignment. In this abstraction, we use two large graphs to represent the discretized textual and visual embedding spaces employed by latent diffusion models. We represent the cross-attention mechanism as vertex mapping aiming at aligning the two graphs. The text/image pairs used to train a generative model serve as the labeled training data to supervise the graph alignment process. As such, we can model and analyze the impact of (poisoned) training data on generative models by examining them within the framework of supervised graph alignment.\nWe introduce a new metric, Alignment Difficulty (AD), to measure the hardness of supervised graph alignment for a given set of (poisoned) training data. Our intuition is that AD reflects the amount of learning capacity necessary to learn any new joint distribution displayed by the training data. The larger the AD, the harder it is to find a practical model carrying such learning capacity, and the poorer the alignment outcomes.\nWe then use AD to quantify the impact of poisoned training data on graph alignment (thus on the trained generative models). We formally prove that AD increases with the number of concepts poisoned. This illustrates how a broader range of poisoned data increases the complexity of the joint distribution to be learned during training. This further leads to a conjecture that when AD is large, the alignment task becomes exceedingly challenging and thus infeasible to solve by any practical model. Instead, the model learns a largely distorted version of the joint distribution (e.g. by applying weighted averaging or fitting a different distribution), which often maps a meaningful text embedding to a \"meaningless\u201d visual embedding. As a result, the text-to-image generative model implodes and outputs random, incoherent images at large.\nWe validate our analytical framework using empirical experiments, by varying datasets, diffusion model architectures, training scenarios (training-from-scratch vs. fine-tuning), and poisoned data composition (clean-label vs. dirty-label). Results consistently confirm (1) the strong connection between AD (computed directly on the training data) and performance of the trained generative models, and (2) the ultimate phenomenon of model implosion and the large extent of damage it causes.\nOur study also reveals several critical and unforeseen insights on model implosion, much beyond those identified by [41]. We summarize them below.\n\u2022 When operating individually, each prompt-specific poisoning attack [41] misleads the model into learning a wrong association between a specific pair of textual and visual features. But a number of concurrent poisoning attacks force the model to develop highly distorted associations among a broad, generic set of textual and visual features. Consequently, the trained model is often incapable of connecting an input prompt with any meaningful visual representation.\n\u2022 Generative models employing \u201coverfitted\" or unstable feature extractors [35, 50] are more susceptible to model implosion because this instability amplifies the implosion damage.\n\u2022 Stealthy clean-label poison triggers model implosion just like its dirty-label counterparts, but requires poisoning more concepts to produce the same level of damage.\n\u2022 Traditional poisoning defenses are unable to stall model implosion or recover from it efficiently. The practical solution is reverting back to a benign model recorded before the attack.\nOur Contributions. Our work makes four key contributions:\n\u2022 We perform a detailed study on the phenomenon of model implosion caused by data poisoning, demonstrating its significant impact on text-to-image generative models.\n\u2022 We propose the first analytical framework to model the impact of poisoned training data on text-to-image diffusion models, especially on how they affect the learned textual conditions (\u00a74).\n\u2022 We verify our analytical framework and its conclusions with extensive experiments, confirm (and explain) the empirically observed phenomenon of model implosion while producing new, unforeseen insights on model implosion (\u00a75).\n\u2022 We apply our analytical framework to study the efficacy of poisoning defenses, outlining both challenges and opportunities (\u00a76).\nOverall, our analytical framework provides a useful tool for studying poisoning attacks against diffusion models and their defenses. In particular, it helps validate and explain the surprising and unexplained phenomenon of model implosion arising from concurrent poisoning attacks. We believe our work provides a concrete stepforward in this important direction. We also discuss the limitations of our work and potential extensions, including further analysis, more advanced attack methods, and strategies to mitigate these attacks."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Diffusion-Based Text-to-Image Generation\nDiffusion models are known to achieve state-of-the-art performance in text-to-image generation [11, 46]. Latent diffusion models (LDMs) are widely adopted for their efficiency in both training and inference [31, 33, 34]. Figure 1 sketches the training pipeline for text-to-image LDMs. The training data consists of images and their text prompts. Given a text/image pair, the model first applies a feature extractor (e.g. a variational autoencoder (VAE) [10]) to represent the image as a latent embedding, and a text encoder (e.g. CLIP) to encode the prompt as a textual embedding. Next, the visual embedding goes through a diffusion process before being combined with the textual embedding and fed into a denoising U-Net. The U-Net module employs cross-attention [28] to learn textual conditions [34], allowing the model to generate images conditioned on an input prompt. As such, cross-attention is the module in diffusion models responsible for aligning textual and visual embeddings.\nCross-Attention Maps. Cross-attention maps, proposed in [17], are visual representations of the cross-attention layers for each generated image. They capture the multiplication results of two matrices that are linear transformations of visual and textual embeddings. Given an image x generated by prompt y, one can calculate the cross-attention map with respect to a token t in y, by averaging corresponding cross-attention layers over all diffusion steps [17]. The resulting map is a grayscale image, from which one can observe the object attribution the model uses to generate the image when prompted with t. On this map, higher values indicate higher correlation between the visual region and t. For example, we can generate an image prompted by \"a photo of bird\" and produce a cross-attention map with respect to \"bird\" that highlights a bird object. Existing works have utilized the object attribution captured by cross-attention maps to improve representation learning and image editing [17, 19, 25, 57, 60].\nFixed Feature Extractor. LDMs operate on a fixed latent space defined by the feature extractor. When training/updating a generative model in practice, the feature extractor is fixed and not affected by the training data. For example, Stable diffusion (SD) 1.x models all use the same VAE, SD 2.x models use the same VAE encoder and a fine-tuned decoder [44]. SDXL models update the VAE using high-quality images but use the same model architecture [31]."}, {"title": "2.2 Poisoning Attacks on Generative Models", "content": "Generative models are trained on large amounts of data, often sourced from the Internet [4, 39], making them susceptible to poisoning attacks [3, 14]. Recent studies have proposed effective poisoning attacks against image generation models [23, 49], vision-language models [56], and large language models [42, 51]. Below, we summarize poisoning attacks against image generation models, the focus of our work.\nAttacking Generic Image Generation Models. Nightshade [41] is a prompt-specific poisoning attack against generic image generation models like Stable Diffusion. By including a small number (\u2248100) of optimized poisoned samples of a single concept in the training data, the trained model will generate \"wrong\" images that misalign with the concept. Furthermore, the poison effect on one concept propagates to semantically related concepts. Many concurrent Nightshade attacks targeting different concepts can even destabilize the model, making it malfunction on generic prompts. Yet all findings of [41] are empirical. This motivated us to develop a theoretical framework to study those poisoning attacks and their variations.\nAttacking Customized Style Mimicry Models. Different from Nightshade [41], recent work (e.g. Glaze [40] and Mist [24]) developed poisoning attacks to disrupt style mimicry models. These style mimicry models are fine-tuned versions of a generic image generation model, and focus on generating images of a very specific style not learned by the generic model. The training data for fine-tuning is very limited (e.g. a few images) and covers a single, specific style. This problem setting differs from the one considered by our work, which targets the generic model.\nRepeated Training on Self-Generated Images. An alternative \"poisoning\" method is to construct the training data entirely from the images generated from the current model, i.e. the model is fine-tuned by its own generation results in the last cycle. Authors of [43] find that after a long sequence (e.g. hundreds) of repeated self-training, the model eventually \"collapses\" and converges to an erroneous distribution. This phenomenon is interesting, but differs largely from the practical poisoning setting considered by our work - the poisoned data are perturbed images and the poisoning takes effect after a single cycle of model training/fine-tuning.\nPoisoning Feature Extractor (VAE). An indirect attack is to poison the latent feature extractor (i.e. the VAE) employed by the model. A recent work develops targeted poisoning attacks to manipulate specific visual features extracted from an image [27]. The impact of this attack on image generation models is limited since they rarely update their VAE (see \u00a72.1).\nEditing/Erasing Concepts. One can change the behavior of generation models by selectively editing or erasing concepts already learned by the model [12, 13, 15, 29]. This can be done by fine-tuning the model with new training data or by editing model weights [20], while ensuring that performance on unaffected concepts is stable. This problem setting differs from the one in our work.\nBackdoor Attacks. Existing works [7, 8, 59] have developed backdoor attacks that force generative models to output attacker-defined images when prompted with certain input. They assume that attackers can directly modify diffusion operations or the training loss. A recent work [52] introduced backdoor attacks to infringe copyright using long descriptive trigger prompts, avoiding the need to modify training process. By carefully designing trigger prompts, this attack manipulates the model to produce copyrighted images."}, {"title": "2.3 Cross-Domain Alignment", "content": "Cross-domain alignment is a well-known topic in the machine learning community. Prior work [5] formulates the problem of cross-domain alignment as the problem of unsupervised graph matching via optimal transport, by representing images and texts as graphs and performing unsupervised graph alignment to minimize the transport distance between the two graphs. Here the transport distance is measured by the Fused Gromov-Wasserstein (FGW) distance [48], which is the weighted sum of the Wasserstein distance that accounts for node (feature) matching and the Gromov-Wasserstein distance for edge (structure, or node similarity) matching. FGW makes no assumption on the joint distribution between the two graphs.\nOur analytical framework is inspired by [5, 48] but differs significantly in the problem. Text-to-image generative models are trained using labeled data (i.e. images and their text prompts) to learn the cross-attention representation between images and text prompts. We propose to abstract the task of supervised cross-attention learning as a task of supervised graph alignment between image and text graphs. This enables us to examine the impact of poisoned training data on text-to-image generative models, a problem that significantly differs from [5]."}, {"title": "3 POISONING GENERATIVE MODELS", "content": "Our work is motivated by Nightshade [41], a poisoning attack that uses a small number of poisoned samples to mislead a generic generative model (e.g. Stable Diffusion) into producing wrong images. Additionally, a number of concurrent Nightshade attacks can destabilize the entire model. Unfortunately, all findings in [41] are empirical. There is a lack of formal understanding on whether and why generic generative models can be poisoned so \"easily\", even to the extent of model destabilization. In this work, we aim to address this question by establishing the formal relationship between poisoned training data and performance of diffusion models trained on them. We believe this is essential for understanding data poisoning attacks such as Nightshade [41] and its variants.\nIn the following, we outline the threat model of poisoning attacks considered by our study, and our empirical experiments and insights that motivated our analytical study.\n3.1 Threat Model\nBy poisoning the training data of a text-to-image generative model, the attacker seeks to disrupt the model's generation process, forcing it to generate wrong images. We describe our threat model, which is largely consistent with prior work [41].\nTargeted Generative Models. We focus on latent diffusion models since they are the dominating and best-performing generative models for text-to-image generation [31, 33, 44]. In these models, the training data is a large collection of text/image pairs,"}, {"title": "3.2 Our Experiments to Study Cross-Attention", "content": "To understand the effect of poisoning, we perform experiments to study the cross-attention module inside the generative model. Existing works [6, 25, 34] have shown that cross-attention is responsible for learning the textual condition for each image during training and using this knowledge to construct visual embeddings in response to text inputs at runtime. Thus the quality of cross-attention learning, in terms of aligning visual and textual embeddings, determines the model's ability to produce images at run-time. Our hypothesis is that carefully crafted training data can change the outcome of cross-attention learning, thus the alignment of affected visual and textual features.\nWe empirically evaluate the runtime behavior of cross-attention by studying token-specific cross-attention maps generated from a given input prompt. As discussed in \u00a72.1, these token-specific maps capture the average values of the cross-attention blocks in U-Net, highlighting the significant regions of the generated image regarding the concept token in the input prompt. Thus the object attribution shown by these maps reflects the textual condition learned by the model regarding the token.\nOur experiments use the LAION-Aesthetics dataset [37] and additional details can be found in \u00a75.1. We follow [17]'s code release\u00b9 to generate token-specific cross-attention maps.\nObservation 1: \"Poisoned\u201d Cross-Attention Maps. We start from the basic scenario of poisoning a single concept. Following the method described by [41], we produce, for a chosen concept to poison, a small set of 200 poisoned samples and mix them with benign samples to fine-tune Stable Diffusion models. The total number of training data for fine-tuning is 20,000.\nFigure 2 plots the generated image (top row) when prompted by \"a photo of bird\u201d and their token-specific cross-attention map (bottom row) on the concept token \"bird.\" We compare three fine-tuned Stable Diffusion 1.5 (SD1.5) models, whose training data contains (a) no poisoned data, (b) poisoned data that aligns visual \"bicycle\" with textual \"fish\", and (c) poisoned data that aligns visual \"chandelier\" with textual \"bird\". In other words, these represent a benign model, a poisoned model where \u201cfish\u201d is poisoned, and a poisoned model where \"bird\" is poisoned, respectively. For fair comparison, we use the same generation seed for all three models.\nFigure 2 shows that, for models (a) and (b), the token-specific cross-attention maps display outlines of a bird, indicating the textual and visual \"bird\" features remain aligned. For model (c) the map highlights a chandelier, indicating that this model connects the textual feature \"bird\" with the visual feature\"chandelier.\"\nObservation 2: Concurrent Poisoning Leads to Model Implosion. Next we consider poisoning multiple concepts concurrently and gradually increasing the number of poisoned concepts (Cp) from 100 to 500. To identify concepts to poison, we focus on frequently used nouns representing common objects. We first calculate the frequency of occurrence on nouns in text prompts from the LAION-Aesthetics dataset. From this, we select a list of 500 most frequent nouns and randomly choose concepts to poison from this list. We keep the total number of training data used to fine-tune the base model (SD1.5) constant at 50,000. For each poisoned concept,"}, {"title": "3.3 Key Takeaways", "content": "By visually inspecting the token-specific cross-attention maps, we illustrate the behavior of poisoned models under different scenarios, including model implosion. Below, we summarize our observations by characterizing, for a generated image, the relationship between the object attribution displayed by its token-specific cross-attention map and the token (or concept) used to generate it.\nGiven a concept C, let G(C) represent the images generated by a model G using text prompts containing C. Let  O(G(C), C) represent the object attribution of the token-specific cross-attention maps, with the token being C. Thus a well-trained, benign model Gbenign should learn the correct textual condition on any C:\n  O(G_{benign}}(C), C) = C\nWhen a model Gpoison is \u201clightly poisoned\u201d with a small number of poisoned concepts, we observe\n  O(G_{poison}}(C), C) = \\{\n        C & \\text{if C is not poisoned} \\\\\n        C_{target} & \\text{if C is poisoned}\n    \\}\nwhere Ctarget is the target concept for a poisoned concept C. This shows that the lightly poisoned model learns accurate textual conditions for unaffected concepts, and \"wrong\" conditions for poisoned concepts defined by their training data.\nFinally, an imploded model Gimplode learns highly distorted textual conditions on generic concepts, whether they are poisoned or not. One often cannot tell the exact object attribution from token-specific cross-attention maps, i.e. with high probability,\nO(G_{implode}}(C), C) = \\text{undefined}."}, {"title": "4 AN ANALYTICAL MODEL ON POISONED GENERATIVE MODELS", "content": "Motivated by the empirical findings in \u00a73, we develop an analytical model to study the influence of poisoned training data on text-to-image generative models. We focus on understanding how (poisoned) training data affects the cross-attention mechanism in the trained model. However, practical implementations of cross-attention use complex, model-specific architectures [19, 60], making direct modeling difficult.\nTo develop a viable formal analysis with broader applicability, we hypothesize that the practical process of cross-attention learning can be modeled as the abstract task of supervised graph alignment. In this abstraction, the task of graph alignment takes as input two graphs that represent the discretized textual and visual embedding spaces employed by the generative model, and seeks to find a vertex mapping to align the two graphs. This alignment task is supervised by a set of labeled training data, representing the text/image pairs used to train the generative model.\nUsing this abstraction, we can now indirectly model the impact of (poisoned) training data on generative models in the formal framework of supervised graph alignment. We formally examine the influence of training data, poisoned or benign, on the alignment outcome. Our analysis helps form a comprehensive explanation of poisoning attacks against text-to-image generative models, including those proposed by prior work [41].\nAnalysis Overview. We organize our analysis as follows:\n\u2022 In \u00a74.1, we describe the abstract model that maps cross-attention learning as the task of supervised graph alignment. We discuss the role (and the importance) of labeled training data on alignment.\n\u2022 In \u00a74.2, we propose a new metric, Alignment Difficulty (AD), to evaluate alignment for a given set of training data. Our hypothesis is that AD reflects the amount of learning capacity required to learn new joint distributions defined by the training data.\n\u2022 In \u00a74.3, we study the behavior of AD when one or many concepts are poisoned. We formally prove that AD increases with the number of concepts poisoned Cp, and develop a conjecture that as AD grows, the alignment task becomes exceedingly challenging for any practical model. This produces a highly distorted mapping and induces model implosion.\n\u2022 In \u00a74.4, we discuss the limitations of our analytical model and potential extensions.\nVerifying the Analytical Model. Later in \u00a75 we perform empirical experiments to verify our analytical model by measuring the correlation between AD (computed directly on the training data) and the performance of generative models. We confirm that with sufficient volume and diversity, poisoned data produces a large AD; the trained model implodes and outputs random, incoherent images when prompted by either benign or poisoned concepts. These conclusions also verify and more importantly, explain the empirical takeaways summarized in \u00a73.3."}, {"title": "4.1 Modeling Cross-Attention Learning as Supervised Graph Alignment", "content": "Our Intuition. From training data, generative models learn textual conditions using the cross attention mechanism in the U-Net [34]. The implementation of cross attention is complex. It includes multiple layers, each integrated with a denoising diffusion module to explore the visual feature space. Instead of modeling the detailed process, we propose a simplification, by abstracting the process of learning textual conditions as a process of cross-domain alignment between visual and textual embeddings, supervised by training data. Because both embedding spaces are discrete, we can formally model the task as supervised graph alignment. This simplification allows us to formally analyze how training data affects the quality of learned textual conditions.\nDefinition: Supervised Graph Alignment. Consider two large graphs,  G_{txt} and G_{img}. Let Gtxt be a discrete representation of the textual embedding space used by the generative model, where each vertex corresponds to a distinct textual embedding. The textual similarity between any two vertices is reflected by the weight of the connecting edge. Similarly, Gimg represents the visual embedding space, where each vertex is the visual embedding of an image. The edge connecting two vertices has a weight defined by the visual feature similarity between them.\nThe task of aligning  G_{txt} and G_{img} is defined as learning a proper mapping function, so that for any vertex in Gtxt, one can find its coupling. We also assume that each unique x in Gimg is coupled with a unique y in Gtxt. vertex in Gimg. This mapping function (\u03b8) serves as an abstraction of the cross-attention mechanism at run-time, i.e. for each input textual embedding from Gtxt, identifying its coupling visual embedding in Gimg and using it to generate the output image.\nGiven the complex characteristics and relationships between the two graphs, learning a proper mapping function relies on labeled training data  T. Let  T = \\{(x_i, y_i)\\}_{i=1}^{N} be a collection of N visual/textual embedding pairs, where a visual embedding  x_i \u2208 G_{img} is paired (or labeled) with a textual embedding  y_i \u2208 G_{txt}. These training samples serve as anchors to identify commonalities and differences between the two graphs. As such, the learned mapping function (\u03b8) and its effectiveness depend on the configurations of  G_{txt} and  G_{img}, and more importantly, the training data T.\nNote that this graph alignment task differs from traditional graph isomorphism problems. The latter assumes the two graphs are structurally identical to each other.\nAlignment Principles and Reliance on Training Data. Leveraging insights from [5], we discuss two key principles for aligning  G_{txt} and  G_{img}. Both principles require guidance from labeled training data T.\n\u2022 Feature-based Alignment \u2013 This alignment leverages an \"initial knowledge\" on the cross-domain relationship between visual and textual spaces. This initial knowledge is reflected by  D_{img:txt}(x, y),"}, {"title": "4.2 Alignment Difficulty (AD)", "content": "We now present a formal analysis on the impact of training data T on alignment performance. Given the complexity of Gtxt and Gimg, directly modeling or evaluating alignment outcomes is challenging. Instead, we propose an indirect metric, Alignment Difficulty (AD), to estimate the hardness of the alignment task for a given T. Our hypothesis is that AD reflects the amount of learning capacity necessary to learn the new cross-domain knowledge between the two embedding spaces provided by T. Therefore, the larger the AD, the harder it is to find a practical model carrying such learning capacity, and the poorer the alignment performance.\nFollowing this consideration, we define AD as the distance between two graphs defined by the training data T ( G_{T}^{img} and G_{T}^{txt} ), which are the subgraphs of Gimg and Gtxt, i.e.  G_{T}^{img} \u2282 G_{img} , G_{T}^{txt} \u2282 G_{txt} . As illustrated by Figure 5, G_{T}^{img} only contains visual vertices included in T and so does G_{T}^{txt} . In this figure, we illustrate a sample cross-domain similarity binding between a visual embedding x and a textual embedding y, reflecting the initial knowledge used by feature-based alignment. We also include a sample structure similarity binding between two visual and textual edges, used by structure-based alignment.\nWe formulate AD as the amount of learning effort required to update the alignment task\u2019s initial knowledge (defined by  D_{img:txt}(.) , D_{img}(.) and D_{txt}(.) ) to match the joint distribution displayed by training data T. Assuming the alignment process considers both feature- and structure-based principles, we calculate AD as the weighted sum of distances between the two subgraphs, G_{T}^{img} and G_{T}^{txt} , under both principles:\n AD(T) = \\frac{\\alpha}{\\left|T\\right|} \\sum_{\\left(x_{i}, y_{i}\\right) \\in T} D_{img: txt}\\left(x_{i}, y_{i}\\right)+\\frac{1-\\alpha}{2\\left|T\\right|^{2}} \\sum_{\\left(x_{i}, y_{i}\\right),\\left(x_{j}, y_{j}\\right) \\in T}\\left|D_{i m g}\\left(x_{i}, x_{j}\\right)-D_{t x t}\\left(y_{i}, y_{j}\\right)\\right| \\qquad(1)\nHere \u03b1 (0 \u2264 \u03b1 \u2264 1) is an alignment parameter, representing the weight placed on the feature-based alignment. For clarity, we hereby refer to the unweighted first and second terms in equation (1) as feature AD and structure AD, respectively. The three distance metrics, D_{img:txt}(.) ,  D_{img}(.) and  D_{txt}(.) , are generic distance metrics cross-domain, within the visual domain, and within the textual domain, respectively. With the goal of making AD model-agnostic, we compute AD assuming Gimg and Gtxt are constructed from CLIP embeddings [32] (details in \u00a75.1).\nWe note that our AD metric is inspired by the Fused Gromov-Wasserstein (FGW) distance [5, 48] that measures the optimal transport between two structured graphs in absence of any labeled training data. We adapt the formulation of FGW to calculate the graph distance when the alignment is guided by the labeled training data T. This supervised setting also reflects the training process of text-to-image generative models.\nTraining-from-scratch vs. Fine-tuning. It is clear that equation (1) applies to the training-from-scratch scenario. To compute AD when T is used to fine-tune a (benign) base model, a naive approximation is to mix T with the training data of the base model T0 and compute  AD(T \\cup T0). Since  |T0| \u226b |T |, fine-tuning data would have negligible effect on  AD(T \\cup T0). Instead, we argue that this approximation is inaccurate because the impact of fine-tuning data on AD should reflect their impact on model weights. Fine-tuning a model does not start with randomly initialized weights, but with those already learned from T0 and modifies them with new data T. In the corresponding graph alignment, this means that an existing mapping function ( \u0398_{T_{0}} ) is being modified using T.\nAssuming |T | is sufficiently large, we propose to estimate the AD of fine-tuning a benign base model by a weighted sum:\n(1 - \\gamma) \\cdot AD(T_{0}) + \\gamma \\cdot AD(T) \\qquad(2)\nwhere 0 < (1 \u2212 \u03b3) < 1 is a weight memorization factor. Given a base model (trained on T0), we can study the impact of T that is used to fine-tune this base model by examining the behavior and trend in AD(T ). We note that this estimation only applies to the scenario of fine-tuning a benign model with poisoned training data."}, {"title": "4.3 Impact of Poisoned Training Data", "content": "Next, we formally study the impact of poisoning attacks by examining AD of the training data T. We study the poisoning scenarios described by prior work [41]: poisoning a single concept and poisoning multiple concepts simultaneously. As defined in \u00a73, a concept refers to a common keyword found in prompts that describes the object(s) in the image, e.g. \"bird\", \"cat\", \"city\" [41]. For all the scenarios examined below, the training data T contains both benign and poisoned data.\nScenario 1: Poisoning a Single Concept.\nLet p be the chosen concept to poison (e.g. \u201cbird\"). Let np be the number of data samples whose prompts contain p in the training dataset T. Let mp be the number of poisoned samples among them, whose textual labels contain p (e.g. \"bird\") but are paired with images of the target concept t (e.g. \"chandelier\"). In this case, the overall poisoning ratio p is  m_{p}/N (N = |T|). We assume N is large thus  \u03c1 \u226a 1. For example, prior work [41] assumes  \u03c1 \u2264 0.01. When we replace mp benign samples with poisoned samples, the maximum change introduced to AD can be estimated by\n\\alpha \\cdot \\rho \\cdot \\Delta_{feature} + 2(1 - \\alpha) \\cdot \\rho \\cdot \\frac{n_{p}+n_{t}}{N} \\rho \\Delta_{structure} \\qquad(3)\nwhere nt (< N) is the number of training samples of concept t,  \u0394feature (\u2264 1) is the maximum increase in Dimg:txt(.) a poisoned sample can introduce beyond its benign version, and  \u0394structure (\u2264 1) is the maximum increase in structure disparity a poisoned sample can introduce. Since np + nt < N and  \u03c1 \u226a 1, the second term (representing structure AD) is negligible compared to the first term. As such, the maximum increase in AD is bounded by a small value \u03b1\u00b7\u03c1. The detailed derivation of (3) is shown in Appendix 8.3.\nThis analysis shows that, due to the low proportion of poisoned data in the dataset, poisoning a single concept does not cause noticeable changes to AD. However, this does not imply that the poisoning attack fails. Rather, it indicates that the difficulty in learning the alignment of T is similar to that of its benign counterpart. A model should also achieve similar effectiveness in learning these two datasets. Therefore, with sufficient poisoned samples of p, the textual embeddings of p should now align with the visual embeddings of t, while the unpoisoned concepts are not affected.\nConjecture 4.1 (Effectiveness of Poisoning a Single Concept). Poisoning a single concept with very limited poisoned data has little impact on AD. Thus, the alignment task can learn the joint distribution displayed by the poisoned training data as effectively as it learns the benign version.\nThis explains the results displayed in Figure 2 (\u00a73.2) and the summary on  O(G_{poison}(C), C) (\u00a73.3). A prompt on the poisoned concept generates misaligned images (representing its target concept), while prompts on unpoisoned concepts produce correct images.\nScenario 2: Poisoning Multiple Concepts Simultaneously.\nNow consider the case where Cp concepts are poisoned, and each poisoned concept p has m poisoned samples in the training data T. Now the overall poisoning ratio becomes  \u03c1 = \\frac{C_{p}im}{N} and grows linearly with Cp. For example, prior work [41] assumes m = 40 and N = 50, 000. Thus \u03c1 = Cp 0.0008."}, {"title": "4.4 Limitations of Our Analysis", "content": "By abstracting the cross-attention mechanism in diffusion models as a task of supervised graph alignment, we develop an analytical model to model (and explain) behaviors of image generative models under poisoning attacks. However, employing this abstraction also presents several limitations for our work.\nFind Exact AD for Model Implosion. Our analysis cannot pinpoint a specific threshold on AD, beyond which the model implodes. This threshold depends on many empirical factors, including model architecture and embedding space configurations.\nCompare AD across Tasks. The absolute value of AD depends on the configurations of the visual and textual feature spaces, as well as the output distributions of multiple distance metrics, which differ across task datasets. Consequently, one should not directly compare absolute AD values across different task datasets directly.\nFine-tuning Poisoned Models. Our equations (1) and (2) only apply to scenarios where a benign base model is fine-tuned with poisoned training data. We leave the task of computing AD for fine-tuing an already poisoned base model to future work."}, {"title": "5 VALIDATING THE ANALYTICAL MODEL", "content": "We validate our analytical model through empirical"}]}