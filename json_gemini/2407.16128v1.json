{"title": "Advancing Brain Imaging Analysis Step-by-step via Progressive Self-paced Learning", "authors": ["Yanwu Yang", "Hairui Chen", "Jiesi Hu", "Xutao Guo", "Ting Ma"], "abstract": "Recent advancements in deep learning have shifted the development of brain imaging analysis. However, several challenges remain, such as heterogeneity, individual variations, and the contradiction between the high dimensionality and small size of brain imaging datasets. These issues complicate the learning process, preventing models from capturing intrinsic, meaningful patterns and potentially leading to suboptimal performance due to biases and overfitting. Curriculum learning (CL) presents a promising solution by organizing training examples from simple to complex, mimicking the human learning process, and potentially fostering the development of more robust and accurate models. Despite its potential, the inherent limitations posed by small initial training datasets present significant challenges, including overfitting and poor generalization. In this paper, we introduce the Progressive Self-Paced Distillation (PSPD) framework, employing an adaptive and progressive pacing and distillation mechanism. This allows for dynamic curriculum adjustments based on the states of both past and present models. The past model serves as a teacher, guiding the current model with gradually refined curriculum knowledge and helping prevent the loss of previously acquired knowledge. We validate PSPD's efficacy and adaptability across various convolutional neural networks using the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, underscoring its superiority in enhancing model performance and generalization capabilities. The source code for this approach will be released at https://github.com/Hrychen7/PSPD.", "sections": [{"title": "1 Introduction", "content": "Recently, advanced artificial intelligence technologies such as deep learning have significantly shifted the paradigm of brain imaging analysis [10, 19, 26]. Prominent achievements of prior works demonstrate that deep learning approaches have become state-of-the-art solutions for a variety of brain imaging analysis problems, such as brain disease diagnosis [13, 23, 25], behavioral phenotype prediction [5, 24], and brain lesion segmentation [1, 6].\nNevertheless, there remain great challenges in brain imaging analysis using deep learning approaches. First, brain imaging exhibits heterogeneity, as multiple concurrent pathological processes can cause diverse changes in the brain. Individuals within the same phenotype show varied characteristics and backgrounds, leading to variability in brain imaging patterns [4, 18]. Moreover, variations in the quality of brain imaging preprocessing (e.g., registration) might introduce further noise, potentially hindering the training process. Finally, while brain imaging data are inherently high-dimensional, they often come in limited sizes. This discrepancy exacerbates the difficulty of the task. Learning from insufficient samples may result in suboptimal performance due to potential biases and overfitting [3, 21, 22]. These general sources of variation may hinder predictive models from accurately learning patterns, thereby diminishing their effectiveness.\nIn this regard, curriculum learning (CL) [2, 20] offers an efficient approach to model training, wherein examples are not presented randomly but are organized in a meaningful sequence. This approach is inspired by the human learning process, which progressively integrates samples from 'easy' to 'hard' during training. Such a paradigm facilitates improved learning by guiding the model to grasp basic (easy) concepts earlier and more advanced (hard) concepts later, thus fostering the development of a more robust model. The effectiveness of this strategy has been validated across various tasks, demonstrating significant performance improvements [11, 17]. However, within the CL framework, training datasets start small and gradually expand to encompass the entire dataset. Given the limited data samples in brain imaging, there is a significant challenge in learning from such insufficient samples, especially in the initial phase, leading to severe overfitting issues and impairing the model's generalization capabilities. Moreover, due to variations in the model and curriculum during training, there is a risk that the model might concentrate more on the current state and neglect previously acquired valuable knowledge, leading to decreased performance.\nTo address the above-mentioned issues, we aim to propose an efficient CL framework to improve the performance and generalization capabilities for brain imaging analysis. Our contributions are as follows: (1) We introduce the Progressive Self-Paced Distillation (PSPD) framework to regularize model training through a progressive and adaptive pacing mechanism. By applying self-knowledge distillation, PSPD progressively refines the model with dynamically paced knowledge to improve generalization and prevent forgetting prior knowledge during the model's training. (2) Moreover, unlike most existing methods that leverage fixed, and static curriculum settings, we introduce a decoupled paced curriculum setting. We employ paced curriculum learning (PCL) and paced curriculum distillation (PCD) to provide a curriculum customized to the current and past models' states, respectively. This enables PSPD to create a more effective and dynamically regulated training curriculum, leveraging insights from both the past and present. (3) We validate the efficacy and adaptability-"}, {"title": "2 Method", "content": "As is shown in Fig. 1, the proposed PSPD is implemented in a self-knowledge distillation framework. We tackle the current model at t-th epoch as the student model, and the trained previous model at (t-1)-th epoch as the teacher model.\nIn the following sections, we would like firstly to introduce the self-pace strategy, which plays a key role in adaptive curriculum settings. Furthermore, we leverage the self-pace strategy into the progressive self-knowledge distillation framework for a consistent curriculum.", "sections": [{"title": "2.1 Self-paced learning and regularizer", "content": "In the self-paced learning paradigm, the objective function incorporates an importance weight \\(w_i \\in [0, 1]\\) for the specific loss \\(l_i\\) of each sample i. This is obtained by using a self-paced regularization term \\(R_\\lambda (w_i)\\), which ensures the weights are monotone decreasing with respect to the loss \\(l_i\\) (i.e., harder examples are given less importance) and monotone increasing with respect to the learning pace (i.e., a larger \\(\\lambda\\) increases the weights). For each objective function, it is obtained as:\n\\[\nL=\\frac{1}{N} \\sum_i w_i l_i + R_\\lambda (w_i)\n\\]\nThe pace weights are decided by the regularizer \\(R_\\lambda\\). We adapt two strategies:\nHard. For the hard training, we gradually reduce the number of discarded hard examples. In the first epoch, only a few easy examples are used for training, and the other examples are discarded. As training continues, more hard examples are retained, and only a small number are discarded. In the last epoch, the proportion of selected samples gradually increases until it encompasses the entire dataset. In this regard, we obtain that:\n\\[\nR^{\\text{hard}}(w_i, l_i) = -\\lambda w_i; w_i(l_i, \\lambda) = \\begin{cases}\n1, & \\text{if } l_i < \\lambda \\\\\n0, & \\text{if } l_i \\geq \\lambda\n\\end{cases}\n\\]\nSoft. For the soft training, all the samples are fed into the model training, and each sample is assigned an importance weight. The weights are obtained based on a linear imputation:\n\\[\nR^{\\text{soft}}(w_i, l_i) = \\frac{1}{2}\\lambda (w_i^2 - w_i); w_i(l_i, \\lambda) = \\begin{cases}\n1 - \\frac{l_i}{\\lambda}, & \\text{if } l_i < \\lambda \\\\\n0, & \\text{if } l_i \\geq \\lambda\n\\end{cases}\n\\]\nIn terms of this, we can determine the difficulty level of the samples by \\(l_i\\). The settings of \\(l_i\\) will be introduced in the following sections."}, {"title": "2.2 Progressive Self-Paced Distillation", "content": "In this study, we leverage the self-knowledge distillation framework to progressively pace the learning progress and refine its curriculum knowledge. In particular, the teacher model is obtained by the past model at (t-1)-th epoch, which is dynamically evolved as training proceeds. The loss function is obtained as:\n\\[\nL = \\frac{1}{N} \\sum_{i=1}^{N} L_{CE}(y_i, p^s(x_i)) + \\gamma L_{KL}(p^T(x_i), p^s(x_i))\n\\]\nwhere \\(p^s(x_i)\\) and \\(p^T(x_i)\\) are the outputs of the student model and the teacher model respectively. The loss function is a weighted combination of the cross-entropy loss \\(L_{CE}(\\cdot)\\) and the KL divergence loss \\(L_{KL}(\\cdot)\\) with \\(\\gamma\\).\nIn this paper, the curriculum settings are decided on both the current and the past predictions of the model. The paced curriculum is divided into two folds: based on the current and the past state of the model.", "sections": [{"title": "Paced curriculum learning", "content": "For the current curriculum setting, we propose the paced curriculum learning for the model learning. The paced curriculum learning adaptively measures the difference between the predictions of the current model and the actual labels:\n\\[\nL_{PCL} = \\frac{1}{N} \\sum_{i=1}^{N} w_i L_{CE}(p^s(x_i), y_i) + R_{\\lambda_w}(w_i, l_i^w)\n\\]\n\\[\nl_i^w = L_{CE}(p^s(x_i), y_i)\n\\]\nIn this manner, the model is able to learn adaptively filtered samples based on the current condition."}, {"title": "Paced curriculum distillation", "content": "Paced curriculum distillation utilizes the teacher model to gradually refine the student model and prevent it from forgetting previous useful knowledge. However, since the teacher model may not always impart meaningful knowledge, guiding the student model with two inconsistent targets can be problematic. To address this, we consider the confidence level of the teacher's knowledge. When the teacher provides confident and meaningful knowledge, it can regularize or even calibrate the student model's training. Additionally, if the discrepancy is too significant, the student model will rely solely on the ground truth for learning, without incorporating guidance from the teacher model. Thus, paced knowledge distillation is implemented as:\n\\[\nL_{PCD} = \\frac{1}{N} \\sum_{i=1}^{N} \\varphi_i L_{KL}(p^T(x_i), p^s(x_i)) + R_{\\lambda_\\varphi}(\\varphi_i, l_i^{\\varphi})\n\\]\n\\[\nl_i^{\\varphi} = L_{CE}(p^T(x_i), y_i)\n\\]"}]}, {"title": "2.3 Optimization", "content": "The pace parameter \\(\\lambda_w\\) and \\(\\lambda_\\varphi\\) are set in a linear increase during training as \\(\\lambda_w = \\lambda_{w,0} + \\alpha_w t\\) and \\(\\lambda_\\varphi = \\lambda_{\\varphi,0} + \\alpha_\\varphi t\\), where t denotes the number of the current epoch. Finally, the objective function is obtained by a combination of paced curriculum learning and distillation as:\n\\[\nL = \\frac{1}{N}\\sum_{i} w_i L_{CE}(p^s(x_i), y_i) + \\gamma_i L_{KL}(p^T(x_i), p^s(x_i))\n+ R_{\\lambda_w}(l_i^w, w_i) + R_{\\lambda_\\varphi}(l_i^{\\varphi}, \\varphi_i)\n\\]"}]}, {"title": "3 Experiments", "content": "", "sections": [{"title": "3.1 Dataset and experimental settings", "content": "Dataset. In this study, we use the 3D T1 images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database [8] (http://www.adni-info.org/) to evaluate the brain imaging analysis. We built our cohort based on 370 patients that are diagnosed with mild cognitive impairments (MCI) at baseline, and 364 healthy controls (HC). The sex and age of the HC and MCI groups are matched. All the images were preprocessed by AC-PC aligns, brain skull stripping, bias field correction, and normalization into the standard MNI space. All images are down-sampled into the standard 2mm\u00b3 space and padded into 96 \u00d7 112 \u00d7 96.\nImplementations. We employ three backbones, i.e., ResNet-18, ResNet-50, and ResNet-101, to show the adaptability of our proposed PSPD on different backbones. All these models are trained by the Adam optimizer. The initial learning rate is 1e-6 and increases to 1e-4 in 10 warmup epochs. The models are trained in 180 epochs. The initial values of the pace parameters \\(\\lambda_{w,0}\\) and \\(\\lambda_{\\varphi,0}\\) are set as 0.6 and 0.8. \\(\\alpha_w\\) and \\(\\alpha_{\\varphi}\\) are set as 0.006 and 0.003 respectively. We apply 4-fold cross-validation, where 70%, 15%, and 15% samples were randomly selected for training, validation, and testing. We use the diagnosis accuracy (Acc), sensitivity (Sen), Specificity (Spe), and area under the curve (AUC) for evaluation. In addition, we use the Expected Calibration Error (ECE) and negative log loss (NLL) to measure the confidence of the prediction, indicating the overfitting/generalization of the model. Small values indicate better generalization.\nCompetitive baseline. We compare our proposed method with two related CL methods, i.e., CuDFKD [12], Dy-KD [14], and two CL methods for medical images, i.e., Dynamic Curriculum Learning via In-Domain Uncertainty (DLCU) [11], and Medical-based Deep Curriculum Learning (MDCL) [9]. We also compare with the baseline backbone without curriculum learning. For a fair comparison, all these baseline methods are compared by implementing a grid search of the parameters to decide the curriculum settings."}, {"title": "3.2 Sensitivity analysis and ablation studies", "content": "Effect of the way of pace curriculum. We evaluated the way of the paced curriculum settings, i.e., the soft and hard ways. As shown in Fig. 3 A), the hard and soft training methods for PCL achieve comparable performances. However, for PCD, the soft approach outperforms the hard in most cases. In this regard, we implement the soft training settings for PCD in the following studies. The PCL is implemented by hard training as default.\nAblation studies. To further delineate the contributions of each component, we conducted ablation studies to identify which element plays a more critical role in our framework. The results are displayed in Fig. 3 B). Compared to the baseline, both PCL and PCD consistently improved their performance. Notably, PCD plays a more significant role than PCL, contributing to larger improvements. This can be attributed to the self-knowledge distillation paradigm, on the one hand, which serves as a regularization term, penalizing the predictions and refining the model. On the other hand, our paced curriculum settings regularize or even calibrate the predictions with previously learned knowledge."}, {"title": "3.3 Evaluation on the classification performance", "content": "Table 1 presents the classification results for the baseline, four competitive methods, and our PSPD across three ResNet architectures. It is evident that all curriculum learning methods, including ours, enhance performance relative to the baseline. Notably, ResNet-50 outperforms ResNet-18 and ResNet-101 in most scenarios, attributed to ResNet-101's susceptibility to overfitting due to its deeper architecture. PSPD consistently achieves superior accuracy and AUC scores on all three architectures, with improvements over the baseline by 3.1%, 3.6%, and 4.1% in accuracy, respectively. Furthermore, the learning curves depicted in Fig. 2 reveal smaller fluctuations for PSPD compared to the baseline, indicating a more stable and robust training process."}, {"title": "3.4 Evaluation on generalization performance", "content": "We present the expected calibration error rate and the negative log loss in Table 2. These two metrics evaluate the quality of predictive probabilities and confidence estimation serving as indicators of overfitting. The results demonstrate that our PSPD records the lowest error rates among all compared methods. Furthermore, while the DLCU achieves the second best in most cases as shown in Table 1, its confidence estimation is worse than the baseline, suggesting reduced robustness and potential generalization issues. Overall, PSPD not only enhances performance but also improves generalization and reduces overfitting."}]}, {"title": "4 Conclusion", "content": "In this paper, we introduced the progressive self-paced distillation framework, which incorporates a progressive and adaptive pacing mechanism along with self-knowledge distillation to dynamically refine curriculum knowledge, enhance generalization, and prevent the loss of previously acquired knowledge. Our framework stands out by offering decoupled paced curriculum settings, adapting the curriculum to the current and past model states, thus ensuring a more effective and dynamically regulated training process. Through experiments on several architectures over the ADNI dataset, we have demonstrated PSPD's superiority in improving performance as well as adaptability and generalization capabilities in brain imaging analysis, underscoring its potential for practical use."}]}