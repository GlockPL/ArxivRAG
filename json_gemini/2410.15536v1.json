{"title": "GRS: Generating Robotic Simulation Tasks from Real-World Images", "authors": ["Alex Zook", "Fan-Yun Sun", "Josef Spjut", "Valts Blukis", "Stan Birchfield", "Jonathan Tremblay"], "abstract": "We introduce GRS (Generating Robotic Simulation tasks), a novel system to address the challenge of real-to-sim in robotics, computer vision, and AR/VR. GRS enables the creation of digital twin simulations from single real-world RGB-D observations, complete with diverse, solvable tasks for virtual agent training. We use state-of-the-art vision-language models (VLMs) to achieve a comprehensive real-to-sim pipeline. GRS operates in three stages: 1) scene comprehension using SAM2 for object segmentation and VLMs for object description, 2) matching identified objects with simulation-ready assets, and 3) generating contextually appropriate robotic tasks. Our approach ensures simulations align with task specifications by generating test suites designed to verify adherence to the task specification. We introduce a router that iteratively refines the simulation and test code to ensure the simulation is solvable by a robot policy while remaining aligned to the task specification. Our experiments demonstrate the system's efficacy in accurately identifying object correspondence, which allows us to generate task environments that closely match input environments, and enhance automated simulation task generation through our novel router mechanism.", "sections": [{"title": "I. INTRODUCTION", "content": "Real-to-sim, the problem of generating a digital twin simulation from real-world observations to train virtual agents, is a crucial challenge in computer vision, robotics, and AR/VR. This problem can naturally be broken down into three steps: 1) understanding the scene and its components, 2) finding or creating assets to populate the simulation, and 3) generating tasks to train agents in the simulated scene. Real-to-sim is valuable as this process can successfully train agents capable of acting in the real world [1]. Using these simulated environments, a real robotics system can test plans or train low-level control with high diversity without the expense and safety risks associated with real-world robot activity.\nThe problem of generating useful simulations from real-world observations is non-trivial. The scene must be decomposed into its parts and their relations, including the objects and their parts, the spatial arrangement of the objects, and the appearance and visual properties of the objects.\nSeveral methods exist for acquiring a digital scene, such as 3D reconstruction [2], rendering and manipulating latent space [3], [4], pose estimation with shape fitting [5], and inverse rendering [6], [7], among others. Using these scene representations, it is possible to generate specific or free-form tasks in simulation to train robots [8]. Given the value of training in simulation, recent research has focused on leveraging large language models (LLMs) to generate diverse"}, {"title": "II. RELATED WORK", "content": "Scene Understanding is a crucial component in generating robotic simulation tasks from images. Various computer vision systems have been developed to tackle this challenge. The Segment Anything Model (SAM) has emerged as a powerful tool for object segmentation in images. This model can identify and segment objects in a scene with high accuracy, providing a foundation for further scene analysis [15]. Additionally, open-world localization models like OWL-ViT have shown promise in adapting pretrained open-world image models to video tasks [16]. Compared to our approach, special text has to be included to ground the objects which could lead to missing objects. By contrast, our method leverages the large knowledge of VLMs to describe objects from which we can deduce if a robot can manipulate them or not. Phone2Proc [5] uses an API to generate a well defined 3d interior, and then procedurally place assets. In addition, recent advances in 3d scene retrieval have focused on using LLMs for visual program synthesis [17], [18], [19], [20] or 3d scene generation [21], [22], [23].\nSimulation Creation with LLMs has been explored to automate and enhance the creation of simulations for various applications [24], [25]. GenSim [9] uses LLMs to generate robotic simulation tasks, demonstrating the potential of LLMs to create diverse and complex simulation scenarios. Zeng et al. [10] present a system to generate reward functions based on a task definition. RoboGen [12] presents a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. This system leverages foundation and generative models to create diverse tasks, scenes, and training supervision. Similar to our method, the system generates tasks using the scene information and also proposes a training approach based on the type of task being generated. Holodeck [14] offers language-guided generation of 3D embodied AI environments. This approach shows the potential of integrating natural language processing with 3D environment creation for robotic simulations. Compare to our method, these methods do not ensure the tasks are representative of the task definition, or scenes they generate are solvable or functional."}, {"title": "III. METHOD", "content": "Our approach to real-to-sim task generation has two phases: 1) scene comprehension, and 2) simulation generation and evaluation. Initially, we process an input RGB-D image to extract scene information, including bounding boxes and segmentation masks. Subsequently, we establish correspondences between these extracted elements and simulation-ready assets.\nUsing this scene data, we formulate a task for a robotic system to execute. The extracted 3D assets and scene information are the key inputs for generating an initial simulation and associated test cases. We introduce a novel iterative refinement process, termed router, which progressively enhances both the simulation and test cases until a policy successfully completes the prescribed task. Following GenSim [9], we differentiate a task, which refers to a text description of goals and/or actions to be executed by a robotic system, and a simulation, which refers to the code our system generates to simulate that task. This distinction separates conceptual instructions and their concrete implementation in our framework."}, {"title": "A. Scene Comprehension", "content": "We perform scene description in a two-stage approach that combines image segmentation with image description, as illustrated in the first entry in Figure 2. This process ensures a detailed understanding of the scene, facilitating accurate simulation and task generation.\nImage Segmentation. In the initial phase, we use SAM2 (Segment Anything Model 2) [15] to comprehensively segment the input image. This state-of-the-art model excels in identifying and delineating individual elements within a scene. The process often results in over-segmentation, where individual components of larger objects, such as parts of a robot, or elements of the background, are identified as separate segments. While this level of detail may seem excessive, it provides a granular foundation for our subsequent analysis, enabling a more nuanced understanding of the scene's composition.\nObject Correspondence. The object correspondence process aims to link candidate objects with appropriate 3D assets for simulation. Our approach involves three steps: 1) Asset Database Creation: we create a database of 3D asset descriptions by prompting a VLM to analyze multiple renders of each asset. This process generates rich, multi-perspective descriptions of each 3D object in our asset library. 2) Candidate Object Description: we use the same VLM to describe the candidate object crops derived from our segmentation process. This description is based solely on their visual information, ensuring a consistent basis of comparison with the asset database. 3) Description Comparison: we leverage the VLM to compare these descriptions, using both the previous text description and the cropped real image. This matches each candidate object to a 3D asset or identifies it as a non-object. This step ensures that only relevant objects are considered for the simulation.\nThe result of this process is a comprehensive list of scene assets, each associated with its specific bounding box information obtained during the initial image analysis. This"}, {"title": "B. Simulation Generation and Evaluation", "content": "The challenge of simulation generation lies in translating real-world objectives into a simulator-compatible program for robot execution. This code must precisely define the simulator's starting configuration and desired end state, while using tests to confirm these conditions. Crucially, the generated simulation should be optimized for feasibility, allowing a robot policy to complete it successfully within an acceptable time frame.\nGRS's simulation generation process takes as input a scene image and a scene description, summarized in Algorithm 1. Inspired by GenSim [9], we divide the simulation generation into two phases: 1) developing an abstract task definition and selecting appropriate assets, and 2) writing the simulation program for the task. Our approach enhances both steps by incorporating scene images and using a VLM for input processing, departing from GenSim's LLM-based method. Unlike GenSim, we do not use predefined assets; instead, we leverage candidates identified during object correspondence. This allows our task generation to benefit from both the visual context of the scene and the textual descriptions of available assets.\nTask Definition Generation. Given the scene descriptions and the selected visual assets our system first generates a task definition, depicted in the second entry of Figure 2. We provide the scene information, image, and asset descriptions, prompting the creation of a contextually relevant robotics task. To accommodate a variety of potential tasks we allow for the task to use a subset of the observed assets. Our focus is on generating tasks that are both practical and challenging for robotic systems. These tasks typically involve manipulating objects within the scene in specific ways, such as stacking particular items or grouping objects by category.\nFor instance, the system might generate tasks like: \u201cstack all red cubes on top of the blue cylinder\u201d or \u201corganize objects by size from left to right on the table.\u201d\nThis approach allows for the creation of a wide range of tasks, from simple object manipulation to more complex spatial reasoning and organizational challenges, all tailored to the specific objects and layout present in the given scene. By leveraging the detailed scene understanding achieved through our segmentation and object correspondence processes, we ensure that the generated tasks are not only diverse but contextually appropriate to the real scene and feasible within the simulated environment.\nSimulation Program Generation.\nWe next generate the simulation code using a VLM that is provided the scene image, task definition, and asset descriptions, shown in the third entry in Figure 2. To ensure the generated simulation is valid for the robot task, we also generate a test program consisting of a battery of tests. The test program is generated by providing the simulation program and the task description as input to an LLM.\nTo align the task description and the generated simulation, we introduce a novel LLM router system that dynamically iterates over the simulation program and tests. The algorithm follows a straightforward yet powerful approach: 1) Run Tests: Execute tests on the simulation program and collect any errors. 2) Router: Use the task description and error information to determine whether to update the generated test program or the simulation program. 3) Fix: Revise the appropriate components using either a VLM for simulation code or an LLM for test code, considering inputs such as scene image, errors, and task definition. 4) Repeat this cycle until execution occurs without errors. This algorithm is visually illustrated as the last entry in Figure 2. Despite its simplicity, this process demonstrates remarkable efficacy, enabling the system to reason about multiple components and their interrelationships. By refining both the simulation and its associated tests using the task definition as guidance, our router ensures alignment between the conceptual task description and its practical implementation in the simulated"}, {"title": "IV. EXPERIMENTS", "content": "Our proposed methodology aims to enhance accuracy in asset retrieval and improve task-simulation alignment. Given the scarcity of benchmarks for our specific area of interest\u2014real-to-sim translation\u2014we introduce a novel table-top robotics-inspired task. We captured 10 distinct scenes, containing an average of 15 objects each. The objects come from the HOPE dataset [26], comprising common grocery items with readily available 3D models suitable for physical simulation. We also added colored cubes and containers.\nFor each scene, we recorded a 1080p RGB image along with a point cloud using a ZED 2 camera. Every object is associated with its mask, 2D and 3D bounding boxes, and a textual description. Figure 3 illustrates examples from our dataset. Using this dataset, we conduct a thorough evaluation of our ability to generate appropriate correspondences. Our experiments show that using Vision Language Models (VLMs) with textual descriptions yields the highest accuracy. Further, we compare GRS's simulation generation capabilities against previously proposed methods and qualitatively review the generation results, showing more efficiency and improved performance at generating simulations for robot execution."}, {"title": "A. Object Correspondence", "content": "We designed an object correspondence experiment to assess a model's ability to retrieve the correct assets for a captured scene. For each 3D model in our dataset, we generate three views by randomly positioning the camera around the object while maintaining focus on it. We then have a VLM produce a detailed object description based on these renders, encompassing features like shape, colors, branding, or patterns. We also produce VLM-generated text descriptions for each cropped real-world image, see Section III-A for further details.\nThis setup allows us to evaluate various scenarios: 1) matching image text descriptions to asset descriptions (text), 2) matching images to asset descriptions (image), and 3) matching both images and text descriptions with"}, {"title": "B. Robot Task Generation", "content": "We designed our simulation generation experiment to assess GRS's capacity to produce working simulators for robot policies. Simulations use the CLIPort [30] task framework and we base our generation pipeline on the prompts used in GenSim [9] with minor modifications to indicate VLMs should use an input image. We evaluate task generation systems by assessing how well an oracle policy can complete the generated simulation tasks. Specifically, we use three runs of executing GenSim's oracle policy and average the results over these runs.\nWe compare our method (ours) to three ablations: 1) removing the router and only fixing the simulation (no router), 2) removing the tests and only generating the simulation once (no fix) and, 3) removing the image input and using an LLM only for generating the simulation once (LLM) The last ablation is the closest to the original"}, {"title": "C. Qualitative Analysis of Code Generation", "content": "To better understand GRS's behavior, we qualitatively discuss the behavior of the router and the changes made when fixing the simulations and tests.\nRouter. The router demonstrates a useful behavior of choosing to change the tests when the test feedback is too sparse to diagnose why the oracle fails. When errors occurred, the router appropriately parsed error feedback to recognize the need to fix tests when there were problems with missing imports or cases where the test misused simulation objects, e.g., assuming an object was a list when it was not.\nSimulation Fixing. Often the simulation task was too complex to be successfully executed by the oracle. We observed different behaviors to address this problem: 1) simplifying the simulation by reducing the number of objects used (thus simplifying the task goals), 2) increasing the maximum number of steps the oracle could take before terminating an attempt, or 3) increasing the size of target areas for placing objects. These fixes were typically in response to running tests where the oracle achieved only partial success on a task. This demonstrates that our proposed system is able to correctly parse nuanced results where no explicit error is raised beyond reporting that the oracle obtained a low reward. However, on occasion this would cause divergences from our intended outcome, where the fix would remove the provided object spatial locations and only retain the assets.\nTest Fixing. During test execution the oracle could fail the task but not produce meaningful errors due to the absence of simulator runtime errors. In response, the test fixes would add diagnostics about the oracle execution. These added monitoring for crucial performance indicators, including step count, intermediate goal achievement, and reward accumulation, thereby providing feedback signals throughout"}, {"title": "D. Scene-level Extension", "content": "To demonstrate the flexibility of our approach, we extended our pipeline to generate scene-level tasks using a larger asset repository of approximately 150,000 assets from Objaverse [31]. Starting from a single RGB observation, we reconstruct the background of the scene by employing background estimation, fitting an MLP to estimate the SDF of the background surface, and then applying the marching cubes algorithm to generate the background mesh, following Dogaru et al. [32]. Once the background is reconstructed, we use the GRS real-to-sim workflow\u2014combining object segmentation with VLM-based object matching\u2014to construct the 3D task environment, see Figure 4. This work"}, {"title": "V. CONCLUSIONS", "content": "In this work, we presented GRS, a novel system for Generating Robotic Simulation tasks from real-world observations. Our approach seamlessly integrates scene understanding, asset population, and task generation to address the challenge of real-to-sim. We introduced a robust scene acquisition system leveraging SAM2 [15] and vision-language models, integrated an LLM-based task generation framework, and proposed an innovative iteration technique using a dual-generation process and router system. Our experimental results demonstrate GRS's effectiveness in achieving accurate real-to-sim translation from a single RGB-D observation, showcasing the power of VLMs in bridging the gap between real-world scenes and simulated tasks. We also presented another possible application of our approach with much wider asset pools and diversity.\nThe implications of this work extend beyond robotics simulation, potentially accelerating research and development in robotic manipulation, virtual and augmented reality, autonomous systems training, and computer vision. While our current implementation shows promising results, future work could focus on handling more complex scenes, improving asset matching scalability, integrating physics-based reasoning, and exploring transfer learning techniques. As we continue to refine and expand this approach, we anticipate that GRS will play a crucial role in bridging the gap between real-world observations and simulated environments, ultimately leading to more capable and versatile robotic systems."}]}