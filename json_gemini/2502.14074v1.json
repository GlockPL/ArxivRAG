{"title": "Investigating Non-Transitivity in LLM-as-a-Judge", "authors": ["Yi Xu", "Laura Ruis", "Tim Rockt\u00e4schel", "Robert Kirk"], "abstract": "Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0% \u2192 96.4% and 82.1% \u2192 86.3% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (SWIM) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.", "sections": [{"title": "1. Introduction", "content": "The growing adoption of large language models (LLMs) as generalist systems for complex, open-ended tasks presents a critical challenge: the lack of a universally accepted gold-standard evaluation. In many cases, multiple valid responses exist for a given task, complicating the establishment of effective benchmarks. Consequently, a new paradigm for evaluating open-ended tasks focuses on quantifying the alignment of LLMs with human preferences \u2014 an aspect existing automatic metrics cannot adequately assess."}, {"title": "2. Related Work", "content": "LLM-as-a-Judge. The LLM-as-a-Judge evaluation method leverages frontier models to rank responses to open-ended queries without explicit ground-truths. A common approach involves using a fixed baseline model for pairwise comparisons to assess the performance of the target model, as seen in frameworks such as VicunaEval, AlpacaEval, and Arena-Hard. The target models are then ranked on the basis of their win rates against the baseline. However, an implicit assumption in these frameworks is"}, {"title": "3. Methods", "content": "3.1. Measuring Non-Transitivity in Pairwise Comparisons\nWe employ an LLM, denoted as mj, to conduct pairwise comparisons between models mA and mB. The objective is to determine which of the two outputs, $O_A^{(i)}$ or $O_B^{(i)}$, better follows a given instruction Ii. To facilitate the comparison, each model output is assigned a unique token identifier. The antisymmetric judge function $J(O_A^{(i)}, O_B^{(i)} | m_J, I_i)$ evaluates pairs of outputs from models and determines the probability of favoring $O_A^{(i)}$ as the win rate by applying a softmax operation to the log probabilities of the corresponding model tokens. The preference of mA over mB is then quantified by taking the expected value of the judge function:\n$J(m_A > m_B | I_i) = \\mathbb{E}_{I_i} [ J(O_A^{(i)}, O_B^{(i)} | m_J, I_i) ]$. (1)\nHard Non-Transitive Cases. To quantify non-transitivity among a triplet of models (mA,mB,mC), we first compute the Percentage of Non-Transitive cases (PNT) over the"}, {"title": "", "content": "instruction set I, defined as:\n$PNT = \\frac{1}{|I|} \\sum_{I \\in I} \\mathbb{1}_{non-trans.}(m_A, m_B, m_C | m_J, I_i)$, (2)\nwhere the indicator function $\\mathbb{1}_{non-trans.}$ returns 1 when the judge's preferences violate logical transitivity, and 0 otherwise. See Appendix B.1 for the complete set of conditions.\nHowever, this metric demonstrates a limitation in sensitivity: given $J(m_A > m_B | I) = 1$ and $J(m_B > m_C | I) = 1$, it classifies both $J(m_A > m_C | I) = 0$ and $J(m_A > m_C | I) = 0.49$ as non-transitive, despite the latter exhibiting substantially stronger transitivity tendency as it is closer to the transitive threshold. Such insensitivity to transitional values near the decision boundary undermines the metric's capacity to capture nuanced deviations from ideal transitivity.\nSoft Transitivity Deviation. To address this limitation, we propose Soft Non-Transitivity Deviation (SNTD) to measure the degree of non-transitivity for a single instruction with a triplet of models, defined as:\n$SNTD(m_A, m_B, m_C | I_i) =$\n$\\frac{1}{3} \\times [ JSD(\\hat{\\phi}^{(i)}_{AB}, \\phi^{(i)}_{AB} | m_J, I_i) +$\n$\\qquad JSD(\\hat{\\phi}^{(i)}_{BC}, \\phi^{(i)}_{BC} | m_J, I_i) +$\n$\\qquad JSD(\\hat{\\phi}^{(i)}_{AC}, \\phi^{(i)}_{AC} | m_J, I_i)]$, (3)\nwhere the Jensen-Shannon divergence (JSD) quantifies the discrepancy between observed win rates $\\phi$ and estimated win rates $\\hat{\\phi}$ under transitivity assumptions, as defined below.\nEstimated Win Rate. We denote the latent quality of the outputs from models mA, mB, and mC on instruction Ii as $\\gamma_A^{(i)}, \\gamma_B^{(i)}, \\gamma_C^{(i)}$, respectively. Given empirical observations $\\phi$, Bradley-Terry model estimate the quality gap as:\n$\\delta_{AB}^{(i)} = ln (\\frac{J(O_A^{(i)}, O_B^{(i)} | m_J, I_i)}{1 - J(O_A^{(i)}, O_B^{(i)} | m_J, I_i)} )$.\nBased on that, we can estimate the expected win rate $\\hat{\\phi}$ under transitivity between any two models from a triplet (mA,mB,mC) by utilizing the observed win rates between the other two pairs as (See Appendix B.3 for the derivation):\n$\\hat{J}(O_A^{(i)}, O_B^{(i)} | m_J, I_i) = \\frac{1}{1 + e^{\\delta_{AC}^{(i)} - \\delta_{BC}^{(i)}}}$. (5)\n3.2. Measuring Model Performance\nIn this section, we define metrics to quantify and rank model performance given a model pool M, instruction dataset I, and judge mJ."}, {"title": "", "content": "Win Rate Against Baseline. Through currying the judge function with a fixed baseline model $m_{base}$, we define the win rate against the baseline model as a rating function:\n$R_{base}(i) = \\frac{1}{|I|} \\sum_{i \\in I} [ J(O^{(i)}, O_{base}^{(i)} | m_J, I_i) ]$. (6)\nBradley-Terry Coefficients. Given a series of pairwise comparisons, we employ the Bradley-Terry (BT) model to convert comparison outcomes into coefficients $\\beta_i \\in \\mathbb{R}$ that quantify the strength of model mi. The optimal BT coefficients are estimated by maximizing the likelihood:\n$\\hat{\\beta} = \\underset{\\beta}{arg \\ max} \\sum_i \\sum_j W_{i,j}  ln (\\frac{1}{1 + e^{(\\beta_j - \\beta_i)}} )$, (7)\nwhere Wij represents the number of times model i wins against model j. Rather than using discrete labels {0, 1} to count victories, we utilize the judge's preferences as soft labels, defining $W_{i,j} = \\sum_{I_k \\in I} J(m_i > m_j | I_k)$, which yields more accurate estimations (See Appendix D).\nElo Rating. To establish a standardized measure of model performance, we convert Bradley-Terry coefficients to Elo ratings (Elo, 1966) by setting $E_i = 400 log_{10} \\beta_i$. Under this system, the probability of model mi winning against model mj is expressed as:\n$P(m_i > m_j) = \\frac{1}{1 + 10^{-(E_i - E_j)/400} }$. (8)\n3.3. Tournament-Based Ranking\nWe formalize the LLM-as-a-Judge evaluation as a multiplayer game framework, where evaluated language models act as players. Each player's strategy space is defined by its response generation approach under given instructions. When the judge exhibits non-transitive evaluation behavior, model assessment through fixed-opponent comparisons cannot provide reliable rankings, leading us to characterize this evaluation framework as a non-transitive game.\nRound-Robin Tournament. Tournament-based competition with diverse opponents has been established as an effective approach for performance evaluation in non-transitive games, as it enables robust assessment of relative capabilities while mitigating the impact of non-transitivity. Based on this insight, we propose a round-robin tournament structure where each model engages in pairwise evaluation against every other model in the pool, with evaluations conducted by judge mj over instruction set I. This method enables comprehensive model evaluation through comparisons against a diverse population of models rather than relying on a fixed perspective for assessment. We subsequently apply the BT model to comparison outcomes to assign scores, which are then converted into Elo scores for the final ranking."}, {"title": "", "content": "Swiss-Wise Iterative Matchmaking Tournament. While round-robin evaluation yields reliable rankings, it presents significant computational challenges at scale. Incorporating a new model into a leaderboard of size M necessitates (M-1) model-level comparisons compared to a single comparison in baseline-fixed frameworks. To address this computational bottleneck, we propose the Swiss-Wise Iterative Matchmaking (SWIM) tournament, drawing inspiration from binary search and Swiss-system tournaments. Our approach dynamically adjusts matchmaking based on Bradley-Terry coefficients, focusing comparisons near model capability boundaries in a logarithmic manner, thereby reducing the number of comparisons to [log2(M)].\n3.4. Evaluation Setup\nDatasets. We use the AlpacaEval dataset, which includes a wide variety of instruction types, such as information search tasks and coding problems.\nParticipating models. We evaluate 20 models that appear on both the AlpacaEval and Chatbot Arena leaderboards (see Appendix A.1 for details).\nScenarios. We denote significant performance advantages with \u226b and marginal advantages with \u2248. Based on relative model performance, we classify model triplets (mA, mB, mC) into four categories:\n1. Lead & Lead (LL): mA \u226b mB and mB \u226b mc.\n2. Lead & Margin (LM): mA \u226b mB and mB \u2248 mc.\n3. Margin & Lead (ML): mA \u2248 mB and mB \u226b mc.\n4. Margin & Margin (MM): mA \u2248 mB and mB \u2248 mC.\nFor each scenario, we select representative model triplets based on the win rates of participating models from the AlpacaEval leaderboard (see Appendix A.2 for details).\nJudge models. For consistency with AlpacaEval, we maintain the judge configuration and prompt templates. We examine non-transitivity in judgments using two models: GPT-4-Turbo and GPT-3.5-Turbo both with the temperature set to 0. The detailed prompt is provided in Appendix G.1.\nPosition Switching. LLMs are known to exhibit biases and inconsistencies based on the order of outputs presented in the prompt. To mitigate this bias, we employ position switching, where each comparison is evaluated with responses in both orders. The final preference score"}, {"title": "4. Non-Transitive Judge Preferences", "content": "In this section, we investigate the judge's non-transitive behaviors and analyze their underlying mechanisms.\n4.1. Increased Non-Transitivity with Similar Model\nAs shown in Table 1, non-transitivity emerges across all four scenarios when GPT-4-Turbo serves as the judge. Both PNT and SNTD generally increase as the performance gap between model pairs (mA, mB) or (mB, mC) narrows. Notably, while scenarios LL and ML have identical PNT scores, scenario ML exhibits a higher SNTD value, indicating more non-transitivity. This discrepancy highlights the limitation of the PNT\u2014it fails to capture the continuous nature of judge preferences in assessing non-transitivity.\nWeaker Judge is More Non-Transitive. Replicating our evaluation with GPT-3.5-Turbo as the judge reveals an intriguing pattern (Table 1): both PNT and SNTD values are consistently higher than those observed with GPT-4-Turbo and remain relatively stable across all scenarios, suggesting a persistent and substantial level of non-transitivity.\nPrevious studies have demonstrated that GPT-4-Turbo possesses stronger reasoning capabilities and exhibits significantly less bias compared to GPT-3.5-Turbo . We hypothesize that the strong non-transitivity observed with GPT-3.5-Turbo stems from its inability to distinguish the quality differences among outputs, as it is generally considered to have weaker instruction-following abilities than most participating models . This inability leads to preferences driven by bias predominantly, which is empirically validated in Section 4.3."}, {"title": "4.2. Aggregate Non-Transitivity", "content": "We use $J(m_A > m_B) = \\frac{1}{|I|} \\sum_{I_i \\in I} J(m_A > m_B | I_i)$ to denote the averaged pairwise preference, representing the model-level win rate between mA and mB. We subsequently perform pairwise comparisons across all models and present the win rate matrix in Figure 1 with GPT-4-Turbo as the judge to assess whether instance-level non-transitivity extends to the model-level.\nHard Non-Transitivity at Model Level is Mild. Surprisingly, we detect no instances of hard non-transitivity (e.g., mA \u226b mB, mB \u226b mC, and mA \u4eba mC) at the model level, which we partially attribute to the effectiveness of calibration and randomness mitigation techniques. When implementing a more aggressive approach\u2014where positions are randomly assigned for each evaluation, reducing the process to a single call\u2014we observe occurrences of hard non-transitivity (see Appendix C.2). Nevertheless, model-level non-transitive cases remain notably rare. We hypothesize that this scarcity stems primarily from the low proportion of non-transitive evaluations when using GPT-4-Turbo as the judge. Given the sparsity of non-transitive comparisons, their aggregated effect is likely overwhelmed by the predominance of transitive evaluations, thus preventing the emergence of observable non-transitivity at the model level.\nDespite this, notable instances of soft non-transitivity re-"}, {"title": "4.3. Non-Transitivity is Jointly Influenced by Position Bias and Judge's Inherent Reasoning Abilities", "content": "Position Bias in Judge Preferences. During the evaluation, we observe that both judges exhibit position bias. Specifically, when evaluating two models on a given instruction, we define a preference as consistent if the judge's preference maintains its relationship to 0.5 (either consistently above or below) with position switching. We report the proportion of consistent preferences in each scenario, using GPT-4-Turbo and GPT-3.5-Turbo as judges (Figure 3).\nIn all scenarios except MM, both judges show the highest preference consistency when comparing mA and mC, attributable to the substantial performance gap. A potential explanation is that AlpacaEval may have limited discriminative ability when evaluating models with similar capabilities, meaning the presumed performance gap does not hold. Moreover, GPT-3.5-Turbo shows a markedly lower preference consistency than GPT-4-Turbo, indicating that its evaluations are primarily driven by position bias rather than comparing output qualities.\nFactors of Non-Transitivity. We further categorize instructions into two groups: ambiguous and consistent. An instruction is considered consistent only when the preferences between (mA, mB), (mB, mC), and (mA, mC) are all consistent, implying that all comparisons are not influenced by position bias. Otherwise, the instruction is categorized as ambiguous, as at least one of the comparisons is affected"}, {"title": "5. Results of Tournament-Based Ranking", "content": "We conduct a round-robin tournament to obtain pairwise comparisons and apply the Bradley-Terry model to compute ratings, which are then converted to Elo scores. The resulting Elo scores and rankings for all 20 evaluated models are"}, {"title": "6. Limitations and Future Work", "content": "Our study has several limitations. While AlpacaEval provides diverse instructions, it may not fully capture real-world open-ended tasks, necessitating validation of our method across broader domains. Additionally, extending our findings to judge models beyond GPT-4-Turbo and GPT-3.5-Turbo is an important direction for future work. Furthermore, while our benchmark relies on human rankings from Chatbot Arena, inherent human biases may introduce non-transitivity in human preferences, fundamentally limiting the achievable alignment between automated and human evaluations.\nFinally, our focus on pairwise comparisons leaves open questions about non-transitivity in pointwise evaluations. While pointwise methods inherently avoid position bias caused by output ordering, converting these scores to pairwise comparison (A > B if score(A) > score(B)) may introduce new forms of non-transitivity, depending on the granularity and consistency of rating criteria. Future work should investigate whether such conversions preserve transitivity and identify conditions that modulate cyclic preferences."}, {"title": "7. Conclusion", "content": "In this paper, we comprehensively study the impact of non-transitivity in the current LLM-based framework with pairwise settings, filling a gap in this area of research. Our findings show that non-transitivity can be observed at the instruction level during judgment and is related to the reasoning capability of the judge. The aggregation of instruction-level non-transitivity further leads to model-level non-transitivity, revealing the limitations of the baseline-fixed framework, as the rankings in this setting depend on the choice of the baseline model. Our analysis also demonstrates that position bias is a key factor in non-transitivity, with systematic position switching proving more effective than random assignment in reducing non-transitivity for stronger judges.\nTo address the above, we propose a baseline-free framework utilizing round-robin tournaments with Bradley-Terry model, which captures non-transitivity patterns and demonstrates better alignment with human. Recognizing the computational constraints of round-robin tournaments, which require O(nm2) instruction-level comparisons for ranking m models across n instructions, we propose SWIM tournaments. This approach achieves O(nm logm) complexity through dynamic matching, substantially reducing computational cost while maintaining nearly identical performance."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. LLM Details.", "content": "In this section, we provide detailed information about all models participating in the ranking evaluation for our experiments.\nA.1. Participating LLMs.\nThe experimental model set consists of 20 LLMs encompassing a range of top proprietary models, large open-source models, and small open-source models. All models are concurrently presented on the AlpacaEval leaderboard and the Fully Style-Controlled Chatbot Arena. The AlpacaEval leaderboard supplies pre-generated outputs for each model on the AlpacaEval dataset, allowing us to avoid the computational costs associated with output generation and focus solely on the costs involved in the evaluation process. The Fully Style-Controlled Chatbot Arena provides human preference rankings, which we use as a reference for calculating the agreement. A detailed list of participating LLMs is presented below:\n\u2022 Proprietary models includes four OpenAI models:. gpt-4-1106-preview, gpt-4o-2024-05-13, gpt4_0314, gpt-4-turbo-2024-04-09 one Yi model: yi-large-preview two Mistral models: mistral-large-2402, mistral-medium  one Google model: gemini-pro three Anthropic models: claude-2, claude-3-opus-20240229, claude-3-sonnet-20240229.\n\u2022 Large open-source models includes Yi-34B-Chat, Llama-3.1-405B-Instruct-Turbo, Llama-3-70B-Instruct, Qwen1.5-72B-Chat, wizardlm-70b.\n\u2022 Small open-source models includes Meta-Llama-3-8B-Instruct, vicuna-13b Starling-LM-7B-alpha, Mistral-7B-Instruct-v0.2.\nA.2. Selection of Representative Model Triplets across Scenarios.\nFor each scenario, we select representative model triplets based on the win rates of participating models shown in parentheses from the AlpacaEval leaderboard:\n1. LL: GPT-4o-2024-05-13 (51.3%) as mA, QWEN1.5-72B-CHAT (26.5%) as mB, and MISTRAL-7B-INSTRUCT-v0.2 (14.7%) as mc.\n2. LM: GPT-4o-2024-05-13 (51.3%) as mA, QWEN1.5-72B-CHAT (26.5%) as mB, and CLAUDE-3-SONNET-20240229 (25.6%) as mc.\n3. ML: YI-34B-CHAT (29.7%) as mA, QWEN1.5-72B-CHAT (26.5%) as mB, and MISTRAL-7B-INSTRUCT-V0.2 (14.7%) as mc.\n4. MM: QWEN1.5-72B-CHAT (26.5%) as mA, CLAUDE-3-SONNET-20240229 (25.6%) as mB, and GPT-4-0314 (22.1%) as mc."}, {"title": "B. Non-Transitivity in Preference", "content": "B.1. Conditions for Non-Transitivity\nIn this section, we define the conditions under which non-transitivity arises in pairwise model comparisons. Consider a triplet of models, (mA, mB,mC), and the corresponding pairwise comparisons on instruction Ii:\n$J(m_A > m_B | I_i)$, $J(m_B > m_C | I_i)$, $J(m_A > m_C | I_i)$\nwhere $J(m_x > m_y | I_i)$ denotes the preference of the judge that model mx outperforms model my under instruction Ii. Non-transitivity occurs if the results of these comparisons form any of the following patterns:\n\u2022 mA > mB, mB > mC, mA \u226f mC"}, {"title": "", "content": "\u2022 mA > mB, mB ~ mC, mA ~ mC\n\u2022 mA > mB, mB ~ mC, mA \u226f mC\n\u2022 mA ~ mB, mB > mC, mA ~ mC\n\u2022 mA ~ mB, mB \u226b mC, mA \u226f mC\n\u2022 mA ~ mB, mB ~ mC, mA \u226b mC\n\u2022 mA ~ mB, mB ~ mC, mA < mC\n\u2022 mA ~ mB, mB < mC, mA \u226f mC\n\u2022 mA ~ mB, mB < mC, mA ~ mC\n\u2022 mA \u226f mB, mB > mC, mA > mC\n\u2022 mA \u226f mB, mB < mC, mA ~ mC\n\u2022 mA < mB, mB < mC, mA \u226f mC\n\u2022 mA < mB, mB < mC, mA ~ mC\nwhere > means the left side wins against the right, < means the left side loses to the right, and ~ represents a tie between the two sides.\nThreshold Setting. In practice, given the continuous nature of probability estimates, ties where $J(m_x > m_y | I_i)$ = 0.5 occur with negligible frequency. Therefore, we introduce the following thresholds to determine the outcome of pairwise comparisons:\n1. If $0.475 < J(m_x > m_y | I_i) < 0.525$, the outcome is treated as a tie (~).\n2. If $J(m_x > m_y | I_i) > 0.525$, the outcome is classified as a win for Mx (>).\n3. If $J(m_x > m_y | I_i) < 0.475$, the outcome is classified as a loss for Mx (\u226f).\nNotably, even without threshold settings, the non-transitivity patterns observed across all four scenarios remain consistent with Section 4, which is shown in Appendix B.2."}, {"title": "B.2. Results with Preferences without the Threshold of Ties", "content": "We observe the same pattern from the table 3 as in the main text, which is with the threshold for ties. When GPT-4-Turbo serves as the judge, both PNT and SNTD increase as the performance gap between any pair of models, (mA,mB) or (mB, mC), decreases. In cases where all three models exhibit similar performance, such as in scenario MM, the incidence of non-transitivity rises significantly. We attribute this to the increased uncertainty judges face when assessing quality differences between similar outputs. When the comparisons between mA and mB, mB and mC, and mA and my are all uncertain, non-transitivity reaches its highest level. Replicating our evaluation with GPT-3.5-Turbo as judge reveals an intriguing pattern: while the PNT remains minimal across scenarios, the consistently high SNTD values indicate substantial non-transitivity. This observation motivates us to define the tie threshold, as ties can serve as an indicator of model uncertainty.\nTo explain the low number of hard non-transitive cases when using GPT-3.5-Turbo as the judge with position switching in Figure 3, we hypothesize that GPT-3.5-Turbo is also affected by other biases, such as verbosity bias and token bias Since GPT-3.5-Turbo struggles to accurately assess the quality of outputs, these combined biases influence the judge's preferences. As a result, even though position switching mitigates the position bias, the averaged preference is still not determined by the actual quality of the outputs but rather by other fixed biases in the prompt, leading to transitive preferences. This observation also motivates us to define the threshold, as it can be used to reduce the impact of other biases."}, {"title": "C. Additional Experimental Results", "content": "C.1. Full Pairwise Comparison Matrix (Position Switching and Two API Calls per Order)\nC.2. Position Switching and Multiple API Calls Reduce the Occurrence of Non-transitivity at the Model Level.\nWe hypothesize that the absence of observed hard non-transitivity in Figure 10 is due to the use of position switching and two API calls per order, which help ensure the consistency of judgments. To validate this hypothesis, we adopt a more aggressive approach by randomly assigning positions for each evaluation, reducing the process to a single API call to mitigate position bias. However, since the preference between each model pair for a given instruction is determined by log probability rather than a binary label (0 or 1), we argue that random assignment may not fully eliminate position bias. As a result, this setup is expected to perform worse than position switching, leading to lower judgment consistency compared to the original setting.\nTo reduce computational costs, the judge's new preference can be interpreted as a random sample from the four API calls made in the original experiment. In other words, in this ablation experiment, the judge's preference is equivalent to selecting one random sample from the pre-computed preferences in Section 4.2.\nFigure 11 presents the corresponding win rate matrix from this ablation. In contrast to Figure 10, we now observe the occurrence of a hard non-transitive case at the model level. Specifically, Qwen1.5-72B-Chat outperforms Yi-34B-Chat, and Yi-34B-Chat outperforms claude-3-opus-20240229. However, claude-3-opus-20240229 outperforms Qwen1.5-72B-Chat, thus exhibiting a clear case of non-transitivity.\nTo further verify that the observation of non-transitivity in the ablated setting is not merely due to randomness, we repeat this ablation experiment 50 times. We quantify the degree of soft non-transitivity in the win rate matrix in a manner similar to Equation 3, but applied at the model level. Specifically, for a set of 20 models, we first compute all possible permutations of triples (mA,mB,mC). For each triplet, we sequentially select two pairs of models and extract their corresponding values from the win rate matrix as ground truth. We then calculate the expected win rate for the remaining model pair and measure the associated SNTD at the model level. Finally, we average the results across all permutations to assess the overall"}, {"title": "C.3. More Prompting Strategies", "content": "We evaluate six prompting strategies in Scenario MM to encourage the judge to exhibit more transitive preferences from a prompting perspective (\u2248\u2248). The prompt templates are provided in Appendix G.1.\n1. Direct Comparison: Standard binary choice comparison identical to our previous experimental setup, serving as the baseline.\n2. CoT Comparison: Requires the judge to output its reasoning through Chain-of-Thought before making a decision."}, {"title": "", "content": "3. Direct Comparison with Checklist: Provides a detailed evaluation checklist for the judgment without explicit reasoning.\n4. CoT Comparison with Checklist: Combines a detailed evaluation checklist with Chain-of-Thought reasoning before judgment.\n5. CoT Comparison (Tie Allowed): Extends the binary choice to three options by introducing the possibility of ties, while maintaining the Chain-of-Thought reasoning process.\n6. CoT Comparison with Checklist (Tie Allowed): Incorporates both the three-choice option and evaluation checklist while preserving Chain-of-Thought reasoning."}, {"title": "", "content": "D. Soft Bradley-Terry Model Yields More Accurate Rankings\nWe explored three methods for computing Wij in Equation (7). The first method, referred to as hard-BT, directly derives discrete win rates from the judge's continuous preferences. In this approach, if $J(m_i > m_j | I_k) > 0.5$, the outcome is counted as a win (1); if $J(m_i > m_j | I_k) < 0.5$, it is counted as a loss (0); and if $J(m_i > m_j | I_k) = 0.5$, it is considered a tie (0.5).\nThe second method, rounded-BT, incorporates a threshold to refine the win/loss definition. Specifically, if $J(m_i > m_j | I_k) > 0.525$, it is considered a win (1); if $J(m_i > m_j | I_k) < 0.475$, it is considered a loss (0); and if $J(m_i > m_j | I_k)$ falls within the range [0.475, 0.525], it is treated as a tie (0.5).\nThe final method, soft-BT, follows the formulation presented in the main text. Instead of discretizing preferences into fixed categories, it directly uses the judge's continuous preference scores to compute Wi,j, allowing for a more nuanced representation of the relative strength between models:\n$W_{i,j} = \\sum_{I_k \\in I} J(m_i > m_j | I_k)$."}, {"title": "E. Swiss-Wise Iterative Matchmaking tournaments", "content": "Algorithm 1 Swiss-Wise Iterative Matchmaking (SWIM) tournament\n1: Input: M unranked models, a dataset I and a judge model MJ.\n2: Output: An ordered ranking of all M models.\n3: R \u2190 empty set \u00d8 to store ranked models\n4: U \u2190 set of all M models\n5: X \u2190 a random model from U\n6: R \u2190 R \u222a {X}, U \u2190 U \\ {X}\n7: while U \u2260 \u00d8 do\n8: P \u2190 a random model from U\n9: U \u2190 U \\ {P}\n10: s \u2190 |R|, c \u2190 \u230amax(log2(s), 1)\u230b\n11: X \u2190 a random model from R\n12: T \u2190 R \\ {X}\n13: for all Ii \u2208 I do\n14: Compute J(mP > mX | Ii)\n15: end for\n16: \u03b2 \u2190 update BT coefficient for R \u222a {P}\n17: for j = 1 to c \u2212 1 do\n18: O \u2190 arg minO\u2208T |\u03b2O \u2212 \u03b2P|\n19: T \u2190 T \\ {O}\n20: for all Ii \u2208 I do\n21: Compute J(mP > mO | Ii)\n22: end for\n23: \u03b2 \u2190 update BT coefficient for R \u222a {P}\n24: end for\n25: R \u2190 R \u222a {P}\n26: end while"}, {"title": "G. Prompt Template.", "content": "G.1. Judge Prompts\nDirect Comparison - Identical to AlpacaEval 2.0 \n[System Part]\nYou are a highly efficient assistant, who evaluates and selects the best large language model (LLMS) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.\n[User Part]\nI require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective## Instruction\n## Model OutputsHere are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.\n## Task\nEvaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M."}, {"title": "G.2. Checklist Generation", "content": "We follow Cook et al. (2024)'s prompt tepmplate to generate checklists.\n[System Part]\nPlease help judge an AI assistant's response to an instruction by providing an evaluation"}]}