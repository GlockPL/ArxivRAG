{"title": "ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study", "authors": ["Eric Modesitt", "Ke Yang", "Spencer Hulsey", "Chengxiang Zhai", "Volodymyr Kindratenko"], "abstract": "Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning LLAMA-3-8B on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69% to 76% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed LLAMA-3-8B-BASE, with GPT-40 evaluations preferring it in 73% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at https://github.com/ModeEric/ORBIT-Llama.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) has transformed natural language processing (NLP) and artificial intelligence (AI), with general-purpose models like GPT-4 (Hurst et al., 2024) and LLaMA (Dubey et al., 2024) demonstrating versatility across tasks such as knowledge retrieval, open-domain question answering, and linguistic applications. However, these models often struggle in specialized domains, such as astronomy, where deep, nuanced understanding and up-to-date factual accuracy are crucial (Singhal et al., 2023). This performance gap arises because general-purpose LLMs must balance performance across a wide range of tasks, diluting domain-specific knowledge (Li et al., 2024; Yang et al., 2024b).\nTo address this limitation, domain-specialized LLMs can allocate their capacity toward mastering specific domains, offering greater depth and accuracy. However, building these models is challenging due to the need for high-quality, domain-specific datasets. Conventional approaches, such as using academic sources like arXiv papers (Nguyen et al.; Pan et al., 2024), tend to focus on highly technical content, neglecting the breadth and diversity needed for effective model generalization. Alternatively, web-sourced datasets offer greater diversity but are often noisy, containing irrelevant or low-quality content. Traditional filtering methods, such as keyword-based or rule-based approaches, frequently fail to balance coverage and quality, potentially excluding relevant data while admitting suboptimal material.\nIn this work, we propose ORBIT, a novel, scalable data curation framework for creating high-quality, domain-specific datasets. ORBIT combines embedding-based similarity matching with a BERT-based regression model to filter large-scale web datasets efficiently. By focusing on both semantic relevance and educational value, this methodology ensures that the curated datasets are both diverse and tailored to specific domains. Using astronomy as the primary case study, we curated a 10-billion-token dataset derived from FineWeb-Edu (Penedo et al., 2024), incorporating a broader range of content compared to prior approaches like AstroLLaMA (Nguyen et al.), which rely solely on arXiv abstracts. The inclusion of web-sourced educational content alongside academic texts enables ORBIT to balance depth and diversity, capturing a more comprehensive understanding of domain-specific knowledge.\nTo demonstrate the generalizability of ORBIT, we also applied it to law and medicine, achieving significant quality improvements in these domains. GPT-4$\\theta$ evaluations rated the curated datasets at an average educational value of 3.05 and 2.9 on a scale of 0-5 per document, respectively, compared to an unfiltered baseline of approximately 0.4. These results highlight ORBIT's ability to extract domain-relevant, high-quality data across diverse fields.\nFine-tuning a LLAMA-3-8B model on a randomly sampled 1B-token astronomy subset of the ORBIT-curated dataset results in substantial improvements on astronomy-specific tasks. Our model (Orbit-LLaMA) achieves a 7-point accuracy gain over the base LLaMA-3-8B model (from 69.08% to 76.3%) on the MMLU astronomy benchmark and outperforms AstroLLaMA (66.45%) by a significant margin. Furthermore, ORBIT-trained models surpass state-of-the-art performance on various astronomy baselines, receiving higher ratings from both GPT-4$\\theta$ evaluations and domain experts in the vast majority of cases. These results underscore the value of ORBIT's methodology in producing specialized datasets that enhance both the depth and breadth of domain-specific knowledge in LLMs.\nThe key contributions of this paper are:\n\u2022 We introduce ORBIT, a generalizable, scalable framework for filtering noisy web data into high-quality, domain-specific datasets, addressing challenges of scalability, noise, and coverage balance.\n\u2022 We demonstrate ORBIT's generalizability by applying it to multiple domains, including astronomy, law, and medicine, achieving significant quality improvements in each field with minimal computational overhead.\n\u2022 We present a specialized astronomy dataset curated using ORBIT, comprising 10 billion tokens that combine academic rigor with web-scale diversity, advancing prior work limited to arXiv-based sources.\n\u2022 We train a state-of-the-art astronomy-specific language model (which we call Orbit), fine-tuned on a subset of the ORBIT-curated dataset, achieving significant performance gains on astronomy-related benchmarks and surpassing existing models, including AstroLLaMA, in expert evaluations.\nBy presenting ORBIT and its application to astronomy, as well as its successful extension to law and medicine, we provide a generalizable framework for developing targeted, domain-specific AI tools. This methodology has the potential to accelerate scientific research, education, and practical applications across a wide range of specialized fields."}, {"title": "2 Related Work", "content": "2.1 Data Curation for Language Models\nRecent research has demonstrated the paramount role of high-quality data in the development of large language models. For instance, the technical reports of models like LLama-3 (Grattafiori et al.) and Qwen-2 (Yang et al., 2024a) emphasize extensive data curation methodologies for general-purpose language models. These efforts have led to significant performance gains, even when model architectures and parameter sizes remain largely unchanged (e.g., the transition from LLama-2 to LLama-3).\nSeveral efforts have focused on automated data curation techniques. Chen et al. (2023) proposed a method to automatically filter and clean web-crawled data to build high-quality training corpora, while Gururangan et al. (2020) developed a data selection method for identifying domain-relevant examples within large datasets. Furthermore, Kreutzer et al. (2022) demonstrated that smaller, carefully curated datasets often outperform larger but noisier datasets.\nHowever, these methods often face limitations when applied to highly specialized domains. Many automated filtering techniques rely on general quality metrics or term whitelisting, which can inadvertently include irrelevant or low-quality content while excluding high-quality data that does not fit predefined patterns. For instance, filtering by specific terms or phrases, such as LaTeX commands, may be effective in domains like mathematics but fails in more diverse fields like astronomy where specialized exact terms do not exist or are more varied. Additionally, many datasets rely on scraped web data, which presents risks related to copyright issues, noise, and incomplete data extraction from APIs, further limiting the potential for domain-specific curation.\n2.2 Domain-Specific Language Models\nAdvances in natural language processing have led to the rise of domain-specific language models that are fine-tuned on specialized corpora. These models are designed to perform well within particular domains, outperforming general-purpose models on domain-specific tasks (Beltagy et al., 2019). However, each of these approaches has notable limitations.\nFor example, Azerbayev et al. (2024) introduced LLEMMA, an open-source language model for mathematics that achieves state-of-the-art results on the MATH benchmark. LLEMMA filters data based on whether it contains LaTeX syntax, a technique well-suited to mathematics but restrictive when applied to other fields, such as astronomy or biology, where such syntactic markers do not exist. This method risks excluding valuable content that lacks LaTeX or including low-quality data simply because it contains LaTeX markup.\nSimilarly, Singhal et al. (2023) developed MedPaLM 2, a medical domain model that achieved 85.4% accuracy on US Medical Licensing Examination (USMLE) questions. However, its approach to fine-tuning is relatively limited, relying primarily on instruction fine-tuning without deep post-training adjustments specific to medical literature, limiting its adaptability for more niche medical tasks.\nOther domain-specific models face similar limitations in data sourcing. Yang et al. (2023) introduced FinGPT, which demonstrates strong performance on financial tasks, but it heavily relies on domain-specific data sources like SEC filings and NYSE transaction reports. These data sources are highly specific to the financial domain and do not generalize well to other fields, limiting the flexibility of such models.\nNguyen et al. introduced AstroLLaMA, a 7-billion-parameter model fine-tuned on the abstracts of 300,000 astronomy papers from arXiv. Furthermore, Ting et al. (2024) builds upon this work with larger and more modern models. While these works show strong performance in generating scientifically relevant text completions, limiting the dataset to only arXiv papers (and in this case, only to certain sections such as the Abstract and Introduction) restricts the breadth and depth of the information available for fine-tuning. The homogeneous distribution of similarly formatted research abstracts leads to a lack of data diversity that reduces the model's capacity to generalize across broader applications within the domain.\nThese models highlight the importance of high-quality, domain-specific datasets for effective model performance but also demonstrate the challenges in collecting and curating sufficiently diverse and representative datasets."}, {"title": "3 Dataset Curation Methodology", "content": "3.1 Choice of Corpus\nFor this study, we selected the FineWeb-Edu dataset (Penedo et al., 2024) as our primary corpus. FineWeb-Edu is a specialized subset of FineWeb, which is a large-scale, high-quality dataset derived from CommonCrawl web data, specifically designed for pretraining large language models. The FineWeb-Edu dataset uses the Open Data Commons License Attribution family. FineWeb-Edu focuses on \"educational content\u201d based on prompt engineering strategies and contains approximately 1.3 trillion tokens, curated by filtering out content with lower educational value. This subset allowed us to begin with a high-quality dataset that is more focused and manageable for the specific tasks required in astronomy. \n3.2 Methodology for Domain-Specific Dataset Curation\nOur research presents a novel approach to curating a high-quality, domain-specific dataset for astronomical language models. This methodology combines advanced natural language processing techniques with rigorous quality assurance measures to produce a dataset that balances complex reasoning tasks with factual content in the field of astronomy. Our approach is designed for cost-effectiveness, using a combination of broad initial filtering and more thorough assessments at later stages to optimize the dataset's quality and relevance. Our full filtering method is shown in Algorithm 1.\n3.2.1 Stage 1: Initial Domain-Specific Filtering\nWe developed a lexicon of 101 single-word astronomy-related terms, encompassing concepts from astrophysics, cosmology, and space exploration. To efficiently process large volumes of text, we implemented a static-embedding-based matching technique utilizing GloVe word embeddings (Pennington et al., 2014). A representative astronomy aggregated embedding vector A was computed by averaging the embeddings of all terms. For each document in FineWeb-Edu, we calculated a document vector and computed its cosine similarity with A. Documents exceeding a similarity threshold of $\\tau$ = 0.2 were retained for further analysis. This threshold was empirically determined to balance dataset size and quality. After this stage, approximately 20B tokens of the corpus remained.\n3.2.2 Stage 2: Educational Value Assessment\nAfter the initial filtering, we applied a more thorough evaluation to refine the dataset further, focusing on its educational merit. Without this second phase, we would be left with a number of low-qualtiy documents, as shown in Figure 3. Furthre-more, if only Stage 2 was applied, the computational cost would increase significantly. For example, if Stage 1 keeps 1% of the total data, the number of NVIDIA A100 GPU hours needed for stage 2 would decrease by 100x.\nWe developed a BERT-based regressor model (Devlin et al., 2019), using Huggingface's HUGGINGFACEFW/FINEWEB-EDU-CLASSIFIER model, trained to evaluate the educational value of astronomy-related text on a scale of 0 to 5.\nThe training dataset for this model was meticulously curated through a multi-step process:\n1. Random sampling of 50,000 documents from the embedding-filtered corpus to ensure topic diversity.\n2. Automated evaluation of each sampled document using GPT-4$\\theta$ model (OpenAI et al., 2024), which was prompted to assess the educational value on a 6-point scale (0-5).\n3. Collection of both quantitative scores and qualitative justifications for each evaluation, used for prompt engineering.\nThe language model was instructed to consider factors such as depth of astronomical content, clarity of explanations, relevance to a general audience, and the presence of advanced concepts. Our prompt, inspired by Yuan et al. (2024) (see Appendix), emphasized educational value specific to the domain of astronomy. We kept any value above or equal to our threshold $\\eta$ = 3, resulting in approximately 10 billion tokens of high-quality, astronomy-relevant content.\n3.2.3 Cross-Domain Validation: Law and Medicine\nTo assess the generalizability of ORBIT, we extended the dataset curation pipeline to two additional domains: law and medicine. Using the same methodology applied to astronomy, we developed domain-specific lexicons for these fields. For law, the lexicon included terms such as \u201clitigation,\" \u201cprecedent,\u201d and \u201ccontract,\u201d while for medicine, it featured terms like \u201cpathology,\u201d \u201concology,\u201d and \u201cmetastasis.\u201d The complete lists of terms for each domain are provided in the Appendix.\nStage 1 filtering, based on embedding-based similarity, was adapted to these domains by computing aggregated embedding vectors from their respective lexicons. For each document in FineWeb-Edu,"}, {"title": "4 Experiments", "content": "To validate the effectiveness of the ORBIT methodology, we conducted a series of experiments focusing on the quality of the curated dataset, the impact of fine-tuning on model performance, and the influence of different thresholding values within the pipeline. These experiments aim to assess how ORBIT's two-stage filtering approach improves dataset relevance and educational value while balancing dataset size and computational cost. Additionally, we evaluate the performance of models fine-tuned on ORBIT-curated datasets with varying similarity and educational value thresholds, examining their impact on downstream tasks. The results provide insights into the trade-offs between dataset size, quality, and curation efficiency, while demonstrating the effectiveness of ORBIT for training astronomy-specialized language models. Below, we outline the experimental setup, datasets, and evaluation metrics used to address these questions.\n4.1 Experimental Setup\nFor our experiments, we utilized the Delta GPU cluster at the National Center for Supercomputing Applications, equipped with 8 NVIDIA A100 GPUs, each with 40GB of memory. The model, named Orbit-LLaMA, was derived from Meta's LLaMA-3-8B (Dubey et al., 2024), an 8-billion-parameter language model optimized for large-scale training. We used the Punkt tokenizer from NLTK for sentence segmentation during preprocessing. LLaMA-3 operates under the LLaMA 3 Community License Agreement.\n4.2 Effect of Thresholding, Embedding Methods, and Keyword Search\nTo explore the effectiveness of various filtering strategies, we tested the impact of:\n1. Different threshold values in embedding similarity filters.\n2. Multiple embedding methods, including fastText, 100-dimensional, and 300-dimensional embeddings.\n3. Keyword filtering approaches compared to unfiltered datasets.\nThis analysis assessed how these methods balance dataset quality and coverage. The performance of each filtering strategy was measured based on average scores obtained from downstream tasks,\nThe results underscore how keyword filters and embedding-based thresholds can improve dataset curation by focusing on the most relevant content.\nThe results demonstrate that:\n\u2022 Higher threshold values generally reduce dataset size while maintaining or improving average scores.\n\u2022 Embedding methods showed slightly varying efficacy.\n\u2022 Keyword filtering, while simpler, achieved competitive performance by focusing on domain-specific terminology.\n\u2022 No filtering resulted in the largest datasets but the lowest scores.\n4.3 Cross-Domain Validation: Law and Medicine\nTo evaluate ORBIT's generalizability, we applied the dataset curation pipeline to two additional domains: law and medicine. Stage 1 filtering was adapted to these domains by constructing domain-specific lexicons, following the methodology described in Section 3.2. For law, the lexicon included terms such as \u201clitigation,\u201d \u201cprecedent,\u201d and \"contract,\" while for medicine, it featured terms like \"pathology,\u201d \u201concology,\u201d and \u201cmetastasis\u201d (see Appendix for full term lists).\nEmbedding-based similarity filtering retained approximately 1.0% of the initial corpus for law and 1.0% for medicine, similar to the retention rate observed for astronomy. The average educational value scores, evaluated using GPT-4$\\theta$, showed significant improvements over the unfiltered baseline (0.3), with 2.9 for medicine and 3.05 for law.\nThese scores align closely with the results obtained for astronomy, indicating that Stage 1 filtering alone is sufficient to extract high-quality, domain-specific content across diverse fields.\n4.4 Benchmarks and Baselines\nWe used multiple-choice perplexity prediction to select answers and conducted qualitative pairwise comparisons rated by expert astronomers and GPT-4 for accuracy, clarity, and reasoning. Baseline models were AstroLLaMA-3-8B (Pan et al., 2024), the prior state-of-the-art in astronomy language modeling, and Meta-LLaMA-3-8B, a general-purpose model.\n4.4.1 Quantitative Evaluation\nWe evaluated Orbit-LLaMa using multiple datasets, including the astronomy section of the MMLU benchmark (Hendrycks et al., 2021) and two versions of AstroBench, the official, validated multiple choice one(Ting et al., 2024), and the Huggingface-only dataset containing three subsets covering important subtests in astronomy (astroBench, 2024a,b,c).\nA total of three datasets were used for quantitative analysis:\n1. MMLU Benchmark: The astronomy section of MMLU evaluates factual knowledge and reasoning across topics like stellar formation and cosmology, testing scientific depth in language models.\n2. Hugging Face AstroBench Subcategories: Organized into subcategories:\n\u2022 Basic Knowledge (BK): Tests core astronomy concepts.\n\u2022 Scientific Calculation (SC): Involves solving astrophysical numerical problems.\n\u2022 Knowledge Application (KA): Assesses applying knowledge to novel scenarios.\nEach subcategory is scored separately for detailed performance analysis.\n3. Official AstroBench Benchmark: A comprehensive dataset of 4,425 multiple-choice questions from 885 Annual Review of Astronomy and Astrophysics articles (1963\u20132023). It provides an aggregated performance score, covering diverse topics such as quasars, cosmological simulations, and the circumgalactic medium.\n4.4.2 Qualitative Evaluation\nWe compared responses from Orbit-LLaMA, AstroLLaMA, and Meta-LLaMA using 24 test questions developed by Astronomy Ph.D. students and faculty. Responses were ranked for accuracy (or, for active areas of research, likelihood), clarity, and reasoning using preference ratings for each model, and detailed feedback on the model's strength's and weaknesses.\n4.5 Experiment Results\nOrbit-LLaMa outperformed baselines on all metrics. On the MMLU astronomy section, Orbit-LLaMa scored 76 compared to 69 (Meta-LLaMA) and 66.45 (AstroLLaMA). On AstroBench subcategories, Orbit-LLaMa excelled in Basic Knowledge (45.53%), Scientific Calculation (30.28%), and Knowledge Application (45.53%). On the official AstroBench, Orbit-LLaMa scored 69.7, surpassing AstroLLaMA (66.4) and Meta-LLaMA (61.5).\nPairwise comparisons confirmed Orbit-LLaMA's superiority, with win rates over 92% against baselines (Table 3). Expert feedback highlighted its accuracy, clarity, and reasoning improvements. \nQualitative results by astronomy graduate students further validate these conclusions\n1. Preference Ratings: Four graduate students selected the best response for each question. Majority consensus was reached for 83% of questions, with Orbit-LLaMA preferred for 66% of total responses (Table 4).\n2. Detailed Feedback: Reviewers noted:\n\u2022 Meta-LLaMA: Responses often repeated content and lacked focus.\n\u2022 Orbit-LLaMA: Delivered clear and concise answers resembling student-created work.\n\u2022 AstroLLaMA: Long, research-style responses with structural and coherence issues."}, {"title": "5 Discussion", "content": "The results demonstrate the utility of the ORBIT methodology in addressing key challenges in domain-specific dataset curation and fine-tuning. By using a two-stage filtering process, ORBIT balances relevance and quality while remaining computationally efficient. Stage 1's embedding-based similarity filtering significantly reduces the dataset size, while Stage 2's educational value assessment ensures the retained data is highly relevant and informative. This layered approach enables the creation of datasets that are both comprehensive and focused, as evidenced by its application to astronomy, law, and medicine.\nFine-tuning Orbit-LLaMA on the ORBIT-curated dataset led to notable improvements across multiple benchmarks, including MMLU astronomy and AstroBench. The gains in both quantitative metrics and qualitative evaluations highlight the impact of curating diverse and high-quality domain-specific data. The inclusion of a mix of academic and educational content allowed the model to excel in tasks requiring both factual knowledge and nuanced reasoning, demonstrating the value of combining depth with breadth in training corpora.\nThe success of ORBIT in multiple domains also suggests its scalability and adaptability. However, differences in domain-specific challenges, such as interdisciplinary overlaps or evolving knowledge in fields like medicine, highlight the need for further refinement. Future work could focus on automating lexicon creation and optimizing threshold selection to streamline application to new domains.\nOverall, the experiments validate the potential of domain-adapted LLMs when supported by robust curation pipelines like ORBIT. This approach addresses limitations in general-purpose models for specialized tasks, emphasizing the importance of targeted datasets for achieving state-of-the-art performance in specific fields."}, {"title": "6 Conclusion", "content": "This paper presents a novel approach to creating high-quality, domain-specific datasets for training language models, with a focus on the field of astronomy. Our methodology, combining embedding-based matching and BERT-based regression for data filtering and selection, has demonstrated significant potential for enhancing the performance of language models in specialized scientific domains. Furthermore, we validated the scalability and generalizability of this approach by extending it to the domains of law and medicine, achieving similar improvements in dataset quality.\nThe key findings of our study include:\n1. The effectiveness of our data curation methodology in creating balanced, high-quality datasets that support both complex reasoning and factual knowledge across multiple domains, including astronomy, law, and medicine.\n2. Significant improvements in model performance on astronomy-related tasks, even with relatively small-scale training data, highlighting the potential for efficient resource utilization.\n3. The adaptability of our methodology to diverse scientific and professional fields, demonstrating that domain-specific models can outperform general-purpose models in specialized tasks.\nIn conclusion, our work represents a significant step toward more efficient and effective AI tools"}, {"title": "7 Limitations", "content": "While the ORBIT methodology and the resulting Orbit model show significant promise, it is essential to acknowledge several limitations that may impact their applicability and effectiveness. These limitations are categorized into technical and social aspects to provide a comprehensive understanding of the challenges involved.\n7.1 Technical Limitations\nThe primary technical limitations of the ORBIT methodology and the Orbit model are as follows:\n\u2022 Domain-Specific Generalizability. Although ORBIT has proven effective in the field of astronomy, its applicability to other domains remains untested. Domains with less structured data or those that are highly interdisciplinary may require additional adaptations to the filtering and evaluation processes. Defining domain-specific terms and educational value criteria in such fields could pose unique challenges that the current methodology does not address.\n\u2022 Dependence on Embedding Models. The embedding-based filtering approach relies heavily on the quality and coverage of pre-trained word embeddings, such as fastText. These embeddings may not fully capture the nuances of highly specialized or emerging astronomical terminology, potentially leading to the exclusion of relevant content or the inclusion of less pertinent material. Enhancing embedding models to better represent domain-specific language could mitigate this limitation.\n\u2022 Computational and Resource Constraints. Despite the efficiency gains from using frameworks like DeepSpeed and FlashAttention v2, the fine-tuning process for large models like Orbit demands substantial computational resources. This requirement may limit accessibility for smaller research teams or institutions with limited budgets. Additionally, scaling the methodology to accommodate larger datasets or models with higher parameter counts may encounter practical barriers related to memory and processing power.\n\u2022 Evaluation Scope. The current evaluations are primarily focused on astronomy-specific tasks and benchmarks such as MMLU and AstroBench. This narrow scope may limit the generalizability of the findings, as broader benchmarks that include interdisciplinary or collaborative tasks have not been assessed. Expanding the evaluation to encompass a wider range of benchmarks would provide a more comprehensive assessment of the model's utility.\n\u2022 Dynamic Nature of Scientific Knowledge. Astronomy is a rapidly evolving field, and the curated dataset represents a specific temporal snapshot. As new discoveries and theories emerge, the model's relevance and accuracy may decline without ongoing updates. Developing methods for efficiently integrating new knowledge into existing models is necessary to maintain their effectiveness over time.\nAddressing these technical limitations will require future work to explore the adaptability of the ORBIT methodology across domains, enhance embedding models for better domain-specific representation, and develop scalable solutions to manage computational demands.\nWe acknowledge the assistance of ChatGPT for paraphrasing and shortening text in this document. All content generated with AI was carefully reviewed and validated by the authors."}, {"title": "8 Ethical Considerations", "content": "The development of domain-specific language models like Orbit raises several ethical considerations that warrant careful examination:\n\u2022 Transparency and Open Sourcing. Opensourcing the methodology, dataset, and codebase promotes transparency and ensures that other researchers can replicate and validate our findings. However, this accessibility also increases the risk of misuse. For example, malicious actors could adapt the approach to create highly specialized LLMs for unethical purposes, such as generating misleading or pseudoscientific content within specialized domains.\n\u2022 Mitigation of Misuse. To mitigate risks of misuse, safeguards such as dataset provenance disclosure, ethical use guidelines, and community oversight should be implemented. Openly documenting the sources and filtering criteria ensures clarity about the data used, while ethical use guidelines can provide clear boundaries for the responsible use of the dataset and methodology. Encouraging the research community to establish and enforce standards for domain-specific LLMs can help prevent misuse.\n\u2022 Bias and Representation. While we have curated a dataset with a focus on educational value and scientific rigor, the model could inadvertently propagate biases present in the source data. Historical datasets may reflect outdated or unbalanced perspectives, such as overrepresenting contributions from certain geographic regions or underrepresenting emerging subfields within astronomy. These biases can perpetuate systemic inequities if not carefully addressed.\n\u2022 Bias Mitigation Strategies. Post-hoc audits can analyze representation across subfields, geographic regions, and demographics of authorship. Iterative refinement, through periodic dataset updates and expanding coverage of underrepresented areas, can further reduce bias. Engaging a diverse group of domain experts to guide future dataset expansions ensures inclusive curation processes.\n\u2022 Representation and Inclusivity. The curated dataset may inadvertently exclude contributions from underrepresented groups or regions, thereby limiting the model's inclusivity. Ensuring diverse representation in the data sources is crucial for developing models that reflect a wide range of perspectives and knowledge bases. Failure to address these disparities can perpetuate existing inequities within the scientific community.\n\u2022 Transparency and Accountability. While documenting dataset provenance and filtering criteria promotes transparency, ensuring accountability in the development and deployment of domain-specific models requires ongoing efforts. Establishing clear ethical guidelines and engaging in community oversight are essential steps toward responsible AI development.\nFineWeb-Edu, our baseline dataset, explicitly addresses the removal of personally identifying and offensive content, as well as trying to address the mentioned issues above. By proactively addressing these ethical considerations, we aim to promote responsible development and deployment of domain-specific language models that support equitable and transparent scientific advancement."}, {"title": "A Appendix", "content": "A.1 Evaluation Prompt for Educational Value of Astronomy Texts\nThe following prompt is utilized to assess the educational value of astronomy-related texts. This scoring system assigns a score from 0 to 5 based on the depth, clarity, and relevance of the content. The prompt guides evaluators in determining the quality of information to ensure only high-value educational material is selected for domain-specific training.\n1 prompt = f\"\"\"Please evaluate the\n2 educational value of the following\n3 astronomy-related text from a web\n4 document. Use this 6-point scoring\n5 system:\n6 0 points: No astronomy content at all.\n7 1 point: Minimal astronomy information,\n8 or astronomy mixed with nonastronomical content.\n9 2 points: Covers basic astronomical\n10 concepts but lacks depth or\n11 comprehensive explanation.\n12 3 points: Clear explanation of concepts\n13 with relevant examples, educational\n14 for a general audience.\n15 4 points: In-depth knowledge, covers\n16 advanced concepts or recent\n17 discoveries, well-structured and\n18 engaging.\n19 5 points: Exceptionally high educational\n20 value, expert-level insights,\n21 connects multiple concepts,\n22 addresses misconceptions, inspires\n23 further learning.\n24 Provide a brief justification (up to 100\n25 words) and conclude with the score\n26 in the format \\\"Score: X\\\".\n27 Here's the text to evaluate:\n28 {text}\"\"\"\nA.2 Domain-Relevant Astronomy Key Terms\nThe following list comprises astronomy-related terms used to construct the domain-specific \u201castronomy vector\u201d within the embedding-based filtering process. This selection encompasses key concepts in astrophysics, observational astronomy, and cosmology, ensuring comprehensive coverage of critical terms relevant to domain-specific filtering.\nA.3 Training Details\nThe training of the Orbit-LLaMA model was conducted using the DeepSpeed framework, leveraging Zero-2 optimization for efficient memory management and scaling. FlashAttention v2 was employed to enhance the efficiency of the self-attention mechanism, improving both memory usage and computational speed.\nTraining Configuration:\n\u2022 Epochs: 1\n\u2022 Block Size: 512 tokens\n\u2022 Effective Batch Size: 8\n\u2022 Learning Rate: 2 \u00d7 10-5\n\u2022 Learning Rate Schedule: Linear warmup over 500 steps followed by cosine decay\n\u2022 Optimizer: AdamW with parameters $\\beta$\u2081 = 0.9, $\\beta$2 = 0.95, and weight decay of 0.01\n\u2022 Gradient Clipping: 1.0\n\u2022 Precision: Mixed precision training enabled with bf16 to reduce memory usage and accelerate training\nOptimization Techniques:\n\u2022 DeepSpeed Zero-2 Optimization: Reduced memory footprint by partitioning optimizer states, gradients, and parameters across GPUs, enabling effective training of large models.\n\u2022 FlashAttention v2: Minimized memory usage during self-attention computations, allowing for faster training without compromising accuracy.\nA.4 Qualitative Evaluation Methodology\nA.4.1 Test Questions and Development Process\nA set of 24 test questions was developed by three Ph.D.-track astronomy graduate students and a faculty member from an anonymized university. These questions were designed to evaluate the models' capabilities across a broad range of topics, including:\n\u2022 Basic Definitions and Conceptual Knowledge: For example, defining astronomical terms.\n\u2022 Problem-Solving in Complex or Ambiguous Scenarios: For instance, addressing under-explored areas of astronomy.\n\u2022 Support for Research-Oriented Tasks: Such as code generation for data analysis or simulations.\nEach question was carefully reviewed to ensure it was appropriate for benchmarking a wide range of tasks and model competencies.\nA.4.2 Evaluation Framework\nThe responses from Orbit, AstroLLaMA, and MetaLLaMA were evaluated using the following criteria:"}, {"title": "A.5 Model Generations Comparison", "content": "The following section presents a comparison of responses from three models-Meta LLaMA-3", "Response": "nCrossing the heliopause poses several challenges for spacecraft. Firstly", "spacecraft.\nEvaluation": "Score: 3. Meta LLaMA-3 provides a reasonable overview of the challenges spacecraft face when crossing the heliopause"}, {"Response": "nThe heliopause", "missions.\nEvaluation": "Score: 4. Orbit LLaMA provides a concise and relevant description of the challenges at the heliopause"}, {"Response": "nCrossing the heliopause presents several challenges for interstellar missions, including the interaction of the spacecraft with the solar wind and its magnetic field, as well as the presence of interstellar material such as interstellar dust and the interstellar magnetic field. The heliopause acts as a barrier, preventing the spacecraft from directly interacting with the interstellar medium."}]}