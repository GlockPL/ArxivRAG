{"title": "Probing Causality Manipulation of Large Language Models", "authors": ["Chenyang Zhang", "Haibo Tong", "Bin Zhang", "Dongyu Zhang"], "abstract": "Large language models (LLMs) have shown various ability on natural language processing, including problems about causality. It is not intuitive for LLMs to command causality, since pretrained models usually work on statistical associations, and do not focus on causes and effects in sentences. So that probing internal manipulation of causality is necessary for LLMs. This paper proposes a novel approach to probe causality manipulation hierarchically, by providing different shortcuts to models and observe behaviors. We exploit retrieval augmented generation (RAG) and in-context learning (ICL) for models on a designed causality classification task. We conduct experiments on mainstream LLMs, including GPT-4 and some smaller and domain-specific models. Our results suggest that LLMs can detect entities related to causality and recognize direct causal relationships. However, LLMs lack specialized cognition for causality, merely treating them as part of the global semantic of the sentence.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) exhibit a diverse range of capabilities in Natural Language Processing (NLP) (Wei et al., 2022a; Ganguli et al., 2022). Though LLMs are still based on statistical machine learning (Bareinboim et al., 2022; Chen et al., 2023), they behave well in some inference and reasoning tasks (Bhagavatula et al., 2020), showing ability for manipulation of causality.\nHowever, intrinsic manipulation of causality remains unclear for researchers. Unfortunately, investigating intrinsic manipulation is not straightforward for LLMs due to complex model structure. They have enormous parameters, magnifying the cost of refactoring models. And more advanced architectures like Mixture-of-Experts (MoE) (DeepSeek-AI et al., 2024; Jiang et al., 2023) proposes challenge for detailed probing, because behaviors of models are hard to guide. Moreover, some existing models do not share technical details. Intuitive research like ablation study is hard to work under such circumstances.\nTo address this challenge, our work proposes an innovative approach of probing intrinsic manipulation of causality for LLMs. As shown in Fig. 1, firstly we construct a classification dataset for detecting entities and relationships of causality in sentences. Then we guide behaviors of LLMs by hierarchically add shortcuts on this classification task. We integrate retrieval augmented generation (RAG) and in context learning (ICL) for providing shortcuts. This takes into account the effects of prompts and pretrained knowledge into consideration while probing. Finally, we observe performance variance under different RAG and ICL, to probe intrinsic manipulation of causality. We conduct experiments on LLMs in various parameters sizes and domain knowledge. The experimental results show that LLMs are sensitive to global semantics in classification, and show a certain ability to identify causal entities with guidance. But they do not have direct cognition of causal relationships, lacking a fixed processing route for causality. This leads to sub-optimal performance in more complex problem scenarios for causality, indicating necessity for further attention in LLMs' training."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Probing LLMs", "content": "The working mechanisms of LLMs remain unclear, raising concerns about the reliability and effectiveness of their generated content. Probing (Hewitt and Manning, 2019) aims to discern the internal behaviors of models. Probing researches on LLMs have offered valuable insights into various topics, like mathematical (Stolfo et al., 2023), sociology (Ramezani and Xu, 2023; Hossain et al., 2023), and pretrained knowledge (Chen et al., 2023). In context learning (Brown et al., 2020) is common approach in probing LLMs, since it enables guidance of LLMs without additional training. Furthermore, furnishing models with specific knowledge has been proven to be an effective probing strategy (Lin et al., 2020; Chen et al., 2023)."}, {"title": "2.2 Evaluation of Causality for LLMs", "content": "Causality in LLMs has been explored through tasks like commonsense inference (Bhagavatula et al., 2020; Talmor et al., 2019), event causal identification (Gao et al., 2019; Mu and Li, 2023), and explanation generation (Du et al., 2022a), with ChatGPT's abilities evaluated (Gao et al., 2023).\nHowever, the integration of causality in real-world domains (Kiciman et al., 2023) contrasts with LLMs' reliance on statistical associations (Ze\u010devi\u0107 et al., 2023). Furthermore, (Jin et al., 2024) confirms LLMs' lack of causal reasoning, pointing towards a gap in theoretical discussion despite practical applications."}, {"title": "3 Dataset Construction", "content": "In this section, we introduce an innovative approach to construct a classification dataset for probing. Our approach focuses on entities and their causal relationships in sentence, and diminishes interference of pretrained knowledge for probing. Moreover, our approach preserve gold standard for the classification tasks, which is feasible for providing \"shortcuts\" and to guide behaviours of models.\nBase Dataset We construct our dataset based on the CMedCausal dataset (Zhang et al., 2022)."}, {"title": "4 Probing Design", "content": "In this section, we propose a hierarchical probing approach. As shown in Fig. 1, our method provides \"shortcuts\" hierarchically to LLMs in classification tasks. These shortcuts include necessary steps for causality manipulation, like entities recognition and alignment, causal relation cognition. By comparing whether these shortcuts are beneficial to tasks performance, intrinsic manipulation of causality is probed. We exploit a combination of RAG and ICL for providing shortcuts to guide LLMs. For evaluation, we rewrite classification tasks in Sec. 3 into a question and answer form, requesting LLMs to judge whether causality of the sentence is right."}, {"title": "4.1 Hierarchical Retrieval Augmented Generation", "content": "We add different augmentation for LLMs, forming a hierarchical structure in Fig. 1, notated as layers. For each layer, we retrieve most relevant sentences and attach them in questions for LLMs. From layer 1 to layer 3, we provide more complex guidance from shortcuts, representing a more ideal and detailed manipulation of causality. And we aim to probe whether models show identical manipulation as guided, which can be observed from performance changes.\nLayer 1: No Augmentation This layer offers no augmentation for LLMs, to demonstrate models native manipulation.\nLayer 2: Original CMedCausal This layer provides the most efficient shortcuts, that is, the original passages used in dataset construction. These shortcuts are derived from the original CMedCausal, serving as gold standard for classification. Consequently, this layer guides models to infer about basic causality, probing causal entities recognition and causality understanding.\nAdditionally, we exploit back-translation for this layer, notated as Layer 2.5: Original CMed-Causal (back-translated), implementation details can be found in Appendix. B.\nLayer 3: Universal Medical-KG This classification dataset is in medical domain w common diseases in Chinese. And we supplement the necessary medical knowledge, aiming to guide models to infer latent causality in sentences. LLMs are required to recognize entities and derive causality in knowledge. To provide proper medical knowledge, we use a Chinese common disease knowledge graph, DiseaseKG4. We discuss about effectiveness of augmented knowledge in Appendix. C.\nRetrieval Augmented Generation To extract medical information from a large corpus, we adopt a retriever-reader pipeline (Chen et al., 2017). By integrating the retrieved knowledge with the questions, the model can gain more medical expertise, enhancing the accuracy of its answers. Additionally, efforts should be made to minimize the influence of specialized knowledge on the model's ability to discern causality. The specific method of retrieval can be referred to in Appendix D."}, {"title": "4.2 In Context Learning Design", "content": "In this section, we mainly exploit ICL for guidance of LLMs. In detail, our main approach include prompt engineering. Moreover, we integrate chain of thought (Kojima et al., 2022; Zhou et al., 2023; Wei et al., 2022b) in prompts as further shortcuts. We provide prompts with necessary information, following a prompt framework in community 5. We https://www.promptingguide.ai/"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experiment Settings", "content": "Model Selection During models selection, language preference, domains preference, parameters size and feasibility of probing are considered during models selection. So we select following models:\nGPT-4 (OpenAI, 2023), GPT-3.5 (Ouyang et al., 2022), ChatGLM (Zeng et al., 2023; Du et al., 2022b) and MedChatGLM 6. For comparison, we use BERT (Devlin et al., 2019) with surpervised learning. Appendix F provides detailed settings.\nEvaluations We extract responses from LLMs with an automatic program, and manually check unmatched responses. Only decisions of models (right or wrong) are regarded as classification results. Unclear answers like I don't know are neglected in summary.\nFor binary classification, we evaluate performance with F1-score (F1). Additionally, we exploit Matthews correlation coefficient (MCC) (Matthews, 1975) to measure coefficient of predictions and labels, in order to distinguish random classifications."}, {"title": "5.2 Results", "content": "Probing native manipulation We probe LLMs with different parameters size (GPT-3.5 and ChatGLM) on simple prompts, and conduct parallel experiments on supervised BERT for comparison. Results are shown in Table 1.\nProbing manipulation on advanced prompt We integrate advanced prompt to better exploit ability of LLMs, results are shown in Table 2. This is main evidence for subsequent probing."}, {"title": "5.3 Analysis", "content": "Overall Analysis (1) Experimental results of MCC show that LLMs have weak causality ability on given classification task. But it is not comparable with supervised models like BERT. (2) Performance of LLMs varies. Reasons may include parameters, training strategies and domain knowledge. We discuss this in Appendix G. (3) Additionally, models show preferences for different actions in dataset, as shown in Fig. 3.\nGlobal Semantics Global semantic is the key for classification, we derive this from actions preferences of models. Action 2 integrates more modification from statistical perspective, which is easy for models to distinguish. We evaluate perplexity of sentences in Appendix. H to prove this. GPT-4 performs better in action 1 when knowledge is given, since it gains more instruction ability. This tendency excludes MedChatGLM, as its MCC approaches to 0 and not indicative for analysis. This tendency persists regardless of the prompts used.\nEntities In Causality (1) Discovering entities in causality is less important in manipulation than global semantics. This is derived from action preferences as mentioned above. Moreover, we conduct back-translation experiments for augmented knowledge in layer 2 and observe a performance drop for most cases. Back-translation tends to preserve entities in sentences, since expressions of medical entities are usually standard in Chinese and English. (2) LLMs lack ability of aligning entities and their establishing systematical relations. Entities in sentence are focused as a result of attention mechanism, but LLMs except for GPT-4 can not exploit augmentation of layer 3. Layer 3 should be beneficial in Appendix. C. Comparing with layer 2.5, layer 3 provides entities in other form, and this indicates LLMs fail to recognize causes and effects in heterogeneous form.\nCausality Cognition LLMs do not show much specific cognition for causality, relying more on linguistic order and positions to demonstrate causality. The performance of action 3 is the worst of the three and has even approached random categorization for some models (ChatGLM and MedChatGLM). Augmentation of layer 1 even causes a performance drop, regardless prompt used for all models including GPT-4. In contrast, the introduction of layer 2 in action 1 improves performance. This is because action 3 disturbs mutual causation. To realize mutual disturbance (especially when gold standard is given in layer 1), models needs to recognize causation specifically first.\nKnowledge Background knowledge has little contributions, and only augmentation of layer 2 assist for classifications after guidance of advanced prompt. (1) LLMs are native to believe its pretrained knowledge for classification. (2) Background knowledge about causality confuses LLMs for classification, and intervene normal manipulation by internal knowledge. (3) LLMs manipulation of causality relies on abstract principles summarized from internal knowledge, which is in an abstract level. Since MedChatGLM diminishes classification abilities of ChatGLM, and approaches to random classification."}, {"title": "6 Conclusion", "content": "In this paper, we introduce an innovative structure tailored to investigate intrinsic manipulations of causality for LLMs. We construct a classification dataset focusing on causal relations and entities in sentences. Then we probe models' performance on this classification dataset. We provide \"shortcuts\" through RAG and ICL, and observe performance change in datasets. Probing conclusion is derived by judging whether such shortcuts are beneficial. Our result indicates that LLMs show certain ability of causal recognition, mainly as a result of global semantic. Causal entities and their relations lack for detailed and specific manipulation, especially for LLMs with smaller parameters. Our probing work still has limitations. (1) Our conclusion is derived as a summary for various LLMs. Relation of causality and LLMs' training strategies should be discussed. (2) Our experiment lacks the ability for detailed discussion about supervised learning and zero-shot cases for causality."}, {"title": "A Datasets Details And Statistics", "content": "The CMedCausal dataset (Zhang et al., 2022) defines three key types of medical causal reasoning relationships: causation, conditionality, and hierarchical relationships, consisting of 9,153 segments of medical text and 79,244 pairs of entity relationships. Our work primarily discusses relationships related to causality, hence, we have discarded hierarchical relationships. Medical concept fragments in the dataset refer to continuous character segments that can act as independent semantic units. These segments may represent medical entities, clinical findings, or specific disease symptoms. From the perspective of expressing causal predicates, these fragments fulfill semantic roles of conditions, causes, or consequences.\nWe translated part of the content from the Chinese dataset into English as an example. For instance, \"Gastrointestinal dysfunction in the human body leads to a decrease in the patient's absorption capacity.\" In this case, \"gastrointestinal dysfunction\" is a medical concept fragment. \"gastrointestinal dysfunction\" is a direct cause of \"decreased absorption capacity\", and \"decreased absorption capacity\" is a direct result of \"gastrointestinal dysfunction\". We can label this data as <\"gastrointestinal dysfunction\", \"decreased absorption capacity\", \"causation\">. Here, \"gastrointestinal dysfunction\" serves as the subject (Head) of the relationship, \"decreased absorption capacity\" as the object (Tail), and \"causation\" as the specific type of relation (Relation).\nIn the dataset, all data can be annotated in the triplet form of <Head, Tail, Relation>. We have conducted statistical analysis on the average length of data and the number of triplets corresponding to each relation in the dataset. The specific statistical results can be found in Table 3.\nMoreover, CMedCausal is in Chinese, and Chinese phrases contain fewer variations. So that it is feasible to modify original dataset with text substitution, and preserve sentences fluency."}, {"title": "B Back Translation Implementation", "content": "Layer 2 provides essential knowledge for classification but may simplify the task, as models may compare two sentences straightforward to judge. This sublayer transforms representations of knowledge in Layer 2, preserving its inherent meaning. We exploit back-translation (Tan et al., 2019), additionally necessitating the capability to identify mentions, as original dataset is in Chinese. Because causal mentions in medical typically follow standard terminologies, receiving fewer modifications during back-translation compared to non-causal contexts. In practice, we utilize the DeepL API 7 to translate texts retrieved from Langchain (consistent with Layer 2) into English and then directly translate them back."}, {"title": "C Discussion of External Knowledge in Layer 3", "content": "Table 4 provides examples of the corresponding knowledge provided to the model in Layer 3 when answering questions. To ensure the reliability of the knowledge provided in Layer 3, we randomly selected 50 samples to check whether the additional medical knowledge provided is related to the content of the question or the entities mentioned in the question. In the 50 samples examined, 43 of them had medical entities in the knowledge section that were related to the question statement, while the rest were unrelated. There were 31 samples that provided clear descriptive help for the causal relationship judgment of the question statement, whereas 19 did not offer significant useful information."}, {"title": "D Retrieval Augmentation Design", "content": "To engage retrieval pipeline, we divide each sentence into chunks, allowing for overlap between them, and then encode each chunk using Sentence Transformer (Reimers and Gurevych, 2019). We treat input of LLMs as a query and utilize FAISS (Facebook AI Similarity Search) (Johnson et al., 2021) to efficiently match the encoded query with locally stored sentence vectors, retrieving the top k (set to 2 practically) most relevant chunks. Since the retrieved sentence fragments may be incomplete, directly providing this knowledge to models https://www.deepl.com/translator would result in receiving inadequate information or incoherent statements. To address this issue, we control locations of retrieved text fragments in the original text and individually expand the head and tail of the fragment until it forms a complete sentence. Specifically, due to the large volume of textual data in the medical knowledge graph at Layer 3, directly using Langchain for item-by-item matching is highly inefficient. Therefore, we have adopted a hierarchical retrieval strategy. As shown in Fig. 4, we first match the input text with disease names in the knowledge graph to select the most relevant diseases. Then, we match the input text with the textual descriptions corresponding to the selected diseases to identify the medical knowledge most relevant to the input text. This selected medical knowledge is ultimately integrated into the external medical knowledge required at Layer 3."}, {"title": "E Prompts", "content": ""}, {"title": "E.1 Structure of Prompts", "content": "Prompts for probing were designed according to Base Prompt Framework by Elvis Saravia. All prompts have the following elements: Instructions, we instruct LLMs with a binary classification tasks. Contexts, we place supplementary knowledge and contexts in this section for problems contexts. In the layer of bare asking, this part is excluded. In-https://www.promptingguide.ai/"}, {"title": "E.2 Examples of Prompts", "content": "When using a simple prompt, we directly connect the additional knowledge with the question content in a straightforward manner, as illustrated by the example in Table 5. In contrast, when using an advanced prompt, we employ multi-turn dialogues to emphasize the task content and separate the parts that provide knowledge from those that pose questions. This approach allows the model to understand the task content, and the boundaries between knowledge and questions more clearly. Examples of this can be found in Table 6."}, {"title": "F Details of Models", "content": "GPT-4 (OpenAI, 2023). We use a static version of GPT-4-06139 for experiment.\nGPT-3.5 (Ouyang et al., 2022). We use GPT-3.5-Turbo10 static version of ChatGPT.\nChatGLM (Zeng et al., 2023; Du et al., 2022b). It is pretrained mainly on Chinese and English corpus, and can recognize Chinese expressions better.\nMedChatGLM is a model under fine-tuning on ChatGLM in Chinese medical corpus.\nBERT (Devlin et al., 2019) 11 is trained on supervised datasets, classification is extracted using masked language model (MLM), in which BERT is trained to fill certain slot with right or wrong."}, {"title": "G Performance Difference of LLMs", "content": "The performance of action 3 is the worst of the three and has even approached random categorization for some models (ChatGLM (Zeng et al., 2023; Du et al., 2022b) and MedChatGLM). The assistance of original passage causes a performance drop, regardless prompt used for all models including GPT-4 (OpenAI, 2023). In contrast, the introduction of layer 2 in action 1 improves performance. This means that model lacks understanding of causation between mentions, relying more on linguistic order and positions. We believe that the ability to judge causal relevance problems is mainly related to the number of model parameters and the training method.\nTraining Strategies GPT-4 and GPT-3.5 uses the RLHF (Ouyang et al., 2022) training strategy, which makes its answer results more similar to human beings. This can improve the logic of its dialogue and improve its ability to discuss causal problems to a certain extent.\nThe Number of Model Parameters Compared with GPT-3.5 and ChatGLM, GPT-4 has a larger https://platform.openai.com/docs/models/gpt-4 10https://platform.openai.com/docs/models/gpt-3-5 11https://huggingface.co/bert-base-chinese number of parameters and a larger knowledge reserve, and it has a stronger ability to understand complex logic."}, {"title": "H PPL of Positive and Negative Instances", "content": "This section presents the specific experimental results of testing various actions using GPT-2 Chinese12 (Radford et al., 2019) to determine the confidence of sentences based on PPL. We test PPL on all actions of datasets, and compare difference of positive and negative instances. When difference is big, dataset are more easier for classification from statistical association.\nAs shown in Fig. 5. PPL is correlated with the model's confidence in a given sentence using statistical associations. Results show that action 2 is more easily distinguishable statistically, with a higher base PPL and a more pronounced increase in negative instances."}]}