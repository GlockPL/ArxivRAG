{"title": "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING", "authors": ["Connor Schenck", "Isaac Reid", "Mithun George Jacob", "Alex Bewley", "Joshua Ainslie", "David Rendleman", "Deepali Jain", "Mohit Sharma", "Avinava Dubey", "Ayzaan Wahid", "Sumeet Singh", "Ren\u00e9 Wagner", "Tianli Ding", "Chuyuan Fu", "Arunkumar Byravan", "Jake Varley", "Alexey Gritsenko", "Matthias Minderer", "Dmitry Kalashnikov", "Jonathan Tompson", "Vikas Sindhwani", "Krzysztof Choromanski"], "abstract": "We introduce STRING: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings [RoPE; 43], a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides exact translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers. We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods. Videos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-robotics.", "sections": [{"title": "1. Introduction and Related Work", "content": "Position encodings (PEs) [6, 25, 26, 53] inject information about the respective locations of tokens into transformers [48]. They are essential for good performance because vanilla attention is a set function, equivariant under permutation. In contrast, the meaning of a sequence of tokens in general depends on its ordering.\nAPEs and RPEs. Practitioners initially relied on absolute PEs [APEs; 26, 30, 48, 49] which add or concatenate fixed, precomputed position embeddings to tokens. These have since been replaced by relative PEs [RPEs; 8, 9, 28, 34, 36, 41], which add a learnable bias term that depends on the distance"}, {"title": "ROPE", "content": "To address the limitations of RPEs and APEs, researchers recently introduced rotary position encodings [RoPE; 22, 43]. These have been widely adopted in large language models [LLMs; 17, 18]. RoPE acts on queries and keys by partitioning them into 2-dimensional blocks, each of which is rotated by an angle proportional to the token's position in the sequence. Whilst queries and keys are rotated separately, the angle of relative rotation is proportional to their separation, combining the best properties of APEs and RPEs. Mathematically, for query and key of dimensionality d, ROPE involves [] Givens rotations [3] acting on disjoint 2D subspaces.\nBesides providing strong empirical gains, two attractive properties have driven the enthusiastic uptake of RoPE.\n1. Separability. RoPE transforms each query and key independently, based on its position. This happens once per token; the PE'd tokens are not recalculated during subsequent processing like autoregressive generation. This makes KV-caching convenient. Separability also makes RoPE compatible with linear attention, e.g. Performers [10, 24]. Here, the attention matrix is not instantiated in memory so explicit RPE mechanisms are not possible.2\n2. Translational invariance. For a query-key pair at positions (i, j) \u2208 N2, the relative rotation angle depends only on i \u2013 j. This improves sequence-length generalization.\nHowever, RoPE is not the only position encoding algorithm with these desirable traits. In this paper, we propose a more general algorithm called STRING: Separable Translationally Invariant Position Encodings. STRING is based on Lie groups. It generalises RoPE via a unifying theoretical framework, incorporating the latter as a special case. In fact, we later prove that STRING is the most general PE algorithm with the properties above, amongst a broad class.\nSTRING for robotics. The above features are especially important in robotics, where efficient 2D/3D token representation and sensible physical priors are key. To demonstrate it, we integrate STRING into Vision Transformers (ViTs), showing strong improvements for open-vocabulary object detection models and various robotics controllers. This showcases the real-world impact of our STRING.\nVideos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-\nrobotics."}, {"title": "Key contributions.", "content": "1. We introduce STRING, a new family of position encodings for multidimensional token coordinates that respect both separability and translational invariance.\n2. We rigorously analyse STRING's theoretical properties ( Sec. 3), proving that it is more general than RoPE. We provide computationally efficient implementations.\n3. We show strong accuracy gains across varied models using Transformers with STRING, on a range of robotics and general vision tasks (see Fig. 1 and Sec. 4)."}, {"title": "2. Preliminaries", "content": "Let {x}\u2081 \u2208 Rd denote a set of N d-dimensional tokens. Assume that d is even. The ith query, key and value vectors are given by qi = Wqxi, k\u2081 = Wkxi and v\u2081 = Wx\u012f respectively, where Wq, Wk, W, \u2208 Rdxd are learned projection matrices (to keep the notation simple, we assume here the one-head setting). The attention mechanism, basic computational unit of the Transformer, can be written as:"}, {"title": "3. STRING: Separable Translationally Invariant Position Encodings", "content": "Recall that our goal is modify queries/keys depending on their respective positions, so that changes to dot products qk; depend on r\u012f \u2013 rj. RoPE achieves this using matrix multiplication [43]. Here, we present STRING: a more general, better-performing algorithm."}, {"title": "3.1. Introducing STRING", "content": "STRING is defined as follows.\nDefinition 3.1. STRING is the mapping R(\u00b7) : Rd\u00a2 \u2192 Rd\u00d7d, from de-dimensional position vectors to d x d matrices, given by\nR(ri) = exp \u2211de k=1 Lk [ri]k \\qquad(6)\nwhere {Lk}de1 C Rdxd is a set of learnable and commuting skew-symmetric generators. Given a set of queries or keys {z}1 C Rd at positions {r}\u2081C Rde, their positions are encoded as:\nzi \u2192 R(ri)zi Vi\u2208 {1, ..., N} \\qquad(7)\nHere, exp() refers to the matrix exponential, defined by its series expansion exp(A) := \u2211 A\u00b2/i! and [ri]k is the kth coordinate of vector r\u012f. By \u2018commuting skew-symmetric generators', we mean that {Lk}1 satisfy\n[Li, Lj] = 0 and  L = \u2212L\u00a1 \u2200i, j. \\qquad(8)\nThere are many ways to parameterize such a set; we give examples in Section 3.2. Remarkably, the following is true.\nTheorem 3.2 (STRING is general). Consider the set of mappings R(\u00b7) : Rd\u00a2 \u2192 Rd\u00d7d that satisfy the group-like translational invariance property R(r\u2081)TR(rj) = R(rj \u2013 ri) \u2200 ri, rj \u2208 Rde, are continuously differentiable with respect to r\u2081, and satisfy R(0) = I\u0105 (with I\u0105 the d-dimensional identity). All such mappings can be expressed as STRING with some set of generators {Lk}=1 < Rd\u00d7d.\nIn this sense, STRING is the most general of all translationally invariant position encoding mechanisms using matrix multiplication. Meanwhile, RoPE is a simple special case of STRING, taking a particular choice of generators. This can be seen as follows."}, {"title": "=ROPE", "content": "is a type of STRING #1). Consider the generators Lk = \u2211d/2 p=1 (82p,2p-1 \u2013 82p-1,2p)\u04e9\u0440,\nwhere {0,}d/2 c R and 8\u2081\u2081j is the delta function. This corresponds to RoPE with rotational frequencies\n{0}01\nProofs of Theorem 3.2 and Theorem 3.3 are in Appendix A."}, {"title": "3.2. Computationally efficient STRING", "content": "Despite being general and notationally compact, the parameterization of the STRING matrices R(ri)\nshown in Eq. 6 may not be convenient for practical applications. Given N tokens at positions {ri}1\none must in general exponentiate and store N dense d\u00d7d matrices. This incurs O(Nd\u00b3) time complexity\nand O(Nd\u00b2) space complexity cost. The problem is exacerbated if the {r}\u2081 differ across training\nexamples and batches, which occurs e.g for point cloud data or color plus depth channel (RGB-D)\nimages. In this case, {R(ri)}1 cannot be cached and reused. This motivates the goal of this section:"}, {"title": "Theorem 3.4 (RoPE is a type of STRING #2). For any STRING position encoding with generators\n{Lk}1, there exists an orthogonal matrix P such that", "content": "R(ri) = PROPE(ri)PT. \\qquad (9)\nNote that the orthogonal matrix P is independent of the coordinates ri, so it can be learned and stored\nonce per attention head and shared across all training examples. Meanwhile, RoPE(r\u2081) is sparse \u2013 it is\nonly nonzero on the super- and subdiagonals \u2013 so multiplying tokens only requires O(Nd) memory\nand O(Nd\u00b2) time, saving a factor of d. This is crucial in the contrastive learning setting where batch\nsizes can become large. Once again, one can see that RoPE is a special case of STRING, this time taking\nP = Id. We emphasize that the parameterization of STRING in Eq. 9 remains just as general as in\nEq. 6. We also note that, since in regular attention one takes dot products between position-encoded\nqueries and keys, the first orthogonal matrix P will always cancel with its counterpart. Therefore,\nin Transformers it is sufficient to take R(ri) = RoPE(ri)P, with P \u2208 O(d) learnable, without loss of\ngenerality. 3"}, {"title": "Example 1: Cayley-STRING", "content": "Equipped with Theorem 3.4, our goal becomes to choose a suitable\nparameterization for the orthogonal matrix P. One option is to take the Cayley Transform,\nPCayley := (Id - S)(Id + S)\u00af\u00b9, \\qquad (10)\nwhere S is a learnable (potentially sparse) antisymmetric matrix [14]. PCayley is convenient since,\nfor a token zi, we can compute (I\u0105 + S)\u00af\u00b9zi efficiently using a linear solver, avoiding matrix inverse\ncomputation. Where we use PCayley, we refer to our algorithm as Cayley-STRING.\nThe unreasonable effectiveness of STRING. In some sense, Theorem 3.4 makes it surprising that\nSTRING outperforms RoPE so comprehensively in all our experiments (see Section 4), given that they\nare related by a change of basis. It appears that the ability to explicitly learn this basis change via P\n(shared between queries and keys), rather than implicitly via existing network weights, substantially\nboosts performance. Conversely, when using linear attention variants, the projected tokens Wqqi and\nWkki are pushed through nonlinearities such as ReLU(\u00b7) before taking the dot product. Hence, in this\ncase, including learnable P does increase the capacity of the network, rather than simply learning a\nbasis change."}, {"title": "Example 2: Circulant-STRING", "content": "We now present a second efficient STRING algorithm within our\nframework. A square matrix is referred to as circulant if it takes the form\n \nAll rows are composed of the same elements, and each row is rotated one element relative to the\npreceding row. The transpose of a circulant matrix C\u2122 is also circulant, and the sum of two circulant\nmatrices is also circulant. It follows that C-C\u2122 is circulant and antisymmetric. Lastly, circulant matrices\ncommute. With these properties in mind, we can simply define Lk = Ck \u2212 C for k \u2208 {1, ..., dc}, with Ck\na learnable circulant matrix parameterized by d scalars {co, ..., Ca\u22121}. We call this Circulant-STRING.\nThis special parameterization is convenient for the following reason."}, {"title": "Theorem 3.5 (Circulant-STRING is fast). Given generators Lk = Ck \u2013 C with Ck circulant, the position\nencoding exp(\u2211 Lk[ri]k)zi for token z\u012f at position r\u2081 can be computed in O(d logd) time and O(d)\nmemory using the fast Fourier Transform (FFT).", "content": "We provide a proof in Appendix A. Circulant-STRING provides another efficient position encoding\nalgorithm that scales gracefully to large, high-dimensional datasets and performs well in spatial\napplications (see Section 4)."}, {"title": "Learnable frequencies with STRING", "content": "Note that the STRING generators from Definition 3.1 are\n(in general) learnable. For Cayley-STRING, the angle-defining frequencies for RoPE and S, the\nantisymmetric matrix from Equation (10) are learned whereas in Circulant-STRING, the scalars\n{co, ..., Cd\u22121} in Equation (11) are learned."}, {"title": "3.3. Loose ends", "content": "Here, we discuss further generalisations of STRING.\nExtension 1: \u2297-STRING. So far, we have followed RoPE in assuming that our position encodings are\napplied via matrix multiplication. However, this can be relaxed whilst preserving separability and\ntranslational invariance. For example, one can transform tokens z\u012b via the outer product with position\nfeature vectors f(ri) \u2208 R2m,\nzi \u2192 vec(f(ri) & zi). \\qquad (12)\nHere, denotes the outer product and vec denotes the \u2018vectorizing' operation that flattens a matrix\nto a vector, so that vec(f(ri) & qi)da+b = f(ri)aqib where a \u2208 {1, ..., 2m} and b \u2208 {1, ..., d}. Since the\ndot product of (flattened) outer products gives the product of dot products, we have\nvec(f(r\u012f) \u00ae q\u2081)\u00afvec(f(rj) \u00ae kj) = q\u012bkj \u2022 f(r\u2081)f(rj). \\qquad (13)\nNow suppose that we take the Fourier features\nf(ri) = 1 / \u221am [cos(wri), sin(wri)]m=1, \\qquad (14)\nwhere {k}_1 C Rd are learnable d-dimensional frequency vectors. Then we have that f(r\u012f)f(r,) =\n1 \u2211 k=1 cos(wk(ri \u2013 rj)) which is clearly a function of r\u012f \u2013 rj. We refer to this novel position encoding\nvariant, orthogonal to previous RoPE-like approaches, as \u2297-STRING.\nExtension 2: General transformation groups. Having focused on translational invariance, another\nnatural question is whether STRING could be repurposed for other continuous transformation groups.\nThese may be more suitable for data with different properties; for example, one might sometimes\nprefer a rotationally invariant position encoding.\nMore formally, recall that a Lie group with parameters \u03c8 \u2208 Rk is a group of transformations of the\nform Ty : Rd \u2192 Rd that are differentiable with respect to \u03c8. Let the parameter 4 = 0 correspond\nto the identity element, so that Tox = x. A canonical coordinate system for G is an injective map p\nfrom Cartesian coordinates to a new coordinate system, satisfying p(\u03a44x) = p(x) + \u2211k=1 \u03c8iei \u2200 Ty \u2208 G,\nwhere e\u2081 is the ith standard basis vector. Observe that the right hand side of this equation represents\na translation in the new basis. Canonical coordinate systems exist for all one-parameter Lie groups\n(k = 1), and more generally for Abelian groups of dimension k \u2264 d [39, 40, 44]. They can be derived\nanalytically by solving a set of first-order PDEs, though for many common transformation groups\nthe canonical coordinate system is obvious. For instance, for azimuthal and polar rotations of points\n(rx, ry, rz) in 3D space (k = 2), a canonical coordinate system is (\u03b8, \u03c6), where sin 0 :="}, {"title": "and tan \u00a2 := ry/rx. Rotating\u2074 simply \u2018translates' the canonical coordinates (\u03b8, \u03c6) \u2192 (\u03b8 + \u2206\u03b8, \u03c6 + \u0394\u03a6) \u2013\na transformation looking much more complicated in the Cartesian basis.", "content": "STRING for Abelian Lie groups. It follows that, simply by replacing Cartesian coordinates {ri}1\nwith their canonical counterparts, we can repurpose STRING to construct position encodings that\nrespect more general invariances."}, {"title": "4. Experiments", "content": "In this section, we provide an exhaustive empirical comparison of STRING with RoPE and vision en-\ncoders leveraging regular APEs. To set up the ground, we start with general non-robotics experiments\nin Sec. 4.1. On our way to robotics applications, we then test STRING for 2D and 3D object detection\nin Sec. 4.2. Finally, we present robotics manipulation experiments in Sec. 4.3 and Sec. 4.4."}, {"title": "4.1. General Experiments: Classification and Retrieval", "content": "We tested STRING for image classification tasks on the ImageNet2012 [13] and Places365 datasets,\nwith Vision Transformer (ViT) [15] as our base model. We compare against RoPE and RoPE-Mixed\n[22], abbreviated to RoPE-M to Circulant-STRING and Cayley-STRING (respectively abbreviated to\nCirculant-S and Cayley-S). The results are shown in Table 1. For both datasets, STRING offers best\nmodels. For ImageNet2012, top two models are STRINGs. Furthermore, for ImageNet2012 STRINGs\nprovide the first absolute gains larger than 1%, as compared to regular ViTs, with only a negligible\nset of extra trainable parameters.\nNext, we lift WebLI [7], a dataset of 10 billion image-text pairs across a variety of languages, into 3D\nby pre-processing a 60-million image subset with Depth-Anything-V2 [51] for metric mono-depth\nestimation. The dataset is filtered using methods from [5] for images that the indoor-finetuned model\nperforms poorly on (pictures with overlays, no visible groundplane, large outdoor scenes, optical\nillusions are removed).\nWe perform contrastive learning on the text to visual representation pairs in the WebLI-3D lifted\ndataset, where the visual representation may be in the form of an RGB image or an RGB-D depth\nimage. Similar to CLIP [35], this is done by maximizing the similarity between the embeddings of\nmatching visual-text pairs, while minimizing the similarity between embeddings of the non-matching\npairs. This enables open-vocabulary detection and classification by comparing the text embeddings of\nall possible classes against those of the visual representation, and selecting the minimum distance\npair. We compare against baseline in Table 2. For all six evaluations, Cayley-STRING is the best and\nCirculant-STRING is second best."}, {"title": "4.2. Improving Open-Vocabulary Object Detection", "content": ""}, {"title": "4.2.1. Open-Vocabulary Object Detection in 2D", "content": "We demonstrate the efficacy of STRING on open-vocabulary object detection for localizing a 2D\nbounding box on standard RGB image benchmarks. For a baseline, we use the official implementation\nof OWL-ViT5 [32] which applies a standard Vision Transformer [15] with light-weight classification\nand localization heads for detection. Table 3 compares the baseline OWL-ViT model with RoPE\nand STRING variants. For all experiments, we followed the standard training pipeline for the B/32\nbackbone with the CLIP loss [35]. Even in this axis-aligned 2D detection setting \u2013 which is favourable\nfor the standard RoPE variant \u2013 Cayley-STRING provides the best overall performance."}, {"title": "4.2.2. Open-Vocabulary Object Detection in 3D", "content": "We tested STRING on the open-vocabulary 3D object bounding box prediction task, similar to those\nfrom Section 4.2.1. Here we modify the output to be the full SE(3) pose and 3D size of the bounding\nbox. We initialize the weights of the vision and text towers of our model with the weights from the\nmodels trained on the WebLI-3D dataset from Section 4.1. We replace the IOU loss from OWL-ViT\nwith an 8-corner vertex loss, but otherwise keep the same matching and loss algorithm. We train on a\nsimulated dataset of 4 million images of indoor and tabletop scenes with groundtruth 3D bounding\nbox labels (see Appendix F.1). From this dataset, we hold out 80 images for evaluation. We evaluate\nboth ViT and ViTD variants. The 3D intersection-over-union (IOU) values for various RoPE and\nSTRING variants on the evaluation data are shown in Table 4. For each configuration, we train 3\nmodels with different random seeds and take the best performing model (see Appendix F.2.4 for\ndetails). Fig. 2 shows example 3D detections for 6 different variants (see Appendix F for details). Note\nthat STRINGs provide much more accurate prediction of the 3D bounding boxes for more challenging\nto localize smaller objects than baseline and RoPE. For ViT, Circulant-STRING is the best, providing\n1.5% relative improvement as compared to the best RoPE variant. For ViTD, Cayley-STRING is the"}, {"title": "4.3. Simulation-Based Robot Manipulation: ALOHA", "content": "We evaluate the performance of STRING on dexterous robotics tasks using ALOHA 2, a bimanual\nparallel-jaw gripper workcell with two 6-DoF arms, within the ALOHA Unleashed [54] simulation\nenvironment (see: Fig. 1). ALOHA Unleashed utilizes a scalable teleoperation framework used to\ncollect human demonstration data.\nWe trained ALOHA Unleashed's Transformer-\nbased neural network with diffusion policies\n(conditioned on vision encoders) on human\ndemonstrations of 12 dexterous tasks (see Ap-\npendix B for descriptions and renders). The\nvision system utilized images from RGB cam-\neras positioned overhead, on each wrist, and at\nthe level of the table. We also deployed our poli-\ncies on real ALOHA 2 robots (see Fig. 3 and Fig.\n11). Due to the large observed variance of the\non-robot evaluation for ALOHA 2, we focused"}, {"title": "4.4. Real-World 3D Robot Manipulation: KUKA", "content": "Establishing STRING as superior to other methods on previous tasks, we let it operate on 3D data to\nobtain new SOTA robotic manipulation policies. This resulted in policies directly using depth and\ndeployed on real hardware. Note that STRING can be naturally applied in that context since it can be\nused for data equipped with general coordinate vectors r\u012f associated with tokens (e.g. 3D)."}, {"title": "4.4.1. Setting", "content": "We evaluated STRING in the vision encoder of a generative policy applying energy-based models [42]\nand deployed on a real industrial KUKA robot arm [46]. The closed-loop feedback policy operates on\nthe RGB-D images, and is learned as a generative model with imitation learning. Its architecture is\nshown in Figure 14 (Appendix G), and consists of the diffusion Transformer and the 3D encoder. The\npolicy was trained on a mixture of scripted and teleoperated data collected for 3 different skills (pick,\nplace and handover) on various objects. It is evaluated exclusively on the pick skill with a diverse set\nof objects. Each evaluation was run as an A/B test for a total of 50 trials."}, {"title": "4.4.2. Regular evaluations", "content": "We experimented with two ways of using depth in the policy."}, {"title": "Implicit depth via normal maps", "content": "In the first\napproach, following [45], depth input is used\nto construct a surface normal map with unit R3\nvalues per pixel. Both RGB and depth inputs are\nthen processed via identical (shared weights)\nembedding layers. The embeddings are con-\ncatenated and processed through Transformer\nlayers. Finally, the embeddings are split and\nfused to yield the final vision embedding. Our\nresults in Figure 5 show that this approach of\nincorporating depth has a detrimental effect on\nthe on-robot deployed policy. We hypothesize\nthat this is caused by the significant amount of\nnoise coming from the depth sensors, leading\nto imperfect surface normal maps."}, {"title": "Lifting patches to 3D for STRING", "content": "In the\nsecond approach, we compute the height for\neach patch via mean-pooling across depth values for all the pixels in the patch, followed by the\nlearnable linear layer. The resulting 3D patches are then fed to the Transformer, with positional\nencoding given by STRING to incorporate depth into the vision encoder. Our results Figure 5 show\nthat STRING improves the success rate over the 2D base policy. Also, when STRING is combined with\nthe first method, it drastically reduces the negative impact of noisy normal maps.\nWe used Circulant-STRING to obtain a particularly compact computational footprint. Note that in\nthis setting, more computationally intense mechanisms, such as [33], run out of memory and could\nnot be trained."}, {"title": "4.4.3. Out-of-distribution evaluation: STRING vs baseline", "content": "To further compare STRING with the baseline and show the advantages of using 3D over 2D policies,\nwe also perform out-of-distribution (OOD) evaluations on the real robot.\nWe vary three different environment settings. These include: (1) lighting changes, (2) adding large\ndistractor objects and (3) changing the height of the table from which the robot has to grasp the\nblock. For each setting, we test multiple different variations, e.g., three different light settings.\nFigure 6 compares STRING with the 2D baseline for each OOD setting. For these evaluations, we\nchoose the best policies from Section 4.4.2. As seen in Figure 6, 3D STRING policies outperform\n2D policies across all OOD settings. For instance, with large distractors (middle), the 2D model's\nperformance decreases from 65% to 57%, while 3D STRING maintains performance similar to non-\nOOD settings (\u2248 74%). In some OOD cases, such as lighting changes, both 2D (\u2248 10%) and 3D\n(\u2248 25%) policies experience a performance decrease vs. the non-OOD setup. This drop in performance\nduring lighting changes is likely due to the significant alteration in image observations, thus affecting\nboth 2D and 3D policies. Finally, the largest performance difference is observed in the table height\nvariation scenario. Here, the 3D policies exhibit significantly higher robustness (\u2248 50%) compared to\nthe 2D policies (\u2248 10%). This suggests that the 3D STRING policy leverages the raw depth signal to\nbetter generalize to table height variations, a change imperceptible to fixed monocular cameras.\nOverall, our results show that 3D STRING policies are highly robust to many variations and significantly\nimprove over 2D policies. Fig. 7 shows a sample episode from the on-robot evaluation of the STRING\ngenerative policy."}, {"title": "From 2D to 3D with STRING", "content": "We have already demonstrated (see the normal map approach from\nSection 4.4.2) that just adding a depth signal does not necessarily improve performance. STRING\ndoes so and exhibits another feature that other approaches (e.g. adding depth as extra channel)\ndo not: it can be trained from a regular 2D pre-trained checkpoint. This is the case since STRING\nincorporates depth by using it to modulate a regular attention matrix, effectively disentangling\n3D specific parameters (defining the modulation) from the core 2D backbone. All training runs in\nSection 4.4 started from the pre-trained 2D backbones."}, {"title": "5. Conclusion", "content": "We introduced a new class of translationally invariant position encodings (PEs) for Transformers,\ncalled STRING. STRING is the most general of all translation-invariant PE methods using matrix\nmultiplications (under weak smoothness assumptions) and contains the prominent class of RoPE\nmethods as its special instantiation. We proposed to apply STRING in robotics for 2D and 3D modeling\nand provided its extensive empirical verification over a range of tasks, from standard classification and\nretrieval, through object localization, to diffusion robotic policies conditioned on Vision Transformers.\nIn all these experiments, we showed consistent gains over RoPE, as well as baselines applying regular\nabsolute position encodings."}, {"title": "6. Impact Statement", "content": "The goal of this work is to contribute to the advancement of the machine learning field which may result\nin potential societal consequences. We acknowledge these potential risks, especially in downstream\nuse-cases of advanced machine learning techniques and advocate for careful consideration of ethical\nimplications in the development and deployment of these techniques. Additionally, as it is the case for\nall papers discussing training Transformer architectures, the corresponding carbon footprint needs\nto be taken into account. STRING plays a positive role here since it reduces computational costs by\nproviding ways of fine-tuning already pre-trained architectures with a negligible set of extra trainable\nparameters."}, {"title": "A. Proofs", "content": ""}, {"title": "A.1. Proof of Theorem 3.2", "content": "To begin, we provide a proof of Theorem 3.2: that STRING is the most general form of transformation\nthat respects the group-like property\nR(ri) R(rj) = R(rj \u2013 ri), \\qquad (15)\nsupposing that R(0) = Id and R(r) is continuously differentiable with respect to r. Note that this is a\nsufficient, but not necessary, assumption for translational invariance.\nProof. Recall STRING is applied as\nqi \u2192 R(ri) qi \\qquad (16)\nwith R(\u00b7) : Rde \u2192 Rd\u00d7d, so that R(ri) \u2208 Rd\u00d7d. Then we require that\nqkj \u2192 q R(ri)R(rj)kj. \\qquad (17)\nRecall that R(0) = Ia, the d-dimensional identity, so that the logit of a query and key at the same\nposition (ri = rj) will be unmodified by the positional encoding. Then clearly R(r\u012f) \u2208 O(d), the\northogonal group in dimension d. For compatability with gradient-based optimisers (a chief concern\nin the machine learning setting), it is convenient to specialise to the connected component (normal\nsubgroup) of O(d) containing the identity matrix: that is, the special orthogonal group SO(d).6 This\nmeans that det(R(ri)) = 1. These transformations are the d-dimensional rotations.\nSince R(ri) \u2208 SO(d), the rotation can be written using its Lie group,\nR(ri) = exp(L(ri)) \\qquad (18)\nwhere the matrix L(r\u2081) is antisymmetric [20]. L(ri) is called the \u2018generator', representing an infinitesi-\nmal rotation. Here, exp(\u00b7) denotes the matrix exponential (not to be confused with the element-wise\nexponential of a matrix, as appears e.g. in softmax). Setting r\u2081 = 0 in Equation (15), it is clear that\nL(-ri) = -L(r\u2081). We then require that\nexp(L(ri)) exp(L(rj)) = exp(L(ri + rj))\n= exp(L(rj)) exp(L(ri)). \\qquad (19)\nClearly L(ri) and L(rj) must commute for all choices of coordinate vector (ri, rj). Therefore, we need\nL(ri + rj) = L(ri) + L(rj), \\qquad (20)\nso L() is linear in its arguments. That is, L(\u00b7) is a linear map from the set of de-dimensional vectors to\na set of commuting antisymmetric matrices. We can write\nL(ri) = \u2211dc k=1 Lk [ri]k, \\qquad (21)\nwith {Lk}1 C Rdxd a set of commuting antisymmetric generators and [ri]k the k-th entry of coordinate\nvector ri. This completes the proof.\n\u03a0"}, {"title": "A.2. Proof of Theorem 3.3", "content": "Now, we prove that generators of the form Lk = \u03a3d/2 p=1 (82p,2p-1 \u2013 82p-1,2p)0p recover RoPE, as described\nin Theorem 3.3.\nProof. Let us initially consider the case d\u0109 = 1, so that the token coordinate r = r \u2208 R and we learn\na single generator. Recall that powers of a block diagonal matrix will remain block diagonal. Each\nblock of the generator Lk is of the form\n[00", "that\n[0\u2212\u03b8\u03b80": 2, "\u03b8200\u2212\u03b82": "."}, {"that\n[0\u2212\u03b8\u03b80": "n={\n[0(-1)n/20(-1)n/2", "24)\n[0(-1)(n\u22121)/2\u2212\u03b8(-1)(n\u22121)/20": "if n is odd.\nCombining and inspecting the Taylor expansions,\nexp[0\u2212\u03b8\u03b80", "cos\u03b8sin\u03b8\u2212sin\u03b8cos\u03b8": "qquad (25)\nwhich is clearly a rotation matrix \u2013 a well-known result. This holds for all the d/2 blocks, each of\nwhich exponentiates to give a 2 \u00d7 2 rotation at a different frequency."}]}