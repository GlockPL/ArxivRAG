{"title": "SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC", "authors": ["Yue Deng", "Yan Yu", "Weiyu Ma", "Zirui Wang", "Wenhui Zhu", "Jian Zhao", "Yin Zhang"], "abstract": "The availability of challenging simulation environments is pivotal for advancing the field of Multi-Agent Reinforcement Learning (MARL). In cooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) has gained prominence as a benchmark for algorithms following centralized training with decentralized execution paradigm. However, with continual advancements in SMAC, many algorithms now exhibit near-optimal performance, complicating the evaluation of their true effectiveness. To alleviate this problem, in this work, we highlight a critical issue: the default opponent policy in these environments lacks sufficient diversity, leading MARL algorithms to overfit and exploit unintended vulnerabilities rather than learning robust strategies. To overcome these limitations, we propose SMAC-HARD, a novel benchmark designed to enhance training robustness and evaluation comprehensiveness. SMAC-HARD supports customizable opponent strategies, randomization of adversarial policies, and interfaces for MARL self-play, enabling agents to generalize to varying opponent behaviors and improve model stability. Furthermore, we introduce a black-box testing framework wherein agents are trained without exposure to the edited opponent scripts but are tested against these scripts to evaluate the policy coverage and adaptability of MARL algorithms. We conduct extensive evaluations of widely used and state-of-the-art algorithms on SMAC-HARD, revealing the substantial challenges posed by edited and mixed strategy opponents. Additionally, the black-box strategy tests illustrate the difficulty of transferring learned policies to unseen adversaries. We envision SMAC-HARD as a critical step toward benchmarking the next generation of MARL algorithms, fostering progress in self-play methods for multi-agent systems. Our code is available at https://github.com/devindeng94/smac-hard.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Multi-agent reinforcement learning (MARL) have led to significant progress in a wide range of applications such as autonomous vehicle teams (Cao et al., 2012) and sensor networks (Zhang & Lesser, 2011). The domain of MARL has experienced rapid progress, significantly driven by the advent of advanced simulation environments that serve as benchmarks to assess the effectiveness of novel algorithms. These benchmarks play a pivotal role in bridging the gap between theoretical developments and practical implementations by enabling the evaluation of MARL strategies in scenarios that reflect real-world complexities. One of the most influential platforms in this space is"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 MARL Environments", "content": "The development of MARL is inseparable from a variety of multi-agent environments. (Peng et al., 2021) adapted multi-agent systems to the MuJoCo environment by decomposing the robot's joints and controlling them collaboratively. MPE (Lowe et al., 2017) involves controlling the movement of different particles in a 2D space to complete a series of tasks, which presents simple communication in the cooperation and competition among agents. PettingZoo (Terry et al., 2021) is a library of MARL environments, encompassing a variety of game types that allow for competitive, cooperative and mixed agent relations. It includes classic scenarios such as Atari (Mnih et al., 2013), board games, and particle control. In addition, many classic human games have also been applied in MARL research. Overcooked-AI (Micah et al., 2019), based on the popular human game, expects agents to learn task distribution and coordination to achieve high rewards in a cooking scenario. Google Research Football (Kurach et al., 2020) offers a physics-based 3D soccer simulation, which presents a challenging RL problem as soccer requires balancing short-term control and skill learning (such as passing) with higher-level strategies. Hok (Wei et al., 2022) is a high-complexity Multiplayer Online Battle Arena (MOBA) environment. Players compete by gathering resources while interfering with their opponents to win the game. With multiple heroes and complex state and action spaces, HOK provides an excellent environment for academic research on complex control problems. SMAC (Samvelyan et al., 2019b) is one of the most popular environments for MARL. Based on the widely known real-time strategy (RTS) game StarCraft II, SMAC scenarios are carefully designed and require learning one or more micromanagement techniques to defeat enemies. Each scenario involves a confrontation between two armies, with varying initial positions, unit types, and terrain features like high ground or impassable obstacles. SMACv2 (Ellis et al., 2024b) addresses the issue of randomness in SMAC by generating unit types for each agent with a fixed probability distribution, making the"}, {"title": "2.2 Algorithms for MARL", "content": "Value-based MARL algorithms have made significant progress in recent years. IQL (Tampuu et al., 2017) treats other agents as part of the environment and trains independent Q-value networks for agents. However, it may not always converge for the non-stationarity of the environment caused by the changing policies of other agents. A series of methods (VDN (Sunehag et al., 2017), QMIX (Rashid et al., 2020), QTRAN (Son et al., 2019), QPLEX(Wang et al., 2020)) decompose the global Q-value function into individual Q-value functions to each agent. LDSA (Yang et al., 2022) proposes an ability-based subtask selection strategy to assign agents to different subtasks reasonably and dynamically group agents with similar abilities into the same subtask. Some other algorithms apply TRPO (Schulman et al., 2015) or PPO (Schulman et al., 2017) to multi-agent problems. For instance, IPPO (Witt et al., 2020) apply PPO and enforce parameter sharing under the assumption that all agents have the same action space. MAPPO (Yu et al., 2022b) enhances IPPO by introducing a joint critic function and improving the implementation techniques. HATRPO and HAPPO (Kuba et al., 2021) extend the theory of trust region learning to cooperative MARL, which holds in general and does not require any assumption that agents share parameters or the joint value function is decomposable."}, {"title": "2.3 LLM for Decision Making", "content": "The emergence of large language models (LLMs) has significantly advanced the research on decision-making for agents in complex environments. LLMs leverage massive human data to gain a deep understanding of various complex scenarios, including robotic control, gaming, etc. In robotics, (Liang et al., 2023) proposed \"code as policy\" for robotic control. In this approach, LLMs generate Python code to process sensory outputs and parameterize control primitives. The Eureka algorithm (Ma et al., 2023) utilizes LLMs for human-level reward design in reinforcement learning tasks, which ables to generate reward functions that surpass those designed by experts. LLMs have also shown exceptional performance across a wide range of game types, from classic board games to open-world video games. ChessGPT (Feng et al., 2024) demonstrated the ability of LLM agents to understand and strategize in strategic games. (Jin et al., 2024) and (Xu et al., 2023) has explored the social deduction game such as Werewolf, which presented unique challenges in strategic communication and decision-making. The MineDojo environment (Fan et al., 2022) facilitated projects like GITM (Zhu et al., 2023) and Voyager (Wang et al., 2023) in Minecraft, demonstrating LLM agents' ability to navigate and perform tasks in complex 3D environments. The Cradle framework (Tan et al., 2024) introduced a novel approach allowing LLM agents to interact with various software and games through a unified interface of screenshots and keyboard/mouse inputs. This demonstrated the potential for general-purpose game-playing agents across multiple commercial video games and software applications."}, {"title": "3 Background", "content": ""}, {"title": "3.1 MARL", "content": "A fully cooperative multi-agent task is described as a Dec-POMDP (Oliehoek & Amato, 2016) task which consists of a tuple $G = (S, A, P,r, Z, O, N, \\gamma)$ in which $s \\in S$ is the true state of the environment in the centralized training phase and $N$ is the number of agents. At each time step, each agent $i \\in N = \\{1,...,n\\}$ chooses an action $a_i \\in A$ which forms the joint action $a \\in A = A^N$. The transition on the environment is according to the state transition function that $P(s, a): S \\times A \\times S \\rightarrow [0, 1]$. The reward function, $r(s, a) : S \\times A \\rightarrow R$, is shared among all the agents, and $\\gamma\\in [0, 1)$ is the discount factor for future reward penalty. Partially observable scenarios are considered in this paper that each agent draws individual observations $z \\in Z$ of the environment during the decentralized execution phase according to the observation functions $O(s,i) : S \\times N \\rightarrow Z$. Meanwhile, the action-observation history, $\\tau_i \\in T = (Z \\times A)^*$, is preserved for each agent and conditions the stochastic policy $\\pi_i(a_i|\\Tau_i) : T \\times A \\rightarrow [0, 1]$.\nValue-based MARL algorithm aims to find the optimal joint action-value function $Q^*(s, a; \\theta) = r(s, a) + E_s [\\max_{a'} Q^* (s', a'; \\theta)]$ and parameters $\\theta$ are learned by minimizing the expected TD"}, {"title": "3.2 SMAC", "content": "Instead of addressing the complexities of the full StarCraft II game, the StarCraft Multi-Agent Challenge (SMAC) concentrates on micromanagement scenarios where each military unit is controlled by an individual learning agent. During testing, units are restricted by a limited field-of-view and lack explicit communication mechanisms. Featuring a diverse range of challenging setups, SMAC has become a widely adopted benchmark in the MARL community for evaluating algorithms. It includes 23 micromanagement scenarios categorized into three types: symmetric, asymmetric, and micro-trick. Symmetric scenarios involve equal numbers and types of units for both allies and enemies. Asymmetric scenarios present additional units for the enemy, making them more challenging. Micro-trick scenarios require specialized strategies, such as in 3s_vs_5z, where three allied stalkers must kite five zealots, or in corridor, where six zealots must block a narrow passage to defeat 24 zerglings without being overwhelmed.\nSMACv2 builds upon the original SMAC by addressing its shortcomings and providing a more robust framework for cooperative MARL evaluation. It emphasizes diversity, scalability, and realism in multi-agent tasks while resolving the exploitable weaknesses of its predecessor. SMACv2 introduces new maps with varied unit compositions, asymmetrical team setups, and heterogeneous agents, offering more intricate and balanced challenges. Additionally, it incorporates stochastic elements in agent behaviors and environmental dynamics, fostering the development of algorithms capable of handling uncertainty. Empirical evaluations highlight SMACv2 as a significantly more demanding benchmark, encouraging research into generalizable MARL approaches. By setting a higher standard, SMACv2 promotes innovation and ensures advancements are relevant to real-world multi-agent systems."}, {"title": "4 Limitation on Default Opponent Strategy", "content": "In this section, we examine how the variation in opponents' strategies affects the performance of two widely used baseline algorithms, QMIX and MAPPO. Our analysis demonstrates that relying on a single deterministic opponent strategy can cause the MARL training process to overfit to a particular policy, leading to a reduction in the generalization capabilities of the trained models. Additionally, when the opponent employs a single deterministic strategy that has certain weaknesses, agents may exploit these vulnerabilities, resulting in suboptimal solutions."}, {"title": "4.1 Default Opponent Policy", "content": "In the StarCraft II (SC2Map) configuration, the default opponent policy is defined within the map files rather than the Python environment of SMAC. These map files can be edited using the StarCraft II Editor, which is officially provided by Blizzard. As illustrated in Figure 1, most map configurations specify two spawning positions for both agents and their opponents. An internal script directs the opponent units to move towards the agents' starting positions. The opponent units will automatically target and attack the nearest agents within their sight range, based on a hate value. This causes the opponent units to continuously adjust their targets.\nTypically, MARL models direct agents toward the opponent's side and execute micro-management tasks based on observations. However, as depicted in Figure 2, there are situations where the opponent"}, {"title": "4.2 Performance on Mixing Policies", "content": "Recent advancements in Reinforcement Learning (RL) have resulted in substantial improvements in tackling complex control systems, including applications in robotics and Atari games. However, one challenge that persists in RL is the tendency for the models to overfit to the specific strategies of the fixed opponent, limiting the transferability of the learned policies. As a result, RL models face difficulties when adapting to new tasks or even when encountering different opponent strategies within the same task.\nTo illustrate this phenomenon, we perform an experiment on the 10m_vs_11m scenario using two distinct, slightly varied opponent strategies alongside a mixed strategy. We employ the QMIX and MAPPO algorithms, utilizing the default hyperparameter settings outlined in the Appendix, and run the simulation for 2 million time steps. Unlike the original script, where the enemies are drawn"}, {"title": "5 SMAC-HARD", "content": "The default opponent strategy in SMAC and SMACv2, as previously discussed, is determined by the map configuration, which limits the variety of policies for each task. To overcome these limitations, we introduce three key modifications: an opponent script editing interface, random selection of scripts, and alignment of the opponent MARL interface with the agent's interface. These changes incrementally enhance the diversity of the opponent strategies, making the tasks more challenging for MARL algorithms while simultaneously improving the transferability of MARL models across different scenarios."}, {"title": "5.1 LLM Script", "content": "While MARL-based models have demonstrated impressive performance in competitive games, they frequently overfit to particular opponent strategies, leading to instability when encountering unfamiliar adversaries. In comparison, decision trees exhibit greater stability across diverse opponents and offer enhanced interpretability. Nonetheless, decision trees demand substantial prior knowledge and are susceptible to errors in edge cases, resulting in unforeseen outcomes."}, {"title": "5.2 Implementation", "content": "According to Figure 5, in terms of source code, the pysc2 package serves as an abstraction of the sc2_protocol, which is included in the StarCraft II binary files. Through pysc2, players can initiate new games, select maps, control units, and establish bases. The SMAC framework further"}, {"title": "6 SMAC-HARD Experiments", "content": "In this section, we present experimental results for our proposed SMAC-HARD framework. Initially, we train widely used and state-of-the-art SMAC baselines on SMAC-HARD to evaluate the impact of the mixed opponent strategies. We select both value-based algorithms, such as QMIX, QPLEX, and LDSA, as well as policy-based algorithms like MAPPO and HAPPO, as our baseline methods. Additionally, we conduct black-box testing of these algorithms and provide detailed analysis. The final performance metrics and learning curves are included in this section, with supplementary details available in the Appendix."}, {"title": "6.1 Baseline Comparisons", "content": "Baseline We select widely adopted and state-of-the-art algorithms for our experiments, including value-based methods QMIX and QPLEX, policy-based method MAPPO, and the latest actor-critic"}, {"title": "6.2 Black-box Evaluation", "content": "To assess the transferability of models trained using MARL algorithms, we conducted a black-box evaluation. During training, agents were exposed only to the default opponent strategy on the original maps, but during evaluation, they faced the new mixed opponent strategies in the SMAC-HARD environment. We trained the baseline algorithms on the SMACv1 environment for 10 million time steps and saved the trained models. These models were then tested in the SMAC-HARD environment to measure their win rates. The black-box evaluation results are summarized in Table 2.\nThe black-box evaluation reveals that MARL algorithms tend to overfit to the specific single opponent strategy they encounter during training, as evidenced by the low evaluation win rates. This suggests that the skills learned by these models are not robust or generalizable strategies but rather aggressive or opportunistic tactics tailored to the specific training scenario."}, {"title": "7 Conclusion", "content": "In this study, we present a series of experimental evaluations to demonstrate that the single, default opponent policy used in SMAC and SMACv2 lacks diversity in policy spaces. To address this limitation, we introduce SMAC-HARD, which supports opponent script editing, probabilistic mixed opponent policies, and self-play interface alignment, significantly mitigating the issue. Our results show that even popular and state-of-the-art MARL algorithms, which achieve near-perfect performance in traditional SMAC environments, struggle to maintain high win rates in SMAC-HARD. Additionally, we conduct a black-box evaluation of models trained using MARL algorithms to highlight the limited transferability of strategies when facing a single, vulnerable opponent policy. Finally, we align the training interface for opponents with that of the agents, providing a platform for potential self-play research in MARL. We believe that SMAC-HARD can serve as a challenging and editable domain, contributing to the MARL research community by capturing practical challenges and fostering further advancements."}, {"title": "A Final Performance at 2M time step", "content": "In the Baseline Comparisons section above, we have listed the final performance of baseline algorithms at 10M time steps. Additionally, we also list the performance at 2M time step. The results may also considered as the benchmark to judge the sample efficiency of an algorithm."}, {"title": "B Return Performance", "content": "In line with the reward calculation methods used in SMAC, there is a general trend where higher returns correlate with higher win rates. However, the reward is determined by multiple factors, including health and shield values, while the win condition is based on which side has no remaining units. This discrepancy can lead to a slight misalignment between the expected return and the actual win rate. For instance, a strategy that results in a single full-health agent surviving would yield significantly higher rewards compared to one where three agents survive but with low health. In this context, we present the expected returns in this section as shown in Figure 7."}]}