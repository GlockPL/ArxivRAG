{"title": "Beyond Words: How Large Language Models Perform in\nQuantitative Management Problem-Solving", "authors": ["Jonathan Kuzmanko"], "abstract": "This study examines how Large Language Models (LLMs) perform when tackling\nquantitative management decision problems in a zero-shot setting. Drawing on 900 responses\ngenerated by five leading models across 20 diverse managerial scenarios, our analysis\nexplores whether these base models can deliver accurate numerical decisions under varying\npresentation formats, scenario complexities, and repeated attempts. Contrary to prior findings,\nwe observed no significant effects of text presentation format (direct, narrative, or tabular) or\ntext length on accuracy. However, scenario complexity-particularly in terms of constraints\nand irrelevant parameters\u2014strongly influenced performance, often degrading accuracy.\nSurprisingly, the models handled tasks requiring multiple solution steps more effectively than\nexpected. Notably, only 28.8% of responses were exactly correct, highlighting limitations in\nprecision. We further found no significant \u201clearning effect\u201d across iterations: performance\nremained stable across repeated queries. Nonetheless, significant variations emerged among\nthe five tested LLMs, with some showing superior binary accuracy. Overall, these findings\nunderscore both the promise and the pitfalls of harnessing LLMs for complex quantitative\ndecision-making, informing managers and researchers about optimal deployment strategies.", "sections": [{"title": "Introduction", "content": "The integration of generative AI technologies such as Large Language Models (LLMs) into\nbusiness environments represents a significant shift in organizational decision-making\npractices. The generative AI integration into business processes can enhance decision-making\nand strategic planning, aligning with the potential for operational efficiency improvements\n(Singh et al., 2024). This trend is particularly notable as AI adoption enhances augmented\ndecision-making by strengthening team perceptions, improving assessment accuracy, and\naccelerating real-time decisions (Gama & Magistretti, 2023).\nWhile LLMs demonstrate impressive capabilities in various domains, including achieving\nsuperior performance across numerous natural language processing tasks (Min et al., 2024),\nthey also face significant challenges. Research shows that LLMs can produce hallucinations\nthat lead to statements that seem plausible but are factually incorrect (Romera-Paredes et al.,\n2024). This limitation is particularly critical in business contexts where decision accuracy is\nparamount. Of particular concern is that neural network models, while showing strong\nmathematical reasoning capabilities, still have underdeveloped fundamental abilities in\nnumber representation and manipulation (Testolin, 2024). This raises important questions\nabout their reliability in quantitative management decision tasks, especially when used basic\nmodels without task-specific training.\nThis research examines whether base LLMs, used in a zero-shot setting without specific\nexamples or fine-tuning, can effectively assist in solving complex management problems\ncharacterized by \"few-steps\" quantitative calculations and inherent constraints. We\ninvestigate how factors such as information presentation complexity, solution steps, and\nirrelevant parameters affect the accuracy of LLMs in management decision-making tasks. By\ncomparing five leading language models across multiple iterations and varying presentation\nformats, this study provides insights into the reliability and consistency of LLM-assisted\ndecision-making in management contexts.\nThe research addresses three primary questions:\nHow does information presentation complexity affect accuracy of management quantitative\ntasks in LLMs?\nHow do scenario complexity factors influence output quantitative accuracy?\nDo language models demonstrate performance differences across iterations and models?\nUnderstanding these aspects is crucial as organizations increasingly integrate LLMs into their\ndecision-making processes. The findings will contribute to both theoretical understanding of\nLLM capabilities and practical guidelines for managers seeking to leverage these tools\neffectively in quantitative decision-making tasks. xd"}, {"title": "1 Theoretical Background", "content": "Recent advances in LLMs have transformed how organizations approach decision-making\nand problem-solving tasks. Research demonstrates significant progress in LLMs' capabilities\nacross various domains, particularly in their ability to handle complex reasoning and decision\ntasks."}, {"title": "1.1 Current State of LLM Capabilities", "content": "Recent studies reveal that LLMs have achieved remarkable progress in handling complex\ntasks. As demonstrated in research by Ryu et al. (2024), GPT3.5 exhibits decision effects that\nparallel those observed in human subjects, suggesting these models can replicate human-like\ndecision-making patterns. The advancement is particularly notable in business contexts,\nwhere with the advent of LLM-powered software, between 47 and 56% of all tasks could be\ncompleted significantly faster at the same level of quality (Eloundou et al., 2023)."}, {"title": "1.2 Mathematical and Quantitative Reasoning", "content": "However, when it comes to mathematical and quantitative reasoning, research identifies\nspecific limitations. While neural network models show strong mathematical reasoning\ncapabilities, they still lack fundamental skills in number representation and manipulation\n(Testolin, 2024). This observation is particularly relevant for management decision-making,\nwhere quantitative accuracy is crucial.\nStudies indicate that information presentation significantly impacts performance. Research by\nYu et al. (2024) has shown that the efficiency of pre-trained LLMs in reasoning tasks depends\nheavily on how information is presented, with structured formats like tables yielding better\nperformance than narrative or unstructured text. This finding suggests that the way\nquantitative information is presented could significantly affect decision accuracy."}, {"title": "1.3 Complexity and Performance", "content": "Research shows that task complexity substantially influences LLM performance. As Zhang et\nal. (2024) demonstrated, tasks that require multiple transformations and higher reasoning\nsteps result in decreased accuracy. This limitation becomes more pronounced when dealing\nwith numerical computations, as generic neural networks, including advanced transformer-\nbased architectures, face challenges with extrapolation and often fail when performing tasks\nthat require intermediate calculations (Testolin, 2024)."}, {"title": "1.4 Learning Effects and Model Behavior", "content": "An important aspect of LLM performance is their potential for learning and adaptation.\nAccording to Zhang et al. (2024), when question-answering involves intermediate data\nrefinement, performance tends to improve across subsequent iterations. Research indicates\nthat LLMs demonstrate some capacity for improvement through reinforcement learning and\nerror analysis (Monea et al., 2024; Ying et al., 2024). However, this capability appears to be\ncontext-dependent, as LLMs excel at intuitive, stepwise tasks that use few examples or\ninstructions, but face difficulties with tasks requiring multistep reasoning (Ryu et al., 2024)."}, {"title": "1.5 Challenges and Limitations of LLMs", "content": "Several challenges remain in LLM implementation. As noted by Romera-Paredes et al.\n(2024), these models can generate hallucinations that lead to statements which appear\nplausible but are factually incorrect. Furthermore, when reasoning tasks contain more\nirrelevant or noisy parameters, they are more likely to produce suboptimal or inaccurate\npredictions (Yu et al., 2024). Comparative analyses have also revealed significant\nperformance variations between LLM models, with different models showing distinct\nstrengths in specific tasks (Haq et al., 2024; Akter et al., 2023)"}, {"title": "1.6 Research Questions and Hypotheses", "content": "These findings lead us to several key research questions about LLMs' capabilities in\nquantitative management decision tasks:\nRQ1: How does information presentation complexity affect accuracy of management\nquantitative tasks in LLMs?\nHla: Presentation format (direct/narrative/table) affects answer's accuracy\nH1b: Text length of problem presentation negatively correlates with accuracy of model's\nanswers\nRQ2: How do scenario complexity factors influence accuracy?\nH2a: Number of constraints negatively affects accuracy\nH2b: Number of solution steps negatively affects accuracy\nH2c: Number of irrelevant parameters negatively affects accuracy\nRQ3: Do LLMs demonstrate learning effects and performance differences?\nH3a: The models answers improved over iterations\nH3b: There are significant differences between models' performance in both binary and\ndistance from optimal answers."}, {"title": "2 Research Methods", "content": "This study employed a systematic approach to evaluate LLMs' performance in quantitative\nmanagement decision-making tasks. We developed and tested 20 management scenarios (a\nmanagerial question with an optimal numerical answer, such as optimal budget allocation,\ncalculation of work hours required to complete a task, etc.). Each scenario presented in three\ndifferent text formats and tested across three iterations answered by five LLM's. This design\nallowed us to examine how various factors, including information presentation, scenario\ncomplexity, and model characteristics, influence decision accuracy."}, {"title": "2.1 Sample", "content": "The dataset comprises 900 observations collected through systematic answers of five leading\nLarge Language Models: Llama 3.3 70b, Gemeni 1.5 Pro, Grok, GPT4o, and Claude 3.5\nSonnet. These models were selected based on specific criteria including release date (post-\n2023), model size (minimum 30 billion parameters), context window capacity (minimum\n8,000 tokens), user base size (>100,000 monthly active users), documented relevance to\nmanagerial decision-making in various domains. Together, these models represent the current\n\"top tier\" in publicly available large language models, offering a comprehensive view of\ncurrent capabilities in AI-assisted decision making.\nThe models were tested on 20 quantitative management scenarios. Each scenario was\npresented in three different formats: direct presentation (average length 198 characters),\nnarrative format (355 characters), and tabular format (227 characters). Every format variation\nwas tested three times with each model, with iterations separated by 24-hour intervals,\nresulting in 900 total observations (20 scenarios \u00d7 3 formats \u00d7 3 iterations \u00d7 5 models). All\nobservations were included in the analysis, with no exclusions.\nEach scenario incorporated between 3-5 base constraints and required 2-5 solution steps (for\nreach the optimal answer), reflecting varying levels of complexity. The scenarios included\nboth relevant parameters necessary for calculation and randomly distributed irrelevant\nparameters. All testing was conducted through the sdk.vercel.ai platform, ensuring\nstandardized evaluation conditions across models. A consistent zero-shot prompting approach\nwas employed, with models receiving identical instructions to act as management consultants\nand provide numerical solutions without explanations or additional context."}, {"title": "2.2 Measures and Data Collection", "content": "The study employed two primary measures of model answer accuracy: binary accuracy and\ndeviation (percentage distance) from the optimal solution. Binary accuracy (Binary_Opt) was\nrecorded to indicate exact matches of model's answer with the optimal numerical solution,\nproviding a straightforward binary assessment of model correctness. The deviation from the\noptimal solution was measured as absolute logarithmic distance (log_distance), calculated as\n$\\|ln(model\\_answer/optimal\\_solution)\\|$. The logarithmic distance measure was chosen for its\nability to normalize the effect of different numerical scales across business scenarios, treat\nproportional differences equally, and reduce the impact of extreme values while maintaining\nthe relative importance of smaller deviations.\nThe study examined three categories of independent variables. (1) Text characteristics\nincluded presentation of scenario format (direct, narrative, or tabular), text length of the\nprompt (measured in characters), number of numerical parameters (relevant and irrelevant) at\nthe question. (2) The solution complexity, measured by the number of solution steps (the\nminimum required steps to reach the optimal solution) and constraints (the number of\nconstrains in the question). (3) Model characteristics included model identity (name of model)\nand iteration number.\nData collection was conducted over a 15-day period through the sdk.vercel.ai platform using\nstandardized one-shot prompting. Due to platform limitations, data collection was structured\nin batches of 60 questions per 24-hour period. Each question was asked in a new chat session\nto ensure independence of responses, with logs maintained through the vercel.ai platform for\nverification purposes.\nThe data collection process was designed to align with our research questions, particularly\nregarding response consistency across iterations. No responses were excluded from the\nanalysis, and there were no missing values in the dataset. Quality control measures focused\nprimarily on ensuring technical validity of responses, monitoring for potential technical\ninterruptions or model failures. Given that response consistency was one of our primary"}, {"title": "2.2.4 Measurement Limitations", "content": "Our measurement approach faces two main technical limitations. First, while Vercel is a\nreliable and widely accepted development platform, the use of API access rather than direct\nmodel interaction represents a potential technical constraint. Second, the focus on measuring\ndistance from optimal solutions, while providing granular performance insights, may not fully\nreflect real-world decision-making contexts where binary outcomes might be more relevant.\nAdditionally, the standardized zero-shot prompting approach, while necessary for\nexperimental control, may not capture all real-world usage scenarios."}, {"title": "2.3 Analysis", "content": "To test our research hypotheses, we employed multiple statistical approaches using Jamovi\nstatistical software. Initial analysis included descriptive statistics and tests of statistical\nassumptions. We conducted Shapiro-Wilk tests to assess normality of distributions for all\nvariables, complemented by examination of skewness and kurtosis. Due to identified non-\nnormal distributions (all Shapiro-Wilk tests p < .001), we proceeded with non-parametric\nanalyses where appropriate.\nDue to non-normal distribution of our dependent variables, we combined logistic regression\nwith non-parametric tests.\nFor examining scenario complexity effects (Hla-H1b), we specified a comprehensive logistic\nregression model for binary accuracy and Spearman correlations for log_distance:\nlogit(Binary_Opt)\n=\n$\\beta_0$\n+\n$\\beta_1$Irrelevant_Params_Count\n+\n$\\beta_2$Solution_Steps\n+\n$\\beta_3$Base_Constraints_Count + $\\beta_4$Text_Length + $\\beta_5$Word_Order_Type + $\\beta_6$Model_Name +\n$\\beta_7$Iteration_Number + $\\epsilon$\nFor analyzing text presentation influence (H2a-H2b), we employed both the logistic\nregression model above and Kruskal-Wallis tests to examine the relationship between\npresentation formats and log_distance.\nFor evaluating learning effects and model differences (H3a-H3b), we utilized the logistic\nregression model for binary accuracy while employing Friedman tests to examine iteration\neffects on log_distance. Model differences in log_distance was assessed using Kruskal-Wallis\ntests."}, {"title": "3 Results", "content": ""}, {"title": "3.1 General Descriptive Statistics", "content": "The analysis included 900 observations across five LLMs, testing their performance in\nquantitative management decision tasks. Two primary accuracy measures were employed: a\ncontinuous measure (logarithmic distance from optimal solution) and a binary measure\n(correct/incorrect solution)."}, {"title": "3.2 Primary Findings", "content": "Analysis of LLM performance in quantitative decision-making tasks revealed distinct patterns\nacross models, presentation formats, and complexity levels."}, {"title": "3.2.1 Model Performance Differences", "content": ""}, {"title": "3.2.2 Complexity Effects", "content": "Different aspects of task complexity showed varying impacts on model performance. Kruskal-\nWallis analysis revealed significant effects of task complexity components:"}, {"title": "3.2.3 Presentation Format Patterns", "content": "Different presentation formats showed varying effects on model performance. Initial analysis\nindicated that tabular presentations showed higher logarithmic distances (M = 0.305, SD =\n0.389) compared to direct (M = 0.225, SD = 0.272) and narrative formats (M = 0.226, SD =\n0.272).\nHowever, these differences did not reach statistical significance for either logarithmic\ndistance ($\\chi^2$ = 5.124, df = 2, p = 0.077, $\\epsilon^2$ = 0.00570) or binary accuracy ($\\chi^2$ = 0.856, df = 2, p\n= 0.652, $\\epsilon^2$ = 0.000952)."}, {"title": "3.3 Hypothesis Testing", "content": ""}, {"title": "3.3.1 Information Presentation Effects (H1a)", "content": "The first set of hypotheses examined the impact of presentation format on model accuracy. As\nshown in Table 8, initial descriptive statistics suggested potential differences between\npresentation formats. Further analysis using Kruskal-Wallis tests (Table 9) revealed that these\ndifferences did not reach statistical significance for either logarithmic distance or binary\naccuracy.\nThe logistic regression analysis provided additional insights, controlling for other variables.\nThe results showed that compared to direct format, neither narrative ($\\beta$ = 0.0999, SE =\n0.43899, p = 0.820) nor tabular formats ($\\beta$ = -0.1287, SE = 0.23410, p = 0.582) significantly\naffected binary accuracy. This held true even when controlling for text length ($\\beta$ = -1.01e-4,\nSE = 0.00257, p = 0.969) and other complexity factors.\nThese findings lead to the not-supporting of both H1a, suggesting that LLMs demonstrate\nsimilar performance across different presentation formats in quantitative management\ndecision tasks without significant difference between the type of text presentation."}, {"title": "3.3.2 Scenario Complexity Effects (H2a, H2b, H2c)", "content": "The second set of hypotheses examined how different aspects of scenario complexity affected\nmodel performance. The analysis considered base constraints (H2a), solution steps (H2b), and\nirrelevant parameters (H2c).\nThe Kruskal-Wallis Analysis of Task Complexity Components (Table 8) revealed significant\neffects of solution steps on both logarithmic distance ($\\chi^2$ = 43.2, df = 3, p < .001, $\\epsilon^2$ = 0.0480)\nand binary accuracy ($\\chi^2$ = 29.2, df = 3, p < .001, $\\epsilon^2$ = 0.0325).\nThe logistic regression analysis (Tables 13-15) provided additional insights while controlling\nfor other variables. Solution steps showed a significant positive effect on binary accuracy ($\\beta$ =\n0.4246, SE = 0.11583, p < .001), contrary to the hypothesized negative relationship. The\nnumber of irrelevant parameters demonstrated the expected negative effect ($\\beta$ = 1.2982, SE =\n0.15403, p < .001), strongly supporting H2c.\nAs shown in Table 2, Spearman's correlation analysis further supported these relationships,\nrevealing significant associations between complexity measures and performance. Base\nconstraints showed moderate correlations with both accuracy measures (rs = 0.294, p < .001\nfor logarithmic distance), while solution steps (rs = 0.126, p < .001) and numerical parameters\n(rs = 0.130, p < .001) showed weaker but significant correlations.\nThe results provided mixed support for the complexity hypotheses. While H2a and H2c were\nsupported, demonstrating significant effects of base constraints and irrelevant parameters on\nmodel performance, H2b was not supported due to an unexpected positive relationship\nbetween solution steps and accuracy. These findings suggest that the relationship between\ntask complexity and LLM performance is more nuanced than initially hypothesized."}, {"title": "3.3.3 Learning Effects and Model Differences (H3a, H3b)", "content": "The final set of hypotheses examined potential learning effects across iterations (H3a) and\nperformance differences between models (H3b).\nAnalysis of iteration effects revealed relatively stable performance across attempts (Iteration\n1: M = 0.256, SD = 0.301; Iteration 2: M = 0.256, SD = 0.343; Iteration 3: M = 0.244, SD =\n0.309).\nThe Friedman test for repeated measures showed no significant improvement of logd_distance\nacross iterations ($\\chi^2$ = 0.0208, df = 2, p = 0.990). The improve of binary correctness much\nstronger but still didn't reach the significant level ($\\chi^2$ = 4.63, df = 2, p = 0.099). This finding\nwas supported by Kruskal-Wallis analysis of both logarithmic distance ($\\chi^2$ = 0.485, df = 2, p =\n0.785, $\\epsilon^2$ = 0.00054) and binary accuracy ($\\chi^2$ = 1.830, df = 2, p = 0.400, $\\epsilon^2$ = 0.00204).\nThese findings were further supported by the logistic regression analysis (Tables 13-15). The\nregression model showed no significant effect of iteration number on binary accuracy ($\\beta$ =\n0.1251, p = 0.203), confirming the absence of learning effects. Model differences remained\nsignificant when controlling for other variables, with all models showing lower performance\ncompared to Claude-3.5-sonnet: Gemeni-1.5-pro ($\\beta$ = -0.5371, p = 0.029), Grok ($\\beta$ = -0.7418,\np = 0.003), and Llama-3.3-70b ($\\beta$ = -0.8514, p < .001)\nRegarding model differences (H3b), as previously shown in Tables 4-7, significant variations\nin performance were observed between models. The Kruskal-Wallis test revealed significant\ndifferences in binary accuracy ($\\chi^2$ = 14.15, df = 4, p = 0.007, $\\epsilon^2$ = 0.01575). Pairwise\ncomparisons indicated that Claude-3.5-sonnet outperformed Grok (W = -3.878, p = 0.048)\nand Llama-3.3-70b (W = -4.397, p = 0.016) in binary accuracy.\nTherefore, while H3a was not supported, showing no significant learning effects across\niterations, H3b was supported, revealing performance differences between models in\nquantitative management decision tasks."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Summary of Findings and Hypotheses Support", "content": "Our research examined the capabilities of LLMs in quantitative management decision tasks,\nfocusing on three key aspects: information presentation, scenario complexity, and learning\neffects. The results provide several interesting insights, some of which challenge existing\nliterature while others strengthen previous findings.\nRegarding information presentation (RQ1), contrary to previous research suggesting that\nLLM reasoning efficiency is substantially affected by how information is presented (Yu et al.,\n2024), our findings showed no significant effect of presentation format on either binary\naccuracy or logarithmic distance from optimal solutions. This unexpected result might be\nattributed to the rapid change of LLM capabilities, as recent research indicates that newer\nmodels demonstrate increased robustness across different presentation formats (He et al.,\n2024). Regarding text length (H1b), while the existing literature shows mixed findings - with\nsome studies finding that extended inputs can degrade performance (Levy et al., 2024) and\nothers demonstrating improved accuracy with constrained length (Nayab et al., 2024) our\nresearch found no significant correlation between text length and model performance.\nThe complexity hypotheses (RQ2) yielded mixed results. While H2a and H2c were supported,\ndemonstrating the negative impact of base constraints and irrelevant parameters on accuracy,\nH2b revealed an unexpected positive relationship between solution steps and accuracy. This\nfinding contrasts with Zhang et al.'s (2024) observation of diminishing accuracy with\nincreased steps, suggesting that contemporary LLMs might have developed better capabilities\nin handling multi-step reasoning tasks. This evolution aligns with recent research showing\nthat newer LLM versions are becoming more adept at handling complex reasoning chains\n(Akter et al., 2023; Chen et al., 2024; Zhang et al., 2024). However, the observed positive\ncorrelation between solution steps and accuracy is not fully understood and requires further\ninvestigation to elucidate the underlying mechanisms.\nRegarding learning effects and model differences (RQ3), our findings partially aligned with\nexisting literature. The absence of significant learning effects across iterations (H3a not\nsupported) aligns with recent research highlighting that while LLMs exhibit human-like\ndecision-making patterns, they fail to show improvement over repeated trials (Ryu et al.,\n2024). However, we found significant performance differences between different models\n(H3b supported), consistent with current literature highlighting varying capabilities across"}, {"title": "4.2 Validity Considerations and Limitations", "content": "While our dataset comprises 900 observations, it is important to note that these are derived\nfrom 20 base questions, each presented in different formats (word_type variable) and tested\nacross multiple models. This structure, while providing robust testing for format and model\neffects, creates inherent limitations in the range and distribution of independent variables such\nas constraint counts and solution steps. The relatively small number of base scenarios\nconstrains the possible combinations and ranges of these variables, potentially affecting our\nability to fully capture their relationships with model performance. A study incorporating a\nwider variety of base scenarios would allow for broader ranges and more varied combinations\nof these independent variables, potentially providing additional insights into their effects on\nLLM performance.\nThe rapid evolution of LLM capabilities presents both a challenge and an opportunity for\ninterpretation. Recent research has shown that \"the performance of LLMs can vary\ndramatically based on the prompt format used\" (He et al., 2024), suggesting that our findings\nmight be sensitive to the specific prompting approaches employed (we have used basic zero-\nshot prompt)."}, {"title": "4.3 Practical Implications and Future Research Directions", "content": "The findings of this study have several important implications for the implementation of\nLLMs in management decision-making contexts. The observed differences in model\nperformance suggest that organizations should implement systematic evaluation processes\nwhen selecting models for specific applications. While LLMs demonstrate significant\npotential for augmenting decision-making processes, their effective deployment requires\ncareful consideration of both model capabilities and task characteristics.\nThe relationship between task complexity and model performance revealed in this study\nchallenges some previous assumptions about LLM capabilities. The positive correlation\nbetween solution steps and accuracy, while unexpected, suggests that contemporary LLMs\nmay be more adept at handling complex, multi-step problems than previously theorized.\nHowever, the negative impact of irrelevant parameters and base constraints emphasizes the\nimportance of structured information presentation and careful prompt engineering in practical\napplications.\nFuture research should address current limitations through expanded scenario ranges to enable\na more comprehensive analysis of task characteristics and model performance relationships.\nAdditionally, given the rapid advancement in LLM technology, longitudinal studies\nexamining performance changes across model versions could provide valuable insights into\ncapability development patterns. Moreover, in light of the models' moderate success in\ndelivering precise outcomes, future studies should prioritize the exploration of task-specific\nmodels trained for quantitative management scenarios and the use of advanced prompting\ntechniques to enhance accuracy and reliability."}, {"title": "Conclusion", "content": "This study examines the performance of LLMs in quantitative management tasks,\nhighlighting key findings and implications for managerial decision-making support. The\nresults show that LLMs achieve similar performance regardless of presentation format and\ndemonstrate effective capability in handling multi-step tasks despite varying text\ncharacteristics as length. However, increasing numbers of parameters and constraints could\nnegatively impact models' performances. Our analysis reveals clearer patterns in approximate\nsolutions near the optimum, while exact solutions showed less consistency. While\napproximate answers may suffice in some managerial decision-making cases, precise answers\ncould require tailored prompting and specially trained models to avoid performance gaps.\nNotably, iterative attempts did not consistently improve accuracy, reinforcing the need for\ncareful design rather than reliance on repeated iterations. The observed performance\ndifferences between models further emphasize the importance of strategic model selection and\nevaluation processes. These findings underscore the need for flexible implementation\nstrategies, particularly given rapid technological evolution and the limited range of test\nscenarios analyzed.\nStudy limitations include the constrained diversity of tested scenarios and absence of\nlongitudinal assessments to capture performance changes over time. Future research should\nexpand scenario diversity, incorporate domain-specific applications, and adopt longitudinal\nstudies to better understand LLM performance trends and optimize their deployment for\nbusiness decision-making.\nFor practitioners, this research highlights the importance of adapting LLM-based systems to\nquantitative task-specific complexity levels while ensuring continuous assessment as\ntechnology evolves. Organizational leaders should prioritize strategic investments in model\nevaluation and integration processes to maximize the effectiveness of their decision-support\nsystems."}]}