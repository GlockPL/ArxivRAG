{"title": "Reinforcement Learning Approach to Optimizing Profilometric Sensor Trajectories for Surface Inspection", "authors": ["Sara Roos-Hoefgeest", "Mario Roos-Hoefgeest", "Ignacio Alvarez", "Rafael C. Gonz\u00e1lez"], "abstract": "High-precision surface defect detection in manufacturing is essential for ensuring quality control. Laser triangulation profilometric sensors are key to this process, providing detailed and accurate surface measurements over a line. To achieve a complete and precise surface scan, accurate relative motion between the sensor and the workpiece is required, typically facilitated by robotic systems. It is crucial to control the sensor's position to maintain optimal distance and orientation relative to the surface, ensuring uniform profile distribution throughout the scanning process. Reinforcement Learning (RL) offers promising solutions for robotic inspection and manufacturing tasks. This paper presents a novel approach using RL to optimize inspection trajectories for profilometric sensors. Building upon the Boustrophedon scanning method, our technique dynamically adjusts the sensor's position and tilt to maintain optimal orientation and distance from the surface, while also ensuring a consistent profile distance for uniform and high-quality scanning. Utilizing a simulated environment based on the CAD model of the part, we replicate real-world scanning conditions, including sensor noise and surface irregularities. This simulation-based approach enables offline trajectory planning based on CAD models. Key contributions include the modeling of the state space, action space, and reward function, specifically designed for inspection applications using profilometric sensors. We use Proximal Policy Optimization (PPO) algorithm to efficiently train the RL agent, demonstrating its capability to optimize inspection trajectories with profilometric sensors. To validate our approach, we conducted several experiments where a model trained on a specific training piece was tested on various parts in simulation. Also, we conducted a real-world experiment by executing the optimized trajectory, generated offline from a CAD model, to inspect a part using a UR3e robotic arm model.", "sections": [{"title": "I. INTRODUCTION", "content": "SURFACE inspection is a critical aspect of quality control in many industries, ensuring that manufactured components meet strict standards and function reliably. Accurate detection and characterization of surface defects are essential to maintaining product integrity and quality.\nMany industries still rely on manual inspection processes performed by human operators. However, manual inspection has already stopped being practical when it comes to the development of industrial demands for accuracy and efficiency. Manual inspection systems are not prone to detecting micron-scale defects. Therefore, advance sensor technologies are needed to serve for the need of small imperfections, which would be impossible to detect by human vision. A famous case study is car body pieces [1]. As the body components have a dimensional tolerance of several tenths of millimeter, the defects, whether it is functional stretching, or just a purely aesthetic protrusion, or imbalance, are sometimes one hundredth of this size.\nFurthermore, to meet the stringent requirements of modern industrial inspection, advanced sensor technologies have emerged as indispensable tools. Among them, laser triangulation is a widely adopted technique due to its superior precision and efficiency [2], [3]. This method involves projecting a laser line onto the surface of an object and capturing the reflected light using a camera or sensor. Through the analysis of the distortion of the projected line, detailed information about the surface topography can be obtained with high accuracy.\nTo achieve an integral scan of the entire surface of the part to be inspected, relative motion between the part and the sensor is required. Robotic systems, including robotic arms, [4], [5], unmanned aerial vehicles (UAVs) [6], drones or unmanned ground vehicles (UGVs) [7], [8], or autonomous underwater vehicles (AUVs) [9], have been increasingly integrated into inspection procedures in different applications to address this requirement. These systems facilitate precise and controlled travel between the inspected part and the sensor, enabling complete surface coverage and efficient inspection processes.\nEffective and accurate inspection requires meticulous planning of the sensor paths over the part surface. While manual planning is sufficient for simpler scenarios, more complex geometries or stringent accuracy standards require the implementation of automated methods. The generation of inspection paths for robotic systems represents a significant challenge, requiring predefined paths that take into account surface geometry, defect characteristics and inspection requirements.\nAlthough several studies related to automated inspection path planning can be found in the literature, highlighting the use of robotic arms, there is a significant gap in research specifically addressing the integration of robotics and profilometric sensors for surface inspection tasks.\nChen et al. highlight this gap in their study[10], where they propose a novel approach for automatically detecting surface defects on freeform 3D objects using a 6-degree-of-freedom manipulator equipped with a line scanner and a depth sensor. Their method involves defining local trajectories for precise defect inspection and optimizing global time for efficient scanning.\nLi et al. propose a method for planning scanning trajectories in automated surface inspection [11]. Their approach is based on a trajectory planning algorithm using a triangular mesh model. They divide the workpiece surface area into regions, determine scanning orientations and points in each region, and then generate scanning trajectories using the minimum enclosing rectangle. This method involves developing a section division algorithm to determine the scanning orientation, followed by generating trajectories that comply with system constraints.\nRecently, a new trend has emerged for trajectory generation in robotics using Reinforcement Learning (RL) methods. While several papers have explored the potential of RL in various applications, few have focused specifically on its application in inspection tasks. This lack of research highlights the need for further exploration and research in this area.\nFor example, Lobbezoo et al. compile in [12] different strategies present in the literature that use RL algorithms for pick and place applications. On the other hand, Elguea-Aguinaco et al. provides in [13] a comprehensive review of the current research on the use of RL in tasks involving intensive physical interactions. These tasks refer to activities where object manipulation involves direct and meaningful contact between the robot and its environment. This study covers research related to a variety of areas, including rigid object manipulation tasks (e.g., assembly, disassembly, or polishing and grinding) and deformable object manipulation tasks (e.g., rope or garment and fabric folding, tensioning and cutting, or object manipulation). The approaches and methodologies employed in each of these areas are compiled and analyzed.\nHan et al. present in [14] a comprehensive investigation of different applications of Deep RL in robotic manipulators, highlighting the main problems faced by RL in robotic applications. One of the most important problems is that the models used often do not perfectly replicate the real system. For example, in machine vision-guided robots, the simulation of RGB images can differ significantly from the actual images captured by cameras, a problem known as Sim-to-Real. This is because the simplified models do not fully capture the system dynamics. These discrepancies make it difficult to transfer simulation-trained models to real environments.\nTo address this issue, we use a realistic simulator presented in our previous work [15], which allows us to accurately represent the measurements obtained by the profilometric laser triangulation sensor.\nAnother problem they highlight is that trajectory generation in robotics is an inherently multidimensional problem, which further complicates the learning and optimization process. Also Ji et al. emphasize in [16] this problem, highlighting that in the field of robotics, most work using RL focuses on the field of mobile robot navigation due to its simple and well-developed theory [17].\nSurface inspection using profilometric sensors is typically performed in a straight line. If the workpiece is too large to be covered in a single scan, parallel passes are used, generally following Boustrophedon-like trajectories [18]. In our approach, we will start with these linear or Boustrophedon trajectories and enhance them using reinforcement learning techniques to optimize sensor movements and ensure comprehensive surface coverage. During each pass, the sensor advances in a predetermined direction while it can adjust its height and pitch angle over the piece, keeping the other orientations constant. Our approach effectively tackles the multidimensional challenge in surface inspection by concentrating on three critical parameters: the sensor's position along the scanning direction, its height adjustment, and pitch orientation. This focus simplifies the problem significantly, eliminating the need to individually manage each robot axis and streamlining sensor control and trajectory planning during inspections.\nReinforcement learning (RL) techniques offer effective solutions for problems with limited action spaces, making them well-suited for optimizing surface inspection tasks. RL's ability to learn from interactions and improve control policies based on rewards makes it a promising tool for addressing challenges in this field. Despite advancements in RL algorithms, its application in inspection tasks remains relatively underexplored compared to other robotics applications. This gap highlights the necessity for further exploration and research in this area.\nIn the realm of inspection applications, Xiangshuai Zeng's research presents PIRATE (Pipe Inspection Robot for Autonomous Exploration) [19], a robot designed for internally inspecting pipes using reinforcement learning. Equipped with multiple bending joints and wheels, PIRATE offers adaptive and efficient mobility. By employing algorithms like PPO and deep neural networks, the robot learns to navigate sharp corners, adjusting its behavior in real-time and adapting to various pipe configurations. The system defines actions, rewards, and observations, with actions including wheel movements and adjustments in bending joints, and observations coming from a 2D laser scanner.\nAnother work focused on inspection applications is presented by Jing et al. in [20], centering on the automatic generation of robotic trajectories for surface inspection in production lines. They use techniques such as Monte Carlo algorithms and greedy search for Coverage Path Planning (CPP), enabling the automatic generation of inspection trajectories adapted to objects of different sizes and geometries. Using a 3D structured light scanner, they generate a 3D point cloud representing the scanned object's geometry. The main objective is to minimize total cycle time, combining View Planning Problem (VPP) and trajectory planning to minimize the total sum of inspection and displacement costs after meeting surface coverage requirements. The trajectory generation process includes random viewpoint selection, robot movement calculation, collision-free path planning, evaluation of visibility for each viewpoint and covered surface portion, and application of an RL-based planning algorithm to select inspection actions until completion. Actions involve selecting a viewpoint and moving the robot to place the 3D scanner at the selected viewpoint. The proposed RL algorithm automatically generates the online inspection policy, taking as input the robot model, target object model, and sensor specifications.\nAligned with prior research, Christian Landgraf, Bernd Meese et al. introduce in [21] an approach for automatic viewpoint planning in surface inspection. They propose a strategy to find optimal sets of viewpoints for 3D inspection of specific workpieces. Their goal is to automate this process for any industrial robotic arm available in ROS and any 3D sensor specification; in their application they employ a stereo camera. Using advanced reinforcement learning algorithms such as Q-learning, Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), each action involves selecting the viewpoint and planning and executing the robot's movement towards this pose. Upon reaching its goal, the sensor generates a 3D point cloud at this specific pose, with the state of the environment constructed using 3D measurement observations and the robot's current pose.\nFor case studies like the one in this article, which uses laser triangulation profilometric sensors for measurements along a line, traditional trajectory planning approaches, such as the mentioned View Planning Problem (VPP), are not suitable. The VPP is intended for finding optimal poses for capturing images or making measurements of objects or surfaces using 3D vision sensors that cover a wide area. This highlights the need for a trajectory planning approach adapted to the specific characteristics of profilometric sensors, focusing on optimizing surface exploration rather than capturing a complete three-dimensional view.\nTo date, no research has been found regarding the generation of inspection trajectories using Reinforcement Learning and profilometric sensors, highlighting a significant gap in the literature and a promising area for robotics and automation research. This study addresses this gap by presenting an RL-based approach designed for generating inspection trajectories using profilometric sensors. Key contributions involve the modeling of the state space, action space, and reward function. We use the PPO (Proximal Policy Optimization) algorithm to efficiently train the RL agent, demonstrating its ability to optimize inspection trajectories with profilometric sensors.\nPPO is an algorithm proposed by OpenAI in [22]. The authors highlight its ability to balance three key aspects in reinforcement learning: ease of implementation, sampling efficiency, and simplicity in hyperparameter tuning. PPO not only offers competitive performance comparable to or better than more advanced reinforcement learning algorithms, but also stands out for its significantly simpler implementation and tuning.\nThe effectiveness of PPO has been demonstrated across a wide range of applications, including robot control such as the PIRATE robot [19] or the development of view planning systems for inspection, as presented in the work of Landgraf, Meese et al. [21]. Additionally, it has been successfully applied in pick-and-place tasks, such as training 7-degree-of-freedom robotic arms for precise object manipulation, as described in [23], as well as in other pick-and-place applications, as discussed in [24]. Furthermore, comparisons between PPO and other reinforcement learning algorithms, such as SAC and TD3, reveal interesting patterns in terms of training efficiency, performance, and convergence. For example, in [25], it was found that PPO tends to perform better in smaller state spaces, while SAC shows advantages in larger state spaces. On the other hand, in [24], PPO and SAC were compared, where SAC demonstrated greater efficiency in sampling, but PPO exhibited greater insensitivity to hyperparameters and more stable convergence in complex problems. These findings support the choice of PPO as the main algorithm for the proposed research."}, {"title": "II. MATERIALS AND METHODS", "content": "In this section, we present the proposed approach for generating inspection trajectories using profilometric sensors and Reinforcement Learning techniques. The goal is to optimize each pass of the Boustrophedon scanning method, seeking an optimal orientation and distance of the sensor relative to the part at each position. This involves dynamically adjusting the position and tilt (pitch) of the sensor to maintain a constant and appropriate pose between the sensor and the surface at all times. The other two sensor orientations will be fixed beforehand, allowing for precise and uniform data capture. Additionally, we aim to optimize the spacing between profiles. This approach ensures complete and detailed coverage of the part's surface, thereby maximizing the quality and accuracy of the captured data.\nTo train the RL algorithms, we use a simulated environment that replicates the conditions of the real system described developed in our previous work [15]. This simulator emulates the measurements of a laser triangulation profilometric sensor,\nincluding sensor noise and speckle noise generated by the object's surface. Thus, a realistic and controlled training environment is obtained.\nThe state space is constructed using the position and orientation of the robot's end-effector. This allows for generalization of the approach and facilitates the transfer of the method to different robotic configurations. Additionally, the state also includes other parameters, such as the mean profile distance, the direction angle, and the advance between consecutive scans.\nThe action space is defined by relative increments in the sensor's position and tilt angle, allowing for precise adjustments and smooth movements of the sensor. The reward function consists of three key components: the distance between the sensor and the surface, the alignment of the sensor with the surface normal, and the spacing between consecutive scans. This comprehensive reward function encourages optimal behaviors in terms of distance, orientation, and sensor advancement.\nNext, each component of the proposed method is described in detail, providing a thorough understanding of its design and implementation."}, {"title": "A. Reinforcement Learning (RL): Basic Concepts", "content": "Reinforcement Learning (RL) is a branch of machine learning inspired by behavioral psychology, which focuses on how agents make decisions within an environment to maximize some measure of accumulated reward over time [26].\nIn RL, the agent dynamically interacts with the environment, observing its current state and selecting actions in response. These actions affect the environment's state and generate a reward signal that guides the agent's behavior. The primary goal of the agent is to maximize the accumulation of these rewards over time, thereby optimizing its performance in the environment. \nThe agent refers to the machine learning algorithm that interacts with the environment. The environment is the adaptive problem space with attributes such as variables, boundary values, rules, and valid actions. Each action represents a step taken by the RL agent to navigate through the environment. The set of all valid actions in a given environment is referred to as the action space A, defined mathematically as A = {a1, a2, ..., an}, where ai represents a specific action in the set, and n is the total number of actions. A state represents the environment at a given point in time. The set of all possible states in an environment is S = {s1, s2, ..., sm}. The reward is the positive, negative, or neutral value the agent receives as a consequence of an action, assessing its quality. The reward at each time step rt depends on each state-action pair rt = r(st,at). The accumulated reward is the sum of all rewards obtained over time.\nIn most cases, the problem to be solved is formally modeled as a Markov Decision Process (MDP). An MDP can be defined as a tuple of 5 elements (S, A, r, P, po), representing respectively the set of all valid states, the set of all valid actions, the reward function, the state-action transition probability function, and the initial state distribution.\nAnother key parameter is the policy (\u03c0), which is the strategy guiding the agent's decision-making within the environment. The policy can be deterministic, where for each state in the environment, the agent selects a specific action predictably or stochastic, meaning that the agent chooses actions based on probabilities, introducing uncertainty and allowing for exploration. The goal of any reinforcement learning algorithm is to select a policy that maximizes the expected return when the agent acts according to it. The expected return J(\u03c0) is represented by equation 1. The expected reward E\u03c0[r(st, at)] is the average of the rewards the agent expects to receive by following the policy in each state s. The objective is to adjust the policy parameters to maximize this reward, using optimization methods such as policy gradient to continuously improve the policy and the agent's performance in the specified task.\n$J(\\pi) = E_{\\pi}[r(s_t, a_t)]$\t(1)"}, {"title": "B. Scanning Characteristics for Surface Inspection with Profilometric Sensors", "content": "During the scanning process using laser triangulation profilometric sensors, the quality of the measured data is directly affected by various parameters associated with the relative position between the sensor and the inspected piece, as detailed in [11]. These parameters are critical to ensuring precise and thorough surface inspection. Therefore, it is essential to carefully consider these factors during the planning of scanning trajectories in order to achieve effective results in surface inspection.\nOne of these crucial parameters is the Optimal Working Distance (Wa), which denotes the ideal distance between the sensor and the object's surface. This distance ensures optimal precision of the captured data by positioning the laser source at the scanning reference plane, typically located at the midpoint of the depth of field.\nThe Depth of Field (Zr) refers to the range of distances within which the sensor can accurately capture surface data during a single scan (Figure 2(b)). Assuming a point in the scanner's coordinate system is (xs, 0, zs), the equation 3 must be satisfied. Operating within this range is critical as it minimizes noise levels associated with deviations from the optimal working distance. Studies [27] have demonstrated that maintaining proximity to the optimal working distance reduces noise, thereby enhancing measurement accuracy.\n$\\frac{W_d}{2} \\leq z_s \\leq W_d+\\frac{Z_r}{2}$\t(3)\nAnother crucial parameter is the Direction Angle (a), which signifies the angle between the sensor's orientation vector T and the normal vector R of the workpiece surface (Figure 2(c). This angle is computed using Equation 4. As the direction angle increases, there is a higher likelihood of introducing noise into the capture. This phenomenon occurs because the scanner may capture unwanted reflections of the laser light and variations in surface reflectivity, negatively impacting the data quality. Previous studies [28], [27] have empirically shown how noise levels correlate with the direction angle, highlighting its significance in achieving precise surface capture.\n$\\alpha = acos(\\frac{\\overrightarrow{n} \\cdot \\overrightarrow{l}}{|n||l|})$\t(4)\nAdditionally, the Distance Between Profiles (As) determines the density of points between consecutive scan profiles. Adequate point density ensures comprehensive coverage and accuracy of the inspected surface, particularly in areas with small features or irregular surfaces where a lower density might compromise inspection quality. See figure 2(d).\nIn addition to considering these parameters, it is crucial to choose the appropriate type of trajectory to achieve a complete scan of the surface of the piece. In laser profilometer inspection applications, one of the most common strategies is to use Boustrophedon paths [29], [18].\nIn a Boustrophedon scan, the sensor moves in a straight line along one axis until it reaches the edge of the surface to be inspected. Then, it shifts laterally a predetermined distance and changes direction to move in the opposite direction along the initial axis. This pattern of movements is repeated until the entire surface is covered. In the context of surface inspection, this method is highly efficient in ensuring that every area of the piece's surface is scanned without omissions, thereby maximizing coverage and inspection accuracy.\nConsidering these types of trajectories, the profilometric sensor collects data only during the parallel passes along the surface of the piece. In Figure 3, these trajectories are shown in red, from the initial point of a pass (Pinii) to its end point (Pfini), where i denotes the number of parallel passes. The movement of the robot between each pass is shown in black. The distance between passes, d, is carefully adjusted to ensure that the scans overlap adequately, thereby completely covering the piece."}, {"title": "C. Simulated Environment", "content": "To train the reinforcement learning algorithms, it is essential to have an environment that simulates the conditions of the real system. Conducting tests directly on the real system can be costly, dangerous, or simply impractical in many situations. Therefore, simulators are used to virtually recreate the environment of interest.\nWe will use the simulator detailed in our previous work [15], designed to replicate the conditions of the real system in a virtual environment. This simulator recreates the measurements of a laser triangulation profilometric sensor, emulating the parameters of any commercial sensor according to its specification sheet. It allows for precise reproduction of measurements on a CAD model of the part to be inspected, including the introduction of inherent sensor noise and speckle noise generated by the object's surface. See Figure 4.\nIn each iteration of the simulator, several critical parameters are obtained that will be used later by the RL algorithm. First, the distance profile is captured, a fundamental representation provided by any profilometric sensor, see Figure 5. Additionally, the 3D position of the scanned points of the CAD model is collected, providing detailed information about the surface geometry of the object, see Figure 6. Furthermore, the simulator also provides data on the normals at those points on the object's surface."}, {"title": "D. State Space", "content": "As previously mentioned the position and orientation of the end-effector are used instead of relying on the positions and velocities of the robot's joints. This choice simplifies the state space and facilitates the transfer of the method to different robotic configurations without the need for specific adjustments in the joints.\nMathematically, the state S is defined as a tuple as follows:\n$S = {P(x, y, z), \\theta, D, \\alpha, \\Delta s}$\t(5)\nHere, P(x, y, z) represents the position of the end-effector, while \\theta denotes its tilt. The parameters D, \\alpha, and \\Delta s correspond to the mean profile distance obtained from the scan, the angle between the sensor and the surface, and the advance between consecutive scans in the 3D space, respectively."}, {"title": "E. Action Space", "content": "The action space is defined by the increments in the position and tilt angle of the inspection sensor. These increments are defined relative to the sensor's own coordinate system. Mathematically, the action space is represented by equation 6.\nA = {Ay, Az, \u2206\u03b8}\t(6)\nWhere Ay refers to the increment in position in the sensor's forward direction (Y), which will be previously defined by a unit vector indicating the scanning direction. Az refers to the increment in position in the sensor's vertical direction (Z), controlling the height of the end-effector relative to the part. A\u03b8 denotes the change in the sensor's pitch orientation, which is the rotation around the X-axis. This is represented in Figure 7.\nThe action space is defined as continuous, meaning that actions span a continuous range of values rather than discrete ones. This approach ensures smooth and controlled sensor movements to avoid abrupt changes that could affect measurement accuracy or cause collisions with the workpiece. Equation 7 establishes the limits for each type of action in the continuous space. Here, Ay, Az, and A\u03b8 are constrained to values between \u00b1\u2206ymax millimeters, \u00b1\u2206zmax millimeters, and \u00b1\u03b8max degrees, respectively.\nAy \u2208 [-\u2206ymax, \u2206ymax]\nAz\u2208 [-\u2206zmax, \u2206zmax]\n\u0394\u03b8\u2208 [-\u2206\u03b8\u0442\u0430\u0445, \u2206\u04e9\u0442\u0430\u0445]\t(7)\n1) Dynamic Action Limitation: To ensure smooth and safe movement of the inspection sensor, the selection of actions is dynamically adjusted based on the environment's observations. This action limitation accelerates the convergence of the reinforcement learning algorithm, enabling the system to learn more efficiently and effectively.\nWhen the sensor is farther from the part surface than the optimal working distance Wa, limits are applied to the sensor's displacement in the Z direction Az to bring it closer to the surface in a controlled manner. Conversely, if the sensor is too close, displacements in the negative direction are limited, as per equation 8.\nAz =\nclip(Az, 0, Azmax) if (D - Wa) \u22650\nclip(Az, -Azmax,0) if (D \u2013 Wa) < 0\t(8)\nHere, clip(x, a, b) limits the value of x between a and b, ensuring that the actions are within the permitted range, according to equation 9.\n{\na if x < a\nclip(x, a, b) =< b if x \u2265 b\nx else\t(9)\nSimilarly, if the sensor's direction angle (a) with respect to the surface normal is positive, indicating excessive tilt, limits are applied to the angular displacement A\u03b8 to correct the sensor's orientation. Conversely, if the tilt angle is negative, limits are applied to the angular displacement in the opposite direction to keep the sensor properly aligned with the inspected surface. This is represented in equation 10.\n{\nclip(0, 0, \u03b8max) if a \u2265 0\n\u03940 =< clip(0, - max, 0) if a < 0\t(10)"}, {"title": "F. Reward Function", "content": "In reinforcement learning, creating an effective reward model is crucial as it guides the agent toward desirable behaviors within the environment. This model assigns a value to each state-action pair, reflecting the immediate benefit or cost associated with the agent's decision. This section details the reward strategy designed in this research.\nR(s, a) = WARD + WaRa + WwsRos\t(11)\nRD represents the reward related to the distance between the sensor and the inspected surface, Ra denotes the reward related to the alignment of the sensor's orientation with the normal of the inspected object's surface, and RAS captures the reward associated with the sensor's movement between consecutive scans in the 3D space corresponding to the point cloud of the inspected piece. wd, Wa, was represent the weights that each component contributes to the overall reward function.\nThe proposed rewards are in the range [0, -1], as the reward function aims to incentivize the agent to perform actions that improve the inspection process. The maximum value of 0 is assigned when the optimal goal is reached, while negative values indicate penalties for deviations from the desired behavior.\n1) Distance Reward (RD): To ensure that the sensor maintains an optimal distance from the inspected surface, a distance reward function RD is defined as a continuous penalty function that decreases as the absolute difference between the observed distance and the optimal working distance Wa increases. The reward function is formulated as follows:\nRD =\n(Wa \u2013 D)2\n()2\t(12)\nWhere Wa represents the optimal working distance, D the observed distance during scanning, and Zr the specified working range of the sensor. This results in a parabolic function with values between [-1,0], corresponding to 0 when operating at the optimal working distance and -1 at the sensor's range limits, as shown in Figure 8. If the distance is outside this range, the penalty is maximum (-1).\n2) Orientation Reward (Ra): To induce the agent to align its orientation with the surface normal, we introduce an orientation reward model (Ralpha). This model is designed to minimize the angular disparity between the sensor direction and the surface normal vector. The function is defined as a continuous penalty function that approaches 0 as the absolute orientation difference decreases, see Figure 9:\nRa = max(-1,-2)\namax\t(13)\nWhere a is the angular difference between the sensor's orientation and the surface normal, and amax is the maximum allowed angular disparity threshold. This model encourages the agent to maintain close alignment with the surface normal, optimizing the quality of the inspection.\n3) Movement Reward (R\u25b3s): In addition to optimizing the distance and orientation of the sensor, ensuring smooth forward movement is crucial for comprehensive surface coverage. Forward scanning movement ensures that each scanned 3D profile extends beyond the previous one, facilitating thorough inspection. The reward function R\u25b3s is expressed as:\nRs = max(-1,\n(As - Sopt)\u00b2)\nAsopt\t(14)\nThis function penalizes the agent when the scanning spacing As is negative, indicating backward movement within the inspection area. Likewise, it behaves parabolically with respect to the scanning spacing As. When the spacing is equal to twice the optimal value Asopt, the reward reaches its minimum value of -1. This indicates a strong penalty for excessively large spacings. As the spacing decreases from this point, the reward gradually increases, reaching a maximum value of 0 when the spacing is exactly equal to the optimal value. Therefore, the reward function motivates the agent to maintain spacing close to the optimal, as both above and below-optimal values result in a decrease in reward, see Figure 10."}, {"title": "G. RL Algorithm: PPO", "content": "The Proximal Policy Optimization (PPO) algorithm [22] is a policy gradient technique designed to provide faster and more efficient updates than previously developed reinforcement learning algorithms, such as Advantage Actor-Critic (A2C) or Deterministic Policy Gradient (DPG).\nPPO was designed as an improvement over Trust Region Policy Optimization (TRPO). It simplifies and accelerates the training process by using first-order gradients and a clipped objective function that stabilizes policy updates.\nPPO employs a clipped surrogate loss function, penalizing excessive changes in the policy, stabilizing training, and preventing significant divergence between new and old policies. The clipped objective function of PPO is defined by equation 15.\n$J_{clip} (\\pi) = E [min (r(\\pi)\\hat{A}, clip (r(\\pi), 1 \u2013 \\epsilon,1 + \\epsilon) \\hat{A})]$\t(15)\nHere, \u03c0 represents the policy, E denotes the expectation over time, r is the ratio of probability under the new and old policies, respectively, \u00c2 is the estimated advantage, and e is a hyperparameter controlling how much the new policies are allowed to differ from the old policies during the optimization process. It is used to compute a penalty function that limits the policy change at each optimization iteration. The probability ratio r(\u03c0) is calculated as the probability of selecting an action under the new policy divided by the probability of selecting the same action under the old policy, i.e.:\n$r(\\pi) = \\frac{\\pi_{new}(a_t|s_t)}{\\pi_{old}(a_t|s_t)}$\t(16)\nFor the PPO algorithm, the hyperparameter configuration involves several key aspects. The neural network architecture defines the structure of the neural network, including the number of hidden layers and units per layer. An activation function, typically ReLU, is applied to the outputs of each hidden layer to introduce non-linearities into the model. The learning rate determines the size of the step taken during weight updates, influencing the speed and stability of learning. Additionally, the clip ratio limits policy changes between updates to ensure training stability. The epoch parameter denotes the number of complete passes through the training dataset during training."}, {"title": "III. RESULTS", "content": "In this section, we present the experiments conducted to validate and evaluate the proposed methods in the context of generating inspection trajectories using reinforcement learning (RL) algorithms. These algorithms were implemented using the open-source library stable-baselines3 [30], which provides enhanced implementations of reinforcement learning algorithms based on OpenAI. To analyze and process the obtained results, we used MATLAB 2023b.\nFirst, we detail the training process of the RL model, including the architecture of the neural network used, the configured hyperparameters, and the training methodology employed.\nSubsequently, the trained RL model is employed to generate inspection trajectories for two different parts using their CAD models: a car door and the body of a Parrot drone, see figure 11. Due to its dimensions, the car door will be scanned using a Boustrophedon trajectory base, whereas the drone will be scanned with a single straight-line scan.\nThe trained RL model takes the CAD model of the part as input and produces a sequence of movements that the sensor must follow to efficiently scan its surface. This trajectory is designed to minimize the error between the actual distance of the sensor to the part and the sensor's optimal working distance, as well as the direction angle. Additionally, it ensures a constant separation between scan profiles, guaranteeing uniform and precise coverage of the entire surface.\nWe perform different experiments, both in simulation and in a real environment. Simulation results obtained during the execution of the trajectories in our inspection simulator are presented and analyze. The results of the scans generated by RL optimized trajectories are compared with conventional methods such as straight line trajectories or Boustrophedon type trajectories.\nFinally, an experiment is conducted using a UR3e robot in a real-world environment to execute the inspection trajectory generated offline by the trained RL model for the Parrot drone. We analyze the results obtained to validate the transferability of the solution from the simulated environment to practical real-world applications."}, {"title": "A. Training Process", "content": "The training process of the RL model for trajectory optimization in robotic inspection was developed using a detailed simulation environment, the characteristics of which are explained in [15", "1": "where position increments are measured in millimeters and pitch angles in degrees.\nTable II summarizes the hyperparameters used for the PPO algorithm. These parameters were selected based on default values recommended by the authors of the stable-baselines3 library. The neural network architecture consists of two hidden layers with 64 units each and ReLU activation function. A learning rate of 0.0003 was employed, with updates performed every 2048 steps and a batch size of 64. The discount factor (\u03b3) was set to 0.99, and a clip ratio of 0.2 was used to stabilize training by limiting policy updates. Training proceeded over 10 epochs to refine the policy effectively. These hyperparameters were chosen to balance exploration and exploitation, ensuring robust and efficient learning within the RL framework.\nDuring the training process of the algorithm, various metrics are employed to assess their performance and convergence capability. These metrics include the reward per episode, the length of episodes, and the number of episodes required to reach a certain performance level. The reward per episode is a crucial metric indicating the total amount of reward accumulated by the agent in each training episode. Generally, a higher reward reflects better performance of the agent in the task.\nHowever, in this specific training context, evaluating solely the accumulated reward per episode might not be the most appropriate approach. This is because the length of episodes can vary significantly depending on the step size, defined as the distance between profiles. Therefore, instead of focusing solely on the accumulated reward, it is preferred to evaluate the globally normalized reward by the length of the episode. This metric provides a more comprehensive assessment of the agent's performance, as it considers both the accuracy of measurements and the efficiency of the trajectory. By doing so, a more precise insight into the overall effectiveness of the model in trajectory optimization and"}]}