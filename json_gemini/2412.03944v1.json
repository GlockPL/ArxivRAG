{"title": "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation", "authors": ["Hao Yang", "Qinghua Zhao", "Lei Li"], "abstract": "Chain-of-Thought prompting has significantly enhanced the reasoning capabilities of large language models, with numerous studies exploring factors influencing its performance. However, the underlying mechanisms remain poorly understood. To further demystify the operational principles, this work examines three key aspects: decoding, projection, and activation, aiming to elucidate the changes that occur within models when employing Chain-of-Thought. Our findings reveal that LLMs effectively imitate exemplar formats while integrating them with their understanding of the question, exhibiting fluctuations in token logits during generation but ultimately producing a more concentrated logits distribution, and activating a broader set of neurons in the final layers, indicating more extensive knowledge retrieval compared to standard prompts. Our code and data will be publicly avialable when the paper is accepted.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable performance across various tasks, yet their efficacy remains suboptimal in certain reasoning tasks (Wei et al., 2022). This underscores the urgent need to understand and optimize LLMs in complex reasoning scenarios. The Chain-of-Thought (CoT) prompting has emerged as a promising solution, significantly enhancing LLMs performance in these challenging tasks (Wei et al., 2022; Zhang et al., 2024). Recent research has extended CoT to more complex structures such as trees and graphs (Yao et al., 2024; Besta et al., 2024), while other studies have explored various factors affecting CoT performance, including the number of reasoning steps, difficulty, length, and correctness of reasoning processes (Webson and Pavlick, 2021; Chia et al., 2023; Wang et al., 2023; Schaeffer et al., 2023; Jin et al., 2024; Prabhakar et al., 2024).\nSeveral hypotheses are proposed regarding the underlying principles of CoT's functionality. Schaeffer et al. (2023) suggest that CoT enhances the reasoning capabilities of LLMs, while Madaan et al. (2023) argue that CoT reduces task difficulty, making tasks more comprehensible to models. Saparov and He (2023) propose that models mimic its format, filling in answers based on the provided exemplar. Merrill and Sabharwal (2024) offer a theoretical analysis of how CoT improves the computational power of models. Although these hypotheses await further experimental validation, they provide valuable insights for exploring the fundamental mechanisms of CoT.\nDespite these advancements, the specific changes that occur both internally and externally within the model when employing CoT remain unclear. This paper aims to address this knowledge gap by examining CoT mechanisms from three perspectives: the decoding phase, the projection phase, and neuron activation. We seek to answer the following questions:\n1. Does the large model merely mimic the patterns found in CoT exemplars?\n2. What changes occur in the model's projection space (e.g., logits) when using CoT prompt as opposed to the standard prompt?\n3. Does CoT enable models' deeper and broader utilization of knowledge acquired during the pre-training phase?\nTo answer these questions, we employ a multi-faceted approach:\n\u2022 In the decoding phase, we analyze CoT-generated text, focusing on four test points: time, action, location, and number. We also conduct a transfer test to evaluate CoT's performance across different datasets.\n\u2022 In the projection phase, we examine horizontal and vertical changes in projected logits,"}, {"title": "2 Related Work", "content": "Recent years have witnessed remarkable progress in LLMs, with models such as GPT-4 (Achiam et al., 2023) and LLaMA (Meta, 2024) demonstrating exceptional capabilities across a wide range of tasks. The introduction of CoT has further elevated their performance, particularly in complex reasoning tasks including arithmetic, commonsense, and symbolic reasoning (Wei et al., 2022).\nThe success of CoT has spurred a wave of research aimed at extending and refining this approach. Notable contributions include the development of self-consistency (Wang et al.), Boosting of Thoughts (Chen et al.), and least-to-most prompting (Zhou et al.). These methods have been complemented by zero-shot CoT (Kojima et al., 2022a), which eliminates the need for task-specific examples, and more specialized techniques such as Program-of-Thought (Bi et al., 2024), Mixture-of-Thoughts (Li and Qiu, 2023), and Contrastive Chain-of-Thought (Zhang et al., 2024). The versatility of CoT has been further demonstrated through its application in enhancing performance across diverse tasks (Tanneru et al., 2024).\nResearch on CoT has uncovered key factors influencing its performance. Studies show that maintaining a constant number of reasoning steps, even with errors, does not hinder performance as long as logical coherence is maintained (Jin et al., 2024; Chia et al., 2023; Wang et al., 2023; Schaeffer et al., 2023; Webson and Pavlick, 2021; Prabhakar et al., 2024). While intermediate steps in CoT do not directly improve problem-solving, they enhance task comprehension (Madaan et al., 2023). CoT also stabilizes significance scores (Wu et al., 2023), and diverse examples complement each other to improve reasoning (Ye et al., 2023).\nUnderstanding why CoT is effective remains limited, but recent research has proposed several hypotheses based on experimental findings. Madaan et al. (2023) use counterfactual methods to modify exemplar content, observing that while intermediate steps in CoT may not enhance task-solving abilities, they improve task comprehension. Saparov and He (2023); Chia et al. (2023); Wang et al. (2023); Wang and Zhou (2024) suggest reasoning abilities are acquired during pre-training, with exemplars guiding response generation. Madaan and Yazdanbakhsh (2022) posit that intermediate steps act as templates for the model to fill in answers rather than aiding task-solving. Nowak et al. (2024); Merrill and Sabharwal (2024) argue CoT boosts computational power. Gudibande et al. (2023) find that while imitation improves style and adherence to instructions, it does not enhance factuality, coding, or problem-solving. Rai and Yao (2024) use GPT-4 to study neuronal activation patterns, explaining the importance of equations, textual explanations, and exemplar diversity, as well as why incorrect reasoning does not necessarily impair performance.\nOur paper contributes to the growing body of research investigating the underlying mechanisms of CoT prompting. Unlike existing work, we approach this question from three distinct perspectives: decoding, pre-decoding steps, and neuronal activation, specifically exploring whether the model mimics exemplars during decoding, how the model's projection space changes with CoT, and whether CoT facilitates deeper knowledge exploration, aiming to provide a more comprehensive understanding of how CoT enhances the reasoning capabilities of language models."}, {"title": "3 Methodology", "content": "3.1 Evaluation Tasks\nFollowing the methodologies outlined by Chia et al. (2023); Madaan et al. (2023); Jin et al. (2024), we conducted experiments on reasoning tasks, specifically focusing on arithmetic reasoning, commonsense reasoning, and symbolic reasoning.\nArithmetic reasoning. To evaluate arithmetic reasoning, we used three well-established datasets: GSM8K (Cobbe et al., 2021) with grade-school math problems, SVAMP (Patel et al., 2021) featuring diverse mathematical structures, and AQUA (Ling et al., 2017) with multiple-choice problems. These challenging benchmarks are widely used in mathematical reasoning research.\nCommonsense reasoning. For commonsense reasoning tasks, we employed the Bamboogle"}, {"title": "3.3 RQ-1: Imitation or Understanding?", "content": "Current views suggest that LLMs develop core problem-solving and analogical skills during pre-training, with exemplars guiding their output by providing style and format cues. Interestingly, even if the exemplars in the prompt contain multiple errors, they do not seem to affect the model's output (Wang et al., 2023; Wang and Zhou, 2024; Madaan and Yazdanbakhsh, 2022).\nHowever, recent studies have shown that LLMs can learn the mapping relationships between inputs and outputs from exemplars, even when these relationships are inconsistent with the knowledge acquired during pre-training (Liu et al., 2023). This raises the question of whether LLMs are merely"}, {"title": "Fine-grained Analysis (Test Points Match)", "content": "Inspired by computer vision methods for comparing actions using key points (Wan et al., 2018; Zou et al., 2024), we applied a similar approach with \"test points\" to assess whether the model's generated content aligns more with the exemplar or pre-trained knowledge. We predefined four types of test points:\n1. time test points: including conjunctions like \"first, therefore, after\u201d that indicate sequence.\n2. action test points: including verbs like \u201cadd, divide\" that denote mathematical operations.\n3. loc&peo test points: including location and person words like \u201ctable, outside, we, he\" that denote specific people or places.\n4. number test points: including \u201ctwo, 53", "results": "the proportion of time test points is relatively high, whereas action test points are relatively low. Since time test points (e.g., \"first\u201d, \"then\u201d,", "so": "etc.) in the exemplars serve to indicate the sequence of reasoning steps, their presence in the generated content suggests that the model is imitating the format presented in the exemplars.\nSecond, from the perspective of models, the proportions of the four types of test points in the generated content do not differ significantly among different models. This indicates that these exemplar formats are easy for models to imitate, even smaller ones like Gemma2-2b.\nFinally, the overall proportion of test points in the generated content is relatively small, which indicates that the majority of the content generated by the model consists of personalized responses based on its understanding of the question."}, {"title": "Coarse-grained Analysis (Transfer Test)", "content": "The aforementioned experiment illustrated the model's mimicry of specific words in the generated content. To evaluate the mimicry at a higher scale, we drew inspiration from the concept of transfer learning (Pan and Yang, 2009), transferring exemplars from one task to another. Specifically, we migrated exemplars within and between three types of reasoning tasks: arithmetic reasoning, commonsense reasoning, and symbolic reasoning.\nAnalysis. Figure 2 shows the test point match between model-generated content and the input question (lower part) and the exemplar (upper part). It can be observed that in the transfer use of CoT prompts across different tasks, the test point most frequently shared between the model-generated content and the question content is \u201cnumber\u201d (e.g., Figure 2a to Figure 2d), while the test point most frequently shared with the tested prompt (i.e., exemplar) is \"time\" (e.g., Figure 2a to 2i). This experiment further supports the above empirical hypothesis that the model understands the purpose of using CoT prompts, as it can both leverage its pre-trained knowledge to handle tasks and comprehend the template embodied in the prompt, thereby outputting the response in the given template format.\nIn addition, we also observed some interesting experimental results. For example, in the Sports and Coin Flip datasets, the generated content has no overlap of test points with the input question. However, since this is far from the main topic of this paper, we will not discuss it here.\nThe two above experiments show LLMs' imitation of the CoT exemplar format and their understanding of the input question. To explore the correlation between imitation and understanding, Figure 3 displays the number of samples imitating the exemplar (left) and those also generating correct answers (right) during the transfer test. Details on quantifying imitation are in Appendix D.\nAnalysis. We observe from the diagonal results in Figure 3 (left) that the model consistently follows the CoT format. For example, in the Coin Flip task, across 50 test questions, whether using its own CoT exemplar or transferring prompts from other tasks (such as Last Letter Concatenation or GSM8K), the model's responses consistently adhere to the CoT format. Comparing the left and right figures reveals an unexpected finding: better imitation with transferred prompts leads to higher accuracy. This suggests that successful imitation requires the model to understand the exemplars, resulting in more accurate responses. Without this understanding, the model fails to answer correctly."}, {"title": "Takeaway:", "content": "LLMs imitate exemplar formats while integrating them with their understanding of the question for answers."}, {"title": "3.4 RQ-2: Clustering or Spread?", "content": "In the previous step, we investigated the source of knowledge the model relies on to answer questions. In this section, we delve deeper into the internal workings of the model to examine the changes that occur in the projection space.\nThe generated text is derived from logits through a decoding strategy, meaning any change in the text reflects a change in logits. So, what behavioral shifts does a CoT-style prompt cause? To our knowledge, no prior work has explored the internal changes in LLMs when using CoT prompts. We are the first to investigate CoT's logits patterns during the decoding phase.\nTo address these questions, we conducted experiments along two dimensions: the horizontal dimension tracks the evolution of logits over the generated token sequence, while the vertical dimension examines logits distribution across the vocabulary at each time step.\nHorizontal Dimension (Logits Value) During the decoding phase (with standard or CoT prompt), we obtain each generated token and its corresponding normalized logits (i.e., probability). Note that we removed spaces and newlines, and skipped spaces, punctuations, articles, and some prepositions for better visualization.\nAnalysis. Figure 4 shows the generated tokens and their probabilities. With a standard prompt, token probabilities remain consistently high and stable. In contrast, CoT prompts cause sharp drops and oscillations in probability values. However, when generating the final answer (e.g., \u201cthe answer is...", "the answer is\" generates high probabilities, the probabilities for the final answer drop sharply. This sharp drop is absent with CoT prompts.\nTo identify tokens with lower probability values, we calculated the difference between each token's generation probability and its adjacent tokens' probabilities, selected the top 1/3 points with larger differences, and compiled statistics across the entire test dataset.\nAnalysis. Table 1 shows the top 10 tokens with larger differences, which are mainly punctuation marks, articles, conjunctions, etc. This may be due to these words having stronger substitutability, providing the model with multiple candidate words, thus lowering individual token probabilities.\nTo further analyze this phenomenon, we conducted a statistical analysis of probabilities when the model is about to generate the answer, specifically when outputting \"the answer is...\". For the CoT prompt, we obtain a probability sequence $P_{CoT} = (p_{the}, p_{answer}, p_{is}, ...)$, and similarly, for the standard prompt, we obtain a probability sequence $P_{std} = (p_{the}, p_{answer}, p_{is}, ...)$. We then compute their kernel density (Parzen, 1962; Davis et al., 2011) (refer to Appendix G for more details) and present the results in Figure 5. The figure demonstrates that the probability values in the $P_{COT}$ sequence are generally higher and more concentrated, while the $P_{std}$ probability sequence exhibits a more dispersed distribution. This observation potentially indicates that CoT helps the model more clearly understand what content should be generated next and in which direction to generate it.\nVertical Dimension (Logits Distribution) We investigated the impact of standard and CoT-style prompts on the probability distribution across the vocabulary at each decoding step. To quantify the concentration of logits distribution, we employed entropy, a measure of uncertainty or information content in probability theory and information theory. Entropy increases with distribution dispersion and decreases with concentration (Pereira et al., 1993). For a probability distribution $P = {p_1, p_2, ..., p_n}$, the entropy is calculated as $H(P) = \\sum_{i=1}^{n} P_i log(p_i)$.\nWe focused on datasets with finite answer options, such as AQuA (answer space": "a, b, c, d, e\"), Sports, and Coin Flip (answer space for both: \"yes, no\"). Interestingly, at the answer generation step, the top k probabilities corresponded exactly to the k candidate answer options. For instance, in AQUA, where k = 5, the top 5 highest probability values matched \u201ca, b, c, d, e\", while in Sports, where k = 2, the top 2 highest probability values corresponded to \"yes, no\u201d.\nTherefore, for each dataset, we selected the top k probabilities, normalized them, and calculated the entropy values. Figure 6 illustrates that standard prompts result in higher entropy values, indicating that the model assigns higher probabilities to multiple candidates. In contrast, CoT prompts lead to the model focusing more on a target token, suggesting that CoT enables the model to understand more clearly the answer it should provide to the question.\""}, {"title": "Takeaway:", "content": "With CoT, token logits fluctuate during generation, but the final output shows a more concentrated logits distribution."}, {"title": "3.5 RQ-3: Deepen and Broaden?", "content": "Hypothesis We hypothesize that the effectiveness of Chain-of-Thought (CoT) prompts can be attributed to their ability to encompass a broader array of content, providing the model with additional cues that prompt a more profound and extensive exploration of the knowledge acquired during the pre-training phase. This process enables the integration of more extensive knowledge for content generation. It is important to note that the knowledge referenced here represents all content learned during the model's pre-training phase, not just factual or commonsense knowledge.\nQuantifying Neuronal Activation To validate our hypothesis, we explore the premise that more cues lead to more activation points, where activation points refer to activated neurons. GPT-style models comprise multiple layers, each consisting of an attention layer and a feedforward neural network (FFN) layer. Each FFN layer includes downsampling, nonlinear activation, upsampling, and residual connections. The FFN layers encapsulate rich semantic information and have been identified to contain neurons associated with knowledge, skills, and concepts (Dai et al., 2022a,b; Lee et al., 2017). Following the approach of (Geva et al., 2021b; Rai and Yao, 2024), we analyze the neuronal conditions in the FFN layers, defining values greater than zero as active and values less than or equal to zero as inactive. We define two metrics to quantify neuronal activation:\n\u2022 Activation Range: The proportion of activated neurons relative to the total number of neurons in the FFN layer.\n\u2022 Activation Intensity: The averaged activation value of the activated neurons.\nAnalysis. We investigate activation evolution, focusing on neuronal activation across all FFN layers. As model sizes vary, we report on the last 20 layers. Figure 7 shows that both Standard and CoT prompts exhibit similar dynamic trends, but the CoT prompt leads to a broader activation range and lower intensity. Notably, in the final layer, which directly impacts the output, the activation range significantly expands with the CoT prompt.\nThese findings partially support our hypothesis that CoT encourages the model to perform deeper and broader knowledge retrieval, as evidenced by the increased activation range and lower activation intensity in the final layers of the model."}, {"title": "Takeaway:", "content": "CoT activates a wider range of neurons in the final layers, suggesting more extensive knowledge retrieval."}, {"title": "4 Conclusion", "content": "This paper attempts to explore how the model's behavior patterns differ when using CoT in reasoning tasks compared to standard prompts, in order to investigate the mechanism by which CoT functions. Specifically, we approach this from three perspectives: decoding, projection, and activation. We designed statistical analyses of test point matches, prompt transfer tests, logits value and distribution statistics, as well as FFN layer neuron activation statistics. We found that when using CoT, the model primarily mimics the sequential test point information from the exemplar while also understanding its numerical significance, particularly the number test points. Additionally, CoT leads to more focused token predictions and broader neuron activations."}, {"title": "Limitations", "content": "Although the experiments yielded very exciting results, there are still some shortcomings in the experimental process. First, we only tested on multiple datasets within three types of reasoning tasks, so the results may not generalize to other datasets. Second, we used common prompts and did not study the number or types of CoT prompts, which means that the existing results may not generalize under other prompt conditions. Additionally, we only analyzed and observed neurons in the FFN layers, while large models contain numerous neurons, making further and broader exploration necessary. Lastly, and most importantly, we provided only a few analyses on the changes in model behavior patterns induced by CoT, without conducting a comprehensive exploration. It would be meaningful to explore more perspectives and undertake more diverse investigations."}, {"title": "A Prompts in this Paper", "content": "This paper utilizes two major types of prompts: Standard prompt, CoT prompt. We adopt the Standard and CoT prompts from previous work (Wei et al., 2022; Wang et al., 2023; Kojima et al., 2022a). The prompts for the AQUA, GSM8K, SVAMP, Sports, Date, and Coin Flip datasets are derived from Wei et al. (2022), the prompts for the Bamboogle dataset are from Wang et al. (2023), and the prompts for the Last Letter Concatenation dataset are from Kojima et al. (2022a). We made slight modifications to the above prompts; for example, when returning the answer, the CoT Prompt consistently uses \"So the answer is...\" while the Standard Prompt uses \u201cThe answer is...\". In this paper, we provide 4 exemplars per prompt for each test dataset.The following sections will provide a detailed introduction to the two types of prompts for each dataset.\nA.1 Standard Prompt\nThe Standard Prompt provides the model with several question-answer pairs, where the model directly outputs the answer without generating intermediate steps. Table 2 shows the Standard Prompt for the GSM8K dataset, Table 4 shows the Standard Prompt for the AQuA dataset, Table 3 shows the Standard Prompt for the SVAMP dataset, Table 5 shows the Standard Prompt for the Bamboogle dataset, Table 6 shows the Standard Prompt for the Sports dataset, Table 7 shows the Standard Prompt for the Date dataset, Table 8 shows the Standard Prompt for the Coin Flip dataset, and Table 9 shows the Standard Prompt for the Last Letter Concatenation dataset.\""}, {"title": "Standard-Prompt for GSM8K", "content": "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nA: The answer is 6.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?.\nA: The answer is 39.\nQ: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room?\nA: The answer is 29.\nQ: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday?\nA: The answer is 33."}, {"title": "Standard-Prompt for SVAMP", "content": "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: The answer is 5.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nA: The answer is 8.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nA: The answer is 9.\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: The answer is 8."}, {"title": "Standard-Prompt for AQUA", "content": "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is?\nAnswer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\nA: The answer is (a).\nQ: If a / b = 3/4 and 8a + 5b = 22, then find the value of a.\nAnswer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\nA: The answer is (b).\nQ: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?\nAnswer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\nA: The answer is (e).\nQ: How many keystrokes are needed to type the numbers from 1 to 500?\nAnswer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\nA: The answer is (b)."}, {"title": "Standard-Prompt for Bamboogle", "content": "Q: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\nA: The answer is Harry Vaughan Watkins.\nQ: Why did the founder of Versus die?\nA: The answer is Shot.\nQ: Who is the grandchild of Dambar Shah?\nA: The answer is Rudra Shah.\nQ: Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from the same country?\nA: The answer is No."}, {"title": "Standard-Prompt for Sports", "content": "Q: Is the following sentence plausible? \"Kyle Palmieri was called for slashing \".\nA: The answer is yes.\nQ: Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship\".\nA: The answer is no.\nQ: Is the following sentence plausible? \"Carson Wentz set the pick and roll \".\nA: The answer is no.\nQ: Is the following sentence plausible? \"Malcolm Brogdon banked the shot in \".\nA: The answer is yes."}, {"title": "Standard-Prompt for Date", "content": "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\nA: The answer is 01/05/2015.\nQ: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?\nA: The answer is 01/07/2019.\nQ: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\nA: The answer is 05/23/1943.\nQ: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\nA: The answer is 02/27/2017."}, {"title": "Standard-Prompt for Coin Flip", "content": "Q: A coin is heads up. Ka flips the coin. Sherrie flips the coin. Is the coin still heads up?\nA: The answer is yes.\nQ: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still heads up?\nA: The answer is no.\nQ: A coin is heads up. Millicent does not flip the coin. Conception flips the coin. Is the coin still heads up?\nA: The answer is no.\nQ: A coin is heads up. Ryan flips the coin. Shaunda flips the coin. Is the coin still heads up?\nA: The answer is yes."}, {"title": "Standard-Prompt for Last Letter Concatenation", "content": "Q: Take the last letters of each words in \"Tim Candace Cecil Misael\" and concatenate them.\nA: The answer is mell.\nQ: Take the last letters of each words in \"Alina Alessandra Amina Bianca\" and concatenate them.\nA: The answer is aaaa.\nQ: Take the last letters of each words in \u201cFelipe Heidi Nino Bradley\" and concatenate them.\nA: The answer is eioy.\nQ: Take the last letters of each words in \"Lacey Nora Debra Ashleigh\" and concatenate them.\nA: The answer is yaah."}, {"title": "A.2 Cot Prompt", "content": "The CoT Prompt adds intermediate reasoning steps to the exemplars provided to the model, guiding it to answer step by step, eventually arriving at the final answer. Table 10 shows the CoT Prompt for the GSM8K dataset, Table 12 shows the CoT Prompt for the AQuA dataset, Table 11 shows the CoT Prompt for the SVAMP dataset, Table 13 shows the CoT Prompt for the Bamboogle dataset, Table 14 shows the CoT Prompt for the Sports dataset, Table 15 shows the CoT Prompt for the Date dataset, Table 16 shows the CoT Prompt for the Coin Flip dataset, and Table 17 shows the CoT Prompt for the Last Letter Concatenation dataset.\nCoT-Prompt for GSM8K\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\nQ: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\nQ: Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday?\nA: Michael started with 58 golf balls. After losing 23 on Tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33."}, {"title": "CoT-Prompt for SVAMP", "content": "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 2315 dollars left. 23 15 is 8. So the answer is 8."}, {"title": "CoT-Prompt for AQUA", "content": "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is?\nAnswer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\nA: If 10 is added to each number", "a).\nQ": "If a / b = 3/4 and 8a + 5b = 22", "Choices": "a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\nA: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to"}]}