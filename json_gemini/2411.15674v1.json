{"title": "Quantile deep learning models for multi-step ahead time series prediction", "authors": ["Jimmy Cheung", "Smruthi Rangarajan", "Amelia Maddocks", "Xizhe Chen", "Rohtiash Chandra"], "abstract": "Uncertainty quantification is crucial in time series prediction, and quantile regression offers a valuable mechanism for uncertainty quantification which is useful for extreme value forecasting. Although deep learning models have been prominent in multi-step ahead prediction, the development and evaluation of quantile deep learning models have been limited. We present a novel quantile regression deep learning framework for multi-step time series prediction. In this way, we elevate the capabilities of deep learning models by incorporating quantile regression, thus providing a more nuanced understanding of predictive values. We provide an implementation of prominent deep learning models for multi-step ahead time series prediction and evaluate their performance under high volatility and extreme conditions. We include multivariate and univariate modelling, strategies and provide a comparison with conventional deep learning models from the literature. Our models are tested on two cryptocurrencies: Bitcoin and Ethereum, using daily close-price data and selected benchmark time series datasets. The results show that integrating a quantile loss function with deep learning provides additional predictions for selected quantiles without a loss in the prediction accuracy when compared to the literature. Our quantile model has the ability to handle volatility more effectively and provides additional information for decision-making and uncertainty quantification through the use of quantiles when compared to conventional deep learning models.", "sections": [{"title": "1. Introduction", "content": "In the realm of time series forecasting, uncertainty quan- tification is a critical component that allows for more in- formed decision-making, particularly in fields characterised by high volatility such as financial markets, energy demand, and weather forecasting. Conventional deep learning models, whilst powerful in multi-step ahead forecasting, often fall short in providing comprehensive measures of uncertainty. This gap can be addressed by integrating quantile regression, a statis- tical technique that offers a mechanism for extreme forecast- ing by predicting the conditional quantiles of a response vari- able. Koenker and Bassett [1] introduced the quantile regres- sion model in the mid-1970s to estimate conditional quantiles, offering a measure of uncertainty rather than single-point pre- dictions as in conventional linear regression models. Quantile regression has been widely used in statistical analysis [2] and finds applications in various fields, including epidemiology [3], economics [4, 5], ecology [6], and finance [7]. For instance, in the field of economics, it has been employed to study salary distributions influenced by returns to education and student ex- perience [8]. In medicine, quantile regression has been used to analyse the effects of different local anesthetics on the duration of nerve blocks [9]. Unlike traditional linear models such as least squares regression [10, 11], quantile regression provides more comprehensive information about the conditional distri- bution, revealing data characteristics across different quantiles as well as the average of the data. Hence, this approach offers a methodology for projecting uncertainties in prediction [12, 13]. Extreme value prediction [14] focuses on forecasting rare and significant events, these are often outliers or extreme val- ues in a dataset and have a low probability of occurrence but can cause major consequences [15]. In a meteorology context, an example is the rapid intensification of cyclones [16]. Given the distribution of a dataset, this statistical modeling approach targets the tail of the distribution where extreme events reside, allowing for the estimation of their probability. It finds applica- tions in various fields, including natural disasters [17, 18, 19], financial crises [20], and system failures [21]. Extreme value prediction is crucial for enhancing risk management as it pro- vides a foundation for developing emergency plans [22] and preventive measures [23]. For instance, it can be used to assess potential casualties in earthquake disasters [19], helping to min- imise losses. Additionally, whilst extreme value prediction tar- gets the rare, extreme events that in the tails of the distribution, quantile regression provides a more generalised approach to es- timate various quantiles, such as the median and 90th percentile of the response variable's conditional distribution [24]. How- ever, classic quantile regression can perform poorly for extreme values. When integrated with extreme value theory (EVT), extreme quantile regression can estimate conditional quantiles that extend beyond the observed data range [25]. Further lit- erature on extreme value prediction using a quantile function model are detailed by Cai et al. [24]. Deep learning models can handle complex, high-dimensional data and extract hidden patterns and features, making them par- ticularly effective for time series prediction, especially with nonlinear and multivariate data [26]. These models have been extensively used for time series forecasting, including univari- ate, multivariate, single-step, and multi-step predictions [27]. In the meteorological field, deep learning can be used to predict extreme weather events [28] such as smog [29], heavy rainfall [30], and declining groundwater levels [31]. By analysing his- torical meteorological data and satellite images, deep learning models can also identify early signals of extreme weather, en- abling advanced preparation to mitigate potential damage [28]. The combination of quantile regression with deep learning is gaining traction [32, 33]. Deep learning-based quantile re- gression has been applied to right-censored survival data [34], utilising the Huber check function and inverse probability cen- soring weights (IPCW) function to more accurately adjust for censoring. This has been validated through simulation studies and applications to breast cancer gene datasets [34]. Recent studies have utilised the quantile regression forests (QRF) model to predict road traffic volume, showing signifi- cant implications for regional development [35]. Furthermore, integrating quantile regression with deep learning models, such as the long short-term memory (LSTM) network has signifi- cantly improved the accuracy and reliability of river runoff pre- dictions [36]. The monotone quantile regression neural net- work (MQRNN) was employed by Hu et al. [37] to address the quantile crossing problem in time series prediction by taking the monotonicity of quantile into consideration. An improved quantile regression neural network (iQRNN) [38] was used for probabilistic load forecasting that utilised deep learning strate- gies such as batch training, early stopping, and dropout regu- larisation that significantly improved the training efficiency and prediction stability of the model. Recent advancements includ- ing MQRNN [39] and the deep partially linear quantile regres- sion neural network (DPLQR) model [40] have addressed quan- tile crossover, where different quantile estimation lines (e.g. 10% quantile, 50% quantile) may cross or stagger during the prediction process in the quantile regression, leading to in- consistency or irrationality in the prediction results, and con- structing confidence intervals for time series predictions. These models highlight the potential of combining deep learning with quantile regression for enhanced uncertainty quantification. The integration of deep learning and extreme value predic- tion has demonstrated significant application potential across various fields. Deep learning has been leveraged to predict ex- treme market fluctuations, aiding investors anticipate the risk of financial crises or market crashes [41]. The combination of extreme value theory (EVT) with neural networks has signif- icantly improved the accuracy of predicting extreme events in financial markets [42]. A hybrid model framework that com- bines EVT and machine learning [43] can more accurately es- timate stock market risks by processing multivariate and high- frequency data, thereby enhancing risk management and invest- ment decision-making accuracy. Furthermore, there is limited work in the area of quantile regression for multi-step ahead forecasting. In this study, we present a novel quantile regression deep learning framework for multi-step time series prediction. In this way, we elevate the capabilities of deep learning models by in- corporating quantile regression, thus providing a more nuanced understanding of predictive values. We evaluate the framework using univariate and multivariate benchmark datasets and focus on multi-step ahead time series predictions under conditions of high volatility and extremes that include cryptocurrency mar- ket, specifically Bitcoin and Ethereum datasets used by Wu et al. [44]. We evaluate the framework with two novel deep learn- ing models that include LSTM networks [45] and convolutional neural networks [46] which have been very promising for multi- step ahead forecasting [47]. We provide open-source Python code and data so that our framework can be extended and ap- plied to various fields that feature extreme values and require uncertainty quantification. The rest of the paper is organised as follows. In Section 2, we provide background and related work, and in Section 3, we present the methodology. Section 4 presents the results and Section 5 and 6 provide the discussion and conclusions, respec- tively."}, {"title": "2. Background and Related Work", "content": ""}, {"title": "2.1. Quantile regression", "content": "The quantile regression model is an extension of linear re- gression that estimates the conditional median (other quantiles) of the response variable using the conditional quantile function. For the T-th quantile (0 < \u03c4 < 1), the quantile model is:\n$Q(\u03c4.\u03a7) = \u03a7\u03b2(\u03c4)$\nwhere: $Q_y(T|X)$ represents the t-th quantile of the dependent variable y given the independent variables X. X is the vector of independent variables, containing n observations. \u03b2(\u03c4) is the vector of coefficients associated with the \u03c4-th quantile. Quan- tile regression estimates the conditional distribution of the de- pendent variables under the different quantiles. It is different from ordinary least squares regression (OLS) which is based on the conditional mean of the estimated dependent variable. Therefore, quantile regression can provide a more comprehen- sive understanding of the data and provide a means of uncer- tainty quantification in predictions. Through quantile regres- sion, we can obtain a more detailed description of the entire distribution of a given dataset by estimating different quantiles (such as the 10th, 50th, and 90th quantiles). Quantile regression is less sensitive to outliers because its es- timation is based on minimising the absolute error with a spe- cialised loss function, rather than the conventional squared er- ror loss in linear models. The quantile loss function is given as follows:\n$\u03c1_\u03c4(u) = u(\u03c4 \u2013 I(u<0))$\nwhere: u = y \u2013 \u03a7\u03b2(\u03c4) represents the residuals. $\u03c1_\u03c4(u)$ is the asymmetric absolute loss function, also known as the \"check loss function.\" I(.) is the indicator function, which takes the value 1 if the condition inside is true, otherwise 0. A simple linear regression example can be used to visualise the concept of quantiles, where regression lines for different"}, {"title": "2.2. Extreme value theory", "content": "Extreme value theory (EVT) [53, 54] forms the basis of ex- treme value prediction which focuses on modelling the prob- ability distribution of the tail of the data. It provides a set of methods and distribution models for modeling and analysing extreme events. The generalised extreme value distribution (GEV) [55] and the generalised Pareto distribution (GPD) [56] are two important tools to define and estimate models in EVT. The block maxima (BM) [57] method and the Peak Over Threshold (POT) [53] method are two common model parame- ter estimation methods for extreme value analysis. The application of the two main theorems of extreme value theory includes (i) the main limit theorem of EVT (proved by Fr\u00e9chet [58] in 1927 for the Pareto-type limit distribution and by Fisher and Tippet [59] in 1928 for the Weibull and Gumbel limit distributions) leading to the GEV distribution, and (ii) the Gnedenko-Pickands-Balkema-de Haan theorem [60] leading to the GPD distribution. In the case when the extreme value distribution of a random variable meets certain conditions, ex- ceedance over a high threshold can be approximated by the"}, {"title": "2.3. Multi-step time series prediction", "content": "Single-step prediction refers to a model prediction one step ahead in time, while multi-step time series is a task that aims to predict multiple time steps into the future [65]. This becomes increasingly complex with the number of forecast steps, par- ticularly with time series data. The strengths and weaknesses of different neural network architectures vary significantly for time series prediction [47]. Since time series prediction de- pends on temporal patterns, it's essential to carefully select the optimal neural network architecture and training method. The prediction errors can accumulate over time, especially when dealing with chaotic time series datasets. This implies that in order to produce precise results, the predictive capabilities and hyperparameters of different models need to be considered and customised to the given dataset. Chandra et al. [47] evalu- ated a variety of deep learning models, including simple recur- rent neural networks (RNN), long short-term memory networks (LSTM), bidirectional LSTM networks (BD-LSTM), encoder- decoder LSTM networks (ED-LSTM), and convolutional neu- ral networks (CNN). These models have then been compared on their performance on univariate time series datasets. They reported that bidirectional LSTM and encoder-decoder LSTM networks performed the best in terms of their prediction accu- racy which highlights the advantages of utilising deep learning models in handling multi-step ahead time series prediction. Chang et al. [66] used real-time recurrent learning for train- ing RNNs for flood forecasting, using an iterative approach for two-step-ahead forecasts. Additionally, Khedkar et al. [67] in- corporated EVT into deep learning models to address extreme flooding issues across Australia's major catchments. They utilised multivariate and multi-step time series prediction and reported that quantile-LSTM outperformed the baseline deep learning models while providing uncertainty estimates in hy- drological forecasting. In recent years, deep learning models have shown consid- erable potential in predicting cryptocurrency prices, which is an area characterised by high volatility and unpredictability."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Deep learning models", "content": "RNNs are neural networks designed to handle sequential data [73] which feature recurrent connections in the hidden layer to represent temporal data [74]. RNNs have been extensively utilised for time series forecasting [47]. LSTM network [75] is a variant of an RNN that addresses the problem of learning of long-term dependencies by conven- tional RNNs such as the Elman RNN [74]. LSTMs are par- ticularly effective for handling temporal data, since they can retain information over longer periods, outperforming conven- tional RNNs. LSTM models enhance traditional RNNs by in- corporating memory cells that feature multiple gates to manage information flow. There are 4 components in a LSTM memory cell (unit): the input gate, the forget gate, the output gate and the cell state. The interaction of these gates is the crucial part, in updating the cell state which aids in combating issues related to vanishing and exploding gradients [75] faced by conventional RNNs [73]. The bidirectional long short-term memory (BD-LSTM) is an advanced LSTM model that handles information in both for- ward and backward directions through two independent hidden layers, as shown in Figure 2. Unlike canonical LSTM models that process information in a single direction, each input se- quence is passed through the RNN twice; once in the forward direction and once in reverse [76]. This made them promi- nent for language modelling and natural language processing (NLP) tasks [77], and also for multi-step ahead time series fore- casting [47, 44]. The encoder-decoder long short-term mem- ory (ED-LSTM) model was designed to handle language mod- elling tasks [78] which is also effective for time series predic- tion due to its ability to capture complex temporal patterns and dependencies over long sequences. Although the convolutional LSTM (Conv-LSTM) network was initially used for weather forecasting problems [79], it is also capable of handling a wide range of time series-related data. Conv-LSTM can effectively harness both spatial and temporal dependencies in data by com- bining the strengths of CNNs [46] and LSTM networks. This capability makes Conv-LSTM particularly suited for tasks in- volving multivariate time series forecasting, such as predicting cryptocurrency prices. Therefore, we used these models for our quantile deep learning framework."}, {"title": "3.2. Quantile deep learning model", "content": "The key and unique feature of these model implementations is the use of the quantile loss function. For each defined model, there will be a 'classic' version with a standard loss function and another version utilising the quantile loss function. This approach allows us to evaluate which set of models performs better, offering more comprehensive predictions that account for the inherent volatility in cryptocurrency markets. The quantile loss function helps in making predictions that are more tailored to specific sections of the dataset. Instead of predicting only the average outcome, it allows for the predic- tion of a set of defined quantiles. Although the quantile loss function does not predict an exact value, we assume the median values for each time step (prediction horizon)to be the predicted values.\n$l_q(x, y) =\\begin{cases}q \\cdot (y - \\hat{y}) & \\text{if } y \\geq \\hat{y} \\\\(q-1) \\cdot (\\hat{y} - y) & \\text{if } y < \\hat{y}\\end{cases}$\nwhere, y is the true value, \u0177 is the predicted value and q is the specific quantile (e.g. q = 0.95). We can interpret that if y \u2265 \u0177, the actual value is greater than or equal to the predicted value. The loss is given as q times the difference between the true and predicted values. Therefore, for higher quantiles, the error is"}, {"title": "3.3. Framework", "content": "The framework presented in Figure 4 outlines the key com- ponents that include data processing and predictions using deep learning models. In Stage 1, we begin by extracting and pro- cessing the selected datasets and applying exploratory data analysis. We need to transform the original time series data into sequences that can be used for prediction. Stage 2 involves preparing the data for model training. In the case of deep learning models, we need to process the data depending on their nature, i.e. univariate and multivariate data for associated models as shown in our framework. This sliding window technique ensures that the model learns from a variety of overlapping sequences, capturing the temporal dependencies in the data. These sequences are then normalised and split into training and testing datasets, as done in previous work in the lit- erature [44]. The univariate time series is divided into overlap- ping windows, each window contains an input sequence vector"}, {"title": "3.4. Data", "content": "We demonstrate the effectiveness of quantile deep learn- ing models using Multivariate and Univariate time series datasets such as cryptocurrency using processed data taken from [44] (Bitcoin and Ethereum) and Sunspot, Mackey-Glass and Lorenz time series from Chandra et al. [47].\n1.  Bitcoin is a Multivariate dataset that contains daily entries of Bitcoin prices - high, price low, open and close prices - along with trade volume and market capitalisation. There are 2991 daily observations, dating from April 2013 to July 2021.\n2.  Ethereum is a Multivariate dataset that contains daily en- tries of Ethereum prices - high, price low, open and close"}, {"title": "3.5. Experiment setup", "content": "After developing the initial models, we considered several factors for selecting the appropriate hyperparameters for each model type. Since Bitcoin and Ethereum are highly volatile, training on continuous data would not adequately prepare the model for handling such fluctuations. Therefore, we created the training dataset using a split that was randomly selected, i.e. 80:20 ratio. The reason for the random train test split is to ensure the models account for data across all time periods. For instance, cryptocurrency data is especially volatile during the COVID-19 pandemic period, which falls only in the test dataset if the train test split wasn't implemented. Our goal is to ensure that the respective models have the ability to manage volatile data effectively. We kept the models consistent with previous work ([47]) and hence used the hyperparameters presented in Table 1. In the re- spective deep learning models, we use adaptive moment estima- tion (Adam) [84] optimiser for training with a learning rate of 0.0001. In the case of the cryptocurrency datasets (Bitcoin and Ethereum), we use 6 as the input window size with 5 outputs (5 prediction horizons) as done by Wu et al. [44]. In other real- world and simulated time series datasets, the input and output window sizes were adjusted to allow comparison with related work by Chandra et al. [47], where the input window is fixed at 5, and the output window at 10 (10 prediction horizons). Fur- thermore, the following needs to be taken into account along with information in Table 1. \u2022 The BD-LSTM model includes both the forward and back- ward LSTM layer. \u2022 ED-LSTM includes two LSTM networks with a time dis- tributed layer, in the Encoder and Decoder submodels. \u2022 The Conv-LSTM includes a 1D convolutional layer for the univariate time series and the 2D layer for the multivariate time series. In the convolutional layer, we use 64 filters with a kernel size of 2. It also utilises LSTM network and a dense layer. we use 64 filters with a kernel size of 2 We report the RMSE mean and 95% confidence intervals from the test dataset based on 30 independent experimental runs. We note that a lower RMSE indicates better model per- formance and high uncertainty is indicated by a high confidence interval. In the case of our quantile-based deep learning mod- els, we calculate the average RMSE across the number of time steps at each quantile (0.05, 0.25, 0.5, 0.75, and 0.95), with the mean representing the median value (0.5)."}, {"title": "4. Results", "content": "As outlined earlier, we develop quantile deep learning mod- els for time series prediction including BD-LSTM, ED-LSTM, and Conv-LSTM as the base models and their corresponding quantile versions (e.g. Quantile BD-LSTM)."}, {"title": "4.1. Cryptocurrency datasets", "content": "Table 2 presents the performance (RMSE) of univariate and multivariate linear regression and deep learning models (BD- LSTM, ED-LSTM, Conv-LSTM) for the Bitcoin test dataset. We highlight in bold the best performance for the respective prediction horizons. We observe that the quantile linear regres- sion accuracy (RMSE) is similar to linear regression (mean and prediction horizons given by the steps). This implies that quan- tile regression can effectively handle the volatility of cryptocur- rency data while providing predictions of the respective quan- tiles, which accounts for uncertainty quantification. An inter- esting observation can be seen for multivariate strategy, where there is a higher mean RMSE but a more condensed confidence interval for multivariate quantile linear model. Greaves et al. [85] demonstrated that neural networks are superior model classification than linear regression for Bitcoin price prediction; hence, we move on to deep learning models. Earlier, Wu et al. [44] showed that the BD-LSTM, ED-LSTM, and Conv-LSTM networks provided the best accuracy ranks in the univariate and multivariate strategies for a wider range of deep learning models. Across both univariate and multivariate strategies in Table 2, the ED-LSTM and quantile ED-LSTM models provide the highest prediction accuracy and consistently the ED-LSTM models outperform BD-LSTM and Conv-LSTM. Specifically, the Quantile-ED-LSTM model provides the best accuracy for all prediction horizons, except step one in the univariate and multivariate strategies. Additionally, in Figure 6 (b), (d), we can observe that ED-LSTM and Quantile-ED-LSTM both pro- vide consistent accuracy as the prediction horizon changes, thus being the most robust and stable model. We can also note that BD-LSTM and Conv-LSTM provide similar performance, but the Quantile-BD-LSTM model consistently provides higher ac- curacy than its counterpart (BD-LSTM) across both univariate and multivariate strategies. In Table 3, we can see the accuracy (mean RMSE) for each quantile, not only do quantile models often provide similar predictions, but they also provide further information (quantiles) for uncertainty quantification. In the Ethereum dataset, Table 5 presents the performance (RMSE) of linear regression and deep learning models for the test datasets, with the best performance highlighted in bold. We observe that the Univariate quantile models provide the best ac- curacy (RMSE) and the Multivariate quantile models provides the most robustness, as indicated by consistently low confi- dence intervals. Figure A.10 presents a visualisation of predic- tions for the respective models and quantiles for the Ethereum time series, where we observe that the quantiles well capture the actual data points. In Figures 7 (c) and (d), the Multivariate strategy shows that consistently the ED-LSTM models outper- form BD-LSTM and Conv-LSTM. In contrast to Bitcoin (Fig- ure 6, the classic ED-LSTM model provides the most accurate predictions overall for all time horizons, with the lowest RMSE value. Moreover, in Figure 7 (a) and (b), the univariate strategy shows that the ED-LSTM models also consistently outperform BD-LSTM and Conv-LSTM in prediction accuracy with the ex- ception of step one, and Quantile-BD-LSTM provides the best prediction accuracy. Furthermore, the Quantile-ED-LSTM is the most robust univariate model for predicting Bitcoin as the prediction horizon increases. We note that although BD-LSTM and Conv-LSTM present consistent results, as the number of prediction days increases, the forecast accuracy gradually de- creases. The Quantile-BD-LSTM and Quantile-Conv-LSTM models also consistently provide higher accuracy than their counterparts for the Univariate strategy. Finally, Conv-LSTM exhibits better performance than the Quantile-Conv-LSTM for multivariate strategies. Table 4 outlines multivariate and univariate strategies for the Ethereum dataset (test dataset mean RMSE across 5 time steps at different quantiles) for 30 independent model training runs. Since the median quantile is our prediction, it has the lowest RMSE compared to any other quantiles. This is logically con- sistent, as other quantiles cover more extreme prediction values. Additionally, it has a much smaller confidence interval which highlights that quantile models excel in reducing percentage- based errors, making them particularly effective in dealing with"}, {"title": "5. Discussion", "content": "This study explored quantile deep learning models for mul- tivariate and multi-step ahead time series prediction with uni- variate and multivariate models for selected cryptocurrency and time series prediction datasets. Table 7 presents a sum- mary of the results, highlighting that both the conventional ED- LSTM and Quantile-ED-LSTM models consistently deliver the strongest predictive performance across all datasets. We can also observe that for the cryptocurrency datasets, neither the BD-LSTM nor the Conv-LSTM (including their quantile vari- ants) show a clear performance hierarchy. This aligns with the RMSE results outlined in Tables 2 and 4, where all four models exhibit competitive accuracy (close performance). Addition- ally, the primary goal of this study is to enhance the represen- tation of uncertainty with the quantile loss function, rather than to improve the forecast accuracy of the conventional models. Therefore, we expected a similar performance of quantile mod- els relative to conventional deep learning models, and has been demonstrated across all datasets in Figures 6, 7 and 8. In both univariate and multivariate cases, the quantile mod- els performed similar to the conventional deep learning mod- els. In the case of Ethereum, the performance for the quan- tile models are slightly poorer than conventional models (Table 4). However, in the Bitcoin dataset (Table 2), the predictions remained similar to the univariate datasets, demonstrating that deep learning models have different predictive abilities depend- ing on the dataset. We can gather that the model with the most consistent and accurate predictions is the Quantile-ED-LSTM model. Furthermore, we can review the results of the other datasets. The best models in the cryptocurrency datasets do not automatically imply they are the best across all other datasets. We ran similar experiments on three other volatile datasets (i.e. sunspots, mackey-glass and lorenz). The BD-LSTM standard model proved to be particularly unreliable, with very large con- fidence intervals and RMSE values (Figure 8). The BD-LSTM quantile model grew in RMSE value across the time steps but has a small confidence interval, demonstrating the model's ro- bustness. Consistent with the cryptocurrency results, the ED- LSTM models outperformed the BD-LSTM models as seen in Table 6, where the ED-LSTM models have a higher mean rank than the BD-LSTM models across all datasets. We next review the multivariate results for the cryptocur- rency datasets (Tables 2 and 4) where the distinguishing fea- tures between each model and their quantile counterparts have more clarity and definition. The multivariate results remained consistent with the univariate parts, where the ED-LSTM out- performs both the Conv-LSTM and BD-LSTM models, we can see the contrast between the three models in Figures 6 and 7. Since ED-LSTM models process the entirety of the given historical data through the encoder and then make predictions through the decoder, they are able to take into account the en- tire history of the dataset and are hence able to make more valid predictions. BD-LSTM model also performs poorly in compar- ison to the ED-LSTM models as seen in Table 7, as they are not as capable in handling long term dependencies in the data, which is a key feature in volatile datasets. Our findings have better results than those reported in the related study [44], as their test mean for the ED-LSTM multi- variate Bitcoin data was 0.0373 compared to our quantile ED- LSTM result of 0.0112 as seen in Table 2. However, do note that the train test split ratio are different across the two papers where we used 80:20 in comparison to 70:30 by Wu et al [44]. The lower mean RMSE from our ED-LSTM quantile regression analysis particularly emphasises that the median (0.5 quantile) results yield superior predictive accuracy compared to tradi- tional methods. This enhancement in performance underscores the robustness of quantile regression for time series forecast- ing. Furthermore, quantile regression not only improves pre- diction accuracy but also offers a probabilistic interpretation by providing a spectrum of potential outcomes, thereby enriching the decision-making process with a more comprehensive risk assessment. In the literature, conventional deep learning mod-"}, {"title": "6. Conclusion", "content": "In this study, we investigated the combination of quantile re- gression in selected deep learning models for multi-step ahead time series prediction. Our results demonstrated that the com- bining quantile regression with deep learning models has been very effective, even in volatile environments such as the cryp- tocurrency markets. In the case of the cryptocurrency datasets,"}]}