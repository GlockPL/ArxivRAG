{"title": "MOTION-GROUNDED VIDEO REASONING: UNDERSTANDING AND PERCEIVING MOTION AT PIXEL LEVEL", "authors": ["Andong Deng", "Tongjia Chen", "Shoubin Yu", "Taojiannan Yang", "Lincoln Spencer", "Yapeng Tian", "Ajmal Saeed Mian", "Mohit Bansal", "Chen Chen"], "abstract": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GROUNDMORE, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MORA achieves respectable performance on GROUNDMORE outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation.", "sections": [{"title": "INTRODUCTION", "content": "Understanding motions (Aggarwal & Cai, 1999; Corona et al., 2020; Zhou et al., 2012; Tevet et al., 2022) in dynamic video scenes has long been an important topic in the computer vision community. It plays a crucial role in many vital real-world applications, such as scene/video understanding (Saleemi et al., 2010; Sturgess et al., 2009; Mottaghi et al., 2016; Tsai et al., 2011; Fan et al., 2018), autonomous driving (Chen et al., 2015; Singh et al., 2022; Leon & Gavrilescu, 2019; Hu et al., 2023), and human-computer interaction (Aggarwal & Park, 2004; Wren & Pentland, 1999; Schmidt, 2000). Existing motion understanding tasks (e.g., action recognition (Soomro et al., 2012; Carreira & Zisserman, 2017), temporal action localization (Caba Heilbron et al., 2015; Jiang et al., 2014), spatiotemporal action/object detection (Gkioxari & Malik, 2015; Gu et al., 2018; Li et al., 2021; Vu et al., 2018; Jiang et al., 2020), video object segmentation (Xu et al., 2018; Seo et al., 2020; Khoreva et al., 2019; Cheng et al., 2023b; Ding et al., 2023)) are designed to either comprehend spatial interactions or detect motions in temporal span.\nHowever, motion is a complex spatiotemporal concept involving interactions between visual entities over time. Understanding motion-related attributes abstracted from dynamic scenes is crucial for comprehensive motion understanding."}, {"title": "RELATED WORK", "content": "Motion Understanding in Videos. Motion understanding is pivotal in video analysis, serving as the basis for interpreting dynamic scenes and activities. Action recognition (Carreira & Zisserman, 2017; Soomro et al., 2012) identifies specific actions in videos, while temporal action localization (Caba Heilbron et al., 2015; Jiang et al., 2014) pinpoints the exact time intervals of these actions, requiring a thorough grasp of motion patterns over time. Spatiotemporal action detection (Gkioxari & Malik, 2015; Gu et al., 2018; Li et al., 2021) and video object detection (Vu et al., 2018; Jiang et al., 2020) predict object bounding boxes in both spatial and temporal domains. Video object segmentation (VOS) (Xu et al., 2018) and video tracking (Cheng et al., 2023b) capture moving objects in videos relying on objects appearance. To fully understand motion, it is crucial to comprehend its spatiotemporal contexts, including the involved objects and temporally adjacent information. In this paper, we introduce Motion-Grounded Video Reasoning, a new task that aims to reason based on the spatiotemporal context of motion and respond with video object masks.\nSpatiotemporal Video Grounding. Spatiotemporal video grounding involves leveraging temporal cues to localize, identify, and interpret objects based on natural language expressions. Existing pipelines either focus on enhancing visual/textual semantic understanding (Baradel et al., 2018; He & Ding, 2024; Khoreva et al., 2019; Miao et al., 2024; Lin et al., 2023; Li et al., 2023) or strengthening cross-modal interaction (Wu et al., 2023; Gu et al., 2024; Ding et al., 2022; Liu et al., 2021; Wu et al., 2022a;b; Miao et al., 2023). Action grounding (Regneri et al., 2013; Zeng et al., 2020) localizes actions indicated by the input descriptions, and referring VOS (Seo et al., 2020; Khoreva et al., 2019) aims to ground objects at pixel level based on object-related expressions and recent work MeViS (Ding et al., 2023) introduces more challenging motion expressions, demanding advanced motion understanding to segment moving objects. These advanced frameworks achieve outstanding performance in grounding objects of interest in both spatial and temporal dimensions, however, these works primarily focus on context-level understanding and cannot perform complex reasoning and motion context perceiving. Recent works (Lai et al., 2023; Huang et al., 2024; Munasinghe et al., 2023; Zhang et al., 2024a; Rasheed et al., 2024; Zhang et al., 2024b) connects reasoning abilities of LLMs to the grounding task. PG-Video-LLaVA (Munasinghe et al., 2023) is a video-LLM equipped with pixel-level grounding modules but struggles with implicit reasoning/referring. LITA (Huang et al., 2024) leverages LLM for 1-D video temporal span localization with text query. In this paper, we present a novel baseline model, MORA, that handles both complex spatiotemporal reasoning and grounding for the proposed Motion-Grounded Video Reasoning task.\nVideo Reasoning. Video reasoning (Wang et al., 2024a; Wu et al., 2021; Tapaswi et al., 2016; Jang et al., 2017; Yu et al., 2019; 2024; Wang et al., 2024b; Zhang et al., 2023) is an advanced domain in multimodal video understanding, enabling models to answer questions based on video by comprehen-"}, {"title": "GROUNDMORE FOR MOTION-GROUNDED VIDEO REASONING", "content": "3.1 MOTION-GROUNDED VIDEO REASONING\nTask Definition. We propose Motion-Grounded Video Reasoning as a comprehensive motion under-standing task. Basically, the input is a video clip $V \\in R^{t\\times h\\times w\\times 3}$ (t, w, h, 3 represent video length, width, height, and channel numbers, respectively), and a corresponding question Q that is related to a specific motion, the direct answer is an object in this video clip. To let the model understand when/where the motion occurs and generate a grounded response at the pixel level, we require binary object segmentation masks $M \\in R^{t' xhxw}$ ($t' < t$) related to the motion as the output.\nTask Challenges. The key challenges of the proposed Motion-Grounded Video Reasoning lie in the following: 1) motion-related reasoning ability towards questions and 2) pixel-level understanding ability of the target moving object in both spatial and temporal dimensions. Concretely, for the first point, the model needs to grasp the relationship between the target motion and its spatiotemporal context, for instance, in the video where \u201cthe girl fed the dog with a piece of dog food after taking the dog food out from the cabinet"}, {"title": "\u0391\u039d\u039dOTATION PIPELINE", "content": "We recruited a team of 15 computer science students with experience in video understanding as our paid annotators to ensure high-quality annotations, 10 of them were assigned to question annotation and the rest focused on mask. For ease of the annotation, we design a 2-stage annotation pipeline for our question annotation: 1) motion-related expression annotation; 2) LLM-assisted QA generation.\nQuestion Annotation Stage 1: Motion-related expression annotation. Formally, interaction-causal expressions are with the following format: <obj_A, motion, obj_B, to do something>. Such expression could reveal the motivation behind a specific motion. Interaction-temporal expressions enable the analysis between temporally adjacent motions, which follows the format: <obj_A, motion, obj_B, before/after another motion>. In this setting, we want the model to understand motion in a temporal context and the question generated from this expression could assess the temporal awareness of the models. Moreover, we also have descriptive expression, which includes general dynamic scene descriptions and motion-related attributes that are abstracted from specific motions. The second descriptive expression could be much more challenging since it did not mention any motions here but requires detailed cross-modal and commonsense reasoning.\nQuestion Annotation Stage 2: LLM-assisted QA generation. We define 4 types of questions in our GROUNDMORE dataset: Causal questions are generated from interaction-causal expressions, which challenge models to understand the complex relationship within interactions based on some motivations behind them. Sequential and Counterfactual questions are both generated from interaction-temporal expressions. The former investigates the chronological relations between different motions and the latter requires outstanding reasoning ability to imagine situations where it conflicts with reality. Descriptive questions are converted from descriptive questions. It assesses the ability to understand general scenes and use visual commonsense reasoning. Several QA examples are shown in Figure 2 and the detailed question type statistics can be found in Appendix A.1.\nBefore question generation, we ask our annotators to additionally annotate an index for each object related to the potential answer in our expressions in order to point out what to target in each question for the LLM we use. Basically, we leverage the strong text generation ability of GPT-4 for our question generation. We carefully design a prompt in an in-context manner (details in Appendix A.2) that requires GPT-4 to generate a question and the corresponding answer based on the expression and the target objects. The annotators manually check all of the QAs to ensure the quality."}, {"title": "DATASET STATISTICS", "content": "We compare our GROUNDMORE with existing popular RVOS datasets Ref-YouTube-VOS (Seo et al., 2020), Ref-Davis17 (Khoreva et al., 2019), and the recent MeViS (Ding et al., 2023). Our GROUND-MORE contains 1,715 videos 7,577 questions and 249K object masks as well as 3,942 objects. And the average video clip duration is 9.61 seconds. GROUNDMORE is split into 1,333 training and 382 test videos. The dataset is now available at https://huggingface.co/datasets/groundmore/GroundMoRe.\nAs shown in Figure 3a, most of the clips have a duration between 5s and 15s, which is long enough to include sufficient motion semantics. This range ensures that the clips capture complete actions and interactions, providing a rich context for question formulation. In Figure 3b, it is evident that most motions in GROUNDMORE have a duration from 2s to 6s, highlighting the challenge of temporal localization in our dataset. These short-duration motions require precise temporal understanding and segmentation, adding to the complexity of the GROUNDMORE. Besides, the average motion (segment) ratio in each video clip is 51%. As seen in Figure 3c, for most clips, the number of questions is more than 2, with a significant number having up to 4 or more questions. This indicates that GROUNDMORE provides a diverse set of questions per clip, ensuring a comprehensive evaluation of the clip's content. It also implies that each clip contains multiple distinct motion semantics that warrants varied questioning. In Figure 3d, the distribution shows that most questions are sufficiently long, typically ranging from 7 to 15 words. This length reflects the complexity and detail required in the questions, underscoring the difficulty level of our GROUNDMORE. The substantial word count in"}, {"title": "EXPERIMENTS", "content": "In this section, we first list popular image/video grounding frameworks (Sec. 4.1). Then we introduce our proposed baseline Motion-Grounded Video Reasoning Assistant (MORA) (Sec. 4.2). Next, we provide detailed evaluation results and analysis in terms of reasoning ability, temporal context, and the localization branch (Sec. 4.3).\n4.1 BASELINE MODELS FOR EVALUATION\nWe choose baselines including 1) Referring VOS Models: ReferFormer (Wu et al., 2022b), SgMg (Miao et al., 2023), HTR (Miao et al., 2024), and LMPM (Ding et al., 2023), that are pure visual segmentation models and without LLMs. 2) Image Reasoning Segmentation Models: LISA (Lai et al., 2023) and PixelLM (Zhongwei et al., 2023) that have strong LLM and are equipped with extra spatial grounding heads. We adapt them to videos in a frame-by-frame manner. 3) Video Reasoning Segmentation Models: PG-Video-LLaVA (Munasinghe et al., 2023) that is build upon video-LLM (Maaz et al., 2023) and strong grounding modules (Kirillov et al., 2023b; Liu et al., 2023b; Cheng et al., 2023a), and VISA Yan et al. (2024) that is constructed based on LLaMA-VID Li et al. (2025) and SAM Kirillov et al. (2023a). Since our task could be solved in a non-end-to-end, two-stage manner (answering first, segmentation next), we also evaluate 4) Two-stage Baselines that are composed by strong general video-language model (ViLA (Lin et al., 2024) and VideoChat2 (Li et al., 2024)) / video QA models (SeViLA (Yu et al., 2023a)) and Referring VOS models.\n4.2 OUR METHOD: MOTION-GROUNDED VIDEO REASONING ASSISTANT\nOur Motion-Grounded Video Reasoning Assistant (MORA) is built upon LISA (Lai et al., 2023), which is an image-based reasoning segmentation framework, equipping the strong LLaVA (Liu et al., 2023a) and SAM (Kirillov et al., 2023b). To perform an efficient frame encoding, we take advantage of the spatiotemporal pooling mechanism in Video-ChatGPT (Maaz et al., 2023). We leverage the segmentation token [SEG] in LISA for spatial segmentation. However, one of the most challenging points in our task is that we need not only to segment the objects in the spatial dimension but also to localize them temporally. Therefore, as shown in Figure 4, to construct a unified LLM-based"}, {"title": "EVALUATION AND ANALYSIS", "content": "Metrics. Following prior works (Khoreva et al., 2019; Seo et al., 2020; Ding et al., 2023), we use the popular metrics: Jaccard index ($I$) (Jaccard, 1912) and F-measure ($F$) (Dice, 1945). $I$ estimates the IoU of the predicted and the GT masks, F indicates contour accuracy. We also report J&F to reflect overall performance. We evaluate models on GROUNDMORE across question types, revealing their grounding and reasoning ability from different aspects.\nBaseline Comparisons. As shown in Table 3, we first replace the questions with the titles of the corresponding YouTube videos and run as an RVOS task with noisy text labels using ReferFormer (Wu et al., 2022b) as the random baseline. Compared with the random baselines, RVOS models achieve reasonable improvements, especially LMPM (Ding et al., 2023), which is also trained by MeViS (Ding et al., 2023) data that contains more motion-related data than simple referring VOS datasets (Seo et al., 2020; Khoreva et al., 2019). Surprisingly, image reasoning segmentation baselines (Lai et al., 2023; Zhongwei et al., 2023), with strong LLM, are lower than RVOS models. The reason could be the lack of temporal modeling in those image-level models, which makes it hard to propagate target object information across frames. For PG-Video-LLaVA (Munasinghe et al., 2023), though it is a video reasoning segmentation/grounding model, the performance is not even higher than the best RVOS model. A potential reason could be that it tends to ground all salient objects given the scene description due to the redundant response of its video LLM (Maaz et al., 2023), resulting in more false positives. Besides, VISA Yan et al. (2024), the latest model for video reasoning segmentation, does not perform well on our benchmark. A likely reason for this is that its frame sampling strategy fails to effectively target keyframes, as the ground truth occupies only part of the temporal span, which results in terrible error accumulations in their mask propagation process. In contrast, two-stage baselines (Yu et al., 2023a; Lin et al., 2024; Li et al., 2024) show generally stronger results on GROUNDMORE, particularly SeViLA Yu et al. (2023a), likely due to their enhanced reasoning capabilities, which yield more accurate object responses. Details of the video LLMs in the two-stage baselines can be found in Appendix A.6. For different question types, we can also observe that in Causal and Descriptive questions, two-stage baselines built upon ViLA and SeViLA perform better than MORA, we hypothesize that ViLA and SeViLA maintain their strong reasoning ability in these two types of questions when not trained with an additional grounding module; while in the temporal-related questions (i.e., Sequential and Counterfactual), the temporal head in our MORA makes a difference.\nConclusively, our MORA achieves new state-of-the-art, outperforming the best existing video reasoning grounding model (PG-Video-LLaVA) by an average of 11.28. The reasons could be two-fold: (1) the language model in PG-Video-LLaVA provides ambiguous response for its grounding modules while the [SEG] token in MORA is trained in an end-to-end manner, conveying more informative features of target objects; (2) PG-Video-LLaVA, as well as other baselines, does not include any temporal localization design while the [LOC] in MORA, supervised by the timestamps of the motion, could lead to accurate temporal estimation.\nHowever, the design of our MORA is still basic and there is substantial room for future improvements in both model training and model design. For instance, the LLaVA could be replaced with better LLMs which are trained with more motion-sensitive language corpus to enhance visual-language alignment in dynamic scenes; the spatiotemporal pooling, though efficient, could inevitably cause information loss; and better time-sensitive modeling could also replace the simple temporal localization head.\nDataset Diagnosis. In order to showcase that our GROUNDMORE indeed introduces challenges mentioned in Sec. 3.1, we diagnose GROUNDMORE from two aspects, implicit reasoning and"}, {"title": "CONCLUSION", "content": "In this paper, we propose a new video task called Motion-Grounded Video Reasoning for comprehen-sive motion understanding. We consider motion as a combination of its spatiotemporal contexts and"}]}