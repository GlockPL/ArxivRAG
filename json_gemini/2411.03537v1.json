{"title": "TWO-STAGE PRETRAINING FOR MOLECULAR PROPERTY PREDICTION IN THE WILD", "authors": ["Kevin Tirta Wijaya", "Minghao Guo", "Michael Sun", "Hans-Peter Seidel", "Wojciech Matusik", "Vahid Babaei"], "abstract": "Accurate property prediction is crucial for accelerating the discovery of new molecules. Although deep learning models have achieved remarkable success, their performance often relies on large amounts of labeled data that are expensive and time-consuming to obtain. Thus, there is a growing need for models that can perform well with limited experimentally-validated data. In this work, we introduce MoleVers, a versatile pretrained model designed for various types of molecular property prediction in the wild, i.e., where experimentally-validated molecular property labels are scarce. MoleVers adopts a two-stage pretraining strategy. In the first stage, the model learns molecular representations from large unlabeled datasets via masked atom prediction and dynamic denoising, a novel task enabled by a new branching encoder architecture. In the second stage, MoleVers is further pretrained using auxiliary labels obtained with inexpensive computational methods, enabling supervised learning without the need for costly experimental data. This two-stage framework allows MoleVers to learn representations that generalize effectively across various downstream datasets. We evaluate MoleVers on a new benchmark comprising 22 molecular datasets with diverse types of properties, the majority of which contain 50 or fewer training labels reflecting real-world conditions. MoleVers achieves state-of-the-art results on 20 out of the 22 datasets, and ranks second among the remaining two, highlighting its ability to bridge the gap between data-hungry models and real-world conditions where practically-useful labels are scarce.", "sections": [{"title": "1 INTRODUCTION", "content": "A reliable molecular property prediction model enables researchers to efficiently screen vast numbers of potential compounds, reducing the need for costly experimental validation to only a select few. To this end, deep learning-based approaches have demonstrated remarkable accuracy in predicting a range of molecular properties including electronic, physical, and bioactivity properties (Yang et al., 2019; Rong et al., 2020; Fang et al., 2022; Zhou et al., 2023). However, these models typically rely on large datasets with hundreds of thousands of labeled data to achieve strong predictive performance. Yet, in real-world scenarios, labeled molecular data is often limited. For example, of the 1,644,390 assays in the ChemBL database (Zdrazil et al., 2024), only 6,113 (0.37%) contain 100 or more labeled molecules. This raises doubts about whether existing models are suitable for molecular property prediction in the wild, i.e., in real-world scenarios where experimentally-validated data is scarce.\nIn this work, we introduce MoleVers, a versatile pretrained model designed for molecular property prediction in data-scarce scenarios. MoleVers is pretrained in two stages to maximize its generalizability to various types of downstream properties. In the first stage of pretraining, we employ masked atom prediction (MAP) and dynamic denoising with relatively large noise scales to improve the generalizability of the learned representations. While previous studies have shown that increas-"}, {"title": "2 RELATED WORKS", "content": "Deep learning-based molecular property prediction has demonstrated remarkable successes. Early approaches use graph neural networks (GNNs) to learn molecular representations directly from molecular structures (Kipf & Welling, 2017; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018). GNNs typically learn molecular representations by updating the node (atom) and edge (bond) features through a series of message passing accross neighboring atoms. Recently, popular property prediction benchmarks such as MoleculeNet (Wu et al., 2018) are dominated by transformer-based models (Luo et al., 2022; Zhou et al., 2023; Yang et al., 2024) that leverage self-attention mechanisms to learn long-range interactions between atoms in a molecule.\nParallel to architectural advancements, pretraining has emerged as an effective strategy to improve property prediction peformance when labeled data is limited. By pretraining on a large, unlabeled dataset, a model can learn robust and transferable molecular representations that generalize well to a variety of downstream tasks. Various pretraining strategies have been proposed, including masked predictions (Wang et al., 2019; Xia et al., 2023; Zhou et al., 2023; Yang et al., 2024) and contrastive learning (Liu et al., 2022; Xia et al., 2023; Wang et al., 2022). Additionally, denoising atom coordinates and pairwise distance (Zaidi et al., 2023; Zhou et al., 2023; Liu et al., 2023) have been shown to lead to strong downstream performance. Denoising pretraining is equivalent to learning an approximate molecular force field (Zaidi et al., 2023; Liu et al., 2023), which could explain its effectiveness for improving downstream property prediction performance."}, {"title": "3 TWO-STAGE PRETRAINING", "content": "Our primary objective is to obtain an accurate molecular property prediction model without the need for additional, difficult-to-acquire labels for downstream tasks. To address this challenge, we propose a two-stage pretraining framework specifically designed to improve the generalization capability of our model, MoleVers. This approach enables accurate property prediction while minimizing the need for downstream labels during finetuning."}, {"title": "3.1 STAGE 1: MASKED ATOM PREDICTION AND DYNAMIC DENOISING", "content": "The properties of a molecule are strongly influenced by the spatial arrangement of its atoms in the three-dimensional (3D) space. Consequently, self-supervised pretraining that involves both atom types and 3D structures is crucial for achieving strong performance in the downstream datasets. In the first stage of our pretraining framework, we employ masked atom prediction (MAP) and dynamic denoising to train MoleVers on a large, unlabeled dataset. This encourages the model to learn representations that are transferable to downstream datasets."}, {"title": "3.1.1 MASKED ATOM PREDICTION", "content": "Inspired by masked token prediction in natural language processing (NLP) (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), masked atom prediction (MAP) involves training a model to predict the correct atom types in a partially-masked molecule. This encourages the model to learn contextual relationship between atom types, capturing how they co-exist in various molecules. Multiple works (Zhou et al., 2023; Xia et al., 2023; Yang et al., 2024) have demonstrated the effectiveness of MAP as a pretraining task, which ultimately leads to better prediction models for the downstream datasets."}, {"title": "3.1.2 DYNAMIC DENOISING", "content": "To learn information from 3D structures, we employ coordinate and pairwise distance denois-ing. Zaidi et al. (2023) and Liu et al. (2023) have shown that denoising tasks are equivalent\nto learning a molecular force field that is approximated with a mixture of Gaussians, $p(m) \\approx\nq_\\theta(m) := \\sum_{i=1}^N q_\\theta(m|m_i)$, where $p(m)$ is the force field, $q_\\theta(m|m_i) = \\mathcal{N}(m; m_i, \\sigma^2)$, and\n$m_1, m_2, ..., m_N$ are the equilibrium molecules in the pretraining dataset $\\mathcal{D}_{train}$.\nWe hypothesize that using a dynamic noise scale with larger values, e.g., drawn from a uniform\ndistribution $\\sigma\\sim \\mathcal{U}(a,b)$, where $b > 1$, could improve the generalization ability of the model.\nIncreasing $b$ broadens each Gaussian distribution, allowing the learned force field to better cover\nmolecules not seen in $\\mathcal{D}_{train}$. Alternatively, dynamic denoising can be viewed as an augmentation\ntechnique. This follows the simple intuition that a larger $\\sigma$ exposes the model to a wider set of\nnon-equilibrium molecular configuration in a similar way to how diffusion models are trained (Ho\net al., 2020; Song et al., 2021)."}, {"title": "3.1.3 DECOUPLING MAP FROM DENOISING", "content": "Unfortunately, previous works have found that setting $\\sigma$ to larger values often reduces the pretrain-ing quality and downstream performance (Zhou et al., 2023; Yang et al., 2024). This phenomenon\ncan be explained by comparing the complexities of the MAP and denoising tasks during pretraining.\nIn MAP, the model learns to map masked atoms ($A_{mask}$) to their corresponding atom logits ($A$), $f(A_{mask}) = A$, while in coordinate denoising, it learns to map noisy coordinates to their pristine\nvalues, $g(\\hat{P}) = P$. The MAP function is relatively simpler because it maps a finite set of inputs\n(atom types) to a relatively compact set of outputs (softmax-normalized logits). In contrast, denois-\ning deals with continuous input and output coordinates, making it more complex as the number of\npossible mappings is much larger. When a single model handles both MAP and denoising, the over-\nall complexity is dominated by the more challenging denoising task. The downstream performance\ncould then be negatively affected if the model struggles to accurately fit the complex denoising\nfunction.\nThis motivates us to introduce a branching encoder architecture, shown in Figure 1, that decouples\nthe MAP and denoising pipelines. The branching design ensures that the complexity of the MAP\ntask is minimally affected by the denoising. Furthermore, we propose to connect the two encoders\nwith an aggregator module so that information can flow between the two pipelines."}, {"title": "3.1.4 BRANCHING ENCODER", "content": "Inspired by prior works in NLP that have found masked prediction to often be the most effective\npretraining tasks (Lewis et al., 2020; Raffel et al., 2020), we set the MAP encoder as the primary en-\ncoder of the model. The primary encoder, shown in Figure 1, will be passed to the second pretraining\nstage (Figure 2 and used for predictions in the downstream datasets."}, {"title": "3.2 STAGE 2: AUXILIARY PROPERTY PREDICTION", "content": "We further improve the generalization capability of the primary encoder by incorporating auxiliary\nproperty prediction in the second pretraining stage. This approach is inspired by multitask learning\n(Caruana, 1997), where a model is trained to solve both the primary task and related auxiliary tasks at\nthe same time. For example, in facial analysis, the primary task might be to predict facial landmarks,\nwhile the auxiliary tasks could be to estimate head poses and infer facial attributes Zhang et al.\n(2014). Since these tasks share common features, the model can use the training signals from the\nauxiliary tasks to improve its performance in the primary task.\nGiven that molecular properties are heavily influenced by molecular structure, it is reasonable to\nassume that representations useful for predicting one type of property could also help in predicting\nothers. Based on this intuition, we propose to construct an auxiliary dataset of properties that can\nbe computed using relatively inexpensive computational methods, but are not necessarily identical\nto the properties in the downstream datasets. Specifically, we select HOMO, LUMO, and Dipole\nMoment as the auxiliary properties because they can be accurately computed using Density Func-\ntional Theory (DFT). We also note that computing the auxiliary labels with the DFT is cheaper than\nobtaining more downstream labels via real-world experiments.\nIn this second pretraining stage, the model is trained in a supervised manner,\n$F = \\phi_\\theta^p(X,D), ~~~ (\\hat{y}_{homo}, \\hat{y}_{lumo}, \\hat{y}_{dipole}) = \\psi_\\theta^a(F)$,\nwhere $\\psi_\\theta^a$ is the auxiliary predictor and $\\hat{y}_{homo}, \\hat{y}_{lumo}, \\hat{y}_{dipole}$ are the predicted auxiliary properties.\nAfterward, we append the primary predictor for the downstream property to the primary encoder\nand finetune the model using the downstream dataset, as illustrated in Figure 2."}, {"title": "4 MOLECULAR PROPERTY PREDICTION IN THE WILD BENCHMARK", "content": "The majority of existing molecular property prediction benchmarks rely on datasets with large num-\nbers of data points, which do not reflect real-world scenarios where such large datasets are rare.\nFor instance, out of 1,644,390 assays available in the ChemBL database, only 6,113 assays (0.37%)\ncontain 100 or more molecules, demonstrating the scarcity of molecular data in the wild where the\nmolecular properties are validated through real-world experiments. As a result, molecular property\nprediction models that perform well on existing benchmark may struggle to maintain the same level\nof performance in real-world applications where labeled data is limited.\nTo address this issue, we introduce Molecular Property Prediction in the Wild (MPPW), a new\nbenchmark specifically designed for property prediction in low-data regimes. Unlike existing bench-\nmarks that often assume the availability of large and labeled datasets, the majority of datasets in\nthe MPPW benchmark contain 50 or fewer training samples. This reflects the challenge faced by\nmolecular property prediction models in the wild. Specifically, we have curated 22 assays from\nthe ChemBL database (Zdrazil et al., 2024) that encompass a diverse set of properties that includes\nphysical properties, toxicity, and biological activity. A detailed description of the datasets, including\ntheir soruces, can be found in Appendix A.1."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "In this section, we address the following questions through a series of experiments: (1) Does the\ntwo-stage pretraining framework improve the downstream performance on datasets with limited\nlabels? (2) How does each individual pretraining stage contribute to the improvements? (3) Is our\nassumpation that larger noise scales improve the generalization capability of the model correct? (4)"}, {"title": "5.1 EXPERIMENT SETTINGS", "content": "We use GDB17 (Ruddigkeit et al., 2012) as the pretraining dataset for our model and other models\nto which we compare. We randomly select 1M unlabeled molecules from the 50M subset to be used\nin the first pretraining stage. We then sample 130K molecules out of the 1M subset to construct\nthe auxiliary datasets for the second pretraining stage. The labels for the auxiliary dataset are com-\nputed with Psi4 (Smith et al., 2020). We use RDKit to generate 3D conformations from SMILES\n(Weininger, 1988) for models that take 3D graphs as inputs.\nFor benchmarking purposes, we use the same pretraining dataset to minimize any performance gains\nthat might arise from the use of higher-quality pretraining datasets. Additionally, we use identical\ndata splits for all pretraining methods to ensure fair and consistent comparisons."}, {"title": "5.2 IMPLEMENTATION DETAILS", "content": "The primary and auxiliary encoders of MoleVers are built on the UniMol encoder architecture (Zhou\net al., 2023). Each encoder comprises 15 layers, with an embedding dimension of 512 and a feed-\nforward dimension of 2048. During the first pretraining stage, the model is trained for 1 million\niterations using a batch size of 32, with a masking ratio of 0.15 for the MAP task. In the second\npretraining stage, the model is trained for 50 epoch, maintaining the same batch size of 32. We\nemploy the Adam optimizer with a learning rate of $10^{-4}$ and utilize a polynomial decay learning rate\nscheduler. We run all experiments on an NVIDIA Quadro RTX 8000 GPU."}, {"title": "5.3 RESULTS ON THE MPPW BENCHMARK", "content": "In the MPPW benchmark, we compare MoleVers with four baselines: state-of-the-art GNNs, Graph-MVP (Liu et al., 2022) and Mole-BERT (Xia et al., 2023), as well as state-of-the-art transformers, Uni-Mol (Zhou et al., 2023) and Mol-AE (Yang et al., 2024). All models are implemented in Py-Torch (Paszke et al., 2019) and trained from scratch using publicly available source code. We also provide comparisons with more baselines on large downstream datasets in Section 5.7.\nFor each downstream dataset, we construct three distinct train/test splits with a 1:1 train-test ratio. All models are finetuned for 50 epochs on the training splits, and the downstream performance of the last epoch is recorded in Table 1. We evaluate the downstream performance using two metrics: mean absolute error (MAE) and the coefficient of determination (R2), which indicate how well the model can explain the variance in the data.\nAs shown in Table 1, the R2 scores for the current state-of-the-art models are relatively low. This indicates that existing models could not consistently learn molecular representations that are useful for property prediction. This highlights the need for more effective pretraining methods suited to low-data regimes. In contrast, MoleVers outperforms other baseline models in 20 out of the 22 as-says, and achieving a close second rank in the remaining two. Notably, no other method consistently ranks among the top two across all assays. These results demonstrate that the two-stage pretraining framework is an effective approach for improving downstream performance when labeled data is extremely limited.\nOne might argue that the performance gains shown in Table 1 are simply due to the additional labels, as other methods are pretrained without auxiliary labels. To address this concern, we have conducted further experiments, detailed in Table 8 in the Appendix, where other models are also pretrained with the auxiliary labels. In such a setup too, our model achieves state-of-the-art MAE in 20 out of 22 assays, while placing a close second in the remaining two. This demonstrates that the gains achieved by MoleVers are not solely because of the additional data, but rather stem from the synergy between the two pretraining stages. Additionally, we emphasize that the second pretraining stage offers a cost-effective solution for improving downstream performance, as the computational cost of obtaining auxiliary labels is negligible compared to the costs of acquiring downstream labels through wet-lab experiments."}, {"title": "5.4 ABLATION OF PRETRAINING STAGES", "content": "We study the influence of each pretraining stage on the downstream performance of MoleVers through a series of ablation studies. As shown in Table 2, incorporating either the first or sec-ond pretraining stage into the pipeline always leads to better downstream performance compared with directly training the model on the downstream datasets. Interestingly, the improvements vary across assays: some benefit more from the first pretraining stage, while others see more gains from the second pretraining stage. This variation could be due to the auxiliary properties we have chosen-HOMO, LUMO, and Dipole Moment-which are more related to intrinsic molecular properties (e.g., assay 1), rather than complex interactions (e.g., assay 3). Overall, the combination of both pretraining stages consistently yields the best downstream performance across all assays."}, {"title": "5.5 IMPACT OF NOISE SCALE ON DOWNSTREAM PERFORMANCE", "content": "In Section 3.1.2, we hypothesized that using larger noise scales for the denoising tasks can improve the downstream performance. In Table 3, we show the downstream performance of MoleVers with various noise scales drawn from a uniform distribution, $\\sigma \\sim \\mathcal{U}(0, b)$, where $b$ is the maximum noise scale. Note that, similar to what has been observed in a prior work (Yang et al., 2024), the pretraining become unstable when excessively larger noise scales, e.g., $b=20$, are used. Therefore, we limit our ablation study to a maximum value of 10.\nWe can see from Table 3 that, as the maximum noise scale increases, we observe consistent improve-ments in performance. The results confirm our hypothesis that larger noise scales could improve the downstream performance if implemented carefully. This also highlights the importance of the pro-posed branching encoder, which facilitates denoising pretraining with larger noise scales."}, {"title": "5.6 IMPACT OF PRETRAINING DATASET QUALITY ON DOWNSTREAM PERFORMANCE", "content": "In Section 4, we hypothesized that much of the performance gains observed in previous works may stem more from the quality of the pretraining datasets than from the pretraining method itself. Therefore, it is important to fix the pretraining dataset used in a benchmark. To test this, we examine two factors: the size of the pretraining dataset and its molecular diversity. Intuitively, a larger and more diverse set of pretraining molecules should lead to a better pretrained model compared to smaller pretraining datasets with less variation.\nTable 4 shows the downstream performance of MoleVers when pretrained on datasets of varying sizes in the first stage. We observe a general trend of improved downstream performance as the pretraining dataset size increases. One exception occurs in Assay 2, where the model pretrained on 100K samples outperforms the one pretrained on 1M samples. However, the R\u00b2 difference be-tween these two models is relatively small compared to other assays, therefore, the overall trend remains valid. Furthermore, we investigate the impact of pretraining dataset diversity by filtering out molecules containing specific atom types. As shown in Table 5, downstream performance gen-erally improves as the molecular diversity of the pretraining dataset increases.\nThese results confirm that large and diverse pretraining datasets can improve molecular property on downstream datasets. They also highlight the importance of standardizing pretraining datasets when comparing different pretraining methods. Specifically, using the same pretraining datasets, as was done in the MPPW benchmark, ensures that any observed downstream performance improvements are the results of the pretraining strategy itself rather than variations in the pretraining dataset quality."}, {"title": "5.7 RESULTS ON LARGE DOWNSTREAM DATASETS", "content": "We further evaluate the performance of MoleVers on the MoleculeNet benchmark (Wu et al., 2018), focusing on large-scale regression datasets such as QM7, QM8, and QM9. These datasets, which range from thousands to over a hundred thousand labeled molecules, provide insights into the ef-fectiveness of our pretraining strategy in data-abundant scenarios. As shown in Table 6, MoleVers"}, {"title": "6 CONCLUSION", "content": "In this work, we addressed the challenge of molecular property prediction in the wild, i.e., in real-world scenarios where molecular property labels that are validated through experiments are scarce. We introduced a two-stage pretraining strategy that employs masked atom prediction, dynamic de-noising, and auxiliary property prediction to learn robust molecular representations. To enable ef-fective denoising pretraining with larger noise scales, we proposed a novel branching encoder that decouples the MAP pipeline from the denoising pipeline. We evaluated our model on a new bench-mark, Molecular Property Prediction in the Wild, designed to reflect real-world data limitations. Our model consistently outperforms previous state-of-the-art baselines in both low-data and high-data regimes. Our results highlight the effectiveness of the two-stage pretraining strategy, making it suitable for real-world applications where labeled data are extremely limited."}, {"title": "A APPENDIX", "content": "A.1 DETAILS OF DATASETS USED IN THE MPPW BENCHMARK\nThe Molecular Property Prediction in the Wild (MPPW) benchmark uses two types of datasets: pretraining datasets and downstream datasets. For our first-stage pretraining, as well as in the pre-training of other models shown in Table 1, we randomly select 1M unlabeled molecules from the GDB17 dataset (Ruddigkeit et al., 2012). For the second-stage pretraining, we sample around 130K molecules from the 1M subset and calculate the auxiliary labels\u2014HOMO, LUMO, and Dipole Mo-ment-using Psi4 (Smith et al., 2020). This smaller subset is also used to pretrain other models shown in 8.\nFor downstream evaluation, we curated 22 small datasets from the ChemBL database (Zdrazil et al., 2024), representing a diverse set of molecular properties as detailed in Table 7. To ensure consistency across datasets, we filter out any molecules containing atoms not present in the GDB17 dataset. As a result, only molecules containing the atoms {H, C, N, O, S, F, Cl, Br, I} are included in the downstream datasets. For evaluation, each dataset is randomly sampled to create three train/test splits with a 50:50 ratio, and all models in Tables 1 and 8 are assessed using these same splits. The processed datasets can be accessed through this URL."}, {"title": "A.2 MORE RESULTS ON THE MPPW BENCHMARK", "content": "As an additional experiment, we evaluated the downstream performance of MoleVers alongside the baseline models. In this experiment, the baselines are first pretrained using their original pretraining strategy, followed by a second-stage pretraining via auxiliary property prediction. The results, pre-sented in Table 8, show that MoleVers achieves state-of-the-art MAE in 20 out of the 22 datasets, and ranks second in the remaining two. In terms of R2 scores, MoleVers achieves the best perfor-mance in 19 out of the 22 datasets, and ranks second in the other three. Note that none of the other models consistently rank in the top two across all datasets. Since all models are pretrained with the auxiliary labels, the results in Table 8 further highlight the benefits of our branching encoder which enables denoising pretraining with larger noise scales."}, {"title": "A.3 IMPACT OF FINETUNING DATASET SIZE ON DOWNSTREAM PERFORMANCE", "content": "To assess the impact of finetuning dataset size on downstream performance, we gradually reduce the number of training labels used to finetune MoleVers, and validate it on fixed validation sets. We conduct this experiment using two large datasets outside the MPPW benchmark, as the datasets in the benchmark contain only a limited number of molecules. As shown in Figure 3, the MAE curves show exponential decay as the number of finetuning labels increases, while the R2 curves exhibit logarithmic growth. This demonstrates a sharp drop in prediction quality, especially when"}]}