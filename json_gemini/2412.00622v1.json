{"title": "Visual Modality Prompt for Adapting Vision-Language Object Detectors", "authors": ["Heitor R. Medeiros", "Atif Belal", "Srikanth Muralidharan", "Eric Granger", "Marco Pedersoli"], "abstract": "The zero-shot (ZS) performance of object detectors (OD) degrades when tested on different modalities, such as IR and depth. While recent work has explored image translation techniques to adapt detectors to new modalities, these methods are limited to a single modality and apply only to traditional detectors. Recently, vision-language (VL) detectors, such as YOLO-World and Grounding DINO, have shown promising ZS capabilities, however, they have not yet been adapted for other visual modalities. Traditional fine-tuning (FT) approaches tend to compromise the ZS capabilities of ODs. The visual prompt strategies commonly used for classification with vision-language models apply the same linear prompt translation to each image making them less effective. To address these limitations, we propose ModPrompt, a visual prompt strategy to adapt VL detectors to new modalities without degrading ZS performance. In particular, an encoder-decoder visual prompt strategy is proposed, further enhanced by the integration of inference-friendly task residuals, facilitating more robust adaptation. Empirically, we benchmark our method for modality adaptation on two VL detectors, YOLO-World and Grounding DINO, and on challenging IR (LLVIP, FLIR) and depth (NYUv2), achieving performance comparable to FT while preserving the model's ZS capability. Code available at: https://github.com/heitorrapela/ModPrompt.", "sections": [{"title": "1. Introduction", "content": "Object detection (OD) is one of the most popular and challenging tasks in computer vision, where the goal is to localize and classify desired objects in images [58]. Recent advancements in OD, particularly driven by applications such as autonomous driving [33, 43], surveillance [7, 38], and robotics [8, 17, 36], have led to substantial advancements in real-world systems. A major driver of these advancements is the incorporation of diverse visual modalities, such as infrared [24, 31, 44] and depth [25, 46, 47]. These modalities provide additional spatial information, which enhances visibility in low-light and obstructed conditions, resulting in more robust and accurate object detectors [2]. Another significant leap in adding new modalities to the detectors has been the incorporation of language, enabling open-vocabulary detection capabilities [50], these models are referred to as Vision Language Models (VLMs). VLMs for OD, such as Grounding DINO [28] and YOLO-World [5], exemplify this trend by leveraging the synergy between visual and linguistic information. VLMs can interpret visual cues along with textual context, allowing for more accurate object identification and flexibility even in scenarios involving previously unseen objects [53]. By grounding object references in images based on textual descriptions, these models significantly expand the boundaries of OD and open up new possibilities for real-world applications.\nGenerally, open-vocabulary detectors are pre-trained on large-scale RGB datasets, making them capable of zero-shot detection [28, 53]. However, their performance degrades if the domain shift is large, as in a different modality, such as infrared or depth. A common approach to mitigate this is by finetuning the VLMs on the downstream modality dataset. However, this requires substantial computational resources and the model may overfit to a specific dataset. It also makes the VLMs lose their zero-shot capabilities due to catastrophic forgetting [22]. Some techniques have been explored to adapt the VLMs to downstream tasks for image classification. These include text-prompt tuning (TPT) [55, 56] and visual prompt tuning (VPT) [1, 18]. Unlike traditional finetuning, prompt tuning involves adding learnable prompts to the model's input, allowing it to remain unchanged, and preserve its zero-shot capability. VPT methods for classification learn visual prompt tokens to adapt the classifier of transformer-based models to the downstream tasks. Visual prompts are effective when adapting VLMs to a downstream task of the same modality (normally RGB). However, these methods are less effective if the downstream data is from another modality with a large modality shift between pre-trained data and downstream data.\nFor traditional ODs, some methods have explored image translation for modality adaptation [14, 34]. These approaches primarily optimize the image translation loss rather than directly focusing on performance. More recent approaches, such as HalluciDet [32] and ModTr [30], have demonstrated improvement by emphasizing detection loss"}, {"title": "2. Related Work", "content": "Open-Vocabulary Object Detection. OD is a computer vision task that focuses on identifying objects within an image, providing both labels and their locations [51]. Recent advancements in open-vocabulary OD leverage vision-language models (VLMs) and multimodal alignment at the region level to enable novel concept recognition. For instance, RegionCLIP [54] introduces region-level pseudo labels for contrastive pretraining, effectively supporting both open-vocabulary and zero-shot OD. Gu et al. [12] frame open-vocabulary detection as knowledge distillation, aligning with CLIP's teacher of visual-text region embeddings. Lin et al. [26] use bipartite matching for image-text alignment and validate their model on open-vocabulary benchmarks. GLIP [23] employs deep multimodal fusion with self-generated box data, demonstrating robust zero-shot detection. GLIPv2 [53] further enhances vision language tasks using region-grounded pretraining and shows improvements by contrastive learning with other image regions. DetCLIP [48] parallelizes concept formulation, treating phrases as categories and constructing a concept dictionary for alignment. CORA [45] adapts CLIP by bridging the domain gap with region prompting and utilizing a class-aware localizer for efficient inference. More recently, Grounding DINO [28] pretrains a multiphase image-text fusion module for both open-vocabulary and referring OD and YOLO-World [5] introduces a reparametrizable fusion approach that enables efficient inference in a lightweight model.\nDownstream Adaptation of Large Vision Models. Recent seminal works explore ways to adapt large-scale vision models to downstream tasks without losing prior knowledge and perform adaptation in parameter-efficient ways. Co-op [56] learns a continuous representation of context prompt in the text space for downstream task learning. Visual prompt Tuning [18] explores adding training parameters at different stages in the network, demonstrating competitive performance across multiple recognition tasks. Bahng et al. [1] adapt large-scale vision models to new tasks by performing linear probing at the image space once. CLIP-Adapter [10] adapt in feature space for downstream tasks through bottleneck layer and residual blending instead of adapting at the input side. Yu et al. [49] improve downstream task performance while preserving prior knowledge using task residual learning.\nImage Translation for Object Detection. Image translation methods map images from one domain to another by adjusting the domain characteristics while preserving essential content [35]. Most methods rely on generative approaches [11, 21, 35]. A notable method, Pix2Pix by Isola et al., uses a paired generator-discriminator setup to generate domain-specific images based on paired data and labeled guidance [16]. On the other hand, CycleGAN by Zhu"}, {"title": "3. Proposed Method", "content": "3.1. Preliminary Definitions\nVision Language Object Detection. The training dataset for traditional object detectors can be represented as $\\mathcal{D} = {(\\mathbf{X}, \\mathbf{Y})}_{i=1}^N$. Here, $\\mathbf{X} \\in \\mathbb{R}^{W \\times H \\times C}$ represents an image with dimensions $W \\times H$ and $C$ channels, and $\\mathbf{Y} = {(\\mathbf{b}_i, c_i)}_{i=1}^N$ consist of bounding boxes $\\mathbf{b}_i$ and object category $c_i$. For the visual language model, the annotations are reformulated as region-text pair $\\mathbf{Y} = {(\\mathbf{b}_i, t_i)}_{i=1}^N$, where $t_i$ corresponding to the text for the region $\\mathbf{b}_i$. Specifically, $t_i$ is the name of the object category in $\\mathbf{b}_i$. The objective of the object detection task is to accurately identify all objects in a given image. The average precision (AP) metric across classes is used as the standard evaluation protocol.\nAn object detector is formally represented as the mapping $f_\\theta: \\mathbb{R}^{W \\times H \\times C} \\rightarrow \\hat{\\mathbf{Y}}$, where $\\theta$ denotes the parameter vector. The detection cost function $C_{det}(\\theta)$, a differentiable proxy for the AP metric, is computed as an average loss of the detection loss $\\mathcal{L}_{det}$ over the entire dataset $\\mathcal{D}$, described mathematically as:\n$C_{det}(\\theta) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{X}, \\mathbf{Y}) \\in \\mathcal{D}} \\mathcal{L}_{det} (f_\\theta(\\mathbf{x}), \\mathbf{Y}).$ (1)\nVisual Prompt for OD. Given a frozen pre-trained detector, the objective of Visual Prompt method is to learn a task-specific visual prompt $\\mathbf{v}_\\phi$, parametrized by $\\phi$, which can be combined with the input $\\mathbf{x}$ to improve the final detection performance. During test time, this visual prompt is added to the test images to adapt it for the desired detection task. As described in the following equation:\n$C_{vp}(\\phi) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, \\mathbf{Y}) \\in \\mathcal{D}} \\mathcal{L}_{det} (f_\\theta(\\mathbf{x} + \\mathbf{v}_\\phi), \\mathbf{Y}).$ (2)"}, {"title": "3.2. ModPrompt", "content": "The visual prompt methods discussed above learn simple linear transformation, like adding a fixed patch to each image. These transformations are learned during the training process and are not conditioned on the input image at inference, making them less effective. In ModPrompt, we incorporate a function $h_\\psi$, an encoder-decoder model similar to U-Net [40], dependent on the input image $\\mathbf{x}$, which is trained conditioned on labels $\\mathbf{Y}$. This function is responsible for adapting the input image to a modality representation optimally suited to address detection in the target modality. Our idea of a visual ModPrompt is inspired by text conditional prompt [55] work, which incorporates a small network for the text encoder conditional adaptation. In our approach, the visual prompt depends on the input image. The ModPrompt training cost can be defined by the following equation:\n$C_{mp}(\\psi) = \\frac{1}{|\\mathcal{D}|} \\sum_{(\\mathbf{x}, \\mathbf{Y}) \\in \\mathcal{D}} \\mathcal{L}_{det} (f_\\theta(\\mathbf{x} + h_\\psi(\\mathbf{x})), \\mathbf{Y}).$ (3)"}, {"title": "3.3. Efficient Text-prompt Tuning with Knowledge Preservation", "content": "The vision-language models take both text and image as input. ModPrompt aims to transform the input image to adapt to the new modality. For the adaptation of the textual part, text prompt tuning methods [55, 56] have been explored for classification. But, these methods require back-propagating through the whole text backbone making them computationally expensive. Recently, task residual [49] was proposed that tunes the precomputed text embedding of the vision-language classifiers. Inspired by this, we design a"}]}