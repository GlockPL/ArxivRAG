{"title": "ComfyGI: Automatic Improvement of Image Generation Workflows", "authors": ["Dominik Sobania", "Martin Briesch", "Franz Rothlauf"], "abstract": "Automatic image generation is no longer just of interest to researchers, but also to practitioners. However, current models are sensitive to the settings used and automatic optimization methods often require human involvement. To bridge this gap, we introduce ComfyGI, a novel approach to automatically improve workflows for image generation without the need for human intervention driven by techniques from genetic improvement. This enables image generation with significantly higher quality in terms of the alignment with the given description and the perceived aesthetics. On the performance side, we find that overall, the images generated with an optimized workflow are about 50% better compared to the initial workflow in terms of the median ImageReward score. These already good results are even surpassed in our human evaluation, as the participants preferred the images improved by ComfyGI in around 90% of the cases.", "sections": [{"title": "1. Introduction", "content": "The quality of diffusion models for image generation has improved significantly in recent years (Ho et al., 2020; Dhariwal & Nichol, 2021; Rombach et al., 2022). A large number of models are available on platforms such as Hugging Face\u00b9 and can be freely used by everyone. Some of the recent models are even specialized, e.g., for the generation of digital art (Huang et al., 2022) or even photorealistic images (Saharia et al., 2022). In addition, many free tools have recently emerged that not only make automatic image generation usable for researchers and programmers, but also allow designers to experiment with image generation techniques via a user-friendly interface. One of these tools is ComfyUI\u00b2, which has usually made the latest innovations in image generation available very quickly. Thanks to its modular approach, which makes it easy to link different models and other modules in a design workflow, ComfyUI is not only an interesting tool for beginners, but also for advanced users.\nHowever, despite all the accessible tools, there are still many possible configurations in image generation, which can be further increased by using complex design workflows, which have a substantial influence on, e.g., the alignment with the given image description and the perceived aesthetics of the generated image (Wang et al., 2022). Manually tuning all the prompts and settings can be time-consuming for the designer and often involves the user heavily in the evaluation process (Liu & Chilton, 2022), which does little to improve the situation. So tools are needed that perform this optimization completely automatically and, just like ComfyUI, remain easily extensible so that they are not immediately made obsolete by new innovations in image generation.\nIn the field of software development, genetic improvement (GI) (Petke et al., 2017) is a method that uses search-based strategies on the source code of software to optimize, for example, its non-functional properties. In software development, these are typically properties such as runtime, memory requirements, or energy consumption. This is achieved by a step-by-step improvement through small changes applied to the source code of the software (mutations) and a given objective function that guides the search process. In the image generation domain, the principles of GI can also be applied to ComfyUI's design workflows, which are processed and stored in JSON format. And with the recently released ImageReward model (Xu et al., 2024), an effective objective function can be defined that evaluates generated images according to their alignment with the given description as well as their aesthetics.\nAccordingly, we introduce ComfyGI\u00b3, the first method that applies GI techniques to ComfyUI's workflows, significantly increasing the quality of the automatically generated images in terms of the output image's alignment with the given description and its perceived aesthetics. Through its interaction with ComfyUI and simple extensibility, ComfyGI is suitable for researchers as well as practitioners.\nTo improve a given design workflow and enable the gener-"}, {"title": "2. Background", "content": "In this section, we present related work on image optimization and assessment and give a brief introduction to ComfyUI as well as genetic improvement."}, {"title": "2.1. Image Optimization and Quality Assessment", "content": "Generative text-to-image models like GANs (Goodfellow et al., 2020; Reed et al., 2016; Tao et al., 2022), autoregressive models (Ramesh et al., 2021; Ding et al., 2021), and diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021; Saharia et al., 2022) can produce high quality images. Especially stable diffusion (Rombach et al., 2022) has shown great performance in recent years. These models generate images based on a textual description (prompt). However, the quality of outputs generated by these models is highly sensitive to both the prompt as well as the hyper-parameter settings (Wang et al., 2022). Additionally, the output might not be aligned with human preferences or not capture the intent of the user (Xu et al., 2024).\nSome approaches try to align the model as a whole with human preferences (Dong et al., 2023; Lee et al., 2023; Wu et al., 2023; Xu et al., 2024) while others aim to optimize the prompt directly either with humans in the loop (Martins et al., 2023) or using automatic metrics (Hao et al., 2024; Wang et al., 2024). Another approach uses genetic algorithms to optimize latent representations using both"}, {"title": "2.2. ComfyUI", "content": "ComfyUI is a browser-based tool that can be used to create and run design workflows for automatic image generation without any programming knowledge. In these workflows, the elements required for image generation, such as prompts or sampler settings, are specified and their interaction is defined. Figure 1 shows how such a workflow could look like in ComfyUI. The example shows a module for defining the checkpoint model used, a module setting the image's dimensions, modules for the positive and negative prompts, as well as modules for the sampler settings and for saving the final image. The individual modules are interconnected by wires. This free combination of modules makes ComfyUI relevant also for more professional users but also easy to expand with new modules, so it is not surprising that new innovations, such as ControlNet (Zhang et al., 2023) or IP adapters (Ye et al., 2023), are usually quickly available in ComfyUI.\nFurther, the design workflows are saved in a JSON format. This allows effective workflows to be shared online with the design community, but also provides an accessible interface to automatically optimize the workflows."}, {"title": "2.3. Genetic Improvement", "content": "In software development, genetic improvement (GI) (Petke et al., 2017) is a technique that can be used to functionally or non-functionally optimize existing software by employing search-based approaches. Functional improvement includes, e.g., automatic bug fixing (Yuan & Banzhaf, 2020), while non-functional improvement can optimize properties like runtime (Langdon et al., 2015), memory requirements (Callan & Petke, 2022), or energy consumption (Bruce et al., 2015). During the search, mutations, i.e. small changes to the source code, are made to change the software in order to gradually create a patch that can later be applied to the software. Mutations can, for example, insert, swap, or delete lines of code. Changes directly to the abstract syntax tree (AST) are also possible (An et al., 2018), as is the integration of LLMs to generate alternative code variants within a mutation operator (Brownlee et al., 2023; 2024). An objective function guides the search in the desired direction.\nGI has already been used to successfully improve software"}, {"title": "3. Method: ComfyGI", "content": "Next, we describe ComfyGI's search method including the mutation operators used, present the used diffusion models and benchmarks, and explain our approach for the human evaluation."}, {"title": "3.1. Search Method and Mutation Operators", "content": "ComfyGI uses GI techniques to improve a given design workflow and enable the generation of a high quality image that is aligned with the input prompt and is also aesthetically appealing. To achieve this goal, we employ a hill climbing method to search for a patch that can be applied to improve the given workflow in JSON format. With this updated workflow, we generate the optimized image.\nFigure 2 illustrates the steps that are iteratively performed by ComfyGI. First, we take the input workflow in JSON format, use it to generate the initial image, and assign a score to this image with the ImageReward model (step 1). In the example illustration, the assigned score is -0.93. After that, we search the neighborhood and apply small mutations to the workflow (step 2). These mutations range from changes to the sampling configuration to improvements of the prompts with an LLM and are explained in detail below. Following that, we generate an image from each updated workflow (step 3) and evaluate all generated images with the ImageReward model and compare the score of all images (step 4). If the score of the best image in the current generation is better than the best score recorded so far, we add the mutation that led to this successful improvement to the patch (step 5). In the example, the best image of the current generation has a score of 1.99 and is therefore better than the best image from the previous generation (initial"}, {"title": "4. Experiments and Results", "content": "In this section, we present and discuss the results achieved with ComfyGI, including visual examples as well as the outcome of the human evaluation."}, {"title": "4.1. Results of the ComfyGI Runs", "content": "To assess the performance of ComfyGI, we generated and evaluated images for 42 prompts out of 3 categories from the ImagenHub (Ku et al., 2024) benchmark. To obtain reliable results despite the nondeterministic nature of the image generation models, we performed 10 independent runs for every benchmark prompt. For each of the 10 runs we always used a random checkpoint model and a random seed in the initial workflow. In addition, in each generation we consider 30 neighboring solutions for each mutation operator used for a total of 150 possible different mutations per generation.\nFirst, we start with a visual inspection of some example images optimized with ComfyGI. Figure 3 shows the improvement over several generations for the initial prompt \"storefront with 'diffusion' written on it\u201d. For every generation, the image and the score for the best found patch so far is shown. We see that the initial image (Gen. 0 with a score of 1.468) contains many errors and the text on the storefront is hard to read and misspelled. Over the generations, the images get better and better. The structures become clearer and the colors get brighter and more expressive. In the final image (Gen. 5) with a score of 1.933, even the text on the storefront is written correctly. This was achieved by applying, among others, the mutation operators prompt_llm, checkpoint and ksampler.\nFigure 4 shows further examples of generated images. The left-hand images show the initially generated image and the right-hand images show the counterpart optimized with ComfyGI. The intermediary steps are omitted due to space"}, {"title": "4.2. Results of the Human Evaluation", "content": "We also carried out a human evaluation study to further confirm our findings. Therefore, we recruited 100 participants on the Prolific platform \u2013 10 participants for each performed run. Every participant had to evaluate 42 image pairs.6 Participants took 12:41 minutes median time to complete the study. This resulted in a median hourly wage of \u00a314.19.\nThe participants were 57% male and 43% female with a median age of 39 years. In addition, all participants rated their knowledge of the English language as very good. Overall, one participant failed the attention checks and was therefore not included in the results. For further details on the demographics, we refer the reader to Appendix G.\nFigure 8 shows box-plots for the win rate in the human evaluation for the initial as well as for the optimized images, where the win rate is the proportion of human annotators that prefer an image (initial or optimized) in a direct comparison to its counterpart. We see that the human evaluation confirms the previous findings as the optimized images were"}, {"title": "4.3. Influence of the Mutation Operators", "content": "In addition to the objective function, the mutation operators are the driving factors of ComfyGI's guided search. Consequently, we study their influence on the overall improvement.\nFigure 9 shows the average improvement of the ImageReward score over all studied prompts and runs. As before in Sect. 4.1, we see that the improvement is most effective in the first generations. For example, we see an average improvement of over 1.75 points with the checkpoint mutation operator in the first generation. In addition also the ksampler and prompt-11m operators perform very well in the first generation. Especially the prompt_llm operator is also important in the following generations (see generations 3-5).\nAs the biggest improvements are found in the first generation, we take a closer look at these improvements. Figure 10 shows box-plots for the applied mutations in the first generation over all runs and considered benchmark prompts. As before, we see that the checkpoint operator has the highest impact. So changing the checkpoint model in the first generations plays a crucial role. However, it is by no means the case that the most advanced model is always selected, which in our case would be Stable Diffusion 3 Medium."}, {"title": "5. Conclusion", "content": "In this paper, we introduced ComfyGI, an approach that uses GI techniques to automatically improve workflows for image generation without the need for human intervention. This allows images to be generated with a significantly higher quality in terms of the output image's alignment with the given description and its perceived aesthetics.\nIn our analysis of ComfyGI's performance, we found that overall, the images generated with an optimized workflow are about 50% better than with the initial workflow in terms of the median ImageReward score. This was also confirmed by a human evaluation with 100 participants as the improved images where preferred in about 90% of the cases.\nIn future work, we will investigate more complex workflows and additional mutation operators, due to the easy extensibility of ComfyGI."}, {"title": "6. Impact Statement", "content": "ComfyGI makes it easier to use image design workflows, which could have a positive social impact through increased inclusivity. In its current form, we therefore do not see any negative impact. However, due to the simple extensibility of ComfyGI, the objective function could, e.g., be changed and"}]}