{"title": "Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning", "authors": ["Jianming Chen", "Yawen Wang", "Junjie Wang", "Xiaofei Xie", "Jun Hu", "Qing Wang", "Fanjiang Xu"], "abstract": "Explaining multi-agent systems (MAS) is urgent as these systems become increasingly prevalent in various applications. Previous work has provided explanations for the actions or states of agents, yet falls short in understanding the black-boxed agent's importance within a MAS and the overall team strategy. To bridge this gap, we propose EMAI, a novel agent-level explanation approach that evaluates the individual agent's importance. Inspired by counterfactual reasoning, a larger change in reward caused by the randomized action of agent indicates its higher importance. We model it as a MARL problem to capture interactions across agents. Utilizing counterfactual reasoning, EMAI learns the masking agents to identify important agents. Specifically, we define the optimization function to minimize the reward difference before and after action randomization and introduce sparsity constraints to encourage the exploration of more action randomization of agents during training. The experimental results in seven multi-agent tasks demonstrate that EMAI achieves higher fidelity in explanations than baselines and provides more effective guidance in practical applications concerning understanding policies, launching attacks, and patching policies.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed sensational advances in reinforcement learning (RL) across many prominent sequential decision-making problems. As these problems have grown in complexity, the field has transitioned from using primarily single-agent RL algorithms to multi-agent RL (MARL) algorithms, which are playing increasingly significant roles in various domains, e.g., unmanned aerial vehicles (Liu et al. 2023b; Lv et al. 2024; Feng et al. 2023), industrial robots (Luo et al. 2023; Wu et al. 2022; Qiu et al. 2023), camera network (Pan et al. 2022; Ci et al. 2023), and auto-driving (Petrillo et al. 2018). However, deep RL policies typically lack explainability, making them intrinsically difficult for humans to comprehend and trust. This issue is even more pronounced in multi-agent systems (MAS) due to the interactions and dependencies among agents. To broaden the adoption of RL-based applications in critical fields, there is a pressing need to enhance the transparency of RL agents through effective explanations.\nAlthough some in-training explainable RL approaches (e.g., credit assignment) can simultaneously provide intrinsic explanations of the model when accomplishing tasks, they cannot work in black-box settings. Prior work on post-training explanations for the black-box agent can be roughly divided into two categories. The first category offers the observation-level explanation, i.e., revealing the regions of features within the observations that exert the most significant influence on the decisions of agent (Greydanus et al. 2018; Puri et al. 2020; McCalmon et al. 2022). The second category delves into step-level explanations, aiming to identify the time-steps that are most or least pivotal to the agent's ultimate reward (Amir and Amir 2018; Huang et al. 2018; Cheng et al. 2023). Although previous research on post-training explanation shows great potential in helping users understand the behavior of the black-box agent, they cannot assess the importance of an agent at any specific state within the MAS.\nIn MAS, the increase in the number of agents significantly contributes to the complexity of team strategies, and each agent plays its unique role and cooperates with others towards a common goal. Evaluating the importance of individuals in MAS helps reveal potential issues and vulnerabilities in collaboration, such as low-contributing agents (i.e., \"lazy\" agents) that limit system performance, or excessively high individual contributions that may indicate a lack of cooperation among agents (Liu et al. 2023a). This can then potentially lead to a better training strategy for improving the overall performance of MAS. Additionally, by identifying the important agents at each state, targeted and efficient interventions (e.g., launching attacks and patching policies) can be carried out more effectively (Guo et al. 2021; Cheng et al. 2023).\nWe propose EMAI, a novel agent-level Explanation approach for the MAS which pinpoints the Importance of each individual agent at every time-step (i.e., state). In this paper, the agent required to be explained is black-boxed and called the target agent. Motivated by counterfactual reasoning, which assesses the importance of a factor or decision to an outcome by envisioning scenarios contrary to reality, we define importance as the change in reward resulting from the random actions of target agents. The more important the agents, the greater the effect of their random actions on the reward. Intuitively, one might attempt to achieve explanations by performing multiple random actions and observing the resulting changes in rewards. However, due to the space explosion problem (Jamroga and Kim 2023) inherent in MAS and the necessity for multiple random transformations per interpretation, this approach would be highly inefficient. To address this challenge, this work aims to learn the policy that guides the selection of specific agents for action randomization at each time step to more accurately and cost-effectively reveal the importance of the agents.\nIn this sense, we introduce the concept of masking agents, which learns a policy to mask unimportant target agents (i.e., make them take random actions). The importance of the target agent can be represented by its masking probability. Since the importance of agents may be manifested through joint actions with other agents or delayed effects in subsequent time-steps (Chen et al. 2020), we model the learning of masking agents as a MARL problem, which decides whether or not to mask the target agents at each time-step, to capture these dependencies between agents and across time-steps. Then, we utilize Centralized Training with Decentralized Execution (CTDE) paradigm (Foerster et al. 2016) to address the challenges of a holistic evaluation of each agent's value and the exponential growth of joint action space with the number of agents in MAS (Saner, Trivedi, and Srinivasan 2022; Cui, Zhang, and Du 2023). To train the masking agents, we design the optimization objective to minimize the difference in performance before and after masking the target agents. Besides, we design the sparsity constraint to encourage the exploration of masking as many target agents as possible during training.\nWe evaluate EMAI in seven popular multi-agent tasks and compare it with three commonly used and state-of-the-art baselines. Results show that the explanations derived from EMAI have higher fidelity, with relative improvement ranging from 11% to 118% compared to baselines. Besides, based on the results of a manual evaluation, EMAI can help understand the policies by marking important agents in the visualization. Model attackers can use EMAI to identify critical agents for attacking. The attacks guided by EMAI show the best performance, with the relative improvement ranging from 14% to 289% compared to baselines. Finally, users can enhance the performance of MAS through patching critical agents identified by EMAI. Compared to baselines, the greatest improvements are achieved when guided by EMAI. The contributions of this paper are as follows:\n\u2022 A novel agent-level approach for explaining MAS by learning the importance of agents, which models the problem as MARL to learn the policy (masking agents) to randomize the actions of unimportant target agents.\n\u2022 Experimental evaluation on the fidelity of EMAI on seven multi-agent tasks with promising performance, outperforming three commonly used and state-of-the-art baselines.\n\u2022 The demonstration of practical applications of this work, by evaluating the effectiveness of understanding policies, launching attacks, and patching policies with guidance from EMA\u0399."}, {"title": "Related Work", "content": "RL explanation. Existing research on RL explanation primarily focuses on in-training explanations and post-training explanations. (1) The in-training explainable RL models aim to design RL training algorithms that can simultaneously provide interpretable intermediate outcomes, enabling users to understand how the agent makes decisions and accomplishes tasks. Examples of such approaches include hierarchical RL (Zhang et al. 2020; Eckstein and Collins 2020), model approximation (Coppens et al. 2019; Bewley and Lawry 2021), and credit assignment (Li et al. 2021; Wang et al. 2020). Since the main goal of these approaches is to train better RL models, the provided interpretation ability is often a byproduct and tends to lack accuracy (Jacq et al. 2022). More importantly, this explanation is provided by the model itself. It cannot be used to explain a black-box target agent, in which case one can only query the actions taken by the agent under specific observations. While post-training explanation approaches can provide an interpretation under black-box settings.\n(2) The post-training explanation approaches involve explaining the decision-making process and strategies of the target agent after it has been trained. According to the perspective of explanation objectives, existing post-training explanation approaches can be mainly divided into two categories. The first category focuses on the observation-level explanation, which explains the regions of feature in the agent's observations that have the most significant impact on decisions, such as constructing saliency maps (Atrey, Clary, and Jensen 2020; Greydanus et al. 2018; Puri et al. 2020) and learning strategy representations (Bewley and Lawry 2021; McCalmon et al. 2022). Regarding the second category of approaches, most of them provide step-level explanations to indicate the critical time-steps throughout the episode for achieving the final reward, e.g., the value function-based approaches (Amir and Amir 2018; Jacq et al. 2022; Huang et al. 2018) and the approaches learning state-reward relationships (Cheng et al. 2023; Guo et al. 2021; Yu et al. 2023). However, they typically cannot assess the importance of each agent per time-step, which is quite crucial for MAS.\nCounterfactual reasoning. Counterfactual reasoning is a widely-used approach for explaining supervised learning models, e.g., explaining image classification models (Fong and Vedaldi 2017; Chang et al. 2019; Goyal et al. 2019). These approaches involve perturbing the input and observing the impact on the classification outcome to reveal the reasons behind the specific predictions of models. Notably, Shapley value (Louhichi et al. 2023) is a concept related to, but distinct from, counterfactual reasoning. It considers all possible subsets of combinations that include the target participant. The focus is on marginal contribution, which refers to the incremental increase or decrease brought about by adding or removing a participant. Calculating the Shapley"}, {"title": "Approach", "content": "We consider a problem setting where a MARL joint policy \\(\\pi = {\\pi_1, ..., \\pi_\\eta}\\) has been well trained for n agents in the MAS. At each time-step t in an episode, i-th agent obtains a local observation \\(O_{t,i}\\) from the global state \\(s_t\\) according to the observation function. The policy \\(\\pi_i : O_i \\rightarrow a_i\\) denotes the individual policy of i-th agent, which takes action \\(a_{t,i}\\), depending on its local observations \\(O_{t,i}\\). The joint action \\(a_t = {a_{t,1},..., a_{t,n}}\\) leads to the next state \\(s_{t+1}\\) with the state transition probability \\(P(S_{t+1}|S_t, a_t)\\). Thereby, a global reward \\(r_t\\) is obtained according to reward function \\(R(S_t, a_t, S_{t+1})\\).\nConsidering the variance in the importance of agents at different time-steps, we aim to explain the MAS containing n agents, by identifying the importance of target agents at each time-step t, i.e., \\(imp = {imp_1^t, ..., imp_n^t}\\). Our approach works under the black-box setting where only each agent's observation and corresponding action decision can be queried, which is more rational and practical, i.e., the value function and parameters of target agents are unavailable.\nTo measure the importance of a particular agent for the final reward, we draw inspiration from some works based on counterfactual reasoning (Goyal et al. 2019; Cheng et al. 2023). These approaches are based on the fundamental assumption that modifying the most important elements will exert the greatest impact on the outcome. Similarly, in our problem, we can decide whether a particular agent is important or not by randomizing its actions at various time-steps and observing the change in the final reward. The importance of an agent can be reflected as \"how the final total rewards will change when its action is randomized\u201d. If the reward difference is large, it indicates that the agent is highly important. Conversely, a minimal difference in rewards implies low importance.\nWe aim to learn a decision policy that generates a probability distribution, dictating the likelihood of selecting each agent for action randomization at each time step. The optimization goal of the policy is to minimize the reward difference before and after randomization. Thus, a lower probability of an agent being selected for randomization indicates its higher importance. We model the learning of this decision policy as a MARL problem, to take into account the inter-agent and cross-time-step action dependencies (i.e., the cooperative relationships of joint actions and the delayed effects of actions across time-steps). Specifically, we introduce the EMAI, which incorporates our defined multiple masking agents. For each i-th masking agent, we aim to develop a policy (denoted as \\(\\pi^{\\theta_i}\\)) that determines whether to randomize the action of the corresponding i-th target agent in the original MAS at each time-step, i.e., masking that target agent. We treat the target agents with fixed joint policy \\(\\pi\\) as part of the environment. Then the decision processes of masking agents is modeled as decentralized partially observable Markov decision processes (DEC-POMDP) (Oliehoek, Amato et al. 2016; Hausknecht and Stone 2015), which can be defined by a tuple \\(G =\\langle S, A^m, O, P, R, \\eta, \\gamma \\rangle\\). The observation function O, state transition probability P, and reward function R share the same definition in the origin environment of the target MAS. S is the global state space. \\(A^m\\) is our defined action space \\({0,1}\\) of each masking agent. The \\(a^m_i \\in A^m\\) represents the masking action of the i-th masking agent, where \\(a^m_i = 0\\) denotes not masking and \\(a^m_i = 1\\) denotes masking the i-th target agent. The observation \\(o^i_t\\) for i-th agent is generated by the observation function O(s, i). \\(\\gamma\\) is the discount factor applied to the rewards. We define the policy of the i-th masking agent as \\(\\pi^{\\theta_i} : O_i \\rightarrow a^m_i\\) parameterized by \\(\\theta_i\\).\nFollowing the above, the workflow of our proposed EMAI is illustrated in Figure 1a. First, based on the observation generated from the state of the environment, each target agent takes actions through its fixed policy \\(\\pi\\). Meanwhile, EMAI trains the policy \\(\\pi^{\\theta}\\) for masking agents to take masking actions at each time-step. Then the final action for the i-th target agent \\(\\tilde{a}_i\\) is defined by the following operation:\n\\begin{equation}\n\\tilde{a}_i = aai = \\begin{cases}\na_i, & \\text{if } a^m_i = 0, \\\n\\text{random action}, & \\text{if } a^m_i = 1, \\\n\\end{cases}\n\\label{eq:1}\n\\end{equation}\nwhere \\(a^m_i = 0\\) indicates that the i-th target agent retains its own action. Otherwise when \\(a^m_i = 1\\), the action of target agent is replaced with a random action. Then the final joint action \\(\\text{action} = {\\tilde{a}_1, ..., \\tilde{a}_n}\\) that actually affects the environment can be acquired. Notably, we do not assume that EMAI only applies to discrete or continuous action space. In discrete action space, random action is randomly selected from a finite set \\({d_1, ..., d_k}\\) consisting of predefined k discrete values. If the action space is continuous, it is randomly sampled from an environmentally given continuous range [lb, ub], e.g., to randomly select 0.88 from the range [-1,1].\nTo ensure accurate masking of the low-importance target agent, we need a suitable objective function for training \\(\\pi^{\\theta}\\). The objective of \\(\\pi^{\\theta}\\) is for masking the target agents while minimizing the following difference of the expected rewards:\n\\begin{equation}\nobj(\\pi^{\\theta}) = \\underset{\\theta}{\\text{arg min}}|J(\\pi) - J(\\pi^{\\theta})|,\n\\label{eq:2}\n\\end{equation}\nwhere J(\\(\\pi\\)) represents the expected reward obtained by the target multi-agent with fixed policy \\(\\pi\\). Following previous research (Cheng et al. 2023), J(\\(\\pi\\)) can be estimated as a constant in advance using the Monte Carlo method. Specifically, we have the target multi-agent run the game 500 times and calculate the average expected discounted reward as follows:\n\\begin{equation}\nJ(\\pi) = E_{s,a \\sim \\pi}[\\sum_{t} \\gamma^t (R(s_t, a_t, s_{t+1}))].\n\\label{eq:3}\n\\end{equation}\nThe J(\\(\\pi^{\\theta}\\)) in Equation 2 represents the expected reward when the actions of target agents are disrupted by the policy \\(\\pi^{\\theta}\\) of masking agents, as follows:\n\\begin{equation}\nJ(\\pi^{\\theta}) = E_{s,a^m \\sim \\pi^{\\theta}}[\\sum_{t} \\gamma^t (R(s_t, \\tilde{a}, s_{t+1}))].\n\\label{eq:4}\n\\end{equation}\nIn addition, to encourage the exploration of masking more target agents during training, we set up a masking reward as sparsity constraint: \\(R_m (a^m) = \\beta \\sum_{i=1}^n a^m_i\\), which reflects the number of target agents masked at time-step t. The \\(\\beta\\) is the weight hyper-parameter of the sparsity constraints. The final total expected discounted reward is defined as:\n\\begin{equation}\nJ'(\\pi^{\\theta}) = J(\\pi^{\\theta}) + E_{s,a^m}[\\sum_t (\\gamma^t R_m (a^m))].\n\\label{eq:5}\n\\end{equation}\nDue to the complex cooperation and diverse division of responsibilities within a MAS, the importance of each agent must be considered from a holistic perspective. At the same time, the joint masking action space \\(\\Pi_{i=1}^{n} A^m\\) of all n masking agents grows in an exponential manner with n. Therefore, in our proposed EMAI, we apply CTDE (Foerster et al. 2016; Oliehoek, Amato et al. 2016) for MARL training. In this framework, global information (including observations and actions of all agents) can be used to guide individual learning processes to consider their global impact, while each agent independently makes decisions based on its own observation, aiding in decomposing the joint action space.\nAs shown in Figure 1b, EMAI consists of two networks: the \\(\\pi^{\\theta}\\) of masking agents with weight parameters \\(\\theta\\), and the central critic network \\(C^w\\) with weight parameters w. Specifically, as mentioned above, the network \\(\\pi^{\\theta}\\) learns the policy for the masking agents, based on the observations \\([o_i]_{i=1}^n\\), it outputs the masking actions \\([a^m_i]_{i=1}^n\\). The network \\(C^w\\) is constructed to evaluate the joint action of all masking agents from a global perspective.\nFollowing value-based CTDE (Rashid et al. 2018; Sunehag et al. 2017), we evaluate the value of masking actions when learning \\(\\pi^{\\theta}\\). Firstly, we train \\(\\pi^{\\theta}\\) to learn individual value function \\(Q_i(o_i, a^m_i; \\theta)\\) for each one of the n masking agents to assess individual policy, which represents the value of taking action \\(a^m_i\\) for observation \\(o_i\\). Secondly, the \\(C^w\\) learns the centralized value function \\(Q_{tot}(o, a^m; w)\\) to evaluate the collective policy, which is the estimate of J'(\\(\\pi^{\\theta}\\)).\nTo ensure the maximization of both individual and total values simultaneously, it can be achieved by satisfying the following Individual-Global-Max (IGM) principle (Hong, Jin, and Tang 2022; Xu et al. 2023).\n\\begin{equation}\n\\underset{a^m \\in A^m}{\\text{arg max}} Q_{tot}(o,a^m;w) = \\underset{a^m_1}{\\text{arg max}} Q_1(o_1, a^m_1; \\theta), ..., \\underset{a^m_n}{\\text{arg max}} Q_n(o_n, a^m_n; \\theta).\n\\label{eq:6}\n\\end{equation}\nSpecifically, in the EMAI, we constrain the weights of the central critic network to be non-negative (Rashid et al. 2018) to ensure adherence to the Equation 6, which can be defined as the following form:\n\\begin{equation}\nQ_{tot}(o,a^m;w) = \\sum_{i=1}^n w_i (Q_i(o_i, a^m_i; \\theta)), \\forall i \\in \\{1, ..., n\\}; \\omega \\geq 0.\n\\label{eq:7}\n\\end{equation}\nThe Equation 7 can be replaced with other implements such as VDN (Sunehag et al. 2017), QPLEX (Wang et al. 2021), and QTRAN (Son et al. 2019)."}, {"title": "Problem Definition", "content": "We consider a problem setting where a MARL joint policy \\(\\pi = {\\pi_1, ..., \\pi_\\eta}\\) has been well trained for n agents in the MAS. At each time-step t in an episode, i-th agent obtains a local observation \\(O_{t,i}\\) from the global state \\(s_t\\) according to the observation function. The policy \\(\\pi_i : O_i \\rightarrow a_i\\) denotes the individual policy of i-th agent, which takes action \\(a_{t,i}\\), depending on its local observations \\(O_{t,i}\\). The joint action \\(a_t = {a_{t,1},..., a_{t,n}}\\) leads to the next state \\(s_{t+1}\\) with the state transition probability \\(P(S_{t+1}|S_t, a_t)\\). Thereby, a global reward \\(r_t\\) is obtained according to reward function \\(R(S_t, a_t, S_{t+1})\\).\nConsidering the variance in the importance of agents at different time-steps, we aim to explain the MAS containing n agents, by identifying the importance of target agents at each time-step t, i.e., \\(imp = {imp_1^t, ..., imp_n^t}\\). Our approach works under the black-box setting where only each agent's observation and corresponding action decision can be queried, which is more rational and practical, i.e., the value function and parameters of target agents are unavailable."}, {"title": "Problem Modeling", "content": "To measure the importance of a particular agent for the final reward, we draw inspiration from some works based on counterfactual reasoning (Goyal et al. 2019; Cheng et al. 2023). These approaches are based on the fundamental assumption that modifying the most important elements will exert the greatest impact on the outcome. Similarly, in our problem, we can decide whether a particular agent is important or not by randomizing its actions at various time-steps and observing the change in the final reward. The importance of an agent can be reflected as \"how the final total rewards will change when its action is randomized\u201d. If the reward difference is large, it indicates that the agent is highly important. Conversely, a minimal difference in rewards implies low importance.\nWe aim to learn a decision policy that generates a probability distribution, dictating the likelihood of selecting each"}, {"title": "Algorithm 1: The training algorithm of EMAI.", "content": "Input: The policy \\(\\pi\\) of target agents, the original expected reward J(\\(\\pi\\)), the observations \\({o_1, ..., o_n}\\)\nOutput: The policy \\(\\pi^{\\theta}\\) of masking agents\nInitialization: The networks of \\(\\pi^{\\theta}\\) and \\(C^w\\)\nfor each training batch do\nGet original joint action from \\(\\pi\\):\n\\({\\tilde{a}_1, ..., \\tilde{a}_n} = \\pi(o_1, ..., o_n)\\)\nGet joint masking action and values from \\(\\pi^{\\theta}\\):\n\\({a^m_1, ..., a^m_n}, {Q_1, ..., Q_n} = \\pi^{\\theta}(o_1, ..., o_n)\\)\nGet the final joint action:\naction = {1, ..., an}\n{a, ..., am}\nExecute action of target agents and get reward from environment\nCalculate the global value Qtot using \\(C^w\\) network\nUpdate w and \\(\\theta\\) by the TD loss with reward, J(\\(\\pi\\)), and Qtot, as shown in Equation 10\nend\nFinally, once Qtot has been determined, we use the following one-step TD loss (Hausknecht and Stone 2015) for the iterative optimization of \\(C^w\\) and \\(\\pi^{\\theta}\\), which minimizes the error between expected and estimate values \\(Q_{tot} (o, a^m)\\).\n\\begin{equation}\nL_e(w, \\theta) = \\underset{(\\omega,\\theta)}{\\text{arg min }} E[((y_{tot} - Q_{tot}(o,a^m))^2],\n\\label{eq:8}\n\\end{equation}\nwhere \\(Y_{tot} = reward + \\gamma \\underset{\\hat{a}^m}{\\text{max }}Q_{tot}(\\hat{o}, \\hat{a}^m)\\), which denotes the expected value, and reward is calculated by \\(R(s_t, a^m, s_{t+1}) + R_m(a^m)\\), as introduced in Equation 5. The \\(Q_{tot} (\\hat{o}, \\hat{a}^m)\\) is calculated for the next time-step by the stale network. Previous researches (Mnih et al. 2015; Liu, Zhu, and Chen 2023) have demonstrated the feasibility and stability of implementing updates in this manner. Additionally, to minimize the difference between J(\\(\\pi\\)) and J(\\(\\pi^{\\theta}\\)) as shown in Equation 2, we calculate the following loss function:\n\\begin{equation}\nL_a(w, \\theta) =\n\\underset{(\\omega,\\theta)}{\\text{arg min }} E\\left[(J(\\pi) - \\sum_t \\gamma_t(Q_{tot} (o, a^m) - R_m(a^m)))^2\\right].\n\\label{eq:9}\n\\end{equation}\nThus, the total loss function for EMAI is:\n\\begin{equation}\nL_{total}(w, \\theta) = L_e(w, \\theta) + \\lambda L_a(\\omega, \\theta),\n\\label{eq:10}\n\\end{equation}\nwhere \\(\\lambda\\) is the weighting term to balance the two loss functions. Algorithm 1 briefly presents our training process."}, {"title": "Experimental Setup", "content": "Multi-Agent Environments Our experiments are conducted on three popular multi-agent benchmarks with different characteristics, selecting two to three environments from each benchmark as follows.\nStarCraft Multi-Agent Challenge (SMAC). SMAC (Samvelyan et al. 2019) simulates battle scenarios in which a team of controlled agents must destroy the built-in enemy team. SMAC is characterized by dense rewards and adversarial tasks. We consider three tasks in this environment which vary in the number and types of units controlled by agents.\nGoogle Research Football (GRF). GRF (Kurach et al. 2020) provides the scenarios of controlling a team of players to play football against the built-in team, characterized by sparse rewards and adversarial tasks. We choose two tasks in GRF, which vary in the number of players and the tactics.\nMulti-Agent Particle Environments (MPE). MPE (Lowe et al. 2017) consists of navigation tasks, where agents need to control particles to reach the target landmarks, which have the characteristics of dense rewards and cooperative tasks. We study two of these tasks, which mainly differ from whether explicit communication is required between the agents.\nBaseline Approaches We implement three popular and state-of-the-art baseline approaches in each multi-agent task to explain the importance of agents. One approach works under the black-box setting, while the other two work under the white-box setting.\nStateMask (Cheng et al. 2023): the state-of-the-art post-training approach that analyzes the importance of the state for the final reward at each time-step. When utilizing StateMask for interpreting the important agent in this paper, we treat the remaining agents as part of the environment and use it to compute the importance of time-step for each agent, thereby representing the importance of each agent at this time-step.\nValue-Based (VB) (Huang et al. 2018): a commonly-used in-training explanation approach for MAS, which represents the MARL work for the credit assignment or value decomposition problem and relates importance to the value function. The value function is learned by addressing the credit assignment, e.g., the Q-value in QMIX (Rashid et al. 2018) or its variants (Wang et al. 2021; Liu, Zhu, and Chen 2023).\nGradient-Based Attribution (GBA) (Srinivas and Fleuret 2021): an approach for the in-training explanation that utilizes the gradients of the output logits, i.e., the log probability log p(action), for explaining each agent's importance."}, {"title": "Fidelity Evaluation", "content": "Evaluation Metric. Our work is able to identify the critical agents for obtaining the final reward at each time-step. Thus the fidelity of the explanation needs to measure the accuracy of the identified critical agents by demonstrating the high influence of these agents. Based on existing work (Cheng et al. 2023; Guo et al. 2021; Yu et al. 2023), one intuitive way to assess fidelity is to randomize the actions of selected agents by the explanation approach and then measure the difference in reward before and after the action manipulation. If the selected agents are indeed critical to the final reward, then randomizing the actions of the critical agents should lead to greater reward variation compared to other agents.\nTherefore, at each time-step, we select the most critical agent based on the explanation approach and randomize its action, while the rest of the agents act according to their policy decisions, thus resulting in the episode reward denoted as Re. The episode reward obtained by the agents' original actions is denoted as Ro. Additionally, the magnitude of reward variation may differ due to varying reward designs across different environments. Therefore, based on the relative reward difference (RRD) introduced in previous work (Yu et al. 2023), we use random selection to normalize the reward variation for each environment, i.e., we randomly select agents as critical ones to change its action, and the episode reward obtained is denoted by Rr. Then the fidelity can be expressed as: RRD = |Re - Ro|/|Rr - Ro|.\nFor each experiment, we perform 500 episodes and compute the mean value of episode rewards. The larger RRD represents better explanation fidelity. Note that if the reward variation is even smaller than the random selection (i.e., RRD is less than 1), then the explanation is extremely inaccurate.\nResult. Figure 2 compares the explanation fidelity of our proposed EMAI with baseline approaches in seven multi-agent tasks. It can be observed that EMAI achieves the highest RRD (i.e., fidelity) in all tasks, with the relative improvement ranging from 11% to 118% compared to baselines. This is because EMAI can accurately recognize the importance of each individual of multiple agents. In contrast, StateMask's lack of effectiveness in the multi-agent setting is due to the fact that it focuses on the importance of a sequence of time-steps rather than on cross-sectional comparisons among agents. VB also has lower fidelity scores compared to EMAI. This is because the value function is learned with the primary goal of guiding the agent to accomplish the task, while explaining the agent is merely a byproduct, leading to lower accuracy. For similar reasons, GBA using log-probability fails to achieve good explanation performance. In addition, in the task with the largest number of agents (SMAC-27m_vs_30m), the RRD of the baseline approach is closer to or even less than 1. It indicates that the critical agents selected by these explanation approaches are similar to, or even worse than, random selection.\nOur proposed EMAI's superior performance compared to the baseline indicates its better interpreting ability. This advantage stems from the causal analysis abilities of counterfactual theory and the MARL approach's capacity to learn complex dependencies (both between agents and across time steps) in MAS. The combination effectively addresses the"}, {"title": "Practicability Evaluation", "content": "Following existing work", "aspects": "understanding policies", "tasks": "SMAC-27m_vs_30m and GRF-counter_attack.\nFigure 3a illustrates a portion of time-steps in a winning episode of the target MAS (red team) in SMAC-27m_vs_30m. All agents are of the same unit type, yet the enemy holds a numerical advantage. A key factor in accomplishing this task is the formation unfolding strategy, which is less intuitive and cannot be easily identified through unit hitting and being hit. EMAI successfully identifies the important agents on the flanks of the team. Initially, these agents maneuver and spread out towards the upper and lower sides, establishing a semi-encirclement. By maneuvering and dispersing their units, the red team can get greater firepower coverage and disperse the enemy's firepower to reduce the damage sustained by each unit. This strategy is"}]}