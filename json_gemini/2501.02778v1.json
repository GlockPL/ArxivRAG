{"title": "ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction", "authors": ["Binyu Zhang", "Zhu Meng", "Junhao Dong", "Shichao Li", "Fei Su", "Zhicheng Zhao"], "abstract": "Survival prediction is a crucial task in the medical field and is essential\nfor optimizing treatment options and resource allocation. However, cur-\nrent methods often rely on limited data modalities, resulting in suboptimal\nperformance. In this paper, we propose an Integrated Cross-modal Fusion\nNetwork (ICFNet) that integrates histopathology whole slide images, ge-\nnomic expression profiles, patient demographics, and treatment protocols.\nSpecifically, three types of encoders, a residual orthogonal decomposition\nmodule and a unification fusion module are employed to merge multi-modal\nfeatures to enhance prediction accuracy. Additionally, a balanced negative\nlog-likelihood loss function is designed to ensure fair training across different\npatients. Extensive experiments demonstrate that our ICFNet outperforms\nstate-of-the-art algorithms on five public TCGA datasets, including BLCA,\nBRCA, GBMLGG, LUAD, and UCEC, and shows its potential to support\nclinical decision-making and advance precision medicine. The codes are avail-\nable at: https://github.com/binging512/ICFNet.", "sections": [{"title": "1. Introduction", "content": "Recently, deep learning has exhibited great potential for various medical\ntasks [1, 2]. In the field of cancer research, survival analysis plays a vital role\nin estimating the death risk of patients in the cancer prognosis. By scrutiniz-\ning the survival durations of patients, researchers endeavor to pinpoint key\ndeterminants and assess the efficacy of therapeutic interventions.\nAn increasing number of deep learning-based methods for survival pre-\ndiction have been proposed [3]. As the gold standard for diagnosing cancers,\nhistopathology whole slide images (WSIs) are widely utilized for survival\nanalysis. Due to the extreme scale of WSIs and the limitation of graphics\nprocessing units (GPU) computing power, multi-instance learning (MIL) is\nusually adopted for WSI-based methods. In parallel, the performance of the\nTransformer model [4] and CLIP [5] in image analysis opens up a path to\nuse attention-based models for survival prediction.\nHowever, although WSIs can provide the morphological information of\ntumor tissue and the hints for cancer diagnosis, the inherent sparse distribu-\ntion of vital information within WSIs and the training methodology of MIL\nlead to sub-optimal model performance. Hence, some researches consider\nto utilize the genomics data to improve the accuracy of survival prediction,\nbecause cancer patients exhibit intrinsic genetic mutations. By conducting\ngenomics analysis of patients, a more precise understanding of their cancer\ncondition can be achieved [6]. As studies have shown that the genes ex-\npression cam reveal the morphological characteristics in histopathology WSI\n[7, 8], some methods [9, 10] attempt to use both histopathology and genomics\ndata. They leverage multi-modal network to extract and fuse the features\nfrom different modality by calculating the similarity score between each histo-\ngenomics data pair. However, histopathology and genomics data are still not\nable to provide enough information for prognosis. In fact, the survival of\npatients depends not only on their clinical diagnosis, but also on variety of\nother factors, such as their physical conditions, demographic characteristics\nand the treatment they receive.\nTherefore, aiming to achieve more precise prognostic predictions with\nvarious information we propose an Integrated Cross-modal Fusion Network\n(ICFNet) to incorporate the demographic data and the treatment protocols.\nICFNet takes four types of data as input, including histopathology WSIs,\ngenomics data, patient information and treatment protocols. As the input\ndata is of different modalities, ICFNet employs different strategies to extract"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Multi-instance learning for medical image analysis", "content": "Due to the inherent gigapixel nature of WSIs, MIL has been widely\nadopted as a bag-level supervised approach. Impressive results had been\nachieved. For example, AttnMIL [12] utilized the attention mechanism to\naggregate patch features, while Raju et al. [13] first constructed the patch\ninstances into graphs with distance and cluster information, then proposed\na graph attention network to fuse patch features for accurate cancer stag-\ning. To overcome the challenge of training patch feature extractors, C2C [14]\nproposed an end-to-end framework. DSMIL [15] introduced a self-supervised\ncontrastive learning method with a masked non-local block designed to ag-\ngregate the instances. TransMIL [16] co-related Transformer [4] and MIL into\nthe framework. To explore the enlarged patch bags in MIL, DTFD-MIL[17]\nintroduced the concept of pseudo-bags and proposed a double-tier MIL frame-\nwork to effectively use intrinsic features. To relieve the overfitting problem\ncaused by limited samples, SH-Transformers [18] introduced a sparse atten-\ntion mechanism to learn the hierarchical WSI representation. Attempting to\nfind the critical information among thousands of patches, dMIL-Transformer\n[19] proposed a double Max-Min MIL strategy to select the suspected top-\nK positive patches to further improve the inference. Considering different\nmagnifications of WSIs were likely to have positive contribution for diagno-\nsis, TGMIL [20], CS-MIL [21] and Tsiknakis et al. [22] incorporated three\ndifferent magnifications and combined features with different mechanisms.\nTo investigate the interpretability, SI-MIL [23] provided an interpretable-by-\ndesign MIL method. These impressive approaches highlighted the effective-\nness of MIL in analyzing WSIs and consequently were facilitated to various\ndownstream medical tasks, including survival prediction."}, {"title": "2.2. Cancer survival prognosis", "content": "Prognosis, as one of the most critical tasks in clinical practice, aims to\npredict patient survival duration and prevent over-treatment or wastage of\nmedical resources as well. Consequently, numerous deep learning-based ap-\nproaches for patient prognosis have been proposed. DeepAttnMISL [24] in-\ntroduced both siamese MI-FCN and attention-based MIL pooling to effi-\nciently learn imaging features and then aggregated WSI-level information to\npatient-level. Aiming to better aggregate instance-level histology features to\nmodel local- and global-level topological structures, Patch-GCN [25] treated\nWSIs as 2D point clouds and leveraged GCN for feature fusion. Sandarenu\net al. [26] attempted to cluster the patch features and fuse them with atten-\ntion modules. Surformer [27] was proposed to quantify specific histological\npatterns via bag-level labels. As a distinctive characteristic of cancer pa-\ntients, their gene expression undergo alterations. Consequently, SNN [6]\nemployed genomics information to predict patient survival. To give model\nmore supervision and enhance the model performance, LNPL-MIL[28] and\nHistMIMT [29] introduced multi-tasks, such as diagnosis, genomics expres-\nsion prediction and survival prediction. However, these methods typically\nconsider information from only a single modality or limited data, resulting\nin a partial understanding of the patient, consequently, prognostic models\ncould only achieve sub-optimal results.\nTo gain a more comprehensive understanding of patient conditions, sev-\neral existing approaches took multi-modal data into account, and leveraged\ndiverse information sources to integrally predict patient survival time. As the\nWSI and genomics data are critical for cancer diagnosis, Pathomic [30] en-\ncoded WSI patches, cell spatial graphs and genomic profiles and fused the fea-\ntures for survival prediction. However, there was no interaction between the\nextracted features. Therefore, MCF[31] proposed a hierarchical predictive\nscheme to reliably link the multi-modality features and multiple classifiers,\nwhile MCAT [9] proposed a genomic-guided co-attention module to assist\nthe network in learning genomics-related features from WSIs. Although the\nco-attention improved the model performance, it focused on only dense local"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overview and problem formulation", "content": "MIL strategy is also adopted for the network training and data bags are\nformulated in advance.\nWSI bags formulation. As the tissue in WSI is likely to be sparsely\ndistributed, a foreground segmentation algorithm [3] is applied to crop the\ntissue from the background. Then the cropped images are split into patches\nand formulate as a WSI bag:\n$X_i = \\{x_{in}^i\\}_{n=1}^{N_{p,i}}$"}, {"title": "3.2. Network", "content": "To extract and fuse above four types of information, we propose ICFNet,\nshown in Fig.1, which consists of three components: a feature extractor, a\nmulti-modal feature interaction and a classification module. The training\nand inference share the same algorithm flow."}, {"title": "3.3. Loss functions", "content": "Due to the presence of censored data in prognostic datasets, NLLLoss\n[11] is widely adopted in prognostic tasks,\n$L_{nll} = -c_i \\cdot log(S_{surv,i}(y_i))\n-(1 - c_i) \\cdot log(S_{surv,i}(y_i - 1))\n- (1 - c_i) \\cdot log(S_{haz,i}(y_i))$\nin which $c_i$ represents the censorship of the patient, $S_{surv,i}(y_i)$ denotes the\nsurvival probability of the patient till the ground truth survival time bin $Y_i$,\nand $S_{haz,i}(y_i)$ stands for the hazard score of the patient in the $y_i$ time bin.\nAs for $S_{surv,i}(Y_i)$, it is a cumulative probability, which can be expressed as\n$S_{surv,i}(Y_i) = \\prod_{n=0}^{Y_i} (1 - S_{haz,i,n}),$\nin which $S_{haz,i,n}$ is the predicted hazard score for the $i$th patient in $n$th time\nbin.\nHowever, because $S_{haz,i}$ is a value between 0 and 1, $S_{surv,i}$ tends to be\ndisproportionately larger for higher values of $y_i$. This inherently causes the\nnetwork to focus more on instances with larger $y_i$ (longer survival times) while\ninadequately training on patients with smaller $y_i$ (shorter survival times).\nTo address this issue, we propose Balanced Negative Log-Likelihood Loss\n(BNLLLoss) to balance the weights between different classes by incorporating\nthe loss of $S_{haz,i}$ into terms of $S_{surv,i}$ in NLLLoss,\n$L_{bnll} = -c_i \\cdot log (S_{surv,i}(Y_i) \\cdot (1 - S_{haz,i}(Y_i))^{N_b-1-Y_i})\n- (1 - c_i) \\cdot log (S_{surv,i}(Y_i - 1) \\cdot (1 - S_{haz,i}(Y_i - 1))^{N_b-1-Y_i})\n- (1 - c_i) \\cdot log (S_{haz,i}(Y_i)),$\nwhere $L_{bnll}$ represents the BNLLLoss function. Via BNLLLoss, each class\ncan be trained with same weights. As a uni-modal classifier is utilized for\neach feature, BNLLLoss is also applied to supervise the uni-modal classifiers.\nMoreover, to ensure the redundancy can be reduced, a cosine similarity\nloss is adopted in the ROD module. Hence, the total loss function can be\nrepresented as\n$L_{total} = L_m + \\alpha \\cdot (L^p_u + L^{pg}_{cos} + L^g_u + L^{pt}_{cos} + L^t_u + L^u_{cos}),$\nwhere $L_m$ represents the loss for the multi-modal classifier, each $L_u^x$ denotes\nthe loss for the corresponding uni-modal classifier, $L^{px}_{cos}$ and $L^{tx}_{cos}$ represent the\ncosine similarity between $f_i^{p,x}$ and $f_i^o$ as well as $f_i^o$ and $f_i^x$, respectively,\n$\\alpha$ and $\\beta$ are hyper-parameters which are set as 0.1 to balance the model\ntraining in our experiments.\nFinally, as the training and inference phase share the same procedure, we\ncan obtain the risk scores by adding up all the $S_{surv,i}$\n$S_{r,i} = \\sum_{n=1}^{N_b} S_{surv,i}(n),$\nwhere $S_{r,i}$ is the predicted risk score for the $i$th patient. Patients with longer\nsurvival times are associated with lower risk, and vice versa."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets and evaluation metrics", "content": "To evaluate the performance of ICFNet, a series of experiments are con-\nducted over five open-source cancer datasets in TCGA, including Bladder"}, {"title": "4.2. Implementation details", "content": "To ensure a fair comparison, we follow MOTCat [10] and conduct all\nexperiments. To formulate the WSI patch bags, OTSU method is firstly\napplied to segment tissue regions, and then non-overlapping patches with the\nsize of 256 \u00d7 256 are extracted from the tissue region over 20\u00d7 magnification.\nAs for the image encoder, an ImageNet-pretrained ResNet50 is adopted to\nextract the WSI patch features before the training stage. Note that, to\nprevent the pre-trained model from extracting excessively high-dimensional\nfeatures, we exclusively select the output from layer3 as the patch features.\nSNN [6] is utilized as the genomic encoder to encode the genomic bag. CLIP-\nAdapter [43] is adopted as the text encoder and a two-layer MLP is adopted to\nencoder tensorized data. The CLIP-Adapter consists of a frozen CLIP model\nand an adapter for fine-tuning. In the training section, Adam optimizer is"}, {"title": "4.3. Experimental results", "content": "Comparison with the state-of-the art methods. Following the\ndataset partition in MOTCat [10], we conduct the 5-fold experiments on the\nfive datasets. To have fair comparsion with existing approaches, we repro-\nduce some of them with the same setting and compare with the performance\nof ICFNet. As shown in Table 2, ICFNet achieves new state-of-the-art scores\namong the survival prediction approaches. On the five datasets, our ICFNet\noutperforms MOTCat [10] by 5.29% C-index score on average. Specifically,\nICFNet gains the improvement of 6.48% on BLCA, 11.40% on BRCA, 1.43%\non GBMLGG, 4.04% on LUAD, and 3.28% on UCEC.\nBy comparing the performance of models with different numbers of input\nmodalities, it can be observed that the more modalities are included, the bet-\nter performance the model can achieve. To fairly compare the performance\nof ICFNet with existing methods, demographic and treatment information\nare incorporated into MOTCat [10], denoted as MOTCat+Text. Specifically,\nthe text & MLP encoders are introduced to encode the relevant information\nfrom demographic and treatment data, obtaining text features. However,\nthese text features are not interacted with any other modality features. In-\nstead, they are directly concatenated with other features before the classifier\nand used for classification. The performance of MOTCat+Text shows the\neffectiveness of the demographic and treatment for prognosis. However, due\nto the utilization of different encoders for each modality and the lack of in-\nteraction between the extracted features, MOTCat+Text can only obtain\nsub-optimal performance.\nTo better demonstrate the statistical difference of the ICFNet predictions\nfor patients, the Kaplan-Meier (KM) survival curves are visualized for differ-\nent approaches. Specifically, patients are divided into two groups, named as\nlow-risk and high-risk, based on the predicted risk scores. Then, as shown\nin Fig.4 the statistics of ground truth survival time are presented for each\ngroup. Moreover, the Logrank test is conducted to measure the statistically"}, {"title": "4.4. Visualization", "content": "To demonstrate the interpretability of the model, GradCAM [47] is uti-\nlized to visualize the class activation maps (CAM) of certain slides. As shown\nin Fig.5, the red region represents the area contributing the most to the pa-\ntient's risk score, while the blue region indicates the opposite. By observing\nthe morphologies of tissue in these red regions, we can summarize the proto-\ntypes that lead to high patient risk, facilitating rapid analysis by experts."}, {"title": "4.5. Discussion", "content": "The value of ICFNet for clinical decision-making. Different from\nprevious prognostic models that rely solely on histopathology or genomic\nexpression features, ICFNet integrates patient demographics and treatment\nprotocols to provide a more comprehensive evaluation, thereby enhancing\nprognostic performance. It's important to note that for clinicians, a patient's\nexisting information, such as histopathology, genomics, and demographics,\nis fixed, whereas treatment protocols are variable. This means that dur-\ning clinical decision-making, clinicians can adjust the text input for ICFNet\nto observe differences in patient prognosis, ultimately identifying treatment\nplans that yield better prognostic outcomes and provide a more quantifiable\nbasis for treatment decisions.\nTo illustrate the impact of different treatment protocols on prognosis,\nFig.6 shows the range of prognostic variations for Fold 0 from the related\ndatasets when different treatment options are applied. Fig.6(a) presents\nthe survival predictions for each patient under various treatment methods\nas given by ICFNet. Fig.6(b) shows the probability density estimates ob-\ntained by fitting the survival prediction results to a normal distribution. In\nthis figure, the green line represents the survival predictions for the actual\ntreatment protocols used in the dataset, while the orange and blue lines\nrepresent the best and worst prognostic outcomes under different treatment\nprotocols, respectively. Fig.6 demonstrates, both locally and globally, that\ntreatment protocols significantly impact prognosis. This impact indicates\nthat the treatment decisions made at the time, possibly constrained by pa-\ntient preferences and medical conditions, might not be optimal for the best\nprognostic outcome. However, clinicians can use ICFNet to directly observe\nsurvival estimates, making clinical decisions more quantifiable and efficient.\nReasons for discrepancy between ICFNet's survival predictions\nand actual survival times. Plenty of factors affect the accuracy of survival\npredictions. Intrinsic factors, such as individual heterogeneity, can influ-\nence disease progression rates and the effectiveness of medical interventions.\nAdditionally, patient-specific conditions like comorbidities and psychological\nhealth may impact the accuracy of prognosis for a single disease. Extrinsic\nfactors, including the patient's living environment and adherence to treat-\nment protocols, also play a substantial role in creating variability in survival\nestimates. In summary, survival prediction is inclined towards higher prob-\nability events when dealing with large sample sizes, aligning with the core\nprinciples of data-driven artificial intelligence algorithms. Although there is\nstill some discrepancy compared to actual survival times, ICFNet, which ac-\ncounts for a broader range of factors, has achieved state-of-the-art prognostic"}, {"title": "5. Conclusion", "content": "In this paper, a multi-modal prognosis framework is proposed, named as\nICFNet, which achieves the state-of-the-art performance on multiple modal-\nity datasets. ICFNet incorporates four types of information, including histopathol-\nogy WSIs, genomics expression, patient demographic and treatment profiles.\nFirstly, the MIL strategy is used to formulate the bags for training, while\ntext templates and prompts are designed for demographic and treatment in-\nformation. Secondly, three different encoders are adopted to extract features\nfrom different modalities. Then, the features are interacted with OT based\nmodules to explore the relationship between the features. To reduce redun-\ndancy among modality and enhance modality-specific features, ROD module\nis proposed. Furthermore, we design a unification fusion module to align fea-\ntures and train the network using a dense loss function. Finally, BNLLLOSS\nis introduced to ensure the network fairly treats with every patient. The\nexperimental results indicate the effectiveness of our ICFNet, which can con-\nduct a prior assessment of the treatment administered to patients, thereby\navoiding over-treatment or wastage of resources."}]}