{"title": "Interactions Across Blocks in Post-Training Quantization of Large Language Models", "authors": ["Khasmamad Shabanovi", "Lukas Wiest", "Vladimir Golkov", "Daniel Cremers", "Thomas Pfeil"], "abstract": "Post-training quantization is widely employed to reduce the computational demands of neural networks. Typically, individual substructures, such as layers or blocks of layers, are quantized with the objective of minimizing quantization errors in their pre-activations by fine-tuning the corresponding weights. Deriving this local objective from the global objective of minimizing task loss involves two key simplifications: assuming substructures are mutually independent and ignoring the knowledge of subsequent substructures as well as the task loss. In this work, we assess the effects of these simplifications on weight-only quantization of large language models. We introduce two multi-block fine-tuning strategies and compare them against the baseline of fine-tuning single transformer blocks. The first captures correlations of weights across blocks by jointly optimizing multiple quantized blocks. The second incorporates knowledge of subsequent blocks by minimizing the error in downstream pre-activations rather than focusing solely on the quantized block. Our findings indicate that the effectiveness of these methods depends on the specific network model, with no impact on some models but demonstrating significant benefits for others.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) [Zhang et al., 2022, Touvron et al., 2023, Jiang et al., 2023] have transformed the field of natural language processing, achieving impressive results on various challenging language tasks [Wei et al., 2022, Bubeck et al., 2023]. Nevertheless, these models, often containing billions of parameters, typically demand substantial computational power. Post-training quantization (PTQ) has emerged as a practical method for reducing the size and computational requirements of LLMs without the need for retraining and requiring only a small set of calibration data [Frantar et al., 2022, Xiao et al., 2023, Lin et al., 2024, Shao et al., 2024]. By converting high- precision weights and activations to lower-precision representations, PTQ enables the deployment of LLMs on resource-constrained devices, expanding their applicability in real-world scenarios. In this study, we focus on weight-only quantization, as model weights are the primary factor impacting memory bandwidth and, consequently, the runtime of LLM inference [Kim et al., 2023]."}, {"title": "2 Methods", "content": "In this section, we revisit the derivation of the local objective from the global one and define quantization. Building upon the visualizations in Figure 1, we formally introduce SB-PTQ, LA-PTQ, and MB-PTQ."}, {"title": "Global to Local Objective", "content": "Finding the optimal quantization can be formulated as the following global optimization objective that minimizes the error in the task loss caused by quantization:\narg min E [L(x, y, w + \u2206w) \u2013 L(x, y, w)],\nAw\n(1)\nwhere L is the task loss, x are the inputs, y are the true labels, w is the flattened vector of all model weights, and Aw is the perturbations to the weights introduced by quantization. The expectation is over x and y.\nNagel et al. [2020] approximated this objective by a second-order Taylor expansion around w where H(w) is the Hessian of the task loss with respect to weights w and the first-order term is ignored since the model is assumed to have converged:\narg min E [AWH(W)Aw].\nAw\n(2)\nTo obtain the local, layer-wise objective two simplifications are applied. First, the layers are as- sumed to be mutually independent, resulting in a block-diagonal structure for the Hessian matrix. Consequently, each layer l can be optimized separately in Equation 2:\narg min E [Aw(e) H(we) w()].\nAw(e)\n(3)\nHowever, this objective remains computationally challenging, as calculating H(w()) requires com- puting the Hessian \u22072(e) L of the task loss with respect to the pre-activations z(l):\nH(w()) = E [x(l-1)x(-1)\u2297\u22072z(e)L ],\n(4)\nwhere \u2297 denotes the Kronecker product. To simplify this objective, we further assume this Hessian to be a constant diagonal matrix, i.e. \u22072(e) L = c \u00d7 \u00d7 I, effectively ignoring the knowledge of downstream layers and the task loss. Hence, we arrive at the local, layer-wise optimization objective that minimizes the error in the pre-activations caused by quantization:\narg min E [Aw()x(-1)x(-1) Aw]\nAw(l)\n= arg min E\nAw(l)\n[(Aw(6)x(-1))\u00b2].\n(5)\nLi et al. [2021] extended the previous objective to encompass blocks containing any number of layers, demonstrating that the global objective can be effectively approximated by locally minimizing the error in block outputs."}, {"title": "Weight Quantization", "content": "Following Cheng et al. [2023] we quantize the high-precision weights W to b-bit precision by\nW\n= s\u00b7clip (W/s +v,0,2b-1),\n(6)\nwhere V is a learnable parameter to adjust rounding, [\u00b7] is the round-to-nearest (RTN) operation, and clip(x, n, m) restricts the value of x to lie within the range [n, m]. The scaling factor is defined as\nmax(W) \u00b7 \u03b1 - min(W) \u00b7 \u03b2\ns =\n2b-1\n,\n(7)\nwhere \u03b1, \u03b2 \u2208 [0, 1] are learnable parameters."}, {"title": "LA-PTQ", "content": "In LA-PTQ, the learnable parameters \u03b1, \u03b2 and V of the k-th transformer block are optimized, using the outputs of the k + n-th block as reconstruction target. To facilitate the discussion, we refer to n as the number of look-ahead blocks. In practice, if the network has L blocks, the reconstruction target is set to block min(k + n, L), though we omit this detail here for simplicity. In this optimization setting, only the parameters of the block k are tuned while the parameters of the other blocks are kept frozen. Let (.)(k) denote the parameters of the k-th block. Then, the optimization target is expressed as follows:\narg min E [T(X, W(),..., W(k+n)) - T(X, W(k), Wk+1,..., W(k+n)||F],\na(k),\u03b2(k),(k)\n(8)\nwhere T(X, W(i), ..., W(i+n)) denotes the transformation applied to the input X over n transformer blocks with their respective weights W(i), ..., W(i+n). The expectation is over the input X and ||\u00b7 ||F denotes the Frobenius norm. Starting with the first block (k = 1), we sequentially optimize one block at a time, progressively quantizing the neural network's weights. For each block k, the output from the already quantized part of the network serves as the input X. Single block PTQ is the special case of n = 0."}, {"title": "MB-PTQ", "content": "In MB-PTQ, the learnable parameters of n blocks are jointly optimized. More formally, for blocks k to k + n - 1 to be optimized this is expressed as follows:\narg min E [||T(X, W(),..., W(k+n-1)) - T(X, W(),.., W(k+n-1)||F],\n\u03b1,\u03b2,\u03bd\n(9)\nwhere \u03b1, \u03b2, and V denote the parameters of all n blocks. In contrast to LA-PTQ, after quantizing the parameters of the current n blocks, we move on to the next set of n blocks, without overlap between the sets of blocks. This approach differs from that of Ding et al. [2023], who permit consecutive sets of n blocks to overlap, thereby optimizing the overlapping blocks multiple times."}, {"title": "3 Experiments", "content": "In this section, we first describe the details of our experimental setup, followed by a comparison of our proposed approaches, LA-PTQ and MB-PTQ, against the baseline method, SB-PTQ."}, {"title": "3.1 Setup", "content": "We use the abbreviations LA-n and MB-n to refer to LA-PTQ and MB-PTQ with n blocks, respec- tively. LA-n refers to the setting where one block is fine-tuned with n 1 look-ahead blocks and MB-n refers to the setting where n blocks are jointly fine-tuned. Note that in both cases there are a total of n blocks involved in each optimization round (compare Figure 1b to 1c). As reference, we also provide the accuracy for the full precision (FP) and round-to-nearest (RTN) cases. In the RTN scenario, weights are quantized, but not fine-tuned.\nWe evaluate our methodology on Llama-2-7B [Touvron et al., 2023], Mistral-7B-v0.1 [Jiang et al., 2023], OPT-6.7B, and OPT-125M [Zhang et al., 2022]. Our primary metric is the average accuracy across 11 zero-shot tasks, including HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2020], LAMBADA [Paperno et al., 2016], TruthfulQA [Lin et al., 2022], OpenBookQA [Mihaylov et al., 2018], BoolQ [Clark et al., 2019], RTE [Dagan et al., 2010], ARC- Easy, ARC-Challenge [Clark et al., 2018], and MMLU [Hendrycks et al., 2021], which we compute by using the 1m-evaluation-harness [Gao et al., 2024]. We report the average accuracy across these tasks in the main body of the paper and provide the individual accuracy results for each task in Appendix B.\nIn general, our experimental setup follows that of Cheng et al. [2023] if not specified otherwise. Quantization is limited to the weights of the linear layers in transformer blocks excluding the embedding and the final linear layer. Weights are quantized down to 4 bits, where each group of 128 weights share a learnable scaling factor (see Equation 6 and 7). Calibration data is randomly sampled using the same seed from the publicly available pile-10k dataset, which consists of the first"}, {"title": "3.2 Results", "content": "Evaluation on Zero-Shot Tasks We evaluate MB-PTQ and LA-PTQ with an increasing number of blocks to capture the impact of progressively introducing more cross-block dependencies and incorporating knowledge from blocks further ahead. Specifically, we fine-tune Mistral-7B-v0.1, Llama-2-7B, and OPT-6.7B using LA-PTQ with up to 3 look-ahead blocks and MB-PTQ with substructures of up to 4 blocks. For OPT-125M, as the end-to-end optimization of this model fits into the memory of a single GPU, we iterate over all possible configurations.\nWe observe that the effect of LA-PTQ and MB-PTQ on the task accuracy depends on the model (see Figure 2). While for Mistral-7B-v0.1 and OPT-6.7B the accuracy of both LA-PTQ and MB-PTQ does not improve compared to SB-PTQ, Llama-2-7B shows an improvement with an increasing number of blocks that saturates at 2 blocks. The absence of improvement for OPT-6.7B can be likely explained by SB-PTQ already being sufficient to recover the full-precision performance. For OPT-125M, we observe that both LA-PTQ and MB-PTQ achieve higher accuracy compared to the baseline SB-PTQ for certain block configurations, with LA-PTQ showing more consistent improvement than MB-PTQ. However, the overall differences in accuracy for this model are small (compare FP to RTN in Figure 2d), and these trends are not statistically significant. In contrast to the other models, for OPT-125M, we do not significantly benefit from fine-tuning single blocks in isolation (compare SB-PTQ to RTN in Figure 2b and 2d).\nAblations on Hyperparameters We validate the choice of our learning rate on the example of MB-3 and LA-4 (for details, see Table A.1). Generally, we observe a low sensitivity of the accuracy on the learning rate. However, fine-tuning Mistral-7B-v0.1 with MB-3 using a learning rate of"}, {"title": "4 Discussion", "content": "We investigated how incorporating knowledge of subsequent transformer blocks and interactions across blocks effect the fine-tuning of quantized weights in LLMs. We found that the effectiveness of these approaches is model-specific and cannot be generalized across all models. While we do not observe improvements for the Mistral-7B-v0.1, OPT-125M and OPT-6.7B models, the Llama-2-7B model shows enhanced task accuracy. However, it is important to note that including more blocks in the optimization process increases computational costs. Further research is needed to explore how the effectiveness of our methods depends on different network models and to extend this investigation to larger LLMs.\nIn both LA-PTQ and MB-PTQ, the reconstruction loss is applied to the output of downstream blocks. This increases the complexity of the optimization landscape due to the additional blocks and their inherent non-linearities. While MB-PTQ may alleviate this increased complexity through a greater number of free parameters and cross-block optimization compared to LA-PTQ, it does not show superior performance (see Figure 2). This holds true even after ensuring that overfitting, either from a small calibration dataset or excessive training iterations, does not occur (see Figure 3). While we observe a significant performance improvement with LA-2 compared to LA-1 for Llama-2-7B (see Figure 2b), further increasing n to extend the look-ahead does not yield additional benefits. Therefore, setting n = 2 appears to offer a favorable balance between enhanced accuracy and computational efficiency.\nExtending the fine-tuning to the full network, in combination with the original training pipeline and dataset, should ideally yield the best task accuracy. However, in our study, increasing the number of calibration samples does not enhance performance and may even be detrimental (see Figure 3 left). This could indicate a co-variate shift, i.e. a mismatch between the distributions of the calibration and test datasets [Moreno-Torres et al., 2012]. On the other hand, increasing the number of fine-tuning iterations improves the results (see Figure 3 right). Nevertheless, this substantial increase in computational demands, especially in comparison to the 200 iterations used by Cheng"}]}