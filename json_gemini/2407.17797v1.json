{"title": "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models", "authors": ["Haonan Zheng", "Xinyang Deng", "Wen Jiang", "Wenrui Li"], "abstract": "With Vision-Language Pre-training (VLP) models demonstrating powerful multimodal interaction capabilities, the application scenarios of neural networks are no longer confined to unimodal domains but have expanded to more complex multimodal V+L downstream tasks. The security vulnerabilities of unimodal models have been extensively examined, whereas those of VLP models remain challenging. We note that in CV models, the understanding of images comes from annotated information, while VLP models are designed to learn image representations directly from raw text. Motivated by this discrepancy, we developed the Feature Guidance Attack (FGA), a novel method that uses text representations to direct the perturbation of clean images, resulting in the generation of adversarial images. FGA is orthogonal to many advanced attack strategies in the unimodal domain, facilitating the direct application of rich research findings from the unimodal to the multimodal scenario. By appropriately introducing text attack into FGA, we construct Feature Guidance with Text Attack (FGA-T). Through the interaction of attacking two modalities, FGA-T achieves superior attack effects against VLP models. Moreover, incorporating data augmentation and momentum mechanisms significantly improves the black-box transferability of FGA-T. Our method demonstrates stable and effective attack capabilities across various datasets, downstream tasks, and both black-box and white-box settings, offering a unified baseline for exploring the robustness of VLP models.", "sections": [{"title": "1 Introduction", "content": "ViT provides an effective Transformer-based encoder for the visual modality [14], ensuring the feature extraction of multimodal input through a unified encoding manner, significantly advancing the Vision-and-Language tasks [16, 1, 47, 52]. Various VLP models [23, 51, 45, 40] continually improve performance in V+L downstream tasks through diverse pre-training tasks and architectural designs [2, 45, 20, 27, 29]. However, the previous research in unimodal fields such as Computer Vision (CV) and Natural Language Processing (NLP) highlights the vulnerability of neural networks to adversarial attacks [15, 24]. Although adversarial robustness, particularly in CV, has been extensively explored in terms of attack strategies [4, 36], defence mechanisms [34], and transferability [12, 46], the study of adversarial robustness in VLP models remains challenges [26, 54, 55, 33]. Our study aims to develop a unified architecture to explore commonalities between multimodal and unimodal tasks from the perspective of adversarial attacks. In other words, we seek to bridge the gap, allowing rich findings in unimodal adversarial robustness to be directly applied to the multimodal scenario.\nThe first question we consider is \"Which modality should be paid more attention?\" We primarily focus on perturbations in the image modality, with perturbations in the text modality serving as orthogonal (1) Semantic consistency: Visual adversarial examples maintain semantic consistency, i.e., noise addition within reasonable limits doesn't change human comprehension. Conversely, text adversarial examples risk semantic distortion, potentially introducing spelling errors. (2) Differentiability: Image inputs are continuous and differentiable, unlike text tokens which are discrete and non-differentiable making text-only attacks less effective. (3) Accessibility: In real-world scenarios, text often serves as the primary means of interaction between users and AI models, with limited opportunities for attackers to modify user-generated text. In contrast, models can automatically acquire image data, simplifying the process for attackers to introduce perturbations. In fact, [33] also primarily focuses on enhancing attack strategies in the visual modality to improve adversarial transferability and [55] does not involve text attacks.\nThe second question is \"How to unify multimodal and unimodal scenarios in exploring adversarial robustness?\" We conceptualize image adversarial attacks as a feature-guided process. For unimodal models which primarily learn to understand images through detailed annotation information (such as category labels), attacking an image involves steering its embedding away from the feature vector linked to its correct annotation [15]. This deviation induces a biased comprehension of the image within the network. Alternatively, the image embedding can be guided closer to the feature vector associated with an incorrect annotation, thereby leading the network to make a predetermined error [25]. In the multimodal scenario, models are encouraged to understand images from raw"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Unimodal Adversarial Attack", "content": "From an access perspective to the model, unimodal attacks can be divided into white-box and black-box attacks. In the black-box scenario, due to the target network's opaque weights, attackers typically conduct white-box attacks on an accessible source network, then transfer the adversarial examples to the target network. Therefore, the attack's transferability is also crucial.\nWhite-box Attack. Based on how to constrain perturbation, adversarial attacks on the visual modality can generally be divided into two categories. (1) Global attacks typically involve perturbing all pixels of an image, usually constraining the distance between adversarial and original images based on $l_\\infty$, $l_2$, or $l_1$ norms. Representative methods include FGSM [15], PGD [34], APGD [10], CW [4], etc. (2) Patch attacks, which confine the perturbation to a small area, such as 2% of the image, and allow unrestricted modification of image pixels within that area. Representative methods include LaVAN [19] and Depatch [7]. Since patch attacks are more practical, physical world attacks are usually based on this form. In the text domain, due to the discrete nature of text data, such attacks typically involve subtle modifications to the original text, such as replacing synonyms, inserting additional words, or adjusting sentence structure, without significantly altering the meaning of the text. A representative method is BertAttack [24].\nBoosting Transferability. Enhancing the transferability of adversarial attacks is essentially a generalization problem. The two main approaches to solving the generalization issue are data augmentation and improving the optimization algorithm, thus dividing transfer attack methods into two categories. (1) Typical methods that boost transferability through data augmentation, such as DI [49] (Diverse Inputs), TI [13] (Translation Invariant) and SI [30] (Scale Invariant). (2) Typical schemes that improve optimization algorithm, such as MI [12] (Momentum Iterative), NI [30] (Nesterov Iterative), VMI [46] (Varied Momentum Iterative), VNI [46] (Varied Nesterov Iterative)."}, {"title": "2.2 Multimodal Adversarial Attack", "content": "This subsection discusses relevant VLP models and multimodal adversarial attack methods.\nVLP Models. VLP models based on different combinations of pre-training tasks can be roughly divided into three categories. (1) Aligned models: CLIP [40] contains two unimodal encoders to align multimodal embeddings based on Image-Text Contrastive (ITC) loss. (2) Fused models by matching: ViLT [20] introduces both Image-Text Matching (ITM) and Masked Language Modeling (MLM) pre-training for V+L tasks. Models like ALBEF [23], TCL [51], BLIP [22], and VLMo [3] build on it, first aligning multimodal features using ITC loss, then fusing cross-modal features using ITM and MLM losses. (3) Fused models by Masked Data Modeling (MDM): BEIT [2] and BEiTV2 [38] propose and improve Masked Image Modeling (MIM) loss. BEiT3, based on it, first aligns multimodal features using ITC loss, then fuses cross-modal features using MIM, MLM, and Masked Language-Vision Modeling (MLVM) losses.\nMultimodal Attack. Attacking VLP models is a novel topic. Existing work has provided valuable insights. Co-Attack [54] designs general optimization objectives based on different embeddings (unimodal or multimodal) and experimentally demonstrates that using text attacks or image attacks alone is not as effective as using both in combination, providing a general baseline for subsequent works. SGA [33] points out that improving the diversity of multimodal"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Feature Guidance", "content": "An image feature extractor $\\mathbb{E}$ (e.g., an image contrastive representation encoder [18, 5, 17] or a VLP model's visual encoder) projects the image into a feature vector for various visual tasks like image classification and object detection. Without regarding the subsequent usage, the most intuitive approach to generate an adversarial example $x'$ for an image $x$ is to encourage the feature vectors $\\mathbb{E}(x')$ and $\\mathbb{E}(x)$ to be as distant as possible [54]. This universal strategy is termed \"Feature Deviation Attack\" (FDA), which involves maximizing the loss function:\n$\\mathcal{L}_{dev} = -\\mathbb{E}(x') \\cdot \\mathbb{E}(x).$ (1)\nwhere $\\cdot$ represents the dot product of vectors, $\\mathbb{E}(x) \\in \\mathbb{R}^d$.\nAssuming that in the embedding space, there exists a set of guiding vectors $\\mathbb{W} = \\{\\omega_i\\}_{i=1}^m, \\omega \\in \\mathbb{R}^d$, and there is a set of guiding labels $\\mathbb{Y} = \\{y_i\\}_{i=1}^n, y \\in \\{1, 2, ..., m\\}$ specifying that $\\mathbb{E}(x')$ should be distant from the guiding vectors $\\{\\omega_{y_i}\\}_{i=1}^n \\in \\mathbb{W}$. We refer to this strategy as \"Feature Guidance Attack\" (FGA). To realize the above concept, we need to maximize the loss function:\n$\\mathcal{L}_{gui} = \\sum_{i=1}^n \\log \\frac{\\exp(\\mathbb{E}(x') \\cdot \\omega_{y_i})}{\\sum_{j=1}^m \\exp(\\mathbb{E}(x') \\cdot \\omega_j)}.$ (2)\nwhere $\\exp(\\cdot)$ represents the exponential function with Euler's number e as the base, and $\\log(\\cdot)$ stands for the logarithm to the base e. Based on $\\mathcal{L}_{gui}$ or $\\mathcal{L}_{dev}$, we can apply the PGD process [34], gradually pushing the clean example $x$ along the gradient direction to maximize the loss function, ultimately obtaining the adversarial example $x'$. By the chain rule of gradients, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbb{E}(x')} \\frac{\\partial \\mathbb{E}(x')}{\\partial x'}$ represents the direction of perturbation added to the input example. While we focus on $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbb{E}(x')}$, which represents the movement direction of the feature vector:\n$\\frac{\\partial \\mathcal{L}_{dev}}{\\partial \\mathbb{E}(x')} = -\\mathbb{E}(x)$ (3)\n$\\frac{\\partial \\mathcal{L}_{gui}}{\\partial \\mathbb{E}(x')} = \\sum_{i=1}^n \\frac{\\exp(\\mathbb{E}(x') \\cdot \\omega_{y_i})}{\\sum_{k=1}^m \\exp(\\mathbb{E}(x') \\cdot \\omega_k)} \\cdot \\omega_{y_i} + \\sum_{i=1}^n \\omega_k$ (4)\nIt can be observed that feature deviation loss promotes the movement of $\\mathbb{E}(x')$ towards $-\\mathbb{E}(x)$, which means moving away from $\\mathbb{E}(x)$. While, Regarding the first term of $d\\mathcal{L}_{gui}/d\\mathbb{E}(x')$, it encourages $\\mathbb{E}(x')$ to move away from the guiding vectors $\\{\\omega_{y_i}\\}_{i=1}^n$, and assigning equal weight 1/n to each of them. The second term encourages $\\mathbb{E}(x')$ to approach the guiding vector $\\omega_k \\in \\mathbb{W}$, with a weight of $\\exp(\\mathbb{E}(x')\\cdot \\omega_k)/\\sum_{j=1}^m \\exp(\\mathbb{E}(x') \\cdot \\omega_j)$, which means the closer $\\mathbb{E}(x')$ is to a guiding vector, the greater the weight assigned to it. Due to the presence of the first term, $\\mathbb{E}(x')$ is far from $\\{\\omega_{y_i}\\}_{i=1}^n$, resulting in the weight of $\\omega_{y_i}$ being almost zero in the second term."}, {"title": "3.2 Attacking after Fuse", "content": "In this scenario, we focus on the fused embedding $\\mathbb{E}_m(\\mathbb{E}_v(v), \\mathbb{E}_t(t))$. For different V+L downstream tasks, it needs to be fed into different subsequent models which can be uniformly understood as comprising a projector P and a linear classification head h. P projects the fused embedding into a task-specific downstream embedding space, followed by h performing classification on this embedding. We can rewrite $P(\\mathbb{E}_m(\\mathbb{E}_v(v), \\mathbb{E}_t(t)))$ as $\\mathbb{E}(v|t)$, obtaining an image encoder conditioned on the textual modality. The weight of the linear classification head, $\\mathbb{W} = \\{\\omega_i\\}_{i=1}^c \\in \\mathbb{R}^{d \\times c}$ (c is the number of categories), serve as guiding vectors. Using label information as guiding labels Y, we thus have all the necessary components to implement the Feature Guidance Attack. See Appendix A for more details and explanations about Visual Question Answering (VQA), Visual Reasoning (VR), etc."}, {"title": "3.3 Attacking before Fuse", "content": "In this scenario, only two unimodal encoders are used: $\\mathbb{E}_v$ and $\\mathbb{E}_t$. The primary intention of VLP models is to learn directly from raw text descriptions of images, utilizing a broader source of supervision [40]. This implies that in CV models, image understanding comes from pre-provided labels, such as image categories, pixel categories, or annotated bounding boxes. In contrast, in VLP models, the understanding of images originates from raw text. Consequently, using text as supervisory information to generate image adversarial examples becomes a natural approach. To implement this approach, we first acquire a text set $\\mathbb{T} = \\{t_i\\}_{i=1}^{m_1}$. Then, We use the text encoder to obtain a set of guiding vectors $\\{\\omega_i\\}_{i=1}^{m_1} = \\{\\mathbb{E}_t(t_i)\\}_{i=1}^{m_1}$. To obtain this text set $\\mathbb{T}$, all texts are gathered from the dataset. Here, by utilizing the dataset's annotations, we can identify which texts in the text set match with the image $v$, thereby obtaining the guiding labels $\\mathbb{Y}$. By this point, all elements necessary for executing FGA have been acquired: the image encoder $\\mathbb{E}_v$, the set of guiding vectors $\\{\\mathbb{E}_t(t_i)\\}_{i=1}^{m_1}$, and the guiding labels $\\mathbb{Y}$. By maximizing $\\mathcal{L}_{gui}$, the feature vector $\\mathbb{E}_v(v')$ will diverge from the text representations $\\{\\mathbb{E}_t(t_y)\\}_{y \\in \\mathbb{Y}}$ that match $v$, thereby generating adversarial images $v'$. For more details on executing iterations of FGA and how it can be combined with typical attack strategies in the unimodal domain, refer to Appendix B."}, {"title": "3.4 Boosting Transferability before Fuse", "content": "SGA[33] points out that multimodal adversarial examples have better transferability than unimodal adversarial examples. Therefore, we need to introduce text attack into FGA before fuse. We consider an image minibatch $\\mathbb{V} = \\{v_i\\}_{i=1}^n$ and the text set $\\mathbb{T}_i$ represents all texts that match with the image $v_i$. Firstly, for each text $t \\in \\mathbb{T}_i$, we handle the following optimization problem to generate adversarial text $t'$:\n$t' = \\underset{t'}{\\text{argmax}} \\bigg(-\\frac{\\mathbb{E}_t(t') \\cdot \\mathbb{E}_v(v_i)}{\\|\\mathbb{E}_t(t')\\|_2 \\|\\mathbb{E}_v(v_i)\\|_2}\\bigg)$ (6)\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean distance.\nAt this point, we obtain the adversarial text set $\\mathbb{T}'$, and we denote $\\mathbb{T} = \\mathbb{T}_1 \\cup \\mathbb{T}_2 ... \\cup \\mathbb{T}_i \\cup \\mathbb{T}'_1 \\cup \\mathbb{T}'_2 ... \\cup \\mathbb{T}'_i$. Secondly, for $v_i \\in \\mathbb{V}$, where $\\mathbb{T}_i$ is its matching texts and $\\mathbb{T}'$ is the adversarial texts, to generate adversarial example $v$, we use the feature guidance loss to encourage $\\mathbb{E}(v)$ to simultaneously move away from both $\\mathbb{E}_t(\\mathbb{T}_i)$ and $\\mathbb{E}_t(\\mathbb{T}')$:\n$\\mathcal{L}_{gui} (v) = \\frac{1}{leni}\\sum_{t^* \\in \\mathbb{T}_i \\cup \\mathbb{T}'} \\log \\frac{\\exp(\\mathbb{E}_v(v) \\cdot \\mathbb{E}_t(t^*))}{\\sum_{t \\in \\mathbb{T}_i \\cup \\mathbb{T}'} \\exp(\\mathbb{E}_v(v) \\cdot \\mathbb{E}_t(t))}$ (7)\n$leni$ represents the length of text set $\\mathbb{T}_i \\cup \\mathbb{T}$.\nBuilding on this foundation, we further introduce two strategies to enhance transferability: (1) Following SGA [33], we preset a set of resize parameters $\\mathbb{S} = \\{s_1, s_2, ..., s_m\\}$, where $h(v, s_k)$ denotes the resizing function that takes the image $v$ and the scale coefficient $s_k$ as inputs. After data augmentation, the objective function we aim to maximize is no longer $\\mathcal{L}_{gui}(v)$ but rather $\\sum_{k=1}^m \\mathcal{L}_{gui}(h(v, s_k))$, where $h(v, s_k)$ represents the augmented image. (2) Following MI-FGSM [12], we introduce the momentum mechanism, where the current perturbation direction is determined by both the current gradient and the historical gradients from previous iterations. See Appendix B.2 for more details."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setting", "content": "4.1.1 VLP Models. Our experimental section involves four typical VLP models: CLIP, ALBEF, TCL and BEiT3. CLIP is a typical aligned model, consisting solely of two unimodal encoders. The latter three are fused models, containing two unimodal encoders and a multimodal encoder. ALBEF and TCL share the same architecture with some differences in the details of ITC loss. Besides, ALBEF and BEiT3 have two main differences: (1) Different Pre-training Tasks: ALBEF is based on three pre-training tasks: ITC, ITM and MLM. In contrast, BEiT3 is based on three MDM tasks: MLM, MIM, and MVLM. (2) Different Model Structures: In ALBEF, the three encoders are independent of each other. BEiT3, however, uses the Multiway Transformer to split the feed-forward layer into three parallel paths, thereby obtaining three encoders.\n4.1.2 V+L Downstream Tasks. In this part, we will introduce each downstream task involved in the experiments, along with the models and datasets used to perform these tasks.\nVisual Entailment (VE) is a fine-grained visual reasoning task, where given a pair of (v, t), the model needs to determine whether the text is supported by the image (entailment), whether the text is unrelated to the image (neutral), or whether the text contradicts the content of the image (contradictory). This task will be conducted based on the ALBEF model and the SNLI-VE [50] dataset.\nVisual Question Answering (VQA) requires the model to predict a correct answer given an image and a question [16, 37]. It can be viewed as a multi-answer classification problem, or as an answer"}, {"title": "4.2 Attack Effectiveness after Fuse", "content": "This subsection explores the effectiveness of attacks on the fused feature vector. Since VE, VQA, VG, and VR rely on this vector, we choose to evaluate these four tasks. As Table 1 illustrates, the evaluation follows the baseline set by [54], TA represents alone text attack using BertAttack. IA represents image attack based on feature deviation loss. SA stands for separate unimodal attack, indicating that TA and IA are executed separately without modal interaction, and CA denotes the multimodal white-box attack Co-Attack [54] which introduces cross-modal interaction. For fairness, we set the $l_\\infty$ perturbation constraint for the image modality in FGA and FGA-T to $\\epsilon = 2/255$ with 10 iterations, consistent with IA, SA, and CA. Additionally, we explore the attack effectiveness when the number of iterations is 1, i.e., single-step attack, namely FGA$^1$ and FGA-T$^1$. We also investigate the effectiveness under the $l_1$ constraint, namely FGA$_{l_1}$, with $\\epsilon_{l_1} = 255$ and 20 iterations. (See Appendix B.1 for more details.) Besides, we perform FGA in patch form, namely FGA$_{pat}$, with 100 iterations, a single-step $l_\\infty$ constraint of $\\alpha = 8/255$, and a patch area of 2% of the total image area, with a random location. (See Appendix B.3 for more details.) Furthermore, when involving text attack, BertAttack is used with a restriction of 1 perturbable token, following [54, 33].\nWe can observe from Table 1: (1) Under all tasks, FGA-T consistently achieves the best white-box attack performance, validating the effectiveness of the feature guidance approach and its orthogonality with text attack. (2) Even with only a single step, the feature guidance method is sufficient to produce effective adversarial examples, performing on par with or even better than the baseline. This provides a faster and more convenient attack strategy. (3) The feature guidance approach exhibits good orthogonality with other attack strategies in Computer Vision. When combined with $l_1$ attack or patch attack, it demonstrates strong performance."}, {"title": "4.3 Attack Effectiveness before Fuse", "content": "In this subsection, we explore the effectiveness of attacks on two unimodal encoders. The tasks of ZC and ITR primarily rely on two unimodal embeddings. We first conduct attacks on the ZC task. Since the text input in the ZC task is predefined and cannot be altered, we only use attacks involving the visual modality. When conducting FGA, we construct the text \"A photo of a {object}.\" using the categorys' name to obtain the text set $\\mathbb{T} = \\{t_i\\}_{i=1}^c$, where $c$ represents the number of categories, following [40]. We extract features through $\\mathbb{E}_t$ to construct the guiding vectors $\\{\\mathbb{E}_t(t_i)\\}_{i=1}^c$, and the true category of the image serves as the guiding label $y \\in \\{1, 2, ..., c\\}$. The attack results are presented in the Table 2. We observe that even FGA$^1$ outperforms IA which is an iterative feature deviation attack.\nWhen executing the ITR task on the CLIP model with ViT image encoder, to construct guiding vectors for FGA, we use not only all"}, {"title": "4.4 Boosting Transferability", "content": "In this subsection, we transition the attack from the white-box setting to the black-box setting, which is a more common scenario. We use four VLP models: ALBEF, TCL, CLIP$_{VIT}$, and CLIP$_{CNN}$. The TCL model is identical to ALBEF except for differences in the"}, {"title": "4.5 Visualization of Targeted Patch FGA", "content": "FGA pushes $\\mathbb{E}_v(v')$ away from matching text embeddings. Conversely, we can also push $\\mathbb{E}_v(v')$ closer to a specified text embedding to produce a predetermined error. In unimodal scenarios, this form of attack is called the targeted attack. For example, we have a text set $\\{t_i\\}_{i=1}^n$ and want to push $\\mathbb{E}(v')$ closer to a specified text $t_k$. In this case, we need to maximize the following function:\n$\\mathcal{L}_{gui}^{target} = \\log \\frac{\\exp(\\mathbb{E}(v') \\cdot \\mathbb{E}_t (t_k))}{\\sum_{i=1}^n \\exp(\\mathbb{E}(v') \\cdot \\mathbb{E}_t(t_i))}$ (8)\nWe add perturbation to the clean image $v$ in patch form, maximizing $\\mathcal{L}_{gui}^{target}$ to obtain the adversarial patch image $v'$. We execute"}, {"title": "4.6 FGA's Principle of Proximity", "content": "In subection 3.1, it is mentioned that $\\frac{\\partial \\mathcal{L}_{gui}}{\\partial \\mathbb{E}(x')}$ not only \u201cguides $\\mathbb{E}(v')$ away from $\\{\\omega_y\\}_{i=1}^n$\u201d, but also \u201cselects a nearby guiding vector that does not belong to the set $\\{\\omega_y\\}_{i=1}^n$ and moves closer to it\u201d. The performance decline of VLP models on various V+L downstream tasks in previous experiments sufficiently demonstrates the former. We further prove the latter based on the ZC task and the CLIP"}, {"title": "4.7 Ablation Experiments", "content": "We investigate the impact of the number of iterations (step) and intensity of noise ($\\epsilon$) based on the image-text retrieval task, Flickr30k dataset and the BEiT-3 model, as shown in Fig 6. We can observe that as $\\epsilon$ and step increase, the effectiveness of the attack gradually strengthens and tends to converge. Specific experimental configurations are detailed in Appendix C."}, {"title": "5 CONCLUSION", "content": "In this paper, we attempt to construct a unified understanding of adversarial vulnerability regarding unimodal models and VLP models. We abstract visual modality attack into a feature guidance form and combine it with text attack and other enhancement mechanisms to establish a general baseline for exploring the security of the VLP domain. In fact, our approach is theoretically orthogonal to many other attack schemes in the unimodal domain, which facilitates further exploration of the vulnerabilities of VLP models and the design of defence algorithms in subsequent work. We hope our code can be beneficial to the community."}, {"title": "A Attack Details", "content": "Three key elements are required to implement FGA, namely an image encoder, guiding vectors, and guiding labels. Below, we will elaborate on the construction of these elements in four scenarios: VE, VOA, VG, and VR."}, {"title": "A.1 Visual Entailment", "content": "Task Detail. We conduct the attack experiment on the VE task [50] with ALBEF [23] which treats VE as a three-classification problem and connects a multi-layer perceptron (MLP) after the [CLS] vector. The input layer and all hidden layers of the MLP constitute P, obtaining the image encoder $\\mathbb{E}(v|t) = P(\\mathbb{E}_m(\\mathbb{E}_v(v), \\mathbb{E}_t(t)))$. The output layer of the MLP is a linear layer, whose weight matrix is $\\mathbb{W} = \\{\\omega_0, \\omega_1, \\omega_2\\}$. The three guiding vectors are associated with three categories \"contradiction, neutral and entailment\", and the label $y \\in \\{0, 1, 2\\}$ of the input image-text pair (v, t) provides the direction of the attack, that is, guiding $\\mathbb{E}(v'|t)$ the embedding of adversarial image $v'$ deviates from the guiding vector $\\omega_y$.\nDataset Detail. The SNLI-VE dataset [50] is a benchmark for visual entailment, which aims to determine whether an image supports, contradicts, or is neutral to a given natural language statement. This task extends the concept of natural language inference (NLI) to the visual domain, presenting challenges in image and text understanding. The dataset is constructed based on two existing datasets: SNLI (Stanford Natural Language Inference) and Flick30k. We use its test split, which contains 1000 images, 5973 entailment texts, 5964 neutral texts, and 5964 contradiction texts."}, {"title": "A.2 Visual Question Answering", "content": "Task Detail. We conduct the attack experiment on the VQA task [16] with ALBEF which performs this task in the manner of text generation. The image-question pair is fed into ALBEF to extract the fused embedding, which is then sent to a decoder to generate an answer. The dictionary size of the decoder is 30522, so the end of the decoder is a linear classification head, with weight matrix $\\{w_i\\}_{i=0}^{30521}$.\nThe VQA 2.0 dataset [16] provides 3,128 candidate answers. To align with this task, ALBEF only considers 3,128 output possibilities. We follow this by selecting 3,128 vectors from the weights of the linear layer to form the guiding vectors $\\{w_i\\}_{i=0}^{3127}$, each corresponding to an answer. To perform FGA, we denote the decoder excluding the linear classification head as P, and we still lack guiding labels. For convenience, we directly use the network's prediction results as the guiding labels, which is argmax$_i(P(\\mathbb{E}_m(\\mathbb{E}_v(v'), \\mathbb{E}_t(t))) \\cdot w_i)$.\nDataset Detail. The VQA2.0 dataset includes images from the MS COCO (Microsoft Common Objects in Context) dataset [31], providing a diverse set of real-world images depicting various objects, scenes, and activities. For each image, multiple questions are generated, covering a wide range of topics such as object recognition, counting, colour identification, spatial relationships, and more. Each question is accompanied by multiple answers, provided by different human annotators. The answers can be in the form of single words, phrases, or numbers. It contains 83k images for training, 41k for validation, and 81k for test. We conduct attack tests based on the test-dev and test-std splits."}, {"title": "A.3 Visual Grounding", "content": "Task Detail. We conduct the attack experiment on the VE task with ALBEF which extends Grad-CAM [41] to acquire heatmaps and use them to rank the detected proposals provided in advance. During this task, after the fused encoder, ALBEF is followed by a linear image-text matching binary classifier, the weight matrix of which is $\\mathbb{W} = \\{\\omega_0, \\omega_1\\}$. The larger the inner product between the fused embedding and $\\omega_1$, the more the input image-text pair (v, t) matches. ALBEF backpropagates the gradient based on the loss value $\\mathbb{E}_m(\\mathbb{E}_v(v), \\mathbb{E}_t(t))\\cdot\\omega_1$, obtains the heatmap, and then performs the VG task. Consequently, we use FGA to guide $\\mathbb{E}_m(\\mathbb{E}_v(v'), \\mathbb{E}_t(t))$ away from $\\omega_1$ as the attack strategy.\nDataset Detail. RefCOCO+ [53] is a dataset designed for referring expression comprehension in the context of images. It is an extension of the original RefCOCO dataset and specifically aims at addressing the challenge of grounding referring expressions that require fine-grained distinctions between objects. The key components of the RefCOCO+ dataset are: (1) Images: The dataset uses images from the Microsoft COCO (Common Objects in Context) dataset, which contains a wide variety of everyday scenes with multiple objects. (2) Referring Expressions: For each image, there are several referring expressions provided by human annotators. These expressions describe specific objects or groups of objects in the image. (3) Object Annotations: Each referring expression is associated with an object annotation, a bounding box that identifies the location of the referred object in the image."}, {"title": "A.4 Visual Grounding", "content": "Task Detail. We perform the VR task based on the BEiT3 model [45]. In this task, the input example pair of the model is $(v_0, v_1, t, y)$, where $y \\in \\{0, 1\\}$. $y = 1$ means that the text matches at least one of two images. BEiT3 splits an example pair into two image-text pairs $(v_0, t)$ and $(v_1, t)$ as inputs, thereby extracting two fused embeddings. After concatenating the two embeddings and performing operations such as nonlinear projection, the final feature vector is obtained. This feature vector is fed into a binary classifier, whose weight matrix is $\\{w_0, w_1\\}$. At this point, we only need to guide the feature vector away from the guiding vector $w_y$ through FGA, and simultaneously update the input images $v_0, v_1$ along the gradient direction to obtain the adversarial images $v'_0$ and $v'_1$.\nDataset Detail. NLVR2 [43] (Natural Language for Visual Reasoning for Real) is a natural language processing dataset designed for the visual reasoning task. It aims to evaluate models' ability to reason about visual information combined with natural language descriptions. NLVR2 is an extended version of the NLVR dataset, featuring more images and more complex language descriptions. The NLVR2 dataset contains approximately 107,000 human-written sentences describing visual relationships in a set of images. Each sample includes a sentence and a pair of images. The content described in the sentence may match one of the images, both, or neither. The task for models is to determine whether the sentence correctly describes at least one of the images. This dataset is used for various vision-language tasks, such as visual question answering, image-text matching, and multimodal reasoning. NLVR2 advances the research and development of vision-language models' reasoning"}, {"title": "B Combine FGA with unimodal attacks", "content": "We design FGA as a universal attack strategy, which is theoretically orthogonal to all unimodal attack schemes."}, {"title": "B.1 Global Perturbation", "content": "This subsection discusses how"}]}