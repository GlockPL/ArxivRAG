{"title": "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models", "authors": ["Haonan Zheng", "Xinyang Deng", "Wen Jiang", "Wenrui Li"], "abstract": "With Vision-Language Pre-training (VLP) models demonstrating powerful multimodal interaction capabilities, the application scenarios of neural networks are no longer confined to unimodal domains but have expanded to more complex multimodal V+L downstream tasks. The security vulnerabilities of unimodal models have been extensively examined, whereas those of VLP models remain challenging. We note that in CV models, the understanding of images comes from annotated information, while VLP models are designed to learn image representations directly from raw text. Motivated by this discrepancy, we developed the Feature Guidance Attack (FGA), a novel method that uses text representations to direct the perturbation of clean images, resulting in the generation of adversarial images. FGA is orthogonal to many advanced attack strategies in the unimodal domain, facilitating the direct application of rich research findings from the unimodal to the multimodal scenario. By appropriately introducing text attack into FGA, we construct Feature Guidance with Text Attack (FGA-T). Through the interaction of attacking two modalities, FGA-T achieves superior attack effects against VLP models. Moreover, incorporating data augmentation and momentum mechanisms significantly improves the black-box transferability of FGA-T. Our method demonstrates stable and effective attack capabilities across various datasets, downstream tasks, and both black-box and white-box settings, offering a unified baseline for exploring the robustness of VLP models.", "sections": [{"title": "1 Introduction", "content": "ViT provides an effective Transformer-based encoder for the visual modality [14], ensuring the feature extraction of multimodal input through a unified encoding manner, significantly advancing the Vision-and-Language tasks [16, 1, 47, 52]. Various VLP models [23, 51, 45, 40] continually improve performance in V+L downstream tasks through diverse pre-training tasks and architectural designs [2, 45, 20, 27, 29]. However, the previous research in unimodal fields such as Computer Vision (CV) and Natural Language Processing (NLP) highlights the vulnerability of neural networks to adversarial attacks [15, 24]. Although adversarial robustness, particularly in CV, has been extensively explored in terms of attack strategies [4, 36], defence mechanisms [34], and transferability [12, 46], the study of adversarial robustness in VLP models remains challenges [26, 54, 55, 33]. Our study aims to develop a unified architecture to explore commonalities between multimodal and unimodal tasks from the perspective of adversarial attacks. In other words, we seek to bridge the gap, allowing rich findings in unimodal adversarial robustness to be directly applied to the multimodal scenario.\nThe first question we consider is \"Which modality should be paid more attention?\" We primarily focus on perturbations in the image modality, with perturbations in the text modality serving as orthogonal (1) Semantic consistency: Visual adversarial examples maintain semantic consistency, i.e., noise addition within reasonable limits doesn't change human comprehension. Conversely, text adversarial examples risk semantic distortion, potentially introducing spelling errors. (2) Differentiability: Image inputs are continuous and differentiable, unlike text tokens which are discrete and non-differentiable making text-only attacks less effective. (3) Accessibility: In real-world scenarios, text often serves as the primary means of interaction between users and AI models, with limited opportunities for attackers to modify user-generated text. In contrast, models can automatically acquire image data, simplifying the process for attackers to introduce perturbations. In fact, [33] also primarily focuses on enhancing attack strategies in the visual modality to improve adversarial transferability and [55] does not involve text attacks.\nThe second question is \"How to unify multimodal and unimodal scenarios in exploring adversarial robustness?\" We conceptualize image adversarial attacks as a feature-guided process. For unimodal models which primarily learn to understand images through detailed annotation information (such as category labels), attacking an image involves steering its embedding away from the feature vector linked to its correct annotation [15]. This deviation induces a biased comprehension of the image within the network. Alternatively, the image embedding can be guided closer to the feature vector associated with an incorrect annotation, thereby leading the network to make a predetermined error [25]. In the multimodal scenario, models are encouraged to understand images from raw text, providing a broader and more accessible source of supervision [40]. This also offers more flexible guiding information for the adversarial attack. By guiding the image embedding away from the correct text description, we induce the VLP model to develop an incorrect understanding of the image itself. Similarly, directing the embedding towards an incorrect text description intentionally misleads the model into adopting a specific erroneous interpretation. This strategy is termed Feature Guidance Attack (FGA). Expanding upon FGA, we employ adversarial texts from text attacks as guiding information to generate adversarial images, thus obtaining a novel multimodal attack. This approach exacerbates the model's misinterpretation called Feature Guidance with Text Attack (FGA-T). Furthermore, we introduce additional orthogonal mechanisms to enhance the adversarial transferability of FGA-T in the black-box scenario. Code: https://github.com/LibertazZ/FGA\nOur contributions can be summarized as follows:\n\u2022 We provide FGA, using original text as the supervision source for the adversarial attack on VLP models, inducing the network to misinterpret adversarial images.\n\u2022 We introduce cross-modal interaction through adversarial text, forming a novel multimodal adversarial attack that enhances white-box attack strength, and improves black-box transferability through additional mechanisms.\n\u2022 Our approach is theoretically orthogonal to any unimodal attack enhancement mechanism. Empirical evidence based on multiple datasets and VLP models demonstrates the broad applicability of our method to various V+L multimodal tasks, providing a unified baseline for the exploration of multi-modal robustness."}, {"title": "2 Related Work", "content": "From an access perspective to the model, unimodal attacks can be divided into white-box and black-box attacks. In the black-box scenario, due to the target network's opaque weights, attackers typically conduct white-box attacks on an accessible source network, then transfer the adversarial examples to the target network. Therefore, the attack's transferability is also crucial.\nBased on how to constrain perturbation, adversarial attacks on the visual modality can generally be divided into two categories. (1) Global attacks typically involve perturbing all pixels of an image, usually constraining the distance between adversarial and original images based on l\u221e, 12, or l\u2081 norms. Representative methods include FGSM [15], PGD [34], APGD [10], CW [4], etc. (2) Patch attacks, which confine the perturbation to a small area, such as 2% of the image, and allow unrestricted modification of image pixels within that area. Representative methods include LaVAN [19] and Depatch [7]. Since patch attacks are more practical, physical world attacks are usually based on this form. In the text domain, due to the discrete nature of text data, such attacks typically involve subtle modifications to the original text, such as replacing synonyms, inserting additional words, or adjusting sentence structure, without significantly altering the meaning of the text. A representative method is BertAttack [24].\nEnhancing the transferability of adversarial attacks is essentially a generalization problem. The two main approaches to solving the generalization issue are data augmentation and improving the optimization algorithm, thus dividing transfer attack methods into two categories. (1) Typical methods that boost transferability through data augmentation, such as DI [49] (Diverse Inputs), TI [13] (Translation Invariant) and SI [30] (Scale Invariant). (2) Typical schemes that improve optimization algorithm, such as MI [12] (Momentum Iterative), NI [30] (Nesterov Iterative), VMI [46] (Varied Momentum Iterative), VNI [46] (Varied Nesterov Iterative)."}, {"title": "2.2 Multimodal Adversarial Attack", "content": "This subsection discusses relevant VLP models and multimodal adversarial attack methods.\nVLP models based on different combinations of pre-training tasks can be roughly divided into three categories. (1) Aligned models: CLIP [40] contains two unimodal encoders to align multimodal embeddings based on Image-Text Contrastive (ITC) loss. (2) Fused models by matching: ViLT [20] introduces both Image-Text Matching (ITM) and Masked Language Modeling (MLM) pre-training for V+L tasks. Models like ALBEF [23], TCL [51], BLIP [22], and VLMo [3] build on it, first aligning multimodal features using ITC loss, then fusing cross-modal features using ITM and MLM losses. (3) Fused models by Masked Data Modeling (MDM): BEIT [2] and BEiTV2 [38] propose and improve Masked Image Modeling (MIM) loss. BEiT3, based on it, first aligns multimodal features using ITC loss, then fuses cross-modal features using MIM, MLM, and Masked Language-Vision Modeling (MLVM) losses.\nAttacking VLP models is a novel topic. Existing work has provided valuable insights. Co-Attack [54] designs general optimization objectives based on different embeddings (unimodal or multimodal) and experimentally demonstrates that using text attacks or image attacks alone is not as effective as using both in combination, providing a general baseline for subsequent works. SGA [33] points out that improving the diversity of multimodal interaction can enhance the transferability of multimodal adversarial examples. AdvCLIP [55] provides a framework to learn a universal adversarial patch on pre-trained models for transfer attacks on downstream fine-tuned models. These works are limited to VLP models and ignore the connection between unimodal and multimodal scenarios, which is our main motivation."}, {"title": "3 Methodology", "content": "An image feature extractor E (e.g., an image contrastive representation encoder [18, 5, 17] or a VLP model's visual encoder) projects the image into a feature vector for various visual tasks like image classification and object detection. Without regarding the subsequent usage, the most intuitive approach to generate an adversarial example x' for an image x is to encourage the feature vectors E(x') and E(x) to be as distant as possible [54]. This universal strategy is termed \"Feature Deviation Attack\" (FDA), which involves maximizing the loss function:\n$$L_{dev} = -E(x') \\cdot E(x).$$\nAssuming that in the embedding space, there exists a set of guiding vectors W = {wi}m\u2081, w \u2208 Rd, and there is a set of guiding labels Y = {yi}n\u2081=1, y \u2208 {1, 2, ..., m} specifying that E(x') should be distant from the guiding vectors {@yi}n\u2081=1 \u2208 W. We refer to this strategy as \"Feature Guidance Attack\" (FGA). To realize the above concept, we need to maximize the loss function:\n$$L_{gui} = \\sum_{i=1}^{n} \\log \\frac{\\exp(E(x') \\cdot W_{y_i})}{\\sum_{j=1}^{m} \\exp(E(x') \\cdot \\omega_j)}$$\nwhere exp() represents the exponential function with Euler's number e as the base, and ln() stands for the logarithm to the base e. Based on Lgui or Ldev, we can apply the PGD process [34], gradually pushing the clean example x along the gradient direction to maximize the loss function, ultimately obtaining the adversarial example x'. By the chain rule of gradients, $\\frac{\\partial L}{\\partial E(x')}$  represents the direction of perturbation added to the input example. While we focus on $\\frac{\\partial L}{\\partial E(x')}$, which represents the movement direction of the feature vector:\n$$\\frac{\\partial L_{dev}}{\\partial E(x')} = -E(x)$$\n$$\\frac{\\partial L_{gui}}{\\partial E(x')} = \\sum_{i=1}^{n} \\frac{\\exp(E(x') \\cdot \\omega_k)}{\\sum_{j=1}^{m} \\exp(E(x') \\cdot \\omega_j)} \\cdot \\omega_{y_i} + \\omega_k$$\nIt can be observed that feature deviation loss promotes the movement of E(x') towards -E(x), which means moving away from E(x). While, Regarding the first term of $\\frac{dL_{gui}}{dE(x')}$, it encourages E(x') to move away from the guiding vectors {@y\u2081}n\u2081=1, and assigning equal weight 1/n to each of them. The second term encourages E(x') to approach the guiding vector \u03c9k \u2208 W, with a weight of $\\exp(E(x')\\cdot \\omega_k)/\\sum_{j=1}^{m} \\exp(E(x') \\cdot w_j)$, which means the closer E(x') is to a guiding vector, the greater the weight assigned to it. Due to the presence of the first term, E(x') is far from {@yi}n\u2081=1, resulting in the weight of $w_{y_i}$ being almost zero in the second term. Consequently, the second term effectively facilitates E(x') in selecting a nearby guiding vector that does not belong to the set {@yi}n\u2081=1 and moving closer to it.\nMost existing VLP models typically consist of two unimodal encoders, a text encoder Et and a visual encoder Ev, along with a multimodal feature fusion encoder Em. For a single image-text paired example (v, t), it is first mapped to a shared feature space separately by Et and Ev for aligning image and text features. Subsequently, cross-modal feature fusion is conducted through Em. Therefore, VLP models focus on three key embeddings:Ev (v), Et (t), and Em (Ev (v), Et (t)), all corresponding to the [CLS] vector. We will focus on finding guiding vectors in the embedding space and constructing guiding labels to execute FGA on VLP models."}, {"title": "3.2 Attacking after Fuse", "content": "In this scenario, we focus on the fused embedding Em (Ev (v), Et (t)). For different V+L downstream tasks, it needs to be fed into different subsequent models which can be uniformly understood as comprising a projector P and a linear classification head h. P projects the fused embedding into a task-specific downstream embedding space, followed by h performing classification on this embedding. We can rewrite P(Em (Ev (v), Et (t))) as E(vt), obtaining an image encoder conditioned on the textual modality. The weight of the linear classification head, W = {wi}c=1 \u2208 Rd\u00d7c (c is the number of categories), serve as guiding vectors. Using label information as guiding labels Y, we thus have all the necessary components to implement the Feature Guidance Attack. See Appendix A for more details and explanations about Visual Question Answering (VQA), Visual Reasoning (VR), etc."}, {"title": "3.3 Attacking before Fuse", "content": "In this scenario, only two unimodal encoders are used: Ev and Et. The primary intention of VLP models is to learn directly from raw text descriptions of images, utilizing a broader source of supervision [40]. This implies that in CV models, image understanding comes from pre-provided labels, such as image categories, pixel categories, or annotated bounding boxes. In contrast, in VLP models, the understanding of images originates from raw text. Consequently, using text as supervisory information to generate image adversarial examples becomes a natural approach. To implement this approach, we first acquire a text set T = {ti}m\u2081. Then, We use the text encoder to obtain a set of guiding vectors {wi}m\u2081=1 = {Et(ti)}m\u2081. To obtain this text set T, all texts are gathered from the dataset. Here, by utilizing the dataset's annotations, we can identify which texts in the text set match with the image v, thereby obtaining the guiding labels Y. By this point, all elements necessary for executing FGA have been acquired: the image encoder Ev, the set of guiding vectors {Et (ti)}m\u2082=1, and the guiding labels Y. By maximizing Lgui, the feature vector Ev (v') will diverge from the text representations {Et (ty)} yey that match v, thereby generating adversarial images v'. For more details on executing iterations of FGA and how it can be combined with typical attack strategies in the unimodal domain, refer to Appendix B."}, {"title": "3.4 Boosting Transferability before Fuse", "content": "SGA[33] points out that multimodal adversarial examples have better transferability than unimodal adversarial examples. Therefore, we need to introduce text attack into FGA before fuse. We consider an image minibatch V = {vi}n=1 and the text set Ti represents all texts that match with the image vi. Firstly, for each text t \u2208 Ti, we handle the following optimization problem to generate adversarial text t':\n$$t' = \\underset{t'}{\\operatorname{argmax}} \\left(-\\frac{E_t(t') \\cdot E_v(v_i)}{||E_t(t')||_2 ||E_v(v_i)||_2}\\right)$$\nwhere ||\u00b7||2 denotes the Euclidean distance.\nAt this point, we obtain the adversarial text set T', and we denote T = T\u2081 \u222a T\u2082 . . . \u222a Tn \u222a T\u2081'\u222a T\u2082' . . . \u222a T'. Secondly, for vi \u2208 V, where Ti is its matching texts and T'i is the adversarial texts, to generate adversarial example v, we use the feature guidance loss to encourage Ev(v) to simultaneously move away from both Et (Ti) and Et (T'):\n$$L_{gui} (v) = \\frac{1}{len_i} \\sum_{i=1}^{n} \\log \\frac{\\exp(E_v(v) \\cdot E_t(t^*))}{\\sum_{t \\in T_i \\cup T'_i} \\exp(E_v(v) \\cdot E_t(t))}$$\nleni represents the length of text set Ti \u222aT'i.\nBuilding on this foundation, we further introduce two strategies to enhance transferability: (1) Following SGA [33], we preset a set of resize parameters S = {S1, S2, ..., Sm}, where h(v, sk) denotes the resizing function that takes the image v and the scale coefficient sk as inputs. After data augmentation, the objective function we aim to maximize is no longer Lgui (v) but rather $\\sum_{k=1}^{m} L_{gui}(h(v, s_k))$, where h(v, sk) represents the augmented image. (2) Following MI-FGSM [12], we introduce the momentum mechanism, where the current perturbation direction is determined by both the current gradient and the historical gradients from previous iterations. See Appendix B.2 for more details."}, {"title": "4 Experiments", "content": "Our experimental section involves four typical VLP models: CLIP, ALBEF, TCL and BEiT3. CLIP is a typical aligned model, consisting solely of two unimodal encoders. The latter three are fused models, containing two unimodal encoders and a multimodal encoder. ALBEF and TCL share the same architecture with some differences in the details of ITC loss. Besides, ALBEF and BEiT3 have two main differences: (1) Different Pre-training Tasks: ALBEF is based on three pre-training tasks: ITC, ITM and MLM. In contrast, BEiT3 is based on three MDM tasks: MLM, MIM, and MVLM. (2) Different Model Structures: In ALBEF, the three encoders are independent of each other. BEiT3, however, uses the Multiway Transformer to split the feed-forward layer into three parallel paths, thereby obtaining three encoders."}, {"title": "4.1.2 V+L Downstream Tasks", "content": "In this part, we will introduce each downstream task involved in the experiments, along with the models and datasets used to perform these tasks.\nVisual Entailment (VE) is a fine-grained visual reasoning task, where given a pair of (v, t), the model needs to determine whether the text is supported by the image (entailment), whether the text is unrelated to the image (neutral), or whether the text contradicts the content of the image (contradictory). This task will be conducted based on the ALBEF model and the SNLI-VE [50] dataset.\nVisual Question Answering (VQA) requires the model to predict a correct answer given an image and a question [16, 37]. It can be viewed as a multi-answer classification problem, or as an answer generation problem. We use the VQAv2 [16] dataset and the ALBEF model, which performs the VQA task through text generation.\nVisual Grounding (VG) requires the model to find parts of the image that match the given textual description. We perform this task based on the RefCOCO+ [53] dataset and the ALBEF model.\nVisual Reasoning (VR) requires the model to predict if a given text describes a pair of images. This task necessitates that the model not only understands the content of individual images but also compares and reasons about the relationship between two images. Therefore, the input consists of a pair of images and a piece of text. We use the BEiT3 model and the NLVR2 [43] dataset to perform this task.\nrequires using predefined category descriptions (such as \"a cat,\" \"a car,\" etc.) as text inputs and mapping these descriptions to the embedding space by text encoder. Then, for a given image, the similarity between the image embedding and each category description embedding is calculated, and the image is classified into the category with the highest similarity. Due to the CLIP model's strong zero-shot capacity, we use it along with three datasets: CIFAR-10 [21], CIFAR-100 [21], and ImageNet [11], to perform this task.\nImage-Text Retrieval (ITR) involves retrieving relevant images from an image database given a text query, and vice versa [52, 32, 48]. We perform this task based on the CLIP, ALBEF and TCL models, and the Flickr30k [39] and MS COCO [31] datasets."}, {"title": "4.2 Attack Effectiveness after Fuse", "content": "This subsection explores the effectiveness of attacks on the fused feature vector. Since VE, VQA, VG, and VR rely on this vector, we choose to evaluate these four tasks. As Table 1 illustrates, the evaluation follows the baseline set by [54], TA represents alone text attack using BertAttack. IA represents image attack based on feature deviation loss. SA stands for separate unimodal attack, indicating that TA and IA are executed separately without modal interaction, and CA denotes the multimodal white-box attack Co-Attack [54] which introduces cross-modal interaction. For fairness, we set the l\u221e perturbation constraint for the image modality in FGA and FGA-T to \u03f5 = 2/255 with 10 iterations, consistent with IA, SA, and CA. Additionally, we explore the attack effectiveness when the number of iterations is 1, i.e., single-step attack, namely FGA\u00b9 and FGA-T\u00b9. We also investigate the effectiveness under the l\u2081 constraint, namely FGAle\u2081, with el\u2081 = 255 and 20 iterations. Besides, we perform FGA in patch form, namely FGApat, with 100 iterations, a single-step l\u221e constraint of \u03b1 = 8/255, and a patch area of 2% of the total image area, with a random location. Furthermore, when involving text attack, BertAttack is used with a restriction of 1 perturbable token, following [54, 33].\nWe can observe from Table 1: (1) Under all tasks, FGA-T consistently achieves the best white-box attack performance, validating the effectiveness of the feature guidance approach and its orthogonality with text attack. (2) Even with only a single step, the feature guidance method is sufficient to produce effective adversarial examples, performing on par with or even better than the baseline. This provides a faster and more convenient attack strategy. (3) The feature guidance approach exhibits good orthogonality with other attack strategies in Computer Vision. When combined with 11 attack or patch attack, it demonstrates strong performance."}, {"title": "4.3 Attack Effectiveness before Fuse", "content": "In this subsection, we explore the effectiveness of attacks on two unimodal encoders. The tasks of ZC and ITR primarily rely on two unimodal embeddings. We first conduct attacks on the ZC task.\nSince the text input in the ZC task is predefined and cannot be altered, we only use attacks involving the visual modality. When conducting FGA, we construct the text \"A photo of a {object}.\" using the categorys' name to obtain the text set T = {ti}c=1, where c represents the number of categories, following [40]. We extract features through Et to construct the guiding vectors {Et(ti)}c=1, and the true category of the image serves as the guiding label y\u2208 {1, 2, ..., c}. The attack results are presented in the Table 2. We observe that even FGA\u00b9 outperforms IA which is an iterative feature deviation attack.\nWhen executing the ITR task on the CLIP model with ViT image encoder, to construct guiding vectors for FGA, we use not only all the texts in the dataset to construct the text set (\"Test Texts\" in Table 3), but also follow the approach of the ZC task: using the 1000 category names from the ImageNet dataset (\"ImageNet Categories\" in Table 3) or the 80 category names from the MS COCO dataset's object detection task (\"MS COCO Categories\" in Table 3) to construct texts \"There is a {object} in this photo.\" to form the text set T = {ti}c=1. Since in the Flickr30k and MS COCO datasets, an image may contain multiple objects, it is possible that the image matches multiple texts in {ti}c=1. In fact, we do not have annotation information indicating which objects are in the image. Therefore, we compare the cosine similarity between Ez(v) and {Et (ti)}c=1 to find the top 5 texts with the highest cosine similarity to v. When performing FGA, we encourage Ev (v') to move away from the feature vectors of these five texts. From Table 3, we can summarize: (1) When using all texts to construct the feature guidance vectors, FGA achieves the best attack effect, which is intuitive. Moreover, we find that without the text attack, CLIP is already incapacitated on the ITR task. (2) ImageNet includes more categories and therefore contains richer guiding information, resulting in better attack effects compared to using categories from COCO.\nSince the CLIP model only contains two unimodal encoders, attacking before fuse actually utilizes the entire CLIP model. However, the ALBEF model additionally includes a multimodal encoder, so attacking before fuse ignores the multimodal encoder. Therefore, it is necessary to validate the effectiveness of FGA before fusion on the ALBEF model. As shown in Table 4, we conduct this experiment based on the ALBEF model and the ITR task and observe phenomena consistent with Table 3."}, {"title": "4.4 Boosting Transferability", "content": "In this subsection, we transition the attack from the white-box setting to the black-box setting, which is a more common scenario. We use four VLP models: ALBEF, TCL, CLIPVIT, and CLIPCNN. The TCL model is identical to ALBEF except for differences in the design of the Image-Text Contrastive (ITC) loss during training, resulting in different final network weights. The two CLIP models use ViT and CNN as visual encoders, respectively. The degree of difference between these four models varies, which will inevitably affect the transferability of adversarial examples. We will observe this phenomenon in the experiments. Our experimental setup is as follows: (1) Task and Dataset: We conduct black-box adversarial example transfer attacks based on the Image-Text Retrieval (ITR) task and the Flickr30k dataset. (2) Source Model and Target Model: The source model is the model for which we generate adversarial examples through white-box attacks, and then use them to attack the target model. Each model will serve as both source and target models. (3) Attack Methods: The methods we use involve attacking both image and text. SA and CA, which do not focus on transferability, serve as baselines. SGA is the state-of-the-art (SOTA) transfer attack and serves as the comparative method. FGA-Taug is based on FGA-T with additional data augmentation using a set of resize parameters S, following SGA. The differences between SGA and FGA-Taug are in the loss function used for generating adversarial images and the attack process (the former's attack order is \"text, image, text\", while the latter's attack order is \"text, image\"). MFGA-Taug additionally introduces the momentum mechanism. (4) Hyperparameters: All texts are allowed to modify only one word, all image perturbations are limited to 2/255 (lo norm), and the number of iterations is 10, following [54]. The resize parameters S = {0.5, 0.75, 1.25, 1.5}, following SGA.\nThe experimental results are shown in Table 5. We observe the following phenomena: (1) SA, CA, and SGA attack the visual modality based on feature deviation. SGA designs a more advanced set-level feature deviation and introduces data augmentation, improving both white-box and black-box attack effects on the baseline. (2) FGA-Taug based on feature guidance, improves SGA further, simultaneously enhancing both white-box and black-box attack effects again. (3) MFGA-Taug slightly reduces the white-box attack effect but further improves adversarial transferability, which is consistent with the observations in [12]. (4) Attacks based on ALBEF transfer better to TCL than to CLIP because ALBEF and TCL only have differences in parameters, while ALBEF and CLIP are completely different models. The same logic applies to attacks based on TCL. (5) Attacks based on CLIPVIT transfer better to CLIPCNN than to ALBEF or TCL because the model difference between CLIPVIT and CLIPCNN is obviously smaller than the difference with ALBEF or TCL. The same logic applies to attacks based on CLIPCNN."}, {"title": "4.5 Visualization of Targeted Patch FGA", "content": "FGA pushes Ev(v') away from matching text embeddings. Conversely, we can also push E\u03c5(v') closer to a specified text embedding to produce a predetermined error. In unimodal scenarios, this form of attack is called the targeted attack. For example, we have a text set {ti}c=1 and want to push E(v') closer to a specified text tk. In this case, we need to maximize the following function:\n$$L_{gui}^{target} = \\ln \\frac{\\exp(E(v') \\cdot E_t (t_k))}{\\sum_{i=1}^{n} \\exp(E(v') \\cdot E_t (t_i))}$$\nWe add perturbation to the clean image v in patch form, maximizing $L_{gui}^{target}$ to obtain the adversarial patch image v'. We execute FGApatch't on the ALBEF model and compute Grad-CAM visualizations on the self-attention maps. As shown in Figure 4, by guiding E(v') closer to the prompt text through FGAtargetpatch', ALBEF's attention area for v' is concentrated on the patch, resulting in a misunderstanding."}, {"title": "4.6 FGA's Principle of Proximity", "content": "In subection 3.1, it is mentioned that \"$\\frac{\\partial L_{gui}}{\\partial E(x')}$ not only \u201cguides E(v') away from {@yi}n\u2081=1\", but also \"selects a nearby guiding vector that does not belong to the set {@yi}n\u2081=1 and moves closer to it\". The performance decline of VLP models on various V+L downstream tasks in previous experiments sufficiently demonstrates the former. We further prove the latter based on the ZC task and the CLIP model. In the ZC task, we collect the text set T = {ti}c=1 and use it to construct the guiding vectors {Et(ti)}c=1. FGA encourages Ev (v') to move away from Et (ty), where y is the true category, and simultaneously encourages E(v') to move closer to the nearest vector from {Et(ti)}c=1,i\u2260y, meaning that in an ideal situation:\n$$\\underset{i, i \\neq y}{\\operatorname{argmax}} \\frac{E_v(v') \\cdot E_t(t_i)}{||E_v(v) || || E_t(t_i)|| } = \\underset{i, i \\neq y}{\\operatorname{argmax}} ||E_v(v') || ||E_t(t_i)||$$\nIn simpler terms, the category predicted for the clean image v, excluding the true category y, will be the category predicted for the adversarial image v'. Based on the CIFAR-10 dataset, we present the statistical results in Figure 5. In an ideal situation, all positions except the main diagonal should be zero. We observe that the actual situation is close to the ideal. This indicates that the FGA attack indeed tends to guide \"E(v') to move closer to the nearest vector from {Et (ti)}c=1,i\u2260y\u201d. In fact, this principle of proximity promotes v' to automatically choose the nearest decision boundary to cross, which is also one of the reasons for the success of FGA."}, {"title": "4.7 Ablation Experiments", "content": "We investigate the impact of the number of iterations (step) and intensity of noise (\u03f5) based on the image-text retrieval task, Flickr30k dataset and the BEiT-3 model, as shown in Fig 6. We can observe that as \u03f5 and step increase, the effectiveness of the attack gradually strengthens and tends to converge. Specific experimental configurations are detailed in Appendix C."}, {"title": "5 CONCLUSION", "content": "In this paper, we attempt to construct a unified understanding of adversarial vulnerability regarding unimodal models and VLP models. We abstract visual modality attack into a feature guidance form and combine it with text attack and other enhancement mechanisms to establish a general baseline for exploring the security of the VLP domain. In fact, our approach is theoretically orthogonal to many other attack schemes in the unimodal domain, which facilitates further exploration of the vulnerabilities of VLP models and the design of defence algorithms in subsequent work. We hope our code can be beneficial to the community."}, {"title": "A Attack Details", "content": "Three key elements are required to implement FGA, namely an image encoder, guiding vectors, and guiding labels. Below, we will elaborate on the construction of these elements in four scenarios: VE, VQA, VG, and VR."}, {"title": "A.1 Visual Entailment", "content": "Task Detail. We conduct the attack experiment on the VE task [50] with ALBEF [23] which treats VE as a three-classification problem and connects a multi-layer perceptron (MLP) after the [CLS] vector. The input layer and all hidden layers of the MLP constitute P", "contradiction, neutral and entailment\", and the label y \u2208 {0, 1, 2} of the input image-text pair (v, t) provides the direction of the attack, that is, guiding E(v't) the embedding of adversarial image v' deviates from the guiding vector wy.\nDataset Detail. The SNLI-VE dataset [50] is a benchmark for visual entailment, which aims to determine whether an image supports, contradicts, or is neutral to a given natural language statement. This task extends the concept of natural language inference (NLI) to the visual domain, presenting challenges in image and text understanding. The dataset is constructed based on two existing datasets: SNLI (Stanford Natural Language Inference) and Flick30k. We use its test split, which contains 1000```json\n{\n  ": "itle", "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models": "authors\": [\n    \"Haonan Zheng", "Xinyang Deng": "Wen Jiang", "Wenrui Li": "abstract\": \"With Vision-Language Pre-training (VLP) models demonstrating powerful multimodal interaction capabilities, the application scenarios of neural networks are no longer confined to unimodal domains but have expanded to more complex multimodal V+L downstream tasks. The security vulnerabilities of unimodal models have been extensively examined, whereas those of VLP models remain challenging. We note that in CV models, the understanding of images comes from annotated information, while VLP models are designed to learn image representations directly from raw text. Motivated by this discrepancy, we developed the Feature Guidance Attack (FGA), a novel method that uses text representations to direct the perturbation of clean images, resulting in the generation of adversarial images. FGA is orthogonal to many advanced attack strategies in the unimodal domain, facilitating the direct application of rich research findings from the unimodal to the multimodal scenario. By appropriately introducing text attack into FGA, we construct Feature Guidance with Text Attack (FGA-T). Through the interaction of attacking two modalities, FGA-T achieves superior attack effects against VLP models. Moreover, incorporating data augmentation and momentum mechanisms significantly improves the black-box transferability of FGA-T. Our method demonstrates stable and effective attack capabilities across various datasets, downstream tasks, and both black-box and white-box settings, offering a unified baseline for exploring the robustness of VLP models.", "sections": [{"title": "1 Introduction", "content": "ViT provides an effective Transformer-based encoder for the visual modality [14], ensuring the feature extraction of multimodal input through a unified encoding manner, significantly advancing the Vision-and-Language tasks [16, 1, 47, 52]. Various VLP models [23, 51, 45, 40] continually improve performance in V+L downstream tasks through diverse pre-training tasks and architectural designs [2, 45, 20, 27, 29]. However, the previous research in unimodal fields such as Computer Vision (CV) and Natural Language Processing (NLP) highlights the vulnerability of neural networks to adversarial attacks [15, 24]. Although adversarial robustness, particularly in CV, has been extensively explored in terms of attack strategies [4, 36], defence mechanisms [34], and transferability [12, 46], the study of adversarial robustness in VLP models remains challenges [26, 54, 55, 33]. Our study aims to develop a unified architecture to explore commonalities between multimodal and unimodal tasks from the perspective of adversarial attacks. In other words, we seek to bridge the gap, allowing rich findings in unimodal adversarial robustness to be directly applied to the multimodal scenario.\nThe first question we consider is \"Which modality should be paid more attention?\" We primarily focus on perturbations in the image modality, with perturbations in the text modality serving as orthogonal (1) Semantic consistency: Visual adversarial examples maintain semantic consistency, i.e., noise addition within reasonable limits doesn't change human comprehension. Conversely, text adversarial examples risk semantic distortion, potentially introducing spelling errors. (2) Differentiability: Image inputs are continuous and differentiable, unlike text tokens which are discrete and non-differentiable making text-only attacks less effective. (3) Accessibility: In real-world scenarios, text often serves as the primary means of interaction between users and AI models, with limited opportunities for attackers to modify user-generated text. In contrast, models can automatically acquire image data, simplifying the process for attackers to introduce perturbations. In fact, [33] also primarily focuses on enhancing attack strategies in the visual modality to improve adversarial transferability and [55] does not involve text attacks.\nThe second question is \"How to unify multimodal and unimodal scenarios in exploring adversarial robustness?\" We conceptualize image adversarial attacks as a feature-guided process. For unimodal models which primarily learn to understand images through detailed annotation information (such as category labels), attacking an image involves steering its embedding away from the feature vector linked to its correct annotation [15]. This deviation induces a biased comprehension of the image within the network. Alternatively, the image embedding can be guided closer to the feature vector associated with an incorrect annotation, thereby leading the network to make a predetermined error [25]. In the multimodal scenario, models are encouraged to understand images from raw text, providing a broader and more accessible source of supervision [40]. This also offers more flexible guiding information for the adversarial attack. By guiding the image embedding away from the correct text description, we induce the VLP model to develop an incorrect understanding of the image itself. Similarly, directing the embedding towards an incorrect text description intentionally misleads the model into adopting a specific erroneous interpretation. This strategy is termed Feature Guidance Attack (FGA). Expanding upon FGA, we employ adversarial texts from text attacks as guiding information to generate adversarial images, thus obtaining a novel multimodal attack. This approach exacerbates the model's misinterpretation called Feature Guidance with Text Attack (FGA-T). Furthermore, we introduce additional orthogonal mechanisms to enhance the adversarial transferability of FGA-T in the black-box scenario. Code: https://github.com/LibertazZ/FGA\nOur contributions can be summarized as follows:\n\u2022 We provide FGA, using original text as the supervision source for the adversarial attack on VLP models, inducing the network to misinterpret adversarial images.\n\u2022 We introduce cross-modal interaction through adversarial text, forming a novel multimodal adversarial attack that enhances white-box attack strength, and improves black-box transferability through additional mechanisms.\n\u2022 Our approach is theoretically orthogonal to any unimodal attack enhancement mechanism. Empirical evidence based on multiple datasets and VLP models demonstrates the broad applicability of our method to various V+L multimodal tasks, providing a unified baseline for the exploration of multi-modal robustness."}, {"title": "2 Related Work", "content": "From an access perspective to the model, unimodal attacks can be divided into white-box and black-box attacks. In the black-box scenario, due to the target network's opaque weights, attackers typically conduct white-box attacks on an accessible source network, then transfer the adversarial examples to the target network. Therefore, the attack's transferability is also crucial.\nBased on how to constrain perturbation, adversarial attacks on the visual modality can generally be divided into two categories. (1) Global attacks typically involve perturbing all pixels of an image, usually constraining the distance between adversarial and original images based on l\u221e, 12, or l\u2081 norms. Representative methods include FGSM [15], PGD [34], APGD [10], CW [4], etc. (2) Patch attacks, which confine the perturbation to a small area, such as 2% of the image, and allow unrestricted modification of image pixels within that area. Representative methods include LaVAN [19] and Depatch [7]. Since patch attacks are more practical, physical world attacks are usually based on this form. In the text domain, due to the discrete nature of text data, such attacks typically involve subtle modifications to the original text, such as replacing synonyms, inserting additional words, or adjusting sentence structure, without significantly altering the meaning of the text. A representative method is BertAttack [24].\nEnhancing the transferability of adversarial attacks is essentially a generalization problem. The two main approaches to solving the generalization issue are data augmentation and improving the optimization algorithm, thus dividing transfer attack methods into two categories. (1) Typical methods that boost transferability through data augmentation, such as DI [49] (Diverse Inputs), TI [13] (Translation Invariant) and SI [30] (Scale Invariant). (2) Typical schemes that improve optimization algorithm, such as MI [12] (Momentum Iterative), NI [30] (Nesterov Iterative), VMI [46] (Varied Momentum Iterative), VNI [46] (Varied Nesterov Iterative)."}, {"title": "2.2 Multimodal Adversarial Attack", "content": "This subsection discusses relevant VLP models and multimodal adversarial attack methods.\nVLP models based on different combinations of pre-training tasks can be roughly divided into three categories. (1) Aligned models: CLIP [40] contains two unimodal encoders to align multimodal embeddings based on Image-Text Contrastive (ITC) loss. (2) Fused models by matching: ViLT [20] introduces both Image-Text Matching (ITM) and Masked Language Modeling (MLM) pre-training for V+L tasks. Models like ALBEF [23], TCL [51], BLIP [22], and VLMo [3] build on it, first aligning multimodal features using ITC loss, then fusing cross-modal features using ITM and MLM losses. (3) Fused models by Masked Data Modeling (MDM): BEIT [2] and BEiTV2 [38] propose and improve Masked Image Modeling (MIM) loss. BEiT3, based on it, first aligns multimodal features using ITC loss, then fuses cross-modal features using MIM, MLM, and Masked Language-Vision Modeling (MLVM) losses.\nAttacking VLP models is a novel topic. Existing work has provided valuable insights. Co-Attack [54] designs general optimization objectives based on different embeddings (unimodal or multimodal) and experimentally demonstrates that using text attacks or image attacks alone is not as effective as using both in combination, providing a general baseline for subsequent works. SGA [33] points out that improving the diversity of multimodal interaction can enhance the transferability of multimodal adversarial examples. AdvCLIP [55] provides a framework to learn a universal adversarial patch on pre-trained models for transfer attacks on downstream fine-tuned models. These works are limited to VLP models and ignore the connection between unimodal and multimodal scenarios, which is our main motivation."}, {"title": "3 Methodology", "content": "An image feature extractor E (e.g., an image contrastive representation encoder [18, 5, 17] or a VLP model's visual encoder) projects the image into a feature vector for various visual tasks like image classification and object detection. Without regarding the subsequent usage, the most intuitive approach to generate an adversarial example x' for an image x is to encourage the feature vectors E(x') and E(x) to be as distant as possible [54]. This universal strategy is termed \"Feature Deviation Attack\" (FDA), which involves maximizing the loss function:\n$$L_{dev} = -E(x') \\cdot E(x).$$\nAssuming that in the embedding space, there exists a set of guiding vectors W = {wi}m\u2081, w \u2208 Rd, and there is a set of guiding labels Y = {yi}n\u2081=1, y \u2208 {1, 2, ..., m} specifying that E(x') should be distant from the guiding vectors {@yi}n\u2081=1 \u2208 W. We refer to this strategy as \"Feature Guidance Attack\" (FGA). To realize the above concept, we need to maximize the loss function:\n$$L_{gui} = \\sum_{i=1}^{n} \\log \\frac{\\exp(E(x') \\cdot W_{y_i})}{\\sum_{j=1}^{m} \\exp(E(x') \\cdot \\omega_j)}$$\nwhere exp() represents the exponential function with Euler's number e as the base, and ln() stands for the logarithm to the base e. Based on Lgui or Ldev, we can apply the PGD process [34], gradually pushing the clean example x along the gradient direction to maximize the loss function, ultimately obtaining the adversarial example x'. By the chain rule of gradients, $\\frac{\\partial L}{\\partial E(x')}$  represents the direction of perturbation added to the input example. While we focus on $\\frac{\\partial L}{\\partial E(x')}$, which represents the movement direction of the feature vector:\n$$\\frac{\\partial L_{dev}}{\\partial E(x')} = -E(x)$$\n$$\\frac{\\partial L_{gui}}{\\partial E(x')} = \\sum_{i=1}^{n} \\frac{\\exp(E(x') \\cdot \\omega_k)}{\\sum_{j=1}^{m} \\exp(E(x') \\cdot \\omega_j)} \\cdot \\omega_{y_i} + \\omega_k$$\nIt can be observed that feature deviation loss promotes the movement of E(x') towards -E(x), which means moving away from E(x). While, Regarding the first term of $\\frac{dL_{gui}}{dE(x')}$, it encourages E(x') to move away from the guiding vectors {@y\u2081}n\u2081=1, and assigning equal weight 1/n to each of them. The second term encourages E(x') to approach the guiding vector \u03c9k \u2208 W, with a weight of $\\exp(E(x')\\cdot \\omega_k)/\\sum_{j=1}^{m} \\exp(E(x') \\cdot w_j)$, which means the closer E(x') is to a guiding vector, the greater the weight assigned to it. Due to the presence of the first term, E(x') is far from {@yi}n\u2081=1, resulting in the weight of $w_{y_i}$ being almost zero in the second term. Consequently, the second term effectively facilitates E(x') in selecting a nearby guiding vector that does not belong to the set {@yi}n\u2081=1 and moving closer to it.\nMost existing VLP models typically consist of two unimodal encoders, a text encoder Et and a visual encoder Ev, along with a multimodal feature fusion encoder Em. For a single image-text paired example (v, t), it is first mapped to a shared feature space separately by Et and Ev for aligning image and text features. Subsequently, cross-modal feature fusion is conducted through Em. Therefore, VLP models focus on three key embeddings:Ev (v), Et (t), and Em (Ev (v), Et (t)), all corresponding to the [CLS] vector. We will focus on finding guiding vectors in the embedding space and constructing guiding labels to execute FGA on VLP models."}, {"title": "3.2 Attacking after Fuse", "content": "In this scenario, we focus on the fused embedding Em (Ev (v), Et (t)). For different V+L downstream tasks, it needs to be fed into different subsequent models which can be uniformly understood as comprising a projector P and a linear classification head h. P projects the fused embedding into a task-specific downstream embedding space, followed by h performing classification on this embedding. We can rewrite P(Em (Ev (v), Et (t))) as E(vt), obtaining an image encoder conditioned on the textual modality. The weight of the linear classification head, W = {wi}c=1 \u2208 Rd\u00d7c (c is the number of categories), serve as guiding vectors. Using label information as guiding labels Y, we thus have all the necessary components to implement the Feature Guidance Attack. See Appendix A for more details and explanations about Visual Question Answering (VQA), Visual Reasoning (VR), etc."}, {"title": "3.3 Attacking before Fuse", "content": "In this scenario, only two unimodal encoders are used: Ev and Et. The primary intention of VLP models is to learn directly from raw text descriptions of images, utilizing a broader source of supervision [40]. This implies that in CV models, image understanding comes from pre-provided labels, such as image categories, pixel categories, or annotated bounding boxes. In contrast, in VLP models, the understanding of images originates from raw text. Consequently, using text as supervisory information to generate image adversarial examples becomes a natural approach. To implement this approach, we first acquire a text set T = {ti}m\u2081. Then, We use the text encoder to obtain a set of guiding vectors {wi}m\u2081=1 = {Et(ti)}m\u2081. To obtain this text set T, all texts are gathered from the dataset. Here, by utilizing the dataset's annotations, we can identify which texts in the text set match with the image v, thereby obtaining the guiding labels Y. By this point, all elements necessary for executing FGA have been acquired: the image encoder Ev, the set of guiding vectors {Et (ti)}m\u2082=1, and the guiding labels Y. By maximizing Lgui, the feature vector Ev (v') will diverge from the text representations {Et (ty)} yey that match v, thereby generating adversarial images v'. For more details on executing iterations of FGA and how it can be combined with typical attack strategies in the unimodal domain, refer to Appendix B."}, {"title": "3.4 Boosting Transferability before Fuse", "content": "SGA[33] points out that multimodal adversarial examples have better transferability than unimodal adversarial examples. Therefore, we need to introduce text attack into FGA before fuse. We consider an image minibatch V = {vi}n=1 and the text set Ti represents all texts that match with the image vi. Firstly, for each text t \u2208 Ti, we handle the following optimization problem to generate adversarial text t':\n$$t' = \\underset{t'}{\\operatorname{argmax}} \\left(-\\frac{E_t(t') \\cdot E_v(v_i)}{||E_t(t')||_2 ||E_v(v_i)||_2}\\right)$$\nwhere ||\u00b7||2 denotes the Euclidean distance.\nAt this point, we obtain the adversarial text set T', and we denote T = T\u2081 \u222a T\u2082 . . . \u222a Tn \u222a T\u2081'\u222a T\u2082' . . . \u222a T'. Secondly, for vi \u2208 V, where Ti is its matching texts and T'i is the adversarial texts, to generate adversarial example v, we use the feature guidance loss to encourage Ev(v) to simultaneously move away from both Et (Ti) and Et (T'):\n$$L_{gui} (v) = \\frac{1}{len_i} \\sum_{i=1}^{n} \\log \\frac{\\exp(E_v(v) \\cdot E_t(t^*))}{\\sum_{t \\in T_i \\cup T'_i} \\exp(E_v(v) \\cdot E_t(t))}$$\nleni represents the length of text set Ti \u222aT'i.\nBuilding on this foundation, we further introduce two strategies to enhance transferability: (1) Following SGA [33], we preset a set of resize parameters S = {S1, S2, ..., Sm}, where h(v, sk) denotes the resizing function that takes the image v and the scale coefficient sk as inputs. After data augmentation, the objective function we aim to maximize is no longer Lgui (v) but rather $\\sum_{k=1}^{m} L_{gui}(h(v, s_k))$, where h(v, sk) represents the augmented image. (2) Following MI-FGSM [12], we introduce the momentum mechanism, where the current perturbation direction is determined by both the current gradient and the historical gradients from previous iterations. See Appendix B.2 for more details."}, {"title": "4 Experiments", "content": "Our experimental section involves four typical VLP models: CLIP, ALBEF, TCL and BEiT3. CLIP is a typical aligned model, consisting solely of two unimodal encoders. The latter three are fused models, containing two unimodal encoders and a multimodal encoder. ALBEF and TCL share the same architecture with some differences in the details of ITC loss. Besides, ALBEF and BEiT3 have two main differences: (1) Different Pre-training Tasks: ALBEF is based on three pre-training tasks: ITC, ITM and MLM. In contrast, BEiT3 is based on three MDM tasks: MLM, MIM, and MVLM. (2) Different Model Structures: In ALBEF, the three encoders are independent of each other. BEiT3, however, uses the Multiway Transformer to split the feed-forward layer into three parallel paths, thereby obtaining three encoders."}, {"title": "4.1.2 V+L Downstream Tasks", "content": "In this part, we will introduce each downstream task involved in the experiments, along with the models and datasets used to perform these tasks.\nVisual Entailment (VE) is a fine-grained visual reasoning task, where given a pair of (v, t), the model needs to determine whether the text is supported by the image (entailment), whether the text is unrelated to the image (neutral), or whether the text contradicts the content of the image (contradictory). This task will be conducted based on the ALBEF model and the SNLI-VE [50] dataset.\nVisual Question Answering (VQA) requires the model to predict a correct answer given an image and a question [16, 37]. It can be viewed as a multi-answer classification problem, or as an answer generation problem. We use the VQAv2 [16] dataset and the ALBEF model, which performs the VQA task through text generation.\nVisual Grounding (VG) requires the model to find parts of the image that match the given textual description. We perform this task based on the RefCOCO+ [53] dataset and the ALBEF model.\nVisual Reasoning (VR) requires the model to predict if a given text describes a pair of images. This task necessitates that the model not only understands the content of individual images but also compares and reasons about the relationship between two images. Therefore, the input consists of a pair of images and a piece of text. We use the BEiT3 model and the NLVR2 [43] dataset to perform this task.\nrequires using predefined category descriptions (such as \"a cat,\" \"a car,\" etc.) as text inputs and mapping these descriptions to the embedding space by text encoder. Then, for a given image, the similarity between the image embedding and each category description embedding is calculated, and the image is classified into the category with the highest similarity. Due to the CLIP model's strong zero-shot capacity, we use it along with three datasets: CIFAR-10 [21], CIFAR-100 [21], and ImageNet [11], to perform this task.\nImage-Text Retrieval (ITR) involves retrieving relevant images from an image database given a text query, and vice versa [52, 32, 48]. We perform this task based on the CLIP, ALBEF and TCL models, and the Flickr30k [39] and MS COCO [31] datasets."}, {"title": "4.2 Attack Effectiveness after Fuse", "content": "This subsection explores the effectiveness of attacks on the fused feature vector. Since VE, VQA, VG, and VR rely on this vector, we choose to evaluate these four tasks. As Table 1 illustrates, the evaluation follows the baseline set by [54], TA represents alone text attack using BertAttack. IA represents image attack based on feature deviation loss. SA stands for separate unimodal attack, indicating that TA and IA are executed separately without modal interaction, and CA denotes the multimodal white-box attack Co-Attack [54] which introduces cross-modal interaction. For fairness, we set the l\u221e perturbation constraint for the image modality in FGA and FGA-T to \u03f5 = 2/255 with 10 iterations, consistent with IA, SA, and CA. Additionally, we explore the attack effectiveness when the number of iterations is 1, i.e., single-step attack, namely FGA\u00b9 and FGA-T\u00b9. We also investigate the effectiveness under the l\u2081 constraint, namely FGAle\u2081, with el\u2081 = 255 and 20 iterations. Besides, we perform FGA in patch form, namely FGApat, with 100 iterations, a single-step l\u221e constraint of \u03b1 = 8/255, and a patch area of 2% of the total image area, with a random location. Furthermore, when involving text attack, BertAttack is used with a restriction of 1 perturbable token, following [54, 33].\nWe can observe from Table 1: (1) Under all tasks, FGA-T consistently achieves the best white-box attack performance, validating the effectiveness of the feature guidance approach and its orthogonality with text attack. (2) Even with only a single step, the feature guidance method is sufficient to produce effective adversarial examples, performing on par with or even better than the baseline. This provides a faster and more convenient attack strategy. (3) The feature guidance approach exhibits good orthogonality with other attack strategies in Computer Vision. When combined with 11 attack or patch attack, it demonstrates strong performance."}, {"title": "4.3 Attack Effectiveness before Fuse", "content": "In this subsection, we explore the effectiveness of attacks on two unimodal encoders. The tasks of ZC and ITR primarily rely on two unimodal embeddings. We first conduct attacks on the ZC task.\nSince the text input in the ZC task is predefined and cannot be altered, we only use attacks involving the visual modality. When conducting FGA, we construct the text \"A photo of a {object}.\" using the categorys' name to obtain the text set T = {ti}c=1, where c represents the number of categories, following [40]. We extract features through Et to construct the guiding vectors {Et(ti)}c=1, and the true category of the image serves as the guiding label y\u2208 {1, 2, ..., c}. The attack results are presented in the Table 2. We observe that even FGA\u00b9 outperforms IA which is an iterative feature deviation attack.\nWhen executing the ITR task on the CLIP model with ViT image encoder, to construct guiding vectors for FGA, we use not only all the texts in the dataset to construct the text set (\"Test Texts\" in Table 3), but also follow the approach of the ZC task: using the 1000 category names from the ImageNet dataset (\"ImageNet Categories\" in Table 3) or the 80 category names from the MS COCO dataset's object detection task (\"MS COCO Categories\" in Table 3) to construct texts \"There is a {object} in this photo.\" to form the text set T = {ti}c=1. Since in the Flickr30k and MS COCO datasets, an image may contain multiple objects, it is possible that the image matches multiple texts in {ti}c=1. In fact, we do not have annotation information indicating which objects are in the image. Therefore, we compare the cosine similarity between Ez(v) and {Et (ti)}c=1 to find the top 5 texts with the highest cosine similarity to v. When performing FGA, we encourage Ev (v') to move away from the feature vectors of these five texts. From Table 3, we can summarize: (1) When using all texts to construct the feature guidance vectors, FGA achieves the best attack effect, which is intuitive. Moreover, we find that without the text attack, CLIP is already incapacitated on the ITR task. (2) ImageNet includes more categories and therefore contains richer guiding information, resulting in better attack effects compared to using categories from COCO.\nSince the CLIP model only contains two unimodal encoders, attacking before fuse actually utilizes the entire CLIP model. However, the ALBEF model additionally includes a multimodal encoder, so attacking before fuse ignores the multimodal encoder. Therefore, it is necessary to validate the effectiveness of FGA before fusion on the ALBEF model. As shown in Table 4, we conduct this experiment based on the ALBEF model and the ITR task and observe phenomena consistent with Table 3."}, {"title": "4.4 Boosting Transferability", "content": "In this subsection, we transition the attack from the white-box setting to the black-box setting, which is a more common scenario. We use four VLP models: ALBEF, TCL, CLIPVIT, and CLIPCNN. The TCL model is identical to ALBEF except for differences in the design of the Image-Text Contrastive (ITC) loss during training, resulting in different final network weights. The two CLIP models use ViT and CNN as visual encoders, respectively. The degree of difference between these four models varies, which will inevitably affect the transferability of adversarial examples. We will observe this phenomenon in the experiments. Our experimental setup is as follows: (1) Task and Dataset: We conduct black-box adversarial example transfer attacks based on the Image-Text Retrieval (ITR) task and the Flickr30k dataset. (2) Source Model and Target Model: The source model is the model for which we generate adversarial examples through white-box attacks, and then use them to attack the target model. Each model will serve as both source and target models. (3) Attack Methods: The methods we use involve attacking both image and text. SA and CA, which do not focus on transferability, serve as baselines. SGA is the state-of-the-art (SOTA) transfer attack and serves as the comparative method. FGA-Taug is based on FGA-T with additional data augmentation using a set of resize parameters S, following SGA. The differences between SGA and FGA-Taug are in the loss function used for generating adversarial images and the attack process (the former's attack order is \"text, image, text\", while the latter's attack order is \"text, image\"). MFGA-Taug additionally introduces the momentum mechanism. (4) Hyperparameters: All texts are allowed to modify only one word, all image perturbations are limited to 2/255 (lo norm), and the number of iterations is 10, following [54]. The resize parameters S = {0.5, 0.75, 1.25, 1.5}, following SGA.\nThe experimental results are shown in Table 5. We observe the following phenomena: (1) SA, CA, and SGA attack the visual modality based on feature deviation. SGA designs a more advanced set-level feature deviation and introduces data augmentation, improving both white-box and black-box attack effects on the baseline. (2) FGA-Taug based on feature guidance, improves SGA further, simultaneously enhancing both white-box and black-box attack effects again. (3) MFGA-Taug slightly reduces the white-box attack effect but further improves adversarial transferability, which is consistent with the observations in [12]. (4) Attacks based on ALBEF transfer better to TCL than to CLIP because ALBEF and TCL only have differences in parameters, while ALBEF and CLIP are completely different models. The same logic applies to attacks based on TCL. (5) Attacks based on CLIPVIT transfer better to CLIPCNN than to ALBEF or TCL because the model difference between CLIPVIT and CLIPCNN is obviously smaller than the difference with ALBEF or TCL. The same logic applies to attacks based on CLIPCNN."}, {"title": "4.5 Visualization of Targeted Patch FGA", "content": "FGA pushes Ev(v') away from matching text embeddings. Conversely, we can also push E\u03c5(v') closer to a specified text embedding to produce a predetermined error. In unimodal scenarios, this form of attack is called the targeted attack. For example, we have a text set {ti}c=1 and want to push E(v') closer to a specified text tk. In this case, we need to maximize the following function:\n$$L_{gui}^{target} = \\ln \\frac{\\exp(E(v') \\cdot E_t (t_k))}{\\sum_{i=1}^{n} \\exp(E(v') \\cdot E_t (t_i))}$$\nWe add perturbation to the clean image v in patch form, maximizing $L_{gui}^{target}$ to obtain the adversarial patch image v'. We execute FGApatch't on the ALBEF model and compute Grad-CAM visualizations on the self-attention maps. As shown in Figure 4, by guiding E(v') closer to the prompt text through FGAtargetpatch', ALBEF's attention area for v' is concentrated on the patch, resulting in a misunderstanding."}, {"title": "4.6 FGA's Principle of Proximity", "content": "In subection 3.1, it is mentioned that \"$\\frac{\\partial L_{gui}}{\\partial E(x')}$ not only \u201cguides E(v') away from {@yi}n\u2081=1\", but also \"selects a nearby guiding vector that does not belong to the set {@yi}n\u2081=1 and moves closer to it\". The performance decline of VLP models on various V+L downstream tasks in previous experiments sufficiently demonstrates the former. We further prove the latter based on the ZC task and the CLIP model. In the ZC task, we collect the text set T = {ti}c=1 and use it to construct the guiding vectors {Et(ti)}c=1. FGA encourages Ev (v') to move away from Et (ty), where y is the true category, and simultaneously encourages E(v') to move closer to the nearest vector from {Et(ti)}c=1,i\u2260y, meaning that in an ideal situation:\n$$\\underset{i, i \\neq y}{\\operatorname{argmax}} \\frac{E_v(v') \\cdot E_t(t_i)}{||E_v(v) || || E_t(t_i)|| } = \\underset{i, i \\neq y}{\\operatorname{argmax}} ||E_v(v') || ||E_t(t_i)||$$\nIn simpler terms, the category predicted for the clean image v, excluding the true category y, will be the category predicted for the adversarial image v'. Based on the CIFAR-10 dataset, we present the statistical results in Figure 5. In an ideal situation, all positions except the main diagonal should be zero. We observe that the actual situation is close to the ideal. This indicates that the FGA attack indeed tends to guide \"E(v') to move closer to the nearest vector from {Et (ti)}c=1,i\u2260y\u201d. In fact, this principle of proximity promotes v' to automatically choose the nearest decision boundary to cross, which is also one of the reasons for the success of FGA."}, {"title": "4.7 Ablation Experiments", "content": "We investigate the impact of the number of iterations (step) and intensity of noise (\u03f5) based on the image-text retrieval task, Flickr30k dataset and the BEiT-3 model, as shown in Fig 6. We can observe that as \u03f5 and step increase, the effectiveness of the attack gradually strengthens and tends to converge. Specific experimental configurations are detailed in Appendix C."}, {"title": "5 CONCLUSION", "content": "In this paper, we attempt to construct a unified understanding of adversarial vulnerability regarding unimodal models and VLP models. We abstract visual modality attack into a feature guidance form and combine it with text attack and other enhancement mechanisms to establish a general baseline for exploring the security of the VLP domain. In fact, our approach is theoretically orthogonal to many other attack schemes in the unimodal domain, which facilitates further exploration of the vulnerabilities of VLP models and the design of defence algorithms in subsequent work. We hope our code can be beneficial to the community."}, {"title": "A Attack Details", "content": "Three key elements are required to implement FGA, namely an image encoder, guiding vectors, and guiding labels. Below, we will elaborate on the construction of these elements in four scenarios: VE, VQA, VG, and VR."}, {"title": "A.1 Visual Entailment", "content": "Task Detail. We conduct the attack experiment on the VE task [50", "23": "which treats VE as a three-classification problem and connects a multi-layer perceptron (MLP) after the [CLS", "contradiction, neutral and entailment\", and the label y \u2208 {0, 1, 2} of the input image-text pair (v, t) provides the direction of the attack, that is, guiding E(v't) the embedding of adversarial image v' deviates from the guiding vector wy.\nDataset Detail. The SNLI-VE dataset [50": "is a benchmark for visual entailment", "datasets": "SNLI (Stanford Natural Language Inference) and Flick30k. We use its test split, which contains 1000"}]}]}