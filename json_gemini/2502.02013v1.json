{"title": "Layer by Layer: Uncovering Hidden Representations in Language Models", "authors": ["Oscar Skean", "Md Rifat Arefin", "Dan Zhao", "Niket Patel", "Jalal Naghiyev", "Yann LeCun", "Ravid Shwartz-Ziv"], "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have driven remarkable progress in natural language processing (NLP), achieving state-of-the-art results on many tasks (Brown et al., 2020; Devlin, 2018; Li et al., 2022). At the heart of most applications lies a common assumption: final-layer representations are the most useful for downstream tasks. Yet a fundamental question remains: does the final layer always yield the best representation?"}, {"title": "2. Related Work", "content": "Understanding Neural Representations. A long line of research has aimed to understand how deep neural networks encode and organize information. Early studies employed linear probes to interpret intermediate layers (Alain & Bengio, 2017), while subsequent efforts introduced more sophisticated techniques such as SVCCA (Raghu et al., 2017) to compare learned features across architectures and training regimes. Although these approaches shed light on representation dynamics, most focus on vision backbones or relatively shallow models. In contrast, our work extends layer-wise analysis to large-scale language models, highlighting specific behaviors of intermediate layers in autoregressive Transformers, state-space models (SSMs), and beyond.\nLayer-wise Analysis in Language Models. Transformer-based LLMs have sparked significant interest in which layers capture linguistic properties such as syntax and semantics (Liu et al., 2019; Tenney et al., 2019; Voita et al., 2019). More recent work (Jin et al., 2024; Gurnee & Tegmark, 2023; Fan et al., 2024) has shown that mid-depth layers sometimes hold surprisingly robust features, challenging the typical focus on final layers. Our contribution unifies and expands these observations through a large-scale, theoretical-empirical framework that quantifies the quality of every layer's representation via information theory, geometry, and invariance metrics.\nArchitectural Comparisons. Transformers remain the dominant architecture for NLP (Vaswani et al., 2017), but they come in multiple variants. Encoder-only models (e.g., BERT (Devlin, 2018)) typically use bidirectional attention and masked-language objectives, while decoder-only architectures (e.g., GPT (Brown et al., 2020)) follow an autoregressive paradigm. Meanwhile, newer state-space models (SSMs) such as Mamba (Gu & Dao, 2024) use recurrent-style dynamics for efficient long-sequence processing. Although these designs differ significantly in attention mechanisms and sequence modeling strategies, there has been little direct comparison of hidden-layer representations across them. In our work, we analyze Transformers (both encoder-and decoder-only) and SSMs under a common set of metrics, highlighting contrasts in how intermediate layers compress or preserve information and showing that intermediate-layer representations can excel across multiple architectures.\nRepresentation Quality Metrics. A variety of metrics have been proposed to quantify the \"quality\" of learned representations. We group them into three main categories:\n\u2022 Information-theoretic measures capture how much a model's internal representations compress or preserve relevant information. For example, the Information Bottleneck (Shwartz-Ziv & Tishby, 2019; Shwartz-Ziv, 2022) analyzes whether intermediate layers discard noise while retaining essential features.\n\u2022 Geometric measures focus on the structure of embeddings in high-dimensional space. Classical approaches include analyzing singular values or effective rank of the representation matrix (Garrido et al., 2023), while more recent work explores curvature (Hosseini & Fedorenko, 2023) to quantify how smoothly tokens are mapped across consecutive positions or time steps.\n\u2022 Task-based or invariance metrics evaluate how well representations support downstream goals. For instance, augmentations-based approaches such as InfoNCE (Oord et al., 2018) and LiDAR (Thilak et al.,"}, {"title": "3. A Unified Framework for Neural Representations", "content": "Key Takeaway: Matrix-based entropy unifies seemingly disparate metrics of representation quality, providing a single theoretical lens for analyzing compression, geometry, and invariance.\nA central challenge in analyzing internal representations is determining how to assess their quality. Although existing work draws on numerous ideas\u2014from mutual information to geometric manifold analysis to invariance under augmentations\u2014these threads can seem disparate. In this section, we consolidate them into a unified theoretical framework that shows how these seemingly different metrics connect and why they collectively measure \u201crepresentation quality.\u201d"}, {"title": "3.1. Notation and Motivation", "content": "Consider a neural network that maps inputs x (e.g., tokens in a sequence) to internal hidden states Z. We denote $Z \\in \\mathbb{R}^{N \\times D}$ as a matrix of N data samples (or tokens) in D dimensions. Some key questions arise:\n1. How compressed are these representations?\n2. How robust are they to small perturbations or augmentations?\n3. How do they geometrically organize different inputs?\nThe answers can illuminate which layers strike the right balance between preserving relevant features and discarding noise."}, {"title": "3.2. Matrix-Based Entropy: A Common Theoretical Thread", "content": "We focus on a key quantity known as matrix-based entropy (Giraldo et al., 2014; Skean et al., 2023), which applies directly to the Gram matrix $K = ZZ^T$. Let {$\\lambda_i(K)$} be the (nonnegative) eigenvalues of K. For any order $\\alpha > 0$, define:\n$S_{\\alpha}(Z) = \\frac{1}{1 - \\alpha} \\log\\left(\\frac{\\sum_{i=1}^r \\lambda_i(K)}{\\text{tr}(K)}\\right)^{\\alpha}$\nwhere $r = \\text{rank}(K) \\leq \\min(N, D)$. Intuitively, if only a few eigenvalues dominate, $S_\\alpha(Z)$ is small\u2014indicating a highly compressed representation. Conversely, if Z is spread out across many principal directions, $S_\\alpha(Z)$ is large. By varying $\\alpha$, one smoothly transitions between notions like collision entropy ($\\alpha = 2$) and von Neumann entropy ($\\alpha \\rightarrow 1$). We will typically use $\\alpha = 1$ for simplicity.\nImplication: bridging geometry, invariance, and local vs. global features. A key benefit of matrix-based entropy is that it unifies multiple representational perspectives:\n\u2022 Compression or information content: A handful of large eigenvalues in $K = ZZ^T$ indicates that Z is low-rank, i.e. the model has collapsed much of the input variation into fewer dimensions. In contrast, a more uniform eigenvalue spectrum implies higher-entropy, more diverse features.\n\u2022 Geometric smoothness: If tokens within a prompt follow a trajectory in embedding space with sharp turns, that curvature can manifest as skewed eigenvalue spectra (Hosseini & Fedorenko, 2023). Curvature also differentiates local transitions (token-to-token) from global structural patterns across longer segments or entire prompts.\n\u2022 Invariance under augmentations: Metrics like InfoNCE (Oord et al., 2018) and LiDAR (Thilak et al.,"}, {"title": "3.3. Representation Evaluation Metrics", "content": "Key Takeaway: Information-theoretic, geometric, and invariance-based metrics offer complementary perspectives on representation quality that can all be understood through matrix-based entropy.\nWe now introduce the seven representation evaluation metrics used in our experiments, grouped into three broad categories: (1) information-theoretic, (2) geometric, and (3) augmentation-invariance. All relate back to the Gram matrix K and hence to Eq. (1)."}, {"title": "3.3.1. INFORMATION-THEORETIC METRICS", "content": "Prompt Entropy. Following Wei et al. (2024), we apply matrix-based entropy (Eq. 1) to the token embeddings within a single prompt. This prompt entropy quantifies how widely tokens are spread in the embedding space. Higher entropy indicates more diverse, less redundant token-level features; lower entropy implies stronger compression.\nDataset Entropy. We can also aggregate embeddings across N prompts by taking the mean token embedding of each prompt to form $Z \\in \\mathbb{R}^{N \\times D}$. Applying entropy to Z yields a dataset-level measure of global diversity\u2014revealing how distinctly the model separates different inputs.\nEffective Rank (Roy & Vetterli, 2007a) can be shown to be a lower bound to $\\exp(S_1(Z))$, highlighting how dimensionality effectively shrinks if the representation is strongly compressed. We prove this connection later in Theorem 1. This has implications for popular representation evaluation metrics such as RankMe (Garrido et al., 2023) and LiDAR (Thilak et al., 2024), which are both inspired by Effective Rank."}, {"title": "3.3.2. GEOMETRIC METRICS", "content": "Curvature. Proposed by Hosseini & Fedorenko (2023), curvature captures how sharply the token embeddings turn when viewed as a sequence in $\\mathbb{R}^D$. For a prompt of length L, let $v_k = Z_{k+1} - z_k$ be the difference between consecutive tokens. The average curvature is:\n$C = \\frac{1}{L-2} \\sum_{k=1}^{L-2} \\arccos\\left(\\frac{v_{k+1}^T v_k}{||v_{k+1}|| \\cdot ||v_k||}\\right)$\nHigher curvature means consecutive tokens shift direction abruptly and more local level features; lower curvature suggests a smoother trajectory and global level features."}, {"title": "3.3.3. AUGMENTATION INVARIANCE METRICS", "content": "Lastly, we assess how stable the model's representations are to small perturbations of the same input (e.g., random character swaps, keyboard-level changes; see Appendix). Suppose $p_1$ is augmented into $p_a)$ and $p_b)$. After embedding these, we compare the row vectors in $Z_1, Z_2 \\in \\mathbb{R}^{N \\times D}$ under different scoring criteria:\nInfoNCE. This self-supervised objective (Oord et al., 2018) encourages matched samples to lie close in embedding space while pushing unmatched samples away. A lower InfoNCE loss indicates stronger invariance to augmentation.\nLiDAR. LiDAR (Thilak et al., 2024) uses a linear discriminant approach that measures within-class versus between-class scatter. Treating each prompt as its own class, LiDAR checks how well augmentations form tight clusters.\nDiME. Similarly, DiME (Skean et al., 2023) is grounded in matrix-based entropy. It compares real paired samples against random pairings to estimate how uniquely aligned correct augmentations are."}, {"title": "3.4. Core Theoretical Results", "content": "Key Takeaway: Our theoretical framework establishes concrete connections between representation entropy and downstream performance through properties like effective rank and invariance.\nHere, we summarize key statements that justify why these metrics meaningfully measure representation quality. We refer to the appendix F for details and proofs. Beyond serving as a unifying view, matrix-based entropy also connects to foundational concepts like majorization, Schur concavity, and mutual information. Furthermore, we can directly relate the eigenvalue entropy to the matrix entropy, most naturally via the Effective Rank (Roy & Vetterli, 2007b). The following theorem makes this connection explicit.\nTheorem 1 (Lower Bound via Effective Rank). For Shannon-based entropy ($\\alpha \\rightarrow 1$),\n$\\text{EffRank}(Z) \\leq \\exp(S_1(Z))$,\nmeaning a large effective rank implies a high entropy."}, {"title": "4. Empirical Results", "content": "In this section, we empirically validate our theoretical framework through extensive experiments across architectures, scales, and training regimes. Our investigation centers on three key questions:\n\u2022 Do intermediate layers consistently outperform final layers across diverse downstream tasks?\n\u2022 How do these intermediate representations differ across architectures, training stages, and scales?\n\u2022 How does post-training methods (e.g., fine-tuning and chain-of-thought) reshape representations?"}, {"title": "4.1. Downstream Task Performance", "content": "Key Takeaway: Intermediate layers of language models consistently outperform final layers across all architectures and tasks, challenging the conventional wisdom of using final-layer representations.\nIn this section, we use intermediate layers for downstream embedding tasks and employ our unified framework from Section 3, measuring all the embeddings across all layers."}, {"title": "4.1.1. EXPERIMENTAL SETUP", "content": "Models We evaluate three distinct architectural families: Pythia and Llama3 (decoder-only transformer) (Biderman et al., 2023; Dubey et al., 2024), Mamba (state space model) (Gu & Dao, 2024), BERT (encoder-only transformer) (Devlin, 2018) and LLM2Vec models (bidirectional attention) (BehnamGhader et al., 2024).\nTasks We test each layer's embeddings on 32 tasks from the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022), spanning classification, clustering, and reranking. This comprehensive evaluation provides insight into how different layers capture task-relevant features. For a full list of the tasks, refer to the Appendix."}, {"title": "4.1.2. INTERMEDIATE LAYERS OFTEN OUTPERFORM FINAL LAYERS", "content": "A key question is whether final-layer embeddings are indeed optimal for downstream tasks. In Figure 1, we compare average performance on MTEB tasks across all layers of the three models.\nKey observation. In nearly every task, some intermediate layer outperforms the final layer. The absolute improvement ranges from 2% to as high as 16% on average, and the best layer often resides around the mid-depth of the network. This phenomena is consistent across all the different architectures. This confirms emerging observations in recent work for generation tasks (Bordes et al., 2023; El-Nouby et al., 2024; Chen et al., 2020; Fan et al., 2024) and extends them to a wider range of benchmarks and tasks.\nWhy do these layers matter? From our theoretical perspective, intermediate layers appear to strike a balance between retaining sufficient information (avoiding over-compression) and discarding low-level noise. Later in Section 4.2, we show that these sweet spots are not random but tied to how intermediate layers are processing information."}, {"title": "4.1.3. LAYER-WISE METRICS CORRELATE WITH DOWNSTREAM PERFORMANCE", "content": "To validate our framework's relevance, we analyze how each metric (entropy, InfoNCE, etc.) correlates with downstream performance. Figure 3 and Figure 8 show distance correlations between metrics and task scores for Pythia-410M. We make several key observations:\n\u2022 All metrics show strong relationships with performance\n\u2022 DiME, curvature, and InfoNCE exhibit particularly strong correlations\n\u2022 Associations remain robust across different correlation measures (Spearman, Kendall)\nThese relationships suggest that our metrics effectively capture what makes intermediate representations powerful for downstream tasks."}, {"title": "4.2. Architectural and Scale Differences", "content": "Key Takeaway: Different architectures exhibit distinct patterns of information compression. Autoregressive models show mid-layer bottlenecks while bidirectional models maintain more uniform trends.\nAside from strong correlations with downstream performance, we can use our evaluation framework to assess the internal behaviors of LLMs. In both this section and Section 4.3, we use WikiText-103 (Merity et al., 2017) for analyzing our representation metrics on standard textual data. To investigate how architecture and model size influence representation quality, we compare three fundamentally different LLM variants\u2014BERT (encoder-only), Pythia (decoder-only), and Mamba (state-space model)\u2014and then scale up Pythia to observe emerging trends.\nEncoder vs. Decoder vs. SSM. Figure 2 shows how prompt entropy, curvature, and augmentation metrics evolve across each model's layers. BERT, which encodes the entire input bidirectionally, generally maintains high entropy across layers, suggesting minimal compression: the model can see all tokens at once and need not discard as much information. By contrast, the decoder-only Pythia exhibits a strong mid-layer entropy dip, reflecting its autoregressive objective's tendency to filter or prune non-local details in the middle of the network. As a result, Pythia's \"sweet spot\" for downstream tasks often lies around mid-depth, where it balances essential context and compression. Mamba, meanwhile, processes sequences through a state-space approach that yields flatter, more uniform curves across depth: it neither retains as much information as BERT nor compresses as aggressively as Pythia's mid-layers."}, {"title": "4.3. Impact of Training Progression", "content": "Takeaway: Significant changes during training occur in intermediate layers and early layers stabilize quickly, supporting the detokenization hypothesis.\nWe measure Pythia's metrics at multiple checkpoints to understand how layer-wise representations evolve throughout training (Figure 4). Two main observations emerge:\nIntermediate Layers Undergo the Most Change. The largest shifts in representation quality occur in mid-depth layers. Specifically, prompt entropy steadily decreases there as training progresses, implying that intermediate layers increasingly compress and abstract the input. Meanwhile, LiDAR scores are minimal in these same layers. Likewise, curvature becomes smoother in the middle of the network, suggesting the model refines its internal structure to capture longer-range or more nuanced patterns in language.\nEarly Layers Stabilize Quickly. In contrast to the intermediate layers, the earliest layers change very little after the initial phase of training. This observation aligns with the \u201cdetokenization\u201d hypothesis (Lad et al., 2024), which posits that early layers mainly convert raw tokens into a basic embedding space and then remain relatively fixed. As a result, the most substantial improvements in representation quality\u2014such as enhanced compression\u2014are driven primarily by the intermediate layers, reinforcing their importance for learning robust, high-level features."}, {"title": "4.4. Impact of Chain-of-Thought Finetuning", "content": "Key Takeaway: CoT finetuning enables models to maintain richer context throughout their layers.\nRecent work has highlighted Chain-of-Thought (CoT) finetuning as a powerful strategy for improving reasoning capabilities (Arefin et al., 2024; DeepSeek-AI, 2025). To examine its effects, in Figure 5 we compare Qwen 2.5 and Qwen 2.5-Math (Yang et al., 2024a;b), where the latter underwent additional math pretraining and CoT finetuning. Measuring token-level prompt entropy across sequence length reveals that the finetuned model maintains higher entropy with lower variance across examples.\nThese findings suggest that CoT finetuning encourages models to preserve more context throughout their hidden layers, enabling better multi-step reasoning. Our framework provides a quantitative lens into how CoT fine-tuning pushes models to maintain richer internal representations across sequences, explaining its effectiveness in multi-step tasks. While CoT traces can be inspected directly in these models, our approach is particularly valuable for analyzing models that reason in continuous latent space (Hao et al., 2024)."}, {"title": "5. Extreme Input Conditions", "content": "To gain a deeper understanding of the underline factors which effect the representation quality, we check how each layer responds to different types of inputs. We use Pythia 410M to three types of extreme prompts and measure prompt entropy across layers (Figure 6). We find that:\n1. Token repetition compresses intermediate layers. As p increases (i.e., more repeated tokens), prompt entropy decreases sharply in mid-depth layers. This indicates that the model effectively recognizes and encodes these repetitive patterns, discarding redundancy in its internal representation.\n2. Random tokens inflate early-layer entropy. When we introduce token-level randomness, entropy increases significantly in the first few layers, revealing that these initial layers are especially sensitive to noise. By contrast, deeper layers appear more robust to such perturbations.\n3. Prompt length raises raw entropy but grows sublinearly once normalized. Longer inputs naturally boost the unnormalized entropy because more tokens create more variation. However, normalized entropy expands at a slower rate, suggesting each additional token contributes less unique information.\nOverall, these results confirm that intermediate layers play a major role in handling complex or unusual inputs, selectively compressing or filtering out repetitive patterns while retaining crucial distinctions. At the same time, early layers respond more sensitively to noise, and the incremental benefit of adding more tokens diminishes with prompt length. This behavior highlights the diverse ways in which different layers balance the trade-off between preserving and discarding information, further underscoring the unique strengths of intermediate representations."}, {"title": "6. Comparison to Vision Transformers", "content": "Although our focus has mainly been on language models, similar questions arise in computer vision. Vision architectures and training regimes differ widely, ranging from fully supervised methods to self-supervised approaches, and from bidirectional encoders to autoregressive transformers.\nTo investigate whether our findings generalize to vision models, we examine five representative vision approaches: ViT (Dosovitskiy et al., 2021), a supervised Transformer trained on labeled data; BEIT (Bao et al., 2022), a self-supervised encoder that reconstructs masked patches, analogous to masked token prediction in language; DINOv2 (Oquab et al., 2024), a self-supervised approach leveraging augmentations and exponential moving average teachers; MAE (He et al., 2022), a self-supervised framework that masks patches and reconstructs them, akin to masked autoencoders in language; and AIM (El-Nouby et al., 2024), an autoregressive Transformer that predicts the next patch in an image sequence (GPT-style next-token prediction). We evaluate each model on ImageNet-1k via layer-wise probing and our framework's metrics.\nA departure from language models for most vision Transformers. Figure 14 shows that ViT, BEiT, DINOv2, and MAE exhibit strictly increasing downstream accuracy toward final layers, unlike language models. These models also show steadily increasing invariance metrics with depth, suggesting that without an autoregressive objective, vision Transformers have less need for drastic transformations at mid-depth.\nAIM exhibits behavior similar to language models. In contrast, AIM\u2014which is explicitly autoregressive over image patches\u2014shows an entropy \"valley\" and corresponding peak in downstream accuracy at its intermediate layers (El-Nouby et al., 2024). This mimics the patterns we observe in LLMs like Pythia, suggesting that autoregressive training induces an information bottleneck mid-depth. As in language modeling, forcing a strictly left-to-right (or patch-to-patch) prediction can drive the model to compress non-local details earlier, then re-expand relevant features.\nAutoregression as the driving factor. Taken together, these results indicate that the strong mid-layer compression observed in LLMs is not purely a property of \"sequential token data\u201d vs. \u201cimage patch data,\u201d but rather a byproduct of autoregressive training. While various self-supervised (or fully supervised) objectives in vision often foster more uniform feature building across layers, autoregressive vision models develop the same mid-layer bottlenecks and sweet spots that we see in language. Thus, the architectural and objective design\u2014especially whether or not a model is autoregressive\u2014appears crucial in shaping layer-wise representation quality, regardless of domain."}, {"title": "7. Discussion and Conclusion", "content": "In this work, we investigate the representation quality of intermediate layers in LLMs, shedding light on their critical role in downstream task performance. We introduce a unified framework of evaluation metrics, establish theoretical connections among them, and apply these metrics to analyze Transformer-based architectures, SSMs, and vision models. One key phenomenon unveiled by prompt entropy was an information bottleneck in the middle layers of autoregressive transformers in both vision and language domains. Furthermore, our results reveal that intermediate layers often surpass final layers in representation quality, emphasizing their importance for feature extraction. DiME, curvature, and infoNCE correlate very well with downstream performance, suggesting a fundamental connection between representation and generalizability.\nIn conclusion, our study deepens the understanding of internal representation dynamics in LLMs. These insights not only enrich the theoretical foundations of model representations but also offer practical implications for optimizing model design, training strategies, and real-world applications. Future research could investigate the underlying causes of intermediate layer compression and develop specialized metrics tailored to LLMs, enabling more precise and effective representation evaluation."}, {"title": "Impact Statement", "content": "Our paper studies the inner workings of large language models with findings that may challenge typical assumptions about the importance of intermediate layers in large language models and the representations they learn. Our findings suggest that representations from these layers can yield better performance on a variety of downstream tasks, which can have implications for model interpretability, robustness, and efficiency.\nFrom an ethical standpoint, the ability to leverage intermediate-layer representations could impact fairness and bias considerations in evaluating model performance or in model deployment. By helping better identify latent features and representations, our approach may amplify latent biases. We welcome and encourage future work to explore methods that can ensure that intermediate-layer representations do not disproportionately reinforce biases or lead to unintended disparities in real-world applications."}, {"title": "A. Architectural Details", "content": "In this section, we elaborate on the specific architectures of Transformers and State Space Models (SSMs). We outline the mathematical foundations, including the weight matrices, attention mechanisms for Transformers, and the state transition matrices for SSMs. Detailed equations and parameter configurations are provided to facilitate replication and deeper understanding."}, {"title": "A.1. Transformer", "content": "The Transformer architecture (Vaswani et al., 2017) utilizes self-attention mechanisms. Given an input x, the key (K), query (Q), and value (V) matrices are computed as:\n$Q = xW_Q, K = xW_K, V = xW_V,$\nwhere $W_Q, W_K \\in \\mathbb{R}^{d \\times d_k}$ and $W_V \\in \\mathbb{R}^{d \\times d_v}$ are learned weights.\nThe attention weights are calculated using:\n$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)$\nwhere M is a mask to enforce causality in autoregressive tasks.\nThe output is then:\ny = AV."}, {"title": "A.2. State Space Models", "content": "SSMs (Gu & Dao, 2024) model sequences using recurrent dynamics. The hidden state $h_t$ and output $y_t$ at time t are updated as:\n$h_t = Ah_{t-1} + Bx_t,$\n$y_t = Ch_t + Dx_t,$\nwhere $A \\in \\mathbb{R}^{n \\times n}, B \\in \\mathbb{R}^{n \\times d}, C \\in \\mathbb{R}^{d \\times n}, and $D \\in \\mathbb{R}^{d \\times d}$ are learned parameters."}, {"title": "B. Discussion on Prompt Entropy", "content": "The first measure of token embedding diversity we call prompt entropy. This entropy is measured on the intermediate tokens and captures how diverse the token representations are.\nWe follow the work of (Wei et al., 2024) and use a-order matrix-based entropy (Giraldo et al., 2014; Skean et al., 2023; 2024), which serves as a tractable surrogate for traditional R\u00e9nyi's a-order entropy (R\u00e9nyi, 1961). The quantity is calculated using a similarity kernel $\\kappa$ on a batch of samples drawn from a distribution, without making explicit assumptions on what the true distribution is. The choice of kernel $\\kappa$ is flexible and can be any infinitely divisible kernel such as the Gaussian kernel, linear kernel, or Laplacian kernel, among others. For this work, we restrict ourselves to the linear kernel $\\kappa(a, b) = ab^T$. This choice is motivated by the linear representation hypothesis (Park et al., 2024b) which finds that large language model representations encode high-level concepts such as truth (Burns et al., 2022), honesty (Mallen & Belrose, 2024), and part-of-speech (Mamou et al., 2020) in linearly separable manifolds.\nThe equation for matrix-based entropy was previously defined in Eq. 1. One way to interpret Eq. 1 is as the a-order R\u00e9nyi entropy of the Gram matrix eigenvalues\u00b9. Notice how each eigenvalue is divided by tr(Kz) before being raised to the a power. This is so that the eigenvalues of Kz sum to one (because $\\text{tr}(\\cdot) = \\sum_{i=1}^r \\lambda_i(\\cdot)$), which is a necessary condition to treat the eigenvalues as a probability distribution. Futhermore, each eigenvalue of Kz signifies the variance of samples in a particular principal component direction (Scholkopf & Smola, 2018). If entropy is low, then\nThe non-zero eigenvalues of the Gram matrix $ZZ^T$ are equivalent to those of the covariance matrix $Z^T Z$. Using the covariance matrix instead of the Gram matrix in Eq. 1 makes no difference and is more computationally efficient if $D < N$."}, {"title": "C. Dataset Details", "content": "C.1. Wikitext Dataset\nWe used the wikitext dataset (Merity et al., 2017) for the majority of our experiments in Sections 4.2 and 5. This was downloaded from Salesforce/wikitext on huggingface. The dataset consists of 100 million tokens scraped from the Featured articles on wikipedia. We filtered out prompts which were less than 30 tokens or were wikipedia section headings.\n\u0421.2. \u041c\u0422\u0415\u0412\nThe 32 tasks we used from the Massive Text Embedding Benchmark (MTEB) are detailed in Table 1. They are English language tasks covering clustering, classification, reranking, and sentence-to-sentence."}, {"title": "D. Prompt Augmentations", "content": "For the augmentation-invariance metrics such as infoNCE, LiDAR, and DiME, we use the NLPAug library (Ma, 2019) to augment our prompts. We use three types of augmentations.\n\u2022 The SplitAug augmentation randomly splits words into two parts by adding a space.\n\u2022 The RandomCharAug augmentation randomly inserts, substitutes, swaps, or deletes characters.\n\u2022 The Keyboard augmentation randomly substitutes characters with other characters that are at a distance of one as measured on a QWERTY keyboard. For instance, the character \"k\" may be replaced with \"i\", \"l\", \"m\", or \"j\".\nWe use the pseudocode below to do our augmentations using three types of augmentations, using the default library settings for each type. When computing augmentation-invariance metrics like infoNCE or DiME, we use the two augmented prompts rather than using one augmented prompt alongside the original prompt. Note that these augmentations may change the token length T of a prompt."}, {"title": "E. Extreme Prompts", "content": "E.1. Increasing Repetition\nWe take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability p. We draw replacements tokens by sampling a random token from within the prompt. We show examples below for varying levels of p.\n\u2022 (p = 0) Mint records indicate the first gold dollars were produced on May 7...\n\u2022 (p = 0.1) Mint records indicate the first gold dollars were Mint Mint May 7...\n\u2022 (p = 0.5) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7...\n\u2022 (p = 1.0) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint...\nE.2. Increasing Randomness\nWe take regular prompts from the wikitext dataset, tokenize them, and then for each token we randomly replace it with probability p. We draw replacements uniformly from the tokenizer distribution. We show examples below for varying levels of p. Unlike the character-level random noise added to prompts in Section with random noise discussed in Appendix D which might change the number of tokens T of the prompt, the token-level random noise used here does not do so.\n\u2022 (p = 0) Mint records indicate the first gold dollars were produced on May 7...\n\u2022 (p = 0.1) Mint records indicate salivary first gold dollars were produced on May NaCl...\n\u2022 (p = 0.5) Mint records Dallas actively first dollars persufors on Mayder129 18...\n\u2022 (p = 1.0) arf emulsion minorensteinorian-mega_TOStack potsRecip Installifykeeping..."}, {"title": "F. Theorems", "content": "Definition 1. (Majorization) Let p", "P[1": "P[n", "q[1": "q[n", "satisfy": "n\u03a3[i", "\u03a3[2": "for k = 1", "\u03c3\u00bf": "and \u2081 := \nIt is straightforward to show that \u03c3\u00b2 = \u03bb\u2081. Because \u03c3\u03b5 \u2264 1, we have that \u03c3\u03b5 \u2265 \u03bb\u03af. This implies that\u5165\u5c0f0. Therefore, S\u2081 (\u03c3) \u2264 S1(\u03bb) effective"}]}