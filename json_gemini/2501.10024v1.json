{"title": "AUTOMATIC SPEECH RECOGNITION FOR SANSKRIT WITH TRANSFER LEARNING", "authors": ["Bidit Sadhukhan", "Swami Punyeshwarananda"], "abstract": "Sanskrit, one of humanity's most ancient languages, has a vast collection of books and manuscripts on diverse\ntopics that have been accumulated over millennia. However, its digital content (audio and text), which is\nvital for the training of AI systems, is profoundly limited. Furthermore, its intricate linguistics make it hard\nto develop robust NLP tools for wider accessibility. Given these constraints, we have developed an automatic\nspeech recognition model for Sanskrit by employing transfer learning mechanism on OpenAI's Whisper model.\nAfter carefully optimising the hyper-parameters, we obtained promising results with our transfer-learned model\nachieving a word error rate of 15.42% on V\u0101ksa\u00f1cayah dataset. An online demo of our model is made available\nfor the use of public and to evaluate its performance firsthand thereby paving the way for improved accessibility\nand technological support for Sanskrit learning in the modern era.", "sections": [{"title": "INTRODUCTION", "content": "Sanskrit is one of the oldest languages in the world with a rich literary tradition. A vast collection of ancient texts such as the\nVedas, Upanishads, epic poetry (e.g., Ramayana and Mahabharata) [4], and other scientific texts on topics such as mathematics,\nmedicine and astronomy [17] were all composed in Sanskrit. The language served as a unifying thread for Indian culture and\nknowledge for centuries. Its influence permeates not only on modern Indian languages such as Hindi, Bengali, Kannada, and\nMarathi, but also on southeast Asian languages like Thai, Burmese and Javanese [10, 18].\nThe modern era poses numerous challenges to the preservation of Sanskrit, as its usage in both spoken and written forms is\nlimited in today's world [9]. The paucity of resources in digital form also hinders its accessibility and learning for a broader\naudience. Recent efforts to revitalise the language, particularly its spoken form, through educational initiatives, contemporary\nliterature, and advancements in technology, are ongoing [5].\nTraditional methods of learning and focusing primarily on grammar can make Sanskrit tough and tedious for new learners. In an\neffort to make Sanskrit learning exciting and more accessible to wider audience, we developed an Automatic Speech Recognition\n(ASR) model for Sanskrit and this paper discusses the approach that was employed in building such a model. The developed\nASR model could be used for transcription, practising pronunciation, etc.\nWhile designing the model we noted the inherent linguistic complexities of Sanskrit that challenge ASR. For instance: (i) Unlike\nlanguages with fixed word forms, Sanskrit allows for the dynamic combination of morphemes, resulting in multiple pronuncia-\ntions for a single concept [10]. This variability, combined with the lack of standardised pronunciation rules, poses difficulties for\nASR models in accurately mapping sounds to words [14]. (ii) Sanskrit's diverse phonetic inventory possesses a rich set of sounds,\nincluding retro-flexes and breathy consonants, which are absent in many modern languages [14]. Accurately distinguishing these\nsubtle nuances, especially in noisy environments, is a challenge for ASR models trained in simpler languages [16]. (iii) Sanskrit\npronunciation has undergone significant changes over time, creating further discrepancies between written and spoken forms [20].\nThis necessitates ASR models to be capable of handling all these nuances and complexities ingrained in the linguistics of the\nlanguage.\nTraditional statistical approaches for ASR such as Acoustic Models and Language Models often fall short while working on\nSanskrit language for the reasons mentioned above. In this paper, we employ transformer architecture [19] for the task of ASR\nfor Sanskrit. It is essentially a sequence-to-sequence model that is capable of addressing to a large extent the above-mentioned\nchallenges posed by the Sanskrit language for building an ASR. It utilises a self-attention mechanism that analyses relationships\nbetween various sounds within a speech segment rather than working on isolated sounds, leading to more accurate recognition of\nsubtle variations like retroflex vs. dental consonants."}, {"title": "LITERATURE REVIEW", "content": "Early research in Sanskrit ASR primarily employed statistical models like Hidden Markov Models and Gaussian Mixture Models\n[13] [11]. These models managed to map basic acoustic patterns in controlled environments, such as recitations of the Ramayana.\nHowever, the complexities of real-world scenarios \u2013 background noise, diverse speaker accents, and spontaneous speech \u2013 proved\ninsurmountable. These models struggled amidst the rich linguistic intricacies of spoken Sanskrit. They were unable to effectively\ncapture the language's sophisticated grammar rules and vast vocabulary.\nStatistical-based approaches for ASR such as Acoustic Models (AMs) and Language Models (LMs) have been used in several\nworks. For instance, in [3] the authors propose a linguistically inspired new syllable-level unit segmentation technique, which\nthey term as vowel segmentation. Three encoding schemes for tokenization are investigated: word-based encoding, Byte Pair\nEncoding (BPE), and vowel segmentation. The best reported Word Error Rate (WER) is 21.94% using the Sanskrit Library\nPhonetic basic encoding scheme (SLP1) script with BPE encoding for the LM unit and Grapheme encoding for the AM unit\non a dataset that they specially created for Sanskrit ASR evaluation called V\u0101ksa\u00f1cayah. Nonetheless, these models may fail to\ncapture contextual relationships when processing on long speech audios spanning several seconds.\nMany researchers have employed deep learning techniques, which offer advanced analytical tools for the task of ASR. For\ninstance, in [15], the authors train a neural network using the Connectionist Temporal Classification (CTC) approach for the task\nof automatic speech recognition for Sanskrit. Notably, the model uses a single bidirectional Long Short-Term Memory (LSTM)\nlayer for direct character sequence prediction, bypassing separate phoneme classification stages leading to improved efficiency\nand accuracy. To address the issue of limited training data, the research employs spectrogram augmentation. The model achieves\na word error rate (WER) of 7.6% on their proprietary dataset.\nAnother study [7] evaluates four grapheme-to-phoneme (G2P) conversion schemes for ASR for Sanskrit using a proprietary\nspeech corpus of about 15 hours. They use a factorised time-delay neural network trained on an objective function using a\nmodified version of SLP1 script to solve the ASR for Sanskrit achieving a WER of 8.4%.\nThe work of [8] evaluates syllable-based modelling units for end-to-end speech recognition in Indian languages, utilising various\ntext representations such as native script, SLP1 encoding, and syllables. The results on the V\u0101ksa\u00f1cayah dataset indicate that\nsyllable-based BPE units perform effectively in monolingual speech recognition but not in cross-lingual transfer learning, where\nSLP1 character units demonstrate superior performance. According to their syllable-based modelling units, different languages\nhave different syllable-based representations so using one syllable-based representation in one language may not be able to\nrepresent the syllable of other languages. Their results also show that the Sanskrit-based syllable representation does not perform\nwell in Tamil and Kannada languages. Nonetheless, their paper reveals that syllable-BPE representation achieves the best results\nfor the Sanskrit dataset. The limitation is that the Syllable-based tokens do not support cross-lingual transfer learning due to\nvarying syllable distributions and associations across languages.\nIn recent years, transformers have emerged as a foundational architecture across various fields. Their capacity to model long-range\ndependencies and capture intricate relationships has driven significant advancements in natural language processing and beyond.\nLeveraging their success, this paper presents the development of a Sanskrit ASR model using the Transformer architecture.\nGiven the limited availability of Sanskrit data, a transfer learning approach was employed. Hyperparameter tuning substantially\nenhanced the model's performance. The details of the proposed methodology are described in the next section."}, {"title": "PROPOSED METHODOLOGY", "content": "Our model is based on OpenAI's Whisper, a state-of-the-art pre-trained transformer architecture for Automatic Speech Recogni-\ntion (ASR). To perform ASR for Sanskrit, we employ a transfer learning approach. Specifically, we trained the Whisper model\nfor ASR on a new langauge, Sanskrit, utilising the V\u0101ksa\u00f1cayah dataset [3], which contains speech audio files rendered exclu-\nsively in Sanskrit. Such audio inputs were not encountered by the pre-trained Whisper model previously. We leveraged on the\nexisting linguistic knowledge that Whisper had learnt after being trained on a huge English audio corpus while adapting it to the\nspecificities of Sanskrit grammar, pronunciation and prosody through our training. This transfer learning approach was crucial\ndue to the scarcity of data available for training Sanskrit ASR models.\nBefore delving into the implementation details, we present a brief overview of the Whisper model [12]. The model marks a\nsignificant achievement in ASR technology. Trained on a massive dataset of labeled audio and text exceeding 680,000 hours,\nWhisper demonstrates exceptional accuracy and robustness in transcribing spoken language. Although, the model is capable of\nseveral other speech processing tasks such as multilingual speech translation, language identification, voice activity detection, we\nprimarily used it for the purpose of ASR for Sanskrit. The model architecture of Whisper family is shown in Fig. 1"}, {"title": "Train Phase", "content": "During training, the model receives both speech audio and its corresponding Devanagari script as inputs. The audio undergoes\npreprocessing: resampling, normalization, and windowing, followed by conversion into a log Mel spectrogram. This spectrogram\nis processed by convolutional neural networks and combined with sinusoidal position embeddings before being fed to the encoder.\nThe encoder processes the input sequence and generates hidden states that capture relevant information from the speech audio.\nThese hidden states are then passed to the decoder as indicated in Figure 1.\nThe other input, Devanagari transcript (ground truth), is preprocessed before being sent to the decoder. It is tokenized, word\nembedded, and position encoded. This allows the decoder to iteratively generate tokens and refine its predictions based on the\nencoder's output and previously generated text. This process continues until the decoder generates an end-of-sequence token,\nsignifying the transcription's completion.\nThe model parameters are optimised to minimise the transcription error rate on a validation set, assessed using the WER metric."}, {"title": "Test Phase", "content": "During testing, the speech audio is the sole input. The encoder processes it as in training. The decoder, however, starting with a\nstart-of-sequence token, iteratively extends the sequence, token by token, based on the encoder's output and previously generated\ntext to produce the transcribed text. For more details on the transformer architecture, please refer to [19]. We built our model\nusing Hugging Face, an open-source platform, that provides a streamlined framework for utilising the Whisper model and its\nassociated components [1]."}, {"title": "Hyper-parameter Optimisation", "content": "As with any other neural network training, choosing the right set of hyper-parameters is crucial for the optimal performance of\nthe model. For our model, we employed a randomised search algorithm on the key hyper-parameters used in the model. The\nrange of these hyper-parameters that was given to the search algorithm are as follows:\n1. MLP Dropout: Range was set from 0.2 to 0.6 to control the dropout rate within the model's multi-layer perceptrons\n(MLPs) to prevent over-fitting.\n2. Attention Dropout: Range was set from 0.2 to 0.6 to regulate the dropout rate within the model's attention mechanism\nfor optimal information flow and to prevent over-fitting.\n3. Learning Rate: Fine-tuning was based on the model size, with ranges of le-5 to 3e-5 for the WTL-Small model and\n5e-5 to 8e-5 for the WTL-Medium model as recommended in [12].\n4. Optimiser: Various optimisers were evaluated, including adagrad, adamw, adam_hf, and adam, to identify the most\neffective algorithm for updating the model parameters.\nWe also explored various learning rate schedulers including linear, linear-warmup, cosine, and reduce-on-plateau to dynamically\nadjust the learning rate for optimal convergence during the training.\nThe optimal configuration of the hyper-parameters within these ranges were identified for both the WTL-small and WTL-medium\nmodels. This hyper-parameter optimisation on the V\u0101ksa\u00f1cayah dataset [3] along with the transfer learning employed on the\npre-trained Whisper models were effective in capturing the characteristics of the Sanskrit language and contributed to superior\nperformance than those obtained with baseline configurations (ie. using the default configuration, with no hyper-parameter\ntuning).\nFor optimal performance and analysis, a suite of additional libraries was utilised from Hugging Face. For instance, the Accelerate\nlibrary from Hugging Face was leveraged for efficient and speedy training on GPUs, benefiting from its optimised training\nmechanisms. The Weights & Biases library was used to track each training run with its corresponding hyper-parameter settings,\nallowing for efficient analysis and identification of the best-performing configurations for both WTL-small and WTL-medium\nmodels.\nTo showcase the accuracy and robustness of our transfer-learned Whisper model for Sanskrit ASR, we developed an online demo\nusing the Gradio library [2] on the Hugging Face Spaces. A screenshot of the online demo is shown in Figure 2. This interactive\nplatform allows users to evaluate the model's capabilities first-hand, where the spoken Sanskrit speech audio is transcribed with\nease.\nThe demo further expands its accessibility by supporting both microphone recordings and audio file uploads. This caters to\ndiverse preferences and application scenarios, ensuring the model's usability for a wider audience. Additionally, the ability to\nautomatically split longer audio files exceeding 30 seconds [12] into manageable chunks demonstrates the model's proficiency in\nhandling extended speech while maintaining consistent performance. To access the Sanskrit ASR demo, visit the page3."}, {"title": "RESULTS", "content": "To evaluate our WTL model we used a publicly available dataset for Sanskrit ASR called V\u0101ksa\u00f1cayah [3]. It has over 78\nhours of audio recordings, comprising of 45,953 sentences spoken by 27 speakers with diverse linguistic backgrounds. This\nvariety ensures the corpus captures the natural variations encountered in spoken Sanskrit. The dataset includes transcripts in both\nDevanagari script and SLP1 format. The texts are sourced from sacred verses, contemporary narratives, radio programs, and\nextemporaneous speeches, providing a well-rounded representation of spoken Sanskrit across various contexts. Furthermore, it\nhas a separate out-of-domain test set featuring speakers included neither in the training nor validation dataset which serves as a\ncrucial benchmark for assessing the generalisability of ASR models trained on V\u0101ksa\u00f1cayah. We used the training, validation\nand testing split as recommended by the authors of the dataset.\nWe report our results on the small and medium sized models (see Table 1). As mentioned earlier, hyper-parameter optimisation\nwas crucial to improve the performance of the models for this low-resource language. We used the WER as our metric in our\nevaluations. The results obtained from our experiments are shown in Table 2. We note that the WERs of both these models\non the test set are significantly lower than that of the model proposed in [3] thereby demonstrating the efficacy of our transfer\nlearning approach employed on this low resource language. In particular, we note a percentage reduction of 29.8% in WER by\nusing the WTL-Medium model when compared to that of the model proposed in [3]. Also, the WTL-Medium model exhibited a\nrobust performance on the out-of-domain test set, with a WER of 37% while the model proposed in [3] obtained 44%, achieving\na percentage reduction of 15.7% in WER. The result demonstrates the proposed model's resilience against noise and unfamiliar\nspeech patterns. Such a robust performance indicates the model's potential for real-world applications beyond controlled test\nenvironments.\nIt is noteworthy that while working on the small speech audio dataset, V\u0101ksa\u00f1cayah, we encountered the common phenomenon\nof model overfitting. To address this issue, we adopted the following measures:\n\u2022 Early Stopping: We monitored the validation loss and stopped training when it started to increase;\n\u2022 Regularisation: We incorporated techniques such as dropout and weight decay to control model complexity\n\u2022 Data Augmentation: We employed data augmentation techniques. A subset of audio files was randomly selected and\nsubjected to transformations such as pitch shifting, time stretching, and the addition of silence or background noise to\nenhance data diversity.\nWe trained the WTL-Medium model on a single NVIDIA Quadro GV100 GPU, and the WTL-Small model on a single NVIDIA\nGeForce RTX 2080 Ti GPU. Average training times were approximately 36 and 14 hours, respectively. The system ran on Linux\nkernel version 5.15.0-86-generic-x86_64 with glibc2.31 and Python 3.11.5. To conveniently track the hyper-parameters and to\nvisualise various plots in our experiments, we utilised Weights & Biases CLI version 0.15.11 platform [6]."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "This paper employed a transfer learning approach to train a Whisper model, which is pre-trained on a massive English-specific\ncorpus, for Sanskrit ASR using the V\u0101ksa\u00f1cayah dataset. This leveraged the model's inherent linguistic knowledge (learnt due\nto the previous training) while adapting it to the specificities of Sanskrit grammar, pronunciation and prosody (learnt from the\ntraining on the V\u0101ksa\u00f1cayah dataset). Furthermore, optimising the model's hyper-parameters led to significant improvements in\nits performance.\nOur best model achieved a WER of 15.42% on the V\u0101ksa\u00f1cayah test dataset, surpassing previous benchmarks. Additionally, it\ndemonstrated robust performance against noise and various speech accents that existed in the out-of-domain test set.\nWith better transcription accuracy compared to the previously existing models, our model can be used to transcribe historical\nrecordings and literary works, thereby improving accessibility for researchers, educators, and enthusiasts. This capability to\nanalyse large audio datasets can drive research in language evolution, cultural studies, and other fields enriched by Sanskrit.\nAs part of the future work, the following avenues may be investigated:\n\u2022 Paucity of data: Transformer models require vast amounts of high-quality data for optimal performance. Thus, the\ncreation of new Sanskrit datasets with vast and diverse speech content is vital. We noted that the dataset we employed\nwas limited in size especially for transformer based models.\n\u2022 Sanskrit-Specific Tokenization: Our current model utilises Whisper's default multilingual tokenization. Developing a\ncustomised tokenizer specifically designed for Sanskrit's unique morphology and phonology could significantly improve\naccuracy.\n\u2022 Better evaluation metrics: WER may not fully capture the nuances of Sanskrit, where homonyms and words with variant\nspellings (sandhi) convey the same meaning. Exploring alternative metrics which incorporate semantic knowledge of\nSanskrit could provide a more accurate assessment of the model's performance.\n\u2022 Multimodal Learning: Integrating additional modalities, such as lip-reading or visual context, into the model could\npotentially improve its performance, particularly in noisy or ambiguous situations.\n\u2022 Domain-Specific Adaptation: Fine-tuning the model for specific domains, such as Vedas, contemporary dialogues, or\ntechnical lectures, could further improve accuracy and cater to specialised research needs.\nBy working on these possible future avenues, we can further improve the performance and robustness of our model."}]}