{"title": "Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift", "authors": ["Qingyuan Zeng", "Yunpeng Gong", "Min Jiang"], "abstract": "Studying adversarial attacks on artificial intelligence (AI) systems helps discover model shortcomings, enabling the construction of a more robust system. Most existing adversarial attack methods only concentrate on single-task single-model or single-task cross-model scenarios, overlooking the multi-task characteristic of artificial intelligence systems. As a result, most of the existing attacks do not pose a practical threat to a com-prehensive and collaborative AI system. However, implementing cross-task attacks is highly demanding and challenging due to the difficulty in obtaining the real labels of different tasks for the same picture and harmonizing the loss functions across different tasks. To address this issue, we propose a self-supervised Cross-Task Attack framework (CTA), which utilizes co-attention and anti-attention maps to generate cross-task adversarial perturbation. Specifically, the co-attention map reflects the area to which different visual task models pay attention, while the anti-attention map reflects the area that different visual task models neglect. CTA generates cross-task perturbations by shifting the attention area of samples away from the co-attention map and closer to the anti-attention map. We conduct extensive experiments on multiple vision tasks and the experimental results confirm the effectiveness of the proposed design for adversarial attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the widespread application of artificial intelligence (AI) systems has brought dramatic changes in various fields. As AI technologies become more pervasive in our daily lives, people start to worry about their robustness and safety. Adversarial attacks [1]\u2013[12] are techniques that use small perturbations imperceptible to humans to deceive Al systems, and have become an important research topic. The goal is to reveal model weaknesses and help developers build more robust systems. To provide a strong baseline for deep learning robustness research, many studies have proposed various effective attack methods to generate adversarial samples.\nExisting adversarial attack methods can be classified ac-cording to three criteria: 1. sample-specific or cross-sample, 2. model-specific or cross-model, 3. task-specific or cross-task. Extensive research has been conducted on the first two criteria. Some researchers conducted pioneering research on cross-sample attacks, aiming to obtain a perturbation that can disturb multiple samples simultaneously [13]\u2013[17]. On the other hand, some researchers conducted research on cross-model attacks, aiming to improve the transferability of adversarial pertur-bations by adding additional randomness [18]\u2013[23]. Cross-sample attacks can speed up the production of adversarial example sets, while cross-model attacks can increase the possibility of black-box attacks under specific tasks.\nMost existing research on adversarial attacks mainly focuses on single-task scenarios, ignoring the multi-task characteristics of Al systems. In practical applications, AI systems need to cooperate with multiple tasks for decision-making [24]\u2013[26]. Neither cross-sample nor cross-model attacks can effectively threaten AI systems in practical applications.\nUnlike cross-sample and cross-model attacks, the core of the cross-task adversarial attack methods is to find and destroy the common characteristics of different vision tasks. DR [27] first proposed a cross-task attack method called Dispersion Reduction. DR considers that the common characteristics of different vision tasks is the feature extractor. However,"}, {"title": "II. RELATED WORKS", "content": "A. Single-task Single-model Attack\nSingle-task single-model attack means that the attacker designs an adversarial example that can deceive the target model while knowing the parameters and structure of the target model. The basic idea of single-task single-model attack can be divided into two categories, one is to use gradient ascent in the image pixel space to maximize the loss function [1], [2], [30], [31], and the other is to use complex optimization process to find the optimal solution leading to wrong prediction [3], [32].\nB. Single-task Cross-model Attack\nIn order to enable adversarial samples to attack different models under specific tasks, many studies have investigated how to improve the transferability of adversarial samples. DIM [18] incorporated random transformations in the gradient iter-ation process to increase the diversity of adversarial perturba-tions. TI-FGSM [19] added a translational data augmentation method to increase the translational invariance of the adversar-ial examples. SI-FGSM [33] utilized the scalability invariance of deep learning models and proposes adding random scaling in the gradient iteration process. DAS [28] achieved single-task attack by suppressing Grad-CAM heatmaps [34]. S2I-FGSM [23] applies spectral transformation in the frequency domain to enhance the mobility of adversarial samples. These works all rely on the loss function of a specific task and they cannot guide the movement direction of the attention area of the adversarial example, which makes them unable to attack other visual tasks.\nC. Cross-task Attack\nDR [27] introduced a method for generating adversarial examples without relying on specific-task loss functions. By utilizing VGG [35] to extract image feature maps and reducing the standard deviation, they obtained adversarial examples that can disrupt feature extraction, thus achieving cross-task attacks. RB [22] proposed random blur (RB) during itera-tive optimization against perturbations, which improves the diversity of adversarial perturbations. RB can slightly improve the performance of DR in scenarios of cross-task attacks. These works are cross-task attack methods that do not depend on task-specific loss functions, but their cross-task attack performance is weak because they cannot guide the attention shift direction of adversarial samples.\nDifferent from the idea of perturbing feature extractors in the existing cross-task attack methods, we solve the problem of cross-task attack from a novel perspective. We pioneered the concepts of co-attention and anti-attention maps, and utilized them to guide the direction of attention-shift for adversarial samples. The attention of the adversarial examples is shifted to regions that are not concerned by various vision tasks to enable cross-task attacks."}, {"title": "III. THE PROPOSED METHOD", "content": "In this section, we introduce how our proposed Cross-Task Attack (CTA) shifts the attention of adversarial samples from important areas to areas that are overlooked by various visual tasks.\nA. Overview of the Framework\nIn order to obtain cross-task adversarial samples, we first need to identify the regions of the samples that are not of interest to various visual tasks, and then use adversarial pertur-bations to shift attention to these regions. We propose a self-supervised cross-task attack method named CTA, as shown in Figure 3. CTA consists of two stages: attention extraction stage and attention shift stage. The attention extraction stage is to obtain the co-attention and anti-attention maps of clean samples. The former reflects the important areas that different vision task models need to pay attention to, while the latter reflects the unimportant areas that are ignored. The attention shift stage is to shift the adversarial samples' attention from the co-attention area to the anti-attention area by adding adversarial perturbations, thus enabling cross-task attacks.\nB. Attention Extraction Stage\nThe process of attention extraction stage is the bottom part of Figure 3. First, we use ready-made pre-trained models and Grad-CAM to obtain attention maps of clean samples in different vision tasks. Because the attention of different models of the same task is similar, it is enough to choose one model for each vision task [28]. Specifically, the formula for calculating the Grad-CAM attention heatmap $A$ is\n$A(i, j) = max \\bigg\\{ 0, \\sum_{k} \\sum_{i} \\sum_{j} \\frac{\\partial y_{c}}{\\partial F_{k}(i, j)} F_{k}(i, j) \\bigg\\} ,$ (1)\nwhere $Z$ is the total number of pixels in the feature map, $y_{c}$ is the probability that classifier $f_{cls}$ predicts that the input image $x$ belongs to class $c$, and $F_{k}(i,j)$ is the value of the k-th feature map of the last convolutional layer at position $(i, j)$. After obtaining the attention map $A$ for each vision task, we need to find which regions of the picture are the focus of all vision tasks. Therefore, we fuse the attention heatmaps of different visual tasks to obtain the co-attention map, which represents the common focus area of different visual tasks. We used a simple and effective method for feature fusion, calculated as follows:\nco-attention$(i, j) = Scale \\bigg(\\sum_{k} A_{k}(i, j)\\bigg) ,$ (2)\nwhere $Scale$ means to normalize the value range of the heatmap to [0,1], $K$ represents the number of visual tasks, and $A_{k}(i, j)$ represents the value of attention heatmap of the k-th visual task at position $(i, j)$. The high-value pixel area of co-attention map is the area that different visual tasks all focus on.\nAt last, we invert the co-attention map to get the anti-attention map as follow:\nanti-attention$(i, j) = 1 \u2212$ co-attention $(i, j),$ (3)\nanti-attention represents regions that are not attended to by all vision tasks, which can be used as labels for self-supervised training in attention shift stage.\nC. Attention Shift Stage\nThe process of attention shift stage is the upper part of Figure 3. To shift the attention of input image, we use a generator to generate adversarial perturbations and add them to the image to change its mapping in the feature space. The calculation process for adversarial sample is as follows:\n$x' = x + G(x),$ (4)\nwhere $x$ represents the input clean sample, $x'$ represents the adversarial sample without range constraints, $G$ represents the generator. To increase the invisibility of adversarial samples, we need to crop the adversarial sample at the pixel level:\n$x_{adv}(i, j) = min(x(i, j) + \\epsilon, max(x' (i, j), x(i, j) \u2212 \\epsilon)),$ (5)\nwhere $x_{adv} (i, j)$ and $x(i, j)$ represents the value of adversarial sample and clean sample at position $(i, j)$, $\\epsilon$ represents disturbance range threshold. Each pixel of the adversarial sample $x_{adv}$ is cropped to the range of $[x - \\epsilon, x + \\epsilon]$.\nIn order to obtain the attention heatmaps of adversarial samples, we used the parameter-frozen feature extractor (ResNet50) to calculate $y^{c}$. By substituting $y^{c}$ into Equation 4, the attention map $A_{adv}$ of the adversarial sample can be obtained. We use the distance between anti-attention map and $A_{adv}$ as the loss function to update the parameters of generator $G$. More precisely, the loss function is\n$L = \\frac{1}{N} \\sum_{i,j} (anti-attention(i, j) \u2212 A_{adv} (i, j))^{2},$ (6)\nwhere $L$ is the loss function and $N$ is the total number of pixels in the image.\nBy updating the parameters of generator $G$ through mini-mizing $L$, CTA can generate more effective cross-task pertur-bations. These perturbations guide the attention of adversarial samples towards regions of high numerical value in the anti-attention maps, which are typically ignored by all visual tasks. The detailed process of our method CTA is outlined in Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct extensive quantitative experiments on three classic visual tasks: image classification, object detection, and semantic segmentation, to evaluate the effectiveness and robustness of our proposed method CTA for cross-task attack. We also perform qualitative experiments to observe the trend of Grad-CAM attention heatmaps across the training iterations.\nA. Experimental Setup\n1) Evaluation datasets.: Following the experimental set-tings of previous work [22], [27], [36], we randomly select 10 samples from 1000 classes in the ImageNet validation set, totaling 10000 samples, as the validation set for the classification task. We use the complete validation set of PASCAL VOC 2012 as the validation set for object detection and semantic segmentation tasks.\n2) Generator training.: We use a ResNet architecture com-posed of downsampling blocks and upsampling blocks as generator $G$ [37]. We use the images in the VOC 2012 training set to train the generator $G$. It is worth noting that the training of CTA does not require any ground true labels, as we use ready-made models to extract anti-attention graphs for self-supervised training. The pretrained model for each visual task adopts classic and ready-made models, with ResNet50 as classification model $f_{cls}$, SSD as detection model $f_{det}$, and U-nets as segmentation model $f_{s}$. The feature extractor $D$ for adversarial samples adopts ResNet50. We use Adam optimizer for training, learning rate is set to 1e-3, first and second moment exponential decay rates are set to 0.5 and 0.99. We train two versions of perturbation generator $G$ based on different perturbation range thresholds, corresponding to epsilon 10 and 16 respectively. Our experimental device uses three GPU of RTX2080ti with 11GB memory and a CPU of Intel(R) Core(TM) i5-10400F.\n3) Comparison attack algorithms.: We choose five adver-sarial attack methods for comparison: 1. DR, a cross-task ad-versarial attack method that does not rely on any specific task loss function, it reduces the feature map standard deviation to create adversarial examples that fool multiple visual tasks; 2. RB-DR, which adds a random blur (RB) data augmentation method on the basis of DR to increase the success rate of at-tack. 3. S\u00b2I-FGSM, a single-task cross-model attack algorithm"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel cross-task adversarial attack method CTA, which can generate adversarial examples that can fool multiple visual tasks simultaneously. Unlike existing attack methods, CTA can directionally guide the attention shift of adversarial samples. CTA utilizes Grad-CAM to extract common attention regions from different visual task models, and uses a generator to generate adversarial samples that can shift attention to areas overlooked by all visual tasks, thereby achieving cross-task attacks. CTA does not rely on any specific task loss function or ground true label, making it a general and flexible method for cross-task attack. Our extensive experiments have shown that our method outperforms the comparative methods in object detection and semantic segmentation tasks. In image classification task, our method outperforms existing cross-task attack methods and approaches the single-task attack methods designed for classification task. We also visualize the Grad-CAM attention heatmaps of our method CTA, and intuitively demonstrates the attention movement process of adversarial samples with increasing training iterations."}]}