{"title": "Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift", "authors": ["Qingyuan Zeng", "Yunpeng Gong", "Min Jiang"], "abstract": "Studying adversarial attacks on artificial intelligence (AI) systems helps discover model shortcomings, enabling the construction of a more robust system. Most existing adversarial attack methods only concentrate on single-task single-model or single-task cross-model scenarios, overlooking the multi-task characteristic of artificial intelligence systems. As a result, most of the existing attacks do not pose a practical threat to a comprehensive and collaborative AI system. However, implementing cross-task attacks is highly demanding and challenging due to the difficulty in obtaining the real labels of different tasks for the same picture and harmonizing the loss functions across different tasks. To address this issue, we propose a self-supervised Cross-Task Attack framework (CTA), which utilizes co-attention and anti-attention maps to generate cross-task adversarial perturbation. Specifically, the co-attention map reflects the area to which different visual task models pay attention, while the anti-attention map reflects the area that different visual task models neglect. CTA generates cross-task perturbations by shifting the attention area of samples away from the co-attention map and closer to the anti-attention map. We conduct extensive experiments on multiple vision tasks and the experimental results confirm the effectiveness of the proposed design for adversarial attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the widespread application of artificial intelligence (AI) systems has brought dramatic changes in various fields. As AI technologies become more pervasive in our daily lives, people start to worry about their robustness and safety. Adversarial attacks [1]-[12] are techniques that use small perturbations imperceptible to humans to deceive Al systems, and have become an important research topic. The goal is to reveal model weaknesses and help developers build more robust systems. To provide a strong baseline for deep learning robustness research, many studies have proposed various effective attack methods to generate adversarial samples.\nExisting adversarial attack methods can be classified according to three criteria: 1. sample-specific or cross-sample, 2. model-specific or cross-model, 3. task-specific or cross-task. Extensive research has been conducted on the first two criteria. Some researchers conducted pioneering research on cross-sample attacks, aiming to obtain a perturbation that can disturb multiple samples simultaneously [13]\u2013[17]. On the other hand, some researchers conducted research on cross-model attacks, aiming to improve the transferability of adversarial perturbations by adding additional randomness [18]-[23]. Cross-sample attacks can speed up the production of adversarial example sets, while cross-model attacks can increase the possibility of black-box attacks under specific tasks.\nMost existing research on adversarial attacks mainly focuses on single-task scenarios, ignoring the multi-task characteristics of Al systems. In practical applications, AI systems need to cooperate with multiple tasks for decision-making [24]\u2013[26]. Neither cross-sample nor cross-model attacks can effectively threaten AI systems in practical applications.\nUnlike cross-sample and cross-model attacks, the core of the cross-task adversarial attack methods is to find and destroy the common characteristics of different vision tasks. DR [27] first proposed a cross-task attack method called Dispersion Reduction. DR considers that the common characteristics of different vision tasks is the feature extractor. However, the attack performance of DR is weak because the feature extractors vary greatly among different tasks and models.\nIn this paper, we propose a self-supervision generative framework based on attention shift to enable cross-task attack. Our approach, called Cross-Task Attack (CTA) and illustrated in Figure 3, is inspired by previous explorations around the principles of adversarial attacks [28], [29]. Adversarial samples can fool neural networks because the perturbations make the models' attention move to unimportant areas [29]. So we presume that cross-task attack can be achieved by directing the attention of the models to areas that all visual tasks neglect.\nWe use co-attention map to represent the regions that multiple visual tasks focus on, while anti-attention map to represent the regions that all visual tasks neglect. As shown in Figure 1, co-attention is the union of attention regions from different visual tasks, while anti-attention is the complement of co-attention. By using perturbation d to shift the attention of adversarial sample from point A in co-attention region to point A' in anti-attention region, cross-task attack can be achieved. It is worth noting that co-attention and anti-attention maps are obtained using ready-made pre-trained models, so CTA does not require any ground truth labels for training.\nBased on the experimental conclusions of previous work [28], attention heatmap is a model-agnostic shared feature in specific task. As shown in Figure 2, we can see that the attention heatmaps of different tasks are very different, which means that attention heatmap is shared in a specific task, but not shared in different tasks. This explains why adversarial examples based on single-task attacks fail on other tasks, because single-task attacks only divert the attention of adversarial examples from the attention area of a specific task, but may be moved to attention area of other tasks.\nContributions. The main contributions of this paper are as follows:\n\u2022\tWe conduct an intuitive principle analysis of existing single-task and cross-task attack methods, explaining their weaker performance in cross-task scenarios from the perspective of attention.\n\u2022\tWe are the first to apply common attention from different visual tasks in adversarial attacks. We propose a self-supervised generative framework CTA to shift the attention of images to regions that are overlooked by variable visual task models, enabling cross-task attack."}, {"title": "II. RELATED WORKS", "content": "A. Single-task Single-model Attack\nSingle-task single-model attack means that the attacker designs an adversarial example that can deceive the target model while knowing the parameters and structure of the target model. The basic idea of single-task single-model attack can be divided into two categories, one is to use gradient ascent in the image pixel space to maximize the loss function [1], [2], [30], [31], and the other is to use complex optimization process to find the optimal solution leading to wrong prediction [3], [32].\nB. Single-task Cross-model Attack\nIn order to enable adversarial samples to attack different models under specific tasks, many studies have investigated how to improve the transferability of adversarial samples. DIM [18] incorporated random transformations in the gradient iteration process to increase the diversity of adversarial perturbations. TI-FGSM [19] added a translational data augmentation method to increase the translational invariance of the adversarial examples. SI-FGSM [33] utilized the scalability invariance of deep learning models and proposes adding random scaling in the gradient iteration process. DAS [28] achieved single-task attack by suppressing Grad-CAM heatmaps [34]. S2I-FGSM [23] applies spectral transformation in the frequency domain to enhance the mobility of adversarial samples. These works all rely on the loss function of a specific task and they cannot guide the movement direction of the attention area of the adversarial example, which makes them unable to attack other visual tasks.\nC. Cross-task Attack\nDR [27] introduced a method for generating adversarial examples without relying on specific-task loss functions. By utilizing VGG [35] to extract image feature maps and reducing the standard deviation, they obtained adversarial examples that can disrupt feature extraction, thus achieving cross-task attacks. RB [22] proposed random blur (RB) during iterative optimization against perturbations, which improves the diversity of adversarial perturbations. RB can slightly improve the performance of DR in scenarios of cross-task attacks. These works are cross-task attack methods that do not depend on task-specific loss functions, but their cross-task attack performance is weak because they cannot guide the attention shift direction of adversarial samples.\nDifferent from the idea of perturbing feature extractors in the existing cross-task attack methods, we solve the problem of cross-task attack from a novel perspective. We pioneered the concepts of co-attention and anti-attention maps, and utilized them to guide the direction of attention-shift for adversarial samples. The attention of the adversarial examples is shifted to regions that are not concerned by various vision tasks to enable cross-task attacks."}, {"title": "III. THE PROPOSED METHOD", "content": "In this section, we introduce how our proposed Cross-Task Attack (CTA) shifts the attention of adversarial samples from important areas to areas that are overlooked by various visual tasks.\nA. Overview of the Framework\nIn order to obtain cross-task adversarial samples, we first need to identify the regions of the samples that are not of interest to various visual tasks, and then use adversarial perturbations to shift attention to these regions. We propose a self-supervised cross-task attack method named CTA, as shown in Figure 3. CTA consists of two stages: attention extraction stage and attention shift stage. The attention extraction stage is to obtain the co-attention and anti-attention maps of clean samples. The former reflects the important areas that different vision task models need to pay attention to, while the latter reflects the unimportant areas that are ignored. The attention shift stage is to shift the adversarial samples' attention from the co-attention area to the anti-attention area by adding adversarial perturbations, thus enabling cross-task attacks.\nB. Attention Extraction Stage\nThe process of attention extraction stage is the bottom part of Figure 3. First, we use ready-made pre-trained models and Grad-CAM to obtain attention maps of clean samples in different vision tasks. Because the attention of different models of the same task is similar, it is enough to choose one model for each vision task [28]. Specifically, the formula for calculating the Grad-CAM attention heatmap A is\n$A(i, j) = max \\0, \\sum\\sum\\sum \\frac{\\partial y_c}{\\partial F_k(i, j)} F_k(i, j) \\$ (1)\nwhere Z is the total number of pixels in the feature map, $y_c$ is the probability that classifier $f_{cls}$ predicts that the input image x belongs to class c, and $F_k(i,j)$ is the value of the k-th feature map of the last convolutional layer at position (i, j).\nAfter obtaining the attention map A for each vision task, we need to find which regions of the picture are the focus of all vision tasks. Therefore, we fuse the attention heatmaps of different visual tasks to obtain the co-attention map, which represents the common focus area of different visual tasks. We used a simple and effective method for feature fusion, calculated as follows:\n$co-attention(i, j) = Scale (\\sum A_k (i, j) )$ (2)\nwhere Scale means to normalize the value range of the heatmap to [0,1], K represents the number of visual tasks, and $A_k(i, j)$ represents the value of attention heatmap of the k-th visual task at position (i, j). The high-value pixel area of co-attention map is the area that different visual tasks all focus on.\nAt last, we invert the co-attention map to get the anti-attention map as follow:\n$anti-attention(i, j) = 1 \u2212 co-attention (i, j)$ (3)\nanti-attention represents regions that are not attended to by all vision tasks, which can be used as labels for self-supervised training in attention shift stage.\nC. Attention Shift Stage\nThe process of attention shift stage is the upper part of Figure 3. To shift the attention of input image, we use a generator to generate adversarial perturbations and add them to the image to change its mapping in the feature space. The calculation process for adversarial sample is as follows:\n$x' = x + G(x)$ (4)\nwhere x represents the input clean sample, x' represents the adversarial sample without range constraints, G represents the generator. To increase the invisibility of adversarial samples, we need to crop the adversarial sample at the pixel level:\n$x_{adv} (i, j) = min(x(i, j) + \\epsilon, max(x' (i, j), x(i, j) \u2013 \\epsilon))$ (5)\nwhere $x_{adv} (i, j)$ and x(i, j) represents the value of adversarial sample and clean sample at position (i, j), $\\epsilon$ represents disturbance range threshold. Each pixel of the adversarial sample $x_{adv}$ is cropped to the range of $[x - \\epsilon, x + \\epsilon]$.\nIn order to obtain the attention heatmaps of adversarial samples, we used the parameter-frozen feature extractor (ResNet50) to calculate $y^c$. By substituting $y^c$ into Equation 4, the attention map $A_{adv}$ of the adversarial sample can be obtained. We use the distance between anti-attention map and $A_{adv}$ as the loss function to update the parameters of generator G. More precisely, the loss function is\n$L = \\frac{1}{N} \\sum (anti-attention(i, j) \u2013 A_{adv} (i, j))^2$ (6)\nwhere L is the loss function and N is the total number of pixels in the image.\nBy updating the parameters of generator G through minimizing L, CTA can generate more effective cross-task perturbations. These perturbations guide the attention of adversarial samples towards regions of high numerical value in the anti-attention maps, which are typically ignored by all visual tasks. The detailed process of our method CTA is outlined in Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct extensive quantitative experiments on three classic visual tasks: image classification, object detection, and semantic segmentation, to evaluate the effectiveness and robustness of our proposed method CTA for cross-task attack. We also perform qualitative experiments to observe the trend of Grad-CAM attention heatmaps across the training iterations.\nA. Experimental Setup\n1) Evaluation datasets.: Following the experimental settings of previous work [22], [27], [36], we randomly select 10 samples from 1000 classes in the ImageNet validation set, totaling 10000 samples, as the validation set for the classification task. We use the complete validation set of PASCAL VOC 2012 as the validation set for object detection and semantic segmentation tasks.\n2) Generator training.: We use a ResNet architecture composed of downsampling blocks and upsampling blocks as generator G [37]. We use the images in the VOC 2012 training set to train the generator G. It is worth noting that the training of CTA does not require any ground true labels, as we use ready-made models to extract anti-attention graphs for self-supervised training. The pretrained model for each visual task adopts classic and ready-made models, with ResNet50 as classification model $f_{cls}$, SSD as detection model $f_{det}$, and U-nets as segmentation model $f_{seg}$. The feature extractor D for adversarial samples adopts ResNet50. We use Adam optimizer for training, learning rate is set to 1e-3, first and second moment exponential decay rates are set to 0.5 and 0.99. We train two versions of perturbation generator G based on different perturbation range thresholds, corresponding to epsilon 10 and 16 respectively. Our experimental device uses three GPU of RTX2080ti with 11GB memory and a CPU of Intel(R) Core(TM) i5-10400F.\n3) Comparison attack algorithms.: We choose five adversarial attack methods for comparison: 1. DR, a cross-task adversarial attack method that does not rely on any specific task loss function, it reduces the feature map standard deviation to create adversarial examples that fool multiple visual tasks; 2. RB-DR, which adds a random blur (RB) data augmentation method on the basis of DR to increase the success rate of attack. 3. S\u00b2I-FGSM, a single-task cross-model attack algorithm that belongs to the FGSM adversarial attack family, it relies on a specific task loss function, and uses frequency domain transformation to enhance the transferability of adversarial examples and improve the cross-model attack effect. To the best of our knowledge, S\u00b2I-FGSM has been proven to be the state-of-the-art method for single-task cross-model attack. 4. S2I-SI-TI-DIM, where we have aggregated existing popular single-task attack methods, including S\u00b2I-FGSM, TI-FGSM, SI-FGSM and DIM, to achieve the strongest single-task cross-model attack for comparison. 5. Gaussian noise, the performance baseline for adversarial attacks. The hyperparameter settings for S\u00b2I-FGSM and S\u00b2I-SI-TI-DIM are set according to the default settings in S\u00b2I-FGSM [23]. The hyperparameter settings for DR and RB-DR are set according to the default settings in DR [27].\n4) Evaluation metric.: For the classification task of Imagenet, we use Top-1 accuracy as the evaluation metric. For the obeject detection task of PASCAL VOC 2012, we use mean Average Precision (mAP) and mean Average Recall (mAR) as evaluation metrics. For the semantic segmentation task of PASCAL VOC 2012, we use Global Correct Rate (GCR) and mean Intersection over Union (mIoU) as evaluation metrics.\nB. Attack Normally Trained Models\n1) Image classification task results.: In the image classification task, we choose VGG19 [35] and IncResv2 [38] pretrained on ImageNet as the attack target models. Table I shows the classification accuracy of our proposed CTA attack method and the compared attack methods on the ImageNet validation set. It can be observed that the cross-task attack method DR performs very weakly in classification tasks, only reducing the accuracy rate by average 13.92% compared to clean samples. After applying random blur (RB) on the basis of DR, RB-DR has an about 2% improvement in attack performance. S\u00b2I-FGSM and S\u00b2I-SI-TI-DIM are adversarial attack methods designed for classification, which use the loss function and real labels of the classification task, thus having very strong attack performance in classification tasks. We regard S2I-FGSM and S2I-SI-TI-DIM as the upper bound of attack performance for classification tasks. Compared to DR, our CTA reduces the accuracy rate by 54.08% and is close to the upper bound of classification attack performance, demonstrating its effectiveness in classification scenarios.\n2) Object detection task results.: In the object detection task, we choose YOLOv3 [39] and Faster-RCNN [40] pretrained on PASCAL VOC 2012 as the attack target models. Table I shows the mAP and mAR of our proposed CTA attack method and the compared attack methods on PASCAL VOC 2012 validation set. Figure 4 shows the mAP of 20 categories of Faster-RCNN on different adversarial samples. As shown in Table I, DR can reduce the mAP and mAR by an average of 20.6% and 15.6% compared to clean image. Random blur (RB-DR) can slightly improve DR's attack performance. The attack performance of S\u00b2I-FGSM is similar to RB-DR, but the performance of S\u00b2I-SI-TI-DIM is significantly better compared to S2I-FGSM. The reason is that S\u00b2I-FGSM, TI-FGSM, SI-FGSM and DIM are designed to improve transfer ability, so their combined method S\u00b2I-SI-TI-DIM has stronger transfer ability than any single component, making it perform well in cross-task scenarios. Compared to existing attack methods, our CTA achieves the lowest mAP and mAR in all cases. As shown in Figure 4, CTA has the lowest mAP in 14 out of 20 categories. Our experiments demonstrate CTA's effectiveness in object detection scenarios.\n3) Semantic segmentation task results.: In semantic segmentation, we choose DeepLabv3 [41] and FCN [42] pretrained on PASCAL VOC 2012 as the attack target models. Table I shows the GCR and mIoU of our proposed CTA attack method and the compared attack methods on PASCAL VOC 2012 validation set. Figure 5 show the mIoU of 21 categories of deeplabv3 on different adversarial samples. As shown in Table I, DR can reduce the GCR and mIoU by an average of 4.62% and 15.25% compared to clean image. Random blur (RB-DR) can slightly improve DR's attack performance. S\u00b2I-FGSM and S\u00b2I-SI-TI-DIM exhibit greater attack performance than DR and RB-DR due to their strong transfer ability. Compared to existing attack methods, our CTA achieves the lowest GCR and mIoU in all cases. As shown in Figure 5, CTA has the lowest mIoU in 15 out of 21 categories. Our experiments demonstrate CTA's effectiveness in semantic segmentation scenarios."}, {"title": "C. Attack Adversarially Trained Defense Models", "content": "A deep learning model trained on adversarial examples can weaken the effectiveness of adversarial attacks. To further demonstrate the effectiveness of our adversarial attack method, we conducted experiments on attacking defense models in classification, detection, and segmentation tasks. Referring to the work [43], we conduct ensemble adversarial training on IncResv2, Faster-RCNN and FCN to obtained dense models adv-incResv2, adv-Faster-RCNN and adv-FCN for different tasks. As shown in Table II, it can be seen that DR and RB-DR have lost their ability to attack defense models for classification and segmentation tasks under the condition of $\\epsilon$=10. Compared with existing attack methods, our proposed CTA method is still the best in object detection and semantic segmentation scenarios, and is also very close to S2I-FGSM and S2I-SI-TI-DIM in classification tasks."}, {"title": "D. Attention Visualization", "content": "We visualized the Grad-CAM heatmap of the adversarial samples corresponding to each epoch of the training iteration. As shown in Figure 6, as the number of training iterations increases, the attention of non-important regions (i. e., backgrounds) that should be ignored becomes high, while the attention of important regions (i. e., cow) that should be paid attention to becomes low. This experiment intuitively demonstrates the attention movement process of adversarial samples using our CTA."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel cross-task adversarial attack method CTA, which can generate adversarial examples that can fool multiple visual tasks simultaneously. Unlike existing attack methods, CTA can directionally guide the attention shift of adversarial samples. CTA utilizes Grad-CAM to extract common attention regions from different visual task models, and uses a generator to generate adversarial samples that can shift attention to areas overlooked by all visual tasks, thereby achieving cross-task attacks. CTA does not rely on any specific task loss function or ground true label, making it a general and flexible method for cross-task attack. Our extensive experiments have shown that our method outperforms the comparative methods in object detection and semantic segmentation tasks. In image classification task, our method outperforms existing cross-task attack methods and approaches the single-task attack methods designed for classification task. We also visualize the Grad-CAM attention heatmaps of our method CTA, and intuitively demonstrates the attention movement process of adversarial samples with increasing training iterations."}]}