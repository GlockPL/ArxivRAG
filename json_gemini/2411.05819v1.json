{"title": "Hierarchical Sentiment Analysis Framework for Hate Speech Detection: Implementing Binary and Multiclass Classification Strategy", "authors": ["Faria Naznin", "Md Touhidur Rahman", "Shahran Rahman Alve"], "abstract": "A significant challenge in automating hate speech detection on social media is distinguishing hate speech from regular and offensive language. These identify an essential category of content that web filters seek to remove. Only automated methods can manage this volume of daily data. To solve this problem, the community of Natural Language Processing is currently investigating different ways of hate speech detection. In addition to those, previous approaches (e.g., Convolutional Neural Networks, multi-channel BERT models, and lexical detection) have always achieved low precision without carefully treating other related tasks like sentiment analysis and emotion classification. They still like to group all messages with specific words in them as hate speech simply because those terms often appear alongside hateful rhetoric. In this research, our paper presented the hate speech text classification system model drawn upon deep learning and machine learning. In this paper, we propose a new multitask model integrated with shared emotional representations to detect hate speech across the English language. The Transformer-based model we used from Hugging Face and sentiment analysis helped us prevent false positives. Conclusion. We conclude that utilizing sentiment analysis and a Transformer-based trained model considerably improves hate speech detection across multiple datasets.", "sections": [{"title": "Chapter 1: Introduction", "content": "In recent decades, people have become more and more addicted to social media platforms where they unleash their thoughts/opinions or even daily moods. However, the downside of this trend is some or most harmful and racist material that gets spread on a mass level to target one person or community. What constitutes hate speech? It has no formal definition, but it is generally agreed to be speech that seeks to harm or enforce sins against socially disadvantaged groups [1,2]. The European Commission's General Policy Recommendation No. 15 offers a comprehensive definition, characterizing it as \"advocacy, promotion, or incitement, in any form, of the denigration, hatred, or vilification of a person or group of persons, as well as any harassment, insult, negative stereotyping, stigmatization, or threat in respect of such a person or group of persons.\" This includes justification based on various personal characteristics or status.\nIn today's digital landscape, the sheer volume of daily web content makes manual content monitoring impractical. To address and counter the spread of online hate speech, the European Commission, in May 2016, brokered an agreement with Facebook, Microsoft, Twitter, and YouTube to establish a \"Code of Conduct on countering illegal hate speech online.\"[3] Over the years, Instagram, Snapchat, and other sites like Dailymotion or Jeuxvideo. com and TikTok joined this initiative. Nevertheless, maintaining compliance with the EU Code of Conduct is difficult for online platforms. A tool that comes to the rescue in this case is Natural Language Processing (NLP). The hate speech detection systems designed by the NLP community have utilized different techniques and approaches primarily based on traditional machine learning and deep learning. Nevertheless, they are proving to be performing decently. Hate speech, on the other hand, generates real-life violence, increases polarization in societies, and perpetuates injustices. Higher levels of divisive language may provoke mental health issues, creating fear for those being targeted and preventing freedom of speech. In fact, hate speech is also a driving force behind the erosion of trust and the rise in toxicity that exists today within online communities.\nIn conclusion, individuals found guilty of employing hate speech often face substantial fines and even imprisonment. Additionally, this injurious and demeaning content can detrimentally impact an individual's mental well-being [4].\nThe majority of studies in sentimental analysis center on the foundational human emotions delineated by Ekman [3], encompassing anger, fear, sadness, joy, surprise, and disgust. Psychological research emphasizes that [5] negative sentiment messages frequently indicate emotions like anger, disgust, fear, or sadness, while positive textual content is associated with feelings of joy. Moreover, it is essential to acknowledge that abusive language and conduct are inherently intertwined with the emotional and psychological condition of the speaker [6]. Notably, messages characterized by hate speech have been consistently shown, in recent years,[7] to exhibit negative sentiments and emotions such as anger, disgust, fear, and sadness, as substantiated by multiple studies [8].\nConsidering the significant impact of emotions on hate speech messages within this study, we have explored some novel techniques to identify new hateful cases from the data collected. This required the inclusion of Sentiment Analysis (SA), polarity identification, emotion classification, and a transformer-based ensemble in text criticism as additional hate speech detection mechanisms gradually increased."}, {"title": "", "content": "This study introduces an innovative technique for hate speech (HS) classification on social media platforms, specifically focusing on English texts. Our model evaluation is performed on Hugging Face datasets that come with hateful instances, and this dataset needs to be manually annotated as hate. Notably, our research highlights the effectiveness of Multi-Task Learning (MTL) based models, in particular MTLsent+emo that combines with a monolingual Transformer model- BETO has incorporated adopted polarity and emotion knowledge for better HS detection. For benchmarking, we evaluate the performance of MTLsent+emo against a monolingual Transformer-based baseline model (re-trained for Romanian) and previous state-of-the-art results in English hate speech detection. We empirically evaluate various benchmark suites and, within our analysis, prove to be significantly superior in performance over the baseline model, walking through multi-task learning leveraging sentiment from hate tweets for compelling predictions. Our error analysis further presents critical analyses of how the MTL model performs and describes the unique ways in which English-speaking users are expected to produce hate speech. The current study helped enhance the work done to check on the spread of hatred, like an epidemic worldwide, specifically through English as an internet language.\nIn the future, our model aims to enhance its ability to identify all offensive language instances. This is particularly challenging because people often employ highly offensive terms in various qualitative contexts. For instance, some African Americans commonly use the term \"n*gga\" as a colloquial expression online [9]. We aim to address such prevalent language on social media by introducing fine-grained labels that categorize data from Hugging Face into three distinct groups: hate speech, offensive language, or neither. We will then proceed to train a model capable of effectively distinguishing between these categories, subsequently analyzing the outcomes to gain a deeper understanding of this differentiation. This research contributes to the broader field of hate speech detection and emphasizes some of the primary challenges associated with accurate classification. In conclusion, future endeavors should emphasize considering contextual factors and the diverse ways hate speech is employed [10]."}, {"title": "Chapter 2: Research Literature Review", "content": "In recent years, the prevalence of hate speech in online content has brought attention to its importance in dealing with text data. This is especially true in the field of Natural Language Processing (NLP), where a wide range of machine learning approaches have been tried as part of different research efforts. A considerable part of these investigations is based on finding Hate Speech in different social media networks. Bag-of-words methodologies typically show improvements in recall rates. However, it may lead to increased false positive rates because as more hate vocabulary is included in a lexicon, tweets mentioning those words get mislabeled as instances of Hate Speech [11,12]. At the beginning of the research, traditional machine learning algorithms were used for trials, such as support vector machines, random forests, decision trees, and logistic regression. These features were combined with different types of syntactic, semantic, lexical sentiment, and lexicon-based diverse feature sets [13,14,15]. The difference between hate speech and other types of offensive language is often a question of subtle linguistic distinctions. Syntactic attributes have been utilized to enhance the identification of both the specific targets and the degree of intensity associated with hate speech.\nDue to learning mechanisms employed by Convolutional Neural Networks (CNNs) as features embeddings, the authors in [16], proposed a technique to detect hate speech in Twitter texts. These embeddings were one-hot characters and word vectors. The authors suggested that integrating character n-grams had only a limited impact on the process of detection. In [17, 18], the authors considered a wide range of traditional and deep learning methods. According to their findings, the best results are obtained by using a combination of LSTM networks with Gradient Boosted Decision Trees. The authors evaluated a number of models based on Transformers to analyze hate speech detection in the Spanish language. (Sohn, H. & Lee H, 2019) presented a multi-channel BERT model that concatenates the representations of multiple child variant BERT models trained in various languages. Therefore, the model is also able to capture diverse meanings across languages. As an example of sentimental method [19] used a research about prediction of hatred speech on textual content. The method itself is the result of combining lexicon-based and machine-learning methods. The experiment results revealed that extracting affective cues from text helps mitigate hate speech prediction. In addition, the authors proposed a hybrid neural architecture and integrated this module into it by coupling its bidirectional LSTM with CNN components. This amalgamation enables the model to attend to a broad context of local and sequential information in natural language texts. Multiple hate speech detection models have also been used in a wide range of languages, such as Arabic and Spanish. Though the algorithms and steps may differ, it is still a hate speech detection task. These works follow the single- task learning paradigm. Single Task Learning (STL) is a process that modifies the weights of neural networks concerning an input sequence from only one classification task with data belonging to it."}, {"title": "System Design", "content": ""}, {"title": "Chapter 5: Impacts of the Project", "content": "Understanding the relationship between online hate speech and offline hate crimes allows policymakers, public safety agencies, and supportive services to react more effectively in order to protect communities from harm [20]. The more we learn to recognize and treat it, the better our online conversations will be.\nBy identifying and monitoring hate speech and radicalization activity online, we can identify people or groups at risk of perpetuating VE activities. In doing so, the identification of individuals early helps in being able to avail preemptive intervention and prevention strategies, thereby preventing or limiting radicalization into violence [21].\nThere is an argument that the suppression of hate speech diminishes free public discourse by silencing minority voices and narrowing diversity of opinion [22]. Hate speech laws that systematically target certain perspectives might also inadvertently nurture more intolerance [23]. The excessive carefulness that comes with identifying hate speech may well raise legitimate concern about infringements on the freedom of expression, a right which is one of the basic foundations in various democratic societies. Therefore, biased training data and algorithms in hate speech detection models can lead to targeting some groups unfairly and potentially ignoring others which cause fairness problems [24]. Given this, it is crucial to accurately recognize and reduce these biases when building and deploying hate speech detection systems in order to ensure fairness as well as accuracy.\nThe project \"Hierarchical Sentiment Analysis Framework for Hate Speech Detection: Implementing Binary and Multiclass Classification Strategy\" is one of the factors that determines public health safety. The AI takes the form of algorithms, and its job is to identify hate speech and harmful digital content all essential for creating a safer cyberspace. It can go a long way in protecting the mental and emotional health of an individual against some negative psychological impacts that online harassment may have on him or her. This is also essential in avoiding even worse, real-world harm that can stem from hate speech driving up internet conflict. In addition, it improves mental health somewhat indirectly but effectively. This, though indirectly in nature, improves its citizens' security and psychological well-being. Reducing the spread of harmful content ensures that fewer people are affected by harm and creates a safer internet space.\nSpecifically, exposure to hate speech has been shown create negative impacts on individuals in terms of self-esteem, reduced task performance and levels out anxiety or fear. In addition, it can result in a myriad of harmful effects such as radicalization, violence events and hate crimes, aggressiveness or incivility attitudes in society even mistrust [25]. The identification and reduction of hate speech can prevent many of those consequences, making the online atmosphere a healthier one.\nHate speech regularly focuses on marginalized communities, leading to a more hostile online space. Less hate speech means more people can contribute to the global conversation, and everyone can participate in newfound inclusive online spaces [26].\nAlgorithmic censorship could put unimaginable power in the hands of social platforms to control not just public but private conversations. This might lead to a decreased diversity of opinions and may also narrow the spectrum of perspectives available for constructive debates [28,29].\nEven benign opinions could then be stifled as users sit back and think they might get incorrectly flagged by the detection systems. This can have a chilling effect, as this recent example highlights."}, {"title": "", "content": "Hence, anyone using derogatory language to describe Native Americans as \"merciless Indian savages,\" imputing them with inferiority is hate speech (to use simple common sense). Believe it or not, this is a particular quote from the Declaration of Independence. In the context of that user's history, it does not promote hate speech but may have been referring to a historical document (sorry, I cannot say what was shared) for some other purpose. This underscores the importance of considering user intent and context in detecting hate speech [30].\nAnother remaining challenge is that automatic hate speech detection is a closed-loop system; individuals are aware that it is happening, and actively try to evade detection. For instance, online platforms removed hateful posts from the suspect in the recent New Zealand terrorist attack (albeit manually), and implemented rules to automatically remove the content when re-posted by others. Users who desired to spread the hateful messages quickly found ways to circumvent these measures by, for instance, posting the content as images containing the text, rather than the text itself. Although optical character recognition can be employed to solve the particular problem, this further demonstrates the difficulty of hate speech detection going forward. It will be a constant battle between those trying to spread hateful content and those trying to block it [30]."}, {"title": "Chapter 6: Conclusions", "content": "A novel way of classifying Hate Speech on social media, this project focuses on English language content. Based on datasets from 'Hugging Face,' the study deals with identifying and annotating hate speech. Its main reason is to make it even more accurate and efficient in recognizing hate speech with the help of sophisticated machine learning techniques.\nROBERTa-base and XLM-ROBERTa, two-strong transformer-based models, were used in this project. RoBERTa-base has been selected due to its high quality and performance in NLP tasks [8]. At the same time, XLM-ROBERTa is chosen because of its multi-lingual model, which could be helpful for cross-lingual scenarios. Notorious for its state-of-the-art performance in text classification, sentiment analysis, named entity recognition, and question answering.\nThe process begins with merging underrepresented labels into a new single class to balance classes. The classifying step is binary, where the dominant category is differentiated from the combined one. A multi-class classification is then carried out on the merged class to determine their original labels. Our two-step classification scheme also harnesses the power of a transformer- based model, which allows easier adaptation to imbalanced datasets and thus boosts robustness.\nThe steps of pre-processing data, training, and evaluation are done with the utmost care using the ktrain library, which makes transformers easy to use. The final model tuning includes training a binary classification model to separate between the original dominant class and combined one, then fine-tuning a classifier for multi-class task separating between initial labels inside combining classes.\nThis work makes a considerable contribution to dealing with hatred and, more specifically, hate speech, in the most used language by native speakers on an international level, which is English. In the next phase, we will improve its capacity to detect offensive language more fully because of how tricky these are with context usage by profanity words that occur very often. This study demonstrates the power of innovative machine learning algorithms in combating one of the most critical issues facing social media today."}, {"title": "Limitations", "content": "The project also has several limitations, mainly due to the lack of an internationally acknowledged definition of hate speech. One person's hate speech is another person's free words. It is the same fundamental subjectivity that makes it a difficult task to ascertain whether hate speech should be considered an instance of free speech. This also can be said for the datasets that are used for this as they all get affected by no firm, & or universal definition of them.\nMoreover, one of the significant constraints behind hate speech detection is ambiguity. Many things can be interpreted in a million ways, making it hard to put together an exact and complete data set. Another factor that hinders the development of a viable hate speech detection system is the variety in language. These are the main constraints that one comes across in this field.\nThese considerations are pretty limited, and it is best to take them with a grain of salt while still acknowledging that combatting hate speech has become far more nuanced, but the effort may be as noble as before. Researchers and practitioners must overcome these limits, facing the fact that strategies for dealing with hate speech in digital spaces are under continuous development."}, {"title": "Future Improvement", "content": "This project is promising for future improvements. Initially developed for English, the methodologies may go beyond this scope and detect hate speech in other languages as well. Moreover, the principles of this project could be adjusted for voice messages so that its use is not limited to text. The main aim of the model in the upcoming years is to perform a better job at picking possibly abusive language in all the senses. However, it is a challenge to identify the most offensive terms because they are used in different qualitative contexts."}]}