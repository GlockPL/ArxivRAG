{"title": "Edge-AI for Agriculture: Lightweight Vision Models for Disease Detection in Resource-Limited Settings", "authors": ["Harsh Joshi"], "abstract": "This research paper presents the development of a lightweight and efficient computer vision pipeline aimed at assisting farmers in detecting orange diseases using minimal resources. The proposed system integrates advanced object detection, classification, and segmentation models, optimized for deployment on edge devices, ensuring functionality in resource-limited environments. The study evaluates the performance of various state-of-the-art models, focusing on their accuracy, computational efficiency, and generalization capabilities. Notable findings include the Vision Transformer achieving 96 accuracy in orange species classification and the lightweight YOLOv8-S model demonstrating exceptional object detection performance with minimal computational overhead. The research highlights the potential of modern deep learning architectures to address critical agricultural challenges, emphasizing the importance of model complexity versus practical utility. Future work will explore expanding datasets, model compression techniques, and federated learning to enhance the applicability of these systems in diverse agricultural contexts, ultimately contributing to more sustainable farming practices.", "sections": [{"title": "1 Introduction", "content": "In the era of precision agriculture, the application of computer vision and deep learning technologies holds immense potential for automating critical tasks such as fruit classification and disease detection. These technologies promise to enhance crop management, improve yield prediction, and facilitate early disease intervention. However, the deploying such advanced systems in real-world agricultural settings faces significant challenges, primarily due to the scarcity of large, annotated datasets and the limited computational resources available to many farmers and agricultural workers.\nThis study aims to develop lightweight and efficient computer vision pipelines that assist farmers in detecting orange diseases using minimal resources. The proposed system integrates object detection, classification, and segmentation models to provide an automated solution for detection, species classification and disease diagnosis. By optimizing these models for deployment on edge devices, the system is designed to function effectively even in resource-limited environments, making it accessible to a wider range of users. Furthermore, the study evaluates the performance of several state-of-the-art models, examining their accuracy, computational efficiency, and generalization capabilities. Through rigorous testing and validation, the study demonstrates the potential of computer vision in addressing real-world agricultural challenges and improving disease management for orange crops."}, {"title": "2 Literature Review", "content": "Recent advancements in computer vision have significantly impacted agricultural image processing, particularly in the domains of semantic segmentation, disease detection, and crop classification. Convolutional Neural Networks (CNNs) have emerged as powerful tools for these tasks, with architectures like U-Net and Fully Convolutional Networks (FCNs) demonstrating superior performance [1] . achieved 98% accuracy in fruit and vegetable classification using DenseNet with squeeze-excitation blocks, highlighting the potential of enhanced CNN architectures [2]. However, the scarcity of high-quality labeled datasets often hinders model performance,\nnecessitating the development of various data augmentation techniques, including Manipulation-Based Data Augmentation (MBDA) and synthetic image generation using Generative Adversarial Networks (GANs) [3],[4].\nDisease detection in agriculture has seen notable progress through the integration of multiple data sources and advanced architectures. Chen et al. developed a framework combining image and sensor data, achieving 94% mIoU in lesion segmentation, surpassing traditional models like UNet++ [5]. This trend towards data fusion and specialized architectures represents a significant advancement in agricultural disease detection capabilities. In citrus disease detection, CNNs have significantly improved identification accuracy for diseases such as Greening, Canker, Anthracnose, and Melanose [6]. The integration of image-text multimodal fusion and knowledge assistance has shown promise in improving detection accuracy in complex backgrounds, suggesting a potential direction for future work [6].\nThe field has evolved to incorporate multiclass semantic segmentation for comprehensive agricultural analysis. Khan et al. introduced a multifaceted approach integrating frequency-domain image co-registration and transformer-based segmentation, achieving over 94% accuracy in crop classification [7]. Li et al. enhanced agricultural image segmentation through an Agricultural Segment Anything Model Adapter (ASA), significantly improving segmentation accuracy for specific agricultural tasks [8]. Despite these advancements, challenges persist in developing robust models for agricultural applications. Future research should explore advanced data augmentation techniques, integrate multimodal data, and develop scalable, unified frameworks for comprehensive agricultural analysis [1,[9]. The ongoing development of specialized models and techniques promises to further enhance the capabilities of computer vision systems in agriculture, potentially leading to more efficient and accurate crop management and disease detection methods.\nThis review addresses key challenges in agricultural computer vision: creating integrated, scalable systems and implementing multi-task learning for field deployment. We propose a cohesive framework combining orange detection, counting, species classification, and disease segmentation. Using oranges as our test subject, we aim to develop a resource-efficient, scalable pipeline. Our approach seeks to optimize real-time processing and enhance generalizability across diverse orange varieties and growing conditions."}, {"title": "3 Research Methodology", "content": "The oranges were sourced from multiple markets, representing a diverse array of orchards, which introduces variability across different environmental and cultivation practices. This diversity enhances the dataset's representativeness, supporting model generalization to varied real-world conditions. Mainly 5 species of Oranges (Tangerine, Navel, Blood Oranges, Bergmout, Tangelo) and Disease species (Citrus cranker, Black spots, Sooty mould, Blue-green mould, Citrus greening) were chosen. For creating the dataset, the images were annotated with bounding boxes to created object detection dataset and then exported into coco and yolo format. The classification task specific dataset with 5 orange species (47-52 images per class ) was created by taking images of the oranges from various angles, under various lighting condition and a smart phone was used to imitate real world scenarios. In the case of segmentations data, it was created using same images where disease were annotated as per the class using review of the farmer.\nMethodology of this paper (Figure: 1), For image classification a set of algorithms was evaluated, the algorithms consist of convolutional models (ResNet50 [10],\nEfficientNet [11], InceptionNet [12]) and transformer-based [13] architectures (Vision\nTransformer (ViT) [14], coatnet [15], Shifted window Transformer (swin)[16]), regnet\n[17], DenseNet [18], and MobileNetV3 [19]. Additionally knowledge distillation [20]\n(ResNet+ DenseNet) and Teacher-Student model were used, to improve performance\nby transferring learned features from larger to more efficient models. Each model was\ninitialized with ImageNet [21] pretrained weights to facilitate transfer learning and\nimprove convergence .The TIMM Python package was used for training, and mod-\nels were fine-tuned using Cross Entropy Loss with the Stochastic Gradient Descent\n(SGD) optimizer, combined with a ReduceLROnPlateau learning rate scheduler over\n50 epochs. Early stopping and learning rate decay were implemented to adaptively\ncontrol learning rates and prevent overfitting [22]. Model's performance was evalu-\nated on basis of Recall, Precision, F1-score, Accuracy and Area under RoC curve and\ncross validation (5-fold) was applied to comprehensively capture model performance\nacross various classification aspects [23]."}, {"title": "4 Results", "content": "Classification results\nThe study compared the performance of several deep learning models, ranging from lightweight architectures like MobileNetV3 to complex ensembles such as ResNet + DenseNet, using weighted ROC AUC, accuracy, and macro-averaged precision, recall, and F1-score. Significant performance variations were observed, reflecting differences in model architecture and their ability to handle multi-class classification of orange species."}, {"title": "2. Segmentation Model Performance", "content": "For segmentation tasks, the models were evaluated based on Intersection over Union (IoU), pixel accuracy, precision, recall, and F1-score. These tasks focused on identifying diseases such as Blackspot, Sooty Mold, and Citrus Canker on orange surfaces.\n\nAmong the tested models, LinkNet demonstrated the highest IoU (0.9039) and\nF1-score (0.9255), shown in table 3 outperforming the others in segmenting diseased\nregions. DeepLab followed closely with comparable metrics, while FPN achieved\nsimilar accuracy despite its significantly smaller size (22 MB). U-Net++, while effec-\ntive, had a slightly lower IoU (0.8967) but maintained strong performance in terms\nof precision and recall.\nWhile larger models such as LinkNet and DeepLab achieved the highest segmen-\ntation performance, their substantial size may restrict their applicability in real-world\nscenarios. In contrast, lightweight models like FPN provide a viable alternative, offer-\ning competitive performance with a significantly reduced model size. This trade-off\nbetween model size and accuracy is crucial for applications in resource-constrained\nenvironments."}, {"title": "3. Object detection", "content": "YOLOv8-S stands out for its exceptional speed (10.9ms per image) and accuracy\n(mAP50: 0.949), making it ideal for real-time applications. It achieves a high balance\nbetween precision and recall (Table 4)."}, {"title": "5 Discussion", "content": "Our research addresses key challenges in agriculture, particularly for small-scale farmers in resource-constrained environments. Notably, our selected models achieve high accuracy with approximately 50 training images per class, demonstrating efficient learning from limited datasets. This efficiency is especially beneficial in agricultural contexts, where large datasets are difficult to obtain due to resource limitations and crop-specific variations [39]. Achieving robust performance with small datasets accelerates deployment in new settings and supports adaptability across different crop varieties, making it a valuable tool for precision agriculture. Our findings highlight the potential of lightweight computer vision pipelines for classifying orange species and detecting diseases\u2014a topic of significant interest in recent agricultural research [4]. Among the models evaluated, the Vision Transformer (ViT) provided the highest accuracy for classification, while LinkNet outperformed others in segmentation tasks, underscoring the value of advanced architectures in precision agriculture [40],42]. However, the trade-off between accuracy and computational efficiency remains an important consideration (Table 2), especially when deploying models in low-resource environments.\nTo address such challenges, we prioritized lightweight models for deployment.\nYOLOv8-S (6 MB) for object detection, MobileNet (24 MB) for classification, and\nFPN (22 MB) for segmentation emerged as the best candidates due to their small\nsize and suitability for edge devices. This choice is critical for farmers in rural\nareas or developing regions, where high-performance infrastructure may be lacking\n[41]. Optimizing model size and computational demand expands access to advanced\nagricultural technology, making it feasible for small-scale farmers to adopt these\nsolutions without expensive hardware.[42]\nAn additional advantage of our pipeline is its speed, processing images in approxi-\nmately 1 second per instance while maintaining high accuracy. Real-time processing\nis essential in agricultural applications, as rapid decision-making can significantly\naffect crop health and quality. For example, a conveyor belt in a packing facility\ncould efficiently sort oranges by species and detect potential diseases, enhancing\nquality control processes [43]. This balance between speed and accuracy ensures the\npracticality of our pipeline for real-world agricultural use.\nOur approach demonstrates high accuracy with limited data, making it espe-\ncially suited to specialized agricultural applications where extensive datasets are\noften unavailable [44]. This efficiency with small datasets also facilitates deployment\nin diverse environments and adaptability across crop varieties, addressing a signifi-\ncant need in precision agriculture [45]. Furthermore, deploying lightweight models,\nsuch as YOLOv8-S for object detection, allows for effective usage on edge devices\nor in low-resource areas, expanding accessibility for small-scale farmers. Achieving\nthis balance between accuracy and computational demands helps optimize resource\nutilization, making advanced agricultural solutions viable for those with limited in-\nfrastructure.\nDespite performing well with a small dataset, our models would benefit from\nfurther testing on larger, more diverse datasets to enhance robustness under various\nenvironmental conditions. Expanding the dataset to include a broader range of\nlighting, growth stages, and rare orange species could improve model generalizability\nand application across different agricultural contexts. Although Vision Transformer\n(ViT) showed strong performance, its size (323 MB) may limit use in highly resource-\nconstrained settings. Future work could focus on model compression techniques to\nreduce its size without sacrificing accuracy, enabling high-performing models on low-\npower devices.\nAs this study may not encompass the full range of environmental conditions or\norange species, a broader dataset incorporating factors like variable weather, soil\ntypes, and growth stages could further increase applicability. Future research could\nalso explore federated learning to leverage distributed datasets while ensuring data\nprivacy. Incorporating temporal data might enable disease progression tracking,\nwhile developing models that adapt to local conditions with minimal retraining could\nenhance practical utility in precision agriculture."}, {"title": "6 Conclusion", "content": "This research demonstrates the efficacy of modern deep learning architectures in addressing critical agricultural challenges through computer vision. The Vision Transformer achieved remarkable performance in orange species classification with 96% accuracy and a 0.95 F1-score, while LinkNet demonstrated superior segmentation capabilities with an IoU of 0.9039. The lightweight YOLOv8-S model's exceptional performance in object detection (mAP50: 0.949) with minimal computational overhead (10.9 ms/image) particularly stands out. These results establish that sophisticated computer vision tasks can be effectively accomplished with limited training data, a crucial finding for specialized agricultural applications.\nThe study reveals a nuanced relationship between model complexity and practical utility. While larger models like ViT (323 MB) achieved superior accuracy, lightweight alternatives such as MobileNet (24 MB) and FPN (22 MB) demonstrated competitive performance with significantly reduced computational demands. This trade-off becomes particularly significant in resource-constrained agricultural environments, where the ability to process images in approximately one second while maintaining high accuracy represents a viable solution for real-time applications.\nOur findings have broader implications for the democratization of agricultural technology. The success of these models with just 50 training images suggests a practical pathway for developing specialized agricultural applications where large datasets are traditionally unavailable. However, future research should address limitations in environmental variability coverage and explore federated learning approaches for leveraging distributed datasets. Additionally, investigating model compression techniques and temporal data analysis could further enhance the practical utility of these systems. These advancements could significantly impact small-scale farming operations by providing accessible, efficient tools for crop monitoring and disease detection, ultimately contributing to more sustainable agricultural practices."}]}