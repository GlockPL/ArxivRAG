{"title": "Efficient Event-based Semantic Segmentation with Spike-driven\nLightweight Transformer-based Networks", "authors": ["Xiaxin Zhu", "Fangming Guo", "Xianlei Long", "Qingyi Gu", "Chao Chen", "Fuqiang Gu"], "abstract": "Abstract-Event-based semantic segmentation has great po-\ntential in autonomous driving and robotics due to the advan-\ntages of event cameras, such as high dynamic range, low latency,\nand low power cost. Unfortunately, current artificial neural\nnetwork (ANN)-based segmentation methods suffer from high\ncomputational demands, the requirements for image frames,\nand massive energy consumption, limiting their efficiency and\napplication on resource-constrained edge/mobile platforms. To\naddress these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-\nbased semantic segmentation. Specifically, SLTNet is built on\nefficient spike-driven convolution blocks (SCBs) to extract rich\nsemantic features while reducing the model's parameters. Then,\nto enhance the long-range contextural feature interaction, we\npropose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLT-\nNet employs a high-efficiency single-branch architecture while\nmaintaining the low energy consumption of the Spiking Neural\nNetwork (SNN). Finally, extensive experiments on DDD17 and\nDSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30%\nand 3.30% mIoU, respectively, with extremely 5.48\u00d7 lower\nenergy consumption and 1.14\u00d7 faster inference speed.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation [1] aims to divide visual data\ninto distinct regions with clear semantic properties, thereby\nenabling a deep understanding and analysis of the scene. This\ntechnology plays a crucial role in various cutting-edge fields\nsuch as autonomous driving, security monitoring [2], and\nrobotics [3], forming the foundational component for envi-\nronmental perception capabilities of intelligent robotics [4].\nHowever, with the increasing accuracy and efficiency de-\nmands for robotics in high-dynamic, complex scenarios, tra-\nditional image-based semantic segmentation methods show\nsignificant performance degradation when processing high-\ndynamic or variable lighting scenes [3].\nEvent cameras, leveraging their innovative biomimetic\nevent generation characteristics, have inspired considerable\nresearch interests in computer vision and robotics commu-\nnities [5]. Event cameras [6] excel at detecting variations\nin light intensity at the pixel level with remarkable mi-\ncrosecond resolution. They asynchronously produce an event\nstream that consists of time stamp t, spatial coordinates\n(x,y), and polarity (+, -). The event-driven nature of these\ncameras shows several superior advantages, including HDR\nperception, low latency, low energy cost, and enhanced\nsparsity, showing promising solutions to computationally in-\ntensive semantic segmentation. Although research on event-\nbased semantic segmentation is still in its infancy, several\nworks have been proposed. Ev-SegNet [7] develops the\nfirst baseline using solely events but requires a substantial\ncomputational load. Following this, DTL [8], Evdistill [9],\nand ESS [10] attempt to compensate for the lack of visual\ndetails in event data through transfer learning or knowledge\ndistillation technologies from image processing branches.\nHowever, existing methods either are still computationally\nexpensive or require auxiliary images. Therefore, a critical\nchallenge is how to design an efficient and robust event-based\nsegmentation model with lower computational and energy\ncosts, sufficient event feature extraction ability, and without\nrequirement for auxiliary modality.\nWith the development of biological computing, Spiking\nNeural Network (SNNs) is a potential solution inspired by\nmechanisms of information generation and transmission in\nthe brain, which can convey dynamically generated binary\nspikes across spatial and temporal dimensions [11]. Thus,\nSNNs demonstrate excellent advantages in computation ef-\nficiency, temporal memory capacity, and biological inter-\npretability. The basic building block of SNNs is the spiking\nneuron, which shares a highly similar generation mechanism\nwith the event cameras, making SNNs theoretically well-\nsuited to work with such cameras. Encouraged by these\nadvantages, Kim et al. [12] attempt to integrate SNNs\nwith classical ANN-based segmentation networks, but the\nperformance is poor due to the plain feature extraction\nmodule. This indicates that combining the SNNs with event\nsegmentation still requires further exploration.\nTo address the aforementioned issues, we propose SLT-\nNet, a Spike-driven Lightweight Transformer-based semantic\nsegmentation network that utilizes events only to achieve\nenergy-efficient and robust performance. SLTNet is a hi-\nerarchical single-branch SNN-based method equipped with\nan encoder-decoder structure. It can extract both detail and\nsemantic information while maintaining high efficiency and\nlow energy consumption. Specifically, as shown in Fig. 3,\nSLTNet contains four stages in the encoder part, which\nincludes three novel Spike-driven Convlution Blocks (SCBs)\nthat capture fine-grained textural information, and two newly\ndesigned Spike-driven Transformer Blocks (STBs) that are\nresponsible for extracting long-range contextual features in"}, {"title": "II. EFFICIENT BUILDING BLOCKS", "content": "To construct a robust evolutionary spiking neuron (ESN)\nin SLTNet, we employ the Leaky Integrate-and-Fire (LIF) as\nthe basic neuron due to its excellent biological interpretabil-\nity and computational efficiency. We show its workflow\nin Fig. 1. Mathematically, we explicitly formulate ESN's\niterative forward propagation process as follows:\n$M^{t} = \\tauU^{t-1} +W^{t}X^{t-1}$,\n$S^{t} = H(M^{t}-U_{th})$,\n$U^{t} = U_{reset}S^{t}+U^{t-1}(1 - S^{t}),$\nwhere the $U^{t}$ is the membrane potential of neurons in the $l$th\nlayer at time step $t$, $W^{t}$ is synaptic weight matrix between\nlayers $l - 1$ and $l$, $S^{t}$ is the output of current layer, and $\\tau$\nis membrane time constant. The LIF module assumes that\nneurons accumulate membrane potential like a capacitor.\nThey receive the leaky membrane potential of the previous\ntime step $\\tauU^{t-1}$ and calculate current potential $W^{t}X^{t-1}$.\nWhen the accumulation reaches a certain threshold $U_{th}$, the\nneuron fires, generating a spike $S^{t}$ using Heaviside step\nfunction $H()$; otherwise, it remains zero. After firing, the\nmembrane potential rebound to $U_{reset}$.\nWe train our model with an end-to-end strategy, so the\nback-propagation in ESN module can be described as:\n$\\frac{\\partial L}{\\partial w^{t}} = \\sum_{t}(\\frac{\\partial L}{\\partial S^{t}}\\frac{\\partial S^{t}}{\\partial U^{t}} + \\frac{\\partial L}{\\partial U^{t+1}}\\frac{\\partial U^{t+1}}{\\partial U^{t}}\\frac{\\partial U^{t}}{\\partial w^{t}})$\nwhere L represents the loss function, $W^{t}$ denotes the weights,\nand $S^{t}$ and $U^{t}$ indicate the activated spike and membrane\npotential, respectively.\nThe gradient of the spiking activity function $S^{t}/U^{t}$ signi-\nfies their non-differentiable nature: either at a standstill or\nescalate to infinite magnitudes. We solve this problem by\nutilizing the evolutionary surrogate gradient function, called\nEvAF [13]. It will dynamically adapt to generate substitute\ngradient values as epochs iterate:\n$\\varphi(x) = \\frac{1}{2}(tanh K(i)(x-U_{th})+1)$,\n$K(i) = \\frac{(10 - 1)K_{max} + (10 - 10^{i}/_{N})K_{min}}{9}$\nwhere $i \\in [0, N - 1]$ is the index of training epoch. Following\nthe common setup in [13], we set $K_{min} = 1$ and $K_{max} = 10."}, {"title": "B. Spiking Lightweight Dilated Module", "content": "As illustrated in Fig. 2, the Spiking Lightweight Dilated\n(Spike-LD) module adopts a bottleneck structure [14] with\na membrane shortcut. In this design, the module establishes\na shortcut connection between the membrane potential of\nspiking neurons, addressing the vanishing gradient problem\nby achieving identity mapping in a spike-driven mode. Then,\nit integrates the spiking paradigm by following the weight\ncomputation stage with batch normalization (BN) and the\nESN model introduced previously.\nIn the feature extraction stage, considering the parameter\nincrease that traditional 3 \u00d7 3 convolutions might cause, the\nmodule utilizes decomposed convolution, a combination of\n1x3 and 3\u00d7 1 convolutions. This allows the network to\nindependently extract features from both the width and height\ndirections while maintaining sensitivity to local features"}, {"title": "C. Spike-driven Transformer Block", "content": "Inspired by Yao et al. [16], we design a spike-driven\ntransformer block (STB) as shown in the bottom of Fig. 3.\nIt comprises a spike-driven multi-head self-attention module\n(SDMSA) aimed at capturing spatial relationships and a two-\nlayer MLP complementing channel information. Specifically,\nthe main process of STB is formulated as:\n$Q = SN(RepConv1(M))$,\n$K = SN(RepConv2(M))$,\n$V = SN(RepConv3(M))$,\n$SDMSA(Q,K,V) = SUM(Q \\otimes K)) V$\n$M' = M+RepConv4(SDMSA(Q,K,V))$,\n$MLP(M') = Linear2(SN2(Linear1(SN1(M')))$,\n$M'' = M' +MLP(M'),$\nwhere SN, RepConv, and $\\otimes$ are spiking neurons, re-\nparameterization convolution [17], and Hadamard product,\nrespectively. We omit the BN operation for clarity.\nThe design of residual connections in spike transformers\nis critical; unlike the vanilla shortcut [18] and the spike-\nelement-wise shortcut [19], we use the membrane shortcut,\nwhich performs shortcuts between membrane potentials to\nenhance performance while maintaining spiking features.\nSTB offers two key advantages over traditional CNN-\ntransformer blocks. Firstly, as described in Eq. 5, the com-\nputational complexity is reduced to $O(N)$ instead of $O(N^{2})$.\nSecondly, due to the use of spiking neurons, floating-point\nmatrix multiplications are transformed into masked opera-\ntions. This means that the entire block only involves floating-\npoint addition operations, significantly reducing energy con-\nsumption and enhancing the computation efficiency."}, {"title": "III. OVERALL ARCHITECTURE OF SLTNET", "content": "To reduce computational load while maintaining seman-\ntic information from events, we design a Spike-driven\nLightweight Transformer-based Network, called SLTNet, for\nevent-based semantic segmentation based on the tailored\nefficient Spike-LD modules and STBs."}, {"title": "A. Event Representation", "content": "We encode the raw asynchronous event stream $e_{i} =$\n${(x_{i}, V_{i}t_{i}, p_{i})}_{i \\epsilon t}$ into voxel grid $V_{k,x,y} \\in K \\times H \\times W$:\n$V_{k,x,y} = \\sum_{i \\epsilon \\Delta T} \\delta([\\frac{t_{i}}{\\Delta t}] = k)\\delta(x_{i} = x)\\delta(y_{i} = y)p_{i},$\nwhere $\\Delta t$ is time interval, time index $k = [\\frac{t_{i}}{\\Delta t}]$, $\\delta$ denotes\nKronecker Delta function used for indicating whether event\n$e_{i}$ is within time period k and located at (x,y), $p_{i} \\in {\u22121,+1}$\nrepresents the polarity. Furthermore, to adapt the spiking\nparadigm, we transform the voxel grid $V_{k,x,y}$ into an event\ntensor, formatted as $E(x,y,t) \\in T \\times K \\times H \\times H$ that SNNs\ncould calculate through the temporal dimension. $E(x,y,t)$\nwill be fed into our SLTNet as event representations."}, {"title": "B. Spike-driven Encoder/Decoder of SLTNet", "content": "As shown in the top of Fig. 3, the preprocessed events\n$E(x,y,t)$ will pass through a hierarchical spiking encoder,\na lightweight spiking decoder, and two segmentation head,\ngenerating two prediction probabilities maps, $P_{1}$ and $P_{2}$.\nThe hierarchical spiking encoder starts with an initial\nblock, which stacks three spike convolution layers, each\nconsisting of a 3 \u00d7 3 vanilla convolution, batch normalization,\nand an ESN. Then, it goes through four stages: the first three\nstages are spike-driven convolution blocks (SCBs), each con-\nsisting of a downsampling layer and three spike-LD modules\nwith different dilation rates, and the last stage includes two\nSTBs. The SCB can extract local detail information, while\nthe STB enriches the long-range context information that\ncomplements the local features. The downsampling layers\napplied in stages 1 to 3 consist of stridden convolutions\nand pooling operations, transforming the feature dimensions\nfrom $C\u00d7 H \u00d7 W$ to $ \\frac{8C}{8} \u00d7 \\frac{H}{8} \u00d7 \\frac{W}{8}$. However, to reduce the"}, {"title": "C. Segmentation Loss Design", "content": "As illustrated in Fig. 3, the designed loss consists of a\ncross-entropy (CE) semantic loss $l_{1}$ using Online Hard Ex-\nample Mining (OHEM) [21] and an early-stage CE semantic\nloss $l_{2}$. Specifically, we use $l_{1}$ equipped with OHEM to\nimprove the generalization ability by selecting the hardest\nsamples from each mini-batch for training:\n$l_{1} = \\sum_{i \\epsilon s}(-\\frac{1}{c} \\sum_{j=1}^{C}y^{i}_{j}log(p^{i}_{j})), l_{2} = \\sum_{i \\epsilon s}(-\\frac{1}{c} \\sum_{j=1}^{C}y^{i}_{j}log(early^{i}_{j})),$\nwhere $S$ is the set containing the top k percent of the largest\nloses among $l_{i}$. $l_{2}$ is employed to promote rapid conver-\ngence. It measures the discrepancy between the predicted\nprobabilities, which are generated from the second-to-last\nSpike-LD module, and ground-truth label.\nTherefore, the entire loss in our SLTNet is: $Loss = \\lambda_{1}l_{1} +$\n$\\lambda_{2}l_{2}$. In our experiments, $\\lambda_{1}$=1.0, $\\lambda_{2}$=0.4, k=0.7."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We validate our method on two public large-scale event-\nbased automotive driving datasets: the commonly used"}, {"title": "D. Power and Computation Cost Analysis", "content": "Floating-point operations (FLOPs) are widely recognized\nfor evaluating the computational load of neural networks.\nIn ANNS, FLOPs almost come from floating-point matrix\nmultiplication and accumulation (MAC). Since the event-\ndriven characteristics of SNNs, the binary spikes transmitted\nwithin them result in almost all FLOPs being accumulation\n(ACC). Combining the timestep T = 1 and spiking firing rate\nR = 0.5, we can estimate the energy consumption of SNNs:\n$E = E_{MAC} \\times FL_{1} + E_{ACC} \\times FL_{2} \\times T \\times R.$\nwhere $FL_{1}$ can be FLOPs of the convolution or linear layer.\nTo maintain consistency with previous work [12], we set\n$E_{MAC}$ = 4.6pJ and $E_{ACC}$ = 0.9pJ during calculation. Com-\npared to ANN-based models, our proposed SLTNet excels\nin energy efficiency. Specifically, Ev-SegNet, Evdistill, and\nESS consume 30.20x, 96.31\u00d7, and 37.90\u00d7 more energy\nthan our model, respectively. Although Spiking-DeepLab\nand Spiking-FCN also utilize spiking architectures, their\nstraightforward components and design result in 5.48\u00d7 and\n12.00\u00d7 higher power consumption."}, {"title": "E. Ablation Studies", "content": "In SNNs, the\nquery, key, and value in self-attention are represented as\nbinary spikes rather than continuous floating-point values.\nTherefore, traditional self-attention modules designed for\nANNs are not directly applicable to SNNs. Developing\nan efficient self-attention mechanism specifically optimized\nfor SNNs remains a challenge. In Table III, we present"}]}