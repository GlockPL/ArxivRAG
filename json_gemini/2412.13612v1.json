{"title": "Are LLMs Good Literature Review Writers? Evaluating the Literature Review Writing Ability of Large Language Models", "authors": ["Xuemei Tang", "Xufeng Duan", "Zhenguang G. Cai"], "abstract": "The literature review is a crucial form of academic writing that involves complex processes of literature collection, organization, and summarization. The emergence of large language models (LLMs) has introduced promising tools to automate these processes. However, their actual capabilities in writing comprehensive literature reviews remain underexplored, such as whether they can generate accurate and reliable references. To address this gap, we propose a framework to assess the literature review writing ability of LLMs automatically. We evaluate the performance of LLMs across three tasks: generating references, writing abstracts, and writing literature reviews. We employ external tools for a multidimensional evaluation, which includes assessing hallucination rates in references, semantic coverage, and factual consistency with human-written context. By analyzing the experimental results, we find that, despite advancements, even the most sophisticated models still cannot avoid generating hallucinated references. Additionally, different models exhibit varying performance in literature review writing across different disciplines.", "sections": [{"title": "Introduction", "content": "The literature review is a form of academic writing designed to summarize, analyze, and evaluate existing research findings, theoretical perspectives, and methodologies. However, writing a literature review often requires extensive reading and summarizing of relevant literature. Literature reviews in popular fields may require tens or even hundreds of references, which is a complex and labor-intensive task. In recent years, deep learning models have been developed to assist in the automated writing of literature reviews (Kontonatsios et al., 2020; Aliyu et al., 2018). The advent of large language models (LLMs) has brought promising advances in automating certain stages of literature review writing. With their powerful generative capabilities, enhanced by training on vast amounts of academic text, LLMs have emerged as a formidable tool for writing literature reviews (Wang et al., 2024). However, how well LLMs perform in writing literature reviews remains unclear. To the best of our knowledge, no comprehensive studies have yet evaluated the literature review writing capabilities of LLMs. Literature review writing requires, on one hand, deep familiarity with the studies in a specific field, and on the other hand, the ability to summarize the core contributions of that literature. While techniques such as Retrieval-Augmented Generation (RAG) can enhance LLMs' domain-specific knowledge, most researchers use LLMs without RAG such as ChatGPT for literature review writing. Therefore, it is necessary to evaluate the literature review writing capabilities of these native LLMs.\nTherefore, in this paper, we propose a framework for automatically assessing literature review writing ability in LLMs. The evaluation of LLMs' ability to write literature reviews remains an open question. In this study, we use human-written literature reviews as the gold standard and employ external tools for a comprehensive evaluation. Specifically, we first collect human-written literature reviews to create the dataset. Then, we ask LLMs to complete three tasks: generating references, writing an abstract, and writing a literature review based on a given topic. The generated results are then evaluated from several dimensions, including the presence of hallucinations in the references, as well as the semantic coverage and factual consistency of the generated literature review compared to the human-written context.\nOur contribution can be summarized as follows.\n\u2022 First, we propose a framework for automatically evaluating the literature review writing ability of LLMs, without requiring any human involvement. This framework encompasses multiple stages, including dataset construction, LLMs gnenrated data collection, and evaluation.\n\u2022 Second, we collect 1,106 literature reviews from 51 journals across 5 disciplines as the ground truth. We then design three tasks: generating references, writing an abstract, and writing a literature review based on a given topic.\n\u2022 Then, we evaluate the writing results of the LLMs from multiple aspects, including the hallucination rate in generated references, factual consistency with human-written results, and the comparison of semantic coverage.\n\u2022 Finally, we assess four LLMs using the proposed framework. By analyzing the experimental results, we find that hallucinated references remain an unavoidable issue for current LLMs. Additionally, the performance of LLMs in writing literature reviews varies across different academic disciplines."}, {"title": "Related Work", "content": "Recent studies have increasingly focused on using LLMs for writing literature reviews. For example, Wang et al. (2024) proposed a literature survey writing process for LLMs, called AutoSurvey. By RAG retrieving the latest relevant paper incorporates real-time knowledge. In evaluation, Muti-LLM-as-judge is used to evaluate citation quality and content quality.\nAdditionally, more attention has been paid to the hallucination evaluation of reference citations. According to whether the output contradicts the prompt or not, OpenAI et al. (2023) categorized hallucination phenomenon as open domain and closed-domain hallucinations. Closed-domain hallucination assessments require only comparisons with input contexts, whereas open-domain hallucination assessments require more extensive research (Bubeck et al., 2023), such as human evaluation (Ji et al., 2023).\n? used LLMs (ChatGPT, GPT-4, and Bard LLM) to generated 11 systematic reviews under the shoulder rotator cuff pathology topic, then to evaluate the hallucination rates of reference. They found that Bard had significantly higher rates of hallucinations than ChatGPT and GPT-4. For example, Agrawal et al. (2024) chose 200 computer science topics to evaluate LLMs' hallucination rate. They first asked LLMs to generate 5 reference titles, then use BING Search API to label the reference automatically as true or not. The LLMs were then examined to see if they could actively discover the hallucinated literature through direct queries (asking directly if the paper exists) and indirect queries (who wrote the paper?).\nAthaluri et al. (2023) assessed the hallucinations of LLMs in research proposal writing. They used ChatGPT to generate 50 research proposals based on the given 50 topics. ChatGPT generated 178 references in these proposals and then they employed five researchers to manually check these references and DOIs on Scopus, Google and PubMed search engines and found that among the references 109 references had valid DOIs. score Aljamaan et al. (2024) given 5 medical topics, then used ChatGPT 3.5, Bard, Perplexity, Bing, Elicit, and SciSpace to generated 10 relevant references for each topic, and SciSpace to generate 10 relevant references for each topic, with each reference including 7 items: title, journal name, author, DOI, publication data, link to the paper, and relevance of the reference to the keyword prompt. Then, they proposed the Reference Hallucination Score (RHS) for citation hallucination assessment of LLMs, assigning different hallucination scores to each item, e.g., a score of 2 corresponding to the title and 1 corresponding to the publication date. The existing studies on evaluating citation hallucinations in literature reviews are limited to specific academic topics and do not provide a comprehensive assessment of LLM performance. Moreover, these evaluations often involve manual human intervention."}, {"title": "Methodology", "content": "In this section, we present the framework for evaluating LLMs' literature survey writing ability. The framework as shown in Figure 1, consists of three main stages: first, dataset construction; second, task design for evaluation; and third, assessment of the generated results."}, {"title": "Dataset Construction", "content": "As illustrated in Figure 1, we first collect literature survey data from the Annual Reviews website (https://www.annualreviews.org/). Annual Reviews is an independent, nonprofit scholarly publishing company, which publishes 51 journals of review articles. The articles on the website are peer-reviewed, so they are of high quality and can be used as experimental materials. We crawl all articles published in 2023, including their title, keywords, abstracts, contents, and references. Then clean them as the experimental dataset. Assessing the ability to write literature reviews is a challenging task, as evaluating the quality of content is inherently complex. In this paper, we use human-written reviews as the gold standard, which simplifies the evaluation process to some extent.\nThen, the dataset D is the article set from K journals, $D = {j_0, ..., j_k, ..., j_K}$, the number of journals is 51, so K = 50. For each journal $j_k$, there is a set of articles $j_k = {p_0, \u2026\u2026\u2026, p_i, \u2026\u2026\u2026, p_M }$, where M represents the number of articles, and each article $p_i = {t_i, W_i, a_i, C_i, R_i}$, where $t_i, W_i, a_i, c_i, R_i$ represent the title, keywords, abstract, context, and reference set $R_i = {r_1, ..., r_l, ..., r_L}$."}, {"title": "Task Design", "content": "We design three tasks as follows.\n\u2022 Given the article title $t_i$ and keywords $w_i$, ask LLMs to find the 10 most relevant studies to the research topic. Each citation study must include 7 metadata elements: title, authors, journal, year, volumes, first page, and last page. In this task, we evaluate whether LLMs can recommend reliable references based on the given topic.\n\u2022 Given the article title $t_i$ and keywords $w_i$, ask LLMs to write an abstract according to the research topic provided in the title and the keywords. The length of the generated abstract is required to be the same as the length of the original abstract of the article. In this task, we evaluate whether LLMs can write an abstract based on the given topic.\n\u2022 Given the article $t_i$ and keywords $w_i$, and abstract $a_i$, ask LLMs to write a literature review according to the research topic provided in the title, keywords, and abstract. The literature survey should be about 1000 words long. LLMs also need to back up claims by citing previous studies (with a total of 10 citations in the literature survey). In this task, we evaluate whether LLMs can write a high-quality literature survey and cite truth studies.\nThree task prompts are shown in Appendix Table 5."}, {"title": "Evaluation Metrics", "content": "Based on the type of generated text, we divide the evaluation of the model's results into two parts: first, the hallucination rate of the references generated by LLMs, and second, a comparison of the generated context with human-written results, including two dimensions: factual consistency and semantic coverage.\nReference hallucination evaluation metrics. Given that LLMs are trained on vast corpora, including academic sources, we aim to evaluate whether they can generate accurate references. In this section, we introduce the calculation process of the reference accuracy $Acc.$ and title search rate $S_t$ for each LLM.\nFor each article $p_i \u2208 D$, each LLM generates N references $R = {r_1,...,r_n, ...,r_N}$ in Task1 and Task3, each $r_n = {T, A, J, Y, V, FP, LP}$ and includes 7 elements corresponding to the article title, authors, journal, year, volume, first page, and last page. Each element corresponding to a state label represents whether it is accurate or not ${e_d | d = 0, 1, . . ., 7}, e_d = 1  or 0$.\nNext, we describe how to obtain ${e_d | d = 0, 1,..., 7}$. First, we use the generated titles T and the first author in A as the queries and search them separately from external academic search engines. Then, we obtain two candidate sets $C_t$ and $C_a$ respectively. Subsequently, we compare the generated $r_n$ with the article from candidate sets $C_t$ and $C_a$. For example, if the title of the article candidate is the same as that of $r_n$, then $e_0 = 1$. Finally, we find the best candidate article based on the sum of ${e_d | d = 0, 1, . . ., 6}$, and the one with the largest sum is the best candidate article $c_a$ of $r_n$.\nThen, we compare the algin degree between the generated $r_n$ and the best candidate article $c_a$ to determine whether $r_n$ corresponds to a true article (As shown in Eq 1). On the one hand, when its title T is correct and one other element is correct, we consider $r_n$ corresponds to a true article. Here its title T is correct or not, we give LLM a certain margin of error, as long as the title has a match rate of 80%, it will be judged as accurate, so we need another piece of information that is correct to ensure that the article is reliable. On the other hand, when its title T is incorrect, certain other information is correct which can also help us to locate the article. therefore, when the other three elements in $r_n$ are correct, we also consider it as a reliable article.\n$True(r_n) = \\begin{cases}\n1 & \\text{if } e_0 = 1 \\text{ and } \\sum_{i=0}^{6} e_i  \\geq 2 \\\\\n1 & \\text{if } e_0 = 0 \\text{ and } \\sum_{i=0}^{6} e_i  \\geq 3 \\\\\n0 & \\text{otherwise}\n\\end{cases}$   (1)\nFor each paper $p_i$ in the dataset, we formulate the accuracy of the references generated by the LLM as Eq 2. Then, for each journal $j_k$, we can obtain the accuracy of generated references $Acc_{ref.}(j_k)$ based on Eq 3. Subsequently, we obtain the accuracy of generated references for each LLM $Acc.$ by averaging $Acc_{ref.}(j_k)$ across journals, as shown in Eq 4. Finally, we defined the reference accuracy metric $Acc$ as Eq 4. A high accuracy metric indicates a low hallucination rate.\n$Acc_{ref.}(p_i) = \\frac{1}{N} \\sum_{n=0}^{N} True(r_n)$    (2)\n$Acc_{ref.}(j_k) = \\frac{1}{M}\\sum_{i=0}^{M} Acc_{ref.}{(p_i)}$    (3)\n$Acc. = \\frac{1}{K} \\sum_{k=0}^{K} Acc_{ref.}{(j_k)}$    (4)\nOn the other hand, intuitively, the title is the key element that reflects the generated reference's faithfulness. In Agrawal et al. (2024)'s work, they determined by ground-truth labels assigned using the Bing search API. Encouraged by this work, we also calculate the title search scores of each journal and each LLM, formulated as follows.\n$S_t = \\frac{1}{(NMK)}\\Sigma\\Sigma\\Sigma S_{p_i}$  (5)\n$S_{p_i} = \\sum_{r\\in R} \\begin{cases}\n1 & \\text{if the Term has return value from Semantic Scholar API} \\\\\n0 & \\text{otherwise}\n\\end{cases}$  (6)\nwhere s is the number of correct generated reference titles for $p_i$. We search the generated title from an external academic search engine to estimate whether the title can be matched.\nContext evaluation metrics. In this paper, we use the human-written article as the gold truth and then evaluate LLM-generated context from factual consistency and semantic coverage aspects. The resemblance of natural language inference (NLI) to factual consistency evaluation has led to utilizing NLI models for measuring factual consistency (Gao et al., 2023). Encouraged by previous works, we also use the NLI method to evaluate the factual consistency between LLMs generated and human-written text. For example, we calculate the NLI score Entaili between the original article abstract $a_i$ and the LLM-generated abstract $a'$ as follows."}, {"title": "Experiments", "content": "We present the results for the three tasks in Table 1, 2, and 3. The performance of different models on each task is analyzed as follows.\nResults for Task 1. By analyzing the results in Table 1, we observe that Claude-3.5 achieves the highest accuracy score and title search rate, whereas Llama-3.2 records the lowest performance on both metrics for Task 1. Additionally, when evaluating the accuracy of LLM-generated references, treating the author item as correct if the first author is identified correctly results in a 3\u20135% increase in accuracy scores across all models. This finding highlights that accurately generating complete author lists remains a significant challenge for LLMs.\nResults for Task 2. We use various metrics to evaluate model performance. As shown in Table 2, Claude-3.5 consistently achieves the best overall performance across nearly all metrics. The average semantic similarity between the abstracts generated by Claude-3.5 and human-written abstracts is 81.17%. Furthermore, Claude-3.5 demonstrates the highest factual consistency, with a TURE model evaluation score of 78.10% and a GPT-40 assessment score of 96.77%. Additionally, we observe that Llama-3.2 achieves the highest score on the ROUGE-L metric, but does not show a clear advantage on other metrics. Given the diversity of outputs produced by LLMs, it is crucial to employ multiple evaluation metrics when assessing their performance on our tasks.\nResults for Task 3. As shown in Table 3, we observe that, compared to the accuracy scores in Task 1, all LLMs exhibit a significant increase in accuracy when generating references in Task 3. Previous studies have shown that citing real external sources can reduce hallucination rates in the generated text. In our experiment, we find that when LLMs are tasked with generating references for the literature review, the accuracy of the generated references increases significantly. This suggests that the generated references and the literature review text impose constraints on each other."}, {"title": "Analyze LLM-Generated References from Different Dimensions", "content": "In both Task 1 and Task 3, we ask LLMs to generate references. The overall performance was discussed in the previous section. In this section, we provide a detailed comparison of the accuracy of LLM-generated references across various dimensions, as shown in Figure 2. As seen in Figure 2(a), Claude-3.5 demonstrates a clear advantage over other models across all dimensions in Task 1. Additionally, the accuracy of reference generation in Task 1 for Claude-3.5, GPT-40, and Qwen-2.5 shows a consistent trend across all dimensions, with the highest accuracy observed in the title dimension. The accuracy for journal name, page, and author is also relatively high. In contrast, Llama-3.2 shows higher accuracy in the page and author dimensions compared to other dimensions, but overall, Llama-3.2 does not exhibit a competitive advantage in reference generation accuracy.\nNext, we examine the accuracy of LLM-generated references across various dimensions in Task 3, as shown in Figure 2(b), and comparing it with Figure 2(a), we observe improvements across all dimensions for Claude-3.5, GPT-40, and Qwen-2.5. Notably, the accuracy of GPT-40 in certain dimensions approaches that of Claude-3.5. However, the performance of LLaMA-3.2 remains suboptimal."}, {"title": "Cross-Disciplinary Analysis", "content": "In this section, we compare the performance of LLMs across different disciplines. First, based on Dewey's Decimal Classification, we categorize 51 journals into five disciplines: Biology, Chemistry, Mathematics, Physics, Social Science, and Technology. After categorization, there are 460 articles in the Biology category, 90 in Chemistry, 50 in Mathematics, 113 in Physics, 299 in Social Science, and 94 in Technology.\nWe then present bar charts in Figures 3, 4, 5, which illustrate the performance of different models across various tasks and disciplines.\nFirst, we observe that in Task 1, as shown in Figure 3, almost all models exhibit the highest accuracy in the Mathematics discipline. Additionally, Claude-3.5 demonstrates relatively high accuracy in both Biology and Social science. GPT-40, besides having the highest accuracy in Mathematics, shows higher accuracy in Physics and Social science compared to Biology and Technology. Qwen-2.5 also exhibits a relatively high accuracy in Physics, but it has the lowest accuracy in Chemistry. Llama-3.2 has the highest accuracy in Mathematics, but its accuracy in Technology is the lowest. Finally, we calculate the results of the one-way ANOVA for each LLM across five disciplines to validate the differences. The results are as follows: Claude-3.5(f=7.65, p<.001), GPT-4o(f=11.87, p<.001), Qwen-2.5(f=12.39, p<.001), Llama-3.2(f=10.78, p<.001). These results indicate significant differences in performance across disciplines for each model.\nSecondly, as shown in Figure 4, we present the NLI scores of each model across different disciplines as evaluated by TRUE in Task 2. We observe that all models exhibit the lowest entailment scores in Social science and the highest scores in Biology and Technology. In terms of this metric, GPT-40 outperforms Claude-3.5 in Mathematics and Technology, indicating that while GPT-40 has a lower accuracy in reference generation, the difference between the two models in the abstract generation task is relatively small. Additionally, we conduct an ANOVA test to evaluate the models' performance across different disciplines. The results are as follows: Llama-3.2 (f= 11.06, p<.001) Qwen2.5 (f= 16.88, p<.001) GPT-4o(f= 18.88, p<.001) Claude-3.5(f= 23.72, p<.001). These results indicate that, in Task 2, there are significant differences in the performance of each model across different disciplines.\nFinally, we examine the accuracy of each model across five disciplines in Task 3, as illustrated in Figure 4. It is evident that the accuracy of Claude-3.5 and GPT-40 are significantly higher than those of Qwen-2.5 and LLaMA-3.2 across all disciplines. Furthermore, the trend aligns with that observed in Task 1: almost all models exhibit the highest accuracy in Mathematics, followed by relatively high scores in Social Science, while lower accuracy scores are observed in Chemistry and Technology. The results of the one-way ANOVA for each models across five disciplines are as follows: Claude-3.5(f=33.33, p<.001), GPT-4o(f=39.09, p<.001), Qwen-2.5(f=32.84, p<.001), Llama-3.2(f=39.02, p<.001), indicating significant differences in each models' performance across the various disciplines."}, {"title": "Comparison of LLM-Cited and Human-cited References", "content": "In this section, we also compare the LLM-generated references with those from the human-written original articles. Specifically, we examine the overlap between the references generated by each model and those that are cited in the human-written article. As shown in Table 4, we observe that in Task 1, 25.21% of the references generated by Claude-3.5 are also cited in the human-written article, indicating a relatively high overlap rate. Furthermore, we find that, compared to Task 1, the reference overlap rate with human citations increases across all models in Task 3. Notably, GPT-40 models show a 10% increase in the overlap rate. Next, we provide a more detailed comparison of the references across various dimensions. As shown in Figure 6, for Task 1, we observe that the overlap rate is higher in the \u201cTitle\u201d and other numerical dimensions, while the overlap rates for the \"Journal\" and \"Author\" dimensions are relatively lower. For Task 3, Claude-3.5 and GPT-40 exhibit a higher overlap rate on the \u201cAuthor\u201d dimension compared to Task 1. The possible reason is that, in the generated text, the LLMs tend to cite the first author's name, which may lead the models to place more emphasis on this dimension."}, {"title": "Conclusion", "content": "In this paper, we present a framework to assess the literature review writing abilities of LLMs. This framework includes three tasks designed to evaluate LLMs' literature review writing capabilities. The generated outputs are then evaluated from multiple dimensions using various tools, such as Semantic Scholar and NLI models, focusing on aspects like hallucination rate, semantic coverage, and factual consistency compared to human-written texts. Finally, we analyze the performance of LLMs in writing literature reviews from the perspective of different academic disciplines.\nWe select four LLMs for task evaluation and find that Claude-3.5-Sonnet outperforms GPT-40, Qwen-2.5-72B, and Llama-3.2-3B across all three tasks, particularly excelling in the task of generating accurate references. This advantage is likely influenced by the training data of each model. Additionally, we observed that each model has different strengths across disciplines. Overall, for the reference generation task, nearly all models perform better in Mathematics, while their performance is weaker in Chemistry and Technology. However, when writing abstracts, all models exhibit the lowest factual consistency in Social Science, as indicated by the entailment scores, compared to human-written texts.\nWhen comparing the references generated by the models in Task 1 and Task 3, we find that in Task 3, nearly all models generate more accurate references. This suggests that LLMs cite references during the writing process, which improves the authenticity of the references. Moreover, the inclusion of the first author's name in the generated context also enhances the accuracy of the author dimension.\nIn the future, we will deploy this evaluation framework on Hugging Face to enable real-time assessments of newly released LLMs."}, {"title": "Limitations", "content": "In this paper, we evaluate the ability of LLMs to write literature reviews. However, instead of assessing from conventional perspectives, such as the fluency of the generated context or topic coverage, we compare the model-generated results with human-written ones. Additionally, when processing the LLM-generated results, we often need to handle abbreviations of author names and journal titles. While we have made efforts to address these details thoroughly, some discrepancies may still exist."}]}