{"title": "TRAINING ON MORE REACHABLE TASKS FOR\nGENERALISATION IN REINFORCEMENT LEARNING", "authors": ["Max Weltevrede", "Matthijs T. J. Spaan", "Caroline Horsch", "Wendelin B\u00f6hmer"], "abstract": "In multi-task reinforcement learning, agents train on a fixed set of tasks and have\nto generalise to new ones. Recent work has shown that increased exploration im-\nproves this generalisation, but it remains unclear why exactly that is. In this paper,\nwe introduce the concept of reachability in multi-task reinforcement learning and\nshow that an initial exploration phase increases the number of reachable tasks the\nagent is trained on. This, and not the increased exploration, is responsible for the\nimproved generalisation, even to unreachable tasks. Inspired by this, we propose a\nnovel method Explore-Go that implements such an exploration phase at the begin-\nning of each episode. Explore-Go only modifies the way experience is collected\nand can be used with most existing on-policy or off-policy reinforcement learn-\ning algorithms. We demonstrate the effectiveness of our method when combined\nwith some popular algorithms and show an increase in generalisation performance\nacross several environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite major advances in reinforcement learning (RL), it is fairly rare to encounter RL outside\nof the academic setting. One of the remaining challenges of adopting it in the real world is the\nability of an agent to generalise to novel scenarios, that is, those not encountered during training.\nFor example, we do not want a house-cleaning robot to stop working when the owner moves their\ncouch. This is the main research question investigated in the zero-shot policy transfer setting (ZSPT,\nKirk et al., 2023). Here the agent trains on several variations of an environment, known as tasks,\nand must generalise to new ones. This differs from the commonly studied single-task RL setting, in\nwhich the agent trains and tests on the same environment instance.\nThere exists a surprising interaction between ZSPT generalisation and exploration of the training\nenvironments. A single-task RL agent must trade off between exploring for better futures and ex-\nploiting what it already knows. Once a good enough policy is found, a single-task agent ceases\nexploration to focus on collecting rewards. In multi-task RL, however, Jiang et al. (2023) have re-\ncently demonstrated that more effective exploration, that never stops throughout the entire training\nprocess, improves generalisation to unseen tasks.\nHowever, it is not yet entirely clear in which tasks we can expect exploration to improve generalisa-\ntion, nor is it clear when to use it to benefit generalisation the most.\u00b9 For example, exploration might\nhelp a cleaning robot to know what to do when it is activated at an unusual location in the house.\nHaving seen more of the environment means the robot will be familiar with that area. However, if"}, {"title": "2 BACKGROUND", "content": "A Markov decision process (MDP) M is defined by a 6-tuple M = {S, A, R, T, po, y}. In this\ndefinition, S denotes a set of states called the state space, A a set of actions called the action space,\nR:S\u00d7AR the reward function, T : S \u00d7 A \u2192 P(S) the transition function where P(S)\ndenotes the set of probability distributions over states S, po : P(S) the starting state distribution\nand \u03b3\u2208 [0, 1) a discount factor. The goal is to find a policy \u03c0 : S \u2192 P(A) that maps states to\nprobability distributions over actions in such a way that maximises the expected cumulative dis-\ncounted reward \u0395\u03c0[\u2211t=0 Ytrt], also called the return. The expectation E\u201e is over the Markov chain\n{so, ao, ro, 81, A1, r1...} induced by policy \u03c0 when acting in MDP M (Akshay et al., 2013). An\noptimal policy \u03c0* achieves the highest possible return. The on-policy distribution p\u2122 : P(S) of the\nMarkov chain induced by policy \u03c0 in MDP M defines the proportion of time spent in each state as\nthe number of episodes in M goes to infinity (Sutton & Barto, 2018)."}, {"title": "2.1 CONTEXTUAL MARKOV DECISION PROCESS", "content": "A contextual MDP (CMDP, Hallak et al., 2015) is a specific type of MDP where the state space\nS = S' \u00d7 C can in principle be factored into an underlying state space S' and a context space C,\nwhich affects rewards and transitions of the MDP. For a state s = (s', c) \u2208 S, the context c behaves\ndifferently than the underlying state s' in that it is sampled at the start of an episode (as part of the\ndistribution po) and remains fixed until the episode ends. The context c can be thought of as the task\nan agent has to solve and from here on out we will refer to the context as the task.\nThe zero-shot policy transfer (ZSPT, Kirk et al., 2023) setting for CMDPs Mic is defined by a\ndistribution over task space P(C) and a set of tasks Ctrain and Ctest sampled from the same dis-\ntribution P(C). The goal of the agent is to maximise performance in the testing CMDP M|Ctest,\ndefined by the CMDP induced by the testing tasks Ctest, but the agent is only allowed to train in the\ntraining CMDP M|Ctrain. The learned policy is expected to perform zero-shot generalisation for\nthe testing tasks, without any fine-tuning or adaptation period."}, {"title": "3 THE INFLUENCE OF REACHABILITY ON GENERALISATION", "content": "In general, the task c can influence several aspects of the underlying MDP, like the reward function\nor dynamics of the environment. As a result, several existing fields of study like multi-goal RL (task\ninfluences reward) or sim-to-real transfer (task influences dynamics and/or visual observations) can\nbe framed as special instances of the CMDP framework. To analyse which tasks can generalise to\neach other, we assume the full state is observed in a representation s = $(s', c), such that two tasks\nthat behave the same\u00b2 are represented the same. This means tasks e only differ in the distribution of\ntheir starting states so ~ po(c). Many interesting problems are represented in this fashion, including\nseveral environments from the popular Procgen, DeepMind Control Suite and Minigrid benchmarks\n(Cobbe et al., 2020; Tassa et al., 2018; Chevalier-Boisvert et al., 2023).\nIn this setting, the agent starts a task in a different state but may still share states st with other tasks\nlater in the episode. For example, if tasks have different starting positions but share the same goal,\nor if the agent can manipulate the environment to resemble a different task. This is not necessarily\nalways true, though. An example of this is shown in Figure la: even if the agent in Task 1 moves\nto the starting location in Task 2, the background colour will always be different. In this setting, we\ncan refer to tasks c\u2208 C and states s \u2208 S interchangeably, since we can think of any s as a starting\nstate and therefore as a task. From now on, we will refer to a set of tasks C as a set of starting states\nSo."}, {"title": "3.1 REACHABILITY IN MULTI-TASK RL", "content": "To argue how exploration can benefit generalisation we introduce the reachability of tasks. To do\nso, we first define the reachability of states in a CMDP M|strain. The set of reachable states\nSr(M|strain) (abbreviated with S, from now on) consists of all states sr for which there exists a\nsequence of actions that give a non-zero probability of ending up in 8, when performed in M|strain .\nPut differently, a state s is reachable if there exists a policy whose probability of encountering that\nstate during training is non-zero. In complement to reachable states, we define unreachable states\nSu as states that are not reachable.\nUsing these definitions, we define (un)reachable tasks as tasks that start in a(n) (un)reachable state.\nWe define two instances of the ZSPT problem as follows:\nDefinition 1 (Reachable/Unreachable generalisation). Reachable/Unreachable generalisation\nrefers to an instance of the ZSPT problem where the start states of the testing environments Stest\nare/are-not part of the set of reachable states during training, i.e. Stest Sr or Stest \u2229 S = 0.\nThis definition has some interesting implications: due to how reachability is defined, in the reach-\nable generalisation setting all states encountered in the testing CMDP M|stest are also reachable.\nNote that the reverse does not have to be true: not all reachable states can necessarily be encoun-\ntered in M|stest. Furthermore, we assume in the unreachable generalisation setting that all states\nencountered in Mistest are also unreachable. Note that this is still considered in-distribution gener-\nalisation since the starting states for both train and test tasks are sampled from the same distribution."}, {"title": "3.2 GENERALISATION TO REACHABLE TASKS", "content": "In the single-task setting, the goal is to maximise performance in the MDP M in which the agent\ntrains. There, it is sufficient to learn an optimal policy in all the states s\u2208 S encountered by\nthis policy in M. This is because acting optimally in all the states encountered by the optimal\npolicy in M guarantees maximal return in M. Exploration thus only has to facilitate learning the\noptimal policy on the on-policy distribution p\u2122\u2122 of M. In fact, once the optimal policy has been\nfound, learning to be optimal anywhere else in M would be a wasted effort that potentially allocates\napproximation power to unimportant areas of the state space."}, {"title": "3.3 GENERALISATION TO UNREACHABLE TASKS", "content": "For unreachable generalisation, the states encountered in pr* of M|stest are not part of the reachable\nspace Sr of M strain, so it is not obvious on which parts of Sr our agent should train."}, {"title": "4 EXPLORE-GO: TRAINING ON MORE REACHABLE TASKS", "content": "As argued in the previous section, training on more reachable tasks is more desirable for generali-\nsation than extended exploration. We propose a novel method Explore-Go\u2074 which effectively trains"}, {"title": "5 EXPERIMENTS", "content": "We perform an empirical evaluation of Explore-Go on some environments from two benchmarks:\nan adaptation of Four Rooms from Minigrid (Chevalier-Boisvert et al., 2023) and Finger Turn and\nReacher from the DeepMind Control Suite (DMC, Tassa et al., 2018). These environments can all be\nexplored sufficiently with e-greedy exploration and therefore for the pure exploration policy we sim-\nply sample uniformly from the action space (equivalent to setting \u20ac = 1). Due to its discrete nature\nand smaller size, we use the Four Rooms environment to demonstrate the versatility of Explore-Go.\nThis also allows us to enumerate all possible states and tasks and formulate optimal policies and\nvalues, which we can use to further analyse our method. We evaluate Explore-Go when combined\nwith several on-policy, off-policy, value-based and/or policy-based RL algorithms: PPO (on-policy,\npolicy-based), DQN (off-policy, value-based) and soft actor-critic (SAC, off-policy, policy-based,\nHaarnoja et al., 2018)."}, {"title": "5.1 EXPLORE-GO WITH VARIOUS ALGORITHMS", "content": "We use the Four Rooms environment from Minigrid, modified to have a reduced action space,\nsmaller size, and to be fully observable (see Appendix C.2 for more details). The environment\nconsists of a grid-world of four rooms with single-width doorways connecting all of the rooms. The\nagent starts in one of the rooms and must move to the goal location, which may be in a different\nroom. Tasks differ from each other in the starting location and orientation of the agent, the goal lo-\ncation, and the position of the doorways connecting the four rooms. In our experiments, the agents\ntrain on 40 different training tasks and are evaluated on either 120 reachable tasks or 120 unreach-\nable tasks. In this environment, a task is reachable if and only if both the positions of the doorways\nand the goal location are the same as at least one task in the training set. In Figure 2 we see that\nExplore-Go improves the testing performance on unreachable tasks when combined with PPO, DQN\nand SAC, whilst leaving the training performance mostly unaffected. The Explore-Go agent has a"}, {"title": "5.2 REACHABLE STATES VS REACHABLE TASKS", "content": "Our method Explore-Go aims to create additional reachable tasks on which the agent trains. We\nargue that this, and not simply more continued exploration, will improve generalisation. To inves-\ntigate this, we compare Explore-Go with an exploration approach that is similar to what is used in\nJiang et al. (2023). One of their core algorithmic components is the temporally equalised explo-\nration (TEE) which assigns different fixed exploration coefficients to the parallel workers collecting\nrollouts. This is necessary because, due to function approximation, the model may lose knowledge\nacquired through exploration if it does not keep exploring throughout training.\nIn the following experiment, we analyse the DQN agent from the previous section, which collects\nrollouts with 10 parallel workers. For the TEE agent, we assign each of the workers a different,\nfixed value of \u20ac (used in e-greedy exploration). We assign e according to the relation \u20ac\u2081 = (N-1),,\nwhere e is the exploration coefficient for worker i, N is the total number of workers (N = 10 in our\ncase) and a is a coefficient determining a bias towards more exploration (a < 1) or less exploration\n(a > 1).\nWe compare Explore-Go with a baseline DQN agent using TEE with coefficient a = 0.1. This was\ndecided by evaluating multiple coefficients a and finding that DQN-TEE with coefficient a = 0.1\ndoes the most exploration, and thus acts as an upper bound on the performance achievable with\nthis approach. (see Appendix D.2 for more results with different values of a). Figure 3 shows that\nExplore-Go achieves significantly higher testing performance for both the reachable and unreachable\ntest sets, whilst training performance is largely similar.\nIn Figure 4 we show that despite discovering a larger fraction of the state-action space (Figure\n4a), maintaining higher diversity in the replay buffer (Figures 4b and 4c), and learning the optimal\naction on a larger fraction of the reachable state space (Figure 4d), TEE generalises worse than\nExplore-Go (as seen in Figure 3). We refer to Appendix C.2 for more details on how these metrics\nare calculated. This suggests that generalisation is not about how much you explore or how many\nof the reachable states you are optimal in, but rather when you explore and how many reachable\ntasks you can solve optimally. Our method Explore-Go leverages exploration at the start of every\nepisode to explicitly increase the number of tasks the agent trains on, resulting in consistently higher\ngeneralisation performance."}, {"title": "5.3 SCALING UP TO DEEPMIND CONTROL SUITE", "content": "To further demonstrate the scalability and generality of our approach we evaluate Explore-Go on\nsome of the continuous control environments from the DeepMind Control Suite. In the DMC en-\nvironments, at the start of every episode, the initial configuration of the robot body (and in some\nenvironments, target location) is randomly generated based on some initial seed. Typically, the\nDMC benchmark is not used for the ZSPT setting and training is done on the full distribution of\ntasks (initial configurations). To turn the DMC benchmark into an instance of the ZSPT problem,\nwe define a limited set of seeds (and therefore initial configurations) on which the agents are allowed\nto train. We then test on the full distribution. Note that only some of the environments test for un-\nreachable generalisation: Reacher, Finger Turn, Manipulator, Stacker, Fish and Swimmer. For the\nother environments, all tasks are reachable from one another. For more details on these experiments,\nwe refer to Appendix C.3.\nIn Figure 5 we show the training and testing performance of SAC and Explore-Go on Finger Turn\nand Reacher. The Explore-Go agent has a maximum of K = 200 pure exploration steps at the start\nof every episode. In the figure, we see it achieves higher test performance whilst leaving training\nperformance largely unaffected. In Appendix D.3 we also show the results for the Cheetah Run and\nWalker Walk environments. However, there appears to be no significant generalisation gap between"}, {"title": "6 RELATED WORK", "content": "The contextual MDP framework is a very general framework that encompasses many fields in RL\nthat study zero-shot generalisation. Some approaches in this field try to improve generalisation by\nincreasing the variability of the training tasks through domain randomisation (Tobin et al., 2017;\nSadeghi & Levine, 2017) or data augmentation (Raileanu et al., 2021; Lee et al., 2020). Others try\nto explicitly bridge the gap between the training and testing tasks through inductive biases (Kansky\net al., 2017; Wang et al., 2021) or regularisation (Cobbe et al., 2019; Tishby & Zaslavsky, 2015).\nWe mention only a small selection of approaches here, for a more comprehensive overview we refer"}, {"title": "7 CONCLUSION", "content": "Recent work shows that more thorough and prolonged exploration can improve generalisation to\nunseen tasks in multi-task RL. This effect was explained as a result of encountering the same states\nin testing as were seen during the additional exploration in training. To understand this phenomenon\nbetter, we define the notion of reachability of states and tasks. This novel perspective makes it\nclear the above explanation only applies to reachable tasks, whereas unreachable tasks only benefit\nindirectly from the data augmentation that comes with training on more reachable tasks. It also\nimplies that continuous exploration (as in TEE) is not optimal for multi-task generalisation, as the\nexploratory episodes find more reachable states, but do not learn the task starting from there.\nInstead, we define the novel method Explore-Go, which begins each episode with a pure exploration\nphase, before standard learning is resumed. This results in training on more reachable tasks, and thus\nimproves generalisation even to unreachable tasks by data augmentation. We show this empirically\nin the Four Rooms environment: here TEE explores more states, keeps a more diverse replay buffer,\nand learns a policy that is optimal in more reachable states than Explore-Go. However, Explore-Go\ngeneralises better to both reachable and unreachable test tasks. This suggests that generalisation is\nnot about how much you explore or how many of the reachable states you are optimal in, but rather\nwhen you explore and how many reachable tasks you can solve optimally.\nAs an added benefit, Explore-Go only requires a simple modification to the sampling procedure,\nwhich can be applied easily to most RL algorithms, both on-policy and off-policy. We demonstrate\nthat the method increases multi-task generalisation in the Four Rooms environment with SAC, DQN\nand PPO. We also show that Explore-Go scales up to more complex tasks from the DeepMind Con-\ntrol Suite, both on the underlying state and on images of the task. We hope to provide practitioners\nwith a simple modification that can improve the generalisation of their agents significantly."}, {"title": "A RELATED WORK", "content": "A.1 EXTENDED RELATED WORK\nA.1.1 GENERALISATION IN CMDPS\nThe contextual MDP framework is a very general framework that encompasses many fields in\nRL that study zero-shot generalisation. For example, the sim-to-real setting often encountered in\nrobotics is a special case of the ZSPT setting for CMDPs (Kirk et al., 2023). An approach used\nto improve generalisation in the sim-to-real setting is domain randomisation (Tobin et al., 2017;\nSadeghi & Levine, 2017; Peng et al., 2018), where the task distribution during training is explic-\nitly increased in order to increase the probability of encompassing the testing tasks in the training\ndistribution. This differs from our work in that we don't explicitly generate more (unreachable)\ntasks. However, our work could be viewed as implicitly generating more reachable tasks through\nincreased exploration. Another approach that increases the task distribution is data augmentation\n(Raileanu et al., 2021; Lee et al., 2020; Zhou et al., 2021). These approaches work by applying a\nset of given transformations to the states with the prior knowledge that these transformations leave\nthe output (policy or value function) invariant. In this paper, we argue that our approach implicitly\ninduces a form of invariant data augmentation on the states. However, this differs from the other\nwork cited here in that we don't explicitly apply transformations to our states, nor do we require\nprior knowledge on which transformations leave the policy invariant.\nSo far we have mentioned some approaches that increase the number and variability of the training\ntasks. Other approaches instead try to explicitly bridge the gap between the training and testing tasks.\nFor example, some use inductive biases to encourage learning generalisable functions (Zambaldi\net al., 2018; 2019; Kansky et al., 2017; Wang et al., 2021; Tang et al., 2020; Tang & Ha, 2021).\nOthers use regularisation techniques from supervised learning to boost generalisation performance\n(Cobbe et al., 2019; Tishby & Zaslavsky, 2015; Igl et al., 2019; Lu et al., 2020; Eysenbach et al.,\n2021). We mention only a selection of approaches here, for a more comprehensive overview we\nrefer to the survey by Kirk et al. (2023).\nAll the approaches above use techniques that are not necessarily specific to RL (representation learn-\ning, regularisation, etc.). In this work, we instead explore how exploration in RL can be used to\nimprove generalisation."}, {"title": "A.1.2 EXPLORATION IN CMDPS", "content": "There have been numerous methods of exploration designed specifically for or that have shown\npromising performance on CMDPs. Some approaches train additional adversarial agents to help\nwith exploration (Flet-Berliac et al., 2021; Campero et al., 2021; Fickinger et al., 2021). Others try\nto exploit actions that significantly impact the environment (Seurin et al., 2021; Parisi et al., 2021)\nor that cause a significant change in some metric (Raileanu & Rockt\u00e4schel, 2020; Zhang et al.,\n2021c;b; Ramesh et al., 2022). More recently, some approaches have been developed that try to\ngeneralise episodic state visitation counts to continuous spaces (Jo et al., 2022; Henaff et al., 2022)\nand several studies have shown the importance of this for exploration in CMDPs (Wang et al., 2023;\nHenaff et al., 2023). All these methods focus on trading off exploration and exploitation to achieve\nmaximal performance in the training tasks as fast and efficiently as possible. However, in this paper,\nwe examine the exploration-exploitation trade-off to maximise generalisation performance in testing\ntasks.\nIn Zisselman et al. (2023), the authors leverage exploration at test time to move the agent towards\nstates where it can confidently solve the task, thereby increasing test time performance. Our work\ndiffers in that we leverage exploration during training time to increase the number of states from\nwhich the agent can confidently solve the test tasks. Closest to our work is Jiang et al. (2023),\nZhu et al. (2020) and Suau et al. (2024). Jiang et al. (2023) don't make a distinction between\nreachable and unreachable generalisation and provide intuition which we argue mainly applies to\nreachable generalisation (see Appendix A.2). Moreover, their novel approach only works for off-\npolicy algorithms, whereas ours could be applied to both off-policy and on-policy methods. In\nZhu et al. (2020), the authors learn a reset controller that increases the diversity of the agent's start\nstates. However, they only argue (and empirically show) that this benefits reachable generalisation.\nThe concurrent work in Suau et al. (2024) introduces the notion of policy confounding in out-of-"}, {"title": "A.2 DISCUSSION ON RELATED WORK", "content": "Jiang et al. (2023) argue that generalisation in RL extends beyond representation learning. They do\nso with an example in a tabular grid-world environment. In the environment they describe the agent\nduring training always starts in the top left corner of the grid, and the goal is always in the top right\ncorner. During testing the agent starts in a different position in the grid-world (in their example, the\nlower left corner). This is according to our definition an example of a reachable task. They then\nargue (in the way we described in Section 3.2) that more exploration can improve generalisation to\nthese tasks.\nThey extend their intuition to non-tabular CMDPs by arguing that in certain cases two states that\nare unreachable from each other, can nonetheless inside a neural network map to similar represen-\ntations. As a result, even though a state in the input space is unreachable, it can be mapped to\nsomething reachable in the latent representational space and therefore the reachable generalisation\narguments apply again. For this reason, the generalisation benefits from more exploration can go\nbeyond representation learning.\nRelating it to the illustrative example we provide in Figure 1, we argue this intuition considers the\ngeneralisation benefits one might obtain from learning to act optimally in more abstracted states.\nFor example, in Jiang et al. (2023)'s grid-world the lower states would have normally unseen values,\nwhich is represented by increasing the number of columns on which we train in Figure 1c and 1d.\nHowever, in Section 3.2 we argue that specifically unreachable generalisation can benefit as well\nfrom training on more states belonging to the same abstracted states (represented by increasing the\nnumber of rows on which we train in Figure 1c and 1d). Training on more of these states could\nencourage the agent to learn representations that map different unreachable states to the same latent\nrepresentation (or equivalently, abstracted states). As such, we argue the generalisation benefits from\nmore exploration can in part be attributed to an implicit form of representation learning."}, {"title": "B GENERALISATION TO REACHABLE TASKS", "content": "In this section, we elaborate on why a policy that is optimal in all reachable states, is guaranteed to\nperform well when testing on reachable tasks. As a first step, we point out a corollary of definition\n1 about reachable states:\nCorollary 0.1. Any state s' that is reachable from a state s \u2208 Sr(M|strain) in the reachable set,\nhas to be itself in the reachable set: s' \u2208 Sr(M|strain).\nWhy this is the case is clear to see with the definition of reachability in terms of sequences of actions:\nconcatenate the sequence of actions with a non-zero probability of ending up in s with the sequence\nof actions with a non-zero probability of ending up in s' when starting from s. This will result in a\nsequence of actions with a non-zero probability of ending up in s'. In short, this corollary states that\nyou cannot leave the reachable set S, (M|strain) through interaction with the environment.\nFrom this logically follows the following corollary:\nCorollary 0.2. An optimal policy \u3160 that achieves maximal return from any state in the reachable\nstate space Sr(M|strain), will have optimal performance in the reachable generalisation setting.\nRecall that performance in a ZSPT problem is defined as the performance in the testing MDP\nMstest, which in the case of reachable generalisation, has a state space that consists only of reach-\nable states (due to Corollary 0.1). It follows naturally that a policy that is optimal on the entire\nreachable state space Sr(M|strain) also has to be optimal in Mistest."}, {"title": "C EXPERIMENTAL DETAILS", "content": "C.1 ILLUSTRATIVE CMDP\nTraining is done on the four tasks in Figure la and unreachable generalisation is evaluated on new\ntasks with a completely different background colour. For pure exploration, we sample uniformly\nrandom actions at each timestep (e-greedy with \u20ac = 1). We compare Explore-Go to a baseline using\nregular PPO. In Figure 1b we can see that the PPO baseline achieves approximately optimal train-\ning performance but is not consistently able to generalise to the unreachable tasks with a different\nbackground colour. PPO trains mostly on on-policy data, so when the policy converges to the op-\ntimal policy on the training tasks it trains almost exclusively on the on-policy states in Figure 1c.\nAs we hypothesise, this likely causes the agent to overfit to the background colour, which will hurt\nits generalisation capabilities to unreachable states with an unseen background colour. On the other\nhand, Explore-Go maintains state diversity by performing pure exploration steps at the start of every\nepisode. As such, the state distribution on which it trains resembles the distribution from Figure\n1d. As we can see in Figure 1b, Explore-Go learns slower, but in the end achieves similar training\nperformance to PPO and performs significantly better in the unreachable test tasks. We speculate\nthis is due to the increased diversity of the state tasks on which it trains.\nENVIRONMENT DETAILS\nThe training tasks for the illustrative CMDP are the ones depicted in Figure la. The unreachable\ntesting tasks consist of 4 tasks with the same starting positions as found in the training tasks (the end-\npoint of the arms) but with a white background colour. The states the agent observes are structured\nas RGB images with shape (3,5,5). The entire 5 \u00d7 5 grid is encoded with the background colour\nof the particular task, except for the goal position (at (2, 2)) which is dark green ((0,0.5,0) in RGB)\nand the agent (wherever it is located at that time) which is dark red ((0.5,0,0) in RGB). The specific\nbackground colours are the following:"}, {"title": "C.2 FOUR ROOMS", "content": "In all of our Four Rooms experiments, we will train on 40 different training tasks and test on either\na reachable or unreachable task set of size 120. The 40 training tasks differ in the agent location,\nagent direction, goal location and the location of the doorways (see Figure 7 for some example tasks\nin Four Rooms).\nIn this environment, reachability is regulated through variations in the goal location and location of\nthe doorways. If two states share their doorways and goal location, then they are both reachable\nfrom one another. Conversely, if two states differ in either the doorways or goal location, they are\nunreachable. The reachable task set is constructed by taking every training task and changing only\nthe agent location and agent direction (keeping the location of the doorways and goal location the\nsame). This is repeated four times to generate a total number of reachable tasks of 4 \u00d7 40 = 120. For\nthe unreachable task set, we take 40 different configurations of the doorways that all differ from the\nones in the training task. For each of those 40 different doorway configurations, we generate four\nnew goal locations, agent locations and agent directions. This also generates a total of 4 \u00d7 40 = 120\nunreachable tasks.\nENVIRONMENT DETAILS\nThe Four Rooms grid world used in our experiments is adapted from the Minigrid benchmark\n(Chevalier-Boisvert et al., 2023) and differs in certain ways from the default Minigrid configura-\ntion. For one, the action space is reduced from the default seven actions (turn left, turn right, move\nforward, pick up an object, drop an object, toggle/activate an object, end episode) to just the first\nthree actions (turn left, turn right, move forward). Also, the reward function is changed slightly to\nreward 1 for successfully reaching the goal and 0 otherwise (as opposed to the 1 \u2013 0.9 * (step count)\ngiven upon success by the default Minigrid environment). Additionally, the size of the environment\nis reduced from the default 19 (8 \u00d7 8 rooms) to 9 (3 \u00d7 3 rooms).\nmax steps\nFurthermore, the observation space is made fully observable and customised. Our agent receives a\n4\u00d79\u00d79 tensor that is centred around the agent's current location. The four binary-encoded channels\ncontain the following information:"}, {"title": "C.3 DEEPMIND CONTROL SUITE", "content": "For the DeepMind Control Suite we adapt the environment so that at the start of each episode the\ninitial configuration of the robot body and target location are drawn based on a given list of random\nseeds. This allows us to control the task space of the environment so that we can define a limited\nset of tasks on which the agent is allowed to train. To compute mean performance and confidence\nintervals we average all our DMC experiments over 10 seeds for the agent. Each agent seed trains\non its own set of training tasks. For a training set of size N, agent i gets to train on tasks generated\nwith seeds {i * N, i * N + 1, ..., i * N + N \u2212 1}. Testing is always done on 100 episodes from\nthe full distribution. For the state-based experiments we train on N = 5 training tasks and for the\nimage-based experiments, we train on N = 30 training tasks.\nThe standard DMC benchmark has no terminal states and instead has a fixed episode length of 1000\nafter which the agent times out. However, for the Finger Turn and Reacher environments, an episode"}, {"title": "D ADDITIONAL EXPERIMENTS", "content": "D.1 ADDING PURE EXPLORATION EXPERIENCE TO THE BUFFER\nIn Figure 8 we show an ablation of Explore-Go where we also add all the pure exploration experience\nto the replay buffer (Explore-Go with PE, green). It shows that adding this experience to the buffer\nmakes the performance of Explore-Go worse. This could be due to the highly off-policy nature of\nthe pure exploration data.\nD.2 TEE WITH DIFFERENT COEFFICIENTS a"}]}