{"title": "ENHANCING FREE-HAND 3D PHOTOACOUSTIC AND ULTRASOUND RECONSTRUCTION USING DEEP LEARNING", "authors": ["SiYeoul Lee", "SeonHo Kim", "Minkyung Seo", "SeongKyu Park", "Salehin Imrus", "Kambaluru Ashok", "DongEon Lee", "Chunsu Park", "SeonYeong Lee", "Jiye Kim", "Jae-Heung Yoo", "MinWoo Kim"], "abstract": "This study introduces a motion-based learning network with a global-local self-attention module (MoGLo-Net) to enhance 3D reconstruction in handheld photoacoustic and ultrasound (PAUS) imaging. Standard PAUS imaging is often limited by a narrow field of view and the inability to effectively visualize complex 3D structures. The 3D freehand technique, which aligns sequential 2D images for 3D reconstruction, faces significant challenges in accurate motion estimation without relying on external positional sensors. MoGLo-Net addresses these limitations through an innovative adaptation of the self-attention mechanism, which effectively exploits the critical regions, such as fully-developed speckle area or high-echogenic tissue area within successive ultrasound images to accurately estimate motion parameters. This facilitates the extraction of intricate features from individual frames. Additionally, we designed a patch-wise correlation operation to generate a correlation volume that is highly correlated with the scanning motion. A custom loss function was also developed to ensure robust learning with minimized bias, leveraging the characteristics of the motion parameters. Experimental evaluations demonstrated that MoGLo-Net surpasses current state-of-the-art methods in both quantitative and qualitative performance metrics. Furthermore, we expanded the application of 3D reconstruction technology beyond simple B-mode ultrasound volumes to incorporate Doppler ultrasound and photoacoustic imaging, enabling 3D visualization of vasculature. The source code for this study is publicly available at: https://github.com/guhong3648/US3D", "sections": [{"title": "INTRODUCTION", "content": "Diagnostic ultrasound (US) is a widely employed modality for examining various organs and tissues, owing to its real-time imaging capabilites. Particularly, it serves as a preferred tool for guiding medical procedures such as biopsies and injections [1]. In addition, US plays a pivotal role in dynamically monitoring the vascular system through the Doppler effect [2]. Meanwhile, the integration of photoacoustic (PA) imaging with conventional US imaging (PAUS imaging) has been extensively studied because of its promising clinical potential. This integration aims to enhance the existing benefits of ultrasound while introducing novel capabilities for both interventional and functional imaging [3].\nIn the PAUS system, a handheld transducer is responsible for emitting US or/and laser pulses and receiving resulting acoustic wave signals. While this configuration allows users the flexibility to manually scan a region of interest, it is accompanied by certain limitations. Notably, the field of view is restricted, providing only a narrow 2D cross-section of the image area, which hampers the understanding of the topological 3D structures of the target. Although some transducers are specifically designed for 3D imaging, their physical dimensions often render them impractical for handheld use [4].\nAn alternative approach is the 3D freehand method, which involves the sequential alignment of 2D image sections acquired from freehand sweeps with a standard transducer. Users can playback a series of 2D US image frames as a video through a CINE loop, but the visualization of the 3D rendering volume or structure of the target through accumulating frames is more straightforward. This method proves ideal in clinical practice when tissue motion is static. However, a primary technical challenge of this approach lies in estimating sweeping motion for correct 3D reconstruction. Although an external positional device can be attached to the transducer, it results in making the transducer bulkier and often provides inaccurate measurements in a clinical environment due to various optical or electrical disturbances [5].\nSeveral studies have explored the estimation of scan trajectories directly from 2D US images, eliminating the need for external sensors [6, 7, 8]. The key idea is to exploit and track tissue speckle patterns in US B-mode images. Recent approaches have employed deep learning (DL) frameworks to better leverage speckle features for predicting scan motion, typically defined by six parameters. Since the pioneering work of Prevost et al. [9], which introduced the first DL model for this task, subsequent studies have proposed advanced models [10, 11, 12, 13, 14] to improve stability and accuracy. However, their overall estimation accuracies were still insufficient for clinical practice, and they were rarely validated for long elevational scan trajectories, which are common in clinical practice."}, {"title": "RELATED WORKS", "content": "Traditional approaches exploited tissue speckle patterns in US B-mode images, as speckle content between successive frames tends to be preserved, even in out-of-plane motions. Since the pioneering work by Trahey et al. [16], several studies have focused on estimating scan motion by analyzing the correlation between adjacent frames [6, 7]. Gee et al. [8] proposed an adaptive speckle decorrelation technique, leading to improved performance. These studies have demonstrated the feasibility of sensorless free-hand US through gradual improvements in various factors such as estimation accuracy, generalizability, and less constrained scan protocols.\nWith the rapid advancements in DL, Prevost et al. [9] were the first to attempt 3D US volume reconstruction using a convolutional neural network (CNN). Guo et al. [10] introduced a deep contextual learning network (DCL-Net), utilizing 3D convolution to exploit the sequential context information in US scan frames, along with an innovative loss function based on correlation values. Building on this, Guo et al. [17] expanded their previous work by developing a deep contextual-contrastive network (DC2-Net), which applied a margin triplet loss for contrastive learning in a regression task.\nAround the same time, Luo et al. [11] made key contributions to free-hand US imaging by proposing an RNN-based model with a novel self-supervised and adversarial learning strategy. This approach enabled plausible visual reconstructions, even in more challenging scanning scenarios. Luo et al. continued their active contributions with the development of MoNet, a motion network incorporating an inertial measurement unit (IMU) sensor, a lightweight sensor for capturing acceleration data [18]. Their multi-branch DL architecture leveraged both US images and IMU sensor data for improved performance. In later work, Luo et al. utilized multiple IMU sensors to further enhance reconstruction accuracy [19].\nRecently, Luo et al. [14] introduced an online learning reconstruction framework (RecON), imposing constraints such as motion-weighted training loss, frame-level contextual consistency, and path-label similarity, which significantly improved the accuracy of motion estimation in complex scan motions. Other recent DL approaches have adapted popular models for US imaging. Miura et al. [13] proposed a sequential CNN-RNN structure for both relative and absolute pose estimation, demonstrating the efficacy of relative pose integration via RNNs. Inspired by pyramid warping techniques [20], Xie et al. [21] developed a network that extracts multi-scale features from US frames to better capture low-frequency B-mode information. With the increasing popularity of transformers, Ning et al. [12] applied a transformer architecture to combine local and long-range information from a CNN-based backbone encoder and IMU sensors. Li et al. [15] further identified long-term dependencies between sequential US frames and the influence of anatomical scan protocol factors.\nDespite these advancements, achieving the level of precision required for clinical applications remains a challenge. Many existing methods have not been validated on extensive, real-world datasets, nor have they adequately addressed the effects of image processing techniques, such as speckle reduction, which can distort critical information in B-mode images. Furthermore, there has been limited exploration of adapting these DL-based methods for other imaging modes, such as power Doppler (PD) and photoacoustic (PA) imaging.\nThe potential of DL-based scan motion tracking systems to enable vascular visualization over large regions is significant. By integrating optimized ultrasound and laser sequencing, raw data reconstruction, and post-processing techniques, these systems can extend their utility to PA imaging and PD-mode US imaging. Such advancements promise to open new avenues for clinical applications, including more accurate visualization of vascular structures and enhanced interventional guidance."}, {"title": "METHODS", "content": "The primary aim of this study is to reconstruct a 3D US volume from sequential 2D B-mode frames captured by sweeping a standard transducer over time. Accurate assembly of the volume hinges on the precise estimation of each frame's position, that is, the accumulation of transducer's motion. In handheld PAUS imaging, the real-time B-mode images serve as the foundation. Due to their distinct anatomical layers and tissue speckle patterns, they can aid in tracking the motion."}, {"title": "Problem Definition", "content": "Assuming a scan sequence comprises a total of $N$ B-mode frames, the absolute position vector $\\theta_i$ of the $i$th B-mode frame $B_i \\in \\mathbb{R}^{256 \\times 256}$ (or transducer at the time) is represented by 6-degree of freedom (DoF) vector, defining the 2D Euclidean plane for each image frame within the 3D space:\n$\\theta_i = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5, \\theta_6]^T \\in \\mathbb{R}^6$,\nwhere $[\\theta_1, \\theta_2, \\theta_3]$ and $[\\theta_4, \\theta_5, \\theta_6]$ denote the 3D Euclidean coordinates and the Euler angles. Here, $\\theta_1, \\theta_2, \\theta_3$ correspond to the positions along the axial, lateral, and elevational axes, while $\\theta_4, \\theta_5, \\theta_6$ represent the rotations around the pitch, yaw, and roll axes, respectively. For $i = 0$, $\\theta_0$ is initialized as 0.\nThe vector $\\theta_i$ can be converted into the homogeneous transformation matrix $T_i = [R_i, t_i; 0^T, 1]$ where $R_i \\in \\mathbb{R}^{3 \\times 3}$ and $t_i \\in \\mathbb{R}^3$ indicates the rotation matrix and translation vector, respectively. Then, the relative transformation $\\Delta T_i$ between adjacent frames ($B_i, B_{i+1}$) can be obtained from the transformation matrices ($T_i, T_{i+1}$) as $\\Delta T_i = T_{i+1}T_i^{-1}$. The cumulative product of the relative transformation matrices results in the absolute transformation matrix as\n$T_{n+1} = \\prod_{i=0}^{n} \\Delta T_i T_0$,\nwhere $T_0 = I$. Finally, the relative position vector $\\Delta\\theta_i$ is extracted from $\\Delta T_i$ for use in supervised learning.\nOur goal is to estimate the relative motion $\\Delta\\theta_i$ between two consecutive frames ($B_i, B_{i+1}$) using information from the two frames as well as preceding frames. This can be formally expressed as\n$f(B_i, B_{i-1}, B_{i-2}, ...) = \\Delta \\hat{\\theta_i}$,\nwhere $f$ is the DL model, and $\\Delta \\hat{\\theta_i}$ is the estimated relative motion. The estimate $\\hat{\\theta_i}$ is then used to derive the absolute transformation matrix $\\hat{T_i}$, which is employed to map voxel positions and reconstruct the 3D volume."}, {"title": "Deep Learning Framework", "content": "As shown in Fig. 1, we developed a motion-based learning network, called MoGLo-Net (motion-based learning network with a global-local self-attention module), to estimate the relative scan motion $\\Delta\\theta_i$ from B-mode sequences."}, {"title": "Data Preparation", "content": "Prior to feeding data into the model, two B-mode sequences were generated. The first sequence, denoted as ${B_i}_{i=n}^{n+s}$, consists of frames from $B_n$ to $B_{n+s}$. The second sequence, ${B_{i+1}}_{i=n}^{n+s}$, consists of frames from $B_{n+1}$ to $B_{n+s+1}$. These sequences are processed in parallel using a ResNet-based Encoder Block 1 [22], ensuring consistent refinement of features. The resulting feature maps are represented as ${({E_i, E_{i+1}})}_{i=n}^{n+s}$."}, {"title": "Feature Extraction", "content": "From these features, correlation volumes ${C_i}_{i=n}^{n+s}$ are extracted to capture relationships between successive frames. The concatenated features from the two sequences are then fed into Encoder Block 2, producing refined feature maps, denoted as ${E_i^2}_{i=n}^{n+s}$. These features are further processed by Encoder Block 3, which outputs ${E_i^3}_{i=n}^{n+s}$. Next, the concatenation of the correlation volumes ${C_i}_{i=n}^{n+s}$ and the features from Encoder Block 3, ${({E_i^3})}_{i=n}^{n+s}$, is passed through Encoder Block 4, yielding ${E_i^4}_{i=n}^{n+s}$."}, {"title": "Global-local Attention", "content": "To refine both global and local contextual information, we employed not only conventional attention but also self-attention mechanism. These combined attention mechanisms recalibrate both global and local features while effectively highlighting critical regions for motion estimation within the local features. The resulting final features, denoted as ${G_i}_{i=n}^{n+s}$ and ${L_i}_{i=n}^{n+s}$, which are derived from the global and local features, respectively. The final features are then contrasted using a triplet loss that leverages motion vectors to improve the model's ability to differentiate subtle variation in motion."}, {"title": "Motion Estimation", "content": "For motion estimation, the model employs two RNN-based motion estimators to predict six-dimensional relative motion vectors. The motion, denoted as ${\\Delta g_i}_{i=n}^{n+s}$, is derived from the global features ${G_i}$, while the motion, denoted as ${\\Delta l_i}_{i=n}^{n+s}$, is derived from the local features ${L_i}$. The final motion predictions, ${\\Delta \\theta_i}_{i=n}^{n+s}$, are obtained by combining these motion vectors. The predictions are supervised using a motion-based mean absolute error (MMAE) and a correlation loss to ensure accurate motion estimation."}, {"title": "Testing Phase", "content": "The predicted relative motions ${\\Delta \\hat{\\theta_i}}_{i=1}^{n}$ are transformed into relative transformations, denoted as ${\\Delta \\hat{T_i}}_{i=1}^{n}$. Using the cumulative product of these transformations, as defined in Equation (2), the absolute transformations ${\\hat{T_i}}_{i=1}^{n}$ are obtained. These absolute transformations are then used to reconstruct the 3D US volumes."}, {"title": "Correlation Operation", "content": "We devised a patch-wise correlation operation to extract the relationship between adjacent frames, inspired by speckle decorrelation techniques [6, 7, 8]. This operation is performed on the feature maps ($E_i, E_{i+1}$), which are initially refined from the B-mode sequences using Encoder Block 1 (Fig. 1).\nAs shown in Fig. 2, the first step in this operation is to define two regions of interest (RoIs), represented by dotted squares, at the same locations in both feature maps ($E_i, E_{i+1}$). Within these Rols, pairs of patches are extracted. While the patch in $E_i$ (red filled square) remains stationary at the center of the RoI, the patch in $E_{i+1}$ (black filled square) is moved across the RoI. A correlation array is computed from all possible pairs of patches within the RoIs. By stacking these correlation arrays from multiple Rols, the correlation volume $C_i$ is generated."}, {"title": "Global-Local Attention", "content": "In recent advancements within the fields of natural image and language processing, numerous attention mechanisms have been developed to capture dependencies between large-scale (global) and small-scale (local) contexts [23, 24, 25]. Drawing inspiration from these mechanisms, we designed a global-local attention module as a self-attention mechanism. This module highlights local features (semantics in specific regions) based on global features that summarize semantic information across the entire image, allowing the motion estimator to leverage this information efficiently.\nAs illustrated in Fig. 3, the local feature $E_i^2 \\in ]\\mathbb{R}^{128 \\times 64 \\times 64}$, obtained by passing B-mode frames through a few encoding layers, reduces the spatial domain from 128 \u00d7 128 to 64 \u00d7 64 while preserving local features of the B-mode domain. This feature is then used to derive multiple local feature blocks.\nFirst, both global and local features are recalibrated using standard attention mechanism. For the local feature blocks, the feature map $E_i^2$ is divided into 256 local feature blocks, ${E_i^{2k}}_{k=1}^{256}$, through a patchnization, where $E_i^{2k} \\in \\mathbb{R}^{128 \\times 4 \\times 4}$. Then, the pooled feature map is calculated as $P_i^L = \\Gamma(E_i^{2k}) \\in \\mathbb{R}^{128}$, where $\\Gamma(\\cdot)$ denotes an adaptive pooling operation. The channel attention score is then obtained as:\n$S_i^L = \\sigma(W_2W_1P_i^L) \\in \\mathbb{R}^{128}$,\nwhere $W_1 \\in \\mathbb{R}^{128 \\times 8}$ and $W_2 \\in \\mathbb{R}^{8 \\times 128}$ are the weights of a multi-layer perceptron (MLP), and $\\sigma(\\cdot)$ is the sigmoid function. The recalibration of each local patch block $E_i^{2k}$ is then conducted using the channel attention score:\n$R_i^k = S_i^L E_i^{2k} \\in \\mathbb{R}^{128 \\times 4 \\times 4}$,"}, {"title": "Loss Functions", "content": ""}, {"title": "Motion-based Mean Absolute Error (MMAE)", "content": "There is a class imbalance among the elements of motion, as cases with high amplitudes (fast motion) are significantly less frequent. Moreover, estimating fast motion poses a greater challenge than estimating slow motion, primarily because the correlation between adjacent frames diminishes. To address this issue, we propose the motion-based mean absolute error (MMAE), a type of weighted metric:\n$L_{MMAE} = \\frac{1}{6(\\textit{s}+1)} \\sum_{i=n}^{n+s} \\sum_{k=1}^{6} \\frac{|\\theta_{ik} - \\hat{\\theta_{ik}}|}{w_{i} \\Delta \\theta_i + \\epsilon}$,\nwhere $\\textit{s} + 1$ is the length of the B-mode sequence, and $w_{i} = |\\Delta \\theta_i + \\epsilon|$ is a weighting vector defined by the motion vector. The error for fast motions is weighted more heavily than for slow motions, as $w_i$ increases for fast motions. Here, $\\epsilon$ is a smoothing factor to reduce the effect of the weighting."}, {"title": "Correlation Loss", "content": "To assist in model supervision, we adapted correlation loss [10], which is defined as:\n$L_{Corr} = \\frac{1}{6(\\textit{s}+1)} \\sum_{i=n}^{n+s} \\sum_{k=1}^{6} (1 - \\Phi(\\Delta \\hat{\\theta}_{ik}, \\Delta {\\theta}_{ik}))$,"}, {"title": "Margin Triplet Loss", "content": "Although each scan can have different appearances in B-mode images due to varying scan protocols or anatomical variations, similar motions can still be found among scan frames. To further assist model convergence, we employed contrastive learning with a margin triplet loss [26]:\n$L_{Triplet} = max(0, dist(F_a, F_p) - dist(F_a, F_n))$,\nwhere $F_a$ is an anchor feature map serving as a center criterion. To determine the positive and negative feature maps, $F_p$ and $F_n$, which correspond to the near and far samples, the cosine similarity between labels is used.\nThe triplet loss $L_{Triplet}$ encourages the model to contrast the feature maps in latent space, facilitating their convergence and improving robustness."}, {"title": "Final Loss Function", "content": "The final loss function is a linear combination of the individual loss components:\n$L_{Final} = \\alpha_1L_{MMAE} + \\alpha_2L_{Corr} + \\alpha_3L_{Triplet}$,\nwhere $\\alpha_1$, $\\alpha_2$, and $\\alpha_3$ are positive real values used to balance the magnitude of each loss function."}, {"title": "EXPERIMENTAL RESULTS", "content": ""}, {"title": "Meterials", "content": ""}, {"title": "Dataset 1", "content": "We used a programmable US machine (Verasonics, Vantage System 64LE) equipped with a 1D array transducer (L11-5v) to acquire US data, and the scan motion was tracked using a mounted electromagnetic sensor (Polhemus) at an acquisition rate of 60 Hz (Fig. 4). The B-mode images were reconstructed using 31 different angle plane waves to enhance their quality and have dimensions of 256\u00d7256 pixels, with a pixel resolution of 0.1484\u00d70.1484 mm. The image frame rate was set at 20 Hz. We collected 54 scans, primarily from the forearms of 9 individuals, and randomly divided them into 30, 6, and 18 scans for the training, validation, and testing sets, respectively, ensuring that the subjects were distinct across sets. Each scan consisted of 900-1000 frames, capturing the entire length of the forearm through an arbitrary S-shaped trajectory. This scanning procedure was conducted three times for each forearm. The local Institutional Review Board (Pusan National Univ. IRB, 2023_74_HR) granted approval for the acquisition and use of the data."}, {"title": "Dataset 2", "content": "To construct 3D vascular structures, we acquired US Doppler data using the same US acquisition system. Iimmediately following the emission of 31 plane waves for a B-mode image frame, 20 plane waves with a pulse repetition frequency (PRF) of 3000 kHz were transmitted for a Power Doppler (PD) image frame. To minimize clutter signals, a filter based on singular value decomposition (SVD) was utilized. Additionally, photoacoustic (PA) data were collected to construct 3D vascular structures. As shown in Fig. 4 (a), an Q-switched Nd:YAG lasers system (OPOTEK Phocus Mobile) was integrated with the US machine to enable a real-time PAUS imaging. Specifically, the US system received a flash lamp trigger signal from the laser system, waited for a specified delay time (optical build-up time for the laser), and then commenced data acquisition precisely when the laser fired. The laser was delivered via fibers arranged on one side of the linear array US transducer (Fig. 4 (c)). Subsequent to the 31 plane waves used for a B-mode image frame, a laser with a wavelength of 750 nm was employed to insonify for a PA image frame. PA images were reconstructed using a standard delay-and-sum (DAS) method."}, {"title": "Dataset 3", "content": "Our dataset may contain inherent biases that could positively influence our model's performance. To validate the model's performance in different environments and mitigate the effects of such biases, we utilized a publicly available dataset [15]. This dataset comprises transverse and longitudinal scans of the forearm with diverse-shaped trajectories, collected from 19 subjects, resulting in a total of 19\u00d712 scans. For each subject, both forearms were scanned using three distinct trajectory shapes (S, C, and L) in two orientations: parallel and perpendicular to the forearm, yielding 12 scans per subject.\nThe ultrasound images were acquired using an Ultrasonix machine (BK, Europe) operating at 20 Hz with a convex transducer (4DC7-3/40). The probe motion was tracked by an NDI Polaris Vicra (Northern Digital Inc., Canada). The B-mode images were processed with a median level of speckle reduction. Each scan consists of 36 to 430 frames with a resolution of 480x640 pixels, and the probe travel distance ranges approximately between 100 and 200 mm. The dataset was randomly divided into training, validation, and testing subsets based on subjects, with a ratio of 10:4:5.\nSince images captured using the convex probe have a sector shape with background regions outside the sector, we extracted a square region of interest (320\u00d7320 pixels) from the central foreground region to focus on relevant areas. To handle the increased input size, the batch size was reduced to 12."}, {"title": "Implementation Details", "content": "For the comparative experiments, we selected models currently recongnized for motion estimation, including CNN [9], DC2-Net [17], EfficientNet [15], and LSTM [14]. We made minor adjustments to the structure and hyperparameters of each model to tailor them to our task. During the training phase, an arbitrary sequence of length s is sampled from each person in every epoch. This method requires a relatively large number of epochs (20,000) to ensure model convergence.\nThe Adam optimizer was employed with an initial learning rate of 1e-5, which was reduced by a factor of 0.8 every 100 epochs to facilitate the convergence process. Training was performed on an NVIDIA RTX 4090 GPU (24 GB) with a batch size of 14. Under these settings, the proposed MoGlo-Net required 4.7 hours to train and achieved a processing speed of 95 frames per second (FPS) with a batch size of 32 during the inference phase. Additionally, reconstructing a 3D US volume consisting of 950 frames took 1.8076 seconds.\nAll experiments were conducted under identical conditions, including sequence length, dataset splitting, and training parameters, to ensure a fair comparison among the models."}, {"title": "Evaluation Metrics", "content": "For the 6-dimensional output, we utilized the relative Average Error (rAE; mm, \u00b0) and accumulated Average Error (aAE; mm, \u00b0) to evaluate $\\theta_i$ and $\\hat{\\theta_i}$. The rAE measures the relative motion estimation accuracy, while the aAE quantifies the accumulated motion error over the scan trajectory. For the reconstructed 3D ultrasound volumes, we measured the Euclidean distance of the grid points between the true and predicted frames. The relative Frame Error (rFE; mm) reflects the relative displacements between frames, whereas the accumulated Frame Error (aFE; mm) accounts for the cumulative displacements along the scan path. Additionally, we used Correlation (Corr), defined as cosine similarity, to measure the underlying trends of reconstructed trajectories in 3D Euclidean space. The Final Drift (FD; mm), which represents the aFE of the final frame, increases proportionally with the scan length. To normalize FD, we employed the Final Drift Rate (FDR; mm), calculated by dividing FD by the total length of the scan trajectory. FDR serves as a subsidiary metric, as FD typically exhibits relatively large deviations in S-shaped scans."}, {"title": "Results using Dataset 1", "content": ""}, {"title": "Quantitative Results", "content": "As shown in Table 1, the DCL shows better performance than conventional CNN, with further improvement achieved through contrastve learning in DC2. Efficient yields more precise results by leveraging sequencial information, while LSTM provides more reliable performance by incorporating motion-based constraint. The MoGLo (ours) achieved the best performance across all metrics among the comparison models."}, {"title": "Qualitative Results", "content": "Consequently, the final 3D US volume can be reconstructed using the estimated motion and B-mode images. As illustrated in Fig. 5 (a), CNN exhibits pronounced distortion, while DC2 and Efficient show noticeable improvements over CNN. However, both models display considerable FD due to their limited generalization capabilities. Among the comparison groups, LSTM demonstrates the most reliable results in terms of both trajectory similarity and FD. Models that utilize correlation volume and global-local attention tend to futher improve the quality of 3D US volumes, with the proposed MoGLo-Net showcasing the most accurate results, closely resembling the ground truth."}, {"title": "Ablation Study 1", "content": ""}, {"title": "Ablation Study 2", "content": ""}, {"title": "Results using Dataset 2", "content": "The benefit of motion tracking extends the clinical applications beyond the compilation of B-mode images. In this study, we adapted the acquisition sequence in the ultrasound system to obtain not only B-mode images but also Power Doppler (PD) mode images, which are specialized for visualizing vessels. B-mode images were utilized for motion estimation through MoGLo-Net, and both the motion estimates and PD images were employed to construct 3D vessels.\nFig. 6 illustrates examples of reconstructed 3D vessels (radial artery) in a forearm. Notably, each vessel appears almost straight and closely resembles the natural anatomical shape, even though the scan motion trajectory was wavy."}, {"title": "Results using Dataset 3", "content": "As shown in Table 4, performance metrics varied across models due to differences in dataset features, such as hardware setup, scan protocol, and image processing. However, the overall performance trends remained consistent across models.\nThe consistency in the performance order observed between the in-house dataset and the open dataset strengthens the reliability of the experimental findings. This consistency indicates that the observed performance is not specific to a particular dataset but instead reflects inherent characteristics of the models. Fig. 8 illustrates 3D trajectories using ground-truth and estimated positions."}, {"title": "DISCUSSION AND CONCLUSION", "content": "The primary advantage of handheld PAUS systems is the flexibility they offer during scanning. However, this comes at the cost of requiring skilled and experienced operators with a deep understanding of anatomical 3D structures, as standard 1D array transducers provide only 2D images with a restricted FoV. These challenges can be addressed by reconstructing 3D volumes, which allow the visualization of complex 3D structures and provide arbitrary cross-sections of the RoI. Historically, there have been various hardware-based approaches [28, 29] to obtain 3D volumes in both PA and US fields.\nThe ideal solution, however, lies in software-only approaches, which are more cost-effective. The freehand 3D approach is an extension of the panoramic image mode, which has already been commercialized in many ultrasound products. Nevertheless, one of the key challenges remains addressing elevational motion (out-of-plane motion), where two adjacent frames have relatively low correlation. Some groups have introduced the use of IMU sensors attached to the transducer as minimal hardware support to aid motion estimation in software. However, bulkier transducers hinder the operator's ability to scan, particularly for PAUS systems, which are already large due to the inclusion of laser fibers.\nOur proposed model, MoGLo-Net, is optimized for tracking motion without the need for additional sensors. The model employs correlation operations between feature maps from adjacent B-mode images, explicitly utilizing their closeness. Since the correlation tensor captures all correlations between subspaces across sequential images, the model can track not only in-plane motion but also out-of-plane motion.\nIn additional experiments, we observed that the 3D volume generated from encoded feature maps ($E_i$) and $E_{i+1}$) is more efficient than one generated directly from B-mode images ($B_i$ and $B_{i+1}$). Fig. 9 (a) shows the 2D map obtained by averaging the correlation volume (from B-mode images) over the z-axis in Fig. 2, while Fig. 9 (b) shows the corresponding 2D map obtained in the same manner from encoded feature maps. The feature correlation demonstrates better contrast than B-mode correlation, displaying clear patterns. The relative value differences over space in the map are expected to aid in estimating in-plane motion, while the overall mean value in the space is expected to aid in estimating out-of-plane motion.\nAdditionally, the global-local attention module generates local representations of images by learning attention weights that emphasize significant sub-regions for global motion features. This allows the model to effectively highlight sub-regions, such as fully-developed speckle areas, which are critical for motion estimation.\nIn extra experiments, we visualized the patch-wise attention scores in the module (Fig. 3), as shown in Fig. 9. Under conditions of slow elevational motion, adjacent frames share significant overlapping regions, preserving speckle patterns. As a result, the model focused on uniform areas, such as fully-developed speckle regions (Fig. 9 (c)). In contrast, under conditions of fast elevational motion, speckle patterns become disrupted due to the larger gap between adjacent frames. Despite this, high-echogenic tissues often maintain consistency along the elevational axis. In such cases, the model highlighted these regions, as illustrated in Fig. 9 (d).\nFinally, the model utilizes both global and local representations to leverage complementary mutual information, while the LSTM processes features from the B-mode sequences to capture temporal motion patterns. The MoGLo-Net demonstrated consistent improvements across all metrics over the state-of-the-art (SOTA) model [14], producing more realistic 3D US volumes. Moreover, we validated the effect of each component of MoGLo-Net through an ablation experiment by omitting key parts, such as the attention module, correlation volume, and motion-based MAE. In addition, we examined the impact of potential information sources, such as IQ data, raw B-mode images, and speckle patterns. As a result, we found that speckle patterns, which are highly related to scan motion, serve as critical cues for scan motion estimation.\nTo the best of our knowledge, this is the first attempt at free-hand 3D vascular imaging using ultrasound Doppler acquisition data or photoacoustic data. In this study, we specifically targeted small vessels to emphasize their shape and structure. For ultrasound imaging, we chose power-Doppler over color-Doppler image processing due to its superior sensitivity in detecting weak blood flow signals in smaller vessels. For photoacoustic (PA) imaging, we selected an optimal laser wavelength to maximize blood signal detection and reconstructed the images from raw data using appropriate hyper-parameters in the DAS and filtering procedures."}]}