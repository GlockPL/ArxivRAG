{"title": "People will agree what I think: Investigating LLM's False Consensus Effect", "authors": ["Junhyuk Choi", "Yeseon Hong", "Bugeun Kim"], "abstract": "Large Language Models (LLMs) have recently been widely adopted on interactive systems requiring communications. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Especially psychologists focused on the False Consensus Effect (FCE), which can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to deeply examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when the strength of FCE becomes larger or smaller compared to normal usage.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently been widely applied for interactive systems requiring communications, such as education, customer service, or healthcare (Sch\u00f6n et al., 2023; Altay and \u00c7etint\u00fcrk, 2024; Waikar, 2020). Within these applications, controlling cognitive biases is essential because providing biased information may harm the utility of such systems. For instance, in a tutoring system, false beliefs about students' learning styles may make the system provide an inappropriate learning aid (Sch\u00f6n et al., 2023). So, before applying LLMs to those applications, we need to verify whether the LLMs have been exposed to any cognitive biases (Echterhoff et al., 2024). Among those cognitive biases, psychologists have mainly focused on the False Consensus Effect (FCE). FCE can hinder smooth communication because a person with FCE has a false belief about the social consensus; by definition, a person with FCE believes their own behavioral choices as relatively common choices in a given situation while viewing other choices as uncommon in society (Ross et al., 1977; Wojcieszak and Price, 2009; Wetzel and Walton, 1985). Thus, we suspect that an LLM-based interactive system (e.g., Sch\u00f6n et al. (2023)) can provide misleading information because of false beliefs about the consensus between LLMs and human users. For instance, a tutoring system with FCE may recommend a learning style that it regards as preferred in society but which is actually inappropriate for the given situation. Despite the significance of FCE in communication, experimental methods in previous studies have less investigated whether LLMs may expose FCE in ordinary applications. Studies have conducted experiments to identify underlying reasons for other cognitive biases in LLMs (Koo et al., 2023; Opedal et al., 2024a; Talboy and Fuller, 2023) or to mitigate other cognitive biases in LLMs (Echterhoff et al., 2024; Itzhak et al., 2023; Lin and Ng, 2023). However, these experimental methods have three limitations when applying them to FCE investigation: (1) confounding biases, (2) general situations, and (3) prompting methods.\nFirst, for the confounding biases, previous studies have less considered the effect of other biases during the experiments. Several cognitive biases exist in the human thought process and affect human verbalization. So, psychologists have tried to disentangle a target bias from confounding biases within an experiment. Like humans, we suspect LLMs can have multiple cognitive biases, as recent studies reported (Schmidgall et al., 2024; Echterhoff et al., 2024). Therefore, to prevent introducing confounding biases within LLMs' generation process, we need to control them. However, previous studies have yet to control such confounding biases. So, in this paper, we adopt a psychological experiment properly to control confounding biases."}, {"title": "2 Related Work", "content": "Our paper is closely related to literature examining cognitive biases. So, we review previous approaches in examination and prompting methods.\n2.1 Examining bias in LLMS\nInspired by psychological experiments, researchers have recently focused on examining cognitive biases in LLMs. We categorize these studies in terms of whether they follow the experimental procedure of psychology when examining cognitive biases.\nFirst, researchers designed their own experiments to examine cognitive biases (Koo et al., 2023; Schmidgall et al., 2024; Opedal et al., 2024b; Itzhak et al., 2023). Though psychological theories inspire these studies, researchers proposed new experiments using previous natural language datasets\u00b9. For example, Koo et al. (2023) modified existing datasets to expose LLMs to various cognitive biases, including bandwagon effect. Though these benchmarks identified biases in LLMs empirically, however, these methods cannot ensure whether we can exclude confounding bias from the result. In other words, it is difficult to ensure the results are due to the bias we wanted to investigate.\nSecond, researchers attempted to adopt psychological experiments as it is (Xie et al., 2024; Aher et al., 2023; Macmillan-Scott and Musolesi, 2024; Talboy and Fuller, 2023). Studies in this category have psychological experiments as their basis, so they mainly follow the materials or experimental procedures provided by psychological studies. Some researchers aimed to replicate psychological studies using LLMs. For example, Aher et al. (2023) replicated well-known experiments (e.g., Milgram Experiment). Though they demonstrated how psychological experiments can be reproduced using LLMs, the paper has yet to focus on explaining how cognitive bias occurs in LLMs. Others examined whether LLMs have cognitive biases with psychological experiments (Xie et al., 2024; Macmillan-Scott and Musolesi, 2024; Talboy and Fuller, 2023). Note that these experiments try to control demographic and situational differences. As such differences can affect LLMs through changes in prompts, it is necessary to consider those differences when conducting such experiments.\nAmong these studies, as far as we know, the only experiment considering FCE on LLM was"}, {"title": "2.2 Mitigating biases of LMs", "content": "As LMs generate text based on the input prompt, researchers have reported that subtle changes in the prompt affect LMs' output (Jia and Liang, 2017; Cheng et al., 2019; Xie et al., 2024; Guo et al., 2024). For example, Jia and Liang (2017) and Cheng et al. (2019) reveal that adding or modifying input prompts can change the answer. Similarly, Xie et al. (2024) showed that prompt settings can alter the result of a psychological experiment about trust. They changed demographic information (e.g., gender) and prior trust in a prompt, and the result revealed that such changes affect the behavior of LLMs. Hence, different prompts may invoke different strengths of a cognitive bias.\nThus, researchers have developed methods to control cognitive biases in LLMs (Echterhoff et al., 2024; Itzhak et al., 2023; Schmidgall et al., 2024). Some researchers tried to modify the generation procedure to control the bias (Itzhak et al., 2023). However, this approach cannot be generalized to a black-box model, including GPT-4. So, others suggested a method using prompt changes. For example, (Echterhoff et al., 2024) tries to control cognitive biases by providing additional zero-shot prompts or examples that can make LLMs aware of cognitive biases. Similarly, (Schmidgall et al., 2024) suggested bias mitigation strategies for medical QA, which includes zero-shot educating prompts or biased/unbiased examples for QA.\nHowever, current methods have not taken into account prompt engineering techniques such as CoT (Wei et al., 2022b) or Reflection (Shinn et al., 2024). As such well-known prompt techniques enhance question-answering tasks with deep reasoning, they likely reduce byproducts of intuitive thinking, including cognitive bias. Although Opedal et al. (2024b) assessed the impact of CoT on cognitive bias, they did not propose a way to mitigate such bias. Therefore, based on previous research, we need to test two dimensions of modification: pro-"}, {"title": "3 Study 1: Examining FCE of LLM", "content": "Study 1 conducts an experiment inspired by psychological experiments to investigate whether the False Consensus Effect (FCE) emerges in LLMs (Ross et al., 1977; Choi and Cha, 2019).\n3.1 Procedure\nTo confirm whether LLMs have FCE, we conduct an experiment that mainly follows a well-defined psychological experiment for revealing FCE (Ross et al., 1977; Choi and Cha, 2019). In the following paragraphs, we illustrate the participants and procedure for our experiment. Each paragraph begins with how psychologists conducted their experiments to help readers understand.\nParticipants: Originally, psychological studies usually recruited college students. There is no screening procedure to select a specific group of students to identify the general phenomenon of human students. Also, as cultural or gender biases may affect FCE (Choi and Cha, 2019), participants' demographic information should not be biased.\nIn our study, we make LLMs pretend to be college students, as in the original experiment. Simply, we plant some idea about a character to LLM by giving a system prompt like 'Your name is [name]. You are an undergraduate student. You are [gender]. You are [nationality].' Also, to control cultural or gender biases, we used 10 characters for each of the two cultures and each gender\u00b2. For detailed information about the 40 characters that we used, see Appendix B.2."}, {"title": "3.2 Tested LLMs", "content": "For the experiment, we use four LLMs: GPT-4 (Achiam et al., 2023), Claude 3 Opus (Anthropic, 2024), LLaMA 270B (Touvron et al., 2023), and Mixtral 8x7B (Jiang et al., 2024). We selected these models because they have shown outstanding performance on question-answering tasks, and the largest model in the family has been published publicly through API or model parameters.\nNote that these four LLMs have a refinement policy that avoids social or ethical issues. As our experiment tries to reveal social biases in LLMs, we briefly summarize refinement policies here. First, GPT-4 and Claude 3 have a procedure that refuses answers to questions concerning personal information or questions sensitive politically, religiously, or culturally. Meanwhile, LLaMA 2 and Mixtral were trained to avoid making dangerous or unethical utterances and to retain strong neutrality when making choices. We expect that this difference in refinement policy affect the experimental result.\nAll the experiments were done by calling APIs. We used the official API provided by GPT-4 and Claude 3. For LLaMA 2 and Mixtral, we used the free API provided by Groq\u00b3. All the experiments were conducted from February 24, 2024, to June 10, 2024, by calling 320 API calls for each LLM. Also, to reduce the effect of a stochastic generation procedure and focus on LLMs' deterministic computation of generating tokens, we set the temperature value as zero for all API calls\u2075. The detailed environmental setup is described in Appendix C."}, {"title": "3.3 Analysis", "content": "Originally, psychologists used statistical tests to identify whether humans have FCE. Researchers computed the average perceived agreement for each option as each participant provided a perceived agreement for others' thoughts. Mathematically, let $A_c(p)$ be the perceived agreement on a specific option $p$, averaged on participants who chose option $c$. Then, psychologists verified FCE by testing whether $\u2206 := A_1(1) \u2013 A_2(1)$ is positive, indicating that participants with option 1 overestimated their preferred choice compared to participants with option 2. For statistical tests, they conducted a two-sample t-test or Mann-Whitney U test (Nachar et al., 2008), a non-parametric alternative for t-test.\nOur study mainly follows analysis methods in original experiments (Ross et al., 1977; Choi and Cha, 2019). We set three hypotheses: one for verifying whether LLMs have FCE and two for identifying the effect of demographic factors on FCE."}, {"title": "3.4 Result and Discussion", "content": "The experimental result revealed two findings: (1) LLMs do have FCE in general; (2) FCE exists regardless of the demographic bias that we provided.\nH1-1 (FCE) Table 2 shows the result of the Mann-Whitney U test for H1-1\u2077. In general, FCE is observed in all four models. GPT-4 and Mixtral showed statistically significant FCE in three of four stories, except Story 1. Similarly, Claude 3 and LLaMA 2 showed FCE in three stories, except Story 3. Note that Stories 2 and 4 query situations that affect participants' direct interests (e.g., grade or money) less than the other two stories.\nSo, we suspect that the refinement policy of LLMs affects the demonstration of FCE when the options are related to any social issue, including law. For example, let us consider Story 3. The story illustrates a traffic ticket with incorrect information issued to a driver who drove 38 mph in a 25-mph zone. Participants should select either paying the fine or contesting the ticket. Because the situation involves legal issues, the refinement policy may regard the 'contesting' option as refusing legal judgment, which seems illegal or unethical. Such interpretation may cause the model to adjust its answer to 'paying fine' when we ask LLMs to estimate other human peers' thoughts.\nH1-2 (Culture) The top of Table 3 shows the result of the Kruskal-Wallis test for H1-2\u2078. The result reveals that the strength of FCE $\u2206$ differs across cultures in three LLMs. GPT-4, Claude 3, and LLaMA 2 showed statistically significant differences in Story 2. In addition, Claude 3 and LLaMA 2 showed cultural differences in Story 4 and Story 1, respectively. Meanwhile, Mixtral did not show any statistical difference in the four stories.\nThe result suggests that LLMs may have cultural biases in their parameters. As the only change is cultural background, LLMs are likely affected by such cultural differences. Therefore, we suspect that the refinement process of LLMs may fail to identify cultural biases when answering our experiment. Though the process attempts to address culturally sensitive issues, the four stories pose culturally insensitive questions to LLMs in this experiment. So, LLMs cannot identify potential cultural bias in their generated answer, which allows us to observe cultural bias on FCE. Such cultural bias can be reduced by using a mixture of models, as shown in the result of Mixtral. This tendency is similar to a report that a mixture can address social biases (Jiang et al., 2024).\nH1-3 (Gender) The bottom of Table 3 shows the result of the Kruskal-Wallis test for H1-3\u2079. The result generally indicates that gender difference does not affect the strength $\u2206$. Out of sixteen test results, only two results are statistically significant: GPT-4 on Story 3 and LLaMA 2 on Story 1. Other test results are statistically insignificant. Especially gender differences did not affect Claude 3 and Mixtral in all four stories.\nThus, the result suggests that LLMs can reduce the effect of gender differences under a prompt that can invoke FCE. As gender difference is one of the main topics in controlling social biases, LLMs or their refinement policies may have prepared for gender differences. For example, these four LLMs have tested on bias benchmark BBQ (Parrish et al., 2021), which contains about 30% of gender-related questions. However, it is yet questionable why gender differences behave differently compared to cultural differences, even though bias benchmarks include not only gender-related questions but also culture-related questions."}, {"title": "4 Study 2: Mitigating FCE with Prompt", "content": "After identifying FCE in LLMs, in Study 2, we compare different prompting styles to control the strength of FCE in LLMs.\n4.1 Procedure\nInspired by previous work (Echterhoff et al., 2024; Opedal et al., 2024b), we test two aspects of prompting styles: (1) type of provided information and (2) depth of reasoning chain.\nProvided information: Even a subtle change in prompt affects the experimental result (Jia and Liang, 2017; Cheng et al., 2019). So, we design four conditions. The detailed prompts and experimental methods are described in Appendix E.1\u00b9\u2070.\n(P1) None: The prompt has no information other than the original FCE question. We just ask LLMs to estimate perceived agreement.\n(P2) Supportive: Before questioning a perceived agreement, we provide supportive reasoning about the participant's choice to LLMs.\n(P3) Opposite: Similar to P2, but we provide a reasoning opposite to the participant's choice.\n(P4) Irrelevant: Similar to P2, but we provide reasoning totally unrelated to the situation.\nReasoning chain: As FCE can be seen as a byproduct of intuitive thinking, we can remove FCE using deep reasoning, as in the QA task (Wang et al., 2023). So, we design four conditions. Note that we exclude methods utilizing external observations to avoid confounding biases. The detailed prompts for conditions are shown in Appendix E.2.\n(R1) Direct: The prompt just asks LLMs to estimate perceived agreement without reasoning.\n(R2) Simple: The prompt asks LLMs to estimate perceived agreement with reasoning.\n(R3) CoT: The prompt asks LLMs to generate reasoning first and then estimate.\n(R4) Reflexion: The prompt asks LLMs to generate reasoning first, rethink their reasoning, and finally estimate perceived agreement."}, {"title": "4.2 Analysis", "content": "We set the following two hypotheses for each prompting style. To test each hypothesis, we conduct a series of statistical tests. First, we conduct the Kruskal-Wallis test to identify whether the conditions have different strengths of FCE $\u2206$. If the test finds some difference, we conduct Dunn's post-test and Mann-Whitney test to compare the strength between conditions.\nH2-1. The strength of supportive information is higher than the others, i.e., $\u2206_{P2} > \u2206_{Pk}$ for any $k = 1, 3, 4$.\nH2-2. Deeper reasoning decreases the strength, i.e., $\u2206_{R1} > \u2206_{R2} > \u2206_{R3} > \u2206_{R4}$.\nIn testing these two hypotheses, we control other aspects. For example, we fix the reasoning method as R1 when we test H2-1. Similarly, we fix the provided information as P1 when we test H2-2. Though the control can simplify the situation, whether the result can be applied to the mixture of two aspects, such as a combination of P2 and R3, is questionable. So, we additionally look for a trend in 4 x 4 = 16 pairs of two aspects. Though this is not an exact statistical analysis, it may provide insight into the interaction between two effects."}, {"title": "4.3 Result and Discussion", "content": "The result suggests two findings: (1) opposite reasoning can reduce FCE; (2) deeper reasoning can reduce FCE. Additionally, we describe a tendency of interaction between two factors to control FCE.\nH2-1 (Provided Information) The top of Table 4 describes the result of the Kruskal-Wallis test for H2-1. Due to the page limit, the Table only presents the result in Stories 2 and 4, which showed FCE in H1-1\u00b9\u00b9. In general, the strength of FCE changed in LLMs when we provided additional information. In all stories, providing additional information affects GPT, LLAMA 2, and Mixtral models. Claude is also affected by the provided information in Stories 1, 2, and 4. The Table also elaborates on pairs of conditions that showed differences in both Dunn's post-test and Mann-Whitney U test."}, {"title": "5 Conclusion", "content": "Using a psychological experiment, we aimed to understand whether Large Language Models (LLMs) exhibit False Consensus Effect (FCE). Despite significance of FCE in human communication, previous studies have yet to examine FCE in LLMs thoroughly. So, they have limitations regarding confounding biases, general situations, and prompt changes. To address these limitations, we borrowed a well-known psychological experiment for FCE and designed two studies that can provide insights about FCE in LLMs. In Study 1, we conducted a psychological experiment on LLMs as it is. The result revealed that LLMs do have FCE, and FCE phenomenon may differ across cultural backgrounds given to LLMs. In Study 2, we examined the change in strength when we altered prompts. As a result, we found that providing supportive information without querying any reasoning can maximize FCE, and giving opposite information with multi-step reasoning can minimize FCE. We hope this work improves understanding of LLMs' behavior."}, {"title": "Limitation", "content": "This work has two limitations when one attempts to apply our findings in other work. First, in a real-world situation, as other confounding biases can affect the result, our mitigation methods may not work properly. For example, a detailed persona setting may introduce different biases, such as the similarity effect or the conformity effect. Such detailed settings can also introduce a change of prompts, which can affect an LLM's output. Similarly, when someone uses a prompt engineering method, such as retrieval-augmented generation, which introduces external information to the generation process, the strength of FCE may vary due to cognitive biases invoked by external information, e.g., confirmation bias. Future work for extending this research to a broader situation is required.\nSecond, as we cannot interpret a neural network fully, this work does not identify a direct cause of FCE. For example, LLaMA occasionally generates neutral answers for our questionnaire, even when we force it to answer one of two options. In that case, we cannot identify why it refused to choose one option; it may be (1) because its refinement policy refused to provide a possibly problematic answer or (2) because its computational result is indeed neutral. Similarly, we cannot identify a fundamental cause of FCE in GPT-4 or Claude 3 since these models only provide the last computation result. So, future work for identifying the deeper cause of FCE from the computational structure of a neural network is required."}, {"title": "B Prompt Design of Study 1", "content": "This section illustrates how we designed the prompt used in Study 1.\nB.1 Input prompts\nTo experiment with Study 1, we need to deliver a specific persona and situation to LLMs. First, to deliver a specific persona to control gender and cultural biases, we set the character information in the system prompt using the following structure. For the detailed character specification, see Section \u0392.2.\nAfter setting the global prompt, we delivered a hypothetical story and the question as the user prompt. For the detailed stories and questions for LLM's own choice, see Section B.3."}, {"title": "C Environment for Experiment", "content": "Here, we briefly illustrate the environment used for our experiment.\nC.1 Study 1\nAll the experiments were done in the following environment. For the hardware system, we used a Macbook Pro with an Apple M3 Pro chip. For the software system, the system has MacOS Sonoma 14.1 with Python 3.10.13. We also used Python libraries including openai 0.28.0, groq 0.4.2, anthropic 0.21.1, pandas 2.1.4, statsmodels 0.14.0, and scipy 1.11.4 for the experiment.\nC.2 Study 2\nThe experimental setup is the same as that of Study 1, except for the prompting styles. We tested the same LLMs, GPT-4, Claude 3, LLaMA 2, and Mixtral, as in Study 1. All the experiments were conducted from February 24, 2024, to June 10, 2024, by calling 5120 API calls\u00b9\u00b3 for each LLM\u00b9\u2074."}, {"title": "D Detailed result for Study 1", "content": "D.1 Shapiro-Wilk normality test\nBefore conducting a statistical test, we checked whether our experimental result followed a normal distribution using the Shapiro-Wilk test. The null hypothesis of this test is that 'the data follows a normal distribution.' Thus, a p-value under 0.05 indicates that the provided data is not normal. Table 5 (page 14) shows the result. As shown, more than half of our experimental results are not normal. Therefore, using a t-test or an ANOVA is not suitable because they assume normality. Thus, we chose to use non-parametric analyses in further statistical analyses.\nD.2 Comparing LLMs with human (H1-1)\nThough we aim to adopt human experiments to understand LLMs' behavior, one can ask whether the tendency is similar to humans. As we borrowed the experimental design from Ross et al. (1977) and Choi and Cha (2019), we can compare our results on LLMs with previous reports on human FCE. Table 6 shows the result of human FCE with our experimental result. Note that the last four rows are the same as Table 2, as we copied the data from that Table. Note that a human study used a parametric test (t-test) to identify FCE."}, {"title": "E Prompt design of Study 2", "content": "To experiment with Study 2, we (1) added reasoning information to the user prompts and (2) adopted deep reasoning methods. Basically, the structure of the input prompt is the same as Study 1 until we ask LLMs to estimate the perceived agreement of other peers. First, for additional reasoning information, we used the following prompts to deliver reasoning of LLM's choice before asking about perceived agreement. See Section E.1 for the detailed reasoning input. Second, for deep reasoning methods, we modified the last question to ask direct reasoning for an LLM's estimation. See Section E.2 for the details."}, {"title": "E.1 Provided information", "content": "For simplicity, we focus on the effect of reasoning text generated when LLM makes its own choice. So, we simulate LLM's own reasoning in P2-P4. The following subsections describe the actual reasoning generated by each LLM and used in this study. We used them as supportive or opposite information in testing H2-1 of Study 2. For example, if we are simulating the assistant's answer as option 1, we use the reasoning of option 1 as supportive and option 2 as the opposite. For the irrelevant reasoning, we used the following paragraph."}, {"title": "E.1.1 GPT-4", "content": "Reasoning used for Story 1\n\u2022 Option 1 (68 tokens): I chose 'individual paper' because it allows you to have full control over the quality and outcome of your work. In a group setting, there's a risk of unequal effort distribution, potentially impacting your grade negatively. Individual work ensures that your grade reflects your own effort and understanding, providing a clear measure of your mastery of the course material.\n\u2022 Option 2 (67 tokens): I chose the group paper option because it fosters collaboration, allowing team members to pool their knowledge and skills, leading to a more comprehensive and diverse paper. It also offers an opportunity to learn from peers, develop teamwork skills, and share the workload, potentially reducing the individual stress and time commitment that comes with completing a large project alone.\nReasoning used for Story 2\n\u2022 Option 1 (68 tokens): I chose 'Sign release' because if you genuinely like the store and your comments were sincere, allowing them to use your feedback can help the store you appreciate and possibly contribute positively to your community. Plus, it can be a unique experience to see yourself contributing to a local business's marketing efforts, fostering a sense of community and personal connection.\n\u2022 Option 2 (66 tokens): I chose 'Not sign release' because your consent was not obtained before recording, raising ethical concerns. Signing the release after being unknowingly filmed can set a precedent for privacy infringement. Moreover, without control over how your comments are edited or used, you risk misrepresentation or unintended endorsement, which could impact your personal image or privacy.\nReasoning used for Story 3\n\u2022 Option 1 (70 tokens): I chose 'Pay speeding fine' because contesting the charge would likely require time, effort, and potentially additional costs that might outweigh the $20 fine. Given that the primary violation (speeding) is admitted to be accurate, contesting the inaccuracies in details may not significantly impact the outcome but could lead to unnecessary legal complexities and personal inconvenience.\n\u2022 Option 2 (68 tokens): I chose 'Contest charge' because the inaccuracies on the citation regarding weather, visibility, time, and location suggest a lack of attention to detail, which could undermine the credibility of the speeding charge. Contesting provides an opportunity to address these discrepancies, possibly leading to a dismissal or reduction of the charge, despite acknowledging the speeding violation."}, {"title": "E.1.2 Cluade3", "content": "Reasoning used for Story 1\n\u2022 Option 1 (67 tokens): I would choose an individual paper to have full control over my work and grade. Relying on others for a major paper is risky, as their performance directly impacts my grade. Working individually allows me to manage my time, ensure the quality of my work, and be graded based on my own effort and understanding of the course material.\n\u2022 Option 2 (68 tokens): Choosing a group paper allows for collaboration, sharing of ideas, and division of work, potentially leading to a higher quality paper. It also provides a safety net, as the risk of a low grade is spread among the team. However, individual work allows for full control and recognition of one's efforts, which may be preferable for some students.\nReasoning used for Story 2\n\u2022 Option 1 (66 tokens): I would sign the release because I genuinely enjoy shopping at the supermarket and have no issue with sharing my honest opinion. The commercial could help promote a local business that I support, and my comments were truthful and positive. As long as the footage is unedited and used appropriately, I see no harm in allowing them to use it.\n\u2022 Option 2 (67 tokens): I would not sign the release because I value my privacy and do not want my unplanned, candid comments used in a commercial without my informed consent. The supermarket should have been transparent about their intentions from the start. I would politely decline and suggest that they find willing participants who can provide a more thoughtful, prepared endorsement of their store."}, {"title": "E.1.3 LLAMA2", "content": "Reasoning used for Story 1\n\u2022 Option 1 (66 tokens): I chose individual paper because I prefer to have full control over my work. This ensures that time available brings a desirability that adds flexibility and superior comprehension with work deliverables, including case study scenarios and essays written collaboratively but individually accounted for a higher academic assessment. This, in turn makes proper citations advised for personal success.\n\u2022 Option 2 (66 tokens): I chose group paper because it allows me to collaborate with others, share ideas and learn from one another. It also distributes the workload and provides an opportunity to develop teamwork skills, which are essential in today's workplace. Additionally, I believe that the diverse perspectives and skills of three people can create a more comprehensive and well-rounded paper."}, {"title": "E.1.4 Mixtral", "content": "Reasoning used for Story 1\n\u2022 Option 1 (68 tokens): I chose individual papers because it provides a better opportunity to demonstrate my own knowledge and abilities, and I believe it will result in a more accurate and fair assessment of my performance. Additionally, working individually allows for greater flexibility in choosing a topic and schedule, as well as the development of important skills such as self-motivation and time management.\n\u2022 Option 2 (68 tokens): I chose group paper because it allows for collaboration and diverse perspectives, which can lead to a higher quality paper. Additionally, group work can help build teamwork and problem-solving skills, which are valuable in the major area of study. The risk of receiving a lower grade is shared among the group, and the potential for learning and growth is increased.\nReasoning used for Story 2\n\u2022 Option 1 (69 tokens): I chose to sign the release because the supermarket in question genuinely offers high-quality meat and produce at reasonable prices, and I stand by my positive review of the store. By signing the release, I allow the supermarket to use my comments in their TV commercial, potentially encouraging others in my community to shop there as well, and supporting a local business.\n\u2022 Option 2 (68 tokens): I chose not to sign the release because the situation was unexpected and unsolicited. The filming was not previously disclosed, and the request for a release was made only after the footage had already been taken. It's important to maintain control over one's image and personal information, and signing a release without proper consideration could potentially lead to unintended consequences.\nReasoning used for Story 3\n\u2022 Option 1 (68 tokens): I chose to pay the speeding fine because the radar evidence for the speed violation is likely accurate, while the other details on the citation are inaccurate but not material to the charge. Contesting the charge may result in additional fines, time, and effort, and the outcome is uncertain. Paying the fine is a quicker and more straightforward resolution.\n\u2022 Option 2 (69 tokens): I recommended contesting the charge because the citation contains inaccuracies regarding weather, visibility, time, and location of the violation. These inaccuracies could potentially weaken the case against the driver, and increase the chances of having the charge reduced or dismissed. It's important to ensure that all information on a citation is accurate before deciding to pay a fine."}, {"title": "E.2 Reasoning chain", "content": "E.2.1 R1: Direct condition\nWhen asking about the perceived agreement, we used the following prompt:\nE.2.2 R2: Simple condition\nWhen asking about the perceived agreement, we used the following prompt:\nE.2.3 R3: CoT condition\nWhen asking about perceived agreement, we let the model generate reasoning first and then generate the answer using the following prompts.\nE.2.4 R4: Reflection condition\nWhen asking for perceived agreement, we let the model generate reasoning first, rethink their reasoning, and generate the answer using the following prompts."}, {"title": "F Detailed result for Study 2", "content": "F.1 Detailed result for H2-1 (Info)\nIn this section, we present the results for H2-1 (Provided information). The Kruskal-Wallis Test results for H2-1 are at the top of Table 4 and Table 12. Additionally, detailed experimental results for H2-1 can be found in Table 13, Table 15, Table 17, and Table 19.\nStory 1 Significant differences are noted for GPT-4 between P1 - P4, P2 P4, and P3 - P4. For Claude3, significant differences are present between P1 - P3, P1 - P4, P2 P3, and P2 - P4. LLaMA2 shows significant differences between P1 - P2, P1 - P3, and P1 - P4. Mixtral exhibits significant differences between P1 - P3, P2 - P3, P2 - P4, and P3 - P4.\nStory 2 For GPT-4, significant differences are noted"}]}