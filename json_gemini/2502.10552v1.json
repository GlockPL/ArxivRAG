{"title": "Synthesis of Dynamic Masks for Information-Theoretic Opacity in Stochastic Systems", "authors": ["Sumukha Udupa", "Chongyang Shi", "Jie Fu"], "abstract": "In this work, we investigate the synthesis of dynamic informa-tion releasing mechanisms, referred to as \"masks\", to minimize information leakage from a stochastic system to an external observer. Specifically, for a stochastic system, an observer aims to infer whether the final state of the system trajectory belongs to a set of secret states. The dynamic mask seeks to regulate sensor information in order to maximize the observer's uncertainty about the final state, a property known as final-state opacity. While existing supervisory control literature on dynamic masks primarily addresses qualitative opacity, we propose quantifying opacity in stochastic systems by conditional entropy, which is a measure of information leakage in information security. We then formulate a constrained optimization problem to synthesize a dynamic mask that maximizes final-state opacity under a total cost constraint on masking. To solve this constrained optimal dynamic mask synthesis problem, we develop a novel primal-dual policy gradient method. Additionally, we present a technique for computing the gradient of conditional entropy with respect to the masking policy parameters, leveraging observable operators in hidden Markov models. To demonstrate the effectiveness of our approach, we apply our method to an illustrative example and a stochastic grid world scenario, showing how our algorithm optimally enforces final-state opacity under cost constraints.", "sections": [{"title": "1 INTRODUCTION", "content": "Networked robotics and IoT Cyber-Physical Systems (CPSs) have become more prolific and capable in recent years, enabling them to perform tasks in increasingly open environments. As a result, these systems pose significant security and privacy risks to users. Recent studies [5, 30] have highlighted concerns about inadvertent leakage of location data through widely used mobile applications, such as Strava and Fitbit, which track users' activity and exercise routines. Despite features to obscure details like trip origins and destinations, they can still to expose sensitive information. This is particularly problematic when the users include individuals in high-security roles, such as law enforcement or government personnel.\nMotivated by these security incidents, this paper studies how to synthesize a dynamic sensor information releasing policy, referred to as a dynamic mask, to enforce information security and opacity of CPSs by strategically selecting which sensors can release information to the public or unauthorized observers. Opacity has been proposed to generalize secrecy, anonymity, privacy, and other confidentiality properties against attacks [8, 14, 29, 36]. A system is opaque if its secret or private behaviors cannot be reliably deduced by an observer with partial observations. In this context, we model a stochastic system as a Markov chain, whose state is partially observable to an external intruder (an observer). The system is to optimize a state-based opacity, measured by the uncertainty of the observer about whether the last state of a finite trajectory is in a set of secret states. The observer has prior knowledge about the secret based on the knowledge of the system dynamics. To optimize opacity, the system can dynamically change the observer's observation function, by masking sensors accessible to the observer. To illustrate the application scenario, consider a self-driving taxi navigating through an urban environment. If an external observer with access to the traffic camera network can infer the car's final state -- whether it has reached a specific destination -- then the privacy of the user is compromised. However, dynamic masking can be leveraged for privacy protection by determining at runtime which images about the vehicles shall be masked from the camera.\nTo this end, we introduce quantitative opacity measured by the conditional entropy of the secret given the observer's partial observations. Since dynamic masking often comes with data management and operational costs, we are interested in addressing the following question: \"How to synthesize a budget-constrained dynamic mask-ing policy that minimizes information leakage with respect to a given secret, in the presence of a well-informed observer?\" Here, a well-informed observer has full knowledge of the system dynamics, the observation function, and the dynamic mask."}, {"title": "1.1 Related Works", "content": "Opacity was first introduced by Mazar\u00e9 [14] in the context of cryptographic protocols and has since been expanded to address the security of various types of secrets. For example, state-based opacity ensures that an observer cannot determine whether a secret state has been reached, while language-based opacity prevents the observer from discerning whether the system's execution belongs to a set of secret trajectories [1, 2, 17]. In system and control literature, opacity has been extensively studied in supervisory control of discrete-event system (DES)s, with a qualitative measure: A system is qualitatively opaque if an observer, with partial observations of the system, cannot infer any secret information with certainty [8, 15, 18, 32, 35]. In an opaque system, a trajectory satisfying the secret property must be observation-equivalent to a trajectory that violates it. For stochastic systems, existing work employs probabilistic opacity, which measures the probability that a secret may be disclosed [12, 17, 19] or quantifies the security level of a system by the probability of generating an opaque trajectory [1, 26]. A comprehensive review of various notions of opacity and their enforcement techniques is provided [8].\nThe enforcement of qualitative opacity in DESs can be broadly classified into two approaches: control design to restrict system behavior or dynamic information releasing to limit the observer's observations. In the first approach, opacity is enforced by designing a supervisory controller [15] or by solving an opacity-enforcement game [7, 27]. However, it may not always be feasible or practical to modify or restrict the system's behavior. For example, in the case of mobile applications unintentionally disclosing personal information, it is not practical to restrict a user's movement or prevent them from visiting certain locations. Therefore, an alternative approach explored in the literature involves altering the public observations relayed to the observer.\nThis alternative line of work is most closely related to our work and focuses on enforcing opacity by restricting/altering the observer's observations. In [3], the authors introduce the concept of a \"mask\" to limit the system's observable outputs, either statically or dynamically. A dynamic mask changes the external observer's observation mapping at each execution step for opacity enforcement. Dynamic masks have been designed to enforce current-state opacity [3] and infinite-state opacity [35]. Alternatively, the work[10, 11] developed selective insertion and/or deletion of output observations for opacity enforcement.\nA dynamic mask can be viewed as an information-flow control mechanism that regulates the information to the observer by enabling or disabling the associated sensors [34]. Closely related to dynamic masking, dynamic sensor activation has been extensively studied in the context of DESs [20, 33], for fault diagnosis [4, 25, 28] and detectability [22]. The work [37] studied the problem of maximum information release while ensuring the opacity of DES. More recently, the authors [31] presented a dynamic obfuscation framework that enables an intended recipient to infer sensitive information while preventing unintended recipients from making similar deductions.\nBuilding on the insight from qualitative opacity enforcement using dynamic masks and dynamic sensor activation, our work distinguishes itself by focusing on quantitative, information-theoretic definitions of opacity [21] and optimal opacity enforcement under cost constraints. Specifically, we use conditional entropy as a measure of information leakage, subject to a generic cost associated with masking. The use of conditional entropy as the measure of information leakage allows us to have a symmetric notion of opacity, i.e., the opacity will be minimal when the observer is consistently confident that the agent either visited or avoided the secret states given an observation. This sets our approach apart from qualitative opacity-enforcement approaches [3, 33].\nIn addition to opacity, differential privacy has been widely studied in CPSs [6]. It protects privacy by adding calibrated noise, balancing privacy and data accuracy. Our approach differs fundamentally from differential privacy in that it uses different measures of information leakage and leverages existing noises in the stochastic environment dynamics and imperfect observation channels for opacity, rather than introducing noise to the system."}, {"title": "1.2 Contributions", "content": "Our contributions to this work are as follows:\n(1) We introduce conditional entropy as a novel measure of information leakage regarding the secret property of a stochastic system, given an observer's partial observations. This metric quantifies the uncertainty about the secret state, providing a framework for analyzing the opacity of a stochastic system or an observer with imperfect observations.\n(2) We propose a new method to compute the approximate gradient of conditional entropy with respect to the parameters of a dynamic masking policy. This method leverages the observable operators in hidden Markov model (HMM)s. Using the computed gradient, we formulate the problem of enforcing maximal opacity with dynamic masking under cost constraints as a constraint optimization problem. We then employ primal-dual gradient-based optimization to compute a locally optimal policy that maximizes opacity while satisfying the cost constraint on the dynamic masks.\n(3) We demonstrate the effectiveness of our proposed algorithm through experiments and share our insight into how the dynamic mask protects the opacity in a stochastic system."}, {"title": "2 PRELIMINARIES AND PROBLEM FORMULATION", "content": "Notations. Given a finite set Z, let \\(D(Z)\\) be the set of all probability distributions over Z. Given a distribution \\(d\\in D(Z)\\), let \\(\\text{Supp}(d) = \\{z \\in Z \\mid d(z) > 0\\}\\) be the support of this distribution. The set \\(Z^T\\) denotes the set of sequences with length T composed of elements from Z, \\(Z^{\\leq T}\\) denotes the set of sequences with length < T and \\(Z^*\\) denotes the set of all finite sequences generated from Z. \\(\\mathbb{R}\\) and \\(\\mathbb{N}\\) represent the real and natural numbers respectively. \\(P\\) represents the probability measure. We denote random variables by capital letters and their realization by lowercase i.e., X and x. The sequence of random variables and their realizations of length T are denoted as \\(X[0:T]\\) and \\(x[0:T]\\) respectively.\nWe consider the interaction between two agents in a stochastic system: an observer and a masking agent. The observer aims to infer, from his partial observation of the system, if the last state of a finite trajectory is a secret. A collection of sensors generates partial"}, {"title": "DEFINITION 2.1 (State-based final state opacity).", "content": "Given the stochastic process \\(M\\pi := \\{S_t, E_t, O_t, t \\geq 0\\}\\), a finite horizon \\(T > 0\\), and a set \\(G\\) of secret states, for any \\(0 \\leq t \\leq T\\), let the random variable \\(W_t\\) be defined by\n\\[W_t = \\mathbb{1}_G(S_t).\\]\nThat is, \\(W_t\\) is the Boolean random variable representing if the state \\(S_t\\) at time \\(t\\) is in the secret set \\(G\\). The quantitative final-state opacity under dynamic mask \\(\\pi\\) a given horizon \\(T\\) is defined by\n\\[H(W_T|O_{0:T}; \\pi) = \\sum_{W_T\\in \\{0,1\\}} \\sum_{O_{0:T} \\in O^T} P_T (W_T, O_{0:T}) \\cdot \\log P_T(W_T | O_{0:T}),\\]\nwhich is the conditional entropy of \\(W_T\\) given observation \\(O_{0:T}\\).\nREMARK 2. Traditional supervisory control employs the notion of qualitative current (or final) state opacity [16, 18], which stipulates that the observation of a trajectory satisfying the secret property (i.e., the final state is in the set G) is observation-equivalent to a trajectory that does not satisfy it (i.e., the final state is not in the set G). Equivalent non-secret behavior ensures that the observer cannot distinguish whether the current (or final) state belongs to the set of secret states. In contrast, quantitative opacity offers a symmetrical measure that quantifies the observer's confidence in identifying whether the agent is in a secret state. This is achieved by comparing the observer's ability to infer the secret before and after analyzing the available observations.\nWe can now formulate our problem as a problem of designing a dynamic masking policy to maximize final state opacity."}, {"title": "PROBLEM 1 (COST-CONSTRAINED OPTIMAL DYNAMIC MASK FOR FINAL-STATE OPACITY).", "content": "Given the HMM \\(M\\), a set of secret states \\(G\\), a finite horizon \\(T\\), compute a dynamic mask \\(\\pi\\) that maximizes the conditional entropy \\(H(W_T | O_{0:T}; \\pi)\\) of the random variable \\(W_T\\) given the observation sequence \\(O_{0:T}\\), while ensuring the total expected discounted cost of masking does not exceed a given threshold \\(\\epsilon\\).\n\\[\\underset{\\pi}{\\text{maximize}} \\quad H(W_T|O_{0:T}; \\pi)\\]\n\\[\\text{subject to} \\quad \\mathbb{E}_{\\pi} \\Big[\\sum_{t=0}^{T-1} \\gamma^t C(S_t, \\Sigma_t, \\Sigma_{t+1})\\Big] \\leq \\epsilon.\\]\nwhere \\(\\gamma \\in [0, 1]\\) is the discounting factor and the expectation is taken with respect to the mask \\(\\pi\\)-induced stochastic process \\(M\\pi\\).\nIn other words, despite knowing the hidden Markov model and the dynamic mask, the observer is maximally uncertain regarding whether the final state is a secret state. While achieving the maximum opacity through dynamic masking, it is also required for the"}, {"title": "3 MAIN RESULTS", "content": "We first show that the planning problem with a dynamic mask can be formulated as the following Markov decision process (MDP)."}, {"title": "Definition 3.1.", "content": "The MDP for the dynamic mask is a tuple\n\\[M = (Z, \\Sigma, O, P, E, C, \\mu_0)\\]\nwhere,\n*   \\(Z = S\\times \\Sigma\\) is the set of states.\n*   \\(\\Sigma\\) is the masking action space, i.e., the set of all possible sensor configurations.\n*   \\(O\\) is the finite set of observations.\n*   \\(P: Z\\times\\Sigma \\to D(Z)\\) is the probabilistic transition function, defined as follows: for every pair of states \\(z = (s, \\sigma)\\) and \\(z' = (s', \\sigma')\\), and a masking action \\(\\sigma^\\circ \\in \\Sigma\\),\n\\[P(z' | Z, \\sigma^\\circ) = \\begin{cases} P(s'|s) & \\text{if } \\sigma' = \\sigma^\\circ \\\\ 0 & \\text{otherwise}. \\end{cases}\\]\n*   \\(E: Z\\to D(O)\\) is the emission function (observation function) that maps a state \\(z = (s, \\sigma)\\) to a distribution over \\(o \\in O\\) given by\n\\[E(z) = E(s, \\sigma).\\]\n*   \\(C: Z\\times\\Sigma \\to \\mathbb{R}\\) is the transition-based cost function that defines the cost of taking a masking action at a state and is given by\n\\[C(z, \\sigma') = C(s, \\sigma, \\sigma'),\\]\nfor every \\(z = (s, \\sigma) \\in Z\\) and \\(\\sigma' \\in \\Sigma\\).\n*   \\(Z_0\\) is the initial state, which is a random variable with the following distribution.\n\\[P(Z_0 = (s, \\sigma_0)) = \\mu_0(s),\\]\nfor each \\(s \\in S\\) and \\(\\sigma_0\\) the initial sensor configuration. And with a slight abuse of notation, we use \\(\\mu_0(z)\\) to represent the probability that \\(z\\) is the initial state."}, {"title": "3.1.", "content": "The value function in the above MDP, given a dynamic mask \\(\\pi\\), is \\(V:Z\\to\\mathbb{R}\\), which is defined as follows: for any \\(z\\),\n\\[V^{\\pi} (z) = \\mathbb{E}_{\\pi} \\Big[\\sum_{k=0}^{T} \\gamma^k C(Z_k, \\pi(Z_k) \\mid Z_0 = z\\Big],\\]\nwhere \\(\\mathbb{E}_{\\pi}\\) is the expectation with respect to the probability distribution induced by the dynamic mask \\(\\pi\\) from \\(M\\). And \\(Z_k\\) is the \\(k\\)-th state in the Markov chain induced by the dynamic mask from \\(M\\)."}, {"title": "3.2 Primal-Dual Policy Gradient", "content": "For notational convenience, we introduce an index for the finite-state set \\(Z = \\{1,..., N\\}\\), where \\(N = |S \\times \\Sigma|\\). Consider a set of parameterized dynamic masks \\(\\{\\pi_\\theta | \\theta \\in \\Theta\\}\\) where \\(\\Theta\\) is a finite-dimensional parameter space. For any dynamic mask \\(\\pi_\\theta\\) parameterized by \\(\\theta\\), the Markov chain induced by \\(\\pi_\\theta\\) from \\(M\\) is denoted \\(M_{\\pi_\\theta} : \\{Z_t, O_t, t \\geq 0\\}\\). The Markov chain follows the transition probability \\(P_\\theta\\) such that the probability of transitioning from state \\(z\\) to \\(z'\\) in one step is defined by\n\\[P_\\theta(z, z') = \\sum_{\\sigma' \\in \\Sigma} P(z' | Z, \\sigma')\\pi_\\theta(\\sigma' | z).\\]\nThus, Problem 1 can now be formally formulated as a constrained optimization problem under the above parametrization as follows:\n\\[\\underset{\\theta}{\\text{maximize}} \\quad H(W_T | Y; \\theta)\\]\n\\[\\text{subject to : } \\quad V (\\mu_0, \\theta) \\leq \\epsilon.\\]\nwhere \\(Y = O_{0:T}\\) is the finite sequence of observations. The value function \\(V (\\mu_0, \\theta)\\) is computed by evaluating the dynamic mask \\(\\pi_\\theta\\) given the initial state distribution \\(\\mu_0\\), i.e., \\(V (\\mu_0, \\theta) := V_{\\pi_\\theta} (\\mu_0)\\), and \\(H(W_T | Y; \\theta) := H(W_T | Y; M_{\\pi_\\theta})\\).\nWe then formulate this problem with inequality constraints into an unconstrained optimization problem by introducing the following Lagrangian \\(L(\\theta, \\lambda)\\), and thereby incorporating the constraint into the objective function:\n\\[L(\\theta, \\lambda) = H(W_T | Y; \\theta) + \\lambda(\\epsilon - V (\\mu_0, \\theta)),\\]\nwhere \\(\\lambda \\geq 0\\) is the Lagrange multiplier.\nWith this, we can now express the original constrained optimization problem as the following max-min problem:\n\\[\\underset{\\theta}{\\text{max}} \\underset{\\lambda\\geq 0}{\\text{min}} \\quad L(\\theta, \\lambda).\\]\nWe use the primal-dual gradient descent-ascent algorithm such that in each iteration \\(k\\), we have,\n\\[\\theta_{k+1} = \\theta_k + \\eta \\nabla_\\theta L(\\theta, \\lambda),\\]\n\\[\\lambda_{k+1} = \\lambda_k - \\kappa (\\epsilon - V (\\mu_0, \\theta)),\\]\nwhere \\(\\eta > 0\\) and \\(\\kappa > 0\\) are step sizes. The gradient of the Lagrangian function with respect to \\(\\theta\\) is\n\\[\\nabla_\\theta L(\\theta, \\lambda) = \\nabla_\\theta H(W_T | Y;\\theta) - \\lambda \\nabla_\\theta V (\\mu_0, \\theta).\\]\nThe gradient of the value function can be computed using the standard sampling-based policy gradient algorithms like REINFORCE [24]. But, to obtain the gradient of the Lagrangian function, we also need to compute the gradient of the conditional entropy"}, {"title": "3.3 Gradient Computation for the Conditional Entropy", "content": "Consider a dynamic mask \\(\\pi_\\theta\\), from the observer's perspective the stochastic process is an HMM \\(M = (Z, O, P_\\theta, E, \\mu_0)\\), where \\(Z\\) is the state space, \\(O\\) is the observation space, \\(P_\\theta\\) is the transition function induced by \\(\\pi_\\theta\\) ((3)) and \\(E = \\{e_i, i \\in Z\\}\\) is the emission probability distribution where \\(e_i(o) = E(s_i, \\sigma_i)\\) for \\(z_i = (s_i, \\sigma_i)\\).\nWe can now re-write the conditional entropy of \\(W_T\\) given an observation sequence \\(Y\\) as follows:\n\\[H(W_T | Y;\\theta) = - \\sum_{Y\\in O^T} \\sum_{W_T\\in \\{0,1\\}} P_\\theta (W_T, y) \\log P_\\theta(w_t|y).\\]\nNow, we compute the gradient of \\(H(W_T | Y;\\theta)\\) with respect to the dynamic mask parameter \\(\\theta\\). Using the log-derivative trick \\(\\nabla_\\theta P_\\theta(y) = P_\\theta(y) \\nabla_\\theta \\log P_\\theta(y)\\) and the property of conditional probability, we have\n\\[\\nabla H(W_T | Y; \\theta) =\\\\\n- \\sum_{Y\\in O^T} \\sum_{W_T\\in \\{0,1\\}} \\Big[ P_\\theta(w_T, y) \\log P_\\theta(w_t | y)\\\\\n+ P_\\theta(w_T, y) \\nabla_\\theta \\log P_\\theta (W_T | y)\\Big]\\]\n\\[=- \\sum_{Y\\in O^T} \\sum_{W_T\\in \\{0,1\\}} \\Big[ P_\\theta(y) \\nabla_\\theta \\frac{P_\\theta(w_T | y)}{P_\\theta(y)} \\log P_\\theta(w_t | y)\\\\\n+ P_\\theta(w_Ty) \\frac{\\nabla_\\theta P_\\theta (W_T | y)}{P_\\theta(y)} \\log P_\\theta (W_T | y) \\Big]\\]\n\\[=- \\sum_{Y\\in O^T} P_\\theta(y) \\sum_{W_T\\in \\{0,1\\}} \\Big[ \\log P_\\theta(W_T | y) \\frac{\\nabla_\\theta P_\\theta(W_T | y)}{P_\\theta(y)}\\\\\n+ P_\\theta(w_T | y) \\log P_\\theta (W_T | y) \\frac{\\nabla_\\theta P_\\theta(y)}{P_\\theta(y)} + \\frac{\\nabla_\\theta P_\\theta(w_T | y)}{\\log 2} \\Big].\\]\nWe now present a novel method to compute \\(P_\\theta(y)\\), \\(P_\\theta(W_T | y)\\), and the gradients \\(\\nabla_\\theta P_\\theta(y)\\), \\(\\nabla_\\theta P_\\theta (W_T | y)\\) for \\(w_T \\in \\{0, 1\\}\\) using observable operators.\nWe first introduce the matrix notation for HMMs. We can write the reversed state transition probability matrix as \\(T_\\theta \\in \\mathbb{R}^{N\\times N}\\), with\n\\[T_\\theta [i, j] = P_\\theta (Z_{t+1} = i | Z_t = j),\\]\nand the observation probability matrix \\(B \\in \\mathbb{R}^{M\\times N}\\), where \\(M\\) is the number of all possible observations, with\n\\[B[i, j] = E(O_t = i | Z_t = j),\\]\nwhere \\(O_t\\) and \\(Z_t\\) represent the observation and state at time \\(t\\). Thus, we have the probability distribution of any sequence of states and observations fully characterized by \\(T_\\theta\\), \\(B\\), and \\(\\mu_0\\)."}, {"title": "Definition 3.3 (Observable operator([9])).", "content": "Given the HMM \\(M_\\theta\\), for any observation at time \\(t\\) given as \\(o_t\\), the observable operator"}, {"title": "PROPOSITION 3.4.", "content": "Given the initial distribution \\(\\mu_0\\), the joint probability of observing \\(o\\) and arriving at next hidden state \\(i\\) is\n\\[P_\\theta (O_1 = o, Z_2 = i) = \\mathbb{1}_i^T A_o \\mu_0,\\]\nwhere \\(\\mathbb{1}_i\\) denotes an \\(N \\times 1\\) one-hot vector with the \\(i\\)-th entry being assigned value 1.\nPROOF. Consider the right-hand side and from the definition of the observable operators, we have,\n\\[\\begin{aligned} &\\mathbb{1}_i^T A_{o_1} \\mu_0 = \\left[\\begin{array}{ccccccc} 0 & ... & 1 & ... & 0 \\end{array}\\right] \\left[\\begin{array}{cccc} T_\\theta[1, 1]E(o_1|1) & ... & T_\\theta[1, N]E(o_1|N) \\\\ T_\\theta[2, 1]E(o_1|1) & ... & T_\\theta[2, N]E(o_1|N) \\\\ \\vdots & & \\vdots \\\\ T_\\theta[N, 1]E(o_1|1) & ... & T_\\theta[N, N]E(o_1|N) \\end{array}\\right] \\left[\\begin{array}{c} \\mu_0(1) \\\\ \\mu_0(2) \\\\ \\vdots \\\\ \\mu_0(N) \\end{array}\\right] \\\\ & = \\Big[T_\\theta[i, 1]E(o_1|1) \\qquad ... \\qquad T_\\theta[i, N]E(o_1|N)\\Big] \\left[\\begin{array}{c} \\mu_0(1) \\\\ \\mu_0(2) \\\\ \\vdots \\\\ \\mu_0(N) \\end{array}\\right] \\\\ & = T_\\theta[i, 1]E(o_1|1)\\mu_0(1) + ... + T_\\theta[i, N]E(o_1|N)\\mu_0(N) \\\\ & = \\sum_j P_\\theta(o_1, i|j)\\mu_0(j) = P_\\theta(O = o_1, Z_2 = i). \\end{aligned}\\]"}, {"title": "PROPOSITION 3.5.", "content": "The joint distribution of a sequence of observations and the arrived hidden state is given as\n\\[P_\\theta(o_{1:t}, Z_{t+1} = i) = \\mathbb{1}_i^T A_{o_t} ... A_{o_1} \\mu_0.\\]\nPROOF. By induction. From Prop. 3.4, we have for a single observation case that \\(P_\\theta (o_1, Z_2 = i) = \\mathbb{1}_i^T A_{o_1} \\mu_0\\). Thus, the statement holds for the base case for \\(t = 1\\).\nAssume that the statement holds for \\(t = p\\), \\(p > 1\\), i.e.,\n\\[P_\\theta (o_{1:p}, Z_{p+1} = i) = \\mathbb{1}_i^T A_{o_p} ... A_{o_1} \\mu_0.\\]\nThen, in the case of \\(t = p + 1\\), by the definition of joint probabilities we have,\n\\[P_\\theta (o_{1:p+1}, Z_{p+2} = i) = \\sum_{j=1}^N P_\\theta (o_{1:p+1}, Z_{p+1} = j, Z_{p+2} = i).\\]\nApplying the chain rule, we have,\n\\[P_\\theta (o_{1:p+1}, Z_{p+2} = i) = \\sum_{j=1}^N T_\\theta[i, j] \\cdot E(o_{p+1} | j) \\cdot P_\\theta(o_{1:p}, j).\\]\nTherefore, according to the induction hypothesis, we get\n\\[\\begin{aligned} P_\\theta(o_{1:p+1}, Z_{p+2} = i) & = \\sum_{j=1}^N T_\\theta[i, j] \\cdot E(o_{p+1} | j) \\mathbb{1}_j^T A_{o_p} ... A_{o_1} \\mu_0 \\\\ & = \\sum_{j=1}^N \\Big[ T_\\theta[i, j] \\cdot E(o_{p+1} | j) \\mathbb{1}_j^T \\Big] \\Big[ A_{o_p} ... A_{o_1} \\mu_0 \\Big] \\\\ & = \\sum_{j=1}^N \\Big[ A_{o_{p+1}}[i, j] \\Big] \\Big[ A_{o_p} ... A_{o_1} \\mu_0 \\Big] \\\\ & = \\mathbb{1}_i^T A_{o_{p+1}} A_{o_p} ... A_{o_1} \\mu_0. \\end{aligned}\\]\nThe last step is because the step before it represents the summation of the multiplications of the [i, j]-th element of the matrix \\(A_{o_{p+1}}\\) with the j-th element of vector \\(A_{o_p} ... A_{o_1} \\mu_0\\). This summation equals"}, {"title": "4 EXPERIMENTAL VALIDATION", "content": "This section shows the effectiveness of the algorithm with two sets of experiments. The computations are conducted using Python and PyTorch on a Windows 10 machine with an Intel Core i7 CPU @ 3.2 GHz, 32 GB RAM, and 8 GB RTX 3060 GPU."}, {"title": "4.1 The illustrative example", "content": "Example 4.1 (Part III). We now implement the primal-dual policy gradient algorithm on the illustrative example, Example 2.2. It is recalled that the secret states set is \\{\\(s_4, s_6\\)\\}. We employ soft-max policy parametrization, i.e.,\n\\[\\pi_\\theta(\\sigma|s) = \\frac{\\exp(\\theta_{s,\\sigma})}{\\sum_{\\sigma'\\in\\Sigma} \\exp(\\theta_{s, \\sigma'})},\\\\]\nwhere \\(\\theta \\in \\mathbb{R}^{|S|\\times 2|\\Sigma|}\\) is the policy parameter vector. The soft-max policy has essential analytical properties including completeness and differentiability.\nWe implemented the primal-dual policy gradient algorithm described above for 1000 iterations and used 1500 sample trajectories of length \\(T = 2\\) for each iteration. First, we computed the approximate conditional entropy with no masking and then with masking for two different cost budgets \\(\\epsilon = 60\\) and \\(\\epsilon = 20\\). The approximate conditional entropy and the approximate gradient for the conditional entropy were computed based on (18) and (19).\nFigure 3a shows the conditional entropy for varying budgets of masking costs (\\(\\epsilon\\)). Recall that a higher entropy implies a higher level of uncertainty. For comparison, we first computed the entropy of the prior distribution about the secret \\(W_T\\), which is 0.9172. This value is the upper bound on the conditional entropy given observations. Figure 3a shows the approximate conditional entropy for no masking drawn in black. It is observed that the conditional entropy is approximately 0.0895. Thus, without dynamic masking, the observer on average is certain whether or not a secret state is reached. In Figure 3a for \\(\\epsilon = 60\\), the algorithm converges after about 65 iterations to an approximate conditional entropy of 0.7132. Likewise, for \\(\\epsilon = 20\\), the algorithm converges at about 75 iterations to an approximate conditional entropy of 0.658."}, {"title": "4.2 Case Study: Stochastic gridworld", "content": "Consider a secure pharmaceutical research facility responsible for testing and ensuring the safety of various drug batches before they are released to the market represented by the \\(6 \\times 6\\) grid world as shown in Figure 4.\nIn this scenario", "locations": "the chemical analysis lab in cell 23, the biological activity testing chamber in cell 9, or the human testing and analysis lab in cell 20. The robot also must steer clear of the bio-hazard disposal units in cells 1, 13, 15, and 35. The goal states of the robot, cells 9, 20, 23, are also the secret states. Cells 17 and 19 are walls. A competitor company intends to know the progress on the breakthrough drugs being designed by the research facility. Thus, it is a passive observer (P2) that observes the robot's behavior using a deployed sensor network, specifically, observes the final state of the robot. We have a masking agent (P1) in the environment that aims to maximize the final state opacity to P2.\nEnvironment dynamics. The robot can move in one of four compass directions (North, South, East, West) within the grid world. Every action of the robot carries a degree of uncertainty. That is, when the robot moves in a specific direction, it can reach the intended cell with a probability \\(p\\) and reach the unintended cells (i.e., if north is the intended cell, the cells in east and west are the unintended cells) with a probability \\((1"}]}