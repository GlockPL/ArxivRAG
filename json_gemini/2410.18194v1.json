{"title": "ZIP-FIT: EMBEDDING-FREE DATA SELECTION VIA COMPRESSION-BASED ALIGNMENT", "authors": ["Elyas Obbad", "Iddah Mlauzi", "Brando Miranda", "Rylan Schaeffer", "Kamal Obbad", "Suhana Bedi", "Sanmi Koyejo"], "abstract": "Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution. Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation. Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise. We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution. Our key insight is that compression-based similarity captures both syntactic and structural patterns relevant to the target task, enabling more precise selection of truly task-relevant data. In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4. Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1% faster than baselines, demonstrating that better task alignment leads to more efficient learning. In addition, ZIP-FIT performs selection up to 65.8% faster than DSIR and two orders of magnitude faster than D4. Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data. Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment. By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Choosing training data is crucial for the performance of language models (LMs) in both general-purpose and domain-specific applications (Brown et al., 2020; Gururangan et al., 2020; Hoffmann et al., 2022). To date, most research on data curation has focused on creating diverse pre-training datasets to enhance model performance across a wide range of tasks (Sorscher et al., 2022; Xie et al., 2023b; Tirumala et al., 2023; Abbas et al., 2023; Xie et al., 2023a; Lee et al., 2023; Wettig et al., 2024; Penedo et al., 2024; Li et al., 2024; Sachdeva et al., 2024), and while these methods have been demonstrated to work well for general pre-training, they fall short in domain-specific fine-tuning, where data relevance is crucial. This raise a key question: How should we, in a general purpose manner, effectively select fine-tuning data for a domain-specific target task?\nOne approach is to train binary classifiers to identify relevant data. For example, a mathematical language model called DeepSeekMath (Shao et al., 2024) utilized OpenWebMath (Paster et al., 2023), a compilation of high-quality mathematical texts, to train a FastText classifier to retrieve analogous texts from the Web (Bojanowski et al., 2017). Although effective, this method relies on the availability of large and well-annotated data sets, something that is often missing in niche tasks where"}, {"title": "2 ZIP-FIT: AN EMBEDDING-FREE DATA SELECTION ALGORITHM VIA COMPRESSION-BASED ALIGNMENT FOR LM FINE-TUNING", "content": "Before introducing ZIP-FIT, it is essential to understand the desired attributes of effective data selection algorithms. Ideally, such algorithms should be performant, computationally economical, fast, scalable, and designed to improve the efficiency of model training. These characteristics ensure that the data filtering process can be applied broadly and effectively in various machine learning"}, {"title": "2.1 BACKGROUND", "content": "gzip compression: uses two main techniques for compression, LZ77 and Huffman coding. Together, these methods compress sequences by exploiting repeated patterns in the data. LZ77 works by identifying repeated substrings and replacing them with references to their earlier occurrences. Huffman coding further compresses the data by assigning shorter binary codes to more frequent symbols, optimizing the overall length of the compressed text. For more details, see Appendix A.\nAutoFormalization (AF): refers to the task of translating natural language mathematical statements into a formal mathematical programming language, like Lean4 Moura et al. (2015). This process requires precise understanding and representation of mathematical formal syntax, making the selection of well-aligned training data crucial for effective model training."}, {"title": "2.2 ZIP-FIT ALGORITHM", "content": "Setup: Given a set of examples \\({x_1,x_2,...,x_t}\\) from a target distribution \\(p\\) and a large source dataset \\({x_1,x_2,...,x_N}\\) from an arbitrary distribution \\(q\\), ZIP-FIT aims to select a subset of \\(k\\) examples (where \\(k \\ll N\\)) from \\(q\\). The selected subset is used for model training, in order to improve performance for tasks associated with \\(p\\). This approach is intended to maximize the efficacy and efficiency of model training by focusing on the most relevant data samples.\nMethod: ZIP-FIT uses gzip compression as a metric to measure the alignment of each example in \\(q\\) with the target \\(p\\), focusing on capturing patterns and redundancies.\nTo address the challenge of selecting highly aligned data, we propose the ZIP-FIT algorithm:"}, {"title": "3 HIGHER ALIGNMENT INTERVENTIONALLY ACHIEVES BETTER MODEL PERFORMANCE", "content": "Experiment: We validate compression as an alignment metric by evaluating the impact of a model fine-tuned on more ZIP-FIT-aligned data with a target task and the corresponding cross-entropy (CE) loss. We chose ProofNet (test) as the target benchmark and then fine-tuned GPT-2 Radford et al. (2019) and Mistral7B Jiang et al. (2023a) ) LMs on datasets with varying ZIP-FIT alignment.\nResults: Figure 3 shows a strong negative correlation (R2 of 0.90) between gzip alignment scores and CE loss for GPT-2 and 0.75 for Mistral7B. This implies that data alignment plays a crucial role"}, {"title": "4 HIGHER ALIGNMENT LEADS TO MORE EFFICIENT TRAINING", "content": "Experiment: We fine-tuned GPT-2 (124M) and Mistral7B for the AutoFormalization task using different datasets scored with ZIP-FIT alignment. We used ProofNet (test) for the evaluation. The curves represent different datasets with varying alignment to the target domain (ProofNet validation).\nResults: More aligned data reduces CE loss quickest, as shown by the steep decline for high-alignment datasets. This is most evident as ProofNet (validation). Less aligned data require significantly more tokens to achieve similar performance. This demonstrates that targeted data selection with ZIP-FIT accelerates fine-tuning and improves performance, reducing computational costs."}, {"title": "5 COMPARATIVE EVALUATION OF ZIP-FIT FOR EFFICIENT FINE-TUNING", "content": "We evaluate ZIP-FIT on two domain-specific tasks: Autoformalization and Python Code Generation. Our goal is to show ZIP-FIT's data selection leads to superior fine-tuning."}, {"title": "5.1 AUTOFORMALIZATION", "content": "Experiment: Our source dataset comprised approximately 185,000 sequences from LeanDojo, Proof-Pile 2, C4, and WikiText Yang et al. (2023); Azerbayev et al. (2024). For details, refer to Appendix A.2. Alignment was computed using ZIP-FIT, DSIR and D4 with the ProofNet's validation split (our target distribution). For a fair comparison, we did not modify how any of the methods rank sequences. To compare each method, we selected the n sequences ranked highest for several values of n (353k, 695k tokens, etc.). For each selected data set at each value of n we fine-tune InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B (Ying et al. (2024); Team et al. (2024)). Performance was evaluated using the CE loss ProofNet's test split."}, {"title": "5.2 CODE GENERATION", "content": "Experiment: We conducted code generation experiments using ZIP-FIT, DSIR, and D4 to select data from a mix of sources: MBPP (Austin et al. (2021), Python docstrings, Proof-Pile 2, C4, WikiText. The latter two are included to study whether the data selection methods considered are robust to misaligned data. For details, refer to Appendix A.3. The datasets were utilized to fine-tune both CodeGemma-2B and Gemma2-2B models, with the focus on translating function signatures and docstrings into executable Python code. For the selection process, we used HumanEval for validation and a separate hold-out portion for final testing. For a fair comparison, we did not modify how any of the methods rank sequences. To compare each method, we selected the n sequences ranked highest for several values of n (800k, 930k, 1.6M tokens, etc.).\nResults: Across all tested n values, ZIP-FIT consistently outperformed DSIR and D4 in reducing cross-entropy loss, demonstrating faster and more effective fine-tuning. In particular, the CodeGemma-2B model, already optimized for code-related tasks, showed the most improvements with ZIP-FIT, confirming its ability to select highly relevant and beneficial training data. Rapid loss reduction under ZIP-FIT emphasizes its efficiency, especially noted in its 25% faster data processing compared to DSIR. Most notably, the flattening of the DSIR and D4 curves indicate diminishing returns, suggesting that additional tokens would not achieve the performance of ZIP-FIT.In"}, {"title": "6 IMPACT OF DATA MISALIGNMENT ON MODEL PERFORMANCE", "content": "Existing research showed that data alignment plays a critical role in improving model performance and learning efficiency for downstream tasks. In this section, we explore how misalignment in data can affect model performance and how ZIP-FIT addresses this issue with data selection.\nExperiment: We fine-tuned the Mistral7B model on the same source dataset we used for the Aut-oFormalization experiment (see Appendix 5.1), filtering data with ZIP-FIT at different alignment thresholds (>0.1, >0.2, >0.3). Each threshold creates a progressively more aligned dataset, where the >0.3 dataset is the most aligned, and the >0.2 dataset is a superset of the >0.3 dataset, including less aligned data. Similarly, the >0.1 dataset is a superset of both >0.2 and >0.3. Figure 6 shows cross-entropy test loss (y-axis) versus the number of training tokens (x-axis). The dashed line marks the pretrained Mistral7B baseline.\nResults: ZIP-FIT selected data achieves lower cross-entropy loss faster than training on all data, showing improved performance and efficiency. Higher alignment thresholds result in a steeper loss reduction, confirming that filtering out misaligned data enhances fine-tuning.Misalignment in train-ing data can introduce noise and irrelevant patterns, which typically require more training data and computational resources to overcome. By applying higher alignment thresholds, ZIP-FIT ensures that only the most relevant and helpful examples are used for training. This targeted selection leads to a more efficient learning process as evidenced by the sharper decline in cross-entropy loss for higher alignment thresholds. Such efficiency is crucial in scenarios where computational resources are limited or costly.\nPractical Considerations: For practitioners, these results suggest that investing in better data cu-ration and alignment tools can significantly cut down the cost and time of model training without compromising performance. It also highlights the potential pitfalls of using large, uncurated datasets that might slow down the learning process or lead to poorer generalization on specific tasks.\nFuture Directions: Further research could explore adaptive alignment thresholds based on real-time validation performance, potentially automating the selection process to optimize both speed and accuracy during training.\nThese results further validate the empirical performance gains and computational efficiency achieved by ZIP-FIT, as outlined in our contributions. By filtering out misaligned data, ZIP-FIT acceler-ates fine-tuning and reduces computational overhead, confirming its utility in low-resource settings."}, {"title": "7 RELATED WORKS", "content": "Curating pre-training data for Language Models often involves using classifiers to filter high-quality data from large corpora like Common Crawl, as done for models like GPT-3 and PaLM2 (Brown et al., 2020; Google, 2023; Shao et al., 2024). While effective, this process requires significant computational resources and large volumes of curated data. In contrast, ZIP-FIT efficiently selects relevant data without relying on external models, making it especially useful in data-scarce environments.\nDeduplication techniques, such as SemDeDup (Abbas et al., 2023) and D4 (Tirumala et al., 2023), improve data efficiency by removing duplicate or semantically similar examples using embedding-based clustering. However, these methods are computationally expensive and not tuned to the target task. ZIP-FIT is embedding-free and task-aware, making it both scalable and more effective at selecting relevant data.\nMixture weights are essential when drawing from multiple domains, as they significantly influence the performance of language models (Du et al., 2022; Xie et al., 2023b). While Domain Reweighting with Minimax Optimization DoReMi) (Xie et al., 2023a) proposes a robust domain-level reweight-ing strategy suitable for diverse target distributions. DoReMi is not designed for example-level data selection, as it primarily focuses on domain-level reweighting. Adapting it to select individual data points for specific target distributions would require substantial modifications to its foundational al-"}, {"title": "8 LIMITATIONS", "content": "While ZIP-FIT provides a computationally efficient method for data selection, it has several limitations. First, the gzip compression-based alignment may not fully capture nuanced semantic relationships that dense representations can, potentially affecting its effectiveness for complex domains like natural language understanding, where paraphrasing is important. Second, ZIP-FIT's reliance on gzip means that its performance could vary depending on the nature of the textual data, particularly in highly diverse datasets where compression gains are less apparent."}, {"title": "9 DISCUSSION AND FUTURE WORK", "content": "ZIP-FIT introduces an efficient, embedding-free approach for data selection in language model fine-tuning. By leveraging gzip compression to capture redundancies in data, ZIP-FIT enables the alignment of large-scale datasets with a target domain without the computational burden of neural embeddings. Our results show that using compression-based alignment leads to faster con-"}, {"title": "10 CONCLUSION", "content": "In this work, we introduced ZIP-FIT, an efficient and scalable data selection method that leverages gzip-based compression to enhance the downstream performance of language models for domain-specific tasks. Our experiments demonstrate that ZIP-FIT not only accelerates the fine-tuning process but also significantly improves downstream performance by aligning training data more closely with target tasks. By comparing against established methods like DSIR and D4, ZIP-FIT proved superior in selecting highly-aligned data, especially in complex tasks such as Autoformalization and code generation. This methodology sets a new standard for resource-efficient and effective data selection for model training, providing a step in understanding the choice of training data for downstream transfer in LMs."}, {"title": "A GZIP COMPRESSION DETAILS", "content": "gzip is a lossless data compression algorithm that combines two primary techniques: LZ77 compression and Huffman coding. Here, we provide additional technical details on how gzip works.\nLZ77 Compression: LZ77 works by identifying repeated substrings in the input text and replacing them with backward references. Mathematically, LZ77 can be described as follows:\nGiven an input sequence \\(S = s_1, s_2, ..., s_n\\), the algorithm searches for the longest prefix of the remaining sequence \\(S' = s_i, s_{i+1}, ..., s_n\\) that matches a substring within a predefined window of previous characters. If a match is found, it is replaced by a tuple \\((d, l, c)\\), where:\n\u2022 d is the distance from the current position to the start of the matching substring,\n\u2022 l is the length of the matching substring, and\n\u2022 c is the character following the match (if any).\nFor example, the substring \\(s_i, s_{i+1}, ..., s_{i+l-1}\\) can be replaced by the tuple \\((d, l, c)\\), thereby reducing redundancy in the data.\nHuffman Coding: After applying LZ77, gzip employs Huffman coding to further reduce the size of the compressed data. Huffman coding assigns variable-length codes to symbols based on their frequency of occurrence, with shorter codes assigned to more frequent symbols.\nThe expected length \\(L(X)\\) of the Huffman code for a sequence of symbols \\(X = x_1, x_2, ..., x_n\\) is calculated as:\n\\[\nL(X) = \\sum_{i=1}^{n} p(x_i) \\cdot len(C(x_i)),\n\\]\nwhere:\n\u2022 \\(p(x_i)\\) is the probability of symbol \\(x_i\\),\n\u2022 \\(len(C(x_i))\\) is the length of the Huffman code for \\(x_i\\).\nThis further minimizes the size of the compressed data by leveraging the statistical properties of the input.\nCombined gzip Compression: The total compressed size \\(C(S)\\) after applying both LZ77 and Huffman coding can be approximated as the sum of the lengths of the backward references and the Huffman-coded symbols:\n\\[\nC(S) = \\sum_{(d, l, c)} len(d, l, c) + \\sum_{i=1}^{n} len(C(x_i)).\n\\]\nNormalized Compression Distance (NCD): gzip's effectiveness in data selection stems from its ability to measure the alignment between two sequences A and B based on how efficiently they compress together. The Normalized Compression Distance (NCD) is given by:\n\\[\nNCD(A, B) = \\frac{C(A+B) - \\min(C(A), C(B))}{\\max(C(A), C(B))},\n\\]\nwhere \\(C(A)\\) and \\(C(B))\\) are the compressed lengths of sequences A and B, and \\(C(A + B)\\) is the length of the compressed concatenation of both sequences. A lower NCD indicates greater alignment between the sequences.\nA.1 WHY USE COMPRESSION?\nCompression algorithms, such as gzip, provide a computationally efficient way to detect patterns and minimize redundancy in data."}, {"title": "Limitations of n-grams", "content": "Many traditional methods, including hashed n-grams, focus on capturing immediate textual correlations by simplifying text into discrete, fixed-size buckets. Although these techniques are computationally efficient, they may not adequately capture syntactic or structural relationships within the data. Additionally, the introduce noise due to collisions during hashing."}, {"title": "Challenges with Neural Embeddings", "content": "Neural embeddings offer a powerful tool for capturing semantic relationships, but they come with significant computational costs. These embeddings are typically pre-trained on large corpora and fine-tuned for specific tasks, which requires substantial re-sources. Given the scalability challenges of embedding-based methods, we conjecture that a simpler method like compression can provide a more scalable and resource-efficient alternative.\nWe hypothesize that compression \u2013 in this case gzip, but perhaps a different compression algorithm -serves as a strong proxy for capturing syntactic and structural relationships in textual sequences. gzip's ability to compress data based on redundancy minimization can be leveraged as a metric to align text with a target distribution."}, {"title": "A.2 COMPOSITION OF THE SOURCE DATASET FOR AUTOFORMALIZATION", "content": "The source dataset for the AutoFormalization task was compiled from a variety of datasets to ensure a diverse mix of mathematical, general textual, and code-related content. Below are the details of the datasets included:\n\u2022 UDACA/AF: 4,300 samples from informal formalization statements.\n\u2022 C4: 10,000 samples from the clean crawl of the internet, ensuring a broad linguistic variety.\n\u2022 LeanDojo: 10,000 samples from task-oriented proofs and tactics.\n\u2022 LeanDojo Informalized: 10,000 samples combining traced tactics with informal descrip-tions, aiming to bridge formal reasoning and natural language.\n\u2022 UDACA/AF-split: 10,000 samples, a variant of the UDACA/AF dataset with split annota-tions.\n\u2022 WikiText: 10,000 samples from a collection of professionally curated articles, providing a rich linguistic framework.\n\u2022 Algebraic Stack: Samples from various subsets of mathematical and programming lan-guages, capped at 10,000 samples per subset or fewer if the total subset size was under this threshold.\nEach dataset was selected to complement the others by covering different aspects of language use, from technical to informal, ensuring the model's exposure to a wide range of linguistic structures and contents. The total dataset size aggregated to approximately 185,000 sequences, which were then subjected to alignment scoring and further processing for model training."}, {"title": "A.3 COMPOSITION OF THE SOURCE DATASET FOR CODE GENERATION", "content": "The source dataset for the Code Generation task was assembled from various data sources to pro-vide a diverse range of coding and natural language contexts. Below are the details of the datasets included:\n\u2022 MBPP (Google Research): A total of 964 samples focusing on Python coding challenges.\n\u2022 Python Code Instructions (18k Alpaca): 5,000 sequences providing natural language prompts for Python code, fostering a practical approach to code generation.\n\u2022 Python Docstrings (Calum/The Stack): 5,000 sequences each of Python function doc-strings integrating detailed natural language documentation of python functions.\n\u2022 Python Docstrings (Calum/The Stack): 5,000 sequences each of Python function code bodies, integrating raw python code without documentation.\n\u2022 C4 (AllenAI): 10,000 samples from a clean web crawl.\n\u2022 WikiText: 10,000 samples from a collection of curated articles, providing rich natural language training material."}, {"title": "A.4 HYPERPARAMETERS FOR MODEL FINE-TUNING", "content": "All models in our experiments were fine-tuned with the following unified setup, aimed at ensuring a consistent evaluation across different models and data selection strategies.\nModels and Tokenizer: The fine-tuning was performed using the following models:\n\u2022 InterLM-Math-Plus-1.8B\n\u2022 Gemma2-2B\n\u2022 Mistral7B\nTraining Settings: The key hyperparameters used across all models are as follows:\n\u2022 Block Size: 1024 tokens\n\u2022 Learning Rate: 7.5 \u00d7 10-7\n\u2022 Batch Size: 4 (per device)\n\u2022 Number of Epochs: 1\n\u2022 Weight Decay: 0.01\n\u2022 Maximum Gradient Norm: 1.0\nTraining was facilitated using the Trainer class from Hugging Face's Transformers library, with the Accelerate library handling model parallelism to efficiently utilize available computational re-sources.\nEvaluation Metrics: For model evaluation, we employed:\n\u2022 Cross-Entropy Loss at the end of training to measure the effectiveness of the fine-tuning.\nFine-tuning was performed under controlled conditions to ensure fair comparison between data se-lected by ZIP-FIT, DSIR, and manual curation methods. The effectiveness of each method was assessed based on how the models performed on the ProofNet and HumanEval.\nData Handling and Logging: All logs, model checkpoints, and tokenizer settings were system-atically saved in designated directories for thorough analysis post-experiment\nThis comprehensive and standardized approach to fine-tuning ensures that our experimental results are robust, reproducible, and transparent, providing clear insights into the effectiveness of the data selection methodologies employed in our study."}, {"title": "B RATIONALE FOR THE METHOD NAME ZIP-FIT", "content": "We chose the name ZIP-FIT for two reasons:\n1. ZIP refers to the use of gzip compression for data selection, where compression aligns the data for better future fine-tuning (or FITting).\n2. The name also references scaling laws, as ZIP-FIT consistently reduces loss faster than competing methods, implying better power-law scaling parameters, drawing a parallel to Zipf's law Piantadosi (2014), which describes similar scaling behavior in language models."}, {"title": "Remark:", "content": "Zipf's law Piantadosi (2014) describes the inverse relationship (thus power law \\(f(r) \\propto 1/r^s\\), where \\(r\\) is the rank and \\(f(r)\\) is the frequency of the word with rank \\(r\\)) between a word's frequency and its rank in natural language, a pattern that reflects scaling behavior. Rank in this context is the position of the word after sorting with respect to frequency in the text."}, {"title": "Limitations of n-grams:", "content": "Many traditional methods, including hashed n-grams, focus on capturing immediate textual correlations by simplifying text into discrete, fixed-size buckets. Although these techniques are computationally efficient, they may not adequately capture syntactic or structural relationships within the data. Additionally, the introduce noise due to collisions during hashing."}, {"title": "Challenges with Neural Embeddings:", "content": "Neural embeddings offer a powerful tool for capturing semantic relationships, but they come with significant computational costs. These embeddings are typically pre-trained on large corpora and fine-tuned for specific tasks, which requires substantial resources. Given the scalability challenges of embedding-based methods, we conjecture that a simpler method like compression can provide a more scalable and resource-efficient alternative.\nWe hypothesize that compression \u2013 in this case gzip, but perhaps a different compression algorithm -serves as a strong proxy for capturing syntactic and structural relationships in textual sequences. \\(gzip\\)'s ability to compress data based on redundancy minimization can be leveraged as a metric to align text with a target distribution."}]}