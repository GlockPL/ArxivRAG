{"title": "DECISION INFORMATION MEETS LARGE LANGUAGE MODELS: THE FUTURE OF EXPLAINABLE OPERATIONS RESEARCH", "authors": ["Yansen Zhang", "Qingcan Kang", "Wing Yin Yu", "Hailei Gong", "Xiaojin Fu", "Xiongwei Han", "Tao Zhong", "Chen Ma"], "abstract": "Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.", "sections": [{"title": "INTRODUCTION", "content": "Operations Research (OR) has a long history of optimizing complex decision-making processes, such as in logistics, finance, investment, transportation, and healthcare, etc., where even small improvements can lead to significant operational profits. As these optimization algorithms increasingly contribute to daily life, it is essential to ensure their trustworthiness and reliability through explanations, which build user confidence (Faulhaber et al., 2021). Governments are also responding to this need by enacting laws like the General Data Protection Regulation (GDPR) of the European Union (Goodman & Flaxman, 2017), emphasize the \"right to explanation\" for algorithmic decisions (Selbst & Powles, 2018) in automated systems.\nIn recent years, Large Language Models (LLMs) have emerged as powerful tools in the OR domain, offering new opportunities to automate and enhance the modeling process. Current research of LLMs in OR, such as works (Xiao et al., 2023; AhmadiTeshnizi et al., 2024; Tang et al., 2024; Zhang et al., 2024), explore the potential to streamline the formulation and solutions of complex OR problems. However, LLMs in OR have primarily focused on improving efficiency and accuracy by generating codes for external solvers to obtain OR solutions, with less attention to enhancing solution explainability, especially in real-time collaborative automated systems.\nMeanwhile, several studies (\u010cyras et al., 2019; Li et al., 2023; Erwig & Kumar, 2024; De Bock et al., 2024) have explored explainable optimization related to OR, but there are still limitations. For example, (Erwig & Kumar, 2024) focuses specifically on combinatorial optimization problems,"}, {"title": "RELATED WORK", "content": ""}, {"title": "LLMS FOR OR", "content": "LLMs show great promise for OR, offering innovative approaches to optimize and automate modeling processes (Xiao et al., 2023; AhmadiTeshnizi et al., 2024; Tang et al., 2024; Zhang et al., 2024; Huang et al., 2024; Mostajabdaveh et al., 2024). Although LLMs have shown potential in various OR tasks, their application has primarily focused on automating modeling processes and enhancing computational efficiency. In contrast, our approach distinguishes itself by using LLMs to provide detailed, context-aware explanations of OR solutions, addressing the gap in explainability."}, {"title": "EXPLANATIONS FOR OR", "content": "Explanation in OR is essential for clarity and transparency, helping various stakeholders understand complex decision-making processes. Despite its significance, there is a notable lack of research, with only a few related works, such as (Thuy & Benoit, 2024; Erwig & Kumar, 2024; De Bock et al., 2024) and OptiGuide (Li et al., 2023), which only focus on the easy what-if analysis. Our approach focuses on more complex what-if analysis, seeking a broad range of methods that can uniformly quantify constraint changes or parameter changes (sensitivity analysis). We leverage LLMs to embed detailed, context-aware explanations directly within OR solutions, filling this gap. The comparison between EOR and OptiGuide is shown in Table 1. More details about the differences between the concepts are discussed in Appendix A.1."}, {"title": "METHODOLOGY", "content": ""}, {"title": "PROBLEM FORMULATION", "content": "Given an OR problem p, along with a user query q related to the problem, our goal is to utilize an LLM to generate comprehensive explanations of solutions for the queries in real time. The LLM will provide two types of explanations: 1) Attribution Explanation, which outlines the general attributes and structure of the problem, and 2) Justification Explanation, which elucidates the correctness and derivation of the solutions. The mathematical formulation is as follows:\nInput: The origin problem description $p = (d, \\delta, c)$, and a user query $q = (\\delta', c')$. The updated problem description, incorporating the user query, is denoted as $p' = (d, \\delta', c')$. Here, d represents the set of decision variables, $\\delta$ and $\\delta'$ are the objective functions to be maximized or minimized, c and c' denote the original and modified constraints in p and p', respectively, that the decision variables must satisfy. In our setting, we assume the decision variables remain unchanged.\nOutput: We denote the problem solutions for p and p' as s and s', respectively. The output comprises two types of explanations, 1) Attribution Explanation: A detailed description of the elements $d, \\delta, \\delta', c, c', s$, and $s'$ within the context of the problem. 2) Justification Explanation: A rationale for the correctness of s and s', and clarifies how s' is derived from s."}, {"title": "THE EOR FRAMEWORK", "content": "Our proposed framework, EOR, is an end-to-end solution designed to enhance OR model transparency using LLMs. Unlike current methods that provide limited and shallow explanations, primarily the form of attribution explanations, our framework emphasizes delivering clear, actionable"}, {"title": "THE WORKFLOW OF EOR", "content": "As shown in Figure 2, EOR framework comprises three key agents: Commander, Writer, and Safeguard, each serving a distinct role to ensure an efficient, accurate, and secure optimization process.\nCommander Agent: The Commander acts as the central hub or \u201cdata bus\u201d in the system, responsible for receiving user queries and managing data flow between the agents. When an end-user submits a new user query, the Commander first interprets the query's context, identifies the intent, and then forwards the message to the appropriate agents.\nWriter Agent: Upon receiving the processed query from the Commander, the Writer initially assumes the role of analyzing and modifying the code. Based on the query's requirements, the Writer determines whether to add, delete, or update specific constraints and parameters. By leveraging LLMs, the Writer guarantees that the generated code accurately reflects the intended changes. Subsequently, the updated code is sent to the Safeguard for verification. Once the Safeguard provides a SAFE confirmation, the Writer transitions into an interpreter role, generating detailed explanations for the modifications and the rationale behind the decision-making process.\nSafeguard Agent: The Safeguard is responsible for ensuring the safety and correctness of the generated code. It conducts thorough checks to verify whether the code adheres to predefined safety standards and is free from logical or syntactical errors that could compromise the optimization process. If the code passes these safety checks, the Safeguard approves it for execution; otherwise, it triggers a debugging process where the Writer regenerates and corrects the code as necessary.\nThe overall workflow: The EOR workflow starts when a user submits a query to the Commander (1), who relays it to the Writer with a code prompt (2). The Writer analyzes the query, determining whether to add, delete, or update code, and returns the updated code to the Commander (3). This step ensures that the generated code aligns with the updated problem requirements. The Commander then sends the code to the Safeguard for verification (4). If the Safeguard determines the code is safe (5), the process moves to (6), where the Commander sends an interpreter prompt to the Writer. If the code is deemed dangerous, the process loops back to (2), with the Commander sending a debug code prompt to the Writer. Once the code is safe, the Writer generates answers about explanations for the modifications and results (7) and sends them to the Commander. Finally, the Commander sends these explanations to the user (8). This iterative process, which repeats until a satisfactory answer or timeout, ensures robust, explainable solutions tailored to the user's query. The detailed design of the prompt template is presented in Appendix A.2."}, {"title": "JUSTIFICATION EXPLANATION GENERATION", "content": "In our framework, explanations are generated to ensure transparency and trustworthiness in the decision-making process. These explanations are divided into two main categories:\nExplanation of Correctness: This type of explanation serves to validate the code modifications introduced by the Writer, offering a detailed rationale for the changes made. It clarifies the necessity of these modifications in addressing the problem's requirements and ensures that they adhere to safety standards and logical constraints. Through this process, the reliability of the generated code is substantiated, thereby enhancing the accuracy of the model prior to execution.\nExplanation of the Results: Once the code is executed and the results are obtained, this type of explanation focuses on interpreting the outcome. It breaks down the results into understandable terms, illustrating the impact of the code changes on the final solution. This explanation connects the modifications to their direct effects, providing users with a clear understanding of how the new solution addresses their initial query and any resulting trade-offs or benefits.\nBefore formalizing the concept of \u201cDecision Information\u201d, consider the following user query from an OR problem in flight operations: How should the aircraft configuration be adjusted if the company limits Type A aircraft to 15 and Type B aircraft to 30? In this context, \"Decision Information\" refers to the new constraints, limiting the number of Type A and Type B aircraft to 15 and 30, respectively, which directly modify the optimization model. These changes reshape the solution space, requiring adjustments to meet demand within the newly imposed limits. Thus, \u201cDecision Information\" captures the key query elements that drive changes in the problem.\nDefinition 1 Decision Information: Decision Information encompasses the parameters and constraints specified in a user's query for an OR problem, capturing the essential details needed to reconfigure the problem according to the user's intent.\nFollowing this definition, we turn to a quantitative evaluation of \u201cDecision Information\" to assess its impact on decision-making processes. However, existing methods lack a measure for assessing changes in decision information caused by constraint modifications, focusing only on sensitivity analysis in parameter changes. Inspired by (Xing et al., 2024), we convert both the updated and original programs into a standardized format and then calculate their differences to determine the importance of information changes. This process can be outlined in three steps:\nConversion to Linear Programs (LPs) in General Form: Both the updated code (resulting from user queries) and the original code are first parsed and translated into a standardized LP format, that is widely adopted by various LP solvers, including CPLEX (Cplex, 2009), Gurobi (Bixby, 2007), COPT (Ge et al., 2023) an so on. This conversion includes expressing all decision variables, objective functions, and constraints uniformly to allow a direct comparison. The general LP form captures the essence of both the initial and modified decision scenarios, providing a clear basis for analyzing how changes in input data or constraints affect the outcome.\nFormally, an LP with n variables and m constraints can be represented as:\n$\\begin{aligned} &\\min_{x \\in \\mathbb{R}^{n}} c^{T} x \\\\ &\\text { s.t. } A x \\leq u_{s} \\\\ &l_{x} \\leq x \\leq u_{x}, \\end{aligned}$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the constraint matrix, $c \\in \\mathbb{R}^{n}$ is the cost vector, $x \\in \\mathbb{R}^{n}$ is the decision variables, $l_x \\in \\mathbb{R}^{n}$ and $u_x \\in \\mathbb{R}^{n}$ are lower/upper bound of the decision variables $x$, and $l_s \\in \\mathbb{R}^{n}$ and $u_s \\in \\mathbb{R}^{n}$ are lower/upper bound of the constraint.\nGraph Representation Conversion: The standardized LPs can be transformed into bipartite graphs, which consist of two distinct sets of nodes: decision variables and constraints (Gasse et al., 2019; Fan et al., 2023; Xing et al., 2024). In this representation, edges between nodes signify dependencies or relationships, with edge weights indicating the strength or nature of these connections. We define the bipartite graph as $G=(S\\cup X, E)$, where $S = \\{s_i|i \\in [m]\\}$ represents the set of constraint nodes, $X = \\{x_j|j \\in [n]\\}$ represents the set of decision variable nodes, and $E = \\{e_{i,j}|i \\in [m], j \\in [n]\\}$ represents the edges between them. Here, $[\\cdot]$ denotes a set of consecutive numbers. The attribute of a constraint vertex $s_i$ is expressed as $attr(s_i) = [l_i, u_i]^T$, indicating"}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EVALUATION BENCHMARK", "content": "Despite the availability of numerous open-source datasets in OR, such as NL4OPT (Ramamonjison et al., 2023), ComplexOR (Xiao et al., 2023), NLP4LP (Holzer et al., 2024), and IndustryOR (Tang et al., 2024), these datasets are limited to problem descriptions and are primarily suited for OR modeling needs. There remains a significant gap in datasets specifically tailored for explainable OR, which are crucial for advancing transparency and interpretability in decision-making processes.\nTo address this issue, we developed a novel benchmark dataset based on the open-source commercial IndustryOR, specifically designed to evaluate explainability in OR tasks. The benchmark includes 30 categorized problems across various domains (e.g., supply chain management, financial investment, logistics management, etc.). Each problem is paired with 10 unique queries that involve diverse or combined modifications to parameters or constraints (e.g., deleting, adding, or updating constraints and parameters). These queries were crafted by OR experts with significant industry experience to ensure both diversity and practical relevance. The dataset's quality and comprehensiveness are validated through iterative expert reviews and comparisons with real-world cases.\nFor every problem, we provide corresponding Python code and employ the Gurobi optimization solver (Bixby, 2007) to determine optimal solutions. Additionally, we also include the ground truth labels for each query for each problem to ensure accurate evaluation. Notably, the question sets and the queries in this benchmark are developed from scratch and managed in-house, guaranteeing that they have not been part of LLM training data, enhancing the robustness of the benchmark for assessing model performance. We provide a complete example in Appendix A.3."}, {"title": "EVALUATION METHODOLOGY", "content": "Our evaluation focuses on two aspects: Modeling Accuracy and Explanation Quality. For the modeling accuracy assessment, we recognize that different code implementations can produce the same optimization results. For instance, two programs solving a linear programming problem may employ different formulations or techniques (e.g., distinct constraint orderings or variable namings), but both can still yield identical optimal solutions. Therefore, rather than directly comparing the generated code to a reference, we evaluate accuracy by comparing the optimization outcomes to ensure correctness and consistency across implementations.\nRegarding the explanation quality, although our task is highly specialized and requires expert-level interpretability in OR, we aim to develop an automated evaluation method. Drawing inspiration from (Kondapaneni et al., 2024) and utilizing the capabilities of LLMs for text quality evaluation (Chen et al., 2023; Chiang & Lee, 2023; Hu et al., 2024; Chu et al., 2024; Zytek et al., 2024), we establish expert-crafted templates and use LLMs to assess explanation quality, aiming for a human-level standard. However, this evaluation approach has certain limitations. To address this, we propose two methods in this paper. First, we establish a structured template that specifies key criteria for effective explanations, such as clarity, relevance, and logical coherence. For instance, explanations should explicitly describe the rationale for modifying specific parameters or constraints and explain how these changes affect the optimization results. Second, we conduct a blind review process where OR experts anonymously score the explanations generated by different methods. This approach helps minimize bias, providing a reliable measure of how effectively the explanations convey meaningful insights to users. We will assess whether the proposed automated evaluation method aligns with human evaluation. This dual approach enables us to assess the consistency between the proposed automated method and human evaluation."}, {"title": "BASELINES", "content": "We employ two baselines to ensure a comprehensive evaluation: Standard and OptiGuide (Li et al., 2023). The Standard involves a proprietary LLM (e.g., GPT-4, GPT-4-Turbo, etc.) that generates updated programs and explanations, serving as a basic comparison point for assessing overall performance. The OptiGuide represents a specialized method used in supply chain optimization, providing a domain-specific comparison that evaluates the adaptability and effectiveness of LLMs in industry-relevant scenarios."}, {"title": "MODEL SETUP", "content": "For our experiments, we evaluate the performance of the proposed baselines using four LLMS, GPT-4 (Achiam et al., 2023), GPT-4-1106-preview, GPT-4-0125-preview, and GPT-4-Turbo, under both zero-shot and one-shot learning settings. The zero-shot setting requires the models to generate updated programs and explanations without any prior examples, testing their inherent understanding and generalization capabilities. The one-shot setting provides a single example to guide the models, allowing us to assess the impact of minimal contextual information on their ability to perform the tasks accurately and coherently. To ensure fairness and reproducibility, we fix the hyperparameter temperature at 0 and apply the same examples for all models in the one-shot setting, and all models are implemented under the framework AutoGen (Wu et al., 2024). This paper focuses on fully automating all processes, excluding user involvement. However, as our implementation is built on the AutoGen, it inherently supports seamless integration of user feedback. A detailed hyperparameter sensitivity analysis is provided in the Appendix A.4. The source code is available at https://github.com/Forrest-Stone/EOR."}, {"title": "QUANTITATIVE PERFORMANCE", "content": ""}, {"title": "COMPARISON OF MODELING ACCURACY", "content": "Table 2 shows the accuracy results across different models. We have the following observations. In the zero-shot setting, EOR consistently outperforms both Standard and OptiGuide across all LLM versions. These results emphasize the robustness of the EOR in zero-shot tasks, which substantially improves performance compared to other methods. For instance, GPT-4-Turbo achieves 88.33% accuracy with EOR, while Standard and OptiGuide methods yield only 63.00% and 30.33%, respectively. Moreover, for all LLM models except GPT-4, Standard outperforms OptiGuide, indicating that the LLM's modeling capabilities are quite strong.\nIn the one-shot setting, EOR continues to outperform Standard and OptiGuide in all LLM versions, achieving an average of 90.00% accuracy and even reaching 95.33% accuracy on the GPT-4-Turbo, the highest among all results. Additionally, we find that providing an example can significantly improve modeling accuracy, and nearly all methods perform better in the one-shot setting than in the zero-shot setting. Specifically, the accuracy of OptiGuide on GPT-4 improves from 30.33% to 75.00%. However, OptiGuide still produces the worst result except on GPT-4.\nOverall, EOR consistently outperforms other methods in both zero-shot and one-shot settings, particularly with models like GPT-4-Turbo and GPT-4-0125-preview. The accuracy gains observed when transitioning from zero-shot to one-shot highlight the importance of using examples in improving model performance. In terms of model comparisons, GPT-4-Turbo demonstrates the highest adaptability across both settings, achieving the best overall accuracy. While OptiGuide provides modest improvements, it does not match the performance of Standard and EOR. These results underscore the value of carefully selecting both models and example strategies to maximize accuracy, with EOR emerging as the most effective for high-accuracy tasks."}, {"title": "COMPARISON OF THE QUALITY OF EXPLANATIONS", "content": "As discussed in Sec. 4.2, a critical part of our evaluation is assessing the quality of the explanations generated by the models. It is important to note that explanations based on incorrect results are irrelevant, as explaining failure cases offers no meaningful insights. Therefore, we first filter out all incorrect modeling cases, ensuring that only correct outputs are evaluated. We adopt two evaluation methods: an automated evaluation (Auto) and expert evaluation (Expert), both introduced in Sec. 4.2. These evaluations focus on two main aspects: Explanation of Correctness (EC) and Explanation of Results (ER). EC assesses the clarity and correctness of the explanation regarding the model's output, while ER evaluates how well the explanation conveys the reasoning behind the optimization results. Additionally, we also measure the overall explanation quality (Overall). The explanations are scored on a 0-10 scale, with 0 indicating poor quality and 10 indicating excellent quality. The prompt template for evaluating the explanation quality is provided in Appendix A.5. The results of these evaluations using GPT-40 are summarized in Table 3.\nAs shown in Table 3, we have the following observations. Firstly, Auto is nearly as effective as Expert, with minimal differences in scores across all metrics, indicating that Auto is a reliable and valid approach. It is worth noting that Auto scores individual queries on EC, although it does not explicitly evaluate explanations. Secondly, EOR consistently outperforms both Standard and OptiGuide in explanation quality, achieving the highest scores in both zero-shot and one-shot settings. This demonstrates the superiority of our method in providing clearer and more accurate interpretations compared to other models. Finally, we find that Auto scores slightly higher on Standard than Expert, while for other models, the opposite is observed, with Auto scoring slightly lower than Expert. One possible reason is that LLMs may be biased toward the results they generate."}, {"title": "CASE STUDY", "content": "In this section, we provide an example of the code and explanations generated by EOR, as illustrated in Figure 3, where simple explanations are highlighted in green to represent attribute explanations, while deeper ones in yellow indicate justification explanations with quantitative analysis.\nResults from other models are provided in Appendix A.6. The EOR approach stands out by not only providing a clear explanation of the updated code but also offering detailed insights into how the query affects the outcomes. It explicitly highlights the newly added constraints that limit the number of Type A and Type B aircraft, demonstrating transparency in addressing the operational limits specified in the query. Unlike other methods that merely report result differences, EOR incorporates a quantitative analysis, explaining the $15,000 increase in operational costs due to the restricted solution space caused by the newly added constraints. This step-by-step explanation not only justifies the modifications but also clarifies the broader impact of the constraints, making EOR more comprehensive and contextually aware than other models."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this paper, we present EOR, a novel framework that addresses transparency and interpretability challenges in OR. We introduce the concept of \"Decision Information\" through what-if analysis"}, {"title": "APPENDIX", "content": ""}, {"title": "THE COMPARISON BETWEEN WHAT-IF ANALYSIS, SENSITIVITY ANALYSIS, AND DECISION INFORMATION ANALYSIS", "content": "What-if Analysis: What-if analysis explores the impact of changing conditions, including parameters, on overall outcomes, allowing users to adjust inputs to observe potential scenarios manually. This approach emphasizes scenario exploration at a macro level, helping users understand how different \"what if\" situations might influence results without delving into specific details.\nSensitivity Analysis: Sensitivity analysis measures how minor variations in input parameters affect the model's output, identifying which inputs have the most significant influence on results. This method provides detailed quantitative insights into the effects of parameter changes, making it particularly effective in optimization contexts where analytical solutions can be derived.\nDecision Information Analysis: Decision information analysis extends sensitivity analysis by focusing on how changes in constraints impact decision-making and outcomes. This approach examines the sensitivity of the model to variations in constraints, identifying key decision factors while also capturing the broader implications of both constraint and parameter changes, particularly in complex optimization problems without analytical solutions.\nIn summary, decision information and sensitivity analysis are more detailed, with the former centering on decision-related changes (constraints or parameters) and the latter on parameter sensitivity, while both can be part of a broader what-if analysis framework. Previous research has predominantly focused on sensitivity analysis through the simplex method, which is recognized for its efficiency, precision, and ability to provide analytical solutions. However, the simplex method has limitations in evaluating the effects of changes to constraints. In contrast, our proposed approach leverages bipartite graphs to assess the impact of constraint modifications, addressing this gap and providing a more generalized solution."}, {"title": "PROMPT TEMPLATE DESIGN", "content": ""}, {"title": "PROMPT TEMPLATE FOR WRITER AGENT", "content": "The prompt template for Writer with system message for the ChatCompletion inference:\nWRITER_SYSTEM_MSG =\n\"\"\"\n**Role: You are a chatbot tasked with:\n(1) Writing Python code for operations research-related projects.\n(2) Explaining solutions using the Gurobi Python solver.\nProblem Description:\n{description}\nSource Code:\n{source_code}\nDocumentation:\n{doc_str}\nExample Q&A:\n{example_qa}\nOriginal Execution Result:\n{execution_result}\n**Task:**\nYou are provided with the original problem description and the correct code for an operations research problem. Based on the user's query,\nupdate the code accordingly. Your task may involve either deleting constraints or adding new data or constraints."}]}