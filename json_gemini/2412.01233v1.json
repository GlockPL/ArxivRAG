{"title": "Best Practices for Large Language Models in Radiology", "authors": ["Christian Bluethgen", "Dave Van Veen", "Cyril Zakka", "Katherine E Link", "Aaron Hunter Fanous", "Roxana Daneshjou", "Thomas Frauenfelder", "Curtis Langlotz", "Sergios Gatidis", "Akshay Chaudhari"], "abstract": "At the heart of radiological practice is the challenge of integrating complex imaging data with clinical information to produce actionable insights. Nuanced application of language is key for various activities, including managing requests, describing and interpreting imaging findings in the context of clinical data, and concisely documenting and communicating the outcomes. The emergence of large language models (LLMs) offers an opportunity to improve the management and interpretation of the vast data in radiology. Despite being primarily general-purpose, these advanced computational models demonstrate impressive capabilities in specialized language-related tasks, even without specific training. Unlocking the potential of LLMs for radiology requires basic understanding of their foundations and a strategic approach to navigate their idiosyncrasies. This review, drawing from practical radiology and machine learning expertise and recent literature, provides readers insight into the potential of LLMs in radiology. It examines best practices that have so far stood the test of time in the rapidly evolving landscape of LLMs. This includes practical advice for optimizing LLM characteristics for radiology practices along with limitations, effective prompting, and fine-tuning strategies.", "sections": [{"title": "1 Introduction", "content": "Radiology has a key role in healthcare, bridging complex medical imaging and clinical practice. The field is tasked with ensuring the correct application of imaging modalities, describing and integrating imaging findings with the patient's history, interpreting the gathered information, and appropriately communicating and documenting the results.\u00b9 This process relies heavily on a precise use of language.\nGiven the recent breakthroughs in large language models (LLMs) capable of processing and generating human language, there exists a large promise for their use with the abundant textual data in radiology. While powerful general-purpose\u00b2 computational models like ChatGPT excel in many (domain-)specific tasks without additional modifications, their flexibility and adaptability comes with a degree of unpredictability, making LLMs akin to double-edged swords.\u00b3 Not unlike trying to obtain a sensible answer from Douglas Adams' fictional supercomputer Deep Thought, effectively interacting with LLMs and unlocking their full potential for radiology requires understanding what to expect of them, to understand their foundations, and systematic approaches for handling the new class of models' idiosyncrasies.\nAs the field is heavily working on identifying use cases and testing LLMs for various purposes, it is necessary to consider best practices for effective development and integration of LLMs in radiology."}, {"title": "2 Technical Foundations of Language Models", "content": "Language models (LMs) are a class of probabilistic models designed to learn statistical patterns in natural language. These models predict the probability of a subsequent text part (termed token, e.g. a word or a character) based on preceding tokens in a sequence. The advent of deep learning in natural language processing (NLP) entailed training LMs using vast, web-scale amounts of text, relying on self-supervised learning, which utilizes the inherent structure of the data itself instead of additional labels. Such pretrained LMs outperform traditional methods on a variety of practical tasks such as translation and text classification.7"}, {"title": "2.2 Transformer-based LMs", "content": "As of 2024, most modern LMs employ the transformer architecture, which first breaks up inputs like text sequences into smaller units (tokens). Transformers integrate multiple essential components, most notably 1) embeddings (numerical vector representations that encode semantic information of tokens), 2) the attention mechanism that can model relationships between input elements even over long distances in the sequence, and 3) feed-forward networks that perform further computation over these representations. The attention mechanism is particulary notable as it allows the model to dynamically weight the importance of different input parts based on their relevance in the context.\nTransformer-based LMs include an encoder (which converts text into embeddings), a decoder (which converts embeddings into text), or a combination of both. For instance, BERT (Bidirectional Encoder Representations from Transformers), an encoder-only model, excels at tasks like text classification. In contrast, GPT (generative pretrained transformer), a decoder-only model, is adept at generating text.10 Tasks like translation benefit from both an encoder that processes the source text to generate meaningful, contextualized representations, and a decoder that generates the target text in another language."}, {"title": "2.3 Emergence of LLMs", "content": "The performance of transformers scales as a function of the dataset size (often measured in trillion tokens) and model size (often measured in billion parameters),11 giving rise to large language models (LLMs). The scale of LLMs grants them unprecedented versatility across a broad spectrum of tasks. 10 LLMs are characterized by abilities like following natural language instructions, and in-context learning, the ability to flexibly adhere to specific tasks by providing examples in natural language at inference time (i.e., when generating text, after training). Notably, LLMs acquire these abilities even without direct, task-specific supervision.10 While LLMs are versatile, adapting them for specific tasks can further enhance their effectiveness.12\nMost recent LLMs are pretrained by learning to predict the most likely next token, given a sequence of input tokens13(Fig. 1). At this stage, the LLMs possess a general \u201cunderstanding\u201d of language but cannot yet respond to instructions. Therefore, the next stage aims at supervised training on high-quality data consisting of instructions and answers (instruction tuning). After this, modern LLMs can follow natural language instructions but are not yet aligned with human preferences and may respond in undesirable or even harmful ways. This necessitates methods for alignment tuning, such as reinforcement learning with human feedback (RLHF).14"}, {"title": "2.4 Enhancing LLMs with Non-Textual Inputs, Retrieval-augmented Generation and Tools", "content": "An emerging practice has seen the expansion of LLMs beyond common language tasks, evolving them into even more versatile multi-functional tools. This progression is marked by the integration of multimodal capabilities, enabling LLMs to process and interpret non-textual (e.g., visual or auditory) information. One way to achieve this is to convert images into a series of patches, akin to textual tokens, and feed them into the model alongside textual data (often after \"translating\" the visual tokens with the help of additional model components), enabling the output of text grounded in visual information15, 16 a scenario very relevant to radiological image interpretation and report writing.\nTo further enhance the versatility of LLMs, additional external information can be retrieved and provided to LLMs, a process known as retrieval-augmented generation (RAG)(Fig. 2). This information can come from curated text sources (e.g., databases with guidelines), or provided through the addition of external tools like browsers, calculators, and Application Programming Interfaces (APIs). This has been pivotal in enabling LLMs to access real-time information, perform complex calculations, and interact with various web services to improve the reliability and relevance of their outputs. 17,18"}, {"title": "3 LLMs in Radiology: Applications and Desiderata", "content": "Radiology requires the use of language to acquire and interpret imaging data and to communicate key findings. Clinical context is a key for medical decision making, and its absence has been identified as a significant limitation of current medical AI systems. 19 Through their versatility to process complex, contextualized information and to interact in natural language, LLMs are a big step towards overcoming this limitation and uniquely position them to augment workflows and substantially influence how the field of radiology operates."}, {"title": "3.1 Applications", "content": "Clinical tasks. Initial work has demonstrated the use of LLMs for report summarization, simplification, structuring, quality assurance, detection of speech recognition errors, as well as generating a report from structured knowledge following a preferred reporting style.20\u201322 LLMs could potentially draft recommendations that follow from radiology reports (e.g. by referencing guidelines) or suggest actions based on those recommendations through alert and scheduling systems. Using an LLM as a copilot informed by image and text data or additional specialized software could significantly speed up report generation. For instance, preparing multidisciplinary boards is a tedious task that an LLM could support by pre-structuring case information according to the board's preferences and augment the presentation by providing relevant information derived from guidelines and scientific literature.\nOperational tasks. LLMs could potentially increase operational efficiency and accessibility of radiology services. Exam, service and staff scheduling, reporting and resource allocation are complex tasks that require integration of information from different sources, including patient records, availability of resources and departmental guidelines. Despite standardization efforts, this information is often unstructured and distributed across several systems and could greatly benefit from enhanced general language processing abilities.\nResearch. Clinical research relies on the identification of suitable patient cohorts for conducting research studies and often requires manually sifting vast amounts of health records. This process can be facilitated by LLM-based screening of records based on defined inclusion criteria and endpoints. LLMs can also be an effective tool for retrieving and summarizing scientific literature on topics of interest to generate hypotheses.23 Increasingly, LLMs are being explicitly supported24 to assist in the communication of scientific results within traditional media (e.g. through manuscript preparation and review) or within the growing field of alternative media, such as online platforms.\nEducation. Traditionally, effective radiology training includes frequent case discussions and feedback on the reports drafted by residents by more experienced radiologists. Conversational LLMs drawing information from high-quality case files and guidelines could provide a new tool to engage with trainees and guide them towards proficiency in an interactive and individualized way."}, {"title": "3.2 Desiderata for LLMs in Radiology", "content": "As LLMs demonstrate increasing feasibility in radiological applications, the field must define clear requirements and desired properties for their effective integration into research and clinical practice. Key desiderata include: improving patient outcomes, enhancing radiologist workflow efficiency (without increasing the risks of burnout), and maintaining high standards of clinical care and privacy preservation. Pursuing these overarching desired goals requires figuring out how to use LLMs to their fullest potential, while respecting ethical principles guiding how to promote wellbeing, minimize harm, provide appropriate transparency and add value while being dependable, acting responsibly, ecologically and economically viable. 19,25\nThis will help pave the way for narrow applications, as well as for more complex developments like LLM-based agents capable of autonomously interacting and solving problems in multi-step approaches(Fig. 3), and multimodal models harnessing several input modalities (e.g., images, videos and text) to further add value to the radiology workflow.26,27\nWhile this review focuses on the technical aspects of LLM adaptation, practical applications need to ensure applicable regulatory compliance, such as HIPAA and GDPR, as well as future AI-specific issues like the White House executive order on AI or the EU AI Act. Beware that LLM applications in radiology may be classified as medical devices depending on their impact on clinical decisions (e.g., for triage or diagnostic support), with regulatory requirements varying by region and uses.28"}, {"title": "4 Best Practices for LLMs in Radiology", "content": "LLM research is moving extremely fast, and important developments often outpace scientific peer-review timescales. Drawing from current literature, practical experience, and interdisciplinary discussions, we identify best practices for using and adapting LLMs in radiology. Current reviews operating at varying levels of domain-specificity and technical depth contain further details. 5,29-33"}, {"title": "4.1 Awareness of Challenges and Limitations", "content": "Confabulations. Arguably the biggest hurdle in using LLMs in radiology settings is the propensity to generate plausible-sounding, yet incorrect information (\u201challucination\u201d, although \u201cconfabulation\u201d may be more appropri-ate34,35). Fabricating information is not limited to text-only LLMs: Multimodal LLMs can hallucinate objects in provided images. 36 Confabulations are aggravated by the LLMs' ineptitude to reliably convey a degree of confidence in their assessments, which is pivotal for relaying relevant medical information.37 Several techniques like chain-of-thought prompting (instructing the model to break up a problem into intermediary steps, e.g. \"Let's think step-by-step\u201d), in-context learning (i.e., providing examples in the input), self-consistency (repeatedly use the same input to evaluate the consistency of the responses) and RAG can mitigate confabulations.38 Still, due diligence is warranted to reduce the risk of negative outcomes associated with incorrect generations."}, {"title": "Open vs. Closed Models", "content": "Several levels of accessibility exist for LLMs. Open LLMs (e.g., BLOOM(27)) are distributed by sharing model weights(i.e., open-access) and associated code (i.e., open-source). Closed models provide no access to weights or code. Model access can also be regulated through gating mechanisms, such as hosting closed models (e.g., OpenAI's ChatGPT), or by imposing usage restrictions (e.g., Meta's LLaMA 2 and 342). Fully closed LLMs are only accessible to those directly involved with the model (e.g., Med-Gemini43).\nDifferent benefits and risks exist along this gradient. Closed models provide greater risk control, allowing the devel-opers to monitor inputs and employ usage guardrails. Although both open and closed models can be customizable and HIPAA-/GDPR-compliant, closed models provide lower auditability and transparency, which can also affect downstream reproducibility. They also usually rely on the developer for reliability and data privacy assurances, and may have use case restrictions (such as explicitly excluding the use for processing medical information). Hosted, closed LLMs can be updated without notice, potentially affecting performance and outputs. Awareness of these changes is vital for consistent application.\nOpen models, on the other hand, enable broader perspectives to use and evaluate the model fully and transparently, and can be hosted and updated on the user's computational infrastructure of choice. However, these open models may often not match performance of closed, hosted models."}, {"title": "Reproducibility and Explainability", "content": "Reproducibility is challenging to achieve with LLMs due to prompt brittleness (the sensitivity to even small changes in the input), the inherent stochasticity of LLMs (by design, the LLM predicts a probability distribution to be sampled from when outputting the next token) and the randomness that becomes apparent at a large scale (when even very rare events such as hardware errors can have an effect). Silent updates to closed source LLMs due to retraining can exacerbate this issue. Within healthcare, interpretable and explainable models are standard within a degree of reason.19 While current LLMs lack comprehensive interpretability mechanisms, conversational LLMs can provide some transparency by explaining their outputs through dialogue. Additionally, techniques like chain-of-thought prompting can improve model reasoning by encouraging step-by-step problem solving while at the same time allowing the user to see possible intermediate considerations.44"}, {"title": "Risk of bias", "content": "LLMs pick up patterns from the training data at all stages of their training, including various types of bias (e.g., associated with gender, race, religion) inherent to the training data.30,45 For instance, an LLM instructed to complete the sentence \u201cthe doctor is a 35 year old\" may predict \u201cman\u201d significantly more frequently than \u201cwoman\u201d whereas \u201cthe nurse is a 35 year old\u201d can lead to opposite results.46 Other forms of bias affecting LLM integration include clinical confounding bias, technical bias and automation bias.19 These biases can be partially mitigated through careful curation of training and fine-tuning data and through alignment efforts.47"}, {"title": "Data privacy and security concerns", "content": "Data privacy challenges stem from both the training and inference phases of LLMs. Training LLMs can put sensitive health information at risk of being regurgitated if not adequately anonymized, as such data can potentially be extracted from LLMs after training.48 Additionally, during a chat session, the prompt context is not \u201cforgotten\", and potentially sensitive input data can reappear within the same session.49 It is recommended to de-identify data before using LLMs, although this standard can be challenging to achieve.\nThe versatility of LLMs has another downside: even aligned LLMs are susceptible to prompt-based attacks that elicit unwanted behavior (jailbreaking), such as revealing sensitive information or providing harmful content. New exploits are discovered frequently and large organizations like OpenAI employ red-teaming (i.e., deliberately provoking or stress-testing the LLM with challenging inputs to discover its limitations and potential risks or unwanted behavior) and offer \"bug bounties\u201d.50 LLMs that can handle image inputs often include an optical character recognition (OCR) functionality, which allows them to read and interpret text in the image, rendering manipulated images a potential Trojan horse for adversarial attacks.51"}, {"title": "Creating and Adapting LLMs", "content": "Training LLMs on domain-specific data, like radiology reports and images, can enhance the performance on downstream tasks.26,52 Detailed technical best practices for LLM adaptation are case-specific, rapidly evolving and are discussed in more detail in the literature.31,33\nThe high level of computer and machine learning engineering expertise required for LLM training makes close collaboration with computer scientists essential for developing, monitoring and deploying LLMs. Radiologists, on the other hand, should not be relegated to the role of data providers. Instead, they should contribute their domain expertise by leading the development of these powerful tools, defining specific objectives, contributing to data set design and conducting thoughtful and domain-oriented evaluation."}, {"title": "Pretraining LLMs", "content": "Pretraining modern large LLMs exceeding a billion parameters demands immense resources, including massive datasets\u00b9, substantial infrastructure2, and technical expertise. High costs leave little room for trial and error or even repeated iteration at scale and necessitate meticulous planning. Key decisions involve choosing the right model architecture, size, compute cluster setup, ,53\u201355 and meticulously curating datasets with trillions of tokens while ensuring data quality through content filtering, deduplication, and optimal tokenization methods.33,55 Detailed best pretraining practices constitute a volatile area of research, and are discussed in recent literature. 33\nFine-tuning pretrained LLMs is much less resource-intensive\u00b3 and should be prioritized. The model size should balance the necessary capacity to fulfill envisioned tasks and computational resources and ecological footprint. The LLaMA 2 model family is a prominent example of open-source pretrained LLMs available in different sizes (7B, 13B, 70B parameters) that has spawned successful \u201claminoid\u201d models like Vicuna,56 Alpaca57 and the multimodally capable LLaVa.16 With a few notable exceptions,58,59 larger LLMs tend to perform better across evaluations than their smaller-scale counterparts, at the cost of higher memory and compute requirements. Smaller fine-tuned models may outperform larger general-purpose LLMs.60 Recently, alternatives to the predominant single decoder transformer models have become increasingly popular, such as mixture-of-expert (MoE, e.g. Mistral),61 and structured state space models.62"}, {"title": "Instruction tuning", "content": "Starting from a pretrained LLM, models can be fine-tuned in a supervised way to increase the ability to follow instructions (instruction tuning) and improve their performance on downstream tasks.68 An instruction-following dataset can be constructed with instances following a pattern like {instruction(+input)}{output}.57 These instances can be created from paired sources (e.g. medical exams questions, flashcards69), by restructuring existing datasets (e.g. CheXinstruct26) or synthesized by another LLM.57 LLMs perform best on tasks appearing frequently in their training data. Balancing instruction datasets is therefore relevant to radiology and other medical specialties who deal with the long tail problem70 and can mitigate overfitting on certain tasks. Non-English LLM users benefit from the insight that English-language instruction tuning can also increase an LLM's task-specific capabilities in other languages.71 The typical size of an instruction tuning dataset is in the range of thousands of examples (e.g. 52k in Alpaca), while already tens of examples can improve perfor-mance on certain tasks.58 For instruction tuning, ensuring high data quality is much more important than during pretraining. Data quality and diversity increase the model's generalization ability and take priority over dataset size.33 Instruction tuning can boost domain- and task-specific performance of LLMs across several dimensions, e.g. accuracy, robustness, and fairness.72 Fine-tuned, instruction-following models can be used directly or further fine-tuned with a similarly structured dataset."}, {"title": "Aligning LLM Outputs with Human Preferences", "content": "Following instruction tuning, alignment tuning seeks to use human feedback on generated outputs to align an LLM with human preferences. In RLHF, an instruction-tuned model generates multiple na\u00efve outputs for the same prompt, which are then ranked by humans according to their preferences. This ranking data is used to train a reward model, which is in turn used to inform further fine-tuning of the LLM towards following human preferences. Curating large high-quality preference datasets from human annotators is costly, and successfully training an reinforcement learning (RL) model is considerably more challenging than instruction tuning.73 Newer approaches aim to obviate the RL step in lieu of simpler objectives and facilitate collecting alignment data by requiring less complex decisions (\u201cwhich output is preferred?\u201d or \u201cis this output good?\") 73,74 Other promising research uses LLMs for generating and evaluating the quality of supervised training datasets, 75,76 or for validating the accuracy of LLM outputs.77\""}, {"title": "LLMs on a budget", "content": "Parameter-efficient techniques that approximate the performance of full fine-tuning while only updating a much smaller number of parameters can facilitate LLM adaptation. LoRA (low-rank adaptation) is a popular method which introduces trainable rank decomposition matrices into the LLM, while the original model is kept frozen.78 It is possible to fine-tune several low-rank matrices for specific tasks, while keeping just one copy of the large main LLM, thereby greatly reducing memory requirements. Another approach trains task-specific prompt vectors that are introduced alongside the rest of the prompt (soft prompt tuning).79,80 Proxy tuning enhances the capabilities of a larger-scale LLM by integrating the differences in token predictions between two smaller models with the predictions of the larger LLM,81 rivaling the more expensive process of fine-tuning the larger LLM directly in performance. PEFT is a popular library implementing several parameter-efficient techniques.82\nQuantization is the process of mapping higher-bit values to lower-bit approximations (e.g. converting models weights or activations from 16-bit float values to 8-bit integers). Quantization entails a quantization error due to rounding operations, but reduces memory requirements and increases inference speed. QLoRA is a method that combines the benefits of LoRA and quantization.83 Distillation techniques can significantly decrease the requirements for hosting and running LLMs and thus facilitate deployment in constrained environments. These methods aim at reducing training data84 or model size, for instance by training smaller models (\"student\") to replicate the behavior of larger LLMs (\u201cteacher\u201d).85 All presented methods have a trade-off in performance, 83 but make it possible to run smaller LLMs with reasonable performance on desktop computers.86"}, {"title": "Multimodality", "content": "While a variety of approaches to implement multimodality exists,87 here, we refer to models with an LLM backbone that can process inputs from \u22652 data sources (e.g., images and text), mirroring the setting that is common in radiology, as large multimodal models(LMMs). It is generally expected that fine-tuned LMMs will play an increasing role in radiology over the next few years, but so far, general-domain LMMs dominate the literature88 and can sometimes outperform domain-specific models.89 Most notably, GPT-4V is a closed LMM that has shown promising yet flawed performance on analyzing radiological images.?,90 Med-PaLM M is a closed healthcare-centered LMM evaluated for radiology report generation, but no further radiology-specific benchmarks are documented.91 CheXagent is a recently introduced imaging modality-specific open LMM outperformed several other general and medical domain LLMs on a set of CXR-specific tasks.26 Other early examples of radiology LMMs include RadFM,92 RaDialog,93 CXR-LLaVA.94 LMMs typically require images as part of their instruction tuning set.88"}, {"title": "4.1.1 Model Cards and Versioning", "content": "Model Cards and Versioning. Regardless of whether the model is shared or solely used within the department, model cards detailing training datasets, evaluation metrics, and known limitations such as biases should be used to promote transparency, informed usage and reproducibility.?,64 Intermediate model checkpoints should be saved, assigned a version number and documented as they can give insights about the models scaling behavior\u201d and can be used to return to a previous model state (e.g., when observing performance drops post training)."}, {"title": "4.2 Evaluating LLMs", "content": "Despite general language understanding, generation and reasoning skills, LLMs may lack proficiency in specialized domains like radiology, which have unique terminology, concepts, and tasks. Single metrics can fall short in evaluating complex tasks.6 Thus, radiology LLM evaluation frameworks should cover domain-specific tasks and include expert human assessment.\nAutomatic scoring metrics for conditional text generation assess LLM responses in comparison to (human) references. These metrics measure the overlap of sentence parts (BLEU,\u201d ROUGE-L), the similarity of medical concepts (e.g., MEDCON\u201d), or transformer-based similarity with general (BERTScore100) and domain-specific focus (F1CheXBERT, 101 RadGraph Score102). Another method is to transform complex tasks like question-answering (QA) into multiple-choice questions (e.g. PubMedQA103), and measure the LLM's accuracy in selecting the correct answer.\nSingle metrics often fall short in evaluating complex, open-ended tasks like QA and summarization,96 necessi-tating benchmark frameworks for comprehensive model assessment.72,104, 105 Radiology texts, with their specific terminology and general anchoring on medical imaging data, require tailored benchmarks. Early examples like RaLEs106 evaluate LLMs on language radiology-specific tasks, such as anatomical relationship extraction, procedure selection and report summarization, while RadBench\u201d focuses on multimodal tasks including visual QA and report generation. Despite the need for public evaluation frameworks for transparency and reproducibility, concerns arise that such data might inadvertently be used for training, skewing results.107 Gated access to evaluation data may be an option to prevent data contamination.\nHuman expert evaluation is paramount for assessing the real-world correctness, conciseness, and completeness of outputs, but hard to scale, and may need calibration for consistent feedback. Notably, medical expertise exists on a spectrum; for instance, clinicians, medical students, radiology residents and board-certified or subspecialized radiologists may all focus on different aspects of a report, affecting their interpretation. This warrants a careful selection of readers and documentation of experience level.\nEffective evaluation for LLMs in radiology could be constructed as a multi-step framework harnessing the qualities of all presented methods: automatable metrics (single or as part of a benchmark) are used to compare, optimize and select different approaches to get an optimal configuration, which is subsequently evaluated by human experts. Aside from task-specific performance, outputs should be evaluated for criteria like potential harmfulness. Evaluation should continue after deployment, as updates to the LLM or shifts in input data could lead to unexpected and potentially harmful outputs."}, {"title": "4.3 Using and Augmenting LLMs", "content": "When not to use an LLM. LLMs have substantially democratized NLP research, leading to a steep increase in publications and peer-review work. The versatility of LLMs makes it attractive to conduct more and more refined feasibility studies. However, building and using LLMs are resource-intensive and carry considerable ecological footprints. 108, 109 While transfer learning hopes to mitigate train time, continuous training will also have an effect on CO2 emissions.110 Using an LLM for tasks that could be solved more efficiently with simpler approaches (for instance, by using a smaller LM or through employing regular expressions for traditional NLP tasks) constitutes over-engineering and should be avoided.\nPrompting LLMs. A distinctive and revolutionary feature of modern LLMs is the direct applicability to various downstream tasks without the need to retrain the model. Most radiologist users will interact with an LLM through prompting. Accurately instructing the model is often enough to get good results with current general-domain LLMs. However, creating and refining an effective prompt can be challenging.111 Prompt engineering is the process of designing and iteratively adjusting instructions to optimize the LLM output for the envisioned task, as even little variations can lead to significantly different outputs (prompt brittleness). Some good practices for designing effective prompts have emerged112\u2013114(Table 1). The adherence to a task can be increased by providing examples (few-shot prompting, a form of in-context learning with usually 2-5 examples), which can be seen as implicit fine-tuning without updating the model weights.10 While many LLMs can be prompted to use a specific output format directly (e.g. Python code or JSON115), providing a template or output schema (e.g., using Pydantic116) can greatly increase the adherence to such a request.\nApart from the prompt itself, outputs can be influenced by how the next token is sampled. Modern LLMs often use nucleus sampling, which is parameterized by temperature and top-p.117 Temperature rescales the probability mass function, affecting the probability of sampling lower-probability tokens, while top-p truncates the distribution to only include the smallest set of tokens whose cumulative probability exceeds p. Lower values for temperature and top-p push the output towards the most probable tokens, decreasing diversity. Setting the temperature to 0 makes the model theoretically deterministic (i.e., the most probable token is always picked); however, in practice, random effects can still influence the LLM output, so perfect reproducibility should not be taken for granted. For most tasks in radiology, when conciseness and the use of specific expressions are a priority over diverse outputs, lower temperature values will likely be preferred, as demonstrated in a clinical text summarization study.52 Frequency and presence penalty are parameters preventing the LLM from repeating itself too often. For radiology report generation, where certain expressions (e.g., \u201clesion\u201d, \u201cnodule\u201d) are often suitable more than once, and diversity of expression is not a priority, lower penalty values may be better.\nEnriching prompts with external information Retrieval-augmented generation (RAG) enhances LLM outputs by adding relevant context to user queries. 118 RAG pairs the LLM with a retrieval module that sources information from databases or curated sites, matching content to queries by similarity. This involves converting chunked documents into vector representations (embeddings) and retrieving and ranking them based on similarity. Domain-specific retrievers such as MedCPT may increase retrieval performance.119 Alternatively, additional information can be retrieved from external tools (e.g., calculators). Retrieved information can then be concatenated with the original prompt and used as input for the LLM. An integrated example of this approach in a healthcare setting is presented with Almanac.107 Using RAG with curated, high-quality sources considerably boosts the factual correctness and quality of LLM outputs over natively prompting the LLM."}, {"title": "5 Conclusion", "content": "In this work, we discussed technical considerations of using LLMs in radiology, and emphasized the importance of understanding their capabilities and limitations. We argue that pretraining on radiology-specific data is impractical for most labs and application-focused departments. Instead, we recommend an iterative approach: starting with optimized prompting of general-purpose models, then enriching context through in-context learning (providing examples) and retrieval-augmented generation (providing relevant input), enabling access to suitable tools, and finally fine-tuning if necessary. For fine-tuning, meticulous dataset curation is essential. We recommend using locally deployed open models to protect sensitive data while ensuring reproducibility. Ultimately, human expert evaluation and interdisciplinary teamwork between radiologists and computer scientists are the cornerstones to guide the development of safe and effective development and use of LLMs for radiology. As the field is rapidly evolving, best practices need to be periodically reassessed, since the effectiveness of these techniques may vary across different LLM types and versions. We hope this review inspires radiologists and others in the field to explore LLM technology for improving radiology practice."}]}