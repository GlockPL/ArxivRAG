{"title": "OMNIEDIT: BUILDING Image EDITING GENERALIST\nMODELS THROUGH SPECIALIST SUPERVISION", "authors": ["Cong Wei", "Zheyang Xiong", "Weiming Ren", "Xinrun Du", "Ge Zhang", "Wenhu Chen"], "abstract": "Instruction-guided image editing methods have demonstrated significant poten-\ntial by training diffusion models on automatically synthesized or manually an-\nnotated image editing pairs. However, these methods remain far from practical,\nreal-life applications. We identify three primary challenges contributing to this\ngap. Firstly, existing models have limited editing skills due to the biased synthesis\nprocess. Secondly, these methods are trained with datasets with a high volume\nof noise and artifacts. This is due to the application of simple filtering methods\nlike CLIP-score. Thirdly, all these datasets are restricted to a single low resolu-\ntion and fixed aspect ratio, limiting the versatility to handle real-world use cases.\nIn this paper, we present OMNI-EDIT, which is an omnipotent editor to handle\nseven different image editing tasks with any aspect ratio seamlessly. Our con-\ntribution is in four folds: (1) OMNI-EDIT is trained by utilizing the supervision\nfrom seven different specialist models to ensure task coverage. (2) we utilize im-\nportance sampling based on the scores provided by large multimodal models (like\nGPT-40) instead of CLIP-score to improve the data quality. (3) we propose a new\nediting architecture called EditNet to greatly boost the editing success rate, (4) we\nprovide images with different aspect ratios to ensure that our model can handle\nany image in the wild. We have curated a test set containing images of different\naspect ratios, accompanied by diverse instructions to cover different tasks. Both\nautomatic evaluation and human evaluations demonstrate that OMNI-EDIT can\nsignificantly outperform all the existing models. Our code, dataset and model will\nbe available at https://tiger-ai-lab.github.io/OmniEdit/", "sections": [{"title": "INTRODUCTION", "content": "Image editing, particularly when following user instructions to apply semantic transformations to\nreal-world photos, has seen significant advancements. Recently, text-guided image editing (Brooks\net al., 2023) has gained prominence over traditional methods such as mask-based or region-based\nediting (Meng et al., 2022). With the rise of diffusion models (Rombach et al., 2022; Podell et al.,\n2024; Chen et al., 2024a; Sauer et al., 2024), numerous diffusion-based image editing techniques\nhave emerged. Generally, they can be roughly divided into two types: (1) Inversion-based meth-\nods (Parmar et al., 2023; Kawar et al., 2023; Gal et al., 2023; Xu et al., 2023; Tumanyan et al.,\n2023; Tsaban & Passos, 2023) propose to perform zero-shot image editing by inverting the diffusion\nprocess and manipulating the attention map in the intermediate diffusion steps to achieve desired\nediting goal. (2) End-to-end methods (Brooks et al., 2023; Zhang et al., 2024a; Sheynin et al., 2024;\nZhao et al., 2024; Fu et al., 2024) propose to fine-tune an existing diffusion model on large-scale\nimage editing pairs to learn the editing operation in an end-to-end fashion. End-to-end methods have\ngenerally achieved better performance than inversion-based methods and gained higher popularity."}, {"title": "PRELIMINARIES", "content": ""}, {"title": "TEXT-TO-IMAGE DIFFUSION MODELS", "content": "Diffusion models (Song et al., 2021; Ho et al., 2020) are a class of latent variable models parameter-\nized by \\theta, defined as $p_{\\theta}(x_0) := \\int p_{\\theta}(x_{0:T}) dx_{1:T}$, where $x_0 \\sim q(x_0)$ represents the original data,\nand $x_1,..., x_T$ are progressively noisier latent representations of the input image $x_0$. Through-\nout the process, the dimensionality of $x_0$ and the latent variables $x_{1:T}$ remains consistent, with\n$X_{0:T} \\in \\mathbb{R}^d$, where $d$ corresponds to the product of the image\u2019s height, width, and channels. The\nforward (diffusion) process, denoted as $q(x_{1:T}|X_0)$, is a predefined Markov chain that incrementally\nadds Gaussian noise to the data according to a pre-defined schedule $\\{t\\}_{t=1}^T$. The process of forward"}, {"title": "diffusion is defined as:", "content": "$q(X_{1:T}|X_0) = \\prod_{t=1}^T q(x_t|x_{t-1}), q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I),$"}, {"title": "", "content": "where $\\mathcal{N}$ denotes a Gaussian distribution, and $\\beta_t$ controls the amount of noise added at each step.\nThe objective of diffusion models is to reverse this diffusion process by learning the distribution\n$p_{\\theta}(x_{t-1}|x_t)$, which enables the reconstruction of the original data $x_0$ from a noisy latent $x_t$. This\nreduces to a denoising problem where the model $\\epsilon_{\\theta}$ is trained to denoise the sample $x_t \\sim q(x_t|x_0)$\nback into $x_0$. The maximum log-likelihood training objective breaks down to minimizing the\nweighted mean squared error between the model\u2019s prediction $x_{\\epsilon}(x_t, c)$ and the true data $x_0$:\n$\\arg \\max_{\\theta} \\log p_{\\theta} (x_0|c) = \\arg \\min_{\\theta} \\mathbb{E}_{(x_0,c) \\sim \\mathcal{D}} [\\mathbb{E}_{\\epsilon,t} [w_t. ||\\hat{\\epsilon}_{\\theta}(x_t, c) - x_0||^2]],$"}, {"title": "", "content": "where $(x_0, c)$ pairs come from the dataset $\\mathcal{D}$, with $c$ representing the text prompt. The term $w_t$ is\na weighting factor applied to the loss at each timestep $t$. For simplicity, prior papers (Song et al.,\n2021; Ho et al., 2020; Karras et al., 2022) will set $w_t$ to be 1."}, {"title": "INSTRUCTION-BASED IMAGE EDITING IN SUPERVISED LEARNING", "content": "Instruction-based image editing can be formulated as a supervised learning problem. Existing meth-\nods (Brooks et al., 2023; Zhang et al., 2024a) often adopt a paired training dataset of text editing\ninstructions and images before and after the edit. An image editing diffusion model is then trained\non this dataset. The latent diffusion objective is defined as:\n$\\arg \\max_{\\theta} \\log p_{\\theta} (x'_0|x_0, c) = \\arg \\min_{\\theta} \\mathbb{E}_{(x'_0,x_0,c) \\sim \\mathcal{D}} [\\mathbb{E}_{\\epsilon,t}||\\epsilon_{\\theta}(x_t, c) - x'_0||^3],$"}, {"title": "", "content": "where $(x'_0, x_0, c)$ triples are sampled from the dataset $\\mathcal{D}$ with $x_0$ denoting the source image, $c$\ndenoting the editing instruction and $x'_0$ denoting the target image."}, {"title": "LEARNING WITH SPECIALIST SUPERVISION", "content": "In this section, we introduce the entire specialist-to-generalist learning framework to build OMNI-\nEDIT. We describe the overall learning objective in subsection 3.1. We then describe how we\nlearn the specialists in subsection 3.2 and the importance weighting function in subsection 3.3. In\nFigure 2, we show the overview of the OMNI-EDIT training pipeline."}, {"title": "LEARNING OBJECTIVE", "content": "We assume there is a groundtruth editing model $p(x'|x, c)$, which can perform any type of editing\ntasks perfectly according to the instruction $c$. Our goal is to minimize the divergence between\n$p_{\\theta}(x'|x, c)$ with $p(x'|x, c)$ by updating the parameters $\\theta$:\n$\\mathcal{L}(\\theta) := \\sum_{x,c} D_{KL}(p(x'|x, c)||p_{\\theta}(x'|x,c)) = - \\sum_{x,c} \\sum_{x'}p(x'|x, c) \\log p_{\\theta} (x'|x, c) + C$"}, {"title": "", "content": "where $C$ is a constant, which we leave out in the following derivation. However, since we don't\nhave access to $p(x'|x, c)$, we adopt importance sampling for approximation:\n$\\mathcal{L}(\\theta) = - \\sum_{x,c} \\sum_{x'}q(x'|x, c) \\frac{p(x'|x, c)}{q(x'|x, c)} \\log p_{\\theta} (x'|x, c)$\n$\\approx -\\mathbb{E}_{(x,c)\\sim \\mathcal{D}} [\\mathbb{E}_{x' \\sim q(x'|x,c)} [\\lambda(x', x, c) \\log p_{\\theta} (x'|x, c)]]$\n$\\approx -\\mathbb{E}_{(x,c)\\sim \\mathcal{D}} [\\mathbb{E}_{x' \\sim q_s(x'|x,c)} [\\lambda(x', x, c) \\log p_{\\theta} (x'|x, c)]]$"}, {"title": "", "content": "where $q(x'|x, c)$ is the proposal distribution and $\\lambda(\\cdot)$ is the importance function. To better approx-\nimate the groundtruth distribution $p(x'|x, c)$, we propose to use an ensemble model $q(x'|x, c)$. In\nessence, $q(x'|x, c) := q_s(x'|x, c)$, where $q_s$ is a specialist distribution decided by the type of the"}, {"title": "", "content": "instruction c (e.g. object removal, object addition, stylization, etc). Combing with Equation 3, our\nobjective can be rewritten as:\n$\\arg \\min L(\\theta) = \\arg \\min \\mathbb{E}_{(x,c)\\sim \\mathcal{D}}\\mathbb{E}_{x' \\sim q_s(x'|x,c)}\\lambda(x', x, c) [\\mathbb{E}_{e,t}||\\chi_{\\theta} (x_t, x, c) - x'||^2].$"}, {"title": "", "content": "The whole process can be described as: we first sample a pair from dataset $\\mathcal{D}$, and then choose the\ncorresponding specialist $q_s$ to sample demonstrations $x'$ for the our editing model $\\chi_{\\theta} (x_t, x, c)$ to\napproximate with an importance weight of $\\lambda(x', x, c)$. We formally provide the algorithm in 1. In\nour specialist-to-generalist framework, we need to have a series of specialist models $\\{q_s\\}_s$ and an\nimportance function $\\lambda(\\cdot)$. We describe them separately in subsection 3.2 and subsection 3.3."}, {"title": "CONSTRUCTING SPECIALIST MODELS", "content": "We group the image editing task into 7 categories as summarized in Table 2. For each category,\nwe train or build a task specialist $p_s(x' | x, c)$ to generate millions of examples. Table 2 provides"}, {"title": "IMPORTANCE WEIGHTING", "content": "The importance weighting function $\\lambda$ takes as input a tuple of source image, edited image, and edit-\ning prompt. Its purpose is to assign higher weights to data points that are more likely to be sampled\nfrom the ground truth distribution, and lower weights to the unlikely ones. This is essentially a qual-\nity measure to up-weight high-quality samples. Unlike previous work, we do not use CLIP score\nbecause prior work (Jiang et al., 2024) has shown its low correlation with human judges. Instead,\nwe propose to use large multimodal models (LMMs) to approximate the weighting function, as they\ndemonstrate strong image understanding. Following VIEScore (Ku et al., 2024), we designed a\nprompting template for GPT-40 (Achiam et al., 2023) to evaluate the image editing pairs and output"}, {"title": "EDITNET", "content": "We found that directly fine-tuning a pre-trained high-quality diffusion model like SD3 using channel-\nwise image concatenation methods (Brooks et al., 2023) compromises the model\u2019s original repre-\nsentational capabilities (see Figure 7 and Section 5.2 for details comparison).\nTo enable a diffusion transformer to perform instruction-based image editing while preserving its\noriginal capabilities, we introduce EditNet to build OMNI-EDIT. EditNet can effectively transform\ncommon DIT models like SD3 into editing models. As illustrated in Figure 4, we replicate each\nlayer of the original DIT block as a control branch. The control branch DIT blocks allow interaction\nbetween the original DIT tokens, conditional image tokens, and the editing prompts. The output of\nthe control branch tokens is then added to the original DIT tokens and editing prompts. Since the\noriginal DIT blocks are trained for generation tasks and are not aware of the editing instructions\nspecifying which contents to modify and how to modify them, this design allows the control branch\nDIT to adjust the representations of the original DIT tokens and editing prompts according to the\nediting instruction, while still leveraging the strong generation ability of the original DIT. Com-\npared to ControlNet (Zhang et al., 2023), our approach offers two key advantages that make it more\nsuitable for image editing tasks: First, ControlNet does not update text representations, making it\nchallenging to execute editing tasks based on instruction, particularly object removal, as it fails to\nunderstand the \u201cremoval\u201d intent (see Figure 6). Secondly, ControlNet's control branch operates in\nparallel without access to the original branch's intermediate representations. This fixed precompu-"}, {"title": "EXPERIMENTS", "content": "In this section, we first provide statistics of the OMNI-EDIT training set and test set in Table 5. Then\nwe introduce the human evaluation protocol in Section 5, and comparative baseline system in 5. We\npresent the main results in Section 5.1, highlighting the advantages of OMNI-EDIT in tacking multi-\naspect ratio, multi-resolution, and multi-task image editing. In Section 5.2, we study the advantages\nof importance sampling for synthetic data. In Section 5.2, we perform an analysis to study the design\nof OMNI-EDIT."}, {"title": "OMNI-EDIT Training Dataset.", "content": "We constructed the training dataset D by sampling high-resolution\nimages with a minimum resolution of 1 megapixel from the LAION-5B (Schuhmann et al., 2022)\nand OpenImageV6 (Kuznetsova et al., 2020) databases. The images cover a range of aspect ratios\nincluding 1:1, 2:3, 3:2, 3:4, 4:3, 9:16, and 16:9. For the task of object swap, we employed a spe-\ncialist model to generate 1.5 million entries. We then applied InternVL2 for importance weighting,\nretaining samples with scores of 9 or higher, resulting in a dataset of 150K entries for this task. Sim-\nilarly, we generate 250k-1M samples for each task, then keep the top 10% as the final dataset. The\nfinal training dataset comprises 775K entries, with detailed information provided in Appendix 6."}, {"title": "OMNI-EDIT-Bench.", "content": "To create a high-resolution, multi-aspect ratio, multi-task benchmark for\ninstruction-based image editing, we manually collected 62 images from pexels (2024) and LAION-\n5B (Schuhmann et al., 2022). These images cover a variety of aspect ratios, including 1:1, 2:3, 3:2,\n3:4, 4:3, 9:16, and 16:9. We ensured that the images feature a diverse range of scenes and object\ncounts, from single to complex compositions. Additionally, we selected images with a relatively\nhigh aesthetic score to better align with the practical use cases of image editing. For each image, we\ntasked the model with performing 7 tasks as outlined in Table 2. This results in a total of 434 edits."}, {"title": "OMNI-EDIT implementation details.", "content": "The OMNI-EDIT model is built upon Stable diffusion 3\nMedium(Esser et al., 2024) with EditNet architecture. The stable diffusion 3 has 24 DiT layers.\nEach layer has a corresponding EditNet layer. We train OMNI-EDIT on the 775K OMNI-EDIT train-"}, {"title": "MAIN RESULTS", "content": "We provide a qualitative comparison with baseline models in Figure 5. We show the top 4 baselines\nwith OMNI-EDIT on a subset of the OMNI-EDIT-Bench. We provide more results in Figure 10 and\nFigure 11. Our main results are detailed in Table 3, where we provide the VIEScore and conduct\nhuman evaluation on the Top2 baselines and OMNI-EDIT. In Figure 1, OMNI-EDIT demonstrates\nits capability to handle diverse editing tasks across various aspect ratios and resolutions. The results\nare notably sharp and clear, especially in the addition/swap task, where new content is seamlessly\nintegrated. This underscores the effectiveness of the Edit-Net design in preserving the original image\ngeneration capabilities of the base text-image generative model. Similarly, in Figure 5, OMNI-EDIT\nuniquely adds a clean and distinct NASA logo onto a T-shirt. Table 3 corroborates this with OMNI-\nEDIT achieving the highest Perceptual Quality (PQ) score among the models evaluated.\nWe highlight the efficacy of our proposed specialist-to-generalist learning framework. Unlike base-\nline models that utilize a single method for generating synthetic data\u2014often the prompt-to-prompt\nmethod-This method typically alters the entire image, obscuring task-specific data. In contrast,\nOMNI-EDIT leverages task-specific data curated by experts, resulting in a clearer task distribution\nand improved adherence to editing instructions. Both the VIEScore and human evaluations in Ta-\nble 3 demonstrate that our method significantly outperforms the best baseline in following editing\ninstructions accurately and minimizing over-editing. For instance, baseline models frequently mis-\nunderstand the task intent as illustrated in Figure 5, where the CosXL-Edit model fails to recognize\nthe removal task and incorrectly interprets a bird addition as a swap between a panda and a bird.\nLastly, baseline models often produce blurry images on the OMNI-EDIT-Bench, as they are trained\nat resolutions limited to 512x512 or even 256x256, and they perform poorly on non-square aspect\nratios. For example, with a 3:4 aspect ratio, the baselines struggle to perform editing. OMNI-EDIT,"}, {"title": "ABLATION STUDY", "content": "In this section, We provide an ablation study w.r.t importance weighting and EditNet."}, {"title": "Ablation study on the importance sampling.", "content": "We study a baseline that utilizes the same archi-\ntecture as OMNI-EDIT, but instead of applying importance scoring and filtering, we sample 775K\nexamples directly from the 5M pre-filtering dataset as specified in Table 6 and compare it with\nOMNI-EDIT. As shown in Table 4, we observe a significant decrease in VIEScores for both PQ and\nSC metrics."}, {"title": "Ablation Study on OMNI-EDIT Architecture Design.", "content": "We conducted an analysis of OMNI-EDIT\n's architectural design in comparison to two baseline models: OMNI-EDIT-ControlNet and OMNI-\nEDIT-ControlNet-TextControl and show the result in Table 5. OMNI-EDIT-ControlNet represents\nthe SD3-ControlNet architecture trained on the OMNI-EDIT dataset, where the source image serves\nas the conditioning image for the control branch. OMNI-EDIT-ControlNet-TextControl is a variant\nof OMNI-EDIT-ControlNet with an added modification: at each layer, we incorporate the text-token\noutput from the control branch into the text-token in the main image generation branch. So this\nbaseline can update the text representation in the main branch but doesn't have the intermediate\nrepresentation interaction design in EditNet."}, {"title": "RELATED WORK", "content": "Image Editing via Generation. Editing real images according to specific user requirements has\nbeen a longstanding research challenge (Crowson et al., 2022; Liu et al., 2020; Zhang et al., 2023;\nShi et al., 2022; Ling et al., 2021). Since the introduction of large-scale diffusion models, such as\nStable Diffusion (Rombach et al., 2022; Podell et al., 2024), significant progress has been made in\ntackling image editing tasks. SDEdit (Meng et al., 2022) introduced an approach that adds noise to\nthe input image at an intermediate diffusion step, followed by denoising guided by the target text"}, {"title": "DISCUSSION", "content": "In this paper, we identify the imbalanced skills in the existing end-to-end image editing methods and\npropose a new framework to build more omnipotent image editing models. We surveyed the field\nand chose several approaches as our specialists to synthesize candidate pairs and adopt weighted loss\nto supervise the single generalist model. Our approach has shown significant quality boost across\nthe broad editing skills. Throughout the experiments, we found that the output quality is highly\ninfluenced by the underlying base model. Due to the weakness of SD3, our approach is still not\nachieving its highest potential. In the future, we plan to use Flux or other more capable base models\nto see how much further we can reach with the current framework."}, {"title": "TRAINING DATA GENERATION DETAILS", "content": ""}, {"title": "OBJECT REPLACEMENT", "content": "We trained an image-inpainting model to serve as the expert for object replacement. During training,\ngiven a source image xsrc and an object caption Cobj, we employ GroundingDINO and SAM to\ngenerate an object mask Mobj. The masked image is then created by removing the object from the\nsource image:\n$x_{masked} \\coloneqq x_{src} \\odot (1 - M_{obj}).$"}, {"title": "", "content": "Here, $\\odot$ denotes element-wise multiplication, effectively masking out the object in $x_{src}$. Both the\nmask $M_{obj}$ and the object caption $C_{obj}$ are provided as inputs to the expert model $q_{obj\\_replace}$. The\nexpert $q_{obj\\_replace}$ is trained to reconstruct (inpaint) the original source image $x_{src}$ from the masked\nimage.\nTo generate data for object replacement, we sample 200K images from the LAION and OpenImages\ndatasets, ensuring a diverse range of resolutions close to 1 megapixel. For each image, we utilize\nGPT-40 to propose five object replacement scenarios. Specifically, GPT-4o identifies five interest-\ning source objects $C_{src\\_obj}$ within the image and suggests corresponding target objects $C_{trg\\_obj}$ for\nreplacement.\nFor each proposed replacement, we perform the following steps:"}, {"title": "OBJECT REMOVAL", "content": "Similar to object replacement, we trained an image inpainting model to serve as the expert for object\nremoval. During training, given a source image xsrc and an image caption Csrc, we randomly apply\nstrikes to create a mask Msrc. The masked image is then created by:\n$x_{masked} = x_{src} \\odot (1-M_{src})$"}, {"title": "", "content": "Both the mask Msrc and the image caption Csrc are provided as inputs to the expert model qobj_removal.\nThe expert qobj_removal is trained to reconstruct (inpaint) the original source image xsrc from the\nmasked image. To generate data for object removal, we also sample 200K images from the LAION\nand OpenImages datasets, ensuring a diverse range of resolutions close to 1 megapixel. For each\nimage, we utilize GPT-40 to propose five objects to remove and predict the content of the space after\nremoval. Specifically, GPT-4o identifies five interesting source objects Csrc_obj within the image and\npredicts the new content after removing the object Ctrg_background. For each proposed removal, we\nperform the following steps:"}, {"title": "OBJECT ADDITION", "content": "We conceptualize the object addition task as the inverse of the object removal process. Specifically,\nfor each pair of editing examples generated by the object removal expert, we swap the roles of the\nsource and target images to create a new pair tailored for object addition. This approach leverages the\nnaturalness and artifact-free quality of the original source images, ensuring high-quality additions.\nGiven a pair of editing examples (Xsrc_removal, Xedit_removal, Cremoval) generated for object removal and"}, {"title": "ATTRIBUTE MODIFICATION", "content": "We adapt the Prompt-to-Prompt (P2P) (Hertz et al., 2023) pipeline where a text-guided image gen-\neration model is provided with a pair of captions (Csrc, Cedit) and injects cross-attention maps from\nthe input image generation to that during edited image generation. For example, a pair could\nbe \"a blue backpack\", \"a purple backpack\") with the corresponding editing instruction \"make the\nbackpack purple\".\nTo enable precise attribute modification on the object we want (in our example, the \u201cbackpack\"),\nwe adapt the method from Sheynin et al. (2024) where we provide an additional mask Mobj that\nmasks the object. Specifically, to obtain a pair of captions, we obtain source captions Csrc from\nZhang (2024) and let GPT4 to identify an object Cobj in the original caption Csrc, propose an editing\ninstruction that edits an attribution of Cobj and output the edited caption Cedit with object's attribution\nreflected.\nWe first let the image generation model to generate a source image xsrc using Csrc. We then use\nGroundingDINO to extract mask Mobj that masks the object from the source image. We then apply\nP2P generation with caption pair (Csrc, Cedit). During the generation, we use the mask to control\nprecise image editing control. In particular, let xsrc,t denote the noisy source image at step t and\nXedit,t denote the noisy edited image at step t, we apply the mask and force the new noisy edited\nimage at time t be Mobj Xedit,t + (1 - Mobj) Xsrc,t. In other words, we keep background the same\nand only edit the object selected.\""}, {"title": "ENVIRONMENT MODIFICATION", "content": "For environment modification, we use P2P pipeline to generate original and edited image. To ensure\nstructural consistency between two images, we apply a mask of the foreground to maintain details\nin the foreground while changing the background. In particular, given a source image caption Csrc,\nwe use GPT4 to identify the foreground (e.g., an object or a human) and apply GroundingDINO to\nextract mask Mforeground. During the generation, let xsrc,t denote the noisy source image at step t and\nXedit,t denote the noisy edited image at t. We apply the mask so that the new noisy edited image at\ntime t is Mforeground Xsrc,t + (1 - Mforeground) Xedit,t. We also set Tenv = 0.7 so that this mask\noperation on noisy image is only applied at the first Tenv of all timesteps."}, {"title": "BACKGROUND SWAP", "content": "We trained an image inpainting model to serve as the specialist Qobj_background_swap. We use a sim-\nilar procedure as in the object replacement but use an inverse mask of the object to indicate the\nbackground to guide the inpainting."}, {"title": "STYLE TRANSFER", "content": "We use CosXL-Edit (Boesel & Rombach, 2024) as the expert style transfer model. We provide\nCosXL-Edit with (xsrc, c) and let it generates the edited image xedited."}, {"title": "IMPORTANCE SAMPLING", "content": "We apply the importance sampling as described in Section 3.3. Example prompts that are provided\nto LMMs are shown in Figure 8 and 9. We compute the Overall score following (Ku et al., 2024)\nas the importance weight. After importance sampling, we obtain our training dataset described in\nTable 6."}]}