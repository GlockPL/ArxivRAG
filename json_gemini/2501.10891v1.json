{"title": "OpenEarthMap-SAR: A Benchmark Synthetic Aperture Radar Dataset for Global High-Resolution Land Cover Mapping", "authors": ["Junshi Xia", "Hongruixuan Chen", "Clifford Broni-Bediako", "Yimin Wei", "Jian Song", "Naoto Yokoya"], "abstract": "High-resolution land cover mapping plays a crucial role in addressing a wide range of global challenges, including urban planning, environmental monitoring, disaster response, and sustainable development. However, creating accurate, large-scale land cover datasets remains a significant challenge due to the inherent complexities of geospatial data, such as diverse terrain, varying sensor modalities, and atmospheric conditions. Synthetic Aperture Radar (SAR) imagery, with its ability to penetrate clouds and capture data in all-weather, day-and-night conditions, offers unique advantages for land cover mapping. Despite these strengths, the lack of benchmark datasets tailored for SAR imagery has limited the development of robust models specifically designed for this data modality. To bridge this gap and facilitate advancements in SAR-based geospatial analysis, we introduce OpenEarthMap-SAR, a benchmark SAR dataset, for global high-resolution land cover mapping. OpenEarthMap-SAR consists of 1.5 million segments of 5033 aerial and satellite images with the size of 1024\u00d71024 pixels, covering 35 regions from Japan, France, and the USA, with partially manually annotated and fully pseudo 8-class land cover labels at a ground sampling distance of 0.15-0.5 m. We evaluated the performance of state-of-the-art methods for semantic segmentation and present challenging problem settings suitable for further technical development. The dataset also serves the official dataset for IEEE GRSS Data Fusion Contest Track I. The dataset has been made publicly available at https://zenodo.org/records/14622048.", "sections": [{"title": "I. INTRODUCTION", "content": "High-resolution Synthetic Aperture Radar (SAR) imagery plays a crucial role in land cover mapping, especially for applications like land use planning [1], disaster response [2], [3], and resource management [4]. While medium-resolution satellite imagery, e.g., 10-30m ground sampling distance (GSD), has been widely used in global land cover mapping [5], [6], high-resolution SAR (sub-meter GSD) provides significant advantages, especially in detecting features like buildings, roads, and other infrastructure in challenging conditions, such as adverse weather or night-time.\nRecent developments in SAR-based land cover mapping have seen major progress, with several studies successfully mapping building footprints over large areas using high-resolution SAR imagery [1], [7]. Unlike optical imagery, SAR has a unique set of characteristics that make it particularly valuable for geospatial mapping and monitoring, especially in regions where optical imagery faces significant limitations. One of the most notable advantages of SAR is its insensitivity to cloud cover and weather conditions. SAR's all-weather capability means it can provide continuous and consistent monitoring over long periods, which is crucial for tracking dynamic changes in the environment, such as urban growth, deforestation, or disaster response. This consistency is particularly valuable for real-time monitoring applications, as it reduces the data gaps typically associated with optical imagery when clouds or adverse weather block the view [8]-[10].\nWith the rise of deep learning technologies, substantial efforts have been made to create benchmark datasets for high-resolution SAR image analysis [1]\u2013[3], [11]. Datasets providing large-scale SAR data have significantly advanced the development of object detection methods, supporting key applications like vehicle detection and infrastructure monitoring. Benchmark datasets such as SIVED [12], SADD [13], MSAR [14], and SARDet-100K [15] have been crucial in driving this progress. Additionally, SAR datasets have been used for change detection and disaster damage mapping [2], [3], further demonstrating their real-world applicability.\nThe challenge with benchmark datasets for semantic segmentation at sub-meter SAR resolution lies in the limited availability of high-quality public datasets, poor annotation quality, and the difficulties associated with detailed annotation. Publicly available datasets are scarce due to the lack of high-resolution SAR imagery in many regions and the commercial restrictions on redistributing such data. Moreover, the cost and complexity of manually annotating sub-meter resolution SAR images are substantial, requiring significant expertise and resources, making it difficult to achieve consistent, reliable, and high-quality annotations. As a result, many datasets resort to using coarse annotations, such as bounding boxes or minimal class labels, or rely on indirect sources like OpenStreetMap, which may not fully capture the complexity of the land cover. The inherent challenges of SAR sensing mechanisms further complicate direct annotation. SAR images have limitations in terms of the visibility of certain land cover classes due to their sensitivity to surface roughness, moisture,"}, {"title": "II. THE DATASET", "content": "In this work, we have carefully selected 35 diverse regions from the USA, Japan, and France , encompassing a wide range of urban and rural landscapes. These regions were chosen to represent various geographic and environmental conditions, ensuring that the dataset captures the complexity and diversity of real-world land cover scenarios.\nThe SAR datasets are sourced from Umbra Space 1, featuring Spotlight mode with a resolution ranging from 0.15m"}, {"title": "A. Data preparation", "content": "Classes: We provide annotations and pesudo labels with eight classes: bareland, rangeland, developed space, road, tree, water, agriculture land, and building. The class selection is consistent with our previous benchmark datasets, Open-EarthMap [21] as well as other benchmark datasets (e.g., LoveDA [26] and DeepGlobe [27]) with sub-meter GSD.\nAnnotations: The pseudo labels are generated from pre-trained OpenEarthMap models [21]\u2013[25]. In addition, for each region, we manually annotated 20 images to ensure the quality and consistency of the dataset, thereby improving the accuracy of the labels.\nAdditionally, when assessing the performance using pseudo-labels and 700 real labels, the mean agreement rate between the two sets of annotations drops to 68% (seen in Table III), especially for the class Bareland and Agriculture Land. This lower agreement indicates a noticeable gap in the quality of pseudo-labels compared to manually-labeled ones. The discrepancy arises because pseudo-labels are typically generated"}, {"title": "B. Classes, Annotations, and Data Split", "content": "Data split: The dataset is split into several subsets for different tasks, with distinct training, validation, and testing configurations. For the semantic segmentation task, the models will be evaluated using 490 images from the manually labeled dataset, which contains 14 real labels per region. The following training scenarios are defined:\n\u2022 Using only pseudo labels: The model will be trained exclusively on 4333 images, respectively, all containing pseudo labels.\n\u2022 35 real labels (1 per region) + pseudo labels: The model will be trained using 35 real labels (1 per region) combined with pseudo labels from the unannotated images.\n\u2022 175 real labels (5 per region) + pseudo labels: The model will be trained using 175 real labels (5 per region) combined with pseudo labels from the unannotated images.\n\u2022 Real labels (1/5 per region) only: The model will be trained exclusively on the real labels (1 or 5 per folder).\nFor the semantic segmentation task, we will evaluate the performance with Optical, Optical, and Optical+SAR data combinations to assess the models across different modalities."}, {"title": "C. Comparison with other datasets", "content": "The OpenEarthMap-SAR dataset stands out in comparison to other high-resolution SAR benchmark datasets in terms of its region coverage and class diversity. While datasets like FU-SAR [16], GF-3 Building [17], and BDD [18] focus on"}, {"title": "III. LAND COVER SEMANTIC SEGMENTATION", "content": null}, {"title": "A. Baselines", "content": "For the land cover semantic segmentation task, current mainstream architectures, including those based on CNN, Transformers and Mamba, were evaluated and compared on the OpenEarthMap-SAR dataset. More specifically, the chosen models are U-Net [28], SegFormer [29], and VMamba [30]."}, {"title": "B. Results", "content": "In this experiment, the performance of baseline models for semantic segmentation was analyzed across different modalities: Optical, SAR, and their combination (SAR+Optical). The models evaluated include U-Net, SegFormer, and VMamba, with results presented for varying difference cases (P, P+R1, P+R5, R1, R5) in Table IV. Fig. 3 provide a visual comparison of semantic segmentation performance using U-Net across three modalities\u2014Optical, SAR, and SAR+Optical-based on both pseudo labels (P) and real labels (R5). Across all scenarios, Optical and SAR+Optical consistently outperformed SAR alone. For VMamba, SAR+Optical provided better performance than Optical. However, for U-Net and SegFormer, SAR+Optical either matched or slightly underperformed compared to Optical.\nFor all three models, performance with Optical data improved significantly when moving from pseudo labels (P) to real labels, especially with the use of 5 real labels per region (R5). For example, U-Net's mIoU increased from 56.56% (P) to 65.10% (R5) with Optical data, while SegFormer and VMamba also exhibited similar improvements. This indicates that real labels are crucial for enhancing the model's segmentation accuracy, particularly for Optical-based models.\nIn contrast, while the performance of models trained on SAR data slightly improved with the inclusion of real labels in the pseudo-labels, it remained consistently lower than those trained with Optical data. Even when real labels were included, the performance did not reach the level of pseudo-labeled data. For instance, the mIoU for VMamba with SAR improved from 34.74% (P) to 31.90% (R5) with real labels. This indicates that SAR data, although valuable, still does not offer as much discriminative power for segmentation tasks as RGB data, particularly when only a limited number of real labels are available.\nThe combination of SAR and Optical (SAR+Optical) produced improved results compared to using SAR alone, but still did not slightly improve the performance of Optical-based models. This indicates that while combining modalities can help enhance performance, Optical remains the more effective modality for semantic segmentation tasks. The best performance was achieved using VMamba when 5 real labels per"}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "We introduce OpenEarthMap-SAR, a comprehensive benchmark dataset designed for global high-resolution land cover mapping using synthetic aperture radar (SAR) data. OpenEarthMap-SAR consists of 1.5 million segments of 5033 aerial and satellite images, each with a resolution of 1024x1024 pixels, spanning 35 regions in Japan, France, and the USA. The dataset features partially manually annotated and fully pseudo-labeled 8-class land cover categories, with a ground sampling distance ranging from 0.15 to 0.5 meters. OpenEarthMap-SAR offers a unique combination of high-resolution SAR data with extensive geographic coverage, making it an invaluable resource for training and evaluating machine learning and deep learning models for land cover classification.\nThe dataset's contribution is substantial, offering a platform for advancing land cover mapping models by providing both high-resolution imagery and diverse regional data. We present several challenges in the dataset, such as the integration of SAR data with optical imagery for more comprehensive segmentation tasks, and we propose new problem settings for further research. These challenges provide opportunities for developing more robust models capable of handling complex, real-world geospatial tasks. However, OpenEarthMap-SAR also has certain limitations. Firstly, while it includes SAR imagery, the data quality can vary due to factors like weather conditions and the inherent noise in SAR data. Furthermore, the pseudo-labels derived from optical images, while efficient, may introduce inaccuracies compared to manually labeled SAR data due to differences in data acquisition times, sensor angles, and the inherent characteristics of each modality. Variations in the timing of data collection can result in changes in the landscape not captured in both datasets. Additionally, the differences in how SAR and optical images represent objects-SAR focusing on surface structure and optical imagery capturing color and texture can lead to mismatches. These discrepancies can contribute to label noise, making it more challenging to achieve accurate land cover classification.\nLooking forward, there are several opportunities for expanding OpenEarthMap-SAR to further enhance its utility. The addition of other data modalities, such as height data or multi-spectral optical imagery, could provide complementary information to improve segmentation and classification accuracy. Incorporating more sophisticated pseudo-labeling techniques and leveraging expert manual annotations could further refine the dataset's quality and applicability. Additionally, expanding OpenEarthMap-SAR to cover more diverse regions and types of land cover would make it even more useful for training generalized models that can be deployed in a wide range of geospatial applications.\nOpenEarthMap-SAR has significant potential beyond land cover classification. The dataset can be used in multimodal remote sensing research, such as in land cover change detection, environmental monitoring, and disaster management. It can also support the development of remote sensing foundation models, enabling AI models that generalize across different tasks, geographies, and sensor modalities. By combining SAR with optical data, OpenEarthMap-SAR offers a foundation for building versatile models capable of improving the performance of various downstream applications in real-world geospatial settings.\nIn conclusion, OpenEarthMap-SAR represents a significant advancement in global high-resolution land cover mapping. Its ability to provide all-weather monitoring lays the foundation for diverse applications in environmental monitoring and disaster response. We anticipate that this dataset will be instrumental in developing robust, scalable models that enhance the accuracy and timeliness of geospatial analysis, offering long-term benefits for areas such as urban planning, and climate change mitigation."}]}