{"title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models", "authors": ["Yizheng Huang", "Jimmy X. Huang"], "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.", "sections": [{"title": "1 Introduction", "content": "The advent of ChatGPT has significantly impacted both academia and industry due to its interactive capabilities and widespread application, establishing itself as a leading artificial intelligence tool (Laskar et al., 2023; Jahan et al., 2023; Huang and Huang, 2024). At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by (OpenAI et al., 2023), which has seen numerous enhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language Processing (NLP) tasks (Laskar et al., 2020). Despite these advancements, the adoption of LLMs has highlighted several critical issues primarily due to their reliance on extensive datasets. This reliance restricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data to maximize accessibility and applicability results in subpar performance in specialized areas. Second, the rapid creation of online data, combined with the significant resources required for data annotation and model training, hinders LLMs' ability to stay updated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as \"hallucinations\", which can mislead users.\nAddressing these challenges is crucial for LLMs to be effectively utilized across various domains. A promising solution is the integration of Retrieval-Augmented Generation (RAG) technology, which supplements models by fetching external data in response to queries, thus ensuring more accurate and current outputs. Since its introduction by Lewis et al. (Lewis et al., 2020b) in 2020, RAG technology has undergone significant advancements, particularly influenced by ChatGPT's success. However, there is a noticeable gap in the literature regarding a thorough analysis of RAG's mechanisms and the progress made by subsequent studies. Furthermore, the field is characterized by diverse research focuses and the use of ambiguous terminology for similar methods, leading to confusion. This paper aims to clarify these aspects by offering a structured overview of RAG, categorizing various methods, and delivering an in-depth understanding of this research area. This survey will primarily focus on textual applications of RAG, reflecting the current emphasis of research efforts in this area.\nRAG combines retrieval methods and advanced deep learning to address two main questions: effectively retrieving relevant information and generating accurate responses. The workflow of RAG is outlined in Section 2, categorizing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. These sections, from 3 to 6, provide an in-depth analysis of the technologies within these phases. Section 7 offers summaries of the reviewed studies, along with the retrievers and generators utilized. Section 8 details the evaluation methodologies for RAG. Section 9 explores future research directions, concentrating on text-based studies and extending to image and multimodal data considerations. The conclusion is presented in Section 10.\nThe contributions of this paper are threefold: This paper offers a comprehensive framework for understanding the RAG domain, identifying areas for improvement and challenges for future research. It provides a detailed analysis of RAG's core technologies, examining their strengths in addressing retrieval and generation. Additionally, it introduces the evaluation methods used in RAG research, highlighting current challenges and suggesting promising directions for future studies."}, {"title": "2 RAG Framework", "content": "The hallucinations are largely attributed to LLMs' inability to access up-to-date information. This limitation stems from the models' reliance on their training datasets. RAG proposes a solution to this issue by supplementing the LLM's training data with current information from external sources through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more cost-effective alternative to the extensive training and fine-tuning processes typically required for LLMs. It allows for the dynamic incorporation of fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly integrate this new data into the LLM. This feature makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through RAG is derived from real-world data, authored by humans, which not only simplifies the generation process but also increases the reliability of the generated responses. Research by Khandelwal et al. (Khandelwal et al., 2020) demonstrates that accessing relevant information from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG has evolved from a means of providing supplementary information to enabling multiple interactions between the retrieval and generation components. This involves conducting several rounds of retrieval to refine the accuracy of the information retrieved and iteratively improve the quality of the generated output. Platforms such as LangChain\u00b9 and LlamaIndex\u00b2 have modularized the RAG approach, enhancing its adaptability and expanding its range of applications. Despite these platforms employing diverse methodologies to tackle different aspects of RAG-from multiple search iterations to iterative generation-they maintain adherence to the fundamental RAG workflow. This consistency is crucial for understanding their operation and pinpointing opportunities for further development."}, {"title": "2.1 Basic RAG Workflow", "content": "The foundational workflow of RAG begins with the creation of an index comprising external sources. This index serves as the basis for retrieving relevant information through a retriever model based on a specific query. The final step involves a generator model, which combines the retrieved information with the query to produce the desired output."}, {"title": "2.1.1 Indexing", "content": "Efficient retrieval begins with comprehensive indexing, where data preparation is key. This stage involves text normalization processes such as tokenization, stemming, and the removal of stop words to enhance the text's suitability for indexing (Manning et al., 2008). Text segments are then organized into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing through the use of pretrained LMs for generating semantic vector representations of texts. These"}, {"title": "2.1.2 Retrieval", "content": "While traditional retrieval methods, such as the BM25 algorithm (Hancock-Beaulieu et al., 1996), focus on term frequency and presence for document ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT (Devlin et al., 2019), which capture the semantic essence of queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities. This is typically achieved by measuring vector distances between documents and queries, combining traditional retrieval metrics with semantic understanding to yield search results that are both relevant and aligned with user intent."}, {"title": "2.1.3 Generation", "content": "The generation phase is tasked with producing text that is both relevant to the query and reflective of the information found in the retrieved documents. The usual method involves concatenating the query with the retrieved information, which is then fed into an LLM for text generation (Li et al., 2022). Although ensuring the generated text's alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity. The generated text should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data."}, {"title": "2.2 RAG Paradigm", "content": "The RAG paradigm organizes research within the domain, offering a straightforward yet robust framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for generating high-quality outcomes. Therefore, this paradigm is structured into four main phases from a retrieval perspective: pre-retrieval, retrieval, post-retrieval, and generation. Both single-hop and multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase structure."}, {"title": "2.2.1 Pre-Retrieval", "content": "The pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and query preparation, ensuring efficient information retrieval. This phase includes essential tasks to prepare for effective data access."}, {"title": "Indexing", "content": "The process starts with indexing, which establishes an organized system to enable fast and accurate retrieval of information. The specificity of indexing depends on the task and data type. For example, sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas."}, {"title": "Query Manipulation", "content": "After indexing, query manipulation is performed to adjust user queries for a better match with the indexed data. This involves query reformulation (Jansen et al., 2009; Yu et al., 2020), which rewrites the query to align more closely with the user's intention; query expansion (Huang et al., 2013), which extends the query to capture more relevant results through synonyms or related terms; and query normalization, which resolves differences in spelling or terminology for consistent query matching."}, {"title": "Data Modification", "content": "Data modification is also critical in enhancing retrieval efficiency. This step includes preprocessing techniques like removing irrelevant or redundant information to improve the quality of results and enriching the data with additional information such as metadata to boost the relevance and diversity of the retrieved content (Bevilacqua et al., 2022a)."}, {"title": "2.2.2 Retrieval", "content": "Search & Ranking The retrieval stage is the combination of search and ranking. It focuses on selecting and prioritizing documents from a dataset to enhance the quality of the generation model's outputs. This stage employs search algorithms to navigate through the indexed data, finding documents that match a user's query. After identifying relevant documents, the process of initially ranking these documents starts to sort them according to their relevance to the query."}, {"title": "2.2.3 Post-Retrieval", "content": "The post-retrieval phase serves to refine the initially retrieved documents to improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed at optimizing the document selection for the final generation task."}, {"title": "Re-Ranking", "content": "In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized. The objective is to more accurately highlight the documents most relevant to the query and diminish the importance of the less relevant ones. This step involves incorporating additional metrics and external knowledge sources to enhance precision. In this context, pre-trained models with superior accuracy but lower efficiency can be effectively employed due to the limited set of candidate documents available (Huang and Hu, 2009)."}, {"title": "Filtering", "content": "Filtering aims to remove documents that fail to meet specified quality or relevance standards. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level. Furthermore, the use of feedback from users or prior relevance evaluations assists in adjusting the filtering process, guaranteeing that only the most relevant documents are retained for text generation (Khattab and Zaharia, 2020; Huang and Huang, 2023)."}, {"title": "2.2.4 Generation", "content": "The generation stage is a crucial component of the RAG process, responsible for leveraging retrieved information to enhance the quality of the generated response. This stage encompasses several sub-steps aimed at producing content that is readable, engaging, and informative."}, {"title": "Enhancing", "content": "At the heart of the generation phase is the enhancement step, where the objective is to merge the retrieved information with the user's query to create a coherent and relevant response. This includes the process of elaboration, adding extra details to the retrieved content to enrich it. Efforts are focused on improving the output's quality by increasing its clarity, coherence, and stylistic appeal through methods such as rephrasing and restructuring. Information from various sources is combined to offer a comprehensive perspective, and verification is conducted to ensure the accuracy and relevance of the content."}, {"title": "Customization", "content": "Customization is an optional step, involving the adjustment of content to align with the user's specific preferences or the context of the request. This tailoring includes adapting the content to meet the needs of the target audience or the format in which it will be presented and condensing the information to succinctly convey the essence of the content. The process also entails creating summaries or abstracts that emphasize the key points or arguments, ensuring the output is both informative and concise."}, {"title": "3 Pre-Retrieval", "content": ""}, {"title": "3.1 Indexing", "content": "The integration of the k-nearest neighbor (kNN) algorithm with pre-trained neural LMs, as demonstrated in KNN-LMs (Khandelwal et al., 2020), represents significant progress in language modeling."}, {"title": "3.2 Query Manipulation", "content": "Studies such as FiD (Izacard and Grave, 2021), COK(Li et al., 2023), and Query2doc (Wang et al., 2023a) emphasize the significance of creating new queries or refining existing ones to achieve more pertinent retrieval results. These research efforts highlight the necessity of efficiently gathering evidence from multiple passages and tailoring queries to suit various knowledge sources, whether structured or unstructured. Techniques ranging from the creation of pseudo-documents to enhance queries have shown to bolster retrieval performance across diverse information retrieval datasets.\nFurther exploration into query manipulation has been conducted by Step-Back (Zheng et al., 2023) and PROMPTAGATOR (Dai et al., 2023), which focus on abstracting high-level concepts or utilizing LLMs for prompt-based query generation. These strategies strive to better align queries with the retrieval system's functionality by rephrasing tasks into more generalized versions or crafting task-specific queries from limited examples. Such methodologies enhance the consistency between queries and indexed data, facilitating the retrieval of more pertinent and insightful information.\nMoreover, KnowledGPT (Wang et al., 2023b) and Rewrite-Retrieve-Read (Ma et al., 2023) introduce approaches for query manipulation through \"program of thought\" prompting and innovative query rewriting techniques. KnowledGPT innovates by generating code to interface with knowledge bases, converting user queries into structured search commands. In contrast, Rewrite-Retrieve-Read utilizes a trainable compact LM for query reformulation, adjusting them to more effectively reflect the user's intent and context.\nLastly, FLARE (Jiang et al., 2023) presents a strategy based on confidence for query formulation,"}, {"title": "3.3 Data Modification", "content": "RA-DIT (Lin et al., 2023b) and RECITE (Sun et al., 2023) emphasize enhancements through internal data modifications. RA-DIT distinguishes between fine-tuning datasets for LLMs and retrievers, aiming to bolster the LLM's contextual comprehension and the retriever's ability to align with queries. RECITE, on the other hand, utilizes passage hints and synthetic question-passage pairs to increase the variety and relevance of its generated recitations and responses. This approach seeks to broaden the model's knowledge base and improve its response accuracy.\nUPRISE (Cheng et al., 2023a) and GENREAD (Yu et al., 2023a) target the refinement of external data. UPRISE converts raw task data into a structured format and refines the selection of prompts to enhance retrieval outcomes. In contrast, the Clustering-Based Prompts method employed by GENREAD generates documents from questions and clusters them to eliminate irrelevant data, enriching the input with varied contextual insights. This technique aims to improve the performance of the generative model by providing it with a richer set of information.\nFurthermore, KnowledGPT (Wang et al., 2023b) is dedicated to augmenting raw text data with structured, semantically rich information through entity linking. This enrichment process not only structures the data more cohesively and makes it more amenable to queries but also boosts the model's retrieval efficiency. It leverages precise, linked knowledge to enhance the model's understanding and its ability to generate relevant responses, thereby improving its overall performance."}, {"title": "4 Retrieval", "content": ""}, {"title": "4.1 Search & Ranking", "content": "Atlas (Izacard et al., 2023) investigates few-shot learning approaches, including Attention Distillation and Perplexity Distillation, to steer the retriever toward retrieving more relevant documents. IRCOT (Trivedi et al., 2023) integrates retrieval with reasoning to improve the effectiveness of retrieval. SURGE (Kang et al., 2023) employs a subgraph retriever to extract relevant subgraphs from a knowledge graph, while AAR (Yu et al., 2023b) modifies search preferences to help LLMs in fetching pertinent documents.\nPRCA (Yang et al., 2023a) focuses on employing domain-specific abstractive summarization to extract relevant and context-rich information from documents, using a supervised learning strategy to prioritize content crucial for accurate query responses. Meanwhile, MEMWALKER (Chen et al., 2023a) leverages an internal search and ranking mechanism in the constructed memory tree to identify pertinent information for long-context question answering. Additionally, the Confidence-based Active Retrieval approach of FLARE (Jiang et al., 2023) dynamically triggers information retrieval based on the confidence levels of generated sentences, utilizing the insight that low-confidence tokens signal a need for external knowledge."}, {"title": "5 Post-Retrieval", "content": ""}, {"title": "5.1 Re-Ranking", "content": "Re2G (Glass et al., 2022) introduces a sequence-pair classification approach for re-ranking, utilizing a BERT transformer to simultaneously analyze the query and passage. This interaction model, employing cross-attention between sequences, offers a contrast to the representation model typically used in initial retrieval phases. PROMPTAGATOR (Dai et al., 2023) also employs a cross-attention model for re-scoring. Its \u201cLift Yourself Up\" strategy iteratively selects the best candidate from a pool for further generation rounds, progressively improving content quality via self-generated content.\nRe-ranking is also a significant focus of In-Context RALM (Ram et al., 2023). Two approaches to reranking are explored: zero-shot reranking using language models and predictive reranking through trained models. This step is aimed at refining the selection of documents based on their expected utility for improving language model performance. ITER-RETGEN (Shao et al., 2023), in particular, leverages knowledge distillation from the re-ranker to the dense retriever, fine-tuning retrieval efforts based on relevance signals from LLM outputs. This optimization of the retrieval model aims to more accurately capture query nuances, thereby improving document selection.\nDKS-RAC (Huang et al., 2023) presents the Dense Knowledge Similarity (DKS) for aligning the knowledge between answers and retrieved passages at the sequence level. This approach is categorized under re-ranking due to its direct impact on passage selection based on knowledge similarity, refining the match between queries and documents.\nFiD-light (Hofst\u00e4tter et al., 2023) introduces a listwise autoregressive re-ranking method that employs source pointers to optimize the ranking order. This method maintains a link between the generated text and source passages, enabling a more structured generation process. By incorporating textual citations within the model's output as pointers to relevant information sources, this approach facilitates an organized retrieval and generation process, enhancing the overall coherence and relevance of the generated content."}, {"title": "5.2 Filtering", "content": "COK (Li et al., 2023) presents the Progressive Rationale Correction technique, aimed at iteratively refining rationales with retrieved knowledge. This method constitutes a continuous optimization process, significantly enhancing the relevance and quality of information used in content generation. Self-RAG (Asai et al., 2023) introduces a self-reflection mechanism to efficiently filter out irrelevant content. By employing critique tokens, this approach evaluates the relevance, supportiveness, and utility of retrieved passages, ensuring the integration of only high-quality information into the content generation process.\nAdditionally, FiD-TF (Berchansky et al., 2023) and RECOMP (Xu et al., 2023) are dedicated to the removal of irrelevant or redundant tokens and information from retrieved documents. FiD-TF employs a dynamic mechanism to identify and eliminate unnecessary tokens, enhancing the efficiency of information processing. RECOMP, on the other hand, compresses documents into concise summaries, focusing on selecting only the most pertinent content for the generation process. These methods streamline the content generation workflow by ensuring that only relevant and supportive information is utilized, thereby improving the overall quality and relevance of the generated content."}, {"title": "6 Generation", "content": ""}, {"title": "6.1 Enhancing", "content": "DSP (Khattab et al., 2022) introduces a framework designed to generate multiple retrieval queries to summarize and answer questions, drawing upon information aggregated from various passages. This framework employs CombSUM (Fox and Shaw, 1994) to calculate a cumulative probability score for passages across different retrieval lists, facilitating the compilation of a comprehensive response from multiple sources.\nPRCA (Yang et al., 2023a) outlines a Reward-Driven Stage, wherein the distilled context is refined based on feedback from the generator. Utilizing reinforcement learning, this stage adjusts the parameters of PRCA according to the rewards received for providing relevant context. The objective is to fine-tune the extracted context to meet the specific requirements of the generator, thereby optimizing the generation process.\nREPLUG (Shi et al., 2023) proposes a method for prepending retrieved documents to the input context before the final prediction by the black-box LM. It introduces an ensemble strategy to encode retrieved documents in parallel, overcoming the limitations of LM context length and enhancing accuracy through the allocation of increased computational resources. This approach improves the generation process by ensuring that the LM has access to a broader range of relevant information.\nRECITE (Sun et al., 2023) implements a self-consistency technique, which involves generating multiple recitations independently and employing a plurality/majority vote system to determine the most appropriate answer. This method is designed to increase the reliability and accuracy of the answers, thereby improving the quality and credibility of the output."}, {"title": "6.2 Customization", "content": "The PKG framework, introduced by (Luo et al., 2023), represents an approach to customizing the output of LMs. By generating background knowledge internally using a pre-trained model, PKG eliminates the need for traditional external retrieval processes. This method directly integrates domain- or task-specific knowledge into the generation step, significantly enhancing the LM's capacity to produce responses that are specifically tailored to the given context or requirements.\nSelf-RAG (Asai et al., 2023) offers a strategy that incorporates reflection tokens within a customizable decoding algorithm. This technique permits dynamic adjustment of the model's retrieval and generation behaviors based on the specific task, facilitating more versatile response generation. Depending on the requirements, this approach can be tuned for accuracy or creativity, providing flexibility in generating outputs that meet diverse needs.\nSURGE (Kang et al., 2023) achieves customization through the application of graph-text contrastive learning. This method ensures that the generated dialogue responses are in tight alignment with the knowledge contained in the retrieved subgraph, yielding responses that are specific, relevant, and deeply rooted in the dialogue context. By maintaining consistency between the retrieved knowledge and the generated text, SURGE is capable of producing outputs that precisely reflect the detailed knowledge of the subgraph, enhancing the relevance and specificity of the responses."}, {"title": "7 Comparisons of RAG", "content": ""}, {"title": "7.1 The Comprehensive Summary of RAG", "content": "Table 1 presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows that the majority of these studies have utilized external data sources to enrich the content of LLMs. A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search rounds generally yield superior results. In other words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval stage, more studies focus on manipulating the query to improve retrieval performance. Additionally, there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in the research. However, there seems to be a scarcity of studies concentrating on customization in the generation stage, pointing to this as a potential area for future exploration. Overall, while the goal of RAG is to enhance the response quality of LLMs, greater efforts have been directed towards improving retrieval aspects."}, {"title": "7.2 Retriever and Generator", "content": "In RAG, the retriever and the generator are the primary components. Table 2 summarizes the retrievers and generators used in the studies discussed in this paper. It is clear from the table that while most generators utilize advanced language models, a significant number of retrievers still employ the traditional BM25 due to its efficiency. The method of retrieval is a crucial aspect in RAG, highlighting the importance of exploring ways to enhance retrieval performance without compromising efficiency. Similarly, not many studies have adopted powerful LLMs such as LLaMA2, GPT-3.5, or GPT-4 as their generators. LLMs like T5 remain popular, yet fundamental models like BERT and Transformers are rarely used in 2023. Compared to generators, it is evident that not many IR-based LLMs are used in retrievers, indicating a promising direction for developing such models in the future."}, {"title": "8 Evaluation in RAG", "content": "To understand the effectiveness of LMs in generating more accurate, relevant, and robust responses by leveraging external knowledge, the evaluation of RAG systems has become a significant research area. With the popularity of dialogue-based interactions, recent works have been focused on assessing the performance of RAG models on such downstream tasks using established metrics like Exact Match (EM) and F1 scores. Furthermore, a wide array of datasets has been utilized for this purpose, including TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019), and T-REX (ElSahar et al., 2018).\nHowever, evaluation solely from the perspective of downstream tasks falls short in addressing the evolving needs of RAG development. Recent research has introduced various frameworks and benchmarks that aim to evaluate these systems across multiple dimensions, including the quality of the generated text, the relevance of retrieved documents, and the model\u2019s resilience to misinformation, as shown in Table 3. These evaluations focus on assessing specific capabilities such as noise robustness, negative prompting, information integration, and counterfactual robustness, highlighting the complex challenges faced by RAG systems in practical applications. The continuous development of evaluation frameworks and metrics is crucial for advancing the field, broadening the applicability of RAG systems, and ensuring they meet the demands of a complex and evolving information landscape."}, {"title": "8.1 Retrieval-based Aspect", "content": "In information retrieval, the quality of search results is typically evaluated using standard metrics such as Mean Average Precision (MAP), Precision, Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) (Radlinski and Craswell, 2010; Reimers and Gurevych, 2019; Nogueira et al., 2019). These metrics primarily assess the relevance of retrieved documents to a given query.\nRetrieval-based Metrics in RAG focus on the effectiveness of retrieving relevant information to support generation tasks. These include Accuracy, which measures the precision of retrieved documents in providing correct information for answering queries, and Rejection Rate (Chen et al., 2023b), assessing a system's ability to decline answering when no relevant information is found. Additionally, Error Detection Rate (Chen et al., 2023b) evaluates the model's capability to identify and disregard incorrect or misleading information from retrieved documents. Context Relevance is another essential metric, assessing the pertinence of the retrieved documents to the query. It's vital to ensure the information used to generate responses is directly related to the query's context. Faithfulness (Shahul et al., 2023) measures the accuracy with which the generated content reflects the information in the retrieved documents, ensuring that the generation process with no misinformation."}, {"title": "8.2 Generation-based Aspect", "content": "Evaluating the quality of text produced by LLMs involves analyzing their performance on various downstream tasks using standard metrics. These metrics assess linguistic quality, coherence, accuracy, and the extent to which the generated text reflects ground-truth data. Linguistic quality and coherence are evaluated through metrics such as BLEU (Papineni et al., 2002), which measures fluency and similarity to human-produced text, and ROUGE-L (Lin, 2004), which quantifies the overlap with reference summaries to gauge the text's capacity to encapsulate main ideas and phrases. Accuracy and overlap with ground-truth data are gauged using metrics like EM and F1 Score, which respectively determine the percentage of answers that are entirely correct and offer a balanced assessment of precision and recall in retrieving relevant answers while minimizing inaccuracies.\nBeyond these standard metrics, the evaluation may also incorporate task-specific criteria and novel metrics tailored to particular applications. For instance, in dialogue generation, perplexity and entropy are used to evaluate response diversity and naturalness. Additionally, metrics such as Misleading Rate and Mistake Reappearance Rate (Liu et al., 2023) gauge a model's ability to avoid misinformation and inaccuracies. Other specialized metrics include Answer Relevance (Shahul et al., 2023), assessing the precision of responses to queries; Kendall's tau (Saad-Falcon et al., 2023), for evaluating the accuracy of RAG system rankings; Micro-F1 (Saad-Falcon et al., 2023), which fine-tunes accuracy evaluation in tasks with multiple correct answers; and Prediction Accuracy, directly measuring the alignment of generated answers with expected responses, thereby offering a direct insight into a system's effectiveness in generating accurate content."}, {"title": "9 Future Directions", "content": ""}, {"title": "9.1 Retrieval Quality", "content": "The integration of RAG into LLMs faces significant hurdles due to the vast amounts of unreliable information on the internet, including fake news. This presents a challenge for accurately retrieving useful knowledge, leading to the unreliable generation of responses by LLMs. As a result, LLMs may generate content based on incorrect information, undermining their reliability. Recent research efforts are directed towards enhancing retrieval methods to improve the efficiency, scalability, and effectiveness of LLMs in generating accurate and reliable responses.\nDifferentiable Search Indices (Tay et al., 2022) and (Bevilacqua et al., 2022b) developed differentiable search indices that integrate the retrieval process within a Transformer model, enabling direct mapping of text queries to document identifiers. These approaches offer superior performance and potential for more efficient and scalable retrieval.\nGenerative Models for Search GERE (Chen et al., 2022a) can directly generate document titles and evidence sentences for fact-verification tasks. PARADE (Li et al., 2024) is a method for document reranking that aggregates passage representations into a unified document relevance score. Both of them demonstrate significant improvements in retrieval quality over traditional methods.\nFine-tuning Pre-trained Language Models RankT5 (Zhuang et al., 2023) is a model that fine-tunes the T5 framework specifically for text ranking. It leverages ranking losses to optimize performance metrics and exhibits promising zero-shot performance on out-of-domain data.\nNoise Power (Cuconasu et al., 2024) provide a comprehensive analysis of the impact of IR components on RAG systems, revealing that the inclusion of irrelevant documents can significantly improve accuracy. It challenges conventional retrieval strategies and underscores the potential for developing specialized approaches that integrate retrieval with language generation models."}, {"title": "9.2 Multimodal RAG", "content": "The multimodal RAG domain has experienced significant growth, highlighting a pivotal advancement at the confluence of text and visual comprehension. The introduction of MuRAG (Chen et al., 2022b) marked a breakthrough by amalgamating textual and visual information for language generation, establishing a new standard for multimodal datasets. This model showcased the efficacy of utilizing a multimodal memory system to boost the accuracy in question-answering and reasoning tasks.\nAfter MuRAG, studies such as REVEAL (Hu et al., 2023) and Re-Imagen (Chen et al., 2023c) have focused on enhancing visual question answering and text-to-image generation. They achieved this through the incorporation of dynamic retrieval mechanisms and the improvement of image fidelity, respectively. These advancements laid the groundwork for further models by researchers like Sarto et al. (Sarto et al., 2022) for image captioning, and Yuan et al. (Yuan et al., 2023) for text-to-audio generation, broadening the scope of RAG's application across different modalities and improving the quality and realism of the generated outputs. Furthermore, Re-ViLM (Yang et al., 2023b) refined image captioning capabilities through a retrieval-augmented visual language model. By fine-tuning model parameters and implementing innovative filtering strategies, it has made strides in producing more precise and contextually appropriate captions. By tapping into external resources, these models have provided significant enhancements over traditional benchmarks, highlighting the advantage of integrating diverse sources of knowledge."}, {"title": "10 Conclusions", "content": "In this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing the capabilities of LLMs. Through a structured overview of RAG, categorizing various methods, and an in-depth analysis of its core technologies and evaluation methods, this study illuminates the path for future research. It identifies crucial areas for improvement and outlines potential directions for advancing RAG applications, especially in textual contexts. This survey aims to elucidate the core concepts of the RAG field from a retrieval perspective, and it is intended to facilitate further exploration and innovation in the accurate retrieval and generation of information."}, {"title": "11 Limitations", "content": "This survey comprehensively examines existing RAG models, summarizing their core techniques into four main steps from a retrieval perspective. It recognizes that some methods may encompass multiple steps and that decoupling these steps could potentially obscure their intrinsic connections. Nevertheless, the primary objective is to simplify the complexity of the approach, clearly delineating the specific problems it addresses. This allows for a clearer identification of areas ripe for further optimization and improvement. Despite the thorough investigation, the rapid evolution of the field and page limits mean that certain aspects might not have been fully analyzed and explored, or recent developments could have been missed. While the paper references evaluation methods"}]}