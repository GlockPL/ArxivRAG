{"title": "Diffusion Models without Classifier-free Guidance", "authors": ["Zhicong Tang", "Jianmin Bao", "Dong Chen", "Baining Guo"], "abstract": "This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at github.com/tzco/Diffusion-wo-CFG.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) have become the cornerstone of many successful generative models, e.g. image generation (Dhariwal & Nichol, 2021; Nichol et al., 2022; Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024) and video generation (Ho et al., 2022; Blattmann et al., 2023; Gupta et al., 2025; Polyak et al., 2024; Wang et al., 2024) tasks. However, diffusion models also struggle to generate \u201clow temperature\" samples (Ho & Salimans, 2021; Karras et al., 2024) due to the nature of training objectives, and techniques such as Classifier guidance (Dhariwal & Nichol, 2021) and Classifier-free guidance (CFG) (Ho & Salimans, 2021) are proposed to improve performances.\nDespite its advantage and ubiquity, CFG has several drawbacks (Karras et al., 2024) and poses challenges to effective implementations (Kynk\u00e4\u00e4nniemi et al., 2024) of diffusion models. One critical limitation is the simultaneous training of unconditional model apart from the main diffusion model. The unconditional model is typically implemented by randomly dropping the condition of training pairs and replacing with an manually defined empty label. The introduction of additional tasks may reduce network capabilities and lead to skewed sampling distributions (Karras et al., 2024; Kynk\u00e4\u00e4nniemi et al., 2024). Furthermore, CFG requires two forward passes per denoising step during inference, one for the conditioned and another for the unconditioned model, thereby significantly escalating the computational costs.\nIn this work, we propose Model-guidance (MG), an innovative method for diffusion models to effectively circumvent CFG and boost performances, thereby eliminating the limitations above. We propose a novel objective that transcends from simply modeling the data distribution to incorporating the posterior probability of conditions. Specifically, we leverage the model itself as an implicit classifier and directly learn the score of calibrated distribution during training.\nAs depicted in Figure 1, our proposed method confers multiple substantial breakthroughs. First, it significantly refines generation quality and accelerates training processes, with experiments showcasing a \u2265 6.5\u00d7 convergence speedup than vanilla diffusion models with excellent quality. Second, the inference speed is doubled with our method, as each denoising step needs only one network forward in contrast to two in CFG. Besides, it is easy to implement and requires only one line of code modification, making it a plug-and-play module of existing diffusion models with instant improvements. Finally, it is an end-to-end method that excels traditional two-stage distillation-based approaches and even outperforms CFG in generation performances.\nWe conduct comprehensive experiments on the prevalent Imagenet (Deng et al., 2009; Russakovsky et al., 2015) benchmarks with 256 \u00d7 256 and 512\u00d7512 resolution and compare with a wide variates of concurrent models to attest the effectiveness of our proposed method. The evaluation results demonstrate that our method not only parallels and even outperforms other approaches with CFG, but also scales to different models and datasets, making it a promising enhancement for diffusion models. In conclusion, we make the following contribution in this work:\n\u2022 We proposed a novel and effective method, Model-guidance (MG), for training diffusion models.\n\u2022 MG removes CFG for diffusion models and greatly accelerates both training and inference process.\n\u2022 Extensive experiments with SOTA results on ImageNet demonstrate the usefulness and advantages of MG."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Diffusion and Flow Models", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) are a class of generative models that utilize forward and reverse stochastic processes to model complex data distributions.\nThe forward process adds noise and transforms data samples into Gaussian distributions as\n$q(x_t|x_0) = \\mathcal{N} (x_t; \\sqrt{\\bar{a}_t}x_0, (1 - \\bar{a}_t)I),$ (1)\nwhere $x_t$ represents the noised data at timestep $t$ and $\\bar{a}_t = \\prod_{i=1}^{t} a_i$ is the noise schedule.\nConversely, the reverse process learns to denoise and finally recover the original data distribution, which aims to reconstruct score (Sohl-Dickstein et al., 2015; Song et al., 2021b) from the noisy samples $x_t$ by learning\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N} (x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),$ (2)\nwhere $\\mu_\\theta$ and $\\Sigma_\\theta$ are mean and variance and commonly predicted by neural networks.\nIn common implementations, the training of diffusion models leverages a re-parameterized objective that directly predicts the noise at each step (Ho et al., 2020)\n$\\mathcal{L}_{simple} = \\mathbb{E}_{t,x_0,\\epsilon}||\\epsilon_\\theta (x_t, t) - \\epsilon||^2,$ (3)\nwhere $x_t$ is derived from the forward process in Equation (1) with $x_0$ and $\\epsilon$ drawn from dataset and Gaussian noises.\nConditional diffusion models allow users to generate samples aligned with specified demands and precisely control the contents of samples. In this case, the generation process is manipulated with give conditions $c$, such as class labels or text prompts, where network functions are $\\epsilon_\\theta(x_t, t, c)$.\nFlow Models (Lipman et al., 2023; Liu et al., 2023; Albergo et al., 2023; Tong et al., 2024) are another emerging type of generative models similar to diffusion models. Flow models utilize the concept of Ordinary Differential Equations (ODEs) to bridge the source and target distribution and learn the directions from noise pointing to ground-truth data.\nThe forward process of flow models is defined as an Optimal Transport (OT) interpolant (McCann, 1997)\n$x_t = (1-t)x_0 + t\\epsilon,$ (4)\nand the loss function takes the form (Lipman et al., 2023)\n$\\mathcal{L}_{FM} = \\mathbb{E}_{t,x_0,\\epsilon} ||u_\\theta(x_t) - u_t(x_t|x_0)||^2,$ (5)\nwhere the ground-truth conditional flow is given by\n$u_t(x_t|x_0) = x_0 - \\epsilon.$ (6)"}, {"title": "2.2. Classifier-Free Guidance", "content": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality.\nThe key design of CFG is to combine the posterior probability and utilize Bayes' rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict\n$\\epsilon(x_t, t, c) \\propto -\\nabla_{x_t} \\log p_\\theta(x_t|c),$ (7)\n$\\epsilon(x_t, t, \\emptyset) \\propto -\\nabla_{x_t} \\log p_\\theta(x_t),$ (8)\nwhere $\\emptyset$ is an additional empty class introduced in common practices. During training, the model switches between the two modes with a ratio $\\lambda$.\nFor inference, the model combines the conditional and unconditional scores and guides the denoising process as\n$\\hat{\\epsilon}_\\theta(x_t, t, c) = \\epsilon_\\theta(x_t, t, c) + w \\cdot (\\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta(x_t, t, \\emptyset)),$ (9)"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Rethinking Classifier-free guidance", "content": "Due to the complex nature of visual datasets, diffusion models often struggle whether to recover real image distribution or engage in the alignment to conditions. Classifier-free guidance (CFG) is then proposed and has become an indispensable ingredient of modern diffusion models (Nichol & Dhariwal, 2021; Karras et al., 2022; Saharia et al., 2022; Hoogeboom et al., 2023). It drives the sample towards the regions with higher likelihood of conditions with Equation (9), where the images are more canonical and better modeled by networks (Karras et al., 2024).\nHowever, CFG has with several disadvantages (Karras et al., 2024; Kynk\u00e4\u00e4nniemi et al., 2024), such as the multitask learning of both conditional and unconditional generation, and the doubled number of function evaluations (NFEs) during inference. Moreover, the tempting property that solving the denoising process according to Equation (9) eventually recovers data distribution does not hold, as the joint distribution does not represent a valid heat diffusion of the ground-truth (Zheng & Lan, 2024). This results in exaggerated truncation and mode dropping similar to (Karras et al., 2018; Brock et al., 2019; Sauer et al., 2022), since the samples are blindly pushed towards the regions with higher posterior probability. The generation trajectories are distorted in Section 1, the images are often over-saturated in color, and the content of samples is overly simplified.\nCFG originates from the classifier-guidance (Dhariwal & Nichol, 2021) that incorporates an auxiliary classifier model $p_\\theta (c|x_t)$ to modify the sampling distribution as\n$p_\\theta(x_t|c) \\propto p_\\theta(x_t|C)p_\\theta(c|x_t)^w,$ (10)\nand estimates the posterior probability term with Bayes' rule\n$p_\\theta(c|x_t) = \\frac{p_\\theta(x_t|C)p_\\theta(c)}{p_\\theta(x_t)},$ (11)\nwhere $p_\\theta(x_t|c)$ and $p_\\theta(x_t)$ are conditional and unconditional distributions, respectively.\nThe unconditional model is usually implemented by randomly replacing labels by an empty class with a ratio $\\lambda$. During inference, each sample is typically forwarded twice, one with and one without conditions. The finding naturally leads us to the question: can we fuse the auxiliary classifier into diffusion models in a more efficient and elegant way?"}, {"title": "3.2. Model-guidance Loss", "content": "Conditional diffusion models optimize the conditional probability $p_\\theta(x_t| c)$ by Equation (3), where $x_t$ is the noisy data and $c$ is the condition, e.g., labels and prompts. However, the models tend to ignore the condition in common practices and CFG (Ho et al., 2020) is proposed as an explicit bias.\nTo enhance both generation quality and alignment to conditions, we propose to take into account the posterior probability $p_\\theta(c|x_t)$. This leads to the joint optimization of $p_\\theta(x_t|C) = p_\\theta(x_t|C)p_\\theta(c|x_t)^w$, where $w$ is the weighting factor of posterior probability. The score of the joint distribution is formulated as\n$\\nabla_{x_t}\\log p_\\theta (x_t| c) = \\nabla_{x_t} \\log p_\\theta(x_t|c)+w\\cdot\\nabla_{x_t} \\log p_\\theta (c|x_t)$ (12)\nThe first term corresponds to the standard diffusion objective in Equation (3). However, the second term represents the score of posterior probability $p_\\theta(c|x_t)$ and cannot be directly obtained, since an explicit classifier of noisy samples is unavailable. Inspired by Equation (11), we transform the diffusion model into an implicit classifier and let it guide itself. Specifically, we employ Bayes' rule to estimate\n$\\log p_\\theta (c|x_t) = \\log p_\\theta(x_t|c) - \\log p_\\theta (x_t) + \\log p_\\theta (c)$\n$\\propto \\log p_\\theta (x_t|c) - \\log p_\\theta(x_t)$ (13)\nNext, we use the diffusion model to approximate the scores\n$\\nabla_{x_t} \\log p_t (x_t|C) = \\frac{1}{\\sigma_t} \\epsilon_\\theta(x_t, t, c),$ (14)\n$\\nabla_{x_t} \\log p_t(x_t) = \\frac{1}{\\sigma_t} \\epsilon_\\theta(x_t, t, \\emptyset),$ (15)\n$\\frac{1}{\\sigma_t} (\\epsilon_\\theta (x_t, t, \\emptyset) - \\epsilon_\\theta (x_t, t,c)).$ (16)\nThen, our method applies the Bayes' estimation in Equation (13) online and trains a conditional diffusion model to directly predict the score in Equation (12), instead of separately learning Equations (14) and (15) in the form of CFG. A straight-forward implementation is to adopt the objective in Equation (3) with a modified optimization target\n$\\mathcal{L}_{MG} = \\mathbb{E}_{t, (x_0,c),\\epsilon}||\\epsilon_\\theta (x_t, t, c) - \\epsilon' ||^2,$ (17)\n$\\epsilon' = \\epsilon + w \\cdot sg(\\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta (x_t, t, \\emptyset)).$ (18)\nWe apply the stop gradient operation, $sg(\\cdot)$, which is a common practice of avoiding model collapse (Grill et al., 2020). We also use the Exponential Mean Average (EMA) counterpart of the online model, $\\bar{\\epsilon}_\\theta(\\cdot)$, to stabilize the training process and provide accurate estimations. For flow-based models, we have the similar objective\n$\\mathcal{L}_{MG} = \\mathbb{E}_{t, (x_0,c),\\epsilon}||u_\\theta (x_t, t, c) - u' ||^2,$ (19)\n$u' = u + w \\cdot sg(u_\\theta(x_t, t, c) - u_\\theta(x_t, t, \\emptyset)).$ (20)\nwhere $u$ is the ground-truth flow in Equation (6).\nDuring training, we randomly drop the condition $c$ in Equations (17) and (19) to $\\emptyset$ with a ratio of $\\lambda$. These formulations transform the model itself into an implicit classifier and adjust the standard training objective of diffusion model in a self-supervised manner, allowing the joint optimization of generation quality and condition alignment with the minimum modification of existing pipelines."}, {"title": "3.3. Implementation Details", "content": "With the MG formulation in Equations (17) and (19), we have adequate options in the detailed implementations, such as incorporating an additional input of the guidance scale $w$ into networks, replacing the usage of empty class with the law of total probability, and whether to manually or automatically adjust the hyper-parameters.\nScale-aware networks. Similar to other distillation-based methods (Frans et al., 2024), the guidance scale $w$ can be fed into the network as an additional condition. When augmented with $w$-input, our models offer flexible choices of the balance between image quality and sample diversity during inference time. Note that our models require only one forward per step for all values of $w$, while standard CFG needs two forwards, e.g., one with condition and one without condition. In particular, we sample guidance scale from an specified interval, and the loss function are modified into the following form\n$\\mathcal{L}_{MG} = \\mathbb{E}_{t, (x_0,c),\\epsilon,w}||\\epsilon_\\theta(x_t, t, c, w) - \\epsilon'||^2,$ (21)\n$\\epsilon' = \\epsilon + w \\cdot sg(\\epsilon_\\theta(x_t, t, c, 1) - \\epsilon_\\theta(x_t, t, \\emptyset,0)).$ (22)\nRemoving the empty class. Another option is whether to perform multitask learning of both conditional and unconditional generation with the same model. In CFG, the estimator in Equation (11) requires to train an unconditional model. However, the multitask learning can distract and hinder model capability. Using the law of total probability\n$\\nabla_{x_t} \\log p_t (x_t) = \\sum_{c} \\nabla_{x_t} \\log P_t(x_t|C)p_t(c)$\n$\\approx \\frac{1}{N} \\sum_{i=1}^N \\epsilon_\\theta (x_t, t, c_i),$ (23)\nwhere $N$ different labels are used to estimate the unconditional score, our models focus on the conditional prediction and avoid the introduction of additional empty class.\nAutomatic adjustment of the hyper-parameter $w$. While the scale $w$ in Equations (18) and (20) plays an important role, it is tedious and costly to perform manual search during training. Therefore, we introduce an automatic scheme to adjust $w$. We begin with $w = 0$ that corresponds to vanilla diffusion models, then update the value with EMA according to intermediate evaluation results. The value of $w$ is raised when quality decreases and suppressed otherwise, leading to an optimums when the training converged."}, {"title": "4. Experiment", "content": "We first present a system-level comparison with state-of-the-art models on ImageNet 256 \u00d7 256 conditional generation. Then we conduct ablation experiments to investigate the detained designs of our method. Especially, we emphasize on the following questions:\n\u2022 How far can MG push the performances of existing diffusion models? (Tables 1 and 2, Section 4.2)\n\u2022 How does implementation details influence the gain of proposed method? (Tables 3 to 6, Section 4.3)\n\u2022 Can MG scales to larger models and datasets with efficiency? (Tables 7 and 8, Figures 4 to 6, Section 4.3)\nImplementation and dataset. We follow the experiment pipelines in DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). We use ImageNet (Deng et al., 2009; Russakovsky et al., 2015) dataset and the Stable Diffusion (Rombach et al., 2022) VAE to encode 256 \u00d7 256 images into the latent space of $\\mathbb{R}^{32\\times32\\times4}$. We conduct ablation experiments with the B/2 variant of DiT and SiT models and train for 400K iterations. During training, we use AdamW (Kingma, 2014; Loshchilov, 2019) optimizer and a batch size of 256 in consistent with DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024) for fair comparisons. For inference, we use 1000 sampling steps for DiT models and Euler-Maruyama sampler with 250 steps for SiT.\nBaseline Models. We compare with several state-of-the-art image generation models, including both diffusion-based and AR-based methods, which can be classified into the following three classes: (a) Pixel-space diffusion:"}, {"title": "5. Conclusion", "content": "This work addresses the limitations of the commonly used Classifier-free guidance (CFG) of diffusion models, and proposes Model-guidance (MG) as an efficient and advantageous replacement. We first investigate the mechanism of CFG and locate the source of performance gain as a joint optimization of posterior probability. Then, we transcend the idea into the training process of diffusion models and directly learn the score of the joint distribution, $\\nabla \\log p_\\theta (x_t| c) = \\nabla \\log p_\\theta (x_t|C)p_\\theta(c|x_t)$. Comprehensive experiments demonstrate that our method significantly boosts the generation performance without efficiency loss, scales to different models and datasets, and achieves state-of-the-art results on ImageNet 256 \u00d7 256 dataset. We believe that this work contributes to future diffusion models."}, {"title": "Impact Statements", "content": "This paper propose methods in association with generative methods. There might be potential negative social impacts, e.g. generating fake portraits, as the core contribution of our work is a new algorithm of generative modeling. As possible mitigation strategies, we will restrict the access to these models in the planned release of code and models. We also validate that current detectors can effectively determine our generation results about human portraits."}]}