{"title": "Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering Approach", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "abstract": "The large size of DNNs poses a significant challenge for deployment on devices with limited resources, such as mobile, edge, and IoT platforms. To address this issue, a distributed inference framework can be utilized. In this framework, a small-scale DNN (initial layers) is deployed on mobile devices, a larger version on edge devices, and the full DNN on the cloud. Samples with low complexity (easy) can be processed on mobile, those with moderate complexity (medium) on edge devices, and high complexity (hard) samples on the cloud. Given that the complexity of each sample is unknown in advance, the crucial question in distributed inference is determining the sample complexity for appropriate DNN processing. We introduce a novel method named DIMEC-DC, which leverages the Data Cartography approach initially proposed for enhancing DNN generalization. By employing data cartography, we assess sample complexity. DIMEC-DC aims to boost accuracy while considering the offloading costs from mobile to edge/cloud. Our experimental results on GLUE datasets, covering a variety of NLP tasks, indicate that our approach significantly lowers inference costs by more than 43% while maintaining a minimal accuracy drop of less than 0.5% compared to performing all inferences on the cloud.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the size of Deep Neural Networks (DNNs) has expanded considerably, leading to exceptional perfor- mance, particularly in Natural Language Processing (NLP) tasks [1], [2]. However, this increased scale demands extensive computational resources, posing challenges for deployment on resource-limited platforms such as mobile and edge devices. To mitigate these challenges, several approaches have been de- veloped, including model pruning [3], [4], weight quantization [5], [6], knowledge distillation [7], [8], early exits [9]\u2013[11], and cloud offloading [12]. Approaches such as model pruning, weight quantization, and knowledge distillation aim to reduce model size through various techniques, often leading to a notable decrease in model accuracy. These methods typically compress the model to fit within the memory limits of mobile devices, which can compromise the backbone's effectiveness. Many large DNN models have smaller versions tailored for resource-constrained environments [13], [14]. For instance, the BERT [13] model has smaller variants like DistilBERT [7] and TinyBERT [8] for mobile deployment, BERT-base for edge deployment, and BERT-Large and Extra-Large for cloud deployment. Although these smaller models are computationally efficient and fit in mobile devices, they suffer a significant loss in accuracy. Mobile and edge devices frequently struggle with perform- ing inference on large models due to limitations in space and memory. Cloud offloading takes advantage of high-capacity services and extensive computational resources, enabling the deployment of full-scale DNNs for inference tasks. Neverthe- less, offloading samples to the cloud introduces extra costs due to the physical separation from mobile terminals. In real-world applications, the input tasks contain a mix of easy and hard samples, which do not all require the same level of computational effort. For instance, Figure 1 displays a plot of the mean confidence and variance of samples across epochs. Here, confidence and variance are defined as the mean and variance of the ground-truth label probabilities across epochs for the SST-2 dataset using the BERT-base model [13]. The green samples, which have high confidence and low variance, are easily learned by the model. Conversely, the blue samples, which exhibit high confidence but also high variance, indicate that the model's confidence in the ground-truth label fluctuates. The challenging samples with high variance are those where the model is less confident; however, a larger model can learn to correctly classify these samples. Finally, the most difficult samples are those with low variance but where the model is consistently incorrect; these are often out-of-distribution samples, making accurate prediction challenging even with a full-fledged DNN. This analysis demonstrates that not all samples require the same amount of computation; easier samples can achieve correct inference locally, even with smaller models deployed on resource-constrained devices, thus significantly reducing cost and time complexity. However, harder samples necessitate the use of a full-fledged DNN for accurate inference, which is not feasible on resource-constrained devices. To tackle this issue, we employ a distributed inference strategy: deploying the initial layers of the DNN on the mobile device, a more extensive model with additional layers on the edge device, and the complete model on the cloud. Considering the diverse complexity of real-world samples, it is beneficial to leverage mobile, edge, and cloud resources according to the complexity of incoming samples. Since the complexity of these samples is not predetermined, the challenge lies in accurately assessing them. We propose a"}, {"title": "method to determine the complexity of each sample based on data cartography", "content": "[15]. It enables the decision of whether the sample should be processed on the mobile device, edge device, or cloud. The inference location decision is crucial as it balances accuracy and various costs; while processing more samples on the cloud can achieve higher accuracy, it incurs greater offloading costs. Conversely, processing more samples on the mobile or edge device can compromise accuracy while keeping the cost low. Data Cartography is a technique designed to enhance the generalization capability of DNNs. It identifies the most am- biguous samples within a large dataset, pinpointing those that are most challenging for the model to classify. This approach leverages the prediction confidence across multiple training epochs to gauge the complexity of samples. Samples with high confidence and low variance are deemed 'easy', those with low confidence are considered 'hard' irrespective of variance, and samples with high confidence and high variance are seen as 'ambiguous', indicating the model's fluctuating confidence and confusion. Our method enhances resource efficiency across mobile, edge, and cloud environments through distributed inference, leveraging the data cartography technique. We deploy three versions of the DNN: the initial layers on the mobile device, a larger portion on the edge device, and the complete model on the cloud. The allocation of layers to each device is determined based on the available resources on the mobile and edge devices, as detailed in Section IV-E. Each device includes a classifier at its final layer, enabling inference at each device. Figure 2 illustrates our inference process, where easy samples are handled by the mobile device, moderately complex samples are offloaded to the edge, and only the most challenging samples are offloaded to the cloud for processing. To determine sample complexity during inference, we cate- gorize samples into easy, moderate, and hard pools by assess-"}, {"title": "ing their confidence and variance over various training epochs.", "content": "To create these pools, we initially utilize a smaller version (initial layers) of the backbone model intended for deployment on a mobile device. The model's weights from each training epoch are utilized to compute the confidence and variance of the samples on the validation dataset. Based on these metrics, we classify the samples' complexity and append their word embeddings to the corresponding easy, medium, or hard pool. For example, if a sample is classified as easy, its word embedding is added to the easy pool. During inference, we utilize these pre-established pools to evaluate the complexity of incoming samples and assign them to an appropriate device. It is important to note that only the embedding layer is required on the mobile device to determine the samples' complexity. In our method, named DIMEC-DC: Distributed Inference on Mobile, Edge, and Cloud: A Data-Cartography based Clus- tering Approach, we leverage distributed inference to allocate computational resources according to the complexity of each sample. This method dynamically determines the computa- tional needs of an incoming sample on the fly, avoiding the ne- cessity for samples to be processed by the mobile device first. This alleviates the load on individual devices and addresses the challenge of running large models on mobile devices. Additionally, our method effectively balances the accuracy- efficiency trade-off, significantly enhancing efficiency while maintaining accuracy comparable to that of the final layer. This strategy ensures efficient and accurate processing by dy- namically deciding whether to process samples locally, at the edge, or offload them to the cloud based on their complexity, thus optimizing both processing and communication costs. We utilize the BERT-base/large [13] model as our backbone due to its well-established performance and efficiency, making it an ideal candidate for evaluating our approach. This choice allows us to assess our method's effectiveness across various NLP tasks, as described in Section V. Our experiments, which include sentiment classification, entailment classification, and natural language inference, reveal that DIMEC-DC is resilient to different cost structures, effectively accommodating devices with diverse processing capabilities and communication tech- nologies such as 3G, 4G, 5G, and Wi-Fi. Notably, our method achieves a substantial cost reduction (> 43%) while incurring only a slight decrease in accuracy (< 0.5%) compared to scenarios where all samples are processed at cloud. Our main contributions are summarized as follows: \u2022 We develop a mechanism based data cartography to decide if a sample can be inferred at DNN available on mobile, edge or cloud. \u2022 Our approach demonstrates resilience to variations in cost structures, maintaining accuracy even when mobile and edge devices or communication networks are altered. \u2022 The negligible reduction in accuracy is due to our method preserving the backbone's optimality, as it does not involve any parameter reduction from the model. \u2022 Through empirical evaluation, we show that our method effectively reduces costs and, in some cases, improves performance compared to previous baseline approaches."}, {"title": "II. RELATED WORKS", "content": "In this section, we discuss the existing methods for inference on resource-constrained devices. Neurosurgeon, as introduced in [16], explores the strategies for optimizing the splitting of DNNs based on cost consider- ations associated with selecting a specific splitting layer. In a similar vein, BottleNet [17] and Bottlefit [18] incorporate a bottleneck mechanism within split computing. This approach involves deploying part of the DNN on a mobile device to encode the input into a compact representation, which is then processed further in the cloud. Various training strategies have been proposed for training the encoder on the edge device within this setup such as BottleNet++ [19] and [20] employs cross-entropy-based training approaches in the context of split computing, Matsubara [21] performs knowledge distillation- based training, CDE [22] and Yao [23] perform reconstruction- based training and Matsubara [24] perform head-network distillation training method to effectively encode the input to offload efficiently. The cloud offloading methods utilize split computing where part of the DNN is deployed on mobile and the remaining on the cloud where the sample is encoded into smaller bits so that offloading cost can be reduced. But in this setup, all the samples are required to be offloaded to the cloud irrespective of sample complexity. Also, some computation is performed on the mobile device for each sample hence the method adds to both mobile processing cost as well as offloading cost. AdaEE [25] employs a combination of Early Exit DNNs [9]\u2013[11] and DNN partitioning to facilitate offloading data from mobile devices to the cloud using early exit DNNs. LEE (Learning Early Exit) [26], DEE (Dynamic Early Exit) [27] and UEE-UCB [28] leverage the multi-armed bandit (MAB) framework to determine optimal exit points up to which the DNN can be processed on the mobile device, while SplitEE [29] and I-SplitEE [29], [30] utilizes the MAB [31] setup to get the optimal splitting points under domain change scenarios in edge-cloud co-inference setup. LEE and DEE are specifically designed for efficient edge device inference, particularly in cases of service disruptions, employing utility functions that require ground-truth labels. The above-mentioned methods utilize cloud offloading with early exits to perform dynamic inference on mobile or edge devices. Although these methods have the option to perform inference on mobile devices based on sample complexity, these methods decide on the sample complexity after processing the sample through a mobile device, hence all samples add to the processing cost. Our approach differs from previous works in various as- pects: 1) We are the first ones to leverage the data cartography method for clustering to perform distributed inference. 2) During inference, our method does not require processing the sample on the mobile device and then offloading instead it decides on the fly, which device could be appropriate based on sample complexity. 3) Only hard samples are offloaded to the cloud and easy and medium ones are inferred locally on"}, {"title": "III. DATA CARTOGRAPHY", "content": "In this section, we provide a concise overview of the data cartography technique, which is an approach designed to identify the most beneficial data from a large corpus. This technique addresses the question of how datasets can be categorized based on their contribution to achieving op- timal performance both within and outside of distribution. By leveraging training dynamics\u2014specifically, the behavior of the model throughout the training process-data cartography categorizes datasets into three categories: easy, hard, and ambiguous. The classification is based on the confidence and variance values observed across training epochs for each sample, as illustrated in Figure 1. The primary objective is to identify a subset of the dataset that enhances the model's generalization capabilities."}, {"title": "A. Training dynamics", "content": "Consider a training dataset of size N, $D = \\{(x_i, y_i)\\}_{i=1}^N$, where the ith instance consists of an observation $x_i$ and its ground-truth label $y_i^*$. The model used for training outputs a probability distribution over labels given an observation. A stochastic-gradient descent-based optimization procedure is employed, with training instances randomly ordered at each epoch, across E epochs. The training dynamics of instance i are defined as statistics calculated across the E epochs. These measures serve as the coordinates in the data map. The first measure captures how confidently the learner assigns the true label to the observation, based on its probability distribution. Confidence is defined as the mean model probability of the true label (y) across epochs: $\\mu_i = \\frac{1}{E} \\sum_{e=1}^E P(y_i^*|x_i, \\theta_e), (1)$ where P denotes the model probability and $\\theta_e$ represents the parameters at the end of the eth epoch. The second coordinate for the data map is variability, which measures the spread of $P(y_i^*|x_i, \\theta_e)$ across epochs using variance (The paper [15] originally uses standard deviation, but we use the variance directly): $\\hat{\\sigma}_i = \\frac{\\sum_{e=1}^E (P(y_i^*|x_i, \\theta_e) - \\mu_i)^2}{E} (2)$ The variability also depends on the ground-truth label $y_i^*$. Instances for which the model predicts the same label con- sistently (whether accurately or not) will have low variability, whereas instances, where the model is indecisive across train- ing, will exhibit high variability. Most samples fall into the high-confidence and low- variability region of the map (Figure 1, top-left). The model consistently predicts these instances correctly with high con- fidence, labeling them as easy-to-learn. A smaller group is characterized by low variability and low confidence (Figure 1, bottom-left). These samples are rarely predicted correctly during training and are thus referred to as hard-to-learn. The"}, {"title": "third group includes ambiguous examples with high variability", "content": "(Figure 1, right-hand side); the model's confidence in these instances fluctuates, indicating indecision. These are referred to as ambiguous samples. Following this classification of training data, the model is trained on the top 33% of samples from each group. It has been demonstrated that training on ambiguous samples yields the best generalization performance on out-of-distribution tasks."}, {"title": "IV. PROBLEM SETUP", "content": "In this section, we will discuss the adaptation of Data Cartography techniques for distributed inference. Note that we have a different objective of assessing the complexity of an unseen sample hence instead of training, we use the validation dataset to create the clusters. The confidence in the validation set instances is slightly lower than that of the training set as it is unseen to the backbone during training. We also need to find the thresholds that can perfectly classify the data such that the accuracy is not degraded. We start with a pre-trained language model such as BERT. We assume that during inference, the first m layers will be deployed on the mobile, the first n layers on the edge and the full BERT model on the cloud. Hence during training, we attach classifiers at the mth and nth layers that map the hidden representations of the BERT model to class probabilities. This makes the model capable of performing inference on any device. Also, this corresponds to the case similar to a smaller model such as TinyBERT (4 layers) on mobile, DistilBERT (6 layers) on the edge and full-fledged BERT-base (12 layers) on the cloud."}, {"title": "A. Training exit classifiers", "content": "Recall that we used D to represent the distribution of the dataset with a label class C used for backbone fine-tuning. Let us assume that there are l layers in the backbone. For any input sample, $(x,y^*) \\sim D$ and for any classifier h, the loss can be computed as: $L_h(\\theta) = L_{CE}(f_h(x, \\theta), y^*) (3)$ Here, $f_h (x, \\theta)$ is the output of the classifier at the hth layer, where $\\theta$ denotes the set of learnable parameters, and $L_{CE}$ is the cross-entropy loss. We learn the classifiers of all the devices simultaneously hence the overall loss function could be written as $L = L_m+L_e+ L_c$ where $L_m, L_e$ and $L_c$ are the losses of the classifiers at the mobile, edge and cloud. Also, let $P_h(c)$ denote the estimated probability class $c \\in C$."}, {"title": "B. Preparation of dataset pool", "content": "During the model training phase, we perform the training for E epochs. After every epoch during training, the model weights are saved. Then we obtain the values of $\\hat{\\mu}_i$ and $\\hat{\\sigma}_i$ using equation 1 and 2 on the validation dataset with the parameters saved after every epoch. In our case, the probability is calculated at the mth layer i.e., the mean value is now:"}, {"title": null, "content": "$\\mu_i = \\frac{1}{E} \\sum_{e=1}^E P_m (y_i^*|x_i, \\theta_e) (4)$ Similarly $\\hat{\\sigma}_i$ is computed, after computing $\\mu_i$ and $\\hat{\\sigma}_i$, we define two thresholds $\\alpha$ and $\\beta$ to decide the complexity of the sample. To create the pool, if confidence exceeds the threshold $\\alpha$ and variance is less than $\\beta$, we classify that as an easy sample. If the confidence is less than the threshold $\\alpha$ and irrespective of variance we add the sample to the hard pool of samples. Finally, if a sample has confidence greater than $\\alpha$ and variance greater than $\\beta$, then the sample is added to the medium pool of samples. This procedure is given in the Algorithm 1. Note that $embed(x)$ is the output of the embedding layer."}, {"title": "C. Choice of threshold $\\alpha$ and $\\beta$", "content": "The threshold $\\alpha$ and $\\beta$ are a crucial part of our pool creation as the easiness and hardness of the pool depend on the values of these parameters. It also models the size of the pools as a smaller value of $\\alpha$ can increase the number of easy samples and can even classify some hard samples as easy while a larger value can add the easy sample to the harder pool. Both these scenarios affect the model performance as the former case affects the accuracy of the model and the latter case affects the cost. Hence it is very crucial to set the threshold properly. In that line, we first define the different types of costs that we consider, 1) Processing cost is the cost to process the sample through one layer of the DNN in the mobile and edge denoted as $\\lambda_m$ and $\\lambda_e$ respectively. 2) Offloading cost from mobile to edge and mobile to cloud denoted as $o_e$ and $o_c$ respectively. We also assume that there is a constant cost $\\gamma$ charged by the cloud platform for each sample. We also define $C_h$ as the confidence score in the estimate at the hth classifier i.e., $C_h := \\max_{c \\in C} P_h(c)$, it is the maximum of the output probability distribution at the hth classifier. Note that $C_h$ is different from $\\hat{\\mu}_i$ as this does not have any ground-truth label requirements. To choose the threshold $\\alpha$ and $\\beta$, we define a reward function as: $r(\\alpha, \\beta) = \\begin{cases} C_m - \\lambda_m & \\text{if } i \\geq \\alpha \\text{ and } \\hat{\\sigma}_i \\leq \\beta \\\\ C_n - \\lambda_e - o_e & \\text{if } i \\geq \\alpha \\text{ and } \\hat{\\sigma}_i > \\beta (5) \\\\ C_l - \\gamma - o_c & \\text{otherwise} \\end{cases}$ The reward function could be interpreted as, if an observa- tion x falls in the easy pool of sample i.e., $x \\in P_e$ then it is to be inferred at the mobile device i.e., at the mth layer hence the reward will be the confidence $C_m$ at the mth layer subtracted by the cost of processing the sample on the mobile device. Similarly, if the observation falls in a medium pool of samples i.e., $x \\in P_e$, then it will be inferred at the edge i.e., at the nth layer then the reward will be the same as that of the mobile device with an additional cost of offloading and a reduced processing cost. Finally, if the sample falls in the hard pool of samples i.e., $x \\in P_h$, then the sample will be inferred at the cloud i.e., at the final layer of the BERT, then the reward will be the inference at the final layer subtracted by the cost of the cloud platform and offloading cost. The expected reward function could be written as: $E[r(\\alpha, \\beta)] = E[C_m-\\lambda_m| \\text{mob. inference}]P[\\text{mob. inference}] + E[C_n - \\lambda_e - o_e|\\text{edge inference}]P[\\text{edge inference}] + E[C_l - \\gamma \u2013 o_c| \\text{cloud inference}]P[\\text{cloud inference}] (6)$ Now the objective is to maximize E[r(\u03b1, \u03b2)] and could be expressed as $\\max_{(\\alpha,\\beta)\\in S_1 \\times S_2} E[r(\\alpha, \\beta)]$ where the set $S_1$ and $S_2$ are the possible choices for the $\\alpha$ and $\\beta$ values respectively. $P[\\text{mob. inference}]$, $P[\\text{edge inference}]$ and $P[\\text{cloud inference}]$ is the probability that the sample will be inferred at mobile, edge and cloud respectively and depend on the value of $\\alpha$. Note that we maximize this reward function on the validation dataset as we already have the pools created for the validation split of the dataset."}, {"title": "D. Post-Deployment Inference", "content": "Fixed: After storing the values $P_e$, $P_m$ and $P_h$ consisting of embeddings of easy, moderate and hard samples respectively. We calculate the average of these pools and name it as $\\bar{P_e}$, $\\bar{P_m}$ and $\\bar{P_h}$ respectively. The sample can be classified as easy, moderate or hard using the average values as in K- means clustering algorithm, as a sample arrives, the distance of the incoming sample is calculated from $\\bar{P_e}$, $\\bar{P_m}$ and $\\bar{P_h}$ and classifies the sample as easy, moderate or hard based on the minimum distance of the sample from the mean values of different pools. After this the easy samples are inferred locally at the mobile device incurring only processing cost, moderate samples are offloaded directly to the edge without any computation on mobile incurring small offloading cost and processing cost and the hard samples are directly offloaded to the cloud with higher offloading cost as well as cost charged by the cloud platform. Adaptive: In fixed inference, the pools are created using the validation dataset, however during test time there might be a shift in the dataset distribution. For such cases, we dynamically"}, {"title": "update the pool averages such that the distribution shift can be properly captured.", "content": "In this setup, as the sample arrives, it is classified as easy, moderate or hard in a similar way but this time the average is recalculated based on the incoming sample's complexity. For instance, if a sample is easy, then the value the average $\\bar{P_e}$ is recalculated and updated. In this manner, the shift in the test dataset is captured and the trade- off of accuracy-cost is not affected."}, {"title": "E. Analysis of Layer distribution", "content": "We assume that the mobile device contains DNN's first m layers and the edge has DNN's first n layers where $1 < m < n < l$ where the cases m = 1 is when there is no mobile device, m = n means there is either no mobile or edge device. If n = l, it means there is no edge device. We discuss the impact of the values of m and n. These values are important as they model the overall cost and are user-defined. They are used to decide the quantity of workload on different devices, i.e., mobile, edge or cloud. A higher value of m means more layers are deployed on the mobile device and the processing cost e.g. battery depletion will be high, however since more layers are in the mobile device there will be a lower chance of a sample being offloaded reducing the latency cost. If the value of n is high, then there will be fewer samples being offloaded to the cloud reducing the latency costs, however, it will increase load on the edge device. If both m and n are kept small then since less number of layers will inferred earlier, more samples will be offloaded to the cloud increasing the offloading cost and the charges of the cloud platform."}, {"title": "V. EXPERIMENTS", "content": "In this section, we provide all the experimental details of the paper and experimentally validate our method."}, {"title": "A. Dataset", "content": "We used the GLUE [2] datasets for the evaluation of our method. We evaluate our method on three types of tasks viz. sentiment classification, entailment classification and natural language inference. The datasets used are: 1) MRPC: Microsoft Research Paraphrase Corpus is a semantic equivalence classification dataset containing sen- tence pairs extracted from online news sources. 2) QQP: Quora Question Pairs is also a semantic equivalence clas- sification dataset but the sentence pairs are extracted from the community question-answering website Quora. 3) SST- 2: Stanford-Sentiment Treebank is a sentiment classification dataset. 4) CoLA: Corpus of Linguistic Acceptability with a task of linguistic acceptability of a sentence. 5) QNLI: Question-answering natural language inference is a dataset with a labelling task indicating whether the answer logically entails the question's premise. 6) MNLI: Multi-Genre Natural Language Inference also contains sentence pairs as premise and hypothesis, the task is to classify them as entailment, contradiction or neutral."}, {"title": "B. Baselines", "content": "We compare the model against various baselines that are detailed below: 1) BERT model: In this baseline, we report the results of the original BERT backbone. We assume that the BERT model is deployed on the mobile device and only processing cost is incurred. This is the main baseline for us. 2) Random: In this baseline, the incoming sample is randomly assigned to one of the given devices i.e. mobile, edge or the cloud. This is created to show that our assignment based on the pooling of samples makes a significant difference. 3) DeeBERT: is the baseline where we assume that the model is deployed completely on the cloud device but with attached exits i.e., it lowers the charge by the cloud as then only partial resource is used for each sample. This baseline shows that splitting the model also helps due to the presence of hard samples. 4) AdaEE: This method is an adaptive method that uses multi-armed bandits to learn the optimal threshold to decide offloading in an edge-cloud co-inference setup. 5) I-SplitEE: This method learns the optimal splitting layer based on the accuracy-cost trade-off in an online setup. The method uses multi-armed bandits to learn the optimal layer in an edge-cloud co-inference setup where the test dataset contains distortions. 6) Ours-F: is our method that uses a fixed pool average and does not update it during inference. 7) Ours-D: is our method that dynamically updates the pool averages and covers any domain shift occurring during inference. We use the same hyperparameters for all the baselines as given in their respective codebases. The cost for all the baselines is calculated using our cost structure which is very similar to most of the previous methods. There are three key phases in our experimental setup."}, {"title": "C. Training the backbone", "content": "To evaluate our method, we use the widely accepted BERT- base/large model. We add a linear output layer after the final layer on the mobile and, after the final layer on the edge to act as a classifier that maps the intermediate outputs of the backbone to class probabilities. We split the dataset into three parts: 80% for training, 10% for validation and 10% for test. We train the backbone using the train split. We run the model for 5 epochs. We also perform a grid search over a batch size of {8,16,32} and learning rates of {1e-5, 2e-5, 3e-5, 4e-5, 5e-5} with Adam [32] optimizer. We apply an early stopping mechanism and select the model with the best performance on the validation set."}, {"title": "D. Pool creation and cost", "content": "We create the pool using the validation split of the dataset. The values of m and n are chosen using the cost structure and we choose $m = 4, \\eta = 6$ and $m = 6, \\eta = 12$ for BERT-base and large respectively, for the BERT-base model it maps to TinyBERT (4 layers) on mobile, Dis- tilBERT (6 layers) on edge and BERT-base on the cloud while on BERT-large (24 layers), it corresponds to Distil- BERT on mobile, BERT-base on edge and BERT-large on the cloud. The set of thresholds for confidence and variance are chosen from the set $S_{\\alpha} = \\{0.55, 0.6, 0.65,0.7,0.8\\}$ and $S_{\\beta} = \\{0.05,0.08, 0.11, 0.14, 0.17\\}$. Hence, the search space for (\u03b1, \u03b2) is $S_{\\alpha} \\times S_{\\beta}$. The values of \u03b1 and \u03b2 are chosen based on the pair that maximizes the reward in equation 5 on the validation split of the dataset. Recall, that we have denoted the processing cost for the mobile device as $\\lambda_1$ and the processing cost for the edge device as $\\lambda_2$, $o_1$ as the offloading cost for mobile to edge and the offloading cost for mobile to cloud as $o_2$. We also assume the cost charged by the cloud platform as $\\gamma$. We convert all the costs in terms of the smallest unit. Without loss of generality, we assume the smallest cost as"}, {"title": "the processing cost of the edge device, we assume $\\lambda_2 = \\lambda, \\lambda_1 = (3/2)\\lambda, o_1 = (5/2)\\lambda$ and finally $o_2 = 3\\lambda$ to show the results but in the ablation studies, we experiment by varying these costs (see section VII-B).", "content": "For results obtained in Table I we have fixed these values, however, in section VII-B, we show that how the cost changes affect the accuracy and overall cost, this ablation study shows that our model can perform better with any mobile, edge and cloud device as the performance does not degrades with changes in cost. The choice of the cost values is user-specific and processing cost could be chosen as the mobile and edge device computational power and offloading costs depend on the communication networks."}, {"title": "E. Inference", "content": "During inference, we use a batch size of 1 as data arrives sequentially. As a sample arrives, the word embedding of a sample is obtained on the mobile device. Then the distance of the word embedding of the sample is calculated against pool averages and the sample is assigned to the closest pool average. If the closest pool is the easy pool, then the sample is inferred on the mobile device. If the closest is the moderate pool, then the sample is offloaded to the edge device. Else, the sample is offloaded to the cloud. All the experiments were conducted on NVIDIA RTX 2070 GPU with an average runtime of ~ 3 hours and a maximum runtime of ~ 10 hours for the MNLI dataset."}, {"title": "VI. RESULTS", "content": "In Table I and II, we show the main results of our paper, our method outperforms all the existing baselines both in terms of cost as well as accuracy for both BERT-base and large models. The reduction in cost is larger for the BERT-large model which is intuitive as the large variant is more overparameterized."}, {"title": "lesser charges as the sample might not require to pass-through the complete backbone.", "content": "AdaEE also has lower accuracy as the method mostly works better under domain change scenarios, but in our case the domain shift is minor, it simply reduces to an early exit model with dynamic learning of threshold. Due to this dynamic learning of threshold, it outperforms vanilla early exiting. Finally, the I-SplitEE model also has lower accuracy again due to the case that, it works better in domain shift scenarios. In terms of cost, these models are higher as they require all the samples to be processed on the mobile device before offloading. Our method outperforms all the baselines, the higher ac- curacy comes from the appropriate assignment of the sample to various devices and a smaller cost as compared to other methods since all the complexity of the sample is decided based on the word embedding that does not require much processing on mobile reducing the processing cost to a larger extent while in other methods, this cost is very high as it is required for all the samples. Also, note that for the QQP dataset our method even outperforms the vanilla BERT inference, this is because of the overthinking issue during inference. This issue occurs when an easy sample is passed through the complete backbone leading to the extraction of irrelevant features which in turn results in a wrong prediction as pointed out in [10]."}, {"title": "VII. ABLATION STUDY AND DISCUSSION", "content": "In this section, we perform ablation studies and also discuss the choice of layers using the computational powers of mobile and edge and offloading costs."}, {"title": "A. Individual device inference", "content": "We stated that our method uses a distributed inference method between mobile, edge and cloud. In Figure 4a, we show the effect on accuracy when all the samples are inferred on one of the given devices. It means that instead of dis- tributing the inference, performing the inference on a single device. We plot the accuracies of the individual devices and our model. Since the cloud contains the full-fledged DNN, it has the highest accuracy; however"}]}