{"title": "CERTIFIED ROBUSTNESS UNDER BOUNDED LEVENSHTEIN DISTANCE", "authors": ["Elias Abad Rocamora", "Grigorios G. Chrysos", "Volkan Cevher"], "abstract": "Text classifiers suffer from small perturbations, that if chosen adversarially, can\ndramatically change the output of the model. Verification methods can provide ro-\nbustness certificates against such adversarial perturbations, by computing a sound\nlower bound on the robust accuracy. Nevertheless, existing verification meth-\nods incur in prohibitive costs and cannot practically handle Levenshtein distance\nconstraints. We propose the first method for computing the Lipschitz constant\nof convolutional classifiers with respect to the Levenshtein distance. We use these\nLipschitz constant estimates for training 1-Lipschitz classifiers. This enables com-\nputing the certified radius of a classifier in a single forward pass. Our method,\nLipsLev, is able to obtain 38.80% and 13.93% verified accuracy at distance 1\nand 2 respectively in the AG-News dataset, while being 4 orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the impressive performance of Natural Language Processing (NLP) models (Sutskever et al.,\n2014; Zhang et al., 2015; Devlin et al., 2019), simple corruptions like typos or synonym substitutions\nare able to dramatically change the prediction of the model (Belinkov and Bisk, 2018; Alzantot\net al., 2018). With newer attacks in NLP becoming stronger (Hou et al., 2023), verification methods\nbecome relevant for providing future-proof robustness certificates (Liu et al., 2021).\nConstraints on the Levenshtein distance (Levenshtein et al., 1966) provide a good description of the\nperturbations a model should be robust to (Morris et al., 2020) and strong attacks incorporate such\nconstraints (Gao et al., 2018; Ebrahimi et al., 2018; Liu et al., 2022; Abad Rocamora et al., 2024).\nDespite the success of verification methods in the text domain, existing methods can only certify\nprobabilistically via randomized smoothing (Cohen et al., 2019; Ye et al., 2020; Huang et al., 2023),\nor can only handle specifications such as replacements of characters/words, stop-word removal or\nword duplication (Huang et al., 2019; Jia et al., 2019; Shi et al., 2020; Zhang et al., 2021).\nOn the performance side, most successful certification methods rely on Interval Bound Propagation\n(IBP) (Moore et al., 2009), which in the text domain requires multiple forward passes through the\nfirst layers of the model (Huang et al., 2019), unlike in the image domain where a single forward\npass is enough for verification (Wang et al., 2018). Moreover, IBP has been shown to provide a\nsuboptimal verified accuracy in the image domain (Wang et al., 2021).\nIn the image domain, a popular approach to get fast robustness certificates is computing upper\nbounds on the Lipschitz constant of classifiers, and using this information to directly verify with\na single forward pass (Hein and Andriushchenko, 2017; Tsuzuku et al., 2018; Latorre et al., 2020;\nXu et al., 2022). These methods cannot be trivially applied in NLP because they assume the input\nto be in an $l_p$ space such $R^d$, which is not the case of text input, where the input length can vary and\ninputs are discrete (characters). Therefore, we need to rethink Lipschitz verification for NLP.\nIn this work, we introduce the first method able to provide deterministic Levenshtein distance certi-\nficates for convolutional classifiers. This is achieved by computing the Lipschitz constant of interme-\ndiate layers with respect to the ERP distance (Chen and Ng, 2004). Our Lipschitz constant estimates"}, {"title": "2 RELATED WORK", "content": "Lipschitz verification: Hein and Andriushchenko (2017) are the first to study the computation of\nthe Lipschitz constant in order to provide formal guarantees of the robustness of support vector\nmachines and two-layer nueral networks. Tsuzuku et al. (2018) compute Lipschitz constant upper\nbounds for deeper networks and regularize such upper bounds to improve certificates. Since then,\ntighter upper bounds for the Lipschitz constant have been proposed (Huang et al., 2021; Fazlyab\net al., 2019; Latorre et al., 2020; Shi et al., 2022). A variety of works propose constraining the\nLipschitz constant to be 1 during training in order to have automatic robustness certificates (Cisse\net al., 2017; Qian and Wegman, 2019; Gouk et al., 2021; Xu et al., 2022). All previous works center\nin the standard $l_p$ norms and cannot be applied to the NLP domain. Our work provides the first\n1-Lipschitz training method for the Levenshtein distance.\nVerfication in NLP: Jia et al. (2019) propose using Interval Bound Propagation via an over-\napproximation of the embeddings of the set of synonyms of each word. Concurrently, Huang et al.\n(2019) incorporate this technique for verifying against replacements of nearby characters in the\nEnglish keyboard. Bonaert et al. (2021); Shi et al. (2020) propose zonotope abstractions and IBP\nfor verifying against synonym substitutions in transformer models. Zhang et al. (2021) propose a\nverification procedure that can handle a small number of input perturbations for LSTM classifiers.\nDeviating from these approaches, Ye et al. (2020) propose using randomized smoothing techniques\nCohen et al. (2019) in order to verify probabilistically against character substitutions. Huang et al.\n(2023) used similar techniques in order to probabilistically verify under Levenshtein distance spe-\ncifications. Zhao et al. (2022) propose a framework to verify under word substitutions via Causal\nInterventions. Sun and Ruan (2023) derive probable upper and lower bounds of the certified radius\nunder word substitutions. Zhang et al. (2024) employ randomized smoothing to verify against word\n(synonym) substitutions, insertions, deletions and reorderings. Zeng et al. (2023) propose a ran-\ndomized smoothing technique that does not rely on knowing how attackers generate synonyms. In\nTable 1 we highlight the differences with existing works in NLP verification."}, {"title": "3 PRELIMINARIES", "content": "Let $S(\\Gamma) = \\{c_1c_2......c_m : c_i \\in \\Gamma \\forall m \\in \\mathbb{N}\\}$ be the space of sequences of characters in\nthe alphabet set $\\Gamma$. We represent sentences $S\\in S(\\Gamma)$ as sequences of one-hot vectors, i.e., $S\\in$"}, {"title": "3.1 INTERVAL BOUND PROPAGATION (IBP)", "content": "Existing robustness verification approaches rely on IBP for verifying the robustness of text models\n(Huang et al., 2019; Jia et al., 2019). IBP relies on the input being constrained in a box. Let $x, l, u \\in\n\\mathbb{R}^d$, every element of $x$ is assumed to be in an interval given by $I$ and $u$, i.e., $l_i \\leq x_i < u_i \\forall i \\in [d]$\nor $x \\in [l, u]$ for short. These constraints arise naturally when studying robustness in the $l_\\infty$ norm,\nas the constraint $x \\in \\{x^{(0)} + \\delta : ||\\delta||_\\infty \\leq \\epsilon\\}$ can exactly be represented as $x \\in [x^{(0)} - \\epsilon, x^{(0)} + \\epsilon]$.\nIBP consists in a set of rules to obtain interval constraints of the output of a function, given the\ninterval constraints of the input. In the case of an affine mapping $f(x) = Wx + b$, we can easily\nobtain the interval constraints $f(x) \\in [l_{f(x)}, u_{f(x)}], \\forall x \\in [l, u]$ with:\n$l_{f(x)} = W^+l + W^-u + b$, $u_{f(x)} = W^+u + W^-l + b$,\nwhere $W^+$ and $W^-$ are the positive and negative parts of $W$. In the case of the ReLU activation\nfunction $\\sigma(x) = \\max\\{0, x\\}$, we have that:\n$l_{\\sigma(x)} = \\sigma(\\iota)$, $u_{\\sigma(x)} = \\sigma(u)$.\nBy applying recursively the simple rules in Eqs. (2) and (3), one can easily verify robustness prop-\nerties of ReLU fully-connected and convolutional networks (Wang et al., 2018).\nNevertheless, IBP has two main limitations:\na) IBP assumes the input space to be of fixed length, e.g., $\\mathbb{R}^d$.\nb) IBP can only handle interval constrained inputs, e.g., $x \\in [l, u]$.\nLimitation a) makes it impossible to verify Levenshtein distance constraints as they include insertion\nand deletion operations, which change the length of the input sequence. In the literature, limitation\na) forces existing verification methods to only consider replacements of characters/words (Huang\net al., 2019; Jia et al., 2019; Shi et al., 2020; Bonaert et al., 2021; Zhang et al., 2021).\nLimitation b) can be circumvented by building an over approximation of the replacement constraints\nthat can be represented with intervals. In the case of text, one can directly build an over approxima-\ntion of the embeddings. Let $Z = SE \\in \\mathbb{R}^{m\\times d}$, where $S \\in S(\\Gamma)$ is the sequence of one-hot vectors\nrepresenting each character/word, and $E \\in \\mathbb{R}^{|\\Gamma| \\times d}$ is the embedding matrix. Let $d_{edit}$ be the edit\ndistance without insertions and deletions, our constraint in the edit distance (Eq. (1)) translates in\nthe embedding space to the set:\n$Z_k(S) = \\{S'E : d_{edit}(S, S') \\leq k, S' \\in S(\\Gamma)\\}$,\nwhere $d_{edit} (S, S') = \\sum_i ||S_i - S'_i||_\\infty$ for any length $m$ sequences of one-hot vectors $S, S'$. We can\noverapproximate this set with interval constraints such that $2Z \\in [L, U]$, with $l_{ij} = \\min_{z\\in Z_k(S)} Z_{ij}$"}, {"title": "4 METHOD", "content": "In Section 4.1 we cover the verification procedure once the Lipschitz constant of a classifier is\nknown. In Section 4.2 we cover the convolutional architectures employed in Huang et al. (2019)\nand our Lipschitz constant estimation for them. Lastly, we introduce our training strategy in order\nto achieve non-trivial verified accuracy in Section 4.3. We defer our proofs to Appendix B."}, {"title": "4.1 LIPSCHITZ CONSTANT BASED VERIFICATION", "content": "Motivated by the success and efficiency of Lipschitz constant based certification in vision tasks\n(Huang et al., 2021; Xu et al., 2022), we propose a method of this kind that can handle previously\nstudied models in the character-level classification task (Huang et al., 2019), and provide Leven-\nshtein distance certificates.\nOur goal is to compute the global Lipschitz constant. Let $g_{y,\\hat{y}}(S) = f(S)_y - f(S)_{\\hat{y}}$ be the margin\nfunction for classes $y$ and $\\hat{y}$, we would like to have for some $S$:\n$|g_{y,\\hat{y}}(S) - g_{y,\\hat{y}}(S')| \\leq G_{y,\\hat{y}} \\cdot d_{Lev}(S, S') \\forall S' \\in S(\\Gamma)$,\nfor some $G_{y,\\hat{y}} \\in \\mathbb{R}^+$. Given Eq. (4) is satisfied, the maximum distance up to which we can verify\nEq. (1), is lower bounded by:\n$\\max\\{k : g_{y,\\hat{y}}(S') > 0 \\forall S' \\in S(\\Gamma) : d_{Lev}(S, S') \\leq k\\} \\geq \\frac{g_{y,\\hat{y}}(S)}{G_{y,\\hat{y}}}$\nLet $k_{y,\\hat{y}}(S) := \\frac{g_{y,\\hat{y}}(S)}{G_{y,\\hat{y}}}$, we denote $k(S) := \\min_{\\hat{y} \\neq y} k_{y,\\hat{y}}(S)$ to be the certified radius."}, {"title": "4.2 LIPSCHITZ CONSTANT ESTIMATION FOR CONVOLUTIONAL CLASSIFIERS", "content": "Let $S \\in S(\\Gamma)$ be a sequence of one-hot vectors, our classifier is defined as:\n$f(S) = \\sum_{i=1}^{m+l-(q-1)} f^{(i)}(S) W$, where $f^{(i)}(S) = \\begin{cases} \\sigma\\Big(C^{(j)}\\big(f^{(j-1)}(S)\\big)\\Big)  \\frac{S E}{M(E)} & \\forall j = 1,...,l \\\\end{cases}$\nwhere $E \\in \\mathbb{R}^{v \\times d}$ is the embeddings matrix, $C^{(i)}, \\forall i = 1,......,l$ are convolutional layers with\nkernel size $q$ and hidden dimension $k$. $\\sigma$ is the ReLU activation function and $W \\in \\mathbb{R}^{k \\times o}$ is the last\nclassification layer. This architecture was previously studied in verification by (Huang et al., 2019;\nJia et al., 2019).\nOur approach to estimate the global Lipschitz constant of such a classifier is to compute the Lipschitz\nconstant of each layer. Then, since the overall function in Eq. (6) is the sequential composition of\nall of the layers, we can just multiply the Lipschitz constants to obtain the global one. However, in\norder to be able to do this, we need some metric with respect to which we can compute the Lipschitz"}, {"title": "4.3 TRAINING 1-LIPSCHITZ CLASSIFIERS", "content": "Models trained with the standard Cross Entropy loss and Stochastic Gradient Descent (SGD) recipe\nare not amenable to verification methods, resulting in small certified radiuses. This has motivated\nthe use of specialized training methods in the image domain (Mirman et al., 2018; Gowal et al.,\n2018; Mueller et al., 2023; Palma et al., 2024). Verification methods in the text domain also require\ntailored training methods to achieve non-zero certified radiuses (Huang et al., 2019; Jia et al., 2019).\nMotivated by methods enforcing classifiers to be 1-Lipschitz in the image domain (Xu et al., 2022),\nwe enforce this constraint during training in order to improve certification.\nIn order to achieve a 1-Lipschitz classifier, we enforce 1-Lipschitzness of every layer by dividing\nthe output of each layer by its Lipschitz constant. This results in our modified classifier being:\n$f(s) = \\frac{\\sum_{i=1}^{m+l-(q-1)} f^{(i)}(S)}{\\frac{M(W)}{M(W)}}$, where $f^{(i)}(S) = \\begin{cases} \\frac{\\sigma\\Big(C^{(j)}\\big(f^{(j-1)}(S)\\big)\\Big)}{\\frac{M(K^{(i)})}{M(E)}} & \\forall j = 1,...,l \\\\end{cases}$\nwhere $M(W) = \\max_{y,\\hat{y}\\in[o]} ||W_y - w_{\\hat{y}}||$. Note that the last layer is made 1-Lipschitz with respect\nto the worst pair of class labels. Incorporating this information and Remark 4.5, we end up with the\nfinal Lipschitz constant for the classifier:"}, {"title": "5 EXPERIMENTS", "content": "In this section, we cover our experimental validation. In Section 5.1 we cover the experimental setup\nand training mechanisms shared among all experiments. In Section 5.2 we compare performance of\nour approach with existing IBP approaches and the naive brute force verification baseline. Lastly,\nin Section 5.3 we cover the hyperparameter selection of our method. We define our performance\nmetrics and perform additional experiments in Appendix A."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We train and verify our models in the sentence classification datasets AG-News (Gulli, 2005; Zhang\net al., 2015), SST-2 (Wang et al., 2019), IMDB (Maas et al., 2011) and Fake-News (Lifferth, 2018).\nWe consider all of the characters present in the dataset except for uppercase letters, which we token-\nize as lowercase. Each character is tokenized individually and assigned one embedding vector via\nthe matrix $E$. For all our models and datasets, following Huang et al. (2019), we train models with a\nsingle convolutional layer, an embedding size of 150, a hidden size of 100 and a kernel size of 5 for\nthe SST-2 dataset and 10 for the rest of datasets. Following the setup used in Andriushchenko and\nFlammarion (2020) for adversarial training, we use the SGD optimizer with batch size 128 and a\n30-epoch cyclic learning rate scheduler with a maximum value of 100.0, which we select via a grid\nsearch in a validation dataset, see Appendix A.5. For every experiment, we report the average results\nover three random seeds and report the performance over the first 1,000 samples of the test set. Our"}, {"title": "5.2 COMPARISON WITH IBP AND BRUTE FORCE APPROACHES", "content": "In this section, we compare our verification method against a brute-force approach and a modifica-\ntion of the IBP method in (Huang et al., 2019) to handle insertions and deletions of characters.\nWith the brute-force approach, for every sentence $P$ in the test dataset, we evaluate our model in\nevery sentence in the set $\\{Q : d_{lev}(P, Q) \\leq k\\}$ and check if there is any missclassification. Since\nthe size of this set grows exponentially with $k$, we only evaluate the brute-force accuracy for $k = 1$.\nIn the case of IBP, we evaluate the classifier up to the pooling layer in every sentence of $\\{Q :\nd_{lev}(P, Q) \\leq k\\}$ and then build the overapproximation. In (Huang et al., 2019) it was enough to\nbuild this overapproximation for $k = 1$ and re-scale it to capture larger $k$s. This is not the case for\ninsertions and deletions, this constrains IBP with Levenshtein distance specifications to work only\nfor $k = 1$. Overall, the complexity of IBP is the same as the brute-force approach without providing\nthe exact robust accuracy. Because Huang et al. (2019) only considered perturbations of characters\nnearby in the English keyboard, the maximum perturbation size at $k = 1$ was very small, e.g., 206\nand 722 sentences for SST-2 and AG-News respectively. In our setup, the maximum perturbation\nsizes are 33, 742 and 85,686. This makes it impractical to perform IBP verified training. We train\n3 models for each dataset and $p\\in \\{1,2,\\infty\\}$ and verify them with the three methods. We report the\naverage time to verify and the clean, adversarial and verified accuracies at $k \\in \\{1,2\\}$."}, {"title": "5.3 REGULARIZING THE LIPSCHITZ CONSTANT", "content": "In Section 4.3 we describe how to enforce our convolutional classifier to be 1-Lipschitz. But, is\nthere a better way of improving the final verified accuracy of our models? Because our Lipschitz\nconstant estimate in Theorem 4.3 its differentiable with respect to the parameters of the model,\nwe can regularize this quantity during training in order to achieve a lower Lipschitz constant and\nhopefully a better verified accuracy. In practice we regularize $G = M(W) \\cdot M(K^{(1)}) \\cdot M(E)$ as\ndefined in Theorem 4.3 and Corollary 4.6.\nWe train single-layer models with a regularization parameter of $\\lambda\\in \\{0,0.001,0.01, 0.1\\}$, where\n$\\lambda = 0$ is equivalent to standard training. We initialize the weights of each layer so that their Lipschitz\nconstant is 1. We use a learning rate of 0.01. We measure the final Lipschitz constant of each model\nand their clean and verified accuracies in a validation set of 1,000 samples left out from the training\nset. As a baseline, we report these metrics for the models trained with the formulation in Eq. (7).\nIn Table 3 we observe that for all the studied norms, when regularizing the Lipschitz constant $G$,\nwe cannot easily match the performance when using Eq. (7). Regularized models converge to either\nclose-to-constant classifiers (55.7% clean accuracy for SST-2) or present a close-to-zero verified\naccuracy, This behavior has also been observed practically and theoretically for $l_p$ spaces (Zhang\net al., 2022). The formulation in Eq. (7) allows us to obtain verifiable models without the need to\ntune hyperparameters."}, {"title": "5.4 THE INFLUENCE OF SENTENCE LENGTH IN VERIFICATION", "content": "In this section we study the qualitative characteristics of a sentence leading to better verification\nproperties, specifically, we study the influence of the sentence length in verification. We compute"}, {"title": "6 CONCLUSION", "content": "In this work, we propose the first approach able to verify NLP classifiers using the Levenshtein\ndistance constraints. Our approach is based on an upper bound of the Lipschitz constant of convolu-\ntional classifiers with respect to the Levenshtein distance. Our method, LipsLev is able to obtain\nverified accuracies at any distance $k$ with single forward pass per sample. Moreover, our method\nis the only existing method that can practically verify for Levenshtein distances larger than $k = 1$.\nWe expect our work can inspire a new line of works on verifying larger distances and more broadly\nverifying additional classes of NLP classifiers. We will make the code publicly available upon the\npublication of this work, our implementation is attached with this submission.\nFuture directions and challenges: A problem shared with verification methods in the image do-\nmain is scalability (Wang et al., 2021). Scaling verification methods to production models is a\nchallenge, that becomes more relevant with the deployment of Large Language Models and their re-\ncently discovered vulnerabilities (Zou et al., 2023). Even though our method is the first to practically\nprovide Levenshtein distance certificates in NLP, neither the formulation of Huang et al. (2019) or\nour formulation covers modern architectures as Transformers (Vaswani et al., 2017). We highlight\nthe main challenges as follows:\ni) Tokenizers: Modern Transformer-based classifiers utilize popular tokenizers such as Sen-\ntencePiece (Kudo and Richardson, 2018), which aggregate contiguous characters in tokens\nbefore feeding them to the model. In order to deal with such non-differentiable piece,\nmethods for computing the Lipschitz constant of tokenizers are needed.\nii) Poor performance on character-level tasks: In the case no tokenizer is used, transformers\nare known to fail in character-level classification tasks like the IMDB classification problem\nof Long Range Arena (Tay et al., 2021).\niii) Non-Lipschitzness of Transformers: Transformers are known to have a non-bounded\nLipschitz constant (Kim et al., 2021). In the image domain, verification methods modify\nthe model to be Lipschitz (Qi et al., 2023; Bonaert et al., 2021) or compute local Lipschitz\nconstants (Havens et al., 2024). Nevertheless, it is non-trivial to extend such approaches\nfrom $l_p$-induced distances to the Levenshtein distance.\nOur work sets the mathematical foundations of Lipschitz verification in NLP, opens the door to\naddressing these challenges and to achieving verifiable architectures beyond convolutional models."}, {"title": "A ADDITIONAL EXPERIMENTAL VALIDATION", "content": "In Appendix A.1 we present the definition of the performance metrics employed in this work. In\nAppendix A.5 we present our grid search for selecting the best learning rate for each dataset and $p$\nvalue in the ERP distance Definition 4.1. In Appendix A.6 we evaluate the effect of training with a\ndifferent number of layers and report their latencies."}, {"title": "A.1 DEFINITION OF PERFORMANCE METRICS", "content": "In this section we define the key metrics used to evaluate our models and verification methods. Let\n$\\mathbb{1}(\\cdot)$ be the indicator function, given a classification model $f : S(\\Gamma) \\rightarrow \\mathbb{R}^o$ assigning scores to each\nof the $o$ classes and a dataset $D = \\{S^{(i)},y^{(i)}\\}_{i=1}^n$ with $S^{(i)} \\in S(\\Gamma)$ and $y^{(i)} \\in [o]$, the clean,\nadversarial and verified accuracy are defined as:\nDefinition S1 (Clean accuracy). The clean accuracy is a percentage in [0, 100] that is computed as:\nAcc.$(f, D) = \\frac{100}{n}\\sum_{i=1}^n \\mathbb{1}(y^{(i)} = \\arg \\max_{j\\in[o]} f(S^{(i)})_j )$\nDefinition S2 (Adversarial accuracy). Given an adversary $A : S(\\Gamma) \\rightarrow S(\\Gamma)$ that perturbs a sen-\ntence. The adversarial accuracy is a percentage in [0, 100] that is computed as:\nAdv. Acc.$(f, D, A) = \\frac{100}{n}\\sum_{i=1}^n \\mathbb{1}(y^{(i)} = \\arg \\max_{j\\in[o]} f(A(S^{(i)}))_j )$\nDefinition S3 (Verified accuracy). Given a verification method $v$ returning the certified radius (see\nSection 4.1) for a given model $f$ and sample $(S, y)$ as $v(f, S, y) \\in \\{0\\} \\cup \\mathbb{N}$. The verified accuracy\nat distance $k$ is a percentage in [0, 100] that is computed as:\nVer. Acc. $(f, D, v,k) = \\frac{100}{n}\\sum_{i=1}^n \\mathbb{1}(v(f, S^{(i)},y^{(i)}) \\geq k)$.\nFor simplicity, the arguments of each accuracy function are omitted in the text as they can be inferred\nfrom the context."}, {"title": "A.2 SENTENCE LENGTH DISTRIBUTION", "content": "In this section, we provide additional details about the sequence length distribution of verified and\nnot verified sentences. Specifically, in Fig. S2 we provide the full distribution of lengths from the\nexperiment in Fig. 1."}, {"title": "A.3 STANDARD DEVIATIONS", "content": "In Table S4 we report the results from Table 2 with standard deviations for completeness."}, {"title": "A.4 LARGER DISTANCES FOR FAKE-NEWS", "content": "In Section 5.2 we reported the performance in the Fake-News dataset over the first 50 samples of\nthe test set and only up to $k = 2$. Nevertheless, the speed of Lips Lev allows for more samples and\nlarger distances. In Table S5, we evaluate the performance of LipsLev over the first 1, 000 test\nsamples and up to $k = 10$."}]}