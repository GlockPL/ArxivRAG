{"title": "A Meta-Learning Approach for Multi-Objective Reinforcement Learning in Sustainable Home Energy Management", "authors": ["Junlin Lu", "Patrick Mannion", "Karl Mason"], "abstract": "Effective residential appliance scheduling is crucial for sustainable living. While multi-objective reinforcement learning (MORL) has proven effective in balancing user preferences in appliance scheduling, traditional MORL struggles with limited data in non-stationary residential settings characterized by renewable generation variations. Significant context shifts that can invalidate previously learned policies. To address these challenges, we extend state-of-the-art MORL algorithms with the meta-learning paradigm, enabling rapid, few-shot adaptation to shifting contexts. Additionally, we employ an auto-encoder (AE)-based unsupervised method to detect environment context changes. We have also developed a residential energy environment to evaluate our method using real-world data from London residential settings. This study not only assesses the application of MORL in residential appliance scheduling but also underscores the effectiveness of meta-learning in energy management. Our top-performing method significantly surpasses the best baseline, while the trained model saves 3.28% on electricity bills, a 2.74% increase in user comfort, and a 5.9% improvement in expected utility. Additionally, it reduces the sparsity of solutions by 62.44%. Remarkably, these gains were accomplished using 96.71% less training data and 61.1% fewer training steps.", "sections": [{"title": "Introduction", "content": "Reducing greenhouse gas emissions has become an essential concern. Electricity production contributes to more than a quarter of the emission [2], where the residential sector plays a considerable role. It has consumed around 26.8% of the global electricity in 2022 [10]. Although renewable generations can mitigate this challenge, the intermittent and verifying nature limits their utilization [21]. Effective energy management techniques have the potential to overcome this.\nOne of the candidate techniques is reinforcement learning [8, 30]. While the single-objective reinforcement learning (SORL) method is widely used, in residential energy management in practical scenarios the user always needs to make trade-offs between multiple objectives e.g. saving costs and increasing comfort [21]. It is therefore more reasonable to render the problem as multi-objective reinforcement learning (MORL) [18, 14, 5]. Given that MORL in residential energy management effectively addresses practical scenarios, it is essential to keep validating the latest MORL method in this field for further improvement. We look into the residential appliance scheduling in this work specifically which is one of the most important parts of energy management.\nAlegre et al. recently proposed two MORL algorithms, i.e. genearalized policy improvement linear support (GPI-LS) and genearalized policy improvement prioritized dyna (GPI-PD) [5]. The agent guarantees rapid training by identifying the corner weight [32] align which the whole policy set can achieve the largest improvement. GPI-PD is the first model-based MORL algorithm and GPI-LS is the model-free version. However, they were only evaluated in simulated environments in the original work, and it is challenging for them to work well with the non-stationary environment (which arises from intermittent renewable energy production predominantly) of appliance scheduling. In fact, we found in our experiment that the policy trained with one-month data fails to even surpass a simple rule-based policy.\nOne intuition is to finetune the policy in response to significant context shifts. However, context shifts are hard to identify due to the absence of explicit labels for these qualitative changes. Furthermore, our experimental findings indicate that merely finetuning a trained policy with new data does not enhance performance effectively. Another intuitive approach is to train the policy with more data, e.g. entire year's data, but this method suffers from the expense of computation overhead. The non-stationary environment nature introduces two critical challenges: (i) With as little data as possible, how can the policy generally be good and easy to finetune? (ii) How to identify the environment context shifts in an unsupervised manner?\nMeta-learning [11] and [28], is particularly adept at handling scenarios involving the distribution of environment contexts and identifying a parameter initialization that can be rapidly and effectively finetuned to adapt to new contexts. To address the first challenge we have extended the GPI algorithms with meta-learning to enable their few-shot finetuning ability. For the second challenge, we have adopted an AE model as an unsupervised method for detecting qualitative shifts in context[6, 43, 44]. This approach enables the identification of significant shifts in environmental contexts.\nThe contributions of this research are as follows:\n1. We identify residential energy management as intrinsically compatible with meta-learning and extend GPI algorithms with the meta-learning paradigm. Our top-performing method significantly"}, {"title": "Related Work", "content": "MORL methods are designed for multi-objective sequential decision-making problems. The MORL agent is trained to make trade-offs among multiple objectives. The return from the interaction between the agent and the environment in MORL is a vector rather than a scalar as in SORL. Nevertheless, as the return vector is usually scalarized as a utility by calculating the inner product with the preference weight vector [13], one intuitive way to sort a MORL problem is to separate a MORL policy training from the training of multiple SORL policies based on different preference weight vectors [24, 35, 36, 33, 26]. This method, however, is computationally expensive when there are many candidate preference weights, or even intractable when the preference is not given.\nThis limitation is mitigated by the MORL paradigm conditioned on the preference weight. Usually, a conditioned MORL model generates actions based on the given preference weight [3, 41, 15]. They do not need to know the user's specific preference but a sample from the preference weight space during training. Yang et al. proposed Envelope Q learning[41], that adopts vectorized update. At each training step, it samples preference weights randomly. However, rather than randomly sample preference for training, enhancements were conducted on this algorithm where the weight that can achieve the largest improvement is sampled. Basaklar et al. [7] proposed a method to efficiently update the model based on the angle between the Q-values and the weight vectors. Alegre et al. [5] leveraged GPI to find an update direction with the largest potential to propel the improvement of the whole policy set."}, {"title": "MORL for Energy Management", "content": "MORL has been widely used in energy management, e.g. micro-grid control [40, 20], water heating system oversight [31], energy control in vehicles [45, 39], and the management of residential energy systems [21, 22, 23]. In assessing multi-objective energy management tasks, linear scalarization is usually employed to determine the aggregated return from various objectives [13]. The weights reflecting the potential preferences of users are on a simplex. As the number of objectives increases, the complexity of the problem escalates exponentially. A noteworthy consideration is that a finite number of weight factor combinations are available [32]. This has prompted the application of fuzzy logic techniques to streamline the solution set in MORL for energy management [9, 29, 34, 42]. Liu et al. [20] introduced a policy-based, model-free MORL algorithm that employs the Borg MOEA approach [12] for policy improvement. Despite these advancements, a value-based evolutionary MORL algorithm remains a gap. Although an actor-critic model of evolutionary reinforcement learning is proposed [17], it is tailored for single-objective scenarios and has not been adapted for MORL contexts. Moreover, the model-based MORL algorithm [5] is not evaluated within residential energy management yet.\nWu et al. have used a prioritized dueling double DQN to formulate a multi-objective energy management system, integrating multiple objectives within a singular value function to optimize cumulative rewards. This strategy, similar to the approaches adopted by Riebel et al. [31] and Xu et al. [40], involves incorporating various objectives into one reward function. This potentially results in significant computational demands when the preference for the objectives changes. Cutting-edge MORL algorithms can dynamically adjust to changes in the weight combination of objectives [5], yet their practical evaluation in the context of residential energy management remains unexplored."}, {"title": "Meta Learning", "content": "Humans can get satisfactory performance on a new task with a few attempts if they have some related knowledge. Although AI players can reach the human levels in many cases, they always need more samples. Meta-learning was proposed to train a model to quickly adapt to new scenarios by using a small amount of data[11]. Vettoruzzo et al. conducted a comprehensive review of the meta-learning technologies. They provide a taxonomy of meta-learning methods, i.e. black-box meta-learning methods; optimization-based meta-learning methods; meta-learning via distance metric learning; and hybrid approaches [37].\nIn this work, we concentrate on optimization-based meta-learning methods. Consequently, while hybrid approaches, such as those discussed in [38], offer valuable insights and advancements by integrating various meta-learning strategies, they extend beyond the scope of our current analysis.\nA foundational work in optimization-based meta-learning is Model-Agnostic Meta-Learning (MAML) [11], renowned for its broad applicability and impact. MAML learns initial model parameters which can fast adapt to a new task with only few-shot training. There is an inner loop and an outer loop in MAML. In the inner loop (fast adaptation), for each sampled task, the current model parameters are used to conduct one or several gradient descent steps. This is to obtain the task-optimized parameters. In the outer loop (meta-update), the performance across all tasks is evaluated. This is used to back-propagate through the model's initial parameters for update. With these two loops, a set of initial parameters that allows the model to quickly adapt to new tasks is found.\nNevertheless, due to the computational burden associated with the bi-level gradient optimization in MAML, algorithms like Reptile and FOMAML [28] have been developed to alleviate these issues. Reptile seeks to identify a set of initial parameters that are near the optimal parameters for individual tasks. In our research, we employ Reptile as the meta-learning methodology due to its computational efficiency and its compatibility with neural network architectures."}, {"title": "Preliminary", "content": "We model the multi-objective residential energy management problem as a multi-objective Markov decision process (MOMDP), i.e. $M := (S, A, T, \\gamma, \\mu, R)$ [13]. The spaces of states and actions are denoted as S and A; $T : S\\times A\\times S \\rightarrow [0, 1]$ is the probabilistic transition function; $\\gamma \\in [0, 1)$ is a discount factor; $\\mu : S_0 \\rightarrow [0, 1]$ is the initial state probability distribution; The function $R : S \\times A \\times S \\rightarrow R^d$ is a vectorized reward function, focusing on two main objectives in this work: maximizing comfort and minimizing energy costs. We start with introducing some necessary concepts of MORL setting.\nIn MORL setting, the value vector, starting from an initial state distribution $\\mu$ then following policy $\\pi$ is $v^{\\pi}:= E_{s_0 \\sim \\mu}[q^{\\pi}(s_0, \\pi(s_0)]$. The i-th component of $v^{\\pi}$ represents the value returned for the i-th objective. With the value vector of policy, we can define the Pareto dominance relation $(>_p)$: $v^{\\pi} >_p v^{\\pi'} \\leftrightarrow (\\forall i: v^{\\pi}_i \\geq v^{\\pi'}_i) \\cap (\\exists i: v^{\\pi}_i > v^{\\pi'}_i )$ We say that $\\pi$ is non-dominated when at least one element of $v^{\\pi}$ is greater than all other $v^{\\pi'}$. The Pareto front (PF) is therefore defined: $F := \\{v^{\\pi}|\\nexists\\pi' s.t. v^{\\pi'} >_p v^{\\pi}\\}$\nIn MORL, policy evaluation is dependent on the preference vector given. To involve different user-defined preferences over the objectives, a utility function is used [13] to scalarize the reward vector. Linear utility function [13] is the most frequently used utility function. A linear weight vector w denoting the user preference of importance over each objective is given to scalarize $v^{\\pi}$.\n$\\langle v^{\\pi}, w \\rangle = v = v^{\\pi} \\cdot w  \\qquad (1)$\nwhere w from the simplex $W: \\Sigma w_i = 1, w_i \\geq 0$. The convex coverage set, CCS represents a finite convex selection of the Pareto front (PF). Each point in the CCS is at least one optimal policy corresponding to a certain linear preference. Formally defined as:\n$CCS := \\{v^{\\pi} \\in F|\\exists w s.t.\\forall v^{\\pi'} \\in F, v^{\\pi} \\cdot w > v^{\\pi'} \\cdot w\\}$ When using MORL in residential energy management, as renewable energy source is introduced, different weather conditions over the year can cause very different generation backgrounds. Traditional appoaches often struggle to cope with such environmental variability.\nMeta-learning aims at fast adaptation to new tasks through training on a variety of tasks, which aligns seamlessly with our scenario. The next challenge is to recognize when the underlying environment changes. We use the unsupervised anomaly detection technique to distinguish different renewable generation contexts as distinct tasks and utilize a meta-learning approach to identify initial model parameters that are suitable for rapid fine-tuning during evaluation.\nOur concern is to enhance the GPI-based approach with the meta-learning method to initialize the model with a set of parameters that performs generally well in all contexts and can be effectively fine-tuned through a few-shot manner."}, {"title": "R-GPI-LS/PD Algorithm", "content": "We first introduce the method we used to do context shift detection. Then we outline the meta-learning GPI algorithms."}, {"title": "Context Detection", "content": "In this work, the context shifting points in residential energy management are defined as the points in time when changes in weather cause significant variations in the power output of renewable energy sources (substantial changes in the MDP). Those shifting points are not easily detectable, and there is also a lack of labeled data to support the supervised learning model. We leverage the unsupervised learning paradigm based on AE architecture [6, 43, 44].\nThe AE is designed to try to reconstruct the input data. Such a model, when trained on data from a specific context can gain good reconstruction ability. We reversely use its design mechanism to detect the context-shifting points. When data from a different context comes, the reconstruction loss will see a significant rise.\nWe first train the AE on an initial context data. Once the training converges (or even overfits), we proceed with the reconstruction of the subsequent data, while monitoring the AE's reconstruction loss. We average the loss values over a moving window. When the reconstruction loss for a specific context's data points exceeds the rolling average, it indicates that a significant shift in context has occurred. At this point, we designate the current context as a new context and continue training the AE on this segment of context. This process is repeated until the detection is complete for the entire year's data. See Algorithm 1 for more detail.\n```\nAlgorithm 1 Context Detection\nInput Windowed annual dataset {win1, ..., winn}; Initial AE\nmodel; Threshold value $\\delta = -\\infty$; Reconstruction loss function\nL; List of reconstruction loss [ ] c = []; Last window of this\ncontext $win_c = win_1$; Context list [ ] context = []\nfor wini in {win1, ..., winn} do\n    if $L(win_i, AE(win_c)) > \\delta$ then\n        Add i to [ ]context\n        Empty the reconstruction loss list [ ] = [ ]\n        Train AE with the data {$win_c, ..., win_i$}\n    end if\n    Add $L(win_i, AE(win_i))$ to []c\n    Update the threshold $\\delta \\leftarrow mean([]c)$\nend for\nOutput[] context\n```"}, {"title": "Reptile and GPI Algorithm", "content": "To make the model able to adjust to non-stationary environments while saving computational overhead, we use the meta-learning paradigm to improve the GPI-based algorithm. We use Reptile [28] to do the meta-learning task. Reptile is a first-order, gradient-based meta-learning algorithm that we favor due to its ability to avoid second-order gradient computations [11] and save computational resources."}, {"title": "R-GPI-LS/PD Meta Training", "content": "```\nAlgorithm 2 R-GPI-LS/PD Meta Training\nInput Learning rate $\\epsilon$; Initial model $\\phi$; Epochs number $N_{epochs}$;\nRun Algorithm 1 to find the set of different []context\nfor iteration = 1 to $N_{epochs}$ do\n    Sample a context from []context\n    Get updated parameter: $\\phi' \\leftarrow GPI(\\pi_{\\phi}, context)$\n    Do update the original parameter: $\\phi \\leftarrow \\phi + \\epsilon(\\phi' - \\phi)$\nend for\nOutput Few-shot finetune MORL policy model\n```"}, {"title": "Finetune R-GPI-LS/PD", "content": "After the meta-training phase, the model is capable of being few-shot finetuned. The finetuning procedure for the R-GPI-LS/PD model is elaborated in Algorithm 3. Initially, the meta-trained model, denoted as $\\Phi$, is finetuned using data from the first day. Subsequently, the policy continues to operate until a contextual shift is detected. Upon detection, the original $\\Phi$ is finetuned with data from the current day. This repeats throughout until the entire year's data is used up. During the finetuning process, the rewards obtained are recorded and subsequently summed at the end of the process to calculate the expected utility for the entire year.\n```\nAlgorithm 3 R-GPI-LS/PD Finetuning\nInput Learning rate $\\alpha$; Pre-trained (with Algorithm 2) model $\\Phi$;\nWindowed annual dataset {win1, ..., winn};\nfor wini in {win1, ..., winn} do\n    if wini is recognized as a new context by Algorithm 1 then\n        Sample $win_{sub} \\subset win_i$  $\\triangleright$ Few-shot finetuning\n        $\\Phi_i \\leftarrow GPI(\\pi_{\\Phi}, win_{sub})$\n    end if\n    Conduct $\\pi_{\\Phi_i}$ in the environment\nend for\n```"}, {"title": "Experiments", "content": "In this section, we describe the simulation environment, which is constructed using real-world data. We detail the baselines, specifying the data volume and the number of training steps (interchangeably referred to as the training budget) allocated to each. We then introduce the metrics used, i.e. expected utility, PF approximation visualization, hypervolume, sparsity and the return vector for two specific cases either cares more about the cost or maximize the comfort."}, {"title": "Experiment Settings", "content": "The weather data and background power demand data used in this study are derived from residential settings in London (Latitude 51.331, Longitude 0.033, Elevation 182.3m, Hourly-based); Specifically, the weather data is obtained from [19] and processed using SAM [27] to simulate renewable energy generation (Using Solar Panel of SunPower Performance 17 SPR-P17-335-COM). Residential energy consumption data comes from [16], and electricity pricing is sourced from [1] with British Gas electricity rates at 36.62p/kWh during 08:00 - 23:00 and 15.18p/kWh from 23:00 - 08:00.\nWe have removed the solar heat pump and other related boilers from the house. Instead, we have integrated the Ariston VELIS EVO 80 L Electric Storage Water Heater with a capacity of 1.5 KW, scheduling its operation for 4 hours between 0:00 - 8:00 every day. The agent manages the heater operation on an hourly basis.\nThe two objectives for MORL are:\n1. to save the cost (\u00a3);\n2. to maximize the comfort (try the best to make the appliance to work in the assigned time slot.).\nThe state space of our model is defined by a tuple consisting of the following elements: Background Power Demand (kW), Time (hrs), Remaining Task (hrs), and Renewable Generation (kW).\nThe action space is binary, with only two options: 0 or 1. An action of \"0\" indicates turning the appliance off, whereas an action of \"1\" means turning the appliance on.\nThe reward space is constructed from two primary components: the hourly bill and comfort. The comfort reward is set to 1 when the appliance operates between 0:00 and 8:00 and the remaining task > 0, otherwise 0. The hourly bill reward is calculated as the negative value of the energy cost (\u00a3), penalizing higher bill and encouraging energy-efficient behaviors."}, {"title": "R-GPI-LS/PD & Finetune R-GPI-LS/PD", "content": "We summarize our methods, i.e. R-GPI-LS/PD and Finetune R-GPI-LS/PD.\ni. R-GPI-LS/PD: It centers on meta-training the GPI-LS/PD policy with daily data at contextual shift points, emphasizing the ability to swiftly adapt to significant context shifts. Through this process, the method establishes a set of initial parameters that are easy to finetune.\nii. Finetune R-GPI-LS/PD: With the initial parameters from R-GPI-LS/PD. During the interaction with the environment, when new contextual shifts are detected, the policy is finetuned with the current day's data. This continuous finetuning is can further improve the model's performance throughout changing contexts over the year."}, {"title": "Baselines", "content": "We detail the baselines we used in this Section. The primary baselines are variants of GPI-LS/PD as it is the current state of the art. Notably, other cutting-edge MORL algorithms such as SFOLS [4] and Envelop Q Learning [41] were compared with GPI-LS/PD in the work of [5] and were found to be less effective."}, {"title": "Baselines in Main Evaluation", "content": "To evaluate of our methods, several baseline models are used:\ni. GPI-LS/PD(month): It involves training a plain GPI-LS/PD model on data collected from January 2014 for 40,000 steps.\nii. Finetuning GPI-LS/PD: Starting with GPI-LS/PD(month), it is retrained with the current day's data of detected contextual shifts (12) for an additional 5,000 steps each.\niii. Rule-based policies: Rule 1: operates the appliance between 0:00 and 4:00. Rule 2: operates the appliance between 4:00 and 8:00."}, {"title": "Baselines in Ablation Study", "content": "To understand the contributions of various components of our method, we conduct an ablation study from two distinct perspectives:\ni. Without the meta-learning method and context detection:\nGPI-LS/PD (year): A plain GPI-LS/PD model is trained with the entire year data (2014-2015). This evaluates the performance of the GPI-LS/PD model simply with much more train data and without any meta-learning or context-detection approach.\nii. Without meta-learning method but with context detection: Joint Training GPI-LS/PD: It incorporates solely context detection to the plain GPI-LS/PD to assess the impact of context detection on the model's adaptation to context shifts. We use a strong baseline i.e. joint training, which was also used in [28]. The plain GPI-LS/PD model is trained with concatenated day-based data from the 12 shifting points. iii. The same rule-based policies as Section 5.1.3.1."}, {"title": "Candidate Methods Setting", "content": "We detail the training data volume and the training budget (the number of train steps) in Table 1 to compare the data and train efficiency. The data volume is determined by multiplying the number of days by the 24 hours. Specifically, the GPI-LS/PD (month) and GPI-LS/PD (year) models utilize 720 and 8,650 data samples, respectively. Joint Training GPI-LS/PD, R-GPI-LS/PD and Finetune R-GPI-LS/PD, only use 288 data samples. Finetune GPI-LS/PD uses 720+288=1008 data samples.\nGPI-LS/PD(month), Finetune GPI-LS/PD, GPI-LS/PD(year), and Joint Training GPI-LS/PD use substantially more training budget. Finetune GPI-LS/PD are trained for 5000 steps at each shifting point. R-GPI-LS/PD are trained through 3 iterations on 10 shifting points sampled out of the 12 at each iteration (each point for 480 steps). Finetune R-GPI-LS/PD are trained for 96 steps on each shifting point. However, the 14400 timesteps should also be counted as it is trained based on R-GPI-LS/PD."}, {"title": "Evaluation Metric", "content": "All baselines are evaluated on the performance over a full year (2014-2015). We use multiple MORL metrics to evaluate our methods and baselines, i.e. expected utility (EU), PF approximation visualization, hypervolume (HV), and sparsity (Sp) of the PF solutions.\nFor the specific evaluation for the energy community, we also evaluate the candidate methods by comparing the energy bill on preference [0.9,0.1] and comfort on preference [0.1,0.9].\nAs emphasized in the work of Hayes et al. [13], EU is a preferable metric as it is more suitable to assess the solution's practical value for the user, while the HV is a bit problematic for comparing solutions in real-scenario especially when utility function is known to be linear, and Sp is the metric to measure the density of coverage of the whole PF as a alternates of HV. The solution set with a higher HV and lower Sp is preferable. Given the high level of user involvement in our scenario, we prioritize the evaluation metrics accordingly, placing the greatest emphasis on the EU, followed by HV and Sp together to reflect the user-centric assessment. Simply speaking, the higher the EU and HV and the ratio of HV/Sp are, the lower the Sp is, the better the solutions are. For the energy area specified two metrics, we say that the lower the bill is when given [0.9,0.1], and the higher the comfort when given [0.1,0.9], the better the solution set is."}, {"title": "Experiment Result and Discussion", "content": "This section consists of the result of context detection, the main evaluation, the ablation study, and the discussion and results summary."}, {"title": "Result of Context Shifting Detection", "content": "We present the outcomes of detecting context-shifting points (12 shifting points on days 1, 28, 42, 56, 70, 84, 112, 161, 203, 231, 266, and 357.)\nOne interesting observation is that the shifting points are more heavily concentrated towards the start of the year. This probably stems from the \"spring cold snap\", where spring temperatures suddenly drop to near-winter levels. This usually happens because cold air from the north can still affect warmer regions during spring and may cause sudden clouds, rain, or hail and influence renewable generation. Conversely, \"autumn cold snap\" is not common. This may be because the gradual temperature drop typically progresses steadily into winter without the sudden cooling seen in spring. It is therefore more stable than the spring season."}, {"title": "Main Evaluation", "content": "Evaluation results on EU, HV, Sp, and HV/Sp are shown in Fig. 2. The GPI-LS (month) and Finetune GPI-LS models display similar median values but the latter has a smaller variance. GPI-PD (month) has the highest median and max values among the first four baselines. It is counter-intuition that Finetune GPI-PD is the worst among the four baselines as the environment model was supposed to help with the policy improvement. This may be because that the environment is hard to be accurately modelled during finetuning and therefore hinders the policy improvement. Unfortunately, neither of these four baselines' median exceeds the rule-based policy. This shows that the pure GPI-LS/PD struggle in such changing environments."}, {"title": "Ablation Study", "content": "We conduct the ablation study and show the results in Fig. 4 and Fig. 5. Recall that the ablation study focuses on two aspects, i.e. without both context detection and meta-learning, i.e. GPI-LS/PD (year) and only without meta-learning, i.e. Joint Training GPI-LS/PD.\nEach candidate's EU medians surpass rule-based policies. In most cases, GPI-PD variants see larger variance than their GPI-LS counterparts, except the joint training cases where the GPI-PD variant is marginally lower in variance. This is consistent with Fig. 2 that GPI-PD variants can bring more variance. This may stem from the fact that some state elements are outside the agent's control, e.g. renewable generation, which make the environment model's prediction of the next state incorrect and hinder the training. Notably, only the joint training setting and the month-based training setting have seen better performance from model-based variant. As our statement in last Section, i.e. \"finetune process makes it hard to model the environment\", we now further analyze this. The internal similarity of GPI-PD (month) and Joint Training GPI-PD are the training environment. GPI-PD (month) is trained with a almost stable environment (1-30 days) where the next shifting point is just detected at day 28. Joint Training GPI-PD is trained with the day-based concatenated 12 shifting points. It is a synthetic environment and shifting points are evenly distributed. GPI-PD(year) can be deemed as a data-dense version of joint training where the different contexts are also concatenated but not evenly distributed. However, the difference of context distribution between year-based train and joint training has seen contrary results. These observations mean that the model-based MORL algorithm needs either a stable environment or an environment that if the uncertainty is brought by states outside of the agent control it would be better that those context shifts are evenly distributed. We leave this for future work that the model-based RL algorithm needs improvement to handle these problems, i.e. unevenly distributed contexts, and performance drop in finetune."}, {"title": "Result Summary", "content": "We summarize the evaluation in Table 2. See percentage comparison in Appendix A and full solution visualization in Appendix B.\nIt does not surprise us that simply finetuning the plain GPI models cannot achieve performance improvement (even cause decrease). This is because the internal representation of the trained model may only be suitable to specific contexts but not to others. When doing finetuning, these \"stubborn\" representations can hinder the parameter update. With meta-learning, the model can find the most context-sensitive parameters that are capable of later fintuning.\nThe ablation study validates all components of our algorithms. Notably, GPI-LS/PD (year) accesses diverse and more extensive data. Common sense suggests that training with more data should enhance model performance, however, this is not always true according to our experiment, i.e. the Joint Training GPI-PD is better than GPI-PD (year). As we mentioned in Section 6.3, this should be due to the unevenly distributed context shifting.\nJoint Training GPI-LS/PD still are outperformed by Finetune R-GPI-LS/PD, and merely match R-GPI-LS/PD. As stated in [28], that the joint-training method seeks to optimize on several tasks, while Reptile considers second-and-higher order of derivatives when multiple gradient updates are conducted. This can explain the superior outcomes of our method.\nOur Finetune R-GPI-LS is the best performing algorithm when compared with the second best method GPI-LS(year) where it uses 96.71% less data and 61.1% less training steps to achieve 5.9% improvement on EU, 62.44% improvement on Sp, 3.28% on bill and 2.74% improvement on comfort but only has 3.1% drop on HV. We conduct a detailed comparison in Table 3 in Appendix A."}, {"title": "Conclusion", "content": "In this paper, we highlighted the suitability of meta-learning for energy management and demonstrate the superior performance by extending state-of-the-art MORL algorithms with Reptile. We applied and evaluated the GPI-LS/PD in residential appliance scheduling.\nThere are several interesting questions identified in our research that merit further investigation:\n1. We notice that finetuning technique cannot seamlessly fit model-based MORL. This was once mentioned in the work of Mendonca et al. [25] however, it is not yet fully explored in MORL settings. It remains a open-question that how to finetune the environment model to positively help with the training of MORL policy when the task contexts are not evenly distributed.\n2. The environment used is only 2-objective, evaluations in the environment with more objectives (e.g. peak shaving) are still an open question. We also plan to do further investigation on our algorithm about the performance in other environments.\n3. Our work only talks about single-agent cases, however, the real-life scenario usually involves multiple appliances for scheduling. This renders the problem as a multi-agent problem which needs further exploration."}, {"title": "Full Solution Set", "content": "We show the full solution set rather than the PF of the methods. The full solution set of the main evaluation is shown in Fig. 6 while Fig. 7 shows the full solution set from the ablation study.\nIt demonstrates that our methods are better at maximizing user comfort while they are slightly underperformed in bill cutting. Another observation is that for GPI-LS/PD(month) and Finetune GPI-LS/PD, there is a bump in the middle of the solution set, as shown in Fig. 6, this means that these methods misaligned the solution with preference. In Fig. 7 and the result of our methods, the solution set is smoother and no obvious bump is seen, this means the policy can figure out the preference and make consistent decisions which is further reflected in the Sp evaluation, and also support the statement that HV is problematic from [13]. The points in Fig. 7 are more condensed than the ones in Fig. 6. This demonstrates that either with a larger data volume and training budget or by using context detection can the policy be effectively improved."}]}