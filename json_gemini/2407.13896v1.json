{"title": "Data-Algorithm-Architecture Co-Optimization for Fair Neural Networks on Skin Lesion Dataset", "authors": ["Yi Sheng", "Junhuan Yang", "Jinyang Li", "James Alaina", "Xiaowei Xu", "Yiyu Shi", "Jingtong Hu", "Weiwen Jiang", "Lei Yang"], "abstract": "As Artificial Intelligence (AI) increasingly integrates into our daily lives, fairness has emerged as a critical concern, particularly in medical AI, where datasets often reflect inherent biases due to social factors like the underrepresentation of marginalized communities and socioeconomic barriers to data collection. Traditional approaches to mitigating these biases have focused on data augmentation and the development of fairness-aware training algorithms. However, this paper argues that the architecture of neural networks, a core component of Machine Learning (ML), plays a crucial role in ensuring fairness. We demonstrate that addressing fairness effectively requires a holistic approach that simultaneously considers data, algorithms, and architecture. Utilizing Automated ML (AutoML) technology, specifically Neural Architecture Search (NAS), we introduce a novel framework, BiaslessNAS, designed to achieve fair outcomes in analyzing skin lesion datasets. BiaslessNAS incorporates fairness considerations at every stage of the NAS process, leading to the identification of neural networks that are not only more accurate but also significantly fairer. Our experiments show that BiaslessNAS achieves a 2.55% increase in accuracy and a 65.50% improvement in fairness compared to traditional NAS methods, underscoring the importance of integrating fairness into neural network architecture for better outcomes in medical AI applications.", "sections": [{"title": "1 Introduction", "content": "The democratization of AI is rapidly expanding the use of machine learning, notably neural networks, across various medical disciplines [34,36], with dermatology leading due to the availability of comprehensive skin lesion datasets [9]. However, unlike general-purpose image datasets like ImageNet [18], skin lesion datasets often exhibit biases, particularly regarding skin tone. This imbalance poses a significant challenge for machine learning in dermatology, as it can result"}, {"title": "2 Related Work", "content": "With the biased data in hand, traditional approaches can be divided into two directions: (1) data bias removal, and (2) fair training. Data bias removal: one way to remove the bias is by building a balanced dataset, however, it is a time-consuming process. An alternative way is to employ data augmentation. For example, [6] generates biased sets to increase the minority data artificially. In addition to data balance [11,29], techniques were proposed to modify the training algorithms in addressing the fairness issue. Authors in [28,19] applied adversarial training and add a discrimination module to improve fairness.\nOur work stands at a different point to consider the neural architecture in addressing the fairness issue. We propose a framework to jointly optimize neural architectures, training algorithms, and data augmentation. The above-mentioned debiasing methods can be integrated into our framework."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Fairness Metric Definition and Factor Investigation", "content": "Given a neural architecture N and datasets (T, D) where T is the training set and D is the validation set, N is trained on T to generate the model $f_N$, which is then validated on D to obtain accuracy $A(f_N, D)$. Fairness exists because data in D have additional attributes (e.g., skin tones), which will divide D into groups, denoted {$D_{g1}, D_{g2}, \\cdots, D_{gK} $}. For example, if a dataset contains two skin tones (i.e., $g_1$ = light_skin and $g_2$ = dark_skin), the accuracy of model $f_N$ on group $g_i$ is denoted as $A(f_N, D_{g_i})$.\nWe define the \"unfairness score\" based on the overall accuracy and the group accuracy, denoted as $U(f_N, D)$. Specifically, in this project, we calculate the unfairness score [19] $U(f_N, D)$ as the L1-norm:\n$U(f_N, D) = \\sum_{\\forall g_i \\in G} {\\vert A(f_N, D_{g_i}) - A(f_N, D) \\vert }$. \t(1)\nResults in Fig 1 (ii) illustrate that different architectures (N) have different unfairness scores. We further investigate the influence of the training approach and data preprocessing. In Fig. 1 (ii), we modify the loss function in training to consider fairness in the training process, denoted as \"Training Imp.\", and we conduct data balancing to increase the samples in minority groups aiming at improving fairness, denoted as \"Data Imp.\". It is clear that both approaches can reduce the unfairness score. More interestingly, the three factors N, $f'$, and D are coupled with each other, which indicates that optimizing them simultaneously is best to minimize the unfairness score."}, {"title": "3.2 Biasless NAS Framework", "content": "Overview of BiaslessNAS framework: Fig. 2 shows the overview of BiaslessNAS, which is composed of 4 components: \u2460 reinforcement learning (RL)"}, {"title": "\u2460RL Optimizer", "content": "The controller iteratively predicts the hyperparameters of the batch generation method BGM and the child network N. In each iteration, the controller receives a reward to update the RNN network. The reward R is generated based on the outputs of the evaluator (see \u2463), including accuracy $A(f_N, D)$, and unfairness score $U(f_N, D)$. R is computed as follows.\n$R = \\begin{cases}\n\\alpha \\cdot A(f_N, D) \u2013 \\beta \\cdot U(f_N, D) & A(f_N, D) \\geq AC \\\\\n-1 & otherwise\n\\end{cases}$\n \t(2)\nwhere \u03b1, \u03b2 are two scaling factors that could be adjusted according to the specific demands on accuracy or fairness, and AC is the requirement of the model accuracy on the full dataset D.\nBased on the reward, we employ reinforcement learning to update the controller. Specifically, we apply the Monte Carlo policy gradient algorithm [35]:\n$VJ( \\theta ) = \\frac{1}{m} \\sum_{k=1}^{m}  \\sum_{t=1}^{T} \\gamma^{T-t} log \\pi_{\\theta} (a_t|s_{t-1}) (R_k \u2013 b)$\n \t(3)\nwhere m and T are the batch size and step in each episode. Rewards are discounted by an exponential factor \u03b3, and b is the average exponential moving."}, {"title": "\u2461Data/Architecture Fusing Search Space", "content": "The search space is composed of two sets of hyperparameters: (1) hyperparameters for BGM, and (2) hyperparameters for child network architecture N.\nBatch Generation. The idea of creating BGM is to adjust the composition of data from different groups in one training data batch. We define $o_i$ to be a ratio, indicating the percentage of images in one batch comes from sub-dataset $D_{g_i}$. Let BS be the batch size, then, we have $o_i \\times BS$ to be the number of images from sub-dataset $D_{g_i}$, and we have the constraint that $\\sum_{\\forall g_i \\in G} {o_i} = 1$.\nTo avoid accuracy degradation caused by oversampling of minority groups, we additionally have the following constraint: $ \\forall g_i \\in G, g_j \\in G, if  |D_{g_i}| \\leq |D_{g_j}|$, then $o_i \\leq o_j$, where $|D_{g_k}|$ indicates the size of sub-dataset $D_{g_k}$.\nNeural Architecture. We apply a linear array of a block as the backbone architecture. The design of basic blocks is inspired by the existing popular convolutional neural networks, including VGG-Net [31], MobileNet [13], and ResNet [12]. In this work, as shown in Fig. 22, we involve four types of basic blocks, including MobileNetV2-inspired ones (i.e., MB and DB), ResNet-inspired block (RB), and VGG-inspired block (CB). The basic blocks have four hyperparameters, including channel numbers (CH1, CH2, and CH3) and kernel sizes (K)."}, {"title": "\u2462Fairness-aware Trainer", "content": "Given an identified architecture (i.e., child network N) and BGM, the fairness-aware trainer trains the child network to generate a trained model $f_N$. Specifically, we first create batches of data using BGM on the validation dataset. Then, the model is trained using a fairness-aware loss function. Finally, after the iterative training process, we can obtain $f_N$.\nParticularly, the fairness-aware loss function is formulated by leveraging the hyperparameters in BGM. Denote $Bg_i$ as the sub-batch of samples from sub-dataset $D_{g_i}$, and we have $|Bg_i| = o_i \\times |D_{g_i}|$, where $|*|$ is the size of a dataset/batch, and $o_i$ is the ratio in BGM. For each sample $s \\in Bg_i$, it has a target label $T_s$, and a prediction results $P_s$. After the forward propagation, we apply Cross Entropy to compute the loss, as follows,\n$L = - \\sum_{\\forall g_i \\in G} \\sum_{s \\in Bg_i} { \\frac{arg \\ max_{g_i \\in G} o_i}{o_i} T_s log P_s },$\t(4)\nwhere $arg \\ max_{g_i \\in G} o_i$ identifies the ratio of the largest group to compose a batch, which is used to form a scaling factor. The final generated fair loss is also used to complete the backward propagation."}, {"title": "\u2463Fairness and Accuracy Evaluator", "content": "With the trained model $f_N$, the accuracy can be obtained. Meanwhile, the unfairness score can be calculated based"}, {"title": "4 Experiment", "content": "Dataset and settings We use the Fair and Intelligent Embedded System Chal-lenge (ESFair) dataset [3], which is composed of data from ISIC2019, Dermnet[2],and Atlas[1]. Thera are 5 dermatology diseases for classification. We compare solutions obtained by BiaslessNAS with a set of existing neural architectures, including MobileNetV2 [27], ResNet [33], and MnasNet [?]. All models are trained from scratch with the same hyperparameters on a GPU cluster with 48 RTX3080. The learning rate starts from 0.01 with a decay of 0.9 in 20 steps; while the batch size is 32 with 500 epochs.\nEvaluation of BiaslessNAS. Table 1 reports the evaluation results. These two architectures were obtained from BiaslessNAS with the lowest unfairness score and the highest accuracy, respectively. Two hyperparameters are used in the framework: (1) Alpha is the scalable parameter for accuracy, and (2) Beta isfor fairness. We explore two settings: BiaslessNAS-Fair has a larger Beta (0.8) and a smaller Alpha (0.2), while BiaslessNAS-Acc has a larger Alpha (0.8) anda smaller Beta (0.2). For a fair comparison of different neural architectures (N),all competitors are trained using the proposed fairness-aware data processing (D) and trainer (f'). As shown in Table 1, it is clear that BiaslessNAS-Faircan achieve competitive accuracy with the lowest unfairness score over others. More specifically, the unfairness score of BiaslessNAS-Fair is only 0.0779 on average, which achieves an improvement of 65.59% compared with MobileNet V2regarding fairness. On the other hand, BiaslessNAS-Acc achieves the highest accuracy with the lowest unfairness score against other existing models.\nNeural Architecture Visualization. Fig. 3(a)-(b) showcase the neural archi-tectures derived from BiaslessNAS, highlighting the structural nuances between"}, {"title": "Biasless NAS is Fairer on Different Metrics", "content": "In addition to the unfairness score defined in Equation 1, we further evaluate BiaslessNAS on other two commonly used fairness metrics: Disparate impact (DI) [10] and StatisticalParity Difference (SPD) [19]. Fig. 3(c)-(d) present a comparison. In Fig. 3(c),BiaslessNAS-Fair stands out by achieving the highest DI value, indicating its superiority in fairness over other examined architectures. Fig. 3(d) reveals thatmodels with SPD scores closer to zero are preferable, with BiaslessNAS-Fair and BiaslessNAS-Acc emerging as the top performers in this regard. These findings collectively demonstrate that BiaslessNAS effectively identifies solutions thatsurpass conventional neural architectures in fairness across different metrics."}, {"title": "Evaluation of fairness-aware trainer", "content": "This ablation study is conducted by fixing the same N&D and comparing results for different f'. Fig. 4 shows the evaluation results of the fairness-aware trainer on 4 existing neural architectures.\nThe baseline for each architecture has the setting of $\\frac{o_d}{O_{gd}}$, which means that the batch generator will use the same ratio between the number of dark-"}, {"title": "Evaluation of different optimization combinations", "content": "This ablation study evaluates various optimization combinations to assess the benefits of co-optimize.The results, summarized in Table 2, contrast different strategies against a base-line MobileNetV2 architecture. Initially, we examine MobileNetV2 in its stan-dard form, followed by versions enhanced with a fairness-aware trainer (denoted as f') and then with both a co-optimized trainer and data augmentation (D+f'). The outcomes illustrate that co-optimization significantly enhances the fairness of MobileNetV2, as indicated by improvements in unfairness scores and disparateimpact metrics. In a further analysis, a fairness-aware Neural Architecture Search(NAS), termed \"FairNAS,\" is introduced. FairNAS seeks to identify fair neuralarchitectures without incorporating a fairness-aware trainer or data augmen-tation. Interestingly, FairNAS surpasses the fairness metrics of MobileNet V2paired with f' alone but falls short of the combination of MobileNetV2 with D + f' in fairness metrics, albeit with a slight advantage in accuracy. Introduc-ing BiaslessNAS-Acc, which integrates data-algorithm-architecture (D+ f' + N) reveals that this approach outperforms FairNAS by achieving higher accuracy and further enhancing fairness. This comprehensive co-optimization of data, al-gorithm, and architecture emerges as the most effective strategy, showcasing the superior efficacy of simultaneous optimization across these dimensions for advancing both accuracy and fairness in machine learning models.\nThe above results give us the following three insights. (1) Neural architecture indeed affects fairness. It can even make a larger impact on fairness than the"}, {"title": "5 Conclusion", "content": "In this paper, we delve into the factors influencing fairness in ML systems, unveiling that optimizing models, algorithms, and data collectively can better balance accuracy and fairness. We introduce a novel framework, BiaslessNAS, designed for this holistic optimization approach, specifically targeting the inherent biases in skin lesion datasets. To ensure accuracy and fairness, BiaslessNAS incorporates a fairness-aware training mechanism that creates balanced data batches and refines weighted loss to enhance the fairness of minority groups. Additionally, a reinforcement learning optimizer steers the co-optimization process, proving that this integrated approach markedly surpasses traditional methods that optimize data, algorithms, and architecture separately. Our evaluations confirm that co-optimization significantly enhances fairness without compromising accuracy."}]}