{"title": "Data-Algorithm-Architecture Co-Optimization\nfor Fair Neural Networks on Skin Lesion Dataset", "authors": ["Yi Sheng", "Junhuan Yang", "Jinyang Li", "James Alaina", "Xiaowei Xu", "Yiyu Shi", "Jingtong Hu", "Weiwen Jiang", "Lei Yang"], "abstract": "As Artificial Intelligence (AI) increasingly integrates into\nour daily lives, fairness has emerged as a critical concern, particularly\nin medical AI, where datasets often reflect inherent biases due to so-\ncial factors like the underrepresentation of marginalized communities\nand socioeconomic barriers to data collection. Traditional approaches\nto mitigating these biases have focused on data augmentation and the\ndevelopment of fairness-aware training algorithms. However, this paper\nargues that the architecture of neural networks, a core component of\nMachine Learning (ML), plays a crucial role in ensuring fairness. We\ndemonstrate that addressing fairness effectively requires a holistic ap-\nproach that simultaneously considers data, algorithms, and architecture.\nUtilizing Automated ML (AutoML) technology, specifically Neural Ar-\nchitecture Search (NAS), we introduce a novel framework, BiaslessNAS,\ndesigned to achieve fair outcomes in analyzing skin lesion datasets. Bi-\naslessNAS incorporates fairness considerations at every stage of the NAS\nprocess, leading to the identification of neural networks that are not only\nmore accurate but also significantly fairer. Our experiments show that Bi-\naslessNAS achieves a 2.55% increase in accuracy and a 65.50% improve-\nment in fairness compared to traditional NAS methods, underscoring\nthe importance of integrating fairness into neural network architecture\nfor better outcomes in medical AI applications.", "sections": [{"title": "1 Introduction", "content": "The democratization of AI is rapidly expanding the use of machine learning,\nnotably neural networks, across various medical disciplines [34,36], with derma-\ntology leading due to the availability of comprehensive skin lesion datasets [9].\nHowever, unlike general-purpose image datasets like ImageNet [18], skin lesion\ndatasets often exhibit biases, particularly regarding skin tone. This imbalance\nposes a significant challenge for machine learning in dermatology, as it can result"}, {"title": "2 Related Work", "content": "With the biased data in hand, traditional approaches can be divided into two\ndirections: (1) data bias removal, and (2) fair training. Data bias removal: one\nway to remove the bias is by building a balanced dataset, however, it is a time-\nconsuming process. An alternative way is to employ data augmentation. For\nexample, [6] generates biased sets to increase the minority data artificially. In\naddition to data balance [11,29], techniques were proposed to modify the training\nalgorithms in addressing the fairness issue. Authors in [28,19] applied adversarial\ntraining and add a discrimination module to improve fairness.\nOur work stands at a different point to consider the neural architecture in\naddressing the fairness issue. We propose a framework to jointly optimize neural\narchitectures, training algorithms, and data augmentation. The above-mentioned\ndebiasing methods can be integrated into our framework."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Fairness Metric Definition and Factor Investigation", "content": "Given a neural architecture N and datasets (T, D) where T is the training set\nand D is the validation set, N is trained on T to generate the model fy, which is\nthen validated on D to obtain accuracy A(f'v, D). Fairness exists because data in\nD have additional attributes (e.g., skin tones), which will divide D into groups,\ndenoted {Dg1, D92,\uff65\uff65\uff65, D9K }. For example, if a dataset contains two skin tones\n(i.e., g1 = light_skin and g2 = dark_skin), the accuracy of model f'y on group\ngi is denoted as A(f'n, Dgi).\nWe define the \"unfairness score\" based on the overall accuracy and the group\naccuracy, denoted as U(f', D). Specifically, in this project, we calculate the\nunfairness score [19] U(f', D) as the L1-norm:\n\\(U(f'_{N}, D) = \\sum_{\\forall g_{i} \\in G} {\\mid A(f'_{N}, D_{g_{i}}) - A(f'_{N}, D) \\mid }.\\)\nResults in Fig 1 (ii) illustrate that different architectures (N) have different\nunfairness scores. We further investigate the influence of the training approach\nand data preprocessing. In Fig. 1 (ii), we modify the loss function in training\nto consider fairness in the training process, denoted as \"Training Imp.\", and\nwe conduct data balancing to increase the samples in minority groups aiming at\nimproving fairness, denoted as \"Data Imp.\". It is clear that both approaches can\nreduce the unfairness score. More interestingly, the three factors N, f', and D are\ncoupled with each other, which indicates that optimizing them simultaneously\nis best to minimize the unfairness score."}, {"title": "3.2 Biasless NAS Framework", "content": "Overview of BiaslessNAS framework: Fig. 2 shows the overview of Bi-\naslessNAS, which is composed of 4 components: \u2460 reinforcement learning (RL)"}, {"title": "4 Experiment", "content": "Dataset and settings We use the Fair and Intelligent Embedded System Chal-\nlenge (ESFair) dataset [3], which is composed of data from ISIC2019, Dermnet[2],\nand Atlas[1]. Thera are 5 dermatology diseases for classification. We compare\nsolutions obtained by BiaslessNAS with a set of existing neural architectures, in-\ncluding MobileNetV2 [27], ResNet [33], and MnasNet [?]. All models are trained\nfrom scratch with the same hyperparameters on a GPU cluster with 48 RTX\n3080. The learning rate starts from 0.01 with a decay of 0.9 in 20 steps; while\nthe batch size is 32 with 500 epochs.\nEvaluation of BiaslessNAS. Table 1 reports the evaluation results. These\ntwo architectures were obtained from BiaslessNAS with the lowest unfairness\nscore and the highest accuracy, respectively. Two hyperparameters are used in\nthe framework: (1) Alpha is the scalable parameter for accuracy, and (2) Beta is\nfor fairness. We explore two settings: BiaslessNAS-Fair has a larger Beta (0.8)\nand a smaller Alpha (0.2), while BiaslessNAS-Acc has a larger Alpha (0.8) and\na smaller Beta (0.2). For a fair comparison of different neural architectures (N),\nall competitors are trained using the proposed fairness-aware data processing\n(D) and trainer (f'). As shown in Table 1, it is clear that BiaslessNAS-Fair\ncan achieve competitive accuracy with the lowest unfairness score over others.\nMore specifically, the unfairness score of BiaslessNAS-Fair is only 0.0779 on\naverage, which achieves an improvement of 65.59% compared with MobileNet V2\nregarding fairness. On the other hand, BiaslessNAS-Acc achieves the highest\naccuracy with the lowest unfairness score against other existing models.\nNeural Architecture Visualization. Fig. 3(a)-(b) showcase the neural archi-\ntectures derived from BiaslessNAS, highlighting the structural nuances between"}, {"title": "Evaluation of fairness-aware trainer", "content": "This ablation study\nis conducted by\nfixing the same N&D and comparing results for different f'. Fig. 4 shows the\nevaluation results of the fairness-aware trainer on 4 existing neural architectures.\nThe baseline for each architecture has the setting of \\(\\frac{O_{d}}{D_{gd}}\\), which means\nthat the batch generator will use the same ratio between the number of dark-"}, {"title": "Evaluation of different optimization combinations", "content": "This ablation study\nevaluates various optimization combinations to assess the benefits of co-optimize.\nThe results, summarized in Table 2, contrast different strategies against a base-\nline MobileNetV2 architecture. Initially, we examine MobileNetV2 in its stan-\ndard form, followed by versions enhanced with a fairness-aware trainer (denoted\nas f') and then with both a co-optimized trainer and data augmentation (D+f').\nThe outcomes illustrate that co-optimization significantly enhances the fairness\nof MobileNet V2, as indicated by improvements in unfairness scores and disparate\nimpact metrics. In a further analysis, a fairness-aware Neural Architecture Search\n(NAS), termed \"FairNAS,\" is introduced. FairNAS seeks to identify fair neural\narchitectures without incorporating a fairness-aware trainer or data augmen-\ntation. Interestingly, FairNAS surpasses the fairness metrics of MobileNet V2\npaired with f' alone but falls short of the combination of MobileNetV2 with\nD + f' in fairness metrics, albeit with a slight advantage in accuracy. Introduc-\ning BiaslessNAS-Acc, which integrates data-algorithm-architecture (D+ f' + N)\nreveals that this approach outperforms FairNAS by achieving higher accuracy\nand further enhancing fairness. This comprehensive co-optimization of data, al-\ngorithm, and architecture emerges as the most effective strategy, showcasing\nthe superior efficacy of simultaneous optimization across these dimensions for\nadvancing both accuracy and fairness in machine learning models.\nThe above results give us the following three insights. (1) Neural architecture\nindeed affects fairness. It can even make a larger impact on fairness than the"}, {"title": "5 Conclusion", "content": "In this paper, we delve into the factors influencing fairness in ML systems, unveil-\ning that optimizing models, algorithms, and data collectively can better balance\naccuracy and fairness. We introduce a novel framework, BiaslessNAS, designed\nfor this holistic optimization approach, specifically targeting the inherent biases\nin skin lesion datasets. To ensure accuracy and fairness, BiaslessNAS incorpo-\nrates a fairness-aware training mechanism that creates balanced data batches and\nrefines weighted loss to enhance the fairness of minority groups. Additionally,\na reinforcement learning optimizer steers the co-optimization process, proving\nthat this integrated approach markedly surpasses traditional methods that opti-\nmize data, algorithms, and architecture separately. Our evaluations confirm that\nco-optimization significantly enhances fairness without compromising accuracy."}]}