{"title": "Auto-ADMET: An Effective and Interpretable AutoML Method for Chemical ADMET Property Prediction", "authors": ["ALEX G. C. DE S\u00c1", "DAVID B. ASCHER"], "abstract": "Machine learning (ML) has been playing important roles in drug discovery in the past years by providing (pre-)screening tools for prioritising chemical compounds to pass through wet lab experiments. One of the main ML tasks in drug discovery is to build quantitative structure-activity relationship (QSAR) models, associating the molecular structure of chemical compounds with an activity or property. These properties - including absorption, distribution, metabolism, excretion and toxicity (ADMET) are essential to model compound behaviour, activity and interactions in the organism. Although several methods exist, the majority of them do not provide an appropriate model's personalisation, yielding to bias and lack of generalisation to new data since the chemical space usually shifts from application to application. This fact leads to low predictive performance when completely new data is being tested by the model. The area of Automated Machine Learning (AutoML) emerged aiming to solve this issue, outputting tailored ML algorithms to the data at hand. Although an important task, AutoML has not been practically used to assist cheminformatics and computational chemistry researchers often, with just a few works related to the field. To address these challenges, this work introduces Auto-ADMET, an interpretable evolutionary-based AutoML method for chemical ADMET property prediction. Auto-ADMET employs a Grammar-based Genetic Programming (GGP) method with a Bayesian Network Model to achieve comparable or better predictive performance against three alternative methods - standard GGP method, pkCSM and XGBOOST model on 12 benchmark chemical ADMET property prediction datasets. The use of a Bayesian Network model on Auto-ADMET's evolutionary process assisted in both shaping the search procedure and interpreting the causes of its AutoML performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) and Machine Learning (ML) fields have been empowering drug discovery with predictive methods and tools not only to accelerate its internal pipelines - e.g., computationally (pre-)screening molecules with adequate properties, such as Absorption, Distribution, Metabolism, Excretion and Toxicity (ADMET) properties [8, 23, 27, 31, 34, 37, 38, 40, 41, 50] \u2013 but also to propose new chemical molecules (e.g., with generative AI, GenAI) \u2013 and, possibly aiming to design new drugs [1, 20]. Although predictive or generative models might be currently and extensively used to yield new molecules, finding those with correct properties is still challenging and time-consuming. Therefore, while developing customised chemical property prediction models will be a proper way to avoid later obstacles during in vitro and in vivo testing, it will also be important as these models may be well-integrated into molecular optimisation frameworks because of that [23, 27, 38, 40, 50].\nIn this work, we primarily focus on ADMET properties of chemical compounds because of their relationships with drug design and optimisation [8, 34, 37, 47]. ADMET properties, related to the pharmacokinetics and toxicity of compounds, provide an understanding of how the chemical compounds move into, through, and out of the body. In addition, these properties assess how the molecules affect the human body by assessing how toxic/safe they are to cells, organs and the genome, for example.\nApart from the extensive work on ADMET methods linking the molecule (sub)structure with ADMET properties, most works provide methods, web servers or tools with non-customised and static predictive models. Given the large number of pharmaceutical companies and research institutes working by building compound libraries, we surely have the molecular data drift happening, because of the change of the chemical space. Therefore, it is impractical to assume that these models will maintain their predictive performance in the long run, mainly because the new compounds may be dissimilar to those used to train ADMET property prediction models. With new data, the current solution is to propose new versions of the tools or web servers with updated ADMET models, which is not scalable enough to deal with the demand for novel ADMET property prediction models.\nAn efficient alternative to deal with this issue is using Automated Machine Learning (AutoML) methods [26] to recommend customised predictive pipelines to the molecular data at hand. Nevertheless, the majority of previously introduced AutoML methods do not take into account the nature of (bio)chemical data, hence not being able to capture all the steps necessary to succeed in chemical ADMET property prediction tasks, such as molecular representation, molecular data splitting, feature extraction, and data imbalance [5]. These steps have been considered in novel AutoML research in the field of AutoML for drug discovery and computational chemistry.\nAccordingly, this work proposes Auto-ADMET, an interpretable evolutionary-based AutoML method for chemical ADMET property prediction. Auto-ADMET relies on a Model- and Grammar-based Genetic Programming (GGP) method. While we used a GGP method to evolve valid pipelines in the context of chemical ADMET property prediction tasks, we used a Bayesian Network Classifier (BNC) model in Auto-ADMET to guide and interpret this evolutionary algorithm's decisions.\nTwelve (12) chemical ADMET property prediction benchmark datasets were used to validate Auto-ADMET and compare it with alternative methods (i.e., standard GGP method, pkCSM method and XGBOST model). The achieved results of Auto-ADMET on these 12 molecular datasets highlight its predictive power since they show Auto-ADMET's superior performance in eight (8) out of 12 datasets against alternative methods. Moreover, the use of a BNC causal model to assist the GGP algorithm was also relevant to comprehending the AutoML choices during the method's evolution, indicating which algorithms and hyperparameter choices are actually causing the AutoML performance."}, {"title": "2 RELATED WORK", "content": "We divide this section into two subsections. First, we describe evolutionary algorithms that are used to solve computa- tional chemistry problems, which is an area that encompasses chemical ADMET property prediction. Next, we analyse a few works on AutoML related to computational chemistry, focusing on recent works that build and recommend customised predictive pipelines based on (bio)chemical data."}, {"title": "2.1 Evolutionary Algorithms for Computation Chemistry", "content": "There have been several efforts to solve computational chemistry or cheminformatics problems using evolutionary computation (EC) problems. The survey of Yu et al. [49] mainly explores the use of EC for drug discovery, including the development of EC methods for docking, lead compound generation (such as ligands) and exploring the quantitative structure-activity relationships (QSAR) of compounds.\nNevertheless, this section focuses on using EC for molecular generation and optimisation, and machine learning tasks, which are more related to Auto-ADMET's goals. The works of Soto et al. [43] and [42] utilise single-objective and multiple-objective genetic algorithms for selecting the best set of descriptors (or features) in ADMET property prediction tasks, respectively. They tested different machine learning models for these studies, including decision trees, k nearest neighbours and polynomic non-linear function regression models to estimate the quality of a given feature set.\nLiu et al. [30], in turn, proposed and developed ECoFFeS, which is an evolutionary-based feature selection software designed for drug discovery. ECOFFeS encompasses both single-objective and multi-objective bioinspired or evolutionary algorithms. Whereas ECoFFeS' single-objective algorithms include ant colony optimization (ACO) [15], differential evolution (DE) [44], genetic algorithm (GA) [24] and particle swarm optimization (PSO) [28], its multi-objective counterpart supports only two well-known Multi-Objective Evolutionary Algorithms (MOEAs) \u2013 i.e., MOEA/D [51] and NSGA-II [11].\nEC may also be used to optimise chemical compounds, where evolutionary operators can be applied to molecules to derive new ones [18]. For example, in Fromer and Coley [18], it is argued that during the evolutionary process aiming to optimise new molecules, a mutation operator might be used to add or remove atoms, bonds or molecular fragments. On the other hand, the crossover operator may be used to exchange molecular fragments among molecules [18].\nIn terms of evolutionary computation, molecular optimisation and generation, and large language models, we have the work of da Silva et al. [6], which modelled de novo drug design as a many-objective optimization problem (MaOP) since in drug discovery we do have problems with several conflicting objectives (e.g., potency versus safety versus proper pharmacokinetic properties). da Silva et al.'s work involved in developing genAI approaches combined with multi- and many-objective evolutionary algorithms (MOEAs and MaOEAs) for drug development, specifically for combining a generative deep learning model's latent space with MOEA/MaOEA (NSGA-II/NSGA-III) for designing new and diverse molecules."}, {"title": "2.2 AutoML for Cheminformatics", "content": "Several ML methods have been proposed for dealing with cheminformatics tasks, including but not limited to pharmacoki- netics, human and environmental toxicity, pharmacodynamics and pharmacogenetics [3, 7, 8, 14, 19, 22, 34, 37, 46-48]."}, {"title": "3 AUTO-ADMET: AUTOML FOR ADMET PROPERTY PREDICTION", "content": "Figure 1 illustrates the general workflow followed by Auto-ADMET. It starts by receiving the chemical molecules library targeting ADMET properties as input. Since Auto-ADMET only deals with classification tasks (at the moment), each molecule in the input set contains its associated ADMET labels. Next, Auto-ADMET finds the most suitable combination of ML building blocks for a given ADMET property prediction task, including chemical feature extraction, data preprocessing, ML algorithm selection and hyperparameter optimisation. All these building blocks are automatically selected by Auto-ADMET, which outputs the best-performing and personalised predictive pipeline to the input chemical data. After being fit, this pipeline is able to perform predictions to new molecules\u00b9.\nNext, we present details on the main components of Auto-ADMET. First, we describe Auto-ADMET's search space, highlighting its main ML building blocks, hyperparameters and algorithm options. Second, we introduce the Bayesian network- and evolutionary-based search method, which employs the search space to find and optimise ML pipelines in the context of ADMET prediction tasks."}, {"title": "3.1 Search Space", "content": "A context-free grammar defines the search space employed by Auto-ADMET, comprehending five (5) chemical extraction methods (and all of their combinations), five (5) scaling techniques (and the possibility of not using any scaling on the dataset), six (6) feature selection approaches (and the possibility of not using any feature selection on the dataset) and 10 machine learning algorithms.\nThe excerpt of the grammar defining Auto-ADMET's search space can be found in Frame 1. The grammar is formally presented as a four-tuple <N, T, P, S>, where: N is a set of non-terminals; T is a set of terminals; P is a set of production rules; and S (a member of N) is the start symbol. The production rules in the grammar derive the language by combining the grammar symbols. In addition, the symbol \"|\" represents a choice, and the non-terminals surrounded by the symbols \"[\" and \"]\" are optional, i.e., they can appear or not in the production rules.\nThe start rule-<Start> in the grammar shown in Frame 1-defines the four main components of the ADMET prediction pipeline: (i) molecular representation (captured by the non-terminal <feature_definition>), (ii) feature scaling, (iii) feature selection, and (iv) machine learning modelling (represented by the non-terminal <ML_algorithms>).\nFor molecular representation \u2013 which relates to the chemical feature extraction step, 31 different combinations of chemical compound representation techniques are available. These fall into five main categories: molecular descrip- tors, advanced molecular descriptors, graph-based signatures, fragments, and toxicophores [8, 37]. This search space component essentially determines the features used to characterise compounds based on their biochemical structure. Feature scaling is handled using standard approaches from the scikit-learn library [35, 39], including Normalizer, Min-Max Scaler, Max Abs Scaler, Robust Scaler, and Standard Scaler. This step modifies the numerical representation of chemical compounds, ensuring that feature values are appropriately scaled. However, the grammar also allows the option of bypassing feature scaling, as specified in the <Start> rule.\nFeature selection is another key component involving the selection of relevant features using methods from scikit- learn [39]. The grammar includes Variance Threshold, Select Percentile, Recursive Feature Elimination (RFE) and selection methods based on False Discovery Rate (FDR), False Positive Rate (FPR), and Family-Wise Error Rate (FWE). It is worth noting that RFE requires a predictive model on top of the feature selection method. As with scaling, the"}, {"title": "3.2 Search Method", "content": "Inspired by the Bayesian Optimisation Algorithm (BOA) [36], Auto-ADMET makes use of both the Grammar-based Genetic Programming (GGP) method and a Bayesian Network Classifier (BNC) to help the GGP search to explore and exploit more promising areas of the search space.\nFigure 2 illustrates the workflow followed by Auto-ADMET. First, a chemical dataset containing compounds is set as input to Auto-ADMET. This dataset could correspond to any particular chemical compound property, including but not limited to absorption, distribution, metabolism, excretion and toxicity (ADMET) properties.\nNext, the GGP method gets its population of individuals (in our case, individuals are machine learning pipelines) initialised at random but following the grammar rules expressed in the previous section. All individuals are converted into scikit-learn pipelines to be evaluated in it. Provided the results of the evaluation (see Section 3.3 for more details), we train a Bayesian Network Classifier (BNC) [17] taking into account the main building blocks in feature extraction, feature scaling, feature selection and ML algorithm modelling. The target of the BNC is the actual performance of classification pipelines, where a threshold of 80% and 60% of the current best predictive performance is used to set good and bad pipelines, respectively.\nFrom the BNC, we take its Markov Blanket\u00b2 and use it to sample in areas of the search space that are actually causing the performance. To build the BNC, we used a Hill Climbing with a Bayesian Information Criterion as the scoring metric. We used the aGrUM/pyAgrum's implementation for this step [16] to train and build the Bayesian Network. Based on this, a proportion of the new population will have only algorithms sampled based on the BNC's Markov Blanket. The next step involves checking whether the stopping criterion has been met. If it has not, the pipelines undergo selection based on the fittest individuals based on tournament selection and the application of GGP operators, specifically Whigham's crossover and mutation, and crossover followed by mutation [32]. If both operators are used, mutation modifies the recombined pipelines resulting from the crossover operation. Importantly, both crossover and mutation adhere to the grammar constraints, ensuring that only valid individuals are generated. Additionally, elitism is employed, preserving the top n pipelines from the previous generation to maintain high-performing solutions.\nThis evolutionary process, illustrated in Figure 2, continues iteratively until the stopping criterion is satisfied. Once met, the best-performing pipeline from the final evaluated population is returned, along with the optimal hyperparameters identified through the GGP method with the BNC model."}, {"title": "3.3 Fitness Function", "content": "A similar evaluation process to de S\u00e1 and Ascher [9]'s work is used here. However, instead of employing traditional cross-validation, Auto-ADMET relies on nested 3-fold cross-validation with three (3) trials to ensure the correct fitness of the pipeline.\nTo assess the quality of each pipeline, the fitness function is defined as the average of Matthew's correlation coefficient (MCC) [4] over the employed cross-validation procedure. MCC is a widely used performance metric in classification tasks, particularly valuable in cases of data imbalance due to its robustness in evaluating model performance, which is the case for chemical datasets. The MCC formula is given in Equation 1:\n$$MCC = \\frac{((TP \\times TN) - (FP \\times FN))}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FP) \\times (TN + FN)}}$$\nIn a Bayesian network, the Markov Blanket of a node is the set of the nodes corresponding to the node's parents, its children and its co-parents"}, {"title": "4 EXPERIMENTS", "content": "This section outlines the key aspects of the AutoML experiments for ADMET chemical data, including a description of the datasets (Section 4.1), the configuration of the grammar-based genetic programming (GGP) method with the Bayesian Network Classifier (Section 4.2), and the benchmarking with alternative approaches (Section 4.3)."}, {"title": "4.1 Chemical ADMET Datasets", "content": "Twelve (12) chemical ADMET datasets were used to evaluate the performance of Auto-ADMET. These datasets represent binary classification tasks related to absorption, metabolism, and excretion based on experimental in vivo or in vitro tests on small chemical molecules. The number of molecules (# Molecules) in these datasets varies significantly, ranging from 404 to 18,558. This variation in dataset size presents a challenge for Auto-ADMET, requiring it to effectively adapt and optimise the search process for different pipeline configurations. Table 1 encompasses the employed datasets and their characteristics.\nIt is important to emphasise that although we selected these 12 datasets to validate Auto-ADMET, any other chemical predictive task would be suitable to be applied as an Auto-ADMET input. We plan to assess the performance of Auto-ADMET in a broad range of datasets in future work."}, {"title": "4.2 Parameter Configuration", "content": "The GGP (with BNC) parameters were configured in the following way. 100 individuals representing ML pipelines are evolved for one hour. Each individual has at most 5 minutes to run. Otherwise, its run is interrupted, and its score is multiplied by 0.7. Crossover mutation operators are employed with a probability rate of 0.0 and 0.15, respectively. Crossover followed by mutation is applied at a rate of 0.05. Over the generations, the best current individual is kept for the next generations (i.e., the elitism size is equal to 1). From the individuals sampled from the BNC's Markov Blanket, we select a maximum of 10% of the population size (i.e. 10) of new individuals (ML pipelines) to compose the new population. Finally, to avoid overfitting happening on the final model generated by the best pipeline, we add a random pipeline into the population if we have cases where 70% of the ML pipelines in the population are the same, indicating convergence. Table 2 highlights these Auto-ADMET parameters."}, {"title": "4.3 Benchmarking", "content": "We evaluated the best pipelines discovered by Auto-ADMET (Section 3) by comparing them against three alternative approaches. First, after running Auto-ADMET 20 times-to ensure reliable statistical analysis-and collecting the resulting pipelines, we assessed their performance using a 3-fold nested cross-validation procedure. The best AutoML-discovered pipeline was then compared to pkCSM [37], a widely recognised method for predicting the chemical ADMET properties. Second, we compared the best-selected pipelines and the best AutoML-optimised pipeline against XGBoost [2] with its default parameters. XGBoost was chosen due to its widespread use in machine learning and its frequent application in predictive modelling.\nMoreover, we assessed the best pipelines found by Auto-ADMET by comparing them against those discovered by the standard GGP method proposed by de S\u00e1 and Ascher [9]. Its results are also included in the following section.\nFinally, to statistically compare all four (4) methods, we applied Iman-Davenport's modification of Friedman's test [12]. If the test yielded significant results, we conducted a Nemenyi post hoc test to perform pairwise comparisons, assessing whether the predictive performance of the AutoML method and its best-found pipeline significantly differed from the alternative approaches."}, {"title": "5 RESULTS", "content": "This section presents the results considering the experiments detailed in the previous section. We first provide a comparison in terms of predictive performance in Section 5.1 among all the methods (among best pipelines found on cross-validation) - the work of de S\u00e1 and Ascher [9], pkCSM [37], XGBoost [2] and Auto-ADMET. Next, we analyse a case of study on the dataset Caco-2 to understand how the evolutionary process is taking advantage of the Bayesian Network Classifier to evolve machine learning pipelines better."}, {"title": "5.1 Benchmarking against Alternative Methods", "content": "When performing the analysis of the benchmarking of Auto-ADMET against alternative methods (the AutoML method proposed by de S\u00e1 and Ascher (2024), pkCSM and XGBOOST) in Table 3, we noticed that Auto-ADMET achieves the highest average MCC (0.618) across all datasets, outperforming de S\u00e1 and Ascher (0.530), pkCSM (0.456), and XGBoost (0.497). In terms of average ranking, Auto-ADMET is also ranked as the best method (1.667), followed by de S\u00e1 and Ascher (2024) (1.917), pkCSM (3.333) and XGBoost (3.083)."}, {"title": "5.2 Auto-ADMET's interpretation through Bayesian Network Analysis", "content": "Figure 3 depicts the Markov Blankets of the Bayesian Network Classifiers (BNCs) throughout the evolutionary process followed by Auto-ADMET. Apart from using these BNCs to sample from regions of the search space that are more prominent to cause the performance, they can also be used to understand the decisions the evolutionary algorithm (i.e., grammar-based genetic programming) is (partially) making over time.\nWe can observe from Figure 3 that the BNC also evolve from simpler to more complex, as data regarding machine learning pipeline evaluations targetting ADMET properties is increasing over time. For example, in the first generation, performing feature selection is not an aspect that causes performance, as opposed to the second generation. However, there is an alternation of pipeline components over time. The 30th generation shows the power of machine learning algorithms to cause performance, as most of the nodes of the Markov blanket of the BNC are actually algorithm components, even if using or not using feature selection (Recursive Feature Elimination and No Feature Selection are present in the 30th generation)."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "This paper introduces Auto-ADMET, a novel, robust and interpretable AutoML method for predicting chemical ab- sorption, distribution, metabolism, excretion and toxicity (ADMET) properties. While Auto-ADMET pays attention to recommending good predictive pipelines composed of molecular representation, scaling, feature selection and ML modelling, one of its main goals is also to have its decisions easily explained by an evolutionary algorithm guided by a Bayesian Network Classifier.\nPreliminary results on 12 ADMET datasets demonstrate Auto-ADMET's capabilities in selecting and configuring cheminformatics pipelines for ADMET predictive tasks, although not limited to them. The performed analysis on Auto-ADMET's results against alternative methods in terms of Matthew's correlation coefficient put Auto-ADMET as the best method in outputting predictive pipelines that will yield good models and results.\nNevertheless, although these results indicate a good step forward in proposing new specific AutoML methods for cheminformatics and ADMET problems, we still plan to improve the search and optimisation methods in Auto-ADMET to translate them to improved predictive performance. In fact, one of our further studies is to design the Bayesian Network Classifier differently. For instance, instead of interactively and locally building the Bayesian Networks across the evolutionary process, we can actually perform a complete study a priori and use it to build a better causation model to guide Auto-ADMET's evolution.\nMoreover, we expect to compare Auto-ADMET to similar methods in future work, such as ZairaChem [45], Uni- QSAR [21], QSARtuna [33], Deepmol [5] and Model Training Engine (MTE) [29]. With these comparisons, we would be able to understand where Auto-ADMET is at in terms of AutoML predictive performance. In this evaluation, we will standardise the comparison by using the chemical datasets found in the ADMET group of Therapeutics Data Commons [25].\nWe trust the evolutionary algorithm assisted with the Bayesian Network to model performance causation will benefit reiterations to model the AutoML search space, and therefore, lead to both improved AutoML search space and search method designs. Alternative ideas on how to model predictive performance and its causation effects are also targeted for future work."}]}