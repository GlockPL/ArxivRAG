{"title": "Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition", "authors": ["Lilang Lin", "Lehong Wu", "Jiahang Zhang", "Jiaying Liu"], "abstract": "Generative models, as a powerful technique for generation, also gradually become a critical tool for recognition tasks. However, in skeleton-based action recognition, the features obtained from existing pre-trained generative methods contain redundant information unrelated to recognition, which contradicts the nature of the skeleton's spatially sparse and temporally consistent properties, leading to undesirable performance. To address this challenge, we make efforts to bridge the gap in theory and methodology and propose a novel skeleton-based idempotent generative model (IGM) for unsupervised representation learning. More specifically, we first theoretically demonstrate the equivalence between generative models and maximum entropy coding, which demonstrates a potential route that makes the features of generative models more compact by introducing contrastive learning. To this end, we introduce the idempotency constraint to form a stronger consistency regularization in the feature space, to push the features only to maintain the critical information of motion semantics for the recognition task. Our extensive experiments on benchmark datasets, NTU RGB+D and PKUMMD, demonstrate the effectiveness of our proposed method. On the NTU 60 xsub dataset, we observe a performance improvement from 84.6% to 86.2%. Furthermore, in zero-shot adaptation scenarios, our model demonstrates significant efficacy by achieving promising results in cases that were previously unrecognizable. Our project is available at https://github.com/LanglandsLin/IGM.", "sections": [{"title": "1 Introduction", "content": "Skeletons represent human joints through 3D coordinate locations, providing a compact and efficient modality of representing human motion compared to RGB videos and depth data. Owing to their simplicity and superior discriminative capabilities for analysis, skeleton representations have been extensively employed in the field of action recognition tasks."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Skeleton-Based Action Recognition", "content": "Skeleton sequences encode the motion trajectories of human joints, representing rich information about human actions. Thus, skeleton data serves as a suitable modality for human action recognition. Skeleton can be obtained by applying pose estimation algorithms on RGB videos or depth maps. Early studies focused on extracting hand-designed spatial and temporal domain features from skeleton sequences for human movement recognition. In later work, efforts were made to model the positional information and higher-order temporal difference information of the human skeleton. Additionally, graphical models were built by tracking the trajectory of human joints to capture joint information in video sequences.\nRecently, there has been a surge of interest in using graph structures for learning models. Graph Neural Network (GNN) is one such model capturing intra-graph dependencies through information transfer between nodes. Various approaches have been proposed, such as spatio-domain inference networks, recurrent neural networks (RNNs), and graph convolutional networks (GCNs), to exploit graph structures for human action recognition. These models automatically learn spatio-temporal patterns from skeleton data, facilitating strong action generalization. Moreover, attention mechanisms, multiscale aggregation schemes, and lightweight convolution operations have been integrated into GCN-based models to enhance their effectiveness and reduce computational costs."}, {"title": "2.2 Self-Supervised Learning", "content": "The self-supervised task aims to extract data features from a large amount of unlabelled data. It can be widely used in semantic segmentation, image classification, action recognition and many other tasks. These tasks are mainly classified into methods based on reconstruction and on contrastive learning.\nReconstruction based approach after masking part of the original data, the network is used to reconstruct the masked part of the data. He et al. proposed Mased Auto-Encoder (MAE) to encode the visible patches and decode the visible and masked patches. This approach has been extended to the video domain and has been used in several studies. These methods typically use a visual Transformer as the backbone network in order to perform the mask reconstruction task. Feichtenhofer et al. extended the image-based masked auto-encoder to use spatio-temporal learning to randomly mask spatio-temporal segments of a video and learn am auto-encoder for reconstruction at the pixel-level reconstruction. Similarly, in MaskFeat, Wei et al. used several video cubes and utilized the model to predict them using the remaining information.\nContrastive learning pushes pairs of positive sample together while pushing pairs of negative sample further apart. To generate negative samples, contrastive learning pairs anchor frames with frames from other videos. There are various ways of generating positive and negative samples, which is the main factor that distinguishes different contrastive methods.\nMost of these methods generate positive and negative samples by different ways in order to minimize and maximise the distance between them respectively. In the image domain, positive samples are usually generated by enhancing the image in different ways. These enhancements include rotation, cropping, random greyscale and colour change. Scaling these methods in video can be difficult because each video comparison increases the memory required, especially if multiple enhancements are used for multiple positive samples. Another challenge is incorporating the temporal domain into the enhancement. Some methods simply apply the same enhancement in the image to each frame. Some methods include additional frame alignments that may be based on the temporal domain. Finally, some methods rely on motion and optical flow maps as positive samples."}, {"title": "3 Idempotency Generation Network (IGN)", "content": ""}, {"title": "3.1 Self-Conditional Generative Models as Maximum Entropy Coding", "content": "Self-conditional generative modeling is frequently employed as a pre-training task in self-supervised learning. It is generally structured as an auto-encoder. Formally, given the input skeleton data x, the reconstruction loss is:\n$\\mathcal{L} = \\mathbb{E}_{x \\sim p_x} [D(g(z), x)] = \\mathbb{E}_{x \\sim p_x} [\\mathbb{E}_{z \\sim p_{z|x}}[-\\log p(x|z)]] = H(x|z),$"}, {"title": "3.2 Idempotent Generative Models as Spectral Contrastive Learning", "content": "The idempotence of a self-conditional generative model refers to its stability in re-encoding. More precisely, if we denote the original data as x, the encoder as f(\u00b7), the encoded feature as z = f(x), the decoder as g(\u00b7), and the reconstruction as x = g(z), then the self-conditional generative model is considered idempotent:\n$f(x) = z \\text{ or } g(f(x)) = x.$\nIdempotence is frequently employed in the generative domain to augment the perceptual loss of generated images. The idempotent loss is formulated as:\n$\\mathcal{L}_{ide} = ||f(x) - z||^2 = 2 - 2f(x)^Tf(x),$\nwhere $z^Tz = 1$ because we normalize the feature space. Therefore, the idempotent generative model maximizes the entropy of the feature space while simultaneously minimizing the feature distance between the data and the generated data. The total loss of the idempotent generative model is expressed as:\n$\\mathcal{L} = \\mathcal{L}_{ide} - \\mathcal{L} = -2\\sum_{x,x} p(x,x) f(x_i)^T f(x_i) + \\sum_{x,x'} p(x) p(x') (f(x)^T f(x'))^2 + R$\n$= -2\\mathbb{E}_{(x,x) \\sim p(x,x)} [f(x)^T f(x)] + \\mathbb{E}_{(x,x') \\sim p(x)p(x')} [(f(x)^T f(x'))^2] + R$\n$= -2Tr (FAFT) + Tr ((FTF)^2) + R = 2Tr (FLFT) + Tr ((FTF)^2)$\n$= ||A \u2013 FFT|| + R + C,$\nwhere $A \\in \\mathbb{R}^{m \\times m}$ is the adjacency matrix defined by the data generation and C is a constant. $F = \\mathbb{Z}diag(\\sqrt{p(x)})$ The weights $A_{x,x} = \\frac{p(x,x)}{\\sqrt{p(x)p(x)}}$ $L = I - A$ is the Laplacian matrix. This demonstrates its equivalence to spectral contrastive learning. And the advantage of our approach over spectral contrastive learning is that we additionally optimise the residual term R to capture higher order information.\nFurther, we exploit data idempotence and feature idempotence to enhance representation learning and action generation. The unified generative-perceptual model contains both an encoder f(.) and generator g(.). And our idempotency constraints pay attention to both the data and the feature distributions, which improves both generation and feature learning.\n$(g \\circ f)(x) = x \\text{ and } (f \\circ g)(z) = z.$"}, {"title": "3.3 Relationship to Masked Auto-Encoder", "content": "As mentioned in Eq. 2, the generative network without idempotent constraints has a conditional entropy H(z|x) of 0 because the encoding process is deterministic. Idempotent generative networks, on the other hand, treat features z as a random variable sampled from the distribution of features across all of the generated data x for the same data x, thus transforming into a non-deterministic process:\n$z = f(x), \\tilde{x} \\sim G(x),$\nwhere G() is the generation process. Therefore, idempotent constraints are essentially about diminishing conditional entropy H(z|x), which in turn maximizes the mutual information between features and data.\nIn contrast, methods like MAE implicitly prioritize maximizing feature similarity across masked samples of the same data:\n$z = f(x), \\tilde{x} \\sim M(x),$\nwhere M(.) is the random masking process. Consequently, features from two distinct data that undergo similar transformed or generated data are clustered into the same class. However, the data obtained through data transformation may not be the real data and thus far from the real data distribution."}, {"title": "3.4 Relationship to Downstream Tasks", "content": "Through the analysis of previous work on spectral contrastive learning, the error rate $P_e = P[\\psi(x) \\neq y_x]$ of the downstream linear evaluation $\\psi(\\cdot)$ can be bounded by the generated adjacency matrix A and clustering error probabilities $a = P[y_x \\neq \\tilde{y}_x]$:\nTheorem 1. If $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_m$ are the eigenvalues of A, and if the clustering purity is 1-a, we obtain:\n$P_e \\le C_1 \\sum_{i=d+1}^m \\lambda_i + C_2a,$\nwhere $C_1, C_2$ are some constants.\nThis theorem illustrates the constraints on accuracy imposed by the purity 1-a. A large purity and a small number of clusters result in a low error rate. When the diversity in the generated data is insufficient, the sum of small singular values of the adjacency matrix become large, resulting in less tightly clustered groups. Conversely, excessive diversity in the generated data may compromise the preservation of motion information, thereby increasing the error rate in clustering and undermining overall clustering effectiveness.\nTherefore, to make the feature space of the idempotent generative model more capable of clustering, it is necessary to increase the diversity of the generated data for a stronger feature consistency constraint. However, a paradox is demonstrated here. Ordinary generative processes result in limited diversity under self-conditional generation due to constraints on the distance between the generated data and the original data. So in order to simultaneously obtain diverse and motion semantics preserving generated data, we propose an idempotent self-conditional generation model based on the diffusion generation model. The diversity of the generated data is provided by the noise sampling process of the diffusion model."}, {"title": "3.5 Idempotent Diffusion Generation Model", "content": "Our model consists of three parts, an encoder f(.), a generator g(.) and an adapter h(.). The encoder f(\u00b7) extracts features z as conditions for the generator g(\u00b7) and also as inputs to the downstream task classifier $\\phi(\\cdot)$. And the generator g() reconstructs the skeleton data based on the features. The adapter h(.), in turn, is responsible for projecting and fusing the features extracted by the encoder f(.) into the generator's feature space to be used as conditions.\nEncoder f(): We start by applying some data augmentations to the data x to obtain data $\\tilde{x}$ for increasing diversity. Then, spatio-temporal position embeddings Pt and Pv are added after projection to the feature space by linear projection:\n$z = LinearProj(\\tilde{x}) + P_t + P_v.$\nFollowing that, layers of vanilla transformer blocks are employed to extract latent representations z. Each block consists of a multi-head self-attention (MSA) module and a feed-forward network (FFN) module. Residual connections are utilized within each module, which are then followed by layer normalization (LN).\nGenerator g(.): The generator g(.) and encoder f(.) maintain the same structure. But the input is the noise data $x_t$ obtained by sampling in the diffusion. The generator g(\u00b7) predicts the noise magnitude by taking noise data $x_t$ and feature conditions z as inputs."}, {"title": "Adapter h()", "content": "The adapter h(\u00b7) merges the features extracted by the encoder f() into the generator g(\u00b7). This is necessary because high-level tasks like recognition operate in a different feature space compared to low-level tasks like generation. Recognition tasks necessitate capturing high-frequency action movements while generation primarily focuses on optimizing principal component space (with large singular values), such as walking or waving, which rely more on bottom component subspace like velocity. Thus, we introduce a feature fusion method that decouples principal and bottom component subspace, allowing the encoder features to focus more on high-frequency information, making them more suitable for high-level tasks such as action recognition. These features are then injected into the bottom component feature space of the generator.\n\u2022 Manifold Decoupled Feature Fusion Module: To derive discriminative features for use as semantic guides, we draw inspiration from negative samples in contrastive learning. We assume that regions with motion semantics have the lowest similarity to other regions in the same sequence. $z = [z_1,..., z_l] \\in \\mathbb{R}^{d \\times l}$, where l is the length of tokens of z. The uniformity loss in contrastive learning is:\n$\\mathcal{L}_{uni} = \\mathbb{E}_{z : } [\\log \\mathbb{E}_{z_j : } [\\exp (z_i^Tz_j)]] = Tr (\\log (deg (\\exp(z_i^Tz_j)))),$\nthe derivative of $\\mathcal{L}_{uni}$ is as follows:\n$\\frac{\\partial \\mathcal{L}_{uni}}{\\partial z} = z - \\eta D'^{-1} A'z,$\nwhere $A' = \\exp(z_i^Tz_j)$ and $D' = deg(A')$. $D'^{-1}A' = SoftMax(z_i^Tz_j)$. $\\tilde{z}$ removes low-frequency information. Based on this analysis, we extract the high-frequency information of the features as semantic information:\n$\\tilde{z} \\leftarrow (1 + \\eta)z \u2013 SoftMax(z_i^Tz_j)z.$\nThrough this high-pass filtering, we filter out some low-frequency information of principal component space such as the mean value in the sequences, which is not very meaningful for recognition, and retain the semantic information, which is more important for recognition. This module also mitigates dimensionality collapse, making features more informative.\nWe then fuse the features into the generator by replacing LayerNorm (LN) with Adaptive LayerNorm (AdaLN) with the following equation:\n$AdaLN(h, z, t) = \\tilde{z}_s \\cdot (t_s \\cdot LN(h) + t_b) + \\tilde{z}_b$\nwhere h represents the hidden representation of the generator, $(t_s, t_b)$ and $(\\tilde{z}_s, \\tilde{z}_b)$ are obtained from linear projection of timestep embedding t and high-frequency condition $\\tilde{z}$, respectively. Through AdaLN layers, the condition $\\tilde{z}$ guides the denoising process by scaling and shifting the normalized hidden representation.\nIdempotence Generation Loss: Our loss function comprises two components: the noise prediction loss of the diffusion model and the idempotency constraint."}, {"title": "\u2022 Noise Prediction Loss", "content": "The diffusion model is trained by predicting the noise from the input noise data:\n$\\mathcal{L}_{gen} = ||g(x_t, h(z), t) \u2013 \\epsilon||^2,$\n$x_t = \\sqrt{\\bar{\\alpha}_t}x + \\sqrt{1 \u2013 \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim N(0, I).$\n\u2022 Idempotence Constraint: To obtain consistency constraints on features, we adopt two types of idempotency losses, feature idempotency constraint and distribution idempotency constraint.\n1) Feature idempotency constraint performs on features. We use the predicted noise to perform a step of de-noising to get the estimated generated data $x_0$:\n$x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t \u2013 \\sqrt{1 \u2013 \\bar{\\alpha}_t}g(x_t, h(z), t)) .$\nTherefore, the feature idempotency constraint based on this generated data $x_0$ is formulated as:\n$\\mathcal{L}_{ide\\_feat} = f(\\tilde{x})^T f(\\tilde{x}_0, z_{t'}, t, t'),$\n$\\tilde{z}_{t'} = \\sqrt{\\bar{\\alpha}_t}z + \\sqrt{1 \u2013 \\bar{\\alpha}_{t'}}\\epsilon, \\epsilon \\sim N(0, I).$\nSince the generated data may be noisy, we input the features with noise and the number of time steps as auxiliary information.\n2) Distribution idempotency constraint aims to align the feature distributions of the generated and original data. It is essential to maintain the manifold structure of the generated data consistent with the manifold of the original data. We capture the feature manifold structure through inter-feature similarity:\n$P(\\tilde{x}_0) = f(\\tilde{x}_0)^T f(X_0) = [f(\\tilde{x}_0)^T f(x_1), ..., f(\\tilde{x}_0)^T f(x_n)],$\nwhere $x_i$ is i-th token data. We align it to the feature structure of the original ground truth data:\n$\\mathcal{L}_{ide\\_dist} = D(P(\\tilde{x}_0), P(x)),$\nwhere $D(\\cdot,\\cdot)$ is the distance metric between two distributions. The feature idempotency constraint captures richer structural information and allows for the construction of tighter clusters. This is because the adjacency matrix not only connects different generated data of the same data but also connects different data with similar features. Based on this idempotent alignment, we enhance the generative power of the model for stronger perceptual performance, while the encoder learns stronger feature consistency. This results in reduced singular values $\\sum_{i=d+1}^m \\lambda_i^2$ of the adjacency matrix and better downstream task performance."}, {"title": "4 Experiment Results", "content": "To evaluate the effectiveness of our approach, we conducted experiments on two benchmark datasets: the NTU RGB+D dataset and the PKUMMD dataset."}, {"title": "4.2 Evaluation and Comparison", "content": "For a comprehensive assessment, we conduct comparative analysis of our approach with other methodologies across diverse scenarios.\nLinear Evaluation. In the linear evaluation framework, we utilize an encoder f(.) to process the extracted features and a linear classifier $\\phi(.)$ for action classification. The evaluation metric employed is the accuracy of action recognition. Notably, the encoder f(\u00b7) remains unchanged throughout the linear evaluation protocol. Our model demonstrates superior performance on the datasets outlined in Table 1 compared to other methodologies.\nKNN Evaluation. In the K-Nearest Neighbors (KNN) evaluation setup, where the fixed encoder $f_q(\\cdot)$ extracts features without any trainable parameters, our model showcases superiority in action recognition accuracy on the presented datasets. Table 2 highlights the effectiveness of our approach compared to other methods in this evaluation mechanism.\nTransfer Learning. In the transfer learning scenario, we assess the generalization capability of our model by pretraining it on the source data using a self-supervised task. We then evaluate the model's performance on the target dataset using the linear evaluation mechanism, with the encoder f(\u00b7) maintaining fixed parameters without additional fine-tuning. Our approach demonstrates superior performance in the transfer learning setting, as illustrated in Table 3.\nZero-Shot Domain Generalization. By applying 4 types of corruption to the validation sets of all datasets, we assess the generalization of our proposed method compared to baseline approaches. For joint noise, we add noise with a probability of p with a variance of $o^2$ to some joints. We leverage the generative capability of our model, enabling us to denoise noisy skeleton data at test time. Subsequently, we utilize the generated skeleton data for recognition, significantly enhancing the generalization ability of our model. In Table 4, our proposed approach shows consistent and substantial performance improvements.\nReconstruction Evaluation. In this section, we implement IGM for mask prediction tasks. We input the masked data into the encoder to extract features as conditions for generation, noting that the reconstruction task does not require adding data transformations to the conditions. Our method is compared with diffusion-based methods DDPM and MDM in Table 5. Figs 3 and 4 show visualizations and feature visualizations of both the generated data and the ground truth data. Despite sharing the same feature distribution, the generated samples exhibit some diversity due to the noise introduced in the conditions."}, {"title": "4.3 Ablation Study", "content": "Here's the modified text for the ablation experiments:\nAnalysis of Module Combination. We investigate the performance of various combinations of modules and observe that each module contributes to a certain degree of improvement. Optimal performance is attained when all three modules are combined. As depicted in Table 6, each module enhances performance.\nAnalysis of Mitigating Dimensional Collapse. The analysis points out that the feature space of the generated model is susceptible to dimensionality collapse, resulting in the extracted features losing the information needed for recognition. Fig. 2 shows the feature space of the encoder trained using the generative model and the feature space after Adapter. The token after removing similarity by Adapter network has higher feature values, i.e., the dimension collapse phenomenon is mitigated."}, {"title": "5 Conclusions", "content": "In this research, we propose the skeleton-based idempotent generative model (IGM) for unsupervised representation learning, presenting a novel framework that maximizes the potential of generative models for representation learning. By implementing idempotence at both the feature level and distribution, our model enriches features with semantic information about motion, making them more suitable for recognition tasks. Additionally, as the generative model primarily focuses on the principal component space, it is more susceptible to dimensional collapse. Conversely, recognition tasks rely more on the bottom subspace. To address this imbalance, we design an adapter that fuses encoder and generator features from different subspaces, thereby enhancing the effective feature dimension of the feature space."}]}