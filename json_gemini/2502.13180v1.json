{"title": "Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization", "authors": ["Hongxu Wang", "Zhu Sun", "Yingpeng Du", "Lu Zhang", "Tiantian He", "Yew-Soon Ong"], "abstract": "Recommender systems (RSs) play a crucial role in shaping our digital interactions, influencing how we access and engage with information across various domains. Traditional research has predominantly centered on maximizing recommendation accuracy, often leading to unintended side effects such as echo chambers and constrained user experiences. Drawing inspiration from autonomous driving, we introduce a novel framework that categorizes RS autonomy into five distinct levels, ranging from basic rule-based accuracy-driven systems to behavior-aware, uncertain multi-objective RSs-where users may have varying needs, such as accuracy, diversity, and fairness. In response, we propose an approach that dynamically identifies and optimizes multiple objectives based on individual user preferences, fostering more ethical and intelligent user-centric recommendations. To navigate the uncertainty inherent in multi-objective RSs, we develop a Bayesian optimization (BO) framework that captures personalized trade-offs between different objectives while accounting for their uncertain interdependencies. Furthermore, we introduce an orthogonal meta-learning paradigm to enhance BO efficiency and effectiveness by leveraging shared knowledge across similar tasks and mitigating conflicts among objectives through the discovery of orthogonal information. Finally, extensive empirical evaluations demonstrate the effectiveness of our method in optimizing uncertain multi-objectives for individual users, paving the way for more adaptive and user-focused RSs.", "sections": [{"title": "INTRODUCTION", "content": "In today's digital age, recommender systems (RSs) [60] have become the backbone of information dissemination, revolutionizing the way we access and engage with content. These intelligent systems work tirelessly behind the scenes, analyzing our behaviors and preferences based on historical data to curate personalized information feeds tailored to our tastes and needs. From e-commerce [23] and social media [39] to education [58] and healthcare [7], RSs, widely investigated in academia and applied in industry [40], have transformed how we discover and consume information, shaping our digital experiences and influencing our decision-making processes.\nEarly works on RSs mainly focus on improving recommendation accuracy [38]. However, the singular focus on accuracy has inadvertently created echo chambers [55], where narrowly tailored recommendations confine users to limited information spaces, stifling diversity of thought and experience. As such, more studies have considered comprehensive ethical aspects to enhance the beyond-accuracy performance of RSs [30], e.g., diversity [55], explanation [50] and fairness [49]. Despite the great success, these methods suffer from a major limitation, i.e., the objectives of optimizing accuracy and beyond-accuracy performance are typically combined with pre-defined hyperparameters, indicating all users in RSs share the same objectives. Thus, it fails to reflect real-world complexities, where users may have diverse or uncertain requirements for RSs. For instance, some users may prioritize content diversity, while others might value fairness in their recommendations.\nTo elevate user experience and optimize Al's service to humanity, it's imperative to develop more intelligent RSs, which can autonomously adapt to individual user preferences and objectives, offering truly personalized interactions. Drawing parallels with autonomous driving [8], we first propose a novel framework that defines distinct levels of autonomy for RSs based on their ability to independently determine and pursue recommendation objectives. Overall, there are five different levels.\n\u2022 Level 0: Rule-based Accuracy-objective. RSs are entirely independent of individual user preference but built upon pre-defined or extracted rules according to the statistical interaction data, such as item popularity or association rules.\n\u2022 Level 1: Profile-based Accuracy-objective. RSs rely on static user or item profiles to generate recommendations (aka. content-based RSs [40]) by optimizing a single accuracy-oriented objective.\n\u2022 Level 2: Behavior-based Accuracy-objective. RSs use historical personalized user behaviors to make recommendations (aka. collaborative filtering [40]), optimizing the accuracy-oriented objective.\n\u2022 Level 3: Behavior-based Pre-defined Multi-objectives. RSs use historical personalized user behaviors to make recommendations that optimize pre-defined multiple (i.e., accuracy and beyond-accuracy) objectives, without considering personalized user needs.\n\u2022 Level 4: Behavior-based Uncertain Multi-objectives. RSs use historical personalized user behaviors to make recommendations that optimize uncertain multiple objectives, i.e., the importance of different objectives is automatically learned by considering personalized user needs, instead of pre-defined hyperparameters.\nIn this paper, our goal is to build a more intelligent RS at Level 4, automatically modeling the importance of different objectives by considering personalized user needs to improve the overall performance of multiple objectives. Intuitively, assigning personalized weights of objectives to users is a straightforward solution to improve the overall performance of multi-objectives. For example, we should lower the weight of the diversity objective in multi-objective learning if a user shows a narrow interest, because blindly increasing diversity may largely harm other objectives such as recommendation accuracy. However, there remain challenges in determining the appropriate weights in multi-objective recommendation quantitatively.\nFirst, assigning empirical weights (e.g., measured by users' historical behaviors) can not guarantee the desired multi-objective tradeoffs in RSs. For ease of illustration, let $l_{uo}(\\Theta)$ and $P_{uo}(\\Theta)$ denote the recommendation loss (e.g., BPR loss [41]) and the performance (e.g., NDCG [55]) of a specific objective o (e.g., accuracy) for the user u, respectively. Specifically, even if we have $\\min l_{uo}(\\Theta) \\leftrightarrow \\max P_{uo}(\\Theta)$ for each of the O different objectives, optimization their combination with empirical weights may not guarantee the optimal performance of multi-objectives, given by,\n$\\min \\sum_{o=1}^O l_{uo}(\\Theta) \\rightarrow \\max \\sum_{o=1}^O emp \\cdot P_{uo}(\\Theta)$,\n(1)\nwhere $emp$ is the empirical weight. It mainly lies in multi-objectives may conflict with each other and their optimization is essentially achieved by proxy losses, leading to the uncertain relationship between the assigned weights and the performance of multi-objectives. Secondly, learning trainable weights (e.g., learn weights through overall loss) may lead to the degradation of certain objectives, i.e.,\n$\\min \\sum_{o=1}^O \\lambda_{io}(\\Theta) \\cdot l_{io}(\\Theta) \\rightarrow \\max \\sum_{o=1}^O P_{uo}(\\Theta)$,\n(2)\nwhere $\\lambda_{io}(\\Theta)$ is the trainable weight. This may lead to trivial solutions for multi-objective learning, that is, a lower loss $l_{uo}(\\Theta)$ gets a larger weight $\\lambda_{io}(\\Theta)$. Thus, some objectives may dominate others, resulting in imbalanced optimization and sub-optimal performance.\nAccording to Equations (1) and (2), the main difficulty lies in the uncertain relationship between the weights and overall objective in multi-objective learning, remaining the black box to determine weights in an empirical or learnable way. To open this black box for autonomous multi-objective learning in RSs, we adapt the Bayesian optimization (BO) to accommodate the personalized needs of individual users, which can efficiently explore the search space in the black box and quantify uncertainties between the weights and overall objective. For each trail of BO, it is typically to train a multi-objective model with specific weights for overall performance measurement. To this end, we propose to accelerate and enhance the training of multi-objective model from two aspects. Firstly, to make use of the correlation between different multi-objective models for efficient training, we propose to utilize meta-learning [42] to facilitate the parameter learning for each new set of aggregation weights, leveraging the shared knowledge across similar optimization tasks. Secondly, to alleviate the conflict among different objectives for effective training, we equip meta-learning with the orthogonal gradient descent strategy to avoid the invalid updating of conflict gradients for better convergence.\nIn summary, our main contributions lie three-fold.\n\u2022 We are the first to propose a novel framework that defines distinct levels of autonomy for RSs based on their ability to independently determine recommendation objectives. Meanwhile, it is also the first trial to open the black box between assigned weights and the overall performance of objectives in multi-objective learning.\n\u2022 We propose a novel Bayesian optimization method by boosting Bayesian optimization with an orthogonal meta-learning paradigm, abbreviated as BOOML, to efficiently help optimize the uncertain multi-objective task in RSs. Specifically, it considers the collaborative signals among different multi-objective models for fast convergence and alleviates invalid updating of conflict gradients for better performance.\n\u2022 We conduct empirical studies on three real-world datasetes to demonstrate the effectiveness of our proposed method in exploring the uncertain multi-objectives for individual users."}, {"title": "RELATED WORKS", "content": "Early RSs at Level 0 rely on generic rules or broad statistical patterns, such as recommending the most popular items, or frequently co-occurred items mined by association rules [40], thus failing to provide personalization. Later, RSs at Level 1 began leveraging static user or item profiles, aka. content-based RSs [40], for instance, a user who indicates a preference for 'romance' in their profile would receive recommendations for romantic movies. Hence, a basic level of personalization is introduced. Advancing to Level 1, RSs at Level 2 resort to dynamic historical user behaviors to learn user preference, aka. collaborative filtering based RSs [40]. Different techniques are adopted, ranging from simple matrix factorization (MF) [15], to complex deep learning, e.g., MLP [39], RNN [38], GCN [32], Transformer [59] and LLMs [23, 47]. However, RSs at Levels 0-2 aim to purely improve recommendation accuracy, ignoring other essential ethical aspects, e.g., diversity and fairness."}, {"title": "RSs at Level 3", "content": "RSs at Level 3 exploit dynamic historical user behaviors to learn user preference by optimizing pre-defined multi-objectives beyond accuracy. As we primarily focus on two key ethical aspects \u2013 diversity and fairness, we limit our discussion to research relevant to these areas. Studies on other ethical aspects, e.g., explanation and privacy-perseveration, will be explored in our future work.\nDiversity bias would cause filter bubbles, which grow along the feedback loop and inadvertently narrow user interests [17]. Thus, a vital branch is to enhance recommendation diversity while maintaining accuracy, mainly divided into three categories: post-processing heuristic methods [22, 33, 37], determinantal point process methods [4, 11, 26, 48, 52] and end-to-end learning methods [3, 5, 6, 21, 24, 27, 34, 36, 43, 44, 54, 62]. However, they suffer from different limitations: (1) some follow a two-stage paradigm, i.e., train offline models to score items on accuracy and then re-rank items considering diversity; and (2) others incorporate accuracy and diversity objectives with a pre-defined \"trade-off\" hyperparameter, overlooking the uncertainty of personalized user needs.\nFairness is another critical ethical issue of RSs [9, 20, 46] that can affect personal experience and social good since RSs serve a resource allocation role in society by allocating information to users and exposure to items. Extensive work has encouraged equal exposure across item groups partitioned by item features, such as category and popularity\u00b9. Early studies design data-oriented methods [10] to alleviate the unfairness issue by changing training data. Another branch focuses on re-ranking based methods [25, 37] to adjust the outputs of recommendation models to promote fairness. Recent studies propose ranking-based methods to improve fairness by (1) using linear programming to add fairness constraints [35]; (2) adding a fairness-related regularization term to the recommendation loss [1, 63]; (3) leveraging adversarial learning to learn fair representations or predicted scores [2, 51, 64]; (4) adopting reinforcement learning to achieve long-term fair recommendations [12]; and (5) balancing accuracy and fairness for various stakeholders with heuristic strategies [31, 53] or Pareto optimality guarantee [13, 49]. Despite the effectiveness, most of them mainly seek a uniform \"trade-off\" between accuracy and fairness across all users while ignoring personalized user needs."}, {"title": "RSs at Level 4", "content": "Some studies attempt to achieve RSs at Level 4. For instance, in [45], the authors propose a new recommender prototype called User Controllable RS, which enables users to actively control the mitigation of filter bubbles. Nevertheless, it relies on user feedback and only considers the balance between accuracy and diversity. MMOE [28] adapts the Mixture-of-Experts structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network to optimize each task. However, it only learns the gates at the task level instead of the individual user level. A recent work on arXiv [19] introduces a deep Pareto reinforcement learning model for multi-objective RSs, which accounts for the relationships between different objectives and implements"}, {"title": "UNCERTAIN MULTI-OBJECTIVES", "content": "This section first introduces different objectives in RSs by considering accuracy and different ethics, followed by the formulation of our uncertain multi-objective function. In this paper, we focus on three objectives without loss of generality, including accuracy, diversity, and fairness. Note that, our framework can be easily adopted and adapted to more objectives.\nNotations. Let $U = \\{U_1, U_2, . . ., U_{|U|} \\}$, $V = \\{0_1, 0_2, ..., 0_{||} \\}$ and $C = \\{C_1, C_2, ..., C_{|C|} \\}$ denote the user, item and item category sets, respectively. $R \\in R^{|U|\\times|V|}$ denotes the user-item interaction matrix, where its entries $r_{ij} = 1$ represents user $u_i$ interacted with item $v_j$; otherwise 0. For each item $v_j$, it has a categorical feature $c(v_j) \\in C$. To model users and items in the latent space, we embedding them into the user representation matrix $U \\in R^{|U|\\times d}$ and the item representation matrix $V \\in R^{|V|\\times d}$, where d is dimension of the latent space.\nProblem Statement. Given the user-item interaction R, our goal is to provide a personalized recommendation list (RL) with the ranking of K items to each user, aiming to better hit her preference while meeting her personalized requirements regarding different ethical aspects, e.g., diversity and fairness."}, {"title": "Different Objectives and Metrics", "content": "Accuracy Objective. The primary goal of RSs is to provide accurate recommendations to hit user preference (e.g., ground-truth interacted items). The accuracy can be measured with widely-used ranking metrics, e.g., Precision, Recall, and NDCG [41]. In our study, we adopt NDCG as the evaluation metric, denoted as ACC, as it evaluates whether (1) the target items are correctly recommended and (2) the correctly recommended items are top-ranked. Larger values of NDCG indicate better ranking accuracy.\nAccuracy Optimization. We adopt the BPR loss [41] to maximize the preference gap between positive and negative items for all users,\n$f_{acc} (\\Theta) = - \\sum_{(u_i,v_j,v_k) \\in D_T} log \\sigma (f_{ij} - \\hat{f}_{ik})$,\n(3)\nwhere $r_{ij} = u_i v_j$ is the estimated preference score of user $u_i$ to item $v_j$; $u_i$ and $v_j$ denote the encoding of user $u_i$ and item $v_j$, respectively; $D_T$ denotes the training set meaning $u_i$ engaged $v_j$ instead of $v_k$, i.e., $r_{ij} = 1$ and $r_{ik} = 0$; and $\\sigma(x) = 1/(1 + exp(-x))$ is the sigmoid function.\nDiversity Objective. To alleviate filter bubbles [43], it is necessary to provide diversified recommendations rather than focusing narrowly on specific categories of items. Typically, the recommendation diversity can be measured with pairwise diversity metrics, e.g., ILD (intra-list distance), entropy-and-diversity score [56]. In our method, we adopt ILD to measure the average Euclidean distance between every pair of items in the RL, i.e.,\n$DIV = \\frac{1}{|U|} \\sum_{u_i \\in U} \\frac{\\sum_{(v_j,v_k) \\in RL_{u_i}, v_j \\ne v_k} ||v_j - v_k||_2}{|RL_{u_i}| \\times (|RL_{u_i}| - 1)}$,\n(4)\nwhere $RL_{u_i}$ denotes the recommendation list (RL) for user $u_i$. A larger value of IDL indicates a more diverse result in the RL.\nDiversity Optimization. In our study, we propose to maximize the diversity measured by the negative entropy of estimated category probability distribution for all users as in [56],\n$f_{div}(\\Theta) = - \\sum_{u_i \\in U} Entropy(p_i) = \\sum_{u_i \\in U} \\sum_{l} p_{il} log p_{il}$,\n(5)\nwhere $p_i$ is the estimated category probability distribution for user $u_i$, satisfying $\\sum_{l} p_{il} = 1$; and $p_{il}$ denotes user $u_i$'s preference towards category $c_l$. Specifically, $p_{il}$ can be estimated by aggregating $u_i$'s preference towards all items belonging to category $c_l$,\n$p_{il} = Softmax(\\sum_{v_j \\in V} r_{ij} \\cdot I(c(v_j) = c_l))$,\n(6)\nwhere $I()$ denotes the indicator function. The Softmax function making it a probability distribution, ensuring non-negativity $p_{il} \\ge 0$ and $\\sum_{l} p_{il} = 1$ for $p_i$.\nFairness Objective. Fairness aims to ensure the recommendation results are not dominated by popular products but include long-tail items [14]. The recommendation fairness regarding popularity can be measured by several metrics, e.g., ARP (Average Recommendation Popularity) [14], RR (Recommendation Rate) [61], and PR (Popularity Rate) [12]. For generality, we use ARP to measure the average popularity of the recommended items, i.e.,\n$FAIR = \\frac{1}{|U|} \\sum_{i=1}^U \\sum_{v_j \\in RL_{u_i}} \\varphi(v_j)$,\n(7)\nwhere $\\varphi(v_j)$ represents the popularity of item $v_j$. Smaller values of ARP indicate fairer recommendation results.\nFairness Optimization. Intuitively, since popular items are more frequently interacted with by users, their representations are likely to be pulled closer to user representations during the model training process, leading to systematic higher scores. Inspired by Biased-MF [18], we propose to remove such bias by minimizing the gap between the estimated preference score of individual users over individual items and the estimated average score of the system,\n$f_{fair} (\\Theta) = \\frac{1}{|U|} \\sum_{i=1}^U \\sum_{j=1}^V (\\sigma (r_{ij}) - \\hat{f}_{UV})^2$,\n(8)\nwhere $\\hat{f}_{UV} = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sigma (r_{ij})/(|U|\\cdot |V|)$ is the average predicted score for all users towards all items."}, {"title": "Uncertain Multi-Objectives", "content": "In this paper, we aim to improve the overall performance of multiobjectives, while keeping validation of each objective, i.e.,\n$\\underset{\\Theta}{\\text{max}} g(ACC, DIV, FAIR)$,\ns.t. $ACC > \\tau_{acc}$, $DIV > \\tau_{div}$, $FAIR < \\tau_{fair}$,\n(9)\nwhere g() denotes the overall performance of multiple objectives; and $\\tau_{acc}$, $\\tau_{div}$, and $\\tau_{fair}$ represent the thresholds of minimal requirement for accuracy, diversity, and fairness objectives, respectively.\nIn real-world scenarios, users may have diverse or uncertain requirements in RSs, leading to varying importance in optimizing multiple objectives for different users. For example, if a user shows a narrow interest in items, blindly increasing recommendation diversity may largely harm other objectives such as recommendation accuracy. To this end, we propose to optimize the personalized multi-objectives to capture users' uncertain requirements in RSs, enabling RSs to function as more ethical and intelligent user-centric assistants. Specifically, we assign personalized weights for different objective losses for multi-objective optimization to improve the overall performance of multi-objectives,\n$F(\\lambda, \\beta) = \\sum_{u_i \\in U} [f_{acc} (\\Theta_i) + \\lambda_i f_{div} (\\Theta_i) + \\beta_i f_{fair} (\\Theta_i)]$,\n(10)\nwhere $\\lambda_i$ and $\\beta_i$ are the personalized weights of diversity and fairness objectives for $u_i$. However, challenges persist in quantitatively determining the appropriate weights using existing methods.\nWhy Not Empirical Weights? Assigning empirical weights via the grid search for different objectives [1, 5, 43, 63] has been widely used in multi-objective learning due to its simplicity for RSs at Level 3. However, for RSs at Level 4, the scale of grid search is exponential to the size of objectives and users, leading to unacceptable costs in the training phase. Worsely, it is intractable to clarify the certain relationship between weights and multi-objective performance through empirical investigation\nWhy Not Trainable Weights? Some methods attempt to learn trainable weights that aggregate multiple objectives for unified learning [19], however, it may lead to the degradation of certain objectives. For example, a trivial solution that assigns a lower loss with a larger weight, results in imbalanced optimization and sub-optimal performance."}, {"title": "BAYESIAN OPTIMIZATION BOOSTED VIA ORTHOGONAL META-LEARNING", "content": "Guided by the analysis in Section 3.2, it is hard to determine optimal weights for multi-objective learning through empirical investigation or direct optimization. To explore the uncertain relationships between the weights and multiple objectives, we propose a novel Bayesian optimization method to open the black box that achieves balanced optimization among different objectives and bridges the gap between objective losses and performances. Most importantly, for more efficient and effective optimization, we design an orthogonal meta-learning paradigm to enhance the optimization of each objective by considering their correlations and potential conflicts."}, {"title": "Bayesian Optimization for Group-Level Personalization", "content": "Recall Equation (10), it is impractical to directly leverage Bayesian optimization to find out the optimal $\\lambda_i$ and $\\beta_i$ for each user $u_i$, as the search space is huge due to the large volume of users in RSs. To this end, we allocate users into different groups based on the statistics of their behaviors, as similar users may share a similar need (e.g., tendency toward diversity and fairness) for items. Specifically, we utilize three kinds of user behavior statistics, including the total number of engaged items, the ratio of engaged categories to items, and the average popularity of engaged items, which could reflect users' preferences towards diversity and fairness of recommendation results. Thus, we cluster users into W different groups $(\\{G_1,\\ldots, G_w \\})$ based on these statistical features. For each group $G_w$, we assign a personalized parameter pair $(\\lambda_w, \\beta_w)$ for multi-objective learning. Accordingly, the group-level uncertain multi-objective function is given by:\n$\\underset{\\lambda, \\beta}{E} (\\Theta) = \\sum_{i=1}^{|U|} \\underset{\\lambda, \\beta}{I} (\\Theta)$,\n$\\underset{\\lambda, \\beta}{I} (\\Theta) = \\sum_{w=1}^W (u_i \\in G_w) \\cdot [f_{acc} (\\Theta_i) + \\lambda_w f_{div}(\\Theta_i) + \\beta_w f_{fair} (\\Theta_i)]$,\n(11)\nwhere $I(u_i e G_w)$ aims to select $\\lambda_w$ and $\\beta_w$ for user $u_i$; $\\lambda = [\\lambda_1,\\ldots, \\lambda_W]$ and $\\beta = [\\beta_1,\\ldots,\\beta_W]$; and $\\Theta = [\\Theta_1,\\ldots, \\Theta_{|U|} ]$, where $\\Theta_i$ denotes the learnable parameters related to user $u_i$ and her engaged items. Hence, our goal is to find out the optimal $\\lambda_w$ and $\\beta_w$ for each group $G_w$, thus satisfying users' uncertain requirements regarding various ethical aspects at the group-level.\nFor optimal weights $\\lambda$ and $\\beta$, we formulate Equation (9) as a Bayesian optimization (BO) problem,\n$\\underset{\\lambda,\\beta}{\\text{max}} g (ACC(\\Theta_{\\lambda,\\beta}),DIV (\\Theta_{\\lambda,\\beta}), FAIR(\\Theta_{\\lambda,\\beta})) - \\kappa \\cdot const(\\Theta_{\\lambda,\\beta})$,\n$\\Theta_{\\lambda,\\beta} = \\underset{\\Theta}{min} F_{\\lambda.\\beta} (\\Theta)$,\n(12)\nwhere $\\Theta_{\\lambda,\\beta}$ denotes the solution of multi-objective function $F_{\\lambda,\\beta} (\\Theta)$ with weights $\\lambda$ and $\\beta$. The soft constraint $const(\\Theta_{\\lambda,\\beta}) = [\\tau_{acc} - ACC(\\Theta_{\\lambda,\\beta})]_+ + [\\tau_{div} - DIV (\\Theta_{\\lambda,\\beta})]_+ + [FAIR(\\Theta_{\\lambda,\\beta}) - \\tau_{fair}]_+$ penalize the unsatisfied constraints in Equation (9) with a penalty coefficient $\\kappa >> 0$. For the function g(), we define the overall performance of multiple objectives in two ways:\n\u2022 Rescaled Sum. It seeks the maximal sum of different objectives. However, measuring objectives with different metrics usually has different scales, e.g., NDCG \u2208 [0, 1], whereas ILD may be larger than 1 and ARP possess the opposite trend with NDCG and ILD (i.e., smaller ARP values indicate fairer recommendation). To this end, we adopt the rescaled sum to formulate $g(NDCG, ILD, ARP) = NDCG + \\sigma(ILD) + \\sigma(1/ARP)$.\n\u2022 Harmonic Mean. It seeks the maximal harmonic mean of different objectives. Considering the opposite trend of ARP compared with NDCG and ILD, we, therefore, formulate the harmonic mean of these three metrics as $g(NDCG, ILD, ARP) = 3/[NDCG^{-1} + \\sigma(ILD)^{-1} + \\sigma(1/ARP)^{-1}]$.\nFollowing the standard procedure of BO, we iteratively update a surrogate model to approximate the objective function g(.) and guide the search for optimal weights $\\lambda$ and $\\beta$. Specifically, the procedure includes the following steps: We start by selecting an initial set of points (where a point is a combination of $\\lambda, \\beta$) and evaluate the objective function. A Gaussian process surrogate model is then fitted to approximate the objective. We adopt expected improvement as an acquisition function EI(\u00b7) to balance exploration (searching unexplored regions) and exploitation (refining known promising areas), where points with high expected improvement are more likely to be sampled as the next candidate point added to the training data. This process iterates, refining the surrogate model and optimizing the acquisition function, until a convergence criterion is met or the search budget is exhausted."}, {"title": "Orthogonal Meta-Learning for Efficient and Effective Optimization", "content": "Each acquisition in BO requires a whole process of multi-objective learning, leading to high cost if each acquisition is conducted independently. To this end, we propose an efficient and effective training optimization for two aspects, namely meta optimization and orthogonal gradient descent. The meta optimization can reduce the times of gradient updating by exploiting shared knowledge across similar tasks, leading to efficient optimization to a new task. The orthogonal gradient descent can alleviate the conflict among different objectives, therefore further improving the effectiveness of the meta optimization.\nTo optimize the model effectively, we integrate group correlation and collaborative information into the meta optimization process, enabling the model to generalize better across users by leveraging shared patterns. Specifically, the meta optimization process involves two critical steps: inner loop optimization and outer loop validation, designed to achieve fast adaptation and balance between objectives. To optimize model parameters and validate performance, we divide behaviors of user $u_i$ into a support set $S_i$ and a query set $Q_i$.\nFor inner loop optimization (support set training), we optimize the parameters $\\Theta_i$ on the support set $(S_i)$ by minimizing the group-level multi-objective loss for each user $u_i$. The updated parameters are computed as:\n$\\Theta_i = \\Theta_i - \\eta_1 \\nabla_{\\Theta_i} F_{\\lambda,B} (S_i, \\Theta_i)$,\n(13)\n$F_{\\lambda,B} (S_i, \\Theta_i) =f_{acc} (S_i, \\Theta_i) + \\lambda_w f_{div} (S_i, \\Theta_i) + \\beta_w f_{fair} (S_i, \\Theta_i)$,\nwhere $\\eta_1$ is the learning rate, and $F_{\\lambda,B} (S_i, \\Theta_i)$ represents the multi-objective loss function for the support set of user $u_i$. This step leverages group-level personalized weights $(\\lambda_w, \\beta_w)$ to capture user-specific multi-objective preferences.\nFor outer loop validation (query set evaluation), we evaluate the model's generalization based on the meta-loss on the query set $(Q_i)$ using the updated parameters $\\Theta$:\n$L_{meta, i} = F_{\\lambda,B} (Q_i, \\Theta_i - \\eta_1 \\nabla_{\\Theta_i}F (S_i, \\Theta_i))$.\n(14)"}, {"title": "EXPERIMENTS AND ANALYSIS", "content": "We conduct extensive experiments on three real-world datasets to verify the efficacy of our proposed method BOOML by answering the following four research questions2:\nRQ1: How does BOOML perform compared with state-of-the-art (SOTA) multi-objective recommendation approaches?\nRQ2: How do different components of BOOML affect its performance regarding effectiveness and efficiency?\nRQ3: How does BOOML perform across different user groups?\nRQ4: How do essential hyper-parameters affect the performance of our proposed BOOML?"}, {"title": "Experimental Setup", "content": "We adopt three real-world datasets with varying domains, sizes, and sparsity levels collected from Amazon.com [29], including Games, Electronics, and Movies. The datasets contain users' ratings on the scale of [1, 5] stepped by 1 towards products in the three domains. Following [41], we convert the interactions"}, {"title": "Results and Analysis", "content": "Table 3 presents the performance of all methods. Several major observations are noted.\nFirst, across all encoders, our BOOML demonstrates a positive improvement on ResSum and HarMean in all cases compared to baseline methods. This highlights BOOML's superiority to balance multi-objective performance by leveraging orthogonal meta-learning to alleviate conflicts among different objectives.\nSecond, BOOML achieves better performance on accuracy, measured by NDCG, across all cases. However, its performance on diversity and fairness, measured by ILD and ARP, is worse than the best-performing baselines. This is attributed to some baselines focusing only on specific objectives while largely sacrificing other objectives. For example, the fairness-oriented methods TFROM-MF and TFROM-LGCN perform exceptionally well in ARP but significantly undermine both accuracy and diversity. Particularly, it consistently produces the worst NDCG in all cases compared with other methods. The diversity-oriented methods GFN4Rec-MF and GFN4Rec-LGCN perform well in ILD but compromise both accuracy and fairness.\nThird, diversity-oriented baselines (e.g., SMORL and GFN4Rec) generally outperform fairness-oriented baselines (e.g., FairRec and TFROM) in terms of ILD across most cases. Conversely, fairness-oriented baselines defeat diversity-oriented ones regarding ARP. Additionally, the diversity-oriented method DGRec consistently surpasses the generic multi-objective baseline MMOE in both NDCG and ILD but performs worse on ARP, but DGRec achieves better overall performance than MMOE, suggesting its stronger ability to balance multiple objectives.\nLastly, different baselines show varying sensitivity to encoders across all metrics. For example, in the aspect of accuracy, BOOML and SMORL are particularly sensitive to encoders and achieve the best NDCG performance when using LGCN as the encoder. Furthermore, all baselines show sensitivity to encoders in ILD and ARP except for TFROM which remains largely insensitive to encoders for ARP. These results underscore the importance of selecting the most suitable encoder for different multi-objective baselines to achieve optimal performance. Beyond BOOML and SMORL, we also observe that the diversity-oriented DGRec, built on GNN, achieves relatively strong performance on NDCG, highlighting the potential of GCN/GNN structures in enhancing the accuracy of multi-objective optimization."}, {"title": "Ablation Study (RQ2)", "content": "To examine the efficacy of different components of our BOOML, we compare it with different variants. Specifically, (1) SGD directly uses SGD with constant weights to optimize the recommender, i.e., $\\lambda_w = \\beta_w = 1.0$ for all user groups; (2) BO adopts vanilla Bayesian optimization to search the optimal weights of different objectives for each group; (3) BOML adopts meta-learning in the BO process to search the optimal weights of different objectives by considering the correlations among different user groups; and (4) BOOML is our proposed method which exploits orthogonal meta-learning in the BO process by considering the correlations of different groups and potential conflicts among various objectives. Four key findings can be identified.\nFirst, BO generally outperforms SGD, especially on the diversity and fairness metrics (i.e., ILD and ARP), which verifies the necessity and effectiveness of using BO to search for the optimal weights for better multi-objective performance.\nSecond, compared with BO, BOML generally delivers superior performance with LGCN as the encoder. However, BOML exhibits lower performance on NDCG when using MF as encoders but gains significant improvements in diversity and fairness metrics. For example, BOML-MF results in a 52% decrease on Games dataset in NDCG, it achieves a 108% improvement in ILD and a 63.52% reduction in ARP, ultimately leading to a 27% increase in ResSum. These results, on one hand, highlight the effectiveness of meta-learning in improving recommendation performance by capturing correlations among different user groups; on the other"}, {"title": "Performance Across Different Groups (RQ3)", "content": "Table 5 shows the learned weights for different objectives and the corresponding performance across different metrics of our BOOML-MF on Games and Electronics, respectively. In the table, the 'Learned Weights' means the original weights learned by our BOOML for different objectives. For ease of analysis, we also calculate the 'Normalized Weights'; we highlight the higher weights for different objectives (e.g., Accuracy) across"}]}