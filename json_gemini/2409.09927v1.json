{"title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "authors": ["Vinay Samuel", "Yue Zhou", "Henry Peng Zou"], "abstract": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed. However, these approaches are often validated with traditional benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness when evaluating state-of-the-art LLMs on the contamination of more challenging benchmarks. To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation. Our analysis reveals that (1) Current methods have non-trivial limitations in their assumptions and practical applications; (2) Notable difficulties exist in detecting contamination introduced during instruction fine-tuning with answer augmentation; and (3) Limited consistencies between SOTA contamination detection techniques. These findings highlight the complexity of contamination detection in advanced LLMs and the urgent need for further research on robust and generalizable contamination evaluation.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) consistently achieve higher state-of-the-art results across various benchmarks (Rae et al., 2022; Srivastava et al., 2023; Liang et al., 2023; Zou et al., 2024), the lack of curation in and the limited disclosure of massive training datasets raise a critical question: Does the model performance arise from model generalizability or mere memorization? Furthermore were the test sets possibly contaminated without notice? These questions have become crucial in accurately gauging LLMs' performance and has led to a critical area of research: detecting data contamination in LLMs.\nRecent work in detecting data contamination in LLMs has primarily focused on detecting contamination through validating the log probability of the data in the datasets (Oren et al., 2024; Shi et al., 2023) or determining contamination through prompting-based approaches (Golchin and Surdeanu, 2024b,a). Additional studies have been aimed at understanding the different types of contamination such as differentiating between training/development split contamination and testing set contamination (Sainz et al., 2023) and contamination occurring in the pertaining phase versus the supervised fine-tuning stage (Jacovi et al., 2023).\nHowever, existing research in this area has several limitations. Firstly, most methods show their effectiveness on traditional benchmarks, which are likely overexposed online and to LLMs. In contrast, challenging datasets that test the limits of LLM capabilities are neglected. These newer benchmarks are often more complex with novel formats. Secondly, the tested LLMs are often early-staged ones, such as GPT-J, while the rapid pace of LLM development has left a gap in understanding contamination in the latest models. Thirdly, previous research predominantly focuses on (possibly unintentional) contamination occurring during pretraining, where models are exposed to data in its original form. However, it overlooks contamination during instruction fine-tuning, where original data is subtly modified by, for instance, answer augmentation with chain-of-thought reasoning steps. Such variations may cause difficulties in contamination detection yet are rarely considered when evaluating the effectiveness of contamination detection methods. Lastly, while various detection methods have demonstrated effectiveness (with a particular set of datasets and LLMs of their choice), there is a lack of a comprehensive cross-comparison to assess the consistency and reliability of these techniques, particularly for more recent LLMs, benchmarks, and training paradigms.\nTo bridge these gaps, we evaluate five distinct data contamination detection approaches, including three state-of-the-art methods recently published at ICLR, a simple prompting method based on token perturbation, and our proposed pilot prompt-based method, which queries the LLM about its knowledge of the original order of data points. Our study covers eight benchmarks, including six challenging datasets frequently used to evaluate modern LLMs and two traditional benchmarks. We apply these methods with four language models: GPT-4 (OpenAI, 2023), Claude 3 Sonnet, LLaMA-3-Chat (70B) (AI@Meta, 2024), and LLaMA-2-Chat (70B) (Touvron et al., 2023). To provide a gold standard for assessing the effectiveness of these methods, we create an oracle using LLaMA-2 (70B) by intentionally contaminating the model with varying portions of the six challenging benchmarks in the format of instruction fine-tuning with answer augmentation. This setup allows us to observe the performance of the five detection methods given the known contamination status. By these setups, we seek to answer the following research questions: i. Are the latest state-of-the-art LLMs, which consistently achieve higher performance, contaminated with these challenging benchmarks? What do the detection methods indicate? ii. Can these methods detect contamination that occurred during instruction fine-tuning with data variations instead of the original format in pretraining? iii. Do different \"well-accepted\" detection methods corroborate each other's findings for a given dataset? Do they yield inconsistent results?\nOur experimental results and analysis reveal several critical findings about current data contamination detection in LLMs: First, all existing methods have limitations in their underlying assumptions or practical applications. Second, while some metrics suggest possible contamination in traditional benchmarks, we observe no consistent agreement for methods contamination in newer, more challenging benchmarks. Third, all methods struggle to robustly reflect our oracle contamination created by instruction fine-tuning with answer augmentation. This finding highlights an urgent need for this research direction. Finally, we observe a surprising lack of agreement between different detection methods, suggesting that these well-accepted approaches cannot be simultaneously valid. This disagreement casts doubt on the reliability of current contamination detection techniques and highlights the critical need for more robust, consistent, and comprehensive approaches."}, {"title": "2 Related Work", "content": "Detecting training data through probability inference and reconstruction has long been a well-established approach (Shokri et al., 2017; Carlini et al., 2021). Recently, the challenge of data contamination in large language models has garnered significant attention due to its potential to skew model evaluation and misrepresent true performance. Sainz et al. (2023) highlighted the risks of contamination, particularly emphasizing that while test set contamination invalidates benchmarks, contamination in training and validation sets is less concerning unless zero or few-shot learning claims are made. Contamination is most likely during the pre-training phase, where massive text corpora are scraped with minimal curation. Additionally, Balloccu et al. (2024) found that 42% of papers evaluating models such as GPT-3.5 and GPT-4 contained leaked data, affecting millions of instances, further underscoring the widespread impact of contamination.\nSeveral methods have been proposed to detect data contamination, focusing on either log probability analysis or prompting-based techniques. Log probability-based methods, such as those developed by Oren et al. (2024) and Shi et al. (2023), assess the likelihood of data being present in a model's training set. In contrast, prompting-based approaches by Golchin and Surdeanu (2024b) and Golchin and Surdeanu (2024a) directly query the model to detect contamination. However, these methods have primarily been tested on traditional benchmarks and early-stage models, leaving a gap in understanding their effectiveness on more recent, advanced LLMs and complex datasets.\nMoreover, much of the existing research assumes contamination occurs during pre-training, overlooking the potential impact of instruction fine-tuning, which is increasingly used to enhance LLM capabilities. This stage can introduce variations in the data that are not adequately tested by current methods. Our work seeks to address these gaps by evaluating a broader range of detection techniques across diverse benchmarks and models, with particular attention to more challenging datasets and the instruction fine-tuning phase."}, {"title": "3 Benchmark Datasets", "content": "There exists a significant disparity between the benchmarks commonly used in data contamination research and those employed to evaluate state-of-the-art (SOTA) LLM capabilities. Table 4 illustrates this mismatch, showing that while many contamination detection methods are validated using traditional benchmarks, modern LLM evaluations focus on more challenging tasks, such as mathematical reasoning and code generation. This observation raises intriguing questions: Could the performance gains of SOTA LLMs on these newer, more challenging benchmarks be attributed, in part, to data contamination? What insights do SOTA data contamination detection methods provide when applied to these essential modern benchmarks?\nTo address this, we selected six challenging benchmarks commonly used in evaluating SOTA LLMs, complemented by two traditional benchmarks frequently featured in data contamination studies. We aim to ensure broad coverage of relevant task domains while allowing for comparison with previous contamination detection work. The selected benchmarks are as follows:\n\u2022 GSM8K (Cobbe et al., 2021) contains linguistically diverse grade school-level math questions with moderate difficulties.\n\u2022 MMLU (Hendrycks et al., 2020) contains multiple choice questions across multiple domains.\n\u2022 BIG-Bench-Hard (BBH) (Suzgun et al., 2022) contains multitask questions believed to be beyond the capabilities of LLMs at the time of release.\n\u2022 ARC-Challenge (Clark et al., 2018) contains questions from the ARC dataset that were answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\n\u2022 DROP (Dua et al., 2019) a new reading comprehension benchmark requiring discrete reasoning over paragraphs.\n\u2022 HumanEval (Chen et al., 2021) is a challenging becnhamrk for the coding domain.\n\u2022 AGNews (Zhang et al., 2015) contains text classification questions drawn from over 1 million news articles\n\u2022 IMDB (Maas et al., 2011) is a binary sentiment classification datasets from movie reviews."}, {"title": "4 Evaluated Methods and Limitations", "content": "We examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowest-probability tokens, with a high result suggesting the text's presence in the training data.\nLimitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC - instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate.\nLimitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated.\nLimitations: (1) Part of the evaluation is by GPT-4, prompting GPT-4 to determine a \u201cnear match\" can be ambiguous and subject to biases. (2) It is unclear how different parts of the original data points can affect the completion and, thus, the ROUGE score and p-value.\n\u2022 Word Perturbation Quiz (Golchin and Surdeanu, 2024a) detects data contamination by presenting an LLM with a multiple-choice quiz, where the options include word-level perturbed versions of a dataset instance and the original. The LLM's tendency to select the original instance indicates potential contamination from its pre-training.\nLimitations: While perturbed answers may retain their semantic meaning, they often lack the natural fluency of the original text. This discrepancy in linguistic nuance can inadvertently provide cues to the model, making it easier to identify the unperturbed, ground truth answer. Moreover, the original perturbation prompts do not safeguard proper nouns and numerical values from alteration. Given that the labels remain unperturbed, these distinctive elements can serve as additional indicators for the model to differentiate between original and perturbed content. Consequently, the model's ability to select the correct answer may stem from recognizing these linguistic and contextual inconsistencies rather than from accurate memorization or contamination, potentially leading to overestimating contamination levels.\n\u2022 Local Order Quiz (Ours) In this work, we are also interested in exploring whether prompt-based approaches can detect data contamination in LLMs. These approaches could offer the advantage of being applied to both closed-source and open-source models. Unlike token perturbation methods, which may entangle an LLM's recognition of original data points with its sensitivity to perturbations, we propose an approach focusing on the model's ability to identify the original order of dataset examples. Specifically, we randomly sample a target example t from dataset D and provide N other examples, one of which appeared immediately after t in the original dataset. We then prompt the LLM to identify this subsequent example. While pretraining typically involves randomized batches, local order can be preserved to some extent. If the LLM can accurately identify the correct subsequent example, especially in cases with no inherent information suggesting the order (e.g., similar categories or content), this could indicate potential data contamination.\nLimitations: We acknowledge the challenging nature of this task, as it requires the LLM not only to have been exposed to the data points in their original order but also to understand their semantics and retrieve this information accurately."}, {"title": "5 Experiments", "content": "In this section, we describe our experimental settings for each method and the oracle, and the experiment results.\n5.1 Implementation Details\nFor all detection methods, we follow their official implementations when available. For the Oracle setup, we instructed fine-tuned LLaMA-2-70b-chat using the original answers of the examples replaced by chain-of-thought reasoning. Specifically:\nMin-K% Prob We used k = 20 as recommended in the paper. The authors used their proposed WikiMIA dataset as ground truth and reported AUC in the original paper. However, the threshold for determining contamination is not provided, and in real-world settings, the ground truth is unavailable. Thus, we used the mean and standard deviation for each split directly for the contamination indicator, as the authors claim Min-K%\nPorb is most informative compared with other probability-based metrics.\nCanonical Order Statistical Testing Due to resource constraints, we adapted the method to use 100 instances with 10 shards and 25 permutations. Examples within each shard were concatenated with \\n. All the datasets were processed so that only the question and answer were included for each instance.\nToken Completion Overlap Score We used the exact implementation without changes, including only ten instances. GPT-4 was employed to determine near/exact matches, with one exact match and two near matches used as the threshold for contamination detection. The temperature was set to 0 for inference.\nWord Perturbation Quiz LLaMA-3-70b-chat was used for perturbation across all datasets. The perturbation prompt was adjusted for different dataset formats, with care to prevent altering proper nouns. For perturbation, temperature and top p were set to 0.9, while for inference, temperature was 0. Dataset-specific adjustments included: Humaneval: Only docstrings were perturbed. DROP: Full passage and question were perturbed. MMLU/AGNews/IMDB/ARC-Challenge/GSM8K: Answer choices were removed before perturbation to avoid confusion in the quiz.\nLocal Order Quiz (Ours) We randomly selected options for each instance, ensuring options from the same category for datasets like MMLU and BBH. The prompt included a dataset description, name, data split, and options. We used the number of options of 4 as a hyperparameter, meaning a random guess would achieve about 25% accuracy. All datasets maintained their original format as initially processed.\nOracle Setup To investigate the challenges of detecting data contamination during the fine-tuning stage, we developed an oracle setup that mimics real-world instruction fine-tuning scenarios. We hypothesize that contamination occurring during fine-tuning is significantly more difficult to detect compared to pretraining contamination for several key reasons: (1) Exposure Frequency: During fine-tuning, the model typically encounters data for only a few epochs (usually 1-3), whereas in pretraining, data chunks are often seen repeatedly due to sliding window approaches ([starting position: starting position + window size]). (2) Data Modification: Modern fine-tuning techniques, particularly instruction fine-tuning, often incorporate more complex answers, such as chain-of-thought reasoning, to enhance the LLM's analytical capabilities. This process modifies the original data, preserving the questions while replacing answers with elaborate solutions. To simulate these conditions, we fine-tuned LLaMA-2-70B-chat using varying proportions of the six newer benchmarks in our study. Our goal was to observe how different con-tamination detection metrics respond to varying levels of data exposure during fine-tuning. For each data point, we replaced the original answer with a chain-of-thought solution generated by LLaMA-2-70B, mimicking the data augmentation often used in instruction fine-tuning. To maintain some semblance of the original data structure, we packaged four examples as one training instance, preserving the local order as it appears in the original dataset. This allows to test whether detection methods can identify contamination when local order information is partially retained."}, {"title": "5.2 Main Results", "content": "Table 1 shows the results of the five contamination detection methods on eight benchmarks with GPT-4, Claude-3, and LLaMA-3. Results for the Min-K% and Canonical Order methods are unavailable for GPT-4 and Claude-3, as these approaches require access to model parameters. We can observe that: (1) Perturbation accuracy is unusually high across most benchmarks for all three models. This consistent pattern suggests potential inflation of results, possibly due to the models recognizing the perturbations themselves rather than indicating contamination or memorization. (2) Both our proposed method and the token overlap approach provide significant evidence that the IMDB dataset may be contaminated for all three models. (3) The token overlap method yields conflicting results depending on the specific metric. Based on p-values, it suggests contamination in HumanEval for Claude-3 and in BBH for GPT-4. However, no contamination is detected for any of the newer benchmarks when considering exact matches and near matches. (4) For LLaMA-3, both Min-K% and Canonical Order P-value show possible contamination on BBH. However, in general, there is no clear agreement among the methods regarding contamination in the newer benchmarks. Notably, the GPT-4 report indicates that DROP and HumanEval are approximately 21-25% contaminated. However, our results show that none of the methods detected contamination in DROP, while only our method partially detected contamination in HumanEval (accuracy = 0.36).\nTable 2 presents the oracle results, comparing the metric values of the five detection methods before (upper half) and after (lower half) instruction fine-tuning. The Portion column indicates the percentage of data used for fine-tuning, ranging from 0% (no contamination) to 100% (entire dataset split used). Ideally, a robust detection method should (1) consistently reflect the portion of contamination through its metric values and (2) not be influenced by the training set contamination when the test set remains uncontaminated. For instance, probability-based approaches might be susceptible to false positives in test set contamination detection when the training set is contaminated due to the similarity in distribution between training and testing data. Therefore, post-fine-tuning results should show increased accuracy/mean probability values and decreased p-values for non-zero contamination portions in an ideal scenario. However, our observations reveal significant challenges in detecting fine-tuning contamination: (1) None of the metrics demonstrate a consistent trend that aligns with the varying portions of data contamination. This lack of correlation suggests that current methods may not be sensitive enough to detect or quantify the degree of contamination introduced during such a fine-tuning paradigm. (2) Surprisingly, all Min-K% probability values decrease after fine-tuning. This counterintuitive result is particularly noteworthy given that the original questions are preserved in the fine-tuning process, even though we used instruction fine-tuning with chain-of-thought solutions. These findings highlight the complexity of detecting contamination in instruction-tuned models and suggest that existing methods, originally designed for pretraining contamination detection, may not be directly applicable or reliable in fine-tuning scenarios.\nTable 3 shows the Spearman rank correlation between the metric results of the five detection methods based on all experiments conducted. While we acknowledge slight statistical liberty in comparing rank correlations between p-values and other metrics, this analysis provides insights into the relationships between different contamination detection approaches.\nFirst, we observe no strong correlations or agreement between these metrics across the various LLMs and datasets tested. This lack of consensus is particularly concerning, as it suggests that different methods may yield contradictory conclusions about the presence or extent of contamination in a given model-dataset pair. We do, however, note some weak correlations: (1) Between the Min-K% probability value and word perturbation quiz accuracy, (2) Between Min-K% probability and token overlap p-values, and (3) Between canonical order p-values and our proposed method. The weak correlation between our method and the canonical order statistical testing suggests the potential for prompting information about local order as a proxy for canonical statistical testing, especially in gauging data contamination in closed-source LLMs.\nHowever, the lack of strong agreement between methods raises a critical concern: these well-established approaches cannot all be simultaneously correct in their contamination assessments, which poses a significant challenge to contamination detection in LLMs. This observation calls for further investigation into the more robust and consistent contamination detection methods for distinct scenarios."}, {"title": "6 Conclusions and Future Work", "content": "This paper evaluates five distinct data contamination detection methods across eight benchmarks and four state-of-the-art LLMs, including an oracle setup to mimic instruction fine-tuning contamination. Our study reveals significant challenges in current methods, with inconsistent results across different benchmarks and models. Detecting contamination introduced during instruction fine-tuning proved especially difficult, and the weak correlations between different detection methods raise concerns about their collective reliability. While prompt-based methods show certain evidence of detection abilities, they are very limited at this stage. These findings underscore the complexities in accurately quantifying data contamination in LLMs and highlight the urgent need for more robust, unified detection frameworks. As LLMs continue to advance, developing reliable contamination detection techniques remains crucial for ensuring the integrity and trustworthiness of AI systems."}, {"title": "Limitations", "content": "Our study offers valuable insights into data contamination detection in state-of-the-art LLMS over diverse benchmarks and models. However, the detection methods we examined, including ours, showed limited effectiveness in accurately reflecting the known contamination in our oracle setup. This unexpected result points to the complexity of contamination mechanisms in instruction-tuned models and opens new avenues for research into how fine-tuning affects the detectability of contamination."}, {"title": "Ethics Statement", "content": "This study on data contamination detection in LLMs has ethical implications on the importance of transparency in AI development and the potential risks of overestimating model capabilities based on potentially contaminated evaluations. We emphasize that our findings on the limitations of current detection methods call for caution in making definitive claims about data contamination and highlight the urgent need for more robust and general detection methods."}, {"title": "A Prompts", "content": "A.1 Word Perturbation Quiz\nAs part of the Word Perturbation Quiz, there are two components namely the perturbation and the quiz components. In the perturbation prompt, the model is prompted to create a four choice quiz by only making word level perturbations. Several guidelines are also included in the prompt to guide the model on what words are allowed to perturbed and what formatting components must remain the same. In the standard quiz component, the model is given the 3 perturbed options and the original data instance in a random order and prompted to select the correct data instance that showed up in the given dataset and split. Both of these prompts closely follow the prompts in (Golchin and Surdeanu, 2024a).\nPerturbation Prompt.\nInstruction: Your task is to create a four-choice quiz by replacing the words in the provided \"Input Text\" with their contextually relevant synonyms. The meaning and overall structure of the four options must exactly match every detail and the structure in the Input Text. You must not include the provided Input Text as an option. Each option in the four-question quiz you generate must include both the underlying text and answer choices in the Input Text but with word-level perturbations. You must make sure that:\n(1) You generate distinct options based on the provided Input Text;\n(2) The only difference between options is word-level perturbations.\n(3) Each Option must still include the main part of the text in the Input Text with word level perturbations\n(4) Each option must still include all answer choices present in the Input Text with no changes\n(5) No numbers or proper nouns are changed.\n(6) Options are ordered;\n(7) There is not any extra explanation;\n(8) You comply with the following \"Format\" to generate options\n(9) You comply with every specific symbol and letter detail in the given Input Text; and\n(10) All options retain the exact label from the Input Text, if there is one.\nInput Text:\ninstance\nFormat:\nOption 1 -\nOption 2 -\nOption 3 -\nOption 4 -\nStandard Quiz Prompt.\nInstruction: You are provided with a five-choice quiz. Your task is to correctly select the option that exactly corresponds to an instance from the [split] split of the [dataset] dataset.\nWhen selecting the option, you must ensure that you follow the following rules:\n(1) You must ensure that you only generate a single option letter as your answer.\n(2) If you do not know the dataset or the correct answer, you must select option \"(E) None of the provided options.\"\n(3) You must output the answer in your final sentence like \"The correct answer is ...\"\nHint: While all the following options seem similar in terms of meaning, there is only one option that reflects the exact wording with respect to the original instance.\n(A) choices[\"(A)\"]\n(B) choices[\"(B)\"]\n(C) choices[\" (C)\"]\n(D) choices[\"(D)\"]\n(E) choices[\"(E)\"]\nAnswer:\nA.2 Local Order Quiz\nIn the local order quiz, the model is given a description of the dataset, a specific example from the dataset, and options for the next occurring example in order in the dataset. The model is then queried to choose which of the given options is the next occurring example in the dataset in order. The description of the the dataset is pulled from the GitHub page or Hugging Face page of the dataset if once exists. The rationale here is that while web scraping, if a dataset was contaminated then the dataset description on the GitHub/Hugging Face would have been contaminated as well. If a description foes not exist on the Github or Hugging Face then the description from the paper related to the dataset is used.\nLocal Order Quiz Prompt.\n[Description of dataset]\nGiven the target data example in the [split] of the [dataset name] dataset, Which of the following examples was next to it in the original order of the dataset? Exactly one of the choices must be selected and you need to output the answer in your final sentence like \"The answer is ...\"\nTarget example: [tar example]\nOptions: [options]"}]}