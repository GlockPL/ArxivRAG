{"title": "Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference", "authors": ["Rithvik Prakki"], "abstract": "Active inference provides a powerful mathematical framework for understanding how agents-biological or artificial interact with their environments, enabling continual adaptation and decision-making. It combines the principles of Bayesian inference and free energy minimization to model the process of perception, action, and learning in uncertain and dynamic contexts. Unlike reinforcement learning, active inference integrates both exploration and exploitation seamlessly, driven by a unified objective to minimize expected free energy. In this paper, we present a continual learning framework for agents operating in discrete time environments, using active inference as the foundation. We derive the core mathematical formulations of variational and expected free energy and apply these principles to the design of a self-learning research agent. This agent continually updates its beliefs and adapts its actions based on new data, without manual intervention. Through experiments in dynamically changing environments, we demonstrate the agent's ability to relearn and refine its internal models efficiently, making it highly suitable for complex and volatile domains such as quantitative finance and healthcare. We conclude by discussing how the proposed framework generalizes to other systems and domains, positioning active inference as a robust and flexible approach for adaptive artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "The Free Energy Principle (FEP), proposed by Friston, provides a unifying computational framework that integrates learning, perception, action, and decision-making. This principle posits that biological systems (or artificial agents) maintain themselves in their characteristic states by minimizing the difference between the predictions of their internal model and their actual sensory data, a quantity known as free energy. In essence, FEP suggests that living systems are driven by the need to reduce surprise in their interactions with the environment, formalized through the minimization of variational free energy."}, {"title": "1.1 Active Inference and Free Energy Principle", "content": "At the core of Active Inference (AIF), which operationalizes the Free Energy Principle, are two key objective functions:\n\u2022 Variational Free Energy (VFE): This function quantifies the fitness of an agent's internal model concerning the sensory observations it receives from its environment by mapping latent (hidden) states to sensory outcomes. The agent minimizes this free energy to maintain coherence between its beliefs about the world and incoming sensory evidence.\n\u2022 Expected Free Energy (EFE): This function governs the agent's policy selection by combining extrinsic value (goal-oriented behavior, aligned with the agent's prior preferences) and epistemic value (exploration-driven behavior to reduce uncertainty in the agent's model of the world).\nDiscrete-Time Active Inference is framed in the context of Partially Observable Markov Decision Processes (POMDPs), where agents use sensory and proprioceptive information to form probabilistic beliefs about the hidden (latent) states of the world."}, {"title": "1.2 Mathematical Derivation of Variational Free Energy (VFE)", "content": "Variational Free Energy (VFE) is a quantity that measures the dissimilarity between an agent's internal model of the world and the real-world sensory data it receives. It is used to approximate Bayesian inference in scenarios where calculating exact posteriors is computationally intractable. To derive VFE, we begin with Bayes' rule:\n$P(s|o) = \\frac{P(o|s)P(s)}{P(o)}$\nHere, $P(s|o)$ is the posterior distribution over hidden states s given observations o, $P(o|s)$ is the likelihood (i.e., how likely the observations are given the hidden states), $P(s)$ is the prior distribution over hidden states, and $P(o)$ is the marginal likelihood, also known as the evidence.\nIn active inference, the agent cannot compute this posterior $P(s|o)$ directly, as the marginal likelihood $P(o)$ is intractable due to the high-dimensional nature of real-world data. Instead, the agent approximates the posterior with a simpler distribution $Q(s)$, known as the variational distribution, which the agent iter-atively improves. The agent minimizes the difference between the true posterior $P(s|o)$ and the variational approximation $Q(s)$ using the Kullback-Leibler (KL) divergence:\n$D_{KL}[Q(s)||P(s|o)] = \\int Q(s) \\ln \\frac{Q(s)}{P(s|o)} ds$\nSince the true posterior $P(s|o)$ is unknown, we aim to minimize this divergence indirectly by minimizing the free energy F, which is an upper bound on the negative log-evidence, - ln P(o). To express this in terms of known quantities, we substitute Bayes' rule into the expression for the KL divergence:\n$D_{KL}[Q(s)||P(s|o)] = \\int Q(s) \\ln \\frac{Q(s)}{P(o|s)P(s)/P(o)} ds$\nThis can be split into three terms:"}, {"title": "1.2.1 Expected Free Energy (EFE)", "content": "While Variational Free Energy (VFE) focuses on inferring hidden states based on current observations, Expected Free Energy (EFE) extends this principle into the future by evaluating potential actions (or policies) that an agent can take. Just as VFE is used to update the agent's beliefs about the hidden states s given observations o, EFE evaluates the likely outcomes of different policies \u03c0, guiding the agent to select actions that minimize future free energy.\nSimilar to VFE, the derivation of EFE follows from Bayes' rule. However, instead of inferring hidden states based solely on observations, we now consider how policies influence both the hidden states and the observations. The posterior over hidden states and policies is given by:\n$P(s|o, \\pi) = \\frac{P(o|s, \\pi)P(s, \\pi)}{P(o, \\pi)}$\nHere, $P(s|o, \u03c0)$ is the posterior over hidden states given observations and policies, $P(o|s, \u03c0)$ is the likelihood of the observations conditioned on both the hidden states and the policy, $P(s, \u03c0)$ is the joint prior over states and policies, and $P(o, \u03c0)$ is the marginal likelihood of observations and policies.\nFrom this starting point, we can derive the expected free energy, G\u03c0, following a similar process as we used for VFE. The goal of EFE is to select policies that minimize the free energy expected over future observations, balancing the agent's desire to gather information (epistemic value) and achieve its goals (extrinsic value)."}, {"title": "1.3 Learning in Active Inference: The Dirichlet-Categorical Model", "content": "In Active Inference, learning occurs by updating the generative model's parameters based on new observations. This process is typically modeled with a Dirichlet-Categorical framework, where Bayesian inference is performed using a categorical distribution as the likelihood and a Dirichlet distribution as the prior."}, {"title": "1.4 Learning the A Matrix", "content": "A crucial part of the generative model in Active Inference is the A matrix, which encodes the likelihood mapping from hidden states s to observations o. Learning the A matrix is central to the agent's ability to adapt to its environment, as it allows the agent to refine its understanding of how hidden states generate sensory data.\nThe update rule for the A matrix is expressed as:\n$a_{t+1} = \\omega a_t + \\eta \\cdot \\sum_\\tau o_\\tau s_\\tau^T$\nWhere:\n\u2022 at are the concentration parameters for the A matrix at time t,\n\u2022 or is the observation at time T,\n\u2022 s is the inferred hidden state at time \u0442,\n\u2022 \u03b7 is the learning rate that controls the adaptation speed, and\n\u2022 w is the forgetting rate that controls the extent to which past observations are discounted.\nThis Hebbian-like learning rule updates the A matrix by strengthening associations between states and observations that co-occur. For example, if the agent observes o = [1,0,0]T while in state s = [1,0]T, the association between state 1 and observation 1 is reinforced. This allows the agent to build stronger beliefs about the causal structure of the environment.\nIn addition, the process of policy selection relies on the minimization of Expected Free Energy (EFE), which includes a novelty-seeking term that drives exploration. This term quantifies the expected change in beliefs as a result of receiving a new observation. The EFE in this context is:\n$G_\\pi = D_{KL} [q(o_\\tau|\\pi)||p(o_\\tau)] + E_{q(s_\\tau|\\pi)} [H[p(o_\\tau|s_\\tau)]] - E_{p(o_\\tau|s_\\tau)q(s_\\tau|\\pi)} [D_{KL} [q(A|o_\\tau, s_\\tau)||q(A)]]$\nThe third term drives the agent to seek novel observations that maximize the difference between prior and posterior beliefs about the mappings in the A matrix. By doing so, the agent can actively explore and refine its internal model, improving its understanding of how states generate observations."}, {"title": "1.5 Application to Self-Learning Agents", "content": "In this paper, we extend this framework to the design of self-learning agents capable of continually updating their generative models to adapt to changing environments. Specifically, we focus on the learning of the A matrix, as it is essential for the agent's ability to infer the dynamics of its environment. By using Active Inference and Dirichlet-Categorical models, we propose a mechanism for integrating continual learning into agentic systems, allowing agents to optimize their performance over time while responding to novel observations.\nThis ability to learn continually over time ensures that the agent can interact with dynamic environments without requiring constant updates or interventions by a human operator. This capability is especially relevant in domains such as quantitative finance, where strategies can become obsolete over time due to market fluctuations, or in research, where resources constantly evolve in their relevance. In these scenarios, Active Inference provides an abstraction layer that continuously tunes the agent's performance, thus adapting to new challenges in real-time."}, {"title": "2 Active Inference as an Agentic Learning Mechanism", "content": null}, {"title": "2.1 Motivating the Problem", "content": "For every aspect of an agentic workflow, there are elements that need to be optimized over time. Consider an agent that operates within a dynamic environment, where the relationships between states, observations, and actions continuously evolve. If the system lacks the ability to adapt to these changes, its performance will degrade. Traditional methods require manual intervention, continual updates, or predefined structures that can become obsolete.\nHowever, with Active Inference, by simply defining the environment and creating pathways for the agent to act on and receive observations from the environment, we can build agents that are self-sustaining and highly adaptable over time. Active Inference agents can automatically learn new structures and processes as needed without the need for external updates, making them resilient to change."}, {"title": "2.2 Developing the Framework", "content": "This section develops the logic behind the active inference framework used for implementing self-learning within any agentic workflow. While the following discussion focuses on an example research agent, the framework can be generalized to other systems and domains."}, {"title": "2.2.1 Definitions", "content": "Operating in discrete time, a Partially Observable Markov Decision Process (POMDP) models states, ob-servations, policies, and all inputs and outputs of Free Energy. In this framework, the A matrix represents the likelihood mapping from hidden states to observations. The state factors and observation modalities are learned over time rather than pre-specified, allowing the agent to adapt as new data becomes available.\nFor example, in a research process, the state factors may represent different research methods, while the observation modalities represent the usefulness of a research output. Over time, the system refines its understanding of which research methods yield the most useful results in a given context."}, {"title": "2.2.2 Generative Model", "content": "In this framework, the generative model is hierarchical, structured across two levels. This hierarchy is crucial for providing context to the agent, enabling it to understand both the environment it is interacting with and the specific processes within that environment.\nThe top level of the generative model consists of one state factor: the industry, and one observation modality: the industry cue. The purpose of this level is to provide the agent with an understanding of the high-level context essentially, which industry it is interacting with at a given time. The agent observes an industry cue that informs it about the current industry in which the research processes are unfolding. This ensures that the agent can interpret all subsequent observations with the knowledge of the specific industry it is dealing with.\nThe bottom level of the generative model consists of two state factors: the industry and the research process, with three observation modalities:\n\u2022 Research process: Observes which research process is being used.\n\u2022 Outcome: Observes the result of the research process in a given industry (i.e., a mapping between the research process and industry).\n\u2022 Industry cue: Observes the industry cue again, passed down from the top level, to maintain awareness of the industry context.\nThus, the agent is provided with both state and observation information from the top level (industry and its cue), which is passed down to the bottom level to act as a continuous context during its exploration of research processes. When the agent observes data from the research process, it knows which industry these observations are tied to, based on the industry cue provided at the top level. The observation of the research process itself allows the agent to recognize which process is being applied, while the outcome provides information about the result of applying that research process in the given industry.\nThe outcome in this simplified model is a preset simulation that maps each research process to a specific result within the industry. For example, in Industry 1, Research Process 1 may always yield an \"Excellent\" result, while Research Process 2 may yield a \"Good\" result, and so on. In more practical applications, the outcome would be determined directly by interacting with the environment, but in this simplified model, the outcome serves as a simulation to help the agent learn these mappings.\nBy structuring the generative model in this hierarchical way, the agent is capable of maintaining awareness of its industry context while refining its understanding of how different research processes perform within that context. The hierarchical POMDP ensures that observations about industry cues are continuously processed, providing the necessary context for learning the dynamic relationships between research processes and their outcomes."}, {"title": "2.2.3 Generative Process", "content": "The generative process in this model simulates the environment that the agent interacts with. In an ideal scenario, the generative process would involve real-world data being generated by the environment in response to the agent's actions. However, for the purpose of this research model, the generative process is simplified and simulated based on the state-observation mappings present in the A matrix.\nIn this setup, the generative process takes the agent's policy selections and uses them to sample obser-vations from the predefined mappings in the A matrix. Specifically, after the agent selects a policy (which represents its action in terms of selecting a research process), the generative process samples from the A matrix to determine the corresponding outcome for the given research process in the current industry. The A matrix contains the probabilities that govern how observations (i.e., outcomes) are generated based on the agent's belief about the current state (industry and research process).\nFor example, if the agent selects Research Process 1 in Industry 1, the A matrix may indicate that there is a high probability that the outcome will be \"Excellent.\" The generative process samples from this probability distribution, providing the agent with an observation of \"Excellent.\" The agent then uses this observation to update its beliefs about the mappings between research processes and outcomes in the current industry.\nThe generative process is essentially a simulation of how the environment might behave, providing feed-back to the agent about its actions. The agent is tasked with learning the structure of the environment by minimizing Expected Free Energy (EFE). This involves selecting policies that are expected to reduce the agent's uncertainty about the state-observation mappings (i.e., learning the A matrix).\nBecause the agent is providing observations to itself via the sampling of the A matrix, the process of learning becomes one of continuous refinement. Over time, the agent uses the observations it generates (from its interaction with the environment) to improve its internal model, refining its understanding of how different research processes produce outcomes in different industries. This allows the agent to optimize its performance, selecting policies that are expected to maximize the accuracy of its predictions about the environment.\nThus, the generative process in this framework is not only responsible for generating observations but also for driving the agent's learning by simulating feedback loops between the agent's policies and the environment it is modeling."}, {"title": "3 Results", "content": "To demonstrate the continual learning capabilities of the active inference agent, we tested it in two different environments: the first with predefined state-outcome mappings and the second with modified mappings for certain industries to simulate a shift in the environment. We used a scoring mechanism based on the agent's confidence in the correct outcomes, which is calculated from the belief values assigned to each possible outcome in the lowercase 'a' matrix. This matrix represents the agent's internal belief mappings between states (industries and research processes) and observations (outcomes)."}, {"title": "3.1 Scoring Mechanism", "content": "To evaluate the agent's learning performance, we define a scoring mechanism that reflects both the accuracy and confidence of the agent in its internal belief state. The agent's belief about the relationship between the hidden states (e.g., industries and research processes) and observations (e.g., outcomes) is represented by the lowercase 'a' matrix, denoted by $a_{ijk}$, where:\n\u2022 i indexes the possible outcomes (e.g., Excellent, Good, etc.),\n\u2022 j indexes the hidden states (e.g., industry),\n\u2022 k indexes the research processes.\nFor each combination of hidden state j and research process k, the agent assigns a probability $a_{ijk}$ to each outcome i, which reflects the agent's belief in the likelihood of that outcome."}, {"title": "3.1.1 Score Calculation", "content": "The score for each state-process pair (j, k) is calculated based on the agent's confidence in the correct outcome relative to its confidence in the incorrect outcomes. Let $i^*$ denote the index of the correct outcome for the given pair (j, k), and let $i_{max}$ denote the index of the incorrect outcome with the highest belief value (i.\u0435., the highest $a_{ijk}$ where i \u2260 $i^*$).\nThe score $S_{jk}$ for the pair (j, k) is defined as the difference between the agent's belief in the correct outcome and the highest belief in any incorrect outcome:\n$S_{jk} = a_{i^*jk} - \\max_{i\\neq i^*} a_{ijk}.$\nThus, if the agent has a high belief in the correct outcome $a_{i^*jk}$ and low belief in the incorrect outcomes, $S_{jk}$ will be positive. Conversely, if the agent incorrectly assigns higher belief to an incorrect outcome, the score will be negative."}, {"title": "3.1.2 Total Score for an Iteration", "content": "The total score for an iteration t, denoted $S^{(t)}$, is calculated as the sum of normalized scores across all industries j and research processes k:\n$S^{(t)} = \\sum_{j=1}^{16} \\sum_{k=1}^{4} S_{jk}^{norm}$\nThis total score reflects the overall performance of the agent in the current iteration, taking into account how well the agent has learned the correct mappings for each industry and research process."}, {"title": "3.1.3 Negative Scores", "content": "Negative scores occur when the agent's belief in the correct outcome is lower than the belief in an incorrect outcome. For example, if the belief values for Excellent and Good were reversed:\n$a_{11k} = 0.2$, $a_{21k} = 5$, $a_{31k} = 0.1$, $a_{41k} = 0.1$, $a_{51k} = 0.05$,\nthen the score would be:\n$S_{1k} = 0.2 - 5 = -4.8$,\nindicating the agent has mistakenly assigned much higher confidence to the incorrect outcome."}, {"title": "3.1.4 Overall Evaluation", "content": "By calculating and summing the scores across all industries and research processes, we obtain a measure of the agent's overall learning performance and how well it adapts to changes in the environment. This scoring mechanism allows us to assess both the accuracy and confidence of the agent's beliefs about the environment."}, {"title": "3.2 First Environment: Predefined Outcomes", "content": "The first environment consists of predefined state-outcome mappings for each of the 16 industries and their respective research processes. The agent begins by interacting with the environment, learning these mappings over the first 10 trials. The first 10 trials shown in 2) and 3) are both the calculated score per iteration for just state-observation mappings in the 1st industry and state-observation mappings for all industries, respectively."}, {"title": "3.3 Second Environment: First Industry Modified", "content": "The second 20 trials in 2) were carried out by the same active inference agent placed in a new environment in which only the state-observation mappings for the first industry were changed. The mappings for the other 15 industries were the same, so only the score for Industry 1 was calculated. The steep drop-off is caused by an environment change where none of the agents' former beliefs are relevant. The agent is able to learn this new environment in a linearly increasing fashion, but doesn't reach its former maximum."}, {"title": "3.4 Third Environment: More Modifications", "content": "The second 20 trials in 3) were carried out by the active inference agent initially trained in environment 1 being placed in a new environment. Here several state-observation mappings were changed across a much bigger set of industries, representing a bigger regime shift in this environment than in the second environment.\nHere the agent is able to relearn the environment once again, but at a higher rate than in the sec-ond environment. In fact, the agent overtakes its previous maximum score by the 18th trial in the new environment."}, {"title": "4 Conclusion", "content": "The results from the different environments demonstrate key properties of Active Inference agents in terms of adaptation and relearning.\nIn the second environment, where only one industry's mappings were changed, the agent required more iterations to relearn the altered relationships, which suggests that localized changes lead to more gradual adaptation. The slower rate of learning can be attributed to the agent's reliance on prior beliefs about the unchanged aspects of the environment, which can introduce friction when adapting to small, isolated changes. The lower learning rate can also potentially be attributed to the score calculation. Since the score was only being calculated on the 1st industry it didn't have the benefit of ballooning the score due to increased confidence from other industries.\nIn contrast, the third environment, where more industries experienced changes, showed faster adaptation. This can be interpreted as the agent being more flexible when faced with global environmental shifts. With multiple industries changing simultaneously, the agent can globally adjust its model, resulting in faster convergence and even surpassing the peak performance seen in the first environment. This quicker learning rate may also be due to the opposite effect potentially seen in environment two. Since, there are several unchanged mappings they're increasing scores may have ballooned the overall score more than expected.\nThese results suggest that the structure of the environmental changes may impact adaptation rate. The results conclusively show that the active inference agent is able to reassess its views when placed in changing environments and adapt its internal model in response."}, {"title": "5 Discussion", "content": "This paper demonstrates the potential of Active Inference as a framework for creating self-learning agents that continuously adapt to their environment. By engaging in continual interaction with their surroundings and refining internal models, Active Inference agents are able to autonomously adjust to changes over time without requiring manual intervention. This feature makes Active Inference highly applicable in domains where adaptability is crucial, such as research, quantitative finance, and healthcare."}, {"title": "5.1 Application to Quantitative Finance", "content": "In quantitative finance, trading strategies often become obsolete due to shifting market conditions. Using an Active Inference agent, we can build a system where the agent continuously observes market indicators like asset prices, volatility, and economic factors. The state factors could include market conditions and interest rates, while observation modalities would encompass price movements and volume.\nThe agent would learn and update its internal model based on real-time data, adjusting its trading strategies as market conditions evolve. The generative process simulates possible future market scenarios, enabling the agent to select policies that minimize risk and maximize returns. This adaptability makes the agent resilient to rapid market fluctuations, reducing the need for constant manual updates to trading strategies."}, {"title": "5.1.1 Application to Healthcare", "content": "In healthcare, patient data changes over time, requiring constant adaptation in treatment plans. An Active Inference agent could be applied to a clinical decision support system where state factors include patient health and treatment options, and observation modalities could involve test results and symptom reports.\nThe agent would continuously update its model based on incoming patient data, refining its understanding of which treatments are most effective. By simulating different treatment outcomes, the agent can recommend the best course of action, ensuring that treatment strategies evolve as the patient's condition changes. This continual learning leads to more personalized and effective healthcare without constant human intervention."}, {"title": "5.1.2 Future Work", "content": "Moreover, future work could explore more sophisticated generative models and processes, allowing agents not only to learn new mappings but also to discover new state factors and observation modalities. This would enable agents to move beyond pre-defined structures, learning entirely new frameworks as they interact with their environment. Additionally, integrating Active Inference with other AI paradigms, such as large language models or reinforcement learning, could yield even more adaptable and capable agents. Here, active inference agents would serve as a mechanism to increase the temporal effectiveness of other agents they can manipulate.\nActive Inference offers significant potential for real-world applications that require resilience and long-term sustainability. Whether in finance, healthcare, or other fields, this framework has the ability to keep AI systems relevant and adaptive, reducing the need for human intervention and making it a key tool for the future of adaptive artificial intelligence."}, {"title": "6 Data Availability", "content": "Find all relevant code and figures at https://github.com/RPD123-byte/Demonstrating-the-Continual-Learning-Capabilities-and-Practical-Application-of-Discrete-Time-Active. Subroutines for spm-MDP-VB-X and spm-MDP-check among others can be found in the spm12 package."}]}