{"title": "H\u00b3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs", "authors": ["Selim Furkan Tekin", "Fatih Ilhan", "Tiansheng Huang", "Sihao Hu", "Zachary Yahn", "Ling Liu"], "abstract": "Alignment of pretrained LLMs using instruction-based datasets is critical for creating fine-tuned models that reflect human preference. A growing number of alignment-based fine-tuning algorithms and benchmarks emerged recently, fueling the efforts on effective alignments of pre-trained LLMs to ensure helpful, harmless, and honest answers from both open-source and closed-source LLMs. This paper tackles this problem by developing an alignment fusion approach, coined as H\u00b3 Fusion, with three unique characteristics. First, H\u00b3 Fusion ensembles multiple individually aligned LLMs to create a final fine-tuned alignment model with enhanced capabilities beyond those of individual models, delivering robust alignment through promoting helpful, harmless, honest fusion. Second, H\u00b3Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We first freeze the multi-head attention weights of each individual model while tuning the feed-forward network (FFN) layer during alignment fusion. Then we merge the aligned model weights with an expert router according to the type of input instruction and dynamically select a subset of experts that are best suited for producing the output response. Finally, we boost the performance of the resulting H\u00b33Fusion model by introducing gating loss and regularization terms. The former penalizes the selection errors of the expert-router, and the latter mediates the expert weights drifting during fine-tuning and dynamically adjusts the fusion behavior of the resulting model by canalizing the activations on the experts. Extensive evaluations on three benchmark datasets show that H\u00b33Fusion is more helpful, less harmful, and more honest from two aspects: it outperforms each individually aligned model by 11.37%, and it provides stronger robustness compared to the state-of-the-art LLM ensemble approaches by 13.77%. Code is available at https://github.com/sftekin/h3fusion.", "sections": [{"title": "1. Introduction", "content": "The rise of large language models (LLMs) [1, 17, 33, 35] has highlighted the importance of creating AI systems that are reliable, safe, and align with the goals and values of the humans who use them. Being helpful, harmless, and honest becomes the ultimate goal for every LLM [2, 27]. Current approaches showed that fine-tuning the pretrained language models with instructions for one property can affect the other properties [4]. For example, LLMs should be designed to help users effectively, but being too helpful can lead to misinformation due to hallucinations. The problem is heightened when an unsafe prompt may contain dangerous instructions that can lead to violence, discrimination, or harmful behaviors. Recent work has proposed that the fusion of the data set with safety instructions can make the model safer, but excessive safety data make models overly cautious [4]. Thus, alignment for safety is also tricky since models that are aligned for safety can behave extra conservatively and fail to assist users effectively. A similar phenomenon is also observed for the truthfulness alignment [25, 45], showing that both the dataset selection and the fine-tuning process are critical for minimizing the probabilities that the models are misaligned, and hallucinate to produce harmful responses.\nIn this paper, we present an alignment fusion approach to fortifying the efficiency and robustness of LLM alignments for generating Helpful, Harmless, and Honest responses, coined as H\u00b3Fusion. H\u00b3Fusion combines individually aligned models for helpful, harmless or honest responses to multiple downstream tasks by advancing mixture-of-experts based consensus learning with several unique design characteristics. First, we introduce instruction tuning and summarization fusion as two modern ensemble learning techniques in the context of helpful-harmless-honest (H\u00b3) alignment of pretrained LLMs. Through extensive evaluation on the Alpaca-Eval, BeaverTails, and TruthfulQA datasets, we demonstrate that H\u00b3Fusion surpasses the leading LLM ensemble methods and outperforms both individually aligned models and the model alignment using all three types of datasets. Second, we propose a mixture-of-experts (MoE) methodology to combine three independently aligned LLaMA-2 7B models over helpful, harmless or honest datasets respectively, which not only scales the model alignment capacity bot also enhances the computational efficiency by reducing the model parameters down"}, {"title": "2. Related Work", "content": "LLM Alignment. Supervised fine-tuning sets the foundations of alignment by human preference dataset and is vigorously used in instruction tuning of foundational models [46]. On top of supervised fine-tuning, more complex techniques are introduced by reinforcing the model via a separate reward model (RLHF), which is also trained by human annotated datasets [2, 7, 8, 43]. Since this dataset may not be present for each task, in this paper, we focus our evaluation on supervised fine-tuning-based alignment, yet we emphasize that our proposed solution could potentially be extended to RLHF. To the best of our knowledge, our work is the first to perform alignment using an ensemble approach.\nEnsemble Learning in LLMs. Many works have proposed inference-time ensemble methods by exploiting majority voting [10, 23, 40, 41]. The downside of majority voting is the definition of equality between divergent answers. Two threads of research further improve majority voting, one work utilizes the BLEU score as the heuristic to compare answers [20] another is to enhance the BLEU score-based answer combination method by either assigning weights [44] or by creating a debate environment [6, 9, 24, 38]. Due to lengthy and complex prompt strategies of former works, supervised summarization LLM ensemble methods are proposed such as LLM-Blender [18] and TOPLA-Summary [34]. These methods formalize the ensemble as a summarization problem using a seq2seq model.\nMixture-of-Experts. The supervised ensemble techniques, however, require observational dataset to train the ensemble algorithm and all the base models must be active during the inference. Recently, authors of [17] updated standard transformer architecture in [37] by replacing stan-"}, {"title": "3. Problem Definition", "content": "For a task P, let x denote the input prompt and y be the desired output of a dataset denoted by (x, y) \u2208 Dp. In alignment by supervised fine-tuning process, a (x, y) tuple is sampled from the data set to fine-tune an LLM with parameters denoted by \u0177 ~ M\u2084(.|x), where the goal is to make M provide task-aligned responses, that is, make \u0177 similar to y. The model is optimized by finding the best model parameter & that will maximize the joint distribution over the target tokens. The model auto-regressively generates the output sequence and follows cross-entropy loss (LCE) to penalize its parameters:\n$L_{CE} = - \\sum_{t=1}^{T}logp(y_t|Y_{<t-1}, x; \\varphi),$\nwhere T represents the sequence length. That is, we perform causal language modeling, in which the model is trained to predict the next token based on preceding tokens.\nIn the case of multiple datasets and tasks, our goal is to generate an output that will represent the capabilities of each task. Specifically, for the helpfulness, safety, and truthfulness tasks, M\u03c6, \u039c\u03c2, and My are aligned models that are fine-tuned on their respective data sets with the Equation 1. Here we assume that there are datasets for each of the three tasks, denoted by Dtruth, Dhelpful, and Dsafe respectively. We aim to find an optimal ensemble function:\n$O_{best} = argmin \\mathcal{L}(y, \\tilde{y})$\n$s.t. \\tilde{y} = f_{\\theta}(x, M_{\\varphi}, M_{\\varsigma}, M_{\\psi}),$\nwhere @ is the ensemble function parameters and L is the loss representing the dissimilarity between the desired and the generated output. In the following section, we show how we model this function with three different approaches."}, {"title": "4. Ensemble for Multi-task Alignment", "content": "In this section, we present the design of our H\u00b3Fusion method by introducing three alternative designs of multi-task ensemble function fe to generate Helpful, Harmless and Honest (HHH) outputs. The first two represent the ensemble methods of LLMs in the literature, and we adapt them in the context of creating HHH outputs. The third method presents an original design that addresses both the performance generalizability challenge and the computational complexity in creating the H\u00b3Fusion model."}, {"title": "4.1. Ensemble by Instruct Prompting", "content": "The most common and easy-to-apply methodology is to combine the generated outputs of the aligned models with a new prompt and feed it to another language model. This technique is widely used to summarize the points made by the multi-agents in a debate environment and generate a new output based on the candidate's answers [28, 38]. As shown in Figure 1, the prompt contains three elements; the system instruction, the task description, and the responses generated by each aligned model, e.g, \u01771 ~ M\u2084(.|x) represents the response generated by the helpfulness aligned model."}, {"title": "4.2. Ensemble by Fusion Summarization", "content": "One caveat of the ensemble by instruct-prompting (recall Section 4.1) is that it requires lengthy and complex prompts since some instructions may require lengthy outputs suchs as generating a recipe or python script. To address the limited context window and computational complexity concerns, we define our second ensemble function, f\u00b2)(.) by leveraging LLM-TOPLA [34]. Our goal is to enable the summary-based ensemble model, denoted by H\u00b3Fusion-Summary, to scale linearly with the input sequence. One approach is to utilize the sliding window attention pattern [3] to reduce the complexity and increase the context length of ensemble fusion through TOPLA-summary module.\nGiven \u039c\u03c6, \u039c\u03c2, and M\u03c8, let each aligned model (say M) generate the predicted sequence denoted by Zo = {\u01751,..., \u0175\u03c4\u03bf} and T denote the sequence length of the model output of M4, and let Z = {\u2248\u03c6, \u0396\u03c2, \u03c8} denote the collection of predicted sequences of tokens by individually aligned models. The H\u00b3Fusion summary model is optimized by finding the best model parameter 0 that will maximize the joint distribution over the target tokens p(yx, Z; 0). It performs auto-regressive generation using the following cross-entropy loss for a target output y = {W1,...,\u03c9\u03c4}, where T is the sequence length of the ensemble fusion output sequence:\n$L_{SUM} = \\sum_{t=1}^{T}log p(w_t/w_{<t-1}, x, Z; \\theta)$\nWe perform training using Dmix dataset, similar to the H\u00b3Fusion-Instruct, which iteratively updates the parameters using Stochastic Gradient Descent (SGD) through back-propagation. As the training progresses in iterations, the H\u00b3Fusion-summary model learns to generate the correct token sequence by performing fusion on the information provided by each candidate's answer.\nFor long context window, we leverage the attention mechanics in TOPLA, which takes the input sequence in a modified format in which the relation between the candidate's answers and the instruction is stressed in an instruction-format. Concretely, the input sequence is the"}, {"title": "4.3. Ensemble by Mixture of Experts", "content": "In this section we introduce the H\u00b3Fusion-MoE, which is motivated by the following three observations with our experiences of developing H\u00b3Fusion-Instruct and H\u00b3Fusion-Summary. First, the previous approaches demand two-step preparation to create Dmix dataset to perform training. This requires inference on each aligned model for each task asynchronously, i.e., we need to create all the responses by each model for a given instruction. Second, the computational complexity significantly rises when all the base models and the ensemble model are loaded into the same hardware. During forward and backward passes, this problem is exacerbated since all the base models are activated. Third, we are in pursuit of bootstrapping from the expertise of each aligned model in a collaborative way that enhances the individual capabilities of each model beyond those of the aligned models. Therefore, we want to start from the initial parameters of the aligned models and jointly fine-tune them with the minimum complexity and maximum generalization. We aim for a more efficient mechanism that can switch between experts based on the incoming data so that we do not need to activate all component models for each forward pass.\nTo this extent, we take pretrained LLaMA-2 7B [35] as a blueprint and modify its feed forward neural network (FFN) layers by replacing them with Sparsely-Gated MoE layers. This allowed us to scale the FFN layers by the individual experts. Rather than using random initialization, these experts share the same parameters as the individually aligned models, while retaining the original self-attention layers. This way, we only introduce router weights as additional parameters to perform fine-tuning and to balance the behavior, which usually is efficient with only a few iterations. Overall, our MoE optimized ensemble function, coined as f\u00b3)(.), can effectively compare and combine individually and independently aligned component models by creating a router-enhanced MoE layers with gating loss optimization.\nThe attention layer follows the standard self-attention:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$\nwhere the operations are performed in each layer of the Transformer model architecture [37] and Q, K, and V contain query, key, and value vectors for all the tokens. Differently, the FFN layer of LLaMA consists of three weights named as up, gate, and down projection weights denoted by Wup \u2208 Rdxdh, Wgate \u2208 Rdxdh, Wdown \u2208 Rdh\u00d7d. Given an input hidden vector h \u2208 Rd the FFN layer performs:\n$FFN(h) = W_{down} (W_{up}h \\; Swish(W_{gate}h))$\nThe layer outputs d-dimensional hidden vector and Swish is the SwiGLU [29] activation function."}, {"title": "5. Gating Loss", "content": "In this section, we introduce an auxiliary loss term that encourages the H\u00b3Fusion-MoE model to route input tokens to the appropriate experts based on the category of the incoming task. The main intuition can be summarized as follows: when the prompt is unsafe, the experts aligned for generating a harmless response should be activated. Thus, we register a forward hook to each router weight, Wr, in each layer. The hooks accumulate assigned weights to the experts and we take the mean to find the average weight assigned to each expert by dividing it by the number of layers, given by:\n$L_{G} = \\frac{1}{N} \\sum_{j=1}^{n} \\sum_{i=1}^{N}t_{i} log(q_{ij})$\nHere, ti represents the task type of the input, and qij = softmax(Wh) is the weight assigned to the i-th expert (e.g., individually aligned model) at j-th layer. We jointly train the model parameters by adding A * LG to the overall loss, LCE weighted by \u5165, which represents the degree of penalization applied to the model."}, {"title": "6. Regularization Loss", "content": "One of the major concerns with MoE architectures is the overfitting problem during fine-tuning, which is investigated in [48] with extensive experiments on the Super-GLUE benchmark. In the context of helpful, harmless and honest alignment, we model such problem in terms of embedding drift during fine-tuning of the expert layers, and introduce the following regularization on these layers to control the drift.\n$L_{R} = \\sum_{i=1}^{n} \\gamma_i (||W_{up}||_2 + ||W_{down}||_2 + ||W_{gate}||_2)$\nHere the inner term represents the L2 norm of all expert weights of i-th expert, namely the gate, up, and down projections, and Yi is the regularization weight assigned to this expert. Increasing the regularization enhances the generalizability of the experts, causing the embeddings to drift further from the base model.\nBy putting all the loss terms together, we update the parameters of our MoE model by suffering the loss:\n$L = L_{CE} + \\lambda L_G + L_R(\\gamma_1, \\gamma_2, \\gamma_3)$"}, {"title": "7. Experiments", "content": "We validate the effectiveness of our H\u00b3Fusion approach through extensive evaluations on benchmarks representing helpfulness, harmlessness, and honesty. We show that all three ensemble functions under our H\u00b3Fusion framework can efficently improve the performance of individually aligned component models by creating a generic and more balanced fusion model. Furthermore, we examine and report the performance and behavioral changes in our MoE model through ablation studies and sensitivity analysis."}, {"title": "7.1. Dataset and Evaluation Metrics", "content": "The experiments contain three different datasets targeting each type of property. For helpfulness, we use [32] dataset containing the 20,000 instructions and helpful responses The samples are generated in the style of self-instruct shown in [41] using text-davinci-003 which is the instruct-following GPT-3.5 [5]. Following [32], we followed the same prompt structure. This dataset is the test-bed for the helpfulness task but we need to measure to what extent the given answer meets our needs and for that purpose, we employ Alpaca-Eval library [22]. The library compares two responses from different models to the same instruction and selects the preferred response based on its alignment with human preferences, which are simulated using GPT-40 [1]. In our evaluation for helpfulness, we compare the responses given by our models with text-davinci-003 to make a fair comparison, and we report the Win Rate (%) calculated by #samples \u00d7 100. Therefore, a higher win rate indicates that the model is more helpful. Alpaca-Eval uses 805 unseen instructions as test samples.\nIn the case of safety, we use the safe/unsafe samples from the alignment dataset of BeaverTails [15]. The dataset contains 30,207 QA-pairs across 14 potential harm categories. While 27,186 samples in this dataset are used for the alignment, 3,021 of them are used for the testing. During alignment for safety, we only used the safe QA-pairs of the alignment dataset, and in testing, we used only the questions from the test dataset. To measure the harmfulness, we employed a moderation model, beaver-dam-7b, from [15] to classify the model output under 14 categories given unseen malicious instructions. Thus, we define the safety score (%) as the ratio of unsafe output to the total number of samples, represented by #samples \u00d7 100. A lower score indicates a safer model. This scoring method is commonly used in the literature, including in works such as [12-14].\nLastly, [25] introduces the TruthfulQA dataset, which"}, {"title": "7.2. Performance of Ensemble Functions", "content": "Table 2 shows experiments on Alpaca-Eval, BeaverTails, and TrurthfulQA datasets, where we compare the scores of each aligned based model (LLaMA-2 7B) in the pool with the three ensemble learners of H\u00b3Fusion: Summary, Instruct and MoE. Here, the Base Model is the pre-trained LLaMA-2 7B without alignment. The Model IDs of H\u00b3Fusion denote the models in the ensemble set. We set the hyperparameters of the MoE model as \u03bb = 0.001, \u00a51 = 0,2 = 0.0001, \u0443\u0437 = 0, and k = 2 saying two experts is active on each layer. Examining the base model and comparing it with individually aligned models, we observe that the best-performing model for each of the HHH tasks is the one specifically aligned to that task. Cross-evaluation of the"}, {"title": "7.3. Ablation Study", "content": "To further observe the effect of the MoE and auxiliary losses, we execute two ablation studies shown in Table 3 and in Figure 3. First, we align a LlaMA-2 7B model on mixed dataset Dmix and compare its performance with the MoE standard model, with gating loss, and finally gating loss plus regularization loss. During the comparison we kept all the other parameters same, e.g., number of epochs, batch-size, fine-tuning steps, and the training-dataset, Dmix. As shown in Table 3, with the gating loss, the average model performance is comparable to that of the fine-tuned mixed model, with a 4% improvement in safety but a 2% reduction in helpfulness. After finding the best gate loss, to compensate for the loss in helpfulness, we applied regularization solely to the safety expert, setting 2 = 0.001. This decreased the safety of the model by 1.4% while increasing its helpfulness by 4%. With the parameter sweep the model performance can be improved as shown in Table 2.\nFigure 3 report the result of our second ablation study: the effect of gating loss and regularization loss on the MoE model. The first plot shows that gating loss makes the model more helpful, truthful and safe with average performance improvement by 3.34%.  The second figure visualized the activation of the routers' selection in each layer based on incoming data category after we apply the gate loss.\n7.4. Sensitivity Analysis of Hyperparameters\nIn this section we further delve into the performance change of H\u00b3Fusion (MoE) based on its hyperparameters. Figure 5 reports the results. The left most plot shows the performance change as the number of fine-tuning steps performed on Dmix dataset without using any auxiliary loss.We make two observations:\n8. Conclusion\nWe have presented H\u00b3Fusion, an alignment fusion approach to address the challenge of creating a Helpful, Honest, and Harmless alignment of pre-trained large language models. We formulated this problem as a multi-task ensemble fusion to integrate individually aligned task-specific models, aiming to generate more accurate, more helpful, and safer responses to unknown prompt queries. To better motivate the design of our H\u00b3Fusion framework, we first introduce an instruct-fusion ensemble method through instruction tuning and another summary-based fusion method to show the advantages of each and their inherent limitations. We carry our experiences to design the H\u00b3Fusion-MoE method, a mixture-of-experts (MoE) based ensemble fusion approach to compare and combine individually aligned task-specific models, with dual goals: increasing modeling capacity while minimizing fusion computation complexity. We tackle the overfitting problem of MoE architectures by introducing gating loss and regularization terms. We use gating lost to penalize the selection errors of the expert-router, and use regularization loss to mediate the expert weights drifting during fine-tuning. Extensive benchmarking measurements demonstrate that our H\u00b3Fusion approach outperforms each individually aligned model, as well as representative LLM ensemble methods."}]}