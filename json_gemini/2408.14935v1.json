{"title": "Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures", "authors": ["Tomi Silander", "Janne Lepp\u00e4-aho", "Elias J\u00e4\u00e4saari", "Teemu Roos"], "abstract": "We introduce an information theoretic criterion for Bayesian network structure learning which we call quotient normalized maximum likelihood (qNML). In contrast to the closely related factorized normalized maximum likelihood criterion, qNML satisfies the property of score equivalence. It is also decomposable and completely free of adjustable hyperparameters. For practical computations, we identify a remarkably accurate approximation proposed earlier by Szpankowski and Weinberger. Experiments on both simulated and real data demonstrate that the new criterion leads to parsimonious models with good predictive accuracy.", "sections": [{"title": "INTRODUCTION", "content": "Bayesian networks Pearl (1988) are popular models for presenting multivariate statistical dependencies that may have been induced by underlying causal mechanisms. Techniques for learning the structure of Bayesian networks from observational data have therefore been used for many tasks such as discovering cell signaling pathways from protein activity data Sachs et al. (2002), revealing the business process structures from transaction logs Savickas and Vasilecas (2014) and modeling brain-region connectivity using fMRI data Ide et al. (2014).\nLearning the structure of statistical dependencies can be seen as a model selection task where each model is a different hypothesis about the conditional dependencies between sets of variables. Traditional model selection criteria such as the Akaike information criterion (AIC) Akaike (1973) and the Bayesian information criterion (BIC) Schwarz (1978) have also been used for the task, but recent comparisons have not been favorable for AIC, and BIC appears to require large sample sizes in order to identify appropriate structures Silander et al. (2008); Liu et al. (2012). Traditionally, the most popular criterion has been the Bayesian marginal likelihood Heckerman (1995) and its BDeu variant (see Section 2), but studies Silander et al. (2007); Steck (2008) show this criterion to be sensitive to hyperparameters and to yield undesirably complex models for small sample sizes.\nThe information-theoretic normalized maximum likelihood (NML) criterion Shtarkov (1987); Rissanen (1996) would otherwise be a potential candidate for a good criterion, but its exact calculation is likely to be prohibitively expensive. In 2008, Silander et al. introduced a hyperparameter-free, NML inspired criterion called the factorized NML (fNML) Silander et al. (2008) that was shown to yield good predictive models without such sensitivity problems. However, from the structure learning point of view, fNML still sometimes appears to yield overly complex models. In this paper we introduce another NML related criterion, the quotient NML (qNML) that yields simpler models without sacrificing predictive accuracy. Furthermore, unlike fNML, qNML is score equivalent, i.e., it gives equal scores to structures that encode the same independence and dependence statements. Like other common model selection criteria, qNML is also consistent.\nWe next briefly introduce Bayesian networks and review the BDeu and fNML criteria and then introduce the qNML criterion. We also summarize the results for 20 data sets to back up our claim that qNML yields parsimonious models with good predictive capabilities. The experiments with artificial data generated from real-world Bayesian networks demonstrate the capability of our score to quickly learn a structure close to the generating one."}, {"title": "BAYESIAN NETWORKS", "content": "Bayesian networks are a general way to describe the dependencies between the components of an n-dimensional random data vector. In this paper we only address the case in which the component Xi of the data vector X = (X1,..., Xn) may take any of the discrete values in a set {1,...,ri}. Despite denoting the values with small integers, the model will treat each Xi as a categorical variable."}, {"title": "Likelihood", "content": "A Bayesian network B = (G,\u03b8) defines a probability distribution for X. The component G defines the structure of the model as a directed acyclic graph (DAG) that has exactly one node for each component of X. The structure G = (G1, ..., Gn) defines for each variable/node Xi its (possibly empty) parent set Gi, i.e., the nodes from which there is a directed edge to the variable Xi.\nGiven a realization x of X, we denote the sub-vector of x that consists of the values of the parents of Xi in x by Gi(x). It is customary to enumerate all the possible sub-vectors Gi(x) from 1 to qi = \\prod_{l \\in G_i} r_l. In case Gi is empty, we define qi = 1 and P(Gi(x) = 1) = 1 for all vectors x.\nFor each variable Xi there is a qi \u00d7 ri table \u03b8i of parameters whose kth column on the jth row \u03b8ijk defines the conditional probability P(Xi = k | Gi(x) = j; \u03b8) = \u03b8ijk. With structure G and parameters \u03b8, we can now express the likelihood function of the model as\n$P(x|G, \\theta) = \\prod_{i=1}^{n} P(x_i | G_i(x); \\theta_i) = \\prod_{i=1}^{n} \\theta_{iG_i(x)x_i}$"}, {"title": "Bayesian Structure Learning", "content": "Score-based Bayesian learning of Bayesian network structures evaluates the goodness of different structures G using their posterior probability P(G|D,\u03b1), where \u03b1 denotes the hyperparameters for the model parameters \u03b8, and D is a collection of N n-dimensional i.i.d. data vectors collected to a N \u00d7 n design matrix. We use the notation Di to denote the ith column of the data matrix and the notation DV to denote the columns that correspond to the variable subset V. We also write Di,Gi for D{i}UGi and denote the entries of the column i on the rows on which the parents Gi contain the value configuration number j by Di,Gi=j, j \u2208 {1, ..., qi}.\nIt is common to assume the uniform prior for structures, in which case the objective function for structure learning is reduced to the marginal likelihood P(D|G, \u03b1). If the model parameters \u03b8ij are further assumed to be independently Dirichlet distributed only depending on i and Gi and the data D is assumed to have no missing values, the marginal likelihood can be decomposed as\n$P(D|G, \\alpha) = \\prod_{i=1}^{n} \\prod_{j=1}^{q_i} P(D_{i,G_i=j}; \\alpha) = \\prod_{i=1}^{n} \\prod_{j=1}^{q_i} \\int P(D_{i,G_i=j}|\\theta_{ij})P(\\theta_{ij}; \\alpha) d\\theta_{ij}$"}, {"title": "Problems, Solutions and Problems", "content": "Finding satisfactory Dirichlet hyperparameters for the Bayesian mixture above has, however, turned out to be problematic. Early on, one of the desiderata for a good model selection criterion was that it is score equivalent, i.e., it would yield equal scores for essentially equivalent models Verma and Pearl (1991). For example, the score for the structure X1 \u2192 X2 should be the same as the score for the model X2 \u2192 X1 since they both correspond to the hypothesis that variables X1 and X2 are statistically dependent on each other. It can be shown Heckerman et al. (1995) that to achieve this, not all the hyperparameters \u03b1 are possible and for practical reasons Buntine Buntine (1991) suggested a so-called BDeu score with just one hyperparameter \u03b1 \u2208 R++ so that \u03b1ijk ~ Dir(\\frac{\\alpha}{q_ir_i}, ..., \\frac{\\alpha}{q_ir_i}). However, it soon turned out that the BDeu score was very sensitive to the selection of this hyperparameter Silander et al. (2007) and that for small sample sizes this method detects spurious correlations Steck (2008) leading to models with suspiciously many parameters.\nRecently, Suzuki Suzuki (2017) discussed the theoretical properties of the BDeu score and showed that in certain settings BDeu has the tendency to add more and more parent variables for a child node even though the empirical conditional entropy of the child given the parents has already reached zero. In more detail, assume that in our data D the values of Xi are completely determined by variables in a set Z, so that the empirical entropy HN(Xi|Z) is zero. Now, if we can further find one or more variables, denoted by Y, whose values are determined completely by the variables in Z, then BDeu will prefer the set ZUY over Z alone as the parents of Xi. Suzuki argues that this kind of behavior violates regularity in model selection as the more complex model is preferred over a simpler one even though it does not fit the data any better. The phenomenon seems to stem from the way the hyperparameters for the Dirichlet distribution are chosen in BDeu as using Jeffreys' prior, \u03b8ijk ~ Dir(\\frac{1}{q_ir_i}, ..., \\frac{1}{q_ir_i}), does not suffer from this anomaly. However, using Jeffreys' prior causes marginal likelihood score not to be score equivalent. In Section 3.4, we will give the formal definition of regularity and state that qNML is regular. In addition, we provide a proof of regularity for fNML criterion, which has not appeared in the literature before. The detailed proofs can be found in Appendix B in the Supplementary Material.\nA natural solution to avoid parameter sensitivity of BDeu would be to use a normalized maximum likelihood (NML) criterion Shtarkov (1987); Rissanen (1996), i.e., to find the structure G that maximizes\n$P_{NML}(D; G) = \\frac{P(D|\\hat{\\theta}(D; G))}{\\sum_{D'} P(D'|\\hat{\\theta}(D'; G))}$ where \\hat{\u03b8} denotes the (easy to find) maximum likelihood parameters and the sum in the denominator goes over all the possible N \u00d7 n data matrices. This information-theoretic NML criterion can be justified from the minimum description length point of view Rissanen (1978); Gr\u00fcnwald (2007). It has been shown to be robust with respect to different data generating mechanisms where a good choice of prior is challenging, see Eggeling et al. (2014); M\u00e4\u00e4tt\u00e4 et al. (2016). While it is easy to see that the NML criterion satisfies the requirement of giving equal scores to equal structures, the normalizing constant renders the computation infeasible.\nConsequently, Silander et al. Silander et al. (2008) suggested solving the BDeu parameter sensitivity problem by using the NML code for the column partitions, i.e., changing the Bayesian mixture in equation (2) to\n$P_{NML}(D_{i,G_i=j}; G) = \\frac{P(D|\\hat{\\theta}(D_{i,G_i=j};G))}{\\sum_{D'} P(D'|\\hat{\\theta}(D'; G))}$ where D' \u2208 {1,...,ri}|Di,Gi=j|. The logarithm of the denominator is often called the regret, since it indicates the extra code length needed compared to the code length obtained using the (a priori unknown) maximum likelihood parameters. The regret for PNML depends only on the length N of the categorical data vector with r different categorical values,\n$reg(N,r) = log \\sum_{D\\in\\{1,...,r\\}^N} P(D|\\hat{\\theta}(D)).$ While the naive computation of the regret is still prohibitive, Silander et al. approximate it efficiently using a so-called Szpankowski approximation Kontkanen et al. (2003):\n$reg(N,r) \\approx \\frac{\\sqrt{2r}}{\\sqrt{3\\pi N}} \\frac{\\Gamma(\\frac{r}{2})}{\\Gamma(\\frac{r+1}{2})} + (\\frac{r-1}{2}) log(\\frac{N}{2}) - \\frac{1}{2}log(\\frac{r}{3}) + \\frac{r^2 \\Gamma^2(\\frac{r}{2})}{9\\Gamma^2(\\frac{r+1}{2})} + \\frac{2r^3+3r^2 - 2r + 3}{36N}$ However, equation (6) is derived only for the case where r is constant and N grows. While with fNML it is typical that N is large compared to r, an approximation for all ranges of N and r derived by Szpankowski and Weinberger Szpankowski and Weinberger (2012) can also be used:\n$reg(N,r) \\approx N(\\log \\alpha + (\\alpha + 2) \\log C_\\alpha) - \\frac{r}{2}\\log(Ca+2)$ where \u03b1 = \\frac{r}{N} and Ca = \\frac{1}{2} + \\sqrt{\\frac{1}{4} + \\frac{1}{\\alpha N}}. These approximations are compared in Table 1 to the exact regret for various values of N and r. For a constant N, equation (6) provides a progressively worse approximation asr grows. Equation (7) on the other hand is a good approximation of the regret regardless of the ratio of N and r. In our experiments, we will use this approximation for implementation of the qNML criterion.\nfNML solves the parameter sensitivity problem and yields predictive models superior to BDeu. However, the criterion does not satisfy the property of giving the same score for models that correspond to the same dependence statements. The score equivalence is usually viewed desirable when DAGs are considered only as models for conditional independence, without any causal interpretation. Furthermore, the learned structures are often rather complex (see Figure 1) which also hampers their interpretation. The quest for a model selection criterion that would yield more parsimonious, easier to interpret, but still predictive Bayesian networks structures is one of the main motivations for this work."}, {"title": "QUOTIENT NML SCORE", "content": "We will now introduce a quotient normalized maximum likelihood (qNML) criterion for learning Bayesian network structures. While equally efficient to compute as BDeu and fNML, it is free from hyperparameters, and it can be proven to give equal scores to equivalent models. Furthermore, it coincides with the actual NML score for exponentially many models. In our empirical tests it produces models featuring good predictive performance with significantly simpler structures than BDeu and fNML.\nLike BDeu and fNML, qNML can be expressed as a product of n terms, one for each variable, but unlike the other two, it is not based on further partitioning the corresponding data column\n$\\delta^{qNML}(D; G) := \\sum_{i=1}^{n} \\delta^{INML}(D_i; G) := \\sum_{i=1}^{n} \\log \\frac{P_{NML}(D_{i,G_i}; G)}{P_{NML}(D_{G_i}; G)}$ The trick here is to model a subset of columns as though there were no conditional independencies among the corresponding variables S \u2286 X. In this case, we can collapse the \u03a0xi\u2208S ri value configurations and consider them as values of a single variable with \\prod_{xi \\in S} r_i different values which can then be modeled with a one-dimensional PNML code. The sqNML score does not necessarily define a distribution for D, but it is easy to verify that it coincides with the NML score for all networks that are composed of fully connected components. The number of such networks is lower bounded by the number of nonempty partitions of a set of n elements, i.e., the nth Bell number.\nWe are now ready to prove some important properties of the qNML score."}, {"title": "qNML Is Score Equivalent", "content": "qNML yields equal scores for network structures that encode the same set of independencies. Verma and Pearl Verma and Pearl (1991) showed that the equivalent networks are exactly those which a) are the same when directed arcs are substituted by undirected ones and b) which have the same V-structures, i.e. the variable triplets (A, B, C) where both A and B are parents of C, but there is no arc between A and B (in either direction). Later, Chickering Chickering (1995) showed that all the equivalent network structures, and only those structures, can be reached from each other by reversing, one by one, the so-called covered arcs, i.e. the arcs from node A to B, for which B's parents other than A are exactly A's parents (GB = {A} U GA).\nWe will next state this as a theorem and sketch a proof for it. A more detailed proof appears in Appendix A in the Supplementary Material.\nTheorem 1. Let G and G' be two Bayesian network structures that differ only by a single covered arc reversal, i.e., the arc from A to B in G has been reversed in G' to point from B to A, then\n$\\delta^{qNML}(D;G) = \\delta^{qNML}(D; G').$ Proof. Now the scores for structures can be decomposed as $\\delta^{qNML}(D; G) = \\sum_{i=1}^{n} S^{INML}(D_i;G)$ and $\\delta^{qNML}(D; G') = \\sum_{i=1}^{n} S^{INML}(D_i;G')$. Since only the terms corresponding to the variables A and B in these sums are different, it is enough to show that the sum of these two terms are equal for G and G'. Since we can assume the data to be fixed we lighten up the notation and write PNML(i, Gi) := PNML(Di,Gi;G) and"}, {"title": "qNML is Consistent", "content": "One important property possessed by nearly every model selection criterion is consistency. In our context, consistency means that given a data matrix with N samples coming from a distribution faithful to some DAG G, the qNML will give the highest score to the true graph G with a probability tending to one as N increases. We will show this by first proving that qNML is asymptotically equivalent to the widely used BIC criterion which is known to be consistent Schwarz (1978); Haughton (1988). The outline of this proof follows a similar pattern to that in Silander et al. (2010) where the consistency of fNML was proved.\nThe BIC criterion can be written as\n$BIC(D; G) = \\sum_{i=1}^{n} log P(D_i | G_i) - \\frac{1}{2} q_i (r_i - 1) log N,$ where DiGi denotes the maximum likelihood parameters of the conditional distribution of variable i given its parents in G.\nSince both the BIC and qNML scores are decomposable, we can focus on studying the local scores. We will next show that, asymptotically, the local qNML score equals the local BIC score. This is formulated in the following theorem:\nTheorem 2. Let ri and qi denote the number of possible values for variable Xi and its possible configurations of parents Gi, respectively. As N\u2192 \u221e,\n$\\delta_i^{INML}(D; G) = log P(D_i | \\hat{\\theta}_i) - \\frac{q_i(r_i - 1)}{2}log N.$ In order to prove this, we start with the definition of qNML and write\n$\\delta_i^{INML}(D; G) = log \\frac{P(D_{i,G_i} | \\hat{\\theta}_{i,G_i})}{P(D_{G_i} | \\hat{\\theta}_{G_i})} - (reg(N, q_ir_i) - reg(N, q_i)).$ By comparing the equations (9) and (10), we see that proving our clam boils down to showing two things: 1) the terms involving the maximized likelihoods are equal and 2) the penalty terms are asymptotically equivalent. We will formulate these as two lemmas.\nLemma 1. The maximized likelihood terms in equations (9) and (10) are equal:\n$\\frac{P(D_{i,G_i} | \\hat{\\theta}_{i,G_i})}{P(D_{G_i} | \\hat{\\theta}_{G_i})} = P(D_i | \\hat{\\theta}_i).$ Proof. We can write the terms on the left side of the equation as\n$\\frac{P(D_{i,G_i} | \\hat{\\theta}_{i,G_i})}{P(D_{G_i} | \\hat{\\theta}_{G_i})} = \\frac{\\prod_{j,k} (\\frac{N_{ijk}}{N_{ij}})^{N_{ijk}}}{\\prod_j (\\frac{N_{ij}}{N})^{N_{ij}}}, and P(D_i | \\hat{\\theta}_i) = \\prod_j (\\frac{N_{ij}}{N})^{N_{ij}}.$ Here, Nijk denotes the number of times we observe Xi taking value k when its parents are in jth configuration in our data matrix D. Also, Nij = \u2211k Nijk (and \u03a3k,j Nijk = N for all i). Therefore,\n$\\frac{P(D_{i,G_i} | \\hat{\\theta}_{i,G_i})}{P(D_{G_i} | \\hat{\\theta}_{G_i})} = \\frac{\\prod_{j,k} (\\frac{N_{ijk}}{N_{ij}})^{N_{ijk}}}{\\prod_j (\\frac{N_{ij}}{N})^{N_{ijk}}} = \\prod_{j,k} (\\frac{N_{ijk}}{N_{ij}})^{N_{ijk}} \\prod_j (\\frac{N}{N_{ij}})^{N_{ijk}} = \\prod_j P(D_i | \\hat{\\theta}_i).$ Next, we consider the difference of regrets in (10) which corresponds to the penalty term of BIC. The following lemma states that these two are asymptotically equal:\nLemma 2. As N \u2192 \u221e,\n$reg(N, q_ir_i) - reg(N, q_i) = \\frac{q_i(r_i - 1)}{2}log N + O(1).$ Proof. The regret for a single multinomial variable with m categories can be written asymptotically as\n$reg(N, m) = \\frac{m-1}{2} log N + O(1).$ For the more precise statement with the underlying assumptions (which are fulfilled in the multinomial"}, {"title": "qNML Equals NML for Many Models", "content": "The fNML criterion can be seen as a computationally feasible approximation of the more desirable NML criterion. However, the fNML criterion equals the NML criterion only for the Bayesian network structure with no arcs. It can be shown that the qNML criterion equals the NML criterion for all the networks G whose connected components are tournaments (i.e., complete directed acyclic subgraphs of G).\nTheorem 3. If G consists of C connected components (G1,...,GC) with variable sets (V1,...,VC), then log PNML(D; G) = sqNML(D;G) for all data sets D."}, {"title": "qNML is Regular", "content": "Suzuki Suzuki (2017) defines regularity for a scoring function QN (X | Y) as follows:\nDefinition 1. Assume HN (X | Y') \u2264 HN (X | Y), where Y' \u2282 Y. We say that QN(\u00b7|\u00b7) is regular if QN (X | Y') \u2265 QN (X | Y).\nIn the definition, N denotes the sample size, X is some random variable, Y denotes the proposed parent set for X, and HN (\u00b7 | \u00b7) refers to the empirical conditional entropy. Suzuki Suzuki (2017) shows that BDeu violates this principle and demonstrates that this can cause the score to prefer more complex networks even though the data do not support this. Regular scores are also argued to be computationally more efficient when applied with branch-and-bound type algorithms for Bayesian network structure learning Suzuki and Kawahara (2017).\nBy analyzing the penalty term of the qNML scoring function, one can prove the following statement:\nTheorem 4. qNML score is regular."}, {"title": "EXPERIMENTAL RESULTS", "content": "We empirically compare the capacity of qNML to that of BIC, BDeu (\u03b1 = 1) and fNML in identifying the data generating structures, and producing models that are predictive and parsimonious. It seems that none of the criteria uniformly outperform the others in all these desirable aspects of model selection criteria."}, {"title": "Finding Generating Structure", "content": "In our first experiment, we took five widely used benchmark Bayesian networks\u00b3, sampled data from them, and tried to learn the generating structure with the different scoring functions using various sample sizes. We used the following networks: Asia (n = 5, 8 arcs), Cancer (n = 5, 4 arcs), Earthquake (n = 5, 4 arcs), Sachs (n = 11, 17 arcs) and Survey (n = 6, 6 arcs). These networks were picked in order to use the dynamic programming based exact structure learning Silander and Myllym\u00e4ki (2006) which limited the number n of variables to less than 20. We measured the quality of the learned structures using structural Hamming distance (SHD) Tsamardinos et al. (2006).\nFigure 3 summarizes the SHD results in all networks by showing the average rank for each score. The ranking was done by giving the score with the lowest SHD rank 1 and the worst one rank 4. In case of ties, the methods with the same SHD were given the same rank. The shown results are averages computed from 5000 values (5 networks, 1000 repetitions). From this, we can see that qNML never has the worst average ranking, and it has the best ranking with sample sizes greater than 300. This suggests that qNML is overall a safe choice in structure learning, especially with moderate and large sample sizes."}, {"title": "Prediction and Parsimony", "content": "To empirically compare the model selection criteria, we took 20 UCI data sets Lichman (2013) and ran train and test experiments for all of them. To better compare the performance over different sample sizes, we picked different fractions (10%, 20%,..., 90%) of the data sets as training data and the rest as the test data. This was done for 1000 different permutations of each data set. The training was conducted using the dynamic programming based exact structure learning algorithm.\nWhen predicting P(dtest|Dtrain, S, \u03b8) with structures S learned by the BDeu score, we used the Bayesian predictive parameter values (BPP) \u03b8ijk \u221d Nijk + \\frac{\\alpha}{r_i q_i}. In the spirit of keeping the scores hyperparameter-free, for structures learned by the other model selection criteria, we used the sequential predictive NML (sNML) parametrization \u03b8ijk \u221d e(Nijk)(Nijk + 1), where e(n) = (n+1)\u22121 as suggested in Rissanen and Roos (2007).\nFor each train/test sample, we ranked the predictive performance of the models learned by the four different scores (rank 1 being the best and 4 the worst). Table 2 features the average rank for different data sets, the average being taken over 1000 different train/test samples for each 9 sample sizes. BIC's bias for simplicity makes it often win (written bold in the table) with small sample sizes, but it performs worst (underlined) for the larger sample sizes (for the same reason), while fNML seems to be good for large sample sizes. The striking feature about the qNML is its robustness. It is usually between BIC and fNML for all the sample sizes making it a \"safe choice\". This can be quantified if we further average the columns of Table 2, yielding the average ranks of 2.95, 2.57, 2.10, and 2.37, with standard deviations 0.49, 0.90, 0.76, and 0.43. While fNML achieves on average the best rank, the runner-up qNML has the lowest standard deviation.\nFigure 1 shows how fNML still sometimes behaves strangely in terms of model complexity as measured by the number of parameters in the model. qNML, instead, appears to yield more parsimonious models. To study the concern of fNML producing too complex models for small sample sizes, we studied the number of parameters in models produced by different scores when using 10% of each data set for structure learning.\nLooking at the number of parameters for the same 20 data sets again features BIC's preference for simple models (Table 3). qNML usually (19/20) yields more parsimonious models than fNML that selects the most complex model for 7 out of 20 data sets."}, {"title": "CONCLUSION", "content": "We have presented qNML, a new model selection criterion for learning structures of Bayesian networks. While being competitive in predictive terms, it often yields significantly simpler models than other common model selection criteria other than BIC that has a very strong bias for simplicity. The computational cost of qNML equals the cost of the current state-of-the-art criteria. The criterion also gives equal scores for models that encode the same independence hypotheses about the joint probability distribution. qNML also coincides with the NML criterion for many models. In our experiments, the qNML criterion appears as a safe choice for a model selection criterion that balances parsimony, predictive capability and the ability to quickly converge to the generating model."}]}