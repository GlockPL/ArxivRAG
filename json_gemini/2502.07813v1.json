{"title": "CryptoX : Compositional Reasoning Evaluation of Large Language Models", "authors": ["Jiajun Shi", "Chaoren Wei", "Liqun Yang", "Zekun Moore Wang", "Chenghao Yang", "Ge Zhang", "Stephen Huang", "Tao Peng", "Jian Yang", "Zhoufutu Wen"], "abstract": "The compositional reasoning ability has long been regarded as critical to the generalization and\nintelligence emergence [Li et al., 2024, Wang et al., 2024] of large language models (LLMs). How-\never, despite numerous reasoning-related benchmarks [Gui et al., 2024, Bill Yuchen Lin, 2024, Ma\net al., 2024], the compositional reasoning capacity of LLMs is rarely studied or quantified in the\nexisting benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for\nthe first time, combines existing benchmarks and cryptographic, to quantify the compositional\nreasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which inte-\ngrates these principles into several benchmarks for systematic evaluation. We conduct detailed\nexperiments on widely used open-source and closed-source LLMs using CryptoBench, reveal-\ning a huge gap between open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional\nreasoning, involving subproblem decomposition, subproblem inference, and summarizing\nsubproblem conclusions. Through analysis based on CryptoBench, we highlight the value\nof independently studying compositional reasoning and emphasize the need to enhance the\ncompositional reasoning abilities of LLMs.", "sections": [{"title": "1. Introduction", "content": "Compositional reasoning (CR) refers to the ability to break down complex problems into simpler\ncomponents and then use those components to form new ideas [Hou et al., 2023]. Existing\nresearch works [Wang et al., 2024, Hou et al., 2023, Cheng et al., 2024] have pointed out that\ncompositional reasoning plays a key role in the generalization and intelligence emergence of\nLLMs. Quantifying the compositional reasoning ability and behavior of LLMs helps reveal how\nthey transfer knowledge and skills from pretraining and alignment data to solve new problems,\nand uncover patterns of emergent generalization [Hendrycks et al., 2020b] as the size of LLMs\nincreases.\nHowever, existing reasoning-related benchmarks are either tightly coupled with specific\ndomains [Cobbe et al., 2021, Hendrycks et al., 2021, Han et al., 2022] or pursuing orthogonality\nof pretraining knowledge [Gui et al., 2024, Ma et al., 2024]. As a result, despite numerous\nexisting reasoning-related benchmarks, the CR capabilities of LLMs have not been well studied"}, {"title": "2. CryptoX: Compositional Reasoning Evaluation Method", "content": "or quantified [Hou et al., 2023]. Previous research work [Wang et al., 2024] emphasizes the\nimportance of CR capabilities by showing how pre-trained models generalize to unseen data\nprocessing under toy experiment settings. But these analysis based on toy data suffer from\ngeneralizing to LLMs, as the nature of toy data differs significantly from text pretraining data [Ma\net al., 2024], which exhibit more diverse reasoning patterns.\nTo address the gap in evaluating CR capabilities, we propose CryptoX, inspired by crypto-\ngraphic techniques [Mushtaq et al., 2017]. CryptoX flexibly transforms existing benchmarks\ninto CryptoBench using instruction encryption and instruction transformation. Instruction\nencryption randomly encodes part of each instruction in the benchmarks using a given codebook.\nInstruction transformation defines additional projection rules from the original answer to the\nCryptoX answer, e.g. the original correct choice answers in MMLU [Hendrycks et al., 2020a]\nrequire an additional Numeric Transformation operation(A \u2192 1, B \u2192 2, . . . ) to be viewed correct\nin Crypto-MMLU. All the additional rules for instruction encryption and transformation are\nclearly stated in the given concatenated instructions. By incorporating instruction encryption\nand instruction transformation, CryptoBench benchmarks aim to assess LLM's CR capabilities\nand reveal LLM's inner mechanism of CR in a flexible manner. We further conduct Mechanistic\nInterpretability experiments on analysis of LLMs' neuron activation and inner workings via\nlogit lens [Nostalgebraist, 2020] to provide more insights about CR behaviour.\nGiven the results of CryptoBench and related mechanical interpretability experiments, we\nshare several key insights about LLMs' compositional reasoning: (1) Most existing LLMs have\nweak CR abilities, and the proposed CryptoBench can measure the CR ability gap between\ndifferent LLMs. (2) The CR ability of the model is influenced by various factors, such as model\nsize, architecture, and other relevant factors. (3) The probing experiments indicate that the\nLLMs summarize the reasoning results of the subtasks to obtain the answer to the CR problem,\nemphasizing the importance of the CryptoBench for evaluating the CR reasoning abilities. (4)\nThe layers of LLMs exhibit a clear hierarchical pattern of executing different subtasks in different\nlayers and then aggregating for compositional reasoning.\nThe contributions of this paper are as follows:\n\u2022 We propose CryptoX, a flexible evaluation method for LLMs' compositional reasoning.\n\u2022 We build CryptoBench and evaluate LLMs' compositional reasoning ability based on it.\n\u2022 We reveal the internal mechanism of LLMs' compositional reasoning by analyzing LLMs'\nneuron activation and inner workings via logit lens."}, {"title": "2.1. Overview", "content": "CryptoX presents a novel approach to evaluate LLMs' compositional reasoning ability by\nintegrating existing benchmarks with cipher-based transformations. The method first encodes\nspecific words in prompts into new ciphered characters and explicitly includes encoding rules\nwithin instructions. To further increase difficulty, some instructions require multi-step reasoning,\nguiding models to follow structured solution steps. LLMs must first decode ciphered characters\nbefore answering original questions, ensuring a rigorous assessment of compositional reasoning.\nBuilding upon this framework, we construct CryptoBench, a benchmark set that systematically\napplies these principles for evaluation."}, {"title": "2.2. Task Definition and Methodology", "content": "Given a benchmark B = {x\u2081,x\u2082 , . . . , xN }, where x\u012b is a prompt consisting of an instruction and\na question Q, B contains N data points. To transform the existing prompt into a compositional\nreasoning prompt, we define a set of transformation rules, where each data point undergoes a\nseries of transformations. Specifically, the transformation process can be described as:\nx' = r\u2098 \u25cb r\u2098\u208b\u2081 0\uff65\uff65 \u25cb r\u2081(x),  (1)\nwhere ri denotes the ith transformation rule, and \u25cb represents the composition of trans-\nformations. Based on our formal definitions, we explore two implementation approaches of"}, {"title": "3. Experiment", "content": "r: instruction encryption, which encodes parts of the prompt, and instruction transforma-\ntion, which restructures it. These transformations enable the creation of diverse compositional\nreasoning tasks.\nInstruction Encryption Specifically, as shown in Figure 2, Instruction Encryption applies three\nencoding rules to the prompt, with its detailed process described in Algorithm 1. It allows\nencoding any number of words in the prompt, ensuring that the encoded prompts, generated\nby flexible self-defined rules, are unlikely to overlap with the pre-training corpus. For simplicity,\nwe use emoji shuffle as an exemplifying encoding rule in the following sections and detailed\nencoding rules are explained in Appendix A.\nInstruction Transformation The vanilla approaches involve prompting the LLM to provide an\nanswer in the format of \u201cAnswer: A\u201d for the multi-choice question tasks. To further evaluate the\ncompositional reasoning capabilities of LLMs in handling OOD scenarios, as shown in Figure\n2, we establish instruction transformation to further increase the number of reasoning hops.\n(1) Numeric Transformation: Based on Qe, we perform numeric transformation. For example,\nmapping \"A \u2192 1, B \u2192 2, C \u2192 3,D \u2192 4\" will require the LLM to answer \u201cAnswer: 1\u201d if the\nanswer is \u201cAnswer: A\u201d, which forces the LLM to perform further reasoning after obtaining the\noriginal answer. (2) Alpha Transformation: Additionally, the task can be made more complex by\nrequiring the LLM to provide both the numerical answer and the first alphanumeric character\nof the corresponding answer content. For example, if the original answer is \u201cAnswer: A\u201d and\nthe answer content is \u201cHappiness\u201d, the LLM would output \u201cAnswer: 1 H\u201d."}, {"title": "C.4. The Performance of Base and Instruct LLMs", "content": "The performance of the base model is generally lower than that of the instruct model, indicating\nthat instruction fine-tuning can enhance the model's compositional reasoning ability."}, {"title": "C.5. Thr Effect of Different Architectures", "content": "Models with non-mainstream architectures perform even worse than smaller models like\nQwen2.5 and Llama-3.1, suggesting that the model architecture is an important factor affecting\ncompositional reasoning ability."}, {"title": "H. Reasoning Stage Analysis", "content": "We are studying the functionality of each layer during LLM inference. We have selected the most likely output tokens for each layer's\nhidden state. Please check the activation of the hidden state's output for the following tokens and summarize in a single sentence\nwhat the layer's function is. Don't list examples of words.\nThe activation format is token<tab>activation. Activation values range from 0 to 10. A layer finding what it's looking for is represented\nby a non-zero activation value. The higher the activation value, the stronger the match. We have provided the input prompt for the\nLLM. You can use this input prompt as a reference to evaluate the functionality of each layer.\nLayer 0\n<start>\n'\\t1\ndisappearing\\t6\nearing\\t10\naping\\t9\nthe\\t4\naffecting\\t3\n<end>\nExplanation of Layer 0 behavior:the main thing this layer does is to focus on present tense verbs ending in 'ing'\nLayer 1\n<start>\narrest\\t10\nigo\\t9\n<end>\nExplanation of Layer 1 behavior:the main thing this layer does is to find words related to physical medical conditions\nLayer 2\n<start>\ntogether\\t3\nness\\t7\ntown\\t1\nwe\\t2\n're\\t4\nall\\t3\nin\\t7\nthis\\t10\ntogether\\t5\n<end>\nExplanation of Layer 2 behavior:the main thing this layer does is to focus on phrases related to community"}, {"title": "G. Case Studies", "content": "To validate the effectiveness of the evaluation methodology in reflecting model generalization\ncapabilities and to investigate prevalent issues such as data leakage and model overfitting\nfollowing the public release of evaluation sets across various domains, we introduce a case study\nmodule. This module aims to provide comprehensive empirical evidence.\nThe study focuses on the Qwen2.5-72B-instruct model, with particular emphasis on its perfor-\nmance discrepancies between the Crypto-MMLU and Crypto-MMLU-Alpha datasets. Notably,\nthe Crypto-MMLU-Alpha dataset is constructed through manual partitioning of the original\nCrypto-MMLU into two subtasks: problem decoding and question answering. Consequently,\nwe refer to tasks on Crypto-MMLU-Alpha as two-stage tasks and those on Crypto-MMLU as\nsingle-stage tasks."}, {"title": "G.1. CryptoBench Effectively Evaluates Model Generalization Capabilities", "content": "As illustrated in Figure 34, the model exhibits significant deficiencies in decoding performance\nwhen directly handling raw problems within two-stage tasks. Specifically, errors occurring\nduring problem decoding phase lead to subsequent answers being generated based on mis-\ninterpretations, resulting in inaccurate responses. In contrast, through stepwise execution of\ndecoding and answering subtasks in single-stage tasks, the model achieves accurate decoding\noutcomes and consequently produces correct answers, which demonstrates that CryptoBench\neffectively reveals models' authentic generalization capabilities in complex task scenarios."}, {"title": "G.2. Potential Overfitting Risks in Models", "content": "Experimental results indicate that while the model demonstrates superior performance on\npublicly available unencrypted evaluation sets across domains, its performance significantly de-\nteriorates on newly constructed encrypted evaluation sets, suggesting potential overfitting risks.\nThis phenomenon may be attributed to either excessive reliance on training data distributions\nor insufficient generalization capacity. Through in-depth analysis of the Qwen2.5-72B-instruct\nmodel, we observed three typical overfitting patterns:\nCorrect Answers Despite Decoding Errors in Two-Stage Tasks As shown in Figure 35, the\nmodel generates correct answers even when making decoding errors in two-stage tasks. Si-\nmultaneously, it produces accurate responses in single-stage tasks without explicit decoding\nprocesses.\nError Propagation from Decoding Failures in Two-Stage Tasks Figure 36 demonstrates that\ndecoding errors in two-stage tasks lead to incorrect answers, whereas the model directly provides\ncorrect responses in single-stage tasks without explicit decoding.\nDecoupling of Decoding and Answering as Overfitting Manifestation As illustrated in\nFigure 37, the model generates correct answers despite decoding errors occurring in both\ntwo-stage and single-stage tasks.\nThese three cases suggest that the model's behavior may excessively rely on pattern mem-\norization from training data rather than semantic understanding-based reasoning, thereby\nrevealing its inherent overfitting vulnerabilities."}]}