{"title": "Vista3D: Unravel the 3D Darkside of a Single Image", "authors": ["Qiuhong Shen", "Xingyi Yang", "Michael Bi Mi", "Xinchao Wang"], "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D.", "sections": [{"title": "1 Introduction", "content": "Since the earliest times, our ancestors gazed upon the luminous moon, a symbol of mystery and wonder. Its bright facade, an elegant sphere in the cosmos, has always made us think about what remains hidden: the moon's obscure and elusive dark side. This curiosity, as ancient as human history itself, represents our innate desire to uncover the concealed dimensions that exist beyond the visible.\nThis quest, once purely philosophical, has now ventured into the realm of practicality, propelled by the advancements in 3D generative model [29,34,42,45, 48]. These technologies enable a broad range of applications, especially in gaming and virtual reality, allowing for the creation of rich, detailed environments and objects without extensive modeling.\nNevertheless, the development of robust large-scale 3D generative models remains a formidable challenge, predominantly due to the limited availability of 3D data. Numerous attempts [1,13,27] have been made to train 3D diffusion models on relatively small 3D datasets, condition on textual or visual prompts; Yet,"}, {"title": "2 Related-works", "content": ""}, {"title": "2.1 3D Generation Conditioned on a Single Image", "content": "The objective of image-to-3D generation is to create 3D objects from a single reference image. Initial methods [8,52] approached this challenge as a variant of sparse view 3D reconstruction. However, these methods often resulted in blurred object outputs due to insufficient priors. Recently, drawing inspiration from text-to-3D initiatives that utilize Score Distillation Sampling (SDS) to elevate 2D diffusion priors into 3D generative models, image-to-3D works [24, 33, 34, 40, 42] have adopted a similar approach for 3D object generation based on a single image. However, 2D diffusion priors alone cannot ensure 3D consistency, as they are typically trained solely on image datasets. To address this, several studies [19-21,39] have attempted to refine 2D diffusion priors with 3D data [5,6], enhancing their ability to model 3D consistency. A notable example is Zero-1-to-3, which can generate novel views condition on single image and camera position.\nIntegrating this refined model with SDS [30, 41] allows for the reconstruction of coherent 3D objects. Moreover, another stream of works [9, 17, 36, 46, 47, 50, 55] pretrained on large-scale 3D dataset [5] directly predicting the representation"}, {"title": "2.2 3D Representations for Generation", "content": "Presently, most zero-shot text-to-3D and image-to-3D models utilize an optimization based pipeline, parameterizing the 3D object as a differentiable representation, which varies among different methods. The most prevalent representation in groundbreaking works like dreamfields [12], dreamfusion [29], and SJC [43] is Neural Radiance Fields (NeRF) [25]. However, training a NeRF is computationally intensive and takes long time to convergence. Magic3D [16] introduced a two-stage representation, initially learning a coarse NeRF, followed by refining the polygon mesh using a differentiable isosurface method, DMTet [37]. Fantasia3D [2] suggested directly optimizing DMTet [37] in separate phases for geometry and texture, but this often leads to mode collapse in the geometry phase and extends training time beyond NeRF. Gaussian Splatting [10, 14, 35, 44, 53] has gained attention for its efficiency in various 3D tasks, with several 3D generative models [3,4,41,49] incorporating it for effective generation. However, as a point-based representation, it cannot yield high-fidelity meshes. In our approach, we employ Gaussian Splatting exclusively to create coarse geometry. This coarse geometry is then transformed into SDF, optimized with a hybrid isosurface representation, FlexiCubes [38], to produce high-fidelity meshes. Additionally, we propose an angular disentangled texture representation, tailored to the specifics of this task."}, {"title": "3 Methodology", "content": "In this section, we outline our framework to generate detailed 3D object from single image with 2D diffusion priors. As depicted in Figure 2, our exploration of the 3D darkside of a single image commences with the efficient generation of basic geometry (Section 3.1), represented through 3D Gaussian Splatting. In refinement stage (Section 3.2), we devise a method for transforming the rudimentary 3D Gaussian geometry into signed distance fields, and thereafter, we introduce a differentiable isosurface representation to further enhance the geometry and textures. To enable diverse 3D darkside of given single image, we present a novel approach to constrain two diffusion priors (Section 3.3), enabling the creation of varied yet coherent darkside textures by bounding gradient magnitude. With these approaches, our method can efficiently generate diverse, high-fidelity meshes from a single image."}, {"title": "3.1 Coarse geometry from Gaussian Splatting", "content": "In the coarse stage of our framework, we focus on constructing a basic object geometry using Gaussian Splatting. This technique, as described in [14], represents 3D scenes as set of anisotropic 3D Gaussians. Compared to other neural inverse rendering methods, such as NeRF [25, 26], Gaussian Splatting demonstrates a notably faster convergence speed in inverse rendering tasks.\nSome works [3,41,49] has attempted to introduce Gaussian Splatting into 3D generative models. In these methods, we found that directly using Gaussian splatting to generate detailed 3D objects requires optimizing a large number of 3D Gaussians, necessitating significant time for optimization and densification, which is still time-consuming. However, Gaussian Splatting can quickly create a coarse geometry from a single image using a limited number of 3D Gaussians within just one minute. Therefore, in our approach, we utilize Gaussian Splatting solely for the initial coarse geometry generation.\nSpecifically, each 3D Gaussians is parameterized by its central position $x \\in \\mathbb{R}^3$, scaling $r \\in \\mathbb{R}$, rotation quaternion $q \\in \\mathbb{R}^4$, opacity $a \\in \\mathbb{R}$, and spherical harmonics $c \\in \\mathbb{R}^3$ to represent color. To generate a coarse 3D object, we optimize a set of these Gaussian parameters $\\Psi = {\\Phi_1}$, where $\\Phi_1 = {x_i, r_i, q_i, \\alpha_i, c_i}$. To render 3D Gaussians to 2D images, we utilized the highly-optimized tile based rasterization implementation [14].\nTo generate the coarse geometry of given single image $I_{ref}$, we adopt Zero-1-to-3 XL [5, 19] as 2D diffusion priors $\\epsilon_{\\phi}$ with pretrained parameters $\\phi$. This prior enables denoising of novel views based on the given image $I_{ref}$ and relative camera pose $\\Delta\\pi$. Accordingly, we optimize the 3D Gaussians $\\Gamma$ with SDS [29]:\n$\\nabla_{\\Psi} \\mathcal{L}_{SDS} = \\mathbb{E}_{t,\\epsilon} \\left[ \\left( \\epsilon_{\\phi} \\left( I_{\\Gamma}; t, I_{ref}, \\Delta\\pi \\right) - \\epsilon \\right) \\frac{\\partial I_{\\Gamma}}{\\partial \\Psi} \\right].$"}, {"title": "Scale & Transmittance Regularization.", "content": "Additionally, We add two regularization terms to encourage Gaussian Splatting to learn more detailed geometry in this phase. A scale regularization is introduced to avoid too large 3d gaussians, and another transmittance regularization is adopted to encourage the geometry learning from transparent to solid. The overall loss function in this stage can be written as:\n$\\nabla_{\\Psi} \\mathcal{L}_{coarse} = \\lambda_{SDS} \\nabla_{\\Psi} \\mathcal{L}_{SDS} + \\lambda_{rgb} \\nabla_{\\Psi} \\mathcal{L}_{rgb}$\n$+ \\lambda_{mask} \\nabla_{\\Psi} \\mathcal{L}_{mask} + \\lambda_{scale} \\sum_{i} ||S_i||$\n$- \\lambda_{tr} \\nabla_{\\Psi} min(\\tau, \\frac{1}{N_{fg}} \\sum_{k} T_k);$\nwhere $\\mathcal{L}_{rgb}$ and $\\mathcal{L}_{mask}$ are two MSE loss computed between the rendered reference view and the given image. The term $T_k = \\sum_{i} \\alpha_i \\prod_{j=1}^{k-1}(1 - \\alpha_j)$ denotes the transmittance value for the k-th pixel in $I_{\\Gamma}$, where $N_{fg}$ is the total number of foreground pixels. Additionally, $\\tau$ serves as a hyperparameter that is gradually annealed from 0.4 to 0.9, effectively regularizing transmittance over time."}, {"title": "3.2 Mesh refinement and texture disentanglement", "content": "In the refinement stage, our focus shifts to transforming the coarse geometry, produced via Gaussian splatting, into signed distance fields (SDF) and refining its parameters using a hybrid representation.\nThis stage is crucial for overcoming the challenges presented in the coarse stage, notably the surface artifacts frequently introduced by Gaussian splatting. Due to the inability of Gaussian splatting to provide direct estimates of surface normals, we cannot employ traditional smoothing methods to alleviate these artifacts. To counter this, our method incorporates a hybrid mesh representation, which entails modeling the 3D object's geometry as a differentiable isosurface and learning the texture using two distinct, disentangled networks. This dual approach not only smooths out the surface irregularities but also significantly improves the fidelity and overall quality of the 3D model."}, {"title": "Geometry representation.", "content": "We utilize FlexiCubes to represent the geometry in our approach. FlexiCubes is a differentiable isosurface representation which allow local flexible adjustments to the extracted mesh geometry and connectivity [38]. The geometry of an object is depicted as a deformable voxel grid with learnable weights. Deformation $d_i \\in \\mathbb{R}^3$ and sign distance field (SDF) $s_i \\in \\mathbb{R}$ is learnt for every vertices $v_i$ in the voxel grid. And interpolation weights $\\beta \\in \\mathbb{R}^{20}$ and splitting weights $\\gamma \\in \\mathbb{R}$ are learnt for each grid cell to position dual vertices and control quadrilaterals splitting. Triangle meshes can be extracted from it differentiablely through Dual Marching Cubes [28]. To bridge the gap between the learned coarse geometry and the isosurface representation, we initially extract a density field from Gaussian splattings using local density queries [41], followed by the application of marching cubes [22] to extract a base mesh $M_{coarse}$. Subsequently, we query this base mesh at grid vertices $v_i$ to obtain the initial Signed Distance Field (SDF) $s(v_i)$. For stable optimization, the queried SDF is then scaled as follows:\n$S(v_i) = \\frac{\\xi \\cdot \\varsigma(v_i)}{max {|s_j| : s_j \\in S, s_j < 0}}, \\quad where \\quad S = {s_i}$\nwhere $s_j < 0$ indicates the field within the object. The scale factor $\\xi$ linearly increases from 1 to 3 during the optimization process."}, {"title": "Disentangled Texture Representation.", "content": "For texture learning, we employ hash encoding followed by a MLP to directly learn albedo. However, distinct from text-to-3D tasks, we recognize two primary supervision sources in this task: the provided reference image and the SDS gradient from 2D Diffusion priors. Typically, a substantial loss weight $\\lambda_{rgb}$ is assigned for the reference image. This dominant reference image supervision can decelerate the convergence of textures in unseen views, particularly when unseen views significantly differ from the reference view.\nTo address this, we separate the texture into two hash encoding, utilizing a ratio that combines with the relative azimuth angle $\\Delta\\theta = \\theta_{\\pi} - \\theta_{ref}$, where $\\theta_{\\pi}$ represents the azimuth of the sampled camera pose $\\pi$, and $\\theta_{ref}$ is the azimuth of the reference image. The hash encoding for a given query point $\\kappa$ in the rasterized triangle mesh is expressed as:\n$E = (1 - \\eta)H_{back}(\\kappa) + \\eta H_{ref}(\\kappa)$\nwhere $H_{ref}$ and $H_{back}$ denote learnable hash encoding facing forward and back, $\\eta = (cos(\\Delta\\theta) + 1)/2$ is the balance factor that varies with the sampled azimuth angle. Then the encoded feature $E$ is fed into a MLP predict albedo values.\nWith these geometry and texture representation, we can render the 3D object to images by memory-efficient rasterization coupled with lambertian shading. Above learnable parameters $\\Theta$ is refined with $\\nabla_{\\Theta} \\mathcal{L}_{refine}$:\n$\\nabla_{\\Theta} \\mathcal{L}_{refine} = \\lambda_{Sps} \\nabla_{\\Theta} \\mathcal{L}_{Sps}$\n$+ \\lambda_{SDF} \\nabla_{\\Theta} \\mathcal{L}_{SDF} + \\lambda_{consistency} \\nabla_{\\Theta} \\mathcal{L}_{consistency}$\n$+ \\lambda_{rgb} \\lambda_{SDS} \\nabla_{\\Theta} \\mathcal{L}_{rgb} + \\lambda_{mask} \\nabla_{\\Theta} \\mathcal{L}_{mask};$"}, {"title": "3.3 Darkside Diversity via Prior Composition", "content": "In implementing our pipeline, we encountered a key challenge related to the lack of diversity in unseen views. This issue largely stems from the reliance on the Zero-1-to-3 XL prior, a model trained on synthetic 3D objects from Objaverse-XL [5]. While this prior is adept at handling 3D-aware generation based on reference images and relative camera poses, it tends to produce oversimplified or overly smooth results in unseen views. This limitation becomes especially pronounced when dealing with objects captured in the real world.\nTo address this, we integrate an additional prior from Stable-Diffusion, known for its ability to synthesize diverse images.\nWe introduce a second prior, $\\epsilon_{\\rho}$ with pretrained parameters $\\rho$, leading to two Score Distillation Sampling (SDS) loss terms $\\mathcal{L}_{SDS}^\\epsilon$ and $\\mathcal{L}_{SDS}^{\\epsilon_{\\rho}}$ (Equation 1) for optimization. The optimal balance between these two priors remains relatively unexplored. While Magic123 [30] uses an empirical loss weight of 1/40 for the latter term, this approach may not fully harness the potential of the 2D prior. The key objective in introducing this 2D prior is to introduce greater diversity in unseen view. A small weight with $\\nabla_{\\Theta} \\mathcal{L}_{SDS}^{\\epsilon_{\\rho}}$ may largely limit its effect.\nTo enhance the diversity in the unseen aspects of the given image, we employ a gradient constrain method to merge these two priors. We reformulate the SDS loss as a score function [29], $\\nabla_{\\Theta} \\mathcal{L}_{SDS}(\\phi, x) = - \\mathbb{E}_{t, z_t|x} \\nabla_{\\Theta} log p_{\\phi}(z_t|y)$, where $t$ is the timestep and $z_t$ is noise latent.\nHere $\\nabla_{\\Theta} \\mathcal{L}_{SDS}^{\\epsilon}$ is a 3D-aware term conditioned on $y = {\\Delta\\pi, I_{ref}}$, while $\\nabla_{\\Theta} \\mathcal{L}_{SDS}^{\\epsilon_{\\rho}}$ is a diverse text-to-image term conditioned on text prompt $y = P_T$. With different condition $y$, the score function of these two SDS term varies. To retain 3D consistency of unseen views, the magnitude of $\\nabla_{\\Theta} log p_{\\epsilon}(z_t|y)$ need to be constrained with respect to the 3D-aware term $\\nabla_{\\Theta} log p_{\\epsilon_{\\rho}}(z_t|y)$. And to avoid the texture to be over-smoothed by the 3D-aware diffusion model, the magnitude of $\\nabla_{\\Theta} log p_{\\epsilon_{\\rho}}(z_t|y)$ is indeed to be constrained with the $\\nabla_{\\Theta} log p_{\\epsilon_{\\rho}}(z_t|y)$ term.\nSince the noise latents $z_t$ in both priors have different encoding spaces, direct evaluation of their magnitudes using the predicted noise difference $\\epsilon_{\\rho} - \\epsilon$ is not feasible. Instead, we evaluate the magnitude of these terms by observing their gradient on the rendered image $x$, specifically $\\nabla_{x} \\mathcal{L}_{SDS}^{\\epsilon_{\\rho}}$. Consequently, we establish upper and lower bounds for the gradient magnitude ratio of these two SDS terms, allowing for a more accurate and feasible evaluation method:\n$B_{lower} (\\eta, i) \\leq G = \\frac{| |\\nabla_{x} \\mathcal{L}_{SDS}^{\\epsilon_{\\rho}} | |^2}{| |\\nabla_{x} \\mathcal{L}_{SDS}^{\\epsilon} | |^2} \\leq B_{upper} (\\eta, i)$\nWhen this ratio exceeds $B_{upper}$, we adjust the magnitude of $\\nabla_{x} \\mathcal{L}_{SDS}^{\\epsilon_{\\rho}}$ using the factor $B_{upper}/G$. Conversely, if the ratio falls below $B_{lower}$, we scale the"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Details", "content": "In this phase, the input image undergoes preprocessing with SAM [15, 23, 34], where the object is extracted and recentered. We initialize all 3D Gaussians with an opacity of 0.1 and a grey color, confined within a sphere of radius 0.5. The rendering resolution is progressively increased from 64 to 512. This stage involves a total of 500 optimization steps, with the densification and pruning of 3D Gaussians occurring every 100 iterations. The top-K densification starts at a ratio of 0.5 and gradually anneals to 0.1, while the pruning opacity remains constant at 0.1. After the first densification, transmittance regularization is activated and selectively applied to the top-80% opacity values of 3D Gaussians to avoid affecting transparent Gaussians. Scale regularization is enforced using L\u2081 norm. The weights of $\\lambda_{scale}$ and $\\lambda_{tr}$ are maintained at 0.01 and 1, respectively, throughout the optimization, whereas $\\lambda_{rgb}$ and $\\lambda_{mask}$ are gradually increased from 0 to 10000 and 1000, respectively. The timestep for SDS is linearly annealed from 980 to 20. For camera pose sampling, the azimuth is sampled in the range of [-180, 180] and elevation in [-45,45], with a fixed radius of $r = 2$. This phase of optimizing the coarse geometry takes about 30 s.\nIn the refinement phase, we configure the grid size of FlexiCubes to 80\u00b3 within the space [-1,1]\u00b3. The coarse geometry obtained from the initial stage is recentered and rescaled to initialize the Signed Distance Field (SDF) for the vertices of this grid. Interpolation weights are set to 1, and all deformations start at 0. For texture, we use two hash encodings with a two-layer Multilayer Perceptron (MLP). The batch size is maintained at 4. The learning rate for deformation and interpolation weights is 0.005, while it's 0.001 for SDF, and 0.01 for texture parameters. The rendering resolution is gradually increased from 64 to 512. In Equation 5, the loss weights are set as follows: $\\lambda_{rgb} = 1500$, $\\lambda_{mask} = 5000$, $\\lambda_{sdf} = 1$, and $\\lambda_{SDS} = 1$. We develop two versions for optimization: Vista3D-S and Vista3D-L. Vista3D-S performs 1000 steps of optimization solely with the 3D-aware prior, aiming to generate 3D mesh within 5 minutes. Vista3D-L undergoes 2000 steps of optimization with two diffusion priors to create more detailed 3D objects. The entire optimization process for Vista3D ranges from 15 to 20 minutes. In this stage, camera poses are sampled using a 3D-aware Gaussian unsampling strategy to expedite convergence (additional details are provided in the supplementary material). All experiments are conducted on an RTX3090 GPU.\nIn SDS optimization, the practice of linearly annealing the timestep t to adjust the noise level has been established as effective for producing higher-quality 3D objects [11]. However, in our experiments, we observed that linear annealing may not be the optimal strategy. Consequently, we"}, {"title": "4.2 Qualitative Comparison", "content": "In Figure 3, we show our efficient Vista3D-S is capable of generating competitive 3D objects with a 20\u00d7 speedup compared to existing coarse-to-fine methods. For Vista3D-L, as depicted in Figure 1 and Figure 4, we highlight our angular gradient constraint which distinguishes our framework from previous image-to-3D methods, as it can explore the diversity of the backside of single images without sacrificing 3D consistency. In Figure 3, we primarily compare our Vista3D-S with two baselines, Magic123 [30] and DreamGaussian [41], for generating 3D objects from a single reference view. Regarding the quality of generated 3D objects, our method outperforms these two methods in terms of both geometry and texture. Regarding Vista3D-L, we compare it with two inference-only single view reconstruction models, specifically One-2-3-45 [18] and Wonder3D [21]. As shown in Fig. 4, One-2-3-45 tends to produce blurred texture and may result in incomplete geometry for more complex objects, while our Vista3D-L achieves more refined textures, particularly on the backside of 3D objects, using user-specified text prompts. And Wonder3D often resorts to simpler textures due to its primary training on synthetic datasets [5], which occasionally leads to out-of-"}, {"title": "4.3 Quantitative Comparison", "content": "In our evaluation, we employ the CLIP-similarity metric [19, 24, 30] to assess the performance of our method in 3D reconstruction using the RealFusion [24] dataset, which comprises 15 diverse images. Consistent with the settings used in previous studies, we sample 8 views evenly across an azimuth range of [-180, 180] degrees at zero elevation for each object. The cosine similarity is then calculated using the CLIP features of these rendered views and the reference view. Table 1 highlights that Vista3D-S attains a CLIP-similarity score of 0.831, with an average generation time of just 5 minutes, thereby surpassing the performance of the Magic123 [30]. Furthermore, when compared to another optimization-based method, DreamGaussian [41], Vista3D-S may take longer at 5 minutes, but it significantly improves consistency, as evidenced by the higher CLIP-Similarity score. For Vista3D-L, we apply an enhancement-only setting. By employing angular diffusion prior composition, our method achieves a higher CLIP-Similarity of 0.868. The capabilities of Vista3D-L, especially in generating objects with more detailed and realistic textures through prior composition, are demonstrated in Figure 4. Additionally, we conduct quantitative experiments on the Google Scanned Object (GSO) [7] Dataset, following the setting in SyncDreamer [20]. We evaluate each method using 30 objects and computed PSNR, SSIM, and LPIPS [54] between the rendered views of the 3D object and 16 ground-truth anchor views. The results, as shown in Tab. 2, reveal that our Vista3D-L achieves SOTA performance among these methods with a large margin. Vista3D-S also demonstrates competitive performance, albeit with a single diffusion prior."}, {"title": "4.4 User study", "content": "In our user study, we evaluate reference view consistency and overall 3D model quality [41]. The evaluation encompasses four methods: DreamGaussian [41], Magic123 [30], and our own Vista3D-S and Vista3D-L. We recruited 10 participants for this user study. Each was asked to sort generated 3D object from different methods in terms of view consistency and overall quality respectively. Thus, the scores presented for each metric range from 1 to 4. The results, presented in Table 3, reveal that our Vista3D-S outperforms the previous methods in both view consistency and overall quality. Furthermore, the adoption of the angular prior composition in Vista3D-L leads to additional improvements in both the consistency and quality of the generated 3D objects."}, {"title": "4.5 Ablation Study", "content": "Our framework integrates a coarse stage to learn initial geometry then a fine stage to refine geometry and shade textures. We validate the necessity of such a coarse-to-fine pipeline in Figure 5 (a). We first commence with isosurface representation to learn geometry directly, finding the geometry optimization is prone to collapse without preliminary geometry initialization. Thus, a coarse initialization becomes imperative. Beside, we present the normal map of a rough mesh extracted from 3DGS from the coarse stage. It is observed that the coarse stage tends to generate rough even non-watertight geometry, both difficult to mitigate. These findings demonstrate that combining both stages is crucial for the optimal performance of Vista3D.\nFor validating the effectiveness of the disentangled texture, we compare adopting both hash encodings with single hash encoding"}, {"title": "5 Conclusion", "content": "In this paper, we present a coarse-to-fine framework Vista3D to delve into the 3D darkside of a single input image. This framework facilitates user-driven editing through text prompts or enhances generation quality using image captions. The generation process begins with a coarse geometry obtained through Gaussian Splatting, which is subsequently refined using an isosurface representation complemented by disentangled textures. The design of these 3D representations enables the generation of textured meshes within a mere 5 minutes. Additionally, the angular composition of diffusion priors empowers our framework to reveal the diversity of unseen views while maintaining 3D consistency. Our approach surpasses previous methods in terms of realism and detail, striking an optimal balance between generation time and the quality of the textured mesh. We hope our contributions will inspire future advancements and foster future exploration into the 3D darkside of single images."}]}