{"title": "Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion", "authors": ["Yanchen Li", "Jiachun Li", "Kebin Sun", "Luziwei Leng", "Ran Cheng"], "abstract": "Drawing on the intricate structures of the brain, Spiking Neural Networks (SNNs) emerge as a transformative development in artificial intelligence, closely emulating the complex dynamics of biological neural networks. While SNNs show promising efficiency on specialized sparse-computational hardware, their practical training often relies on conventional GPUs. This reliance frequently leads to extended computation times when contrasted with traditional Artificial Neural Networks (ANNs), presenting significant hurdles for advancing SNN research. To navigate this challenge, we present a novel temporal fusion method, specifically designed to expedite the propagation dynamics of SNNs on GPU platforms, which serves as an enhancement to the current significant approaches for handling deep learning tasks with SNNs. This method underwent thorough validation through extensive experiments in both authentic training scenarios and idealized conditions, confirming its efficacy and adaptability for single and multi-GPU systems. Benchmarked against various existing SNN libraries/implementations, our method achieved accelerations ranging from 5x to 40x on NVIDIA A100 GPUs. Publicly available experimental codes can be found at https://github.com/EMI-Group/snn-temporal-fusion.", "sections": [{"title": "1 Introduction", "content": "Deep learning has firmly entrenched itself as a transformative field in artificial intelligence. Python, as a preferred programming language, has propelled this transformation. Notably, by leveraging the robust GPU computing supports of the Compute Unified Device Architecture (CUDA) [24], PyTorch [28] stands out in the Python ecosystem, offering a robust platform for researchers working on traditional Artificial Neural Network (ANN) structures.\nAs the deep learning landscape evolves, the integration of biological neural dynamics, manifested in the form of Spiking Neural Networks (SNNs), has"}, {"title": "2 Related Work", "content": "2.1 GPU Acceleration for Deep Learning\nIn the realm of GPU acceleration for deep learning, NVIDIA's CUDA technology is particularly noteworthy for its high-level interface that enables direct interaction with GPUs. CUDA serves as a crucial software layer that allows developers to engage directly with the GPU's virtual instruction set and parallel computing components, thus facilitating efficient execution of kernel computations. This proximity to the hardware layer significantly enhances development flexibility and efficiency.\nTo further streamline the development process, CUDA is complemented by a suite of optimized standard routines. A primary example is cuDNN [3], which provides a comprehensive set of deep neural network operator implementations within the CUDA ecosystem, substantially simplifying the development of deep learning applications. These foundational technologies underpin many high-level deep learning frameworks, enabling robust and efficient experimentation.\nThere have also been significant advancements in parallel and distributed acceleration methods. Initial efforts, such as those by Krizhevsky et al. [20], allocated neural network models across multiple GPUs, paving the way for processing large deep-learning models with multiple GPUs. Following this, more sophisticated distributed training methods have emerged. For example, Megatron-LM [33] introduced an intra-layer model parallelism method for large language models, enhancing accuracy while maintaining performance. GPipe [16] explored serial operators in foundation models, implementing pipelined serial sub-layers and facilitating pipeline parallelism across multiple GPUs. These methods significantly boost model computation efficiency on supported platforms, with many integrated into existing deep learning frameworks to augment the efficiency of large-scale model development and research.\n2.2 Neuromorphic Computing Infrastructures\nIn light of the integration of spiking neuron properties into deep learning, several infrastructures tailored for SNNs have emerged, each focusing on different facets of the domain. BindsNET [13] emphasizes applications in machine and reinforce-Norse [29] and snnTorch [7] expand on SNNs within the PyTorch\n2.3 Spiking Neurons\nAs the fundamental units of the brain, neurons exhibit unique information transfer properties. To emulate these intricate behaviours in computational models, various spiking neuron models have been proposed. Notably, the Spike Response Model (SRM) [10] represents a broad category of spiking neurons, encompassing parameters like membrane potential decay, spike threshold, refractory period, etc. Essentially, the membrane potential of a neuron undergoes continuous decay unless it receives an external stimulus. Upon receiving information, the potential increases until it hits a threshold, resulting in the generation of a spike, followed by an immediate potential drop.\nSpiking neuron models for deep learning are relatively complex in the early stages, e.g., Hodgkin-Huxley [15], Izhikevich [17]. However, complex models, while accurate, often introduce implementation challenges. As the deep learning domain advances, the need for simplicity becomes paramount. Therefore, the Leaky-Integrate-and-Fire (LIF) model and its variants are preferred at present [4, 32, 35]. LIF emerges as a simplified version of SRM, preserving its core information transfer mechanisms while reducing the intricacies of neuronal information transmission. Recognizing the potential of this simplification, recent research has further refined the LIF model, yielding an iterable form [37]. This adaptation ensures that spiking neurons seamlessly integrate into the deep learning paradigm without compromising the salient features of SRM."}, {"title": "3 Method", "content": "We first analyze the properties of LIF neurons in forward and backward propagation, showcasing the potential for acceleration optimization. Then, we delve into the foundational aspects of the temporal fusion method and its theoretical implications when executed on a single GPU. Finally, we explore the method's scalability across multiple GPUs.\n3.1 Parallelism of LIF Spiking Neurons\nFor a specific layer within the iterative LIF model of a SNN, let $v_i^{(t)}$ denote the membrane potential of the i-th neuron at the t-th time step, and $y_i^{(t)}$ represent"}, {"title": "4 Implementation", "content": "This section provides an implementation developed with our temporal fusion method. Specifically, the implementation leverages the foundational architecture of PyTorch with CUDA. While PyTorch supports the definition and utilization of SNNs, CUDA accommodates both single and multiple GPU operator fusion configurations. We begin by introducing the programming models tailored for PyTorch and CUDA based implementation, with a discussion about the implementation schemes and an exploration of the design considerations for the associated functional modules."}, {"title": "5 Experiment", "content": "Our experiment comprises three parts. Initially, we evaluated the temporal fusion method within various SNN architectures across a spectrum of time steps, utilizing both static image and event-based datasets. Subsequent experiments focused on assessing the temporal fusion method's performance using a LIF unit"}, {"title": "6 Conclusion", "content": "In this paper, we present a method, termed \"temporal fusion\", specifically designed for the efficient GPU-accelerated training of SNNs. To facilitate the practical application of this method, we have developed a CUDA-based implementation, complete with a detailed programming model. This implementation is seamlessly integrated with the widely-used deep learning framework, PyTorch, thereby enabling users to easily adopt and apply our method in their SNN research and development projects. We have conducted a series of comprehensive benchmark tests on the temporal fusion method. These tests are meticulously designed to assess various aspects of the method, including its adaptability to different SNN architectures and training scenarios, as well as its impact on the overall efficiency of the training process. The results demonstrate the significant improvements in the training efficiency and wide applicability of our method."}]}