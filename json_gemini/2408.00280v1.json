{"title": "Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion", "authors": ["Yanchen Li", "Jiachun Li", "Kebin Sun", "Luziwei Leng", "Ran Cheng"], "abstract": "Drawing on the intricate structures of the brain, Spiking Neural Networks (SNNs) emerge as a transformative development in artificial intelligence, closely emulating the complex dynamics of biological neural networks. While SNNs show promising efficiency on specialized sparse-computational hardware, their practical training often relies on conventional GPUs. This reliance frequently leads to extended computation times when contrasted with traditional Artificial Neural Networks (ANNs), presenting significant hurdles for advancing SNN research. To navigate this challenge, we present a novel temporal fusion method, specifically designed to expedite the propagation dynamics of SNNs on GPU platforms, which serves as an enhancement to the current significant approaches for handling deep learning tasks with SNNs. This method underwent thorough validation through extensive experiments in both authentic training scenarios and idealized conditions, confirming its efficacy and adaptability for single and multi-GPU systems. Benchmarked against various existing SNN libraries/implementations, our method achieved accelerations ranging from 5x to 40x on NVIDIA A100 GPUs. Publicly available experimental codes can be found at https://github.com/EMI-Group/snn-temporal-fusion.", "sections": [{"title": "1 Introduction", "content": "Deep learning has firmly entrenched itself as a transformative field in artificial intelligence. Python, as a preferred programming language, has propelled this transformation. Notably, by leveraging the robust GPU computing supports of the Compute Unified Device Architecture (CUDA) [24], PyTorch [28] stands out in the Python ecosystem, offering a robust platform for researchers working on traditional Artificial Neural Network (ANN) structures.\nAs the deep learning landscape evolves, the integration of biological neural dynamics, manifested in the form of Spiking Neural Networks (SNNs), has"}, {"title": "2 Related Work", "content": "In the realm of GPU acceleration for deep learning, NVIDIA's CUDA tech- nology is particularly noteworthy for its high-level interface that enables direct interaction with GPUs. CUDA serves as a crucial software layer that allows de- velopers to engage directly with the GPU's virtual instruction set and parallel computing components, thus facilitating efficient execution of kernel computa- tions. This proximity to the hardware layer significantly enhances development flexibility and efficiency.\nTo further streamline the development process, CUDA is complemented by a suite of optimized standard routines. A primary example is cuDNN [3], which provides a comprehensive set of deep neural network operator implementations within the CUDA ecosystem, substantially simplifying the development of deep learning applications. These foundational technologies underpin many high-level deep learning frameworks, enabling robust and efficient experimentation.\nThere have also been significant advancements in parallel and distributed acceleration methods. Initial efforts, such as those by Krizhevsky et al. [20], allocated neural network models across multiple GPUs, paving the way for pro- cessing large deep-learning models with multiple GPUs. Following this, more so- phisticated distributed training methods have emerged. For example, Megatron- LM [33] introduced an intra-layer model parallelism method for large language models, enhancing accuracy while maintaining performance. GPipe [16] explored serial operators in foundation models, implementing pipelined serial sub-layers and facilitating pipeline parallelism across multiple GPUs. These methods signif- icantly boost model computation efficiency on supported platforms, with many integrated into existing deep learning frameworks to augment the efficiency of large-scale model development and research."}, {"title": "2.1 GPU Acceleration for Deep Learning", "content": "In the realm of GPU acceleration for deep learning, NVIDIA's CUDA tech- nology is particularly noteworthy for its high-level interface that enables direct interaction with GPUs. CUDA serves as a crucial software layer that allows de- velopers to engage directly with the GPU's virtual instruction set and parallel computing components, thus facilitating efficient execution of kernel computa- tions. This proximity to the hardware layer significantly enhances development flexibility and efficiency.\nTo further streamline the development process, CUDA is complemented by a suite of optimized standard routines. A primary example is cuDNN [3], which provides a comprehensive set of deep neural network operator implementations within the CUDA ecosystem, substantially simplifying the development of deep learning applications. These foundational technologies underpin many high-level deep learning frameworks, enabling robust and efficient experimentation.\nThere have also been significant advancements in parallel and distributed acceleration methods. Initial efforts, such as those by Krizhevsky et al. [20], allocated neural network models across multiple GPUs, paving the way for pro- cessing large deep-learning models with multiple GPUs. Following this, more so- phisticated distributed training methods have emerged. For example, Megatron- LM [33] introduced an intra-layer model parallelism method for large language models, enhancing accuracy while maintaining performance. GPipe [16] explored serial operators in foundation models, implementing pipelined serial sub-layers and facilitating pipeline parallelism across multiple GPUs. These methods signif- icantly boost model computation efficiency on supported platforms, with many integrated into existing deep learning frameworks to augment the efficiency of large-scale model development and research."}, {"title": "2.2 Neuromorphic Computing Infrastructures", "content": "In light of the integration of spiking neuron properties into deep learning, several infrastructures tailored for SNNs have emerged, each focusing on different facets of the domain. BindsNET [13] emphasizes applications in machine and reinforce- ment learning. Norse [29] and snnTorch [7] expand on SNNs within the PyTorch"}, {"title": "2.3 Spiking Neurons", "content": "As the fundamental units of the brain, neurons exhibit unique information trans- fer properties. To emulate these intricate behaviours in computational models, various spiking neuron models have been proposed. Notably, the Spike Response Model (SRM) [10] represents a broad category of spiking neurons, encompass- ing parameters like membrane potential decay, spike threshold, refractory period, etc. Essentially, the membrane potential of a neuron undergoes continuous decay unless it receives an external stimulus. Upon receiving information, the potential increases until it hits a threshold, resulting in the generation of a spike, followed by an immediate potential drop.\nSpiking neuron models for deep learning are relatively complex in the early stages, e.g., Hodgkin-Huxley [15], Izhikevich [17]. However, complex models, while accurate, often introduce implementation challenges. As the deep learn- ing domain advances, the need for simplicity becomes paramount. Therefore, the Leaky-Integrate-and-Fire (LIF) model and its variants are preferred at present [4, 32,35]. LIF emerges as a simplified version of SRM, preserving its core informa- tion transfer mechanisms while reducing the intricacies of neuronal information transmission. Recognizing the potential of this simplification, recent research has further refined the LIF model, yielding an iterable form [37]. This adaptation en- sures that spiking neurons seamlessly integrate into the deep learning paradigm without compromising the salient features of SRM."}, {"title": "3 Method", "content": "We first analyze the properties of LIF neurons in forward and backward prop- agation, showcasing the potential for acceleration optimization. Then, we delve into the foundational aspects of the temporal fusion method and its theoretical implications when executed on a single GPU. Finally, we explore the method's scalability across multiple GPUs."}, {"title": "3.1 Parallelism of LIF Spiking Neurons", "content": "For a specific layer within the iterative LIF model of a SNN, let $v_i^{(t)}$ denote the membrane potential of the i-th neuron at the t-th time step, and $y_i^{(t)}$ represent"}, {"title": "3.2 Temporal Fusion on a Single GPU", "content": "As given by Eq. (1), the dynamics of the membrane potential in SNNs are intricately tied to both temporal and spatial factors. This entails that the activity of any given neuron is predicated on its preceding state at the last time step, as well as the input it receives from neurons within the previous layer.\nDrawing inspiration from prior research [8], an intriguing observation emerges: Time-step-wise inference is fundamentally equivalent to layer-wise inference, which occurs after the completion of all time steps in each layer of the com- putation. This characteristic inherent to the latter opens a path for optimization across the temporal dimension, particularly when the initial states for all time steps are predetermined.\nNotably, prioritizing the computation across each layer over all time steps yields two primary benefits. First, the element-wise computation inherent to the monolayer LIF model naturally facilitates parallelization at the neuron level, a feature that extends to temporal propagation. Second, executing computations across all time steps mandates the inclusion of data from all preceding time steps. Given that data for each time step retains a consistent format, strategic memory alignment post-concatenation can significantly reduce the overhead tied to batch memory operations, thereby elevating computational efficiency.\nBuilding upon these insights, we introduce the temporal fusion method, which leverages the element-wise nature of spiking neuron layers. This method assigns each neuron's computation to an individual GPU thread, amalgamating memory operations for each layer across all time steps within the GPU kernel. Such fusions substantially reduce the overhead from multiple memory interactions and thus augment computational throughput. The overarching framework is illustrated in Fig. 1(a)."}, {"title": "3.3 Temporal Fusion across Multiple GPUs", "content": "As elaborated in the preceding subsection, the temporal fusion method enhances computational efficiency by minimizing memory access and leveraging operator fusion, even with small time steps. However, as time step sizes increase, a single GPU may encounter bottlenecks due to expanded temporal dimensions, poten- tially leading to storage constraints. The limited memory of a single GPU could necessitate halting computations or reverting to less efficient serial processing upon exceeding capacity. Furthermore, larger time steps contribute to increased computational latency. Utilizing multiple GPUs can mitigate these issues by distributing the computational workload. Consequently, we extend the temporal fusion method to a multi-GPU setup, as illustrated in Fig. 1(b)."}, {"title": "4 Implementation", "content": "This section provides an implementation developed with our temporal fusion method. Specifically, the implementation leverages the foundational architecture of PyTorch with CUDA. While PyTorch supports the definition and utilization of SNNS, CUDA accommodates both single and multiple GPU operator fusion configurations. We begin by introducing the programming models tailored for PyTorch and CUDA based implementation, with a discussion about the im- plementation schemes and an exploration of the design considerations for the associated functional modules."}, {"title": "5 Experiment", "content": "Our experiment comprises three parts. Initially, we evaluated the temporal fu- sion method within various SNN architectures across a spectrum of time steps, utilizing both static image and event-based datasets. Subsequent experiments focused on assessing the temporal fusion method's performance using a LIF unit"}, {"title": "5.1 Experimental Setup", "content": "All experiments were meticulously carried out on NVIDIA A100 GPUs to en- compass runtime comparisons across different platforms and implementations. To ensure equitable comparisons, we aligned the hyperparameters as specified in Eq. (1), setting $V_{rest}$ = 0, $k_r$ = 0.2, and $V_{th}$ = 0.3. For libraries lacking the $k_r$ hyperparameter, we substituted with $\u03c4$ = 1.25 based on the relation $k_r$ = 1-1/\u03c4."}, {"title": "5.2 Main Results", "content": "In this experiment, we selected three architectures: Spiking-ResNet18, Spiking- ResNet34, and Spiking-ResNet50, as informed by existing research [8, 14]. Refer- ring to the SpikingJelly, we integrated these architectures with our acceleration method for comprehensive testing. The learning rate was set to 0.001 in the Adam optimizer [18], with time steps fixed at 32 over 5 epochs, and the em- ployment of cross-entropy loss. We adopted the sigmoid surrogate function as introduced in [36]:\n$\u03b4(x) = \u03c3'(x) = \\frac{\u03b1e^{-\u03b1x}}{(1+e^{-\u03b1x})^2},$"}, {"title": "5.3 Scalable Time Steps", "content": "This experiment assesses our method's scalability with respect to time steps in a single-GPU setting. To focus solely on our method's acceleration performance,"}, {"title": "5.4 Multi-GPU Acceleration", "content": "The multi-GPU experiment involved deploying the temporal fusion method across 1 to 8 NVIDIA A100 GPUs, focusing on a monolayer LIF model with 1,000,000 neurons across various time steps. The reported outcomes are aver- ages over 1,000 independent runs. As shown in Fig. 6, a notable decrease in computational time was observed with increasing GPU count, highlighting our method's robust performance in a multi-GPU setting."}, {"title": "6 Conclusion", "content": "In this paper, we present a method, termed \"temporal fusion\", specifically de- signed for the efficient GPU-accelerated training of SNNs. To facilitate the prac- tical application of this method, we have developed a CUDA-based implemen- tation, complete with a detailed programming model. This implementation is seamlessly integrated with the widely-used deep learning framework, PyTorch, thereby enabling users to easily adopt and apply our method in their SNN re- search and development projects. We have conducted a series of comprehensive benchmark tests on the temporal fusion method. These tests are meticulously designed to assess various aspects of the method, including its adaptability to different SNN architectures and training scenarios, as well as its impact on the overall efficiency of the training process. The results demonstrate the significant improvements in the training efficiency and wide applicability of our method."}]}