{"title": "NEURAL INTERPRETABLE REASONING", "authors": ["Pietro Barbiero", "Giuseppe Marra", "Gabriele Ciravegna", "David Debot", "Francesco De Santis", "Michelangelo Diligenti", "Mateo Espinisa Zarlenga", "Francesco Giannini"], "abstract": "We formalize a novel modeling framework for achieving interpretability in deep\nlearning, anchored in the principle of inference equivariance. While the direct\nverification of interpretability scales exponentially with the number of variables\nof the system, we show that this complexity can be mitigated by treating in-\nterpretability as a Markovian property and employing neural re-parametrization\ntechniques. Building on these insights, we propose a new modeling paradigm\u2014\nneural generation and interpretable execution\u2014that enables scalable verification\nof equivariance. This paradigm provides a general approach for designing Neural\nInterpretable Reasoners that are not only expressive but also transparent.", "sections": [{"title": "1 A TURING TEST FOR INTERPRETABILITY", "content": "Interpretability, much like intelligence, is often subject to debate due to its inherently subjective\nnature (Kim et al., 2016; Miller, 2019; Molnar, 2020). Instead of attempting to provide an exhaustive\ndefinition, in this paper we propose a procedural test\u2014akin to the Turing test (Turing, 1950)\u2014that\nevaluates whether a system is interpretable. We motivate our proposal using the following concrete\nexamples.\nExample 1. Donal Duck attempts to start his car, model 313, but the vehicle fails to start. After\ninspecting the situation, he finds that the fuel level is too low. Once he refuels, the car starts without\nissue. In this instance, Donal clearly understands the problem and its straightforward solution. The\nfollowing day, the car again fails to start despite a full fuel tank. Uncertain of the cause, Donal\nconsults a mechanic. The mechanic, with his expertise in engines, diagnoses an oil leak as the root\nof the problem. After repairing the leak, the car operates normally. Here, while Donal could not\ndiagnose the issue on his own, his recourse to expert knowledge ultimately resolved the problem.\nThese examples illustrate that understanding a system is often subjective and dependent on the\nuser's background (Miller, 2019). However, they also suggest a practical criterion to check whether\na system is interpretable for a user which can informally present as follows:\nA system is interpretable to a user if the user is able to interact with it and accu-\nrately forecast the system outputs.\nThis approach emphasizes the role of user interaction in assessing interpretability and mirrors the\nspirit of the Turing test by focusing on the system behavior."}, {"title": "2 INTERPRETABILITY & EQUIVARIANCE", "content": "Our work is motivated by the idea that a sys-\ntem is interpretable if its internal processes can\nbe reliably translated into outcomes that users\ncan predict. In this section, we formalize this\nnotion as interpretability equivariance, estab-\nlishing that performing inference using the sys-\ntem's mechanisms should commute with the\nprocess of inference performed using the user's\nmechanisms, as illustrated in the following ex-\nample.\nExample 2 (The Donald Duck Comfort Prob-\nlem (Fig. 1)). Donald Duck wants to sleep but\nis uncomfortably cold. To achieve a comfort-\nable sleep, he needs to warm up his environ-\nment to an appropriate temperature. The heat-\ning system is controlled by a thermostat, yet\nDonald has misplaced the user manual. The\nthermostat provides only two pieces of informa-\ntion: a wheel with 8 positions (currently set to\n1) and a numeric display ranging from 0 to 10\n(currently showing 3).\nIn his first attempt, Donald rotates the wheel to position 6. After waiting, he returns to observe that\nthe display now reads 1, and he finds himself sweating and uncomfortable. Donald can explain the\nphenomenon along two equivalent reasoning paths:\nThermostat path: wheel = 6 \u2192 display = 1 \u2192 comfort = no,\nDonald Duck path: wheel = 6 \u2192 heat = high \u2192 comfort = no.\nFrom this, Donald infers that turning the wheel upward increases the room's temperature and causes\nthe display to show lower numbers. To test his hypothesis, he sets the wheel to position 4. Later, he\nchecks the thermostat to find that the display now shows 2, and he expects the room to have cooled\ndown enough to restore his comfort:\nThermostat path: wheel = 4 \u2192 display = 2 \u2192 comfort = yes,\nDonal Duck path: wheel = 4 \u2192 heat = medium \u2192 comfort = yes.\nThe example illustrates that while the thermostat's variables differ in semantics from Donald Duck's\ninternal concepts, they are nonetheless aligned closely enough for him to establish a straightforward\nmapping between the two. For instance, a wheel position within the range [3, 4] might be interpreted\nas medium heat, and a display reading of 2 may be associated with a state of comfort. Furthermore,\nDonald's reasoning process demonstrates that he can deduce the system's state via two equivalent\nroutes-either by consulting the display or by directly sensing the heat output-with both methods\nleading to the same conclusion. In the next paragraph we introduce the notation we will use to\nformalize this equivariance."}, {"title": "2.1 INTERPRETABILITY AS INFERENCE EQUIVARIANCE", "content": "Preliminaries: Transformation of Random Variables Let V denote a set of random variables\nrepresenting different aspects of a system (for example, heating levels, wheel position, etc.) with a\njoint probability distribution\nP(V) = P(V1, V2, ..., Vn).\nIn many scenarios, we are interested in the conditional probability of a particular query variable Va\n(such as the comfort level) given some evidence Ve (like the wheel position), denoted by P(Vq | Ve).\nTo formalize the distinction between the internal, machine-oriented description of the system and\nits human-interpretable counterpart, we index machine-related variables with the superscript m (so\nthat V(m) represents the machine's variables) and human-related variables with h. A translation\nfunction\nT: V(m)\u2192(h)\nis introduced to map the machine variables into the human reference system. Consequently, for any\ndistribution P(V(m)) over the machine variables, the corresponding distribution in the human space\nis given by the push-forward measure:\nP(T(V(m))) = r(P(V(m))).\nThis expression means that applying the translation function to the machine's probability distribution\nproduces a distribution over the human-interpretable variables.\nInference equivariance The principle of inference equivariance, illustrated in the Donald Duck's\nexample, asserts that the process of translating a machine's probability distribution into the human\nreference system and then querying it should yield the same result as first performing the query\nwithin the machine's domain and then translating the result. Formally, this is expressed as\nP(T(V(m)) | T(V(m))) = r(P(V(m) | V(m))).\nThis equality encapsulates the idea that whether one chooses to \"translate then query\" or \"query\nthen translate\", the resulting inference remains consistent, as already observed for causal struc-\ntures (Geiger et al., 2024; Marconato et al., 2023). In the context of the Donal Duck example, this\nprinciple becomes particularly clear. Donal Duck faces a thermostat whose internal variables\u2014such\nas the wheel setting and display reading-are not immediately aligned with his intuitive notions of\nheat and comfort\u00b9. By establishing a mapping between the machine's outputs and his own reference\nsystem, he is able to reliably predict his comfort level.\nFor instance, Donal might first translate the thermostat's raw signal (the display reading) into his\ninternal concept of temperature and then infer his comfort state based on that interpretation. Al-\nternatively, he might directly observe the mechanical behavior (the wheel position) to predict the\ncorresponding change in room temperature, and only afterward translate that information into his\nsubjective experience of warmth. The fact that both routes lead him to the same conclusion-\nwhether he \"translates then queries\" or \"queries then translates\"\u2014demonstrates the principle of\ninference equivariance.\nThis consistency is critical: it ensures that the mapping between machine variables and human\nconcepts is robust, thereby making the system interpretable. In essence, the equality translate,\nthen query = query, then translate guarantees that the user's understanding and predictions of the\nsystem's behavior remain coherent, regardless of the order in which translation and inference occur.\nVerify interpretability via inference equivariance is intractable While the concept of equiv-\nariance provides a robust framework for linking machine and human perspectives, its practical im-\nplementation is fraught with challenges. As the number of variables increases, verifying and main-\ntaining equivariance becomes exponentially more complex. To illustrate, consider a simple scenario\nwhere every variable in the system is Boolean. In this simple case, a complete interpretation of the\nsystem would require to verify the equivariance for all possible states of the system. This corre-\nsponds to extract the full conditional probability table, which contains 2n entries for n variables."}, {"title": "2.2 PROPERTIES OF INTERPRETABILITY THROUGH THE LENSES OF INFERENCE\nEQUIVARIANCE", "content": "Based on inference equivariance, we can highlight several key properties that further clarify the\nnature of interpretability.\nInference equivariance can be asymmetric: In the thermostat example, Donal Duck uses the avail-\nable signals such as the wheel position and the display reading\u2014to form an understanding of the\nsystem's behavior. Importantly, for him to use the thermostat effectively, it is not necessary to have\na complete, invertible mapping from his internal concepts (e.g., comfort level) back to the machine's\nvariables. This one-way, or asymmetric, mapping suffices because Donal only needs to translate ma-\nchine outputs into human-understandable signals. The absence of a reverse transformation does not\nimpedes his ability to predict the system's response, illustrating that the forward mapping (machine\nto human) is all that is required for interpretability (but the opposite might be needed for supervised\nlearning).\nExplanations are a form of selection: An explanation of a system's behavior can be seen as a\nprocess of selection, where conditioning on observed evidence picks out a specific subset from the\nsystem's complete conditional probability table. In the Donald Duck example, when Donald ob-\nserves a particular display reading or wheel position, he effectively selects a corresponding segment\nof the conditional probability table that relates these inputs to his comfort state. This selection\u2014\nformally represented as P(V | V' = v') encapsulates the explanation by narrowing down the\nmyriad potential outcomes to the ones relevant to his observation.\nExplanations might not be interpretable: Not every selection from the conditional probability\ntable yields a meaningful or interpretable explanation. For example, if the mapping between the\nthermostat's signals and Donald's perception of warmth were inconsistent if the transformation\ndid not commute\u2014then the same action might lead to different inferred comfort states, confusing\nthe user. Hence, for an explanation to be interpretable, the diagram representing the transformation\nmust commute, ensuring that no matter how the inference is performed, the resulting explanation is\nconsistent and understandable.\nLocal vs. global equivariance: Equivariance may hold over the entire state space of the system\n(global) or only in certain regions (local). In the case of the thermostat, Donald Duck might have\ndeveloped an accurate translation for a subset of wheel positions, while other settings remain am-\nbiguous. This local equivariance indicates that while the system may be interpretable under specific\nconditions, its interpretability might not generalize across all possible configurations. Recognizing\nthe distinction between local and global equivariance is crucial for assessing the robustness of a\nsystem's interpretability.\nPost-hoc methods complicate rather than simplify interpretability: When applying post-hoc in-\nterpretability techniques, such as using surrogate models to explain the original system, an additional\nlayer of equivariance is required. Suppose Donald employs a surrogate model to better understand\nhis thermostat. In that case, there must be a consistent mapping between the machine variables of\nthe original system and those of the surrogate model and another mapping from the surrogate model\nto Donald Duck. Formally, if we denote the translation and inference functions for the surrogate as\n\u03c4' and \u03b7' respectively, both the original and surrogate systems must satisfy equivariance conditions:\n\u03b7' (r' (P(V(m)))) = \u03c4' (\u03b7' (P(V(m))))\nand\n\u03b7(\u03c4(P(V(m')))) = r(\u03b7(P(V(m')))).\nThis requirement ensures that the explanations generated by the surrogate model faithfully reflect\nthe behavior of the original system, thus preserving interpretability even when using post-hoc meth-"}, {"title": "2.3 SEMANTIC AND FUNCTIONAL EQUIVARIANCES", "content": "Previous works (Geiger et al., 2024; Marconato et al., 2023) focused primarily on semantic equiv-\nariance, emphasizing that equivariance should hold for random variables V. However, less attention\nhas been paid to the functions that describe mappings between random variables; for a user to truly\nunderstand the underlying mechanisms, the structure of the function and its parameters must also\nsatisfy equivariance, as illustrated in the following example.\nExample 3. Consider the conditional model P(V2 | V1) where V2 follows a Gaussian distribution:\nP(V2 | V\u2081; \u03bc, \u03c3) := $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(\\frac{-(v_2-\\mu)^2}{2\\sigma^2})$\nFor this model to be fully interpretable, it is not enough for a human user to simply understand\nthe data representation encoded in V\u2081 and V2. Instead, inference equivariance must extend to the\nfunctional structure and its parameters. In other words, users should be able to modify or update the\nparameters-such as \u03bc or \u03c3, or even alter constants like replacing 2 with 3\u03c0\u2014and still verify that\nthe same equivariant relations hold. This ensures that the underlying functional form of the model\nremains transparent.\nThe intuition behind this is that functional structure and parameters are key components of inter-\npretability, not just the data representations. To capture this formally, we can distinguish between\nvariables representing data, V \u2208 V, and those describing the model's functional structure, \u03b8\u03b5 \u0398.\nThe complete model can then be expressed as P(V, \u0398). Inference equivariance should hold for both\nV, ensuring semantic transparency, and for 0, ensuring functional transparency."}, {"title": "3 BREAKING COMBINATORIAL COMPLEXITY IN VERIFYING\nINTERPRETABILITY", "content": "As we discussed verifying inference equivariance directly is intractable. In this section we discuss\ninterpretability properties and techniques which can be used to break this complexity down."}, {"title": "3.1 INTERPRETABILITY IS A MARKOVIAN PROPERTY", "content": "In the earlier thermostat example, Donald Duck successfully built an intuitive understanding of how\nthe thermostat worked, despite having no specialized knowledge of electronics or physics. This\nobservation illustrates how interpretability is a Markovian property: a user can interpret a system\nat a given level of abstraction without needing to reference lower-level details. In this context,\ninterpretability is achieved locally each step of the inference process can be understood in isolation\nfrom others. Formally, we can formalize this Markovian property of interpretability by writing:\nP(VV pa(V)) | pa(V),\nmeaning that any variable V is conditionally independent of all non-parent variables, given its parent\nvariables. This property allows a user to interpret a single step of the inference process-the one\nconcerning the variable Vi-without needing to backtrack through the entire chain of reasoning.\nThis Markovian property of interpretability attenuates scalability issues, as it permits the analysis of\nindividual steps without the burden of interpreting the entire system at once. This layered approach\nis reflected in models such as Concept Bottleneck Models (Koh et al., 2020), Neural Additive Mod-\nels (Agarwal et al., 2021), Decision Trees (Breiman et al., 1984), or Prototypical Networks (Chen\net al., 2019), where each layer or component is designed to be interpretable on its own. In the Donald\nDuck example, his ability to understand the thermostat's behavior without delving into the intricate\ndetails of its engineering underscores the practical benefits of this Markovian property."}, {"title": "3.2 RE-PARAMETRIZATIONS BREAK EQUIVARIANCE COMPLEXITY WHILE GUARANTEEING\nEXPRESSIVITY AND INTERPRETABILITY", "content": "Interpreting complex systems often entails dealing with a vast number of variables, which can over-\nwhelm human cognitive limits:\nExample 4 (Thermostat with Many Knobs). Consider a new thermostat design featuring 100 knobs,\nwhere a certain (unknown) set of knobs controls the room temperature for a given day of the calendar\nyear. In this scenario, Donald Duck would need to test every possible knob configuration to fully\nunderstand how the thermostat works.\nThis example highlights a fundamental scalability issue: while a machine can, in principle, process\nand manage a large number of independent variables, human users typically can only handle around\n7 \u00b1 2 variables at any one time (Aristotle, (n.d.; Miller, 1956). It clear that even under the assumption\nthat variables operate independently (which is quite common in XAI), the number of interactions\nrequired to understand the system grows linearly with the number of variables. For humans, who\nare limited to processing a constant (i.e., 7 \u00b1 2) number of variables simultaneously, this poses a\nsignificant obstacle to interpretability. The key question then becomes: how can we design a system\nthat presents only a constant number of variables to a human, without sacrificing the system's overall\nexpressivity? A promising approach to manage this challenge is re-parametrization, where a system\nis transformed into an equivalent form that preserves its expressivity while reducing the number of\nvariables that a human must directly consider.\nFunctional Mixtures One effective strategy is to decompose a complex system into a mixture of\nsimpler subsystems, each of which is easy to understand (McLachlan & Basford, 1988). For in-\nstance, imagine a thermostat with 365 knobs (so, even more than the original 100 knobs!), but with\nthe twist that only one knob is active per day, and an indicator light signals which knob is relevant\nat that time. This design ensures that, at any given moment, Donald needs to focus on only one\nknob rather than hundreds. Such re-parametrization retains the full expressive power of the origi-\nnal system while offering local representations that are much more interpretable. Techniques like\nLIME (Ribeiro et al., 2016), ProtopNets (Chen et al., 2019), and Concept Memory Reasoning (De-\nbot et al., 2024) embody this approach by generating simple, locally faithful explanations whose\ncomposition may form arbitrarily non-linear decision boundaries.\nFunctional and semantic re-parametrizations In many classification problems, re-\nparametrization involves two key components: mapping raw variables to higher-level concepts\n(semantic re-parametrization) and decomposing complex function parameters into simpler mixtures\n(functional re-parametrization). In this framework, the original data variables are transformed into\na set of human-interpretable concepts, ensuring semantic transparency as in Concept Bottleneck\nModels (Koh et al., 2020). Simultaneously, the function that governs the model's behavior is\nrestructured into a mixture of simple functions, which preserves the model's expressivity while\nmaking it easier to understand as in Neural Additive Models (Agarwal et al., 2021)."}, {"title": "4 NEURAL INTERPRETABLE REASONING", "content": "Building on our previous discussions of interpretability properties and leveraging techniques such as\nre-parametrizations, we propose a new modeling paradigm that guarantees the scalable verification\nof interpretability as inference equivariance. In this framework, the following elements are essential:\n\u2022 Semantic transparency: The model must employ high-level, human-understandable con-\ncepts, ensuring that the underlying data representations are clear and interpretable.\n\u2022 Functional transparency: It is critical to choose a function that maps these concepts to\nthe desired tasks, where both its structure and parameters are comprehensible to the user.\n\u2022 Markovian property of interpretability: By focusing on a single layer of the system\n(for instance, the final classification layer), this approach breaks down the complexity that\narises from having to interpret the concept generation (which requires a separate verifica-\ntion procedure)."}, {"title": "Neural Generation, Interpretable Execution", "content": "To concretely instantiate our proposal, conside a\nclassification problem where the objective is to predict a target label Y from a set of low-level fea-\ntures (e.g., pixels' intensities) X. Rather than using opaque monolithic model, we propose to lever-\nage expressive power of deep neural networks (DNNs) to generate i) the parameters of a transparent\nmodel W and ii) human-understandable data representations C (a.k.a., concepts)\u2014which together\nform the elements of an interpretable system. The learned transparent model is then symbolically\nexecuted to make predictions Y:\nP(Y | X; 0) = $\\sum_{WC}$ P(Y|C;W) P(C,W | X;\\theta_g)\nThese two factors represent the neural generation component P(C, W | X;0), which re-\nparametrizes concept representations and functional parameters to ensure expressivity, and the sym-\nbolic execution component P(Y | C; W), which guarantees interpretability in the decision-making\nprocess.\nWe refer to the family of models\nimplementing this paradigm as Neu-\nral Interpretable Reasoning. This\nfamily integrates deep neural net-\nwork expressivity with the rigorous\ndemands of interpretability by com-\nbining semantic transparency, func-\ntional transparency, and scalable veri-\nfication of equivariance. Fig 2 shows\na NIR example where a self-driving\ncar must decide whether to brake at\nan intersection. The architecture first\ngenerates both the truth degrees of\nrelevant concepts (e.g., the presence\nof an ambulance or a green light)\nand the weights of a simple linear\nmodel (e.g., an ambulance is assigned\na weight of 2 because it is positively correlated with braking); then, the linear model is executed on\nthese truth degrees to predict whether to brake. Many well-known XAI techniques can be seen\nas special cases within this framework. For example, Prototypical Networks (ProtopNets) (Chen\net al., 2019), Neural Additive Models (Agarwal et al., 2021), and Concept Bottleneck Models (Koh\net al., 2020) all embody aspects of interpretability that align with our proposed approach. More\nrecently, novel approaches such as Concept Memory Reasoning (Debot et al., 2024) and Explana-\ntion Bottleneck Models (Yamaguchi & Nishida, 2024) have begun to fully exploit the potential of\nfunctional re-parametrization retaining the expressivity of traditional, opaque deep neural networks\nwhile supporting the scalable verification of interpretability."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduced a novel framework for assessing and achieving interpretability, anchored\nin the principle of inference equivariance. Drawing inspiration from the Turing test procedure, we"}]}