{"title": "StagFormer: Time Staggering Transformer Decoding for Running Layers In Parallel", "authors": ["Dylan Cutler", "Arun Kandoor", "Nishanth Dikkala", "Nikunj Saunshi", "Xin Wang", "Rina Panigrahy"], "abstract": "Standard decoding in a Transformer based language model is inherently sequential as we wait for a token's embedding to pass through all the layers in the network before starting the generation of the next token. In this work, we propose a new architecture StagFormer (Staggered Transformer), which staggered execution along the time axis and thereby enables parallelizing the decoding process along the depth of the model. We achieve this by breaking the dependency of the token representation at time step i in layer l upon the representations of tokens until time step i from layer l 1. Instead, we stagger the execution and only allow a dependency on token representations until time step i \u2013 1. The later sections of the Transformer still get access to the \"rich\" representations from the prior section but only from those token positions which are one time step behind. StagFormer allows for different sections of the model to be executed in parallel yielding at potential 33% speedup in decoding while being quality neutral in our simulations. We also explore many natural variants of this idea. We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We show how one can approximate a recurrent model during inference using such weight-sharing. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore demonstrate the scalability of the staggering idea over more than 2 sections of the Transformer.", "sections": [{"title": "Introduction", "content": "The Transformer architecture [VSP+17] has seen tremendous success as the primary backbone for language models [CND+22, HBM+22, BMR+20]. It lends itself particularly well for causal language modeling by allowing efficient, highly parallelized training over large datasets. Moreover, the model can be efficiently partitioned across multiple devices [PDC+22] enabling model parallelism across machines. However, it is well known that, during inference, decoding from a Transformer is an inherently sequential task. This task becomes more expensive when trying to decode long sequences due to the cost of attention, which requires computation that scales linearly with respect to sequence length for each additional token.\nConsequently, there has been a significant body of research which tries to make inference from Trans- formers more efficient in practice. Speculative decoding, local attention and other efficient attention variants [TDBM22], KV cache optimizations, blockwise parallel decoding [SSU18] etc. are a few such works. However, there haven't been many works which try to tackle the sequentiality imposed by the"}, {"title": "Related Work", "content": "The Transformer was originally proposed in the seminal work of [VSP+17]. Decoder-only language modeling using the Transformer was originally proposed by [Rad18] and has since become a standard backbone to"}, {"title": "Staggered Transformers (StagFormer)", "content": "In this section we describe our Staggered Transformer (StagFormer) architecture. We begin with a brief background on a decoder-only language models based on the standard Transformer, the backbone for most state-of-the-art language models today.\nLanguage Modeling with the Transformer A Transformer of depth l is a sequence-to-sequence model which takes in a token sequence of length N and generates an output sequence of length N. The tokens are each first mapped to a d-dimensional representation using an embedding layer. Positional information may also be combined into the embedding at this stage. Denote the token embeddings so obtained by to... Then, these representations are progressively modified by applying a sequence of Transformer layers,"}, {"title": "StagFormer", "content": "StagFormer introduces a way to break the sequential dependency of layers within a Trans- former network and still be able to perform efficient and performant causal language modeling. We first partition our l layers into p sub-networks we call stacks. For ease of exposition we will first focus on the simplest case p = 2. Let h = [l/2]. StagFormer allows for execution of the stacks of layers 1,...h and h + 1, ..., l in parallel in a given time step i by staggering the dependency between th and th+1. In particular, we compute th+1 as a function of the original token sequence t\u1ecfand the hth layer representations taken until time step i \u2013 1: th-1. Crucially we exclude a dependency on th. This allows the lower half of layers to begin computing the predictions for the next token in the sequence, t+1, th while the upper layers in the network are finishing computing the final the prediction for position i, t\u016f.\nWe realize this by passing the original token embedding, t\u1ecf as input to the second half of the layers, Lh+1,..., Le, and by augmenting these layers with cross attention to the final activations of the first half of the network on the prior tokens, th, ..., th\u00b9, when computing the final predictions for the next token after position i. Thus th+1 does not actually depend on the prior layers' representation of the token, th, it is a function of the initial token embedding, t\u1ecf, and cross-attends to the previous layers' representations of only past tokens, t,.....\nThis idea can be generalized to p partitions of the l layers by having each new partition stagger an additional time-step. We call this technique staggering the Transformer network over p stacks. A full description of this generalization is presented in Section 3.4.\nThe main advantage of StagFormer is the potential to save latency during decoding by executing stacks in parallel. This can be realized efficiently on today's hardware accelerators such as TPUs and GPUs. Staggering the dependency on the processed representations of tokens until time step i between the first and second stacks of StagFormer can, in principle, lead to a decrease in quality of the model. However, the additional cross-attention parameters in the second stack help ameliorate this decline. In Section 4, we train and evaluate StagFormer for language modeling and observe that a depth l StagFormer with 2 stacks outperforms a depth l regular Transformer (Table 1). At the same time, we measure the potential latency speedup using a setup"}, {"title": "Extensions of the StagFormer", "content": "In this section, we describe certain natural extensions and variants of the StagFormer architecture."}, {"title": "Shared-Weights StagFormer", "content": "In scenarios where we are bound tightly on memory requirements, one can use a variant where we share weights across the different stacks being staggered. Such weight sharing lowers the quality of the model but can save significantly on memory requirements and can be more applicable in memory-constrained settings. Here we use the same weights for self-attention and feed-forward layers for both the passes. The cross-attention weights are the only unique weights for the second pass. So for some input t\u1ecf, we would apply L1,..., Le twice. The first pass processes the input as a standard Transformer network, alternating self-attention and feed-forward layers. The second pass introduces cross-attention layers which allow each token to attend to the final activations of all prior tokens, t\u2081, ..., 1.\nDuring inference, we can have the networks execute the two passes in parallel. This is because, like separate-weights StagFormer, the second pass only depends on the final activations of prior tokens and both passes operate on the same input. The results with shared weights StagFormer are presented in Table 3. We would like to remark that a 2 stack shared-weight StagFormer with each stack having 18 layers performs significantly better than a 18 layer baseline model which has a similar number of parameters. Therefore, StagFormer is an effective way of boosting the performance given a parameter budget.\nNote that shared-weights StagFormer is more similar to looped Transformers than the separate-weights variant, but with an additional cross-attention layers acting as a recurrence mechanism. Extending this idea during inference, once the model has finished processing the prefix, we show that we can use cross-attention to the final activations of the prior tokens to approximate recurrent inference requiring only the second pass in section 3.2."}, {"title": "Shared-Weights StagFormer Approximates a Recurrent Model", "content": "One method we explore for decoding with shared-weights StagFormer is to use the cross-attention to the final activations of prior tokens as a recurrence mechanism. Rather than having the network process each token twice in parallel, with only the second pass using cross-attention, we only have the network operate on each input during decoding once. When doing so, the network cross-attends to the final activations of all prior tokens.\nThis method of decoding resembles a recurrent neural network (RNN) where the final activations of prior tokens are the RNN's hidden state and cross-attention serves as a gating mechanism while processing the current token.\nWe show that it is possible to use shared-weights StagFormer for recurrent decoding using this scheme, even when the model is trained using two separate passes. However, we find that the generated text's quality is not as good as when we process decode new tokens the original way, with two networks running in parallel."}, {"title": "StagFormer with Local Cross-Attention", "content": "If we want stronger latency savings and are willing to take a slight quality hit, a further optimization for StagFormer that is simple to implement is to use local attention for the cross-attention between passes [BPC20]. We observe that StagFormer still performs well even when using local cross-attention with relatively small attention window sizes. StagFormer is also capable of giving non-trivial quality when using an attention window size of 1, which converts the application of the cross-attention in layer Lj on token t-1 to a linear function of t t-1 (recall h = [l/2]).\nSection 4.3 shows the impact of using local attention with window sizes 512, 128, and 1 on StagFormer's performance on pretraining perplexity and downstream tasks. We show local attention can be used successfully with both the separate-weights and shared-weights variants."}, {"title": "StagFormer with More Than Two Stacks", "content": "A natural extension of StagFormer idea we had touched upon earlier is to have h be less than [l/2] and to stagger over more than 2 stacks through the network. For instance, we could have h = [l/3] and stagger the network over 3 stacks. Let p be the number of stacks we stagger the network over, then h = [l/p]. Intuitively, as we increase the number of stacks p, due to progressive staggering, at time step i stack s only gets to see tokens until time step i p + s but needs to produce activations which help predict token i + 1. Thus the job becomes more difficult to learn as p increases, and the depth of each stack reduces which contributes to eventual degradation in quality.\nOur experiments indeed find that model quality suffers when p > 2. However, we find that we can recover significantly by imploring a simple change for StagFormer when p > 2. Rather than using just the final stack's output for computing the final logits, we use a linear combination of each stack's output with learnable coefficients, a1, . . ., ap. Algorithm 4 defines separate-weights StagFormer for when p > 2 in the Appendix.\nOur experiments ablate the linear combination at the end of separate-weights StagFormer when p > 2 to demonstrate its effectiveness. Our results are summarized in Section 4.4. We find that as we increase p model quality suffers, but we are able to recover some of the lost performance by using a linear combination of each stack's output. We explored the settings of p = 3, 4 here, but there might be ways to extend the approach effectively to even larger values of p which we leave for future work."}, {"title": "Experiments", "content": "In this section, we describe our pre-training downstream evaluation setup we used for the different variants of the StagFormer via causal language modeling on the Pile dataset [GBB+20]. We begin by outlining our experiment setting. We also demonstrate the performance of various extensions covered in Section 3."}, {"title": "Experimental Setting", "content": "We performed our experiments using a standard Transformer architecture. The model uses a vocabulary size of 256,000. The model adds global positional embeddings to initial token embeddings and applies Rotary Positional Embeddings (RoPE) in the attention layers [SLP+23]. We compare StagFormer to an 18 layer baseline model with 1.6 billion parameters as well as a baseline where we double the number of layers, resulting in a 2.8 billion parameter model. We pretrained our model on The Pile dataset with a global batch size of 1024 and a max sequence length of 1280 [GBB+20]. We trained the model for 250,000 steps or 327 billion tokens which [GD24] demonstrated should be enough tokens for the model to develop few-shot learning capabilities.\nWe evaluate the model's performance on several few-shot learning tasks [BMR+20]. The evaluation benchmarks include HellaSwag, ARC-E/C, WinoGrande, SuperGLUE, MBPP, Lambada, SQuADv2, and others [ZHB+19, MJF23, SBBC19, WPN+20, AON+21, PKL+16, RJL18].\nFor a full list of evaluation tasks that we used to evaluate StagFormer, see the Appendix (TODO)."}, {"title": "Results", "content": "We first present latency benchmarking results on accelerator hardware which demonstrate the gains we are able to see during decoding with StagFormer compared to a quality matched standard Transformers. The analysis is presented in Table 2.\nAt the 1-3 billion parameter scale, we compare shared-weights StagFormer to a baseline model with the same number of layers.\nWe also compare a model with double the number of Transformer layers with the separate-weights StagFormer which uses the same number of layers as the original baseline model in each pass. We chose to compare StagFormer to a Transformer with double the number of layers to compare the benefits of using staggered passes with adding more layers to the model."}, {"title": "Results with Local Cross-Attention", "content": "We also ran experiments using StagFormer with local cross-attention with both the separate- and shared- weights variants. We present results for experiments with local attention using window sizes 512, 128, and 1 in Table 5."}, {"title": "Results with p > 2", "content": "We also present results from experiments with StagFormer with more than two stacks (p > 2). We show the effect of using more than two stacks on the shared-weights variant, and we show benchmarks for separate- weights StagFormer that use more than two passes to break the network layers into multiple passes. We also include ablations of using a linear combination of outputs for separate-weights StagFormer when p > 2 to demonstrate its impact on model quality. For shared-weights StagFormer, we match training during prefill and run all p stacks, and then switch to recurrent inference for decoding. Note that for p = 4, some evaluation tasks failed due to memory constraints. We find that increasing p surprisingly has a negative impact on model quality. See Table 3 for results."}, {"title": "Conclusion", "content": "We present the StagFormer architecture as a way to increase the capacity of transformer models by allowing lower-level layers to attend to the final activations produced by the same or different networks. With separate- weights StagFormer, we demonstrate that we can use higher level representations of prior tokens to run data-independent transformer layers in parallel to process the current token without sacrificing quality."}, {"title": "Future work and limitations", "content": "There are many aspects of the StagFormer architecture that are not well understood and requires future research. For example, training shared-weights StagFormer only approximates recurrent inference, since training requires a discrete number of passes. Furthermore, using shared-weights with more than 2 passes does not alleviate this issue. Future work could explore how to extend the StagFormer algorithm that either better approximates or fully realizes recurrent decoding with better quality.\nWe also find that when we increase the number of stacks to more than two when using separate-weights StagFormer that the model's performance starts to degrade. Our experiment shows using a linear combination"}, {"title": "Broader impact", "content": "Transformer networks have mainly been used under the assumption that the execution of transformer layers must be done serially. StagFormer shows that it is possible to further parallelize execution of large language models by execution stacks of transformer layers in parallel and match the quality of a deeper model. StagFormer could help reduce the throughput latency of Transformer-based models, which allows these to be served at a lower cost. Efforts to lower the cost of deploying Transformer-based models has a large ecological and economic impact, since the amount of resources to deploy modern language models has become increasingly substantial."}, {"title": "Additional Details on StagFormer Extensions and Experiments", "content": null}]}