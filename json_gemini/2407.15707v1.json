{"title": "Predicting the Best of N Visual Trackers", "authors": ["Basit Alawode", "Sajid Javed", "Arif Mahmood", "Jiri Matas"], "abstract": "We observe that the performance of SOTA visual trackers surprisingly strongly varies across different video attributes and datasets. No single tracker remains the best performer across all tracking attributes and datasets. To bridge this gap, for a given video sequence, we predict the \"Best of the N Trackers\", called the BofN meta-tracker. At its core, a Tracking Performance Prediction Network (TP2N) selects a predicted best performing visual tracker for the given video sequence using only a few initial frames. We also introduce a frame-level BofN meta-tracker which keeps predicting best performer after regular temporal intervals. The TP2N is based on self-supervised learning architectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with VIT-S as a backbone performs the best. The video-level BofN meta-tracker outperforms, by a large margin, existing SOTA trackers on nine standard benchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123, OTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN meta-tracker effectively handling variations in the tracking scenarios within long sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is 88.7% and 91.1% with video and frame-level settings respectively. The best performing tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average overlap is 67.88% and 70.98% with video and frame level settings, compared to the best performing ARTrack, 64.12%. This work also presents an extensive evaluation of competitive tracking methods on all commonly used benchmarks, following their protocols.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual Object Tracking (VOT) is a fundamental and active re-search area in computer vision [29]. Given the location of a generic moving target object in the first frame, the main aim of VOT is to estimate its location in the remaining video sequence [21]. VOT has been employed in a wide range of real-world applications, including video surveillance [20], robotics navigation [59], medical video analysis [2], autonomous driving [8], animal behavior analysis [43], and human activity recognition [1]. VOT is a challenging problem because it requires learning a class-agnostic model of generic target objects in the presence of noise, occlusion, motion blur, and fast motion [30]. In addition, the target may un-dergo significant scale variations, and out-of-plane rotations, and be affected by illumination variations, and background-cluttered [21].\nIn recent years, VOT research has significantly progressed, and several learning-based visual tracking paradigms have been proposed [29], [41], [55], [57] such as the discriminative correla-tion filters [57], deep Siamese networks [3], transformer-driven trackers [55], and large language model-driven trackers [39]. In most areas, over time, the best-performing computer vision methods all tend to follow a single paradigm, such as the visual transformer in object detection [5] and the paradigm becomes the de facto standard.\nThe observation that in visual object tracking, no paradigm or particular tracker currently dominates all others, is the first"}, {"title": "2 RELATED WORK", "content": "To handle wide variations in tracking performance, different types of fusion approaches have been developed in the literature, includ-ing early and late fusion, decision-level fusion, and ensembling of SOTA trackers [53].\nEarly Fusion: In early fusion, different types of feature repre-sentations capturing complementary information about the target object are concatenated together and input into the tracking paradigm for improved tracking performance [4], [10], [23], [27], [36], [38], [49], [52], [64], [72]. The feature-level fusion realizes the fusion process in an embedding space, obtained by an integrated feature extraction performed by a deep neural network. For instance, the MDLatLRR tracker fused RGB and IR images using a nuclear norm-based constraint [38]. The UPDT tracker learns the weights of the individual feature representations for feature fusion [4]. Xu et al. proposed a GFS-DCF tracker for group feature selection in correlation filters using low-rank regularization [64]. More features-level fusion approaches for improving VOT include SiamRPN++ [36], C-RPN [23], SA-Siam [27], MDNET [49], and MAM [10]. In early fusion, different types of features are fused to improve the VOT performance, however, a particular feature type may not be effective for a particular tracking scenario resulting in less than optimal performance. For each tracking scenario, detecting the optimal set of effective feature types is a computationally intractable problem.\nLate Fusion: In late fusion or decision-level fusion, the tracker responses are estimated by varying feature representations as well as different tracker types. The responses obtained by different trackers are then fused for improved VOT performance [42], [70]. For instance, the MEEM tracker uses historical information and constitutes an expert ensemble, where the best expert is selected to restore the current tracker based on a minimum entropy criterion [70]. HCF tracker employs hierarchical deep feature representation and trains correlation filters on each layer [42]. Different correlation filters are then fused using a weighting factor to perform VOT. The MCCT tracker also employs the approach where multiple proposals based on the agreement of multiple feature combinations as well as temporal consistency are selected [58]. Compared to early fusion, late fusion requires multiple responses to be estimated in parallel on a single video sequence, resulting in increased computational complexity [53].\nEnsembling of SOTA Trackers: In late fusion, the set of features or trackers is fixed beforehand, while a badly performing feature"}, {"title": "3 PROPOSED METHODOLOGY", "content": "The system diagram of the proposed \u201cBest of the N Trackers\u201d (BofN) meta-tracker is shown in Fig. 2. It consists of two major steps, including groundtruth label generation from the input video sequence using a set of SOTA visual trackers (Sec. 3.2 & Fig. 2 (steps (A)-(B))) and training a Tracking Performance Prediction Network (TP2N) (Sec. 3.3 & Fig. 2 (step (C))).\n3.1 Problem Formulation\nGiven a set of $K$ training video sequences, $V = {V_1, V_2, \u2026, V_K}$, where $V_j = {f_{jk}}_{k=1}^{m_j}$, $m_j$ is the number of frames in each video $V_j$. Each frame $f_{jk}$ in $V_j$ is annotated with a bounding box/segmentation mask: $B_j = {b_{j1}, b_{j2},..., b_{jm}}$. In the case of a bounding box, $b_{jk} = [x, y, w, h]$ represents the top left corner coordinates with the width and height of the target object. In the case of segmentation-based annotation, $b_{jk}$ represents a segmentation mask of the target object.\nGiven a set of $N$ visual trackers, $T = {T_1, T_2, ..., T_N}$, our task here is to predict the best-performing tracker $T_i$ for a test video sequence $V_j$ such that only the bounding box $b_{j1}$ in the first frame is available at the inference stage. For this purpose, we propose to train a tracking performance prediction network $f(\\theta)$ that will take an input scene from a video sequence and predict the best-performing tracker: $f(\\theta) : S_j \\rightarrow T_i$, where $S_j$ is the subsequence consisting of two or more frames of the video $V_j$ and a bounding box $b_{ij}$.\n3.2 Groundtruth Label Generation (Figs. 2 (A)-(B))\nTo train the TP2N as $f(\\theta)$, each of the trackers $T_i \\in T$ is executed on each training video $V_j \\in V$, and its performance in terms of"}, {"title": "3.3 Fine-tuning the Proposed Tracking Performance Prediction Network (TP2N) (Fig. 2 (C))", "content": "The proposed TP2N may utilize any existing architecture as a classifier or regressor. However, to cope with limited available datasets as developed by groundtruth label generator step, we em-ploy Self-Supervised Learning (SSL) [11] based paradigms such as CNNs-based architectures including MocoV2 [13], SwaV [6], and ViT-based architecture known as DINO [7]. We also employ BarlowTwins [68] as a non-contrastive learning method to pre-train our TP2N. These architectures are pre-trained using a large-scale ImageNet dataset in SSL settings [11]. These methods learn robust representations using pre-text tasks exploiting supervisory signals obtained from the unlabeled data.\nWe further fine-tune these architectures using the proposed dataset for the \"Best of the N Trackers\" prediction. Using the well-established SSL evaluation protocols, the fine-tuning is per-formed in two different settings including linear probing and full fine-tuning. In linear probing, the backbone feature extractor is kept frozen while the remaining parts of the model (e.g., linear"}, {"title": "3.4 Data Augmentation", "content": "The performance of SSL architectures may vary depending on the composition of training data and the selected set of data augmenta-tion methods. In order to adapt the SSL architectures for the task of the \"Best of the N trackers\" prediction, we employ several VOT-specific data augmentation techniques including temporal resolution reduction, spatial resolution reduction, backward video sequence adaptation, and target object scale variations. In addition to that, we also employ often-used image processing augmentation techniques including random orientations, random flipping, and color distortion.\nIn temporal resolution reduction, temporal sub-sampling is performed by varying the frame rates randomly as 10% and 50%. In spatial resolution reduction, spatial sub-sampling is performed by a factor of 10% and 50%. For backward video adaptation, visual trackers are applied in opposite temporal directions. For target scale variations, the target bounding boxes are randomly scaled down using scale factors of 10%, 20%, and 50%. As a result of these augmentations, we introduce significant diversity in our training data resulting in 10 fold increase in the number of training samples."}, {"title": "3.5 Inference using TP2N (Fig. 2 (D)-(E))", "content": "We apply TP2N on unseen test video sequences in two different settings including video-level and frame-level. In video-level pre-diction, we predict the \"Best of the N Trackers\" only once for each video using the initial frames of that video. In the frame-level prediction, the TP2N is applied after fixed temporal intervals such that the prediction overhead does not become dominant while detecting a change in the scene characteristics or the object being tracked as quickly as possible. Such variations may require a switching \"Best of the N Trackers\" resulting in performance improvement. In the experimental results, we observe that frame-level prediction is more accurate than video-level prediction, especially in the long-term VOT sequence. However, it will also incur more overhead due to the increased prediction frequency."}, {"title": "4 EXPERIMENTAL EVALUATIONS", "content": "In order to evaluate the performance of the BofN meta-tracker, a large number of experiments are performed on four different datasets including LaSOT [21], GOT-10K [28], TrackingNet [48], and VOT2022 [34]. We employed 17 SOTA best-performing trackers for the purpose of data annotation including RTS [51], TOMP [44], AiATrack [24], DropTrack [62], OSTrack [66], GRM [25], CiteTracker [39], ARTrack [60], SiamFC [3], SiamRPN++ [37], Ocean [71], TransT [12], KeepTrack [45], STARK [65], SimTrack [9], MixFormer [15], and SwinTrack [40].\n4.1 Implementation Details\nWe fine-tuned our Tracking Performance Prediction Network (TP2N) by combining the training splits of TrackingNet (30130 sequences), LaSOT (1120 sequences), and GOT-10K (9340 se-quences) datasets. We employ four different SSL backbones in-cluding ResNet-50 architecture for MoCo V2 [13], Barlow Twins (BT) [68], and SwAV [6]. For DINO, we employ ViT-small (ViT-S) with 8 \u00d7 8 and 16 \u00d7 16 patch sizes. For each SSL method for fine-tuning the TP2N, the same protocols are used as proposed by the original authors. The scaling rule is used to adjust the learning rate: $lr = lr * batchsize/256$. Each backbone is pre-trained on 4 V100 Tesla GPUs for 200 ImageNet epochs as a pre-training step [56]. We follow the same data pre-processing and training schemes as in [26].\nFor MoCoV2 [13], SGD optimizer is used with an initial learning of 0.3 which is linearly scaled using the aforementioned learning scheme in which the batch size is 4,096. The memory bank size is fixed to 65,536, and momentum of 0.999 is used with a weight decay of $10^{-4}$ is used for regularization. For SwAV [6], the optimizer and learning rate are the same while the batch size is 2,048. For BT, LARS optimizer is used with a learning rate of 0.2 for weights and 0.0048 for biases as recommended by the original authors [67]. The embedding dimension is 8, 192 and training is performed wiht a coefficient of off-diagonal term $\\lambda = 5.10^{-3}$ with a weight decay of $\\lambda = 1.5.10^{-6}$ For both variants of the DINO (8 \u00d7 8 and 16 \u00d7 16 patch sizes), AdamW optimizer is used with a learning rate of 0.0005 which is linearly scaled $lr = lr *batchsize/256$ with a batch size of 1,024. Weight decay follows a cosine schedule form 0.04 to 0.4."}, {"title": "4.2 Datasets", "content": "The following VOT datasets are used in this work:\n\u2022LaSOT [21]: LaSOT is a large-scale benchmark dataset that is mostly employed for long-term object tracking performance. The official training split contains 1120 sequences (2.8M frames) and the testing split contains 28 sequences (685K frames).\n\u2022TrackingNet [48]: The TrackingNet contains a wide vari-ety of real-world object types and tracking challenges. The dataset contains a training split of 30,130 (14M frames) sequences and testing split contains 511 (226k frames) sequences.\n\u2022GOT-10k [28]: The GOT-10K dataset contains a training split of 9340 (1.4M frames) sequences while the testing split has 420 sequences (56K frames). This dataset con-tains a wide variety of tracking challenges and no over-lapped sequence between the training and testing splits.\n\u2022VOT2019 [32], VOT2021 [33], & VOT2022 [34]: The VOT datasets series consists of 60 video sequences, where the frames are in RGB format. The VOT2021 and VOT-2022 contain 19.447k and 19.903k frames, respectively. We employ these datasets for evaluation purposes.\n\u2022OTB100 [63]: The OTB100 dataset contains 100 challeng-ing sequences with 58,897 frames and 356 \u00d7 530 average resolution. This dataset also contains 11 distinct attributes.\n\u2022UAV123 [47]: This dataset contains 123 UAV sequences with diverse tracking attributes. There are total 21.9K frames with an average resolution of 1280 \u00d7 720.\n\u2022WebUAV-3M [69]: The WebUAV-3M dataset contains 4500 video sequences and offers 223 highly diverse tar-get categories with over 3.3 million frames. The train-ing/validation/testing splits contain 3520/200/780 videos. We only evaluate our proposed tracker using the testing split which shows its generalization capability."}, {"title": "4.3 Evaluation Metrics", "content": "Similar to the existing SOTA VOT methods [34], [60], we also employed evaluation metrics including Area Under Curve (AUC), Precision (P), Normalized Precision (PNorm), Average Overlap (AO), Success Rate (SR0.75), SR0.5, Expected Average Over-lap (EAO), Robustness (R), and Accuracy (A) for performance comparison. The performances of VOT2021-2022 datasets are reported using the EAO, A, and R measures while the rest of other metrics are employed for other datasets. Moreover, for evaluating the performance of TP2N, we used top-1 classification accuracy for both frame-level and video-level experiments."}, {"title": "4.4 Evaluation of TP2N", "content": "We perform various experiments for the evaluation of TP2N for prediction of the best-performing tracker. We compare two different architectures ResNet-50 and ViT-S for representation learning and four different SSL methods including MoCo V2, SWAv, BT, and DINO in two settings ($p = 8$ and $p = 16$). Two different types of fine-tunings are employed including Full-fine tuning (Fine-Tune) and Linear probing (Linear) using three linear layers. All methods are evaluated on the testing splits of the four VOT benchmarks including LaSOT, TrackingNet, GOT-1OK, and VOT2022.\nTable 1 shows the video-level performance evaluation of TP2N. Overall, the full fine-tuning experiments have obtained more performance than the corresponding linear probing results. Specifically, on LaSOT dataset, DINOp=8 with full fine-tuning has"}, {"title": "4.5 BofN meta-tracker Performance Evaluation", "content": "Table 3 shows the performance comparison of the proposed BofN meta-tracker on four datasets using different architectures employed during the full-finetuning of the proposed TP2N (see Tables 1 & 2). Two different experimental settings are followed including frame-level evaluation and video-level evaluation."}, {"title": "4.6 Qualitative Results", "content": "To evaluate the performance of the proposed BofN tracker, we present visual results on key frames of 12 challenging sequences selected from the LaSOT and VOT2022 datasets as shown in Fig. 3. The bounding boxes of the target objects are overlaid on the input images and the qualitative results comparisons are shown with existing trackers. Overall, BofN tracker has performed much better than the compared trackers in all sequences, which can be attributed to the tracking performance prediction network in the proposed algorithm."}, {"title": "4.7 Quantitative Evaluations: Performance Comparisons of BofN with SOTA Trackers", "content": "Table 4 shows the performance comparison of the BofN meta-tracker with existing SOTA trackers on LaSOT, TrackingNet, and GOT-10K datasets.\nOverall, both the video-level and frame-level BofN meta-trackers consistently outperform the SOTA visual trackers by a"}, {"title": "4.8 Ablation Studies", "content": "4.8.1 BofN Performance sensitivity to N (size of the tracker's pool):\nTo evaluate the effect of N on the performance of the BofN meta-tracker, we repeat the experiments with N={3, 6, 9, 12, 15, 17} best trackers for each of the four datasets in both frame-level and video-level settings as shown Table 8. The SOTA trackers are arranged according to their performance on a particular dataset and the best N trackers are selected in each experiment. Table 10 shows the exact combination of the SOTA trackers for each dataset. Overall, we observe an increase in the performance with the increasing size of the tracker pool from Bof3 to Bof17. A further increase in the pool size may further increase the performance of the BofN meta-tracker."}, {"title": "4.9 BofN meta-Tracker Overhead", "content": "We compute the overhead of the TP2N for the prediction of the BofN meta-tracker both at frame-level and video-level settings using N=17. In video-level experiments, since only one evaluation is performed on each video, therefore, the overhead cost is 0.84s which is quite insignificant.\nIn the case of frame-level evaluation, the overhead cost de-pends on the number of times the best tracker is predicted in each"}, {"title": "5 CONCLUSION & FUTURE DIRECTIONS", "content": "This work is based on the observation that SOTA visual tracker performance surprisingly strongly varies across different video attributes and datasets. To exploit the phenomenon we propose the"}]}