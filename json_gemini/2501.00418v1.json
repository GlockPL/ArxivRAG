{"title": "GENERALIZING TRUST:\nWEAK-TO-STRONG TRUSTWORTHINESS\nIN LANGUAGE MODELS", "authors": ["Martin Pawelczyk", "Lillian Sun", "Zhenthing Qi", "Aounon Kumar", "Himabindu Lakkaraju"], "abstract": "The rapid proliferation of generative AI, especially large language models (LLMs), has led to their\nintegration into a variety of applications. A key phenomenon known as weak-to-strong generalization -\nwhere a strong model trained on a weak model's outputs surpasses the weak model in task performance\n- has gained significant attention. Yet, whether critical trustworthiness properties such as robustness,\nfairness, and privacy can generalize similarly remains an open question. In this work, we study this\nquestion by examining if a stronger model can inherit trustworthiness properties when fine-tuned\non a weaker model's outputs, a process we term weak-to-strong trustworthiness generalization. To\naddress this, we introduce two foundational training strategies: 1) Weak Trustworthiness Finetuning\n(Weak TFT), which leverages trustworthiness regularization during the fine-tuning of the weak\nmodel, and 2) Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT), which\nextends regularization to both weak and strong models. Our experimental evaluation on real-world\ndatasets reveals that while some trustworthiness properties, such as fairness, adversarial, and OOD\nrobustness, show significant improvement in transfer when both models were regularized, others\nlike privacy do not exhibit signs of weak-to-strong trustworthiness. As the first study to explore\ntrustworthiness generalization via weak-to-strong generalization, our work provides valuable insights\ninto the potential and limitations of weak-to-strong generalization.", "sections": [{"title": "Introduction", "content": "Over the past few years, there has been a rapid proliferation of generative artificial intelligence (AI), particularly large\nlanguage models (LLMs) like GPT-3, GPT-4, and their successors. These models have demonstrated remarkable\ncapabilities across a wide range of tasks, including language comprehension [27], reasoning [7] and tabular data\ngeneration [5]. Their emergent behaviors \u2013 unexpected capabilities that arise as models scale \u2013 have captured the\nattention of both academia and industry, leading to widespread integration into various applications [28, 37].\nOne intriguing key phenomenon observed in LLMs is known as weak-to-strong generalization [8]. In this context,\na \"weak\" model, typically smaller in terms of the number of model parameters, is used to supervise the training of a\nlarger-sized \u201cweak-to-strong\" model. Remarkably, this larger model often surpasses the weak model in performance,\neven when trained solely on the weak model's outputs. Prior research has shown that when a large model is fine-tuned\non the predictions of a smaller model for tasks like sentiment analysis or machine translation, the larger model not only\nlearns the task but also generalizes better to unseen data [8].\nWhile performance improvements are valuable, trustworthiness has emerged as a critical aspect of Al systems,\nespecially as LLMs are increasingly deployed in high-stakes domains like healthcare, finance, and legal services [35].\nTrustworthiness encompasses properties such as fairness (avoiding biases against certain groups), privacy (protecting\nsensitive information), and robustness (maintaining performance under adversarial conditions or distribution shifts).\nEnsuring these properties is essential to prevent harmful outcomes, comply with regulations, and maintain public trust\nin AI technologies. Given the importance of trustworthiness, fundamental yet unexplored questions arise:"}, {"title": "Related Work", "content": "This work is the first to leverage regularization techniques to study if trustworthiness properties transfer from a weak to\na strong model, and one of the first to study weak to strong generalization in large language models. Below we discuss\nrelated works for each of these topics.\nFairness. Unfair outcomes can arise in language models when they inadvertently encode biases present in the training\ndata, leading to discriminatory practices against certain groups based on sensitive attributes like race, gender, or age [4]."}, {"title": "Methodology", "content": "We now present our methodology for investigating the generalization of trustworthiness properties from weak to strong\nmodels. Our approach systematically explores whether and how fairness, privacy, and robustness can be effectively\ngeneralized from weaker to stronger models. The broader issue we address is: Under what conditions can a weaker\nmodel, despite its limitations, most effectively transfer properties such as fairness, privacy, and robustness to a more\npowerful model? We begin by outlining the weak-to-strong training process, followed by techniques for eliciting\nspecific trustworthiness properties in language models. Finally, we introduce a simple yet effective three-stage training\napproach that allows us to examine weak-to-strong trustworthiness generalization under different fine-tuning strategies."}, {"title": "Preliminaries", "content": "Here we present the key training strategies that underlie our work. First, we discuss how we adapt the weak-to-strong\ngeneralization framework introduced by Burns et al. [8]. Following this, we examine widely-used regularization\nstrategies for ML models aimed at enhancing trustworthiness properties such as robustness, fairness, and privacy.\nNotation. We consider training datasets of the form {(xi, Yi)}=1 where yi \u2208 Y is the ground-truth label and a\u017c \u2208 {0,1}\nrepresents a protected attribute (e.g., race or gender) that may be included in the features xi. We denote a classifier\nfo : X \u2192 Y parametrized by 0 \u2208 Rd, mapping inputs x \u2208 X, to labels Y. We define the outputs of a smaller, already\ntrained, fixed classifier fw(x) as weak labels, where w \u2208 Rk denotes a lower-capacity parameterization than @ where\nk\u226a d. Additionally, let l : R \u00d7 R \u2192 R represent an appropriate loss function such as cross-entropy loss."}, {"title": "Eliciting Weak-to-Strong Trustworthiness in Large Language Models", "content": "We break the analysis into three training strategies, each building on the last by varying the regularization applied to the\nweak and weak-to-strong models, respectively.\nNo trustworthiness fine-tuning (No TFT). This training strategy establishes baseline performance by training the\nweak, strong, and weak-to-strong models without applying any trustworthiness regularization, following the approach\noutlined in Burns et al. [8]:\n\u2022 Weak model: We use small, pretrained LLMs as weak supervisors, referred to as weak models. These weak\nmodels are fine-tuned on ground truth labels to generate predictions. Using the resulting fine-tuned weak models\nfw (\u00b7, \u03bb) where X = 0, we create weak labels by having the weak models make predictions on a held-out validation\nset.\n\u2022 Weak-to-strong transfer: To train weak-to-strong models, we fine-tune a strong model using the weak labels\ngenerated by the weak model using equation 1. This model is referred to as the strong student, and its resulting\nperformance is called the weak-to-strong performance.\nWeak trustworthiness fine-tuning (Weak TFT). This training strategy explores whether a trustworthiness regularized\nweak model can influence the trustworthiness property of a vanilla strong model trained solely on the output of the\ntrustworthy weak model:\n\u2022 Trustworthy weak model: We use small, pre-trained LLMs as weak supervisors, but unlike in Phase 1, these\nweak models are fine-tuned on ground truth labels using a trustworthiness regularizer. This regularizer enforces\nspecific trustworthiness properties, such as fairness, privacy, or robustness, during fine-tuning. These models are\nreferred to as trustworthy weak models. Using these trustworthy weak models fw (\u00b7, \u03bb) where x > 0, we generate\nweak labels by making predictions on a held-out validation set.\n\u2022 Weak-to-strong transfer: To assess whether trustworthiness properties can be transferred from a weak to a strong\nmodel, we fine-tune a vanilla strong model using the weak labels generated by the weak model using equation 1.\nWeak and weak-to-strong trustworthiness fine-tuning (Weak+WTS TFT). The last training strategy investigates\nwhether adding trustworthiness regularization to both the weak and weak-to-strong models can further enhance trust\ntransfer (and performance).\n\u2022 Trustworthy weak model: The trustworthy weak model is the same as in the Weak TFT training strategy, where\nthe weak model is fine-tuned on ground truth labels using a trustworthiness regularizer to enforce properties like\nfairness, privacy, or robustness.\n\u2022 Trustworthy weak-to-strong transfer: In this step, we directly assess how well trustworthiness properties can\nbe transferred from the weak model to the strong model. Unlike for the previous training strategy, where the\nstrong model was fine-tuned without any regularization, here we finetune the strong model using a trustworthiness\nregularizer on the weak labels generated by the trustworthy weak model. We provide details on this training\nobjective in Appendix A.1."}, {"title": "Experimental Evaluation", "content": "In Section 4.1, we empirically evaluate the effectiveness of generalizing trustworthiness properties from a weak to a\nstrong model using the three weak-to-strong training strategies introduced in the previous section. Then, in Section 4.2,\nwe perform a thorough sensitivity analysis, varying the trustworthiness regularization strength, model size, and key\nhyperparameters specific to weak-to-strong transfer training. We begin by describing the real-world datasets used in our\nexperiments, followed by an overview of the LLMs and relevant baselines used for comparison.\nDatasets. We evaluate the transfer of trustworthiness properties from small models to large models using four datasets,\npreviously explored by Wang et al. [35], including the Enron Email dataset [17], the Adult dataset [10], the OOD Style"}, {"title": "Evaluating Trustworthiness of the Weak to Strong Model", "content": "We present our results for all four trustworthiness properties across the three phases in Table 1 and Figure 2. We define\nweak-to-strong trustworthiness as the trend of the WTS-Naive and WTS-Aux-Loss models being more trustworthy than\nthe weak model, and the strong ceiling being more trustworthy than the WTS-Naive and WTS-Aux-Loss models.\nFor each property, we used Pythia 14M as the weak model and Pythia 410M as the strong model.\nNo TFT. For the baseline No TFT training strategy, where models are trained without any trustworthiness regularization,\nwe expected no clear weak-to-strong trustworthiness trends, as no regularization is in place to explicitly enforce\ntrustworthiness properties. Surprisingly, for both OOD and adversarial robustness evaluation we observe that the models\ndemonstrate a weak-to-strong trustworthiness trend. Despite the absence of regularization, the stronger models exhibited"}, {"title": "Sensitivity Analysis", "content": "In this section, we conduct a comprehensive sensitivity analysis to explore how various parameter values influence the\ntransfer of trustworthiness properties from weak to strong models. Specifically, we examine the impact of different\nmodel sizes and the regularization strength (Fair, Adv, AOOD, AP) in the trustworthiness loss functions. We continue the\nsensitivity analysis for the auxiliary loss weighting parameter (a) used during weak-to-strong transfer in Appendix A."}, {"title": "Conclusion", "content": "In this paper, we have investigated the critical question of whether trustworthiness properties such as fairness, robustness,\nand privacy can be transferred from weak to strong models via weak-to-strong generalization. We termed this transfer\nprocess weak-to-strong trustworthiness, and introduced two novel approaches aimed at enhancing this transfer. First,\nWeak Trustworthiness Finetuning (Weak TFT) applies trustworthiness regularization during the fine-tuning of the weak\nmodel. Second, Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT) extends this regularization to\nboth the weak and strong models during fine-tuning. Our comprehensive experimental evaluation across real-world\ndatasets reveals that certain trustworthiness properties, namely fairness, adversarial robustness, and out-of-distribution\n(OOD) robustness, show significant improvement in transfer when both models are regularized. However, we observed\nthat privacy did not exhibit signs of weak-to-strong trustworthiness, highlighting the nuanced nature of transferring\ndifferent trustworthiness attributes. Our work is the first to systematically explore the transfer of trustworthiness\nproperties via weak-to-strong generalization. By emphasizing the potential of this approach, our study provides valuable\ninsights and lays the groundwork for future research in this area."}, {"title": "Weak to Strong Training Process", "content": "In this section, we give a detailed description of the loss used for the third training strategy presented in Section 3.2.\nFairness. To incorporate the fairness constraint into the fine-tuning process, we apply regularization twice yielding the\nfollowing objective\nWTS\nW\nWTS\n0* \u2208 arg min Fair (0; Fair, Fair, a, fw)\n\u03b8\n= arg min 1 lwTS-AUX(xi, fo; a, Fair, fw) + Arairs. (ai \u2013 \u0101) \u00b7 fo(xi),   (7)\n\u03b8 N\ni=1\nN\nwhere a \u2208 [0, 1] is the auxiliary confidence loss weight and where a =  ai is the base rate of the protected\ni=1\nattribute. The first term in equation 7 encourages the weak-to-strong model to make correct predictions while the second\nterm acts as an additional fairness regularizer. The hyperparameter Aair corresponds to the regulrization strength of the\nW\nweak model while a controls the regularization strength for training in this stage.\nOut-of-distribution robustness. The objective during fine-tuning is to minimize the following loss\nW\nWTS\nWTS\n0* \u2208 arg min LOOD (0; AOOD, S, AOOD, a, fw)\n\u03b8\n= arg min 1 lwTS-AUX (xi, fo(xi; 15); a, doop, fw),  (8)\nN\n\u03b8 N\ni=1\nwhere a \u2208 [0, 1] is the auxiliary confidence loss weight. Further, AWOD controls the regularization strength of the fixed\nweak classifier, while 5 controls the regularization strength of the transfer process. As A5 = 0, we are back to\nour Weak TFT strategy, and as AOOD = AOOD = 0 the model is trained without any regularization, reverting to the No\nTFT strategy.\nAdversarial Robustness. The training objective combines the losses from both clean and adversarial samples:\nW WTS\n0* \u2208 arg min LAdv (0; Adv, Adv, a, fw)\n\u03b8\nW = arg min 1 (1 \u2013 WTS-AUX(xi, fo; a, Adv, fw) + Ads Adv lwts-aux(x'i, fo; a, Adv, fw),   (9)\n\u03b8 N N\ni=1\nWTS\nwhere Adv controls the regularization strength of the fixed weak classifier, while Adv controls the regularization\nstrength of the transfer process. As Adv 0, we are back to our Weak TFT strategy, and as Adv WTS Adv 0 the\nmodel is trained without any regularization, reverting to the No TFT strategy."}, {"title": "Choosing the Hyperparameters Based on Trade-off Curves", "content": "Adversarial Robustness. In this section, we provide an illustrative example of how we selected the parameters for the\nstrong baselines, using adversarial robustness as a case study. We plotted trade-off curves between the trustworthiness\nproperties and task performance, selecting the parameter that corresponds to the optimal trade-off in the top right corner\nof the Figure A1. We set Adv for the weak and strong model by independently fine-tuning them on training subset and\nevaluating on the test subset. We plot original task performance vs. adversarial performance for different values of\nAdv and pick the value that offers the best trade-off between clean and adversarial accuracy. Figures Ala and Alb\nshow that Adv = 0.3 achieves good accuracy on original and adversarial samples for both models. Fixing Adv for\nthe weak model to 0.3, we repeat the same analysis for the weak-to-strong model trained with the naive loss function.\nFigure Alc shows that Adv = 0.3 offers a reasonable trade-off for the weak-to-strong model as well. Fixing the Adv\nparameter to 0.3 for the weak and weak-to-strong models, we vary the a parameter for the auxiliary loss function and\nplot in figure Ald. We observe that a = 0.1 achieves the highest accuracy on both original and adversarial samples. We\nperform similar analyses for the warm-up period for a and the number of fine-tuning epochs in Figures Ale and Alf\nand pick the values 0.2 and 6, respectively, for these training parameters.\nOOD Robustness. The standard deviation of the Gaussian Noise is set to 2e - 3 for both the weak model (Pythia\n14M) and the strong model (Pythia 410M). This value was chosen as it allows both models to achieve a balanced"}, {"title": "Detailed Sensitivity Analysis", "content": "Impact of Size on OOD Robustness. In this section, we analyze how the sizes of the weak and strong models affect\nthe performance of the weak-to-strong model. We consider two weak model sizes, 14M and 70M, and two strong model\nsizes, 410M and 1B, resulting in four different experiment configurations. Across all configurations, increasing weak\nmodel size consistently leads to noticeable improvements. Increasing the weak model size from 14M to 70M results in\nsignificant gains in both OOD robustness and task performance. For example, when comparing the 14M-410M (Figure\nA5a) and 70M-410M (Figure A5b) configurations, the latter shows enhanced OOD robustness and overall task accuracy.\nThis improvement is even more pronounced when comparing the 14M-1B (Figure A5c) and 70M-1B (Figure A5d)\nsetups. These results suggest that a larger weak model can better capture task-specific patterns, improving both its\ngeneralization to out-of-distribution data and its performance on in-distribution tasks, and thus producing more reliable\nlabels for weak-to-strong finetuning.\nImpact of Size on Adversarial Robustness. In this section, we study the sensitivity of the weak-to-strong trustworthi-\nness fine-tuning to key training parameters like Adv and a. We plot the adversarial robustness and task performance\nfor different values of Adv and a. We observe that adversarial robustness first increases with Adv and then de-\ncreases, achieving a maximum around 0.4. However, task performance decreases monotonically with Adv. For a, the\nweak-to-strong model performance with auxiliary loss decreases monotonically with the parameter value in all cases.\nImpact of Auxiliary Loss Weighting (@max). The auxiliary loss weighting parameter (maximum alpha) plays a crucial\nrole in balancing the adherence to the weak model's outputs and the strong model's confidence in its predictions. We\nexamined the effect of varying max alpha from 0 to 1 on the performance of the strong models during weak-to-strong\ntransfer. Our experiments showed a degradation of performance with increasing max alpha. As alpha increased from 0\nto 1, the performance of the strong models trained with the auxiliary loss (WTS-Aux-Loss) tended to worsen. Higher\nvalues of alpha place more emphasis on the strong model's own predictions rather than closely following the weak\nmodel's outputs. Therefore, selecting an appropriate value of max alpha is essential to maintain a balance between\nleveraging the weak model's trustworthiness and allowing the strong model to develop its capabilities. Our results\nsuggest that lower max alpha values are preferable for effective weak-to-strong trustworthiness transfer. For our models,\nwe chose alpha-max values from 0.1 to 0.3.\nImpact of Larger Models (6.9B). We show that WTS trustworthiness trends are consistent when scaling up the strong\nmodel. As referenced in Section 4.2, Figures A3 to A6, show four different weak/strong model size configurations\n(14M/410M, 70M/410M, 14M/1B, 70M/1B) with consistent property-specific WTS trustworthiness trends holding\nacross model sizes. We also extended our model size sensitivity analysis to include Pythia 6.9B as the strong model for\nfairness, adversarial robustness, and OOD robustness. The 6.9B model required multiple GPUs to train, and DP-SGD\ncurrently does not support multi-GPU computations, so we did not provide 6.9B results for privacy. Figure A9 displays\nthe results and demonstrates similar WTS trustworthiness trends as the previous model configurations. While WTS\ntrustworthiness is inconsistent at the Weak TFT phase, we see consistent WTS trustworthiness at the Weak+WTS TFT\nphase.\nImpact of Additional Metrics. We include multiple trustworthiness metrics to further support the WTS trustworthiness\ntrends we observed. In Figure A10, we examine an additional fairness metric: Equalized Odds (True Positive Rate).\nThe consistent WTS trustworthiness trend is maintained across both Demographic Parity and Equalized Odds."}, {"title": "Dataset and Evaluation Details", "content": "Data usage during training and evaluation. In Figure A12a, we describe which data is used to train the\nweak and the weak-to-strong models, while Figures A12b and A12c describe which data is used for evaluation.\nFigure A12 describes which data is used for both training the weak and the WTS models as well as for evaluation of the\nWTS model.\nData used to train the WTS model. The weak model fw is trained on the labeled dataset Dw = {(xi, Yi)}. Once\ntrained, we use the weak model fw to label the weak-to-strong training dataset Dwts = {(xi, Yi)} resulting in\nDwts' = {(xi, fw(xi))}. We use Dwrs' to train the weak-to-strong model fe. Notably, there is no overlap between\nDWTS and Dw.\nTrustworthiness Evaluation. We evaluate the trustworthiness properties adversarial robustness, OOD robustness as\nwell as Demographic Parity and Equlaized Odds for all models (weak model, WTS model and strong ceiling) on the\nsame held out test set for the respective problem. For privacy, we evaluated the trustworthiness properties of the weak\nand the strong model on their training set Dw while the privacy leakage for the WTS model is evaluated on Dwts. For\nprivacy considerations, we evaluated the trustworthiness properties of the weak and strong models on their training set\nD Dw, while the privacy leakage for the WTS model is assessed on Dwts."}, {"title": "Additional Adversarial Robustness Dataset Details", "content": "In this section, we evaluate the adversarial robustness of the weak-to-strong models and compare with the weak baseline\nand the strong ceiling. We use Pythia 14M as the weak model and Pythia 410M as the strong model. We create training,\nholdout and test subsets of the AdvGLUE++ dataset using 40%, 40% and 20% of samples, respectively, from each\ntask in the dataset. We use the training subset to fine-tune our models to be adversarially robust. We use the holdout\nsubset to generate labels from the weak model to be used in the weak-to-strong training process. To evaluate the clean\nand adversarial accuracy of our models, we evaluate them on a test subset of the AdvGLUE++ dataset and average the\nperformance across the six NLP tasks in this dataset.\nIn particular, to evaluate weak-to-strong trends in adversarial robustness, we use the AdvGLUE++ dataset [35], an\nextension of the AdvGLUE dataset [34]. AdvGLUE++ is a comprehensive benchmark designed to test adversarial\nrobustness across multiple natural language processing (NLP) tasks and adversarial attack algorithms. This dataset\nincludes adversarial examples for six widely used NLP tasks, each representing a distinct domain or linguistic challenge.\nThe Stanford Sentiment Treebank (SST-2) task involves sentiment analysis, requiring the classification of sentences as\nhaving a positive or negative sentiment. The Quora Question Pairs (QQP) task identifies whether two questions convey\nthe same meaning. The Multi-Genre Natural Language Inference (MNLI) task requires reasoning about entailment,\ncontradiction, or neutrality between pairs of sentences. It includes a mismatched variant, MNLI-mm, where validation\nand test data originate from out-of-domain sources, increasing the challenge of generalization. The Question-answering\nNLI (QNLI) task is framed as an entailment problem between a question and an answer candidate. The Recognizing"}, {"title": "Additional OOD Dataset Details", "content": "We use the same OOD data created by Wang et al. [35]. For ID data, we use the original SST-2 dataset but exclude the\nsamples that are source samples for creating the OOD data. We split the ID data into training, validation, and heldout\nsubsets. Specifically, 50% of the ID data is allocated for training and validation, where 95% of that portion is used for\ntraining and the remaining 5% is for validation. The other half represents the held-out data that is used for generating\nlabels from the weak model for weak-to-strong finetuning. For evaluation, we use the in-distribution validation samples\nto measure ID performance and the OOD test samples to obtain OOD performance."}, {"title": "Weak-to-Strong Training", "content": "In this framework, knowledge transfer from a large pre-trained model occurs by fine-tuning\nit on the labels produced by a smaller model. This process incorporates an additional auxiliary confidence loss, weighted\nby \u03b1 \u2208 [0, 1] that adjusts the confidence in the strong model's predictions relative to the weak labels. This auxiliary loss\nencourages the strong model to make confident predictions, even when they diverge from the weak labels, potentially\nenhancing generalization. The loss function is defined as a linear combination of the cross-entropy losses from the\nweak and strong models: The loss function adapted to our weak-to-strong trustworthiness setting is defined as a linear\ncombination of the losses from the trustworthy weak and strong models\n\n l_{WTS-AUX}(x, f_\\theta; \\alpha, \\lambda, f_w) = (1 - \\alpha) \\cdot l(f_\\theta(x), f_w(x; \\vartheta)) + \\alpha \\cdot l(f_\\theta(x; 1), f_{t,\\theta}(x)), (1)\n\nwhere fw (x; 1) is the fixed trustworthy weak model previously trained with trustworthiness property regularization\nstrength \u03bb and fe(x) denotes the strong model. Further, ft,e(x) represents the hardened strong model predictions\naccording to threshold t that is set proportional to the class weights for each dataset. When X = 0, we are in the standard\nweak-to-strong setting previously studied by Burns et al. [8]. When a = 0, we refer to the loss as lNaive since we train\non the outputs of the weak model only. In the following, we describe how we obtain the weak trustworthy models\nfw (\u00b7; \u03bb) through various regularization techniques aimed at improving trustworthiness.\nFairness. Here we discuss how we can enhance fairness through regularization using a widely-used fairness notion\nknown as Demographic Parity which requires:\n\nP(f_w(x) = 1|a = 1) = P(f_w(x) = 1|a = 0). (2)\n\nTo enforce this fairness constraint during fine-tuning, we use the following objective function from Zafar et al. [39],\n\n min_w \\mathcal{L}_{Fair}(w; \\lambda_{Fair}) = min_w (\\frac{1}{N} \\sum_{i=1}^N l(f_w(x_i), y_i) + \\lambda_{Fair} (a_i - \\bar{a}) \\cdot f_w(x_i)), (3)\n\nwhere \\bar{a} = \\frac{\\sum_{i=1}^N a_i}{N} is the base rate of the protected attribute. The first term in equation 7 encourages to make\ncorrect predictions while the second term acts as a fairness regularizer. Specifically, this term minimizes the covariance\nbetween the protected attribute a\u017c and the model outputs fw(xi), encouraging the model to satisfy demographic parity\nby becoming independent of the protected attribute a. The hyperparameter AFair controls the tradeoff between prediction\naccuracy and fairness where a higher value of Fair encourages more emphasis on achieving fairer outcomes.\nAdversarial Robustness. In adversarial training, adversarially perturbed samples are introduced during the training\nprocess, enabling the model to learn to become invariant to small input perturbations and thereby become more robust\nto adversarial attacks. In this setting, the training dataset consists of triplets (x, x', y), where x is a clean input sample,\nx' is an adversarially manipulated version of x and y is the ground truth label of x. The training objective combines the\nlosses from both clean and adversarial samples:\n\nmin_w \\mathcal{L}_{Adv}(w; \\lambda_{Adv}) = min_w (\\frac{1}{N} \\sum_{i=1}^N (1 - \\lambda_{Adv}) l(f_w(x_i), y_i) + \\lambda_{Adv} \\cdot l(f_w(x_i'), y_i)), (4)\n\nwhere Adv controls the tradeoff between clean and adversarial losses. A higher Aadv places greater emphasis on\nrobustness to adversarial perturbations.\nOut-of-distribution robustness. We use embedding perturbations as the method to enhance out-of-distribution\nrobustness, following approaches from Lecuyer et al. [18], Madry [23], Zhu et al. [43]. Specifically, we experiment with\na setting that adds i.i.d. Gaussian noise to the word embeddings [6, 20]. Define e(x) \u2208 Rd as the word embedding of\ninput x, where d is the embedding dimension. We add Gaussian noise z ~ N(0, AOOD Id) drawn from a distribution with\nmean 0 and covariance matrix AOOD \u00b7 Id to the word embedding which yields a noisy embedding: \u1ebd(x; dood) = e(x)+z.\nThis noisy embedding is then used to fine-tune the model. Here, let fw(x; doop) = gw(\u0113(x; doop)) be the output of\nthe language model parametrized by w. The objective during fine-tuning is to minimize the following loss:\n\nmin_w \\mathcal{L}_{OOD}(w; \\lambda_{OOD}) = min \\frac{1}{N} \\sum_{i=1}^N l(y_i, f_w(x_i; \\delta_{OOD})), (5)\n\nwhere AOOD controls the strength of the OOD regularizer. As AOOD \u2192 0 the model is trained without any regularization,\nreverting to the vanilla model.\nPrivacy. In (Ap, d)-differential privacy, the goal is to ensure that the output of an algorithm A is nearly indistinguishable\nwhether or not any single data point is included in the dataset. Specifically, for any two datasets D\u2081 and D2 that differ\nby only one element, the algorithm A satisfies (Ap, \u03b4)-differential privacy if:\n\nP(A(D_1) \\in S) \\leq exp(\\lambda_p) \\cdot P(A(D_2) \\in S) + \\delta, (6)"}]}