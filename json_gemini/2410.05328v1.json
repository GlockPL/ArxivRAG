{"title": "Reward Learning From Preference With Ties", "authors": ["Jinsong Liu", "Dongdong Ge", "Ruihao Zhu"], "abstract": "Reward learning plays a pivotal role in Reinforcement Learning from Human Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses. In preference modeling, the focus is not on absolute values but rather on the reward difference between chosen and rejected responses, referred to as preference strength. Thus, precise evaluation of preference strength holds paramount importance in preference modeling. However, an easily overlooked factor significantly affecting preference strength measurement is that human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence. To address this, we propose the adoption of the generalized Bradley-Terry model - the Bradley-Terry model with ties (BTT) to accommodate tied preferences, thus leveraging additional information. We prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement. Comprehensive experiments further validate the advantages of incorporating ties in preference modeling. Notably, fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties, labeled by state-of-the-art open-source LLMs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022) has played a pivotal role in aligning large language models (LLMs) (Kenton et al., 2021), enhancing specific capabilities of LLMs in various fields, such as summarization (Stiennon et al., 2020), coding (Gao et al., 2023), and medical assistance (Moor et al., 2023). A crucial component of the RLHF process is the reward model, which serves as the primary mechanism for integrating human preferences and feedback into the learning process (Wang et al., 2024). The reward model guides the optimization procedure of RLHF towards objectives aligned with human preferences (Kaufmann et al., 2023). Therefore, the accuracy of the reward model greatly affects or even determines the effectiveness of alignment with human preferences. Moreover, the direct preference optimization (DPO) method (Rafailov et al., 2024) utilizes LLMs to implicitly represent the reward model through mathematical transformations, bypassing the complex RL optimization phase and focusing solely on the reward modeling phase. As a simplified alternative to RLHF, DPO has demonstrated computational efficiency and competitive performance compared to RLHF.\nTo learn a reward model from human preferences, obtaining high-quality human preference data is crucial (Wang et al., 2024), typically achieved by having human labelers annotate previously collected data consisting of a prompt and a pair of responses (Ouyang et al., 2022; Bai et al., 2022). We note that conventional"}, {"title": "2 Related Work", "content": "The reward model plays a crucial role in RLHF, guiding LLMs towards objectives aligned with human preferences (Christiano et al., 2017; Kaufmann et al., 2023). Recent related work has addressed various aspects of reward modeling. Wang et al. (2024) conducted a comprehensive study on reward models, proposing a method to measure the strength of preferences within the data and introducing contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses. Zhu et al. (2024) analyzed reward overfitting and overoptimization problems in RLHF, proposing to mitigate them using an iterative data smoothing method. Dai et al. (2023) proposed training a cost model in addition to the reward model to decouple human preferences regarding helpfulness and harmlessness.\nAs a simplified alternative to RLHF, DPO (Rafailov et al., 2024) has achieved significant success and impact. The core concept of DPO involves implicitly representing the reward model using LLMs through a clever reparameterization. Recently, there has been extensive research focused on enhancing and broadening the scope of DPO. Amini et al. (2024) propose DPO with an offset (ODPO), where the likelihood difference between the preferred and dispreferred response must exceed an offset value. Zhou et al. (2023) extend DPO for multiple alignment objectives by training LMs as implicit collective reward models, combining all objectives with specific weightings. Chowdhury et al. (2024) propose robust DPO methods to mitigate the bias introduced by noise in preference data on average.\nThe preference model serves as the foundation for reflecting human feedback, with the Bradley-Terry (BT) model (Bradley and Terry, 1952) being the most commonly used preference model in RLHF. Indeed, various generalized models based on the BT model have been proposed to address different scenarios, such as handling home advantage (Agresti, 2012), ties (Rao and Kupper, 1967), multiple comparisons (Plackett,"}, {"title": "3 Preliminaries", "content": "RLHF typically comprises three phases: supervised fine-tuning (SFT), reward learning, and reinforcement learning. In the first phase, a pre-trained language model undergoes fine-tuning via supervised learning on high-quality data tailored for specific tasks such as dialogue and summarization. This fine-tuning process yields the model \u03c0SFT. The second phase involves reward learning on a preference dataset. To construct this dataset, prompts x ~ X are fed to \u03c0SFT, generating pairs of responses (Y1, Y2) ~ \u03c0SFT(y | x). These pairs are presented to human labelers, who express preferences. Conventional preference datasets do not allow ties and require one response to be preferred over the other, denoted as yw > y\u0131 | x, where yw and y\u0131 represent the preferred and dispreferred completions among (y1, y2), respectively. The most popular approach to modeling preference is the Bradley-Terry (BT) model, which assumes the human preference distribution p* as:\nPr (Y1\u227bY2 | x) = \\frac{exp (r^* (x, y1))}{exp (r^* (x, y1)) + exp (r^* (x, y2))}\u02d9                                                        (1)\nwhere r*(y, x) is the latent reward model which is inaccessible. Assuming access to a static dataset of comparisons D = {(x(i), yw(i), y\u0131(i))}i=1, y sampled from p*, we can parametrize a reward model rf(x,y) and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\nLR (ry, D) = \u2212E(x,yw,y1)~D [logo (ry (x, yw) \u2013 ry (x, y\u0131))], where o is the logistic function. And the third phase is to solve the following RL problem with the learned reward function:\nmax Ex~D,y~\u03c0\u03bf (y/x) [ry(x, y)] \u2212 \u03b2DKL [\u03c0\u03bf(y | x)||\u03c0SFT(y | x)],                                                (2)\n\u03c0\u03b8\nwhere \u1e9e is a parameter controlling the deviation from the base reference policy \u3160SFT.\nDPO utilizes the fact that the optimization problem (2) has the closed form solution (Go et al., 2023; Korbak et al., 2022; Peng et al., 2019; Peters and Schaal, 2007):\n\u03c0r(y | x) = \\frac{1}{Z(x)} SFT (y | x) expr(x,y)).\nThen a clever reparameterization is applied to express the reward function in terms of its corresponding optimal policy \u03c0\u03c1:\nr(x,y) = Blog \\frac{\u03c0r(y|x)}{\u03c0SFT (y | x)} + Blog Z(x).\nApplying this reparameterization to the ground-truth reward r* and corresponding optimal model \u03c0*, then substituting this reparameterization into the BT model (1), analogous to the reward modeling approach, the loss function of DPO becomes:\nLDPO (To; Trek) = -(x,yw,y1)~D logo (\u03c3\u03b2 Blog  (  \u03c0o (yw | x)          \u03c0SFT (y\u03c9 | x)   \u2212 Blog  ( \u03c0o (y1 | x)        \u03c0SFT (yl | x))   )\nBradley-Terry model with ties (BTT) (Rao and Kupper, 1967) can be employed to model human preference with ties, i.e., the two response (y1, y2) ~ \u03c0SFT(y | x) are considered equal with respect to the prompt x:\nPo (Y1 = Y2 | x) = \\frac{(\u03b82 \u2013 1) exp (r^* (x, y1)) exp (r^* (x, y2))}{(exp (r^* (x, y1)) + 0 exp (r^* (x, y2))) (\u03b8 exp (r^* (x, y1)) + exp (r^* (x, y2)))}\nPo (Y1\u227bY2 | x) =  \\frac{exp (r^* (x, y1))}{exp (r^* (x, y1)) + 0 exp (r^* (x, y2))}"}, {}, {"title": "4 Preference Modeling With Ties", "content": "For a given reward model r, RLHF focuses not on the absolute values r(x, y1), r(x, y2) but on the preference strength between the pair of responses (Wang et al., 2024):\n\u2206r = r(x, y1) \u2013 r(x, y2)\nIn this section, we will explain that if the real preference model is BTT, but we do not provide human labelers with the option of a tie to generate the preference dataset, the learned reward model will exhibit significant deviation from the real reward model in measuring preference strength.", "4.1 Preference Dataset Under BTT": "Since previous preference datasets do not include ties, we will first explain the simulation process for obtaining a preference dataset without ties when assuming the preference model is BTT. Suppose we have n samples, each consisting of a prompt and a pair of responses, denoted as D = {(x, y+, y\u2212)}i=1. With D available, if we assume the true preference model is BTT, we can obtain preference datasets with and without ties using the following methods:\n\u2022 Offer three options to human labelers: Y1 > Y2, Y2 > Y1, or Y1 = Y2. Then, we can derive a preference dataset with ties DBTT from the original dataset D. We denote that DBTT = DBT UDT, where DBT = {(xi, y+, y\u2212)}, y+ > y\u2212, i \u2208 J; DT = {(xi, y+, y\u2212)}, y+ = y\u2212, i \u2208 K, and JUK = {n}.\n\u2022 For the ties dataset DT, ask human labelers to further specify which response is preferred, resulting in the dataset DTN = {(xi, y+, y\u2212)}, y+ > y\u2212, i \u2208 K. We denote DBTTN = DBT U DTN.\nAssumption 4.1. Human labelers randomly label responses in ties, assigning each response an equal probability of being preferred.\nIn summary, if we assume the preference model is the BTT model and provide the option for ties to human labelers, we obtain the preference dataset with ties DBTT. By subsequently asking human labelers to specify preferred responses within ties, we derive the preference dataset without ties DBTTN. Therefore, we can consider conventional preference datasets without ties as DBTTN", "4.2 Bias in Measuring Preference Strength": "Assuming we have both DBTT and DBTTN derived from D, we can illustrate how to estimate the latent reward model using maximum likelihood estimation (MLE). Since we assume that the latent preference model is BTT and thus obtain the dataset with ties, the most accurate log-likelihood is:\nLCEBTT (r, D) = \u03a3logp (yw > Y\u0131 | x) + \u03a3 log p (y1 = Y2 | x) (4)\n(X,Yw,Y1) EDBT (X,Y1,Y2) EDT\nConventional approaches to estimate the latent reward model typically utilize DBTTN to fit the BT model, with the log-likelihood given by:\nLCEBT (r, D) = \u03a3 log pr(Yw > Y\u0131 | x) (5)\n(x,yw,Y1) EDBTTN\nWe can demonstrate that, even if we possess access to the true prompt and response distributions, there may exist a noteworthy discrepancy between the learned and the actual reward model in measuring preference strength, as illustrated by the following results.\nFirst, we can establish the relationship between the true reward model r* and the learned reward model \u2191 by fully optimizing (5) in Theorem 4.2."}, {"title": "Theorem 4.2.", "content": "E [LCEBT (r, D)] \u2264 E [LCEBT (f, D)],\u2200r \u2260 r\nwhere satisfies\nP+(Y1 > Y2 | x) = qq* (Y1 > Y2 | x), \u2200x ~ X, (y1, y2) ~ \u03c0 SFT(y|x)                                            (6)\nand\nq (Y1 > Y2 | x) = P (Y1 > Y2 | x) + \\frac{1}{2}P(Y1 = Y2 | x)                                     (7)\nProof. By Assumption 4.1 we know that the true preference distribution without ties is q. Therefore, it is equivalent to verify that:\nEx~X,(Y1,Y2)~nSFT(y|x),(Yw,Y1)~q* [log pr(Yw > Y\u0131 | x)  qr* (Yw > Yl  | x)] <0\nby Jensen's inequality we have:\nE [log \\frac{Pr(Yw > Yl | x)}{qr* (Yw > Yl  | x)}] < log E [(\\frac{Pr(Yw > Y1 | x)}{qr* (Yw > Y1  | x)}).\n=log E(x,y1,y2) [\\frac{P+(Y1 > Y2 | x)}{Pr* (Y1 > Y2 | x)}]                                                                        + qq+(Y2 > Y1 | x)\n\\\\\\frac{P+(Y2 > Y1 | x)}{Pr* (Y2 > Y1  | x)}]\n= log E(x,y1,y2) [\\frac{P+(Y1 > Y2 | x)}{Pr* (Y1 > Y2 | x)}]                                                                    + qq+(Y2 > Y1 | x)\n\\frac{Pr(Y1 > Y2 | x)}{Pr* (Y1 > Y2  | x)}]\n= log (E(x,y1,y2) [1])= 0"}, {"title": "Theorem 4.3.", "content": "Even if we have the access to the true prompt and response distributions, there can be a bias in measuring preference strength:\n\u2206r = Ar* + log ( (2\u03b8+ (1+\u03b82) exp(-\u2206r*)    1+\u03b82+2\u03b8 exp(-r*)    ) , \u2200(x, Y1, Y2)                            (8)\nwhere Ar = r(x,y1) \u2013 r(x, y2).\nProof Sketch: From (6), we can know that:\nP+(Y1 > Y2 | x) =p- (Y1 > Y2 | x) + \\frac{1}{2}P(Y1 = Y2 | x)\nPr(Y2 > Y1 | x) =P=(Y2 > Y1 | x) + \\frac{1}{2}P(Y2 = Y1 | x)\nBy subtraction, we can get:\nP+(Y1 > Y2 | X) - P+(Y2 > Y1 | x) = P* (Y1 > Y2 | x) \u2013 p* (Y2 > Y1 | x)\nConsequently, we can derive the relation between Ar and Ar*. Detailed proof can be found in the appendix A.\nTo analyze the bias term log (2\u03b8+(1+\u03b82) exp(-\u2206r*)    1+\u03b82+2\u03b8 exp(-\u2206r*) ), we can observe that its sign is opposite to Ar* indicating that the preference strength is attenuated due to latent preference model mismatch. Additionally, the bias term is a sigmoid-shaped function, bounded by log(1+2) in absolute value. However, despite this bound, the bias term can still be substantial. As mentioned earlier, Wang et al. (2024) trained 10 different reward models on the Anthropic's HH-RLHF dataset (Bai et al., 2022), and the mean preference strength of 83.6% of the data falls within the interval [-0.6, 2.94]. In this range, the ratio between the bias term and Ar* can be considerable, as depicted in Figure 1.", "4.3 Preference Strength Bias Correction Algorithm": "Since conventional preference datasets typically lack ties, we propose a novel method to address the model mismatch issue on preference datasets without ties, assuming the latent preference model is the BTT model. We acknowledge that the right side of (8) is a monotonic function with respect to \u2206r*, implying a one-to-one mapping relationship between Ar and Ar*. Thus, during the optimization procedure, when obtaining the value of Ar, we can treat (8) as a nonlinear equation and solve for the value of Ar*, subsequently subtracting the bias term from the current Ar. The detailed description of this method can be found in Alg. 1. We note that this method can be viewed as a variant of DPO with an offset (ODPO) (Amini et al., 2024) when fine tuning with DPO."}, {"title": "5 Experiments", "content": "In this section, we empirically demonstrate the benefits of incorporating ties in preference learning. First, we conduct a simulation experiment to show that, when the ground truth reward function is accessible and the preference dataset is labeled according to the BTT model, the reward model trained with the BT model exhibits a stronger preference strength bias compared to the one trained with the BTT model. Second, we apply Algorithm 1 to address the model mismatch problem on conventional preference datasets without"}, {"title": "5.1 Preference Bias With The Ground Truth Reward", "content": "In this section, we randomly generate a ground truth reward function r*(x, y), x \u2208 N\u207a, y \u2208 [0,1,2,3], along with a preference dataset labeled by the BTT model using r* (with tied pairs randomly assigned preferences). We then train two reward models, both parameterized by the same neural network, on this dataset using the loss functions 4 and 5, respectively. These trained reward models are denoted as rBTT and rBT. Next, we evaluate the average preference bias of these two reward models relative to the ground truth reward under varying preference parameters 0. The preference bias difference, \u0394 = |\u2206rBT \u2013 \u2206r*| \u2013 |\u2206rBTT \u2013 Ar*|, is shown in Table 2. From the results, we observe that the preference bias of rBTT is consistently smaller than that of rBT, indicating that the BTT model effectively reduces the preference bias with respect to the ground truth reward function, resulting in a more accurate reward model. We also find that as e increases, the preference bias difference becomes larger, which aligns with the trends shown in Figure 1, as a larger 0 in ground truth preference model indicates a higher probability of ties occurring."}, {"title": "5.2 DPO With a Bias-Correction Offset", "content": "We apply Alg. 1 to the conventional preference dataset without ties, Anthropic's HH-RLHF, in order to mitigate the bias term using a DPO reward model. It is important to note that this approach can be viewed as a variant of the ODPO method (Amini et al., 2024), with the key difference being the bias-correction term. We train the small Pythia-160M model (Biderman et al., 2023) for one epoch and record the reward preference accuracy on the test set. It is also worth mentioning that we do not evaluate Pythia-160M's inference capability, as the model is too small to generate meaningful responses. The experimental results are presented in Table 3. As shown, when 0 = 1, the bias-correction term is consistently zero, which essentially reduces the method to DPO, serving as our baseline. From Table 3, we observe that all three ODPO methods, with \u03b8\u2208 {2, 5, 10}, significantly outperform DPO, with ODPO at 0 = 5 showing more than a 10% improvement in accuracy."}, {"title": "5.3 Synthetic Preference Datasets with Ties", "content": "The most compelling experiment is to fine-tune two models using BT and BTT, respectively, on a real preference dataset with ties and then compare their win rates. However, due to the lack of human-labeled preference datasets with ties and the high cost of manual annotation and evaluation, we use an LLM to simulate human judgment and label ties in Anthropic's HH-RLHF dataset. We then fine-tune Pythia-2.8B (Biderman et al., 2023) on this synthetic preference dataset with ties, applying the BT and BTT preference models, and evaluate the responses. When using the loss function 4, we refer to this approach as TDPO. To reduce bias, we utilize Llama and Qwen, alternately as labelers and evaluators. The two labeled preference datasets are summarized in Table 5."}, {"title": "6 Discussion", "content": "In this paper, we introduced the concept of incorporating ties into preference modeling. Specifically, we applied the generalized Bradley-Terry model the Bradley-Terry model with ties to more accurately capture human preferences. Additionally, we analyzed the bias in measuring preference strength due to model mismatch and proposed a novel method to mitigate this bias. Extensive experiments demonstrate the benefits of considering ties in preference modeling. A limitation of this work is the absence of real human-annotated preference datasets with ties, as collecting such data is both expensive and time-consuming. Future work involving human-labeled preference datasets with ties could significantly improve the effectiveness of preference modeling."}, {"title": "B Experiment Details", "content": ""}, {"title": "B.1 Experimental Setup", "content": "For each single experiment, we choose the same 64 batch size, RMSprop optimizer, le - 5 learning rate and\n\u03b2 = 0.1. All experiments are conducted on 4 Nvidia A800-80GB GPUs in a single node."}, {"title": "B.2 Win rate prompt for Llama and Qwen", "content": "We use the same prompt for Llama and Qwen to evaluate a pair of responses:\nFor the following query to a chatbot, which response is more helpful?\nQuery: []\nResponse A: []\nResponse B: []\nFIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.\nSECOND, on a new line, state only \"A\", \"B\", \"Neither\" or \"Both\" to indicate which response is more\nhelpful. Your response should use the format: Comparison: [one-sentence comparison and explanation] More\nhelpful: [\"A\", \"B\", \"Neither\" or \"Both\"]"}, {"title": "B.3 Prompt for Llama and Qwen to Label Ties", "content": "We use the same prompt for Llama and Qwen to label whether a pair of responses are tied:\nFor the following query to a chatbot, are the two responses equally good?\nQuery: []\nResponse A: []\nResponse B: []\nAnswer with exactly \"Yes\" or \"No\""}]}