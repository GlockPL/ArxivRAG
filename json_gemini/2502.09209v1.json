{"title": "On LLM-generated Logic Programs and their Inference Execution Methods", "authors": ["Paul Tarau"], "abstract": "Large Language Models (LLMs) trained on petabytes of data are highly compressed repositories of\na significant proportion of the knowledge accumulated and distilled so far. In this paper we study\ntechniques to elicit this knowledge in the form of several classes of logic programs, including propo-\nsitional Horn clauses, Dual Horn clauses, relational triplets and Definite Clause Grammars. Exposing\nthis knowledge as logic programs enables sound reasoning methods that can verify alignment of LLM\noutputs to their intended uses and extend their inference capabilities. We study new execution meth-\nods for the generated programs, including soft-unification of abducible facts against LLM-generated\ncontent stored in a vector database as well as GPU-based acceleration of minimal model computation\nthat supports inference with large LLM-generated programs.", "sections": [{"title": "1 Introduction", "content": "While the multi-step dialog model initiated by ChatGPT is now available from a few dozen online or\nlocally run open source and closed source LLMs, it does not cover the need to efficiently extract salient\ninformation from an LLMs \u201cparameter-memory\u201d that encapsulates in a heavily compressed form the\nresult of training the model on trillions of documents and multimodal data.\nSteps in this direction have been taken, relying on ground-truth involving additional information\nsources (e.g., collections of reference documents or use of web search queries). Among them, we men-\ntion work on improving performance of Retrieval Augmented Generation (RAG) systems [7] by recur-\nsively embedding, clustering, and summarizing chunks of text for better hierarchical LLM-assisted sum-\nmarization [15], multi-agent hybrid LLM and local computation aggregators [3] and deductive verifiers\nof chain of thoughts reasoning [9].\nA more direct approach is recursion on LLM queries, by chaining the LLM's distilled output as input\nto a next step and casting its content and interrelations in the form of logic programs, to automate and\nfocus this information extraction with minimal human input [18, 20]. Like in the case of typical RAG\narchitectures [7, 15], this process can rely on external ground truth but it can also use new LLM client\ninstances as \u201coracles\" deciding the validity of the synthesized rules or facts.\nWith focus on automation of this unmediated salient knowledge extraction from the LLM's parameter\nmemory and its encapsulation in the form of synthesized logic programming code, we will extend in this\npaper the work initiated in [18, 20] with:\n\u2022 new LLM input-output chaining mechanisms\n\u2022 new types of generated logic programs\n\u2022 new relational representations elicited from LLM output steps\n\u2022 scalable execution mechanisms that accommodate very large logic programs at deeper recursion\nlevels\n\u2022 soft-unification based execution of LLM-generated logic programs as a principled encapsulation\nof the RAG retrieval process\nThe rest of the paper is organized as follows. Section 2 overviews the DeepLLM architecture described\nin [20] and its new extensions supporting the results in this paper. Section 3 overviews the generation of\nHorn clause programs with the online DeepLLM app. Section 4 explains the LLM-based generation of\nDual Horn clause programs and their uses to explore counterfactual consequences and theory falsifica-\ntion, Section 5 introduces the use of DCG grammars as a representation of LLM-generated answer and\nfollow-up question pairs. Section 6 describes fixpoint and GPU-supported minimal model computation\nfor the generated programs. Section 7 describes relation-extraction and visualization from the minimal\nmodels of the LLM-generated propositional programs. Section 8 introduces the soft-unification based\nencapsulation of the information retrieval against facts extracted from authoritative document collections.\nSection 9 discusses related work and Section 10 concludes the paper."}, {"title": "2 Recursive exploration of LLM dialog threads", "content": "Generative AI, with often human-like language skills is shifting focus from typical search engines to\nmore conversational interactions. Yet, the challenge remains that humans must still process and verify\nthis information, an often tedious task.\nOur answer to this, as implemented in the DeepLLM system is to automate the entire process. We\nstart with a simple \u201cinitiator goal\" and let the LLM dive recursively in its parametric memory and de-\nliver a detailed answer focused on the initiator and the trace of this chain of steps summarized as the\nshort term-memory maintained via its API. This automation also helps to minimize common issues like\ninaccuracies, made-up information, and biases that are often associated with LLMs.\nWe refer to [20, 18] for details of implementation of the DeepLLM system, as well as to its open-source code\u00b9 and its online demo2.\nThe DeepLLM system's active components (subclasses of the Agent class) are Interactors, Recursors,\nand Refiners:\n\u2022 Interactors manage input prompts and task breakdown\n\u2022 Recursors handle iterative exploration of subtasks\n\u2022 Refiners enhance clarity and relevance of LLM responses\nTo validate its reasoning steps, the system also relies on stored knowledge resources:\n\u2022 Ground truth facts: sentences collected from online sources or local documents\n\u2022 Vector store: enabler of \u201csemantic search\u201d via embeddings of sentences\nStarting from a succinct prompt (typically a nominal phrase or a short sentence describing the task)\nan Interactor will call the LLM via its API, driven by a Recursor that analyzes the LLM's responses and\nactivates new LLM queries as it proceeds to refine the information received up to a given depth.\nRefiners are Recursor subclasses that rely on semantic search in an embeddings store containing\nground-truth facts as well as on oracles implemented as specialized Interactors that ask the LLM for"}, {"title": "3 Generating propositional Horn clause programs with the DeepLLM app", "content": "We refer to [18] for an extensive list of LLM-generated Horn clause programs. We will just briefly\ndescribe here the DeeLLM app (see Fig. 1) that we will use for generating our logic programs. In\nthe case of the interaction shown in Fig. 1, the initiator goal \"computing stable models of answer set\nprograms\" starts the \u201cscientific concept explorer\" option and generates in the right side window a Horn\nclause program describing successive refinements of the initiator goal.\nThe DeeLLM app is written with the Streamlit\u00b3 webapp generator and offers the choice between\nGPT4, GPT3.5 or a local LLM, running as a server and supporting an OpenAI compatible API. It then\nlets the user choose between the Recursor, Advisor and Rater agents, providing for the latter a threshold\nlevel slider. The threshold informs the Rater oracle to accept or reject a generated rule head or fact (the\nhigher the threshold the stricter the accept decision). Options to set the maximum recursion depth and\nactivate relation extraction and visualization are also available."}, {"title": "4 Generating propositional Dual Horn clause programs", "content": "A Dual Horn clause is a disjunction of literals with at most one negative literal (or exactly one if it\nis a definite Dual Horn clause). A Dual Horn clause Program is a conjunction of Dual Horn clauses.\nWe represent a Dual Horn clause like \u00acpo V P1 V ... V Pn in an equivalent implicational form po \u2192\nP1 V . . . V pn, similarly to Prolog's representation of Horn clauses. We adopt a Prolog-like syntax, with\n\u2192 represented as \u201c=>\u201d and V represented as \u201c;\u201d. Note also that \u201cs => false\u201d represents a negated fact\nthe same way as \u201cs :- true\u201d would represent a positively stated fact.\nThe objective of Dual Horn programs is to describe (constructively) why something is not true i.e., to\nfalsify the initiator goal by back-propagating from its negative (or more generally, undesirable, unwanted,\nharmful, impossible, etc.,) consequences.\nFor instance, from a clause like p => q ; r ; s, assuming that p were true, we would infer that\nat least one of q, rands should be true. The contrapositive is that if q, rands are all false, then p\nshould be false as well. Like in the case of SLD-resolution on Horn clauses, this triggers a goal oriented\nprocess where successful falsification of all consequences results in falsification of the \u201ccounterfactual\"\nhypothesis that initiated the process.\nBy instructing the LLM to infer the negative consequences of the DeepLLM initiator goal, we can\nobtain a Dual Horn program."}, {"title": "5 From Self-generated follow-up question-answer chains to DCG grammars", "content": "DeepQA6 (see Fig. 2) is a DeepLLM-based application that explores recursively the \u201cmind-stream\" of\nan LLM via a tree of self-generated follow-up questions. Interestingly, by asking the LLM to generate\na set of follow-up questions to its own answers creates (especially when the process recurses) a more\nfocused \"stream of thoughts\u201d, possibly as an emergent property of its \u201cin-context learning\u201d abilities.\nAfter started from an initiator question on a topic of the user's choice, the app explores its tree of\nfollow-up questions up to a given depth. As output, it generates a Definite Clause Grammar that can be\nimported as part of a Prolog program. The DCG, in generation mode, will replicate symbolically the\nequivalent of the \u201cstream of thoughts\u201d extracted from the LLM interaction, with possible uses of the\nencapsulated knowledge in Logic Programming applications."}, {"title": "6 Computing minimal models of LLM-generated logic programs", "content": "At deeper recursion levels, the generated logic programs, providing a symbolic representation of an\nLLM's parameter memory can quickly reach millions of clauses, ready to reason with.\nTo take advantage of the significant acceleration provided by GPUs we have implemented a torch-\nbased linear algebraic minimal model computation algorithm along the lines of [13]."}, {"title": "6.1 Minimal model computation with a GPU-friendly Torch-based Linear Algebra Algorithm", "content": ""}, {"title": "6.2 Fixpoint-based minimal model computation", "content": "It is not unusual to have loops in the propositional Horn Clause program connecting the LLM gener-\nated items by our recursors and refiners that would create problems with Prolog's depth-first execution\nmodel. As using a SAT-solver would be an overkill in this case, given that Horn Clause and Dual Horn\nclause formula satisfiability is known to be polynomial, we have implemented a simple low-polynomial\ncomplexity [6] propositional satisfiability checker and model builder11.\nThe model builder works by propagating truth from facts to rules until a fix point is reached. Given\na Horn Clause h : -b1,b2, ...,bn, when all b\u2081 are known to be true (i.e., in the model), h is also added\nto the model. If integrity constraints (Horn clauses of the form false : -b1,b2,...,bn) have also been\ngenerated by the oracle agents monitoring our refiners, in the advent that all b1,b2,...,bn end up in the\nmodel, b1,b2, ..., bn implying false signals a contradiction and thus unsatisfiability of the Horn formula\nassociated to the generated program. However as the items generated by our recursive process are not\nnecessarily expressing logically connected facts (e.g., they might be just semantic similarity driven asso-\nciations), turning on or off this draconian discarding of the model is left as an option for the application\ndeveloper. Also, the application developer can chose to stop as soon as a proof of the original goal\nemerges, in a way similar to goal-driven ASP-solvers like [1], irrespectively to unrelated contradictions\nelsewhere in the program."}, {"title": "7 Generating relation triplets for knowledge graphs", "content": "Our DeepLLM app offers an option to generate from the minimal model of the program a relation graph\n(see Fig. 3) consisting of implication links (marked with \":\") to which it adds generalization links\n(marked with \"is\").\nImplication links are extracted directly from the logic program while generalization links, serving as\nadditional explanations, are generated by the LLM via an additional request.\nSeveral other types of relation graphs can be generated depending on the planned reasoning method.\nOne of them is extraction of <subject, verb, object> (SVO) triplets obtained by prompting the LLM\nto split a complex sentence in simpler ones and extract from each simple sentence an SVO triplet.\nAnother is a hybrid method, combining relations extracted by using dependency grammars [21],\nembeddings-based similarity relations, Wordnet-based and LLM-generated hypernyms and meronyms."}, {"title": "8 Reasoning with soft unification on noisy facts", "content": "The minimal models of LLM-generated Horn clause programs encapsulate facts and their consequences\nelicited from DeepLLM's initiator queries in the form of natural language sentences. When writing a\nlogic program that performs symbolic reasoning relying on a ground fact database of such sentences, an\ninteresting form of abductive reasoning emerges. When hitting an undefined ground sentence, intended\nas a query to match database facts we can rely on vector embedding of the sentences and proximity of\nthe query and the facts in the vector space as a \u201cgood enough\u201d match, provided that the semantic distance\nbetween them is below a given threshold. We will next describe a proof of concept of this strategy that\nwe illustrate on a small quotation dataset consisting of a few sentences12."}, {"title": "9 Related Work", "content": "By contrast to \u201cneuro-symbolic\u201d AI [14], where the neural architecture is closely intermixed with sym-\nbolic steps, in our approach the neural processing is encapsulated in the LLMs and accessed via a declar-\native, high-level API. This reduces the semantic gap between the neural and symbolic sides as their\ncommunication happens at a much higher, fully automated and directly explainable level.\nOur recursive descent algorithm shares the goal of extracting more accurate information from the\nLLM interaction with work on \u201cChain of Thought\" prompting of LLMs [22, 9] and with step by step [8]\nrefinement of the dialog threads. Our approach shares with tools like LangChain [3] the idea of piping\ntogether multiple instances of LLMs, computational units, prompt templates and custom agents, except\nthat we fully automate the process without the need to manually stitch together the components.\nWe have not found any references to the use of Dual Horn clauses in logic programming but it is\na well known result [16]) that their complexity in the propositional case is polynomial, similarly to\ntheir of Horn clause counterparts. This fact makes them also good generation targets for LLM-extracted\nknowledge processing.\nWe have not found anything similar to generating question-answer-follow-up question chains, al-\nthough it is common practice for chatbots to suggest (a choice between) follow-up questions15.\nOur torch-based model-computation algorithm follows closely the matrix-computation logic of [13],\nour contribution being its succinct and efficient GPU-friendly implementation.\nInterest in several forms of soft-unification has been active [4, 2, 10] as differentiable substitute of\nsymbolic unification in neuro-symbolic systems. By contrast, our focus in this paper is flexible infor-\nmation retrieval of LLM-generated natural language content, for which high quality embeddings were\navailable either from LLM APIs or local resources like the torch-based sentence-transformers [12]."}, {"title": "10 Conclusion", "content": "It is now undeniable that Generative AI is a major disruptor not just of industrial fields ranging from\nsearch engines, automation of software development and robotics to medical and legal advisory systems,\nbut also a disruptor of research fields, including symbolic AI as we know it and machine Learning itself.\nIn particular, results produced by dominant ML or NLP techniques as well as work on integration of\nneural and symbolic systems have become replaceable by much simpler applications centered around\nLLM queries and RAG systems. In fact, by concentrating the knowledge encapsulated in its parametric\nmemory into a single declarative interface, Generative AI can replace complex, labor-intensive software\nfunctionality with a simple LLM API call or a question in one's favorite natural language.\nThis motivates our effort to \"join the disruption\" and explore several new ways to elicit the knowledge\nencapsulated in the LLMs' parametric memory as logic programs, together with an investigation of their\noptimal inference execution methods. We have not just exposed as logic programs the several kinds of\nknowledge snippets extracted by recursive automation LLM dialog threads, but we have also devised\nefficient inference execution mechanisms for them.\nWe hope that this effort has revealed some natural synergies between Generative AI systems and\nlogic programming tools, ready to fill gaps like the lack of rigorous reasoning abilities of the LLMs, their\nlack of alignment to the user's intents and their known deficiencies on factuality."}]}