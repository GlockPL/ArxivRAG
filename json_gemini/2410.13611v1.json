{"title": "H2OVL-Mississippi Vision Language Models Technical Report", "authors": ["Shaikat Galib", "Shanshan Wang", "Guanshuo Xu", "Pascal Pfeiffer", "Ryan Chesler", "Mark Landry", "Sri Satish Ambati"], "abstract": "Smaller vision-language models (VLMs) are becoming increasingly important for privacy-focused, on-device applications due to their ability to run efficiently on consumer hardware for processing enterprise commercial documents and images. These models require strong language understanding and visual capabilities to enhance human-machine interaction. To address this need, we present H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs using 240 hours of compute on 8 \u00d7 H100 GPUs. H2OVL-Mississippi-0.8B is a tiny model with 0.8 billion parameters that specializes in text recognition, achieving state of the art performance on the Text Recognition portion of OCRBench and surpassing much larger models in this area. Additionally, we are releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use cases, exhibiting highly competitive metrics across various academic benchmarks. Both models build upon our prior work with H2O-Danube language models, extending their capabilities into the visual domain. We release them under the Apache 2.0 license, making VLMs accessible to everyone, democratizing document AI and visual LLMs.", "sections": [{"title": "Introduction", "content": "The field of vision-language models (VLMs) has rapidly evolved, with significant strides made in connecting visual encoders to language models to enhance the capabilities of AI in handling diverse visual and textual tasks. While current state-of-the-art models deliver impressive results, they often depend on large architectures that require extensive computational resources. The H2OVL-Mississippi models seek to address this limitation by offering efficient, smaller-scale alternatives that can compete with larger models across various vision-language tasks, especially in Optical Character Recognition (OCR) and document analysis. This paper introduces the H2OVL-Mississippi-0.8B and H2OVL-Mississippi-2B models, detailing their architecture, training methodology, and performance evaluations to highlight their efficiency and adaptability for real-world multimodal tasks. By adopting a data-driven approach, the H2OVL-Mississippi models provide a scalable and efficient solution for applications in document understanding and multimodal reasoning.\nThe development of the H2OVL-Mississippi models is guided by two primary goals: specialization and versatility. The H2OVL-Mississippi-0.8B model is specifically optimized for OCR and document-centric tasks, to provide high accuracy and efficiency in structured information extraction, even in resource-constrained environments. The H2OVL-Mississippi-2B model is designed to be a general-purpose vision-language model, capable of performing a wide range of multimodal tasks such as image captioning, visual question answering (VQA), and reasoning. By combining these two approaches, the H2OVL-Mississippi series aims to deliver models that are not only task-specific but also versatile enough to adapt to diverse visual and textual challenges, ensuring a comprehensive solution for multimodal AI applications."}, {"title": "Related Works", "content": "Early VLMs focused on connecting vision encoders to language models through trainable connectors, allowing models to align visual and textual representations. Notable examples include Flamingo [1] and BLIP-2 [2], which achieved strong results in tasks such as image captioning and visual question answering (VQA) by leveraging pre-trained vision and language components.\nLLaVA [3] extended this approach by introducing multimodal instruction tuning, enabling models to follow human instructions across visual tasks, such as interactive dialogue about images. This capability set a new benchmark for multimodal interaction and improved the model's ability to transfer knowledge across tasks.\nFurther advancements were made with models like PaLI [4], Florence-2 [5], and Unified-IO 2 [6], which jointly trained vision and language components instead of relying on frozen pre-trained encoders. This joint training approach improved the model's performance on complex, cross-modal tasks such as document parsing and visual reasoning.\nDecoder-only models, like Fuyu [7] and CM3 [8], streamlined the architecture by using a single transformer to process both image and text inputs. This simplification increased training and inference efficiency, making these models attractive for scenarios where computational resources are limited.\nRecently, encoder-decoder models, like Qwen2-VL [9] utilize a Naive Dynamic Resolution mechanism, enabling it to process images at varying resolutions by dynamically adjusting the number of visual tokens. This allows the model to handle complex visual tasks such as detailed image captioning and OCR with improved efficiency and accuracy. Similarly, InternVL 1.5 [10] adopts a high-resolution strategy, breaking down images into tiles, which improves the model's ability to capture fine details across a range of vision tasks. Other models, such as Mini-Monkey [11], tackle high-resolution image processing challenges by introducing multi-scale adaptive cropping, which allows models to capture small or irregularly shaped objects more accurately.\nThe H2OVL-Mississippi-0.8B and H2OVL-Mississippi-2B models build on these advancements by utilizing large and diverse datasets to further enhance multimodal performance, ensuring effective handling of a broad range of visual and textual tasks."}, {"title": "Model Architecture", "content": "The architecture of the H2OVL-Mississippi model takes inspiration from the LLaVA [3] and InternVL [10] series, following a ViT-MLP-LLM configuration, as shown in Figure 1. It uses a transformer-based setup comprising a vision encoder, an MLP layer, and a large language model (LLM). The vision encoder extracts features from images, while the LLM generates text. The MLP layer acts as a bridge between the vision encoder and the LLM.\nSpecifically, the H2OVL-Mississippi architecture integrates the InternViT-300M as its vision encoder and supports two variations for the language model: Danube-2 (1.8 billion parameters) [12] and Danube-3 (500 million parameters) [13], providing flexibility based on computational requirements.\nThe architecture uses a dynamic resolution strategy [10] that adjusts image processing based on the image's aspect ratio and resolution. It divides each image into 448x448 pixel tiles, using between 1 and 6 tiles for full\ncoverage of the image (Figure 1a). During training, the number of tiles varies, producing 256 to 1,590 visual tokens, allowing the model to adapt to different image dimensions while optimizing token usage and preserving key details.\nTo enhance computational efficiency, the architecture incorporates a pixel shuffle operation applied to the Vision Transformer (ViT) embeddings, reducing the number of visual tokens per 448x448 tile to 256. Typically used in image super-resolution tasks to rearrange and combine pixels from low-resolution images, pixel shuffling is adapted here to efficiently decrease the token count while maintaining significant information from each tile. This adaptation ensures effective processing of high-resolution images with reduced computational demands.\nFurthermore, the H2OVL-Mississippi-2B model uses a multi-scale adaptive cropping (MSAC) strategy, as outlined in the Mini-Monkey report [11]. MSAC addresses the sawtooth effect [11], a common issue in traditional cropping techniques, by generating multi-scale representations. This capability enables the model to capture features at different scales, improving performance on tasks involving small or irregularly shaped objects, such as document parsing and image recognition. Similar to the dynamic resolution strategy, MSAC varies the number of tiles from 2 to 6, as illustrated in Figure 1(b).\nFinally, a resized version of the original image, scaled to 448x448 pixels, is included in the set of tiles to provide the model with a complete view of the image, improving its ability to capture the overall layout information (Figure 1(c)).\nThese advanced image processing techniques enable the model to balance efficiency and visual detail, ensuring strong performance across multimodal tasks. The dynamic resolution and MSAC strategies allow it to adapt to diverse image sizes and aspect ratios, optimizing token use while preserving image context. This versatility makes H2OVL-Mississippi a scalable and effective solution for tasks that require information extraction from fine-grained images."}, {"title": "Training Methodology", "content": "Training a vision language model involves learning complex relationships between images and corresponding texts by jointly optimizing a pre-trained vision encoder (ViT), a pre-trained language model (LLM), and a randomly initialized MLP projector that connects the two. LLaVA [3] demonstrated that pre-training the connector with image-caption pairs significantly enhances performance outcomes. Qwen2-VL [9] highlighted the benefits of pre-training visual components on large-scale image-text datasets, improving the model's capacity to integrate and interpret multimodal information effectively. Following this evidence, the H2OVL-Mississippi models employ a pre-training and fine-tuning strategy: pre-training focuses on aligning visual and textual features, while fine-tuning is dedicated to task-specific modeling. In the following sections, we describe the intent, training method and dataset distribution for the H2OVL-Mississippi-0.8B and H2OVL-Mississippi-2B models."}, {"title": "H2OVL-Mississippi-0.8B Model", "content": "The H2OVL-Mississippi-0.8B model is designed specifically for OCR and document understanding, with a focus on accurately extracting, recognizing, and interpreting text from images, particularly in complex and structured visual contexts. Its training methodology and datasets are tailored to optimize performance for these tasks.\n\u2022 Pre-training: The pre-training phase utilizes 11 million conversation examples covering a diverse range of tasks, including general QA, image captioning, OCR, and reasoning, as depicted in Figure 2a. This diverse dataset helps the model achieve a well-balanced and unbiased state, establishing a strong foundation for the subsequent OCR-specific fine-tuning. The pre-training process consists of two steps. In step1, only the MLP projector is optimized, while both the ViT and LLM remain frozen, using approximately 3 percent of the pre-training dataset. In step 2, the MLP and LLM are jointly optimized, with the ViT still frozen, this time using the full pre-training dataset.\n\u2022 Fine-tuning: The fine-tuning dataset consists of approximately 8 million examples, with a strong emphasis on OCR tasks such as text recognition, document parsing, and structured information extraction. To enhance the model's specialization in OCR, other general task datasets are excluded, as illustrated in Figure 3a. During this stage, all three components (ViT, MLP, and LLM) are optimized jointly."}, {"title": "H2OVL-Mississippi-2B Model", "content": "The H2OVL-Mississippi-2B model is designed to excel in document intelligence tasks while maintaining versatility as a general-purpose visual language model. During data composition, a significant portion (58%) of OCR and document-related data was incorporated in pre-training to optimize document visual feature extraction and alignment. In the fine-tuning stage, we balanced the data distribution to ensure the model's performance across a diverse range of domains and tasks.\n\u2022 Pre-training: The pre-training dataset consists of 5 million conversation pairs, focusing on three key areas: OCR data, image captioning and text-only datasets. The OCR data trains the model to recognize and interpret text embedded within images, improving its skills in document understanding and text extraction from visual sources. The image captioning data connect visual inputs with corresponding textual descriptions, enhancing the model's ability to associate images with relevant language. The text-only datasets ensure that the model maintains strong language understanding capabilities even when visual inputs are absent. The distribution of this data is illustrated in Figure 2b. During this pre-training phase, only the vision encoder and MLP projector were trained together for 4 epochs, while the LLM remained frozen.\n\u2022 Fine-tuning: The fine-tuning stage of H2OVL-Mississippi-2B utilized 12 million conversation examples to enhance task-specific performance across various domains. The primary tasks included general question-answering (QA), which focused on handling multi-image, single-image, and text-only inputs. Additionally, OCR and document understanding were emphasized for extracting structured information from both multi- and single-image sources. Complex tasks involving reasoning, logic, and programming were also incorporated, requiring problem-solving with mixed input types. Furthermore, the fine-tuning covered captioning, textbook Q&A, image comparison, and chart and table understanding to ensure broad task coverage and versatility, as illustrated in Figure 3b. During this stage, the full model was trained for a total of 3 epochs."}, {"title": "Evaluation", "content": "In this section, we present evaluation of H2OVL-Mississippi across a variety of dimensions, focusing on (1) General Vision-Language benchmarks, and (2) OCR and document-centric benchmarks."}, {"title": "General Vision-Language benchmarks", "content": "Table 3 provides a comprehensive comparison of models across a range of benchmarks, evaluating their strengths and weaknesses. It includes several categories of models, such as current state-of-the-art, legacy state-of-the-art, and similarly sized models. Each model's performance is assessed using benchmarks like MMBench [14], MMStar [15], MMMU [16], Math Vista [17], Hallusion Bench [18], AI2D [19], OCRBench [20], and MMVet [21], offering insights into their versatility and specialized capabilities.\nModels classified under legacy state-of-the-art, such as GPT-4v (1106, detail-high) and Gemini-1.0-Pro, illustrate how quickly the field evolves. These models, though previously considered cutting-edge, now achieve lower scores, especially on advanced benchmarks like MMStar and OCRBench. For instance, GPT-4v scores 56.4 on average, with an OCRBench score of 678, which is considerably behind the newer models.\nWithin the category of similar size models, H2OVL-Mississippi-2B demonstrates competitive performance, with an average score of 54.4. H2OVL-Mississippi-2B excels in benchmarks like Math Vista (56.8) and OCRBench (782), positioning it as a strong model for multimodal and OCR tasks. Compared to its closest peer, Qwen2-VL-2B, H2OVL-Mississippi-2B shows a slight lag in benchmarks like MMBench and MMStar but remains strong in OCR-related tasks, where it outperforms several similarly sized models. The trend among similar size models highlights that while models like H2OVL-Mississippi-2B and Qwen2-VL may not yet reach state-of-the-art performance, they are highly effective for specific use cases such as text extraction and mathematical reasoning tasks.\nWe utilized VLMEvalKit\u00b2 [22] for measuring the performance of the models. For both models, we set the maximum tile number for each image to 6. Additionally, the MSAC image preprocessing function was implemented for H2OVL-Mississippi-2B."}, {"title": "OCR and Document centric benchmarks", "content": "OCR Benchmarks. We conducted a detailed comparative analysis of various vision-language models (VLMs), including the latest general OCR model (e.g., GOT-OCR2.0[23]), across multiple evaluation tasks from OCRBench[20], a benchmark designed to rigorously assess OCR performance.The tasks covered include Text Recognition, Scene Text-centric VQA, Document-oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). Both H2OVL-Mississippi-0.8B and H2OVL-Mississippi-2B demonstrated competitive performance across the board.\nThe H2OVL-Mississippi-0.8B model stands out by achieving the highest score in OCRBench Text Recognition (274), significantly outperforming all other models, including those with much larger parameter sizes, such as InternVL2-26B and MiniCPM-V2.6. This result highlights the model's efficiency and capability, particularly for OCR-specific tasks. Despite having fewer parameters, the 0.8B model consistently surpasses larger models in text recognition, making it an optimal choice for resource-constrained environments where high OCR performance is required.\nThe H2OVL-Mississippi-2B model also demonstrates robust performance across a range of tasks. With a total score of 782, it outperforms several models that have much larger sizes, proving its overall effectiveness. In particular, the 2B model shows competitive results in Text Recognition (252), Scene Text VQA (171), Document-Oriented VQA (140), and KIE (166), making it an excellent candidate for general document understanding and extraction tasks.\nText Oriented VQA benchmarks. In addition to the OCRBench evaluation, we further investigate our model's detailed visual perception capabilities by assessing its performance on text-oriented VQA datasets, including TextVQA [25], DocVQA [26], and InfoVQA [27]. As summarized in Table 5, H2OVL-Mississippi-2B demonstrates commendable overall performance across all tasks. Notably, it achieves better or comparable scores even against much larger models like Cambrian-13B (13B parameters), showing its efficiency in handling text-based VQA tasks with significantly fewer parameters. Despite its smaller size (2.1B parameters), H2OVL-Mississippi-2B performs competitively on TextVQA and DocVQA and demonstrates steady results on InfoVQA, underscoring the model's robustness in diverse visual question-answering contexts."}, {"title": "Conclusions and Future Work", "content": "We introduce H2OVL-Mississippi, a series of small language models consisting of H2OVL-Mississippi-2B and H2OVL-Mississippi-0.8B released open source under Apache 2.0. Our models show competitive performance compared to popular models of similar size across a variety of benchmarks, including general vision-language evaluations, OCR and document-centric tasks. H2OVL-Mississippi is built on our continuous efforts to contribute to the growing ecosystem of open source small language models. We are confident that our models can play a pivotal role in a wide range of applications, from typical chatting and fine-tuning for specific use cases to on-device offline applications on mobile phones or edge devices.\nThrough this project, we gained valuable experience in the end-to-end development of vision-language models, including data collection and preparation, input preprocessing, model architecture selection, training, and hyperparameter tuning. These learnings have prepared us to tackle more complex challenges in future work, such as:\n\u2022 Improving multilingual capabilities to extend model support for diverse languages and scripts.\n\u2022 Incorporating additional modalities, such as video and audio, to enable richer multimodal understanding.\n\u2022 Scaling up model sizes to 4B, 7B, or even larger, to further enhance performance and address more complex tasks.\n\u2022 Addressing agent-based tasks that involve decision-making and real-world interaction, enabling the models to function effectively in dynamic environments.\n\u2022 Enhancing fine-grained visual capabilities to improve performance in tasks that require distinguishing between highly similar objects or parsing intricate scenes."}]}