{"title": "mDAE : modified Denoising AutoEncoder for\nmissing data imputation", "authors": ["Mariette DUPUY", "Marie CHAVENT", "R\u00e9mi DUBOIS"], "abstract": "This paper introduces a methodology based on Denoising AutoEncoder (DAE) for miss-\ning data imputation. The proposed methodology, called mDAE hereafter, results from a\nmodification of the loss function and a straightforward procedure for choosing the hyper-\nparameters. An ablation study shows on several UCI Machine Learning Repository datasets,\nthe benefit of using this modified loss function and an overcomplete structure, in terms of\nRoot Mean Squared Error (RMSE) of reconstruction. This numerical study is completed\nby comparing the mDAE methodology with eight other methods (four standard and four\nmore recent). A criterion called Mean Distance to Best (MDB) is proposed to measure how\na method performs globally well on all datasets. This criterion is defined as the mean (over\nthe datasets) of the distances between the RMSE of the considered method and the RMSE\nof the best method. According to this criterion, the mDAE methodology was consistently\nranked among the top methods (along with SoftImput and missForest), while the four more\nrecent methods were systematically ranked last. The Python code of the numerical study\nwill be available on GitHub so that results can be reproduced or generalized with other\ndatasets and methods.", "sections": [{"title": "Introduction", "content": "With the rapid increase in data collection, missing values are a ubiquitous challenge across various domains.\nData may be missing for several reasons. For instance, it was never collected, records were lost or merging\nseveral datasets failed. It is then generally necessary to deal with this problem before performing machine\nlearning methods on these data. Several options are available to address this issue, including removing or\nrecreating the missing values. Removing rows or columns containing missing data results in a considerable\nloss of information when missing data is distributed across multiple locations in the dataset. One usually\nprefers missing-data imputation, which consists of filling missing entries with estimated values using the\nobserved data. Missing data imputation is a very active research area (Van Buuren, 2018; Little & Rubin,\n2019) with more than 150 implementations available according to Mayer et al. (2021). This paper focuses\non state-of-the-art imputation methods categorized as standard machine learning, deep learning or opti-\nmal transport. Methods based on standard machine learning include, among others, k-nearest neighbours\n(Troyanskaya et al., 2001), matrix completion via iterative soft-thresholded SVD (Mazumder et al., 2010),\nMultivariate Imputation by Chained Equations (Van Buuren & Groothuis-Oudshoorn, 2011) or MissForest\n(Stekhoven & B\u00fchlmann, 2012). Methods based on deep learning include, among others, Generative Adver-\nsarial Networks (Goodfellow et al., 2014; Yoon et al., 2018), Variational AutoEncoders (Kingma & Welling,\n2013; Ivanov et al., 2018; Mattei & Frellsen, 2019; Peis et al., 2022) and methods based on Denoising Au-"}, {"title": "The mDAE method", "content": "AutoEncoders (AE) (Bengio et al., 2009) are well-known artificial neural networks used to learn efficient\nrepresentation of unlabeled data via an encoding function and to recreate the input data via a decoding\nfunction. Here, we are dealing with the special case of tabular numerical data, and we suppose that these\ndata have been normalized so that the p features have zero mean and unit variance. This normalization via\nfeature standardization is more appropriate here than normalizing the values between 0 and 1, as is often\ndone when using autoencoders. The input of the AE is a then set of n observations {x1,..., xn} in R\u00ba which\nforms the rows of a standardized data matrix X = (xij) of dimension n\u00d7p, where p is the number of features.\nThe encoding function fe of a basic autoencoder (see Figure 1) transforms an input x \u2208 R\u00ba into a latent\nvector yi \u2208 R9:\nyi = fo(xi) = s(Wx\u2081 + b),\nwhere W \u2208 RIXP is a weight matrix, b \u2208 RP is a bias vector and s is an activation function (e.g., ReLU or\nsigmoid). The decoding function go then transforms the latent vector yi \u2208 R\u00ba into an output zi E RP:\nzi = gor(yi) = s(W'yi + b'),\nwhere W' \u2208 RPXq and b' \u2208 R\u00ba. Here, the activation function s in the output layer must be the identity\nfunction, since we are trying to reconstruct inputs that take their values in R. In fact, the sigmoid (resp.\nReLu) activation function gives output values between 0 and 1 (resp. positive values) which is not appropriate\nhere.\nIn general, autoencoders have more than one hidden layer and the parameters 0 = (W1, ..., WK, b1, ..., bk)\nof the encoder and 0\u2032 = (W1, ..., W\u02b9k, b\u2081, ..., b'k) of the decoder, are learned by minimization of the so called\nreconstruction loss. With standardized numerical data, the reconstruction loss usually used to learn weights\nand biases of an autoencoder is the L2 loss defined by:\n$L_{AE} = \\sum_{i=1}^{n} ||x_{i} - (g_{er} o f_{o})(x_{i})||^{2} = ||X \u2013 Z||_{F}^{2}, $\nwhere L is the loss function defined here as the squared Euclidean distance between the input xi and its\nreconstruction zi = (go' o fo)(xi), and ||XZ||F is the Frobenius norm between the data matrix X and its\nreconstructed matrix Z. Note that this criterion favors the reconstruction of features (columns of X) with\nhigh variance. It is therefore important that the data matrix X is standardized.\nDenoising AutoEncoders (DAE) (Vincent et al., 2008) are autoencoders defined to remove noise from a\ngiven input. To do this, an autoencoder is trained to output the original data using corrupted data in the"}, {"title": "Imputing missing values using the mDAE methodology", "content": "As Pereira et al. (2020) point out in their review article, almost all works using DAEs to impute missing\ndata boils down to applying a DAE to pre-imputed data (e.g. by mean imputation). Let X be now the\nincomplete standardized data matrix (the data matrix with missing values). Pre-imputation of X by the\nmean of each feature simply consists of replacing missing values with 0 since the data are standardized and,\ntherefore, the feature means are all equal to 0. The pre-imputed data matrix \u2717 writes then as the projection\nof X onto the observed entries:\n$\\overline{X} = {P_{\\Omega}}(X) = \\begin{cases} X_{ij} & \\text{if} (i, j) \\in \\Omega, \\\\ 0 & \\text{if} (i, j) \\notin \\Omega. \\end{cases}$\nwhere \u03a9 is the set of indices (i, j) \u2208 {1, ..., n} \u00d7 {1, ..., p} where the values xij are not missing. The DAE is\nthen trained to reconstruct the pre-imputed data matrix \u2611 by minimization of the reconstruction loss (2)\nwhich in this case is:\n$L_{DAE} = \\sum_{i=1}^{n} ||\\overline{x_{i}} - (g_{er} o f_{o})(N(\\overline{x_{i}}))||^{2} = ||P_{\\Omega}(X) \u2013 Z||_{F}^{2}, $\nwhere Z is now the reconstruction of the pre-imputed matrix X. After training, the missing values in X are\nreplaced by those reconstructed in Z and the imputed data matrix is:\n$X = P_{\\Omega}(X) + P_{\\Omega^{c}} (Z),$\nwhere is the set of indices (i, j) \u2208 {1, ..., n} \u00d7 {1, ..., p} where xij is missing.\nIf using a pre-imputed matrix X solves the problem of the loss function that is unable to handle missing\nvalues, minimizing the reconstruction loss (3) learns the DAE to reconstruct zeros at the locations of the\nmissing values, which is irrelevant (see Figure 3). Our proposal is then not only to apply a DAE to the\npre-imputed data matrix as in previous works, but also to modify the reconstruction error (3) to skip these\nlocations (see Figure 4). This methodology, herafter called mDAE, performs a DAE on standardized and\npre-imputed data, using the following loss function:\n$L_{mDAE} = ||{P_{\\Omega}}(X) \u2013 P_{\\Omega^{c}}(Z)||_{F}^{2},$"}, {"title": "Choice of the hyper-parameter \u03bc", "content": "The hyper-parameter \u03bc of the mDAE methodology for missing values imputation, is the proportion of zeros\nused to corrupt the data with the masking noise (red crosses in N(\u017e\u00bf) in Figure 4). This hyper-parameter\ncan be chosen randomly in a grid of values \u03bcin [0,1]. Alternatively, it can be chosen through an optimized\nprocedure to minimize an error of reconstruction of the missing values. For that purpose, the non-missing\nvalues of the original data are split into two sets: a training set to learn the parameters and a validation set"}, {"title": "Choice of the structure", "content": "Two families of structures are known for autoencoders. The undercomplete case where the hidden layer\nis smaller than the input layer and the overcomplete case where it is bigger. If overcomplete structure is\nnot relevant with autoencoders, it is well-known that denoising autoencoders work well with overcomplete\nstructures. Here, a grid of 6 simple structures (2 undercomplete and four overcomplete) is suggested to\nchoose the \"best\" structure when using the mDAE method (see Figure 5). For each structure in this grid,\nthe error of reconstruction of the missing values is estimated on validation data, using the same procedure as\nfor the selection of the hyper-parameter \u03bc (see section 2.2). Ideally, the hyper-parameter \u03bc and the structure\nshould be chosen simultaneously by exhaustively considering all possible combinations. However, alternative\ngrid search exists, for instance, by sampling a given number of candidates from the parameter space."}, {"title": "Numerical study", "content": "The first part of this numerical study concerns the properties of the mDAE methodology. More specifically,\nan ablation study is conducted to verify the relevance of the choices made to construct this methodology."}, {"title": "Ablation study of the mDAE methodology", "content": "An ablation study is a methodology used to evaluate the importance of different components of an algorithm,\nby comparing the results obtained with and without this component. Here, the following components of the\nmDAE methodology are studied:\n\u2022\nthe use of the modified reconstruction loss (5) rather than the standard loss (3),\n\u2022\nthe use of an optimized value of the hyper-parameter \u03bc (as described section 2.2) rather than a value\nchosen randomly in [0, 1],\n\u2022\nthe use of an overcomplete structure (the 5th structure in Figure 5) rather than an undercomplete\nstructure (the 2nd structure in Figure 5).\nTable 2 shows the results of the ablation study for the seven datasets and 20% of MCAR artificial missing\nvalues. The mean value over the B sets of artificial missing values (\u00b1 the standard deviation) of the RMSE\nof reconstruction of the artificial missing values is calculated for each dataset with the mDAE method, with\nthe method deprived of its modified loss function (i.e. with a standard L2 loss function), with the method\ndeprived of its optimized choice of \u03bc (i.e. with a random choice), with the method deprived of its overcomplete\nstructure (i.e. with an under complete structure). Each time, the loss of imputation quality (i.e. the increase\nof the mean RMSE) is measured between the mDAE without one of the three components (the modified\nloss, an optimized choice of \u03bc or an overcomplete structure) and the complete mDAE. For instance, for the\nbreast cancer dataset, using the standard L2 loss increases the mean RMSE of 46.99% = $\\frac{0.685-0.466}{0.466}$"}, {"title": "Comparison with other methods", "content": "This section compares the mDAE method with four relatively classic and four more recent methods (see\nTable 3). The four first methods are KNN ((Troyanskaya et al., 2001)) where missing values are replaced by\na weighted average of the k-nearest neighbours, SoftImput (Mazumder et al., 2010) based on iterative soft-\nthresholded SVD, and two iterative chained equation methods (Van Buuren & Groothuis-Oudshoorn, 2011)\nwhich model features with missing values as a function of the others: the missForest method (Stekhoven &\nB\u00fchlmann, 2012) is based on Random Forests and the Bayesian Ridge method is based on ridge regressions,\nto estimate at each step the regression functions. The four others (more recent) methods in Table 3 are\nGAIN (Yoon et al., 2018) which is an adaptation of Generative Adversarial Networks (GAN) (Goodfellow\net al., 2014) to impute missing data, MIWAE Mattei & Frellsen (2019) which is an adaptation of Variational\nAutoEncoders (VAE) (Kingma & Welling, 2013), and two methods using optimal transport: the algorithm\ncalled Batch Sinkhorn Imputation proposed by Muzellec et al. (2020), and the method TDM proposed by\nZhao et al. (2023).\nFor KNN and SoftImpute, the hyperparameters are selected through cross-validation. According to the\nimplementations used for the two chained equation methods, the hyperparameters of the Bayesian ridge\nregressions are estimated during the fits of the model. The hyperparameters of the Random Forests are 100\ntrees, and all features are considered when looking for the best split (i.e., bagged trees). The hyperparameter\nsettings recommended in the corresponding papers and implementations are used for the four last methods.\nFor the mDAE method, the settings studied section 3.1 (choice of \u00b5 by crossvalidation and the overcomplete\nstructure 5 of Figure 5) are used. More favourable settings for the mDAE method would have been to select"}, {"title": "Conclusion", "content": "This article proposes a methodology for missing data imputation, based on DAE, as well as a procedure for\nchoosing the hyper-parameters (the proportion of noise u and the structure of the network). An ablation\nstudy of this method was performed with different datasets, different types and proportions of missing data.\nIt showed the relatively small improvement of the results when the hyper-parameter \u03bcis chosen by cross-\nvalidation rather than randomly. On the contrary, using an overcomplete rather than an undercomplete\nnetwork seems appropriate. A specific study is still required to confirm this result, which would enable to\nrecommend the use of a random u and an overcomplete structure.\nThen, a numerical study compared the proposed mDAE method with eight other standard or recent missing\nvalues imputation methods. The results showed the good behavior of SofImput, mDAE and missForest.\nA new criterion called Mean Distance to the Best (MDB) was used to compare the methods globally over\nall the considered datasets and to rank them. The four most recent methods based on deep learning and\noptimal transport were systematically found in the last four positions for all types and proportions of artificial\nmissing values. One might think these methods give better results with image or natural language processing\ndata. This should be tested more thoroughly. The Python code for this numerical comparison will be made\navailable on GitHub so that it can be reproduced with other datasets or completed with other methods.\nFinally, the specific features of the mDAE method should make it possible to consider block-wise missing\nvalues by imposing a block-wise structuring of the masking noise. This type of missing data is frequent, for\ninstance, with Electronic health records, longitudinal studies or time series data, where failures in sensors\nand communication can result in a loss of multiple consecutive data points."}]}