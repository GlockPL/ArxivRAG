{"title": "EXADAM: THE POWER OF ADAPTIVE CROSS-MOMENTS", "authors": ["Ahmed M. Adly"], "abstract": "This paper introduces EXAdam (EXtended Adam), a novel optimization algorithm that builds upon the widely-used Adam [1] optimizer. EXAdam incorporates three key enhancements: (1) new debiasing terms for improved moment estimation, (2) a gradient-based acceleration mechanism for increased responsiveness to the current loss landscape, and (3) a dynamic step size formula that allows for continuous growth of the learning rate throughout training. These innovations work synergistically to address limitations of the original Adam algorithm, potentially offering improved convergence properties, enhanced ability to escape saddle points, and greater robustness to hyperparameter choices. I provide a theoretical analysis of EXAdam's components and their interactions, highlighting the algorithm's potential advantages in navigating complex optimization landscapes. Empirical evaluations demonstrate EXAdam's superiority over Adam, achieving 48.07% faster convergence and yielding improvements of 4.6%, 4.13%, and 2.39% in training, validation, and testing accuracies, respectively, when applied to a CNN trained on the CIFAR-10 dataset [2]. While these results are promising, further empirical validation across diverse tasks is essential to fully gauge EXAdam's efficacy. Nevertheless, EXAdam represents a significant advancement in adaptive optimization techniques, with promising implications for a wide range of machine learning applications. This work aims to contribute to the ongoing development of more efficient, adaptive, and universally applicable optimization methods in the field of machine learning and artificial intelligence.", "sections": [{"title": "Introduction", "content": "Optimization is a fundamental problem in machine learning, where the goal is to minimize a loss function that measures the difference between the model's predictions and the true labels. Stochastic gradient descent (SGD) and its variants are widely used optimization algorithms in deep learning, due to their simplicity, computational efficiency, and ability to handle large datasets.\nHowever, SGD has its limitations, particularly when dealing with noisy or high-variance gradients, which can lead to slow and unstable convergence. To address these challenges, adaptive gradient methods have been proposed, but their effectiveness is still a topic of debate. For instance, Wilson et al. [3] found that adaptive gradient methods tend to generalize less effectively than SGD with momentum across a range of deep learning tasks, including image classification, character-level language modeling, and constituency parsing.\nDifferent hypotheses about the origins of this worse generalization have been investigated, such as the presence of sharp local minima [4, 5] and inherent problems of adaptive gradient methods [3]. These findings highlight the need for a deeper understanding of the underlying mechanisms driving the performance of adaptive gradient methods.\nIn seeking such understanding, researchers have identified momentum as a crucial element in many iterative optimization algorithms. Momentum has been consistently shown to accelerate and improve convergence, as demonstrated by Nemirovskii and Nesterov [6], and frequently yields solutions with enhanced generalization capabilities, as found by Sutskever et al.[7]. Through the accumulation of gradient vectors across successive optimization steps, momentum facilitates the overcoming of minor local fluctuations in the loss landscape, potentially escaping shallow local minima and accelerating progress in plateau regions, as discussed by Qian[8], Ruder [9], and Goh [10].\nIn recent years, adaptive gradient algorithms such as Adam[1], RMSprop[11], and AMSGrad [12] have gained popularity due to their ability to adapt to the geometry of the loss function and stabilize the optimization process. Among"}, {"title": "Methods", "content": ""}, {"title": "New Debiasing Terms", "content": "The Adam optimizer, introduced in 2014, is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient [1]. The original Adam optimizer uses debiasing terms $\\hat{m}$ and $\\hat{v}$ to correct for the bias in the estimates of the first and second moments of the gradient, respectively. However, these terms have limitations that can affect the convergence and stability of the optimization process [12]. Specifically, the original Adam optimizer's debiasing terms treat the first and second moments independently, failing to account for their mutual influence on parameter updates [13]. This independence assumption can lead to suboptimal scaling of updates, particularly in regions of high curvature where the interaction between gradient magnitude and variance is crucial [14]. Additionally, the fixed nature of the debiasing terms doesn't adapt to the local geometry of the loss landscape, potentially resulting in either overly aggressive or conservative parameter updates.\nIn the pursuit of improving the Adam optimizer, I introduce a novel approach to debiasing, which I term $\\tilde{m}$ and $\\tilde{v}$. These new terms aim to rectify the inherent bias in the traditional Adam update rules, thereby enhancing the overall performance and stability of the optimizer.\nThe traditional Adam optimizer relies on the debiased estimates $\\hat{m}$ and $\\hat{v}$, which are computed as $\\hat{m} = \\frac{m}{1-\\beta_1^t}$ and $\\hat{v} = \\frac{v}{1-\\beta_2^t}$, respectively. While these terms effectively mitigate the bias introduced by the exponential moving averages, they still suffer from limitations. Specifically, $\\hat{m}$ and $\\hat{v}$ do not fully account for the interplay between the first and second moments, leading to suboptimal convergence.\nTo address these limitations, I propose the novel debiasing terms $\\tilde{m}$ and $\\tilde{v}$, defined as in Equation 1.\n$\\tilde{m} = \\frac{m}{1-\\beta_1^t} \\left( 1 + \\frac{v}{v + \\epsilon} \\beta_1 \\right)$ and $\\tilde{v} = \\frac{v}{1-\\beta_2^t} \\left( 1 + \\frac{m^2}{m^2 + \\epsilon} \\beta_2 \\right)$                                      (1)\nwhere $m$ and $v$ are the first- and second-moment estimates of the gradient, respectively, $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moment estimates, $\\epsilon$ is a small constant to prevent division by zero, and $t$ is the current iteration number.\nThese terms are designed to capture the intricate relationships between the first and second moments, thereby providing a more accurate and nuanced representation of the gradient statistics.\nThe $\\tilde{m}$ term can be seen as a refinement of the traditional $\\hat{m}$ estimate. By incorporating the second moment $v$ and the learning rate $\\beta_1$, $\\tilde{m}$ better accounts for the variance of the gradient, leading to more informed updates. The additional term $\\left( 1 + \\frac{v}{v + \\epsilon} \\beta_1 \\right)$ serves as a correction factor, which adaptively scales the debiasing process based on the relative magnitude of the variance.\nSimilarly, the $\\tilde{v}$ term builds upon the traditional $\\hat{v}$ estimate, incorporating the first moment $m$ and the scaling factor $\\beta_1$. This allows $\\tilde{v}$ to more effectively capture the covariance between the gradient and its squared value, resulting in a more accurate estimate of the variance."}, {"title": "Gradient-based Acceleration Mechanism", "content": "In this section, I introduce a novel acceleration mechanism for the Adam optimizer, which leverages gradient information to enhance the convergence rate of the algorithm. This innovation is rooted in the observation that the gradient itself contains valuable information about the optimization landscape, which can be harnessed to accelerate the optimization process. To achieve this, I propose a novel component, termed the gradient-based acceleration mechanism, denoted by $\\tilde{g}$. This acceleration mechanism is designed to synergistically interact with the debiased momentum $\\tilde{m}$, enabling the optimizer to more effectively harness the gradient information in a controlled manner and accelerate the convergence process. The gradient-based acceleration mechanism $\\tilde{g}$ is defined as in Equation 2.\n$\\tilde{g} = \\frac{g}{1 - \\beta_1^t} \\left( 1 + \\frac{v}{v + \\epsilon} \\beta_2 \\right)$                                      (2)\nwhere $g$ is the gradient, $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moment estimates, $\\epsilon$ is a small constant to prevent division by zero, and t is the current iteration number.\nThe gradient-based acceleration mechanism $\\tilde{g}$ can be interpreted as a measure of the gradient's \"urgency\" or \"importance\". When the gradient is large, this means the surface is steep, and the optimizer is far from the optimal solution, thus, it should prioritize updating the parameters in the direction of the gradient. In this case, $\\tilde{g}$ increases, indicating that the optimizer should focus on refining the parameters based on the momentum $\\tilde{m}$. On the other hand, when the gradient is small, this means the surface is flat, and the optimizer should explore the parameter space more broadly. In this case, $\\tilde{g}$ decreases, allowing the optimizer to focus on refining the parameters based on the momentum $m$.\nWhen $\\tilde{g}$ increases, it indicates that the optimizer should prioritize updating the parameters in the direction of the gradient. On the other hand, when the gradient is small, $\\tilde{g}$ decreases, allowing the optimizer to focus on refining the parameters based on the momentum $m$.\nThe term $\\left( 1 + \\frac{v}{v + \\epsilon} \\beta_2 \\right)$ provides an adaptive scaling to the gradient. This scaling factor is always greater than or equal to 1, with its magnitude determined by the second moment estimate $v$ and the current timestep $t$.\nThe incorporation of $v$ in the scaling term creates an interaction between the direct gradient and the second moment estimate. In regions of high curvature (large $v$), the gradient term receives additional emphasis, potentially allowing for more aggressive updates in these areas. Conversely, in regions of low curvature (small $v$), the gradient term is downweighted, enabling the optimizer to rely more on the momentum term for guidance.\nThe gradient-based acceleration mechanism $\\tilde{g}$ is incorporated into the update rule as follows:\n$\\theta \\leftarrow \\theta - \\frac{\\alpha \\cdot (\\tilde{m} + \\tilde{g})}{\\sqrt{\\tilde{v}} + \\epsilon}$                                       (3)\nwhere $\\theta$ is the parameter vector, $\\alpha$ is the learning rate, $\\tilde{m}$ is the bias-corrected first-moment estimate, $\\tilde{v}$ is the bias-corrected second-moment estimate, $\\tilde{g}$ is the bias-corrected gradient, $\\epsilon$ is a small constant to prevent division by zero, and $t$ is the current iteration number.\nThe combination of $\\tilde{m}$ and $\\tilde{g}$ in the update rule creates a highly adaptive learning mechanism. It balances the smoothed, historical information captured by $\\tilde{m}$ with the immediate, potentially more volatile information in $\\tilde{g}$. In scenarios"}, {"title": "Dynamic Step Size Schedule", "content": "The traditional Adam optimizer, like many other stochastic gradient descent (SGD) variants, employs a fixed learning rate schedule, which can be suboptimal in practice. A fixed learning rate may not adapt to the changing landscape of"}, {"title": "EXAdam Algorithm", "content": "Let's put everything together and see the complete EXAdam algorithm, which seamlessly integrates all the enhancements discussed earlier. This unified approach combines the novel debiasing terms $\\tilde{m}$ and $\\tilde{v}$, the gradient-based accelerator $\\tilde{g}$, and the dynamic step size $a_t$. Algorithm 1 provides a comprehensive view of EXAdam, showcasing how these innovations work together to create a more adaptive and potentially powerful optimization method. This algorithmic representation encapsulates the essence of my contributions, offering a clear roadmap for implementing and further studying EXAdam's behavior in various optimization scenarios."}, {"title": "Experiments", "content": "I empirically evaluated the effectiveness of EXAdam through a series of experiments on two diverse benchmark tasks. My first experiment involved training a convolutional neural network (CNN) on the CIFAR-10 dataset, a widely used benchmark for image classification tasks. This setup allowed us to assess EXAdam's performance in a relatively smooth gradient landscape. I then trained a MinGPT model on a dataset of Shakespeare works, a challenging task that requires the optimizer to navigate a complex and nuanced gradient landscape. This experiment enabled us to evaluate the optimizer's ability to handle long-range dependencies and adapt to changing gradient statistics.\nIn each of these experiments, I compared the performance of EXAdam to state-of-the-art optimizers using identical hyperparameters and training protocols. By doing so, I aimed to assess the robustness and versatility of EXAdam across different model architectures, datasets, and tasks, and to demonstrate its ability to outperform existing state-of-the-art optimizers in a variety of settings. I implemented all the experiments using PyTorch [15] to ensure the compatibility and reproducibility of the results. The experiment was conducted on Kaggle. The code for the experiment is available on GitHub\u00b9."}, {"title": "Experiment: Image Classification", "content": "I conducted a benchmark experiment involving image classification using the CIFAR-10 dataset. This experiment aimed to assess the convergence properties of EXAdam compared to traditional Adam, AdamW, AdaDelta, SGD with momentum, and RMSProp on a deep convolutional neural network (CNN) model. The goal was to determine whether EXAdam could achieve faster convergence and higher accuracy on the CIFAR-10 dataset, showcasing its effectiveness in practice. The CNN model employed in this experiment is a deep neural network consisting of multiple convolutional and fully connected layers, making 3 million trainable parameters. The network takes as input a 3-channel 32x32 image and outputs a probability distribution over the 10 classes of the CIFAR-10 dataset."}, {"title": "Experiment: Text Generation", "content": "I conducted another experiment to evaluate the performance of EXAdam in the context of text generation by training a 14.3-million parameter MinGPT model, a mini version of the GPT (Generative Pre-trained Transformer) model [17], on a dataset of Shakespeare works\u00b2. The goal of this experiment was to assess the optimizer's ability to handle long-range dependencies and complex loss landscapes, which are common challenges in natural language processing tasks.\nThis language model is based on the transformer architecture [18] and consists of an embedding layer, a series of transformer blocks, and a final linear layer. The embedding layer maps each input token to a dense vector, which is then fed into the transformer blocks. Each transformer block consists of self-attention and feed-forward neural network (FFNN) layers. The self-attention layer computes the weighted sum of the input tokens, while the FFNN layer transforms the output of the self-attention layer. The final linear layer outputs the logits for each token in the vocabulary.\nI used the Shakespeare dataset, which contains a relatively good amount of text that could fit in the memory of the GPUs I had access to. I split the data into training and validation sets, with 90% of the data used for training and 10% for validation. I create a mapping from characters to indices and vice versa, and define encoding and decoding functions to convert between characters and indices.\nIn addition to EXAdam, I also experimented with several other optimizers to compare their performance on the MinGPT model. Specifically, I evaluated Adam, AdamW, AdaFactor [19], SGD with Momentum, AdEMAMix [20], and Signum [21], each with their default hyperparameters. These optimizers were chosen because they have been widely used in deep learning and have shown promising results in various applications. My goal was to assess whether EXAdam's performance advantages hold up against these alternative optimizers, and to identify any potential trade-offs or limitations of each approach.\nThis experimental setup was inspired by the work of Zhao et al. [22], who benchmarked multiple optimizers in the context of large language models. While their study focused on the performance of different optimizers across a range of model sizes and hyperparameters, this experiment aimed to provide a more in-depth analysis of the optimizers'"}, {"title": "Conclusion and Future Work", "content": "In this paper, I have presented a series of novel enhancements to the Adam optimizer, collectively forming EXAdam. These enhancements address several limitations of the original Adam algorithm while preserving its core strengths.\nMy contributions can be summarized as follows:\n\u2022 New Debiasing Terms: I introduced $\\tilde{m}$ and $\\tilde{v}$, which provide more nuanced bias correction by incorporating cross-moment interactions and temporal dynamics. These terms potentially offer improved stability and convergence, particularly in the early stages of optimization.\n\u2022 Gradient-based Acceleration Mechanism: I proposed a novel term $\\tilde{g}$ that directly incorporates the current gradient into the update rule. This mechanism allows for more immediate responsiveness to the current loss landscape while maintaining the benefits of moment-based updates.\n\u2022 Dynamic Step Size Formula: I developed an adaptive learning rate schedule that increases logarithmically with the square root of the iteration count. This formula provides a continually adaptive global step size, potentially enhancing long-term learning capabilities and exploration of the parameter space.\nThese enhancements work in concert to create an optimizer that is potentially more robust, adaptive, and efficient than its predecessors. EXAdam aims to address common challenges in optimization, such as navigating complex loss landscapes, escaping saddle points, and balancing immediate gradient information with historical trends.\nHowever, it is important to note that while my theoretical analysis is promising, the true test of any optimization algorithm lies in its empirical performance across a wide range of tasks and domains. As such, I view this work not as a conclusion, but as a starting point for further research and experimentation. I encourage the community to explore and validate EXAdam on a variety of benchmarks and real-world applications to fully assess its capabilities and limitations.\nI hope that this work will inspire further research into adaptive optimization methods and contribute to the ongoing quest for more efficient, robust, and universally applicable optimization algorithms. As we continue to tackle increasingly complex problems in machine learning and artificial intelligence, the importance of sophisticated optimization techniques cannot be overstated. EXAdam is my contribution to this vital area of research, and I look forward to seeing how it performs in the hands of the broader scientific community."}]}