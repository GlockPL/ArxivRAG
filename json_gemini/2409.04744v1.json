{"title": "LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs*", "authors": ["Yongxin Deng", "Xihe Qiu", "Xiaoyu Tan", "Wei Chu", "Yinghui Xu"], "abstract": "The uncertainty inherent in the environmental transition model of Reinforcement Learning (RL) necessitates a careful balance between exploration and exploitation to optimize the use of computational resources for accurately estimating an agent's expected reward. Achieving balance in control systems is particularly challenging in scenarios with sparse rewards. However, given the extensive prior knowledge available for many environments, it is redundant to begin learning from scratch in such settings. To address this, we introduce Language Model Guided Trade-offs (i.e., LMGT), a novel, sample-efficient framework that leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their adeptness at processing non-standard data forms, such as wiki tutorials. LMGT proficiently manages the exploration-exploitation trade-off by employing reward shifts guided by LLMs, which direct agents' exploration endeavors, thereby improving sample efficiency. We have thoroughly tested LMGT across various RL tasks and deployed it in industrial-grade RL recommendation systems, where it consistently outperforms baseline methods. The results indicate that our framework can significantly reduce the time cost required during the training phase in RL.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) encounters a fundamental challenge in finding a balance between exploration and exploitation [1]. This equilibrium is crucial for ensuring the robustness of RL algorithms when applied in real-world scenarios [2]. Agents operating in such settings often face the exploration-exploitation dilemma due to the unknown and stochastic nature of their environments, making it impossible to deduce the exact environmental model. The interactions between the agent and the environment provide estimates of expected rewards, denoted as \u00ca(R), for different actions. However, estimating the mean of rewards using sample means introduces inherent error, which prevents us from being certain that the action with the highest \u00ca(R) is truly optimal. Consequently, exploration is necessary to reduce the discrepancy between the sample mean and the true mean."}, {"title": null, "content": "However, in situations where resources are limited, it is likely that the action with the highest \u00ca(R) is indeed the best choice. Accurately estimating the optimal action is a fundamental aspect of RL [3], [4], whereas suboptimal actions require less computational evaluation, reflecting the concept of \"exploitation\" in RL. The conflict between exploration and exploitation therefore necessitates the selection of actions that are both \"close to the current estimate of the best action\" while also being \"different from it\"."}, {"title": null, "content": "Numerous studies have attempted to address this challenge. Common strategies include e-greedy [5], Softmax [5], upper confidence bound [6], thompson sampling [7]. The choice of an appropriate exploration-exploitation strategy depends on the specific application context and problem requirements, as different RL problems may require different strategies. Nevertheless, given the broadening scope of RL applications, manually selecting different exploration-exploitation strategies for each distinct environment is impractical. One major difficulty arises from multimodal and long-tailed data distributions. Some adaptive algorithms adjust the exploration-exploitation balance based on the agent's experiences [8], but these algorithms still have constraints that can significantly impact model performance and robustness when applied beyond their scope. Additionally, previous exploration-exploitation balance strategies either rely solely on adjusting the ratio based on the data distribution, without utilizing prior knowledge or require the design and adjustment of strategies based on domain expertise and a deep understanding of the task. The latter approach requires substantial manual effort and can potentially reduce learning performance if not properly designed."}, {"title": null, "content": "To address the limitations of traditional exploration-exploitation strategies, we propose a novel framework called Language Model Guided Trade-offs (i.e., LMGT), that leverages prior knowledge from various sources to guide agents' inefficient learning with limited resources. Jake et al. [9] discovered that valuable insights can be derived from effective offline demonstration data, enabling agents to align themselves correctly. By capturing the patterns of a sound policy in a model and using this model for intrinsic motivation, the RL agent can effectively map itself to a skillful demonstration-defined subspace. In this subspace, even undirected exploration can significantly enhance the agent's understanding of the environment if the deviations align with rational pathways. Our LMGT leverages the text comprehension and generation capabilities of Large Language Models (LLMs) to incorporate prior knowledge, thereby enhancing the agent's environmental understanding and achieving a balance between exploration and exploitation. Leveraging the powerful language processing capabilities of LLMs, our framework obviates the need for highly structured prior knowledge, thereby enabling extensive use of existing human knowledge bases. This distinct advantage sets our approach apart from other methods. Moreover, the text generated by LLMs often reflects the structural patterns of the real world, embedding common-sense knowledge about various aspects of human reasoning and intuition [10], [11]. Compared to some recent methods [12], [13], [14] that directly use LLMs as agents within the RL process, the advantage of our approach lies in the fact that LLMS are only required during the training phase to assist the agent in learning. This fundamentally mitigates the risks associated with LLM hallucinations by confining such risks to the training stage, thereby enhancing the security of the strategy implemented by the agent. Once the training is complete, our agent can be deployed independently without LLMs. In contrast to agents utilizing LLMs kernels, conventional RL agents founded upon multilayer perceptrons or convolutional neural networks exhibit a comparative advantage regarding computational resource utilization [15]. The potential advantages of our architecture become apparent in large-scale application scenarios and latency-sensitive environments. Our interactive process between LLMs and agents involves LLMs processing environmental information and scoring agent behavior to guide exploration and exploitation through reward-shifting mechanisms. Additionally, our method aligns with a key principle: reward shifting is equivalent to modifying the initialization of the Q-function, effectively balancing the exploration and exploitation aspects of RL[16]."}, {"title": null, "content": "We conducted experiments in various settings and environments, demonstrating that our approach effectively utilizes prior knowledge, leading to a reduction in model training costs compared to baseline methods. We also evaluated the performance of different LLMs within our framework, providing a partial assessment of their inferential capabilities. Furthermore, we applied our framework to Google's industrial-grade recommendation algorithm, SlateQ [17]. Our contributions can be summarized as follows:"}, {"title": null, "content": "\u2022 We propose a novel framework that leverages LLMs to balance exploration and exploitation within RL. This framework effectively resolves the exploration-exploitation dilemma and provides precise guidance for the agent's actions. Featuring a distinctive architecture that decouples the LLM from the agent, our approach significantly reduces the risk of LLM hallucinations impacting RL strategies."}, {"title": null, "content": "\u2022 We validated our proposed framework in various automatic control environments and RL algorithms. Experimental results demonstrate that our method significantly reduces the training costs of RL models while maintaining both generality and ease of use."}, {"title": null, "content": "\u2022 We demonstrate the effectiveness of our method in an industrial application context, providing a practical and straightforward solution to reduce the training cost of RL models for industry practitioners."}, {"title": "II. METHODOLOGY", "content": "In this section, we present the overall structure and provide an in-depth exploration of the aspects relevant to prompts within our framework."}, {"title": "A. Framework Structure", "content": "RL methods are categorized into \"on-policy\" and \"off-policy\" based on how data is generated and processed. On-policy and off-policy methods are often viewed as distinct due to significant differences in their policy frameworks and algorithmic implementations in practice. These differences influence algorithm selection and optimization techniques. For instance, off-policy methods must address the importance of sampling issues associated with using data from non-target policies\u2014a challenge not faced by on-policy methods. Broadly, however, on-policy methods can be seen as a subset of off-policy methods, where the behavior policy (which generates the data) aligns with the target policy (the policy under optimization). Thus, off-policy definitions are inherently broader, encompassing all scenarios, even those where the learning and behavior policies coincide. All descriptions related to RL mentioned below refer to off-policy methods. A common RL training process is as follows:"}, {"title": null, "content": "1) Initialization of the evaluation policy and the behavioral policy. The evaluation policy may be initialized as a stochastic policy, such as a random policy, while the behavioral policy may take the form of an \u03f5-greedy policy, incorporating a probability of random exploration."}, {"title": null, "content": "2) The agent engages with the environment based on the behavioral policy, yielding training data in the form of state-action-reward-next state tuples. These data are then archived within an experience replay buffer."}, {"title": null, "content": "3) Training data is sampled from the experience replay buffer, and the agent's parameters are updated based on the evaluation policy and the sampled data, employing techniques such as Temporal Difference (TD) learning or Monte Carlo methods."}, {"title": null, "content": "4) Periodic evaluation of the evaluation policy's performance within the environment, with training termination contingent on the attainment of a predefined performance threshold."}, {"title": null, "content": "5) The process iterates by returning to step 2, with periodic adjustments to the behavioral policy, such as the gradual reduction of \u03f5 in the case of an \u03f5-greedy policy."}, {"title": null, "content": "To ensure the wide-ranging applicability of our improvements, we aim to preserve the fundamental principles of the original RL training process with minimal intervention."}, {"title": null, "content": "LMGT introduces a specific modification to the second step, which involves adjusting the acquired experiences of the agent. We consider the LLM as the \"evaluator\". When the agent observes the environmental state, it selects an action based on the prevailing behavioral policy and communicates this action to the environment. We replicate and transmit both the observable state of the environment and the chosen action to the LLM. The LLM assesses the agent's actions and assigns a score, taking into account the prior knowledge that is embedded in its weights or introduced through the prompt (such as game rules). This score serves as a reward shift, which is incorporated into the reward generated by the environment itself. In contrast to the conventional RL process, LMGT involves the agent recording adjusted rewards instead of relying on the inherent rewards provided by the environment. The agent then learns from these adapted rewards to gain guidance from the LLMs."}, {"title": null, "content": "In situations with sparse rewards, the agent faces challenges in accumulating information through trial and error. Hence, we employ the LLM to guide the agent, to avoid the pursuit of directions that have been previously determined as \"valueless\" based on prior knowledge, as indicated by a negative reward shift. The LLM assigns a positive reward shift for actions identified as \u201cvaluable\u201d according to prior knowledge, encouraging the agent to focus on exploitation. While maintaining the traditional exploration-exploitation strategy from classical RL, the agent intensifies its exploration of actions neighboring those deemed \u201cvaluable\u201d in the prior knowledge, increasing the likelihood of discovering the \"optimal\" action. Additionally, for actions not referenced in prior knowledge, the LLM assigns a \u201c0\u201d reward shift, allowing the agent to explore based on the original exploration policy."}, {"title": null, "content": "The framework of LMGT is presented in Figure 1. For different tasks, the LLM provides various forms of reward shifts, guided by the principle that intricate tasks require more nuanced reward shifts, while simpler tasks require simpler reward shifts, using \u201c+1,\u201d \u201c0,\u201d and \u201c-1\u201d to represent \u201capproval\u201d, \u201cneutral\u201d, and \u201cdisapproval\u201d, respectively."}, {"title": "B. Prompt Design", "content": "In this section, we will discuss the engineering methodology applied to optimize the performance of LLMs. The primary emphasis is placed on the performance attributes of LLMs, specifically concerning the magnitude of embedded prior knowledge in their weight configurations, as well as their capacity to comprehend and harness pre-existing knowledge about textual genres. The efficacy of the reward-shifting mechanism, generated by LLMs, fundamentally dictates the success of our approach and the extent of enhancement in comparison to the baseline."}, {"title": null, "content": "Table III catalogs a detailed inventory of immediate enhancements utilized in our experimental design. It is important to note that the primary distinction between Zero-shot [18] and Baseline involves Zero-shot's integration of specific, task-related information into the prompt, which guides the LLM regarding the appropriate information to produce. The Name method could be perceived as perplexing. It involves attributing a name to an LLM in the prompt with the expectation that this modification could enhance its performance. Nonetheless, our experimental results indicate that this technique does not yield any improvements. For additional details on the experiments, please see Section III-C. It is commonplace to deploy multiple prompt enhancements concurrently."}, {"title": null, "content": "Furthermore, our prompt design is further categorized into two distinct classes: \u201cprior-knowledge-inclusive prompt statements\" and \u201cprior-knowledge-exclusive prompt statements\". The former class provides an all-encompassing evaluation of the LLMs' ability to harness their embedded prior knowledge, including their proficiency in leveraging prior knowledge presented in non-standard linguistic forms, such as natural language text. The latter class, on the other hand, exclusively investigates the LLMs' aptitude for exploiting implicit prior knowledge embedded within their model weights."}, {"title": null, "content": "Section III presents an elucidation of the effects of various prompt methods, along with a rationale for our methodological choices."}, {"title": "III. EXPERIMENT", "content": "The experiment is structured into three distinct parts. In the initial phase, we scrutinize the benefits of our proposed framework over conventional approaches for addressing sparse reward challenges. Specifically, we compare LMGT with Return Decomposition for Delayed Rewards (RUDDER) [19], which is a novel RL approach for delayed rewards in finite MDPs. RUDDER's objective is to neutralize expected future rewards, thereby simplifying Q-value estimations to the average of immediate rewards. Despite RUDDER's expedited processing in scenarios with delayed rewards compared to traditional RL methods, it fails to incorporate prior knowledge-an area where LMGT particularly excels. Consequently, we anticipate LMGT to facilitate the expedited development of effective behavioral strategies by agents. The second segment of the experiment evaluates our framework's versatility by applying it across diverse RL algorithms and environments to ascertain its efficacy. This was accomplished by benchmarking across various standard environments provided by Gymnasium [20], an API platform that supports single-agent RL environments such as cartpole, pendulum, mountain-car, mujoco, and atari. Herein, we also examine the enhancement in performance attributable to our framework across various RL algorithms, compared to the baseline algorithms presented in Stable Baselines3[21]\u2014a collection of robust RL algorithm implementations in PyTorch. This phase further includes an assessment of the impact of different prompting techniques on our framework's performance and an exploratory evaluation of the reasoning capabilities of LLMs within our framework."}, {"title": null, "content": "To ensure that the evaluation conclusions of our framework extend beyond synthetic settings, the final section investigates its practical applications and improvements. Specifically, we explore its integration with Google's SlateQ [17], a sophisticated recommendation algorithm that employs slate decomposition. This approach effectively manages the complexity of recommending multiple items simultaneously, addressing the challenge of large action spaces found in previous RL recommendation algorithms. This implementation was tested within a simulated environment on RecSim [22], a versatile platform for developing simulation environments for recommender systems (RSs), facilitating sequential user interactions."}, {"title": "A. Experimental Settings", "content": "For both LLM inference and agent training, we utilize a single NVIDIA A800-80G GPU. We adhere to the recommended settings by Llama for precise inference, which encompass a temperature of 0.7, top_p of 0.1, a repetition penalty of 1.18, and top_k of 40."}, {"title": "B. Comparison Experiments with Traditional Exploration-Exploitation Trade-off Methods", "content": "We conducted a comparative study of LMGT and RUDDER using a classic pocket watch repair task as a case study. This task involves repairing and selling a pocket watch, where the decision to repair, contingent on the brand, hinges on cost-benefit analysis given a known selling price versus unknown repair and delivery expenses leading to negative rewards. The challenge lies in delayed rewards, where the profitability of repairing a specific brand becomes apparent only after total costs are established. The objective was to equip the agent with a strategy that consistently achieves a \u201cbreak-even decision\u201d ratio exceeding 90%. \"The number of episodes to learn a qualified strategy\u201d (expressed as Episode) and \"the time to learn a qualified strategy\u201d (expressed as Time) served as our evaluation metrics, acknowledging that training time is influenced by hardware performance and hence, primarily offers qualitative insights. In this part of the experiment, to align with the baseline chosen by RUDDER in the example, we applied LMGT to temporal difference (TD). To mitigate random seed effects on outcomes, experiments were conducted using seeds {42, 43, 44, 45, 46}, with results averaged. As evidenced in Table I, LMGT outperformed RUDDER, requiring fewer episodes for strategy acquisition and demonstrating reduced training time. However, the reduced time advantage of LMGT over episodes suggests a potential threshold beyond which the efficiencies derived from LLM guidance might be counterbalanced by computational overheads. This threshold presents a future research direction for us."}, {"title": "C. Evaluation of LMGT among Various Reinforcement Learning Algorithms and Environments", "content": "1) Main experiment: This section undertakes a comprehensive evaluation of the efficacy of our LMGT framework across various RL environments, employing diverse RL algorithms. Detailed experimental findings are presented in Table II, in the \"metric\" column, \"AR\" is an abbreviation for average reward, and \"BR\" is an abbreviation for boosted reward, red numbers indicate that our method is inferior to the baseline in this scenario. Please be advised that all rewards presented in the results correspond to the environments' intrinsic rewards and have not been modified. The environments are identified with their observable states furnished to the LLMs in two distinct formats: a standardized numerical representation, denoted as \"box\" (e.g., a tuple encapsulating information on object positions), and a more intuitively comprehensible visual format referred to as \"human\" (such as a screenshot of the current frame). Our metric for assessing our approach against baseline methods is the \"average reward of the model after a fixed number of training time steps\". Specifically, agents are trained separately using our method and baseline techniques within the same environment, and the trained weights are preserved after a predefined number of time steps. Subsequently, we evaluate the performance of models trained using different methods, employing an equivalent number of training time steps in the same environment, while comparing their average rewards."}, {"title": null, "content": "Throughout these experiments, we maintained a consistent choice of LLM and prompt techniques. Specifically, two prompt methods were employed: CoT and Zero-shot prompt, to formulate our prompts. The 4-bit quantized version of the Vicuna-30B model [23], with GPTQ quantization [24], was utilized as our guiding LLM within our framework. This model is utilized to assess the quality of agent behavior in distinct environmental states. We contend that this configuration optimizes the performance of our framework, and we will delve into the influence of different prompt techniques and LLMs on the framework's performance in other parts of this section."}, {"title": null, "content": "RL environments are seldom conveyed through purely textual descriptions; thus, LLMs necessitate multimodal capabilities to process such information. Common LLMs such as Llama, Llama2, and Vicuna do not inherently support multimodal functionality. To address this limitation, we adopted a pipeline model approach, where multiple single-modal models work synergistically, with each model responsible for processing specific data types and passing results to the next model to accomplish tasks. In our experiments, we integrated LLaVA [30] as the image processing model preceding the LLM. Therefore, in the aforementioned experiments, LLaVA was integrated with the Vicuna-30B model and operated collaboratively, equipping our \"scorer\" with image processing capabilities."}, {"title": null, "content": "Table II illustrates that our framework consistently outperforms baseline methods across a majority of environments and various RL algorithms. It effectively achieves a trade-off between exploration and exploitation in RL methods, enabling agents to acquire skills more rapidly, thus leading to cost savings during training. Moreover, we observed that our framework's performance is relatively inferior in tasks necessitating the utilization of pipeline models to process visual information compared to tasks that exclusively involve text information processing. In essence, if Vicuna-30B is required to handle additional image information from LLaVA, its performance tends to deteriorate. An intriguing observation proposed in [31] suggests that attempting to enforce strict adherence of the LLM to response templates results in reduced performance across all scenarios. We hypothesize that both these scenarios signify a degradation in LLM performance in multitask settings [32]. Within our framework, \"understanding extracted image information\" and \"assigning scores to agent behavior based on a combination of different information\" represent distinct tasks, while the phenomenon mentioned in [31] pertains to \"providing responses based on prompts\" and \"formatting responses as required\" as two separate tasks."}, {"title": null, "content": "We also investigated the influence of different prompt methods on the performance of our framework. Similar to the previous experiments, while keeping other variables constant, we continued to employ the 4-bit quantized version of the Vicuna-30B model as our LLM and the A2C algorithm as our RL technique. We conducted tests on two representative environments, and the experimental results are presented in Table III. It is assumed that prompts in the table all inherently contain prior knowledge. For both simple (Cart Pole) and complex (Blackjack) environments, the most effective prompt method was found to be CoT. COT particularly excelled in enhancing performance for complex tasks. We discovered that the model often overlooked the provided information and resulted in a uniform outcome unless explicitly instructed to employ hierarchical thinking in challenging tasks. Furthermore, we observed that merely assigning a simple name to the model scarcely enhanced its performance."}, {"title": null, "content": "An intriguing observation emerged when comparing prompt methods on our task: the \"Zero-shot prompt\" method outperformed the \"Few-shot prompt\" method. Few-shot prompts often led the Vicuna-30B model to generate results with a sense of \u201cillusion\". Vicuna-30B frequently produced arbitrary extensions based on the provided examples. Furthermore, we observed that incorporating prior knowledge into the prompts can lead to an improvement in the performance of our framework, despite the fact that the weights within the Vicuna-30B model already encompass the requisite prior knowledge for addressing the challenges presented by the environment."}, {"title": null, "content": "We also conducted experiments to assess the performance of different LLMs serving as the \"evaluators\" within our framework, thereby partially evaluating their inferential capabilities, we opted for the Blackjack environment for testing. The experimental results are presented in Table V. \"Vicuna-30B-4bit-GPTQ\" indicates the use of the Vicuna model, with a size of 30 billion parameters, employing GPTQ quantization with 4-bit precision. \u201cLlama2-13B-8bit\" signifies the use of the Llama2 model with a size of 13 billion parameters, without any quantization, running in 8-bit floating-point precision. We kept the prompt statements constant by using CoT and Zero-shot prompt, with the inclusion of prior knowledge, and fixed the RL algorithm (A2C)."}, {"title": null, "content": "From Table V, we observe that the precision of quantization has a limited impact on inferential capabilities in the same model. A well-considered quantization method can effectively mitigate the performance loss resulting from quantization. Model size, on the other hand, has a more significant influence on a model's inferential capabilities, a minimally sized language model fails to yield any significant improvement. Additionally, models of identical scale exhibit variations in their inferential capabilities, confined solely within the scope of our framework."}, {"title": "2) Ablation study:", "content": "In Section III-C.1, we noted that requiring a LLM to perform multiple tasks simultaneously within a single query might compromise its capability [31]. Based on this principle, we designed an ablation experiment to test the performance of LMGT in both the 'box' and 'human' formats within a more visually complex Blackjack environment. For the latter, recognizing card information and converting it into numerical data constitutes a highly specialized task. When the LLM must first process complex visual data, its reasoning ability diminishes. The experimental results, as shown in Table IV, reveal that LMGT's performance in the 'human' format fluctuates around the baseline, indicating performance deterioration in this context. This finding demonstrates that our LMGT effectively leverages the LLM's capabilities to guide the agent's learning: when the LLM's capability is insufficient to provide guidance, the agent's performance reverts to the baseline."}, {"title": "D. Experiments in Industrial Recommendation Scenarios", "content": "In this section, we further apply our framework to Google's RL recommendation algorithm, SlateQ [17], to elucidate its potential in industrial applications."}, {"title": "1) Simulation environment:", "content": "RecSim[22] is a simulation platform for constructing and evaluating recommendation systems that naturally support sequential interactions with users. Developed by Google, it simulates users and environments to assess the effectiveness and performance of recommendation algorithms. We employ RecSim to create an environment that reflects user behavior and item structure to evaluate our LMGT framework."}, {"title": null, "content": "We construct a \"Choc vs. Kale\" recommendation scenario, where the goal is to maximize user satisfaction and engagement over the long term by recommending a certain proportion of \"chocolate\" and \"kale\" elements. In this scenario, the \"chocolate\" element represents content that is interesting but not conducive to long-term satisfaction, while the \"kale\" element represents relatively less exciting but beneficial content for long-term satisfaction. The recommendation algorithm needs to balance these two elements to achieve maximized long-term user satisfaction."}, {"title": null, "content": "In our scenario, the entire simulation environment consists primarily of document models and user models. The document model serves as the main interface for interaction between users and the recommendation system (agent) and is responsible for selecting a subset of documents from a database containing a large number of documents to deliver to the recommendation system. The user model simulates user behavior and reacts to the slates provided by the recommendation system."}, {"title": null, "content": "The database in the document model essentially serves as a container for observable and unobservable features of underlying documents. In this scenario, document attributes are modeled as continuous features with values in the range of [0, 1], referred to as the Kaleness scale. A document assigned a score of 0 represents pure \u201cchocolate\", which is intriguing but regrettable, whereas a document with a score of 1 represents pure\u201ckale\u201d, which is less exciting but nutritious. Additionally, each document has a unique integer ID, and the document model selects N candidate documents in sequential order based on their IDs."}, {"title": null, "content": "The user model includes both observable and unobservable user features. Based on these features, the model responds to the received slate according to certain rules. Each user is characterized by the features of net kale exposure (nket) and satisfaction (satt), which are associated through the sigmoid function \u03c3 to ensure that satt is constrained within a bounded range. Specifically, the satisfaction level is modeled as a sigmoid function of the net kale exposure, which determines the user's satisfaction with the recommended slate:"}, {"title": null, "content": "satt = \u03c3(\u03c4\u00b7 nket)                                                                                (1)"}, {"title": null, "content": "Where \u03c4 is a user-specific sensitivity parameter. Upon receiving a Slate from the recommendation system, users select items to consume based on the Kaleness scale of the documents. Specifically, for item i, the probability of it being chosen is determined by  p ~ e^{1-kaleness(i)}. After making their selections, the net kale exposure evolves as follows:"}, {"title": null, "content": "nket_{t+1} = \u03b2\u00b7nket + \u03bb(k_i - 1/2) + \u039d(0, \u03b7)                                                                                (2)"}, {"title": null, "content": "Where \u03b2 represents a user-specific memory discount, k_i corresponds to the kaleness of the selected item, and \u03b7 denotes some noise standard deviation. Lastly, our focus will be on the user's engagement s_i, i.e. a log-normal distribution with parameters linearly interpolating between the pure kale response (\u03bc\u03ba,\u03c3\u03ba) and the pure choc response (\u03bc\u03b5, \u03c3\u03b5):"}, {"title": null, "content": "s_i ~ logN(k_i*\u03bc_\u03ba + (1 - k_i)*\u03bc_c, k_i*\u03c3_\u03ba + (1 - k_i)*\u03c3_c)                                                                                (3)"}, {"title": null, "content": "The satisfaction variable satt represents the sole dynamic component of the user's state, and thus, we generate the user's observable state based on it. In the simulation, user satisfaction is modeled and computed as a latent state. However, to simulate real-world scenarios, we map the latent state to an observable state by introducing noise to account for user uncertainty."}, {"title": "2) Experimental results:", "content": "The experimental configurations for LMGT and the baseline SlateQ approach are identical. We independently trained agents using both our method and the baseline SlateQ, evaluating their performance over an equivalent number of episodes. In the \"Choc vs. Kale\" scenario, each episode consists of a set number of time steps. As illustrated in Table VI, our results conclusively show that our approach significantly accelerates skill acquisition in agents, enabling them to adeptly navigate the complex challenges of the environment. This rapid development of expertise leverages prior knowledge and skillfully balances the tension between exploration and exploitation. As a consequence, there is an efficient use of sample resources, leading to a marked decrease in the training costs associated with RL models. Nonetheless, our study has its limitations. A notable omission is the analysis of computational resources required for integrating LLMs into the training process. Future research will focus on optimizing the use of computational resources in RL training by applying prior knowledge while addressing the heightened resource demand that comes with incorporating LLMs. Additionally, we have not yet formulated a theoretical framework to explain how LLMs dynamically influence reward structures. Addressing this represents a promising avenue for future research."}, {"title": "IV. CONCLUSION", "content": "To harness the extensive prior knowledge produced by human endeavors and achieve a balance between exploration and exploitation in RL, we introduce a framework named LMGT. This framework ingeniously takes advantage of the inherent domain expertise contained in LLMs and their sophisticated information-processing abilities to navigate agent exploration and exploitation efforts without significantly disrupting existing RL workflows. Through experimental evaluations across different settings and using a variety of algorithms, the LMGT framework has demonstrated its effectiveness. It successfully manages the balance between the exploration and exploitation of agents while simultaneously reducing their training expenses. Further validating its practicality, we have applied LMGT to SlateQ, an industrial-grade recommendation algorithm, underscoring its potential for real-world industrial applications. Despite these advances, our study has not yet explored the impact on computational resources due to the integration of LLMs into the training process. Future research will be directed towards striking a balance between the augmented resource consumption caused by incorporating LLMs and optimizing the use and allocation of computational resources in RL training environments to mitigate it."}]}