{"title": "Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone", "authors": ["Catalina Gomez", "Ruolin Wang", "Katharina Breininger", "Corinne Casey", "Chris Bradley", "Mitchell Pavlak", "Alex Pham", "Jithin Yohannan", "Mathias Unberath"], "abstract": "Primary care providers are vital for initial triage and referrals to specialty care. In glaucoma, asymptomatic and fast progression can lead to vision loss, necessitating timely referrals to specialists. However, primary eye care providers may not identify urgent cases, potentially delaying care. Artificial Intelligence (AI) offering explanations could enhance their referral decisions. We investigate how various AI explanations help providers distinguish between patients needing immediate or non-urgent specialist referrals. We built explainable AI algorithms to predict glaucoma surgery needs from routine eyecare data as a proxy for identifying high-risk patients. We incorporated intrinsic and post-hoc explainability and conducted an online study with optometrists to assess human-AI team performance, measuring referral accuracy and analyzing interactions with AI, including agreement rates, task time, and user experience perceptions. AI support enhanced referral accuracy among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams underperformed compared to Al alone. Participants believed they included AI advice more when using the intrinsic model, and perceived it more useful and promising. Without explanations, deviations from AI recommendations increased. AI support did not increase workload, confidence, and trust, but reduced challenges. On a separate test set, our black-box and intrinsic models achieved an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We identify opportunities of human-AI teaming for glaucoma management in primary eye", "sections": [{"title": "1 Introduction", "content": "Primary care providers, serving as the first point of contact within the healthcare system and trained to manage a wide range of health issues, will refer patients to medical specialists when their conditions require more advanced treatment or diagnosis. The time frame from recognizing a health issue to reaching specialty care can vary depending on the severity and urgency of the conditions, and ultimately specialist availability. Timing is crucial in the early detection of asymptomatic conditions with irreversible damage like glaucoma to prevent severe outcomes such as vision loss, necessitating prompt referral of high-risk patients to glaucoma specialists for more aggressive intervention [19]. Proper care for glaucoma patients is essential, particularly as the disease can progress rapidly or unnoticed, with some patients remaining asymptomatic even until late in the disease course [20]. Therefore, from the initial consultations with primary eye care providers, it becomes imperative to distinguish patients requiring immediate glaucoma treatment and define longer follow-up intervals for patients at lower risk.\nThis initial step is crucial for navigating the complex clinical workflow. Glaucoma assessment and monitoring of disease progression include measurements of intraocular pressure (IOP), visual function via visual field (VF) tests, and changes to the optic nerve head via optical coherence tomography (OCT) [19]. Glaucoma specialists must individually tailor treatment decisions for each patient, taking into account the extent of IOP reduction needed to mitigate progression risk, encompassing options like medications, laser procedures, and incisional surgery [19]. Given this spectrum of interventions, the decision to opt for surgery, clearly documented in electronic health records (EHR), emerges as a pivotal indicator of high-risk glaucoma [25, 1]. Specifically, patients who require surgery within three months are often those at a greater risk of rapid disease progression, compared to those that undergo surgery further in the future. While glaucoma specialists have the expertise to make such critical decisions, primary eye care providers may not have the specialized training required for identifying patients at high-risk and make referral recommendations based on their urgency. Expanding the reach of glaucoma screening becomes increasingly critical in light of the expected growth in glaucoma patients, rendering it impractical for specialists alone to triage all glaucoma eyes and identify those at higher risk [16]. Considering the importance of diagnosis and treating glaucoma early to slow its progression and prevent the severe consequences of delayed intervention, artificial intelligence (AI) may serve as a key enabler to empower primary eyecare providers to make informed referral decisions. In contrast to previously studied AI-based decision support systems, such as skin lesions diagnosis [6, 21], it is unclear whether AI can similarly assist first contact providers in making referral (rather than diagnostic) decisions more accurately. Moreover, questions persist about whether Explainable AI (XAI) provides a significant advantage over black-box AI models and how to tailor the information presented by XAI to best support decision-making in different clinical contexts.\nAI techniques for predictive modeling are capable of analyzing vast datasets to assist in decisions such as target IOP level adjustments, visual field loss evaluation [23, 11, 17], surgical intervention [24, 1] and patient progress monitoring [25], Such techniques offer promising avenues to enhance glaucoma care. Despite a long history of technological advancement with AI to risk stratify glaucoma, less is understood about how to appropriately present the results of these models to clinicians at the time they make decisions for individual patients. Equally important is gaining insight into how clinicians integrate AI recommendations into their clinical decision making. Particularly, in the glaucoma context, additional research is needed to understand the glaucoma clinical workflow and decision-making process for a successful integration of clinical decision support and its possibility to improve glaucoma outcomes [18]. This research must delve into the transparency and interpretability of AI systems for clinician trust and adoption, not only demonstrating technical feasibility but also considering end users in the design and evaluation of human factors [5]. Recent empirical studies have explored explainability techniques within specific contexts, primarily focusing on experts accustomed to the clinical tasks being augmented by AI [14, 21, 15]. Conversely, research involving non-experts reveals potential biases in decision-making, including a tendency for overreliance on AI recommendations [9]. Understanding the biases and challenges that arise for target users without"}, {"title": "2 Methods", "content": "Hypotheses We explored the impact of AI assistance on referral decision-making within the context of glaucoma, focusing on how it affects the user experience and decision outcomes when interacting with AI systems that provide transparent reasoning. In particular, we formulate the following hypotheses:\nH1.a: Groups using AI-based systems exhibit better performance compared to groups that do not use AI-based systems.\nH1.b: Groups with AI explanations demonstrate better performance compared to groups using AI-based systems without explanations.\nH2.a: Groups with AI support have a better decision-making experience compared to groups that do not use AI-based systems.\nH2.b: Groups with explanations have a better decision-making experience compared to groups using AI-based systems without explanations.\nHuman-AI task description and interface To evaluate the effectiveness of an AI-based referral system and providing transparency on its reasoning, we formulate a user study in which primary eye care provides (optometrists) were asked to make referral recommendations to decide if the patient presented in a clinical vignette needs a referral to a glaucoma specialist within 3 months, 3-12 months, or does not need a referral currently, as illustrated in Figure 1. The clinical vignette includes clinical features, such as age, gender, race, IOP, and best visual acuity (VA), outcomes of the ophthalmic tests, and eye medications. The VF test includes the mean deviation (MD) and pattern standard deviation (PSD), and visualizations of total deviation, pattern deviation and percentile plots. The report of the"}, {"title": "Conditions", "content": "We designed four conditions that varied in whether and how an AI-based system assisted the participants in making patient referral decisions.\n\u2022 Human Only: In the Human Only condition, participants were shown only the clinical data, VF data and OCT data of the patient. This condition served as a baseline where participants had to make decisions without any additional guidance or recommendations.\n\u2022 No Explanation: In the No Explanation condition, participants were shown the clinical data, VF data, and OCT data of the patient, with the AI recommendation from a deep learning model (DLM) displayed and no additional information provided. Figure 1 illustrates how the AI prediction is displayed without any further information from the model.\n\u2022 Feature Importance Explanations: In the Feature Importance Explanations condition, par- ticipants were shown the clinical data, VF data, and OCT data of the patient. The AI recommendation from a DLM was displayed along with the top three most important features for an individual prediction, which were calculated using Shapley Additive Explana- tions (SHAP). SHAP values indicate each feature's contribution to the prediction compared to a baseline, providing a local method to explain a prediction based on a single input patient case [13]. As shown in Figure 2, high PSD, low average RNFL thickness, and low MD are identified as the top three most important features influencing the AI recommendation for a specific patient. For simplicity, we only display the feature names and not the magnitude of their contribution to the prediction.\n\u2022 Scoring-based Explanations: In the Scoring-based Explanations condition, participants were shown the clinical data, VF data, and OCT data of the patient. The AI recommendation was displayed with a visual risk scorecard and a transparent mathematical formula for determining a final score referral score. The coefficients for each feature in the formula were"}, {"title": "Models", "content": "A DLM for surgery prediction and the logistic regression based credit scorecard model were utilized to provide referral recommendations. We utilized a vision transformer (ViT) [7] in the DLM to extract features from visual representations of the VF and OCT data. Additionally, the"}, {"title": "Study cases", "content": "We selected 20 cases from a retrospective longitudinal study of glaucoma patients conducted at the Wilmer Eye Institute. The data corresponds to patients who had their first VF, OCT, and clinical ophthalmology assessment (baseline visit) on the same day. Surgical candidates were selected based on the criteria that the period from their baseline visit to their surgery fell within the previously defined time frames, such as within three months and between three months and one year, indicating more urgent cases. Similarly, non-surgical patients were included if the time between their baseline and second ophthalmology consultations fell within the specified time horizons, and they also had a follow-up consultation. Our case selection for the user study was as follows:\n\u2022 Four eyes that underwent surgery within 0-3 months\n\u2022 Four eyes that underwent surgery within 3-12 months\n\u2022 Eight eyes that did not undergo surgery within 12 months\n\u2022 Four eyes that were inaccurately predicted by both the DLM and scorecard model (three false positives and one false negative, mirroring the DLM prediction performance)\nTo ensure that the model's performance does not confound the effects of different types of AI interventions, we specifically chose the cases where both the DLM and credit scorecard models provided identical predictions for all 20 cases. The only difference between the models is the explanation method used. To maintain a uniform difficulty across the four groups (Human only or baseline, No Explanation, Feature Importance, and Scoring-based explanations), we randomly select (without replacement) one eye from the 0-3 month interval, one from the 3-12 month interval, two eyes not needing surgery within a year, and one from the wrongly predicted category (misclassifying cases within 0-3 months and no surgery). This ensures that the accuracy of the AI recommendation in the No Explanation, Feature Importance Explanations, and Scoring-based Explanations stands at 80%. In total, participants evaluated 20 cases that were distributed across the four experimental conditions following the strategy described above. The order of the groups with AI support and the order of the cases within each group were random. Each participant interacted with all four conditions consistent with a within-subjects design."}, {"title": "Study Procedure", "content": "In an online study, participants were first presented with a consent form and instructions. Then the participants went through the following steps: 1) They were asked demograph- ics questions about age, years of experience as an optometrist, completion of optometry residency, familiarity with AI on a 5-point scale. To prevent the participation of individuals pretending to be optometrists, we implemented a screening process requiring participants to correctly answer a multiple-choice question, with a maximum of three attempts allowed. 2) In the main task, participants first assessed the patient vignettes under the Human Only condition. The subsequent three conditions included AI support and were presented in a random order. While the clinical data of 20 cases were fixed, the order and the AI explanations for the cases were randomized for each participant. Participants then make the final referral recommendation to decide if the patient needs a referral to glaucoma specialist for surgical intervention within 3 months, 3-12 months, or does not need a referral currently, along with the confidence level in their responses. 3) For each patient vignette"}, {"title": "Participants", "content": "We recruited participants who are optometrists and >18 years old. We posted the survey on the American Academy of Optometry and various listservs within our institution. Participa- tion in this study is completely voluntary and contingent upon agreement to the informed consent. To ensure data quality, we excluded data from participants whose median responding time was less than five seconds or those who provided the same recommendation to all cases. In total, 87 optometrists joined the study. Participants' age, experience, completion of an optometry residency, proficiency with AI, and the preferred model evaluation are presented in Table 2. Around half of the participants had optometry residency. The median years of experience of participants is 9 (5.0, 19.5). The majority of participants (> 80%) indicated a high level of familiarity with AI . Participants who successfully completed the study entered a lottery to receive one out of ten $100 USD Amazon gift cards. The study was approved by the Johns Hopkins Medicine Internal Review Board. The study took 15 - 20 minutes on average to complete."}, {"title": "Measures", "content": "We used a set of objective and subjective metrics to evaluate participants' experience in the referral decision making task. We collected several objective metrics:\n\u2022 Overall performance: To assess participants' performance, we compared their answers to the ground truth of each patient case and computed the accuracy in each condition. If the participant's answer matched the decision for surgery on the clinical records, it was considered a correct referral. Conversely, if the participant's answer differed from the recorded clinical decision, it was considered an incorrect referral decision.\n\u2022 Time: This metric measures the duration in seconds that each participant spends reviewing the clinical data, considering the AI's recommendations (if provided), making the referral decision, and responding to the objective matrix.\n\u2022 Agreement with the AI: We quantified agreement as the percentage of cases where partici- pants followed the AI suggestions in each condition that involved AI support. In order to assess users' willingness to adhere to the AI's recommendations, we compared participants' answers to the AI suggestions for each case. Agreement with the AI's recommendations was indicated when a participant's response matched the AI suggestion, while a difference in response signified disagreement with the AI's advice. We further analyze cases when participants accepted an AI's incorrect prediction to measure overreliance [22].\n\u2022 Adjustments to AI predictions: presenting participants directly with AI predictions can lead to anchoring bias as they adjust insufficiently from the anchor, i.e., the AI prediction45. Average disagreement is commonly computed to quantify adjustment from AI predictions when the decision task has binary outcome, but with the potential outcomes of referral recommendations, we are interested in a finer grained analysis. We measure the extent of"}, {"title": "AI deviation score", "content": "participant deviations from AI predictions by computing the difference between the AI- suggested referral category and the participant's final response. We define the AI deviation score as a numerical variable, where zero indicates no adjustment, or agreement with the AI prediction. Disagreement between AI suggestions and participant responses can occur across one or two referral categories. For differences spanning one category (deviation=1), scenarios include: i) AI: no referral is currently needed, participant: referral within 3-12 months, ii) AI: referral within 3-12 months, participant: no referral is currently needed, iii) AI: referral within 3 months, participant: referral within 3-12 months, and iv) AI: referral within 3-12 months, participant: referral within 3 months. For differences spanning two categories (deviation=2), the discrepancy is more pronounced: i) AI: referral within 3 months, participant: no referral is currently needed and ii) AI: no referral is currently needed, participant: referral within 3 months.\nWe did not consider the direction of the changes, i.e., whether final responses result in an increase or decrease of the urgency level to be seen by a specialist. Smaller deviations mean participants adjusted less their final responses, and could have been more anchored to the AI prediction.\n\u2022 Over-claims: We identified instances where participants incorrectly increased the perceived urgency of a referral or suggest a referral when none is warranted, and report over-claims as a percentage of such decisions among all cases susceptible to over-claims, i.e., cases that need a referral within 3-12 months and cases that do not need a referral currently. This includes cases where \"The patient doesn't need a referral currently\" is misclassified as \"The patient needs a referral to a glaucoma specialist within 3 months\" or \"The patient needs a referral to a glaucoma specialist within 3-12 months\", and cases where \"The patient needs a referral to a glaucoma specialist within 3-12 months\" is misclassified as \"The patient needs a referral to a glaucoma specialist within 3 months\".\n\u2022 Under-claims: We identified instances where participants incorrectly diminish the perceived urgency of a referral or do not suggest a referral at all when needed, and report under-claims as a percentage of such decisions among all cases susceptible to under-claims, i.e., cases that need a referral within 3 months and cases that need a referral within 3-12 months. This includes cases where \"The patient needs a referral to a glaucoma specialist within 3 months\" is misclassified as \"The patient needs a referral to a glaucoma specialist within 3-12 months\" or \"The patient doesn't need a referral currently\", and cases where \"The patient needs a referral to a glaucoma specialist within 3-12 months\" is misclassified as \"The patient doesn't need a referral currently\".\nIn addition, we collected several subjective metrics using a five-point Likert scale from 1 (Strongly disagree) to 5 (Strongly agree). Participants rated statements regarding confidence in their responses and workload in terms of effort and frustration [3]. In the groups using AI assistance, participants rated their agreement with the previous statements and also statements regarding trust in the AI's suggestions, integration of AI suggestions into their decision making process, helpfulness of the AI's suggestions, and future use of the AI-based system. The complete set of statements is included in the appendix."}, {"title": "3 Results", "content": "For the analysis of results, we first employed one-way repeated measures Analysis of Variance (ANOVA) tests on continuous objective metrics (accuracy, time, agreement rate, referral errors, Al deviation score) and Aligned Rank Transform (ART) ANOVA tests on subjective metrics (in- cluding confidence, workload, perceived challenge, trustworthiness, decision-making, helpfulness, and willingness for clinical practice). In the models, we define the type of decision support as the within subjects fixed effect and participant ID as the random effect. We validated normality and homogeneity of variance assumptions using the Shapiro-Wilk and Levene's tests, and when violated used Friedman's test, the non-parametric alternative for repeated measures ANOVA. For measure- ments with multiple observations within each experimental group, e.g., task time, we employed the non-parametric ART ANOVA. If the results have statistical significance, we further performed post-hoc pairwise statistical comparisons between the two groups using paired t-tests or Wilcoxon tests with Holm method correction. For all the statistical tests reported below, significance levels below .05 were considered as a statistically significant effect. We report effect sizes using partial eta"}, {"title": "Accuracy", "content": "The model's evaluation in our test set showed that the scorecard model has an average accuracy across the three time horizon categories of 0.77 and 0.71 for the black box and scorecard models, respectively. Additional performance metrics are presented in Table 3. Based on the selection of patient cases for the user study, the AI's accuracy in this subset of patients was 0.80 (only one incorrect recommendation in the five patients per experimental group). We observed significant differences in the accuracy across different conditions using Friedman's test (\\(x^2\\)(3) = 22.3, p < .001, W = 0.09). In the baseline condition, participants alone achieved a mean accuracy of 0.51 (95% CI: 0.46, 0.56). In the presence of Scoring-based Explanations, accuracy was on average 0.62 (95% CI: 0.57, 0.66), surpassing both 0.58 in No Explanation (95% CI: 0.57, 0.66) and 0.60 (95% CI: 0.55, 0.64) in Feature Importance Explanations. Post-hoc Wilcoxon tests with the Holm correction method revealed that participants in the AI-based groups had a significantly higher accuracy compared to the baseline condition (No Explanation: p = .005, Feature Importance: p = .001, Scoring-based: p < .001). Participants in the AI-based groups achieved better performance, but there were no statistically significant differences between the No Explanation, Feature Importance Explanations, and Scoring-based Explanations conditions."}, {"title": "Time", "content": "We compared the time required for participants to complete the referral task across all the experimental groups. Because the normality assumption was not met (p < .05), we used an ART ANOVA to measure the effect of the type of support on task time. We found a significant difference in the average time that participants spend based on the type of support they receive (F(3, 258) = 10.57, p < .001, \\(\u03b7^2\\) = 0.11). Post-hoc pairwise comparisons using the Holm-Bonferroni correction indicated that when participants received AI predictions only in the No Explanation group (median=28, IQR=[18, 43]), they spent significantly less time compared to the non-assisted group (median=35, IQR=[22, 57]), and also when comparing against the Feature Importance (median=33, IQR=[22, 50]) and Scoring-based Explanations (median=33, IQR=[22, 50]) groups (all comparisons <.001). No significant differences were observed between the latter two and the human Only group or between the Feature Importance and Scoring-based Explanations."}, {"title": "Agreement with AI predictions", "content": "We evaluated whether participants' agreement with AI predictions was affected by the type of explanations provided using Friedman's test since the normality assumption was violated (p < .05). We observed that there is no significant difference in agreement with AI recommendations across different types of AI support (\\(x^2\\)(2) = 5.88, p = .053). Within the Scoring- based Explanations group, the average agreement with AI suggestions was the highest with 0.79 (95% CI: 0.77, 0.80), followed by group Feature Importance Explanations 0.78 (95% CI: 0.76, 0.79) and No Explanation 0.72 (95% CI: 0.70, 0.74).\nBecause all AI models had an accuracy of 80%, we further analyze agreement with incorrect AI predictions to measure potential overreliance. The normality (p<.001) and variance (p<.05) assumptions were violated and Friedman's test was used to measure whether the type of AI support affects overreliance. We found that overreliance is significantly different across AI support types (\\(x^2\\)(2) = 10.2, p = .006, W = 0.06). Post-Hoc Wilcoxon tests showed significantly higher (p = .02) agreement when AI predictions are presented with Feature Importance explanations (M = 0.92, SD = 0.27) compared to not presenting explanations at all (M = 0.77, SD = 0.42). No significant differences were observed with respect to the Scoring-based Explanations (M = 0.85, SD = 0.36)."}, {"title": "Over-claims and Under-claims", "content": "The Al errors could either be under-claims (i.e. 0-3 months cases predicted as no surgery) or over-claims (i.e. no surgery cases predicted as within 0-3 months). In general, underclaims were less common as a type of Al error (occurred in around 30%). We observed significant differences in the fraction of over-claims across different conditions (F(3,258) = 5.68, p < 0.001, \\(\u03b7^2\\) = 0.06). Post-hoc pairwise comparisons using the Holm-Bonferroni correction revealed that the over-claims rate in the Scoring-based Explanations group (M = 0.38, SD = 0.23) was significantly lower (p = .013) compared to the Human Only condition (M = 0.49, SD = 0.25). The fraction of over-claims was on average 0.42 (SD = 0.24) and 0.40 (SD = 0.25) in the No Explanation and Feature Importance groups, respectively. No further differences were found in the post-hoc comparisons.\nSimilarly, we measured the effect of the type of support on the fraction of under-claims. Since the normality assumption was violated (p < .001), we used a Friedman's test. The test revealed no significant difference in under-claims across the conditions with AI support nor with the Human Only condition (\\(x^2\\)(3) = 3.76,p = .288). In the Human Only group, the average under-claims fraction was 0.27 (SD = 0.33). Under-claims fraction was similar across all the groups that used AI support (No Explanation: M = 0.20, SD = 0.27, Feature Importance: M = 0.19, SD = 0.26, Scoring-based: M = 0.19, SD = 0.25)."}, {"title": "Adjustments to AI predictions", "content": "We assessed the impact of different types of AI support on the discrepancies between participants' final responses and AI predictions by analyzing the AI deviation score. Because both the normality (p<.001) and variance (p < .05) assumptions were violated, we used the non-parametric alternative ART ANOVA. We found a significant effect of the type of AI support on the AI deviation score (F(2,172) = 4.85,p = .008, \\(\u03b7^2\\) = 0.05). The average AI deviation score in the No Explanation group was 0.33 (SD = 0.57), while the average scores in the Feature Importance and Scoring-based Explanations were 0.24 (SD = 0.46) and 0.22 (SD = 0.44), respectively. Post-hoc comparisons indicated significantly larger deviations in participants' responses in the No Explanation group compared to the Feature Importance (p = .030) and Scoring-based Explanations (p = .013) groups. No significant differences were found between the two types of explanations."}, {"title": "Perceived challenge to complete the task", "content": "We observed significant differences in participants' ratings of the perceived challenging and/or frustrating level across different groups (F(3,258) = 3.48, p = .017, \\(\u03b7^2\\) = 0.04). Post-hoc pairwise comparisons using the Holm-Bonferroni correction indicated that participants' perceived level of challenge and frustration in completing the referral task was significantly higher in the Human Only group (M = 2.61, SD = 0.87) compared to the Scoring-based Explanations group (M = 2.38, SD = 0.81,p = .035) and No Explanation group (M = 2.38, SD = 0.84, p = .027). However, no significant differences were observed with Feature Importance explanations (M = 2.46, SD = 0.86) or between the different AI-based systems."}, {"title": "Integration of AI Recommendations", "content": "We observed significant differences in participants' self- reported integration of AI recommendations into their decision making processes across different AI- based conditions (F(2,172) = 4.58, p = .012, \\(\u03b7^2\\) = 0.05). Post-hoc pairwise comparisons using the Holm-Bonferroni correction indicated that participants' perception of integrating AI recommendations into their decision-making was significantly higher in the Scoring-based Explanations group (M = 2.83, SD = 1.09) compared to the No Explanation (p = .027) and Feature Importance Explanations groups (p = .027). No significant differences were found between the No Explanation (M = 2.67, SD = 1.13) and Feature Importance Explanation groups (M = 2.67, SD = 1.10)."}, {"title": "Helpfulness of the AI suggestions", "content": "We observed significant differences in participants' ratings of the usefulness of the AI's suggestions across different AI-based conditions (F(2,172) = 3.85, p = .023, \\(\u03b7^2\\) = 0.04). Post-hoc pairwise comparisons using the Holm-Bonferroni correction indicated that usefulness ratings were significantly higher (p = .023) in the Scoring-based Explanations group (M = 3.34, SD = 0.91) compared to the No Explanation group (M = 3.13, SD = 0.97), while no significant differences were observed against the Feature Importance Explanations group (M = 3.20, SD = 0.90). Likewise, no significant differences were observed between the No explanation and Feature Importance Explanation groups."}, {"title": "Willingness for deployment in clinical practice", "content": "We observed significant differences in partici- pants' ratings of their willingness to deploy AI in clinical practice across different AI-based conditions (F(2,172) = 4.58, p = .012, \\(\u03b7^2\\) = 0.05). Post-hoc pairwise comparisons using the Holm-Bonferroni correction indicated that participants' willingness to deploy AI in clinical practice was significantly higher (p = .037) in the Scoring-based Explanations group (M = 3.28, SD = 1.03) compared to the No Explanation group (M = 3.01, SD = 1.12), while no significant differences were observed compared to the Feature Importance Explanations group (M = 3.09, SD = 1.07). Likewise, no sig- nificant differences were observed between the No explanation and Feature Importance Explanation groups.\nThe majority of the participants preferred the Scoring-based Explanations (54%), followed by the Feature Importance (31%), and working on their own, as in the Human Only group (12.7). Only 2.3% preferred AI support without explanations."}, {"title": "Trust, confidence, and effort", "content": "We observed that there is no significant difference in participants' ratings of trust in the AI recommendations across different AI-based conditions (F(2,172) = 0.11, p = .895). There were no significant differences in participants' confidence in their referral decisions (F(3, 258) = 0.62, p = .602) or in their perceived effort required to complete the referral task across the experimental groups (F(3, 258) = 0.70, p = .556)."}, {"title": "4 Discussion", "content": "We conducted a study to assess the effects of AI support for identifying high-risk glaucoma patients. We compared the following scenarios: black box models that only provide recommendations, trans- parent models that offer explanations post-hoc or directly show how inputs are transformed into outputs, and a control group without any type of AI support. This structure allowed us to evaluate how different types of explanations influence participants' experience and performance. We built these algorithms to provide more realistic predictions to the patient vignettes and the presentation of feasible explanations for such cases."}, {"title": "5 Conclusion", "content": "Our investigation into the use of AI to support primary eye care providers in making referral decisions for glaucoma patients provides critical insights. While our explainable AI algorithms for surgical intervention prediction showcase the technical feasibility and potential to identify high-risk patients, the actual integration of these tools into clinical decisions highlighted both promises and challenges. Our findings suggest that AI support can enhance the ability of non-specialists to identify high-risk patients requiring urgent referrals while simultaneously alleviating the perceived challenges of the task. However, bridging the gap to reach the algorithmic performance levels achieved by the AI alone remains a critical challenge as keeping the human-in-the-loop is critical in this context. Although our results did not show that explanations improve providers' performance by making the AI's decision-making process more transparent, they reveal an opportunity to enhance the collaborative Human-AI decision-making process. Moving forward, the focus should be on developing AI tools that support both clinical effectiveness and user engagement, thereby maximizing the benefits of AI in healthcare."}]}