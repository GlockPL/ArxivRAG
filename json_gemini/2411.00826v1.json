[{"title": "Uncertainty Quantification via H\u00f6lder Divergence for Multi-View Representation Learning", "authors": ["Yan Zhang", "Ming Li", "Chun Li", "Zhaoxia Liu", "Ye Zhang", "Fei Richard Yu"], "abstract": "Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on H\u00f6lder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the \"distance\" between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpass the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, multi-view learning has become pivotal in machine learning, addressing diverse forms of multi-view data [2], [3]. In the field of multi-view learning, researchers have found that the performance of models can be improved by estimating the uncertainty of data distribution. However, incorporating uncertainty considerations in each modality for reliable predictions remains a gap."}, {"title": "II. RELATED WORKS", "content": "a) Multi-View Learning: Multi-view learning leverages diverse data perspectives to enhance machine learning, improving tasks like classification, clustering, and regression [11]\u2013[13]. Canonical Correlation Analysis (CCA) is a key method, optimizing linear feature combinations across views to maximize correlation [14]. Recently, contrastive learning and deep multi-view learning, driven by neural networks, have further advanced this field by improving performance and model sophistication [15]. Moreover, Wu et al. [16] proposed a Self-Weighted Contrastive Fusion method for deep multi-view clustering, which enhances clustering performance by learning a balanced fusion of multiple views while preserving the most informative features from each view. Tan et al. [17] present a method for unsupervised multi-view clustering that integrates and refines knowledge from both individual views and cross-view interactions to improve clustering performance. Gou et al. [18] proposes Reconstructed Graph Constrained Auto-Encoders, a novel framework for improving multi-view representation learning by incorporating graph structure constraints into the auto-encoder architecture.\nb) Evidence Theory: Dempster-Shafer theory [19], introduced by Glenn Shafer in 1976, is a mathematical framework for managing uncertainty and inference [20], [21]. Key principles include evidence, basic belief assignment, combination, and belief functions. Widely applied in machine learning, data mining, and medical diagnosis, it offers robust tools for handling large datasets and uncertainty. In multi-view learning, it enhances information integration from multiple sources, particularly through improved Dempster's combination rule [22]. For instance, Li et al. [22] improved multispectral pedestrian detection using confidence-aware fusion based on Dempster-Shafer theory. Zhang et al. [23] proposed a novel data augmentation method that combines Mixup and Dempster-Shafer theory to enhance model robustness and uncertainty estimation in machine learning tasks. Li et al. [24] proposed a confidence-aware fusion method based on Dempster-Shafer theory to enhance the accuracy and reliability of multispectral pedestrian detection.\nc) Uncertainty Estimation: Despite the success of deep learning in areas like image classification, natural language processing, and autonomous driving, managing uncertainty remains a significant challenge [25]. Uncertainty arises from incomplete or noisy data and complicates decision-making processes in real-world scenarios. Deep neural networks struggle with both data and model uncertainty, as well as accurately propagating uncertainty from inputs to outputs. Robust solutions are needed to address these issues. Recent advances in deep learning for uncertainty estimation include Bayesian methods, uncertainty quantification, and automated machine learning. Bayesian neural networks, which combine deep learning with Bayesian statistics, have been a focus since the 1990s. Monte Carlo methods, such as Monte Carlo Dropout, are also valuable for uncertainty estimation. Recent work on the Dirichlet distribution has further advanced the field. For example, Han et al. [5] introduced the Enhanced Trusted Multi-View Classification (ETMC) algorithm to improve multi-view classification."}, {"title": "Definition 1.", "content": "(H\u00f6lder Statistical Pseudo-Divergence, HPD [8]) HPD pertains to the conjugate exponents \u03b1 and \u03b2, where \u03b1\u03b2 > 0. In the context of two densities, $p(x) \\in L^{\\alpha} (\\Omega, \\nu)$ and $q(x) \\in L^{\\beta} (\\Omega, \\nu)$, both of which belong to positive measures absolutely continuous with respect to \u03bd, HPD is defined as the logarithmic ratio gap, as follows:\n$D_H(p(x): q(x)) = -log(\\frac{\\int_{\\Omega} p^{\\alpha}(x)q^{\\beta}(x)dx}{(\\int_{\\Omega} p^{\\alpha}(x)dx)^{\\frac{1}{\\alpha}} (\\int_{\\Omega} q^{\\beta}(x)dx)^{\\frac{1}{\\beta}}})$\nWhen 0 < \u03b1 < 1 and \u03b2 < 0 or \u03b1 < 0 and \u03b2 < 1, the reverse HPD is defined by:\n$D_H(p(x) : q(x)) = log(\\frac{\\int_{\\Omega} p^{\\alpha}(x)q^{\\beta}(x)dx}{(\\int_{\\Omega} p^{\\alpha}(x)dx)^{\\frac{1}{\\alpha}} (\\int_{\\Omega} q^{\\beta}(x)dx)^{\\frac{1}{\\beta}}})$\nDefinition 2. (Dirichlet Distribution [26]) The Dirichlet distribution of order K (where K > 2) with parameters \u03b1i > 0, i = 1, 2, 3..., K is defined by a probability density function with respect to Lebesgue measure on the Euclidean space $R^{K-1}$ as follows: $Dirichlet_n(\\mu_1,\\cdots, \\mu_K|\\alpha_1,...,\\alpha_K) = \\frac{\\Gamma(\\sum_{i=1}^K \\alpha_i)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)} \\prod_{i=1}^K \\mu_i^{\\alpha_i-1}$, where $ \\mu_i \\in S_K$, and $S_K$ is the standard K-1 dimentional simplex, namely,\n$S_K = {(\\mu_1, \\mu_2, ..., \\mu_K) | \\sum_{i=1}^K \\mu_i = 1, 0 \\leq \\mu_1\\mu_K \\leq 1}$\nand \u0393(.) is the gamma function, defined as: \u0393(s) = $\\int x^{s-1}e^{-x} dx$, s > 0.\nDefinition 3. (Exponential Family Distribution [27]) The probability density function of the Dirichlet distribution is expressed as follows: p (x; \u03b8) = exp{\u03b8TT(x)\u2212F(\u03b8)+B(x)}, where \u03b8 is the natural parameter, T(x) is the sufficient statistic, F(\u03b8) is the log-normalizer, and B(x) is the base measure.\nDefinition 4. (The Exponential form of the Dirichlet Distribution [28]) Exponential formulation of the Dirichlet distribution probability density function can be rewrite as follows:\n$Dirichlet_n (\\mu_1,\\cdots,\\mu_K|\\alpha_1,...,\\alpha_K) = exp{\\sum_{i=1}^{K} (\\alpha_i - 1) log \\mu_i - \\sum_i^{K} log \\Gamma (\\alpha_i) - log \\Gamma(\\sum_i^{K} \\alpha_i)}$\nAllowing us to obtain the canonical form terms: VoT(0):\n$, \\theta = \\alpha, T(\\mu) = ln(\\mu), F(\\eta) = \\psi(\\alpha_k) - \\psi(\\sum_{i=1}^{K} \\alpha_i), B(\\mu) = -ln(\\mu)$, and \u03c8 is the digamma function, defined as: $4(x) = \\frac{d}{dx} ln \\Gamma(x)$"}, {"title": "III. METHODOLOGY", "content": "A. Exploring Multi-Class Classification with Variational Dirichlet Modeling\nIn the field of machine learning, where the representation of compositional data is an integral part of addressing multi-class classification problems, Aitchison [29] introduced the Dirichlet distribution as the primary candidate for modeling such data. Mathematically, within a multi-class classification problem involving K classes, the aim is to determine a function to generate a predicted class label, with the overarching objective of minimizing the disparity between this predicted class label and the ground truth. Generally speaking, in deep learning, it is customary to employ the softmax operator to transform the continuous model output into a set of class probabilities. However, it is worth noting that the softmax operator often leads to overconfidence [5].\nThe Dirichlet distribution, indeed, stands as a versatile and pivotal probability distribution, particularly when it comes to modeling multi-classification problem and Bayesian inference. Its status as the conjugate prior for the multinomial distribution lends it immense utility in Bayesian statistics, as it ensures that the posterior distribution maintains the same form as the prior [30]. This property greatly simplifies the process of Bayesian inference and renders it analytically tractable.\nThe Dirichlet distribution is a versatile tool in probabilistic modeling, offering flexibility, interpretability, and computational advantages, making it suitable for various applications such as Bayesian statistics, natural language processing, and machine learning. Its key advantages include flexibility in modeling categorical data, conjugacy with the multinomial distribution for Bayesian inference, parameter interpretability, smoothing capabilities, and suitability for generative and hierarchical modeling tasks [30]. In multi-view classification, Dirichlet learning offers unique advantages by modeling dependencies between different data views through a stochastic process. It can handle variable-dimensional feature spaces and incorporate prior knowledge effectively, enhancing classification performance and interpretability [30].\nFor instance, the class probabilities, represented as \u03bc = [\u03bc1,\u00b7\u00b7\u00b7, \u03bc\u03ba], can be interpreted as parameters within a multinomial distribution, where $\\sum_{k=1}^{K} \\mu_k$ = 1. This distribution characterizes the likelihood of K mutually exclusive events occurring [31]. On the other hand, the Dirichlet distribution can be employed to capture uncertainty and mitigate issues of overconfidence. Given these considerations, our primary goal is to derive a Dirichlet distribution, which serves as the conjugate prior for the multinomial distribution, thereby allowing us to establish a predictive distribution. Since the consideration of the Dirichlet distribution, we commence by presenting the definition of the exponential family, given its association with this distribution.\nB. Multi-View Classification with Uncertainty-Aware Variational Dirichlet Learning\n\"Multi-view classification with uncertainty-aware variational Dirichlet learning\" is an enhanced algorithm based on the trusted multi-view classification (TMC) algorithm. In trusted multi-view classification, the process involves the acquisition of class probabilities from different modalities, followed by the modeling of these class probabilities using a Dirichlet distribution to derive the distribution of classification results. This process yields \u201cevidence\" regarding the reliability of the classification. Subsequently, utilizing this evidence and employing evidence theory, the algorithm computes the confidence and uncertainty associated with the classification results. Finally, the Dempster-Shafer theory, a method for probabilistic reasoning, is utilized to fuse the classification results obtained from various modalities. However, within the TMC algorithm, the interaction between different modalities occurs primarily at the decision-making level, which can potentially limit its performance in specific scenarios.\nTo illustrate, let's consider a smart home system employing the TMC algorithm, which is divided into three views: data collection, processing, and control. If interactions between these views are limited to the control layer, a situation might arise where a user wishes to adjust room temperature using a smartphone application. The absence of a direct mechanism to link the data collection and data processing views can result in delays or operational errors.\nIn response to this challenge, researchers introduced the enhanced trusted multi-view classification algorithm. This enhancement involves the introduction of an additional \"pseudo-view\" to facilitate interactions between different perspectives. The pseudo-view is generated based on the original model and shares similar structural elements and parameters. It serves as an extension or complement to the original model, enabling the inclusion of additional viewpoints or information sources. By incorporating the pseudo-view, new perspectives can be seamlessly integrated into the existing model, enhancing performance through the utilization of multiple viewpoints and information sources. For instance, in natural language processing tasks, the primary view could be a statistically trained language model, while a neural network-based semantic representation is introduced as a pseudo-view. This enables the system to achieve a more comprehensive understanding of textual content, thus enhancing its expressiveness and inferential capabilities. Empirical results demonstrate that the ETMC algorithm outperforms the TMC algorithm on multi-view datasets. Consequently, in our research, we adopt the ETMC algorithm to achieve our objectives.\nC. Uncertainty Analysis\nIn the ETMC algorithm, modality fusion is primarily grounded in subjective logic [32] and Dempster-Shafer's theory [19]. Throughout the training process, it is imperative to conduct a quantitative analysis of the uncertainty and credibility associate with each modality, yielding specific values. Subsequently, a simplified evidence theory is employed to facilitate modal fusion. Furthermore, an assessment of uncertainty and credibility using subjective logic is conducted on the classification results of the fused modalities.\nTo calculate the uncertainty and credibility of individual modalities in the algorithm, a Dirichlet distribution is introduced. This distribution serves as a \"distribution\" for the features extracted by the model's classification layer. Confidence in the classification results and the quantification of uncertainty are computed through the Dirichlet distribution and subjective logic. Based on this data, modalities are selectively fused using evidence theory. Additionally, to obtain a Dirichlet distribution, the algorithm replaces the commonly used softmax layer with a non-negative activation function layer. The specific steps are as follows: for a K-classification task, each sample contains data from V modalities. For modality M\u00b9 = {$b_k$$_{k=1,V}$, the uncertainty of confidence in the corresponding classification result can be calculated using the Dirichlet distribution. For M\u00b2 = {$b_k$$_{k=1,V^2}$, then employ the simplified evidence theory to calculate the fusion of modality M = M\u00b9 \u2295 M\u00b2. The simplified fusion rules are"}, {"title": "Algorithm 1: Uncertainty Estimation via H\u00f6lder Divergence for Multi-View Representation Learning.", "content": "The second term of loss function:\n$L_{pseudo}(\\{x^m\\}_{m=1}^M, Y_n) = (E_{\\mu^m \\sim Dir(\\mu^m|\\alpha^m)} [logp(y|\\mu^m)] - \\lambda D_H[Dir(\\mu^m|\\alpha^m)||Dir(\\mu^m| [1,\\ldots,1])])$    (6)\nThe third term of loss function:\n$L^m (x^m, y) = (E_{q_{\\phi}(\\mu^m|x^m)}[logp(y|\\mu^m)] - \\lambda D_H[D(\\mu^m | \\alpha^m)||D(\\mu^m | [1,..., 1])])$  (7)\nThe primary component in the objective function corresponds to the variational objective function for M integrated modalities. Essentially, this variational objective function involves integrating the traditional cross-entropy loss over a simplex defined by the Dirichlet function. The secondary component serves as a prior constraint to ensure the creation of a more plausible Dirichlet distribution. In essence, the primary variational objective function assesses the model's performance by comparing its predictions to the true labels while imposing constraints on the generation of a more sensible Dirichlet distribution.\nThe second component within the objective function represents the variational objective function for M integrated pseudo-modalities. The third component within the objective function is focused on deriving the Dirichlet distribution for each individual modality. For a specific modality denoted as \"m\", its loss function can be formulated as previously described. And the overview of the uncertainty estimation via H\u00f6lder divergence for multi-view representation learning is shown in Fig. 2. And the algorithm is shown in 1.\ngiven by $b_k = \\frac{1}{1-C}(b^1_k + b^2_k + b^2_ku^1)$, Where u = $Doulu^2$. In this scenario, each sample contains data from V modalities, resulting in M = M1 M\u00b2 MV.\nD. Variational Inference for H\u00f6lder Divergence\nA generative model can be expressed as $p_{\\theta}(x,z) = p_{\\theta}(x|z)p(z)$, where $p_{\\theta}(x|z)$ is the likelihood, and p(z) is the prior. From the perspective of a Variational Autoencoder (VAE) [33], the true posterior p(z|x) can be approximated by $q_{\\phi}(z|x)$. The evidence lower bound (ELBO) $\\mathcal{L}_{ELBO}(\\theta, \\phi; x)$ for VAE can be formulated as:\n$E_{q_{\\phi}(z|x)} [log p_{\\theta} (x|z)] - D_{KL}(q_{\\phi}(z|x)||p(z))$,   (2)\nAccording to the Cauchy-Schwarz regularized autoencoder [34], the objective function incorporating H\u00f6lder Statistical Pseudo-Divergence regularization can be specified as $\\mathcal{L}_{HLBO} (\\theta, \\phi; x)$:\n$E_{q_{\\phi}(z|x)} [log p_{\\theta} (x|z)] - \\lambda D_H (q_{\\phi}(z|x)||p(z))$,    (3)\nwhere $D_H$ denotes the HPD, and \u03bb is the regularization parameter. In summary, we derive the overall loss function as follows:\n$\\mathcal{L}_{overall}= \\sum_{i=1}^N \\mathcal{L}_{fused} (\\{x^m\\}_{m=1}^M, Y_n) + \\sum_{i=1}^N \\mathcal{L}_{pseudo} (\\{x^m\\}_{m=1}^M, Y_n) + \\sum_{i=1}^N \\sum_{m=1}^{M}\\mathcal{L}_{m} (x_m, Y_n)$.   (4)\nNow, let's delve into the specific components of the loss function. The first term of loss function:\n$\\mathcal{L}_{fused} (\\{x^m\\}_{m=1}^M, Y_n) = (E_{\\mu \\sim Dir(\\mu|\\alpha)} [log p(y|\\mu)] - \\lambda D_H [Dir(\\mu|\\hat{\\alpha}||Dir(\\mu|[1,\\cdots,1])])$.      (5)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct experiments across diverse scenarios to comprehensively evaluate our algorithm. Specifically, we apply our algorithm to a variety of multi-view classification tasks, including RGB-D scene recognition, using four real-world multi-view datasets.\nA. Datasets\na) Classification Datasets: To evaluate the performance of our model on multi-view classification tasks, we utilize the following datasets: 1. SUNRGBD [1]: The SUN RGB-D dataset includes 4,845 training samples, 3,000 testing samples, and 24,869 samples used for combined training and testing across 19 scene categories. 2. NYUDV2 [35]: NYUD2 is an RGB-D dataset with 1,449 image pairs, reorganized into 10 classes, with 795 samples for training and 654 for testing. 3. ADE20K [36]: ADE20K is a semantic segmentation dataset with over 20,000 images across 150+ categories, reorganized into 10 groups, with 795 samples for training and 654 for testing. 4. ScanNet [37]: ScanNet consists of 1,513 indoor scenes with point cloud data, covering 21 object categories, with 1,201 scenes used for training and 312 for testing.\nb) Clustering Datasets: In addition to classification tasks, our model's performance in clustering tasks is evaluated using three multi-view datasets: 1. MSRC-V1 [38]: This image dataset contains eight object classes, each with 30 images. Following [38], we select seven classes: trees, buildings, airplanes, cows, faces, cars, and bicycles. 2. Caltech101-7 [7]: A subset of Caltech101, this dataset includes images from seven selected classes, as identified in previous work [7]. It is commonly used for training and evaluating object recognition algorithms. 3. Caltech101-20 [7]: Another subset of Caltech101, this dataset features images from 20 selected classes based on prior research [7], providing a broader range of objects for testing and refining recognition models.\nB. Evaluation Metrics, Purpose of the Experiment\nIn machine learning, \u201cAccuracy\" is used to assess a model's performance. It is defined as $Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$, where TP (True Positives) and TN (True Negatives) represent correct classifications, and FP (False Positives) and FN (False Negatives) represent incorrect classifications. Accuracy measures the overall correctness of the model's classifications. The clustering accuracy (CA) [39] is defined as: CA = $ \\sum_{i=1}^n \\delta(q_i, map(p_i))$, where \u03b4(a, b) = 1 if a = b,and d(a, b) = 0 otherwise. And map(\u00b7) is the best permutation mapping that matches the predicted clustering labels to the ground truths.\nConsidering practical applications, the objectives of this experiment are threefold: (1). Assess the recognition capability of the exploring uncertainty estimation via H\u00f6lder divergence for multi-view representation learning (HDMVL) algorithm in more intricate and expansive scenarios, comparing the outcomes with previous experiments conducted on smaller datasets. (2). Examine the potential of H\u00f6lder divergence to improve the classification performance of the HDMVL algorithm. Additionally, explore whether fine-tuning H\u00f6lder divergence parameters can enhance the model's performance across diverse datasets. (3). Investigate the impact of uncertainty analysis on refining the classification performance of the model in multi-class classification and clustering tasks that encompass multi-view data.\nC. Data Preprocessing\nMerge and preprocess the samples from the mentioned datasets. In multi-view datasets, images at specific angles typically comprise both color RGB images and depth images. Prior to training, it is necessary to concatenate the image data at specific angles to streamline the classification process.\nD. Model Architecture\nDuring the study, we use three different network architectures. The ResNet-18 [40] pretrained on the ImageNet [41] served as our foundational framework. ResNet-18 is a deep residual neural network comprising 18 layers. The second is the Mamba model [10] that performs well in long sequence modeling tasks. Mamba alleviates the modeling constraints of convolutional neural networks through global field of perception and dynamic weighting, and provides advanced"}, {"title": "E. Experimental Analysis", "content": "For multi-view classification, accuracy (ACC) stands out as a pivotal metric. Our objective in multi-view classification is to accurately classify scenes within the dataset using the network for subsequent analysis.\nIn the intra-class experiments, we assess the HDMVL model's performance across four multi-class datasets, comparing it with the HDMVL model. During testing, we evaluate the classification accuracy of individual modalities separately as well as in their fused form. The experimental results are summarized in Table IV.\nFor the two 10-class datasets, NYUD Depth V2 and ADE20K, the HDMVL model demonstrate superior performance, achieving fusion modality accuracies of 73.64% and 90.87%, respectively\u2014an improvement of 1.21% and 1.09% over the ETMC model. Notably, accuracy for individual modalities also increased after incorporating H\u00f6lder divergence, particularly in the color RGB modality of the NYUD Depth V2 dataset, where recognition accuracy improved by 3.01%. This improvement is even more pronounced in the 16-class ScanNet and 19-class SUN RGB-D datasets. The fusion modality accuracy on the SUN RGB-D reached 62.10%, surpassing the ETMC model by 1.25%. The likely reason for this improvement is that H\u00f6lder divergence, when apply to multi-class data, can more accurately identify the data features of each category.\nThese results suggest that HDMVL maintains high accuracy in more complex scenarios with a greater number of classes, achieving improved classification performance through enhancements to the objective function based on the H\u00f6lder index.\nb) Inter-Class Experimental Results: Inter-class experiments entail a comparison between the HDMVL and pre-existing algorithms that have undergone experimentation on the datasets employ in this study. Subsequent to analyzing the experimental results, NYUD Depth V2 and SUN RGB-D, the two datasets with the most extensive experimentation, are chosen for further scrutiny.\nIn this study, we conduct a comprehensive comparison of our proposed HDMVL with the current state-of-the-art methods using the NYUD Depth V2 and SUN RGB-D datasets. The results clearly demonstrate that our model outperforms these methods on both datasets. Notably, in the classification of fused modalities, our model adeptly integrates information features from RGB and Depth modalities in a highly rational manner, achieving the highest accuracy among similar models at 73.6% and 62.1%, respectively.\nThe experimental findings underscore the positive impact of uncertainty analysis on enhancing the accuracy of multi-view classification models, particularly in the context of fused modalities. Uncertainty analysis enables the model to discern more accurately which modality's information is reliable and precise in a given scene. Consequently, the model places greater emphasis on the information from this modality during fusion, leading to improved results. Furthermore, the refinement of the objective function based on the H\u00f6lder divergence enhances the specificity and granularity of uncertainty analysis results, contributing to a further boost in the model's overall performance. The experimental results are presented in Table VI.\nc) Inter-Class Experimental Results: On the basis of the above experiments, we carry out experiments of different network architectures. The performance of ResNet [40], Mamba [10] and VIT [9] on four multi-class datasets is tested in Table V.\nWe observe that the model maintains strong classification performance after changing the network architecture, particularly when the backbone is replaced with VIT [9], resulting in higher accuracy compared to the other two architectures. This improvement suggests that the global attention mechanism in VIT better captures image features, leading to more reliable classification results. These findings demonstrate that our method is adaptable to different network architectures. Additionally, we validate the model's robustness on noisy datasets. Detailed results are presented in Table III. Gaussian noise with a mean of 0 and variances of [0.01, 0.02, 0.05] is injected into two life scenario datasets, a and b, respectively. The HDMVL model is then trained with a H\u00f6lder index of 1.7.\nd) Clustering Experimental Results: Table VII compares the clustering performance of HDMVL with several state-of-the-art methods on the Caltech101-7, Caltech101-20, and MSRC-v1 datasets. Overview of uncertainty estimation using H\u00f6lder divergence for multi-view representation learning is shown in Fig. 3. t-SNE visualizations of multi-view clustering results on diverse datasets: (a) Caltech101-7, (b) Caltech101-20, and (c) MSRC-V1. These results demonstrate that our model's uncertainty quantification enhances clustering performance and provides a comparative analysis of the outcomes. The results show that most multi-view clustering methods perform worse than ours. Notably, the HDMVL model achieves a higher clustering effect using a single mode compared to other methods using both modes. When utilizing multiple modes, the HDMVL model significantly outperforms the other methods. Although HDMVL is not specifically designed for clustering tasks, it successfully handles these scenarios, demonstrating its robust learning capability even when trained on clustering datasets.\nF. Ablation Study\nTo further clarify, the ADE20K dataset is selected as the experimental basis for training the classification model to evaluate the impact of H\u00f6lder divergence on both individual modality recognition and fused modality recognition. The results, as shown in Table II, demonstrate a significant improvement in accuracy after incorporating H\u00f6lder divergence into the model. This enhancement is particularly pronounced in individual modality recognition, where the model's ability to accurately classify distinct modalities saw a notable increase. Additionally, in fused modality recognition, where information from multiple modalities is integrated, the model achieves higher accuracy compared to its original version. To assess the effect of the H\u00f6lder exponent on model performance, we conduct tests on several different datasets, as presented in Table VIII. The results indicate that the highest accuracy in the fusion mode of the classification model occurs when the H\u00f6lder exponent is 1.7. Deviating from this value, either lower or higher, leads to a decline in fusion mode accuracy.\nThese findings underscore the positive impact of H\u00f6lder divergence on the model's classification capabilities, both for individual modalities and in scenarios involving the fusion of diverse modalities. The implications extend beyond the ADE20K dataset, suggesting potential improvements in classification performance and generalization across various multi-class datasets, particularly in situations with limited sample sizes."}, {"title": "V. CONCLUSION", "content": "This study presents an uncertainty-aware variational Dirichlet learning approach to tackle challenges in multi-view representation learning. By incorporating subjective logic, the DS-combination rule, and H\u00f6lder divergence between Dirichlet distributions, the methodology significantly enhances recognition performances across a wide range of multi-modal benchmarks. Extensive experimental results confirm the approach's theoretical soundness and practical robustness, demonstrating improved performance in complex datasets and the effectiveness of H\u00f6lder divergence in uncertainty measurement."}, {"title": "APPENDIX A", "content": "THE RATIONALE FOR EMPLOYING H\u00d6LDER DIVERGENCE\nHD can be analytically computed for exponential family distributions. Fortunately", "follows": "nLemma 1. (HPD and PHD for Conic or Affine Exponential Family) [8", "closed-form": "n$D_H^\\alpha (p: q) = \\frac{1"}, {"follows": "n$D_H^\\alpha (p : q) = \\frac{1"}, {"inferences": "n$\\frac{1"}, {"alpha\\theta_k)": "sum [log \\alpha\\theta_k + log [\\Gamma (\\alpha\\theta_k)"}, {"alpha\\theta_k)": "k log \\alpha + \\sum log\\theta_k + \\sum log (\\Gamma\\theta_k)$", "conclusions": "n$\\frac{1"}, {"conclusions": "n$\\sum log\\Gamma (\\theta_k + \\mu_k + 1) - log \\Gamma (\\sum (\\theta_k + \\mu_k + 1)) =   \\frac{1"}, {"have": "n$D_{KL"}], "have": "n$D_H(q(z|x)||p(z)) = ( \\frac{1"}, {"becomes": "n$\\mathcal{L"}, {"that": "ELBO_H > ELBO_{KL}$. Since the HPD is more flexible and tunable through the parameters \u03b1"}]