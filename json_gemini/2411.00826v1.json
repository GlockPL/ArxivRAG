{"title": "Uncertainty Quantification via H\u00f6lder Divergence for Multi-View Representation Learning", "authors": ["Yan Zhang", "Ming Li", "Chun Li", "Zhaoxia Liu", "Ye Zhang", "Fei Richard Yu"], "abstract": "Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on H\u00f6lder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the \"distance\" between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpass the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, multi-view learning has become pivotal in machine learning, addressing diverse forms of multi-view data [2], [3]. In the field of multi-view learning, researchers have found that the performance of models can be improved by estimating the uncertainty of data distribution. However, incorporating uncertainty considerations in each modality for reliable predictions remains a gap.\nThere are two categories in the estimation methods of uncertainty. The first category often assigns equal weights to modalities, lacking practicality [4]. The second dynamically assigns weights to each modality, considering uncertainty to avoid unreliable predictions [5]. Regardless of the approach, estimating the uncertainty in the classification results, especially the distribution uncertainty, is critical to the reliability of the model. Current methods often use the Kullback-Leibler divergence (KLD) [6] to estimate the uncertainty of the classification results, but challenges persist in accurately discerning distribution uncertainty [7]. To address this, we use HD [8], superior in clustering experiments, replaces KLD in the models for more precise classification outcomes.\nOur method outperforms existing methods, offering a systematic analysis, identification of critical determinants, and empirical validation across four multi-view scenario datasets. In addition, we also test the performance of the attention mechanism in the field of multimodal image classification. Specifically, this study uses the attention mechanism to extract image features of different modalities, and uses the Visual Transformer (ViT) model [9] and the Mamba model [10] to explore the application of different types of attention mechanisms in the field of image recognition.\nIn summary, the contributions of this paper can be encapsulated as follows:\n\u2022 Enhanced Objective Function: Through an exploration of H\u00f6lder divergence's mathematical properties, we elevate the ETMC model's objective function, resulting in the creation of the HDMVL model. Experimental results across four multi-view scenario datasets conclusively demonstrate that the HDMVL model outperforms the original ETMC model in terms of classification accuracy.\n\u2022 Divergence Formulas: We have delved into the impact of utilizing diverse divergence formulas to formulate objective functions concerning classification outcomes. This exploration yields fresh insights into the enhancement of multi-view classification and clustering models, affirming that an improved objective function can significantly boost classification and clustering efficacy. Furthermore, it underscores the favorable influence of H\u00f6lder divergence on classification and clustering accuracy and model performance within the realm of multi-view learning tasks.\n\u2022 Empirical Validation: Our extensive empirical experiments provide concrete evidence that H\u00f6lder divergence excels over KLD in the context of multi-class classification and clustering tasks, emphasizing its superior performance. It also highlights the adaptability of H\u00f6lder divergence to a variety of multi-class classification and clustering tasks, offering the potential for reduced computational costs through adjustments in the H\u00f6lder index. Additionally, the experiment proves that the global attention mechanism can better integrate information between different modalities and improve the performance of multimodal classification models."}, {"title": "II. RELATED WORKS", "content": "a) Multi-View Learning: Multi-view learning leverages diverse data perspectives to enhance machine learning, improving tasks like classification, clustering, and regression [11]\u2013[13]. Canonical Correlation Analysis (CCA) is a key method, optimizing linear feature combinations across views to maximize correlation [14]. Recently, contrastive learning and deep multi-view learning, driven by neural networks, have further advanced this field by improving performance and model sophistication [15]. Moreover, Wu et al. [16] proposed a Self-Weighted Contrastive Fusion method for deep multi-view clustering, which enhances clustering performance by learning a balanced fusion of multiple views while preserving the most informative features from each view. Tan et al. [17] present a method for unsupervised multi-view clustering that integrates and refines knowledge from both individual views and cross-view interactions to improve clustering performance. Gou et al. [18] proposes Reconstructed Graph Constrained Auto-Encoders, a novel framework for improving multi-view representation learning by incorporating graph structure constraints into the auto-encoder architecture.\nb) Evidence Theory: Dempster-Shafer theory [19], introduced by Glenn Shafer in 1976, is a mathematical framework for managing uncertainty and inference [20], [21]. Key principles include evidence, basic belief assignment, combination, and belief functions. Widely applied in machine learning, data mining, and medical diagnosis, it offers robust tools for handling large datasets and uncertainty. In multi-view learning, it enhances information integration from multiple sources, particularly through improved Dempster's combination rule [22]. For instance, Li et al. [22] improved multispectral pedestrian detection using confidence-aware fusion based on Dempster-Shafer theory. Zhang et al. [23] proposed a novel data augmentation method that combines Mixup and Dempster-Shafer theory to enhance model robustness and uncertainty estimation in machine learning tasks. Li et al. [24] proposed a confidence-aware fusion method based on Dempster-Shafer theory to enhance the accuracy and reliability of multispectral pedestrian detection.\nc) Uncertainty Estimation: Despite the success of deep learning in areas like image classification, natural language processing, and autonomous driving, managing uncertainty remains a significant challenge [25]. Uncertainty arises from incomplete or noisy data and complicates decision-making processes in real-world scenarios. Deep neural networks struggle with both data and model uncertainty, as well as accurately propagating uncertainty from inputs to outputs. Robust solutions are needed to address these issues. Recent advances in deep learning for uncertainty estimation include Bayesian methods, uncertainty quantification, and automated machine learning. Bayesian neural networks, which combine deep learning with Bayesian statistics, have been a focus since the 1990s. Monte Carlo methods, such as Monte Carlo Dropout, are also valuable for uncertainty estimation. Recent work on the Dirichlet distribution has further advanced the field. For example, Han et al. [5] introduced the Enhanced Trusted Multi-View Classification (ETMC) algorithm to improve multi-view classification.\nDefinition 1. (H\u00f6lder Statistical Pseudo-Divergence, HPD [8]) HPD pertains to the conjugate exponents \u03b1 and \u03b2, where \u03b1\u03b2 > 0. In the context of two densities,"}, {"title": "III. METHODOLOGY", "content": "A. Exploring Multi-Class Classification with Variational Dirichlet Modeling\nIn the field of machine learning, where the representation of compositional data is an integral part of addressing multi-class classification problems, Aitchison [29] introduced the Dirichlet distribution as the primary candidate for modeling such data. Mathematically, within a multi-class classification problem involving K classes, the aim is to determine a function to generate a predicted class label, with the overarching objective of minimizing the disparity between this predicted class label and the ground truth. Generally speaking, in deep learning, it is customary to employ the softmax operator to transform the continuous model output into a set of class probabilities. However, it is worth noting that the softmax operator often leads to overconfidence [5].\nThe Dirichlet distribution, indeed, stands as a versatile and pivotal probability distribution, particularly when it comes to modeling multi-classification problem and Bayesian inference. Its status as the conjugate prior for the multinomial distribution lends it immense utility in Bayesian statistics, as it ensures that the posterior distribution maintains the same form as the prior [30]. This property greatly simplifies the process of Bayesian inference and renders it analytically tractable.\nThe Dirichlet distribution is a versatile tool in probabilistic modeling, offering flexibility, interpretability, and computational advantages, making it suitable for various applications such as Bayesian statistics, natural language processing, and machine learning. Its key advantages include flexibility in modeling categorical data, conjugacy with the multinomial distribution for Bayesian inference, parameter interpretability, smoothing capabilities, and suitability for generative and hierarchical modeling tasks [30]. In multi-view classification, Dirichlet learning offers unique advantages by modeling dependencies between different data views through a stochastic process. It can handle variable-dimensional feature spaces and incorporate prior knowledge effectively, enhancing classification performance and interpretability [30].\nFor instance, the class probabilities, represented as \u03bc = [\u03bc\u2081,\u00b7\u00b7\u00b7, \u03bc\u03ba], can be interpreted as parameters within a multinomial distribution, where \u03a3\u03bc_k=1^K = 1. This distribution characterizes the likelihood of K mutually exclusive events occurring [31]. On the other hand, the Dirichlet distribution can be employed to capture uncertainty and mitigate issues of overconfidence. Given these considerations, our primary goal is to derive a Dirichlet distribution, which serves as the conjugate prior for the multinomial distribution, thereby allowing us to establish a predictive distribution. Since the consideration of the Dirichlet distribution, we commence by presenting the definition of the exponential family, given its association with this distribution.\nB. Multi-View Classification with Uncertainty-Aware Variational Dirichlet Learning\n\"Multi-view classification with uncertainty-aware variational Dirichlet learning\" is an enhanced algorithm based on the trusted multi-view classification (TMC) algorithm. In trusted multi-view classification, the process involves the acquisition of class probabilities from different modalities, followed by the modeling of these class probabilities using a Dirichlet distribution to derive the distribution of classification results. This process yields \u201cevidence\" regarding the reliability of the classification. Subsequently, utilizing this evidence and employing evidence theory, the algorithm computes the confidence and uncertainty associated with the classification results. Finally, the Dempster-Shafer theory, a method for probabilistic reasoning, is utilized to fuse the classification results obtained from various modalities. However, within the TMC algorithm, the interaction between different modalities occurs primarily at the decision-making level, which can potentially limit its performance in specific scenarios.\nTo illustrate, let's consider a smart home system employing the TMC algorithm, which is divided into three views: data collection, processing, and control. If interactions between these views are limited to the control layer, a situation might arise where a user wishes to adjust room temperature using a smartphone application. The absence of a direct mechanism to link the data collection and data processing views can result in delays or operational errors.\nIn response to this challenge, researchers introduced the enhanced trusted multi-view classification algorithm. This enhancement involves the introduction of an additional \"pseudo-view\" to facilitate interactions between different perspectives. The pseudo-view is generated based on the original model and shares similar structural elements and parameters. It serves as an extension or complement to the original model, enabling the inclusion of additional viewpoints or information sources. By incorporating the pseudo-view, new perspectives can be seamlessly integrated into the existing model, enhancing performance through the utilization of multiple viewpoints and information sources. For instance, in natural language processing tasks, the primary view could be a statistically trained language model, while a neural network-based semantic representation is introduced as a pseudo-view. This enables the system to achieve a more comprehensive understanding of textual content, thus enhancing its expressiveness and inferential capabilities. Empirical results demonstrate that the ETMC algorithm outperforms the TMC algorithm on multi-view datasets. Consequently, in our research, we adopt the ETMC algorithm to achieve our objectives.\nC. Uncertainty Analysis\nIn the ETMC algorithm, modality fusion is primarily grounded in subjective logic [32] and Dempster-Shafer's theory [19]. Throughout the training process, it is imperative to conduct a quantitative analysis of the uncertainty and credibility associate with each modality, yielding specific values. Subsequently, a simplified evidence theory is employed to facilitate modal fusion. Furthermore, an assessment of uncertainty and credibility using subjective logic is conducted on the classification results of the fused modalities.\nTo calculate the uncertainty and credibility of individual modalities in the algorithm, a Dirichlet distribution is introduced. This distribution serves as a \"distribution\" for the features extracted by the model's classification layer. Confidence in the classification results and the quantification of uncertainty are computed through the Dirichlet distribution and subjective logic. Based on this data, modalities are selectively fused using evidence theory. Additionally, to obtain a Dirichlet distribution, the algorithm replaces the commonly used softmax layer with a non-negative activation function layer. The specific steps are as follows: for a K-classification task, each sample contains data from V modalities. For modality M\u00b9 = {{$b_k$}$_{k=1,K}$}, the uncertainty of confidence in the corresponding classification result can be calculated using the Dirichlet distribution. For M2 = {{$b_k$}$_{k=1,u2}$}, then employ the simplified evidence theory to calculate the fusion of modality M = M\u00b9 \u2295 M\u00b2. The simplified fusion rules are given by"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct experiments across diverse scenarios to comprehensively evaluate our algorithm. Specifically, we apply our algorithm to a variety of multi-view classification tasks, including RGB-D scene recognition, using four real-world multi-view datasets.\nA. Datasets\na) Classification Datasets: To evaluate the performance of our model on multi-view classification tasks, we utilize the following datasets: 1. SUNRGBD [1]: The SUN RGB-D dataset includes 4,845 training samples, 3,000 testing samples, and 24,869 samples used for combined training and testing across 19 scene categories. 2. NYUDV2 [35]: NYUD2 is an RGB-D dataset with 1,449 image pairs, reorganized into 10 classes, with 795 samples for training and 654 for testing. 3. ADE20K [36]: ADE20K is a semantic segmentation dataset with over 20,000 images across 150+ categories, reorganized into 10 groups, with 795 samples for training and 654 for testing. 4. ScanNet [37]: ScanNet consists of 1,513 indoor scenes with point cloud data, covering 21 object categories, with 1,201 scenes used for training and 312 for testing.\nb) Clustering Datasets: In addition to classification tasks, our model's performance in clustering tasks is evaluated using three multi-view datasets: 1. MSRC-V1 [38]: This image dataset contains eight object classes, each with 30 images. Following [38], we select seven classes: trees, buildings, airplanes, cows, faces, cars, and bicycles. 2. Caltech101-7 [7]: A subset of Caltech101, this dataset includes images from seven selected classes, as identified in previous work [7]. It is commonly used for training and evaluating object recognition algorithms. 3. Caltech101-20 [7]: Another subset of Caltech101, this dataset features images from 20 selected classes based on prior research [7], providing a broader range of objects for testing and refining recognition models.\nB. Evaluation Metrics, Purpose of the Experiment\nIn machine learning, \u201cAccuracy\" is used to assess a model's performance. It is defined as Accuracy = $\\frac{TP+TN}{TP+TN+FP+FN}$, where TP (True Positives) and TN (True Negatives) represent correct classifications, and FP (False Positives) and FN (False Negatives) represent incorrect classifications. Accuracy measures the overall correctness of the model's classifications.\nThe clustering accuracy (CA) [39] is defined as: CA = $\\frac{1}{n}\\sum_{i=1}^{n}\\delta(q_i, map(p_i))$, where \u03b4(a, b) = 1 if a = b,and d(a, b) = 0 otherwise. And map(\u00b7) is the best permutation mapping that matches the predicted clustering labels to the ground truths.\nConsidering practical applications, the objectives of this experiment are threefold: (1). Assess the recognition capability of the exploring uncertainty estimation via H\u00f6lder divergence for multi-view representation learning (HDMVL) algorithm in more intricate and expansive scenarios, comparing the outcomes with previous experiments conducted on smaller datasets. (2). Examine the potential of H\u00f6lder divergence to improve the classification performance of the HDMVL algorithm. Additionally, explore whether fine-tuning H\u00f6lder divergence parameters can enhance the model's performance across diverse datasets. (3). Investigate the impact of uncertainty analysis on refining the classification performance of the model in multi-class classification and clustering tasks that encompass multi-view data.\nC. Data Preprocessing\nMerge and preprocess the samples from the mentioned datasets. In multi-view datasets, images at specific angles typically comprise both color RGB images and depth images. Prior to training, it is necessary to concatenate the image data at specific angles to streamline the classification process.\nD. Model Architecture\nDuring the study, we use three different network architectures. The ResNet-18 [40] pretrained on the ImageNet [41] served as our foundational framework. ResNet-18 is a deep residual neural network comprising 18 layers. The second is the Mamba model [10] that performs well in long sequence modeling tasks. Mamba alleviates the modeling constraints of convolutional neural networks through global field of perception and dynamic weighting, and provides advanced modeling capabilities similar to transformers. The last is vision transformer (ViT) [9], which applies a direct transformer to sequences of image patches. Training is performed on a computer equipped with an Intel(R) Core(TM) i9-11900KF CPU @ 3.50GHz, 64.00 GB RAM, and a 4090Ti GPU. The input image size is standardized to 256 \u00d7 256 and further randomly cropped to 224 \u00d7 224. The Adam [42] optimizer is used to training the neural networks with weight and learning rate decay. In the case of HDMVL, the pseudo-view is generated by directly connecting the output of the three backbone networks, where we fix the H\u00f6lder exponent at 1.7. All experiments are implemented using PyTorch [43].\na) Comparison Experiments for Classification: TrecgNet [44]: Enhances scene recognition models by leveraging both RGB and depth modalities for improved robustness and performance. G-L-SOOR [45]: Focuses on RGB-D scene recognition, emphasizing spatial object-to-object relations in image representations to enhance model effectiveness. CBCL-RGBD [46]: Introduces a centroid-based concept learning approach for RGB-D indoor scene classification. CMPT [47]: Proposes a Cross-Modal Pyramid Translation method for RGB-D scene recognition, aiming to enhance cross-modal feature learning."}, {"title": "V. CONCLUSION", "content": "This study presents an uncertainty-aware variational Dirichlet learning approach to tackle challenges in multi-view representation learning. By incorporating subjective logic, the DS-combination rule, and H\u00f6lder divergence between Dirichlet distributions, the methodology significantly enhances recognition performances across a wide range of multi-modal benchmarks. Extensive experimental results confirm the approach's theoretical soundness and practical robustness, demonstrating improved performance in complex datasets and the effectiveness of H\u00f6lder divergence in uncertainty measurement."}, {"title": "APPENDIX A THE RATIONALE FOR EMPLOYING H\u00d6LDER DIVERGENCE", "content": "HD can be analytically computed for exponential family distributions. Fortunately, based on the analysis above, the Dirichlet distribution also falls under the category of exponential family distributions, ensuring practical training and exhibiting favorable properties. In following section, we provide the analytical expression of HD for two Dirichlet distributions.\nHD is introduced for closed-form optimization, offering a distinct advantage over KLD, which lacks closed-form solutions for several distributions. It provides closed-form expressions of HPD for conic and affine exponential families as follows:"}]}