{"title": "It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act", "authors": ["KRISTOF MEDING"], "abstract": "What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for Al models, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First a high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.), most non-discrimination regulations target only high-risk AI systems. (2.), the regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are often inconsistent and raise questions of computational feasibility. (3.) Regulations for General Purpose AI Models, such as Large Language Models that are not simultaneously classified as high-risk systems, currently lack specificity compared to other regulations. Based on these findings, we recommend developing more specific auditing and testing methodologies for Al systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in Al systems.", "sections": [{"title": "1 INTRODUCTION", "content": "How can we ensure fair algorithms in the context of AI Systems? When regulating algorithms and, more specifically, Artificial Intelligence (AI) systems, this question becomes fundamental. While the question is also crucial in an analog world, it becomes increasingly pressing in the digital world. The European Union (EU) has become one of the forerunners of regulations in the digital age with regulatory frameworks, such as the Digital Service Act, the Digital Markets Act, or the Data Act. In order to maintain fair data processing in the machine learning domain, the European Union (EU) recently adopted the Artificial Intelligence Act (AIA). The aim of this AI regulation is to maintain a level playing field for \"ethical principles\" [72] within and outside the European Union (see recital 27 AIA).\nFair algorithmic processing matters also from a computational perspective. Over the past decade, algorithmic fairness has become a well-established field within machine learning that focuses on defining, mitigating, and evaluating discriminatory behavior of Artificial Intelligence models [76]. The importance becomes clear when looking at several discriminatory behaviors of algorithms in the past, ranging from hiring [25] to Social Welfare systems [45]. In recent years, large generative (multimodal) models, particularly Large Language Models (LLMs) with their high accessibility and wide range of applications have posed significant new challenges to the algorithmic fairness domain [21, 57].\nThe relationship between computer science and legislation for efficient regulations within the AI Act is complex. While the interplay between algorithmic fairness and non-discrimination law is often unclear [90], efficient AI legislation requires interdisciplinary collaboration connecting the legal domain and computer science. Although, legal and technical concerns regarding algorithmic fairness and non-discrimination law have a long history [8, 41, 88], recent developments have accelerated the pace. An increasing number of companies and laypersons are using these technologies, and with"}, {"title": "Previous literature: Many papers on discrimination and AI, limited amount on the interaction of the AI Act and discrimination.", "content": "Discrimination in algorithms is not a novel phenomenon and was analyzed in a pre-AI Act era. Already in the 1980s, algorithmic discrimination was observed in admission settings [23, 92], and gender inequalities in educational software were discussed [52]. In 1996, researchers pointed out the levels at which such technical constraints and social institutions' biases can occur [33]. More recently, the interaction between AI and automated decision-making systems with traditional European and US non-discrimination law has been studied extensively. Barocas and Selbst [8] presented a taxonomy of different sources of discrimination and their impact on humans. The relationship among various European non-discrimination laws has also been studied extensively [41, 87, 88, 90, 95]. For example, the seminal study by Wachter et al. [88] presented a fairness metric that connects algorithmic fairness to the jurisprudence of the European Court of Justice in relation to the General Data Protection Regulation. Similarly, the connection between non-discrimination law and the GDPR has been analyzed to \"unlock the algorithmic black box\u201d [41]. Naturally, since these papers were published before the AI Act was passed, they do not contain references to it.\nRecently, several papers have already addressed the AI Act. Short(er) studies such as [26, 78] have analyzed the relationship between fairness and the AI Act. Additionally, sections within papers of broader scope beyond pure non-discrimination analyses have mentioned aspects of the relationship between algorithmic fairness and the AI Act [44, 73, 86]. The AI Act and its relation to gender equality and non-discrimination law have also been discussed [63]. Most similar to our paper in terms of the AI Act, Bosoer et al. [15] investigated the non-discrimination regulations in the draft of the AI Act without a strong focus on the interaction with computer science. However, their focus was on the draft, whereas our paper focuses on the substantially different final version. Most similar in terms of the interaction between computer science and legal research is the paper of Weerts et al. [90]. In contrast to our work they did not focus on the AI Act."}, {"title": "Our contributions: From regulatory clarity to vagueness", "content": "This paper aims to foster and extend an interdisciplinary view on explicit and implicit fairness regulations in the AI Act regarding algorithmic fairness in the ML domain. We aim to introduce the field of algorithmic fairness to legally oriented researchers in Section 2.1. Afterwards, we introduce non-discrimination law to computer scientists in Section 2.2 and discuss the truncated relationship between the two fields in Section 2.3.\nFor the in-depth analysis of the EU AI Act non-discrimination regulations in Section 3, our contributions are as follows (Figure 1 shows our results and findings in a nutshell):\n(1) We present the history, scope, and intentions of non-discrimination regulations in the AI Act (Sec. 3.1).\n(2) We identify which issues of algorithmic fairness are addressed explicitly and implicitly within the AI Act (Sec. 3.2).\n(3) We find that high-risk AI Systems (Article 6 AIA) are most extensively regulated regarding non-discrimination. Regulations are partly explicit and partly implicit. Overall the regulations are inconsistent with respect to algorithmic fairness, with regulations on input and output data at different stages (Sec. 3.3).\n(4) Non-discrimination regulations of General Purpose AI models (GPAI) in Article 53-56 AIA remain insufficiently specific and lack clear definition (Sec. 3.4)."}, {"title": "2 A PRIMER ON NON-DISCRIMINATION FROM AN AI AND A LEGAL PERSPECTIVE", "content": "Interdisciplinary research on algorithmic fairness is challenging, both from a computer science and a non-discrimination law perspective. Understanding legal reasoning can be challenging without prior legal knowledge, just as understanding algorithmic methods is difficult without a computational background.\nHowever, the complexity in interdisciplinary work extends beyond pure knowledge. The challenges begin with terminology: For instance, in computer science, \"fairness\u201d is a term that can refer to different desiderata that aim to prevent socially or morally undesirable behavior or outcomes of algorithms. Although fairness has been discussed as principle in economic law contexts [79], it is not a specific legal term. The arguably closest legal counterpart term is \"non-discrimination\", which focuses on preventing unfair treatment based on characteristics such as race, gender, or other attributes on legal grounds. The interaction between both key concepts from law and computer science is demanding.\nThus, we highlight the differences and common grounds of algorithmic fairness and non-discrimination regulations in the following sections. After presenting an introduction to algorithmic fairness in Section 2.1 for legal researchers and an introduction to EU non-discrimination law in Section 2.2 - for computer science oriented researchers -, we try to bridge the gap between these two areas in Section 2.3."}, {"title": "2.1 Algorithmic fairness: A gentle introduction", "content": "Algorithmic Fairness has emerged as an established computer science and AI related field over the past years. Algorithmic fairness focuses on uncovering and rectifying disadvantageous treatment of individuals or groups in machine learning models [69]. This treatment must be considered within appropriate social, theoretical, and legal contexts [76] and includes evaluation and auditing methods to test for unfairness.\nOrigins of unfairness in algorithms. The origins of unfairness are manifold. Unfair algorithmic behavior can have different sources. In order to discuss the origins, another not well defined term from computer science is often used: biases. There exist many different bias definitions. Since the EU AI Act does not define it itself, we use a definition from the EU commission from 2021. According to this, a bias can be defined as \"[...] bias describes systematic"}, {"title": "2.2 EU non-discrimination Law: A gentle introduction, too.", "content": "Law is different from computer science. Nevertheless - of course legal researchers have also discussed different concepts of discrimination. In contrast to the term \"fairness\", the term \"(non-)discrimination\" is a specific legal concept"}, {"title": "2.3 Interaction of law and computer science: Differentiating between concepts.", "content": "Law and computer science are different fields. While this becomes clear from the different viewpoints above, it also leads to severe challenges when discussing algorithmic fairness and law: the terminology differs. The difficulty stems from the fact that different fields and society at certain times and places define fairness and discrimination in different ways [82]. While the compatibility of fairness metrics with normative legal statements has already been discussed [26, 50, 87, 88], there is still an inherent difficulty in combining these two fields. This interdisciplinary challenge needs to acknowledge that computer science fairness and legal non-discrimination regulation are moving in the same direction; however, they present different concepts [27]. Although inspired by law, the technical algorithmic fairness debate primarily focuses on detecting, preventing and mitigation unfairness in algorithms[27], while legal non-discrimination law focuses on normative statements.\nAs shown in the previous section, the algorithmic fairness domain uses terms like bias or fairness. These concepts are difficult to grasp and are topics of debate regarding their exact meaning [15, 36, 82, 95]. Furthermore, in legal terms, algorithmic fairness is rarely used.\nWhen we discuss algorithmic fairness and non-discrimination regulation, we are going to differentiate terms. First, (algorithmic) fairness refers to the AI-related domain of analyzing, evaluating, auditing, and mitigating unequal treatment between (groups of) individuals based on normative notions [30, 61]. When we use the terms (non-)discrimination, we will refer to classical non-discrimination law in the European Union."}, {"title": "3 EU AI ACT'S NON-DISCRIMINATION REGULATIONS.", "content": "Emergent technology needs efficient regulation. Before the work on the European AI Act started, the EU High-Level Expert Group on Artificial Intelligence had already developed ethical guidelines for Al systems. These guidelines were later further developed for the AI Act [58]. While the AI Act is not the first Al regulation worldwide [93], it presents harmonized rules and obligations for the use and \"placing on the market\" (Article 1(2) AIA) of AI systems in the European Union. Thus, a level playing field for AI products within the Union is established. The AI Act is (predominantly) a product safety law [4]. In fact, the original drafts did not have any individual rights components [44]. The Members of the European Parliament did not prioritize non-discrimination regulation in the beginning [19].\nRisk is a central concept within the AI Act [43]. Similar to other digital regulations in the EU, the AI Act defines risk in Article 3(2) AIA. A risk \u201cmeans the combination of the probability of an occurrence of harm and the severity of that harm\". Based on this definition, the AI Act differentiates between different risk types with special requirements, which is not uncontroversial [15]: prohibited systems (Article 5 AIA), high-risk systems (Article 6 ff. AIA), certain Al systems"}, {"title": "3.2 Non-discrimination regulations within the Al Act: From clarity to vagueness.", "content": "Analyzing non-discrimination regulations starts with identifying regulations. In total, we scanned the AI Act for non-discrimination-related terms (from specific to unspecific concepts): discrimination, Fundamental Rights, fairness, and bias. We noticed that within the definitions of Article 3 AIA, the serious incident (Article 3(49) AIA) and systemic risk (Article 3(65) AIA) refer to Fundamental Rights. Thus, these were included in our analysis as well.\nWhile analyzing the regulations, we observed that terms appear in the majority of cases in the recitals and only very limited in the articles of the AI Act. The recitals are only considered helpful when interpreting the articles [56] but do not have legal binding force [1]. Therefore, our analysis does not focus on the recitals.\nThis analysis shows, first, that the majority of non-discrimination regulations in the EU AI Act concern the regulation of high-risk systems. GPAI models are only regulated implicitly by the systemic risk and serious incident terms. This will have an impact on our further analysis: we focus on high-risk systems compared to GPAI models.\nSecond, most non-discrimination regulations are only implicitly addressed by the reference to Fundamental Rights. Discrimination and bias as threats are explicitly addressed only for high-risk systems. But here too, we find a relation to Fundamental Rights.\nThird, the term \"fairness\" from algorithmic fairness is not used at all within the AI Act. On the one hand, this result is surprising. The fairness aspect reveals a gap between wording in the computer science and ML community and the legal field. On the other hand, however, as the non-discrimination rules are based on established rules within EU legislative frameworks, it is not a surprising finding."}, {"title": "3.3 High-risk Al non-discrimination regulations.", "content": "What is a high-risk AI system? Article 6 AIA defines different aspects of high risks. In combination with Annex III, high-risk systems are, for example, intended to be used as safety components in the management and operation of critical digital infrastructure (Annex III(2) AIA) or systems intended to be used for the recruitment or selection of natural persons (Annex III(4)(a) AIA). The reasoning behind the specific regulations in Article 6 AIA is that systems which pose a specific risk to fundamental rights or safety need tighter regulation [32] than other less risky systems. It is expected that 5%-15% of all AI systems fall under the category of high-risk systems [22].\nWe identified Articles 9, 10, and 15 AIA as main non-discrimination provisions of high-risk systems. These will be discussed in detail in the following paragraphs 10. All other Articles listed in tab 1 will be discussed and summarized in the last paragraph.\nArticle 9 AIA: Undefined discrimination regulation within risk management systems Article 9 of the AI Act covers risk management systems of High-Risk Systems. According to Article 9(2)(a-d) AIA, risk management systems are designed to identify, evaluate, and mitigate risks of AI systems in a continuous, iterative process throughout the entire lifecycle of a high-risk AI System. All risks after mitigation, called residual risks, need to be judged acceptable [81].\nImportant in the context of non-discrimination law is Article 9(2)(a) AIA, which requires the identification and analysis of known and reasonably foreseeable risks that the high-risk Al system can pose to health, safety, or Fundamental Rights. As explained in Section 2.2, Fundamental Rights also include the right of non-discrimination. Thus, indirectly, Article 9 AIA already requires the identification and analysis of non-discrimination issues. Zuiderveen Borgesius et al. [99] pointed out that this Article mandates the identification and mitigation of discrimination in AI systems. The initial"}, {"title": "Article 10(2)(f) AIA: Main input non-discrimination regulation of high-risk system.", "content": "The core of the non- discrimination regulation for high-risk systems is Article 10 AIA. Article 10(2)(f) AIA requires that training, validation, and testing datasets shall be subject to an \u201cexamination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations\".\nBias is a vague term. The first observation we make is that the input data of an Al system should be subjected to a bias analysis. Most importantly, the term bias is neither defined in the AI Act nor is there a common understanding of what bias means (see also Section 2.1) [84]. It seems that the regulators had a more technical definition of bias in mind, focusing on the diversity of training data in different dimensions compared to social, ethical, or structural biases [44]. This makes it very hard to determine the regulatory content.\nDo the negative effects need to be certain or likely? It is an open question to which condition the wording \"that are likely\" refers to. If the wording only refers to the first condition, then the negative impact on Fundamental Rights"}, {"title": "Article 10(2)(g) AIA: Appropriate bias detection, prevention, and mitigation on input data must include factors beyond technical means.", "content": "Article 10(2)(f) AIA is completed by Article 10(2)(g) AIA. Article 10(2)(g) AIA demands that \"appropriate measures to detect, prevent and mitigate possible biases identified according to point (f)\" are considered. Thus, Article 10 AIA not only mandates the examination of biases that lead to discrimination but also the mitigation of these biases. The bias tests must be documented and disclosed according to Article 11 AIA, along with Annexes IV and IXa AIA [99]. It was argued that with this focus on the input side, the AI Act seeks to remedy the root cause of biases [99].\nInput bias data detection, prevention, and mitigation methods remain largely unexplored. These methods are not clearly defined in the AI Act [27, 86]. Article 10(2)(g) AIA appears to be inspired by the technical literature on fairness metrics for algorithmic outputs. However, compared to output fairness metrics, technical bias detection methods for input data are understudied. Furthermore, as previously noted, the causes of biases in ML are multi-causal [10, 34, 35, 47, 68]. Even when data are collected representatively, biases can occur (see [68] for a comprehensive list). Two examples illustrate this argument:\nSocial bias can occur regardless of sample representativeness [69]. For instance, in loan application settings, if certain groups (such as women) systematically receive lower wages, the data will contain inherent bias even with perfect"}, {"title": "Article 10(5) AIA: A legal basis for the processing of personal data in non-discrimination contexts.", "content": "Finally, Article 10(5) AIA of the AI Act allows the processing of Article 9 GDPR data (see also Recital 70 AIA). Article 9 GDPR protects special categories of personal data, such as genetic, biometric, or health data (Article 9(1) GDPR). There is tension [26] between the need for debiasing AI algorithms and data protection law, which Article 10(5) tries to solve. In order to effectively mitigate biases in Al systems, the processing of personal data (for example, to compute fairness metrics) is important [84]. Notable here is that this exception only applies in the high-risk regime. Thus, for non-high-risk systems, this exception is not applicable. However, it may also be a legal basis for the processing of personal data outside of high-risk systems [42]."}, {"title": "Article 15 (4) AIA: Output bias regulation in case of feedback loops.", "content": "In contrast to Article 10(2)(f) AIA, Article 15(4) AIA mandates that \u201cHigh-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops) and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.\" In this case, the output of a system is regulated, in contrast to the input of an Al system in Article 10 AIA. The reasoning behind this is that AI systems should not become echo chambers for biases [99]. According to Recital 67 AIA, this is a special concern when examining historical biases. Most important to note here is the fact, that this output regulation only applies if the systems continue learning after being placed on the market or put into service.\nAdditionally, it remains unclear if the word \"bias\" here refers to the same bias as in Article 10 AIA. Recital 67 AIA, which relates to Article 10 AIA but also mentions feedback loops, indicates that the same biases are addressed.\nBiased output is usually detected and mitigated with fairness metrics. As shown above, the interpretation of fairness metrics is challenging. Therefore, it becomes very challenging to identify how to \"reduce as far as possible the risk of possibly biased outputs\" in Article 15(4) AIA. This requirement is vague."}, {"title": "Other articles regarding high-risk systems regulations.", "content": "While the previously discussed articles provide at least some intuition about addressed concerns, we want to categorize the remaining articles Most of these paragraphs only concern Fundamental Rights impacts which show limited impact for non-discrimination regulation (see above). Thus, the discussion here is rather short. Article 13(3)(b)(iii) AIA mandates that information on the foreseeable misuse and Fundamental Rights impact is provided. Article 14(2) AIA mandates that human oversight is needed to minimize risks to Fundamental Rights. Article 17(1)(i) AIA and Article 26(5) AIA are related to reporting and information of serious incidents. Serious incidents are linked to discrimination through Article 3(49)(c) AIA: \"'serious incident' means an incident or malfunctioning of an AI system that directly or indirectly leads to any of the following: [...] the infringement of obligations under Union law intended to protect Fundamental Rights\". The link to discrimination is even more indirect since it does not directly concern Fundamental Rights but instead Union law intended to protect Fundamental Rights.\nSummary: High-risk systems show vague non-discrimination regulations. Regulations regarding high-risk systems are vague. In contrast to the ambitions of the AI Act, the concrete non-discrimination regulations lack specificity. Only Article 10 AIA and Article 15 AIA explicitly mention non-discrimination and biases. It would be beneficial if Article 15 AIA, Article 9 AIA, and Article 10 AIA regulated the same types of direct, indirect, and intersectional discrimination on both the input and output sides."}, {"title": "3.4 General Purpose Al non-discrimination regulations", "content": "GPAI models are an important model category. Most (Multimodal) Large Language Models fall under this category. While most of our paper focuses on high-risk regulation in the fairness domain, we briefly discuss these important GPAI models as well.\nIntroduction to GPAI models. Besides high-risk systems, the AI Act regulates so-called General Purpose AI (GPAI) systems in Article 51-56 AIA . According to Article 3(6) AIA, a GPAI system is \u201can AI system which is based on a general-purpose AI model and which has the capability to serve a variety of purposes, both for direct use as well as for integration in other Al systems\".\nGPAI models are thus models with a wide variety of capabilities that are not immediately foreseeable after training and rely on model size [91]. From a technical point of view, the definition includes Large Language Models as well as Large Multimodal Models such as OpenAI's GPT-Model family, Anthropic's Claude models, and Google Gemini [83]. If an AI System has GPAI capabilities as well as high-risk properties, it must adhere to both GPAI and high-risk regulations [72]."}, {"title": "Non-discrimination regulation in GPAI models with systemic risks.", "content": "GPAI models and GPAI models with systemic risk must be differentiated. The AI Act also defines GPAI models with systemic risk in Article 51(1) AIA. A GPAI model with systemic risk is either a model that has high-impact capabilities evaluated on the basis of appropriate technical tools and methodologies, including indicators and benchmarks (Article 51(1)(a) AIA), or any GPAI model whose training compute exceeds $10^{25}$ floating point operations (flops). Especially, the last condition is a topic of debate [39, 49, 73]. Many researchers in ML try to build models with ChatGPT-like abilities but with less compute power. The systemic risk itself is defined in Article 3(65) AIA. A systemic risk \u201cmeans a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain\u201d. The risk must be specific to the high-impact capabilities of the GPAI model. Furthermore, the impact on Fundamental Rights - and thus on non-discrimination regulation - needs to be actual or reasonably foreseeable. Adding the word \"reasonable\" before \"foreseeable\" softens the non-discrimination regulation.\nSummary: GPAI models are only vaguely regulated. In summary, GPAI models are currently only vaguely regulated. Perhaps the Code of Practice being developed by the AI Office and the European Commission can provide guidance in this area."}, {"title": "4 SUMMARY & OUTLOOK: HOW TO PROCEED?", "content": "As Mayson [66] already pointed out: Bias in, bias out. The AI act seek to complement existing non-discrimination EU law with it processed based regulation [99]. However, even on this broader level, the AI Act fails to account for major issues.\nFirst, the AI Act heavily focuses on discrimination because of data. Furthermore, it is missing specificity on detection and mitigation methods. That makes it hard for practitioners to follow the law.\nSecond, the AI Act does not include any regulations for none to limited-risk systems. Third, the current Al Act is casuistic and incomplete we believe that the AI Act would benefit from four refinements. (1.) If the legislators decide to target non-discrimination regulation, non-discrimination should be explicitly mentioned. In contrast to vague terms such as bias, Fundamental Rights, or fairness, non-discrimination is a well-established legal concept within other European legislative frameworks. (2.) The non-discrimination regulation should cover input as well as output parts of the algorithms in one single article, also along the full AI life cycle [27]. (4.) If GPAI models are regulated in the current form, a closer link between GPAI models and downstream applications needs to be established . It is especially important to note that this work is best done in joint community efforts of legal researchers and ML researchers. A bridge between these two fields is highly desirable since regulation should be guided by technical appropriateness and feasibility."}]}