{"title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model", "authors": ["Zeyang Liu", "Xinrui Yang", "Shiguang Sun", "Long Qian", "Lipeng Wan", "Xingyu Chen", "Xuguang Lan"], "abstract": "Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.", "sections": [{"title": "1 Introduction", "content": "Recent progress in generative artificial intelligence with models capable of generating creative content has shown attractive prospects for real-world applications, such as image generation (Takagi & Nishimoto, 2023), embodied agents (Brohan et al., 2023b), and chatbots (K\u00f6pf et al., 2024). Most generative models attempt to directly obtain the answer by training on natural language or image datasets and inserting decomposed reasoning steps in few-shot demonstrations. However, these methods do not experience firsthand the situations described by the language and the image. They cannot find the correct answers through trial and error like humans, which is necessary to ground reasoning on complicated problems and transfer learned knowledge to unfamiliar domains. For example, as shown in Figure 1, when asked a complex multi-agent decision problem, one of the most widely-used large language models, GPT4 - though achieving superhuman performance in many reasoning tasks - will generate sketchy and misleading answers."}, {"title": "2 Methodology", "content": "We formulate an interaction simulator as a transition prediction model that, given some state of the world and descriptions of the task, can take some actions as input and produce the consequence of the actions in the form of images, states, and rewards. In this paper, we consider building such simulators for a multi-agent decision-making environment named StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019), known for its rich environments and high control complexity. See Appendix C for more information about SMAC."}, {"title": "2.1 VisionSMAC", "content": "The SMAC benchmark saves a replay of an episode as a SC2REPLAY file rather than providing the image feature during exploration. It is computationally expensive to construct datasets of images by watching such replay within the StarCraft II client and then subsampling a frame that captures meaningful actions. To solve this problem, we introduce VisionSMAC to convert the state into images and languages through a parser f, decoupled from StarCraft, making it easy to create new content. First, we collect offline datasets across ten training maps in the SMAC benchmark by running multi-agent exploration methods named EMC (Zheng et al., 2021) and IIE (Liu et al., 2024). Each dataset contains a large number of interaction trajectories: $ \\tau := \\{s_t, \\{o_t^i\\}_{i=1}^n, \\{u_t^i\\}_{i=1}^n, d_t \\}_{t=0}^T$, where $s_t$ denotes the state, $\\{o_t^i\\}_{i=1}^n$ is the observation of each agent, $\\{u_t^i\\}_{i=1}^n$ is the joint action, and the done signal d = 1 when the agent i is killed, t is the timestep, n and T denote the number of agents and the length of the episode, respectively. We further collect the element images that appear in the game and affect the state, such as the units and the background terrain of training maps.\nWe construct the paired state-image dataset by placing each unit image and its health bar at their positions with the corresponding background terrain. This reconstructed image can closely resemble a specific state in the original replay. We also perform data augmentation to enable better feature abstraction by changing the background to different terrains."}, {"title": "2.2 Training An Interactive Simulator", "content": "With trajectories with corresponding images and language guidance from different scenarios, we can formulate the interactions with StarCraft II as interacting with an interactive simulator. The simulator contains three key components: (1) an image tokenizer that converts raw video frame into discrete tokens, (2) a dynamics model that predicts the next frame and state given past frame and state tokens, (3) a reward model that infers the reward of a state-action pair given a trajectory. The idea behind decomposing the world model into a dynamics model and a reward model is to reuse the dynamics model for different tasks by combining it with any reward function."}, {"title": "Image Tokenizer", "content": "We compress images into discrete tokens to reduce dimensionality and enable higher-quality image generation. We make use of vector quantized variational autoencoder (VQ-VAE) (Van Den Oord et al., 2017), which takes every single image $I \\in \\mathbb{R}^{H \\times W \\times C}$ of the state as input, generating discrete representations $z \\in [B]^n$, where B is the size of the discrete latent space. The tokenizer is trained using a standard VQ-VAE objective."}, {"title": "Dynamics Model", "content": "The dynamics model is a causal transformer q parameterized by $\\psi$, where the target sequence has the following form $x = \\{..., L_t, z_t, s_t, o_t^1, ..., o_t^n, u_t^1, ..., u_t^n, z_{t+1}, s_{t+1}, ...\\}$, where $z_t$ is the image representation generated by the fixed image tokenizer. We utilize the task description $L(s_t)$ to specify the dynamics of the environment, remaining consistent in one sequence. An embedding for each timestep is learned and added to each token. The dynamics model processes all past tokens and predicts future tokens via autoregressive modeling.\nThen, we use the prediction heads to decode the predicted tokens to the corresponding element in the sequence and train them by minimizing the cross-entropy loss for actions and mean-squared error for others. The actions would serve as the reference policy for the learning with the simulated trajectories described in Section 2.3. In particular, we use a dynamics residual term to improve the accuracy and the stability of the generation by changing the target from $s_{t+1}$ to $\\Delta s_{t+1} = s_{t+1} - s_t$ for the state prediction head. We also apply this term to predict image representations. In addition, since the observation is only related to the current state and the vision range of the agents, we filter out the historical memories and use $s_t$ as the input for the observation prediction."}, {"title": "Reward Model", "content": "We resemble the training pipeline of inverse reinforcement learning - maximizing the likelihood of trajectories in the expert demonstrations while minimizing the likelihood of trajecto-"}, {"title": "2.3 Inference: Learning Policy in the Simulator", "content": "We now describe how to generate grounded answers for multi-agent decision-making problems via LBI. Given an image of the initial state and a task description from the user, the agents using a randomly initialized off-policy MARL algorithm (e.g., independent Q-learning) interact with the dynamics model to collect reward-free trajectories in an autoregressive manner. Then, the reward model predicts the immediate reward for each transition pair in the simulation trajectories. These relabeled trajectories are added to the replay buffer, serving as the training data for the policy network.\nIn practice, we construct the MARL problem in the simulator as a behavior-regularized MDP by imposing a behavior-regularization term:\n$max_{\\eta} \\mathbb{E}_{\\tau\\sim\\pi_0} [\\sum_{t=0}^T \\gamma^t (\\sum_{i=1}^N r_i(s_t, u_i) - \\alpha log \\frac{\\pi_i(u_t^i; \\eta)}{q(u_i | x_{< u_i^i}; \\psi)}})]$,\nwhere $\\Pi = \\{\\pi^i(u^i | s; \\eta)\\}_{i=1}^N$ is the joint policy, and $q(u^j | x_{< u^i}; \\psi)$ is the reference policy provided by the dynamics model. The last term will transfer the greedy max from the joint policy to a softened max over the reference policy, enabling in-sample learning and further mitigating the impact of exploring OOD regions in the state-action space.\nSince it is possible for specific agents to become inactive before the game terminates, we mark the terminated timestep for each agent and enemy once its predicted health is less than zero and then use zero vectors as the subsequent actions and observations. It can mitigate the hallucinating unrealistic outcomes - a dead agent performs a \u201cmoving\" action. We also mask the predicted reward after the terminated timestep for the inactive agent to get a more accurate value estimate."}, {"title": "3 Related Work", "content": "This section briefly introduces the recent work of learning world models and imitation learning. See Appendix F for more related work."}, {"title": "3.1 World Models", "content": "There is a long-standing history of learning predictive models of the world. We list three categories of model-based reinforcement learning (MBRL) according to the type of model usage, including planning, analytic gradient generation, and data augmentation.\nThe first category applies planning methods with world model simulation. AlphaGo (Silver et al., 2016) and MuZero (Schrittwieser et al., 2020) learn a transition model and apply Monte Carlo Tree Search to search for an action sequence with the highest accumulated rewards. By contrast,"}, {"title": "3.2 Imitation Learning", "content": "Imitation Learning (Bain & Sammut, 1995) formulates imitating an expert as a supervised learning problem, which has been widely adopted in various domains due to its simplicity and effective-ness (Silver et al., 2016; Swamy et al., 2020). GAIL (Ho & Ermon, 2016) and its extensions (Song et al., 2018; Ghasemipour et al., 2020) stand as a cornerstone approach, which trains a generator policy to imitate expert behaviors and a discriminator to distinguish between the expert and the learner's state-action pair distributions. In light of the recent interest in foundational models, the conditional diffusion model is used to represent and learn an imitation learning policy, which produces a predicted action conditioning on a state and a sampled noise vector Pearce et al. (2022); Chi et al. (2023). These methods achieve encouraging results in modeling stochastic and multimodal behaviors from human experts or play data. DT-style methods (Chen et al., 2021; Wu et al., 2024) formulate the trajectory generation as a sequence modeling problem, which generates states, actions, and rewards by conditioning on a return-to-go token in an autoregressive manner.\nIn contrast, inverse reinforcement learning (IRL) is designed to infer the reward function that underlies the expert demonstrations, taking into account the temporal structure and showing better generalization than direct Behavioral Cloning (Ng & Russell, 2000; Ross et al., 2011; Barde et al., 2020). A main class of algorithms, Maximum entropy (MaxEnt) IRL (Haarnoja et al., 2017) and its extensions (Liu et al., 2021; Rolland et al., 2022), learn a stationary reward by minimizing divergence between the agent and expert distribution. Since the learned reward function can solve downstream tasks and transfer behavior across different dynamics, IRL is also helpful in several broader applications, e.g., IRL with natural language goals (Fu et al., 2018a; Zhou & Small, 2021; Xu et al., 2022), and RL with human feedback (Ziegler et al., 2019; Zhu et al., 2023; Wu et al., 2023), and dynamics learning (Luo et al., 2023). Furthermore, a series of sample-efficient algorithms are proposed to solve the MaxEnt IRL formulation (Fu et al., 2018b; Zeng et al., 2022, 2024). To side-step the expensive online environmental interactions in classic IRL, some work aims to learn a reward function from a static dataset by a variational Bayesian framework (Chan & van der Schaar, 2021), representing reward function via a learned soft Q-function (Garg et al., 2021), or incorporating conservatism into the estimated reward like offline Q-learning (Yue et al., 2022). The"}, {"title": "4 Experiments", "content": "In this section, we conduct empirical experiments to answer the following questions: (1) Is Learning before Interaction (LBI) better than the existing multi-agent reinforcement learning (MARL) methods in complex cooperative scenarios? (2) Can LBI generate long-horizon trajectories and reasonable reward functions at critical states? (3) Does LBI have the zero-shot ability to generalize to unseen tasks? Then, we investigate the contribution of each component in the dynamics and the reward model. We provide the information of training datasets and experimental settings in Appendix B and D. We also discuss this paper's broader impacts and limitations in Appendix A.1 and A.2."}, {"title": "4.1 Performance Comparison", "content": "Reward-free Offline Learning We compare LBI with the following imitation learning baselines: (1) BC: behavior cloning that imitates the whole datasets, (2) MA-AIRL (Yu et al., 2019): using adversarial learning to perform policy imitation, (3) MADT (Meng et al., 2023): utilizing the Decision Transformer (Chen et al., 2021) to perform sequence modeling, (4) MA-TREX: infering the reward according to ranked demonstrations, the multi-agent version of TREX (Brown et al., 2019), (5) MAPT (Zhu et al., 2024): infering the team rewards according to the preference return from a well-trained scripted teacher.\nAs shown in Table 1, LBI outperforms the baselines by a significant margin on various maps with different difficulty levels, indicating the importance and effectiveness of learning reward functions via the proposed world model. In contrast, BC and MA-AIRL fail to achieve success rates in most tasks because they imitate all past interaction sequences and cannot generalize and avoid sub-optimal solutions. MA-TREX and MAPT have plateaued in performance because they use the accumulated rewards and the preference deduced by the scripted teacher to specify the quality of the training data, respectively. MADT performs better than other baselines because Decision Transformer can be thought of as performing imitation learning on a subset of the data with a certain return.\nOffline MARL We also compare LBI with the existing offline MARL methods with ground-truth rewards from the StarCraft Multi-Agent Challenge (SMAC), including the multi-agent version of"}, {"title": "4.2 Generalization on Unseen Tasks", "content": "Since zero-shot generalization ability is crucial for generating grounded answers for multi-agent decision-making problems, we also test LBI's ability to generalize to extensive unseen scenarios without retraining. Specifically, we evaluate our LBI and MADT on the ten unseen testing maps, varying agent numbers, action spaces, and levels of environment complexity. Table 3 shows that LBI consistently outperforms MADT in unseen scenarios by a large margin, successfully transferring knowledge to new tasks without requiring additional fine-tuning. It highlights that learning a reward function has better zero-shot generalization performance than simple policy adaptation."}, {"title": "4.3 Visualization", "content": "This section evaluates the dynamics model as a long-horizon policy-conditioned predictive model. Figure 4 showcases examples of length-40 image trajectories generated by the dynamics model, including MMM2, 3s_vs_5z, and 5m_vs_6m. We do not observe conspicuous compounding errors as the single-step prediction model does, highlighting that LBI has consistency and long-horizon generation ability. In the case of 5m_vs_6m, we present the following frames after taking one of the possible actions, showing that LBI can also perform action-controllable generation.\nWe also investigate the reward prediction at a critical junction in the state-action space that can transit to various states and significantly influence the success rate on the 5m_vs_6m task. At the moment,"}, {"title": "4.4 Ablation Study", "content": "In this section, we conduct ablation studies to analyze the contributions of each component in the dynamics model and the reward model across five evaluation runs on four training maps (6h_vs_8z, 3s5z_vs_3s6z, corridor, and MMM2) and four unseen maps (3s5z_vs_3s7z, 1c3s7z, 3s4z, 1c_vs_32zg). We show the results of the dynamics model in Table 4. Using the dynamics residual term is necessary to reduce the prediction error of the subsequent states and obtain good performance across all training and unseen tasks. The image reference is not so effective, even if we use ground-truth images as the reference. However, since images are more powerful in representing some situations than language or state information, we believe that the image serves as another modality to correct the prediction of the state. We would leave it for future work.\nWe demonstrate the ablation results of the reward model in Table 5. Compared with LBI-wo-RC&BR, the reward constraint and behavior regularization term can improve the overall performance on the training tasks. However, LBI-wo-BR performs better than LBI-wo-RC on unseen tasks, suggesting that the conservatism for reward is more important than the policy when OOD state-action pairs exist. The poor performance of LBI-w-GTR indicates that learning rewards from conditioned demonstrations may be more accessible and valuable for policy updates than reconstructing the pre-defined rewards by human experts."}, {"title": "5 Conclusion and Future Work", "content": "We proposed Learning before Interaction (LBI), a novel paradigm that enables generative models to ground their answers for multi-agent decision-making problems with simulations between the world and the multi-agent system. We formulate an interactive simulator consisting of dynamics and reward models, given some states of the world and the task descriptions, generating the consequence of the actions in the form of images, states, and rewards. We hope the idea of including simulations in the reasoning will instigate broad interest in applying generative models to aid machine intelligence and decision-making."}, {"title": "A Broader Impacts and Limitations", "content": ""}, {"title": "A.1 Broader Impacts", "content": "Learning before Interaction provides grounded answers to complex multi-agent decision-making problems through the generation of simulators and trial-and-error learning. This can benefit those seeking to make decisions through long-term planning. With significant technological advancements, exploring the use of this technology may be crucial for enhancing existing human decision-making capabilities. For instance, negotiators could describe the opponent's personality traits and their decision-making limits to generate better negotiation strategies.\nAt the same time, we recognize that current generative simulators still cannot reliably generate state transitions across multiple domains, and learning joint multi-agent strategies still faces convergence difficulties. Therefore, Learning before Interaction may lead to incorrect decisions in specific fields. If humans intentionally follow the generated answers instead of using them as references, it could lead to unsafe or worse consequences. On the other hand, it could also have negative impacts when Learning before Interaction is misused in harmful applications if the generated environments and answers are sufficiently accurate."}, {"title": "A.2 Limitations", "content": "Although we have already seen significant improvements in reasoning capabilities for complex multi-agent tasks with Learning before Interaction, performance may be affected by the simulator's accuracy and the multi-agent policy learning performance. Unqualified simulators and difficult-to-converge multi-agent policies may lead to erroneous simulation results, which could be more misleading than the vague answers generated by existing visual language models. For example, the world model has limited out-of-domain generalization for domains that are not represented in the training data, e.g., unseen unit types. Further scaling up training data could help, as the parser can quickly and automatically generate images based on a given state.\nWhile the learned reward functions can enhance the speed of multi-agent policy learning compared to other inverse reinforcement learning and online interaction learning methods, it still requires considerable waiting time to obtain a converged policy and the final answer. Such long waiting time is unacceptable in applications requiring real-time feedback, such as chatbots. One possible solution is to replace multi-agent reinforcement learning with planning methods based on the learned rewards and dynamics models, thereby accelerating the reasoning process. We will leave this issue in future work.\nIn addition, this paper is confined to scenarios within the game StarCraft II. This is an environment that, while complex, cannot represent the dynamics of all multi-agent tasks. Evaluation of multi-agent reinforcement learning algorithms, therefore, should not be limited to one benchmark but should target a variety with a range of tasks."}, {"title": "B Dataset Preparation", "content": "The training maps include 3s5z, 1c3s5z, 10m_vs_11m, 2c_vs_64zg, 3s_vs_5z, 5m_vs_6m, 6h_vs_8z, 3s5z_vs_3s6z, corridor, MMM2 in StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019). We use EMC (Zheng et al., 2021) and IIE (Liu et al., 2024) to collect 50000 trajectories for each map and save these data as NPY files. The data includes the states, the observations, the terminated signals, the actions, the available actions, and the rewards. The return distribution on training maps is shown in Table 6. The average return is 19.64 \u00b1 1.63 across ten training maps.\nIn Figure 6, we have presented the whole procedure of converting a state vector into an image for simulation and parsing a trajectory to produce a textual task description. First, as shown in Figure 5, we collect the element images that appear in the game and affect the state, including units and background terrains of training maps.\nGiven a multi-agent system and its interaction trajectory, the parser reads predefined map information, such as the number and races of agents and enemies. Then, the parser converts the original state information into structured information, reading agents' and enemies' positions and health points. It"}, {"title": "C StarCraft Multi-agent Challenge", "content": "StarCraft II is a real-time strategy game featuring three different races, Protoss, Terran, and Zerg, with different properties and associated strategies. The objective is to build an army powerful enough to destroy the enemy's base. When battling two armies, players must ensure army units are acting optimally. StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) is a partially observable reinforcement learning benchmark built in StarCraft II. An individual agent with parameter sharing controls each allied unit, and a hand-coded built-in StarCraft II AI controls enemy units. The difficulty of the game Al is set to the \u201cvery difficult\u201d level.\nOn the SMAC benchmark, agents can access their local observations within the field of view at each time step. The feature vector contains attributes of both allied and enemy units: distance, relative x, relative y, health, shield, and unit_type. In addition, agents can observe the last actions of allied units and the terrain features surrounding them. The global state vector includes the coordinates of all agents relative to the center of the map and other features present in the local observation of agents. The state stores the energy of Medivacs, the cooldown of the rest of the allied units, and the last actions of all agents. Note that the global state information is only available to agents during centralized training. All features in state and local observations are normalized by their maximum values. After receiving the observations, each agent is allowed to take action from a discrete set which consists of move [direction], attack[enemy_id], stop and no-op."}, {"title": "D Experiment Setting", "content": "In this section, we describe the ground-truth environment that agents interact, the implementa-tion details of online learning methods, offline learning methods, and our model Learning before Interaction."}, {"title": "D.1 Online Learning", "content": "We adopt the same architectures for QMIX\u00b9, QPLEX\u00b9, CW-QMIX2, RODE3, MAVEN4, EMC5 as their official implementations (Samvelyan et al., 2019; Wang et al., 2020a; Rashid et al., 2020; Wang et al., 2020c; Mahajan et al., 2019; Zheng et al., 2021). Each agent independently learns a policy with fully shared parameters between all policies. We used RMSProp with a learning rate of 5e-4 and"}, {"title": "D.2 Offline Learning", "content": "We adopt the same architectures for MA-AIRL7, MADT8, MAPT\u00ba, ICQ10, OMAR11, and OMIGA12 as their official implementations (Yu et al., 2019; Meng et al., 2023; Zhu et al., 2024; Fujimoto et al., 2019; Kumar et al., 2020; Yang et al., 2021; Pan et al., 2022; Wang et al., 2024). We implement MA-TREX, BCQ-MA and CQL-MA based on TREX (Brown et al., 2019), BCQ (Fujimoto et al., 2019), and CQL (Kumar et al., 2020), respectively. In particular, we add the task description into MADT's target sequence because it deprecates the reward-to-go term."}, {"title": "D.3 Learning before Interaction", "content": "We train our image tokenizer for 100k steps using the AdamW optimizer, with cosine decay, using the hyperparameters in Table 8. The batch size is 32, and the learning rate is 1e-4.\nWe build our dynamics model implementation based on Decision Transformer13 (Chen et al., 2021). The complete list of hyperparameters can be found in Table 9. The dynamics models were trained using the AdamW optimizer.\nThe reward shares the same architecture as the dynamics model, but the attention mask in the transformer model is modified in order to receive the whole trajectory as input rather than the tokens that have come before the current one. Here are some tricks for reward learning: (1) we control the gap between the rewards of the expert behavior and the policy action we stop the gradient for the reward of the expert behavior at a given state if it is greater than the one of the policy action, where beta is the margin and set to 2; (2) we also set the target of unavailable actions' rewards to 0; (3) we alternate between k-step of policy update and reward update to avoid completely solving the policy optimization subproblem before updating the reward parameters, where k = 5."}, {"title": "E Additional Results", "content": ""}, {"title": "E.1 Additional Visualization Results", "content": "Figure 7 shows the qualitative comparison between the target and the generated sequences. Both trajectories are collected by running the same policy. We can see that the generated sequence can resemble the target one in most frames, but some differences exist in positions and health bars. However, compounding errors in the single-step model, which lead to physically implausible predictions, are not observed in the dynamics model generated by the causal transformer. For example, at the timestep of 10 in the MMM2 scenario, the generated frame does not contain the ally's Medivac, but we can see it in the following frames."}, {"title": "E.2 Comparisons with Online Learning Methods", "content": "A Text-to-Code Converter can generate scenarios using the original game engine and then learn the joint policy. Consequently, we also consider comparing this approach with online MARL methods, including CW-QMIX (Rashid et al., 2020), QPLEX (Wang et al., 2020a), MAVEN (Mahajan et al., 2019), EMC (Zheng et al., 2021), RODE (Wang et al., 2020c), QMIX (Rashid et al., 2018), and"}, {"title": "F Background and Additional Related Work", "content": ""}, {"title": "F.1 Decentralized Partially Observable Markov Decision Process.", "content": "A fully cooperative multi-agent task in the partially observable setting can be formulated as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) Oliehoek & Amato (2016), consisting of a tuple $G = (A,S,\\Omega,O,U, P, r, \\gamma)$, where $a \\in A = \\{1, ..., n\\}$ is a set of agents, S is a set of states, and $\\Omega$ is a set of joint observations. At each time step, each agent obtains its observation $o_a \\in \\Omega$ based on the observation function $O(s, a) : S \\times A \\rightarrow \\Omega$, and an action-observation history $\\tau_a \\in T = (\\Omega \\times U)^*$. Each agent a chooses an action $u_a \\in U$ by a stochastic policy $\\pi_a (U_a | \\tau_a) : T \\times U \\rightarrow [0, 1]$, which forms a joint action $u \\in U$. It results in a joint reward r(s, u) and a transit to the next state $s' \\sim P(\\cdot | s, u)$. The formal objective function is to find the joint policy $\\pi$ that maximizes a joint action-value function $Q^{\\pi} (s_t, u_t) = r(s_t, u_t) + \\gamma \\mathbb{E}_{s'} [V^{\\pi}(s')]$, where $V^{\\pi} (s) = \\mathbb{E} [\\sum \\gamma^t r_t | s_0 = s, \\pi]$, and $\\gamma \\in [0, 1)$ is a discounted factor."}, {"title": "F.2 Inverse Reinforcement Learning", "content": "Suppose we do not have access to the ground truth reward function but have demonstrations D provided by an expert policy $\\pi_E$, where D is a set of M trajectories $\\{\\tau^i\\}_{i=1}^M = \\{\\{(s_t^i, u_t^i)\\}_{t=1}^{T_i}\\}_{i=1}^M$ collected by sampling $s^i \\sim \\eta(s)$, $u_t^i \\sim \\pi_E(u_t | s_t)$, $s_{t+1} \\sim P(s_{t+1} | s_t, u_t)$. Given D, imitation learning aims to directly learn policies that behave similarly to these demonstrations, whereas inverse reinforcement learning (IRL) seeks to infer the underlying reward functions which induce the expert policies. The MaxEnt IRL framework aims to recover a reward function that retionalizes the expert behaviors with the least commitment, denoted as $IRL(\\pi_E)$:\n$IRL(\\pi_E) = arg \\max_{r \\in R} \\mathbb{E}_{\\pi_E} [r(s, u)] - RL(r)$\n$RL(r) = \\max_{\\pi \\in \\Pi} H(\\pi) + \\mathbb{E}_{\\pi} [r(s, u)]$,\nwhere $H(\\pi) = \\mathbb{E}_{\\pi} [-\\log \\pi(u | s)]$ is the policy entropy. It looks for a reward function that assigns high reward to the expert policy and a low reward to other policies, while searching for the best policy for the reward function in an inner loop."}, {"title": "F.3 Additional Related Work", "content": "Offline Q-Learning Offline Q-learning learns a policy from a fixed dataset where the reward is provided for each transition sample. Most off-policy reinforcement learning (RL) algorithms are applicable in offline Q-learning. However, they typically suffer from the overestimation problem of out-of-distribution (OOD) actions due to the distribution shift between the action distribution in the training dataset and that induced by the learned policy (Fujimoto et al., 2019). Several constraint methods are proposed to restrict the learned policy from producing OOD actions by leveraging importance sampling (Sutton et al., 2016; Nachum et al., 2019), incorporating explicit policy constraints (Kostrikov et al., 2021; Fakoor et al., 2021; Fujimoto & Gu, 2021; Tarasov et al., 2024), penalizing value estimates (Kumar et al., 2020; An et al., 2021; Shao et al., 2024), and uncertainty quantification (Wu et al., 2021; Zanette et al., 2021). Another branch resorts to learning without querying OOD actions and thus constrain the learning process within the support of the dataset (Bai et al., 2021; Lyu et al., 2022).\nTransformer Model Several works have explored the integration of transformer models into reinforcement learning (RL) settings. We classify them into two major categories depending on the usage pattern. The first category focuses on representing components in RL algorithms, such as policies and value functions (Parisotto et al., 2020; Parisotto & Salakhutdinov, 2021). These methods rely on standard RL algorithms to update policy, where the transformer only provides a large representation capacity and improves feature extraction. Conversely, the second category aims to replace the RL pipeline with sequence modeling. They autoregressively generate states, actions, and rewards by conditioning on the desired return-to-go during inference (Chen et al., 2021; Lee et al.,"}, {"title": "Multi-agent Reinforcement Learning", "content": "This section briefly introduces recent related work on cooperative multi-agent reinforcement learning (MARL). In the paradigm of centralized training with decentralized execution (CTDE), agents' policies are trained with access to global information in a centralized way and executed only based on local histories in a decentralized way (Oliehoek et al., 2008; Kraemer & Banerjee, 2016). One of the most significant challenges in CT"}]}