{"title": "SUPERMERGE: AN APPROACH FOR GRADIENT-BASED MODEL MERGING", "authors": ["Haoyu Yang", "Zheng Zhang", "Saket Sathe"], "abstract": "Large language models, such as ChatGPT (Achiam et al., 2023), Claude (Anthropic, 2024), or LLaMA (Touvron et al., 2023), are gigantic, monolithic, and possess the superpower to simultaneously support thousands of tasks. However, high-throughput applications often prefer smaller task-specific models because of their lower latency and cost. One challenge of using task-specific models is the incremental need for solving newer tasks after the model is already deployed for existing tasks. A straightforward solution requires fine-tuning the model again for both existing and new tasks, which is computationally expensive and time-consuming. To address this issue, we propose a model merging based approach called SUPERMERGE. SUPERMERGE is a gradient-based method to systematically merge several fine-tuned models trained on existing and new tasks. SUPERMERGE is designed to be lightweight and fast, and the merged model achieves similar performance to fully fine-tuned models on all tasks. Furthermore, we proposed a hierarchical model merging strategy to reduce the peak space requirement without sacrificing the performance of the merged model. We experimentally demonstrate that SUPERMERGE outperforms existing model merging methods (Ilharco et al., 2022; Yu et al., 2024; Yadav et al., 2024; Yang et al., 2024) on common natural language processing and computer vision tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid development of foundational models has led to numerous challenges and opportunities. Majority of the foundational models are generative, wherein they are capable of \"generating\" the output given an input. The input and output both can be free-form text, images, or multi-modal. This is a paradigm shift from the earlier predictive models that had rigid and task-specific inputs and outputs and were unable to generate free-form text or images. Generative capabilities enable foundational models to learn several tasks simultaneously and generalize to tasks unseen during training. This has lead to research in the topics of multi-task learning for several sub-disciplines of machine learning, such as computer vision (Chen et al., 2018; Misra et al., 2016), natural language processing (Collobert & Weston, 2008; Dong et al., 2015), and recommendation systems (Song et al., 2024; Ma et al., 2018).\nIn this context, it has become common to train large foundational models on thousands of tasks that could be performed with modest accuracy. During training these models gain the ability to generate a response based on a vast repository of public data and commonsense knowledge. However, there are situations where the model is not adequately trained on organization-specific tasks, whose data is not available in the public domain. In such cases, fine-tuning these models to perform a specific task (or subset of tasks) is often required. For example, a general model could be outstanding at summarizing text, but could be mediocre at summarizing organization-specific technical jargon.\nFine-Tuning. There are a few major classes of fine-tuning approaches. The first approach is called end-to-end fine-tuning. In this approach all existing model parameters are adjusted to minimize a loss function using mini-batch gradient descent and back-propagation over a fine-tuning data set. For tasks like classification, a new classification head is added to the model, such that it learns to classify on the fine-tuning data set. The parameters in the classification head are initialized randomly. However, for fine-tuning larger models the compute needed for end-to-end fine-tuning becomes"}, {"title": "2 RELATED WORK", "content": "LLM Fine-Tuning Techniques. The core idea of fine-tuning is to extend the knowledge acquired from pre-training and adapt it to a specific target domain through additional training using a task-specific data set. In the natural language processing domain, instruction fine-tuning (Wei et al., 2022; Chung et al., 2024) serves as a widely adopted approach to enhance the model's ability to comprehend and accurately execute desired tasks. To enhance fine-tuning efficiency, parameter-efficient fine-tuning (PEFT) methods have been proposed. PEFT methods typically introduce lightweight task-specific adaptations to the pre-trained model. One approach is to add adapters (Houlsby et al., 2019) as sub-modules to the pre-trained model, which enables efficient learning of new knowledge. Low-Rank Adaptation(LoRA) (Hu et al., 2022) factorizes each weight matrix into a low-rank de-composition that has minimal number of tunable parameters. Infused Adapter by Inhibiting and Amplifying Inner Activations (IA\u00b3) (Liu et al., 2022) proposes a method of learning a subset of additional parameters to re-scale inner activations in the attention and feed-forward modules of transformer-based language models. These methods aim to achieve comparable performance to traditional full fine-tuning, while achieving significant reduction of tunable parameters.\nMulti-Task Learning via Model Merging. Model merging methods aim to combine two or more fine-tuned models into a single model. Most existing methods focus on merging models that are derived from the same base architecture and initialization. The most intuitive approach is calculating a smooth average of the model parameters across different tasks, such as Task Arithmetic (Ilharco et al., 2022) and Fisher-Weighted averaging (Matena & Raffel, 2024). TIES-Merging (Yadav et al., 2024) takes a rule-based approach to resolve sign conflicts and merges with least feature redundancy. DARE (Yu et al., 2024) performs sparsification on each fine-tuned model by randomly dropping and ensembles them with rescaling. AdaMerging (Yang et al., 2024) is the most relevant related work to SUPERMERGE. Unlike SUPERMERGE, AdaMerging is an unsupervised approach that learns the layer contributions using an entropy-based loss function. This also restricts the ability of AdaMerging to only merging predictive models (classification and regression), while SUPERMERGE can merge predictive and generative models. Another direction explores merging models with different initializations. The Git Re-Basin (Ainsworth et al., 2023) method permutes the units of one model to match them with the reference model, enabling the merging of two models in the aligned space. Optimal Transport Model Fusion (Singh & Jaggi, 2020) (or OT Fusion) leverages the Wasserstein barycenter to align weights or activations across different models, performing optimal transport computation in each layer to enable model fusion. Zipit (Stoica et al., 2023) merges models layer-by-layer by using a manually constructed computational graph. It uses this graph to examine the latent feature connections and redundancy that is identified through the similarity of"}, {"title": "3 NOTATION", "content": "The models that we consider for merging are deep learning models composed of numerous layers. Particularly, we assume that we have k fine-tuned models with identical structure each having n layers. We also assume that all the parameters of model i are materialized into a single vector. We denote such a vector by 0(i), where i = (1...k). At times, we need to refer to a particular layer j = (1... n) of a fine-tuned model, which is denoted as of (i, j). Next, we denote by Op the weights of the pre-trained model and Om denotes the weight of the final merged multi-task model. The fine-tuned and merged models have identical architecture as the pre-trained model.\nA particular layer j for pre-trained and merged model is denoted as \u0472p (j) and 0m (j) respectively\u00b9. We also assume that each fine-tuned model uses its own training, validation and test data sets. These data sets are denoted as Dtrain . . . Dtrain, Otrain, Dual... Dual, and Dtest ... Dtest respectively. We use Dtrain, to indicate the union of all the training data sets. Similarly, Dual and Dtest indicate the union of all the validation and test data sets. We assume that a fine-tuned model's parameters of (i) have been already tuned using their corresponding training data set Dtrain."}, {"title": "4 BACKGROUND AND MOTIVATION", "content": "A majority of recent model merging approaches that follow the \"identical architecture, identical initialization\" merging style claim to be training-free\u00b2. In this category, there are three important and well-known model merging techniques relevant to SUPERMERGE: (1) Task-Arithmetic (Ilharco et al., 2022), 2) TIES (Yadav et al., 2024), and 3) DARE (Yu et al., 2024). Let us understand these methods with a toy setup of merging only two models, i.e. k = 2. The Task-Arithmetic approach begins by computing task vectors that are defined as,\n$\\tau(1) = OF (1) - OP$ and $\\tau(2) = Of (2) - \u13be\ud835\udc43$.\n(1)\nThe weights of the merged multitask model are given as,\n$\\\u04e9\u0442 = 0 + (\\\u03c4(1) + \\\u03c4(2))$.\n(2)\nIn the TIES approach, the final weights are still computed similar to Task Arithmetic, but the task vectors (1) and (2) are computed differently. The TIES task vectors are sparsified by first keeping only the top-k entries. Then, positive and negative values of a particular parameter across models are independently averaged. The final parameter value is the one with larger average magnitude. In the DARE approach, the task vectors of each model (1) and (2) are first sparsified by randomly setting the parameters in \u315c(i) to zero with probability p. The remaining parameters are then re-scaled by multiplying them with 1/(1 \u2013 p) to form the modified task vectors \u012b(i) and \u3012(2). Re-scaling is performed to regain the scale that existed before sparsification. The modified task vectors (i) are combined using A to form the merged parameter vector as follows:\n$\\\u04e9\u0442 = 0 + (\\7(1) + \u3012(2))$.\n(3)\nObserve that the so-called \u201ctraining-free\" model merging methods are not entirely devoid of training procedures. Crucially, the weighting parameter A involved in the Task Arithmetic, TIES-Merging, and DARE can be characterized as a form of hyperparameter, whose optimal value is found using the validation data set Dval. These methods typically employ a single hyperparameter A to scale the task vectors and incorporate it into the pre-trained weight. A is determined through a grid search process spanning values between 0 and 1, a non-trivial amount of computational resources are still required to identify this optimal value. As shown in Fig. 2, when we merge models across 11 different NLP tasks, the performance of DARE drop significantly on both sides of the optimal X = 0.3. Similar behavior is observed for TIES. Therefore, although the merging algorithm itself does not require"}, {"title": "5 LEARNING TO MERGE MODELS", "content": "Given the challenges for selecting the optimal value for \u5165 and the large layer-specific variations in the task vectors, we propose a simple supervised approach for model merging called SUPERMERGE."}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "In this section we extensively evaluate SUPER-MERGE across multiple experimental settings. We test SUPERMERGE using both generative tasks from the NLP domain and predictive tasks from the computer vision domain to demonstrate the broad range of tasks that are supported by SUPERMERGE.\nGenerative Tasks. We follow the setting used in (Liu et al., 2022; Yadav et al., 2024). Specifically, we use T0-3B (Sanh et al., 2022) as the base model and we use the IA\u00b3 PEFT method (Liu et al., 2022) for fine-tuning. We fine-tune models on 11 data sets. Three sentence completion data sets (COPA (Roemmele et al., 2011), H-SWAG (Zellers et al., 2019), and Story Cloze (Sharma et al., 2018)). Three natural language inference (ANLI (Nie et al., 2019), CB (De Marneffe et al., 2019), and RTE (Dagan et al., 2005)). Two co-reference resolution data sets (WSC (Levesque et al., 2012) and Winogrande (Sakaguchi et al., 2021)) and a word sense disambiguation data set (WiC (Pilehvar & Camacho-Collados, 2018)). When fine-tuning IA\u00b3 parameters added to the T0-3B model, the prompt are generated using templates from the Public Pool of Prompts (P3 (Bach et al., 2022)).\nPredictive Tasks. We test SUPERMERGE on the predictive image classification task. For this, we use the Vit-B/32 architectures in CLIP (Radford et al., 2021) as our pre-trained models to conduct experiment on eight image classification datasets: Cars (Krause et al., 2013), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), GTSRB (Stallkamp et al., 2011), MNIST (LeCun et al., 1998), RESISC45 (Cheng et al., 2017), SUN397 (Xiao et al., 2016), and SVHN (Netzer et al., 2011). This setting is similar to the experimental setting used by (Ilharco et al., 2022; Yadav et al., 2024; Yang et al., 2023). Some image classification data sets, like MNIST, don't include a validation set. In such cases, we use 10% of the training samples as the validation set.\nBaselines. We use two types of baseline methods: non-gradient based methods and gradient based methods. Non-gradient based methods do not use gradient descent to adjust the tunable parameters. Here we use Task Arithmetic (Ilharco et al., 2022), DARE (Yu et al., 2024), and TIES (Yadav et al., 2024). These methods use the hyper-parameter A to add the merged task vector to pre-trained weight. Gradient based methods use gradient descent to tune the weights. Here we consider: Individual fine-tuned models that are fine-tuned using D\u021brain. A single Multitask model fine-tuned using Dtrain. Finally, we consider AdaMerging\u00b3 (Yang et al., 2023), which is a gradient-based model merging methods. It uses an unsupervised loss to learn the merging weight. We omit older baselines such as Fisher Merging (Matena & Raffel, 2024) and RegMean (Jin et al., 2023) as they are known to be sub-optimal as compared to the recent baselines, such as TIES and DARE that are included in our comparison."}, {"title": "7 CONCLUSION", "content": "We introduced a gradient-based model merging method called SUPERMERGE. SUPERMERGE only has a handful of training parameters and runs significantly faster than full fine-tuning, while maintaining similar performance. Our experiments demonstrate SUPERMERGE outperforms existing model merging methods on numerous tasks. Furthermore, merging several models at once could require substantial memory that increases with the number of tasks and the size of the base model. We"}, {"title": "A APPENDIX", "content": "A.1 ANALYZING OUT-OF-DOMAIN PERFORMANCE OF PREDICTIVE TASKS\nWe analyze performance of the predictive image classification task. In Table 6, we report the results of merging individual base models fine-tuned for tasks SUN397, Cars, SVHN, GTSRB, RESISC45, DTD, and report performance on two out-of-domain datasets of EuroSAT and MNIST. Recall that for the classification we do not merge the classification heads, as the number of classes for each task differ. Unsurprisingly, all the methods show significant performance drops on out-of-domain data sets as the classification heads are not merged. The performance of gradient-based methods, i.e., AdaMerging and SUPERMERGE, decreases more than non-gradient-based methods like TIES and Task Arithmetic, although recall that the gradient-based methods outperform the non-gradient-based methods for the in-domain datasets.\nA.2 CALCULATING PEAK MEMORY\nThe peak memory requirement is computed using the following equation:\nPeak Mem. =4* Npara +4 * NTrainable Para +8 * NTrainable Para + 1merging * 4 * kNTask Vector (6)\nwhere k is the number of tasks. The peak memory requirement consists of four components:\n1. Model Weights: Memory required for storing the weights of the pre-trained model. Here each model parameter requires 4 bytes.\n2. Gradients: Optimization requires extra space to store the gradients of the parameters. Here each trainable parameter takes 4 bytes of memory for storing it's gradient.\n3. Optimizer State: Optimizer states of trainable parameters are also stored. The space required for storing optimizer states depends on the choice of the optimizer. Here, we chose AdamW as the optimizer that needs 8 bytes per trainable parameter to maintain 2 gradient moments.\n4. Task Vectors: Task vectors of all k task-specific models need storage. Note that only model merging approaches need this extra space for the task vectors. Fine-tuning approaches don't need to store task vectors, as all fine-tuning approaches start from a pre-trained model. Thus, we have added an indicator function to the forth term in Eq. (6)."}]}