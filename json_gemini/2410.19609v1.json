{"title": "Open Web Voyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization", "authors": ["Hongliang He", "Wenlin Yao", "Kaixin Ma", "Wenhao Yu", "Hongming Zhang", "Tianqing Fang", "Zhenzhong Lan", "Dong Yu"], "abstract": "The rapid development of large language and multimodal models has sparked significant interest in using proprietary models, such as GPT-40, to develop autonomous agents capable of handling real-world scenarios like web navigation. Although recent open-source efforts have tried to equip agents with the ability to explore environments and continuously improve over time, they are building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents struggle to generalize to realistic settings that require multimodal perception abilities and lack ground-truth signals. In this paper, we introduce an open-source framework designed to facilitate the development of multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets.", "sections": [{"title": "1 Introduction", "content": "Developing autonomous agents that can complete complex tasks such as web navigation has been a significant challenge for the AI community (Zhou et al., 2023; Gur et al., 2023; Deng et al., 2024; Koh et al., 2024). Recent advancements of large language and multimodal models such as Claude (Anthropic, 2024) and GPT-40 (OpenAI, 2024) have made it possible to build such agents via prompt engineering (He et al., 2024; Zheng et al., 2024b; Ma et al., 2023). However, these agents struggle to improve further due to their reliance on closed-source models. Another line of work has explored alternative ways to build agents by starting off with weaker open-source models and gradually improving model performance by iteratively exploring the environment, collecting feedback signals, and updating the policy model (Xi et al., 2024; Putta et al., 2024; Patel et al., 2024). However, existing studies have only focused on building text-only agents in synthetic environments (Song et al., 2024). The synthetic environments provide the benefit of well-defined reward signals, allowing the agents to effectively differentiate the quality of the trajectories and learn accordingly. However, synthetic environments fail to capture the complexity of real-world scenarios, leading to potential generalization issues when applied to real-world tasks. Moreover, real-world environments often do not have built-in reward signals, which poses another challenge in agent's learning and improvement process (He et al., 2024). Additionally, real-world webpages are designed based on human visual preference, ignoring the visual inputs can cause significant information loss that impacts the agent's performance.\nTo address above limitations and explore open-source models in real-world settings, we propose Open Web Voyager, an open-source framework for building multimodal web agents via iterative real-world exploration, feedback and optimization. We show that Open Web Voyager can learn to perform real-world web navigation tasks through an initial imitation learning (IL) phase followed by multiple exploration-feedback-optimization cycles. To do so, we start by compiling a diverse set of web task queries and collecting corresponding agent trajectories using a state-of-the-art multimodal agent Web-Voyager (He et al., 2024) based on GPT-40, which we refer to as WebVoyager-4o. During the imi-"}, {"title": "2 Related Work", "content": "2.1 Multimodal Web Agents\nRecently, there has been a growing interest in building multimodal web agents, particularly those that combine visual and textual understanding capabilities. Unlike traditional HTML-dependent LLM-based agents (Lutz et al., 2024; Zhou et al., 2023; Gur et al., 2023; Nakano et al., 2021; Ma et al., 2023), Large Multimodal Model (LMM)-based agents can perform a wider variety of web tasks and adapt to more complex web environments. The main difference lies in the observation space. To acquire multimodal input signals, SeeAct (Zheng et al., 2024a) focuses on annotating images of web pages using bounding boxes and index labels of candidate web elements. Web Voyager (He et al., 2024) and Visual Web Arena (Koh et al., 2024) both use a JavaScript tool to extract web elements and annotate them on screenshots in a Set-of-Mark (Yang et al., 2023) format. DUAL-VCR (Kil et al., 2024) contextualizes each web element with its neighbors in the screenshot. SCAFFOLD (Lei et al., 2024) introduces dot matrices and coordinates on images to enhance visual grounding. Most of the aforementioned"}, {"title": "2.2 Self-Improving Web Agents", "content": "Researchers also have attempted to boost agents and adapt them to complex environments through self-improvement. Agent GYM (Xi et al., 2024) proposes a framework that unifies a wide range of environments for real-time exploration and evolution of LLM-based agents. AgentQ (Putta et al., 2024) integrates Monte Carlo Tree Search (MCTS) and Direct Preference Optimization (DPO; Rafailov et al., 2024) algorithms to iteratively update the policy of LLM-based web agents based on successful and failed web trajectories. Patel et al. (2024) suggests improvement by utilizing web agents to collect and filter in-domain trajectories, plus out-of-domain tasks along with hypothetical solution trajectories. However, there is still a lack of exploration on how to leverage multimodal web signals to achieve self-improvement. We aim to enable multimodal web agents to adapt to complex and dynamic online environments, enhancing their generality and ability to operate across numerous online websites."}, {"title": "3 Method", "content": "In this section, we introduce Open Web Voyager, an innovative web agent that outlines a path of iterative optimization for LMM-based Web Agents to handle intricate online web tasks. Firstly, we enable the agent to learn web navigation trajectories of Web Voyager-40 in the first stage to gain basic web knowledge and navigation skills, namely Imitation Learning (IL). Subsequently, the agent iteratively explores and improves with the feedback from GPT-40."}, {"title": "3.1 Task Formulation", "content": "In the web browsing environment E, consider the web navigation process as a Partially Observable Markov Decision Process (POMDP). The setup is defined by the tuple (S, O, A, T, R), where S denotes the state space, O represents the observation space, and A is the action space. T is the deterministic transition function that performs web operations in the browser to promote the process. The reward R in this environment is typically a sparse signal indicating success or failure, with values of 1 or 0, respectively.\nGiven a task query q and its corresponding website w, we can initialize the web environment E by setting the state $s_1$ to this web page, and obtain the first step observation $o_1 \\in O$. In this work, we adopt the vision-language setting that the observation in each step will include an accessibility tree and a screenshot, i.e., $o_1 = (o_1^v, o_1^t)$. Let $\\theta$ represents the parameters of the Large Multimodal Models (LMMs). Following the ReAct paradigm, we derive thoughts and actions using LMMs:\n$(h_1, a_1) \\sim \\pi_\\theta(\\cdot | I, q, o_1^v, o_1^t)$,\n$(h_1, o_1) \\sim \\pi_\\theta(\\cdot | I, q, o_1^v, o_1^t, o_1)$,\nwhere I denotes the system prompt, including answer formats, the introduction of web operations and some guidelines. The transition function T is then applied to parse the action and execute it on the web page, obtaining the next state $s_2$. Therefore, at time step t, we have:\n$(h_t, a_t) \\sim \\pi_\\theta(\\cdot | I, q, o_1^v, o_1^t, h_1, a_1, ..., o_t^v, o_t^t)$ (1)\n$s_{t+1} = T(s_t, a_t; E)$. (2)\nThe full trajectory can be represented as $\\tau = (o_1^v, o_1^t, h_1, a_1, ..., o_T^v, o_T^t, h_T, a_T)$, where T is the number of iterations in web navigation, i.e., the length of the trajectory."}, {"title": "3.2 Open Web Voyager Overview", "content": "Environment We adopt the Selenium-based online web navigation environment provided by Web Voyager (He et al., 2024). In contrast to Web Voyager, we do not employ the Set-of-Mark approach"}, {"title": "3.3 Web Task Queries Collection", "content": "Queries for the Imitation Learning Phase The IL phase is crucial as it forms the foundation for subsequent improvements. We aim to gather a diverse set of web tasks of varying difficulty, enabling GPT-40 to generate diverse trajectories. We choose 48 popular websites, then select and synthesize the queries $Q_{IL}$ from multiple perspectives before Imitation Learning. The details of $Q_{IL}$ collection are shown in Appendix C.\nQueries for Real-World Exploration We continue to use the self-instruct (Wang et al., 2022) approach to generate new queries that are similar but not duplicated based on existing queries. In each exploration-feedback-optimization cycle, we automatically generate 480 queries for 48 websites, with 10 queries for each website. The agent then conducts web exploration based on these tasks."}, {"title": "3.4 Imitation Learning", "content": "Trajectories Collection We utilize GPT-40 along with the Web Voyager paradigm (He et al., 2024) to generate web navigation trajectories corresponding to the above queries. The agent is named Web Voyager-40 and configured to receive observations consisting of the latest k steps, including the accessibility trees and screenshots. i.e., for each $q_i \\in Q_{IL}$, $\\tau_i \\sim \\pi_{\\theta_S}(\\tau | I, q_i)$, we clip the long context $c_t$ to avoid performance degeneration when $t > k$:\n$c_{t_{clip}} = (h_1, a_1, h_2, a_2, ..., h_{t-k}, a_{t-k},$\n$o_{t-k+1}, h_{t-k+1}, a_{t-k+1},..., o_t).$ (3)\n$(h_t, a_t) \\sim \\pi_{\\theta_S}(\\cdot | I, q, c_{t_{clip}})$. (4)\nIt is worth noting that we preserve the thought and action of each step to maintain the full reasoning"}, {"title": "3.5 Iterative Optimization", "content": "After the Imitation Learning phase, the trained agent $\\pi_{\\theta_{IL}}$ will proceed to explore websites and undergo multiple cycles of exploration-feedback-optimization. We continue to generate more queries using self-instruct. Instead of relying on Web Voyager-40 to collect trajectories, the agent collects trajectories on its own. At each exploration-feedback-optimization cycle, we employ trajectory-level rejection sampling via GPT-40 to ensure quality trajectories. Let $Q_{S_j}$ be the query set for j-th optimization, for every $q \\in Q_{S_j}$, we sample several trajectories from the model $\\pi_{\\theta_{j-1}}$, with GPT-40 acting as the Auto Evaluator, accepting only trajectories that GPT-40 deems as successfully navigated. We consider this auto evaluation method reliable because assessing the correctness of a trajectory is much easier than obtaining a correct trajectory. He et al. (2024) also demonstrates a high level of evaluation consistency between GPT-40 and humans.\nLet $D_{S_j}$ represent the set of trajectories collected after rejection sampling in the j-th optimization. We mix the collected trajectory sets with $D_{IL}$ and continue fine-tuning $\\pi_{\\theta_j}$ by maximizing the following objective:\n$J(\\theta) = E_{(q,\\tau)\\sim D_{S_1}} \\sum_{t=1}^{T} [log \\pi_{\\theta} (a_t | q, c_{t_{clip'}}, h_t) + log \\pi_{\\theta}(h_t | q, c_{t_{clip'}})]$, (7)\nwhere $j = 1, ..., m$ denotes the times of optimization, $D_{S_1} = D_{IL} \\cup D_j$ denotes the mixed trajectory set and $\\pi_{\\theta_0}$ is set to $\\pi_{\\theta_{IL}}$. The complete procedure is shown in Algorithm 1 in Appendix B."}, {"title": "4 Experiment", "content": "4.1 Dataset and Metric\nTraining Dataset In \u00a73.4, we have outlined the composition of the query set $Q_{IL}$ during the Imitation Learning stage, which includes 48 websites mentioned in Mind2Web (Deng et al., 2024) and Web Voyager (He et al., 2024), along with 1516 relevant task queries collected. We use Web Voyager-40 to gather corresponding trajectories for them, with each query having a maximum of 2 trajectories. Then we retain 1165 finished (including both successful and unsuccessful) trajectories, with a total of 7253 interaction turns. During the j-th exploration-feedback-optimization cycle, we expend 480 queries for 48 selected websites. The trajectories are sampled via $\\pi_{\\theta_{j-1}}$ and the maximum resampling count is set to 5.\nEvaluation Dataset To evaluate the performance of our agent, we use the following datasets: 1) Web Voyager (He et al., 2024) test set, comprising 15 websites seen during training and 643 task queries; 2) Mind2Web (Deng et al., 2024) cross-task test set, which included 33 websites seen during training and a total of 112 queries. 3) Mind2Web cross-website test set, we select 2 websites each from"}, {"title": "4.3 Main Results", "content": "Throughout the entire process of Imitation Learning and exploration-feedback-optimization cycles, we trained four models: OpenWebVoyagerIL, OpenWebVoyageriter-1, OpenWebVoyageriter-2, and OpenWebVoyageriter-3. Table 1 shows the performance of these models on the Web Voyager test set. Table 2 presents the results of these models on the Mind2Web cross-task and cross-website test set. We show the performance changes of our agent on these datasets from imitation learning phase to the third exploration-feedback-optimization cycle in Figure 3.\nFrom the results in Table 1 and Table 2, we observe a general improvement in task success rates in both the Web Voyager test set and the Mind2Web cross-task test set as optimization progressed. This indicates the effectiveness of our method when the webs in the test set have been trained on or explored during the training phase. In the Mind2Web cross-web test set, the exploration-feedback-optimization cycle also provides some enhancement in the model's performance, although not as prominently as in the cross-task set. Also,"}, {"title": "4.4 Discussion", "content": "The average length of trajectories. During inference, we record the length of trajectories when they are finished (the agent provides answers) and successful. The variation of the average length of web navigation trajectories is shown in Table 4.\nIn our experiments, we observe that as iterative optimization progresses, agents tend to complete tasks in fewer interaction steps and navigate more quickly on familiar websites. This phenomenon creates a cycle where trajectories obtained during the exploration-feedback phase become shorter, leading the model to increase its focus on learning from shorter trajectories during optimization.\nHallucination limits agent's performance. We find that agents often directly hallucinate answers that do not appear during the navigation process. The decrease in trajectory length might have increased the frequency of this issue. The agent tends to terminate navigation directly instead of continuing the search after a certain length of the trajectory."}, {"title": "5 Conclusion", "content": "In this paper, we explore how to construct a multimodal web agent via iterative exploration, feedback and optimization. We adopt idefics2-8b-instruct as the backbone LMM model and collect web task queries from numerous websites. Initially, our agent learns the web operation logic of GPT-40 through Imitation Learning. Then it enters the"}, {"title": "Limitations", "content": "First, we only consider the most common executable web actions in the simulated environment, including clicking, typing, and scrolling, without more advanced actions such as dragging and zooming. Additionally, our approach is based on a relatively small LMM Idefics2 with 8B parameters, which may limit the agent's ability to effectively navigate websites of unseen domains and respond to complex user queries. The low performance on complex websites might further affect exploration efficiency, leading to minimal improvement and time-consuming during the exploration-feedback-optimization process. Last, our model still primarily relies on accessibility trees, we hope to improve the visual grounding and multi-image reasoning capabilities so that it can directly use web screenshots for planning like GPT-40."}, {"title": "Ethics Statement", "content": "In light of the potential risks associated with online web navigation, all our experiments adhere strictly to ethical guidelines. Our approach includes human supervision as well as GPT-4's monitoring for content violations. Throughout the sampling of all web task trajectories, no violations by the agent are detected. A small portion of tasks are filtered due to the sensitivity of advertisements or content on news websites. None of the tasks involve private information such as personal names, account passwords, etc. The tasks typically include information-seeking activities and do not include actual bookings or payment transactions. In our work, the web agent's sampled trajectories are intended solely for research purposes. The agent operates in a simulated human-like manner, with a slow sampling frequency, ensuring no pressure is placed on the explored websites."}, {"title": "A Environment and Prompts", "content": "We adopt the framework of Web Voyager for online real-world web navigation. The web actions used are the most basic clicks, inputs, and scroll operations as shown in Table 7. Unlike Web Voyager, we do not use the Set-of-Mark approach to label screenshots. Instead, we combine screenshots and the accessibility tree as observations for the agent to make decisions. Figure 4 illustrates an example of observation.\nBased on the changes in observations, we slightly modify the system prompt of Web Voyager (He et al., 2024) during the Imitation Learning phase to accommodate the paradigm of accessibility tree + screenshot. In terms of web operation implementation, each element in the accessibility tree has pre-saved attribute information, where 'union_bound' labels the position information of the element. We use Selenium to locate the element that appears in this position and then access it.\nIn the Web Voyager framework, in addition to the system prompt, the author has designed error reflection to ensure effectiveness. When a certain action fails, there will be a prompt saying: \"The action you have chosen cannot be executed. Please double-check if you have selected the correct element or used the correct action format. Then provide the revised Thought and Action.\" This prompt serves to remind the agent to correct errors. While training our own Agent, although we no longer use the system prompt, we still retain the error reflection mechanism."}, {"title": "B Algorithm", "content": "In Algorithm 1, we present the complete algorithm of Open Web Voyager. It mainly consists of an Imitation Learning (IL) phase and multiple exploration-feedback-optimization cycles. In the IL phase, GPT-40 ($\\pi_{\\theta_S}$) serves as an expert to sample trajectories via Web Voyager framework, requiring a significant number of OpenAI API calls. In the exploration-feedback-optimization cycle, GPT-40 acts as an expert to evaluate trajectories, with only one API call needed for each trajectory. Hence, during the execution of the algorithm, there is a trade-off. On one hand, we aim to increase the sampling in the IL phase to enhance the model's capabilities and obtain a strong base model ($\\pi_{\\theta_{IL}}$), which can improve exploration efficiency. However, if the improvement in the IL phase is not obvi-"}, {"title": "C Details of Datasets", "content": "Selected Websites In the Imitation Learning phase and exploration-feedback-optimization cycles, we collect task queries from 48 websites for exploration. We utilize all 15 webs from Web Voyager and 37 webs from Mind2Web, totaling 48 webs (with 4 duplicates). Table 8 displays the specific website names used during the training phase. During inference, we employ all task queries from the Web Voyager test set and select some task queries from the Mind2Web cross-task and cross-website test set including both learned and unlearned websites. To facilitate testing, we update the time information of some tasks but do not change their task expressions. Table 9 presents detailed statistics about the test set.\nQueries preparation for Imitation Learning The learning effectiveness during the Imitation Learning phase is not only related to the expertise of GPT-40 but also to the richness of the task queries used. To diversify trajectories as much as possible during the Imitation Learning phase, we"}, {"title": "D Example Trajectories", "content": "In Figures 5 and 6, we present two examples of successful webpage navigations by OpenWebVoyageriter-3. As shown in Figure 5, agent navigates directly on the Google Flights webpage and succeeds. The agent makes decisions based on the screenshots and the specific text information of web elements in the accessibility trees. In Figure 6, the agent mistakenly thinks that logging in is required to search on GitHub, then it chooses to restart from Google Search and finds the answer.\nWe also present an example where an agent hallucinates an answer when it cannot find one. As Illustrated in Figure 7, while navigating the Allrecipes website, the agent fails to locate a chocolate chip cookie recipe that meet the task requirements. However, it provides an answer titled \"Classic Chocolate Chip Cookies.\" This discrepancy may be attributed to the agent interpreting the word \"Classic\" in the accessibility trees as a recipe and even hallucinating a cook time, despite the lack of relevance."}]}