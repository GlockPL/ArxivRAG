{"title": "LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale", "authors": ["Jaehong Cho", "Minsu Kim", "Hyunmin Choi", "Guseul Heo", "Jongse Park"], "abstract": "Recently, there has been an extensive research effort in building efficient large language model (LLM) inference serving systems. These efforts not only include innovations in the algorithm and software domains but also constitute developments of various hardware acceleration techniques. Nevertheless, there is a lack of simulation infrastructure capable of accurately modeling versatile hardware-software behaviors in LLM serving systems without extensively extending the simulation time.\nThis paper aims to develop an effective simulation tool, called LLMServingSim, to support future research in LLM serving systems. In designing LLMServingSim, we focus on two limitations of existing simulators: (1) they lack consideration of the dynamic workload variations of LLM inference serving due to its autoregressive nature, and (2) they incur repetitive simulations without leveraging algorithmic redundancies in LLMs. To address these limitations, LLMServingSim simulates the LLM serving in the granularity of iterations, leveraging the computation redundancies across decoder blocks and reusing the simulation results from previous iterations. Additionally, LLMServingSim provides a flexible framework that allows users to plug in any accelerator compiler-and-simulation stacks for exploring various system designs with heterogeneous processors. Our experiments demonstrate that LLMServingSim produces simulation results closely following the performance behaviors of real GPU-based LLM serving system with less than 14.7% error rate, while offering 91.5\u00d7 faster simulation speed compared to existing accelerator simulators.", "sections": [{"title": "I. INTRODUCTION", "content": "Currently, there is a significant surge in efforts to exploit large language model (LLM) as a crucial component in real-world applications [44], [83]. Given the prohibitively high costs associated with building on-premise infrastructure for LLM inference, the common practice is to offload LLM inference to multi-tenant \u201cinference serving\" systems in the cloud, exemplified by OpenAI's ChatGPT service [47]. The massive compute and memory requirements (both bandwidth and capacity) are forcing these systems to be equipped with many AI accelerators (or NPUs).\nThere has been a large body of research works that aim to develop efficient hardware and software for LLM inference serving systems. Some works target to develop customized hardware techniques for accelerating LLM inference serving [23], [49], while others focus on developing optimized system software on GPU-based scale-out systems [10], [33], [41], [46], [52]. Recently, a few pioneering works propose to take into consideration both hardware and software together for designing holistic end-to-end accelerated systems [22], [48]. However, there is currently a lack of simulation infrastructure that allows researchers to explore their hardware-software proposals in a scale-out setting. This limitation not only makes it difficult for architecture researchers to explore scalable accelerator solutions, but also forces system software researchers to exclusively rely on GPU-based system software in the era of specialized hardware.\nThis paper sets out to address this limitation and develop a LLM inference serving system simulator, called LLMServingSim, that jointly simulates the behaviors of LLM-customized accelerators and LLM inference serving system software. LLMServingSim is built on top of an existing AI system simulator, ASTRA-sim [74], which jointly models both hardware and software for Al workloads. However, there are primarily two algorithmic differences, making the design principles of LLMServingSim and ASTRA-sim largely different, as described below.\nAutoregressive nature of LLM generation. ASTRA-sim focuses on distributed training, which entails millions of \"identical\" iterations of computing that simplify the simulation. On the contrary, we target LLM inference serving that involves autoregressive token generations, producing dynamically changing behaviors across different iterations, requiring independent simulation runs.\nRedundancies across decoder blocks in LLMs. Unlike"}, {"title": "II. BACKGROUND", "content": "Most modern large language models (LLMs) employ decoder-based transformer architecture [72], as described in Figure 1. This architecture consists of its fundamental building blocks: the embedding layer, transformer blocks, and language modeling (LM) head. Each transformer block constitutes three main components: Query-Key-Value (QKV) generation, multi-head attention, and feedforward networks.\nDecoder-based transformer model operates in two distinct phases during their inference: initiation and generation phase. The initiation phase begins with receiving the prompt as input and generates QKV for all input tokens. Generated QKV values pass through subsequent multi-head attention layers and feed-forward networks. This phase predominantly involves General Matrix Multiply (GEMM) operations, which handle the bulk of computation by processing multiple data points collectively. Once the initiation phase is completed, the model outputs one token and transitions to the generation phase, with the generated token as the new input. This generation phase has autoregressive characteristics where each output"}, {"title": "B. Batching and Memory Management for LLM", "content": "To minimize latency and maximize hardware utilization, LLM inference serving system often employs batching, which involves grouping multiple requests into a single group. However, it presents a challenge, particularly with the multi-head attention layer, which makes batching difficult. Additionally, it faces the drawback of needing to complete all requests before proceeding to the next batch, which can lead to inefficiencies. To tackle this challenge, Orca [81] proposes two techniques: selective batching and iteration-level scheduling. Selective batching allows batching in specific layers, such as QKV generation and feed-forward networks, while in multi-head attention layers, it allows a batch to be divided and allocated to multiple workers individually. Iteration-level scheduling involves rescheduling the batch at each iteration, removing completed requests, and adding new ones. This technique enhances hardware utilization and reduces latency by dynamically updating the batch to include only active requests, thereby streamlining the process.\nAnother challenge in the scale-out inference serving system is to effectively handle KV cache. Conventional LLM serving allocates KV cache based on the maximum possible sequence length, and this results in underutilized memory spaces and limited batch sizes. vLLM [33] introduces a paging scheme for memory management that functions similarly to the virtual memory of operating systems. Managing memory on a page-by-page basis, vLLM effectively reduces memory fragmentation, enabling larger batch size and higher throughput."}, {"title": "C. Processing-in-Memory (PIM) for LLM", "content": "In the generation phase, LLM inferencing heavily relies on General Matrix-Vector multiplication (GEMV) operations, especially within the multi-headed attention layers. These GEMV operations are characterized by being memory-bound with low arithmetic intensity due to the lack of matrix reuse. Processing-in-Memory (PIM) techniques are recognized for their ability to accelerate memory-intensive operations such as GEMVs. PIM optimizes memory-intensive tasks by reducing data movement through the placement of compute unit in each memory bank. This approach utilizes aggregated bandwidth to read intermediate values, execute computations, and send only the results to the host system.\nThere has been significant research on using PIM to accelerate LLM inference. TransPIM [86] has proposed a PIM-focused solution specifically for speeding up end-to-end Transformer inference. More recently, AttAcc [48], IANUS [59], and NeuPIMs [22] have developed approaches that integrate"}, {"title": "III. MOTIVATION", "content": "LLMs with parameters ranging from a few hundred million to several hundred billion or even trillions, require enormous computational and memory capabilities for inference. To handle batched requests from multiple users, the scale-out serving systems often constitute hundreds of nodes, each equipped with multiple high-performance AI accelerators with high-bandwidth and high-capacity memories [22], [45], [48], [49]. Recently, several studies have explored solutions involving software, hardware, or both of them for such large-scale LLM serving systems [21], [23], [38]. However, the absence of an effective system-level simulator for scale-out LLM serving systems remains a major barrier for researchers and engineers who continue to explore solutions."}, {"title": "B. Limitation of Existing AI System Simulators", "content": "ASTRA-sim. We are not the first one who propose to develop scale-out system simulators for AI workloads. ASTRA-sim [74] is an effective open-source tool for simulating scale-out system for AI workloads and can be considered as an alternative to LLMServingSim. However, ASTRA-sim focuses on training with repetitive yet identical iterations, which does not align well with the nature of LLM inference serving, where each iteration processes different batches with varying compositions of variable-length prompts. While ASTRA-sim is insufficient for our purposes as it stands, we notice that it offers essential features for simulating a scale-out AI system. Therefore, we decided to avoid reinventing the wheel and integrate ASTRA-sim as a module within LLMServingSim to simulate a single iteration.\nLLM inference simulators. Rather disjointly, there have been recent research efforts to build LLM inference simulators [18], [19], [22], [27], while they are not suitable for system-level simulation at scale. The main reason is that the current simulators operate at a slow pace, rendering them insufficient for simulating large-scale LLM inference serving, which is inherently iterative in nature."}, {"title": "C. Need for Simulators with Heterogeneity", "content": "As discussed in Section II-A, one of the notable characteristics of LLM inference is that compute-intensive operations and memory-intensive operations are intermixed. We analyze and compare the computation and memory usage of each operation using GPT3-7B model with NVIDIA RTX 3090 GPU. Figure 2(b) shows the result of roofline analysis comparing arithmetic intensity of operations during inference. We notice that operators of multi-head attention and layer normalization have low arithmetic intensity and are bandwidth-bound. On the contrary, operators of QKV generation and feed-forward networks have high arithmetic intensity and are compute-bound. These two types of operators require high memory bandwidth and high compute capability, respectively. Meanwhile, another characteristic of LLM inference is that key-value (KV) cache imposes a significant overhead on memory capacity [24], [33], [84], since keys and values are generated for every token of the entire sequence and every transformer block.\nRequirements for high memory bandwidth, memory capacity, and computation power of LLM inference make it difficult to find \"one-fits-all\" solution for acceleration. GPUs equipped with high-bandwidth memory such as NVIDIA H100 [45] appear to be this solution, but they have small and limited scalability of capacity. Several recent studies have proposed solutions with heterogeneous accelerators for LLM inference serving [22], [34], [48], [52]. A system using heterogeneous accelerators maps operators with conflicting properties to devices with different characteristics. For instance, with inference acceleration solutions using PIM and NPU [22], [48], operators with low arithmetic intensity are mapped to PIM devices, and other operators are mapped to NPU devices."}, {"title": "IV. LLMSERVINGSIM", "content": "We design LLMServingSim, a novel system-level simulator for LLM inference workloads that jointly simulates LLM serving system software and heterogeneous hardware accelerators. For simplicity of explanation, we provide an example where LLMServingSim simulates a distributed and heterogeneous system consisting of one host node, NPU nodes, and PIM nodes.  Note that LLMServingSim can be flexibly configured with various system topologies and combinations of heterogeneous accelerators."}, {"title": "A. Simulator Design", "content": "Overview. Figure 4 depicts the LLMServingSim workflow, which is designed to perform iteration-level simulation for distributed system with heterogeneous hardware. LLMServingSim consists of the following components:\n(1) Scheduler receives and organizes user requests into feasible batches based on the scheduling, KV cache management, and operator mapping strategy. It also makes the next scheduling decision based on the results of ASTRA-sim.\n(2) Execution engine stack compiles the model according to the batch configuration created by Scheduler, and performs hardware simulation for a single device. Each heterogeneous accelerator has distinct engine and produces distinct trace. Execution engine stack schedules operators from multiple traces and reconstructs them into a single trace.\n(3) Graph converter generates execution graphs using the given trace from execution engine stack, according to the configured parallelism strategy.\n(4) ASTRA-sim [74] takes execution graph represented in Chakra graphs [65] as inputs, performs system simulation, and returns results back to the scheduler."}, {"title": "Iteration-level scheduling", "content": "LLM processes input prompts autoregressively by generating one token at a time during inference. To efficiently process the iterations, a state-of-the-art LLM serving system, Orca [81], proposes iteration-level scheduling. We employ this technique in LLMServingSim by designing the simulation workflow as repeated alternations of prompt batch scheduling, hardware simulation, and system simulation at the iteration level.\nLLMServingSim scheduler first receives requests and compares their arrival times to the scheduler's timer to select batchable requests. In response to the dynamic changes in requests, the scheduler leverages execution engine stack, consisting of the engine-specific compilers and simulators, to simulate the behavior of accelerators. In a heterogeneous environment, operators are dealt in different accelerators, so the scheduler offloads operators to each execution engine according to the mapping strategy. Each execution engine compiles the model and simulates the hardware with specified input configurations. After hardware simulation, the graph converter converts the simulation results to an execution graph that maps the hardware to the system. This graph is then fed into ASTRA-sim to simulate and analyze the system behavior comprehensively. System simulation results are fed back to the scheduler, and the scheduler's timer, which is used to assemble a new batch for the next iteration, is updated accordingly. This cyclical interaction enables LLMServingSim to progress through iterations efficiently."}, {"title": "Supporting for LLM parallelism strategies", "content": "In the context of LLM inference, parallelism that distributes the model weights and layers of substantial size is crucial for enhancing performance. There are three major types of model parallelism: tensor parallelism, pipeline parallelism, and hybrid parallelism [64]. Tensor parallelism distributes the weight matrix across multiple workers. Pipeline parallelism assigns different layers of the model to different workers. Hybrid parallelism combines features of both tensor and pipeline parallelism.\nLLMServingSim can be configured to utilize a specific parallelism strategy by setting the number of accelerator groups according to the system's topology. When graph converter receives the output trace from execution engine stack, it identifies configured parallelism strategy and constructs execution graph accordingly for each accelerator. For tensor parallelism, it distributes tensors across the entire nodes and inserts ALLREDUCE operators to the execution graph for intermediate synchronization. For pipeline parallelism, it allocates decoder blocks to nodes in sequence, allowing chained computation across them. Hybrid parallelism combines both parallelism strategies by distributing tensors and layers within and across accelerator groups, respectively.\nTo employ selective batching, where attention layers are processed in parallel across different workers, the execution engine stack and graph converter work together. Hardware simulator assigns unique identifiers to the attention layers and records them in the output trace. Graph converter then assigns these attention layers to different nodes based on their identifiers. As illustrated in Figure 3, each node within an accelerator group independently processes distinct inputs with different sequence lengths, parallelizing batch processing."}, {"title": "KV cache-aware memory modeling", "content": "While ASTRA-sim has a simple memory model in its implementation, it lacks some memory constraints such as capacity and memory fragmentation. However, LLM inference is sensitive to memory capacity due to their significant memory usage of model parameters and KV cache. LLMServingSim uses detailed memory modeling scheme with several memory constraints to reduce the gap with actual systems. Memory model of LLMServingSim includes management of KV cache and generated tokens by incorporating demand paging technique from vLLM [33].\nThe management of KV cache and generated tokens in LLMServingSim scheduler is intertwined with iteration-level scheduling, which conducts batch reconstruction each iteration, by checking generated tokens and KV cache size of each batch. First, scheduler assesses the length of incoming requests to determine the required number of KV cache pages and allocates them to the local memory of accelerators accordingly to form a single batch. After an iteration completed, the scheduler reassesses the requests. If increased sequence length due to generated tokens requires additional page or incoming requests need to be added to the batch, new page is allocated on demand. If there is insufficient memory capacity for new pages, the entire page for KV cache and sequence of the last added requests are evicted to host memory. When memory availability permits, evicted KV cache blocks are reloaded from host memory for processing in subsequent batches.\nThe graph converter inserts operators into the execution graph for page eviction and reloading based on the decision of the scheduler. Whenever page eviction or reloading occurs, it inserts memory store or load operators embedded with the time taken to transfer the pages between accelerator device memory"}, {"title": "B. Simulating Heterogeneity in LLM Serving", "content": "Heterogeneous system overview. LLMServingSim supports simulation of heterogeneous systems composed of two or more different types of accelerator hardware beyond homogeneous systems. In this paper, we use example systems consisting of NPU devices for compute-bound operations and PIM devices for memory-bound operations. While we use these particular systems as running examples, it is worthwhile to note that LLMServingSim also supports simulation with hardware accelerators other than NPU or PIM by adding new execution engine to LLMServingSim infrastructure in a plug-in manner.\nAs discussed in Section IV-A, LLMServingSim can flexibly configure the system topology. Figure 5 illustrates two example systems: a heterogeneous system where NPU and PIM devices are directly connected, and a heterogeneous system where there are separate pools of NPU devices and PIM devices. For both example systems, accelerator nodes are connected to other accelerator nodes and hosts through high bandwidth interconnects such as CXL [8].\nAlgorithm 1 describes the overall workflow of LLMServingSim scheduler, execution engine stack, and graph converter to generate execution graph with the given request batch in the context of operator mapping and scheduling. In the following section, we discuss how LLMServingSim's operator mapping and scheduling decisions are made depending on the system topology or computational properties of acceleration hardware.\nOperator mapping. The components that perform operator mapping could be different depending on the heterogeneous system's topology and configuration. In LLMServingSim, the components responsible for operator mapping are execution engine, scheduler, and graph converter. To understand how these three components interplay, we describe operator mappings in the two example systems depicted in Figure 5.\n1 Operator mapping in execution engine. For instance, in a heterogeneous system consisting of NPU and PIM devices, memory-bound operations such as Attend and Score of multi-head attention layers are mapped to the PIM module. And, remaining compute-bound operations are mapped to the NPU module. However, as the NPU and PIM devices are directly connected each other, they act as one node at the system-level, so there is only one execution engine in LLMServingSim. Therefore, in the simulation of NPU-PIM system, mapping decision is done in the internal scheduler of execution engine.\n\u2461 Operator mapping in scheduler. On the other hand, in a heterogeneous system consisting of NPU and PIM pools, operator mapping is done in two components, scheduler and graph converter, rather than execution engine. In Line 6 of Algorithm 1, the scheduler decides which operator will be mapped to which device by considering the characteristics of both the operators and the hardware devices, and creates simulation plans. Then the scheduler delivers simulation plans"}, {"title": "III. MOTIVATION", "content": "while satisfying criteria such as fairness of computation load or memory accesses. Subsequently, each sub-batch undergoes the operator mapping.\nAfter operator mapping, each execution engine compiles and simulates mapped operators, and creates output trace. These output traces include mapping and simulation information for each operator and for each hardware. Operator scheduler of execution engine stack performs operator scheduling within a given batch by utilizing this information. In Line 14 of Algorithm 1, operator scheduling decides the execution order of operators using a greedy heuristic by considering dependencies between operators and the availability of heterogeneous accelerators. It maximizes hardware utilization of heterogeneous accelerators by allowing overlapping between operators and sub-batches."}, {"title": "C. Techniques for Fast Simulation", "content": "LLM typically necessitates lengthy compile and hardware simulation time. To solve the problem, we introduce resultreusing techniques, which reduce computation redundancy.\nModel redundancy reuse. First, we achieve significant time savings by exploiting the redundancy of common LLM architecture. As described in Figure 1, decoder-based LLM architecture consists of an embedding layer followed by repeated transformer blocks. LLMServingSim compiles one transformer block and replicates it, largely reducing the overall compile time required. Another optimization to reduce simulation time involves separating attention layers from non-attention layers. The initiation phase and the generation phase differ only in attention layers, depending on the presence or absence of KV cache. Therefore, LLMServingSim compiles and simulates the time-consuming non-attention layers just once, and subsequently, it simply swaps out the less time-intensive attention layers, cutting down on the total processing time.\nComputation reuse. Given the dynamic nature of input and output lengths in LLM inference, models typically need to be continuously compiled and simulated. LLMServingSim adopts a strategy of reusing previously simulated results through caching. For effective caching, it manages the non-attention and attention layers differently. Non-attention layers take longer than other layers to be processed but can be reused frequently. However, attention layers require more frequent compilation and simulation but take less time. We conduct an evaluation to evaluate the impact of this caching strategy and demonstrate that our optimization technique is effective in reducing the overall simulation time."}, {"title": "V. DISCUSSION", "content": "Pluggability to 3rd-party accelerators. LLMServingSim's infrastructure allows for high configurability in system configuration and simulators of various hardware can be seamlessly attached via interfaces to the LLMServingSim scheduler and graph converter. Therefore, integration with hardware simulators for various third-party accelerators other than NPU or PIM devices introduced in this paper is also possible. Beyond acceleration hardware, it is possible to extend memory features, for instance, by adding storage capabilities like HDDs and SSDs, or incorporating computational storage nodes such as SmartSSDs. This flexibility makes LLMServingSim a highly versatile tool for simulation and development.\nCompatibility with existing machine learning frameworks. LLMServingSim takes the ONNX [42] model format as an input, enabling interoperability with various machine learning frameworks. It allows users to seamlessly integrate and simulate widely-used open-source ONNX models written for frameworks such as PyTorch [50] and TensorFlow [1]. These models can be converted into ONNX format for use within LLMServingSim, facilitating a broad range of model experimentation and deployment scenarios."}, {"title": "B. Limitations and Future Works", "content": "With the rapid advancement in the fields of machine learning and large language models, new variant architectures of LLM such as multi-modal [7], [13], [37], [53], [54], [70], [76], [87], mixture of experts (MoE) [9], [11], [77], and retrieval augmented generation (RAG) [6], [30], [35], [55], [61], [79] have been developed to address limitations of the original architecture. Additionally, lightweight techniques such as quantization [12], [16], [39], [78], [80] and pruning [15], [40], [66], and fine tuning [25], [26] techniques, offer traditional but effective solutions for optimizing LLMs. LLMs using these architectures or technologies have different mathematical and computational characteristics compared to traditional decoder-based architectures, and ongoing studies are exploring accelerator architectures and inference systems to support these new models [3], [36], [41], [52].\nAlthough LLMServingSim currently focuses on traditional decoder-based LLM architectures, it can support new model variants with slight modification or even without any modification. This is due to its inherent flexibility at the system level and its use ONNX models, which allow for flexible model construction. For example, LLMServingSim can support MoE models by assigning each expert to one node and configuring the network topology to route to one of the expert nodes based on the inference results of the gating network. In addition, for RAG models, it is possible to configure the system such that vector storage is simulated in a storage node, and the results retrieved from this storage are used for further inference. Novel systems supporting these models may adopt new scheduling strategies as suggested in existing solutions [14], [46], [75], but we believe they can be accommodated within LLMServingSim's simulation infrastructure."}, {"title": "A. Methodology", "content": "System baselines. We use a homogeneous system consisting of only NPU and a heterogeneous system consisting of NPU and PIM for the validation and evaluation of LLMServingSim. Throughout our evaluation, we use a GPU system equipped"}, {"title": "NPU simulators", "content": "Several simulators have been proposed to accurately model NPU behavior for ML workloads. These simulators can either focus on a single core [58], [60] or model interactions between cores in a multi-core NPU [17], [19], [27]. However, current simulators only consider the behavior of individual NPU chips and don't model systems with multiple interconnected NPU chips."}, {"title": "Non-NPU simulators", "content": "With the emergence of research [20], [21], [67], [73], [82] on accelerating Attention operations in Transformer [72] and PNM (Processing near Memory) / PIM (Processing in Memory) research [22], [28], [31], [48], [49], [59] on accelerating memory-bound operations in LLMs, there is a lot of research on measuring performance with simulation. However, these studies have only modeled the performance of a single accelerator and lack simulations of the system."}, {"title": "ML system simulators", "content": "In the realm of distributed system simulators, tools have been developed to cater to a range of needs from general-purpose workload simulators [43], [57], [68] to those specifically designed for neural networks [56], [63], [74]. Recently, a specialized simulator tailored for LLM training [4] has emerged."}, {"title": "LLM inference serving simulators", "content": "Recently, reflecting the growing interest in LLM inference, a variety of simulators have been introduced [2], [51], [85]. However, these simulators are GPU-based and perform approximate simulation through methods like ML prediction, mathematical modeling, and using latency database instead of cycle-accurate simulations. To overcome the limitations of these existing studies, we propose LLMServingSim. Any hardware simulator that supports LLM operators can be integrated into our system simulator as an execution engine to perform system simulation. LLMServingSim is system simulator that supports multi-device and heterogeneous system configurations, rather than simulating a single hardware device. In addition, LLMServingSim successfully tackles them by exploiting techniques including iteration-level scheduling [81], KV cache paging [33], and the interaction between hardware and system simulators."}, {"title": "VIII. CONCLUSION", "content": "The absence of system simulator for LLM inference serving presents challenges for researchers in system or hardware architecture exploration. In this paper, we address these challenges by introducing LLMServingSim, a fast and accurate hardware-software co-simulation infrastructure for LLM inference serving systems, leveraging unique algorithmic characteristics of LLM serving. We believe that simulation tools for scale-out and heterogeneous LLM serving systems are crucial for accelerating research progress in this field, and LLMServingSim successfully makes a significant initial contribution towards meeting these needs."}, {"title": "ACKNOWLEDGEMENTS", "content": "This work was supported by the National Research Foundation of Korea (NRF) (No.RS-2024-00342148), Institute of Information & communications Technology Planning & Evaluation (IITP) (No.RS-2024-00459797, No.RS-2024-00396013, No.2022-0-01037), and the Graduate School of Artificial Intelligence Semiconductor (No.RS-2023-00256472), funded by the Korea government (MSIT). This work was also partly supported by HyperAccel."}, {"title": "A. Abstract", "content": "LLMServingSim is a fast and accurate hardware-software co-simulation infrastructure for LLM inference serving systems written with C++ and Python. LLMServingSim receives several system configurations and request traces from the user, calculates the cycles and throughput of the system composed of various accelerators, and measures the inference latency for each request."}, {"title": "B. Artifact Check-list (Meta-information)", "content": "Compilation: g++ v7.5.0\nRun-time environment: Ubuntu 18.04 Kernel v4.15.0\nHardware: x86-64\nOutput: standard output, TSV files\nHow much disk space required (approximately)?: Artifact evaluation requires up to 30GB of disk space, but depending on the models and datasets, it may require 1GB ~ 400GB of disk space.\nHow much time is needed to prepare workflow (approximately)?: 5 minutes\nHow much time is needed to complete experiments (approximately)?: Artifact evaluation takes approximately 12 hours, but depending on the models and datasets, it may take 30 seconds ~ 24 hours.\nPublicly available?: Yes\nCode licenses (if publicly available)?: Creative Commons Attribution 4.0 International, MIT License\nWorkflow framework used?: No\nArchived (provide DOI)?: Yes (10.5281/zenodo.12803583)"}, {"title": "C. Description", "content": "How to access:\nZenodo: LLMServingSim is published on Zenodo: https://doi.org/10.5281/zenodo.12803583\nGitHub: LLMServingSim is available on GitHub: https://github.com/casys-kaist/llmservingsim\nHardware dependencies: LLMServingSim requires an x86-64 architecture, and the simulation time may be affected by hardware differences. For similar simulation time results, we recommend using the hardware specified in Section VI-A.\nSoftware dependencies: LLMServingSim has been tested on Ubuntu 18.04 with Python 3.9 and requires gcc and g++ versions 7.5.0 or higher. Additionally, it requires the software prerequisites of ASTRA-Sim [74], Chakra [65], and Polymath [32]. To meet these software prerequisites, we use the latest version of Conda. We provide the instructions for Conda installation in Appendix D and README file. Also, it can be downloaded individually from the following link: https://repo.anaconda.com/archive/.\nData sets: We use ShareGPT [62] and Alpaca [69] datasets to generate arbitrary request trace.\nModels: We use GPT3 [5] and LLaMA [71] with model size of 7B to 175B for our evaluation. Their model architecture follows the decoder-based transformer model."}, {"title": "D. Installation", "content": "Clone the LLMServingSim repository.\n$ git clone --recurse-submodules https://\ngithub.com/casys-kaist/LLMServingSim.\ngit\n$ cd LLMServingSim\nConda install (optional).\n$ curl -0 https://repo.anaconda.com/\narchive/Anaconda3-2024.06-1-Linux-\nx86_64.sh\n$ bash Anaconda3-2024.06-1-Linux-x86_64.sh\nInstall dependencies.\n$ conda env create -p ./env -f ./\nenvironment.yml\n$ conda activate ./env\nBuild submodules.\n$ cd astra-sim\n$ ./build/astra_analytical/build.sh\n$ cd extern/graph_frontend/chakra\n$ pip install .\n$ cd ../../../../execution_engine/polymath\n$ pip install .\n$ cd ../.."}, {"title": "E. Experiment Workflow", "content": "The workflow of LLMServingSim is well illustrated in Section IV-A, particularly in Figure 4. To explain this in detail, the simulator first receives the system configuration and request trace from the user. Then, the scheduler selects requests that can be batched in each iteration using the KV cache information and creates a simulation plan that maps operators to each execution engine. Each engine simulates the operators using this simulation plan, and the results are combined into a single trace through operator scheduling. This trace goes through a graph converter to become an execution graph [65], which is then simulated at the system level by ASTRA-Sim [74]. Finally, the execution results are returned to the scheduler, which uses them to proceed to the next iteration. In this process, the simulator calculates the system's throughput and latency."}, {"title": "F. Evaluation and Expected Results", "content": "The evaluation conducted in this paper can be categorized into five parts, as described in Section VI. To facilitate the execution of these five evaluations, we have created five separate scripts, stored in the evaluation folder, each for running an individual experiment. We also provide a script (evaluation_all.sh) to run all of them at once.\nMove to evaluation folder.\n$ cd evaluation\nRun each evaluation one by one.\n$ ./evaluation1.sh\n$ ./evaluation2.sh\n$ ./evaluation5.sh\nRun all evaluation at once.\n$ ./evaluation_all.sh\nThe results of each script are stored in their respective evaluation folders. Each command within a script generates three files, including (1) text file containing the redirected standard output and (2) two TSV files storing throughput and simulation time."}, {"title": "G. Experiment Customization", "content": "Input configurations: LLMServingSim takes configuration files for hardware and network as input.\nNPU config: a json file that contains configurations of the NPU. It is located at execution_engine/codelets_src/ codelets/examples/genesys/configs/ folder.\nnetwork config: a json file that contains configurations of the system network topology. It is located at astra- sim/inputs/network/analytical/ folder.\nInput dataset: LLMServingSim takes LLM inference request datasets with various request patterns as input.\ndataset: a TSV file that contains the input token length", "parameters": "LLMServingSim has a total of 16 parameters for various simulation configurations. README file provides usage instructions and examples.\nmodel_name: Name of the LLM model. Default value is 'gpt2'.\nnpu_num: Number of NPUs in the system. Default value is 16.\nmax_batch: Maximum batch size. Default value is 0", "limit.\nbatch_delay": "Delay of batching. Default value is 0.\nscheduling: The method of scheduling. Default value is 'orca'", "81": ".", "nparallel": "The method of parallelism. There are three methods: 'pipeline'", "tensor": "and 'hybrid'. Default value is 'hybrid'.\nnpu_group: Number of NPU groups used in hybrid par- allelism. Default value is 1.\nLocal memory size of the NPU in GB. Default value is 40.\nkv_manage: The method of KV cache management. De- fault value is 'vllm'", "33": ".", "npim_type": "The method of using PIM. There are three methods: 'none'", "local": "and 'pool'. Default value is 'none'", "PIM.\nsub_batch": "The method of scheduling when using PIM. It is a flag that turns on for the sub-batch interle"}]}