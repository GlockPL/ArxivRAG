{"title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation", "authors": ["Zhaopeng Feng", "Jiayuan Su", "Jiamei Zheng", "Jiahan Ren", "Yan Zhang", "Jian Wu", "Hongwei Wang", "Zuozhu Liu"], "abstract": "Recent advancements in large language models (LLMs) have given rise to the LLM-as-a-judge paradigm, showcasing their potential to deliver human-like judgments. However, in the field of machine translation (MT) evaluation, current LLM-as-a-judge methods fall short of learned automatic metrics. In this paper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic LLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our findings demonstrate that M-MAD achieves significant advancements by (1) decoupling heuristic MQM criteria into distinct evaluation dimensions for fine-grained assessments; (2) employing multi-agent debates to harness the collaborative reasoning capabilities of LLMs; (3) synthesizing dimension-specific results into a final evaluation judgment to ensure robust and reliable outcomes. Comprehensive experiments show that M-MAD not only outperforms all existing LLM-as-a-judge methods but also competes with state-of-the-art reference-based automatic metrics, even when powered by a suboptimal model like GPT-40 mini. Detailed ablations and analysis highlight the superiority of our framework design, offering a fresh perspective for LLM-as-a-judge paradigm. Our code and data are publicly available at https://github.com/SU-JIAYUAN/M-MAD.", "sections": [{"title": "1 Introduction", "content": "Evaluating natural language generation systems has long been a challenging task. As the quality of these systems continues to improve, the need for robust and precise evaluation methods has become even more critical (Chang et al., 2024). In machine translation (MT) evaluation, learned automatic metrics such as MetricX (Juraska et al., 2023) and XCOMET (Guerreiro et al., 2024) have achieved state-of-the-art performance in benchmarks like the WMT Metrics shared task (Freitag et al., 2023). However, these model-based metrics require extensive human-annotated datasets for training and rely on reference translations for better accuracy.\nThe emerging LLM-as-a-judge paradigm offers a promising alternative by leveraging large language models (LLMs) to directly assess or critique generated content (Chiang and Lee, 2023; Zhang et al., 2023; Lee et al., 2023; Wu et al., 2024; Li et al., 2024; Pavlovic and Poesio, 2024). Early explorations in MT evaluation have shown potential (Kocmi and Federmann, 2023b; Fernandes et al., 2023; Xu et al., 2023; Leiter and Eger, 2024). For example, GEMBA-MQM (Kocmi and Federmann, 2023a) and EAPrompt (Lu et al., 2024) use coupled heuristic Multidimensional Quality Metrics (MQM) (Freitag et al., 2021) prompt to guide LLMs in identifying potential errors, which are aggregated into final quality scores via a weighted scoring system (Freitag et al., 2021). Despite their comparable performance to learned automatic metrics at the system-level, these methods face significant limitations: (1) they exhibit weaker segment-level performance, lagging behind state-of-the-art model-based automatic metrics (see in Table 1); (2) they rely on coupled heuristic MQM templates, which may introduce biases toward specific error types and further influence the quality score; (3) their single-agent single-step evaluation processes fail to fully exploit the reasoning and collaborative capabilities inherent in LLMs.\nIn human evaluation, addressing instability and bias often involves dividing tasks into distinct dimensions and fostering collaboration among multiple annotators (van der Lee et al., 2019; Karpinska et al., 2021). Similarly, in LLM-based multi-agent systems, multi-agent debate has proven effective in generating truthful and factual judgments (Chan et al., 2023; Khan et al., 2024; Liang et al., 2024; Du et al., 2024). These findings highlight that both human and LLM-based collaborative evaluation processes can achieve greater reliability compared to individual assessments.\nMotivated by these observations and insights, we propose the Multidimensional Multi-Agent Debating (M-MAD) framework for MT evaluation. M-MAD operates in three stages: (1) decomposing the heuristic MQM annotation guideline into distinct dimensions for independent LLM-as-a-judge assessments; (2) conducting multi-agent debates within each dimension, harnessing LLMs' inherent knowledge, reasoning, and collaborative abilities; (3) synthesizing the debated outcomes through a final judge agent to produce a comprehensive evaluation judgment. Comprehensive experiments demonstrate that M-MAD not only maintains the strong system-level performance but also significantly enhances segment-level performance compared to existing LLM-as-a-judge approaches and most automatic metrics. Moreover, even with a suboptimal LLM like GPT-40 mini, M-MAD achieves performance comparable to state-of-the-art reference-based automatic metrics.\nOur contributions are summarized as follows:\n\u2022 We introduce the first LLM-based systematic Multidimensional Multi-Agent Debating (M-MAD) framework for MT evaluation. By decoupling the MQM paradigm into distinct evaluation dimensions and integrating multi-agent debates for each dimension, M-MAD enhances the robustness, precision, and reliability of LLM-as-a-judge approaches.\n\u2022 M-MAD achieves significant improvements in segment-level performance, surpassing existing LLM-as-a-judge methods while maintaining top-tier system-level accuracy. Its alignment with human judgments outperforms most advanced model-based automatic metrics. Notably, even when powered by a suboptimal LLM (GPT-40"}, {"title": "2 Multidimensional Multi-Agent Debate Framework", "content": "In this section, we first introduce the background of the MQM paradigm (Freitag et al., 2021). We then elaborate on the principal stages in M-MAD, including dimension partition, multi-agent debate, and final judgment, as shown in Figure 1.", "subsections": [{"title": "2.1 Preliminary: Multidimensional Quality Metrics Evaluation Method", "content": "The MQM framework is a flexible human evaluation method designed to assess and categorize translation errors (Burchardt, 2013; Freitag et al., 2021). Unlike traditional Direct Assessment (DA), which relies on annotators assigning scores on a 0-100 scale, MQM focuses on identifying specific errors and classifying them based on severity and category. Annotators evaluate translations segment by segment, taking into account the context of the entire document. Each error is assigned a severity level (major or minor) and categorized by type. Spans without errors are marked with neutral severity and no category. This approach provides a more granular and structured evaluation compared to DA.\nThe MQM framework derives quality scores automatically by applying a weighted scheme to the identified errors, considering both their severity and category. Segment-level scores range from 0 (perfect translation) to 25 (worst possible translation), with the overall score calculated as the average across all annotators. For some applications, such as correlating with learned metrics, the scores are negated for consistency. MQM has been shown to align more closely with human judgments than DA, offering a robust and interpretable framework for evaluating MT quality (Freitag et al., 2022; Zhao et al., 2024). More details about MQM are provided in Appendix C."}, {"title": "2.2 Multidimensional Multi-Agent Debate", "content": "In the M-MAD framework, given a translation pair (x, y), there are d evaluation dimensions D = {Di}i=1d, where each dimension corresponds to an error category. For each dimension, n agents A = {Ai}i=1n, each powered by an LLM Li, engaged in debating. The debating process lasts for a maximum of R rounds. During each round r, an agent Ai generates a response (si, r) \u223c Ai(si | H, P), where H is the history of previous messages visible to the agent, and P is the prompt. At the end of the debate, the debating group provides their final viewpoint for its dimension, denoted as V(Di). After evaluating all dimensions, the final judge agent J gathers all viewpoints V = {V(Di)}i=1d and synthesizes them into an overall evaluation O(x, y).\nStage 1: Dimension Partition. We first decouple the pre-defined MQM guideline into four distinct evaluation dimensions (d = 4): Accuracy, Fluency, Style and Terminology. We exclude the rare and easily identifiable error type non-translation as well as locale convention, which is not related to translation errors, to ensure a fair comparison with GEMBA-MQM (Kocmi and Federmann, 2023a). For each dimension, an agent A0 performs the initial evaluation s0 \u223c A0(s0 | P0), where P0 is the evaluation template designed for this dimension to identify potential error spans, classify them into subcategories, and assign severity.\nStage 2: Multi-Agent Debate. In this stage, we employ \"Pro-Con Debate,\" a formal discussion format where participants present arguments for or against a topic (Johnson and Johnson, 1985). For each evaluation dimension, we assign a two-agent debating group (n = 2). Starting with the initial evaluation s0, if an error is detected, agent A1 supports s0, while agent A2 holds the opposite. In the first round, A1 generates a statement (s1,1) \u223c A1(s1 | H, P), where H includes the initial standpoints of both sides, and P is the debating prompt. A1 may provide explanations, reinforce its position, or switch sides. Agent A2 then follows the same process to generate (s2, 1). These statements, (s1,1) and (s2, 1), are added to the history H as context for subsequent rounds. After each round, a consensus checker determines if the agents have reached an agreement. If consensus is achieved, the resulting statement se becomes the viewpoint V(Di) for this dimension, and the debate ends. If consensus is not reached after R rounds, the supportive side of s0 is taken as V(Di). We conduct extensive experiments to evaluate different debating strategies and their impact in Section 4.3.\nStage 3: Final Judgement. This stage consists of two steps: viewpoint synthesis and quality score calculation. First, the final judge agent J gathers the set of viewpoints V = {V(Di)}i=1d obtained in Stage 2. Agent J evaluates the validity of each viewpoint, removes redundant information, and synthesizes them into an overall evaluation O(x, y). Next, O(x, y) is used to calculate the translation quality score by counting the number of major and minor errors and applying the following MQM score formula:\nMQM score = \u2212wmajornmajor \u2212wminornminor (1)\nwhere nmajor and nminor represent the counts of major and minor errors, respectively, while wmajor and wminor denote their corresponding severity weights. In line with GEMBA-MQM, EAprompt and WMT Metric Shared Task (Freitag et al., 2023), we set wmajor = 5 and wminor = 1."}]}, {"title": "3 Experiments", "content": null, "subsections": [{"title": "3.1 Experimental Setup", "content": "Datasets. Our experiments utilize MQM ratings (Burchardt, 2013; Freitag et al., 2021) from"}, {"title": "3.2 Main Results", "content": "Improvements over LLM-as-a-judge Methods. Table 2 demonstrates that M-MAD significantly outperforms existing LLM-as-a-judge frameworks for MT evaluation. On the average Meta score across ZH-EN and EN-DE, M-MAD achieves a 3.8% improvement over GEMBA-MQM and a 5.4% improvement over EAPrompt. While maintaining the consistently strong system-level performance, M-MAD achieves notable improvements at the segment level. For instance, on EN-"}]}, {"title": "4 Ablation and Analysis", "content": "We conduct multiple ablations to demonstrate the impact of each component of M-MAD and perform a detailed analysis of our approach.", "subsections": [{"title": "4.1 Decoupled Multidimensional Design Brings Significant Improvements", "content": "As shown in Table 3, Stage 1 (Dimension Partition) contributes the most significant improvements in M-MAD framework. When we remove it from M-MAD, the overall performance dropped by 5.1%. This aligns with the observation of GEMBA-MQM and reinforces our earlier assertion that previous LLM-as-a-judge methods, which rely on coupled multidimensional templates, are suboptimal. Decoupling multidimensional evaluation into separate dimension agents maximizes the potential of each dimension. Dimension partition also provides a more centralized debating focus for Stage 2."}, {"title": "4.2 Segment-Level Improvements from More Accurate Error Span Predictions", "content": "We evaluate M-MAD\u2019s performance on error span prediction in the WMT 23 ZH-EN Metrics Shared"}, {"title": "4.3 More Precise Debating Approaches Yields Better Evaluation Results", "content": "Before implementing dimension partitioning, we adapted existing off-the-shelf debating frameworks for MT evaluation based on the coupled evaluation results from GEMBA-MQM. However, as shown in Table 5, directly debating coupled multidimensional results leads to a significant performance drop. We hypothesize that this occurs because agents struggle to identify specific focal points for the debate when addressing multiple dimensions simultaneously. To isolate the influence of multidimensional coupling, we conduct a controlled experiment after Stage 1. In this setup, debaters are allowed to freely debate within distinct dimensions (referred to as \"Entirety\" in Table 6). Interestingly, the results still show a substantial decline in performance, supporting our hypothesis: unfocused or unstructured debates do not yield positive gains. To identify the most effective debating strategy, we design and test several structured approaches following dimension partitioning, including:\nConsensus: Two debaters argue over the severity of errors, with one supporting the initial evaluation and the other opposes it. The debate ends as soon as consensus is reached. If no agreement is reached, the original evaluation is retained.\nDeliberation: Debaters present opposing arguments over multiple rounds until the maximum round limit is reached. A judge reviews the transcript and makes the final decision based on the presented arguments.\nInteractive Review: After each round, a reviewer introduces questions to the debaters, who must rebut each other\u2019s arguments and respond to the questions. A judge evaluates the complete transcript and determines the final evaluation.\nConsultancy Review: A debater defends the initial evaluation, engaging directly with a reviewer over multiple rounds. The reviewer collaborates with a judge to determine the final outcome.\nOur experimental results in Table 7 demonstrate that debates conducted within a specific, well-defined dimension yield significantly more stable and reliable outcomes compared to coupled multidimensional debates. Furthermore, the Consensus strategy outperforms review-based questioning ap-"}, {"title": "4.4 Performance Convergence as Debating Rounds Progress", "content": "To investigate the impact of debating rounds on M-MAD\u2019s performance, we conduct experiments with up to five rounds of multi-agent debate. As shown in Figure 3, system-level performance fluctuates across rounds but consistently peaks at round 3. Segment-level performance, in contrast, exhibits steady improvement during the first three rounds before plateauing. These findings suggest that three rounds of debate strike a balance between refining evaluations and maintaining stability."}, {"title": "4.5 Case Study", "content": "We provide case studies in Table 8 and Table 10 to illustrate M-MAD\u2019s effectiveness and limitations."}]}, {"title": "5 Related Work", "content": "The success of learned MT metrics (Rei et al., 2020; Sellam et al., 2020; Rei et al., 2022; Perrella et al., 2022; Naskar et al., 2023; Gowda et al., 2023; Juraska et al., 2023; Guerreiro et al., 2024), which fine-tune neural network models pretrained on large amounts of data, highlights the importance of leveraging transfer learning to achieve metrics with higher correlation to human judgments. With"}, {"title": "6 Conclusion", "content": "In this work, we introduce M-MAD, an advanced multi-agent collaboration framework that elevates LLM-as-a-judge methods for MT evaluation. M-MAD employs a three-stage workflow that decouples MQM criteria into distinct dimensions, conducts multi-agent debates within each dimension, and synthesizes the outcomes into a final robust evaluation. Experimental results demonstrate that M-MAD not only outperforms existing LLM-as-a-judge methods but is also competitive with the advanced reference-free and reference-based automatic metrics. Detailed ablations and analyses reveal the mechanisms behind M-MAD\u2019s success, highlight its ability to leverage fine-grained evaluations and collaborative reasoning to enhance reliability and precision. We believe M-MAD offers a groundbreaking approach for advancing MT evaluation and sets the stage for developing next-generation LLM-as-a-judge frameworks."}, {"title": "Limitations", "content": "Due to the meta-evaluation\u2019s unique complexity, the token consumption required by M-MAD is substantial. As a result, we were unable to afford cutting-edge models like GPT-40, 01, or Claude-3.5 Sonnet for our experiments. This constraint limits our ability to explore the upper performance bounds of the framework with these advanced models. Our current research primarily focuses on homogeneous groups of LLMs, which might not capture the full potential of multi-agent collaboration. Future research could explore heterogeneous groups of LLMs, where stronger and weaker, close source and open source models cooperate, offering interesting insights into how various models with different strengths and weaknesses can complement each other."}]}