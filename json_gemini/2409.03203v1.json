{"title": "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "authors": ["Zhuowei Chen", "Lianxi Wang", "Yuben Wu", "Xinfeng Liao", "Yujia Tian", "Junyang Zhong"], "abstract": "Sentiment classification (SC) often suffers\nfrom low-resource challenges such as domain-\nspecific contexts, imbalanced label distribu-\ntions, and few-shot scenarios. The potential of\nthe diffusion language model (LM) for textual\ndata augmentation (DA) remains unexplored,\nmoreover, textual DA methods struggle to bal-\nance the diversity and consistency of new sam-\nples. Most DA methods either perform log-\nical modifications or rephrase less important\ntokens in the original sequence with the lan-\nguage model. In the context of SC, strong\nemotional tokens could act critically on the\nsentiment of the whole sequence. Therefore,\ncontrary to rephrasing less important context,\nwe propose DiffusionCLS to leverage a dif-\nfusion LM to capture in-domain knowledge\nand generate pseudo samples by reconstruct-\ning strong label-related tokens. This approach\nensures a balance between consistency and di-\nversity, avoiding the introduction of noise and\naugmenting crucial features of datasets. Dif-\nfusionCLS also comprises a Noise-Resistant\nTraining objective to help the model generalize.\nExperiments demonstrate the effectiveness of\nour method in various low-resource scenarios\nincluding domain-specific and domain-general\nproblems. Ablation studies confirm the effec-\ntiveness of our framework's modules, and visu-\nalization studies highlight optimal deployment\nconditions, reinforcing our conclusions.", "sections": [{"title": "1 Introduction", "content": "Sentiment classification is a crucial application of\ntext classification (TC) in Natural Language Pro-\ncessing (NLP) and can play a crucial role in multi-\nple areas. However, NLP applications in domain-\nspecific scenarios, such as disasters and pandemics,\noften meet with low-resource conditions, especially\ndomain-specific problems, imbalance data distribu-\ntion, and data deficiency (Sedinkina et al., 2022;\nLakshmi and Velmurugan, 2023; Nabil et al., 2023;\nGatto and Preum, 2023). Recently, the birth of\npre-trained language models (PLMs) and large lan-\nguage models (LLMs) have advanced the NLP field,\ngiving birth to numerous downstream models based\non them. On the one hand, these PLMs take the\nmodels to a new height of performance, on the other\nhand, since these models are highly data-hungry,\nthey struggle to perform satisfactorily on most tasks\nunder noisy, data-sparse and low-resource condi-\ntions (Patwa et al., 2024; Chen et al., 2023b; Wang\net al., 2024; Yu et al., 2023)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Low-Resource Text Classification", "content": "Motivated by the observation that data is often\nscarce in specific domains or emergent applica-\ntion scenarios, low-resource TC (Chen et al., 2018)\nhas recently attracted considerable attention. Low-\nresource TC involves effectively categorizing text\nin scenarios where data is scarce or limited. Goudjil\net al. (2018) and Tan et al. (2019) have explored sev-\neral methods for low-resource TC, which mainly\ninvolve traditional machine learning techniques to\nincrease data quantity and diversity.\nRecently, since the studies by Lan et al. (2019)\nand Sun et al. (2020) demonstrated the impressive\nperformance of PLMs across various NLP tasks,\na significant amount of work has leaned towards\nusing PLMs to address low-resource TC problems\n(Wen and Fang, 2023; Ogueji et al., 2021; Liu et al.,\n2019; Devlin et al., 2018). However, PLMs re-\nquires amounts of annotated samples for finetuning,\ndata-sparce significantly impacts models\u2019 perfor-\nmances and DA could mitigate such problems."}, {"title": "2.2 Textual Data Augmentation", "content": "To address low-resource challenges, various data\naugmentation methods have been proposed, includ-\ning Easy-Data-Augmentation (EDA) (Wei and Zou,\n2019), Back-Translation (BT) (Shleifer, 2019), and\nCBERT (Wu et al., 2019). However, these methods,\nrelying on logical replacements and external knowl-\nedge, often introduce out-domain knowledge and\ndomain inconsistency. Moreover, these methods\nfocus only on a specific original input, resulting in\nlimited diversity.\nAnother type of data augmentation method in-\ncludes representation augmentation approaches."}, {"title": "3 Methodology", "content": "Sentiment classification models often overfit and\nlack generalization due to sample deficiency. To\naddress this, we propose DiffusionCLS, consist-\ning of Label-Aware Noise Schedule, Label-Aware\nPrompting, Conditional Sample Generation, and\nNoise-Resistant Training. A diffusion LM-based\nsample generator is integrated to generate new sam-\nples from the original dataset, enhancing TC model\nperformance.\nFigure 1 illustrates DiffusionCLS. The diffusion\nLM-based sample generator generates new sam-\nples for data augmentation, while the TC model\nis trained for the specific TC task. Label-Aware\nPrompting and Label-Aware Noise Schedule are\ncrucial for training the sample generator, and Con-\nditional Sample Generation and Noise-Resistant\nTraining contribute to the training of the TC model."}, {"title": "3.1 Sample Generator", "content": "To generate usable samples for further TC model\ntraining, there are two crucial rules of success to\nsatisfy, diversity and consistency. Therefore, we\nexpect the generated samples to be as diverse as\npossible with consistency to the TC label and origi-\nnal domain simultaneously. However, higher diver-\nsity also leads to a higher difficulty in maintaining\nconsistency.\nAs He et al. (2023) excavated the potential of\ncombining diffusion models with LMs for sequence\ngeneration, we built the sample generator from the\ndiscrete diffusion model scratch. Precisely, we de-\nsign the Label-Aware Noise Schedule for the diffu-\nsion LM, which helps the model to generate diverse\nand consistent samples. Additionally, we integrate\nLabel-Aware Prompting into the training regime,\nenabling the model to grasp label-specific knowl-\nedge, subsequently serving as the guiding condition\nfor sample generation. These two modules help the"}, {"title": "3.1.1 Label-Aware Noise Schedule", "content": "A proper algorithm of noise schedule could guide\nthe diffusion LM to capture more accurate seman-\ntic relations. Moreover, the effectiveness of time-\nagnostic decoding has been demonstrated, indicat-\ning that incorporating implicit time information in\nthe noise schedule process is effective (Ho et al.,\n2020; Nichol and Dhariwal, 2021; He et al., 2023).\nSince the generated samples are also expected to\nstay consistent with the TC label and the original\ndomain, we proposed Label-Aware Noise Sched-\nule.\nThe Label-Aware Noise Schedule begins by in-\nte grating a proxy model that has been fine-tuned\nfor the TC task. This proxy model allows for the\ndetermination of the importance of each token in\nthe TC process, quantified through attention scores\nbetween the [CLS] token and other tokens, which\nare derived from the last layer of proxy model and\ncalculated as follows.\n$w_{i} = \\frac{1}{H} \\sum_{h=1}^{H} s_{i}^{h}$ (1)\nwhere $s_{i}^{h}$ represents the i-th token attention score in\nthe h-th attention head, and $w_{i}$ denotes the weight\nmeasuring the importance of the i-th token.\nMotivated by He et al. (2023)'s DiffusionBERT,\nwe incorporates absorbing state in the LM noise\nschedule. In our method, during the masking transi-\ntion procedure, each token in the sequence remains\nunchanged or transitions to [MASK] with a certain\nprobability. The transition probability of token i at\nstep t can be denoted as:\n$q_{i}^{t} = 1 - \\frac{t}{T} \\cdot \\lambda \\cdot S(t) \\cdot w_{i},$ (2)\n$S(t) = sin \\frac{\\tau\\pi}{T},$ (3)\nwhere q represents the probability that a token is\nbeing masked, and T denotes the total step number.\n\u039b is introduced to control the impact of $w_{i}$, as a\nhyper-parameter.\nBy introducing strong label-related $w_{i}$, the dif-\nfusion model is guided to recover the tokens with\nlower weight first, then recover the tokens that are\nstrongly related to the classification task later.\nThe probability of a token being masked is tied\nto its attention score relative to the [CLS] token,"}, {"title": "3.1.2 Label-Aware Prompting", "content": "However, such a noise schedule still poses a chal-\nlenge to the conditional generation process. The\ndiversity-consistency trade-off becomes more in-\ntense when important tokens are masked. With\nfewer unmasked tokens provided, the model natu-\nrally has a higher possibility of generating tokens\nthat would break the label consistency."}, {"title": "3.2 Text Classification Model", "content": "In this work, we adopt encoder-based PLM as our\nbackbone model and finetuned them for the TC\ntask. Though diffusion LM is strong enough to\nmaintain consistency and diversity at the same time,\nthe introduction of pseudo samples unavoidably\nintroduced noise data to the training of the TC\nmodel. To mitigate such a problem, we design a\ncontrastive learning-based noise-resistant training\nmethod, further improving the scalability of the\nproposed DiffusionCLS."}, {"title": "3.2.1 Reflective Conditional Sample Generation", "content": "We implement label prompting as a prior for the\nsample generator, akin to Label-Aware Prompting.\nAdditionally, we introduce a novel reflective condi-\ntional sample generation module within the training\nloop of the TC model. This module dynamically\ngenerates masked sequences for the sample gen-\nerator, integrating insights from label annotations\nand attention scores derived from the TC model\nsimultaneously, calculating weights for each token\nwith Eq.1.\nHowever, generating pseudo samples from vary-\ning degrees of masking will result in various de-\ngrees of context replacement flexibility, thus im-\npacting the consistency and diversity of pseudo\nsamples. Essentially, providing a proper amount of\nconditional information will lead to plausible sam-\nples. Thus, we perform multiple experiments to\nsearch for the best condition, which will be further\ndiscussed in Section 4.5."}, {"title": "3.2.2 Noise-Resistant Training", "content": "The introduction of pseudo samples unavoidably\nintroduced noise data to the training of the TC\nmodel. To mitigate such a problem, we design a\ncontrastive learning-based noise-resistant training\nmethod, further improving the scalability of the\nproposed DiffusionCLS."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Baselines", "content": "To measure the efficiency of the propose Dif-\nfusionCLS, we utilize both domain-specific and\ndomain-general datasets comprising samples in\nChinese, English, Arabic, French, and Span-\nish. Namely, domain-specific SMP2020-EWECT\u00b9,\nIndia-COVID-X\u00b2, SenWave (Yang et al., 2020),\nand domain-general SST-2 (Maas et al., 2011). Ad-\nditionally, to compare with the most cutting-edge\nlow-resource TC methods, we utilize SST-2 dataset\nto evaluate our method in the few-shot setting.\nDataset statistic and descriptions are demonstrated\nin Appendix A.\nTo thoroughly explore and validate the capabili-\nties of DiffusionCLS, we compare our method with\na range of data augmentation techniques, from clas-\nsic approaches to the latest advancements for low-\nresource TC. Specifically, we take Resample, Back\nTranslation (Shleifer, 2019), Easy Data Augmenta-\ntion (EDA) (Wei and Zou, 2019), SFT GPT-2 refer-\nenced to LAMBADA (Anaby-Tavor et al., 2020),\nAEDA (Karimi et al., 2021), and GENIUS (Guo\net al., 2022) as our baselines. Also, we compare\nour method in the few-shot setting with a couple of\ncutting-edge methods, namely, SSMBA (Ng et al.,\n2020), ALP (Kim et al., 2022), and SE (Zheng\net al., 2023). More details of our baselines are\ndemonstrated in Appendix B."}, {"title": "4.2 Experiment Setup", "content": "We set up two experimental modes, entire data\nmode and partial data mode, to reveal the effective-\nness of our method in different scenarios. Since\nsevere imbalanced distribution challenges existed,\nwe take macro-F1 and accuracy as our major evalu-\nation metrics.\nAlso, we conduct 5-shot and 10-shot experi-\nments on SST-2 to investigate the performance of\nDiffusionCLS in extreme low-resource conditions.\nFor evaluation, we use accuracy as the metric and\nreport the average results over three random seeds\nto minimize the effects of stochasticity.\nAdditionally, we setup comparisons between\nvariant augmentation policies, namely, generate\nnew samples until the dataset distribution is bal-\nanced, and generate n pseudo samples for each\nsample (n-samples-each), which denoted as B/D\nand G/E in Table 2, and n=4 in our experiments."}, {"title": "4.3 Results and Analysis", "content": "The results of entire-data-setting experiments on\ndatasets SMP2020-EWECT and India-COVID-X\nare mainly demonstrated in Table 2, which we com-\npare DiffusionCLS with other strong DA baselines.\nFor experiments with partial-data and few-shot set-\ntings, results are majorly showed in Figure 5 and\nTable 9.\nResults in Entire Data Mode. As shown in\nTable 2, in general, the proposed DiffusionCLS\noutperforms most DA methods on domain-specific\ndataset SMP2020-EWECT and India-COVID-X,\nespecially under G/E policy. Notably, the Diffu-\nsionCLS positively impacts the TC model across\nall policies and datasets, which most baselines fail.\nOur method excels in dealing with the chal-\nlenge of uneven datasets. Under severe uneven\ndistribution and domain-specific scenarios, i.e., the\ndataset SMP2020-EWECT, most DA baselines fail\nto impact the classification model positively ex-\ncept DiffusionCLS, which achieves the best perfor-\nmance. Also, our method achieves competitive per-\nformance under data-sparse and domain-specific\nscenarios, i.e., in the dataset India-COVID-X, most\nDA methods bring improvement to the classifica-\ntion model, and our DiffusionCLS ranked second.\nRule-based DA methods such as EDA, rather\nlack diversity bringing overfit problems or solely\nrelying on out-domain knowledge therefore break-\ning consistency and impacting the task model neg-\natively. For model-based methods, though most\nmethods significantly increase the diversity of the\ngenerated samples, they rather generate samples\nsolely depending on pretraining knowledge and in-\ncontext-learning techniques or generate samples\nonly conditioned on the label itself, posing a chal-\nlenge of maintaining consistency.\nResults in Partial Data Mode and Few-shot\nSettings. As shown in Figure 5 and Table 9 in\nAppendix C, the proposed DiffusionCLS method\nconsistently improves the classification model. No-\ntably, DiffusionCLS matches the PLM baseline\nperformance on the Arabic SenWave dataset using\nonly 50% of the data samples.\nWe also compare DiffusionCLS with the most\ncutting-edge few-shot methods on SST-2 dataset\nunder 5-shot and 10-shot setting, the results are\nshown in Table 3. Though our method fails to\nsurpass all few-shot baselines, it still achieves com-"}, {"title": "4.4 Ablation Study", "content": "To validate the effectiveness of modules in the pro-\nposed DiffusionCLS, we conduct ablation stud-\nies to study the impacts of each module. Table\n4 presents the results of the ablation experiments.\nIn each row of the experiment results, one of the\nmodules in DiffusionCLS has been removed for\ndiscussion, except D.A., which removes all mod-\nules related to the generator and only applies noise-\nresistance training.\nOverall, all modules in the proposed Diffusion-\nCLS works positively to the TC model, compared\nwith the pure PLM model, the application of Dif-\nfusionCLS leads to 2.11% and 3.66% rises in F1\nvalues on dataset SMP2020-EWECT and India-\nCOVID-X respectively.\nThe results of ablation studies further validate\nthat the Label-Aware Prompting effectively im-\nproves the quality of pseudo samples. Also, the\nNoise-Resistant Training reduces the impact of\nnoise pseudo samples."}, {"title": "4.5 Discussions and Visualizations", "content": "Generating pseudo samples from more masked to-\nkens provides more flexibility for generation and\ntends to result in more diverse samples, however, it\nwill enlarge the possibility of breaking the consis-\ntency since less information is provided.\nTo analyze the optimal amount of masks for\ngenerating new pseudo samples, we conduct ex-\nperiments on the India-COVID-X dataset. During\nconditional sample generation, we gather masked\nsequences from 32 noise-adding steps, group them\ninto sets of eight, and evaluate how varying mask-\ning levels impact the model's performance.\nAs shown in Figure 6, our observations indi-\ncate a unimodal trend. The model's performance\nimproves with increased masking, peaks at the 4th\ngroup, and then declines with further masking. This\nreflects the diversity-consistency trade-off, more\nmasked tokens create more diverse samples, but\noverly diverse samples may be inconsistent with\noriginal labels or domain.\nTo explore the relationship between generated\npseudo samples and original samples, we con-\nduct 2D t-SNE visualization. Figure 7 shows that"}, {"title": "5 Conclusion", "content": "In this work, we propose DiffusionCLS, a novel ap-\nproach tackling SC challenges under low-resource\nconditions, especially in domain-specific and un-\neven distribution scenarios. Utilizing a diffusion\nLM, DiffusionCLS captures in-domain knowledge\nto generate high-quality pseudo samples maintain-\ning both diversity and consistency. This method\nsurpasses various kinds of data augmentation tech-\nniques. Our experiments demonstrate that Diffu-\nsionCLS significantly enhances SC performance\nacross various domain-specific and multilingual\ndatasets. Ablation and visualization studies further\nvalidate our approach, emphasizing the importance\nof balancing diversity and consistency in pseudo\nsamples. DiffusionCLS presents a robust solution\nfor data augmentation in low-resource NLP applica-\ntions, paving a promising path for future research."}, {"title": "Limitations", "content": "Like most model-based data augmentation meth-\nods, the performance of data generators is also\nlimited in extreme low-resource scenarios. This\nlimitation persists because the model still necessi-\ntates training on the training data, even with the\npotential expansion of the dataset through the in-\nclusion of unlabeled data, data deficiency impacts\nthe data generator negatively."}]}