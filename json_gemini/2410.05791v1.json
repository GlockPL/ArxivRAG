{"title": "F\u00fcrElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance", "authors": ["RUOCHENG WANG", "PEI XU", "HAOCHEN SHI", "ELIZABETH SCHUMANN", "C. KAREN LIU"], "abstract": "Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Physically synthesizing human motion has a wide range of applications in character animation, embodied AI, AR/VR, robotics, and biomechanics. Researchers have made great strides in simulating functional and realistic human movements which enable digital agents to physically navigate and interact with environments while maintaining balance. As the application domain expands, the next frontier in human motion synthesis is to create digital agents that not only achieve motion tasks, but also exhibit elite-level athletic techniques and musical precision, comparable to the peak performance of human athletes and musicians. In this work, we take the first step toward synthesis of human peak performance through the lens of the movement of elite pianists.\nPiano playing is a demanding motor skill that requires impeccable precision in finger control to press the correct keys at the correct time, agile coordination to press multiple keys simultaneously, and remarkable dexterity to fluidly play long sequences while anticipating upcoming notes. Previous works on simulating piano playing motions either rely on human-annotated fingering information (which finger to press which key) [Zakka et al. 2023] or are limited to scenarios involving easier compositions [Xu et al. 2022; Zhu et al. 2013]. We believe that a better model requires a deeper understanding of how humans play the piano. However, there is a significant shortfall in large-scale datasets that adequately capture the diversity and complexity of piano performances.\nTo address this gap, we design and build a comprehensive, non-intrusive data capture pipeline to record the 3D hand motions of pianists during their natural performances. This pipeline employs a markerless setup, where multi-view videos are processed using"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Music2Motion", "content": "The problem of generating motions following music has been extensively studied in recent years. Alexanderson et al. [2023]; Li et al. [2021]; Tseng et al. [2023] tackle the problems of generating whole-body dancing motions from input music using diffusion models. Another line of research trains neural networks to generate upper-body motions of musicians from the audio of various instruments [Chen et al. 2023a; Kao and Su 2020; Li et al. 2018; Liu et al. 2020; Shlizerman et al. 2018]. These works typically utilize pose estimation models to estimate 3D joint locations only from"}, {"title": "2.2 Physics-Based Dexterous Control", "content": "Studying the control strategy for physically simulated dexterous hands has wide applications in computer graphics, robotics, and biomechanics. Traditional approaches usually rely on trajectory optimization and/or human-designed heuristic rules to perform control [Chen et al. 2023b; Liu 2008, 2009; Mordatch et al. 2012; Wang et al. 2013; Ye and Liu 2012]. Most recent works on physics-based dexterous control only focus on single-hand scenarios and do not have high precision requirements[Andrychowicz et al. 2020; Liu 2009; Xie et al. 2023; Yang et al. 2022; Zhang et al. 2021; Zhao et al. 2013]. In this study, we focus on piano playing, a task that requires simultaneous bimanual control with exceptional temporal and spatial precision.\nPiano playing is a common but intricate physical activity in daily life. Introducing physics can help generate physically feasible motions for piano playing. Algorithms are proposed to train policies to play piano in simulations using anthropomorphic robot hands [Xu et al. 2022; Zakka et al. 2023] via reinforcement learning. Due to the complexity of the task, Xu et al. [2022] only considers one hand playing on a simplified piano. Zakka et al. [2023] leverages human annotated fingering information (which finger should press which key) to facilitate policy training. Our proposed pipeline, once trained on our large-scale dataset, can play unseen pieces without any additional annotation.\nOur approach follows previous work leveraging reinforcement learning to synthesize motions under the framework of imitation learning [Merel et al. 2017; Peng et al. 2022, 2021; Xu and Karamouzas 2021; Xu et al. 2023]. Though impressive results are achieved in generating realistic motions by imitation learning, it is still a challenging problem to perform learning efficiently from a very large set of reference motions. To better utilize our collected large set of piano-playing motions, we address the problem by developing a hybrid approach to generate and retrieve motions for the policy to synthesize."}, {"title": "3 DATASET", "content": "To study hand motions during piano playing, we collect a large-scale dataset, F\u00fcrElise, with approximately 10 hours of 3D hand motions paired with synchronized audio. In this section, we will first elaborate on the data capture and processing pipeline and provide an analysis of the dataset."}, {"title": "3.1 Data Capture", "content": "We aim to collect a large-scale dataset of piano playing motion performed by professional and conservatory-level pianists with minimal intrusion.\nDevice Setup. We record the data in a typical piano studio familiar to the performers, as shown in Figure 3. To minimize the influence of capture device, we design a markerless setup using multiview RGB cameras. Five calibrated GoPro cameras are placed around a grand piano to record synchronized videos and audio with 59.94 FPS. All the videos have a resolution of 3840\u00d72160. The grand piano is a Yamaha Disklavier DS7X ENPRO, which has a built-in recorder to record the key and pedal pressing events during the performance with high precision in MIDI format, from which the original audio with high fidelity can be reproduced.\nVision-based Motion Reconstruction. Figure 2 summarizes the motion reconstruction process. We first use the state-of-the-art pose estimation model HaMeR [Pavlakos et al. 2024] to predict the hand pose $K_{2D} \\in R^{N \\times 5 \\times 2 \\times 21 \\times 2}$, which are the 2D locations of 21 joints on each hand from all 5 camera views for a sequence of N frames. While HaMeR can generate 3D meshes of MANO hands [Romero et al. 2017] in the camera space, we found that the predicted depths are not usable due to severe inaccuracy. As such, we only leverage the projected 2D keypoints from HaMeR and compute 3D locations of each joint $K_{3D} \\in R^{N \\times 2 \\times 21 \\times 3}$ via triangulation. RANSAC is used to filter out occluded keypoints, while a Butterworth filter is applied to every joint to enhance temporal smoothness, since HaMeR only considers one frame at a time. Next, we fit MANO hand parameters $\\Theta = \\{0, \\beta, t\\}$ to obtain 3D hand meshes for every frame, where $\\theta \\in R^{N \\times 2 \\times 16 \\times 3}$, $\\beta \\in R^{2 \\times 45}$, $t \\in t^{N \\times 2 \\times 3}$ are the joint rotations, shape parameters and global translations of the two hands. The shape parameters are computed with extra hand calibration videos. Other parameters are optimized by minimizing the mean-squared error between the triangulated joint locations and MANO hand joint locations.\nMIDI-based Motion Refinement. Vision-based motion reconstruction achieves reasonable results, but visible artifacts such as incorrect key-pressing or missing keys are quite common in the reconstructed motion. To improve the quality, we utilize the key-press"}, {"title": "3.2 Dataset Analysis", "content": "Data statistics. We collect and reconstruct a total of 10 hours of 3D hand motions paired with synchronized MIDI. 8 male and 7 female elite pianists contribute a total of 153 classical compositions in various genres.\nQuality Evaluation. Following Zakka et al. [2023], we use precision, recall and F1 to quantitatively evaluate the quality of our reconstructed motions according to the recorded MIDI:\n$Precision_{i} = \\frac{TP_{i}}{FP_{i} + TP_{i}}$ $Recall = \\frac{TP_{i}}{TP_{i} + FN_{i}}$\n$F1_{i} = \\frac{2 Precision_{i} Recall_{i}}{Precision_{i} + Recall_{i}}$\n(1)\nwhere $TP_{i}$ computes the number of keys that are correctly pressed, $FP_{i}$ computes the number of keys that are wrongly pressed, and $FN_{i}$ computes the number of keys that the motion failed to press. We do this for every frame i and average over all the frames in the dataset. To extract the pressed keys from reconstructed motions, similar to the IK procedure mentioned earlier, any fingertip horizontally over a key and below a preset threshold is treated as pressing the key. Using this evaluation protocol, we got a precision of 88.55, a recall of 92.53, and an F1 of 86.49 on the whole dataset. We also visualize our reconstructed motion and include the audio of the extracted MIDI in the supplementary video.\nQualitative Examples. To demonstrate the diversity of motions in our dataset, we show examples of various primitive piano playing skills [Neuhaus 2008] in Figure 4."}, {"title": "4 PLAY PIANO WITH PHYSICALLY SIMULATED HANDS", "content": "Leveraging the collected dataset, we aim to train a policy that controls two physically simulated hands in concert to play a given piece of music. Thus, the input to our method is a musical score represented as a list of notes ${O_{i} = (t_{start}, t_{end}, p_{i}) | i \\in \\{1, 2, ..., n\\}\\}$, where $t_{start}, t_{end} \\in R$ is the start and end time of the note, and $p_{i} \\in \\{1,..., 88\\}$ is the pitch, which can also be mapped to one of the 88 piano keys. Our method finally outputs a policy that controls two hands interacting with a piano keyboard physically. A digital sound is generated by matching the pitch of the keys being pressed by the physically simulated hands.\nWe propose a method that combines data-driven and physics-based approaches to achieve the goal (Figure 5). A diffusion motion model [Ho et al. 2020] is trained on the F\u00fcrElise dataset to generate kinematics motions for the given piece. Despite the strong abilities"}, {"title": "4.1 Diffusion Model", "content": "The goal of this module is to generate a kinematic hand trajectory given a piece of sheet music. We leverage a diffusion model, which is proven to be very effective in modeling distributions of human motions [Alexanderson et al. 2023; Li et al. 2023; Tevet et al. 2023; Tseng et al. 2023] to perform kinematic motion generation.\nOverview. The core of the diffusion model [Ho et al. 2020] trains a denoiser network on dataset examples corrupted by different levels of Gaussian noises with the objective function reconstructing the original clean examples. The loss function for a conditional diffusion model is as follows:\n$L = E_{x,t} [[||x \u2212 x_{\\theta} (x_{t}, t, c) ||^{2}]$,\n(2)\nwhere x are the clean examples, $x_{t}$ is the corrupted examples on noise level t, c is the condition vector. After training, conditional samples can be drawn by running the denoiser network iteratively on a trajectory of Gaussian noises.\nMotion Representation. Since the task requires high precision for the location of fingertips, we represent the dual-hand motion as a trajectory of 2 \u00d7 21 joint locations $K \\in R^{M \\times 2 \\times 21}$ defined in MANO hands [Romero et al. 2017], similar to [Liu and Yi 2024]. M is the number of frames considered for the diffusion model. Here we use M = 120 which corresponds to a window of 120 frames. To ensure consistent bone lengths during generation, we fit MANO hand models to the generated trajectory with fixed shape parameters to achieve the final predicted joint locations.\nCondition Representation. To compute the condition vector $c_{t}$, we first quantize input sheet music ${O_{i} = (t_{start}, t_{end}, p_{i}) | i \\in \\{1, 2, ..., n\\}\\}$ into a binary matrix $C \\in \\{0, 1\\}^{N \\times 88}$, where N is the total number of frames in the input music and n is the total number of notes. Then, we divide each non-zero entry in the matrix by the duration of the corresponding key being pressed:\n$C_{i,p} = \\frac{1}{t_{end} - t_{start} + 1}$\n(3)\nIn this way, the key information, as well as the duration information, are encoded into the condition vector $c_{t} \\in R^{88}$.\nModel Architecture. We leverage a transformer-based architecture proposed in EDGE [Tseng et al. 2023] to train our model. In accordance with our dataset, motions and music are quantized into 59.94FPS. The diffusion model, therefore, generates 2 seconds by outputting the results of 120 frames at a time.\nLong-form Generation. Although we train the diffusion model on a window of 2 seconds, we can generate arbitrary long sequences from conditions by denoising a batch of sequences while enforcing the adjacent sequences in the batch share an overlapping path, following [Tseng et al. 2023]."}, {"title": "4.2 Music-Based Motion Retrieval", "content": "To complement the diffusion-generated motions, we retrieve additional reference motions from the whole dataset for reinforcement learning policy to perform imitation learning. To do so, first, we quantize all notes in the dataset ${O_{i} = (t_{start}, t_{end}, p_{i}) | i \\in \\{1, 2, ..., n\\}\\}$ into a binary matrix $M \\in \\{0,1\\}^{N \\times 88}$ that align with the frames of hand motions. We perform the same quantization for the input piece to obtain binary a matrix $M' \\in \\{0, 1\\}^{N' \\times 88}$. Next, we compute a sliding window of length 30 and stride 1 individually over M and M' to obtain $W \\in \\{0,1\\}^{N_{w} \\times 30 \\times 88}$, $W' \\in \\{0,1\\}^{N'_{w} \\times 30 \\times 88}$. $N_{w}$ and $N'_{w}$ are the numbers of windows for the dataset and the input piece. We then compute matching from windows in the target piece"}, {"title": "4.3 Policy Training for Physics-based Control", "content": "We set up our simulation environment using IsaacGym [Makoviychuk et al. 2021]. While the simulation runs at 240 FPS, the control runs at 60 FPS which is consistent with our diffusion model. Our physics-based hand models are modified from [Kumar and Todorov 2015] with geometry optimized according to the mocap subjects. Each hand has 17 links with 27 degrees of freedom (DoFs) driven by PD servos, where the wrist has 6 DoFs, the MCP joints have 2 DoFs except that the thumb MCP has 3, and all the PIP and DIP joints have 1 DoF. This leads to an action space of $a_{t} \\in R^{2 \\times 27}$ for two hands. Similar to our diffusion model, we take the key-based binary vector as the goal representation for key pressing. To balance the goal vector size and the observation horizon, we utilize a compressed representation by merging the same key-pressing goal in consecutive frames into one. We take the future five merged goals as the goal state with an additional timer variable that indicates the time (in terms of the number of simulation frames) left for the associated key-pressing goal. Thus the final goal state vector is of shape $g_{t} \\in R^{5 \\times (88+1)}$. To perform control, we take a 2-frame historical observation composed of the position, orientation, and linear and angular velocities of all the links of two hands. This results in a pose state vector $s_{t} \\in R^{2 \\times 2 \\times 208}$ for two hands.\nDue to the limited performance of the motion generated by the diffusion model, we do not directly perform motion tracking during the control policy training. Rather, we take the generated and retrieved motions as the reference simultaneously, and perform imitation learning using reinforcement learning with a GAN-like architecture [Xu and Karamouzas 2021] for motion synthesis. Following the previous literature [Xu et al. 2023], to utilize the reference motions more effectively, we decouple the motions of two hands and employ two discriminators at the same time for motion imitation of the left and right hand respectively. By doing so, the pose of one hand does not rely on that of the other hand anymore. We, thereby, facilitate the single-hand motion imitation by performing learning independently rather than using a dual-hand state space. The imitation-related reward is computed by\n$r_{imit}^{h}(s_{t}, a_{t}) = \\sum_{n=1}^{N} D_{h}(s_{t}^{h}) - D_{h}(s_{t+1}^{h})$,\n(5)\nwhere h \u2208 {L, R} indicates the imitation of the left and right hand respectively, $s_{t}^{h}$ is the pose state of the single hand h, and $D_{h}$ is the discriminator trained using hinge loss [Lim and Ye 2017].\nTo encourage expected key-pressing behaviors, besides imitation, we also employ a goal-based reward function to evaluate the policy's key-pressing performance at each time step t. The reward definition is different depending on the pressing condition of each key.\nWe assume that a key k is pressed to generate sound if the pressed distance $p_{k}$ is greater than 90% of that key's maximal travel distance $d_{k}$, which is defined using the allowed rotation range of that key. For each target key k that needs to be pressed, we have the reward term to encourage the correct key-pressing behavior:\n$\\begin{cases} 1 \\\\ exp(||p_{i} - p_{k} || +0.01p_{k}/d_{k}) & otherwise, \\end{cases}$\n(6)\nwhere $p_{i}$ is the global position of the target fingertip i, and $p_{k}$ is the target position of the key. To determine the target fingertip, We extract fingering information based on the nearest finger to that key in the diffusion-generated motion. The target position of the key is obtained using the surface center of a key horizontally and the 85% position along the key's length axis vertically.\nFor each non-target key $K$, $r_{t,k}$ measures the errors of key pressing and is employed to penalize incorrect key-pressing behaviors:\n$r_{t,k} = \\begin{cases} p_{k}/0.9d_{k} & if key K is touched and $p_{k}/d_{k} > 0.1$ \\\\ 0 & otherwise. \\end{cases}$\n(7)\nTo emulate a physical piano generating clear sound, we perform penalization even if the key is assumed not to trigger any sound virtually (i.e. $p_{k}/d_{k} < 0.9$) but ignore trivial touch (i.e. $p_{k}/d_{k} < 0.1$). However, in difficult scenarios, key touching cannot be completely avoided. To prevent the policy from achieving a lower error of $r_{t,k}$ by not touching any key, an additional reward term is introduced to encourage correct key pressing behaviors even if some non-target keys are touched.\nThe overall goal-driven reward is defined as\n$r_{t} = \\prod_{k} r^{t,k} - 0.15 \\sum_{K} r_{t,k} + 0.5r_{correct} - 0.05r_{energy}$,\n(8)\nwhere $r_{correct} = 1$ if all target keys are pressed correctly or 0 otherwise, and $r_{energy}$ is a term measuring the energy consumption based on the average linear velocity of fingers and wrists between two frames:\n$r_{energy} = exp \\left[-0.75 \\sum_{h \\in \\{L,R\\}} (||v_{h}|| + 0.1\\sum_{i \\in hand \\{h\\}} (||v_{i}||))\\right]$,\n(9)\nwhere $v_{h}$ is the velocity of one hand's wrist in the global space, $v_{i}$ is the average velocity of each fingertip in the local system defined by its corresponding wrist joint.\nThe policy is trained using a multi-objective framework [Xu et al. 2023] to optimize\n$max E_{t} \\sum_{i} w_{i}A_{t,i}(a_{t} | g_{t}, s_{t})$\n(10)\nwhere $A_{t,i}$ is the standardized advantage that is estimated according to the achieved reward of each objective i, and $w_{i}$ is an associated weight. In our case, we have three objectives (two imitation objectives of left and right hand respectively, and one goal-driven objective). To encourage the policy to perform expected key-pressing behaviors, the associated weights are 0.9 for the goal-driven objective and 0.05 for each imitation objective. Please refer to the supplementary materials for the hyperparameters."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "We evaluate our method quantitatively on 14 pieces of music using the F1 score. We also conduct numerous ablation studies to analyze the impact of each component in our algorithm. Our dataset and method are best evaluated in the supplementary video with the audio turned on."}, {"title": "5.1 Setup", "content": "Data. We use 14 sheets of music to test our proposed pipeline. Although most recorded compositions in our dataset are classical, we include a wider range of genres including popular music, and jazz unseen during training. Because the chosen music pieces are very long with repetition, we select a clip of music from each piece and use it to train our model. The lengths of the clips are in the range from 14.4 to 28.94 seconds and 20.72 seconds on average. We do not modify the speed of the original music.\nMetrics. Similar to our data quality evaluation, we record the key-pressing states of model predictions and compare them with the input sheet music. Precision, recall, and F1 scores are computed for each frame and averaged over the whole piece. For diffusion-generated motions, we use the same heuristics used in data quality evaluation to extract the pressed keys: when a fingertip is below a preset depth and horizontally over a key, we treat the key as pressed. For physics-based policy, we directly query the key-pressing states from the physical simulator.\nImplementation Details. For diffusion models, we train with a window of 120 frames (2 seconds). The training takes around 1 day on 2 NVIDIA A5000 GPUs. We train a single diffusion model for all the testing compositions. Policy trained with reinforcement learning takes around 1-3 days depending on the difficulty of studied music pieces on a single A5000 GPU and consumes about 2 \u00d7 108 to 4 \u00d7 108 training samples."}, {"title": "5.2 Diffusion Generated Motions", "content": "We first show qualitative results in Figure 7. The diffusion models can generate natural and plausible kinematic trajectories on unseen pieces if viewed from a top-down perspective. However, the model cannot press keys accurately. The generated motions frequently float above the keys without pressing them or press the wrong keys, as shown in Figure 8. These observations resonate with other works using diffusion models on whole-body motion and hand-object interactions [Liu and Yi 2024; Yuan et al. 2023]. The observations are also supported by the quantitative results in Figure 6. More visualizations are shown in the supplementary video.."}, {"title": "5.3 Full Pipeline", "content": "Quantitative results of our full pipeline are summarized in Figure 6: the policy outperforms the diffusion model by a large margin. As shown in Figure 7, the policy can handle large wrist motions (Fig 7f), chords (pressing multiple keys at the same time, Fig 7abc), double notes (pressing different pairs of notes sequentially, Fig 7d), as well as arpeggios (pressing individual notes of a chord in sequence, Fig 7e). Despite the average F1 scores being as high as more than 0.8 for all the tested songs, the policy still could perform unexpected key"}, {"title": "5.4 Ablations", "content": "To understand the effect of using an ensemble of motions generated by the diffusion model and those retrieved from the dataset as the reference for the control policy to learn, we design the following ablation studies tested on four music pieces:\n\u2022 RL+Retr. The policy is trained with only the reference motion retrieved from the dataset.\n\u2022 RL+Diff. The policy is trained with only the reference motion generated by the diffusion model.\n\u2022 RL Only. The policy is trained only using the goal-driven reward without motion imitation.\n\u2022 RL+Whole. The policy is trained only using the whole motion dataset as the reference for imitation without motions generated by the diffusion model.\nResults. The performance of each model is listed in Table 1. The training curve is shown in Figure 9. The full model outperforms the ablative models by a large margin in all the tested cases. We show qualitative comparisons visually of the studied ablative models in Figure 10. As we can see, the RL only case performs the worst and behaves in a manner not human-like, which highlights the necessity of using motion imitation to ensure the motion naturalness and to help better key-pressing task execution. When the policies are trained"}, {"title": "6 CONCLUSION", "content": "We present a first-of-its-kind large-scale dataset of 3D hand motion and audio of piano performance. Our dataset, F\u00fcrElise, contains 8 hours of performance from 11 elite-level pianists playing 98 pieces of classical music. Leveraging F\u00fcrElise, we propose a physics-based method to synthesize accurate piano playing motion for music outside the training dataset. We evaluate our method through extensive experiments and ablations.\nOur work takes the first step toward motion synthesis of human peak performance using data collected from musicians for unseen songs. However, there is still a significant gap between the skill level our model achieves and that of human pianists. Several limitations in our current work might contribute to this gap. First, our method does not consider sound amplitude, a critical element in music performance. Consequently, our current model generates music with constant amplitude. However, the key-pressing velocity, which determines amplitude, is recorded in our dataset and can be utilized for future work. Second, we let the model determine fingering, resulting in policies that may struggle with some basic skills such as finger crossover. Future work could incorporate high-level, common fingering rules to facilitate policy learning. Moreover, we leverage F1 scores to evaluate performance averaged over each frame, which may not align well with humans' auditory perception, as humans could be sensitive to some transient errors that contribute little to F1 scores such as breaking a chord or inconsistent tempo. Developing a better audio evaluation metric that meets humans' perceptions would be a great direction for future work. Finally, while the simulated hand models have a reasonably accurate kinematic structure, they can exert unnaturally large joint torques or generate infeasible acceleration. A promising future direction is to consider a realistic hand musculoskeletal model that generates motion through muscle activation, providing a computational tool for biomechanics studies and injury prevention."}]}