{"title": "Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks", "authors": ["Keiichiro Yamamura", "Issa Oe", "Hiroki Ishikura", "Katsuki Fujisawa"], "abstract": "Deep neural networks are vulnerable to adversarial examples, and adversarial attacks that generate adversarial examples have been studied in this context. Existing studies imply that increasing the diversity of model outputs contributes to improving the attack performance. This study focuses on the Auto Conjugate Gradient (ACG) attack, which is inspired by the conjugate gradient method and has a high diversification performance. We hypothesized that increasing the distance between two consecutive search points would enhance the output diversity. To test our hypothesis, we propose Rescaling-ACG (ReACG), which automatically modifies the two components that significantly affect the distance between two consecutive search points, including the search direction and step size. ReACG showed higher attack performance than that of ACG, and is particularly effective for ImageNet models with several classification classes. Experimental results show that the distance between two consecutive search points enhances the output diversity and may help develop new potent attacks. The code is available at https://github.com/yamamura-k/ReACG.", "sections": [{"title": "1 Introduction", "content": "Szegedy et al. [33] noted that the output of deep neural networks (DNNs) can be significantly altered by small perturbations imperceptible to the human eye. These perturbed inputs are referred to as adversarial examples, and it is well-known that this is a crucial vulnerability of DNNs. Addressing this vulnerability is crucial because DNNs have safety-critical applications such as automated driving [16], facial recognition [2], and cyber security [22]. Several defense mechanisms have been proposed to improve the robustness of DNNs against adversarial examples. The most fundamental approach is adversarial training [25], which requires several adversarial examples. The rapid generation of adversarial examples is expected to reduce the computation time for adversarial training and improve the robustness of the obtained DNN."}, {"title": "2 Preliminaries", "content": "Let $f: \\mathbb{D} \\subset \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{C}$ be a locally differentiable function that serves as a $C$-classifier. Assume that the input $x_{org}$ is classified into class $c$ using classifier $f$. Given a positive number $\\varepsilon$ and distance function $d: \\mathbb{D} \\times \\mathbb{D} \\rightarrow \\mathbb{R}$, we define an adversarial example $x_{adv} \\in \\mathbb{D}$ as an input satisfying the following conditions\n$\\arg \\max_{i} f_{i}(x_{adv}) \\neq c, \\quad d(x_{org}, x_{adv}) \\leq \\varepsilon$.\n(1)\nGenerally, an adversarial example is generated by maximizing the objective function $L$ within the feasible region $\\mathbb{S} = \\{x \\in \\mathbb{D} \\mid d(x_{org}, x_{adv}) \\leq \\varepsilon\\}$. This problem is formulated as follows:\n$\\max_{x \\in \\mathbb{S}} L(f(x), c)$\n(2)\nThe condition $\\arg \\max_{i} f_{i}(x_{adv}) \\neq c$ can be rephrased as $\\max_{i \\neq c} f_{i}(x_{adv}) - f_{c}(x_{adv}) \\geq 0$. Hence, objective functions such as the CW loss ($L_{cw}$) [7] and Difference of Logit Ratio (DLR) loss ($L_{DLR}$) [8] are commonly used. Let $z = f(x)$ and $\\pi_{k}$ be the index of the $k$-th largest element of $z$. The CW loss and DLR loss are denoted as $L_{cw}(z, c) = -z_{c} + \\max_{i \\neq c} z_{i}$ and $L_{DLR}(z, c) = \\frac{L_{cw}(z,c)}{z_{\\pi_1}-z_{\\pi_3}}$, respectively. $\\arg \\max_{i \\neq c} f_{i}(x)$ was referred to as CTC in [37]. We define output diversity as the variation of CTC during the attack procedure.\nFor adversarial attacks on image classifiers, the input space is $\\mathbb{D} = [0, 1]^{n}$. Additionally, the Euclidean distance $||\\cdot||_{2}$ or uniform distance $||\\cdot||_{\\infty}$ is often used as the distance function $d$. This study focuses primarily on $l_{\\infty}$ attacks that use $d(u, v) = ||u - v||_{\\infty}$ as a distance function."}, {"title": "2.2 Related work", "content": "Szegedy et al. [33] suggested the existence of adversarial examples, and several gradient-based attacks have been proposed subsequently [7, 8, 20, 25, 37]. Among these, a promising approach is to consider the diversity in the output space. MT-PGD [13] achieves diversification in the output space by sequentially performing targeted attacks using different misclassified target classes. Output Diversified Sampling [34] and its variants [24] randomly sample the initial point, considering the diversity in the output space by maximizing the inner product of a random vector $w$ and logit $f(x)$. These methods assume several restarts based on the changes in the target class or initial points. By contrast, ACG improves the output diversity by moving to the sign of the conjugate gradient. ACG is advantageous in terms of the computational time because output space diversification can be achieved without restarts. Additionally, an ensemble of attacks, such as AutoAttack [8], has attracted research attention in recent years [23, 26]. Although ensemble-based attacks are stronger than simple attacks, such as PGD, their performance depends on the individual attacks in the ensemble. Therefore, it is important to develop individualized attacks."}, {"title": "2.3 Auto Conjugate Gradient attack", "content": "ACG step ACG attack [37] is based on the conjugate gradient method, which updates search points using the following formulas:\n$\\begin{aligned}\ng^{(k)} &\\leftarrow \\nabla_x L \\left(f\\left(x^{(k)}\\right), c\\right), &\\text{(3)}\\\\\ny^{(k-1)} &\\leftarrow g^{(k-1)}-g^{(k)}, &\\text{(4)}\\\\\n\\beta^{(k)} &\\leftarrow \\frac{-\\left(g^{(k)}, y^{(k-1)}\\right)}{\\left(s^{(k-1)}, y^{(k-1)}\\right)}, &\\text{(5)}\\\\\ns^{(k)} &\\leftarrow g^{(k)}+\\beta_{s} s^{(k-1)}, &\\text{(6)}\\\\\nx^{(k+1)} &\\leftarrow P_{\\mathbb{S}}\\left(x^{(k)}+\\eta^{(k)} \\operatorname{sign}\\left(s^{(k)}\\right)\\right), &\\text{(7)}\n\\end{aligned}$\nwhere $(\\cdot, \\cdot)$ is the inner product and $P_{\\mathbb{S}}(\\cdot)$ is a projection onto the feasible region $\\mathbb{S}$. $x^{(k)}$ is referred to as the search point, and $s^{(k)}$ is referred to as search direction.\nStep-size updating rule ACG controls the step size using the same rule as that in APGD. APGD halves the step size and moves to the incumbent solution $x_{adv}$ if conditions C1 and C2 are satisfied at the precalculated checkpoints $w_{j} \\in \\mathbb{W}$.\n$\\mathrm{C1} \\frac{1}{\\left|\\left\\{i \\mid x^{\\left(i+1\\right)} \\in P_S \\right\\}\\right|} \\sum_{i=w_{j-1}}^{w_j-1} L\\left(f\\left(x^{\\left(i+1\\right)}\\right), c\\right) >L\\left(f\\left(x^{\\left(i\\right)}\\right), c\\right)< p \\cdot\\left(w_{j}-w_{j-1}\\right)$\n$\\mathrm{C2} L_{\\max } \\left(f\\left(x^{\\left(w_{j-1}\\right)}\\right), c\\right) =L_{\\max } \\left(f\\left(x^{\\left(w_j\\right)}\\right), c\\right) \\text { and } \\eta^{\\left(w_{j-1}\\right)} = \\eta^{\\left(w_j\\right)},$"}, {"title": "3 Rescaling-ACG", "content": "This section describes the ReACG attack, a modification of ACG aimed at increasing the distance between two consecutive search points and enhancing output diversity. ReACG automatically modifies its search direction and controls the step size using the appropriate checkpoints obtained through multi-objective optimization. Section 3.1 describes the modification of the search direction, and section 3.2 provides the checkpoints used for step size calculation. The pseudocode for ReACG is described in algorithm 1."}, {"title": "3.1 Search direction", "content": "Motivation Search direction such as $s^{(k)}$ or $g^{(k)}$ is one of the main factors affecting the distance between two consecutive search points. Conjugate gradient methods are characterized by the coefficient $\\beta^{(k)}$ used to calculate the conjugate gradient. The preliminary experiment conducted by Yamamura et al. [37] suggests that $\\beta^{(k)}$ significantly affects attack performance. Equation 6 indicates that if $|\\beta^{(k)}|$ is extremely larger than each element of $|g^{(k)}/s^{(k-1)}|$, $s^{(k+1)}$ and $s^{(k)}$ are likely to be the same vector. These results imply that an ACG search may be redundant.\nRescaling condition equation 6 suggests that $s^{(k)}$ and $s^{(k-1)}$ are likely to take approximately the same values for the index $i$ such that $|g^{(k)}/s^{(k-1)}| << |\\beta^{(k)}|$. If"}, {"title": "3.2 Rethinking the step size", "content": "ACG uses precalculated checkpoints to control the step size. The three magic numbers p1, q, and qmin that appear in the checkpoint calculation affect the obtained checkpoints. We searched for the appropriate values of these parameters through multi-objective optimization using Optuna. Based on the experimental results in section 4.1, we set p1 = 0.43, q = 0.24, and qmin 0.08."}, {"title": "4 Experiments", "content": "This section describes the results of comparative experiments on $l_{\\infty}$-robust models trained on CIFAR-10, CIFAR-100, and ImageNet. The attacked models were in RobustBench [9], which is a well-known benchmark for adversarial robustness. The performance evaluation was based on robust accuracy, defined as follows:\n$\\frac{\\text{\\# correctly classified adversarial examples}}{\\text{\\# test samples}} \\times 100$.\n(13)\nA low robust accuracy indicates a high misclassification rate and high attack performance. As with the Robust Bench leaderboard, we used 10,000 images and $\\varepsilon=8 / 255$ for CIFAR-10/100, and 5,000 images and $\\varepsilon=4 / 255$ for ImageNet. The experiments were conducted using an Intel(R) Xeon(R) Silver 4214R CPU at 2.40GHz and NVIDIA RTX A6000. We chose APGD and ACG as the baseline. Both typically perform better for adversarially robust models than early techniques such as FGSM [12] and PGD [25]."}, {"title": "4.1 Experiments on step size control", "content": "This experiment used five robust PreActResNets [1, 4, 15, 31, 36] trained on CIFAR-10. Optuna [3] searched for appropriate parameters through multi-objective optimization, which minimized robust accuracy and maximized the average CW loss value. In each optimization iteration with Optuna, ReACG with CW loss was executed starting at the input points for different values of 0.01 \u2264 p1 \u2264 0.9, 0.01 \u2264 q\u2264 0.5p1, and 0.01 \u2264 qmin \u2264 0.1. To make the step size sufficiently small, the objective function of this multi-objective optimization problem was designed to return a tuple (\u221e,-\u221e) when the number of checkpoints was less than four. Optuna requires 100 iterations per model. The obtained p\u2081 values tend to be larger than those in the APGD setting. The experimental results indicate that p\u2081 = 0.43, q = 0.24, and qmin = 0.08 are effective in several models."}, {"title": "4.2 Experiments on ReACG", "content": "ReACG was compared with APGD and ACG. This experiment used 30 models, including the top 10 for each dataset listed on the Robust Bench leaderboard as of October 1, 2023. All compared methods used the input point as the initial point, $N=100$ as the total number of iterations, and CW/DLR loss as the objective function.  A represents the difference in robust accuracy between ACG and ReACG. From the \"CW loss\" columns , ReACG showed lower robust accuracy for 90% of the 30 models than that in APGD and ACG. With DLR loss, ReACG showed a higher attack performance than APGD and ACG for 93% and 97% of the 30 models, respectively. ReACG showed lower robust accuracy than APGD and ACG by approximately 0.4 to 0.9% and 0.1 to 0.4%, respectively. Additionally, the difference in robust accuracy among APGD, ACG, and ReACG was higher with DLR loss than that with CW loss. These improvements are sufficient considering the recent advances in adversarial attacks based on nonlinear optimization methods.\nFigure 2 shows the percentage of images where APGD/ReACG attacked successfully and ReACG/APGD attacked unsuccessfully. APGD exhibited a lower robust accuracy than that of ReACG for the models proposed by Bai et al. [5]. However, fig. 2 shows that ReACG successfully perturbs the images for which APGD fails. This result suggests that the ensemble of ReACG and APGD is likely to exhibit a high attack performance."}, {"title": "4.3 Effect of rescaling", "content": "Figure 3(a) and (b) show the transitions of $|\\beta^{(k)}|$ and $||x^{(k)} - x^{(k-1)}||_{2}$, respectively. ACG+R adopts $\\beta^{(k)}$-rescaling. Figure 3(a) demonstrates that ACG+R has smaller $|\\beta^{(k)}|$ than that of ACG, which indicates that our rescaling method reduces $|\\beta^{(k)}|$. Additionally, fig. 3(b) shows that the search points of ACG+R move more than those of ACG. These results suggest that our rescaling method enhances the search efficiency by resolving the issue caused by a large $|\\beta^{(k)}|$."}, {"title": "4.4 Difference in CTC variation", "content": "In section 4.3, we discussed the effect of our rescaling method in the input space. This section highlights one reason for the high performance of ReACG by focusing on the difference in the output diversity of ACG and ReACG. Analysis of section 4.3 implies that ReACG performs a more diversified search than that of ACG using appropriate step-size control in addition to rescaling $\\beta^{(k)}$. The analysis in this section is based on CTC variation. Let $c^{(k)}=\\arg \\max_{i \\neq c} f_{i}\\left(x^{(k)}\\right)$ be the CTC at the kth iterateion. The number of CTCs that appear during an attack $\\# CTC$, is defined as $\\# CTC :=\\left|\\left\\{c^{(i)} \\mid i=1, \\ldots, N\\right\\}\\right|$. Figure 4 shows that ReACG has a smaller percentage of images where $\\# CTC=1$ than that in ACG and a larger percentage of images where $\\# CTC>2$. Additionally, ReACG has a larger maximum number of $\\# CTCs$ than that in ACG. These results suggest that ReACG enhances the output diversity compared to that with ACG."}, {"title": "4.5 Ablation study", "content": "Effects of rescaling and step size optimization describes the robust accuracy for nine models, including the top three for each dataset listed in the RobustBench leaderboard. This experiment used the same initial points, total number of iterations, and objective functions as those in section 4.2. ACG+T is ACG that uses the parameters $P_{1}, q$ and $q_{min}$ selected in section 4.1.\nIn table 3, ACG+R and ACG+T show lower robust accuracies than that of ACG in many cases. This result indicates that rescaling $\\beta^{(k)}$ and step size optimization can improve attack performance. Additionally, ACG+T showed the same or slightly lower robust accuracy than that of ACG+R, and ReACG showed a lower robust accuracy than that of ACG+R and ACG+T in several cases.\nRelationship between the number of total iterations and robust accuracy Figure 5 shows the transition of the robust accuracy of ACG and ReACG with N = 50, 100, 200, 400, and 1000. The dashed line in the figure represents the ACG, and the solid line represents the ReACG. At approximately $N< 200$, ReACG recorded a lower robust accuracy than that of ACG; however, at $N \\geq 400$, ACG showed a lower robust accuracy than ReACG."}, {"title": "5 Conclusion", "content": "We hypothesized that increasing the distance between two consecutive search points would enhance the output diversity, resulting in high attack performance. We propose ReACG with an improved ACG search direction and step-size control to test our hypothesis. ReACG automatically modifies $\\beta^{(k)}$ when $|\\beta^{(k)}|$ exceeds the average ratio of the gradient to conjugate gradient. Our analyses show that modifying $\\beta^{(k)}$ and step size control increased the distance between"}]}