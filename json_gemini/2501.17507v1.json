{"title": "Reflections on \"Can AI Understand Our Universe?\"", "authors": ["Yu Wang"], "abstract": "This article briefly discusses the philosophical and technical aspects of AI. It focuses on two concepts of understanding: intuition and causality, and highlights three AI technologies: Transformers, chain-of-thought reasoning, and multimodal processing. We anticipate that in principle AI could form understanding, with these technologies representing promising advancements.", "sections": [{"title": "1. Introduction", "content": "In the article \u201cCan AI Understand Our Universe?\",1 we explore the potential of integrating multi-domain data through a single large language model (LLM) to assess whether AI can function as a unified mind for scientific research. Traditionally, research in astronomy and physics often relies on specialized models tailored to specific tasks and datasets. For instance, estimating redshifts of quasars, classifying astrophysical phenomena, and inferring black hole parameters usually require separate models designed to handle the unique characteristics of each dataset. Our study demonstrates that by fine-tuning a single GPT model, it is possible to handle diverse data and tasks simultaneously.\nIn our astrophysical classification experiments, we used spectral data from the Sloan Digital Sky Survey (SDSS).2,3 The fine-tuned GPT model achieved a classification accuracy of 82% on spectral data, demonstrating strong capabilities in celestial object recognition. Furthermore, by fine-tuning on quasar spectral data, the model achieved a relative accuracy of 90.66% in estimating the redshift, indicating that the GPT model is capable for both classification and regression of astrophysical tasks.\nFor gamma-ray burst (GRB) classification,\u2074 we used spectral information instead of the traditional GRB duration.5 The spectral classification results show 95.15% agreement with those based on GRB durations, revealing distinct spectral properties\""}, {"title": "2. Two Concepts of Understanding", "content": "In the four-dimensional universe we live, humans perceive space and time through their senses, forming an intuitive understanding of the world. Intuition is not only an essential tool to explore the understanding, but also the foundation in daily life for quick decision-making. With the rise of artificial intelligence, an intriguing question emerges: Can AI construct its own intuition without human-like senses, perhaps even surpassing human perception of the world? Furthermore, can AI, through its unique data-processing capabilities, deeply understand causal relationships and provide new perspectives in fields unexplored by humans?"}, {"title": "2.1. Intuition", "content": "Humans perceive space through binocular vision and time through changes in events and the accumulation of memory.14 How, then, is this perception formed? By contrast, how does AI perceive space and time? Can AI experience these dimensions differently and develop an intuition beyond human capabilities, enabling it to understand the world from entirely new perspectives?"}, {"title": "2.1.1. Humans Intuition", "content": "Intuition refers to the immediate understanding humans achieve through sensory experiences, forming a \"clear image\" or \"immediate comprehension\". 15 It arises from integrating sensory inputs like vision, hearing, and touch, enabling direct perception of the world.16 For example, when we see a tree, our vision immediately conveys its shape, color, and distance, without requiring reasoning or abstraction. This direct perception facilitates rapid acquisition of information, resulting in an \"evident\" understanding.\nIntuition is closely tied to the structure of the human brain. 17,18 with sensory systems evolved to process inputs for rapid perception and response. Intuitive experiences aid humans in forming concepts of space, time, and causality. However, these concepts may vary among species. For instance, a vision-deprived organism, though lacking the visual intuition familiar to us, may develop alternative perceptual methods, such as relying on touch or sound, to construct an understanding of the world. 19, 20 Such differences in perceptual mechanisms may still support effective understanding, even if the resulting \"intuition\" differs from ours.\nHuman senses also have significant limitations. Our vision is confined to a narrow band of visible light, and our hearing can only perceive sounds within a limited frequency range. These constraints restrict our \u201cintuitive\" experience of the world. However, technological advancements allow us to overcome these limitations. Telescopes reveal celestial objects and microscopes expose microscopic organisms beyond the reach of the naked eye. 21-24\nAs a result, human understanding of the world is not solely dependent on intuitive experiences but also relies heavily on data-driven reasoning and model construction. Many scientific discoveries and theoretical concepts, such as quantum mechanics and relativity, lie beyond human sensory perception. These advances rely on data analysis, logical reasoning, and mathematical modeling. In this regard, AI resembles humans in some ways: it lacks human sensory organs but can understand the world through data learning and statistical pattern recognition. AI builds knowledge using multimodal data-numerical values, images, and text similar to how humans use instruments to observe phenomena beyond their natural sensory capabilities. Its advantage lies in processing volumes and complexities of data beyond human capacity, identifying correlations and causality through mathematical and statistical models. For example, in medical imaging, AI can detect subtle features imperceptible to humans, improving diagnostic accuracy. 25, 26 Similarly, AI can analyze environmental data to understand complex systems, even without relying on humans \"intuitive experiences\" 27, 28"}, {"title": "2.1.2. AI's Super-Intuition", "content": "The core function of intuition in the understanding process is to provide a \"mediating framework\" that facilitates quick comprehension without relying on complex reasoning or fundamental theories. 29-31 Intuition serves as a bridge between"}, {"title": "2.2. Causality", "content": "What does it mean for humans to \"understand the causality of an event\"? For example, \"The sky rains, so the ground gets wet\" is a causal relationship. But why does it rain? How does rain cause the ground to become wet? If someone only knows \"The sky rains, so the ground gets wet,\" does that mean they truly understand causality, or are they simply performing pattern matching based on experience? We can continue to ask deeper questions until we reach the fundamental root of the issue: what is the ultimate nature of causality?"}, {"title": "2.2.1. Understanding Causality", "content": "\"Understanding the causality of events\" is a complex issue spanning fields like cognitive science, philosophy, and scientific theory. 43-47 For humans, causal understanding ranges from simple pattern matching to deep theoretical explanations.\nAt the most basic level, humans derive causal understanding from observing repeated phenomena. For instance, a child might infer \"Rain causes the ground to get wet\" after repeatedly observing wet ground following rainfall. This causal reasoning is based on correlation and falls under pattern recognition. Though shallow, it is highly efficient in daily life. Human brains excel at quickly grasping causality through intuitive reasoning. For instance, observing a tree branch fall after a strong gust of wind, we intuitively infer that the wind caused it. This type of causal understanding relies on cognitive models in the brain, offering quick but simplified explanations without exploring underlying mechanisms.\nScientific causal understanding is built on theoretical frameworks. For example, the explanation for rain involves air cooling, causing water vapor to condense into droplets, a reasoning rooted in meteorology and physics. Theoretical causality requires extensive background knowledge and abstract thinking, representing a deeper level of understanding. Humans also use counterfactual reasoning to validate causal relationships. For instance, \"Would the ground stay dry if it did not rain?\" This type of reasoning helps distinguish genuine causality from mere correlation by exploring multiple possibilities.\nCausal relationships can be investigated layer by layer, but their ultimate boundary depends on several factors. First, our understanding of causality is limited by current scientific knowledge. While we can explain rain formation, further inquiries might lead to global weather systems, Earth's climate models, or even the sun's energy transfer. These chains may eventually be traced to fundamental physical laws, but the ultimate explanation for these laws remains a mystery. Ultimately, the inquiry into causality may touch on philosophical questions: What is a \"cause\"?"}, {"title": "2.2.2. Causality by AI", "content": "AI shares many similarities with humans in its approach to discovering causal relationships, particularly in pattern recognition and hypothesis testing. 48, 49 Humans identify causal patterns through sensory experiences and repeated observations. Similarly, AI extracts causal relationships from complex environments through training on large datasets and statistical pattern recognition, such as analyzing the relationship between weather data and humidity. This experience- and observation-based exploration of causality enables both AI and humans to identify correlations, forming the foundation for deeper causal reasoning.\nIn both humans and AI, the process of forming and testing hypotheses is another point of similarity. Humans often propose hypotheses intuitively and validate them through experiments or observations, such as hypothesizing, \"If it does not rain, the ground will not get wet,\" and testing this in real-world scenarios. AI, on the other hand, employs causal inference algorithms to automatically generate hypotheses. For instance, it uses causal graphs to test intervention effects between variables, simulates counterfactual scenarios, and verifies causal pathways. 50,51 This iterative process of hypothesis and validation allows both humans and AI to refine their causal understanding dynamically.\nAdditionally, humans and AI share similarities in the fundamental logic of exploring causality through variable integration and hierarchical reasoning. When dealing with complex systems involving multiple variables, humans rely on intuition and experience to identify key variables and construct causal sequences, such as reasoning that \"Clouds form rain, and rain wets the ground.\" AI, however, utilizes parallel computing and causal modeling techniques to extract key variables and identify hierarchical relationships within large datasets, thus unraveling intricate causal networks. 52, 53"}, {"title": "2.2.3. Beyond Human Causality", "content": "Can AI surpass humans in understanding causality? The answer depends on how 'causal understanding' is defined and the inherent limits of human cognition. Given current technological and theoretical advancements, AI may already surpass human abilities in specific domains or possess the potential to do so. This \"surpassing\" manifests in AI's ability to process complex data, uncover hidden patterns, and achieve greater efficiency in reasoning.\nA key advantage of AI is its ability to process vast amounts of data and identify complex causal relationships, especially in fields where human sensory and cognitive"}, {"title": "3. AI Technologies", "content": "AI is rapidly evolving. LLMs based on the Transformer architecture, 59 such as GPT\u00aa and Claudeb, have been described by some researchers as \u201csparks of general artificial intelligence\".60 These models leverage powerful attention mechanisms and parallel processing capabilities, achieving significant advances in multimodal data processing, cross-domain knowledge integration, and step-by-step reasoning."}, {"title": "3.1. Attention: Transformer Architecture", "content": "The Transformer model is a deep learning architecture originally designed for machine translation tasks but now widely used across various natural language processing (NLP) applications.59,61 Its core feature is the attention mechanism, which allows the model to assign different weights to input elements based on their relevance when generating outputs, focusing on the most pertinent information. The Transformer is composed of multiple encoder and decoder layers, each utilizing self-attention mechanisms to capture long-range dependencies in sequences, enabling the model to understand contextual relationships between words in complex sentences.\nIn a Transformer, the attention mechanism is divided into \"self-attention\" and \"multi-head attention\". Self-attention enables each word in the input sequence to \"attend\" to all other words and calculate attention weights based on relevance, capturing inter-word relationships. Multi-head attention extends this by employing multiple parallel attention mechanisms to extract diverse semantic features, allowing the model to focus on different linguistic aspects simultaneously. This parallel processing and global focus make the Transformer superior to traditional networks for handling long texts and complex language structures.\nSpecifically, the self-attention mechanism generates a query vector Q, a key vector K, and a value vector V for each word in the input sequence. By computing the dot product of Q and K, the model quantifies the similarity between the current word (represented by Q) and all other words (represented by K). These similarities are transformed into attention weights\n$A(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V.$\n\nThe softmax function ensures that the attention weights A form a probability distribution where the sum of each row equals 1. These attention weights A are then multiplied by the value vectors V, generating a weighted output representation for the current word. This process enables the model to adjust each word's representation based on the global context, capturing long-range dependencies within the input sequence.\nMulti-head attention extends self-attention by introducing multiple independent attention distributions for the same input sequence, allowing the model to capture diverse relationships or feature spaces. The multi-head attention is mathematically expressed as\n$MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_n) W^O$\nwhere each attention head $head_i$ is computed as\n$head_i = A(Q W_i^Q, K W_i^K, V W_i^V)$"}, {"title": "3.2. Chain-of-Thought Reasoning", "content": "In recent years, Chain-of-Thought (CoT) reasoning has emerged as a key innovation for enhancing the logical reasoning capabilities of LLMs, 65-67 it explicitly generates intermediate reasoning steps. Although LLMs excel at language generation and knowledge-based tasks, traditional approaches often struggle with logical consistency in complex problems. CoT reasoning bridges this gap by guiding the model to simulate human-like thought processes, enabling it to derive solutions step-by-step and construct complete logical chains from input to output."}, {"title": "3.3. Multimodal Processing", "content": "Initially, LLMs were designed to focus on textual data.70 However, with advances in AI, LLMs have evolved to handle multimodal tasks, including image generation, image-text alignment, speech processing, and multimodal question answering.71 This transition is driven by the versatility of the Transformer architecture and the ability to pretrain on large-scale datasets. By incorporating multimodal capabilities, models like GPT can process and generate data across various modalities such as text, images, and video, enabling AI to develop a more comprehensive understanding of the objective world.\nThe implementation of multimodal processing in LLMs involves several key steps: representation, alignment, fusion, and reasoning. These steps form the technical foundation for enabling multimodal understanding, generation, and inference.72-74\nModal Representation is the first step in multimodal processing. Each modality has its unique data structure. For example, textual data consists of discrete token"}, {"title": "4. Discussion and Conclusion: Technology for Understanding", "content": "Building on the previous discussion of intuition and causality, we further explore how the Transformer architecture, Chain-of-Thought reasoning methods, and multimodal capabilities support and expand our understanding of these concepts.\nWhile these technologies mark significant advancements in artificial intelligence, it is important to first acknowledge that AI, particularly artificial general intelligence (AGI), is still in its infancy. Our ambitions are vast, substantial room for improvement remains.\nThe core of the Transformer architecture lies in the attention mechanism, enabling the model to globally attend to all elements in the input data and assign"}]}