{"title": "FP=xINT: A Low-Bit Series Expansion Algorithm for Post-Training Quantization", "authors": ["Boyang Zhang", "Daning Cheng", "Yunquan Zhang", "Fangmin Liu"], "abstract": "Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP) models into quantized versions without training. While existing methods reduce size and computational costs, they also significantly degrade performance and quantization efficiency at extremely low settings due to quantization noise. We introduce a deep model series expansion framework to address this issue, enabling rapid and accurate approximation of unquantized models without calibration sets or fine-tuning. This is the first use of series expansion for neural network quantization. Specifically, our method expands the FP model into multiple low-bit basis models. To ensure accurate quantization, we develop low-bit basis model expansions at different granularities (tensor, layer, model), and theoretically confirm their convergence to the dense model, thus restoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations between isomorphic models in the low-bit expansion, forming an Abelian group to ensure operation parallelism and commutativity. The experiments show that our algorithm achieves state-of-the-art performance in low-bit settings; for example, 4-bit quantization of ResNet-50 surpasses the original accuracy, reaching 77.03%. The code will be made public.", "sections": [{"title": "1. Introduction", "content": "The huge computational cost and memory usage in deep learning have been unable to meet the needs of resource-constrained devices. Researchers have studied model quantization techniques, which aim to convert high-precision parameters and activations into low-precision parameters and activations. Quantization methods are mainly divided into two categories, Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT retrains the model on a labeled training dataset. Although the accuracy loss is small, it is time-consuming and computationally dense.\nIn the context of PTQ, the goal is to preserve the original accuracy of the model even when using extremely low-bit quantization, which facilitates parallel computing. With advancements in hardware capabilities, the ability to maintain the original accuracy becomes even more pronounced. Series expansion offers a systematic mathematical approach to approximate functions with the desired level of accuracy. However, traditional series expansions, such as the Taylor and Fourier series, are not directly applicable to deep learning models, as they suffer from issues like the curse of dimensionality and high computational costs.\nTo address these challenges, inspired by function series expansion, we are the first to use series expansion in deep learning quantization methods. We introduce a model series expansion framework that transforms the computationally inefficient original model into a computationally efficient basis function model class. Specifically, we progressively expand at both the tensor and layer levels. First, we develop the tensor-level and layer-level low-bit expansions and theoretically demonstrate that the expansion converges exponentially to the original tensor or layer. Building on these tensor and layer expansions, we propose a global model series expansion strategy. For efficient operations within and across isomorphic base models, we design the operations AbelianAdd and AbelianMul, allowing the combination of these operations and the base models to form an Abelian group. The series expansion framework thus enables efficient interactions among isomorphic models and constructs a set of base models composed of low-bit or sparse calculations. Finally, we establish convergence to ensure both computational accuracy and efficiency. Experiments show that our algorithm achieves state-of-the-art performance in low-bit settings. For instance, in ResNet-50, 4-bit quantization surpasses the original accuracy, reaching 77.03%. Moreover, the parallelism in our expansion approach enables quantization speeds that exceed most existing methods.\nThe following are the main contributions of this paper:\n1) We design the first model series expansion framework by series expanding the computationally inefficient original model into a computationally efficient basis function model class. 2) We design and construct tensor-level, single-layer-level, and model-level expansions and prove their convergence and efficiency. 3) Experiments show that our expansion framework has a significant improvement for extremely low-bit quantization."}, {"title": "2. Target Formulation", "content": "In this section, we introduce the paradigm of constructing series expansion in neural networks. We aim to construct a function expansion class that can be quickly calculated to approximate the original function. The basic constructor of series expansion is as follows\n$f(x) = \\sum_{i=1}^n a_i h_i(x),$\nwhere f (x) is the original function, and hi(x) forms a class of basis functions that can be quickly calculated. This form uses linear combinations of basis functions to systematically approximate the original precision. At the same time, addition and multiplication construct an Abelian group, which is friendly to parallel computing such as AllReduce or MapReduce. In neural networks, f(x), which is the model and we write as model(sample) in the following parts, is expanded within the domain of the input data. We expect that for any sample in the dataset, the deep learning model, model(sample), can be expanded into a specific series, i.e., model(sample) = $\\sum_{i=1}^n a_i h_i(sample)$, to accelerate the inference speed of the model. However, for deep neural networks, traditional series expansions such as multidimensional Fourier series are not applicable. Traditional series face the curse of dimensionality, especially when the model input dimension is large, the number of terms in the traditional series will grow exponentially. Therefore, we hope to redesign the series expansion formula to adapt to existing neural networks\n$model(sample) = \\sum_{\\Xi} model_i(sample).$"}, {"title": "3. Low-Bit Series Expansion Algorithm", "content": "Since the computational performance of low-bit integers is higher than that of floating-point numbers. Therefore, we perform series expansion on the original neural network, and the expansion terms are constructed through low-bit quantized networks. We construct the low-bit expansion paradigm of the model level by level (tensor-layer-model) and prove that it converges to the unquantized network."}, {"title": "3.1. Tensor Low-Bit Expansion", "content": "We treat integer-quantized functions as the computational kernel for constructing basis functions. To illustrate the concept of quantized basis functions, we provide the following example. Suppose there are two data objects, input1 and input2, to be subjected to a computing operation, such as multiplication. After the quantization process , we have $Q_1 = int(input_1)$ and $Q_2 = int(input_2)$, and obtain $Q_{output} = int(input_1 \\times input_2) \\approx int(Q_1 \\times Q_2 \\times \\frac{scale_1 \\times scale_2}{scale_{output}})$, scaleoutput, scale1, and scale2 are precalculated scale factors that depend on the distributions of input1, input2, and the output. Qi is stored as a lowerprecision data type, such as an integer. All scale terms can be pre-calculated and established ahead of time. Then, during the entire inference process, only the Qi value needs to be calculated, which is fast.\nWe set the tensor which will be quantized as M = (M1, M2, ..., Mn), n \u2208 N and convert it into m bits integer. We analyze several methods of quantization one by one. For symmetric quantization, the zero point of the quantized number is equal to the original vector, and for asymmetric, the quantization zero point is equal to the asymmetric point bias, represented by bias* Mnsy. For (non-) saturated quantization, clip+/- is used, and all $v_i > clip^+$ or $v_j < clip^-$ are set to clip+/-, where the resulting difference is represented by Msa. For different quantization methods, we construct series expansions to prove that they converge to the original neural network function. We can use the following Theorem 1 to expand the original dense float tensor \u039c.\nTheorem 1. $M = M_{sa} + bias * M_{nsy} + \\sum_{i=1}^n scale_i * M_i$, where Msa is a sparse float tensor which is produced by saturation quantization, Mnsy is the tensor whose all elements are 1. Mi is the tensor whose all elements are INT(X) data type and scalei = $2^X * scale_{i+1}$.\nProof. We use the non-saturation and symmetry quantization methods as the base for analysis, and other quantization methods are based on this case.\nIn non-saturated symmetric quantization, for the tensor M, when applying the non-saturation and symmetry with a scaling factor scale1 using X bits, the result is M1. Then we define R1 = M - $\\frac{scale_1 * M_1}{scale_1}$. R1 has following properties that all elements in Ro is smaller than scale1. Next, the values scale and -scale as the max elements in non-saturation and symmetry quantization process in R1's quantization process and the scale is calculated by R1. We build M2 by non-saturation and symmetry quantizing R1, and obtain R2 = M - scale1* M1 - scale2 * M2. This procedure allows for parallel computation, and the relationship $scale_1 = 2^m * scale_2$ holds between the scales. Repeating above process, i.e., non-saturation and symmetry quantizing Ri, we have M = $\\sum_{i=1}^n scale_iM_i + R_{n+1}$. The max value in Rn+1 is smaller than scale. When n \u2192 \u221e, the maximum value in Rn+1 is converged to zero. The size of Mi is equal to the size of M. For saturated symmetric quantization, the clipping operation in the saturated quantization method will produce errors, and we use another sparse tensor Msa to accommodate them. The sparse tensor Msa can be pre-calculated and is related to the data distribution, so Msa is a constant tensor. Then, this situation is converted to the unsaturated case.\nWhen it comes to non-symmetry quantization methods, the main difference between non-symmetry and symmetry quantization methods is that in the non-symmetry quantization process, $\\forall i = bias + scale * q_i$ where qi is the quantization result for original tensor vi. The $bias = \\frac{V_{max} - V_{min}}{2} + V_{min}$ for non-saturation quantization and the $bias = \\frac{clip^+ - clip^-}{2} + clip^-$ for saturation quantization. The bias is the same for all elements for the tensor. Thus, M = bias * Mnsy + scale1 * M1 + R1, where Mnsy is the tensor whose size is equal to the size of M. The method of processing Ri is the same as the non-saturation and symmetry quantization case. Above proof also gives the process of building Msa, Mnsy, Mi."}, {"title": "3.2. Single-layer Low-Bit Expansion", "content": "For a single-layer low-bit expansion in the neural network, we mainly focus on multiplication based on tensor low-bit expansion for the current deep learning model, the main computation kernel is tensor multiplication and it is the bottleneck of model computation performance.\nBased on Theorem 1, replacing the M tensor with weights W and activations A, we can expand W and A can be expanded into $W = W_{sa} + bias_w * W_{nsy} + \\sum_{i=1}^n scale_{w,i} * W_i$ and $A = A_{sa} + bias_a * A_{nsy} + \\sum_{i=1}^n scale_{A,i} * A_i$. We treat bias as scale0, scale-1 = 1, Wnsy as W0, Wsa as W-1, Ansy as A0, and Asa as A-1 for better description.\nThen we have the following Eq. 3, and we term it as tensor multiplication low-bit expansion\n$WA = \\sum_{i,j \\in [-1,n]} scale_{w,i}scale_{A,j}W_iA_j.$"}, {"title": "3.3. Model Low-Bit Expansion", "content": "The deep model consists of multiple single layers. We want to expand it into t2 basis model. We can expand all the single layers whose calculation kernel is multiplication into low-order series items, and duplicate the remaining layers into basis models. Therefore, for the entire deep model, we can follow the steps below to build a low-order model:\n1) Expand all tensor multiplications layer by layer. 2) For other layers, copy it into the basis model and the output of them and multiply . 3) modeli,j, i, j \u2208 [\u22121, n] is the the model, which always chose the Layer; item of the low-order expansion of the original layer of the original model, and then perform multiple base model operations.\nEstablishing the Abelian Group. Unlike the series expansion of single layers and tensors, in the entire neural network, we prioritize designing the base model operations as well as parallelizing multiple series extension models. The original multiplication-addition cascade limits the parallelism and efficiency of the model series expansion. Therefore, we introduce basis functions and new operations to form an Abelian group, as the binary operations in an Abelian group align with the reduction operations used in AllReduce.\nFirst, we expand the addition to adapt to neural networks. We define an operation AbelianAdd (\u3143) to construct an Abelian group.\nDefinition 1. \u311d: In neural networks, the output of each layer is multiplied by the scale, added, and broadcast to the input of the next layer.\nThis operation applies to current neural networks because they are composed of layers with parameters and outputs, and AbelianAdd builds an Abelian group. We use $\\sum_{i=0}^n$ to describe the multi-AbelianAdd operations. Similarly for multiplication, because the the parameter w in layer j is the integer multiple of scalej, thus, the w = scalej * w where w is the low-bit integer matrix.\nThe AbelianAdd satisfies the following property\nModel(W1, A, sample) \u3143 Model(W2, A, sample) =Model(W1+ W2, A, sample),\nModel(W, A1, sample) \u3143 Model(W, A2, sample) =Model(W, A1 + A2, sample),\nwhere W = [W1, W2...] and A = [A1, A2, ...]. Wi, Ai is the weight and activation ith layer. Model(W, A, sample) means the model whose weight is W, activation is A, and the input is sample. It is easy to see that modeli,j = Model (Wi, Aj, sample).\nSecond, we define the AbelianMul (*) to simplify description as following,\nDefinition 2. A vector U = (u1, U2, ..., uk) AbelianMul the model means that the parameter Wi in layer i multiply the ui, U*model(wi) = model(ui * Wi).\nIn the low-bit expansion of the model, the vector U represents scale, and an Abelian group is formed between the operation (AbelianMul, AbelianAdd) and the isomorphic model. This allows the basis function model to be efficiently expanded. Note that the new operation does not change the model structure and can be regarded as a cascade expansion operation adapted to the neural network.\nModel Low-Bit Expansion. In a multi-layer neural network, we construct a high computational efficiency hi in Eq. 1 through the predefined AbelianAdd and AbelianMul and can avoid the dimensionality curse. We use the isomorphic neural network but all parameters and input of the layer are scalej's integer multiple in layer j or tensor matrix as hi. Thus, we have the deep learning model's low-bit expansion that for any deep learning model model. We reconstruct Eq. 2 (model(sample) = $\\sum_i model_i(sample)$),"}, {"title": "Theorem 2.", "content": "Form any locally continuous deep learning model model, whose core kernel is matrix multiplication or can be converted into multiplication, weight is W and activation A we have the following expansion:\n$model = \\sum_{i,j \\in [-1,n]} modeli,j$\\\n$=\\sum_{i,j \\in [-1,n]} scale_{i,j} modeli,j,$\nwhere model is the model whose parameters in a layer are sparse or integer multiple for scale in scale, and model is the model whose parameters in a layer is sparse or low-bit integer.\nProof. Based on the Eq. 5 and Eq. 6, we have\n$=\\sum_{i,j \\in [-1,n]} modeli,j$\\\n$=\\sum_{i,j \\in [-1,n]} Model (W_i, A_j, sample)$\\\n$= Model (\\sum_{i \\in [-1,n]}Wi, \\sum_{j \\in [-1,n]}A_j, sample)$\nBased on Theorem 1, we know that $\\sum_{i \\in [-1,n]} W_i$ is convergent to W exponentially and $\\sum_{j \\in [-1,n]} A_j$ is convergent to A exponentially. So, based on the locally continuous property, model = $\\sum_{\\psi,i,j \\in [-1,n]} modeli,j$.\nBased on Theorem 2, the inference process can be expanded into the pattern shown in Figure 3. In this pattern, all model are low computation resource models whose main computation kernel is low-bit integer or sparse. Each layer computes the quantized activation and tensor multiplication independently, which is shown in the following section, and all reduce the output of each model."}, {"title": "4. The Complexity Analysis of Low-Bit Series Expansion", "content": "Theorem 2 shows the equivalence of the model, but from the perspective of model performance and parallel equivalence, the computational complexity can be further reduced.\nThe Weight Expansion Upper Bound. From the low-order expansion of the tensor multiplication of WA, when W and A are expanded by t term, we have to calculate t2 matrix multiplication to obtain accurate results in Figure 2. However, in post-training quantization, we do not have to compute t2 low-bit matrix multiplications. For a well-trained model on a dataset and its loss function l, the loss function's gradient of weight W is zero, i.e.,$\\frac{\\partial l}{\\partial W} = 0$. Thus, when W is introduced the error, the loss function value l(model(W)) \u2013 l(model(W + error)) = $\\frac{\\partial l}{\\partial W}$ * error = 0. So from the perspective of the loss function, it makes no sense to expand the W matrix by too many terms. For the matrix expansion W = Wsa + biasw * Wnsy + $\\sum_{i=1}^n scale_{w,i} * W_i$ by INTX, the maximum n should satisfy the influence of $scale_n *2^x$ should be described by total differential. The above condition means that the $scale_n * 2^X$ should be small enough, empirically, scalen * 2X < 10-2, and usually, the weight parameter only has to expand 2 or 3 terms. Furtherly, in practice the influence of Asa and Wsa on the loss function is small.\nBased on the above analysis, we only need to expand the A matrix multiple times to obtain better performance, and we have to compute O(t) low-order matrix multiplications instead of O(t\u00b2) low-order matrix multiplications, i.e., model = $\\sum_{\\omega,i \\in [0,k], j \\in [0,t]} scale_{i,j} \\hat{\\omega}model_{i,j}$ and k is small instead of model = $\\sum_{\\omega,i,j \\in [0,1]} scale_{i,j}*model_{i,j}$ from the view of loss function.\nThe Computation Complexity of Mnsy Multiplication. We can see the Mnsy is the tensor whose all elements are one, which means Mnsyn is are low-rank matrix. In fact, $Mnsy_n = one*one$, where one is a vector and one = (1, 1, ..., 1). So, for many matrix multiplication $MMnsy_n = Moneone = (Mone)one$. The computation complexity of the latter process is O(n\u00b2).\nThe Parallelization of Computing M\u2081. As we can see from the proof of Theorem 1, we only use the non-saturation quantization process at the first time to produce the M1. In the computing process of Mi, i > 1, we set the maximum element of Ri, i > 1 as scalei-1. We use this setting because we the property, scale\u2081 = 2X scalei+1, to parallelize the computing process.\nBegin with the non-saturation symmetry case, we can easily gain that the (i, j) element in Mk can be calculated as $M_k(i, j) = INTX(\\frac{M(i,j)}{2^X}) * INTX(\\frac{M(1,1)}{2^X}) * 2^X$. The cases of saturation and non-symmetry cases can be converted into non-saturation symmetry cases by adding Mnsy and Msa."}, {"title": "5. Experiments", "content": "We conduct series expansion quantization experiments on various models on Imagenet and NLP tasks, including ResNet , RegNet , etc. Our method sets hyperparameters consistently on all models and quantizes channel by channel. The basis function selects the class of integer quantization functions. We determine the value of clip+/- during saturation quantization to minimize the impact of Msa. In non-Saturated quantization, we use the expected quantization noise in the Laplace distribution as"}, {"title": "5.3. Ablation", "content": "Ablation of Non-saturation. As shown in Figure 4a, we perform saturation and non-saturation ablations on the quantization basis functions. When the clipping function is not used, the quantization performance on the four models is slightly worse, but still better than existing methods. When using the Laplace clipping function, the accuracy of our algorithm is close to full accuracy.\nAblation of the Expansion. As shown by the orange line in Figure 4b, when the number of expansions increases, the accuracy of ResNet-50 increases and gradually approaches the accuracy of the original model. This is due to the decrease in the difference between the activations of the original model and the quantized model, as shown by the blue line. When the number of expansions is 4, the model has the best accuracy. When the activations continue to expand and the number exceeds 5, the maximum difference will continue to decrease, but the accuracy will change slightly, while the calculation time will increase significantly. Therefore, in the specific implementation, we stipulate that when the maximum difference is less than 10-4, the number of expansions is optimal. It is worth mentioning that the error caused by quantization on weights is smaller than that on activations, so the weights are not expanded more than twice. This also verifies theoretical analysis.\nAs shown in Table 5, we ablate the accuracy of only expanding weights or activations. When the weights and activations are expanded the same number of times, it can be seen that when only the activations are expanded, the impact on the accuracy is greater than that of only the weights. Therefore, series expansions of the activation quantization are necessary to help improve the accuracy."}, {"title": "5.4. Discussion", "content": "Series Expansion \u2260 Ensemble. Our approach fundamentally differs from the combination of multiple integer (INT) models. The ensemble of multiple INT models cannot converge to the original model, while our method can converge to the unquantized model, focusing on computing the series expansion of different low-bit models. Ensemble models combine the parameters of multiple similar quantized models but do not achieve convergence. Experimental results show that when existing ensemble methods are used to combine multiple INT models, there is no performance gain, and the performance actually decreases.\nGeneralization to Large Language Models. As shown in Table 6, although our work primarily focuses on small models for specific tasks, we still explore its application in LLMs. Following the quantization settings of W4A16, our approach remains effective in LLMs. On MMLU, our method achieves accuracy nearly equivalent to the original model and surpasses it in certain areas, demonstrating the efficiency and superiority of series expansion. In the future, we will focus on the quantization of LLMs.\nScalability and Basis Function Diversity. Our goal is to leverage the higher throughput of INT processing units to achieve higher inference speed while using the same hardware to serve the model. Compared to the original floating-point (FP) model with the same number of hardware units, our method quantizes the model with arbitrary bits (e.g., 2-bit) while maintaining high accuracy. The quantization results under INT8 further demonstrate that our algorithm can achieve near full precision with only integer data types and achieve about 4 times the throughput of FP32 on INT8 processing units. Therefore, our method enables inference acceleration on specific hardware that supports accelerated operators, showing great potential for hardware design and implementation of pure INT-based operators.\nAdditionally, our method offers a conventional approach to balancing computational efficiency and model performance. Unlike existing complex quantization methods, our solution focuses on applying series expansion to neural networks and incorporates traditional quantization methods.\nIt is worth mentioning that existing quantization schemes can be embedded in our series expansion algorithm to further improve performance. Finally, the selection of basis functions can be diversified, meeting both computational efficiency and hardware friendliness requirements. In this study, we used integer models as basis functions, but in practice, we could also use sparse models or other computation-friendly models as the base functions."}, {"title": "6. Related Work", "content": "Series Expansion and Parallel Processing. Series expansion is a method of representing a function as an infinite series, which can be used to approximate complex functions. The expanded functions are called basis functions, which can express or approximate other functions through linear combinations. Traditional basis functions are trigonometric functions and polynomial functions, such as Fourier expansion and Taylor expansion. Because today's computers are good at calculating polynomial functions and matrix operations. So we can get computational benefits from series expansion by converging functions. However deep models are designed with a serial architecture, which slows down computational efficiency. So the operation of the expansion terms needs to be processed in parallel. AllReduce is a key operation in parallel computing. To parallelize a process through AllReduce, its operation must be an Abelian group. This means that the reduction operation needs to satisfy both the commutative law and the associative law, so the order of the combined values does not affect the result. The reduction operation in AllReduce corresponds to the binary operation in the Abelian group. Series expansion has the advantages of parallelism and accurate approximation. Our algorithm is an extension of traditional expansion in deep learning, overcoming problems such as the curse of dimensionality and inheriting the advantages of parallelism and accuracy.\nQuantization is one of the most popular techniques for compressing neural networks, generally divided into two main approaches: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT incorporated quantization during the network training phase, whereas PTQ applied quantization after training. PTQ was widely used in network deployment due to its low time and computational resource requirements. HAWQ-v2 used the trace of a layer's Hessian matrix as an indicator of the layer's sensitivity. The work formulated the mixed-precision problem for weights and activations as a Lagrangian optimization problem, where the solution determined optimal precision allocation across weights and activations. AdaQuant optimized its parameters on a calibration set to minimize quantization error for each layer individually. PD-Quant alleviated the overfitting problem in PTQ caused by the small number of calibration sets by adjusting the distribution of activations. Shang et al. introduced mutual information into PTQ calibration to optimize the quantization parameters. These methods focus on the quantization parameters to alleviate the error, but cannot achieve good results at very low bits and run slowly. Our method achieves performance and speed improvements at low bits and does not require calibration sets and fine-tuning."}, {"title": "7. Conclusion", "content": "This paper proposes a deep model series expansion framework that aims to replace the computationally inefficient original model with multiple computationally efficient basis function models. We use it in PTQ to expand the FP model to the INT version and prove its convergence. Theoretical and experimental results show that the algorithm improves the parallel capability of the model and guarantees the performance of the low-bit model without the need for calibration sets and fine-tuning."}]}