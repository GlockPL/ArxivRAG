{"title": "Yi-Lightning Technical Report", "authors": ["Bei Chen", "Chao Li", "Chengen Huang", "Fan Zhou", "Jun Tian", "Peng Liu", "Yanpeng Li", "Yuchi Xu", "Xiaoyi Ren", "Xinyao Niu", "Zhiyuan Liu", "Albert Wang", "C.X. Lv", "Chenglin Cai", "Chujie Zheng", "Heng Ji", "Tianhang Zhu", "Katherine Su", "Xiang He", "Ming Song", "Xiaohui Hu", "Yuxuan Sha", "Shawn Wang", "Ethan Dai", "Feng Hu", "Jiangcheng Zhu", "Lihuan Zhang", "Liying Li", "Mou Li", "Qichen Hu", "Wen Xie", "Xiaobo Chen", "Yongke Zhao", "Zhaodong Yan", "Zirui Zhang", "Shijun Zhou", "Shiyong Li", "Yongzhen Luo", "Alan Wake", "Daniel Cooper", "Howard Qiu"], "abstract": "This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revealed fascinating prospects toward artificial general intelligence (AGI), attracting an enduring enthusiasm and interest of the community [Achiam et al., 2023, Gemini Team et al., 2023, Dubey et al., 2024, Yang et al., 2024, 01.AI, 2024]. With our mission in mind to empower the community with advanced AI technology and exceptional AI service experiences, we release this technical report and introduce our new-generation flagship model, Yi-Lightning. As of its first appearance on October 16, 2024, Yi-Lightning achieved a remarkable overall ranking of 6th place on the Chatbot Arena leaderboard [Zheng et al., 2023b] (as shown in Figure 1), a leading LLM benchmark based on real-world human judgment and comparison. In specialized categories such as Chinese, Math, Coding, and Hard Prompts, it also ranks among the top performers (ranging from 2nd to 4th place), demonstrating a comprehensive high-performance standard in practical scenarios.\nWe attribute Yi-Lightning's excellent performance to our innovations in model architecture (\u00a7 2), training strategies, data engineering (\u00a7 3 and \u00a7 4), and infrastructure (\u00a7 5). These efforts work closely together, consequently enabling Yi-Lightning's efficient training, deployment, inference, and its high practical efficacy:\n\u2022 In terms of model architecture, Yi-Lightning is based on an improved mixture of experts architecture. It employs fine-grained expert segmentation, complemented by a balanced expert routing strategy and cross-layer KV cache sharing design, achieving more efficient training and inference.\n\u2022 Regarding training strategies, Yi-Lightning extensively utilizes specialized multi-stage and strategic training recipes in pre-training, supervised fine-tuning, reward modeling, and human preference alignment optimization, achieving more efficient performance optimization.\n\u2022 In data engineering, building upon general domain foundations, we design data synthesis approaches for difficult and complex tasks (such as mathematics and coding), significantly enhancing Yi-Lightning's problem-solving capabilities.\n\u2022 For infrastructure, we implement substantial optimizations in parallelism strategies, node management and scheduling, storage, and network communication, greatly improving goodput performance.\n\u2022 Additionally, we implemented RAISE, a four-component framework to address the safety concerns across Yi-Lightning's entire lifecycle from development to deployment.\nFinally, we report Yi-Lightning's evaluation results on public academic benchmarks. While Yi-Lightning still performs competitively against other top-tier LLMs, we observe a notable disparity between the evaluation results on these academic benchmarks and real-world human judgments on Chatbot Arena, which probably results from our focus on optimizing toward practical experiences rather than overly paying attention to the benchmark scores. This observation stimulates us to reconsider the role of the current academic benchmarks in guiding more intelligent and powerful AI systems and to design alternative approaches to evaluating model performance in practical scenarios."}, {"title": "2 Model Architecture", "content": "Similar to recent large language models [Dai et al., 2024, Yang et al., 2024, Jiang et al., 2024a, Abdin et al., 2024], Yi-Lightning is fundamentally built upon the Mixture-of-Experts (MoE) architecture. Beyond this, we introduce several architectural innovations in Yi-Lightning, including fine-grained expert segmentation methodology, advanced routing strategies, and optimized key-value cache reduction techniques."}, {"title": "2.1 Fine-grained Expert Segmentation", "content": "Recent research has revealed that as dense models grow in size, their activation patterns become increasingly sparse [Zhang et al., 2024b,a]. This sparsity indicates that parameters are not uniformly utilized during inference, leading to computational inefficiencies. The Mixture-of-Experts (MoE) architecture addresses this challenge by selectively routing tokens to activate only specific neural subsets. However, even with MoE models, the issue of sparse activations persists within individual experts, suggesting a fundamental challenge in parameter utilization efficiency.\nDrawing inspiration from [Dai et al., 2024], we adopt a fine-grained expert segmentation strategy. This approach involves partitioning each expert's Feed-Forward Network (FFN) into smaller functional units, simultaneously reducing intermediate hidden dimensions while increasing the number of experts activated per token. This fine-grained segmentation facilitates more nuanced knowledge decomposition, enhances expert activation combinations, and improves overall parameter utilization efficiency. In practice, we observed that excessive expert segmentation substantially impacted training throughput. Consequently, we opted for a balanced approach, implementing segmentation only to the extent necessary to maintain optimal training efficiency rather than pursuing maximum segmentation for performance gains."}, {"title": "2.2 Expert Routing Strategy", "content": "Expert routing strategy plays a crucial role in optimizing training efficiency and model quality. Following Switch-Transformer (ST) [Fedus et al., 2022], we initially implemented a load balancing mechanism with an auxiliary loss function for N experts and a batch B containing |B| tokens:\n$L_{ST} = \\alpha_{ST} \\cdot N \\cdot \\sum_{i=1}^{N} f_i P_i$.\nHere, $f_i$ represents the fraction of tokens x routed to each expert, and $P_i$ denotes the average routing probability allocated to expert i:\n$f_i = \\frac{1}{|B|} \\sum_{x \\in B} \\mathbb{I}[argmax_j(x) = i], P_i = \\frac{1}{|B|} \\sum_{x \\in B} p_i(x)$,\nwhere $p_i(x)$ represents the probability of token x being assigned to expert i. Given that $\\sum_{i=1}^{N} f_i = 1, \\sum_{i=1}^{N} P_i = 1$, it can be easily shown via the Lagrange multiplier method that the loss function reaches its minimum when tokens are evenly distributed, with both $f_i$ and $P_i$ approaching 1/N for all experts. Therefore, this mechanism can effectively encourage uniform routing and balanced utilization across experts during training.\nHowever, we observed that while this load balancing effectively prevents expert collapse and enhances training efficiency, the per-expert constraints prove overly restrictive even with carefully tuned $\\alpha_{ST}$. We thus propose to relax the constraints from individual experts to Expert Parallel (EP) groups (see \u00a7 5.1) by introducing the EP load balancing mechanism, which optimizes the load balance within each EP group:\n$L_{EP} = \\alpha_{EP} \\cdot N^g \\cdot \\sum_{i=1}^{N^g} f_i^g P_i^g$,\nwhere $N^g$ denotes the number of experts in the EP group, with group-specific calculations:\n$f_i^g = \\frac{1}{|B^g|} \\sum_{x \\in B^g} \\mathbb{I}[argmax_j(x) = i], P_i^g = \\frac{1}{|B^g|} \\sum_{x \\in B^g} p_i(x)$,\nwhere $B^g$ denotes the batch tokens allocated to the specific EP group.\nNevertheless, a significant limitation of the above load balancing approaches still remains, as they are unable to address the token dispatching imbalance during All-to-All communication in expert parallelism. This imbalance results in fluctuating computation and communication intensities across EP groups, reducing training efficiency. The issue is further compounded by expert segmentation as it increases tokens that need dispatching. To address this challenge and enable finer-grained load balancing control, we introduce partitioned EP load balancing (PEP), which splits experts within each EP group into smaller partitions. This approach ensures balanced token distribution across partitions, gradually optimizing communication and computation loads. For a partition (within a group of $N^g$ experts) containing $N^p$ local experts, the loss is computed as:\n$L_{PEP} = \\alpha_{PEP} \\cdot N^p \\cdot \\sum_{i=1}^{N^p} f_i^p P_i^p$,\nwhere $f_i^p$ and $P_i^p$ represent the token fraction and the average routing probability for expert i in this group partition, respectively:\n$f_i^p = \\frac{1}{|B^p|} \\sum_{x \\in B^p} \\mathbb{I}[argmax_j(x) = i], P_i^p = \\frac{1}{|B^p|} \\sum_{x \\in B^p} p_i(x)$.\nTo maintain effective load balancing, we optimize $L_{PEP}$ in conjunction with $L_{ST}$ and $L_{EP}$. In practice, we carefully tuned and set $\\alpha_{PEP}$, $\\alpha_{EP}$, and $\\alpha_{ST}$ to $10^{-3}$, $10^{-4}$, and $10^{-6}$, respectively."}, {"title": "2.3 KV Cache Reduction", "content": "To enhance long-context processing while substantially reducing inference costs, we introduce two key architectural innovations. First, we observed that while most attention heads primarily focus on local context, only a small subset specializes in global information processing. Motivated by this, we implement hybrid attention blocks that combine three sliding window attention [Jiang et al., 2023] layers with one full attention layer, effectively capturing both local patterns and global dependencies. Second, we optimize memory utilization by cross-layer KV cache reuse, which shares key-value (KV) cache states between consecutive full attention layers and reduces memory requirements by half for full attention components. These innovations collectively achieve up to 82.8% memory reduction while maintaining model performance on long sequences."}, {"title": "3 Pre-training", "content": "We then describe the pre-training methodology of Yi-Lightning. While building upon the experience of training our previous Yi models [01.AI, 2024], we specifically optimize data processing and composition, closely integrating these improvements with pre-training progress to design an advanced multi-stage training approach."}, {"title": "3.1 Data", "content": "Our pre-training corpus comprises multilingual web documents (crawled through early 2024), books, academic papers, codebases, and question-answer pairs. Building upon the data processing pipeline from [01.AI, 2024], we strengthen our filtering mechanisms for unsafe content and personally identifiable information (PII), and particularly implement several key improvements detailed below.\nTokenization We employ byte-pair encoding (BPE) for text tokenization [Shibata et al., 1999] with the SentencePiece implementation [Kudo and Richardson, 2018]. In contrast to our previous Yi models [01.AI, 2024], we expand the vocabulary size to 100,352 tokens to enhance multilingual support. To improve the model's comprehension of numerical information, we decompose numbers into individual digits. Additionally, our tokenization strategy incorporates unicode-byte encoding as a fallback mechanism for rare characters, ensuring robust fault tolerance in text processing.\nEnhanced Mathematical and Programming Content We have increased the proportion of mathematical and programming content in our pre-training corpus. Mathematical content is collected from Common Crawl using an iterative classification approach [Shao et al., 2024], supplemented with mathematical materials from books and academic papers. For programming content, we primarily utilize GitHub repositories, following cleaning procedures similar to [Guo et al., 2024]. To prevent data contamination in subsequent evaluations, we filter out entries sharing any 30-gram with the training or test sets of popular benchmarks, such as MATH [Hendrycks et al., 2021], GSM8K [Cobbe et al., 2021], HumanEval [Chen et al., 2021], and MBPP [Austin et al., 2021].\nSemantic-based Document Organization Inspired by Shi et al. [2024], we implement large-scale clustering of documents with similar semantic features and concatenate them into extended sequences. These sequences are segmented into fixed-length pieces (8,192 tokens) for pre-training, with a high-quality subset reserved for subsequent long-context extension training.\nFine-grained Content Classification We develop a series of fine-grained classifiers for text types and topics, trained on annotations generated by smaller Yi models. The final pre-training data composition was determined through extensive experimentation with various dataset weighting schemes. We observed that focused training on a smaller volume of high-quality domain-specific data can enhance key model capabilities."}, {"title": "3.2 Training Strategy", "content": "Our training methodology follows a three-stage approach that optimizes learning rate schedules and strategic data sampling to maximize model performance [Ibrahim et al., 2024, Hu et al., 2024]: initial pre-training, mid-training, and fast-decay training.\nThe initial pre-training stage employs a warm-up schedule where the learning rate decays to half of its peak value. This strategy enables thorough exploration of the parameter space while avoiding premature convergence. During this stage, we emphasize data diversity to establish robust foundational capabilities across diverse domains.\nIn the mid-training stage, we focus on enhancing model capabilities and extending context length through gradual data distribution shifts. We implement an incremental upsampling strategy for high-quality data, emphasizing complex reasoning and multilingual capabilities for low-resource languages. We optimize training efficiency and improve throughput through dynamic batch size adjustments based on loss values.\nThe final fast-decay training stage, consuming about 12.5% of total training tokens, combines an aggressive learning rate decay with dynamic batch size optimization. This stage intensifies high-quality data upsampling and incorporates early instruction-tuning adaptation. It is notably designed to be iteratively flexible, allowing multiple optimization cycles tailored to specific deployment requirements for better practical utility."}, {"title": "3.3 Long Context Extension", "content": "After the fast-decay training stage, we apply additional long-context training to extend the context window to 64K tokens. We employ Rotary Position Embedding (RoPE) [Su et al., 2021] with increased base frequency during extension [Xiong et al., 2023]. This training process systematically upsamples sequences from multiple length intervals (8K-16K, 16K-32K, and 32K-64K tokens) while maintaining consistent data distribution [Fu et al., 2024]. We found that a training corpus of 20B tokens successfully developed robust long-context capabilities without compromising performance on standard benchmarks."}, {"title": "4 Post-training", "content": "We next detail Yi-Lightning's post-training methodology. Our approach incorporates the sequential stages of Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). We particularly elaborate on our strategies for multi-stage model training as well as data curation and synthesis."}, {"title": "4.1 Supervised Fine-tuning", "content": ""}, {"title": "4.1.1 Data and Training Strategy", "content": "Multi-stage Training and Data Curation Our supervised fine-tuning (SFT) process involves two sequential stages, leveraging 1.3M and 300,000 samples respectively. The first stage focuses on enhancing fundamental capabilities in mathematics and coding through extensive synthetic data, while the second stage utilizes diverse, high-quality general-domain data to boost instruction following and problem-solving capabilities. Furthermore, we implement a small-to-large data scaling strategy to expand our dataset systematically across both stages. We first compile a comprehensive prompt set from diverse sources using efficient selection strategies, and then generate corresponding responses through manual curation and synthetic approaches. For example, in the second stage, we methodically expanded from approximately 10,000 initial, high-quality seed samples to the target of 300,000 samples. This two-stage methodology, combined with the progressive data expansion strategy, effectively addresses data imbalance while rapidly enhancing model capabilities.\nSynthetic Data Generation Synthetic data has proved instrumental, particularly for complex tasks including instruction following, code generation, and mathematical problem-solving. We employ multiple synthesis techniques including document augmentation, self-evolution, and language translation for prompt generation. For general tasks, we leverage multiple advanced models for response generation, combining automated systems and manual verification for quality control. For complex tasks like coding and mathematics, we integrate search algorithms, including Monte Carlo Tree Search (MCTS) and Depth-First Search (DFS), with specialized outcome and process reward models [Lightman et al., 2023] to generate diverse, accurate solutions. These methodologies yield a substantial corpus of high-quality, diverse training data, contributing significantly to our model's initial capabilities."}, {"title": "4.1.2 Optimized Implementations", "content": "We implement sample packing, which concatenates multiple samples into single sequences rather than applying individual padding. While this approach substantially reduces training sequences and improves efficiency, it can create artificial multi-turn contexts that potentially compromise certain model capabilities, particularly in multi-turn dialogues. We address this challenge by implementing block causal attention (BCA), which isolates samples within sequences through masking matrices. We also observed that sample packing could introduce potential biases where longer samples disproportionately influence the total loss, potentially undermining short-sample task performance. We thus develop a sample reweighting mechanism that equalizes loss weights across all samples within a batch, which effectively mitigates the length-induced optimization bias."}, {"title": "4.2 Reinforcement Learning from Human Feedback", "content": "Training language models with human feedback has emerged as a crucial step for practical applications to align model behavior with human preferences [Bai et al., 2022, Ouyang et al., 2022, Zheng et al., 2023a, Rafailov et al., 2023, Xiong et al., 2024, Zheng et al., 2024a], which is also key to Yi- Lightning's exceptional performance on Chatbot Arena, where evaluations are based on direct human"}, {"title": "4.2.1 Reward Modeling", "content": "Following Bai et al. [2022, 2023], we implement a two-stage approach for reward model training: preference model pre-training (PMP) and human-feedback fine-tuning (HFFT).\nPMP Data Construction Our PMP dataset incorporates diverse preference datasets from public sources [Cui et al., 2023, Wang et al., 2024b, LLaMA Factory, 2024]. Given the varying quality standards and potential redundancies in these datasets, we implement rigorous cleaning and preprocessing protocols. We assess dataset quality by training individual reward models and evaluating their performance on our in-house benchmarks, retaining only those datasets yielding high benchmark performance.\nHFFT Data Construction The HFFT dataset is constructed using comprehensive human annotations. We collect prompts from curated public datasets and then generate responses using model checkpoints from various SFT training phases. These responses are evaluated across category-specific dimensions. For instance, responses for coding prompts are assessed on instruction adherence, correctness, and code style. We form preference pairs by selecting the highest and lowest-scoring responses for each prompt, based on weighted dimensional scores, where the pairs with insufficient score differentials are excluded.\nReward Model Training With the above curated datasets, the reward model training initializes from a pre-trained model and proceeds through the two sequential stages (PMP and HFFT) using the Bradley-Terry loss [Bradley and Terry, 1952]."}, {"title": "4.2.2 Preference Data", "content": "Prompts We generate prompts for preference learning through two approaches: collecting from public datasets and prompt synthesis. We first collect diverse prompts from various domains (coding, mathematics, general QA, etc.) to establish a comprehensive foundation across multiple instruction contexts. To enhance the model's capability in handling complex queries, we synthesize additional challenging prompts. We assign complexity scores based on instruction context complexity, analytical requirements, and output format specifications. High-scoring prompts are selected as seed prompts and paired with diverse seed contexts collected from high-quality web sources to create synthesized prompts. Finally, we apply multiple deduplication techniques, including n-gram similarity analysis, embedding-based comparisons, and random downsampling, to ensure the uniqueness of prompts while preserving their diversity.\nPreference Pairs To ensure balanced data distribution and rational reward assignment, we categorize prompts across multiple dimensions: complexity level, user intent clarity, and domains (e.g., math, code, general QA). This categorization guides prompt balance adjustments and informs rating criteria weights for different categories. For each prompt, we generate multiple responses using the SFT model with varying temperature settings. These responses are evaluated using the reward model in \u00a7 4.2.1, and preference pairs are formed by selecting the highest and lowest-scoring responses while ensuring a sufficient reward gap to minimize the impact of reward modeling error."}, {"title": "4.2.3 Direct Preference Optimization", "content": "We conduct training for human preference alignment via the direct preference optimization (DPO) algorithm [Rafailov et al., 2023]. Inspired by recent work on iterative DPO training [Yang et al., 2024, Xiong et al., 2024], we conduct the DPO training in two sequential stages: offline and online training. In the offline DPO training stage, we train the model on the preference dataset constructed in \u00a7 4.2.2. In the online DPO training stage, we further extend the offline training with the real-time dataset generated by the most recent model. For each prompt, we sample 16 candidate responses and form a preference pair using the reward model in \u00a7 4.2.1, which is then used for model training in the next iteration. We conducted two iterations of online DPO training in total."}, {"title": "5 Infrastructure", "content": ""}, {"title": "5.1 Parallelism Optimization", "content": "Given the architectural characteristics of MoE models (\u00a7 2), we implement a hybrid parallelization strategy combining expert parallelism and pipeline parallelism. We further enhance the pipeline parallelism through several optimizations, including customized pipeline stage partitioning and fine- grained gradient recomputation strategies. These improvements enable optimal memory utilization and workload distribution across devices while maintaining training stability and enhancing overall throughput.\nTo fully exploit the advantages of both hybrid attention (\u00a7 2.3) and context parallelism in long- context scenarios (\u00a7 3.3), we introduce several refinements to the context parallelism implementation. These modifications enable efficient integration with the hybrid attention mechanism, particularly in optimizing the distribution of sliding window attention computations across the context parallel dimension. Our approach significantly reduces the computational burden on individual context parallel ranks, resulting in an up to 70% training speedup."}, {"title": "5.2 Inference Optimization", "content": "Yi-Lightning leverages a high-performance inference engine optimized specifically for LLM inference, effectively addressing the computational and memory bottlenecks. Through integrated algorithmic and engineering optimizations, the system achieves substantial reductions in resource consumption while delivering exceptional inference efficiency. The key optimizations include:\nAdvanced Asynchronous Scheduling at Engine Level Traditional LLM inference solutions often suffer from suboptimal GPU utilization (typically below 70%) due to serial dependencies between modules causing GPU idle time. We implement sophisticated multi-module, multi-process asynchronous scheduling that decouples task execution and minimizes inter-module latency. This enhancement achieves 95% GPU utilization in high-concurrency scenarios, markedly improving both engine performance and hardware resource efficiency.\nOptimized FP8 Quantization and Hardware-Aware Operator Design Yi-Lightning's architecture is fundamentally designed with GPU hardware characteristics in mind, particularly for FP8 quantization compatibility. The model architecture precisely aligns with hardware specifications, maintaining algorithmic precision while maximizing hardware utilization. Our training infrastructure fully exploits the Nvidia Hopper architecture through custom-developed high-performance operators. A notable example is our implementation of the Mixture-of-Experts (MoE) operator, which employs an expert-parallel strategy achieving 1,200 TFLOPS per card at FP8 precision on Hopper GPUs. This represents a performance improvement exceeding 100% for operator execution, substantially enhancing overall inference efficiency.\nThe combined impact of these optimizations - enhanced hardware utilization through asynchronous scheduling and efficient operator implementation - enables Yi-Lightning to effectively address computational and memory constraints in high-concurrency, high-throughput inference scenarios, making it ideally suited for large-scale AI service deployment."}, {"title": "5.3 Goodput Optimization", "content": "Industry leaders like Meta [Dubey et al., 2024], Google [Gemini Team et al., 2023], and Alibaba [Dong et al., 2024] have achieved goodput levels exceeding 90% by employing advanced techniques such as fast checkpointing, fault-tolerant scheduling, and hardware redundancy. For instance, Meta's Grand Teton AI system [Bjorlin, 2022] leverages RDMA over RoCE and Infiniband networks to maintain high performance, while ByteDance's MegaScale [Jiang et al., 2024b] system focuses on rapid error detection and recovery to minimize downtime. Similarly, Google and Alibaba have implemented proactive hardware health monitoring and network optimizations to sustain near-maximum goodput even in the face of frequent hardware failures.\nBuilding on these insights, we adopt a multi-layered approach to goodput optimization in our large-scale GPU cluster, XCloud:\nFault Tolerance through Proactive and Reactive Mechanisms One of the most significant contributions to goodput optimization is the combination of proactive and reactive fault discovery strategies. Proactive measures such as routine, entrance, and preflight tests ensure cluster health by identifying potential hardware and software issues before they impact workloads. On the reactive side, XCloud employs advanced monitoring tools like node exporters\u00b9 and custom InfiniBand metrics collectors to detect faults in real time. These systems work in tandem to minimize the duration and impact of failures, enabling rapid recovery and reducing wasted computational resources. This dual-layer approach ensures that GPU resources remain optimally utilized even in the face of frequent hardware or network failures.\nMemory-Based Asynchronous Checkpointing Traditional checkpointing systems, like those relying on distributed file systems (e.g., GPFS [Schmuck and Haskin, 2002]), often introduce significant overhead, leading to idle GPU time during save operations. XCloud's memory-based asynchronous checkpointing drastically reduces the time required to save model states, from several minutes to just 3-5 seconds. This innovation not only minimizes the GPU idle period but also encourages more frequent checkpointing, reducing computational waste during recovery. The result is a substantial boost in system resilience and overall efficiency, contributing directly to achieving and maintaining goodput levels above 99%."}, {"title": "6 Safety", "content": "As large language models continue to grow in capability, it is crucial to ensure their safe and responsible operation across varying and complex scenarios [Achiam et al., 2023, Dubey et al., 2024, Zou et al., 2023, Wei et al., 2023, Zheng et al., 2024b, Wang et al., 2024a, Zhao et al., 2024]. To address the safety concerns of Yi-Lightning, we develop RAISE (Responsible AI Safety Engine), a comprehensive safety framework illustrated in Figure 2. RAISE is designed to provide robust safety capabilities throughout the model's entire lifecycle, from development to deployment, effectively minimizing potential risks and threats through systematic safety mechanisms.\nThe RAISE framework comprises four integral components (RAISE-1 to RAISE-4), corresponding to pre-training, post-training, and inference-time input/output processing. Through sophisticated technical approaches and their synergistic integration, these components collectively ensure model safety while maintaining optimal user experience.\nRAISE-1: Pre-training Safety At the pre-training phase, we implement a safety model for pre- training data filtration. We develop classification models based on Transformer and DNN architectures, trained on high-quality compliant datasets. These models form an evaluation and filtering pipeline for the pre-training corpus, ensuring data reliability, minimizing erroneous information and biased content, preventing privacy data leakage, and enhancing model safety and compliance.\nRAISE-2: Post-training Optimization During post-training, we implement fine-tuning strategies to optimize safety performance across different application scenarios. Our approach incorporates evaluation and scoring mechanisms during SFT and RLHF stages, using reward engineering to"}, {"title": "RAISE-3: Input Safety Processing", "content": "For inference-time input processing, we deploy safety assessment mechanisms that analyze and filter content. The system identifies potentially harmful content, including malicious, discriminatory, or hateful elements, while ensuring input safety and compliance. These mechanisms minimize risks of the model being manipulated while maintaining performance under various input conditions."}, {"title": "RAISE-4: Output Safety Control", "content": "The output safety control system implements real-time detection and optimization across key dimensions: value alignment, bias detection, legal compliance, accuracy assessment, and content appropriateness. This component integrates safety mechanisms to ensure output quality while maintaining efficiency, balancing safety requirements with response speed.\nThrough this framework, RAISE provides a foundation for responsible AI development and deployment, ensuring safety across Yi-Lightning's lifecycle while maintaining performance and user satisfaction. The interaction between these components creates a safety ecosystem addressing both current and emerging challenges."}, {"title": "7 Evaluation", "content": "Chatbot Arena As shown in Figure 1, with the initial appearance on Chatbot Arena\u00b2 [Zheng et al., 2023b] on October 16, 2024, our flagship model Yi-Lightning achieved a remarkable overall ranking of 6th place (Arena score 1287), performing on par with GPT-40-0513 (ranked 7th, Arena score 1285). In specialized rankings, Yi-Lightning also exhibited remarkable performance: 2nd in Chinese, 3rd in Multi-Turn and Math, and 4th in Coding, Hard Prompts, and Longer Query categories. Since the Chatbot Arena rankings derive from authentic human comparisons and voting, these results powerfully demonstrate Yi-Lightning's exceptional ability to fulfill user needs and its satisfactory alignment with human preferences in real-world applications.\nAcademic Benchmarks We report the evaluation results on several representative, public academic benchmarks: GPQA [Rein et al., 2023] for general knowledge, MATH [Hendrycks et al., 2021] for mathematical reasoning, HumanEval [Chen et al., 2021] for coding, and IFEval [Zhou et al., 2023] for instruction following. We also conduct the LLM-as-a-judge evaluations\u00b3 on WildBench [Lin et al.,"}, {"title": "Final Discussion", "content": "Finally, we discuss our observed disparity between open-weight and proprietary models' performance on public academic benchmarks and real-world user preferences (as reflected in the Chatbot Arena rankings). This probably results from the fact that our development process paid more attention to our in-house human assessment experience, rather than overly focusing on academic benchmark scores. For instance, when conducting math-specific model training (e.g., in \u00a7 4.1), we did not strictly restrict the model's output format (e.g., ending with \u201cThe final answer is \\boxed{...}\"), as we believe that constraining the model's output content or format might harm its generation diversity, thereby implicitly impacting optimization effectiveness and user experience. These evaluation results prompt us to rethink and reassess the role of public academic benchmarks in guiding the development of more intelligent and powerful Al systems.\"\n    }"}]}