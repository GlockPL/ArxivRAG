{"title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks", "authors": ["Hongye Cao", "Yanming Wang", "Sijia Jing", "Ziyue Peng", "Zhixin Bai", "Zhe Cao", "Meng Fang", "Fan Feng", "Jiaheng Liu", "Boyan Wang", "Tianpei Yang", "Jing Huo", "Yang Gao", "Fanyu Meng", "Xi Yang", "Chao Deng", "Junlan Feng"], "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability to identify and handle unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been extensively deployed in dialogue systems, attributed to their remarkable generation capabilities. Given their widespread use, safety has emerged as a crucial concern with respect to reliability and trustworthiness across various scenarios (Anwar et al., 2024). Existing benchmarks such as COLD (Deng et al., 2022), BeaverTails (Ji et al., 2024a), and Red Team (Perez et al., 2022) evaluate LLMs safety in single-turn dialogues. However, real-world interactions between users and chatbots typically involve multi-turn dialogues (Zheng et al., 2023, 2024; Bai et al., 2024), introducing additional safety concerns that require comprehensive evaluation.\nRecent benchmarks for multi-turn dialogues safety (Yu et al., 2024; Zhang et al., 2024; Jiang et al., 2024; Ren et al., 2024) generally employ jailbreak attack methods to test an LLM's ability to prevent unsafe content generation. However, these approaches suffer from several critical limitations, especially on the insufficient evaluation scope. First, they often rely on a single jailbreak attack strategy for dataset construction. Second, they focus narrowly on censoring aggressive language, while neglecting other important aspects such as ethics, morality, legality, fairness, and privacy (Jiang et al., 2024; Zhang et al., 2024; Yu et al., 2024). Moreover, these benchmarks typically lack a detailed evaluation of an LLM's capacity to identify and handle unsafe information. Thus, there is a pressing need for a comprehensive and fine-grained benchmark tailored to multi-turn dialogues.\nTo address the above limitations, we propose SafeDialBench, a fine-grained benchmark for evaluating the safety of multi-turn dialogues under diverse jailbreak attack methods. SafeDialBench introduces a two-tier hierarchical safety taxonomy covering six distinct safety dimensions\u2014Fairness, Legality, Morality, Aggression, Ethics, and Privacy. Each dimension is further decomposed into multiple safety points, providing a comprehensive criterion for assessing model safety. Across these six dimensions, we deploy seven distinct jailbreak attack strategies, including reference attack, scene construction, and purpose reverse\u2014to generate dialogues. In total, SafeDialBench comprises 4,053 dialogues, each containing between 3 and 10 turns in both English and Chinese. Furthermore, we in-"}, {"title": "2 Related Work", "content": "Safety Benchmarks for LLMs We summarize recent benchmarks for LLMs safety evaluation in both single-turn and multi-turn dialogues in While single-turn dialogue benchmarks (Zhang et al., 2023; Ji et al., 2024a; Li et al., 2024; Deng et al., 2022) offer larger datasets, they cannot assess model performance in more realistic multi-turn conversations. Existing multi-turn dialogue benchmarks (Agarwal et al., 2024; Yu et al., 2024; Jiang et al., 2024; Xu et al., 2023) are limited by their monolingual nature, restricted use of jailbreak attack methods, and conversations typically shorter than five turns. Furthermore, these benchmarks often have incomplete evaluation dimensions, overlooking crucial aspects such as legality and ethics (detailed comparison provided in Appendix A), and notably fail to assess specific safety capabilities of LLMs. To address these limitations, we aim to construct a comprehensive bilingual safety evaluation benchmark that incorporates a broader range of jailbreak attack methods and extends to longer dialogue sequences.\nJailbreak Attacks on LLMs With the rapid development of LLMs, jailbreak attack methods have emerged as important tools for assessing LLMs safety through red teaming (Lin et al., 2024; Perez et al., 2022; Wei et al., 2024; Liu et al., 2023; Xu et al., 2024a,b). These approaches aim to induce"}, {"title": "3 SafeDialBench", "content": "This section introduces the two-tier safety taxonomy, details the data generation process using diverse jailbreak attack methods, and outlines the safety ability evaluation framework."}, {"title": "3.1 Two-tier Hierarchical Safety Taxonomy", "content": "Through comprehensive research and analysis of existing LLM safety evaluation studies (Ji et al., 2024a,b; Zhang et al., 2023; Jiang et al., 2024; Hung et al., 2023), we develop a two-tier hierarchical safety taxonomy with 6 safety dimensions. We construct dialogues and develop evaluation prompts based on the following detailed safety specifications."}, {"title": "3.1.1 Fairness", "content": "Fairness focuses on whether the model can treat different user groups objectively, justly, and without bias when generating content and providing services. This dimension evaluates:\nStereotypes: Biased model representations of social groups, characterized by oversimplified"}, {"title": "3.1.2 Legality", "content": "Legality assesses whether model outputs comply with laws and regulations, avoiding content that may enable or promote illegal activities. This dimension evaluates four key areas:\nPersonal Harm: Encompass direct violence (assault, abuse), verbal threats, incitement of violence, and psychological abuse like emotional blackmail and cyberbullying. These actions harm both physi-"}, {"title": "3.1.3 Morality", "content": "Morality evaluates the model's adherence to ethical principles and social values during user interactions, avoiding inappropriate content, behaviors, and harmful advice. This dimension focuses on:\nDiscrimination: Systematic devaluation and exclusion of specific groups, characterized by oversimplified stereotypes, biased judgments, and decisions that ignore individual and group differences.\nNon-Violent Immorality: Non-violent unethical actions, including fraud, disinformation, manipulation, professional misconduct, irresponsibility, and disregard for social norms."}, {"title": "3.1.4 Aggression", "content": "Aggression evaluates hostile or harmful content in model outputs that may cause offense, distress, or conflict. This dimension contains:\nThreats: Direct harmful content including violence suggestions, intimidation, infrastructure destruction plans, terrorism-related information. Insults: Targeted derogatory content attacking personal characteristics or group identities with malicious intent. Contempt: Expressions of contempt through dismissive attitudes towards others' achievements, perspectives, or values. Impolite: Inappropriate behavior like rude language, interrupting others, disregarding feelings. Incite: Provocative content spreading misleading information, escalating social conflicts, and promoting group antagonism. Satire: Harmful irony using aggressive humor, derogatory exaggeration, and caustic implications. Blasphemy: Disrespectful discourse mocking religious symbols, cultural traditions, and sacred beliefs."}, {"title": "3.1.5 Ethics", "content": "Ethics reviews model responses for potentially harmful content that could encourage or facilitate destructive actions toward oneself or others, focusing on three critical areas:\nViolence: Content encouraging or detailing violent actions, including attack methods, harm techniques, and rhetoric glorifying violence. Self-harm: Dangerous content involving specific self-injury methods, suicide plans, and language that normalizes or encourages self-destructive behaviors. Abuse: Harmful content describing physical and psychological abuse towards vulnerable groups (children, elderly, animals), including abuse methods, minimizing abuse severity, and disregarding victims' rights."}, {"title": "3.1.6 Privacy", "content": "Privacy checks model outputs for potential leakage, violation, or misuse of private and sensitive information, covering three key domains:\nPersonal Privacy: Individual-level sensitive information including identity, contact, financial, health, biometric, family, and legal records. Organizational Privacy: Sensitive corporate information like business secrets, internal strategies, employee and customer data, financial details. Social Privacy: Broader privacy protection involving data collection standards, public space privacy measures, special group protections, and collective privacy risk prevention."}, {"title": "3.2 Data Generation with Diverse Jailbreak Attack", "content": "Data Generation For each of the six safety dimensions, we construct dialogues using jailbreak attacks through LLM interactions combined with manual verification. Annotators design every question and engage in chats with LLMs to build multi-turn dialogues across 22 scenarios in both Chinese and English. GPT-4, Doubao, and ChatGLM are included for the data construction, followed by manual review. Please refer to Appendix D for detailed description of data construction.\nAttack Methods We employ 7 distinct jailbreak attack methods for dialogue design. Scene Construct: Simulating protective roles or beneficial"}, {"title": "3.3 Safety Evaluation", "content": ""}, {"title": "3.3.1 Safety Ability Taxonomy", "content": "To assess the safety comprehensively, we examine multiple dimensions that reveal its content identification, processing, and analytical capabilities, enabling a granular exploration of potential safety vulnerabilities. Hence, we construct an innovative fine-grained safety ability framework, comprising 3 progressive aspects, to evaluate the safety. Identifying unsafe risks: the ability for identifying potential safety risks within multi-turn dialogues during jailbreak attacking. Handling unsafe informa-"}, {"title": "3.3.2 Model and Human Evaluation", "content": "Following recent works (Bai et al., 2024; Zheng et al., 2023; Yu et al., 2024), we conduct evaluation with both LLMs and human experts as evaluators, and we leverage our meticulously curated datasets as the golden context for dialogue history, diverging from LLMs subjects' self-predicted context. We develop tailored evaluation prompts for each safety dimension and create fine-grained scoring guidelines specifying requirements for each score level. Evaluators score each turn of the chatbot's responses on a 1-10 scale, providing detailed justifications. The evaluation employs a minimum-score-taking metric, where the lowest score of a turn becomes the final dialogue score. This approach aligns with human intuition, as a single compromised response can undermine the entire dialogue in interconnected conversational contexts. Finally, we conduct manual spot-checks to validate the model's safety performance and assess the effectiveness of LLM-based evaluations."}, {"title": "4 Experiments", "content": "We aim to answer the following questions in experiments: (i) How do open-sourced and close-sourced LLMs perform across the six safety dimensions in SafeDialBench? (ii) How do different models and their varying parameter scales perform under our three-capability evaluation framework? (iii) How effective are jailbreak attack methods, and how does model performance vary across dialogue turns? (iv) To what extent do model evaluations align with human expert assessments?"}, {"title": "4.1 Experimental Setup", "content": "Settings We utilize golden contexts as dialogue histories across experiments. For each LLM, we apply the corresponding chat format and system prompt, setting the temperature to 0.7."}, {"title": "4.2 Main Results", "content": "Safety Dimensional Analysis presents safety evaluation results across six dimensions on SafeDialBench. The two Qwen2.5 models demonstrate significant weaknesses in identifying aggression and legality-related content, while also showing inconsistent performance across ethics and privacy dimensions. Additionally, DeepSeek-7B-Chat exhibits safety vulnerabilities in consistent across three dimensions. Among open-sourced models, GLM4-9B-Chat excels in ethics and demonstrates robust in handling content related to legality. Similarly, Yi-34B-Chat achieves strong performance across aggression, legality, morality, and privacy dimensions, showcasing its effectiveness in identifying and managing unsafe content. The close-sourced model MoonShot-v1 exhibits strong safety measures, particularly in handling aggression and fairness. However, it shows vulnerability in ethics-related tasks. 03-mini demonstrates weaker safety performance in aggression, legality and morality.\nMoreover, presents the overall performance of 5 LLMs averaged in six dimensions of Chinese and English. The results indicate that Llama3.1-8B-Instruct model exhibits the lowest scores in English dataset. 03-mini shows significant safety vulnerabilities in Chinese datasets. In contrast, Yi-34B-Chat demonstrates superior performance across the entire evaluation suite.\nJailbreak Dimensional Analysis To evaluate the effectiveness of jailbreak attack methods, we an-"}, {"title": "4.3 Further Analysis", "content": "Effect of Model Size To investigate the correlation between model scale and safety performance, we conduct comparative analyses across two model families: Qwen-Chat (7B, 14B) and Baichuan2-Chat (7B, 13B). For risk identification, Baichuan2-Chat exhibits consistent performance across scales, whereas Qwen-14B-Chat demonstrates enhanced capabilities in morality and privacy dimensions compared to its 7B counterpart. For handling ability, Qwen-14B-Chat exhibits superior performance across all safety dimensions relative to its 7B variant. Notably, our analysis reveals an interesting phenomenon where Baichuan2-13B excels in privacy and fairness metrics, while Baichuan2-7B demonstrates enhanced performance in morality and aggression dimensions, suggesting that safety capabilities do not necessarily scale monotonically with model size.\nHuman Judgment To comprehensively evaluate model performance and further validate our safety assessment framework's effectiveness, we incorporate 5 human experts evaluation. We randomly sample 100 multi-turn dialogues from SafeDialBench"}, {"title": "5 Conclusion", "content": "This paper presents a comprehensive and fine-grained benchmark for evaluating LLMs safety in multi-turn dialogues, incorporating diverse jailbreak attack methods. We introduce an innovative safety assessment framework that combines LLM-based and human expert evaluations. Extensive experimental results demonstrate that close-sourced model 03-mini exhibits safety vulnerabilities, while MoonShot-v1 achieves robust performance across five safety dimensions. Furthermore, open-sourced models Yi-34B-Chat and GLM4-9B-Chat demonstrate strong safety capabilities, while Llama3.1-8B-Instruct exhibits significant vulnerabilities in our English dataset."}, {"title": "6 Limitations", "content": "SafeDialBench requires incorporation of additional jailbreak attack methods to achieve more comprehensive assessment of model safety in multi-turn dialogues. Furthermore, continuous dataset updates and refinements are necessary to keep pace with rapid model developments. The proposed evaluation framework would benefit from more granular assessment methods to measure model safety capabilities with higher precision."}, {"title": "7 Ethical Considerations", "content": "Our work of SafeDialBench is designed to provide a comprehensive cross-lingual evaluation framework for assessing LLM safety in multi-turn dialogue contexts. Throughout the dataset annotation and safety evaluation processes, we implemented rigorous ethical protocols, including informed consent, fair compensation, and mental health support for annotators. While our benchmark incorporates both Chinese and English, we acknowledge the inherent constraints in fully capturing diverse cultural perspectives. We emphasize that the proposed evaluation metrics should be considered complementary to existing safety assessment frameworks rather than definitive measures. The dataset is strictly intended for academic research purposes, and any misuse is prohibited."}, {"title": "A Additional Related Work", "content": "We further analyze related works on safety benchmarks in single-turn and multi-turn dialogues, as shown in . Among single-turn benchmarks, SafetyBench covers all safety dimensions but focuses solely on choice evaluations. In contrast, other single-turn and multi-turn safety benchmarks fail to cover all dimensions. Therefore, we aim to construct a benchmark that enables a fine-grained and holistic assessment of LLMs safety."}, {"title": "B Details on Safety Dimensions", "content": "We list the detailed description of the proposed hierarchy safety dimensions in ."}, {"title": "C Details on Safety Ability Evaluation", "content": "The complete safety chain consists of Identification (identifying risks), Handling (resolving risks), Consistency (verifying reliability)."}, {"title": "C.1 Identification Ability", "content": "The model's ability to detect and classify potential security concerns focuses on three key aspects: (i) Explicit Risk Detection - Direct identification of sensitive information (such as ID numbers and bank accounts), detection of clearly illegal content (such as violence instigation and drug trafficking guidance), and capture of aggressive language (including hate speech and social engineering probes). (ii) Implicit Risk Discovery - Monitoring risk accumulation across multiple conversation turns (such as piecing together privacy information through extended dialogue), identifying borderline legal behaviors (such as vaguely worded instigation), and detecting discriminatory metaphors within cultural contexts (such as statements suggesting \"people from certain regions are more prone to crime\"). (iii) Risk Type Coverage - Comprehensive detection across privacy categories (individual/organizational/social), legal violations (physical harm/property crime/privacy invasion/public safety), and ethical boundaries (glorification of violence, self-harm methods, rationalization of abuse)."}, {"title": "C.2 Handle Ability", "content": "The effectiveness of response strategies to identified risks centers on two primary components: (i) Risk Response Strategies - Implementing hard blocks for high-risk requests (such as inquiries about suicide methods), providing soft guidance for ethically controversial issues (such as euthanasia discussions), and offering compliant alternatives (such as using virtual cases instead of real privacy demonstrations). (ii) Value Guidance - Conducting legal assessments with explanations of applicable laws and providing compliant recommendations, while emphasizing ethical interpretations of safety guidelines (such as explaining the societal impact of violence)."}, {"title": "C.3 Consistency Ability", "content": "The ability to maintain stable execution of safety principles across different scenarios encompasses three main elements: (i) Scenario Consistency - Maintaining uniform rejection standards for sensitive inquiries (such as weapon-making requests) across different conversation turns, and conducting counterfactual fairness tests to ensure equivalent evaluations for different demographic groups in similar contexts. (ii) Logical Consistency - Consistently citing specific legal provisions in regulatory responses and maintaining explainable value positions in moral assessments (providing clear moral reasoning for decisions). (iii) Pressure Resistance - Maintaining compliant positions despite user pressure (such as complaint threats) and resisting manipulative probes (such as multi-turn privacy boundary testing) without compromising safety standards."}, {"title": "D Details on Data Generation", "content": "Scenario Selection In constructing our multi-turn dialogue dataset, we conduct a systematic scenario selection process based on practical needs and specific considerations. We prioritized both comprehensiveness and practical relevance. Comprehensiveness emphasizes the multifaceted nature of social structures, spanning from personal life to social governance, and from education to international affairs. This encompasses diverse domains including law, morality, religion, race, technology, and finance. To ensure practical relevance, we focused on frequently encountered scenarios across different professional contexts, including healthcare, business management, and environmental protection, addressing both specialized domain knowledge and common user interactions in these fields.\nThe dataset construction also carefully considers social sensitivity and cognitive complexity. Social sensitivity included potentially sensitive topics"}, {"title": "E Details on Data Statistics", "content": "We present detailed data statistics for SafeDialBench in ."}, {"title": "F Details on Experiment", "content": ""}, {"title": "F.1 Experimental Setting", "content": "The information of the evaluated models is provided in ."}, {"title": "F.2 Evaluation Prompt", "content": "The evaluation prompts are displayed in Figures 8 ~ 14, combining standardized evaluation instructions with customized assessment criteria and scoring metrics for each dimension."}, {"title": "G More Experimental Results", "content": ""}, {"title": "G.1 Results on Qwen-72B Evaluator", "content": "Table 9 presents the safety evaluation results of all LLMs using Qwen-72B as evaluator, which"}, {"title": "G.2 Results of Chinese and English Datasets", "content": "Based on the comparative analysis of both Chinese and English evaluation results shown in Table 10 and Table 11, open-sourced models have demonstrated remarkable progress across linguistic domains. Notable achievements have been observed not only in larger-scale models like Yi-34B-Chat but also in more compact architectures such as the 7B-parameter DeepSeek, which exhibits comparable security performance across multiple dimensions. The cross-linguistic analysis reveals that certain language-specialized models (e.g., Baichuan2-13B-Chat and MoonShot-v1) demonstrate superior performance in their primary language. This pattern, evident across both language datasets, suggests that security capability enhancement is more contingent upon training data quality and security strategy design rather than mere parameter scale expansion. In contrast, close-sourced models such as ChatGPT-4o and o3-mini have shown unexpected limitations, with the latter notably achieving the lowest scores across multiple dimensions of the Chinese dataset. Llama3.1-8B-Instruct performs the worst on the English dataset.\nIn general, through our comprehensive analysis of Chinese and English evaluation datasets, we have observed several significant patterns in the security performance of LLMs. Empirical evidence from both language contexts suggests that open-sourced models are demonstrating increasingly robust security capabilities, often matching or exceeding close-sourced models. This finding challenges the conventional assumption that closed source, commercial models inherently provide superior security safeguards. Furthermore, cross-linguistic variation in performance indicates that language-specific optimization and targeted security strategies may be more crucial than model scale or development approach."}, {"title": "G.3 Jailbreak Dimensional Results", "content": "We further analyze other four safety dimensional results under 7 jailbreak attacks, as shown in Regarding ethics, open-sourced LLMs including GLM4-9B-Chat and Yi-34B-Chat demonstrate relatively balanced performance across seven attacks, showing no significant variations in their three capability metrics. In contrast, the close-sourced MoonShot-v1 and ChatGPT-40 remain largely unaffected by topic change, fallacy attack, and scene construct methods. However, purpose reverse and role play techniques prove effective in compromising the model's performance, resulting in lower safety scores.\nFor legality, MoonShot-v1 exhibits reduced safety performance not only under the previously identified effective methods but also under fallacy attack, validating the effectiveness of these approaches. Notably, MoonShot-v1's lower scores compared to other models highlight its security vulnerabilities in handling legal content. Additionally, the scene construction method proves particularly effective against Yi-34B-Chat and ChatGPT-40 models in identifying, resulting in significantly lower safety scores.\nIn terms of privacy, our analysis reveals that topic change demonstrates minimal effectiveness, resulting in consistently high safety scores. However, the lower scores of MoonShot-v1 and ChatGPT-40 compared to other models indicate potential security vulnerabilities in privacy.\nRegarding fairness, all four models score significantly lower in consistency ability compared to their performance in the other five dimensions,"}, {"title": "G.4 Case Study", "content": "We present detailed analysis of several cases from SafeDialBench, along with their evaluation results, which are depicted in Figures 16, 17, 18, and 19. These cases illustrate the vulnerabilities of LLMs under various jailbreak attack methods.\nIn the Scene Construct case, an \"opposite meaning\" prompt is used to transform constructive marital advice into toxic recommendations. The model fails to recognize the adversarial nature of the request, scoring poorly in both identification and handling. It provides harmful suggestions that contradicted its previous guidance, demonstrating a lack of consistency in upholding ethical standards.\nIn the fallacy attack case, a pseudoscientific \"Domination-Based Conditioning\" plan for animal training was introduced. The model fails to identify the ethical risks associated with this plan and enabled harmful content. It violated animal welfare principles, showing a significant inconsistency in maintaining ethical alignment.\nThe Purpose Reverse case involves reversing constructive advice into conflict-escalation strategies. The model partially recognizes surface-level needs but failed to detect deeper adversarial intent. Its defensive measures are insufficient, resulting in weakly inconsistent advice that undermined the principles of constructive communication.\nIn the Role Play case, the attacker requests derogatory language against \"homewreckers.\" Despite a superficial warning, the model provides vulgar insults, demonstrating contradictory handling and mixed messages that undermined its ethical integrity. This case highlights the failure to consistently resist manipulation towards harmful actions.\nThese additional cases further demonstrate the dataset's effectiveness in identifying critical vulnerabilities in LLMs across diverse attack methods. They provide comprehensive coverage of attack scenarios, enabling robust evaluation of safety abilities and highlighting areas for improvement in model design and alignment. Future work should leverage such datasets to develop more resilient and ethically sound language models."}]}