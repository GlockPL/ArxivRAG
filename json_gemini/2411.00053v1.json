{"title": "ACC-DEBATE: AN ACTOR-CRITIC APPROACH TO MULTI-AGENT DEBATE", "authors": ["Andrew Estornell", "Jean-Fran\u00e7ois Ton", "Yuanshun Yao", "Yang Liu"], "abstract": "Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models, frequently referred to as multi-agent debate (MAD). While debate shows promise as a means of improving model efficacy, most works in this area treat debate as an emergent behavior, rather than a learned behavior. In doing so, current debate frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Debate, an Actor-Critic based learning framework to produce a two-agent team specialized in debate. We demonstrate that ACC-Debate outperforms SotA debate techniques on a wide array of benchmarks.", "sections": [{"title": "INTRODUCTION", "content": "Recently, large language models (LLMs) have rapidly become a cornerstone in various applications, redefining how we process and generate language at scale (Thirunavukarasu et al., 2023; Hadi et al., 2023; Jiang et al., 2024). Their ability to handle diverse tasks, from translation (Zhu et al., 2024; Otter et al., 2020) to answering complex questions (Zhang et al., 2024; Hao et al., 2024; Havrilla et al., 2024), has attracted the attention of both industry as well as academia. However, despite these advancements, LLMs still exhibit notable weaknesses, particularly when it comes to answering factual questions and reasoning (Tonmoy et al., 2024; Rawte et al., 2023; Huang et al., 2023).\nTo address these limitations, several techniques have been proposed, such as Chain-of-Thought (CoT) prompting (Wei et al., 2022), self-reflection (Ji et al., 2023; Shinn et al., 2023), and multi-agent debate (MAD) (Du et al., 2023), to name a few. These approaches aim to improve the reasoning abilities of LLMs by guiding them toward more accurate answers through structured thinking or discourse. However, the majority of these techniques do not involve training the model specifically for these tasks but instead rely on zero-shot or few-shot capabilities.\nIn particular, multi-agent debate approaches make use of off-the-shelf general-purpose LLMs, which are not trained to collaborate. Such approaches rely on collaboration as an emergent, rather than a learned, behavior. While, in some cases, these emergent behaviors are sufficient, the question remains: Can these methods be improved by imbuing models directly with collaborative abilities? To answer this, we propose a novel paradigm to train teams of LLMs to collaboratively solve tasks.\nMost relevant to the main idea of this paper is DebateGPT (Subramaniam et al., 2024). DebateGPT uses debate as a mechanism to attain higher-quality fine-tuning data. Differing from our work, their approach focuses on using debate to generate better training data for a single model that gives single responses, rather than optimizing the LLMs themselves for collaborative problem-solving over multiple rounds of conversation."}, {"title": "RELATED WORK", "content": "Our research is closely related to the emerging field of multi-agent debate (sometimes called multi-LLM deliberation) which examines how to use groups of models to solve tasks through iterative discussion Chan et al. (2023); Liang et al. (2023); Du et al. (2023); Khan et al. (2024); Michael et al. (2023); Rasal (2024); Pham et al. (2023); Abdelnabi et al. (2023); Hong et al. (2023); Irving et al. (2018); Li et al. (2023b;c; 2024a); Wang et al. (2023a); Zhang et al. (2023). Many of these works find that language models have naturally collaborative abilities Singhal et al. (2023); Du et al. (2023); Chan et al. (2023), while others have noted that the collaborative ability of off-the-shelf models can be quite limited Wang et al. (2024); Smit et al.. \nCurrent approaches to multi-agent debate can be broadly cast into two main categories: those that modify model prompts and responses during the debate Liang et al. (2023); Khan et al. (2024); Rasal (2024); Feng et al. (2024); Yang et al. (2024), and those that modify the structure of the debate process Li et al. (2023a); Hong et al. (2023); Liu et al. (2023); Li et al. (2024c); Wang et al. (2023b); Wu et al. (2023); Chen et al. (2023); Chang (2024b). Importantly, both categories use off-the-shelf language models (which have not been trained to collaborate) and work by modifying either the inputs or outputs of these models. Deviating from this line of work, we aim to specifically train a team of models to collaboratively solve tasks.\nTwo works of particular note are that of Subramaniam et al. (2024), which proposes to use debate data to fine-tune models, and Li et al. (2024b), which trains models for adversarial debate. In the former, debate is used to generate higher-quality fine-tuning data and is not used at inference time; differing from this work, we train models directly to collaborate and use debate both during training and inference. In the latter, models are trained to be effective arguers rather than collaborators, i.e., models are trained to give conceiving arguments such that they can win a debate against other LLMs. Differing from this work, we train models to collaboratively solve tasks.\nIn the context of multi-agent deliberation, the concept of divergent opinions is highly relevant to our method. Several approaches to multi-agent debate aim to control the level of disagreement among the agents Liang et al. (2023); Khan et al. (2024); Chang (2024a). Often, these works dynamically increase disagreement to prevent early convergence of debate. In our study, we leverage divergent opinions to generate high-quality training data. In particular, we have debaters change their opinion during the debate and measure whether or not that change increases or decreases the likelihood that the debate converges to a correct answer; this is used to asses the value of a given training example.\nAlso closely related to our work are paradigms that aim to use self-generated data to improve model performance, often in the context of reasoning or chain of thought Trung et al. (2024); Huang et al. (2024); Xiong et al. (2024); Chen et al. (2024); Pang et al. (2024b); Zelikman et al. (2022). Similar to this line of research, we make use of model generations as training data. However, we are the first work to use such data in the context of multiple models debating collaboratively to solve a task."}, {"title": "PRELIMINARIES AND NOTATION", "content": "In this section, we formalize the debate between an Actor (a model that provides answers) and a Critic (a model that provides feedback to the actor) while also introducing the notation that will be used throughout the remainder of the paper.\nLet $(x, y) \\sim D$ be a task-answer pair source from a distribution of tasks and answers $D$. For a given task $x$, two models \u2013 an actor model responsible for providing answers and a critic model responsible for providing feedback and assistance to the actor model \u2013 engage in an iterative discussion over $T$ rounds, to correctly infer the answer $y$. Let $\\theta_a$ and $\\theta_c$ be the parameters of actor and critic models, respectively. The iterative discussion between these two models is as follows:\n1. At round $t = 0$ a task $x$ is given to the actor $\\theta_a$ who provides an initial response $z^{(0)}_a$.\n2. Next, still at round $t = 0$, the critic $\\theta_c$ views task $x$ and $z^{(0)}_a$, then provides feedback $z^{(0)}_c$.\n3. For each round $t > 0$, the actor views the task $x$, its own previous response $z^{(t-1)}_a$ and the critic's feedback $z^{(t-1)}_c$, then provides an updated response $z^{(t)}_a$.\n4. After the actor's new response $z^{(t)}_a$, the critic provides the feedback $z^{(t)}_c$ based on $z^{(t)}_a$.\nThe accuracy of this procedure is measured via the correctness of the actor's final response, i.e., $I[\\zeta(z^{(T)}_a) = y]$. Where $\\zeta$ is a function that extracts answers from text-based responses. For example if $z^{(T)}_a$ = \u201cThe sky is blue\u201d, then $\\zeta(z^{(T)}_a)$ =\u201cblue\u201d. With this notation and formalization of debate, we introduce our framework for training actor-critic teams to debate."}, {"title": "METHODOLOGY", "content": "In this section, we outline our procedure for training a two-agent debate team, consisting of an actor model $f_{\\theta_a}$ (responsible for providing answers to a given task $x$) and a critic model $f_{\\theta_c}$ (responsible for providing feedback and assistance to the actor). At inference time, the two trained agents engage in iterative discussion to solve a given task $x$, generating the final response $z^{(T)}_a$.\nBuilding upon our established notation from the previous section and the general actor-critic framework, we formally define our optimization objective as follows 1:\n$\\theta^*_a, \\theta^*_c = \\arg \\max_{\\theta_a} \\max_{\\theta_c} E_{(x,y)\\sim D} [\\mathbb{I} (\\zeta (f_{\\theta_a} (x, z^{(T-1)}_a, f_{\\theta_c} (x, z^{(T-1)}_a))) = y)]$  (1)\nIntuitively, Eq. 1 aims to simultaneously optimize the actor's parameters $\\theta_a$ and the critic's parameters $\\theta_c$, ensuring that the actor's final output at iteration $T$ matches the correct answer $y$."}, {"title": "AN ACTOR-CRITIC DEBATE FRAMEWORK", "content": "In other words, we optimize the accuracy of the actor's response at time $T$, namely $z^{(T)}_a$\nwhere accuracy is measured as $E[\\mathbb{I}(\\zeta(z^{(T)}_a)) = y]$.\nIt is important to note that the recursive nature of multi-agent debate introduces significant complexity to the optimization process. Each response $z^{(t)}_a$ depends not only on the actor's previous output $z^{(t-1)}_a$ but also on the critic's previous output $z^{(t-1)}_c$. This interaction closely resembles a cooperative dynamic Stackelberg game (Li & Sethi, 2017), where two players engage in hierarchical decision-making over time, leading us to adopt an iterative best-response approach (Fiez et al., 2019). In other words, we first train the critic model, followed by training the actor to best respond to the critic's output. We can then update the critic to adapt to the newly trained actor, and so on. More formally, this process works by first fixing $\\theta_a$, and solving,\n$\\theta^*_c = \\arg \\max_{\\theta_c} E_{(x,y)\\sim D} [\\mathbb{I} (\\zeta (f_{\\theta_a} (x, z^{(T-1)}_a, f_{\\theta_c} (x, z^{(T-1)}_a))) = y)]$  (2)\nthen fixing $\\theta^*_c$ from above, we solve\n$\\theta^*_a = \\arg \\max_{\\theta_a} E_{(x,y)\\sim D} [\\mathbb{I} (\\zeta (f_{\\theta_a} (x, z^{(T-1)}_a, f_{\\theta_c} (x, z^{(T-1)}_a))) = y)]$  (3)\nthis process then repeats until a desired stopping criteria is reached. In practice, we find that a single iteration is sufficient to produce a high-quality debate team.\nWhile this alternating scheme allows us to optimize the actor and critic separately, the objectives of each model still cannot be optimized directly due to the recursive nature of model responses in this objective; responses at round $T$ depend on those given by the model at round $t - 1$ which themselves depend on the response given at round $t - 2$ and so on. To deal with this temporal dependency, we next introduce the concept of Partial Trajectory rewards, which will allow us to capture the signal of each response $z^{(t)}$ for each $t < T$."}, {"title": "PARTIAL TRAJECTORY REWARD", "content": "To address the inter-round dependencies of the above optimization, we proposed a scheme that allows us to determine the \u201cgoodness\u201d of a given response $z^{(t)}$ (from either the actor or the critic) for any $t \\leq T$. Consider a conversation between the actor and the critic that was paused at time $t$, i.e., the most recent response is $z^{(t)}$. To assess the goodness of $z^{(t)}$, one might ask how likely the debate procedure will converge to the correct answer $y$ at round $T$, given that the procedure is already at response $z^{(t)}$. Formally, we can define this as\n$r(z^{(t)}, x, y) = E[\\mathbb{I} (\\zeta(z^{(T)}_a) = y| x, z^{(t)}_a, z^{(t)}_c)]$\nIntuitively, the partial reward captures the expectation of arriving at the correct answer $y$ through debate starting at round $t$ with generation $z^{(t)}$. In practice, $r(z^{(t)}, x, y)$ can be estimated by learning the reward structure $r$ or by using heuristics such as one-step roll-out, i.e., Monte Carlo estimation. Empirically, we observe that one-step roll-out heuristics are effective in producing high-quality training data and have the added benefit of being significantly more efficient than learning-based approaches.\nOur objective will then be to optimize the actor and critic, $\\theta_a, \\theta_c$, so that the responses produced by these models at each timestep $t$, namely $z^{(t)}$, maximize $r(z^{(t)}, x, y)$. That is, we optimize the actor and the critic so that at each timestep $t$, they give a response $z^{(t)}$ which has a high probability of leading the debate to converge to the correct answer at time $T$."}, {"title": "OFF-POLICY TRAJECTORY GENERATION", "content": "In this section, we describe how to generate the preference data needed to optimize the objective in Eq. 1. The classification of a sample as positive or negative is determined by the debate trajectory"}, {"title": "ACC-DEBATE PERFORMANCE", "content": "We begin by examining the performance of our method ACC-Debate (a single round of training) and ACC-Debate+ (two rounds of training) In table 1, we see the average accuracy of each method after five rounds of debate. Our method attains superior performance compared with baseline methods in most cases. The high efficacy of ACC-Debate relative to the baselines indicates that in most cases, only a single round of training is necessary to produce a high-quality debate team. It is worth noting that in some cases, further training rounds may decrease performance (i.e., ACC-Debate+ can have worse performance than ACC-Debate). As such, we recommend maintaining a hold-out set of tasks to determine whether further training degrades performance."}, {"title": "PERFORMANCE INCREASE OF DEBATE", "content": "Average Improvement As noted in Du et al. (2023), the key mechanism behind the success of multi-agent debate (or any of its many variants) is that discussion over multiple rounds allows the models to iteratively refine their answers. Thus, a natural question is: how much does accuracy improve from the initial round $t = 0$ to the final round $t = T$? To measure this, we look at the percent improvement in model accuracy from round $t = 0$ to round $t = 4$ calculated as,\n$\\frac{(acc5-acc0)}{acc0}$ where $acct$ is accuracy at round $t$  (5)"}, {"title": "INDIVIDUAL PERFORMANCE", "content": "Next, we examine the relative effectiveness of both the actor and the critic. To do this, we train an actor and critic via ACC-Debate. Then, during the debate, we pair the trained actor with an untrained critic and pair the trained critic with an untrained actor. In Table 2, column \"Actor\" corresponds to the former, while \u201cCritic\u201d corresponds to the latter. On average, the trained actor attains higher accuracy compared to the trained critic, this aligns with intuition as the actor is responsible for providing answers while the critic plays a supporting role. In most cases, the trained actor (paired with an untrained critic) outperforms SFT and DebateGPT. When the trained actor is paired with a trained critic (either ACC-Debate or ACC-Debate+), its performance is further improved."}, {"title": "WHAT DO THE AGENTS ACTUALLY LEARN?", "content": "Lastly, we are interested in understanding how ACC-Debate improves the Actor-Critic Team. Figure 4 demonstrates an example of the difference in responses between an untrained critic and a critic trained through ACC-Debate. Although the actor provides a wrong answer, the untrained critic is too agreeable and does not provide substantive feedback for the actor to correct their answer.\nIn contrast, the trained critic is more willing to disagree with the actor and provides more detailed feedback. Largely, we observe that this trend is common; untrained critics are too agreeable and are thus less able to change the actor's mind, while trained critics are more willing to disagree. When examining the trained actor's responses, we do not find a notable qualitative change compared to the responses of an untrained actor. However, as shown previously, we do observe a qualitative change in the actor's responses to become more accurate."}, {"title": "CONCLUSION, LIMITATIONS AND IMPACT", "content": "In this paper, we propose ACC-Debate, a novel framework for jointly training a two-agent team (one actor-agent and one critic-agent) to collaboratively solve problems through iterative discussion. To train these agents, we developed an off-policy data generation scheme dubbed \"Guided-debate\", which produces high-quality preference data for collaborative models. We found that ACC-Debate outperforms all baselines on a wide array of domains. In particular, even a single round of training for both the actor and critic results in a high-quality debate team. Of particular note is the effects that ACC-Debate has on the critic model. Without ACC-Debate, the critic model often appears too agreeable in their responses. In contrast, after training with ACC-Debate, the critic is far more likely to provide detailed disagreements during debate. The success of ACC-Debate demonstrates the need for more specialized training when it comes to collaborative models.\nHowever, our framework ACC-Debate also comes with limitations. Firstly, even though ACC-Debate attains superior performance compared to baselines on a wide array of domains, it is important to note that we conduct experiments mainly on question-answering tasks; thus, it remains to be seen whether such a framework would continue to be effective in other types of tasks. Our method makes use of the fact that for each question, correct and incorrect answers can be easily established. Secondly, while we provide results for three families of models, these experiments are performed on 2B, 7B, and 8B models. While our method is effective for these sizes (standard in open-source models), it remains to be seen whether this effectiveness will scale to larger models."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "Here, we outline the details necessary to reproduce our results. We provide an algorithm for our data generation procedure (Algorithm 1), as well as a description of our training procedure in Section 4.3. Each dataset, baseline method, and base model used are specified at the beginning of Section 5. We provide additional experimental details in Section A. Prompts used for our method can be found in Section C."}, {"title": "EXPERIMENTS", "content": "Here, we provide additional details regarding our experiments.\nDatasets Each dataset is split into a training set, a validation set, and a testing set. For datasets that come with an explicit partition of these sets we use the given partitions; this includes BoolQ, MMLU, SCIQ, and ARC. For BBH, we randomly sample roughly 25% and 10% of the questions from each category in BBH to create a test and validation set, respectively; this comes out to 1260 questions for the test set and 500 questions for the validation set. All results are reported on questions in the test set.\nCompute All training was performed on a single Nvidia-H800 GPU. Inference for Llama-3 and Mistral based models is performed on a single Nvidia-v100 GPU, for Gemma-2 based models we used a single Nvidia-H800. All inference is performed with the VLLM library Kwon et al. (2023).\nTraining Training for all models was performed via the trl library, using LoRAs of size 256. When training ACC-Debate with DPO we use a negative log-likelihood (NLL) regularization term (with weight 1) as outlined in Pang et al. (2024a)."}, {"title": "PREFERENCE OPTIMIZATION", "content": "Here, we remark on how other preference optimization schemes can be used in place of DPO. Broadly speaking, preference optimization schemes can broken into two categories: those which optimize reward directly (e.g., PPO) and those which optimize reward indirectly (e.g., DPO). In the case of the latter, the positive and negative pairs produced by our method (Algorithm 1) can be directly plugged into the preference optimizer.\nIn the case of explicit reward maximization, the reward function $r(z^{(t)}, x, y)$ can first be learned automatically by simply simulating debate. The most straightforward way to do this is to first simulate one full debate between the actor and critic, i.e., $((z^{(0)}_a, z^{(0)}_c), ..., (z^{(T)}_a, z^{(T)}_c))$. Then for each $z^{(t)}_a, z^{(t)}_c$ the remaining $T \u2013t$ debate steps can be resampled to estimate the corresponding value of $r(z^{(t)}_a, x, y)$ and $r(z^{(t)}_c, x, y)$. These pairs, namely $(z^{(t)}_a, r(z^{(t)}_a, x, y))$ and $(z^{(t)}_c, r(z^{(t)}_c, x, y))$ can then be used to learn the reward function. This reward function can then be plugged into the desired preference optimization scheme."}, {"title": "PROMPTS", "content": "Here, we provide examples of the prompts used in our experiments. For illustration, we provide prompts for the BoolQ dataset in which agents are asked a yes-no question about a passage."}]}