{"title": "UNSUPERVISED PANOPTIC INTERPRETATION OF\nLATENT SPACES IN GANS USING SPACE-FILLING\nVECTOR QUANTIZATION", "authors": ["Mohammad Hassan Vali", "Tom B\u00e4ckstr\u00f6m"], "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can\nbe mapped to real-world images. Such latent spaces are difficult to interpret. Some\nearlier supervised methods aim to create an interpretable latent space or discover\ninterpretable directions that require exploiting data labels or annotated synthesized\nsamples for training. However, we propose using a modification of vector quan-\ntization called space-filling vector quantization (SFVQ), which quantizes the data\non a piece-wise linear curve. SFVQ can capture the underlying morphological\nstructure of the latent space and thus make it interpretable. We apply this tech-\nnique to model the latent space of pretrained StyleGAN2 and BigGAN networks\non various datasets. Our experiments show that the SFVQ curve yields a general\ninterpretable model of the latent space that determines which part of the latent\nspace corresponds to what specific generative factors. Furthermore, we demon-\nstrate that each line of SFVQ's curve can potentially refer to an interpretable di-\nrection for applying intelligible image transformations. We also showed that the\npoints located on an SFVQ line can be used for controllable data augmentation.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative adversarial networks (GANs) (Goodfellow et al., 2014) are powerful deep generative\nmodels applied to various applications such as data augmentation (Antoniou et al., 2017), image\nediting (H\u00e4rk\u00f6nen et al., 2020), video generation (Wang et al., 2018a). For image data, GANs map a\nlatent space to an output image space by learning a non-linear mapping (Voynov & Babenko, 2020).\nAfter learning such mapping function, GANs can create realistic high-resolution images by sampling\nfrom the latent space (Karras et al., 2019). However, this latent space is a black box such that it is\ndifficult to interpret the mapping between the latent space and generative factors such as gender,\nage, pose (Shen et al., 2020). In addition, the interpretable directions to change these factors are\nnot known (Voynov & Babenko, 2020). Hence, having a more comprehensive interpretation of the\nlatent space is an important research problem that, if solved, leads to more controllable generations.\nIn the literature, supervised and unsupervised methods exist to find interpretable directions in the\nl atent space. Supervised methods (Jahanian et al., 2019; Plumerault et al., 2020; Yang et al., 2021;\nGoetschalckx et al., 2019; Shen et al., 2020) require large data collection together with the use of\npretrained classifiers or human labelers to label the collected data with respect to the user-predefined\ndirections (Shen & Zhou, 2021). In addition, these methods only find the directions that the user\ndefines (Voynov & Babenko, 2020). On the other hand, in unsupervised methods (H\u00e4rk\u00f6nen et al.,\n2020; Shen & Zhou, 2021; Voynov & Babenko, 2020; Y\u00fcksel et al., 2021; Tzelepis et al., 2021;\nAoshima & Matsubara, 2023) the user has to choose the hyper-parameter K (the number of inter-\npretable directions to discover) before training, where a large value for K results in discovering\nrepetitive directions (Y\u00fcksel et al., 2021). Furthermore, in all these unsupervised methods, there is\nno prior knowledge about the specific transformation each of these K discovered directions yields.\nHence, the user has to do an exhaustive search over all available K directions to determine which\ndirections are practical and what they refer to. For instance, as GANSpace (H\u00e4rk\u00f6nen et al., 2020)\napplies principal component analysis (PCA) on the latent space, the number of directions (K) to be\nexamined is large (equal to the latent space dimension). Also, as stated in H\u00e4rk\u00f6nen et al. (2020),\nnot all PCA directions are necessarily useful to change a generative factor."}, {"title": "2 RELATED WORK", "content": "Prior works can be categorized into three principal approaches that aim to make the latent space of\ngenerative models more interpretable.\n1. Introducing structure into the latent space using data labels. The main rationale behind these\napproaches (Klys et al., 2018; Xue et al., 2019; An et al., 2021) is that they take advantage of labeled\ndata (with respect to the features of interest) and train the generative model in a supervised manner\nto learn a structured latent space in which data with specific labels reside in isolated subspaces of\nthe latent distribution. Hence, this structured latent space can be interpretable such that the user\ncould have control over data generation and manipulation with respect to the labels. However, these"}, {"title": "3 METHODS", "content": "3.1 SPACE-FILLING VECTOR QUANTIZATION (SFVQ)\nA space-filling curve is a piece-wise continuous curve created by recursion, and if the recursion\nrepeats infinitely, the curve fills a multi-dimensional distribution (Sagan, 2012). Motivated by space-\nfilling curves, space-filling vector quantization (SFVQ) (Vali & B\u00e4ckstr\u00f6m, 2023) designs vector\nquantization (VQ) as mapping of input data on a space-filling curve, whose corner points are the\ncodebook vectors of VQ. Similar to space-filling curves, SFVQ is trained recursively. SFVQ first\nstarts with N = 4 codebook vectors (2bit) and then it expands the codebook by doubling the\nnumber of codebook vectors at each recursion step. The recursion continues until SFVQ reaches\n$N = \\log_2 (B_{target})$ codebook vectors, where $B_{target}$ refers to SFVQ's target bitrate.\n3.2 PROPOSED METHOD\n3.2.1 SFVQ INITIALIZATION\nAs mentioned, SFVQ training starts with N = 4 codebook vectors. The initialization of these four\ncodebook vectors significantly impacts the final learned SFVQ curve obtained at the end of training.\nIn Vali & B\u00e4ckstr\u00f6m (2023), these four codebook vectors were initialized randomly (from a normal\ndistribution N(0,1)), and as the SFVQ codebook was expanded to reach the target bitrate, there\nwere some outlier codebook vectors which ended up out of the latent space.\nIn this paper, we changed the codebook initialization. As we have access to the pretrained models,\nwe sample $10^3$ random vectors z from the normal distribution and generate their corresponding latent\nvectors in the layer where we intend to train the SFVQ (e.g. intermediate W space in StyleGAN2).\nThen, we compute the Euclidean norm of all latent vectors and sort them in the ascending order\nof their norms. We split these sorted latent vectors into four groups and initialize the codebook\nvectors with the mean of these four groups. From a geometrical viewpoint, this initial SFVQ curve\nspans from one end of latent Euclidean space to its other end and brings a desirable order to SFVQ\ncodebook vectors which aligns with the intrinsic SFVQ codebook arrangement, as the curve starts\nfrom low norm to high norm latent vectors."}, {"title": "3.2.2 SFVQ CODEBOOK EXPANSION", "content": "When training SFVQ, codebook expansion occurs at the\nbeginning of a recursion step such that the codebook size\nis doubled. In Vali & B\u00e4ckstr\u00f6m (2023), the new code-\nbook vectors are defined in the center of the line connect-\ning two adjacent codebook vectors (which already exist\non the curve), i.e. $C_{new} = (C_i + C_{i+1})/2$, where $c_i$ is the\ni-th codebook vector. However, $C_{new}$ can be useless as\nit might be located outside the latent space and it takes a\nlong time to be pushed inside. In this paper, we define the\nnew codebook by shifting the existing codebook vectors\nslightly such that $C_{new} = 0.99 C_i + 0.01 C_{i+1}$. Now, the\nnew codebooks are more likely to reside inside the latent\nspace, and thus, after being selected actively during training, they will be optimized to better loca-\ntions. In contrast to Vali & B\u00e4ckstr\u00f6m (2023), our proposed codebook expansion and initialization\nfor SFVQ lead to no outlier codebooks throughout our experiments."}, {"title": "3.2.3 SFVQ INTERPRETABLE DIRECTIONS", "content": "After training SFVQ on the latent space and obtaining the learned codebook, we generate the images\ncorresponding to SFVQ codebook vectors (fig. 2(a) and fig. 4(a)). This visualization reveals the\nunderlying structure of the latent space regarding generative factors, which we discuss in section 5.1.\nHowever, in this paper, we take one step forward to extract more information from SFVQ's curve,\nwhich results in finding interpretable directions. Because of the intrinsic arrangement in SFVQ codebook vectors, subsequent\nimages refer to similar contents. In other words, they share many similar features while they are\ndifferent in minimal attributes. For example, in fig. 2(a), the images for two subsequent codebook\nvectors of $c_i$ and $c_{i+1}$ share most of the attributes except the rotation. Hence, we can infer that\nthe direction (d) connecting these two vectors refers to the rotation direction. Then, by\nshifting any latent vector along this direction (fig. 2(c)), we observe the change in rotation attribute.\nBy a quick observation of the subsequent generated images from the SFVQ codebook, the user can\nsimply spot the interpretable direction. Hence, the user has a prior knowledge of the direction, and\nthe only required action is to find the proper layers of the GAN to edit along this direction (H\u00e4rk\u00f6nen\net al., 2020). In this way, the user achieves the desired edit with less search effort compared to\nother unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Voynov & Babenko, 2020;\nY\u00fcksel et al., 2021; Tzelepis et al., 2021; Aoshima & Matsubara, 2023), in which apart from the\nlayer-wise search, they should do an exhaustive search over all K discovered directions to inspect\nwhether they are practical and what direction they refer to."}, {"title": "4 EXPERIMENTS", "content": "To evaluate how SFVQ can be used to interpret the latent spaces in GANs, similarly to\nGANSpace (H\u00e4rk\u00f6nen et al., 2020), we chose the intermediate latent space (W) of StyleGAN2 (Kar-\nras et al., 2020) and the first linear layer of BigGAN512-deep (Brock et al., 2018), and then trained\nthe SFVQ on these layers. It has been shown that these layers are more favorable for interpretation\nbecause they render more disentangled representations, they are not constrained to any specific dis-\ntribution, and they suitably model the structure of the real data (Karras et al., 2019; H\u00e4rk\u00f6nen et al.,\n2020; Shen et al., 2020). For StyleGAN2, we employ the pretrained models on FFHQ, AFHQ,\nLSUN Cars, CIFAR10 datasets, and also the pretrained BigGAN on ImageNet.\nWe trained the SFVQ with various bitrates ranging from 2 to 12 bit (4 to 4096 codebook vectors).\nSince the training of SFVQ is not sensitive to hyper-parameter tuning, we adopt a general setup that\nworks for all pretrained models and datasets. In this setup, we trained SFVQ with the batch size\nof 64 over 100k number of training batches (for each recursion step) using Adam optimizer with\nthe initial learning rate of $10^{-3}$. We used a learning rate scheduler such that during each recursion\nstep, we halved the learning rate after 60k and 80k training batches. Since we use pretrained\nmodels, we only need to train the SFVQ exclusively. However, if we want to train the SFVQ jointly\nwith other modules that require gradients, we use our recently proposed noise substitution in vector\nquantization (NSVQ) (Vali & B\u00e4ckstr\u00f6m, 2022) technique to avoid the gradient collapse problem."}, {"title": "5 RESULTS AND DISCUSSIONS", "content": "5.1 STYLEGAN2: UNIVERSAL INTERPRETATION\nTo explore a universal interpretation of latent space, we apply the SFVQ on that distribution and\nplot the generated images from the obtained SFVQ's codebook vectors. According to the inherent\narrangement of SFVQ's codebook vectors, we expect SFVQ to capture a universal morphology of\nthe latent space. As the first experiment, we apply the SFVQ on the intermediate latent space (W)\nof StyleGAN2 (Karras et al., 2020) pretrained on the CIFAR10 dataset. During training, the number\nof extracted latent vectors is unbiased for all CIFAR10 classes. At first glance, we observe a clear arrangement with respect to the image class, such that images\nfrom an identical category are organized into groups. Also, apart from the horse class, all animal\ntypes and industrial vehicles are located next to each other. Furthermore, there are some visible\nsimilarities for subsequent codebook vectors within a class, such as similar objects' rotation, scale,\ncolor, and background. We also see these observations consistently over different bitrates of SFVQ.\nWhen increasing the SFVQ's bitrate by one (doubling the codebook size), the number of specified\ncodebook vectors for each class will be approximately doubled, and as a result, the proportion of\neach class remains unchanged. Therefore, from the SFVQ curve (for any bitrate), we can infer the\nportion that each class occupies the latent space. For instance, the horse class is always the dominant\nclass of data in the StyleGAN2 latent space by occupying about 25% of codebook vectors.\nTo inspect the learned SFVQ from another viewpoint, we plotted the heatmap of Euclidean distances\nbetween all SFVQ's codebook vectors. Again, we observe a clear separation between\ndifferent classes, as each dark box shows a data class. It is important to note that the SFVQ captures\nthis class separation property because of its inherent orderliness and in a completely unsupervised\nway. Also, we spot a bigger dark box shared between cat and dog classes because they are the most\nsimilar classes and reside close to each other in the latent space.\nIn the second experiment, we applied a 5 bit SFVQ on the W space of the pretrained StyleGAN2\non the FFHQ dataset. We observe similarities among neighboring codebook vectors such as baby-aged faces for indices\n6-7, hat accessory for indices 13-16, eyeglasses for indices 18-19, rotation from right to left from\nindex 17 to 20, and rotation from left to right from index 27 to 31. Based on our investigations,\nthe StyleGAN2's W space for FFHQ, AFHQ, and LSUN Cars are much denser and entangled than\nCIFAR10 because they are trained on not very diverse data like CIFAR10. That is why the learned\nSFVQ curve does not show a perfect distinctive universal interpretation. We\nprovided a similar figure for a 6 bit SFVQ for the AFHQ dataset in Appendix A.1.\nAs the third experiment, we examined a 2bit SFVQ applied on the W space of StyleGAN2 pre-\ntrained on the FFHQ dataset We observe a clear\nseparation between females and males, while we only have two individual identities, each repre-\nsenting the average face for females and males. From this SFVQ curve, we can infer some more\ninteresting properties. We hypothesize that each SFVQ line corresponds to an interpretable direction\nShown in  Direction I (direction from codebook vector 1 to 2) is for changing rotation to the\nright, direction II refers to the gender change, and direction III is for changing rotation to the left.\nFor more clarification, we compute the angles between these directions in degrees, which somewhat\nconfirms our hypothesis. Direction II is almost orthogonal to two other directions, and directions I\nand III are approximately inverse (with a difference of 159.6 degrees)."}, {"title": "5.2 STYLEGAN2: INTERPRETABLE DIRECTIONS", "content": "Figure 4(b) reminds us of the PCA-based method of GANSpace (H\u00e4rk\u00f6nen et al., 2020) which finds\nPCA directions as interpretable directions. Similar to the first two PCA directions of GANSpace that\nrefer to the change of gender and rotation, the SFVQ lines are also located along the directions in\nwhich the training data has the most variance, i.e. gender and rotation. However, in the pretrained W\nspace of StyleGAN2, the interpretable directions are not necessarily orthogonal to each other, and\nthat is why only the first 100 (out of 512) GANSpace's PCA orthogonal directions lead to noticeable\nchanges (H\u00e4rk\u00f6nen et al., 2020). In contrast, in the SFVQ curve, each direction (SFVQ's line) could\npotentially work for a meaningful and obvious change. These observations and discussions motivate\nus to use the SFVQ's curve to discover interpretable directions, which we study in the following.\nWe applied SFVQ curves (from 2 to 12bit) on the W space of StyleGAN2 pretrained on FFHQ,\nAFHQ, and LSUN Cars datasets and observed the generated images of SFVQ curves. By observa-\ntion, we spotted some useful interpretable directions, Shown in fig. 5. Columns (a) and (b) represent\nthe discovered direction from two SFVQ's subsequent codebook vectors, column (c) is the test\nvector in the latent space to which we apply the direction, and column (d) is the final result after\napplying the direction. Similar to the GANSpace naming convention, the term Wi-Wj means we\nonly manipulate the style blocks within the range [i-j]. Note that we take the directions only from\nSFVQ's subsequent codebook vectors, but not from two necessarily similar though far apart code-\nbook vectors. Otherwise, one can accidentally find directions by taking two codebook vectors from\nan ordinary VQ that might lead to a meaningful direction. To show the practicality of the discovered\ndirections better, we applied all of them only on one identical test image (except for the Beard and\nBald directions which are specified to males).\nOne great advantage of our proposed method over other approaches is that it almost keeps the iden-\ntity of the test image (column (c)) fixed when applying the interpretable directions (see table 2).\nAnother advantage is that we could find some new and unique directions that were not found in\nprevious methods, such as Hat, Beard for FFHQ, Age, Bicolor for AFHQ, and Classic for LSUN\nCars. These unique directions are not limited to these ones, as users can find other directions by their\nown observations. In addition, our approach detected an inclusive set of directions, whereas other\nmethods in the literature could only find a portion of them. It is important to note that the directions\nfor the AFHQ dataset are class-agnostic, i.e. the direction for one animal works for other animal\nspecies because in fig. 5 we found the directions from Wolf and Cat classes, but we applied them to\na Dog class. However, some directions do not necessarily work for all animal species in the AFHQ\nbecause the transformations are restricted by the dataset bias of individual animal classes (Jahanian\net al., 2019) (see Appendix A.2 for more detail). Another interesting observation is how the Hat\ndirection (discovered for males) works logically but differently for females."}, {"title": "5.3 BIGGAN: INTERPRETABLE DIRECTIONS", "content": "BigGAN512-deep (Brock et al., 2018) samples a random vector z from a normal prior distribution\np(z) and maps it to an image. Since in BigGAN512-deep, the intermediate layers also take the\nrandom vector z as input (i.e. skip-z connections), the vector z has the most effect on the generated\noutput image. Hence, we should find the semantic directions in p(z) space. However, as p(z) is an\nisotropic distribution, it is difficult to find useful directions from it (H\u00e4rk\u00f6nen et al., 2020). There-\nfore, similar to GANSpace, we first train the SFVQ on the first linear layer (L) of BigGAN512-deep\nto search for interpretable directions within this space, and afterward, we transfer these directions\nback to p(z) space. To this end, we sample $10^6$ random vectors from p(z) and generate their"}, {"title": "5.4 COMPARISON WITH OTHER METHODS", "content": "We compared our interpretable directions with GANSpace (H\u00e4rk\u00f6nen\net al., 2020) and LatentCLR (Y\u00fcksel et al., 2021) qualitatively and\nquantitatively. The reason for choosing these methods is that their in-\nterpretable directions for StyleGAN2-FFHQ were readily available in\ntheir GitHub repositories. Hence, we skipped other methods that were\nnot trained on StyleGAN2-FFHQ or did not share their directions. We\nfocus on StyleGAN2-FFHQ for comparisons, because we planned to\nuse the pretrained networks for face attributes rating of Zhang et al.\n(2017); Karkkainen & Joo (2021); Jiang et al. (2021); Doosti et al.\n(2020); Deng et al. (2019) for our quantitative comparisons.\nFigure 6 shows the qualitative comparison. To have a fair compari-\nson, we use the same amount of shift (\u03c3) toward each direction be-\ncause, as mentioned in Tzelepis et al. (2021), it is the advantage of\na direction if it reaches the desired change in the attribute within a\nshorter path. The image in the red square is the initial test image to\nwhich we apply the changes. In Rotation direction, our method ro-\ntates the face better than LatentCLR, while GANSpace yields slightly\nmore rotation than ours in the positive path, and our method brings\nmore rotation than GANSpace in the negative path. For Smile direc-\ntion, our method opens and closes the smile effectively in both positive\nand negative paths while keeping the identity and age attributes almost\nfixed. Whereas both GANSpace and LatentCLR change the identity,\nand particularly, GANSpace is highly entangled with age attribute. In\nHair Color direction, again, our method keeps the identity better than\nthe others, whereas GANSpace alters the face highlights, and Latent-\nCLR mutates the gender, age, beard, and race. For Gender direction,\nGANSpace is entangled with age direction, and our method remains in\nthe valid range of generations better than GANSpace. For Age direc-\ntion, our method covers a wider range of ages than LatentCLR, while\nLatentCLR alters the face highlights. It is possible to test and compare\nthe directions for these methods in our GitHub demo directory."}, {"title": "5.5 ABLATION STUDY", "content": "We did an ablation study on the effect of differ-\nent SFVQ's bitrates (from 2 to 12 bit) on the in-\nterpretations of StyleGAN2 models on FFHQ,\nAFHQ, and LSUN Cars datasets. Regarding\nuniversal interpretation, for all bitrates, we ob-\nserve the inherent structure in SFVQ's subse-\nquent codebook vectors sharing similar gen-\nerative factors such as rotation, background,\nand accessories for FFHQ. When increasing the\nbitrate, we see more diversity in the images\n(e.g. more identities for FFHQ) because we\nmodel the latent space with more clusters (or\ncodebook vectors). We provided images corre-\nsponding to SFVQ codebooks from 2 to 8 bit in\nAppendix A.5.\nRegarding interpretable directions, a higher\nSFVQ bitrate allows the curve to get more\nturned and twisted in the latent space, increas-\ning the chance of spotting more detailed or\nintricate directions. Based on our investiga-\ntions, the directions that alter images more\nstructurally can be found from lower bitrates\nand vice versa. For example, for StyleGAN2-\nFFHQ, we found rotation, gender and age di-\nrections from 2, 5, and 6bit SFVQ, respec-\ntively. On the other hand, we detected the di-\nrections that cause a partial change on the face\nsuch as smile, hair color, makeup, race, and\nbald from 12 bit SFVQ."}, {"title": "5.6 JOINT INTERPRETABLE DIRECTIONS", "content": "By observing images of the SFVQ's curve to find interpretable directions, we can also\ndiscover joint interpretable directions from subsequent codebook vectors that differ in multiple at-\ntributes. By joint, we mean to change, for example, rotation and gender attributes simultaneously.\nTo the authors' knowledge, this is the first time such joint directions are extracted from the latent\nspace. In fact, joint directions are the directions in which multiple attributes are entangled. Su-\npervised methods cannot find joint directions because they use pretrained networks or labeled data\nwith respect to only one attribute. Furthermore, finding joint directions will be laborious for the\nunsupervised methods because 1) their training strategy was not designed for this task, 2) they have"}, {"title": "5.7 CONTROLLABLE DATA AUGMENTATION", "content": "According to the training objective of SFVQ to map in-\nput vectors on the line connecting subsequent codebook\nvectors, SFVQ has the property that its lines are mainly\nlocated inside the distribution's space. This property is\ndesirable for controllable data augmentation because we\nhave many meaningful points (located on SFVQ's curve)\navailable to generate valid images. By looking at images\ncorresponding to SFVQ's curve in fig. 4(a), we have an\nidea of the possible generations from each part of the\ncurve. For instance, to generate baby-aged faces we select\n20 equally-spaced points on the line connecting codebook\nvectors of indices 6 to 7 in fig. 4(a), and we plot the gener-\nations corresponding to these 20 points in the middle row\nof fig. 8. In a similar way, we take 20 equally-spaced points on the line connecting two subsequent\ncodebook vectors of 15 and 16 in fig. 4(a) and generate their corresponding images in the bottom\nrow of fig. 8. We observe that all 20 generations contain the hat accessory for a male person.\nWe also take the line connecting two neighboring codebook vectors (under Euclidean distance) of\na 5 bit VQ and plot similar generations in the top row of fig. 8. To have more diverse generations,\nfor all generations in fig. 8, we added normal noise (N(0, 0.3)) to the selected points. As expected,\nall generations of SFVQ consistently follow the properties of their corner points, such that they are\nall faces of babies or males wearing hats. However, the generations for two neighboring codebook\nvectors of VQ do not follow any specific rule as we observe changes in gender, age, and race among\nthem. Thus, here, by controllable, we mean that the users have control over what type of images\nwith specific characteristics they intend to generate."}, {"title": "6 CONCLUSIONS", "content": "Generative adversarial networks (GANs) are well-known image synthesis models widely used to\ngenerate high-quality images. However, there is still not sufficient control over generations in GANs\nbecause their latent spaces act as a black box and are thus hard to interpret. In this paper, we used\nthe unsupervised space-filling vector quantizer (SFVQ) technique to get a universal interpretation of\nthe latent distribution of GANs and to find their interpretable directions. Our experiments showed\nthat the SFVQ can capture the underlying morphological structure of the latent space and discover\nbetter and more consistent interpretable directions compared to GANSpace and LatentCLR methods.\nSFVQ gives the user proper control for generating images and manipulating them and reduces the\nsearch effort for finding the desired direction of a change. SFVQ is a generic tool for modeling\ndistributions that is neither restricted to any specific neural network architecture nor any data type\n(e.g. image, video, speech, etc.)."}, {"title": "7 REPRODUCIBILITY STATEMENT", "content": "In our GitHub repository, we uploaded the PyTorch code of the space-filling vector quantization\n(SFVQ) and an example of how to train it on a sample Gaussian distribution. In addition, we\nput some other Python codes that give instructions on how to track the training logs manually and\nmake sure that SFVQ is getting trained correctly, how to initialize the codebook vectors for SFVQ,\nand how to expand the SFVQ's codebook at each recursion step. We also shared the discovered\ninterpretable directions of Figure 5 and Figure 7 along with a code that enables the user to apply and\nexperiment with these directions. Furthermore, we uploaded the learned SFVQ's codebook vectors\nfor different datasets over different bitrates (ranging from 2 to 12 bit) for StyleGAN2 and BigGAN\nmodels together with a code to plot the corresponding images from these codebooks. We also put\nthe code for controllable data augmentation (discussed in Section 5.7). In the README.md file, we\nmentioned all relevant details to make our proposed method reproducible."}, {"title": "A APPENDIX", "content": "A.1 STYLEGAN2: UNIVERSAL INTERPRETATION (CONTINUATION)\nSimilar to what was discussed in Section 5.1 of the paper, we apply the SFVQ to capture a universal\nmorphology of the latent space, and we expect that subsequent codebook vectors in SFVQ refer to\nsimilar images. Hence, we applied a 6 bit SFVQ on the W space of the StyleGAN2 model pretrained\non the AFHQ dataset. Images corresponding to the SFVQ's codebook vectors are represented in\nShown in fig. 9. We can see that similar animal species are generally located next to each other. In addition,\nthere are some other similarities among neighboring codebook vectors, such as change in rotation\n(from right to left) when moving from index 0 to index 10, change in rotation (from left to right)\nwhen moving from index 26 to index 34, light-colored animals for indices 22-25, bi-colored animals\nfor indices 26-29, and baby-aged cats for indices 61-62."}, {"title": "A.2 CLASS-AGNOSTIC DIRECTIONS FOR STYLEGAN2 PRETRAINED ON AFHQ DATASET", "content": "According to what was discussed in Section 5.2 of the pa-\nper, in this section we aim to test whether and how the\ndiscovered direction of Bicolor (in fig. 5 of the paper)\nis class-agnostic across different AFHQ animal classes.\nTo this end, we applied this direction to all existing an-\nimal species in the AFHQ dataset and represented the\nShown in fig. 10. We observe that this direction works\nwell for Cat and Dog classes because there exists enough\ndata (i.e. cats and dogs with bicolored faces) within the\nAFHQ dataset. Therefore, the learned latent space sup-\nports this transformation. In addition, this transformation\nmore or less works for Wolf class, since Wolf looks like\nSiberian husky (which exists in AFHQ dataset), and this\ntransformation leads the Wolf class to become similar to\na Siberian husky. However, the Bicolor direction does\nnot work for other animal classes of Fox, Leopard, Chee-\ntah, Tiger, and Lion. The reason is that the learned latent\nspace is constrained by dataset bias of individual classes\n(Jahanian et al., 2019). In other words, the learned latent\nspace does not support this transformation for them since\nthere is no image with a bicolored face from these animal\nclasses within the AFHQ dataset. The o value determines\nthe magnitude of the step we take toward the Bicolor di-\nrection. To make sure whether this direction works for\nthese five animal classes, we used a larger o value (bigger\nsteps) for them. We observe that even with larger steps,\nnot only there is no meaningful transformation effect of\nthe desired direction, but also, in the very last step (30),\nthe images turn to become unrealistic by having some ar-\ntifacts."}, {"title": "A.3 CLASS-AGNOSTIC DIRECTIONS\nFOR BIGGAN PRETRAINED ON IMAGENET DATASET", "content": "As discussed in Section 5.3 of the paper, we found that the discovered directions by SFVQ (in\np(z) space of BigGAN) for the golden retriever class are class-agnostic. It means that the detected\ndirections also work when applied to other data classes within the ImageNet dataset. To confirm\nthis, we applied all five directions found for the golden retriever (in fig. 5 of the paper) on the husky\nclass, and we illustrated the results in fig. 11. The image in the middle column (in red square) is the\ninitial test image to which we apply the directions, such that we step along both sides of a direction.\nAccording to the figure, all five directions are valid for husky class, resulting in meaningful and\nexpected transformations."}, {"title": "A.4 SUBSIDIARY STUDY: TRAVELING SALESMAN PROBLEM", "content": "Space-filling vector quantization (SFVQ) has some parallels with the classic traveling salesman\nproblem (TSP) (Flood, 1956) in bringing order to a set of codebook vectors. One could ask whether\nwe can achieve a better codebook arrangement than SFVQ by applying an ordinary vector quanti-\nzation (VQ) as usual and, afterward, use one of the traveling salesman solutions to reorganize VQ\ncodebook vectors. The scenario of TSP is that we have a list of cities (codebook vectors) and the\ndistances between them, then we aim to discover the shortest possible route to visit each city only\nonce. TSP is an NP-hard problem to solve. We can interpret these cities as the codebook vectors of\na VQ. If we learn an 8 bit VQ as usual and intend to rearrange the codebook vectors to achieve the\nshortest route, then there are 256"}]}