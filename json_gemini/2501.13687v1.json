{"title": "QUESTION ANSWERING ON PATIENT MEDICAL RECORDS WITH PRIVATE FINE-TUNED LLMS", "authors": ["Sara Kothari", "Ayush Gupta"], "abstract": "Healthcare systems continuously generate vast amounts of electronic health records (EHRs), commonly stored in the Fast Healthcare Interoperability Resources (FHIR) standard. Despite the wealth of information in these records, their complexity and volume make it difficult for users to retrieve and interpret crucial health insights. Recent advances in Large Language Models (LLMs) offer a solution, enabling semantic question answering (QA) over medical data, allowing users to interact with their health records more effectively. However, ensuring privacy and compliance requires edge and private deployments of LLMs.\nThis paper proposes a novel approach to semantic QA over EHRs by first identifying the most relevant FHIR resources for a user query (Task1) and subsequently answer-ing the query based on these resources (Task2). We explore the performance of privately hosted, fine-tuned LLMs, evaluating them against benchmark models such as GPT-4 and GPT-40. Our results demonstrate that fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by 0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we examine advanced aspects of LLM usage, including sequential fine-tuning, model self-evaluation (narcissistic evaluation), and the impact of training data size on performance. The models and datasets are available here: https://huggingface.co/genloop", "sections": [{"title": "1 Introduction", "content": "The Fast Healthcare Interoperability Resources (FHIR) [1] standard, developed by HL7, provides a consistent framework for exchanging electronic health records (EHRs) across healthcare systems, enabling improved interoperability. FHIR's structured, machine-readable format supports the seamless transfer of complex healthcare data, making it central to modern health information exchange.\nRecent mandates requiring the use of HL7 FHIR Release 4 to support API-enabled services reflect a shift toward greater data transparency and patient autonomy[2]. Regulatory changes, particularly the Anti-Information Blocking provisions of the 21st Century Cures Act [3], also emphasize the need for patient-centric access to health data. Platforms like Apple Health now integrate FHIR data, giving patients unprecedented access to their medical records and creating new opportunities to transform complex data into actionable health insights. [4]\nDespite these advances in interoperability, patients often face significant challenges when inter-acting with their own health data. Complex medical terminology, language barriers, and limited health literacy make it difficult for many individuals to understand their conditions, diagnoses, or treatment plans [5]. These challenges are not just inconveniences\u2014they can lead to delays in care, misunderstandings of critical information, and ultimately, poorer health outcomes.\nLarge Language Models (LLMs) [6] provide a paradigm for human readability of their health records. LLMs, characterized by their ability to process vast amounts of unstructured data and generate human-like text, are transforming the landscape of healthcare informatics. Their inherent capacity to perform natural language understanding (NLU) and generation enables them to extract, summarize, and interpret complex medical information.\nIn the context of this paper, LLMs serve as a bridge between raw FHIR (Fast Healthcare Interoper-ability Resources) data and end-users, translating intricate clinical terminology into plain language that patients and healthcare providers can easily comprehend. By leveraging Large Language Models (LLMs), healthcare providers have the opportunity to simplify and personalize health data. Such systems can empower patients, making their medical information more accessible and helping them better understand their health, which in turn may enhance patient engagement and adherence to treatment plans. This approach also has the potential to reduce healthcare inefficiencies by offering quicker, more intuitive access to relevant medical records.\nHowever, the application of LLMs for such tasks faces significant barriers, particularly with respect to data privacy and security. Sharing personal health information (PHI) with cloud-hosted, general-purpose LLMs risks violating key privacy regulations, such as HIPAA [7], the California Consumer Privacy Act (CCPA)[8], and the Biometric Information Privacy Act (BIPA, Illinois) [9]. In the healthcare domain, ensuring the confidentiality of sensitive patient data is paramount. For this reason, edge-deployed, privately hosted LLMs present a more viable solution, allowing healthcare systems to maintain control over patient data while still benefiting from the powerful capabilities of LLMs.\nThanks to open-source models like Meta's LLaMA series [10, 11] and Mistral's open source model series [12], it is now feasible to self-host LLMs for specific applications. However, these smaller models often fall short in the accuracy required for sophisticated healthcare tasks such as semantic question answering. Fine-tuning with techniques like LoRA [13, 14, 15] emerges as a critical step to address this gap with computational efficiency. By customizing smaller, domain-specific models to the nuances of the target task, fine-tuning enhances both the accuracy and performance of these models. This adaptation not only makes them suitable for demanding healthcare applications but also ensures they remain efficient and privacy-compliant, aligning with the requirements of edge deployment in sensitive domains like healthcare.\nIn this paper, we break down the task of semantic question answering over medical records into two stages: (1) retrieving the most relevant FHIR resources given a user's medical query, and (2) answering the query based on the retrieved resources. We fine-tune smaller, open-source models for each stage, ensuring that they are well-suited to the unique challenges of medical data processing. To facilitate this process, we generate synthetic patient data and utilize larger general-purpose models to create task-specific synthetic datasets (data collection). These datasets are then refined (data refinement), followed by training multiple models to identify the best-performing configurations (training), and finally, evaluating their performance against established benchmarks (evaluation). The resulting models are deployed in a privacy-compliant setup.\nOur experiments reveal that fine-tuned smaller models significantly outperform larger general models like GPT-4, particularly in terms of accuracy, efficiency, and privacy compliance. This work demonstrates that edge-deployed, fine-tuned LLMs can offer a practical solution for healthcare providers seeking to implement patient-centric semantic question answering without sacrificing data privacy.\nBeyond the core tasks, this study explores several important aspects of LLM behavior in healthcare contexts:\n1. The impact of sequential fine-tuning on multi-task performance.\n2. The tendency of LLMs to exhibit narcissistic behavior by favoring their own outputs.\n3. The influence of dataset size on fine-tuning effectiveness, with implications for resource-constrained environments.\nWe discuss these results in Section 5 (Results and Discussion) 5 and highlight key insights for future research in Section 6 (Conclusion, Limitations, and Future Work) 6."}, {"title": "2 Related Work", "content": "The application of Large Language Models (LLMs) to patient medical data processing has garnered significant attention in recent research. Notably, [16] demonstrated the efficacy of leveraging LLMs to convert clinical texts into FHIR resources with an impressive accuracy rate of over 90 percent, thereby streamlining the processing and calibration of healthcare data and enhancing interoperability. [17] introduced a multi-agent workflow utilizing the Reflexion framework, which employed ICD-10 codes from medical reports to generate patient-friendly letters with an accuracy rate of 94.94%. These studies did not involve direct querying of Electronic Medical Records (EMRs).\nRecently, [18] introduced \"LLM on FHIR\", an open-source mobile application that leverages GPT-4 to allow users to interact with their health records, demonstrating impressive accuracy and relevance in delivering comprehensible health information to patients. [19] developed on top of [18] to replace GPT-4 with fine-tuned LLMs. They divided the FHIR querying process into three tasks: filtering,"}, {"title": "3 Approach", "content": "To approach this task, we break query processing in 2 stages, similar to how retrieval augmented generation [21, 22], is performed.\n1. Task 1: Identifying the FHIR resources from the patient's medical record that are relevant to a given natural language patient query. Each patient will have numerous FHIR resources in their patient record, only some of which are relevant to the patient query. We formulate the problem as a binary classification problem i.e. given a query q, and a FHIR resource r, the problem is setup as F(q, r) = I{0, 1}\n2. Task 2: Answering the medical query of the patient using the FHIR resources that were identified as relevant to the query i.e. generating the response based on (query, list of relevant resources) pairs.\nFigure 1 1 outlines the approach. The main inputs are the user query and the EHR records in the FHIR format. The EHR records can be both relevant or irrelevant when received. The Task 1 classifies relevant FHIRs and help us pick the necessary records to generate the answer for the user's medical question in Task 2.\nLLMs are the intelligence modules for Task 1 and Task 2. In our approach, we fine-tune the top-performing models available at the time of experimentation (Llama-3.1-8B, and Mistral-NeMo) for each of these tasks. We also compare the results switching them with GPT-4 (SOTA model at the time), and Meditron-7b [20] (SOTA medical domain model at the time). More details on the experiments, including dataset generation, are covered in Section 4 (Experiment Details). The Results are discussed in Section 5 (Results and Discussion)"}, {"title": "4 Experiment Details", "content": "We employed Synthea [23], an open-source synthetic patient generator, to produce HL7 FHIR-formatted medical records (JSON files). we started with generating data of 50 patients for Task 1 and an additional 450 patients for Task 2. Each file was processed to retain only patient-relevant resources, excluding entries such as \"SupplyDelivery\". Only the following resource types were included: \"Procedure\", \"Medication\", \"MedicationRequest\", \"Encounter\", \"ImagingStudy\", \"Im-munization\", \"Device\", \"CarePlan\", \"ExplanationOfBenefit\", \"AllergyIntolerance\", \"Observation\", \"Condition\", \"DiagnosticReport\"\nWe ran a custom script to filter and retain only those segments of each FHIR resource that are likely to be relevant to patient queries. The script eliminated non-essential details and sections from which meaningful medical information could not be extracted. This pre-processing step was designed to eliminate irrelevant or redundant information.\nFigure 2 2 shows the complete data preparation flow. More details on creating the training pairs for Task 1 LLM fine-tuning and Task 2 LLM fine-tuning are covered in the following sections."}, {"title": "4.1 Dataset", "content": "We employed Synthea [23], an open-source synthetic patient generator, to produce HL7 FHIR-formatted medical records (JSON files). we started with generating data of 50 patients for Task 1 and an additional 450 patients for Task 2. Each file was processed to retain only patient-relevant resources, excluding entries such as \"SupplyDelivery\". Only the following resource types were included: \"Procedure\", \"Medication\", \"MedicationRequest\", \"Encounter\", \"ImagingStudy\", \"Im-munization\", \"Device\", \"CarePlan\", \"ExplanationOfBenefit\", \"AllergyIntolerance\", \"Observation\", \"Condition\", \"DiagnosticReport\"\nWe ran a custom script to filter and retain only those segments of each FHIR resource that are likely to be relevant to patient queries. The script eliminated non-essential details and sections from which meaningful medical information could not be extracted. This pre-processing step was designed to eliminate irrelevant or redundant information."}, {"title": "4.1.1 Task 1 Data Preparation", "content": "The following additional steps were followed for Task 1 Data Preparation\n1. Data Selection: We randomly selected a batch of 10 FHIR resources from each patient record.\n2. Query Generation: GPT-4 was employed to generate a natural language query based on one or more of the 10 selected FHIR resources. We utilized prompt engineering to ensure the generation of realistic, simple, and non-technical queries. The specific prompt used has been provided in Appendix A.1."}, {"title": "4.1.2 Task 2 Data Preparation", "content": "For Task 2, we generated 450 additional patient records and processed them through the same method as Task 1 Data Preparation. That gave us a total of 500 patient records, with the corresponding relevance label for each resource,.\n1. Answer Generation: For each query, we have relevant and irrelevant FHIR resources. For the task of answer generation, we are interested in only the relevant resources. Therefore, each input example for Task 2 is a pair of (query, [list of relevant resources]). Since we have 10 queries per patient record, we have 5,000 such examples. For each of them, GPT-4 was used to create a relevant answer. The generated answered were reviewed for accuracy along with their context. Finally, we had 5,000 examples of (query, [list of relevant resources], answer). The specific prompt used for GPT-4 based synthetic data generation has been provided in Appendix A.2.\n2. Dataset Creation: We conducted a 98-2 train-test split, resulting in 4900 examples in the training dataset and 100 examples in the test dataset. We also randomly sub-sampled 500 examples from the 4900 example training dataset to create another smaller training dataset to determine the effect of training dataset size on model performance (details covered in following sections)."}, {"title": "4.2 Fine-tuning", "content": "Fine-tuning [13, 14, 15] is a technique by which the original model is modified to adapt to the specific task of concern. Full fine-tuning of LLMs is a compute intensive process. For reference, a 8B parameter model will require around 100GB GPU VRAM, that is available with exclusive GPU machines like H200 costing more than $10,000 per month on rent and hardly available to purchase.\nParameter-Efficient Fine-Tuning (PEFT) [13] gives us a computationally efficient technique to perform fine-tuning with marginal performance difference. It adds additional trainable parameters to the LLM, that work in conjunction with the original pre-trained weights to give desired results. In our experiments, we use a variant of PEFT that provides even better memory efficiency called QLoRA [15]. This technique allows to load the original model weights in 4-bit quantization reducing memory usage, while training additional weights through the PEFT approach. For our experiments, we used a LoRA rank of 16, and trained for 5 epochs. We conducted our fine-tuning and inference experiments on an NVIDIA A100 40GB GPU. More model training configs are available with the model cards on our HF page here: https://huggingface.co/genloop"}, {"title": "4.3 Experiments", "content": "We additionally conduct the following experiments to ascertain how fine-tuned smaller LLMs perform against the general LLMs, and also empirically understand their behaviour:\n1. Experiment 1 - Comparison of Base Models, Fine-tuned Models, Meditron, and GPT-4 Performance: We compare the performance of pretrained models, models fine-tuned on specific tasks using custom datasets, and GPT-4 (and GPT-40) to understand how finetuning improves task-specific performance and how these models stack up against a state-of-the-art model like GPT-4 or GPT-40.\n2. Experiment 2 - Analyzing the Effect of Training Dataset Size on Model Performance: In this experiment, we assess how varying the size of the training dataset impacts model performance.\n3. Experiment 3 - Impact of Sequential Fine-tuning on Task Retention: Through this experiment, we explore cross-task knowledge transfer. Specifically, how fine-tuning a model on a second task affects its ability to retain knowledge from the first task, assessing whether the model \"forgets\" how to perform the original task after being adapted to a new one, and vice versa. Moreover, it investigates whether being initially trained on a first task with knowledge overlap improves or hinders performance on the second task.\n4. Experiment 4 - Investigating Self-Preference and Self-Recognition Bias in LLM-as-a-Judge Evaluation: This experiment analyzes potential biases in large language models when they act as evaluators, specifically focusing on whether the model exhibits a preference for its own outputs or recognizes and favors its own responses over others in an evaluation context.\nFor Task 1, we conducted Experiment 1 and Experiment 3.\n1. For Experiment 1, we fine-tuned Llama 3.1 Base, Llama 3.1 Instruct, Mistral NeMo Instruct, and Mistral NeMo Base using the 5,000-example dataset created for Task 1. We benchmarked these models against GPT-4 and Meditron-7b. The pretrained Llama 3.1 and Mistral Nemo Base models were our baseline.\n2. For Experiment 3, we explored two approaches. First, we fine-tuned Llama 3.1 Base on Task 2 using a 4,900-example dataset, followed by additional fine-tuning on Task 1. In the second approach, we reversed the order by first fine-tuning Llama 3.1 Base on Task 1, then further fine-tuning the resulting model on Task 2. We did an experiment with an extended prompt for the second approach, in order to instruct the model on the classification requirements.\nWe evaluated all these models on this binary classification task using F1, Recall, Precision, and Accuracy, placing greatest emphasis on F1.\nFor Task 2, we conducted all four experiments. In all these experiments, we used the METEOR score [24] as the evaluation metric. We chose METEOR as it incorporates stemming and synonymy"}, {"title": "5 Results and Discussion", "content": "matching, in addition to exact word matching, and aligns closely with human judgment at the sentence/segment level.\n1. For Experiment 1, we fine-tuned Llama 3.1 Base and Mistral NeMo Base using a dataset of 4,900 examples and benchmarked their performance against GPT-40 and Meditron 7B.\n2. For Experiment 2, we fine-tuned Llama 3.1 Base and Mistral NeMo Base using a smaller dataset of 500 examples. We compared these models' performance to those fine-tuned in the first experiment (4,900 examples) to assess the effect of dataset size on model performance.\n3. For Experiment 3, we conducted sequential finetuning on both Llama 3.1 Base and Mistral NeMo Base models, which were initially fine-tuned on Task 1. We further fine-tuned these models on Task 2 explore task transfer and task retention. Additionally, we fine-tuned the Llama 3.1 Base model, which had been first fine-tuned on Task 2 with 4,900 examples, on Task 1 using 5,000 examples, and then evaluated its performance on Task 2 to assess knowledge transfer across tasks.\n4. For Experiment 4, we performed an LLM-as-a-judge evaluation involving the top two performing models from the previous tasks using Claude Sonnet and GPT-40 as judge LLMs. We initially asked GPT-4o to evaluate and select the better response between the two models without knowing which model generated which response. After this, we disclosed which response was generated by GPT-4 and asked GPT-40 to choose again. We then conducted the same blind evaluation process with Claude. In both cases, the reference answer from the dataset was provided, and the evaluation criteria included the following:\n(a) Relevance: Does the answer accurately address the patient query using the relevant FHIR resources?\n(b) Groundedness: Is the answer based on factual, evidence-based information?\n(c) Completeness: Does the answer cover all aspects of the patient query comprehensively?\n(d) Quality: Is the answer well-written, clear, and useful for the patient?\n(e) Conciseness: Is the answer succinct while still being informative?\n(f) Closeness to Reference: How closely does the answer match the content and intent of the reference answer?"}, {"title": "5.1 Task 1 Results", "content": null}, {"title": "5.1.1 Experiment 1", "content": "The results for Task 1 Experiment 1 are shown in Figure 3 3. It can be seen that fine-tuning improves model performance significantly compared to the pre-trained baselines. LLaMA 3.1 Base (Fine-tuned) achieved the highest accuracy of 98.82%, with strong performance in both precision (96.97%) and recall (94.12%), leading to the strongest F1 score of 95.52%. This result highlights the model's effective learning and adaptation to Task 1 after fine-tuning, surpassing both GPT-4 and"}, {"title": "5.1.2 Experiment 3", "content": "The compiled results for this experiment of sequential training on Task 1 is presented in Figure 5 5. As expected, LLM trained on Task 1 and then on Task 2, loses its practice on Task 1 and gives inferior results. The F1 Score drops from 95.52 for a Llama 3.1 Base FT model to 31.62 for a Llama 3.1 Base model fine-tuned on Task 1 and then Task 2, as 67% drop in F1. Understandably, Task 2 is more verbose than the classification problem in Task 1. So we conducted another experiment"}, {"title": "5.2 Task 2 Results", "content": null}, {"title": "5.2.1 Experiment 1", "content": "Figure 6 6 compiles the results from Experiment 1 on Task 2. We see that fine-tuned Llama 3.1 8B Base and Mistral NeMo 12B models surpass their base versions by 200% and 136%, respectively. Mistral NeMo Base fine-tuned on the 4,900 examples achieved the highest performance with a METEOR score of 0.5333, while GPT-4 scored lower at 0.375223. Meditron scored 0.07 on the same task."}, {"title": "5.2.2 Experiment 2", "content": "For Experiment 2, we compared the Llama 3.1 and Mistral Nemo models trained on 4,900 examples against training on 500 examples. As can be seen in Figure 7 7, Mistral NeMo Base fine-tuned with 4900 examples outperformed its counterpart fine-tuned with 500 examples by 4.55% in METEOR score. Similarly, LLaMA Base fine-tuned with 4900 examples showed a 4.39% higher METEOR score compared to Mistral NeMo Base fine-tuned with 500 examples. We conclude that an increase in the training dataset size definitely improves model performance. However, a deeper study is required on more size variations and the limit to such improvement."}, {"title": "5.2.3 Experiment 3", "content": "The compiled results for this experiment of sequential training on Task 2 are presented in Figure 8 8. For Experiment 3, we observed differing effects of fine-tuning strategies between Mistral NeMo-based and LLaMA-based models. For Mistral NeMo, directly fine-tuning on Task 2 resulted in better performance than sequential fine-tuning on Task 1 followed by Task 2. Mistral NeMo Base FT-4900 (1) outperformed Mistral NeMo Sequential FT-4900 (Task 1 \u2192 Task 2) (2), and Mistral NeMo Base FT-500 (4) similarly outperformed Mistral NeMo Sequential FT-500 (Task 1 \u2192 Task 2) (6). However, in the case of LLaMA 3.1-based models, the opposite trend was observed: sequential fine-tuning led to better results. LLaMA Sequential FT-4900 (Task 1 \u2192 Task 2) (3) outperformed LLaMA Base FT-4900 (5), and LLaMA Sequential FT-500 (Task 1 \u2192 Task 2) (7) surpassed LLaMA Base FT-500 (8). Thus, the effect of sequential fine-tuning varies across models, and no conclusive pattern emerged. The reverse fine-tuning approach, where Mistral NeMo (9) was fine-tuned on Task 2 first and then Task 1, resulted in a notably low METEOR score of 0.16375. Similarly, the Llama-based model (10) showed similar results, with a score of 0.05841. This suggests that the model may have \"unlearnt\" important aspects of Task 2 during subsequent fine-tuning on Task 1."}, {"title": "5.2.4 Experiment 4", "content": "For Experiment 4, we selected the two best performing models from Task 2 Experiment 3 above - Mistral NeMo Sequential FT-4900 (Task 1 -> Task 2), Mistral NeMo Base FT-4900 - and compared it against GPT-4 response with LLM-as-a-judge evaluation. The judge LLMs were chosen to be GPT-40 and Claude Sonnet. The first evaluation was conducted without disclosing the identity of the models generating the response. This is marked as the \"blind\" evaluation. The second evaluation"}, {"title": "6 Conclusion, Limitations, and Future Work", "content": null}, {"title": "6.1 Conclusion", "content": "Here are the conclusions from the four experiments conducted, specifically addressing our use cases and the two tasks outlined in this paper:"}, {"title": "6.2 Limitations", "content": "One of the key limitations of our work is that we have used synthetic patient data instead of real patient data. This could impact the generalizability of our findings, as synthetic data may not capture the full complexity and variability present in actual patient records."}, {"title": "6.3 Future Work", "content": "In future work, we plan to investigate multi-task learning (MTL) strategies and continual pretraining (CPT) to improve the performance of models querying Fast Healthcare Interoperability Resources (FHIR) data. By integrating these approaches, we aim to develop a single model capable of efficiently handling both tasks simultaneously."}, {"title": "A Appendix", "content": null}, {"title": "A.1 GPT-4 Prompt for Query Generation: Task 1", "content": "To generate natural language queries, we used the following GPT-4 prompt:\nPretend you are a patient curious about an aspect of your medical history. Come up with a query that this patient might have regarding their medical data. At least one or more medical data points from the given set of FHIR resources should be sufficient to answer the query. Make the question realistic, simple, and non-technical. For example, 'What are my current medicines?' or 'When was my last shot?' or 'What were the complications of my last heart procedure?;'\nGenerate an output in the JSON format below corresponding to each of the 10 inputted resources after generating 1 query based on one or more of the 10 given FHIR resources: {10_resources}. The relevance should be 'relevant' if the resource was used by the model for the particular query, and 'irrelevant' if not. The resource_label should be a natural language label generated for each of the 10 resources in the format: 'Condition Cardiac Arrest 06-19-2018\u2019. Therefore, the output should be the 10 JSON formatted files per resource, with patient_id being the same throughout, relevance can be either relevant or irrelevant if it wasn't used to generate the query. Only one query has to be generated from the 10 resources. So the query label will be the same for all 10. Use the following format for the output:\n{\njson [\n\"resource\": \"{{resource}}\",\n\"query\": \"{{query}}\",\n\"relevance\": \"{{relevance}}\",\n\"patient_id\": \"{patient_id}\",\n\"resource_label\": \"{{resource_label}}\"\n}]"}, {"title": "A.2 GPT-4 Prompt for Answer Generation: Task 2", "content": "You are a knowledgeable and helpful medical assistant. Answer the given query using the list of relevant FHIR resources provided to you. 'Query': {query}, \u2018Resources': {resources}"}]}