{"title": "DynaGRAG: Improving Language Understanding and Generation through Dynamic Subgraph Representation in Graph Retrieval-Augmented Generation", "authors": ["Karishma Thakrar"], "abstract": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim\nto enhance language understanding and generation by leveraging external knowl-\nedge. However, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a novel\nGRAG framework is proposed to focus on enhancing subgraph representation and\ndiversity within the knowledge graph. By improving graph density, capturing entity\nand relation information more effectively, and dynamically prioritizing relevant\nand diverse subgraphs, the proposed approach enables a more comprehensive\nunderstanding of the underlying semantic structure. This is achieved through a\ncombination of de-duplication processes, two-step mean pooling of embeddings,\nquery-aware retrieval considering unique nodes, and a Dynamic Similarity-Aware\nBFS (DSA-BFS) traversal algorithm. Integrating Graph Convolutional Networks\n(GCNs) and Large Language Models (LLMs) through hard prompting further\nenhances the learning of rich node and edge representations while preserving the hi-\nerarchical subgraph structure. Experimental results on multiple benchmark datasets\ndemonstrate the effectiveness of the proposed GRAG framework, showcasing\nthe significance of enhanced subgraph representation and diversity for improved\nlanguage understanding and generation.", "sections": [{"title": "1 Introduction", "content": "Graph-structured data has emerged as a powerful tool for representing and analyzing complex\nrelationships in various domains, from social networks and recommendation systems to natural\nlanguage processing tasks. Knowledge graphs, a type of graph-structured data, capture entities and\ntheir relationships, providing a structured representation of information. Recent advancements in\nlarge language models (LLMs) have revolutionized natural language processing, showcasing their\nexceptional ability to understand and generate human-like text. These artificial intelligence systems\nare increasingly leveraging knowledge graphs to represent and reason about complex information,\nenhancing their performance on various tasks, including question answering, recommendation\nsystems, and natural language generation. However, generating relevant and informative responses\nfrom knowledge graphs remains a significant challenge, as it requires effectively capturing and\nintegrating both textual and topological information.\nConventional approaches for handling text-attributed graphs (TAGs) often rely on Graph Neural\nNetworks (GNNs) and simple encoding techniques, which fail to capture the rich semantic and\ncontextual information present in the textual attributes. Existing Graph RAG frameworks have\nattempted to address these limitations by incorporating multi-level summarization or soft prompts,\nbut these approaches have their own drawbacks, such as computational complexity and compatibility"}, {"title": "2 Related Work", "content": "The integration of external knowledge with LLMs through RAG has gained significant recent attention.\nGao et al. (2024)\u00b9 provide a comprehensive survey of RAG methods, discussing the evolution of\ngeneral paradigms and core components. Graph-based approaches have been explored to augment\nLLMs in various domains, such as knowledge graph generation (Chang et al., 2024; Yao et al.,\n2024)23, bidirectional reasoning (Pan et al., 2024)4, and categorization of graph generation (Guo\nand Zhao, 2022)5; these works primarily focus on broader integration strategies, specific graph\ngeneration tasks, or providing detailed taxonomies and evaluations of existing models. Several\nrecently proposed GRAG frameworks target multi-hop reasoning on textual graphs (Hu et al., 2024)6,\nknowledge graph question answering (Mavromatis and Karypis, 2024)7, relational knowledge (Peng\net al., 2024)8, and addressing challenges such as information hallucination and catastrophic forgetting\n(Sanmart\u00edn et al., 2024)9. While these approaches address specific challenges, they often face\ncompatibility issues with mainstream LLMs, prioritize dense subgraph reasoning and factual accuracy\nover capturing a richer variety of entity relationships, or focus on formalizing and surveying existing\nmethods. Larson et al. (2024) 10 propose a Graph RAG approach for query-focused summarization\nthat relies on multi-level community summaries and while thorough, the overall framework may\nrequire frequent regeneration of summaries as the underlying data evolves. Other works explore\nencoding graph-structured data as text for LLMs to improve graph reasoning tasks (Fatemi et al.,\n2023) 11 or investigate LLMs for learning on text-attributed graphs (Chen et al., 2024) 12. These papers\nprimarily address encoding strategies, graph reasoning challenges in black-box LLM environments,"}, {"title": "3 Methodology", "content": "The first step in GRAG is to construct a knowledge graph from textual data. This process involves\ndividing source documents into text chunks and identifying entities and their relations within these\nchunks. An LLM is used to extract significant entities, their summaries, and the relationships between\nthem. The extracted information is then used to generate embeddings for the entities and relations\nusing a pre-trained language model (PLM). This approach offers flexibility in tailoring the LLM\nprompts to focus on specific domains or themes of interest, enabling the creation of domain-specific\nknowledge graphs that capture the most pertinent information from the source documents based on\nthe user's needs."}, {"title": "3.1 Graph Consolidation", "content": "To enhance the graph representation of data, several novel techniques are employed to improve\ngraph connectivity and capture the prevalence of entities and relations. Central to this process is\nentity de-duplication using a Language Model captured in Figure 1. The LLM identifies highly\nsimilar or synonymous entities by analyzing the semantic similarity between entity labels and their\ncontext within the graph, aligning the graph with human understanding. For example, entities such as\n\"compute\", \"compute resources\", and \"compute resources usage\" can be consolidated into a single\nentity, like \"compute\". This consolidation process effectively handles new terms like 'GPT-7', which\nmodels like Word2Vec often lack vector representations for, by leveraging the LLM's ability to\nunderstand and relate semantic concepts.\nTo further enhance the representation of information, the de-duplication process involves a two-step\napproach to averaging entity embeddings. First, the embeddings of identical entities are averaged,\nensuring that each unique entity is represented accurately. Then, the embeddings of similar entities\naveraged, capturing the variety of entity representations without overemphasizing the prevalence of a"}, {"title": "3.2 Subgraph Retrieval", "content": "To efficiently retrieve relevant information from a large graph structure, a searchable database of pre-\ncomputed ego-graphs centered around each node is created, including nodes within k hops (typically,\nk=3). These ego-graphs are encoded using mean pooling and weighted averaging of node and edge\nembeddings, effectively capturing the significance of each entity within the subgraph. During the\nretrieval process, the top-N subgraphs are ranked and retrieved based on their cosine similarity to the\nquery embedding. To ensure a diverse and comprehensive representation of the graph structure, a\nnovel approach is employed to consider the uniqueness of the most prevalent nodes in each subgraph.\nThe top nodes included in the previously retrieved subgraphs are tracked, and a subgraph is added\nto the result set iteratively only if its top nodes have not been encountered before. This approach"}, {"title": "3.3 Subgraph Pruning", "content": "The retrieved subgraphs are pruned to scale data based on relevance. This process begins by\ncalculating the Euclidean distance between the query embedding and the embeddings of node\nsummaries and edge relation summaries. These distances are then passed through separate MLP\nscalers with sigmoid activation functions to learn relevance scores, which are used to mask nodes and\nedges. By applying these masks, the influence of irrelevant entities within the retrieved subgraphs\nis effectively reduced, allowing the pruning process to focus on the most pertinent information. To\nfurther refine the subgraphs, a Graph Convolutional Network (GCN) is employed to learn the graph\nstructure by aggregating information from the updated neighboring nodes and edges. The GCN\ncomputes node pruning scores using the output of the node pruner, which captures node features and\nneighboring information, while edge pruning scores are computed using the scaled edge embeddings\npassed through the edge pruner. By partially masking irrelevant entities during the message passing\nprocess, the GCN identifies and emphasizes the most pertinent substructures within the already\nfiltered subgraphs. This approach provides an efficient and effective approximation that focuses on\nthe key elements likely to contribute to the final generation."}, {"title": "3.4 Hard Prompting", "content": "In the GRAG framework, hard prompts are generated by traversing the pruned subgraphs. A novel\nDynamic Similarity-Aware BFS (DSA-BFS) algorithm is implemented which adjusts the node\nexploration order based on similarity scores, prioritizing the exploration of highly similar neighbors.\nThis innovative approach reveals deeper connections between nodes that might be missed in a strict\nBFS, enhancing the coherence of the prompt while maintaining the logical structure. This is combined\nwith pre-order traversal algorithms which collects relevant information such as entity summaries,\nedge relationships, and their pruned weights, providing additional context about the importance\nof each entity and relationship within the subgraph. Non-tree edges are also processed, providing\nsupplementary information and connections beyond the basic tree structure. The resulting prompt\nstring reflects the hierarchical structure of the subgraph, aligning with how LLMs understand and\nreason about structured data. By generating informative and structured prompts that capture both the\ntextual and topological aspects of the knowledge graph, GRAG leverages the strengths of LLMs in\nunderstanding and reasoning about hierarchical data."}, {"title": "3.5 Generating Responses", "content": "The query and hard prompt are combined into a single input string, which is then used to generate\nintermediate responses from each retrieved subgraph. These intermediate responses capture different\naspects and perspectives related to the query from various parts of the knowledge graph. The system\ncalculates a helpfulness score for each intermediate response based on relevance, coherence, and\nlevel of detail. The intermediate responses are then sorted by their helpfulness scores in descending\norder. The sorted intermediate responses are fed into the language model, which generates a\ncoherent and comprehensive final response, integrating information from multiple subgraphs while\navoiding redundancy. This approach effectively encourages the LLM to \"think\" first about a question\nbefore forming its final response while ensuring that the generation process maintains the essential\ninformation from the input graph to enable informed, relevant, and contextually appropriate text\ngeneration."}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of the proposed GRAG framework, experiments are conducted compar-\ning it to three other approaches: 1) a framework using regular BFS + retrieval based on node diversity,\n2) a framework using the graph representation but with regular BFS + retrieval based on cosine"}, {"title": "5 Discussion", "content": "The experiments conducted in this study demonstrate the effectiveness of the proposed GRAG\nframework in enhancing reasoning capabilities and generating more accurate and contextually\nrelevant responses compared to existing approaches. By integrating graph learning techniques\nand query-aware subgraph retrieval, GRAG effectively captures complex relationships within the\nknowledge graph, enabling the generation of informative and precise responses.\nThe significance of subgraph representation in organizing information efficiently is evident from the\nresults. Although other methods can yield acceptable results, the approach of prioritizing relevant\nsubgraphs through query-aware retrieval and traversal allows us to achieve the desired outcomes\nmore quickly and consistently. This efficiency in information retrieval and organization facilitates\nbetter resource utilization and opens avenues for further advancements in the field."}, {"title": "6 Conclusion", "content": "This work presents several significant contributions to the field of graph-based language understanding\nand generation. The novel GRAG framework introduced here integrates LLMs and graph learning\ntechniques, incorporating a query-aware subgraph retrieval and traversal algorithm that prioritizes\nrelevant subgraphs. The approach effectively captures complex graph relationships and generates\naccurate and contextually relevant responses from knowledge graphs. The experimental results\ndemonstrate the superiority of the proposed GRAG framework over existing baseline models across\nmultiple benchmark datasets. By sharing the source code and datasets, this framework promotes\nreproducibility and encourage further exploration in this promising direction.\nThe implications of this research extend beyond the scope of this study. The integration of graph\nlearning techniques and query-aware ranking within the RAG architecture opens up new possibilities\nfor enhancing the performance and versatility of AI systems in various domains. As research continues\nto advance augmented reasoning capabilities, the fusion of GRAG with long-context models presents\nan exciting avenue for future research, enabling efficient traversal of extensive contexts and linking\nof seemingly unrelated information. The proposed GRAG framework represents a significant step\ntowards harnessing the power of LLMs in graph-based systems, paving the way for more intelligent\nand context-aware applications."}]}