{"title": "What is the Alignment Objective of GRPO?", "authors": ["Milan Vojnovic", "Se-Young Yun"], "abstract": "In this note, we examine the aggregation of preferences achieved by the Group Relative Policy\nOptimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial\nintelligence models such as DeepSeek-R1-Zero and DeepSeekMath [DeepSeek-AI et al., 2025, Shao\net al., 2024]. The GRPO algorithm trains a policy using a reward preference model, which is computed\nby sampling a set of outputs for a given context, observing the corresponding rewards, and applying\nshift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function\nto discourage deviations from a reference policy.\nWe present a framework that enables us to characterise the stationary policies of the GRPO al-\ngorithm. This analysis reveals that the aggregation of preferences differs fundamentally from stan-\ndard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form\nof preference aggregation arises from the way the reward preference model is defined and from the\npenalty function, which we show to essentially correspond to the reverse Kullback\u2013Leibler (KL) diver-\ngence between the aggregation policy and the reference policy.\nInterestingly, we demonstrate that for groups of size two, the reward preference model corresponds\nto pairwise comparison preferences, similar to those in other alignment methods based on pairwise\ncomparison feedback. We provide explicit characterisations of the aggregate preference for binary\nquestions, for groups of size two, and in the limit of large group size. This provides insights into\nthe dependence of the aggregate preference on parameters such as the regularisation constant and the\nconfidence margin of question answers.\nFinally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to\nuse direct KL divergence as the penalty or to use rewards without scale normalisation.", "sections": [{"title": "1 Introduction", "content": "The recently developed advanced artificial intelligence model, DeepSeek R1, has demonstrated remark-\nable performance in solving complex reasoning tasks, logic problems, and other step-by-step problems\nDeepSeek-AI et al. [2025]. At its core, the system employs reinforcement learning, specifically the Group\nRelative Policy Optimisation (GRPO) algorithm, originally proposed in Shao et al. [2024]. The objective\nis to train a language model using reinforcement learning, where feedback preferences serve as a re-\nward signal alongside a reference language model. This process can be viewed as aligning the reward\nmaximisation preference and the reference model's preference. For example, the rewards may be ac-\ncuracy indicators computed through a rule-based reward system, determining whether a response to a\ngiven question is correct. In this note, we refer to responses as outputs and questions as contexts. GRPO\nextends the previously proposed Proximal Policy optimisation (PPO) algorithm [Schulman et al., 2017]\nin several ways: it introduces a novel method for computing the advantage of outputs in a given con-\ntext by sampling a group of outputs, and it incorporates a new regulariser based on an estimator of the\nKullback-Leibler (KL) divergence to bias the policy towards a reference policy model.\nIn this note, we examine the GRPO algorithm, focusing on its alignment properties and their rela-\ntionship to other alignment algorithms.\nGroup Relative Policy optimisation (GRPO) For a context q sampled from a distribution \u03bc, the\nGRPO algorithm samples a group of outputs $o_1,...,o_G$ from an old policy $\u03c0_{\u03b8^{old}}$, observes their corre-\nsponding rewards $r_1, ..., r_G$, and uses this information, along with a given reference policy $\u03c0_{ref}(\u00b7 | q)$,\nto define the objective function for selecting a new policy. This new policy $\u03c0_\u03b8$ aims to maximise the\nfollowing objective:\n$I_{GRPO}(\u03b8) = E_{q \\sim \u03bc, \\{o_i\\}_{i=1}^G \\sim \u03c0_{\u03b8^{old}}(\u00b7 | q)} \\Bigg[\\frac{1}{G} \\sum_{i=1}^G A_i(\u03b8) - \u03b2 D_i(\u03b8) \\Bigg]$\nwith\n$A_i(\u03b8) = min \\Bigg\\{ \\frac{\u03c0_\u03b8(o_i | q)}{\u03c0_{\u03b8^{old}}(o_i | q)} A_i, clip_\\epsilon\\Big(\\frac{\u03c0_\u03b8(o_i | q)}{\u03c0_{\u03b8^{old}}(o_i | q)}\\Big) A_i \\Bigg\\},$\n$D_i(\u03b8) = -log\\frac{\u03c0_{ref}(o_i | q)}{\u03c0_\u03b8(o_i | q)} \\frac{\u03c0_\u03b8(o_i | q)}{\u03c0_{\u03b8^{old}}(o_i | q)} - 1,$\nwhere G is an integer-valued hyperparameter greater than or equal to two, and \u0454 and \u1e9e are positive-\nvalued hyperparameters. The function clip(x) clips the value x to 1 if x < 1 - \u20ac or x > 1 + e, and\nreturns x otherwise. Additionally, $A_i$ represents the advantage corresponding to the output $o_i$ within\nthe group, defined as:\n$A_i = \\frac{r_i \u2013 mean(r_1,...,r_G)}{std(r_1,...,r_G)}$\nHere, $mean(r_1,...,r_G) \\overset{def}{=} (1/G) \\sum_{i=1}^G r_i$ and $std(r_1,...,r_G) \\overset{def}{=} \\sqrt{(1/G) \\sum_{i=1}^G (r_i \u2013 mean(r_1,...,r_G))^2}$. In\nEquation (2), we define $A_i = 0/0 = 0$ in the case where $r_1 = \u00b7 \u00b7 \u00b7 = r_G$.\nThe objective in (1) consists of two terms: a reward preference model and a reference-policy divergence\npenalty. The reward preference model is designed to favour outputs that achieve a higher reward rel-\native to other outputs within the group. The reference-policy divergence penalty discourages policies\nfrom deviating excessively from the reference policy. The decomposition of the objective function into\na reward preference model and a reference-policy divergence penalty is a common approach in various\nalignment algorithms, with key differences arising in how these terms are specifically defined."}, {"title": "1.1 Related work", "content": "In this section, we review two existing alignment approaches, which we use as baselines to compare\ntheir alignment objectives with that of the GRPO algorithm.\nReinforcement Learning from Human Feedback (RLHF) The standard RLHF paradigm [Christiano\net al., 2017, Stiennon et al., 2020] consists of two main steps: learning the reward model and optimising\nthe policy using the learned reward model. In the first step, the reward model $r_\u03c6(\u00b7 | q)$ is trained on a\ndataset containing examples of human preferences in the form of pairwise comparisons of outputs for\ngiven contexts. In the second step, the objective is to find a policy that maximises the following objective\nfunction:\n$I_{RLHF}(\u03b8) = E_{q\\sim\u03bc, o\\sim\u03c0_\u03b8(:|q)} [r_\u03c6(o | q)] \u2013 \u03b2 E_{q\\sim\u03bc}[KL(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q))]$\nwhere $\u03c0_{ref}(\u00b7 | q)$ is a reference model policy and $KL(\u03c0 || \u03c0')$ is the Kullback-Leibler divergence between\ntwo distributions \u03c0 and \u03c0', $KL(\u03c0 || \u03c0') = E_{x\u223c\u03c0}[log(\u03c0(x)/\u03c0'(x))]$. The objective function in (3) is\noptimised by using PPO [Schulman et al., 2017] or similar optimisation approaches. PPO is an actor-\ncritic RL algorithm that is used in the RL fine-tuning stage of LLMs [Ouyang et al., 2022].\nThe RLHF can be seen as an approach for aggregating a reward preference and a reference-policy\npreference according to:\n$\u03c0_\u03b8(o | q) = \\frac{1}{Z_q}\u03c0_{ref}(o | q)e^{r_\u03c6(o/\u03b2)}$\nwhere $Z_q$ is a normalisation constant. This follows directly by choosing $\u03c0_\u03b8(\u00b7 | q)$ that maximises the\nRLHF objective function (3). The aggregate preference distribution in (4) follows the logarithmic opin-\nion pooling form [Genest et al., 1984]. Logarithmic opinion pooling is a method for aggregating mul-\ntiple probability distributions into a single consensus distribution, where the consensus distribution is\nproportional to some weighted geometric average of the individual distributions. Specifically, (4) de-\nfines the consensus distribution as a weighted geometric average of the reference-policy distribution"}, {"title": "1.2 Summary of our findings", "content": "Our findings can be summarised in the following points:\n\u2022 We present a framework for analysing the stationary policies of the GRPO algorithm, expressing\nthe reward preference model and the reference-policy divergence penalty in a way that reveals\ntheir fundamental role in aligning preferences. This framework clarifies the contribution of indi-\nvidual components and their relationship to previously proposed algorithms for preference aggre-\ngation.\n\u2022 We show that preference aggregation in GRPO corresponds to scaling the reference probability of\nan output, given a context, by a function that increases with the expected advantage of the output\nrelative to the expected advantage of a randomly chosen group of outputs from the aggregate\nprobability distribution. This form of preference aggregation differs from the logarithmic pooling\nused in methods such as RLHF."}, {"title": "1.3 Additional assumptions", "content": "For the optimisation problem to be well defined, we make the following assumptions. For every con-\ntext q, the domain of the distribution $\u03c0_\u03b8(\u00b7 | q)$ is assumed to be contained within the support of the\ndistribution $\u03c0_{ref}(\u00b7 | q)$. This ensures that for every output o in the domain of $\u03c0_\u03b8(\u00b7 | q)$, we have\n$\u03c0_{ref}(o | q) > 0$. Without this condition, the reference-policy divergence penalty would become infi-\nnite whenever $\u03c0_{ref}(o | q) = 0$ and $\u03c0_\u03b8(\u00b7 | q) > 0$ for some output o and context q.\nOur study focuses on characterising the stationary policies of the GRPO algorithm; therefore, we\nignore the clipping function in the GRPO objective function. A stationary policy is a collection of dis-"}, {"title": "2 The alignment objective of the GRPO algorithm", "content": "In this section, we analyse the alignment objective of the GRPO algorithm. We begin by examining the\nreward preference model and the reference-policy divergence penalty separately, before discussing the\nalignment objective as a whole."}, {"title": "2.1 The reward preference model", "content": "We consider a more general setting than in Section 1, where the reward r is allowed to be stochastic for\nany given output o and context q. Let r(o | q) denote the expected value of the reward for output o under\ncontext q. The case of deterministic rewards is a special case, where, for each output o under a context\nq, the reward takes a deterministic value r(o | q). Recall that we ignore the clipping function term in the\nobjective function, as our focus is on characterising stationary policies. Hence, we consider the reward\npreference given by:\n$R_G(o | q) \\overset{def}{=} E_{\\{o_i\\}_{i=1}^{G-1} \\sim \u03c0_{\u03b8^{old}} (\u00b7 | q)} \\Bigg[\\frac{1}{G-1} \\sum_{i=1}^{G-1} \\frac{\u03c0_\u03b8(o_i | q)}{\u03c0_{\u03b8^{old}}(o_i | q)} A_i \\Bigg].$\nThe GRPO's reward preference model can be expressed as follows. Let $P_G(o | \\{o_i\\}_{i=1}^{G-1},q)$ denote\nthe group-relative preference of output o over outputs $o_1,...,o'_{G\u22121}$ for a given context q. For any condi-\ntional distribution $\u03c0'(\u00b7 | q)$ for a given context q, let $P_G(o | \u03c0'(\u00b7 | q),q)$ be the expected group-relative\npreference of output o for a given context q, i.e.,\n$P_G(o | \u03c0' (\u00b7 | q),q) \\overset{def}{=} E_{o_1...o'_{G-1} \\sim \u03c0' (\u00b7 | q)} [P_G(o | \\{o_i\\}_{i=1}^{G-1},q)]$.\nIt can be readily observed that the GRPO's reward preference model can be expressed as:\n$R_G(o| q) = E_{o \\sim \u03c0_\u03b8(\u00b7 | q)} [P_G(o | \u03c0_{\u03b8^{old}} (\u00b7 | q),q)]$\nwhere, specifically,\n$P_G(o |\\{o_i\\}_{i=1}^{G-1},q) \\overset{def}{=} E\\Bigg[ \\frac{r_1 \u2013 mean(r_i,r_1,...,r_G)}{\\sqrt{std(r_i,r_1,...,r_G)}} \\bigg| o_1 = o,o_1 = o_1,..., o_G = o_{G-1},q \\Bigg]$\nwhere the expectation is with respect to the distributions of rewards given their corresponding outputs\nand the context. For the case of deterministic rewards, we have\n$P_G(o |\\{o_i\\}_{i=1}^{G-1},q) = \\frac{r(o | q) \u2013 mean(r(o | q), r(o_1 | q), ...,r(o'_{G-1} | q))}{std(r(o | q),r(o_1 | q),...,r(o'_{G-1} | q))}$\nIt is insightful to consider two extreme cases, one in which the group size is the smallest possible\nvalue of two outputs, and the other where the group size becomes asymptotically large."}, {"title": "2.2 The reference-policy divergence penalty", "content": "We consider the reference-policy divergence penalty in the GRPO's objective function given in Equa-\ntion (1). To this end, for an arbitrary context q, we consider:\n$D(\u03b8 | q) \\overset{def}{=} E_{\\{o_i\\}_{i=1}^{G-1} \\sim \u03c0_{\u03b8^{old}} (\u00b7 | q)} \\Bigg[ \\frac{1}{G} \\sum_{i=1}^G D_i(\u03b8) \\Bigg]$\nAccording to Shao et al. [2024], the reference-policy divergence penalty is defined as an estimator of the\nKL divergence between $\u03c0_\u03b8(\u00b7 | q)$ and $\u03c0_{ref}(\u00b7 | q)$, specifically using as inspiration an estimator discussed\nin Schulman [2020]. It can be readily observed that\n$D(\u03b8 | q) = KL_o(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q); \u03c0_{\u03b8^{old}} (\u00b7 | q))$\nwhere\n$KL_o(\u03c0 || \u03c0^*; \u03c0') \\overset{def}{=} E_{x \\sim \u03c0'} \\Bigg[ log \\frac{\u03c0^*(x)}{\u03c0(x)} \\Bigg] - E_{x \\sim \u03c0'} \\Bigg[ log \\frac{\u03c0^*(x)}{\u03c0(x)} \\Bigg] - 1$.\nIndeed, the GRPO's reference-policy divergence penalty is an unbiased estimator of the KL diver-\ngence $KL(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q))$ in the case where $\u03c0_\u03b8(\u00b7 | q) = \u03c0_{\u03b8^{old}} (\u00b7 | q)$, but not in general. More\nimportantly, for optimisation purposes, it is the gradient of the reference policy divergence that matters,\nand the two divergences have different gradients.\nThe gradient of $KL_o(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q); \u03c0_{\u03b8^{old}} (\u00b7 | q))$ with respect to $\u03c0_\u03b8(\u00b7 | q)$ is given as:\n$\\frac{\u2202}{\u2202 \u03c0_\u03b8(o | q)} KL_o(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q); \u03c0_{\u03b8^{old}} (\u00b7 | q)) = - \\frac{\u03c0_{\u03b8^{old}}(o | q)}{\u03c0_\u03b8(o | q)^2} \\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)} + \\frac{\u03c0_{\u03b8^{old}}(o | q)}{\u03c0_\u03b8(o | q)} +1.$\nFor the KL divergence, we have\n$\\frac{\u2202}{\u2202 \u03c0_\u03b8(o | q)} KL(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q)) = -log \\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)} \\frac{1}{\u03c0_\u03b8(o | q)}.$\nWe observe that the gradients In Equations (8) and (9) are different even in the case where $\u03c0_{\u03b8^{old}}(\u00b7 |$\nq) = $\u03c0_\u03b8(\u00b7 | q)$, in which case\n$\\frac{\u2202}{\u2202 \u03c0_\u03b8(o | q)} KL_o(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q); \u03c0_{\u03b8^{old}} (\u00b7 | q)) =  \\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)} +1$\nwhich is linear in the probability ratio $\u03c0_{ref}(o | q)/\u03c0_\u03b8(o | q)$, rather than logarithmic, as in the gradient\nof the KL divergence in Equation (9).\nIt is noteworthy that the gradient of the reference-policy divergence penalty, when $\u03c0_{\u03b8^{old}}(\u00b7 | q) =$\n$\u03c0_\u03b8(\u00b7 | q)$, is equivalent to the gradient of the reverse KL divergence between $\u03c0_\u03b8(\u00b7 | q)$ and $\u03c0_{ref}(\u00b7 | q)$, i.e.,\n$KL_{Rev}(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q)) = KL(\u03c0_{ref}(\u00b7 | q) || \u03c0_\u03b8(\u00b7 | q)) = E_{o \\sim \u03c0_{ref}(\u00b7 | q)} \\Bigg[ log \\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)} \\Bigg].$\nup to a non-essential additive constant. Indeed, it holds:\n$\\frac{\u2202}{\u2202 \u03c0_\u03b8(\u00b7 | q)} KL_{Rev}(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q)) = \\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)}$\nwhich is equal to the gradient in Equation (10) up to an additive constant of value 1. This additive\nconstant is non-essential for determining stationary policies."}, {"title": "2.3 The alignment objective and stationary policies", "content": "Having discussed the reward preference model and the reference-policy divergence penalty components\nof the GRPO's objective function, we now consider the objective function and its stationary policies.\nFrom our preceding discussion, we have:\n$I_{GRPO}(\u03b8) = E_{q \\sim \u03bc}[I_{GRPO}(\u03c0_\u03b8(\u00b7 | q)|q)],$"}, {"title": "3 Extensions", "content": "The GRPO's alignment objective can naturally be extended in different directions by redefining the\nreward preference model or the reference-policy divergence penalty. Here, we discuss some different\nvariants.\nUsing the direct KL divergence penalty As noted, as far as the stationary policies are concerned, the\nGRPO's reference-policy divergence penalty is essentially the reverse KL divergence between $\u03c0_\u03b8(\u00b7 | q)$\nand $\u03c0_{ref}(\u00b7 | q)$. We can easily convert the reference-policy divergence penalty to correspond to the direct\nKL divergence between the two distributions. This can be done by the standard importance sampling\ntrick for Monte Carlo estimation by redefining the penalty terms in the GRPO objective as follows:\n$D_i(\u03b8) = \\frac{\u03c0_\u03b8(o_i|q)}{\u03c0_{\u03b8^{old}}(o_i | q)} \\Bigg(\\frac{\u03c0_{ref}(o_i | q)}{\u03c0_\u03b8(o_i | q)} log \\frac{\u03c0_{\u03b8^{old}}(o_i | q)}{\u03c0_{ref}(o_i | q)} - 1\\Bigg).$\nWith this new definition, the expected reference-policy divergence penalty defined in Equation (7) corre-\nsponds to the KL divergence between $\u03c0_\u03b8(\u00b7 | q)$ and $\u03c0_{ref}(\u00b7 | q)$, i.e. $D(\u03b8 | q) = KL(\u03c0_\u03b8(\u00b7 | q) || \u03c0_{ref}(\u00b7 | q))$."}, {"title": "A Stationary policies", "content": "We consider the following optimisation problem:\nmaximise $I_{GRPO}(\u03c0_\u03b8(\u00b7 | q) | q)$\nover $\u03c0_\u03b8(\u00b7 | q)$\nsubject to $\u03c0_\u03b8 (o | q) \u2265 0,\u2200o$\n$\\sum_o \u03c0_\u03b8(o | q) = 1$.\nBy the KKT conditions, if $\u03c0_\u03b8(\u00b7 | q)$ is a local optimum, then there exist constants Yo and A such that,\nfor all o, the following conditions hold:\nstationarity: $\\frac{\u2202}{\u2202 \u03c0_\u03b8(o | q)} I_{GRPO}(\u03c0_\u03b8(\u00b7 | q) | q) \u2212 \u03bb + Yo = 0,$\nnon-negativity: $Yo \u2265 0$, and,\ncomplementary slackness: $Yo\u03c0_\u03b8(o | q) = 0$.\nTherefore, either $\u03c0_\u03b8(o | q) = 0$, or $\u03c0_\u03b8(o | q) > 0$ and\n$(PC(o | \u03c0_\u03b8(\u00b7 | q), q) \u2212 \u03b2 \\Bigg(\\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)^2} \u2212 \\frac{\u03c0_{\u03b8^{old}}(o | q)}{\u03c0_\u03b8(o | q)} + \u03c0_{\u03b8^{old}}(o / q) \u2212 \u03bb = 0.$\nUnder the condition $\u03c0_\u03b8(\u00b7 | q) = \u03c0_{\u03b8^{old}} (\u00b7 | q)$, we have\n$\\frac{\u03c0_{ref}(o | q)}{\u03c0_\u03b8(o | q)} = \\Bigg(1 \u2212 \\frac{PC(o | \u03c0_\u03b8(\u00b7 | q), q) \u2212 \u03bb}{\u03b2} \\Bigg) \u03c0_\u03b8(o | q).$"}, {"title": "B Binary questions", "content": "B.1 The GRPO's alignment objective\nGroups of size two We first consider the reward preference model part of the objective. Note that\n$E_{o \\sim \u03c0_\u03b8(\u00b7 | q)} [P_2(o | \u03c0_{\u03b8^{old}} (\u00b7 | q),q)]$\n$=\n\u03c0_\u03b8(a | q)\u03c0_{\u03b8^{old}} (b | q)E[sign(r_1 - r_2) | o_1 = a, o_2 = b, q]$\n$+\n\u03c0_\u03b8(b|q)\u03c0_{\u03b8^{old}}(a | q)E[sign(r_1 - r_2) | o_1 = b, o_2 = a, q]$\n$=\n\u03c0_\u03b8(a | q)\u03c0_{\u03b8^{old}} (b | q)E[sign(r_1 - r_2) | o_1 = a, o_2 = b, q]$\n$-\n\u03c0_\u03b8(b|q)\u03c0_{\u03b8^{old}} (a | q)E[sign(r_2 - r_1) | o_1 = b, o_2 = a, q]$\n$=\n\u03c0_\u03b8(a | q)\u03c0_{\u03b8^{old}} (b | q)E[sign(r_1 - r_2) | o_1 = a, o_2 = b, q]$\n$-\n\u03c0_\u03b8(b|q)\u03c0_{\u03b8^{old}} (a | q)E[sign(r_1 - r_2) | o_1 = a, o_2 = b, q]$\n$=\n(\u03c0_\u03b8(a|q)\u03c0_{\u03b8^{old}} (b | q) \u2013 \u03c0_\u03b8(b | q)\u03c0_{\u03b8^{old}} (a | q)) \u00d7$\n$\u00d7IE[sign(r_1 - r_2) | o_1 = a, o_2 = b, q].$\nNext, note that\n$\u03c0_\u03b8(a | q)\u03c0_{\u03b8^{old}} (b | q) \u2013 \u03c0_\u03b8(b | q)\u03c0_{\u03b8^{old}} (a | q)$\n$=\n\u03c0_\u03b8(a | q) (1 \u2013 \u03c0_{\u03b8^{old}} (a | q)) \u2013 (1 \u2013 \u03c0_\u03b8(a | q))\u03c0_{\u03b8^{old}} (a | q)$\n$=\n\u03c0_\u03b8(a | q) \u2013 \u03c0_{\u03b8^{old}} (a | q),$\nand\n$E[sign(r_1 - r_2) | o_1 = a, o_2 = b, q] = Ya,b,$\nwhere\n$Ya,b \\overset{def}{=} P(a > b | q) \u2013 P(b > a | q).$\nHence, we have\n$E_{o \\sim \u03c0_\u03b8(:|q)} [P_2(o | \u03c0_{\u03b8^{old}} (\u00b7 | q),q)] = Ya,b(\u03c0_\u03b8(a | q) \u2013 \u03c0_{\u03b8^{old}} (a | q)).$\nCombining this with the reference-policy divergence penalty, we have\n$J_{GRPO}(\u03c0_\u03b8(a | q)|q)$\n$=\nY_{a,b}(\u03c0_\u03b8(a | q) \u2212 \u03c0_{\u03b8^{old}} (a | q))$\n$\u03b2 \\Bigg(\u03c0_{\u03b8^{old}} (a | q) \\frac{\u03c0_{ref}(a | q)}{\u03c0_\u03b8(a | q)} + (1 \u2212 \u03c0_{\u03b8^{old}} (a | q)) \\frac{1 \u2212 \u03c0_{ref}(a | q)}{1 \u2212 \u03c0_\u03b8(a | q)}\\Bigg)$\n$+ \u03c0_{\u03b8^{old}} (a | q)log(\u03c0_\u03b8(a | q)) + (1 \u2212 \u03c0_{\u03b8^{old}} (a | q)) log(1 \u2013 \u03c0_\u03b8(a | q)) + const.\nB.2 Using direct KL divergence penalty\nGroups of size two We consider the GRPO reward preference model with the reference-policy diver-\ngence penalty according to the KL divergence between $\u03c0_\u03b8(\u00b7 | q)$ and $\u03c0_{ref}(\u00b7 | q)$. The reward preference\npart of the objective is as given in Equation (16). The objective function is given as follows:\n$J(\u03c0_\u03b8(a | q) | q) = Y_{a,b}\u03c0_\u03b8(a | q) \u2013 \u03b2 KL(\u03c0_\u03b8(a | q) || \u03c0_{ref}(a | q))) + const$"}]}