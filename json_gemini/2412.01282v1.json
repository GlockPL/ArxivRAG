{"title": "Align-KD: Distilling Cross-Modal Alignment Knowledge for\nMobile Vision-Language Model", "authors": ["Qianhan Feng", "Wenshuo Li", "Tong Lin", "Xinghao Chen"], "abstract": "Vision-Language Models (VLMs) bring powerful under-\nstanding and reasoning capabilities to multimodal tasks.\nMeanwhile, the great need for capable aritificial intelli-\ngence on mobile devices also arises, such as the AI assis-\ntant software. Some efforts try to migrate VLMs to edge\ndevices to expand their application scope. Simplifying the\nmodel structure is a common method, but as the model\nshrinks, the trade-off between performance and size be-\ncomes more and more difficult. Knowledge distillation (KD)\ncan help models improve comprehensive capabilities with-\nout increasing size or data volume. However, most of the\nexisting large model distillation techniques only consider\napplications on single-modal LLMs, or only use teachers\nto create new data environments for students. None of these\nmethods take into account the distillation of the most impor-\ntant cross-modal alignment knowledge in VLMs. We pro-\npose a method called Align-KD to guide the student model\nto learn the cross-modal matching that occurs at the shal-\nlow layer. The teacher also helps student learn the pro-\njection of vision token into text embedding space based on\nthe focus of text. Under the guidance of Align-KD, the 1.7B\nMobileVLM V2 model can learn rich knowledge from the 7B\nteacher model with light design of training loss, and achieve\nan average score improvement of 2.0 across 6 benchmarks\nunder two training subsets respectively. Code is available\nat: https://github.com/fqhank/Align-KD.", "sections": [{"title": "1. Introduction", "content": "Vision Language Model (VLM) is an important Multimodal\ntechnology, which build a bridge between vision and text\ndata, and facilitate many real world tasks and applications\nBased on the success of Large Language Models\n, efforts have been done to integrate vi-\nsion modal features with LLMs to extend models' capabil-\nity and their application potential and build up new Vision-\nLanguage Models (VLMs) [11, 21, 24, 27, 52]. However,\nnew issues rise up: as the input features become more com-\nplex, the structures of VLMs also become deeper and heav--\nier, since they have to digest information from different\nmodalities and face even more various scenes [2, 4]. The\ngrowing size and complexity of VLMs makes them diffi-\ncult to be accessed outside the server or high-speed Internet,\nwhich limit the development of these cutting-edge artificial\nintelligence under different scenarios, especially their de-\nployment in off-line devices like mobile phones and robots,\nor some confidential application devices.\nGrowing attentions have been focused on compressing\nVLMs while maintaining their remarkable capability as bet-\nter as could. MobileVLM family models [7, 9] are the first\nworks to scale down VLMs to be able to run on mobile de-\nvices. Both MobileVLM V1 and MobileVLM V2 model"}, {"title": "2. Related Works", "content": "In recent years, Large Language Models (LLMs) like GPT-3\n, OPT [49] and LLaMA [41] significantly break through\nthe borderline of deep learning and its applications. Chat-\nGPT [34] set off a new wave and inspired follow-up work\nsuch as Vicuna [6]. Besides, some works try to intro-\nduce multimodal knowledge into the large model [2, 4, 52].\nLLaVA [27] feed visual tokens into LLM and build up a\ncomprehensive reasoning between visual and text contents,\nand many other works [11, 21, 24, 46] also approach to bal-\nance between vision and text understanding. However, the\ngrowing size of LLMs leads to a high demand on comput-\ning resources, which limits their applications. TinyLLaMA\n[48] and MobileLLaMA [8] scale down the architectures\nand maintain relatively good performance. Meanwhile in\nvision-language model field, MobileVLM family [7, 9] is\nthe first open source work to facilitate the Vision-Language\nModel on mobile devices. Except for the development of\ntraining strategy and special architectures for large model,\nmodel compression techniques including quantization and\npruning [14, 15] also thrive and provide solutions to relief\nburden of the computation resources. Equipped with these\nmethods, LLMs are able to inference faster and lighter with\nlittle drop in accuracy."}, {"title": "2.2. Large Language Model Distillation", "content": "While former techniques are trying to do the subtraction,\nknowledge distillation (KD) [18] techniques are trying to do\nadding. In KD, weaker student model tries to learn from a\nstronger teacher model from different aspects, like the out-\nput or hidden representations. MiniLLM [17] studies the\nKullback-Leibler divergence (KLD) loss on the output dis-"}, {"title": "2.3. Distillation for Vision-Language Model", "content": "Most distillation methods for Vision-Language Model\n(VLM) are designed before Large Model Era. Considering\nthat traditional VLMs rely on vision proposals, Fang et al.\n[12] propose to align the input proposals between teacher\nand student, and enable following transformer blocks to\nalign their attention distributions. To compress VLM, Wang\net al. [45] combine pruning with distillation, conduct-\ning easy output logits imitation and distillation on atten-\ntion and hidden states. Although these works provide a\nthinking of VLM distillation, they are restrained within the\nfield outside Vision-Language Model (VLM), which usu-\nally consists of more transformer layers and more com-\nplex alignment between vision and language modalities. In\nVLM-KD [50], researchers use VLM like LLaVA-NeXT\n[29] to generate text prompts and using contrastive learn-\ning to promote long-tail recognition ability of vision mod-\nels. LLaVA-MoD [37] minimizes the Kullback-Leibler di-"}, {"title": "3. Align-KD", "content": "In Vision-Language Models (VLMs), vision and text em-\nbeddings comprise the input of the large model. However,\nit is obvious that the embedding mechanisms are different\nfor the two modalities, which means the embeddings have\nto go through cross-modal alignment in the feature space.\nThe cross-modal alignment ability is crucial for VLMs, but\nprevious works mainly focus on single modal LLMs distil-\nlation and neglect the importance of teaching student about\nthe alignment knowledge. Here we first explore the cross-\nmodal alignment in VLMs, and then propose our Align-KD\nmethod step by step based on MobileVLM family."}, {"title": "3.1. Where Does the Alignment Happen?", "content": "Most VLMs like MobileVLM [7, 9] design special vision\nprojectors to project the vision embeddings, but this opera-\ntion mainly align the dimension of embedded tokens. The\nalignment of vision and text embeddings into the same high\ndimension space is almost a black box system.\nSun et al. [40] try to figure out the internal work-\ning mechanism of Transformer layers in LLMs. The re-\nsearchers perform both skip and switch operations on every\nTransformer layer in LLaMA 27B, 13B and 70B [42] mod-\nels, and find out that the change in the first and last layer\nbrings the largest drop in performance, while middle lay-\ners only give slight fluctuation. What's more, the parameter"}, {"title": "3.2. First Layer Text-Query-Vision Attention Only", "content": "The first layer of LLM in VLMs is the important place\nwhere the cross-modal alignment happens, as discussed\nabove. Almost all of recent LLMs are built based upon\nthe architecture of Transformer [43], an efficient parallel\nattention structure. While some works have been done to\nimprove the Transformer block [30, 36, 39], the basic atten-\ntion mechanism remains as: project the input features into\nquery Q, key K and value V, and then use them to gen-\nerate Attention values to help self-adaptive fusion among\ndifferent feature tokens.\nAttention values imply tokens' unbalanced focusing\ndegree on others and determine how the input features are\ngoing to be projected. This nature of attention mechanism\nmakes it the perfect information carrier about the cross-\nmodal alignment in VLMs. The attention matrix of VLMS\nare always in the similar mode as the right of Figure 3,\nwhere the input embeddings are the concatenated vision and\ntext tokens. Since VLMs mainly use decoder transformer\nlayers, the attention matrix is a lower triangular matrix and\nhalf masked. The lower part of the matrix consists of three"}, {"title": "3.3. Vision Enhancement Based on Text's Focusing", "content": "The vision projector that generates the input vision tokens\nalso takes up the responsibility to do rudimentary cross-modal alignment. For example, the LDP in MobileVLM\nmodel downsample the embeddings while extracting both\ndetail and semantic features, and project the embeddings\ninto less tokens with same dimension as text embeddings.\nThis rough alignment lacks the perception of text modal in-\nformation, only receiving backward gradients from down-\nstream Transformer layers' cross-attention to get in touch\nwith another modality.\nTo mitigate this shortage, we propose to leverage the\ncross-modal attention to help the projector get more expo-\nsure to cross-modal information. Although the vision to-\nkens are already refined by the projector, the text's attention\non different vision tokens is still rather sparse because of the\ndirectional indication of text prompts. The A1,t-v not only\ncontains information about cross-modal aligning projection,\nbut also reveals which vision tokens the text prompts pay\nmost attention to. Different prompts may focus on different\nembeddings, but some of the tokens are significantly left\nbehind. Instead of inhibiting the learning of temporarily\nunpopular vision tokens, we propose to enhance most pop-\nular tokens at current based on teacher model's A1,t-v, thus\npreventing hurting the learning of others. Having the text-\nquery-vision attention from the first layer of teacher model,\nwe add-up the attention value A\u2081,t-v along the text dimen-\nsion to get the attention score Scoren of vision token n,\n$Scoren = \\sum_{M} A1,t-v,(n,m), M \\in M,$\nwhere EmbTdxx and EmbSIdxx are teacher's and student's\nvision token embeddings within the range of Idxk, and Pv\nis an 1 \u00d7 1 convolution projector.\nExcept for the knowledge injection on current popular\ntokens, the rest should not be overlooked. Low-ranked at-"}, {"title": "3.4. Overall Knowledge Distillation Strategy", "content": "Except for alignment distillation in the front of VLM, we\nfollow MiniLLM to add reverse Kullback-Leibler diver-\ngence (RKLD) loss, which uses the predicted distribution\nof student model as target, between the outputs of student\nand teacher. Since the text prompts and input images are\nvaried, RKLD loss is more suitable than forward Kullback-Leibler divergence (FKLD) for VLM to learn about the\nmean-seeking instead of mode-seeking, preventing from\noverfitting on specific scene. Extract the output prediction\ndistribution \u0440\u0442, ps from teacher and student, the reverse\nKullback-Leibler divergence loss can be formulated as\n$LRKLD = \\sum_{S} PS log \\frac{PS}{PT}.$\nThe alignment loss and RKLD loss work together with\nthe original supervised loss LSup, and our overall Align-KD\nloss on student model can be formulated as:\n$L = LSup + LA1.t-v + Lv + LRKLD.$"}, {"title": "4. Experiments", "content": "Align-KD follows MobileVLM V2 to train on various\ndatasets. During the pretraining stage, ShareGPT4V-PT\n[4] is used to give the student a brief knowledge of vi-\nsion and text. It is a caption dataset and comprises 1.2\nmillion image-text pairs. In multi-task training stage,\nmore data from different tasks like conversation and VQA\nare provided: COCO[5], SBU[35], Visual Dialog[10],\nShareGPT4V[4], SQA[33], IConQA[32], TextVQA[38],\nVSR[26], VIGC[44]. Note that SBU is a re-collected\ndataset and is updated from time to time, therefore some of\nthe original data might have been removed. In this case, we\nwashed the data list in the dataset, which is different from\nthe original MobileVLM training. To evaluate the effec-\ntiveness of our Align-KD method, we test the performance\non different benchmarks, including GQA[20], SQA[33],\nTextVQA[38], MME[16], MMBench[31] and POPE[22]."}, {"title": "4.3. Effectiveness of Align-KD", "content": "After formulating our Long and Short subdatasets, we use\nthem to train MobileVLM V2 1.7B model with proposed\nAlign-KD strategy. Note that we use fully-trained and\nopen-sourced MobileVLM V2 7B model provided by Mo-\nbileVLM work as our knowledge distillation teacher. The\nresults across 6 different benchmarks and two subsets are\nshown in Table 1. Trained with Long set, MobileVLM\nV2 1.7B model achieves an average score of 63.7. When\ntrained with Align-KD policy, the student model witnesses\na universal promotion across all benchmarks, achieving and\naverage score of 65.1. To be more specific, Align-KD helps\nimproving MobileVLM V2 1.7B model to obtain an im-"}, {"title": "4.4. Ablation Study", "content": "We take a step forward to conduct ablation studies to testify\nthe effectiveness of each method in Align-KD. We run all\nablation experiments on Short subdataset for fairness, and\nthe results are shown in Table 3. The reverse Kullback-Leibler divergence (RKLD) loss helps student learn to\nmimic the outputs of stronger teacher model, but brings\nsomehow biased improvement. On the contrary, applying\nfirst layer A1,t-v only loss provides notable and more bal-anced promotion to an average of 63.6. The combination of\ndistillation on focused and all vision tokens further increase\nthe performance by 0.8 on average.\nDuring cross-modal alignment learning, we apply the\nknowledge distillation only on the text-query-vision part of\nthe first layer's attention. We further testify the rationality\nof this design by comparing with other methods, and the re-"}, {"title": "4.5. Discussions and Limitations", "content": "Our Align-KD strategy brings benefit to MobileVLM V2\n1.7B model under both long and short prompt limitations.\nOur Align-KD is designed with relatively light design,\nwhich enable possible expansion to resource-limited sce-\nnarios. Here we provide the working expense comparison\nof Align-KD at Table 5, including the total training time\nand maximum memory occupied during training. The ex-"}, {"title": "5. Conclusion", "content": "Noting the neglect of multimodal alignment knowledge in\npast VLM distillation works, we propose a knowledge dis-\ntillation method for MobileVLM V2 model, namely Align-\nKD. Based on the conjecture that the alignment mainly hap-\npens at the front layer of LLM in VLMs, Align-KD pro-\nposes to conduct knowledge distillation only on the text-\nquery-vision part of the first attention. In addition, the\nvision tokens are unbalancedly enhanced according to the\ntext tokens' focusing. Using MobileVLM V2 7B model as\nteacher, Align-KD enables universal improvements across\n6 benchmarks under both regular training setting and simu-\nlated resource-limited setting."}]}