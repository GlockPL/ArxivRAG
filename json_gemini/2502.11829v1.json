{"title": "CODE-VISION: Evaluating Multimodal LLMs Logic Understanding and\nCode Generation Capabilities", "authors": ["Hanbin Wang", "Xiaoxuan Zhou", "Zhipeng Xu", "Keyuan Cheng", "Yuxin Zuo", "Kai Tian", "Jingwei Song", "Junting Lu", "Wenhui Hu", "Xueyang Liu"], "abstract": "This paper introduces CODE-VISION, a bench-\nmark designed to evaluate the logical un-\nderstanding and code generation capabili-\nties of Multimodal Large Language Models\n(MLLMs). It challenges MLLMs to generate a\ncorrect program that fulfills specific functional-\nity requirements based on a given flowchart,\nwhich visually represents the desired algo-\nrithm or process. CODE-VISION comprises\nthree subsets-HumanEval-V, Algorithm, and\nMATH, which evaluate MLLMs' coding abil-\nities across basic programming, algorithmic,\nand mathematical problem-solving domains.\nOur experiments evaluate 12 MLLMs on\nCODE-VISION. Experimental results demon-\nstrate that there is a large performance differ-\nence between proprietary and open-source mod-\nels. On Hard problems, GPT-40 can achieve\n79.3% pass@1, but the best open-source model\nonly achieves 15%. Further experiments re-\nveal that CODE-VISION can pose unique chal-\nlenges compared to other multimodal reason-\ning benchmarks MMCode and MathVista. We\nalso explore the reason for the poor perfor-\nmance of the open-source models. All data and\ncodes are available at https://github.com/\nwanghanbinpanda/CodeVision.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable progress\nin Multimodal Large Language Models (MLLMs),\nwhich can process and generate information across\ndifferent modalities such as text, images, and\ncode (OpenAI, 2024; Anthropic, 2024a,b; AI, 2024;\nTeam et al., 2024; Yao et al., 2024a). These models\nsuch as GPT-40 (OpenAI, 2024), Claude-3 (An-\nthropic, 2024a), Gemini (Team et al., 2024), and\nLlama-3.2-Vision (AI, 2024) have demonstrated\nimpressive capabilities in various tasks, includ-\ning visual question answering (Singh et al., 2019;\nGoyal et al., 2017; Marino et al., 2019; Li et al.,\n2021), mathematical reasoning (Lu et al., 2024b;\nWang et al., 2024b), and code generation (Li et al.,\n2024c; Shi et al., 2024). The ability to understand\nand reason about visual information while generat-\ning accurate code is particularly crucial for advanc-\ning artificial intelligence systems.\nHowever, existing benchmarks for evaluating\nMLLMs' logic understanding and code genera-\ntion capabilities have significant limitations. While\nbenchmarks like MMCode (Li et al., 2024c) have\ncontributed valuable insights, they often treat visual\ninformation as supplementary rather than essential.\nFor instance, as shown in Figure 1, in MMCode,\nmany programming problems can be solved based\nsolely on text descriptions without requiring vi-\nsual understanding. This limitation makes it dif-\nficult to assess whether MLLMs truly utilize and\ncomprehend visual information in their coding pro-"}, {"title": "2 Related Work", "sections": [{"title": "2.1 Multimodal LLMS", "content": "In recent years, Multimodal Large Language Mod-\nels (MLLMs) have gained significant attention\ndue to their ability to process and generate in-\nformation across various modalities, such as text,\nimages, and audio (Li et al., 2024a; Liu et al.,\n2024; Zhu et al., 2023; Li et al., 2024b; Dai et al.,\n2023; Ye et al., 2024). Some proprietary models\nsuch as GPT-40 (OpenAI, 2024), Claude-3 (An-\nthropic, 2024a), and Gemini (Team et al., 2024)\nshow superior performance, especially on visually\ncomplex reasoning tasks such as MathVista (Lu\net al., 2024b). Open-source MLLMs have made\nstrides as well, notable models include Llama-\n3.2-Vision (AI, 2024), Phi-3-Vision (Abdin et al.,\n2024a), MiniCPM V2.6 (Yao et al., 2024a), Qwen-\nVL (Bai et al., 2023), Deepseek-VL (Lu et al.,\n2024a). The reasoning ability of MLLMs has re-\ncently become a highly regarded research focus\nbecause they can extend beyond language reason-\ning to encompass multiple modalities (such as im-\nages), achieving more comprehensive and complex\nreasoning. Previous research has evaluated the rea-\nsoning ability of MLLMs through tasks like Vi-\nsual Question Answering (VQA) (Mobasher et al.,\n2022; Gurari et al., 2018) and mathematical reason-\ning (Lu et al., 2024b). In this paper, we evaluate the\nlogical understanding and reasoning capabilities of\nMLLMs through code generation."}, {"title": "2.2 Multimodal Reasoning Benchmarks", "content": "The reasoning capabilities of Multimodal LLMs\nare important for the development of Artificial\nGeneral Intelligence (AGI) (Morris et al., 2023).\nTo advance the reasoning capabilities of MLLMs,\nthe research community has constructed several\nmulti-modal reasoning benchmarks (Li and Lu,\n2024). MathVista (Lu et al., 2024b) and Math-\nVision (Wang et al., 2024b) to evaluate the math-\nematical reasoning abilities of MLLMs within vi-\nsual contexts. Additionally, inspired by the effec-"}]}, {"title": "3 CODE-VISION", "content": "In this section, we introduce the benchmark CODE-\nVISION, which evaluates the logical understanding\nand code generation abilities of Multimodal Large\nLanguage Models (MLLMs). We first describe\nhow CODE-VISION tests MLLMs (Sec. 3.1). Then\nwe detail the process of constructing the CODE-\nVISION benchmark (Sec. 3.2) and show the statis-"}, {"title": "3.1 Task Definition", "content": "In CODE-VISION, MLLMs require generating a\ncorrect program P that fulfills specific function-\nality requirements based on a given flowchart F,\nwhich visually represents the desired algorithm or\nprocess.\nGenerating code involves translating visual or\ndescriptive elements from the image into logical\nand structured code that meets the required spec-\nifications. After generating the code P, we eval-\nuate its correctness by using a set of test cases\nX = {(x1,y1),...,(Xn, Yn)}. Specifically, we\nprovide input xi to the generated code P and ob-\ntain the execution result P(xi). If there exists any\ntest case (xi, Yi) \u2208 X that satisfies P(xj) \u2260 yj, it\nindicates that the generated code P does not fulfill\nthe required functionality. If the MLLM generates\na program P that correctly implements the speci-\nfications derived from the flowchart F for all test\ncases (xi, Yi) \u2208 X, P(xi) = yi, it demonstrates\nthe MLLM's capability to accurately interpret and\nconvert visual algorithmic instructions into exe-\ncutable code, ensuring reliability and correctness\nin its outputs. This successful translation confirms\nthe model's understanding of both the structure and\nlogic inherent in the flowchart, marking an impor-\ntant step towards advanced reasoning abilities."}, {"title": "3.2 Data Construction", "content": "In this subsection, we outline the methodology be-\nhind the construction of the CODE-VISION dataset.\nAs shown in Figure 2, it includes Data Collection,\nFlowchart Construction, and Test Cases Genera-\ntion."}, {"title": "3.3 Data Statistics", "content": "In this subsection, we show the statistics of CODE-\nVISION.\nAs shown in Table 1, the CODE-VISION bench-\nmark consists of three subsets: HumanEval-V, Al-\ngorithm, and MATH, covering basic programming,\nalgorithmic, and mathematical problem-solving"}, {"title": "4 Experimental Methodology", "content": "In this section, we describe the evaluated models,\nevaluation metrics, and implementation details of\nour experiments.\nModels. The models we use can be distin-\nguished as proprietary and open-source models.\nFor proprietary models, we include GPT-\n40 (OpenAI, 2024), the Claude family (Claude\n3.5 Sonnet (Anthropic, 2024b), Claude 3 Son-\nnet (Anthropic, 2024a), Claude 3 Haiku (An-\nthropic, 2024a)) and the Gemini family (Gemini\n1.5 Pro (Team et al., 2024), Gemini 1.5 Flash (Team\net al., 2024)).\nFor open-source models, we include the\nLlama-3.2-Vision family (Llama-3.2-11B-\nVision-Instruct (AI, 2024), Llama-3.2-90B-\nVision-Instruct (AI, 2024)), the Phi-3 family\n(Phi-3.5-vision-instruct (Abdin et al., 2024b),\nPhi-3-vision-128k-instruct (Abdin et al., 2024b)),\nMiniCPM-V 2.6 (Yao et al., 2024b) and\nQwen-VL-Plus (Bai et al., 2023).\n,Evaluation Metrics. We follow previous\nwork (Chen et al., 2021; Li et al., 2024c; Wang\net al., 2024a; Yang et al., 2024; Luo et al., 2023)\nand we use Pass@k (Chen et al., 2021) to evalu-"}, {"title": "5 Evaluation Results", "content": "In this section, we first benchmark Multimodal\nLLMs on CODE-VISION. Subsequently, we com-\npare the advantages of CODE-VISION over MM-\nCode and how it can evaluate the reasoning capabil-\nities of MLLMs from different perspectives com-\npared to MathVista. Additionally, we show that\nMLLMs struggle to understand visually complex\nlogic for code generation and analyze the reasons\nfor the poor performance of open-source MLLMs\non CODE-VISION. Finally, case studies are pre-"}, {"title": "5.1 Overall Performance", "content": "Table 2 shows the performance of the MLLMs on\nCODE-VISION dataset.\nOverall, the proprietary models significantly out-\nperform the open-source models. GPT-40 leads the\nproprietary models with an average score of 91.3,\nexcelling in both the Algorithm and MATH cate-\ngories. Following GPT-40, Claude 3.5 Sonnet and\nGemini 1.5 Pro also show robust results. In con-\ntrast, the open-source models lag behind in terms of\noverall performance, with Llama-3.2-90B-Vision-\nIns scoring the highest among them at 36.9 on aver-\nage. The gap between proprietary and open-source\nmodels is especially evident in the Algorithm hard\nand MATH hard categories, where proprietary mod-\nels, particularly GPT-40 and Claude 3.5 Sonnet,\nmaintain relatively high scores compared to their\nopen-source counterparts. This disparity suggests\nthat while open-source models may handle simpler\ntasks effectively, they struggle with more complex\nproblem-solving and reasoning challenges.\nAdditionally, we observe that the MLLMs score\nthe lowest on the Algorithm subset on average,\nand all open-source models fail in all questions\nof Algorithm Hard. This suggests that the prob-\nlems in the Algorithm subset are more challenging\nfor the MLLMs, which demonstrates that open-\nsource MLLMs still have a significant gap com-\npared to proprietary models in more complex rea-\nsoning tasks."}, {"title": "5.2 Comparision with Other Reasoning\nBenchmarks MMCode and Math Visita", "content": "In this subsection, we show the advantages that\nCODE-VISION has over the other multimodal rea-\nsoning benchmarks MMCode and MathVisita.\nCompare with MMCode. MMCode is a multi-"}, {"title": "5.3 MLLMs Struggle to Understand Visually\nComplex Logic for Code Generation", "content": "In this subsection, we compare the performance of\nthe MLLMs when flowchart or mermaid are used\nas input to demonstrate that MLLMs struggle to un-\nderstand complex visual logic for code generation.\nAs shown in Table 4, when using mermaid as"}, {"title": "5.4 Error Analysis and Case Studies", "content": "Figure 5 shows the proportion of error types in\nthe code generated by proprietary and open-source\nmodels across the three subsets of the CODE-\nVISION.\nFrom the pie charts, we can observe that pro-\nprietary models have a significantly higher pro-\nportion of AssertionError compared to open-\nsource models. In contrast, open-source models\nhave a higher proportion of errors on TypeError,\nIndentationError, SyntaxError, ValueError,\nand NameError, with the highest proportion oc-\ncurring in SyntaxError. This indicates that pro-\nprietary models adhere to basic syntax rules more\neffectively and are able to comprehend the logic\nwithin flowcharts. However, open-source models\nlack this refinement, their generated code has over\na 10% likelihood of failing to compile, indicating"}, {"title": "6 Conclusion", "content": "We present CODE-VISION, a novel benchmark\nfor evaluating the logical understanding and code\ngeneration capabilities of Multimodal Large Lan-\nguage Models. CODE-VISION's unique visual-\ncentric design using flowcharts as primary inputs\nprovides a more rigorous test of models' multi-\nmodal reasoning abilities compared to existing\nbenchmarks. Our extensive experiments reveal sig-\nnificant performance gaps between proprietary and\nopen-source models. Moreover, we show that the\nCODE-VISION can pose unique challenges com-\npared to other multimodal reasoning benchmarks\nand further demonstrate that open-source models\nstruggle to understand visually complex logic. In\nthe future, we will build more and harder problems\nthat contain more complex logic and flowcharts to\nchallenge proprietary models."}, {"title": "Limitations", "content": "The dataset size of CODE-VISION is relatively lim-\nited, especially in the Hard category of the Algo-\nrithm and MATH subsets, which may limit the\nevaluation of the model on extreme cases or rare\nproblems. The evaluation focuses on the correct-\nness of the code and does not adequately evaluate\nthe quality of the generated code, such as the read-\nability, efficiency, and style of the code."}, {"title": "Ethics Statement", "content": "Throughout the entire research process and in pre-\nsenting the findings in this paper, we have main-\ntained strict adherence to ethical standards. Our\ndataset is established based on commonly used\ndatasets and authoritative platforms, and the rele-\nvant data and code have undergone stringent ethical\nreview."}], "equations": ["Pass@k := \\mathbb{P} \\left(\\text { correct solution } \\in \\text { top } k \\text { solutions } \\mid \\text { problem } \\right) = \\frac{1}{\\left|\\text { Problems }\\right|} \\sum_{\\text { problem } \\in \\text { Problems }} \\left(1 - \\frac{\\left(\\begin{array}{c}n - c \\\\ k\\end{array}\\right)}{\\left(\\begin{array}{l}n \\\\ k\\end{array}\\right)}\\right)"]}