{"title": "Predictive Modeling of Homeless Service Assignment: A Representation Learning Approach", "authors": ["Khandker Sadia Rahman", "Charalampos Chelmis"], "abstract": "In recent years, there has been growing interest in leveraging machine learning for homeless service assignment. However, the categorical nature of administrative data recorded for homeless individuals hinders the development of accurate machine learning methods for this task. This work asserts that deriving latent representations of such features, while at the same time leveraging underlying relationships between instances is crucial in algorithmically enhancing the existing assignment decision-making process. Our proposed approach learns temporal and functional relationships between services from historical data, as well as unobserved but relevant relationships between individuals to generate features that significantly improve the prediction of the next service assignment compared to the state-of-the-art.", "sections": [{"title": "1 Introduction", "content": "Machine learning has gained significant attention for its ability to solve complex real-world problems across various domains including criminal justice, e-commerce, healthcare, banking, finance, and social service (Sarker 2021). At the same time, a constant rise in the rate of homelessness has been observed (Dej, Gaetz, and Schwan 2020). In the United States alone, homelessness has risen 12.5% between 2022 and 2023, with a particularly sharp increase of 29.5% in chronic patterns of homelessness (Henry et al. 2023). Chronic pattern of homelessness occurs when individuals experience repeated homelessness over an extended period (e.g., at least two years), or endure continuous homelessness for at least 12 months (Fleury et al. 2021).\nTo date, various machine learning approaches have been proposed for predicting whether individuals will reenter the homeless system (Gao, Das, and Fowler 2017; Hong et al. 2018; Kube, Das, and Fowler 2019) as well as assessing the risk of chronic homelessness of individuals (Van-Berlo et al. 2021; Messier, John, and Malik 2021). However, only limited work has addressed the more challenging problem of homeless service assignment (Rahman and Chelmis\n2022; Rahman, Zois, and Chelmis 2023; Pokharel, Das, and Fowler 2024). Specifically, (Pokharel, Das, and Fowler 2024) investigate simple models (e.g., Decision Trees with Short Explainable Rules) to recommend the exact service individuals can benefit from, while (Rahman and Chelmis 2022; Rahman, Zois, and Chelmis 2023) explicitly model the trajectories of individuals within the homeless system. The main limitation of these methods lies in their inability to capture the informative relationships between different feature values (i.e., within-feature interactions) and the complex between-feature interactions.\nAdministrative data collected by the homeless service providers comprises of (i) service assignments and duration of stay, (ii) housing outcome after service assignments, (iii) demographics (e.g., race, gender, ethnicity), educational history, disabling condition, and (iv) time-variant information (e.g., monthly income, health). Most of these features are categorical, and existing methods use one-hot encoding to handle them, mainly for simplicity. Unfortunately, such treatment misses the rich relationships within the features, while exponentially increasing the dimensionality of the data with growing number of categories (Rodr\u00edguez et al. 2018). Instead, we propose a predictive model that learns latent representations for features and services, which are then utilized to capture feature interactions and relationships between individuals. We show that the learned representations significantly improve the accuracy of homeless service assignment compared to the state-of-the-art.\nThis brings us to the core contribution of our work. Similar to (Rahman and Chelmis 2022; Rahman, Zois, and Chelmis 2023) we study the interactions between services found in individuals' history. However, those studies over-simplify the task of predicting the likelihood of reaching the next service based solely on transitional probabilities between past services, while we assert that many other interactions exist that provide valuable context which can in turn enhance predictive performance. Specifically, subsequent services are often assigned to individuals to serve a particular purpose at a specific time. Understanding such functional and temporal relationship between services can provide insights into why a particular service is assigned at a specific time. Our work extends beyond previous studies by incorporating temporal and functional interaction between services. Furthermore, our model explores the relationships"}, {"title": "between individuals. Generally, individuals with similar features tend to receive similar services, and leveraging such information can significantly improve the robustness of predictive models. While (Rahman and Chelmis 2022) incorporates finding the \u201cmost similar individual\u201d in their prediction problem, it does so as an information retrieval task; instead our approach clusters individuals and utilizes the collective patterns within clusters to provide better insights and more accurate predictions.\nIn summary, given the history of services assigned to individuals and their current socio-economic features, we propose an objective function with three distinct components that captures (i) temporal and (ii) functional interactions between services, and (iii) interactions between individuals (instances). Through optimization, we obtain latent representations for services and features, and identify the clusters to which each individual belongs. Next, features derived from these representations are input into a feed-forward neural network for predicting the next service assignment. We state our main contributions as follows:\n1. We propose a representation-based predictive model for homeless service assignment that effectively learns the latent representation of services and features.\n2. We design a novel optimization function that incorporates temporal, functional, and instance interactions to enhance the learning of representations.\n3. We utilize the learnt representations to derive features that significantly improve the performance of service assignment predictions.\nThe rest of the paper is organized as follows. Section 2 summarizes the related work. Section 3 delineates the problem setting. Section 4 introduces the proposed approach. Section 5 describes the data, metrics, and baselines. Section 6 provides detailed discussion of the experimental results. Section 7 concludes with a discussion of the limitations of our study and potential directions for future work.", "content": ""}, {"title": "2 Related Work", "content": "Reentry Prediction and Risk of Chronic Homelessness\nA considerable body of prior work has focused on predicting reentry into homelessness and assessing the risk of homelessness or chronic homelessness. (Gao, Das, and Fowler 2017; Hong et al. 2018) investigated the application of machine learning models such as logistic regression and random forest to predict whether individuals will reenter homelessness. On the other hand, (VanBerlo et al. 2021; Messier, John, and Malik 2021) explored neural networks to assess the risk of chronic homelessness. Additionally, (Vajiac et al. 2024) investigated various machine learning models to evaluate the risk of eviction-caused homelessness and accurately identify individuals in need of assistance. All of these works are formulated as binary classification problems. Our work, on the other hand, focuses on developing a predictive model for service assignment task, which is a more challenging multi-class classification problem.\nHomeless service assignment\nAnother body of work focuses on predicting the service assignment at various conditions, such as (Toros and Flaming 2018; Shinn et al. 2013; Greer et al. 2016) develop models that prioritize housing for those at high risk of homelessness. In contrast, our work centers on individuals who have already experienced chronic patterns of homelessness. On the other hand, (Chelmis et al. 2021) explores machine learning models to predict the exact support corresponding to the assigned service. Similary, (Pokharel, Das, and Fowler 2024) employs decision trees with simple explainable rules (SER-DT) algorithm, which divides the problem into multiple binary classification tasks using a one-versus-all prediction approach for service assignment upon entry. While these studies use one-hot encoding for categorical features, in contrast, our approach leverages representation learning to uncover the rich relationships within these features, moving beyond one-hot encoding. Our research aligns with (Rahman and Chelmis 2022) and (Rahman, Zois, and Chelmis 2023) which address the multi-class service assignment task. Specifically, (Rahman and Chelmis 2022) infers a homelessness network and (Rahman, Zois, and Chelmis 2023) utilizes a Bayesian network to predict the next service assignment. In contrast, our work integrates both history of individuals within the homeless system and socio-economic features to predict the next service assignment. A key distinction is that while previous studies explore simple transitional relationships between services, our approach leverages more complex (temporal and functional) relationships between services, and relevant interactions between individuals to predict the next service assignment."}, {"title": "3 Problem Setting", "content": "We denote the set of individuals with chronic patterns of homelessness as $U = \\{u_1,...,u_{|U|}\\}$. The set of homeless services, such as permanent housing, rapid rehousing, day shelter, is denoted by $A = \\{a_1, a_2, . . ., a_{|A|} \\}$. For each $u_i \\in U$, let $T_{u_i} = \\{(a_{t_1}, t_1),..., (a_{t_n}, t_n)\\}$ represent the history of services assigned to individual $u_i$, where $a_t$ signifies assignment to a service at time $t_i$. Next, we denote $X \\in R^{|U|\\times|F|}$ as the feature matrix which comprises the feature set $F \\in R^{d}$ of individuals $u_i \\in U$ at time $t_{N+1}$.\nGiven matrix $X$ and the history of prior service assignments $T_{u_i} = \\{(a_{t_1}, t_1),..., (a_{t_v,t_N})\\}$ for individuals $u_i \\in U$, the goal of this paper is to accurately predict the next service assignment $a_{t_{N+1}} \\in Y$ for $\\forall u_i$."}, {"title": "4 Proposed Approach", "content": "In this section, we present a comprehensive overview of REPLETE, a REPresentation Learning-based service assignmenT modEl, which consists of two main components: (i) representation learning framework and (ii) prediction model. The representation learning framework is designed to learn the latent representations of services. The prediction model leverages the learned representations to enhance the homeless service assignment decision-making process."}, {"title": "4.1 Representation Learning Framework", "content": "To effectively learn the representations of services, we design an optimization function that captures three different contexts. The first context focuses on the temporal interactions between services, where assignments at different stages in the homeless system are likely to have distinct vector representations. The second context captures the functional interactions between services, where assignments serving similar needs share similar vector representations. The last context models relationships between individuals, where similar individuals are more likely to exhibit similar traits.\nTemporal Context Here, our goal is to learn the latent representations of services and time units based on the intuitive observation that services occurring within similar time units should have analogous vector representations. To formalize this, we define $D \\in R^{|A|\\times|\\Tau|}$, where $D_{ij}$ represents the frequency with which service $a^i$ appears in time unit $T_j$. Here, $\\Tau$ denotes the set of time units (e.g., weeks, months, or years). To derive standardized representations, we employ Non-negative Matrix Factorization (NMF). Specifically, given $D$, we learn the non-negative matrices, $A \\in R^{|A|\\times k}$ for services and $S \\in R^{|\\Tau|\\times k}$ for time units by solving\n$\\underset{A,S\\geq0}{\\text{min}} ||D - AS^T||_F^2 + \\alpha \\sum_{j=1}^{|\\Tau|-1} ||S_{j,:} - S_{j-1,:}||_F^2 + \\lambda(||A|| + ||S||),$\nwhere $\\alpha$, $\\lambda$, and $k$ control the smoothness, sparsity constraints, and dimension of representations, respectively.\nFunctional Context This context is based on the premise that subsequent services are assigned to individuals because they address needs unmet by the preceding service, meaning, the two services together complement each other in satisfying a need. In this context, we capture the latent relationships between such service pairs. We define a transitional probability matrix $T \\in R^{|A|\\times|A|}$, where each element $T_{ij}$"}, {"title": "denotes the probability of assigning service $a^i$ after service $a^j$ (Rahman and Chelmis 2022). Consequently, we learn $m$\nlatent relations between services represented by matrices, $R_P \\in R^{m\\times k}$ for preceding assignment and $R_S \\in R^{m\\times k}$ for succeeding assignments such that for any pair $(a^i, a^j)$, $T_{ij} = \\sigma(a^iR_p[l] + a^jR_s[l])$. We perform symmetric NMF (Kuang, Ding, and Park 2012) to obtain the solution to\n$\\underset{A,R_P, R_S\\geq0}{\\text{min}} ||T - AR_P^T R_A^T||_F^2 + \\lambda(||A|| + ||R_P|| + ||R_S||),$", "content": "where $A$ denotes the latent representations of services. Similar to the temporal context, we incorporate a sparsity constraint with $\\lambda$ controlling the penalty for overfitting, and $k$ denoting the dimension of the representations.\nIndividual Context In this context, our objective is to learn the latent representations of services denoted by $A$ and features denoted by $V\\in R^{|F|\\times k}$, while also identifying clusters to which individuals belong (represented by $C\\in R^{|U|\\times k}$). By clustering individuals with similar features and past service assignments, our goal is to ensure that services assigned to similar individuals (i.e., in the same cluster) have similar representations. We define $H \\in R^{|A|\\times|U|}$, where each element $H_{ij}$ denotes the number of times individual $u_j$ is assigned to service $a^i$. Since X and H exhibit higher sparsity, $L_{2,1}$ norm is applied to prevent rows with greater sparsity from dominating the objective function. Given matrices H and X, we solve\n$\\underset{A,V\\geq0,C}{\\text{min}} ||H-AC^T||_{2,1} + ||X - CVT||_{2,1} + \\lambda(||A|| + ||V|| + ||C||) + \\beta tr(C\\Gamma C^T)$\ns.t. $C\\mathbb{1}_k = \\mathbb{1}_{|U|}, C\\in [0,1]$."}, {"title": "Here, the term $tr(C\\Gamma C^T)$ ensures that similar individuals are clustered together and $\\Gamma \\in R^{|U|\\times|U|}$ records the cosine similarity between individuals in the set $U$. The constraints on $C$ enforce soft clustering by ensuring that the rules of probability are satisfied. Such soft clustering is appropriate for homelessness, a complex domain where individuals can exhibit similarities with members of different clusters. Finally, $\\beta$ controls the strictness of the clusters by determining how tightly or loosely data instances are grouped together.\nOverall Optimization Function With all the previously introduced components, we formulate our joint optimization problem as follows:\n$\\underset{A,S,V, R_P, R_S\\geq0,C}{\\text{min}} ||D - AS^T||_F^2 + \\alpha \\sum_{j=1}^{|\\Tau|-1} ||S_{j,:} - S_{j-1,:}||_F^2 + ||H - AC^T||_{2,1} + ||X - CVT||_{2,1} + \\beta tr(C\\Gamma C^T) + ||T - AR_P^T R_A^T||_F^2 + \\lambda(||A|| + ||S|| + ||V|| + ||C|| + ||R_P|| + ||R_S||)$\ns.t. $C\\mathbb{1}_k = \\mathbb{1}_{|U|}, C\\in [0,1]$.", "content": "(1)"}, {"title": "4.2 Optimization Algorithm", "content": "Jointly updating the variables in Eq. (1) causes the objective function to become non-convex. We therefore optimize the objective function using Alternating Direction Method of Multiplier (ADMM) (Boyd et al. 2011), where variables are updated separately. All proofs are provided in the Appendix within the GitHub repository.\nWe begin by relaxing the constraints on C to orthogonality, specifically $C^T C = I, C \\geq 0$ (Tang and Liu 2012). We then introduce two auxiliary variables: $P = H - AC^T$ and $Q = X - CVT$. Consequently, Eq. (1) is reformulated into the following equivalent problem:\n$\\underset{A,S,V, R_P, R_S, C\\geq0,P,Q}{\\text{min}} ||D - AS^T||_F^2 + ||SB^' ||_F^2 + ||P||_{2,1} + ||Q||_{2,1} + \\beta tr(C\\Gamma C^T) + ||T - AR_P^T R_A^T||_F^2 + \\lambda(||A|| + ||S|| + ||V|| + ||C|| + ||R_P|| + ||R_S||\\}) + \\langle L, X - CVT - Q \\rangle + \\langle K, H - AC^T - P \\rangle + \\langle N, C^T C - I \\rangle + \\frac{\\mu}{2} ||H - AC^T - P||_F^2 + \\frac{\\mu}{2} ||X - CVT - Q||_F^2,$\nwhere $K, L, and N$ are Lagrangian multipliers, $\\mu$ controls the penalty for violating the equality constraints, $\\langle \\cdot, \\cdot \\rangle$ denotes the dot product, and $B^{'}$ represents the matrix equivalent to the smoothing constraint. Next, we summarize the update rules for each variable.\nUpdate P For updating P, we hold the other variables constant and remove the terms that are irrelevant to P. Eq. (2) can be written as:\n$\\underset{P}{\\text{min}} \\frac{\\mu}{2} ||P - (H - AC^T + \\frac{K}{\\mu})||_F^2 + ||P||_{2,1}.$\nLemma 1 provides a closed form solution for Eq. (3).\nLemma 1 (Wang, Tang, and Liu 2015) Given matrix E and a positive scale $\\alpha$, the $i^{th}$ row of the optimal solution $W^*$ of $\\underset{W}{\\text{min}} ||W - E||_F^2 + \\alpha||W||_{2,1}$ is given by:\nw_i^* = \\begin{cases} (1 - \\frac{\\alpha}{||e_i||})e_i, & \\text{if } ||e_i|| > \\alpha \\\\ 0, & \\text{otherwise} \\end{cases}\nUsing Lemma 1, the optimal solution $P^*$ for the above equation is as follows, where $E^P = H - AC^T + \\frac{K}{\\mu}:\nP_i^* = \\begin{cases} (1 - \\frac{1}{\\mu||E_i||})E_i, & \\text{if } ||E_i|| > \\frac{1}{\\mu} \\\\ 0, & \\text{otherwise} \\end{cases}\nUpdate Q The optimal solution $Q^*$ is obtained similar to P, using Lemma 1, where $E^Q = X - CVT + \\frac{L}{\\mu}:\nQ_i^* = \\begin{cases} (1 - \\frac{1}{\\mu||E_i||})E_i, & \\text{if } ||E_i|| > \\frac{1}{\\mu} \\\\ 0, & \\text{otherwise} \\end{cases}"}, {"title": "Next, applying Lemma 1, the optimal solution $P^*$ for the above equation is obtained in closed form, where $E^P = H - AC^T + \\frac{K}{\\mu}:\nP_i^* = \\begin{cases} (1 - \\frac{1}{\\mu||E_i||})E_i, & \\text{if } ||E_i|| > \\frac{1}{\\mu} \\\\ 0, & \\text{otherwise} \\end{cases}\nUpdate S For updating S, we fix the other variables and remove the terms that are irrelevant to S, we get $O_s = \\underset{S}{\\text{min}} ||D-AS^T||_F^2 + \\frac{\\lambda}{2}||S||_F^2 + \\alpha||SB^'||_F^2 - Tr(\\Psi_sS^T)$, where $\\Psi_s$ is the Lagrangian multiplier for $S \\geq 0$ and $Tr(\\cdot)$ denotes trace operator. The partial derivative of $O_s$ is as follows:\n$\\frac{1 dO_s}{2 dS} = -(D - AS^T)A + \\alpha (SB^{'})B^{'} + \\lambda S - \\Psi_s$\nTo obtain the optimal solution, we set the partial derivative equal to zero and get\n$\\Psi_s = -D^TA + S^T A + \\alpha(S(B^{'})^2) + \\lambda S$\nNext, we use Karush-Kuhn-Tucker complementary condition, that is, $\\Psi_s(i, j)S_{ij} = 0$ (Boyd et al. 2011), we get,\n(\\hat{S_{ij}} - \\check{S_{ij}}) S_{ij} = 0$\n$\\hat{S_{ij}}S_{ij} = \\check{S_{ij}}S_{ij}$\n$S_{ij} = \\frac{\\check{S_{ij}}}{\\hat{S_{ij}}} S_{ij}$\nHere, $\\check{S_{ij}}$ and $\\hat{S_{ij}}$ consists of the positive and negative terms, respectively, as shown below.\n$\\check{S_{ij}} = D^TA + [\\alpha(S(B^{'})^2)]^-$\n$\\hat{S_{ij}} = S^T A + \\lambda S + [\\alpha(S(B^{'})^2)]^+$", "content": "M\nHere, for any matrix $M$, $(M)^+ = \\frac{ABS(M)+M}{2}$ and $(M)^- = \\frac{ABS(M)-M}{2}$ are the positive and negative part of M, respectively. ABS(M) consists of the absolute value of elements in M (Shu, Wang, and Liu 2019). We follow similar steps to compute $R_P$, $R_S$, V, C, and A.\nUpdate $R_P$ and $R_S$ For updating $R_P$, we fix the other variables and remove the terms that are irrelevant to $R_P$, we get $O_{R_P} = \\underset{R_P}{\\text{min}} \\frac{\\mu}{2} ||T - AR_P^T R_A^T||_F^2 + \\frac{\\lambda}{2}||R_P||_F - Tr(\\Psi_{R_P} R_P^T)$. The partial derivative of $O_{R_P}$ is as follows:\n$\\frac{1 dO_{R_P}}{2 dR_P} = -T A A^T R_s + A R_P T R_A^T A A^T R_s + \\lambda R_P - \\Psi_{R_P}$\n$\\Psi_{R_P} = -T A A^T R_s + A R_P T R_A^T A A^T R_s + \\lambda R_P$"}, {"title": "Using Karush-Kuhn-Tucker complementary condition (Boyd\net al. 2011), that is, $\\Psi_{R_P}(i, j)R_{P_{ij}} = 0$, we get,\nR_{P_{ij}} \\leftarrow \\frac{\\check{R_{P_{ij}}}}{\\hat{R_{P_{ij}}}} R_{P_{ij}}\n$\\check{R_{P_{ij}}} = T A A^T R_s + (AR_P T R_A^T A A^T R_s)^-$\\n$\\hat{R_{P_{ij}}} =  R_P + (AR_P T R_A^T A A^T R_s)^+$\\nSimilar to $R_P$, we get the following equations for $R_s$.\n$\\frac{\\check{R_{s_{ij}}}}{\\hat{R_{s_{ij}}}} R_{s_{ij}}$\n$R_{s_{ij}} \\leftarrow \\frac{\\check{R_{s_{ij}}}}{\\hat{R_{s_{ij}}}} R_{s_{ij}}$\n$\\check{R_{s_{ij}}} =  T A A^T R_P + (AR_P T R_A^T A A^T R_P)^-$\\n$\\hat{R_{s_{ij}}} = AR_s + (AR_P T R_A^T A A^T R_P)^+$\\nUpdate V For updating V, we fix the other variables and remove the terms that are irrelevant to V, we get $O_V = \\underset{V}{\\text{min}} \\frac{\\mu}{2}||X-CVT-Q||^2 + <(L,X-CVT-Q)>+\\frac{\\lambda}{2}||V||^2 - tr(\\Psi_V V^T)$. The partial derivative of $O_V$ is as follows:\n$\\frac{dO_V}{dV} = -\\mu(X - CVT - Q)TC - LTC + 2XV - \\Psi_V$\\n$\\Psi_V = -\\mu X TC + \\mu V CTC + \\mu Q TC - LTC + 2XV$\nUsing Karush-Kuhn-Tucker complementary condition (Boyd et al. 2011), that is, $\\Psi_V (i, j)V_{ij} = 0$, we get,\nV_{ij} \\leftarrow \\frac{\\check{V_{ij}}}{\\hat{V_{ij}}} V_{ij}\n$\\check{V_{ij}} = \\mu X TC + (LTC)^+$\\n$\\hat{V_{ij}} =  \\mu V CTC + \\mu Q TC + (LTC)^- + 2\\lambda V$\\nUpdate C For updating C, we fix the other variables and remove the terms that are irrelevant to C, we get $O_C = \\underset{C}{\\text{min}} ||H-ACT-P||^2 + ||X-CVT -Q||^2 -\\frac{\\lambda}{2}||C|| -Tr(\\Psi_C C^T)$. The partial derivative of $O_C$ is as follows:\n$\\frac{1 dO_C}{2 dC} = -\\mu(H - ACT - P)T A - \\mu(X - CVT - Q)V - LTC - K T A + 2CN + 2\\beta \\Gamma C + 2\\lambda C - \\Psi_C$\\n$\\Psi_C = -\\mu HTA + \\mu CATA + \\mu PTA - \\mu XV + \\mu CVTV + \\mu QV - LV - KT A + 2CN + 2\\beta \\Gamma C + 2\\lambda C$", "content": "Using Karush-Kuhn-Tucker complementary condition (Boyd et al. 2011), that is, $\\Psi_C(i, j)C_{ij} = 0$, we get,\n$\\frac{\\check{C_{ij}}}{\\hat{C_{ij}}} C_{ij}$\n$\\check{C_{ij}} = HTA + XV + (LV)^+ + (KTA)^+ + 2(CN)^- + 2(\\beta (\\Gamma C)^-$\\n$\\hat{C_{ij}} =  \\mu CATA + \\mu PTA + \\mu CVTV + \\mu QV + (LV)^+ + (KTA)^+ + 2(CN)^+ + 2\\beta (\\Gamma C)^+ + 2\\lambda C$"}, {"title": "4.3 Service Assignment Prediction", "content": "In this section, we delve into the specifics of the service assignment module of REPLETE. Once the representations are obtained by solving Eq. (1), we derive three sets of features: (i) service and feature representations, (ii) feature interactions, and (iii) instance interactions. The overall process is summarized in Algorithm 1.\nService and Feature Representations We utilize the representations of services and features in our prediction model instead of one-hot encoding them. The rational is that by treating services and features as categorical our model would miss the inherent relationships within them. Therefore, we replace the services $\\{a_{t_1},...,a_{t_n} \\}$ within the history of services received by an individual, with their respective representations $\\{A_{a_{t_1}},..., A_{a_{t_N}} \\}$, where $A_{a_{t_i}}$ denotes the representation of service $a_{t_i}$. We additionally obtain the representations of the socio-economic features for each individual by multiplying X and V, where X is the feature matrix and V is the matrix of learned feature representations.\nFeature Interactions Beyond the representations of services and features, we further incorporate two categories of service interactions: temporal and functional (Section 4.1). Given the sequence of services $\\{a_{t_1},..., a_{t_N} \\}$ received by individuals, the temporal and functional interactions within each service pair $(a_{t_i}, a_{t_j})$ are defined as $A_{a_{t_i}}S_{TS}A_{a_{t_j}}^T$ and $A_{a_{t_i}} R_P R_S A_{a_{t_j}}^T$, respectively. Matrix $A_{a_{t_i}}$ denotes the representation of service $a_{t_i}$, whereas matrices S, $R_P$, and $R_S$ are the learned representations for time units, preceding assignments, and succeeding assignments, respectively.\nInstance Interactions We define instance interaction as the similarity between individuals, which we use to identify clusters within the individuals. Since the relaxed optimization on C (i.e., the matrix denoting the clusters to which individuals belong) does not directly provide probabilities for soft clustering, we normalize C such that each row sums to 1, thereby obtaining these probabilities."}, {"title": "Prediction Model We use $Z\\in R^{|U|X|F^{'}|}$, to denote the\nconcatenated feature vector, i.e., service and feature representations, feature interactions, and instance interactions, where U is the set of individuals, and F' is the set of derived features. For the service assignment task, we utilize a single hidden layer feed-forward neural network (FFNN). The rationale behind a single hidden layer architecture is that it balances complexity and computational efficiency.\nPredicted service assignments, denoted by $\\hat{Y}$, are calculated as $\\hat{Y} = \\sigma(ZW_1 + b_1)W_2 + b_2$ where, $W_1$ and $W_2$ are the weight matrices for the hidden and output layers, respectively, $b_1$ and $b_2$ are the bias vectors for the hidden and output layers, respectively, and $\\sigma(\\cdot)$ is the ReLU activation function. The model is trained using cross-entropy loss.", "content": ""}, {"title": "5 Experimental Setup", "content": "Data Description Our analysis utilizes an anonymized dataset from CARES of NY comprising 18,817 records from 6,011 chronically homeless individuals in the Capital Region of New York State, covering the period from 2012 to 2018, including a total of 9 different homeless services. A complete description of the services is available at (United States Department of Housing and Urban Development 2020). Sequence of services longer than N are sampled using a sliding window with a length of N. For sequences shorter than N, missing values are encoded with 0, resulting in a zero vector representation with a length A. After sampling, we denote our dataset as D and perform a random 70 - 30 split to obtain the training set $D_{train}$ and testing"}, {"title": "set $D_{test}$, respectively. Finally, we train and evaluate our approach using $D_{train}$ and $D_{test}$ respectively.\nBaselines We compare REPLETE with the state-of-the-art: TRACE (Rahman and Chelmis 2022) and PREVISE (Rahman, Zois, and Chelmis 2023). TRACE and PREVISE uses service sequences to predict the next assignment. Additionally, following (Rahman, Zois, and Chelmis 2023), we compare our approach with random forest (RF), logistic regression (LR), transformer, feed forward neural network (FFNN1), recurrent neural network (RNN), and long short-term memory (LSTM), which incorporate one-hot encoded features for prediction.\nEvaluation Metrics We evaluate our approach using accuracy, recall, precision, and $F_1$ score.\nImplementation Details All analyses were conducted in Jupyter Notebook using Python 3 on a MacBook Air with an 8-core M1 chip (4 performance and 4 efficiency cores) and 8 GB of memory, running macOS Sonoma. Representation learning framework ran 500 iterations, and the prediction model, on an average, ran 20 iterations in each fold of 5-fold cross validation.", "content": ""}, {"title": "6 Results and Analysis", "content": "Quantitative Analysis Table 1 shows that REPLETE significantly outperforms the baselines in predicting the next service assignment, both in terms of accuracy and $F_1$ score. Table 2 confirms that REPLETE significantly outperforms PREVISE (state-of-the-art) across all metrics and LSTM (the best-performing baseline) in accuracy and F-score. This indicates that (i) the representation learning framework effectively captures the rich relationships within the features and instances, and (ii) derived features based on temporal, functional, and individual contexts, effectively carry these relationships to the FFNN. Together, these factors lead to a significant improvement in the performance of our approach.\nTo analyze the reliability of the representation learning framework, we first examine whether derived features can effectively distinguish between labels (i.e., services) compared to the one-hot encoded features. We utilize the t-distributed Stochastic Neighbor Embedding (t-SNE) method to embed both high-dimensional features into 2-dimensional space. Figure 2 illustrates that distinct clusters"}, {"title": "for each label are formed using the derived features. This indicates that the derived features offer better separability compared to the one-hot encoded features.\nNext, we analyze the performance of REPLETE across different history length N. Table 3 shows that our model's performance remain consistent despite variation in the length of the individual's history considered for prediction. This indicates the robustness of our approach. For the rest of our experiments, we set N = 3, as the model achieves the highest accuracy and $F_1$ score at this value.\nAblation Study We conducted two sets of ablation studies to assess the importance of various model components. First, we analyze the impact of feature interactions and instance interactions on"}]}