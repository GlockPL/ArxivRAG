{"title": "Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings", "authors": ["MITHUN SAHA", "MAXWELL A. XU", "WANTING MAO", "SAMEER NEUPANE", "JAMES M. REHG", "SANTOSH KUMAR"], "abstract": "Photoplethysmography (PPG)-based foundation models are gaining traction due to the widespread use of PPG in biosignal monitoring and their potential to generalize across diverse health applications. In this paper, we introduce Pulse-PPG, the first open-source\u00b9 PPG foundation model trained exclusively on raw PPG data collected over a 100-day field study with 120 participants. Existing PPG foundation models are either open-source but trained on clinical data or closed-source, limiting their applicability in real-world settings. We evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its performance against a state-of-the-art foundation model trained on clinical data. Our results demonstrate that Pulse-PPG, trained on uncurated field data, exhibits superior generalization across clinical and mobile health applications in both lab and field settings. This suggests that exposure to real-world variability enables the model to learn fine-grained representations, making it more adaptable across tasks. Furthermore, pre-training on field data surprisingly outperforms its pre-training on clinical data in many tasks, reinforcing the importance of training on real-world, diverse datasets. To encourage further advancements in robust foundation models leveraging field data, we plan to release Pulse-PPG, providing researchers with a powerful resource for developing more generalizable PPG-based models.", "sections": [{"title": "1 INTRODUCTION", "content": "Unobtrusive, ubiquitous, and cost-effective wearable sensors have demonstrated the potential to revolutionize real-time monitoring of health and wellness by enabling the detection of various physical and mental health states. Photoplethysmography (PPG) in smartwatches has emerged as a widely used modality due to its non-invasive assessment of physiology without the need for firm attachment. It is used for estimating physiological metrics such as heart rate, heart rate variability [68], blood glucose [5], oxygen saturation [61], and blood pressure [22, 27]. For diagnosis, it can detect cardiovascular conditions [52], including atrial fibrillation [56] and detect hypoxia [39]. For mental health and wellness, it can track stress [94], emotion [37], focus [80], and depression [37].\nHowever, PPG-based inference in real-world settings remains challenging due to its high susceptibility to noise from motion artifacts [57], ambient light [14], and skin conditions [4]. This has slowed the progress in realizing the full potential of PPG in the natural environment. For example, high accuracies for stress classification has been reported on lab data [6, 13, 26, 48, 49], but they do not generalize to the field settings [48]. Training models on lab data from the same participants and then applying it to their field data can lead to a better performance [76], but it doesn't scale to unseen participants. Models trained using larger field datasets report low performance [7, 90].\nThe emergence of Foundation Models (FM) offers a new opportunity to address these challenges and accelerate our progress. In other domains such as natural language processing and computer vision, the foundation model paradigm has transformed the development of machine learning solutions to real-world problems. The key property of an FM is that it is pre-trained on a large-scale dataset to ensure that the resulting feature representation encompasses all of the complexity of the data domain, which is then validated by demonstrating that the FM can solve multiple downstream tasks without additional representation learning. In computer vision, the field has moved away from collecting individual special-purpose datasets and training task-specific models to leveraging existing foundation model representations, such as DINOv2 [51] in solving a variety of perceptual tasks. A key enabling property is that, while the training datasets are private and not publicly available, the model weights are released to the research community, enabling everyone to benefit from its powerful representation. This transition in utilizing publicly available FMs has not yet occurred for mHealth, and it is a crucial next step.\nWhile there have been some exciting recent efforts to develop FMs for PPG [2, 54], current approaches suffer from two important limitations: 1) Private models which define the SOTA but whose weights are not available to the research commmunity [2], and 2) Models which demonstrate impressive performance on lab-collected data but have not been developed or evaluated for use in the field environment [54]. The growing availability of"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Foundation Models for PPG signals", "content": "There have been some recent work on PPG-specific foundation models, but none of them are open and designed for wearable PPG signals (i.e., collected from a smartwatch). The closest work with open model availability is PaPaGei, an open-source PPG foundation model that improves classification and regression performance across 14 tasks [54], including some wearable lab tasks. However, it is exclusively trained on clean clinical PPG signals, and was not evaluated on field data in [54]. We show (Section 5.2) that training on this clinical data negatively impacted its generalization performance on wearable tasks involving field-collected data. The most relevant work for wearable PPGs is a foundation model pre-trained on a large-scale wearable field PPG dataset and presented in [2]. However, the model is closed-source and its training datasets are private, restricting accessibility for the research community. Similalry, SiamQuality is a foundation model pre-trained on clinical data that demonstrates generalizability on wearable lab and clinical datasets, but the model remains private [17]."}, {"title": "2.2 Foundation Models in General", "content": "Our work is inspired by prior successes in developing foundation models for other data modalities. These works have shown that models pre-trained on large-scale datasets can capture underlying data representations and are highly adaptable for various downstream applications [11]. These models have had substantial impact in fields like Natural Language Processing and Computer Vision. In the language domain, models such as BERT [16] and the Generative Pretrained Transformer (GPT) [60] have demonstrated remarkable generalizability across a diverse range of natural language processing tasks, catalyzing the development of large language models (i.e. ChatGPT [93]). The vision domain has seen similar breakthroughs with models like CLIP [59] and the Segment Anything Model [36] that are trained on large-scale data and exhibit strong performance on a broad set of downstream tasks. These works have demonstrated that pre-training on large-scale data can yield effective feature representations and serve as the motivation for this work."}, {"title": "3 DESIGNING PULSE-PPG METHODOLOGY", "content": "Our goal is to develop a foundation model capable of learning a generalizable embedding space that can be effectively applied to a diverse set of datasets (ranging from clinical PPG to mHealth field PPG) and downstream tasks (ranging from blood pressure prediction to stress detection). In Section 3.1, we detail our rationale for pre-training the foundation model on noisy field PPG data, as opposed to the traditional approach of pre-training on clean, lab-study-based PPG data. Then, in Section 3.2, we outline the key challenges involved in learning a robust foundation model in this setting and describe how our methodology addresses each of these challenges."}, {"title": "3.1 Motivation for Pre-training on Field PPG data", "content": "Traditionally, prior work has focused on training models using clean, lab-collected PPG data and evaluating them on mHealth datasets [54]. This approach is largely due to the significant challenges associated with utilizing real-world field data, which is inherently noisy and variable. However, we hypothesize that by designing a methodology capable of effectively learning patterns from field data, we can develop representations that are more robust, generalize better, and achieve superior performance on downstream mHealth tasks, such as instantaneous heart rate prediction or activity recognition. In this way, the noise present from the wearable field sensor becomes a benefit, as it could introduce additional contextual information of a person's current state and environment. Importantly, pre-training on mHealth data and evaluating on mHealth data reduces domain shift, as the model is exposed to the same types of noise, variability, and environmental factors during both training and evaluation.\nClinical datasets, while less noisy and more consistent due to their controlled collection processes administered by professionals, lack the dynamic variability and environmental factors present in real-world settings. For example, clinical datasets often involve participants with specific health conditions and are collected under static, controlled conditions, which fail to capture the wide range of physiological and environmental variability encountered in the wild. In contrast, field data includes participants with and without specific conditions, as well as a diverse array of physiological patterns influenced by real-world factors, such as poor sensor contact from loose bands and daily motion. This richness and diversity make field data a more comprehensive source for pre-training a foundation model.\nBy pre-training on field data, we aim to create a foundation model that better reflects the complexity of real-world scenarios and generalizes more effectively across diverse applications."}, {"title": "3.2 Designing the Pre-training Task for the Pulse-PPG Foundation Model", "content": "To build a robust foundation model, we must design a pre-training task that not only captures high-level, obvious semantic features-such as heart rate changes-but also uncovers subtle, latent patterns in the data, such as variations in pulse waveform morphology or transient physiological responses. PPG signals, which measure blood volume changes, encode a wealth of physiological information. However, this information is often obscured by noise, particularly in mHealth settings. Traditional approaches address noise by normalizing or aggregating data into statistical features and training models on these features. While this reduces the impact of noise, it often discards fine-grained details that are critical for accurate inference or discovering hidden clusters and trends in the data.\nTo overcome these challenges, we introduce Pulse-PPG, a method specifically designed to address three key issues: 1) the development of a domain-specific pre-training task that leverages PPG-specific domain knowledge; 2) exploiting the inherent noise of real-world, mHealth PPG data; and 3) the ability to capture subtle, yet meaningful, patterns in the data that are essential for ensuring robustness and generalizability across diverse and dynamic real-world settings."}, {"title": "3.2.1 PPG-Specific Pre-training Task:", "content": "A classic self-supervised objective for time-series foundation models is contrastive learning, where augmentations (e.g., cropping, scaling, jittering, shuffling) create distinct but semantically similar positive pairs [18, 40, 46, 53, 82, 86\u201388, 91]. However, PPG signals face unique challenges: (1) time-series lack image-like invariances (e.g., rotation), and (2) augmentations can distort PPG semantics. For example, scaling alters amplitude (critical for blood volume inference) and shuffling disrupts temporal dependencies (key for heart rate variability). Alternatively, subject-aware contrastive learning avoids augmentations by leveraging long-term recordings from individual subjects [1]. While especially effective for biosignals [31], existing methods rely on naive temporal sampling [75] or supplemental context (e.g. event labels) [30]. To address this, we propose directly exploiting raw PPG semantics in our large-scale unlabeled dataset, eliminating reliance on augmentations or auxiliary data. To unlock the potential of our large-scale unlabeled wearable field PPG dataset, we need to design a methodology that utilizes semantic information from only the raw PPG signals.\nA critical characteristic of raw PPG signals lies in their origins in the cardiopulmonary system. PPG signals are pulsative: quasiperiodic and composed of repeating motifs that correspond to individual cardiac cycles. We define motifs abstractly as the short temporal shapes within the overall PPG morphology that reflect specific semantic information, such as the first upward slope in a specific PPG beat that represents systolic rise time. These motifs encode critical physiological information, such as heart rate variability, blood pressure changes, and vascular dynamics.\nTherefore, we design our pre-training task to learn differences between PPG instances, based upon the differences in their motifs. In this way, we utilize a distance function, $d(X_{anchor}, X_{cand})$ that compares two PPG sequences, $X_{anchor} \\in \\mathbb{R}^T$ and $X_{cand} \\in \\mathbb{R}^T$ by finding the motif in $X_{cand}$ that best matches a given motif $X_{anchor}$."}, {"title": "3.2.2 Exploiting the Noise of Real-World, mHealth PPG data:", "content": "Many prior works that utilize morphology information from raw PPG signals for their task-specific application, such as for blood pressure prediction [65, 72] or stress detection [3, 64], will first utilize a peak detection algorithm to segment out individual PPG beats that correspond to as single cardiac cycle [21, 24, 34]. After segmentation, specific temporal features within the beat can be investigated, such as the dicrotic notch or the systolic rise time. However, these methods are evaluated exclusively on clinical or lab-collected PPG data, where signal quality is controlled and high.\nExploiting PPG morphological information is significantly more challenging in real-world wearable field PPG data, where persistent noise and motion artifacts, make beat-to-beat segmentation and morphological feature extraction infeasible. As shown in Figure 2, the sharp decline in PPG signal quality-ubiquitous in mobile health settings-severely undermines the reliability of traditional segmentation and feature isolation methods.\nTo address this challenge, we utilize a learnable motif-based distance function (Equation 2) trained directly on noisy wearable field PPG data. Unlike traditional approaches that depend on predefined morphological features or clean beat segmentation, our method learns to identify motifs from raw, uncurated signals. By training under real-world noise conditions, we leverage the inherent diversity of field data motifs. Crucially, noise patterns are not discarded but instead leveraged as meaningful contextual cues: non-random artifacts (e.g., from motion or environment) correlate with specific activities, states, or conditions, enhancing motif uniqueness. This enables the model to identify semantic clusters and learn robust representations generalizable"}, {"title": "3.2.3 Capturing Subtle Yet Meaningful Patterns:", "content": "Given our motif-based distance function, we can identify positive and negative pairs to draw together and push apart in the embedding space, respectively. Traditionally, contrastive approaches will utilize the Normalized-Temperature Cross-Entropy Loss (NT-Xent) function (Equation 4), in which the positive pair is pulled towards the anchor, and all negatives are pushed away, equally.\n$l(X_{anc}, X_{pos}, S_{neg}) = - log \\frac{exp(sim(X_{anc}, X_{pos})/\\tau)}{\\sum_{X_{neg} \\in S_{neg}} exp(sim(X_{anc}, X_{neg})/\\tau) + exp(sim(X_{anc}, X_{pos})/\\tau)}$\nThe standard NT-Xent approach's reliance on rigid positive/negative assignments makes it prone to false positives (misclassifying dissimilar instances as positives) and false negatives (overlooking valid positives in the candidate pool). For example, in stress assessment, being stressed due to bad traffic differs from the experience of being tailgated, but the two are more similar compared to being engaged in a verbal conflict. Treating all negatives equally ignores such nuances, resulting in overly coarse clusters that fail to capture subtle semantic relationships.\nAn alternative idea to address this is utilizing a metric learning loss function [28, 32, 35], in which our self-supervised learning encoder learns to embed the PPG instances exactly as described by our motif-based distance function. However, although our learned motif-based distance function will be able to capture semantic information, it was trained without labels and is not expected to always, perfectly identify the correct semantic relationships.\nA \"goldilocks\" approach between these two ideas would be using a relative contrastive learning framework that models the relative positions of the distances. As such, we utilize the concept of relative dissimilarity via multiple negative sets [85]. For each positive pairing, a negative set is created by selecting samples whose distance from the anchor exceeds the distance of the positive pair. Each candidate in the dataset is iteratively used to form a positive pair, while the remaining candidates contribute to the negative set:\n$f_{neg} (X_{anc}, X_{pos}, S) = \\{X \\in S | d(X_{anc}, X) > d(X_{anc}, X_{pos})\\}$"}, {"title": "4 PRE-TRAINING PROCEDURE AND IMPLEMENTATION FOR PULSE-PPG", "content": "In this section, we provide the technical details of our foundation model's implementation and pre-training. Section 4.1 outlines how we employ our learnable motif-based distance function and relative contrastive loss to train the Pulse-PPG foundation model. Section 4.2 describes the composition and characteristics of our pre-training dataset, detailing how we process the wearable field PPG data to align with our pre-training task, as well as highlighting the dataset's inherent properties that make it well-suited for learning robust representations in noisy, real-world conditions. Finally, Section 4.3 describes specific implementation details, such as specific architecture and total GPU compute used. Please refer to Appendix for specific hyperparameter details."}, {"title": "4.1 Model Pre-training Details", "content": ""}, {"title": "4.1.1 Training the Learnable Motif-based Distance Function.", "content": "As mentioned previously in Section 3.2.2, making our distance function learnable is a key strength of our approach, as it enables motif-comparisons between PPG sequences, even when such PPGs are afflicted with noise. Therefore, before utilizing our relative contrastive loss, we learn our distance function in an unsupervised fashion.\nIn our initial label-free pre-training setting, we do not know if a random pair of PPG sequences share class labels, and thus we do not know if reconstruction error should be minimized to learn a motif-matching similarity function between them. Therefore, during training, we set Xq and Xk to be the same value and apply a missingness mask on Xq. This mask has a contiguous segment of missingness that stretches 2 seconds. In this way, our distance function will learn to retrieve the regions from the key Xt that match the missing 2 second motif from the query, Xq, for reconstruction. Then, after training, we can utilize this distance as a static function for the RelCon loss to identify relative distances between Xq and Xk, where Xq and Xk are different instances with different values."}, {"title": "4.1.2 Training the Pulse-PPG Foundation Model.", "content": "Now, once we have our trained motif-based distance function, we can use it to identify relative distances of a candidate set of PPG sequences from an anchor PPG sequence. This candidate set of PPG sequences is sampled from two sources: within-subject and between-subject.\n\u2022 Sampling within-subject, across-time, draws candidate PPG sequences from the same subject, but at a different time-window from the anchor. This allows us to model how an individual's behavioral patterns throughout their day affects their PPG signal, for example via exercise.\n\u2022 Sampling between-subject draws candidate PPG sequences from other persons within a batch. This allows us to model how the similarities and differences in an individual's specific physiology and routines affects their PPG signal.\nGiven this candidate set, we can sort the candidates with our motif-based distance function found in Equation 2 and then apply our RelCon loss function in Equation 5 based on their relative distances from the anchor. As such, our Pulse-PPG foundation model will be able to capture both within-subject and between-subject semantic information within the embedding space."}, {"title": "4.2 Pre-training on a Field Wearable PPG Dataset", "content": ""}, {"title": "4.2.1 Dataset Description:", "content": "Our pre-training dataset is composed of 822,247 unique 4-minute 50 Hz PPG segments, with 120 participants who wore a smartwatch a day for up to 100 days. This is from the Mobile Open Observation of Daily Stressors (MOODS) study dataset. The study was conducted using a WearOS app for smartwatches, a cross-platform smartphone app, and cloud services. Detailed descriptions of the System and Study Design of the MOODS Study are available in [50]."}, {"title": "4.2.2 Minimal Preprocessing:", "content": "To pre-train the physiological model, we utilize raw photoplethysmography (PPG) data collected from the participants. Out of 122 participants, the dataset recorded PPG data from 120 participants, which we use for pre-training. PPG data collected in the field is noisy, however, avoiding signal specific filtering can provide certain advantages for generalized, robust representations e.g., adapting to dataset diversity, preserving subtle meaningful variations, and transferability. Hence, we refrain from applying any particular filtering technique, but focus on addressing between person variability through a global person-specific z-normalization."}, {"title": "4.2.3 Subject-wise Train/Val/Test Splits:", "content": "To pre-train a model capable of handling the substantial within- and between-subject variability, we randomly shuffled participants and created a train/val/test split of 84/18/18. This split was consistently applied across all downstream tasks. We deliberately adopted a subject-based splitting strategy to pose a more challenging task for the model: making inferences on data from entirely unseen individuals, a common scenario in real-world applications."}, {"title": "4.2.4 PPG Sequence Inputs for Pre-Training Model.", "content": "In our modeling approach, we use 4-minute-long windows as sequence inputs for the Pulse-PPG Foundation Model. This choice is grounded in research highlighting the cyclical nature of physiological responses to stress. Studies have shown that stress-induced patterns in heart rate and heart rate variability (HRV) follow cyclical behaviors, with durations typically ranging from 3.5 to 4.2 minutes [8, 38, 81]. For example, heart rate increases during stress and remains elevated until the stressor ends, after which it returns to baseline, forming a cyclical pattern. Field studies, such as those involving stressful conversations or work-related stress, have further validated these patterns, with median cycle durations around 4 minutes [8].\nBy segmenting PPG signals into non-overlapping 4-minute windows, we aim to capture both micro-level perturbations (e.g., transient changes in pulse waveform morphology) and macro-level cyclical patterns (e.g., sustained heart rate elevation during stress). This approach allows our model to learn representations that are sensitive to the temporal dynamics of stress, which are critical for accurate stress detection. The 4-minute window size strikes a balance between capturing meaningful cyclical patterns and retaining fine-grained temporal details. It is important to note that, although our initial pre-training inputs are 4 minutes long, our model incorporates a temporal pooling mechanism that enables it to generalize to inputs of varying lengths. This flexibility ensures that the model can be applied to diverse downstream tasks, where input durations may differ."}, {"title": "4.3 Pulse-PPG Model Implementation Details", "content": ""}, {"title": "4.3.1 Encoder Details:", "content": "We utilize a 1D ResNet-26 network from the https://github.com/hsd1503/resnet1d code repository as our encoder backbone, with a total of 28.5m trainable parameters. There is instance normal-ization placed at the start of the ResNet, in order to account for PPG distribution shifts across users and across datasets. Our ResNet implementation utilizes an initial filter size of 128, a kernel size of 11, and a stride of 2, with 12 residual blocks that linearly increase the filter size every 4 blocks. There is a global average temporal pooling at the end of the network in order for our model to generalize across different time scales. As a result of this, our encoder embeds variable-length PPG signals into a single 1D 512-dimensional embedding vector."}, {"title": "4.3.2 Distance Function Details:", "content": "We utilize the code from the https://github.com/maxxu05/rebar code repos-itory in order to implement the distance function. This is a lightweight model, used as a static function to identify relative distances for the RelCon loss function and only has 127k trainable parameters. The dilated convolution network is a series of a dilated convolution blocks, made up of a dilated convolution layer with residual connection skip, followed by a ReLU and instance norm. There are 5 blocks, with an initial dilation of 1 that doubles after each layer. The filter size is set to 64, the kernel size is set to 15, and stride is set to 10. The input convolution layer to the DilatedConvNet is set to be a partial convolution [43] in order to handle missingness in our pre-training and application procedures. The reconstruction error used as our distance function is a standard MSE loss."}, {"title": "4.3.3 Compute Details:", "content": "Our Pulse-PPG foundation model was trained for 5 days for 6 epochs, where each epoch is composed of 606,833 unique 4-minute PPG segments or about 5,424 unique participant days. The distance function was trained over 10 hours for 20 epochs. We utilized NVIDIA L40S GPUs for our experiments, and models were constructed with the Pytorch python package."}, {"title": "5 RESULTS", "content": "The power of a foundation model lies in its ability to generalize across a broad range of tasks and datasets. This makes them especially valuable to the research community, providing pre-trained, general-purpose representations that can be easily adapted to new tasks, streamlining research efforts without the need for extensive task-specific data collection or manual feature engineering. Therefore, we design our experiments to validate this by evaluating our Pulse-PPG Foundation Model on 11 different downstream tasks, across 5 datasets, spanning typical wearable PPG tasks (i.e., stress classification) and typical hospital PPG tasks (i.e., systolic blood pressure regression). We evaluate this through a series of 3 experiments, 1) Pulse-PPG vs. A Prior PPG Foundation Model, 2) Assessing Field-to-Lab Generalizability, 3) Evaluation of Pulse-PPG Use-cases.\nIn the following sections we will describe our experimental design and results. In Section 5.1, we describe our downstream tasks and datasets. Then in Section 5.2, 5.3, and 5.4, we describe the design for each of our experiments and their results."}, {"title": "5.1 Downstream Datasets and Tasks", "content": "In order to evaluate the generalizability of our Pulse-PPG foundation model, we evaluate our 5 downstream datasets and 11 tasks, which can be seen in Table 1. Our datasets have a range of different PPG settings.\n\u2022 Wearable Field PPGs reflect the real-world setting for wearable applications with low signal quality.\n\u2022 Wearable Lab PPGs originate from controlled lab studies, in which subjects are given a wearable PPG sensor and asked to do a set of scripted activities under supervision. Generally, signal quality is higher.\n\u2022 Clinical PPGs originate from a hospital or another heavily monitored environment, in which PPG signals are collected from a finger sensor on stationary patients. Generally, signal quality is high.\nAdditionally, with each of PPG type, there are a set of canonical downstream tasks associated with them. For wearables, that includes stress classification [50, 70], activity classification [50, 62], and instantaneous HR regression [62]. For clinical settings, this includes sleep disturbance classification [23], blood pressure regression [19], and hypertension [19]. While activity classification is traditionally performed using IMU sensors or motion signals, we include it as a downstream task to showcase the versatility of our model in learning meaningful representations solely from PPG data. Each of these canonical tasks are captured in our downstream datasets. Please see Appendix A.1 for further dataset details and A.2 for evaluation implementation details."}, {"title": "5.1.1 Metrics.", "content": "For evaluating our models across diverse downstream tasks, we utilize task-specific metrics for both regression and classification problems. For regression tasks, we report standard Mean Absolute Error (MAE)"}, {"title": "5.2 Experiment 1: Pulse-PPG vs. A Prior PPG foundation model", "content": ""}, {"title": "5.2.1 Background.", "content": "In order to evaluate the effectiveness of our Pulse-PPG foundation model, we benchmark against the current state-of-the-art open PPG foundation model, PaPaGei [54]. Their PaPaGei-S model uses a 1D ResNet backbone and leverages a novel self-supervised objective with PPG-morphology-specific metrics, like the Inflection Point Area ratio, achieving top performance on primarily, clinically focused downstream tasks. Crucially, they release their model weights and code to the public (https://zenodo.org/records/13983110), which we utilize for our experiments.\nHowever, their pre-training dataset was composed solely of pre-processed, clean clinical PPG signals, and they do not demonstrate generalizability to wearable field PPG settings. As previously discussed in Section 3.2.2, we hypothesize that their PPG-morphology-specific metrics will be unable to generalize well in the much noisier, field setting. This is unlike our learned distance metric, which can potentially learn to do motif-similarity comparisons, even in noisy settings.\nWe also note that while PaPaGei demonstrated strong performance across various datasets in their work, their evaluation methodology involved using a different version of the foundation model for each task. Thus, while PaPaGei sets a critical stepping stone for PPG foundation modeling, our goal is to build a single open-source\u00b9 PPG foundation model that can generalize across many tasks, without re-training the network from scratch."}, {"title": "5.2.2 Experimental Design.", "content": "To ensure a fair comparison, we utilize a linear probing evaluation for each down-stream task for both the Pulse-PPG and PaPaGei-S model. Linear probing is a widely accepted evaluation method for foundation models because it isolates the quality of each foundation model's learned representations by freezing the model's embeddings and training only a simple logistic classifier or linear regression. Both PaPaGei and Pulse-PPG embed PPG sequences into a single 1D 512-dimensional vector, making their linear probe results directly comparable. By using linear probing-training a simple classifier on frozen embeddings-we isolate the quality of the learned representations, avoiding biases from architectural differences."}, {"title": "5.2.3 Results.", "content": "Figure 3 visualizes the difference in performance between our Pulse-PPG foundation model versus the publicly released pre-trained PaPaGei. Full results, can be found in the 1st and 4th data columns (i.e. Wearable Field Pulse-PPG and Clinical PaPaGei) in Table 3 in the Appendix. Our Pulse-PPG achieves much stronger performance, outperforming PaPaGei on 10/11 of our downstream tasks, consistently across the many reported metrics.\nOn the Wearable PPG tasks, our Pulse-PPG achieves a consistently large >.10 increase to F1 scores in the classification tasks (Stress - MOODS: 0.10 / Activity - MOODS: 0.15 / Activity - PPG-DaLiA: 0.12 / Stress (2) - WESAD: 0.15 / Stress (4) - WESAD: 0.14). Then on the instantaneous HR regression task, our Pulse-PPG model has large 10.24 improvement in MAE. These heart rate estimates are conducted on unfiltered wearable PPG data, which is significant due to the widely recognized challenge of noise-induced degradation in heart rate estimation.\nOn the Clinical PPG tasks, our Pulse-PPG model continues to outperform PaPaGei, with ~ 0.5 improvement to MAE for Systolic BP, Diastolic BP, and average HR regression, as well as a .05 improvement to F1 score on hypertension classification. While clinical evaluation tasks show smaller improvements compared to wearable ones, the consistent gains highlight the model's cross-domain generalizability.\nThe only task on which Pulse-PPG underperforms is the Sleep Disturbance task. This can be attributed to the nature of Pulse-PPG's pre-training dataset, MOODS. During the MOODS study, participants were instructed to wear the smartwatch during waking hours [50]. Consequently, Pulse-PPG was not exposed to PPG signals during sleep in its pre-training phase, limiting its ability to learn sleep-related patterns, since human physiology during"}, {"title": "5.3 Experiment 2: Assessing Field-to-Lab Generalizability", "content": ""}, {"title": "5.3.1 Background:", "content": "Traditionally, mHealth models were built and tested in controlled lab settings due to a lack of diverse field datasets. As more real-world data became available, researchers began applying these lab-trained models to field data. However, we hypothesize that pre-training foundation models directly on diverse field data will yield better generalizability and performance."}, {"title": "5.3.2 Experimental Design.", "content": "In these experiments, we assess how pre-training on wearable field PPG compares to pre-training on clean PPG. Similar to the prior experiments in Section 5.2, we utilize a linear probe for evaluation, to ensure fair comparisons. From the prior experiments, we already have our Pulse-PPG pre-trained on field PPGs and PaPaGei pre-trained on the clinical PPGs.\nIn order to construct a large clean clinical PPG pre-training dataset for re-training Pulse-PPG, we utilize the curated MIMIC-III PPG waveform dataset from [83]. This dataset is composed of 151,738 100 Hz 5-minute-long clean clinical PPGs from 18,210 patients. We then use this clean clinical PPG dataset to pre-train our Pulse-PPG model, following the same training procedures and hyperparameters that we used when we were pre-training on the MOODS field PPG dataset."}, {"title": "5.3.3 Results.", "content": "Figure 4 visualizes the difference in performance between Pulse-PPG pre-trained on the wearable field PPGs vs. pre-trained on clinical PPGs. Full results, including comparisons of PaPaGei pre-training with wearable field vs. clinical data can be found in Table 3 in the Appendix, while referencing the second row named \"Pre-Train Data\".\nPulse-PPG consistently exhibits stronger performance on 10 out of 11 tasks when pre-trained on the Field Wearable PPG data versus when pre-trained on the Clean Clinical PPG. For all other tasks except for sleep disturbance, the field wearable pre-training yielded consistently stronger results, sometimes substantially so. For example, for Field Wearable datasets, F1 score for stress improved by .0848 and F1 score for activity improved by.1218. Even for the clinical tasks, we see that pre-training on the wearable PPGs achieves much stronger performance (i.e., .10 F1 improvement in hypertension prediction and ~ 1 MAE improvement in systolic and diastolic BP regression). In some cases, such as in the WESAD evaluation datasets, one metric was worse, but all of the remaining metrics were stronger.\nPre-training on field wearable data appears to yield a more robust and generalizable representation, outper-forming clinical pre-training-even on clinical tasks. Although performance gains in clinical tasks are modest, the consistency demonstrates that pre-training on field wearable data performs as well as, and sometimes better than, pre-training on clinical data. As discussed earlier in Section 5.2.3, relatively poor results on sleep can be explained by lack"}]}