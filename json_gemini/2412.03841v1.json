{"title": "LL-ICM: Image Compression for Low-level\nMachine Vision via Large Vision-Language Model", "authors": ["Yuan Xue", "Qi Zhang", "Chuanmin Jia", "Shiqi Wang"], "abstract": "Image Compression for Machines (ICM) aims to compress images for machine vision tasks rather\nthan human viewing. Current works predominantly concentrate on high-level tasks like object de-\ntection and semantic segmentation. However, the quality of original images is usually not guaran-\nteed in the real world, leading to even worse perceptual quality or downstream task performance\nafter compression. Low-level (LL) machine vision models, like image restoration models, can help\nimprove such quality, and thereby their compression requirements should also be considered. In this\npaper, we propose a pioneered ICM framework for LL machine vision tasks, namely LL-ICM. By\njointly optimizing compression and LL tasks, the proposed LL-ICM not only enriches its encoding\nability in generalizing to versatile LL tasks but also optimizes the processing ability of down-stream\nLL task models, achieving mutual adaptation for image codecs and LL task models. Furthermore,\nwe integrate large-scale vision-language models into the LL-ICM framework to generate more uni-\nversal and distortion-robust feature embeddings for LL vision tasks. Therefore, one LL-ICM codec\ncan generalize to multiple tasks. We establish a solid benchmark to evaluate LL-ICM, which in-\ncludes extensive objective experiments by using both full and no-reference image quality assess-\nments. Experimental results show that LL-ICM can achieve 22.65% BD-rate reductions over the\nstate-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Image Coding for Machines (ICM) has become an emerging research topic that combines\nvisual signal compression and understanding. Existing methods can be categorized accord-\ning to the number of generated bitstreams: a single versatile bitstream for multiple tasks\njointly, two bitstreams for human perception and machine task respectively and multiple\nbitstreams for diverse tasks separately. These ICM bitstreams can be compact representa-\ntions of both original image [1] and extracted deep features [2], which usually have dif-\nferent levels of capability on texture reconstruction and semantic preservation. However,\nall current methods are limited by only optimizing for high-level (HL) vision tasks, while\nneglecting practical compression requirements for low-level (LL) ones.\nLow-level vision tasks, such as denoising, debluring, inpainting, etc., have been studied\nfor decades. The main purpose of these tasks is to recover the details introduced by image\ncapturing and delivering, and enhance the visual quality of the image content. Early works\ntend to propose one neural network for one specific LL vision task. Recently, consider-\ning the similarity of these tasks on pixel-level content analysis and processing, more and\nmore works are trying to create a single model to solve diverse LL vision tasks simultane-\nously. By introducing large vision-language modeling, state-of-the-art all-in-one LL vision"}, {"title": "Method", "content": null}, {"title": "Exsiting ICM frameworks", "content": "We first give a brief introduction to the existing ICM frameworks and analyze their draw-\nbacks. Fig. 1(a) is a conventional image codec optimized for signal fidelity without con-\nsidering the performance of downstream tasks on compression outputs. As machine intelli-\ngence evolves, the compression needs for high-level visual understanding are getting more\nand more attention, leading to the establishment of HL-ICM framework shown in Fig. 1(b).\nIn this framework, the codec is optimized for HL task models, whether their weights are\nfixed or not.\nMost ICM methods focus on HL vision tasks. However, LL tasks are also important for\napplications in the real world. The reason is that the quality of original images is usually\nnot guaranteed. After compressed and transmitted, we need to enhance their quality by\nLL vision processing. The simplest ICM framework for LL-vision tasks is illustrated in\nFig. 1(c), where the compressed image is processed by LL task models, but LL vision\nmodels do not consider the compression artifacts. In recent years, this issue has been\naddressed by the framework shown in Fig. 1(d), where LL vision models are trained with\nadaption to the compression distortion. However, the compression efficiency is neglected.\nTherefore, the LL-ICM framework in Fig. 1(e) offers a better solution that jointly improves\nthe performance of compression and LL vision processing. Nevertheless, there are many\nLL vision tasks and models, and it is impractical to train a codec for each of them. In\nthis paper, we take a step further to propose a unified one-for-all LL-ICM framework to\noptimize the codec for diverse LL vision tasks, which is shown in Fig. 1(f)."}, {"title": "Problem Definition", "content": "LL-ICM aims to jointly optimize the performance of multiple LL vision tasks while min-\nimizing compression costs. Inherently, LL vision tasks target at removing the artifacts\nfrom images causing quality degradations and improving the perceptual quality. Therefore,\nthe objective of LL-ICM can be defined as rate-perception optimization (RPO), which is"}, {"title": "LL-ICM framework", "content": "We present a unified LL-ICM framework in Fig. 3, which integrates a neural image codec\nand a unified LL vision processing model. Notably, we incorporate a pre-trained VLM\nmodel into this framework to extract generalized features for handling different LL vision\ntasks.\nIn this study, we employ MLIC [3] as backbone image codec C due to its superior\nperformance. The frozen VLM model V [4] is used to extract generalized features F as:\n$$F = V(X) = V(C(X)).$$\nAfter that, the LL-vision encoder recieve X and encoded F by encoder $\\varepsilon$ as input\nreference to generate \u0425\u043d:\n$$X_{H} = \\tau(X, \\varepsilon(F)).$$\nWe adopt the image controller in DA-CLIP [5] as the encoder in the feature controller\nof our framework, which encodes the feature F to LL task type 4 and caption \u03c3. For"}, {"title": "Experiment", "content": null}, {"title": "Training Setting", "content": "Our training procedure is divided into two stages. In the first stage, we only train the\nimage codec to achieve high compression efficiency. We train the codec on two NVIDIA\n4090 GPUs for 30 epochs, using the Adam optimizer with a batch size of 16. We randomly\nselect 2 \u00d7 105 images from the COCO2017 and ImageNet datasets as the initial training set,\nand the images are randomly cropped to a patch size of 448x448. Following the settings of"}, {"title": "Testing Setting", "content": "To evaluate the performance of our proposed LL-ICM framework, we test the compres-\nsion and LL vision task performance and compare our method with several state-of-the-\nart image codecs. The selected codecs include both traditional block-based codec like\nEnhance Enhanced Compression Model (ECM) [14], and deep learning-based ones like\nBalle2018 [15], Cheng2020 [16], and MLIC [3]. To ensure the fairness of the comparison,\nwe also use IR-SDE [6] as the LL vision model to generate enhance outputs of the com-\npressed images from these codecs. We conduct objective experiments by calculating both\nfull-reference and no-reference image quality metrics on the generated \u0425\u043d.\nSpecifically, we use the popular full-reference image quality assessment (FR-IQA) met-\nric LPIPS [17] and the state-of-the-art no-reference image quality assessment (NR-IQA),\nQ-Align [18] and LIQE [19], in objective test. As FR-IQA, we use the image pairs in the\ndatasets as Xideal and XH, respectively. In contrast, we evaluate only the generated XH in\nNR-IQA without using Xideal as the reference. Here, we introduce NR-IQA metrics be-\ncause Xideal is often inaccessible or even does not exist in the real world. We also calculate\nthe average coding gains and perceptual quality enhancements using the Bjontegaard Delta-\nrate (BD-rate) metrics. The BD-LPIPS and BD-Q-Align metrics are computed similarly to\nBD-PSNR in [20] by substituting the distortion metric with LPIPS or Q-Align. A negative\nBD-rate value indicates rate savings at equivalent quality levels. Negative BD-LPIPS and\npositive BD-Q-Align values signify quality improvements at the same bit rate."}, {"title": "Rate-percetion Performance Result", "content": "Fig. 4 illustrates the rate-perception (RP) performance results on different tasks. Fig. 4\n(a)-(d) represents the results on dehazing, Raindrop removal, deraining, and deshadowing,"}, {"title": "Qualitative Comparison", "content": "Fig. 6 demonstrates the qualitative comparison of generated \u0425\u2081 images by LL-ICM\nagainst the other anchors. The compressed images are in similar bit rates. Two exemplary\nLL tasks are showcased to demonstrate the superior performance of our LL-ICM frame-\nwork, which are image deraining and raindrop removal. Obviously, our method preserves\nmore intricate details and texture in the reconstructed and enhanced images. For example,\nthe result for the deraining task displayed in Fig. 6 reveals that MLIC introduces signifi-\ncant rippling artifacts, while our method keeps the image content clean. Similarly, for the\nraindrop removal task, the results of other codecs lack details and are not clear, while our\nmethod provides the most satisfying image quality."}, {"title": "Conclusion", "content": "In this paper, we propose the first image compression framework for low level machine\nvision tasks, LL-ICM. LL-ICM aims to increase the performance of both compression\nand LL vision processing, addressing that many original images to be compressed may\nhave quality flaws and need to be enhanced by LL vision models after compression. Built\nupon a capable neural image codec MLIC, we incorporate a large vision-language model\ninto the proposed LL-ICM framework, extracting generalized features to support multiple\ndownstream LL vision tasks simultaneously. The compression and LL vision processing"}]}