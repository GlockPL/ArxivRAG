{"title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings*", "authors": ["Dawei Yan", "Pengcheng Li", "Yang Li", "Hao Chen", "Qingguo Chen", "Weihua Luo", "Wei Dong", "Qingsen Yan", "Haokui Zhang", "Chunhua Shen"], "abstract": "Currently, inspired by the success of vision-language models (VLMs), an increasing number of researchers are focusing on improving VLMs and have achieved promising results. However, most existing methods concentrate on optimizing the connector and enhancing the language model component, while neglecting improvements to the vision encoder itself. In contrast, we propose Text Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the vision encoder with text, offering a new and orthogonal optimization direction. Specifically, inspired by the purpose-driven logic inherent in human behavior, we use learnable latent embeddings as a bridge to analyze textual instruction and add the analysis results to the vision encoder as guidance, refining it. Subsequently, another set of latent embeddings extracts additional detailed text-guided information from high-resolution local patches as auxiliary information. Finally, with the guidance of text, the vision encoder can extract text-related features, similar to how humans focus on the most relevant parts of an image when considering a question. This results in generating better answers. Experiments on various datasets validate the effectiveness of the proposed method. Remarkably, without the need for additional training data, our propsoed method can bring more benefits to the baseline (LLaVA-1.5) compared with other concurrent methods. Furthermore, the proposed method consistently brings improvement in different settings. Code will be made available upon publication.", "sections": [{"title": "Introduction", "content": "By incorporating visual information into large language models (LLMs), visual language models (VLMs) build on the success of LLMs like ChatGPT (OpenAI 2023a) and Llama (Touvron et al. 2023), taking their capabilities a step further. VLMs are not limited to language-based dialogue with humans, they can also discuss the image content, answer questions related to the visual inputs, etc. Recently, centered around VLMs, researchers have conducted extensive work and have made significant progress (Wu et al. 2023; Zhang et al. 2024; Awadalla et al. 2023; Reid et al. 2024).\nCurrent adopted VLMs typically consist of three main components: vision encoder, large language model, and connector. The vision encoder, trained on vast amounts of image-text pairs using contrastive learning, encodes images into a shared space with text. Widely used examples include CLIP (Radford et al. 2021) and SigLIP (Zhai et al. 2023). LLMs such as Llama (Touvron et al. 2023), Vicuna (Chiang et al. 2023), Qwen (Bai et al. 2023a), and Yi (Young et al. 2024) have made significant strides in natural language processing tasks, paving the way for integrating vision with text in VLMs. Connector focuses on aligning visual and language features, serving as bridges between modalities.\nCorresponding to the main architecture of VLMs, current improvement methods primarily focus on optimizing the connector and enhancing the language model component among the three major components. For instance, BLIP2 (Li et al. 2023b) carefully design multiple loss functions for both contrastive and generative learning, which allows it to achieve precise cross-modal alignment through a multi-stage training process. MoE-LLaVA (Lin et al. 2024) incorporates a mixture of experts into the second feature forward network layer to enhance the connector component. DenseConnector (Yao et al. 2024) uses dense connections to merge features from various levels, providing more visual information to the LLM. ImageBind-LLM (Han et al. 2023) transforms image features using a binding network and then integrates these transformed features with the word tokens of the LLM. Besides improving the model structure, increasing the amount of data is a commonly employed strategy."}, {"title": "Related Work", "content": "Visual language models primarily consist of a visual encoder and a large language model, representing prominent architectures in the multimodal domain. Researchers have proposed numerous architectures (Li et al. 2023a; Zhu et al. 2024; Chen et al. 2023c) for integrating visual features into advanced LLM inference pipelines. Llama-Adapter (Zhang et al. 2023) proposes to generate language answer with taking the image input as condition. Flamingo (Alayrac et al. 2022) and LLaVA (Liu et al. 2024c) blend visual tokens with text as inputs to LLM, differing in that Flamingo employs gating mechanisms to inject encoded visual features into LLMs, while LLaVA directly concatenates visual and textual features at input. Complementarily, the availability of high-quality image-text pairs for VLM training is crucial. Several methods use Chat-GPT (OpenAI 2023a) and GPT-4 (OpenAI 2023b) to construct large-scale, high-quality datasets (Zhu et al. 2023; Liu et al. 2024c; Zhao et al. 2023).\nInspired by the compact structure and outstanding performance of LLaVA-1.5 (Liu et al. 2024a), we use LLaVA-1.5 as our baseline and incorporate a text-guided approach, similar to other LLaVA-based methods. Unlike most of these methods, which create additional datasets to enhance performance, our improvements focus entirely on the model architecture itself. This approach can further enhance the performance of methods that rely on extra datasets. The results in fifth and sixth lines of Table 1 verified this point."}, {"title": "Image-Text Alignment", "content": "Align the visual and text information in high semantic level is the base for building VLMs. Centered around this problem, researchers have done extensive work. Previous researchers have typically employed contrastive learning across modalities and autoregressive learning for text. CLIP (Radford et al. 2021) and SigLIP (Zhai et al. 2023) trained encoders on massive datasets, laying foundational work for aligning visual and textual modalities and significantly advancing subsequent VLM developments. BLIP (Li et al. 2022) meticulously design multiple loss functions for contrastive and generative learning, achieving refined cross-modal alignment through multi-stage training. BLIP-2 (Li et al. 2023b) adopts a Q-former structure, interacting with the visual modality using learnable query vectors before merging with the text modality. Many LLaVA-like approaches use simple MLPs for modal alignment, with subsequent works like MobileVLM V2 (Chu et al. 2024).\nBoth image-text alignment methods and our proposed TG-LLaVA recognize the importance of integrating textual and visual information. However, while these methods focus on bridging different modalities, our approach leverages the textual modality to guide and optimize the visual modality. This alignment makes the operation of VLMs more consistent with the purpose-driven logic of human behavior in real-world scenarios."}, {"title": "Visual encoder in VLMS", "content": "To enable the LLM to extract more information from the input visual image, various strategies have been proposed for utilizing visual features. DenseConnector (Yao et al. 2024) employs dense connections to link visual features across different levels, feeding the combined features into a connector. TokenPacker (Li et al. 2024a) merges visual features from the high-resolution branch with those from the low-resolution branch to generate condensed visual tokens. Idefics2 (Lauren\u00e7on et al. 2024) compresses visual features using a perceiver structure, significantly reducing the number of visual tokens compared to other approaches. Approaches like Mini-Gemini (Li et al. 2024b), LLaVA-Next (Liu et al. 2024b), Qwen-VL (Bai et al. 2023b), and InterVLM (Dong et al. 2024) leverage high-resolution images to capture finer visual feature details. ImageBind-LLM (Han et al. 2023) and Llama3.1 (Meta AI 2024b) explore injecting visual modality features into LLMs, with the former using trainable gating modules to add visual features to word tokens, and the latter introducing visual information across different layers of LLM through periodic cross-attention.\nUnlike methods that focus on better utilizing existing visual features, our proposed TG-LLaVA aims to enhance the visual features themselves by using textual guidance. In contrast to ImageBind-LLM and Llama3.1, which incorporate image features into the LLM component, our approach integrates text into the visual encoder."}, {"title": "Method", "content": "In this section, we first review the classic VLM architecture, using LLaVA (Liu et al. 2024c) as a representative example, to provide an overview of the VLM paradigm. Following this, we present a detailed explanation of the proposed TG-LLaVA architecture, focusing on the implementation of the two text-guided modules, text-guided visual feature optimization mask module and text-guided detail perceiver module."}, {"title": "A Revisit of VLMs", "content": "Taking LLaVA (Liu et al. 2024c) as an example, the primary goal of VLMs is to effectively harness the capabilities of pre-trained LLM and visual model. The three key components of such framework can be defined as follows:\n1) Visual encoder $E_v$, typically utilizing a pre-trained vision transformer like CLIP, is designed to partition the input image $I \\in R^{H\\times W\\times C}$ into several patches with equal size and further encode them into visual features $F_i \\in R^{N\\times D}$. Here, H and W represent the size of the input image, C denotes the number of channels, N corresponds to the number of patches in the output features, and D represents the feature dimension of each encoded patch. When the patch size is P, $N = HW/P^2$. 2) Connector C (also referred as Projector) consists of two linear layers with a GELU activation function in between. Its purpose is to map visual features into the embedding space of the LLM, converting $F_i$ into visual tokens $T_v$. 3) LLM L employs a tokenizer and text embedding module to sequentially transform textual data into token IDs and their corresponding embedded tokens $T_t$, effectively converting the language into the feature space of its input. Within the VLM architecture, these textual tokens $T_t$ are concatenated with the aligned visual tokens $T_v$ processed by the connector, forming the input for the LLM to carry out subsequent predictions. For a sequence of length L, the probability of VLM predicting the target answer tokens $T_a = \\{t_i\\}_{i=1}^L$ can be formalized as:\n$p(T_a | T_v, T_t) = \\prod_{i=1}^{L} P_\\theta (t_i | T_v, T_t, <t_i, T_a, <t_i)$, (1)\nwhere $\\theta$ represents all the trainable parameters in the VLM."}, {"title": "Text Guided LLaVA", "content": "Inspired by the reasoning logic humans use in visual question answering scenarios, we design TG-LLaVA, a novel approach that optimizes visual features to align the inference process of VLM more closely with purpose-driven human behavior, thereby further enhancing the capabilities of VLMs. As illustrated in Figure 2, TG-LLaVA primarily consists of two components: Text-guided Visual Feature Optimization Mask (TG-FOM) and Text-guided Detail Perceiver (TG-DP). The former uses learnable latents to parse the global information from textual instructions and attaches it as a mask to the output of visual encoder, optimizing features based on textual instructions. The latter employs another set of latents, first interacting with the detailed information from textual instructions, and then extracting fine-grained details from high-resolution patches of the input image based on these instructions. These details are concatenated with the original features, further refining the visual modality input of VLM. The specifics of this approach will be elaborated in the following sections."}, {"title": "Text-Guided Visual Feature Optimization Mask", "content": "In current VLMs, the visual representations typically originate solely from the final layer features of the visual encoder $E_v$. Features obtained through this pipeline encompass the global information of the input image I. However, the corresponding textual instructions often focus on specific local targets within the image. As a result, the information related to these focal targets is easily compromised when confronted with irrelevant or even contradictory information, leading to distorted judgments by the VLM. To address this issue, we design TG-FOM module to optimize visual features based on textual instructions, thereby endowing VLMs with the advantage of purpose-driven human behavior. Figure 3 illustrates the specific framework of the FOM module. We begin by initializing a set of learnable latent embeddings $L_m$ that are of the same number as visual tokens. The purpose of these latents is to extract linguistic information from the textual instructions and add it as a mask to the original features. Here, we design a single-layer Q-former to parse semantic information from textual instructions, serving as a bridge between global text and visual features. In this structure, the cross-attention layers incorporate the pooled textual instruction features $F_t$ encoded by CLIP text encoder $E_t$ as Key and Value for interaction with Query $L_m$, and the final output is a mask generated based on the textual information, which is then applied to the visual features. We additionally introduce a zero-initialized linear layer to ensure that the optimization of the original visual features remains a gradual process. This process can be formalized as:\n$M_t = Q(L_m, F_t) = FFN(A_{cross}(A_{self}(L_m), F_t))$,\n$F_i' = F_i + Z(M_t)$, (2)\nwhere $M_t$ represents the mask obtained by extracting semantic information from the textual instructions via the learnable $L_m$, $A_{cross}$ and $A_{self}$ denote the cross-attention and self-attention modules, respectively, FFN represents the feed-forward neural network, Z represents the zero-initialized linear layer used as a buffer during feature addition, and $F_i'$ represents the visual features optimized by text guidance."}, {"title": "Text-Guided Detail Perceiver", "content": "When observing images, in addition to selecting focal points based on the instruction, humans can also automatically adjust their focus to obtain more detailed information. Following this idea, we design TG-DP, which is responsible for capturing instruction relevant details.\nAs shown in Figure 2, we scale up the original image I to preserve more details, then divide it into patches that match the size of the original image. This design ensures that we can extract all visual features with a single call to the visual encoder. After obtaining the visual features of these patches, we add positional embeddings and a learnable MLP layer to recover the spatial structure information that was disrupted during the division operation, getting corrected visual features $F_h$. So far, the visual tokens containing detailed information are ready. Along with the learnable latent embeddings $L_h$ and the fine-grained textual instruction features $F_t^g$, these visual tokens will be fed inoto the TG-DP module, where they will be selected and integrated according to the guidance of the text.\nAs shown in Figure 4, we set up $L_h$ to interact with $F_t^g$ output by the text encoder $E_t$. Here, the number of $L_h$ is much smaller than the number of the original visual tokens, ensuring that the visual tokens input to the LLM do not increase significantly, thus maintaining inference efficiency. Ablation studies demonstrate that this compression does not negatively impact the final results.\nThe proposed TG-DP module consists of two perception layers:\n\u2022 The first perception layer is responsible for parsing fine-grained text to generate text guidance tokens. It receives $L_h$ and $F_t^g$, maintaining $L_h$ as the Query and $F_t^g$ as the Key and Value, with the distinction that $F_t^g$ is concatenated with $L_h$.\n\u2022 The second layer is in charge of generating detail perceiver tokens with the guidance of fine-grained text. In the second layer, the Key and Value are replaced by $F_h$, using textual instruction features parsed through the first layer as Query for a second interaction. The output of the second layer is the compressed visual tokens $F_h'$.\nDue to the significant difference between the feature space of $F_h'$ and the original VLM visual features, we design a dedicated connector $C_h$ for $F_h'$. The entire process can be formalized as:\n$F_l^1 = BN_1(A_{Cross}(L_h, CAT(F_t^g, L_h)))$,\n$F_h' = C_h(BN_2(A_{Cross}(F_l^1, CAT(F_t^g, F_h))))$, (3)\nwhere $L_i$ denotes the $i^{th}$ layer within DP module, $F_l^1$ is the output of first layer and CAT represents the concatenation operation. $BN^l(i \\in (1,2))$ denotes Bind Network which can be formalized as:\n$BN(X) = X + (XW_{Up}^2 \\cdot SiLU(XW_{Up}^1)) W_{Down}^3$. (4)\nOverall\nAt this point, with the guidance of input text, we have obtained the optimized visual features $F_i'$, as well as the detail perceiver tokens $F_h'$. We then concatenate the features obtained from the original VLM connector C with $F_h'$, which together form the final visual tokens $T_{in}$ input for the VLM. The final prediction process of VLM can be represented as follows:\n$T_{in} = CAT(C(F_i'), F_h')$,\n$p (T_a | T_{in}, T_t) = \\prod_{i=1}^{L} P_\\theta (t_i | T_{in}, T_t, <t_i, T_a, <t_i)$. (5)"}, {"title": "Experiment", "content": "In this section, we first present the detailed experimental setup of our study. We then enumerate the improvements brought by our proposed TG-LLaVA over the baseline across multiple evaluation metrics, and compare our method with several state-of-the-art (SoTA) approaches under various configurations. Specifically, we visualize the attention map to demonstrate the efficacy of proposed TG-LLaVA. Finally, we conduct ablation studies and provide an analysis of the results."}, {"title": "Experimental settings", "content": "Implementation Details We implement the proposed improvement strategy on top of LLaVA-1.5 (Liu et al. 2024a), whose general applicability in the VLM field facilitates the validation of our method's versatility. Specifically, we maintain consistency with LLaVA-1.5 by employing CLIP-ViT-L/14-336px as the visual encoder. To further validate the generalizability of our proposed method, we also incorporate SigLIP-SO400m-patch14-384, another leading choice, for comparative analysis. In terms of LLM, we compare our method against the baseline using Vicuna-7/13B and extend our approach to Llama3-8B (Meta AI 2024a) and Qwen2-7B (Yang et al. 2024), thereby demonstrating the versatility of our method. For training configurations, we adhere strictly to the settings outlined in the original LLaVA-1.5 paper to ensure fairness, with learning rates of 1e-3 and 2e-5 for pre-training and instruction fine-tuning phases, respectively, and maintaining batch sizes of 256 and 128. DP module introduces 64 additional visual tokens. The training process for TG-LLaVA utilizes the PyTorch framework and employs 8 H100-80G GPUs.\nDatasets Focusing on proposing a novel optimization method for the VLM framework, we do not incorporate any additional data beyond the LLaVA-1.5 open-source dataset (Liu et al. 2024a), which includes 558K image captions for pre-training and 665K conversations for instruction tuning. We also apply our proposed method to the Mini-Gemini dataset (Reid et al. 2024), which consists of 1.2M + 1.5M data, to further highlight the superiority of our approach. For evaluation, we conduct extensive experiments and report results on widely-adopted VLM benchmarks using the VLMEvalKit (Duan et al. 2024) platform to provide robust and comprehensive performance validation for the proposed TG-LLaVA. The evaluation datasets include: MMBench (MMB) (Liu et al. 2023a), MMS (MMStar) (Chen et al. 2024), MMMU (Yue et al. 2024), MV"}, {"title": "Genuine Improvement Over the Baseline", "content": "In Table 1, we present the performance improvements of the proposed method across various configurations compared to the baseline. According to the experimental results, we can draw several phenomenons:\n\u2022 The proposed text-guided strategy demonstrates substantial improvements over the baseline. Compared with the original LLaVA-1.5, TG-LLaVA achieve much better performance. As shown in the first four rows. our method leads on the majority of evaluation datasets. It is noteworthy that TG-LLaVA demonstrates an average improvement of 1.5% over the original LLaVA-1.5 across ten datasets when using Vicuna-7B, highlighting the method's significant value. When juxtaposed with the baseline LLaVA-1.5 Vicuna-7B model, we enhance performance metrics by +2.2% on MMBench, +2.4% on both MMStar and MMMU, and +3.2% on LLaVABenchs, respectively. For LLaVA-1.5 with Vicuna-13B, we also achieve an average performance improvement of 1%. Specifically, we see a +1.6% gain on MMStar, a +2.0% gain on MMMU, and a +3.2% gain on MME. These impressive results further validate the contribution of the proposed TG-LLaVA architecture to visual feature optimization, highlighting the favorable impact of our method.\n\u2022 The proposed TG-FOM and TG-DP modules can be universally applied as a modular plugnin to mainstream VLM frameworks. As shown in the rest part of Table 1, we further validate the versatility of our proposed method under various settings. We replace CLIP with SigLIP and substitute Vicuna with Llama3 and Qwen2 on top of the original LLaVA-1.5 framework. We compare these settings with our method as the baseline. The results in Table 1 confirm that our method continues to maintain a leading advantage across most datasets, demonstrating that the proposed TG-LLaVA exhibits excellent generalizability and possesses strong potential for adaptation to a wide range of VLM architectures."}, {"title": "Comparison with other LLaVA-based methods", "content": "In Table 2, we compare the proposed proposed TG-LLaVA with other concurrent works which also take LLaVA as baseline. The methods we include for comparison are Seeing the Image (Xiao et al. 2024), TokenPacker(Li et al. 2024a), and DenseConnector(Yao et al. 2024). Since these comparison methods are relatively new and have not been evaluated on the OpenCompass leaderboard, we employ the evaluation scripts from LLaVA-1.5 to maintain a fair and consistent framework for our comparisons."}, {"title": "Quantitative Comparison with SoTAs", "content": "We further compare our method with several leading approaches. The methods included in the comparison are MiniGPT4 (Zhu et al. 2023), Qwen-VL(Bai et al. 2023b), VisualGLM(GLM et al. 2024), PandaGPT(Su et al. 2023), mPLUG-Owl2(Ye et al. 2023), Emu2-chat(Sun et al. 2024), Yi-VL(Young et al. 2024) and ShareGPT-4V(Chen et al. 2023a). Table 3 presents the performance comparison across multiple benchmarks.\nRemarkably, despite relying solely on settings from LLaVA-1.5, our TG-LLaVA achieves performance that matches or surpasses the benchmarks set by leading SoTA methods, with a comparatively smaller volume of pre-training and instruction fine-tuning data."}, {"title": "Qualitative Analysis via Visualization", "content": "To demonstrate the optimization effect of the proposed method, we visualize the attention maps between the visual features and text instructions in both the baseline model and our model. These visualizations provide insights into how the proposed visual feature optimization module operates. We aggregate the attention scores between image tokens and textual instruction tokens across all layers to compute the results. As shown in Figure 5, the TG-LLaVA architecture push the model to focus on regions highlighted by the textual instructions, assigning greater attention weights to them."}, {"title": "Ablation Studies", "content": "We further conduct in-depth ablation studies to analyze the effectiveness of each component of our approach. Results are listed in Table 4. By sequentially introducing the FOM and DP modules, we observe significant improvements in model performance, underscoring the effectiveness of our proposed visual feature optimization algorithm. Additionally, we conduct experiments on the number of additional visual tokens introduced by the DP module. The results show that introducing too few tokens yields suboptimal performance gains, while introducing too many tokens can actually harm performance. Therefore, we choose a balanced configuration to achieve optimal performance."}, {"title": "Conclusion", "content": "In this paper, we introduce TG-LLaVA, an innovative VLM optimization technique that guides the vision encoder using text. By emulating human-like purpose-driven logic, we leverage learnable embeddings to analyze text and enhance the vision encoder. Our experiments reveal that TG-LLaVA outperforms similar methods and is adaptable to various frameworks, consistently yielding improvements. This text-guided enhancement of the visual encoder opens up a new pathway for advancing VLMs. For future work, we aim to further refine the visual feature extraction process guided by text to achieve even better performance."}, {"title": "Experiment Settings", "content": "TG-LLaVA employs the same set of hyperparameters as LLaVA-1.5. It is important to note that both the TG-FOM and TG-DP modules in the proposed architecture are trained from scratch without any form of pre-trained parameters. Table 5 summarizes the training hyperparameters for both the first phase of visual-language alignment pre-training and the second phase of visual instruction tuning for TG-LLaVA."}, {"title": "How to guide the vision encoder", "content": "As discussed in main paper, our objective is to explore how to endow VLMs with purpose-driven reasoning capabilities akin to human logic to achieve superior model performance. The key lies in how to utilize textual instructions to guide the VLMs, enabling a \"focus\" on the visual modality.\nIn SAM-Adapter (Chen et al. 2023b), the authors use low-rank parameter matrices to handle task-specific features (such as high-frequency components obtained from Fast Fourier transforms of images to be segmented) and inject these features layer by layer into SAM's (Kirillov et al. 2023) visual encoder (a Vision Transformer structure). This approach enables the transition of SAM from general domains to medical image segmentation. Inspired by this, we attempt a similar approach by using TG-FOM module to inject textual instruction information into the shallow layers of the VLM's visual encoder. As shown in Table 6, we experiment with various strategies: inserting the output of TGFOM module at the head, middle, and tail of the visual encoder, as well as combinations of these positions. We use MME dataset metric as the evaluation standard and CLIP-L with 24 layers as an example, insertion at the head refers to adding features between the Patch Embedding and the first layer of the Transformer Encoder. Insertion at the middle occurs between the 12th and 13th layers, while insertion at the tail refers to applying it at the output. However, the results are disappointing. Except for the final approach, which inserts the module at the encoder's output, the other shallowlayer modifications lead to noticeable degradation in performance. We consider this is due to the fact that in the shallow layers of the visual encoder, the visual features are not yet aligned with the textual features in a unified space. Thus, even with a zero-initialized linear layer as a buffer, the outcome still results in negative gains. Further exploration is required to determine how to effectively integrate textual instruction information during the visual encoding process."}, {"title": "Model Zoo", "content": "We further explore the upper limits of TG-LLaVA by finetuning on Qwen2-7B model with a larger dataset, utilizing the training data introduced by Ovis (Lu et al. 2024). The Ovis training data comprises open-source datasets and a small portion of internal datasets, categorized into three types: visual captions, visual descriptions, and multimodal instructions, used for the three-stage training process. We use the 10M visual captions subset as the training data for the second stage in training, with the results displayed alongside other variants in Table 7. As shown, underpinned by a robust dataset, TG-LLaVA demonstrates a remarkable average increase of 8% across ten datasets. Notably, it achieves a 20.2% improvement on MathVista and a 12.2% increase on ScienceQA, underscoring TG-LLaVA's significant potential and the crucial role that high-quality data plays in enhancing VLM performance."}, {"title": "Training Cost Analysis", "content": "The proposed TG-LLaVA architecture involves multiple attention computations and image patch segmentation, leading to increased computational resource consumption during training. We record the training time for both the baseline LLaVA-1.5 and TG-LLaVA across two stages, with experiments conducted on 8 H100 GPUs. As shown in Table 8, TG-LLaVA incurs approximately 10% more time compared to the baseline. The additional trainable parameters introduced by TG-LLaVA amount to 0.18B compared to LLaVA-1.5 with Vicuna-13B. Currently, the implementation involves processing multiple detailed patches sequentially through the visual encoder, but optimizing this to a batch format could further reduce training delays."}, {"title": "Qualitative Results", "content": "We validate the inference capabilities of TG-LLaVA across various tasks involving understanding and reasoning to demonstrate its effectiveness in practical scenarios. As shown in Figure 6, TG-LLaVA handles complex visual-language reasoning problems in diverse contexts, including image perception and comprehension, mathematics and computation, OCR recognition, and other tasks requiring relevant prior knowledge."}, {"title": "Limitation and Future Work", "content": "To achieve text-guided visual feature optimization, TGLLaVA inevitably introduces additional computational resource consumption, including extended training time and increased parameter count, beyond the original LLaVA-1.5. A viable improvement approach is to compress the number of visual tokens, such as TokenPacker (Li et al. 2024a). Compared to the total parameters of the original VLM, the increased parameter count is relatively small, and we believe that these trade-offs are reasonable without increasing the training data. In addition, our method can be used as a plugin module to further improve the performance of other VLM models with similar architectures. Future work will explore methods to further reduce computational resource consumption while refining text-guided visual enhancement and improving guidance strategies."}]}