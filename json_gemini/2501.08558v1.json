{"title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation", "authors": ["Yiran Tao", "Jehan Yang", "Dan Ding", "Zackory Erickson"], "abstract": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learning-based methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time.", "sections": [{"title": "I. INTRODUCTION", "content": "One of the key challenges in robotic teleoperation systems is mapping a controller's limited degrees of freedom (DOF) to a robot's higher DoF, especially for high-DoF robotic manipulators. This challenge is particularly prominent in assistive applications, where users with motor impairments rely on low-DoF assistive devices such as tongue-based joysticks [1], [2], head orientation systems [3], [4], eye gaze controls [5], [6], and sip-and-puff systems [7], [8] to perform daily tasks.\nSimple controllers, such as 2-DoF joysticks, typically require users to switch between different control modes, where each mode defines a specific mapping of the joystick's four movement directions (up, down, left, right) to specific robot actions, such as translation, rotation, or gripper control. This process becomes cumbersome when users must frequently switch modes to complete a long-horizon, multi-stage task, leading to inefficiency and cognitive strain. For instance, in teleoperating a robotic arm to place a book on a shelf (as shown in the right two subfigures in Fig. 1), the user must switch among different modes for each subtask: translating and rotating the robot end effector to align with the book, closing the gripper to grasp the book, translating and rotating again to align the book with the shelf, and finally placing the book. These frequent mode switches interrupt the workflow, forcing the user to repeatedly recall and select the correct mode, which increases frustration, cognitive load, and reduces task efficiency [9]\u2013[11].\nTo address this, automatic mode switching aims to handle these transitions seamlessly, allowing users to focus on the task itself rather than the control mechanism. Prior works have explored automating mode switching in teleoperation and assistive robotics using heuristic-based approaches [12], [13], reinforcement learning [14], [15], and optimization tech-"}, {"title": "II. RELATED WORKS", "content": "Automatic mode switching has been explored in several prior works to reduce the cognitive load and inefficiencies associated with manual mode switching. For example, Herlant et al. [9] proposed a time-optimal model that uses time as a cost metric, utilizing Dijkstra's algorithm to predict when the robot should automatically change modes. However, as they pointed out in their own paper, algorithms like Dijkstra's, which compute the optimal cost-to-go, incur combinatorial computational costs relative to the size of the search space, making them impractical for high-dimensional systems like high-DoF robotic arms in real-world applications.\nMore recently, Gopinath et al. [12] proposed an approach that performs mode switching by placing the user in control modes that maximally disambiguate between various goals in the scene. However, this approach is dependent on effective human-intent recognition, which remains an ill-defined and challenging problem. Quere et al. [13] took a different approach, dividing tasks into multiple phases, each with different motion constraints and input mappings. In order to do so, this approach requires extensive hand-engineering to define the task phases, constraints, and mappings, limiting its scalability and flexibility across different tasks.\nIn addition, Pilarski et al. [14] and Kizilkaya et al. [15] leveraged reinforcement learning (RL) for automatic mode switching. These methods, however, require substantial training before the RL agent can be deployed, limiting their real-world applicability. Kizilkaya et al. [15] additionally rely on a real-world dataset for training, which further restricts their generalizability to new environments or tasks.\nBeyond these limitations, most existing automatic mode switching methods are task-specific, requiring tailored demonstrations or hand-engineered rules, which limits their scalability and generalizability to novel tasks. In contrast, our framework LAMS, leverages the commonsense reasoning capabilities of LLMs to eliminate the need for task-specific data or predefined heuristics, allowing it to generalize across tasks."}, {"title": "B. Learned Latent Action Models", "content": "Another related line of research addressing the challenge of mapping a controller's limited DoF to a robot's higher DoF utilizes learned latent action models [22]\u2013[26].\nThese approaches train auto-encoders to tackle the challenge: During training, the encoder compresses high-dimensional robot actions into a latent space matching the low-DoF controller, while the decoder reconstructs the original high-dimensional actions. During deployment, the user's control inputs are fed into the decoder to generate the corresponding robot actions.\nWhile these methods provide an alternative to mode switching in addressing the teleoperation challenge, they typically rely on extensive training datasets with substantial expert demonstrations and, in some cases, user-annotation processes to ensure intuitive control [22]. This data collection can be costly, hard to scale, and challenging to adapt to new tasks and environments. Notably, one of these works attempted to minimize the need for human demonstrations [26], but their user study revealed that \"users were confused when the unsupervised robot learned unexpected behaviors\u201d.\nIn contrast, our approach reduces the cost and complexity of data collection while maintaining flexibility and scalability across different teleoperation tasks. Moreover, it preserves user intuitiveness, making it a preferred solution over alternative mode switching methods, as demonstrated in our user study."}, {"title": "III. LAMS: LLM-DRIVEN AUTOMATIC MODE SWITCHING", "content": "We consider the scenario where a human operator teleoperates a high-DoF robot arm to perform a manipulation task with a low-DoF controller. This task is modeled as a sequential decision-making process, defined by tuple $(S, A_u, A_r, T)$, where $s_t \\in S$ denotes the task state at time step t, $a_{u,t} \\in A_u \\subseteq \\mathbb{R}^m$ denotes the user action, $a_{r,t} \\in A_r \\subseteq \\mathbb{R}^n$ denotes the robot action, with $m \\ll n$. T: $S \\times A_u \\rightarrow S$ is an unobserved transition function.\nIn our experiments, the robot's action space is 7-dimensional (i.e., n = 7), represented by the vector:\n$a_{r,t} = (\\Delta x_t, \\Delta y_t, \\Delta z_t, \\Delta \\text{roll}_t, \\Delta \\text{pitch}_t, \\Delta \\text{yaw}_t, \\Delta \\text{gripper}_t)$ where $\\Delta x_t$, $\\Delta y_t$, $\\Delta z_t$ are the deltas in the Cartesian coordinates, $\\Delta \\text{roll}_t$, $\\Delta \\text{pitch}_t$, $\\Delta \\text{yaw}_t$ are the deltas in the Euler angles, and $\\Delta \\text{gripper}_t$ is the delta in the gripper's opening. We use a joystick with two degrees of freedom (m = 2) as the human control interface. The user action $a_{u,t}$ represents joystick movements, which is a 2-dimensional vector:\n$a_{u,t} = (x_{u,t}, y_{u,t})$ where $x_{u,t}$ represents the joystick's lateral movement, and $y_{u,t}$ represents the joystick's longitudinal movement.\nOur goal is to define a function $F(s_t, a_{u,t}): S \\times A_u \\rightarrow A_r$ that transforms the task state and user input into a robot action that is optimal for the task. Since we are focusing on mode-switching, we aim to generate an effective mapping M that aligns each joystick movement direction with a corresponding robot action direction, as illustrated in Fig. 3. Specifically, in our setting, joystick movement directions map to the following robot action directions, depending on the current control mode:\n$\\bullet$ Dup: {move forward, move up, pitch up, open gripper},\n$\\bullet$ Ddown: {move backward, move down, pitch down, close gripper},\n$\\bullet$ Dleft: {move left, yaw left, roll left},\n$\\bullet$ Dright: {move right, yaw right, roll right}.\nRobot velocity is proportional to the magnitude of the user's action $a_{u,t}$: Each element of $a_{u,t}$ is scaled by a constant"}, {"title": "B. Generating LLM Inputs", "content": "At each time step t, when mode switching is required, the input to the LLM is a language instruction $l_t = [l_{\\text{pre}}, l_{\\text{rule}}, l_{\\text{pose}}]$, structured as three main components.\nlpre is a prompt prefix that provides context such as objectives and output format for the LLM. The exact lpre we use can be found in Appendix F.1.\nIrule contains rules that guide the LLM's mode-switching prediction. These rules are derived from user-generated mode-switching examples. Itule is initially empty, incrementally growing as the user interacts with LAMS. Examples of lrule can be found in Appendix F.2. We discuss the details of how Irule is augmented through user interactions in Section III-D.\nThe final component, lpose, provides a description of the current pose of the robot arm and task-relevant objects. For the robot arm, we encode its pose as a dictionary containing the end-effector's Cartesian coordinates and Euler angles, along with the gripper status. For task objects, we describe the relative position of each object with respect to the robot arm's end-effector across six dimensions using natural language statements. We provide additional details on the construction of lpose, along with an instance from our experiments, in Appendix F.3. Our ablation studies (Section IV-B) demonstrate that the natural language grounding of object states is more effective than numeric representations."}, {"title": "C. Processing LLM Outputs for Mode Switching", "content": "When a mode switch is required, which in our experiments occurs either at the beginning of the task or when the user pauses for 1.5 seconds to signal a mode switch need, the LLM is prompted to predict the most likely action direction from each of the action direction groups: Dup, Ddown, Dleft, Dright, resulting in four predicted action directions.\nA naive mode-switching approach would be to directly map the joystick's four movement directions to the LLM's natural language response. However, in certain phases of a task, more than one action may be equally desired by the user. The LLM's response under such circumstances may appear ineffective to the user if the user has already executed one action from Di and is expecting the system to switch modes to a different action mapping.\nTo address this, we leverage an essential mechanism of LLMs: rather than directly generating a single response, LLMs produce a probability distribution over possible next words, denoted as p(wk|W<k), where wk is the word generated at the kth position in the response. Utilizing this probability distribution, we can assess the likelihood of each robot action in Di. Formally, for each robot action with natural language representation di,j in group Di, where j denotes the index of the action within Di, we compute $p(d_{i,j}|o_i)$, where oi represents the LLM's response preceding di,j.\nFor each group, if the robot action with the largest $p(d_{i,j}|o_i)$, i.e., $d^* = \\arg \\max_{d_{i,j}} p(d_{i,j}|o_i)$, has just been executed before the current mode-switching call, we check whether the probability of the second most likely action,"}, {"title": "D. Incremental Improvement via User-Generated Examples", "content": "While LAMS is able to perform useful automatic mode-switching for an unseen task even in the first interaction, it can encounter errors due to limited task knowledge. To address this, we design LAMS to incrementally improve as the user interacts with the system. This is achieved by incorporating user-generated mode-switching examples to augment the rule prompt rule defined in Section III-B.\nParticularly, during task execution, a Graphical User In- terface (GUI) continuously displays the current mode Mt, which shows the four robot action directions mapped to each joystick movement. If the user is dissatisfied with Mt, they can manually switch to another mode $M'$ (details on how manual switching is performed in our real-world experiments are provided in Section IV). This manual mode switch can be expressed as $M_t' = \\Delta_t(M_t)$, which is then converted into natural language format $l_{\\Delta_t (M_t)}$. For example, an instance of $l_{\\Delta_t (M_t)}$ initiated by the user in our experiments was:\n{ \u201cJoystick Up\u201d: \u201cPitch up\u201d}\nThis means that the user switched the mapping of \u201cJoystick Up\u201d from a less preferred action to \u201cPitch Up\u201d.\nTo facilitate LAMS\u2019 incremental improvement, we maintain an example list E for each task. Each time a $M_t' = \\Delta_t(M_t)$ is made, lpose (defined in Section III-B) and $l_{\\Delta_t (M_t)}$ form an example le in natural language format, which is added to E (see Appendix F.4 for an instance of lt).\nInstead of directly incorporating these examples into the LLM\u2019s inputs, our ablation studies (Section IV-B) show that summarizing them into mode-switching guiding rules leads to more effective and robust improvements for LAMS. To achieve this, besides the example list E, we also maintain a rule list R for each task, which starts empty and grows over time as the user performs the task. With E and R, every time a new example is added to E, all examples are shuffled and fed to a separate LLM (distinct from the one used for automatic mode switching) along with a prompt prefix lpre-rule, to generate rules that guide future mode-switching predictions (the exact lpre-rule we use can be found in Appendix F.5). This LLM autonomously generates a variable number of rules $\\{l_k\\}_{k=1}^N$, $N \\ge 0$, where N is the number of generated rules. For instance, one of the rules generated in our experiments was (see Appendix F.2 for more rules generated in our experiments):"}, {"title": "IV. EXPERIMENTS", "content": "All experiments were conducted using a Kinova Gen3 robotic arm, with users controlling the robot via an Xbox controller, as illustrated in Fig. 3. Specifically, the left joystick on the controller is used to generate user actions aut. The mapping between joystick movement directions and robot action directions is either determined by an automatic mode-switching method (e.g. LAMS) or manually adjusted by the user when they are dissatisfied with the automatic switch.\nAs shown in Fig. 3, to perform a manual mode switch (M) in LAMS, the user presses one of the directional buttons on the Xbox controller's D-pad. This action updates the mapping for the corresponding joystick direction without affecting the others. For example, pressing \u201cup\u201d on the D-pad changes the mapping for pushing up on the joystick, while leaving the mappings for other directions unchanged. This manual mode-switching process enables the user to correct errors made by automatic mode switching methods to ensure task completion, while also providing examples that help LAMS improve incrementally over time.\nIn both the ablation study and the user study, the primary evaluation metric is the number of manual mode switches made by the user, with fewer switches indicating more effective automatic mode switching.\nWe evaluated LAMS on two complex, multi-stage tasks:\n1) Water Pouring: Open the cap of a bottle, then pick up the bottle, and pour its contents into a bowl.\n2) Book Storage: Pick up a book lying on the table with its spine facing up, then put it into a bookshelf."}, {"title": "B. Ablation Study", "content": "Prior to conducting the user study, we performed an ablation study on the water pouring task to evaluate key design choices in LAMS. The following alternative designs were tested to assess their impact on system performance:\n$\\bullet$ Num-State: Using relative numeric values to represent the spatial relationship between objects and robot end effector in loose, rather than natural language descriptions.\n$\\bullet$ Top-Action: Always switching to the most likely action predicted in the LLM's natural language response, in contrast to mode switching over probability distributions as discussed in Section III-C.\n$\\bullet$ Direct-Examples: Directly using user-generated mode-switching examples (E) in the LLM input lt, instead of rules (R) generated from these examples.\nFor the ablation study, one researcher ran five experiments for each model on the water pouring task. Each experiment consists of three trials with different task layouts.\nThe average number of manual mode switches of different methods are shown in Fig. 4."}, {"title": "C. User Study", "content": "We conducted a user study with 10 participants (8 males, 2 females) aged 21 to 25 (mean age: 23.7), under a university-approved human subjects safety protocol. Participants reported an average experience level of 2 out of 7 with robotic arms and teleoperation, and 3.5 out of 7 with game controllers, where 1 represents no experience and 7 represents extensive experience. We test the following two hypotheses:\n$\\bullet$ H1: LAMS enables users to complete complex multi-stage tasks with fewer manual mode switches, and is preferred over alternative mode-switching methods.\n$\\bullet$ H2: LAMS improves its automatic mode-switching abil-ity over time as a user repeatedly performs a task, in contrast to a static LLM-based method.\nTo support these hypotheses, we compared LAMS with three baseline methods:\n$\\bullet$ Grouped Mapping: This is a common mode-switching method in robotic arm applications [3], [4], [9], [10], [12], [22], [25], [40], where robot actions are divided into predefined groups. Additional information on this method is provided in Appendix A\n$\\bullet$ Hand-Engineered Heuristic Switching: Inspired by [13], in this method, each task is pre-analyzed and manually divided into distinct subtasks, with optimal joystick mappings assigned for each subtask. Switching between subtasks is triggered by the robot\u2019s kinematic states, such as opening or closing of the gripper. When the task transitions to a new subtask, the pre-assigned mappings automatically switch accordingly.\n$\\bullet$ Static LLM-Based Mode Switching (No Incremental Improvements): This method follows the LAMS pipeline but excludes ltule from lt, meaning it remains static and does not improve through user interaction over time. This method is tested primarily to demonstrate that LAMS\u2019 improved performance is not merely due to users\u2019 increasing familiarity with the system, so as to provide evidence towards H2.\nIn our user study, for each task layout (i.e., each trial), users completed the task with all four methods in a counterbalanced"}, {"title": "V. DISCUSSION", "content": "Our experimental results demonstrate the effectiveness of the LAMS framework. As a preliminary step towards utilizing LLMs for automatic mode switching, in this section, we examine the strengths and limitations of LAMS, focusing on when LLM-based switching works well and when it encounters difficulties. We also discuss potential future directions to address these challenges and further improve the system.\nLAMS\u2019s Advantage over a Static Method: Impact of Incremental Improvement in the First Trial: As shown in our user study, LAMS consistently outperforms the static LLM-based mode-switching method.\nNotably, even in the first trial, LAMS shows improvements during the later stages of the tasks by making more accurate mapping predictions and requiring fewer manual mode switches. Specifically, in the first trial of the water pouring task, the static LLM-based method required an average of 8.6 manual mode switches, whereas LAMS reduced this to 7.5, representing a 12.8% improvement. Similarly, in the book storage task, the static method required 9.1 manual switches, while LAMS reduced this to 6.3, a 30.8% improvement.\nWhile the incremental improvement of LAMS over the static LLM-based method was expected after multiple trials, the improved performance observed in the first trial was an unexpected benefit. To understand this, we examined the errors made by both methods during the tasks.\nWe found that LAMS\u2019s advantage largely stems from reducing the number of incorrect joystick-to-robot action mappings for the \u201cOpen Gripper\u201d and \u201cClose Gripper\u201d commands. The static LLM-based method frequently mapped the joystick\u2019s up movement to \u201cOpen Gripper\u201d or the down movement to \u201cClose Gripper\u201d when these actions were not relevant, whereas LAMS made far fewer such errors.\nSpecifically, in the first trial of the water pouring task, the static method made an average of 6.7 incorrect \u201cOpen Gripper\u201d or \u201cClose Gripper\u201d mappings, while LAMS reduced this to 3.7. In the book storage task, the static method averaged 5.2 false mappings, while LAMS lowered this to just 1.8. False \u201cOpen Gripper\u201d or \u201cClose Gripper\u201d mappings are defined as instances where the joystick\u2019s up or down movements were mapped to gripper actions, but the user manually switched to a different robot action, indicating the error.\nWhile LAMS effectively reduces manual mode switches, we found that it struggled to differentiate between certain rotational movements during the user study.\nSpecifically, LAMS often confused \u201cYaw Left/Right\u201d and \u201cRoll Left/Right\u201d (both corresponding to lateral joystick movements), even by the third trial when the system had more opportunities to learn from user-generated examples. In contrast, because the longitudinal joystick movements correspond only to one type of rotation, \u201cPitch Up/Down\u201d, LAMS performed better in predicting this motion.\nIn particular, LAMS mapped joystick movements to \u201cPitch Up/Down\u201d 80% of the time when the action was required. However, it achieved 40% accuracy for \u201cYaw Left/Right\u201d and 50% for \u201cRoll Left/Right\u201d. Correct predictions were defined where the joystick\u2019s movements were mapped to the corresponding rotational action, and the user executed the action, confirming its correctness. The total number of times when a rotational action was required includes both correct predictions and cases where the joystick\u2019s movements were mapped to a different action but were manually switched to the corresponding rotational movement by the user.\nThis challenge likely arises from the inherent complexity of representing and interpreting 3D rotations in both natural language and mathematical contexts [41], [42]. Future work could focus on exploring alternative ways to express 3D rotations in natural language to mitigate this challenge. Conclusion: In this paper, we present LAMS, which leverages LLMs to perform automatic mode switching for teleoperating a robotic arm. LAMS is able to incrementally improve as a user repeatedly interacts with the system. Our ablation and user studies demonstrate that LAMS outperforms alternative designs, baseline mode-switching methods, and a static LLM-based approach. It enables users to complete tasks with fewer manual mode switches, is preferred by users, and improves performance with use over time."}, {"title": "APPENDIX A", "content": "GROUPED MAPPING: DETAILS AND EXPLANATION\nOne of the baseline methods we tested in our user study, Grouped Mapping, is a common mode-switching method in robotic arm applications [3], [4], [9], [10], [12], [22], [25], [40], where robot actions are divided into predefined groups. For example, in group 1, the joystick controls move forward/backward and move left/right. In group 2, it controls move up/down and roll left/right. Group 3 maps to pitch up/-\ndown and yaw left/right, while group 4 maps the longitudinal movements to gripper open/close, with no lateral mapping. Users switch between these groups. Note that unlike other methods we tested in our user study, Grouped Mapping uses a different manual mode-switching mechanism to preserve group integrity: as shown in Fig. 3, the user presses the X button on the Xbox controller to cycle through groups, changing all four joystick mappings at once. This single-button cycling method was used in to reflect the most commonly used mode-switching approach in current teleoperation systems [3], [4], [9], [10], [12], [22], [25], [40]."}, {"title": "APPENDIX B", "content": "EXPERIMENTAL SETUP & GRAPHICAL USER INTERFACE (GUI)\nFigure B1 shows the experimental setup of the user study. As shown in the figure, the participant is seated behind the robotic arm. Throughout the study, the user can continuously see a GUI displaying the current control mode, which shows the four robot action directions mapped to each joystick movement.\nAn example of the Graphical User Interface (GUI) displayed to participants during the experiments is shown in Figure B2.1. As shown in the figure, the GUI indicates the four robot action directions currently mapped to each joystick movement. It also displays the number of manual mode switches performed by the participant and the object currently being grasped, allowing the experimenters to more easily supervise and record the progress of the experiments.\nAn example of how the GUI updates after an automatic mode switch initiated by the LLM is shown in Figure B2.2. The LLM predicts the most likely robot actions mapped to the four joystick directions. If the predicted robot action for a"}, {"title": "APPENDIX C", "content": "ABLATION STUDY EXTENSION: \u201cSHADOW MODE\u201d ANALYSIS ON USER STUDY DATA\nTo further validate the effectiveness of the key design choices in LAMS described in Section IV-B, we conducted a \u201cshadow mode\u201d analysis based on data collected from our user study. Specifically, at each instance where LAMS invoked the LLM to predict a control mode, we made parallel LLM calls with prompts corresponding to the respective ablated methods. For each simulated method, if the user\u2019s subsequent action deviated from the predicted control mapping, we assumed that a manual mode switch would have been required if the participant had been using that method. These simulated corrections were also considered as examples to be integrated into subsequent LLM calls for incremental improvement of the corresponding methods. The average number of manual mode switches calculated through this approach across the 10 participants in the user study is presented in Figure C.\nIt is important to note that this \u201cshadow mode\u201d analysis provides an approximation rather than a precise evaluation of the ablated methods. Participants\u2019 behavior and decision-making processes may vary when actively using different methods, potentially leading to differences in task execution routes and mode-switching patterns. Nevertheless, this analysis offers meaningful insights by evaluating the relative efficiency of each method under the same task scenarios and interaction points.\nAs shown in Figure C, consistent with the findings in Section IV-B, LAMS achieves the lowest average number of manual mode switches among all methods, with a low standard deviation. This result reaffirms the effectiveness of the key design choices in LAMS."}, {"title": "APPENDIX D", "content": "REASONS BEHIND USER PREFERENCES REVEALED BY OUR POST-STUDY INTERVIEW\nFollowing our user study, we conducted oral interviews to explore participants\u2019 reasons for their preferences regarding"}, {"title": "APPENDIX E", "content": "LIMITATIONS OF OUR WORK\nOne potential limitation of our work is our text-based task context grounding approach, which may face challenges in more complex environments or when object relevance is ambiguous. Future work could test the method\u2019s adaptability in environments with multiple, ambiguous objects.\nWe also acknowledge the potentially higher cost of LLM calls compared to methods like maintaining example libraries and applying nearest-neighbor searches. Compared to alternative approaches, our LLM-driven method likely provides greater generalizability, especially with limited examples and dynamic contexts. However, this comparison was not formally conducted in our work. Future studies could explore these comparisons and examine the trade-offs between cost and the generalizability of LLMs."}, {"title": "APPENDIX F", "content": "PROMPTS USED IN OUR METHOD"}]}