{"title": "HELENE: HESSIAN LAYER-WISE CLIPPING AND GRADIENT ANNEALING FOR ACCELERATING FINE-TUNING LLM WITH ZEROTH-ORDER OPTIMIZATION", "authors": ["Huaqin Zhao", "Jiaxi Li", "Yi Pan", "Shizhe Liang", "Xiaofeng Yang", "Wei Liu", "Xiang Li", "Fei Dou", "Tianming Liu", "Jin Lu"], "abstract": "Fine-tuning large language models (LLMs) poses significant memory challenges, as the back-propagation process demands extensive resources, especially with growing model sizes. Recent work, MeZO, addresses this issue using a zeroth-order (ZO) optimization method, which reduces memory consumption by matching the usage to the inference phase. However, MeZO experiences slow convergence due to varying curvatures across model parameters. To overcome this limitation, we introduce HELENE, a novel scalable and memory-efficient optimizer that integrates annealed A-GNB gradients with a diagonal Hessian estimation and layer-wise clipping, serving as a second-order pre-conditioner. This combination allows for faster and more stable convergence. Our theoretical analysis demonstrates that HELENE improves convergence rates, particularly for models with heterogeneous layer dimensions, by reducing the dependency on the total parameter space dimension. Instead, the method scales with the largest layer dimension, making it highly suitable for modern LLM architectures. Experimental results on ROBERTa-large and OPT-1.3B across multiple tasks show that HELENE achieves up to a 20x speedup compared to MeZO, with average accuracy improvements of 1.5%. Furthermore, HELENE remains compatible with both full parameter tuning and parameter-efficient fine-tuning (PEFT), outperforming several state-of-the-art optimizers. The codes will be released after reviewing.", "sections": [{"title": "1 INTRODUCTION", "content": "LLMs have demonstrated remarkable capabilities across various downstream tasks. Fine-tuning these models has become the standard approach for improving task-specific performance, in which the first-order optimizers like Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951), Adam (Diederik, 2014) and AdamW (Hutter & Loshchilov, 2017) are widely used. While effective, however, these methods demand significant memory resources primarily due to the backpropagation process, which makes fine-tuning challenging, especially for large-scale models. To overcome this limitation, Malladi et al. (2023) proposed a memory-efficient zeroth-order optimizer (MeZO) that estimates gradients using only two forward passes per training step, contributing to considerable memory savings.\nHowever, recent studies show that loss functions in deep learning often exhibit heterogeneous curvatures across different model parameters and different model layers (Sagun et al., 2016; Ghorbani et al., 2019; Zhang et al., 2022; Yao et al., 2020), which poses challenges to zeroth-order (ZO) optimization. This variation in curvature can overall hinder training efficiency and lead to the sub-optimal solution. To address this issue, more advanced techniques are required, such as incorporating second-order information to better account for curvature differences (Liu et al., 2023; Tran &\nCutkosky, 2022; Jahani et al., 2021). However, in ZO optimization, directly computing the Hessian from first-order derivatives is nearly impossible, and partial Hessian evaluations are computationally"}, {"title": "2 PRELIMINARIES", "content": "In this section, we briefly review essential background concepts related to zeroth-order optimization and diagonal Hessian approximation, which are fundamental to the design of our proposed method."}, {"title": "2.1 ZEROTH-ORDER GRADIENT ESTIMATORS AND \u039c\u0395\u0396\u039f", "content": "Zeroth-order (ZO) optimization has long been studied in the context of convex and non-convex objectives. One of the typical ZO gradient estimators is the simultaneous perturbation stochastic approximation (SPSA) (Spall, 1992; Maryak & Chin, 2001). Given a model with parameters $\\theta \\in \\mathbb{R}^d$ and loss function L, SPSA estimates the gradient on a minibatch B as:\n$\\begin{equation} g_{\\epsilon}(\\theta) = \\frac{L(\\theta + \\epsilon z; B) - L(\\theta - \\epsilon z; B)}{2\\epsilon} z \\approx zz^\\top\\nabla L(\\theta; B) \\end{equation}$\nwhere $z \\in \\mathbb{R}^d$ with $z \\sim N(0, I_d)$ and $\\epsilon$ is the perturbation scale.\nBuilding on the basic principles of ZO optimization, MeZO (Malladi et al., 2023) introduces a memory-efficient implementation of ZO-SGD. This approach reduces memory requirements, allowing optimization to proceed with the same memory usage as the inference phase of a model. The key innovation in MeZO lies in its use of a consistent random seed s to sample the random vector z, ensuring the same perturbation z at each step."}, {"title": "2.2 DIAGONAL HESSIAN APPROXIMATION", "content": "While zeroth-order methods like MeZO provide valuable tools for gradient estimation, optimization can be significantly enhanced by incorporating second-order information, such as curvature. However, directly computing and applying the full Hessian matrix is computationally expensive, particularly in high-dimensional parameter spaces. Specifically, directly applying the Hessian pre-conditioner by calculating the inverse Hessian and multiplying it with the gradient vector at each iteration $H^{-1}g$\nis particularly computationally expensive. To address this challenge, inexact Newton methods have been developed, where approximations of the Hessian are used instead of the full matrix (Dembo et al., 1982; Bollapragada et al., 2019; Xu et al., 2020).\nA simple yet effective alternative is to approximate the Hessian by its diagonal elements, which reduces computational complexity while retaining useful curvature information. In this approach, a general descent direction can be written as follows:\n$\\begin{equation} \\Delta \\theta \\approx diag(H)^{-1}g, \\end{equation}$\nwhere diag(H) represents the diagonal elements of the Hessian matrix. This method enhances opti-mization by enabling efficient inverse Hessian application and supporting inexact Newton methods, providing improved convergence in complex problems."}, {"title": "3 METHOD", "content": "In this section, we formally present HELENE in Section 3.2, with pseudo-code provided in Algorithm 1. In Section 3.4, we introduce A-GNB, followed by a detailed discussion of layer-wise clipped diagonal Hessian in Section 3.5."}, {"title": "3.1 MOTIVATION", "content": "Highly variable curvature across different layers and parameters. Fine-tuning large language models (LLMs) has become essential for achieving state-of-the-art performance on various downstream tasks. Commonly employed first-order optimizers such as Stochastic Gradient Descent"}, {"title": "3.2 HELENE: HESSIAN LAYER-WISE CLIPPING AND GRADIENT ANNEALING", "content": "In HELENE, we introduce an annealing mechanism to mitigate bias in SPSA-estimated gradients, combined with a clipped diagonal Hessian pre-conditioner that adjusts parameter update step sizes based on layer-wise curvature. First, the gradient is calculated using the SPSA, while the diagonal Hessian is efficiently estimated by the proposed new A-GNB method, to eliminate the noise incured in sampling labels from the model output used in GNB and Sophia. At each iteration, SPSA provides an estimate $g_t$ using two forward passes with random perturbations, and A-GNB returns $h_t$, the diagonal Hessian of the mini-batch loss.\nWe apply an exponential moving average (EMA) to both the gradient and diagonal Hessian estimates to reduce noise and improve stability. To further enhance convergence, we apply layer-wise magnitude-based clipping to the diagonal Hessian, ensuring extreme values do not disproportionately affect parameter updates. We provide our pseudo code in Algorithm 1 and each module description in the following section in details."}, {"title": "3.3 EMA OF DIAGONAL HESSIAN ESTIMATES", "content": "When using a mini-batch to compute the local Hessian (curvature), the resulting estimates are often noisy. The Hessian diagonal can fluctuate significantly across different parameter dimensions of the problem. Inspired by the exponential moving average (EMA) of gradient moments in Adam, we apply a similar technique to reduce noise in the Hessian diagonal estimates over iterations. The updated Hessian diagonal is computed in the following:\n$\\begin{equation} h_t = \\beta_2h_{t-k} + (1 - \\beta_2)\\hat{h}_t, \\end{equation}$\nwhere $h_t$ represents the denoised Hessian diagonal at iteration t and $h_t$ is the current estimate of the diagonal at the k-th iteration."}, {"title": "3.3.1 ANNEALING MECHANISM", "content": "As illustrated in Figure 5, the native gradient EMA introduces bias, which adversely affects the training process and results in an increase in loss during the later stages. To mitigate these issues, we introduce a gradient annealing mechanism to work in tandem with EMA. Our annealing strategy adjusts the contribution of the current gradient dynamically by reducing the learning rate as training progresses. This adaptive adjustment is crucial for ensuring that the model becomes less influenced by noisy or outdated gradients in later stages. Specifically, the annealing function modulates the step size $\\alpha$, which controls the weight assigned to the most recent gradient update, allowing the optimizer to smoothly reduce the impact of past gradients. Notably, our annealing approach is simple to implement, requiring the tuning of only a single hyperparameter.\nAt each iteration, the annealing mechanism computes $\\alpha$ using an exponential decay schedule in equation 1, where T is a predefined hyperparameter controlling the annealing rate. As t increases, $\\alpha$ gradually decreases, reducing the learning rate and thus mitigating the bias introduced by EMA. This ensures that, in the later stages of training, the model focuses more on stable gradient estimates and less on noisy or rapidly changing updates. The annealing mechanism is incorporated into the"}, {"title": "3.4 ASYMPTOTIC GAUSS-NEWTON-BARTLETT (A-GNB) ESTIMATOR", "content": "The original GNB (Martens, 2020) estimator relies on sampled labels $\\hat{y}_s$ drawn from the categorical distribution based on the model's output. However, this induces stochasticity due to label sampling, which could be problematic when label distributions are highly imbalanced, as is the case in large language model (LLM) training. We propose a new estimator, which we call the Asymptotic Gauss-Newton-Bartlett (A-GNB) Estimator, that replaces the sampled labels $\\hat{y}_s$ with the true labels $y_b$ and asymptotically converges to the true diagonal of the Hessian.\n$\\begin{equation} \\nabla \\nabla^\\top L(\\theta) = J_{\\theta}f(\\theta, x)^\\top \\cdot S \\cdot J_{\\theta}f(\\theta, x) \\end{equation}$\nwhere the diagonal approximation of the Gauss-Newton matrix for the mini-batch loss becomes:\n$\\begin{equation} \\frac{1}{B} \\sum_{b=1}^B \\nabla_{\\theta} L(f(\\theta, x_b), y_b) \\nabla_{\\theta} L(f(\\theta, x_b), y_b) \\end{equation}$\nwhere $J_{\\theta} f(\\theta, x)$ is the Jacobian of the model's output $f(\\theta, x)$ with respect to the parameters $\\theta$, and $S = \\frac{\\partial^2L(t,y)}{\\partial t^2}$ is the second-order derivative of the loss with respect to the logits t. In contrast to GNB estimator, where $\\hat{y}_s$ was used, we replace it by $y_s$, the true label, thereby avoiding the need for post-output label sampling. By eliminating the stochasticity induced by sampled labels $\\hat{y}_s$, we reduce the variance caused by sampling noise, and it is especially beneficial in imbalanced data scenarios, when samples from minor class is seldomly selected unless sampling significantly many times.\nSince the true label $y_b$ is used instead of the sampled label $\\hat{y}_s$, the expectation becomes the true expected gradient. For a mini-batch of size B, the new estimator is:\n$\\begin{equation} E\\left[ \\frac{1}{B} \\sum_{b=1}^B \\nabla_{\\theta} L(f(\\theta, x_b), y_b) \\otimes \\nabla_{\\theta} L(f(\\theta, x_b), y_b) \\right] \\end{equation}$\nThe gradient terms now correspond directly to the true labels, and their outer product sums up to the true Gauss-Newton approximation of the Hessian. As the batch size $B \\rightarrow \\infty$, the A-GNB estimator converges to the true Hessian's diagonal:\n$\\begin{equation} \\lim_{B\\to\\infty} \\frac{1}{B} \\sum_{b=1}^B \\nabla_{\\theta} L(f(\\theta, x_b), y_b) \\otimes \\nabla_{\\theta} L(f(\\theta, x_b), y_b) = \\nabla^2 L(\\theta) \\end{equation}$\nThis holds because the estimator becomes a sum over the entire dataset, and the variance from label sampling is completely eliminated. Therefore, The A-GNB estimator asymptotically converges to the true diagonal of the Hessian as B increases."}, {"title": "3.5 LAYWERWISE CLIPPED DIAGONAL HESSIAN TO HELP NEWTON'S METHOD", "content": "As discussed in the motivating examples, fine-tuning LLMs and optimizing non-convex functions pose challenges for Newton's method, which uses the Hessian as a pre-conditioner. The method may converge to local maxima rather than local minima. Moreover, the inaccuracy of Hessian estimates and changes in the Hessian along the optimization trajectory can render second-order information unreliable. To address these issues, we draw inspiration from Sophia. While Sophia performs clipping"}, {"title": "4 CONVERGENCE ANALYSIS", "content": "In this section, we provide a theoretical analysis of the convergence of our proposed method. The key improvement in our method comes from the use of layer-wise parameters $\\lambda_i$, which reduces the dependency on the total dimension d and instead relies on the maximum layer dimension $max_i d_i$.\nThe theoretical bound for the number of steps T in our method is given by the following theorem with two assumptions:\nAssumption 1. Let $L : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a loss function. We assume L is twice continuously differentiable strictly convex, and has a unique minimizer denoted by $\\theta^*$. For each layer i, define $\\mu_i$ as the minimum eigenvalue of the Hessian matrix of L concerning the parameters of that layer evaluated at its minimizer:\n$\\begin{equation} \\mu_i = \\min(\\nabla^2 L(\\theta^*)) \\end{equation}$\nwhere $\\nabla^2_i$ denotes the Hessian with respect to the parameters of the i-th layer.\nAssumption 2. Regarding the Hessian $\\nabla^2 L(\\theta)$ of the loss function, we assume:\n$\\bullet$ There exists a radius $R_i > 0$ such that for any $\\theta, \\theta' \\in \\mathbb{R}^{d}$ with $||\\theta_i - \\theta_i'||_2 < R_i$, the following inequality holds:\n$\\begin{equation} || \\nabla^2 L(\\theta_i | \\theta_{-i})^{-1} \\nabla^2 L(\\theta'_i | \\theta'_{-i}) ||_2 \\leq 2 \\end{equation}$\nwhere $|| \\cdot ||_2$ represents the spectral norm.\nTheorem 1. Under Assumptions 1 and 2, let $\\eta = \\frac{1}{2}$ and $\\lambda_i = \\frac{\\mu_i}{R_i}$. The update reaches a loss at most $\\epsilon$ in\n$\\begin{equation} T \\leq \\max_i \\left\\{ d_i \\left( L(\\theta_{0,i}) - \\min L \\right) + \\ln \\left( \\frac{\\mu_i R_i^2}{32 d_i \\epsilon} \\right) \\right\\} \\end{equation}$\nsteps, where L is the loss function, $\\theta_{0,i}$ is the initial parameter vector for layer i, $\\mu_i$ is the strong convexity constant for layer i, and $R_i$ is the bound on the distance between $\\theta_{0,i}$ and $\\theta$.\nThe best known theoretical bound for the number of steps T required by Sophia to reach a loss at most $\\epsilon$ is given by Sophia in which $T_{SOPHIA} \\sim O(d)$, where d is the total dimension of the parameter space. This result implies that the convergence rate depends linearly on the total dimension d, which can lead to slow convergence for models with large parameter spaces. In contrast, our method introduces layer-wise parameters $\\rho_i = \\frac{\\mu_i}{R_i}$, where $R_i$ is the bound on the distance between the initial parameters $\\theta_{0,i}$"}, {"title": "5 EXPERIMENTS", "content": "Since the introduction of the Transformer (Vaswani, 2017), language models (LMs) have progressively developed through the use of different Transformer-based architectures. One of the iconic work is BERT (Devlin, 2018), which is based on the encoder architecture of Transformer and pre-trained with techniques like masked language modeling. As the field of natural language processing (NLP) develops, more powerful decoder-only LLMs also have shown their great potential.\nTherefore, to rigorously evaluate the capability and universality of HELENE, we follow the experiments conducted in MeZO on both medium-sized masked LMS (ROBERTa-large (Liu, 2019), 350M) and auto-regressive LLMs (OPT-1.3B (Zhang et al., 2023)) under both few-shot and many-shot settings. Additionally, all optimization algorithms are evaluated with three tuning methods: fine-tuning (FT) and two parameter-efficient fine-tuning (PEFT) methods, LoRA (Hu et al., 2021) and prefix-tuning (Li & Liang, 2021). We also do experiments with zeroth-order (ZO) versions of some optimizers as well as ZO-SGD variants introduced in Zhang et al. (2024), and present them in Section 5.3.\nThe experimental results show that across all settings, HELENE not only outperforms MeZO on most datasets by approximately 1.5% on average, but also makes the convergence process of gradient-free optimization more stable and faster, boosting to 20\u00d7 times the original speed."}, {"title": "5.1 MASKED LANGUAGE MODELS", "content": "For masked LMs, we conduct experiments using RoBERTa-large on three types of NLP tasks, sentiment classification, natural language inference, and topic classification with k = 16 examples per class. We run HELENE for 5,000 steps and FT for 1,000 steps. The experimental results are listed in Table 1."}, {"title": "5.2 AUTO-REGRESSIVE LLMS", "content": "LLMs such as GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), and ChatGLM (Du et al., 2021) have become the predominant models in NLP, we include experiments with auto-regressive LLM OPT-1.3B on three different task: text classification, multiple choice, and text generation. We use various datasets from the SuperGLUE benchmark (Wang et al., 2019), which includes the following datasets: SST-2, RTE, CB, WSC, WIC, COPA, and ReCoRD. Additionally, we also experiment on BoolQ (Clark et al., 2019) and SQUAD (Rajpurkar, 2016). We run HELENE with about 10,000 training steps for each dataset. The results are summarized in Table 2, from which we can have the following observations.\nHELENE has clear performance advantages compared with MeZO. Table 2 shows that HELENE with its LoRA and prefix variants can consistently outperform MeZO. Specifically, the average performances of HELENE with its LoRA and prefix variants remarkably exceed MeZO's by 5.3%, 2.1% and 1.3% on CB, SQUAD and COPA, respectively.\nHELENE accelerates 10\u00d7 times while remaining compatible with PEFT methods. In Figure 3, we present results from four selected datasets across different tasks under three tuning methods. It indicates that HELENE can consistently speed up the convergence by up to 10x times, and also enhances the capability."}, {"title": "5.3 EXPERIMENTS WITH OTHER ZO ALGORITHMS", "content": "It is worth noting that the ZO optimization technique utilized in Malladi et al. (2023) is primarily the basic SGD version (ZO-SGD), and it is still not clear how effective HELENE is when comparing with other ZO optimization algorithms like ZO-SGD, ZO-SGD-MMT, ZO-SGD-Cons, ZO-SGD-Sign and ZO-Adam as introduced in Liu et al. (2020). Therefore, we reference the statistics of performances summarized in Zhang et al. (2024) and experiment under the same setting with them (Table 3), through which HELENE shows good functionality especially for FT and prefix-tuning.\nWe further implement the ZO versions of Adam, AdamW and Lion (Chen et al., 2024) and plot the results in Figure 4. The results indicate that HELENE helps the model converge faster as well as obtain lower validation loss value."}, {"title": "5.4 ABLATION STUDY", "content": "We conduct a comprehensive ablation study on the key techniques of HELENE in Appendix B, including in-depth analysis of the effects of magnitude clipping across different ranges. Additionally, we explore the factors resulting in Sophia's initial convergence and subsequent divergence."}, {"title": "6 CONCLUSION", "content": "In this paper, we present a novel optimizer, HELENE, which is designed to address the challenges of fine-tuning LLMs. HELENE integrates a new asymptotic Gauss-Newton-Bartlett (A-GNB) estimator for diagonal Hessian estimation, and a novel layer-wise clipping with the annealing module. The A-GNB estimator eliminates the need for label sampling, providing an unbiased Hessian approximation and improving the precision of curvature-aware updates. Furthermore, our layer-wise clipping mechanism provably ensures more adaptive Hessian updates based on the curvature of each layer, enhancing stability and scalability. Theoretical analysis shows that HELENE reduces convergence steps from O(d) to O(maxi di), making it highly scalable for modern architectures with many layers. Experimental results on models like ROBERTa-large and OPT-1.3B demonstrate that HELENE achieves up to a 20\u00d7 speedup compared to MeZO and improves performance by 1.5% on average. Compatible with both full parameter tuning and parameter-efficient fine-tuning, HELENE outperforms many state-of-the-art optimizers across diverse tasks and datasets."}, {"title": "A RELATED WORK", "content": null}, {"title": "A.1 ZERO-ORDER OPTIMIZATION", "content": "Zeroth-order optimization, which only relies on the forward passes of neural networks, offers significant memory savings during the training process. Recently, MeZO (Malladi et al., 2023) adapted the traditional zeroth-order SGD optimization method for fine-tuning LMs, achieving performance comparable to full-parameter fine-tuning while significantly reducing memory usage. Thus, zeroth-order optimization is regarded as a promising approach for memory-efficient fine-tuning of LLMs. Several studies have aimed to improve the MeZO algorithm. For instance, Gautam et al. (2024) introduced a zeroth-order optimization algorithm that integrates both full-batch and mini-batch information to produce asymptotically unbiased, low-variance gradient estimations. However, the convergence rate of their approach still leaves room for improvement. In pursuit of better gradient estimation, Jiang et al. (2024) proposed an innovative perturbation sampling technique inspired by the Adam optimizer. Other methods, such as SPSA (Spall, 1992; Maryak & Chin, 2001), have shown to be effective in non-convex multi-agent optimization (Tang et al., 2020; Hajinezhad & Zavlanos, 2018) and in generating black-box adversarial examples (Chen et al., 2017; Cai et al., 2021; Liu et al., 2019; Ye et al., 2018)."}, {"title": "A.2 SECOND-ORDER INFORMATION FOR FINE-TUNING LLMS", "content": "Classic second-order optimization algorithms pre-condition the gradient with curvature informa-tion (BROYDEN, 1970; Nesterov & Polyak, 2006; Conn et al., 2000). Over the years, people have developed numerous ways to adapt these methods to deep learning. To the best of our knowledge, BECKER (1988) was the first to use diagonal Hessian as the pre-conditioner. Martens et al. (2010) approximated the Hessian with conjugate gradient. Schaul et al. (2013) automatically tuned the learning rate of SGD by considering diagonal Hessian. Pascanu (2013) considered Gaussian Newton's approximation of Hessian and Fisher information matrix. Martens & Grosse (2015) and follow-up works (Ba et al., 2017; George et al., 2018; Martens et al., 2018; Zhang et al., 2022) proposed to approximate the Hessian based on the structure of neural networks. Despite these progress on deep learning applications, for decoder-only LLMs, Adam still appears to be the most popular optimizer. The authors of this paper suspect that many previous second-order optimizers face the challenge that the computational / memory overhead due to frequent Hessian computation hinders improvements in wall-clock time (Martens & Grosse, 2015; Gupta et al., 2018). Some of them also depend on specific model architecture or hardware structures, e.g., Anil et al. (2020) offloads hessian computation to CPUs, and George et al. (2018) needs ResNet and very large batch size to approximate the Fisher information matrix. To the best of our knowledge, there was no previous report that second-order optimizers can achieve a speed-up on LLMs in total compute.\nThere is also a concurrent work Zhao et al. (2024) that utilizes Hessian information to guide the gradient estimation process. However, this work may not provide effective solution about the sampling process of perturbation direction, while the perturbation sampling of our method, HELENE, follows a normal distribution $N(0, Hessian^{-1})$, which facilitates the training process and contributes to a faster convergence speed. There may exist some other minor mistakes of the paper Zhao et al. (2024) that for the sampling operation of perturbation, the authors may use Hessian instead of $Hessian^{-\\frac{1}{2}}$ by mistake. Meanwhile, from the results that the authors provide in their paper we can observe that the variance of training loss is still very high, which indicates an unstable training process."}, {"title": "A.3 GRADIENT CLIPPING", "content": "Global gradient clipping has been a widely adopted practice in fine-tuning LLMs (Chen et al., 2020; Zhang et al., 2019; Liu et al., 2022). This technique stabilizes training by mitigating the impact of rare examples and large gradient noise. In addition to gradient clipping, HELENE is the first method to clip the Hessian matrix in second-order optimization techniques. This approach addresses the issue of the Hessian matrix fluctuating along the optimization trajectory and reduces the errors in Hessian approximations."}, {"title": "B ABLATION STUDY", "content": null}, {"title": "B.1 EVALUATING THE IMPACT OF KEY COMPONENTS ON CONVERGENCE AND STABILITY", "content": "Figure 5 illustrates the effectiveness of each component in our algorithm. Adding momentum to MeZO alone doesn't improve performance. Introducing bias in the gradient boosts convergence speed, but causes loss to increase later in training due to biased gradient estimates. To counter this, we added an annealing term to make the gradient asymptotically unbiased, which stabilizes the loss. Inspired by Sophia, we introduced the clipped Hessian to address heterogeneous curvatures, further improving convergence speed. Our ablation study validates both the motivation and effectiveness of these components."}, {"title": "B.2 MAGNITUDE CLIPPING", "content": "Figure 6 addresses the robustness of clipping in our optimizer. Our empirical study is as follows: First, we explored the impact of lower bounds ranging from 1 to 3, all of which demonstrated stability. As a hyperparameter, this lower bound shows consistent robustness. However, when the lower bound was set to 0.9, the model performance dropped by 10 points, leading us to believe that problematic Hessian values are concentrated below 1, while values above 1 are less critical. Second, we argue that layer-wise clipping based on magnitude is reasonable in a zeroth-order setting, as performing percentage-based clipping for each layer would be too time-consuming. Third, this aligns with our intuition that reducing bad Hessian values, along with a smaller step size, is effective in maintaining performance."}, {"title": "B.3 INVESTIGATION INTO THE CONVERGENCE INSTABILITY OF SOPHIA", "content": "We study the reasons for Sophia's failure in the Figure 1 by counting the number of clip triggers. We computed the loss between timesteps 400 and 800, with a mean value of 0.57. The average loss between timesteps 1400 and 1800 was 0.65. We then analyzed the number of times the Sophia clipping mechanism was triggered within these two time intervals. Our analysis covered the Q, K, V matrices, fully connected layers, and bias layers. We found that the frequency of clipping in the interval where the mean loss was 0.65 was 1.18 to 1.22 times higher than in the interval where the mean loss was 0.57.\nBased on these experimental observations, we conclude that Sophia's clipping mechanism tends to be over-triggered in complex data scenarios, particularly when faced with heterogeneous curvature. This over-triggering can result in non-convergence, aligning with our intuition. In the zeroth-order setting, gradients are estimated using SPSA, and excessive clipping of the $g_t$ terms can lead to instability and failure of the model to converge."}, {"title": "C DETAILED CONVERGENCE ANALYSIS", "content": "Lemma 1 (Divergence to Infinity). Under Assumption 1, for each layer i in a neural network model, assume the function $L : \\mathbb{R}^{d_i} \\rightarrow \\mathbb{R}$ is strictly convex, twice continuously differentiable, and has a unique minimizer denoted by $\\theta_i^*$. For any parameter vector $\\Theta_i$ of layer i such that $|| \\theta_i - \\theta_i^* ||_2 \\geq 1$, the function $L(\\theta_i)$ diverges to infinity as $||\\theta_i||_2 \\rightarrow \\infty$.\nProof. By the strict convexity of L, for any $\\theta_i$ such that $|| \\theta_i - \\theta_i^* ||_2 \\geq 1$, we have:\n$\\begin{equation} \\frac{L(\\theta_i) - L(\\theta_i^*)}{|| \\theta_i - \\theta_i^* ||_2} \\geq \\min_{\\xi : ||\\xi||_2=1} \\frac{L(\\theta_i^* + \\xi) - L(\\theta_i^*)}{} \\triangleq \\Delta_i > 0, \\end{equation}$\nwhere $\\xi$ is a unit vector. For the convenience, here $L(\\theta)$ denotes $L(\\theta|\\theta_i)$ where i denotes the parameters in the whole model except $\\theta_i$, and $L(\\theta_i^*)$ denotes $L(\\theta^*|\\theta^*_i)$. Define $\\Delta_i$ as:\n$\\begin{equation} \\Delta_i \\triangleq \\min_{\\xi : ||\\xi||_2=1} L(\\theta_i^* + \\xi) - L(\\theta_i^*), \\end{equation}$\na positive constant due to the strict convexity of L indicating the minimal rate of increase of L around $\\theta_i^*$."}]}