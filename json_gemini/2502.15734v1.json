{"title": "CACHE-CRAFT: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation", "authors": ["Shubham Agarwal", "Sai Sundaresan", "Subrata Mitra", "Debabrata Mahapatra", "Archit Gupta", "Rounak Sharma", "Nirmal Joshua Kapu", "Tong Yu", "Shiv Saini"], "abstract": "Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention-layers in LLMs fully compute the key values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose CACHE-CRAFT, a system for managing and reusing precomputed KVs corresponding to the text chunks (we call chunk-caches) in RAG-based systems. We present how to identify chunk-caches that are reusable, how to efficiently perform a small fraction of recomputation to fix the cache to maintain output quality, and how to efficiently store and evict chunk-caches in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that CACHE-CRAFT reduces redundant computation by 51% over SOTA prefix-caching and 75% over full recomputation. Additionally, with continuous batching on a real production workload, we get a 1.6x speedup in throughput and a 2\u00d7 reduction in end-to-end response latency over prefix-caching while maintaining quality, for both the LLAMA-3-8B and LLAMA-3-70B models.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) allows LLMs to access relevant context from a custom knowledge base outside its training data to generate grounded responses. RAG systems do not require model retraining and are therefore a cost-effective way to customize an LLM's output such that it is relevant and accurate with respect to a target knowledge base. The key components of a RAG-based system are a vector database and an LLM. The vector database stores the embedding of text chunks from a specific domain as indexes. During the retrieval phase, relevant chunks are extracted based on these embeddings, using vector-similarity search [25, 26]. In the generation phase, the LLM uses the retrieved context to generate a response to the user's question. The LLM processes its input prompt (retrieved chunks + user's question) in the prefill phase, building an initial computed-state called Key-Value cache (KV-cache), which is then used in the decode phase for autoregressive token generation. The prefill phase is compute-bound because it processes all tokens of the input prompt in parallel; while the decode phase, which generates one token at a time, is memory-bound."}, {"title": "2 Background and Motivation", "content": ""}, {"title": "2.1 Preliminaries of LLM", "content": "A transformer-based LLM progressively contextualizes a sequence of tokens $S = \\{t_1,\\ldots, t_n\\}$ using L transformer layers. Each layer $l\\in [L]$ receives d-dimensional embeddings of n tokens, $H^l \\in \\mathbb{R}^{n\\times d}$, as input, and outputs contextualized embeddings $H^{l+1} \\in \\mathbb{R}^{n\\times d}$, which are known as hidden states. We denote the LLM operations, from input tokens S all the way up to the last hidden states $H^L$, as $H^L(S)$. The last layer hidden states are used in a task-specific manner. In the text generation task, the hidden embedding of the last token $H^L(S) \\in \\mathbb{R}^{d}$ is used to predict the $(n + 1)$th token."}, {"title": "2.2 Prefill Dominates Decode in RAG", "content": "In a typical RAG-system, the overall prompt sequence S consists of a few initial instruction text chunks, several retrieved chunks from a knowledge base, and the user's question or request U, i.e., $S = C_{1:K}U$, where $C_{1:k}$ denotes the concatenation of k chunks.* In most production RAG systems, between 5 to 15 chunks are retrieved to answer a query U. The overall length of prefill tokens $|S|$ and the lengths of their constituents may vary for different"}, {"title": "2.3 Evidences of Chunk-Reuse", "content": "Since prefill is the primary bottleneck in RAG, we find improvement opportunities by observing repetitions in chunk retrieval. If N is the total number of chunks representing the knowledge base accessible for RAG, a significant portion of N is retrieved multiple times across different user sessions, where a session consists of multiple user requests and LLM responses.\nWe substantiate this by analyzing the retrieval hit rates, defined as the fraction of all retrievals (across multiple sessions) in which a particular chunk is present. The top 5% of chunks are accessed by 60% of the requests in both Sys-X and MuSiQue, and 40% requests in 2wikiMQA. In Sys-X, most chunk reuse occurs across users (94%), with reuse within a session at 55% and across sessions at 67%. Exploiting the high reuse of knowledge chunks can optimize the prefill by reusing caches that are computed in the previous sessions, instead of recomputing for every retrieval. However, cache reuse across different user requests is non-trivial."}, {"title": "2.4 Why Cache-Reuse is non-trivial?", "content": "Limitations of Prefix-Caching: The prefix-cache approach is to store the KV-caches of ordered k-tuple chunks when they are co-retrieved for a request U, and reuse it for a future request U' if the same k chunks are retrieved in the same order. However, the reuse density, defined as the number of ordered k-tuple chunks observed in previous requests, drops significantly w.r.t. k, reducing the reusability of their cache."}, {"title": "3 CACHE-CRAFT Design", "content": "At a high level, CACHE-CRAFT enhances a RAG system by managing the KV-caches of knowledge chunks, as illustrated in Fig.4. We denote the Chunk-Cache of a chunk C that was originally computed from $H^L(C_1 : C_{i-1} :C_kU)$, while serving a request U at the ith position (i.e., $C_i = C$), as\n$\\begin{equation} (C | C_{1:C_{i-1}}) := \\{ (K, v)^l | l \\in [L] \\}, \\tag{2} \\end{equation}$\nwhere $K^l$ and $V^l$ are the key and value vectors in lth layer corresponding to the tokens in C.\nApart from storing the $(C | C_{1:C_{i-1}})$, We also store certain metadata to determine whether a particular of KV-cache C can serve a new request U' in future. CACHE-CRAFT operates in two phases: online and offline. The metadata computation is performed in the offline phase, and the determination of its \"usefulness\" is performed in the online phase, while serving a new request U'.\nIn its online phase, CACHE-CRAFT first selects the most useful (w.r.t. U') version of chunk-cache of C out of all the stored versions (C). Then CACHE-CRAFT selectively recomputes the key and value vectors for a few tokens of C to contextualize w.r.t. U'. Clearly, if there are no chunk-caches of C, then the key and value vectors for"}, {"title": "3.1 Determining Cache Reusability", "content": "From our analysis in Fig. 9 and 10, we observe that reusability can be assessed by determining how much a chunk's KV computation is influenced by external context (tokens outside the chunk) versus its own tokens. If a chunk is mainly influenced by its own tokens, it is more likely to produce high-quality answers when reused.\nIn fact, a chunk with more tokens is more reusable because tokens closer to each other have stronger attention due to positional embeddings, compared to distant tokens from other chunks [69]. To capture the attention within and across the chunks, we define the following two attention-based metrics:\n(1) Inter attention measures the cumulative attention weight from tokens in chunk $C_i$ to tokens in chunk $C_j$ where $i < j$:\n$\\begin{equation} inter(C_i, C_j) = \\sum_{k \\in C_i} \\sum_{l \\in C_j} \\alpha_{kl} \\tag{3} \\end{equation}$\nwhere $\\alpha_{kl}$ is the attention weight from the $k^{th}$ token of chunk i to the $l^{th}$ token of chunk j, computed from the softmax in (1).\n(2) Intra attention measures the cumulative attention weight within chunk $C_i$ from each token to previous tokens in the same chunk:\n$\\begin{equation} intra(C_i) = \\sum_{k,l \\in C_i: k<l} \\alpha_{kl}. \\tag{4} \\end{equation}$\nThe attention weights involved in the inter and intra are used to obtain the output from the attention computation. For instance, in case of 3 chunks $[C_1, C_2, C_3]$, the attention output is\n$\\begin{equation*} \\begin{bmatrix} [V_{C_1}] \\ V_{C_2} \\ [V_{C_3}] \\end{bmatrix} \\begin{bmatrix} intra(C_1) & inter(C_1, C_2) & inter(C_1, C_3) \\\\ 0 & intra(C_2) & inter(C_2, C_3) \\\\ 0 & 0 & intra(C_3) \\end{bmatrix} \\begin{bmatrix} V_{C_1} \\\\ V_{C_2} \\\\ [V_{C_3}] \\end{bmatrix} \\tag{5} \\end{equation*}$\nwhere intra and inter represent the associated attention weights in (3) and (4) without summing them, VC represents the pre-attention value vectors of all tokens in chunk C, and VC represents the corresponding post-attention value vectors.\nReusability of the cache $G(C_3 | C_1 C_2)$ for a new request depends on the new prefixes. Consider 2 cases with prefixes (i) C4-C2-C3 and (ii) C5-C6-C3. The first sequence carries the $C_2$ as a prefix similar to that of $(C_3 | C_1 C_2)$, making it more reusable than the second sequence, which does not have any common chunk in its prefix.\nAssuming that the higher the prefix overlap, the higher will be the reusability, we calculate a Prefix Overlap Score $\\beta$ for a chunk-cache of $C_i$ corresponding to the current prompt sequence $S_{new}$ as:\n$\\begin{equation} \\beta(C_i | S_{new}) = \\frac{\\sum_{j \\in S_{old} \\cap S_{new}} inter(i, j)}{\\sum_{j \\in S_{old}} inter(i, j)} \\tag{6} \\end{equation}$\nwhere $S_{old}$ is the set of chunks forming $C_i$'s old prefix.\nHowever, since $\\beta$ simply sums the inter attention terms for overlapping chunks, it is order-invariant and only captures the subset match between the previous and current prefixes. For instance, consider the two scenarios: C1-C2-C3 and C2-C1-C3. In both cases, $\\beta$ equals 1, yet the potential for reusing the cached chunk $C_3$ can differ significantly due to the reordering of the prefix sequence."}, {"title": "3.2 Fixing Chunk-Cache via Recomputation", "content": "We fix the chunk-caches at runtime to make them reusable across different contexts. Fixing refers to recomputing only the necessary KV values to ensure the output of the reused cache closely mimics the output without any cache reuse. Fig. 12 shows how output deviations increase with higher CCI and higher 1 \u2013 \u03b2', indicating greater fixing requirements. CCI captures the chunk's contextual dependency, while 1 \u2013 \u03b2' reflects prefix mismatch. We use this to define Cache Fix Overhead (CFO) for chunk $C_i$ as\n$\\begin{equation} CFO(C_i | S_{new}) = \\alpha \\cdot CCI \\cdot (1 - \\beta'), \\tag{12} \\end{equation}$\nwhere $\\alpha$ is a scaling hyperparameter that adjusts the recomputation factor. A higher value of CFO indicates a higher fraction of the tokens in $C_i$ needs KV recomputation: CFO = 1 for recomputing all tokens in $C_i$.\nSetting $\\alpha$ in deployment: As we lower the value of $\\alpha$, and corresponding $CFO_\\alpha$, in expectation we would employ less recomputation per request. This might lead to corresponding quality score $(F1_\\alpha)$ to go down below the acceptable level $(F1_{desired})$. We determine \u03b1 from a validation dataset by solving:\n$\\begin{equation} \\alpha^* = arg \\min_\\alpha E [CFO_\\alpha], subject to F1_\\alpha \\geq F1_{desired}. \\tag{13} \\end{equation}"}, {"title": "3.2.1 Token Selection for Recomputation", "content": "We have observed that a small subset of tokens in a chunk significantly impacts the CCI score (Fig. 14). Also, recomputing these critical tokens reduces output deviation (Fig. 15). Hence, to reuse a chunk-cache, we focus on recomputing the top N = [CFO(Ci) |Ci|] tokens with the highest inter-attention scores from prior chunks. We select the top-N contextualized tokens for chunk Ci as\n$\\begin{equation} T(C_i) = arg \\underset{t_k \\in C_i}{topN} \\{ inter(C_j, t_k) \\} \\tag{14} \\end{equation}$\nwhere $inter(t_k, C_j)$ denotes the inter-attention score between token tk in chunk Ci and a prefix chunk Cj. This method ensures the selection of the most contextualized tokens for recomputation."}, {"title": "3.2.2 Adaptive Early Recomputation Termination", "content": "In RAG pipelines, it is a standard practice to retrieve a sufficient number of chunks from a large knowledge base and utilize the LLM to filter out irrelevant content for coherent answers [76]. In CACHE-CRAFT, we leverage this characteristic to reduce runtime recomputation costs. In each layer l, during the recomputation of selected tokens, we monitor the attention between a chunk Ci and the current question U, i.e., interl (Ci, U), to identify the chunks that consistently receive \"focused\" attention from U as shown in Fig. 16. We find that while the inter-attention scores vary during the initial layers, after a certain number of layers, they settle into values that can segregate the focused chunks from the others. Fig. 17 illustrates that for approximately 80% of queries, focused chunks can be detected between layers 10 and 15 for the LLAMA-3-8B model. Consequently, we early-terminate the recomputation of tokens in the \"unfocused\" chunks to minimize unnecessary computations."}, {"title": "3.3 Cache Variants: Retrieval and Eviction", "content": "CACHE-CRAFT maintains a data structure, as shown in Fig. 18, for efficient lookup, retrieval, and eviction. Each chunk-cache is identified by hashing the original chunk texts linked to the RAG vector similarity search (\u00a71). This results in a map where chunk hashes serve as keys and lists of prefixes for each chunk are stored as values. CACHE-CRAFT targets to store NX M chunk-cache instances, starting with N chunks (the number of keys in the map), each having M variants. These variants help CACHE-CRAFT recover from cases where the initial chunk-cache may not be optimal (e.g., excessive token recomputation due to high contextualization), while subsequent chunk-cache variants may be more reusable for common contexts. Each variant stores the CCI value and an ordered list of token indices needing recomputation. To find the best chunk-cache for a request, CACHE-CRAFT calculates the reusability score CFO = CCI \u00d7 (1 - \u03b2') (as discussed in \u00a7 2.3) and selects the variant with the lowest score to minimize token recomputation.\nFor each chunk-cache access, CACHE-CRAFT updates its frequency-reuse (fr) as fr += 1/CFO. Consequently, chunk-caches with higher prefix matches or less contextualization become more reusable, as indicated by increasing fr over time. New variants are added when CACHE-CRAFT encounters a unique chunk and prefix until it reaches N\u00d7M instances. After this, CACHE-CRAFT periodically evicts caches with the lowest fr to make room for more effective variants. This allows diverse configurations, from one popular chunk with N\u00d7M variants to N \u00d7 M chunks, each with a single variant.\nThis design enables CACHE-CRAFT to manage storage dynamically, prioritizing caches that maximize reusability while minimizing recomputation, thus reducing prefill computation. Traditional policies like LRU, LFU, or FIFO do not offer this capability. The choice of M and N is influenced by the popularity and reusability of the chunk-caches, the RAG setting (i.e., the number of retrieved chunks), the architecture (GPU/CPU memory size and interconnects), and the deployment configuration of the LLM."}, {"title": "3.4 Chunk-Cache Reuse Pipeline", "content": "CACHE-CRAFT implements an efficient LLM inference pipeline to minimize redundant computations in RAG by strategically reusing chunk-caches across prefill requests."}, {"title": "3.4.1 Recomputation Planning", "content": "For a user query U, the prefill request consists of ordered chunks $C_1, C_2, ..., C_n$ provided by RAG. The system first queries the Metadata Store, a CPU-memory-based hash-table, to determine which chunks have their chunk-caches available. Based on this, the chunks are then classified into two subsets: $C_{hit}$ (with chunk-caches) and $C_{miss}$ (without chunk-caches).\nIt then generates an Inference Plan, designating chunks in $C_{miss}$ for chunk-cache computation and those in $C_{hit}$ for chunk-cache retrieval. It uses the metadata retrieved from the Metadata Store to compute the Adjusted Prefix Overlap score $(\\beta')$ and the Chunk Context Impact score (CCI) and then determines the Cache Fixing Overhead (CFO) (\u00a73.2). Finally, the top-N contextualized tokens T, that need to be recomputed for each $C_{hit}$ chunk-cache are identified.\nNote that both the Metadata Store and vLLM's KV-block manager are distinct CPU-memory hash-tables. While the Metadata Store tracks RAG chunk metadata, the KV-block hash-table maps tokens to their respective KV-blocks. Details on enabling independent chunk access without relying on prior prefixes are provided in \u00a74."}, {"title": "3.4.2 Layer-wise Preloading of cache-chunks", "content": "CACHE-CRAFT uses a hierarchical caching mechanism across GPU memory, CPU memory, and SSD to expand the effective memory capacity available to store the chunk-caches. To reduce average loading latency, the most frequently used chunk-caches are stored in GPU, less frequently ones in CPU, and infrequent ones in SSD. Further, CACHE-CRAFT uses a layer-wise preloading technique to minimize cache loading delays. While the GPU processes layer l, the chunk-caches for the layer l + 1 are concurrently loaded from host memory or SSD. Specifically, CACHE-CRAFT overlaps the loading of caches for Chit chunks for layer l + 1 with two activities: a) prefill computation of new Cmiss chunks and b) KV recomputation of tokens in Chit chunks in layer l. This ensures that by the time the GPU begins computing attention for layer l + 1, the corresponding chunk-caches are already available in the execution buffer.\nHowever, preloading may not fully overlap with computation if the chunk-cache loading time exceeds the computation time for a layer, particularly when loading from SSDs. To address this, CACHE-CRAFT reserves an HBM read buffer that allows preloading chunk-caches for multiple layers in advance. We determine the optimal preloading depth Lp as:\n$\\begin{equation} L_p = (L - 1) \\cdot (1 - \\frac{T_{prefill}}{T_{load}}) + 1, \\tag{16} \\end{equation}$\nwhere L is the total number of layers, Tprefill is prefill computation time and Tload is KV loading time. The goal is to preload Lp layers such that the chunk-caches for the remaining (L \u2013 Lp) layers can be loaded within the computation time for (L - 1) layers."}, {"title": "3.4.3 Handling Partial Prefill in LLM", "content": "For each layer, the Key-Value pairs (KV) for chunk-caches are fetched from HBM, while the K, V, and Q are computed only for new chunks and recomputation tokens. To support chunk-cache reuse across contexts, CACHE-CRAFT"}, {"title": "3.5 Hierarchical Chunk-Cache Management", "content": "CACHE-CRAFT manages cache storage efficiently across GPU High Bandwidth Memory (HBM), host (CPU) memory, and SSD so that less frequent caches are moved to further locations (from GPU HBM-memory to SSD) without deleting them when the LLM requires more GPU memory.\nTo offset the loading time of caches from non-HBM locations, CACHE-CRAFT employs preloading techniques that start to move the caches to GPU memory, asynchronously, while requests are still in the queue. If the caches are available in GPU memory when the request is ready to be executed, CACHE-CRAFT uses it; otherwise, it defaults to prefill from scratch starting from input text tokens. This technique ensures that highly reusable chunks remain in HBM while low-reuse chunks are progressively swapped to CPU-memory, and later to SSD, before eventual eviction, if not reused.\nUsing such asynchronous as well as layer-wise (\u00a7 3.4.2) preloading, CACHE-CRAFT significantly reduces loading delay to make chunk-caching effective. For example, the loading time required for 5 RAG chunk-caches corresponding to a request in Sys-X takes 0.03s for CPU and 0.59s for SSD. In Sys-X a typical queue wait time is 0.32s, allowing for preloading chunks from CPU or SSD without impacting latency significantly. For higher loads, queue time can completely mask the loading time even from the SSD.\nCache Scaling and Workload Adaptability: In production workloads, the chunk-cache size grows with question diversity"}, {"title": "4 Implementation", "content": "CACHE-CRAFT is a wrapper around vLLM [42], built on Xformers [44] backend optimized with Triton [70]. It enables chunk-cache reuse for prefix and non-prefix tokens by efficiently managing positional shifts and enabling partial recomputation of prefill.\nChunk Storage Management: CACHE-CRAFT manages chunk-caches by implementing a hash table at the granularity of individual RAG chunks. Unlike vLLM, which hashes entire prefixes pointing to the start of the KV cache (spanning multiple chunks), our approach generates independent hashes for each chunk, allowing direct access without dependence on prior context. Each chunk maps to a list of 16-token memory blocks for efficient and independent access. For optimized retrieval, the hash table stores address pointers across memory tiers, prioritizing faster tiers while allowing fallback to slower ones when necessary. Variable chunk sizes are padded to align with 16-token blocks, ensuring a consistent memory layout. Such padding causes negligible output deviation.\nRPE Management: To enable the reuse of chunk-caches in arbitrary positions, CACHE-CRAFT stores all cached chunks without RPE and dynamically applies corrected positional embeddings during runtime based on the current context. To efficiently manage large caches, CACHE-CRAFT employs a custom CUDA kernel to remove RPE from the Keys of the KV cache after processing each request. This kernel reverses the RPE operation, x cos(0) \u2013 y sin(0), by applying its inverse, y cos (0) +x sin(0), where x and y represent the upper and lower 64-dimensional components of each token's 128-dimensional embedding and e is the rotational angle. CACHE-CRAFT applies relative positional encoding (RPE) to cached chunks before attention computation and removes it after decoding, ensuring reusability across varying positions. In batched inference, it optimizes RPE handling by considering shared chunk positions within the batch. For requests with differing chunk positions, RPE is integrated directly into the attention mechanism during the prefill"}, {"title": "5 Evaluation", "content": ""}, {"title": "5.1 Experimental Set up", "content": ""}, {"title": "5.1.1 System Configuration", "content": "We evaluate CACHE-CRAFT on the LLAMA-3 8B and 70B models [24] with tensor parallelism (TP) of 1 and 4 respectively. All our experiments are performed on EC2 p4de.24xlarge [1] instances with 8 A100 GPUs [12] with each having 80 GB GPU (HBM) memory. The host CPU is an Intel Xeon Platinum 8275L processor with 48 cores (96 vCPUs). The instance has 1152 GB of main memory and an 8 TB NVMe SSD with a read throughput of 16 GB/s. The CPU and GPUs are interconnected via PCIe 4.0 \u00d7 16, providing 64 GB/s bandwidth."}, {"title": "5.1.2 Datasets and Workload", "content": "We evaluate our technique with a real production RAG workload (Sys-X) as well as relevant datasets following previous works [10, 40].\n(1) Real-world workloads: Sys-X helps users set up complex workflows for an enterprise SaaS by answering questions and prescribing steps from user manuals. It retrieves top-k=5 chunks based on the query. As Sys-X creates a chunk based on the subsections of the user manual, each of the chunks can have a highly variable number of tokens. This results in a total input size of 1k-20k tokens with a median of 3.3k tokens (Fig. 5a).\n(2) Single-Hop QnA: A question can be answered from a single chunk for this class of datasets. SQUAD [63] focuses on extracting answers from passages, while DROP [23] requires discrete reasoning over chunks. For multi-chunk RAG with k = 5, we selected 200 questions and split them into 512-token chunks.\n(3) Multi-Hop QnA: This class of datasets requires using facts and data from multiple chunks to answer each question properly. We utilize 2WikiMQA [35] and MuSiQue [72], which are benchmarks for evaluating complex answers across multiple documents. We sampled 200 questions.\n(4) Summarization: We use CNN dataset [58] that generates summaries of news articles from CNN, and XSUM [59] that focuses on single-sentence summaries from BBC. For sampling, we split long chunks into smaller segments and randomly selected top-k=5 chunks. This method is applied to 40 large chunks, resulting in 200 summarization tasks."}, {"title": "5.1.3 Evaluation Metrics", "content": "We use two quality metrics: ROUGE-L F1 [49], which measures long-answer quality in Single/Multi-Hop and summarization tasks, and Jaccard Similarity [37], which is used for short answers and True/False questions. We also conduct a user-study with 250 participants to assess response correctness and quality on 2wikiMQA and SQUAD datasets based on Yes/No ratings.\nNote, according to several prior studies [11, 50], a ROUGE-L F1 score \u2265 0.6 is considered good, and a score \u2265 0.8 is considered almost indistinguishable from the original answer. From our user study, we also analyzed this correlation and found that for answers with ROUGE-L F1 scores \u2265 0.6 and \u2265 0.8, 81% and 93% of users have given a YES, respectively. For efficiency, we measure Recompute Savings, Time-to-First-Token (TTFT i.e., prefill latency), System Throughput, and Cost Savings."}, {"title": "5.1.4 Baselines", "content": "We evaluate against the following baselines.\n(1) Prefix Matching: We compare with two methods: (1) PREFIX-CACHE [42], which reuses the KV cache based on exact prefix matches. While this approach offers perfect accuracy, it has low reuse potential. (2) SET-CACHE, which modifies RPE to reorder chunk-caches and finds the longest exact prefix match with the query. While this provides higher reuse, it has lower accuracy.\n(2) Naive KV Reuse (FULL-CACHE): This baseline reuses the KV cache for each chunk irrespective of the previous context, fixing only the RPE at the new position. No recomputation is performed for the chunks.\n(3) Recomputation Strategies: We also evaluate against recomputation methods: (1) RANDOM-RECOMP, which randomly recomputes tokens within each chunk, and (2) PREFILL-H2O [82], which recomputes the most-attended tokens in the chunks. For both strategies, we maintain the same average fraction of recomputed tokens as in CACHE-CRAFT.\n(4) Full Recomp (FULL-RECOMP): This oracle baseline fully recomputes all chunks for a request without utilizing any cache, providing a benchmark for optimal performance.\n(5) Compression Techniques: We compare with prefill compression methods: (1) LINGUA2 [38] that reduces prefill length by discarding less significant tokens using a trained model (e.g., GPT-2), and (2) MAPREDUCE [18] that summarizes context chunks for compression. The compression rates are aligned with CACHE-CRAFT's recomputation, where 80% compression corresponds to 20% recompute."}, {"title": "5.2 Generation Quality with KV Chunk Reuse", "content": ""}, {"title": "5.2.1 Evaluation of Recomputation Strategy", "content": "We evaluate the recomputation strategy of CACHE-CRAFT for Question Answering (Long and Short), True/False, and Summarization tasks using LLaMA-3-8B and LLAMA-3-70B models across multiple datasets.\nFig. 20 shows ROUGE-F1 scores, comparing CACHE-CRAFT with baseline KV-cache reuse techniques and the original LLAMA generation (i.e., FULL-RECOMP with ROUGE score=1). Using FULL-CACHE incurs no recomputation but yields low quality, ROUGE dropping to 0.65 for multi-hop QA datasets like 2wikiMQA and MuSiQue. In contrast, recomputing 20% of tokens with LLAMA-3-8B improves the ROUGE by 30%, and further by 42% with 30% recomputation. This trend is consistent across single-hop QA and summarization datasets, with \u224820-35% improvements for both 8B and 70B models. Moreover, increasing recomputation to 45% and 60% for LLAMA-3-8B, and 30% and 40% for LLAMA-3-70B, further improves ROUGE scores, reaching within 1-5% of FULL-RECOMP across all datasets.\nWe also compare our contextualization-based recomputation against RANDOM-RECOMP (random token selection) and PREFILL-H2O (high-attention token selection). Notably, random selection can lower performance even below FULL-CACHE as it neglects the key contextual tokens and overpowers wrong tokens, which can even shadow/underpower crucial ones. PREFILL-H2O shows only a modest 2-10% improvement over FULL-CACHE but struggles with multi-hop tasks. CACHE-CRAFT identifies and recomputes critical tokens distorted by prior contexts, enhancing performance and minimizing missing or incorrect facts. Fig. 21 further shows that CACHE-CRAFT outperforms FULL-CACHE by up to 50% in short-QA and True/False tasks, achieving ROUGE of 0.87, compared to 0.59.\nPREFIX-CACHE offers exact answers, but due to low prefix match rates, 80-95% of tokens go through regular KV-computation, leading to very low compute savings. SET-CACHE gives slightly more"}, {"title": "5.2.2 Comparison with Prompt Compression Techniques", "content": "We compare CACHE-CRAFT with established context reduction methods such as LINGUA2 [38] and MAPREDUCE [18], using datasets for multi-hop (2wikiMQA), single-hop (SQuAD), and summarization (XSUM), along with real-workload from Sys-X, on for LLaMA-3-8B. In Table 1, it can be observed that with 30% recomputation, CACHE-CRAFT gives ROUGE-F1 scores around 0.9, which is \u2248100% higher than the scores for LINGUA2 (0.4) and MapReduce (0.5), for 70% compression (i.e. comparable to 30% recomputation). The performance gap is due to LINGUA2 and MAPREDUCE's approach of discarding tokens, often losing critical information. In contrast, CACHE-CRAFT retains all tokens by leveraging chunk-cache reuse, ensuring no context is lost. Additionally, CACHE-CRAFT selectively recomputes the most contextually impacted tokens balancing efficiency and quality."}, {"title": "5.2.3 CACHE-CRAFT on Real Production RAG Workload", "content": "We evaluate CACHE-CRAFT on production RAG workloads from Sys-X focused on retrieval-based QA tasks where questions span multiple subsections of user manuals. As shown in Fig. 20, CACHE-CRAFT achieves a ROUGE score of 0.87 with only 20% token recomputation, outperforming FULL-CACHE reuse (0.59) and other recomputation strategies by about 20-30%. We also see that PREFIX-CACHE also saves just 18% prefill tokens, proving ineffective. Table 1 further compares CACHE-CRAFT with prompt compression techniques, where LINGUA2 (0.56) and MAPREDUCE (0.61) score significantly lower on the Sys-X dataset."}, {"title": "5.2.4 User Study", "content": "We conducted a user study with 250 participants with two datasets: 2wikiMQA and SQUAD.\\"}]}