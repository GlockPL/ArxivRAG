{"title": "BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex", "authors": ["Haidong Wang", "Pengfei Xiao", "Ao Liu", "Jianhua Zhang", "Qia Shan"], "abstract": "Drawing inspiration from neurosciences, artificial neural networks (ANNs) have evolved from shallow architectures to highly complex, deep structures, yielding exceptional performance in auditory recognition tasks. However, traditional ANNs often struggle to align with brain regions due to their excessive depth and lack of biologically realistic features, like recurrent connection. To address this, a brain-like auditory network (BAN) is introduced, which incorporates four neuroanatomically mapped areas and recurrent connection, guided by a novel metric called the brain-like auditory score (BAS). BAS serves as a benchmark for evaluating the similarity between BAN and human auditory recognition pathway. We further propose that specific areas in the cerebral cortex, mainly the middle and medial superior temporal (T2/T3) areas, correspond to the designed network structure, drawing parallels with the brain's auditory perception pathway. Our findings suggest that the neuroanatomical similarity in the cortex and auditory classification abilities of the ANN are well-aligned. In addition to delivering excellent performance on a music genre classification task, the BAN demonstrates a high BAS score. In conclusion, this study presents BAN as a recurrent, brain-inspired ANN, representing the first model that mirrors the cortical pathway of auditory recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "HEARING plays a vital role in human sound recognition and is especially important for the comprehension and creation of music. One key task in this domain is genre classification, which involves predicting the genre of a piece of music based on its audio signal. Music genres, such as jazz, rock, and classical, serve as descriptive labels that provide high-level information about a musical piece. As noted by previous work [2], genres are classes introduced by humans to categorize musical works. These genres are defined by shared characteristics among the music that belongs to them. These characteristics are generally linked to specific musical instruments, rhythmic patterns, and harmonic structures. Automating music genre classification can assist or even replace human effort in this process, making it a valuable tool in music information retrieval systems. As Nam [3] highlights, music genre recognition plays a important role in auditory recognition and recommendation algorithms for music streaming services. In solving the tasks of music classification and tagging, it is essential for the computer to recognize and identify musical patterns. The traditional framework's features are key to designing the pipeline, involving both feature engineering and classifier development. When humans distinguish music, they rely on features such as the sound of instruments or rhythm. Teaching machines to recognize these features has traditionally involved hand-engineering using domain-specific knowledge [4]. However, with the rise of deep learning, new approaches and improved performance have emerged. Deep learning methodologies reduce the need for extensive domain knowledge by automatically extracting features. Typically, deep learning models build pipelines through linear transformations, nonlinear activation functions, and optional pooling operations [5]. As data passes through these layers, the model learns to extract features, enabling end-to-end learning. Deep learning has also made significant strides in modeling neural mechanisms within the neuroscience community [6]. Remarkably, in artificial neural networks (ANNs) trained for image classification [7], the middle layers can partially explain why neurons in the visual cortex's middle layer respond to specific image features [8]-[13]. In addition, these networks can predict primate image classification behavior and cortical activations to some extent [14], [15]. Such brain-inspired models open up exciting possibilities to brain-computer interfaces, where these modles can infer expected responses along human cortical pathway [16]. To more accurately model brain processing, relying solely on traditional visual datasets for architectural improvements is becoming increasingly impractical. While deeper ANNs have significantly improved performance [17], [18], raising brain-like scores remains a challenge [14], [19]. Additionally, in earlier stages, only certain modules could be clearly mapped to specific areas of the visual pathway, and these connections between a few visual areas and numerous complex modules in architectures like GoogleNet [20] or other deep network are not well-defined. Furthermore, as networks for audio recognition tasks achieve higher accuracy, they tend to become progressively deeper, with only a few exceptions where brain-inspired architectures are used for audio recognition [21]. To deal with the interpretability challenges of auditory recognition models, we propose that aligning artificial neural networks (ANNs) with neuroanatomy can produce more interpretable, shallower, and brain-like neural networks, which we term the brain-like auditory recognition network (BAN)."}, {"title": "II. RELATED WORKS", "content": "As research in computer vision and human vision advances, there has been significant growth in brain-like vision studies. Compared to models of the brain's ventral stream, which have seen extensive development, image-computable models of the dorsal stream have been relatively scarce. Previous efforts to model the dorsal stream have involved training deep neural networks to detect motion [26] or classify actions [27] using video inputs. However, these models fail to fully align with the neuroscientific understanding that the dorsal stream is responsible for object localization and action guidance, often referred to as the \"where\u201d or \u201chow\" visual pathway. More recent work focused on training a dorsal-stream model to mimic human head movements during visual exploration [28]. Additionally, the predictive learning is employed to train parallel pathways, leading to the emergent development of ventral-like and dorsal-like representations as a result of structural segregation [29]. In addition to brain-like vision, numerous studies have explored brain-like approaches in other domains. These methods have successfully linked the structure of neural activity to computational functions in areas such as audition [30], olfaction [31], [32], thermosensation [33], perceptual discrimination [34], facial recognition [35], and navigation [36], [37]. Additionally, pioneering efforts have demonstrated the ability of simple brain-inspired controllers to replicate animal locomotion [38]-[40], illustrated how biomechanics can shape neural representations of movement [41], and uncovered similarities between the representation of movement in artificial and biological neural networks [42]-[44]. Our study leverages brain-like mechanisms, implemented through a recurrent network, to effectively extract audio features from multiple music fragments. Furthermore, we aim to design a brain-like network that not only achieves a high brain-like auditory score (BAS) but also surpasses existing recognition models on the Music Genres dataset [22]."}, {"title": "B. Auditory Recognition", "content": "Recent researches utilizing artificial neural networks (ANNs) have shed light on the principles based on the development of sensory functions in cortex [45]-[48]. It has been suggested that brain-like sensory encoding can emerge as a by-product of optimizing ANNs to process natural stimuli. For instance, ANN models trained to classify natural images have been shown to predicte visual cortical activations and even influence real neuron responses beyond their natural limits [8], [49], [50]. Similarly, a trained ANN to classify music was able to mimic human auditory cortical activations [30], suggesting that task-specific learning could serve as a efficient method for modeling auditory cortex functions. Additionally, research has explored how music-selectivity in neural circuits might develop as a by-product of adapting to natural audio processing [51]-[54], with the statistical patterns of natural sounds potentially shaping the brain's innate musical foundation. To further align brain-like models with human intelligence and neurosciences, the brain-like score has been introduced as an integrated benchmark for evaluating such models [55]. In this work, we fully leverage the latest advancements in audio perception and music classification techniques. By applying these approches, functional magnetic resonance imaging (fMRI) is processed from the Music Genres dataset [22] to identify cortical regions involved in music genre perception. Furthermore, we developed the BAS to assess the similarity between BAN activations and brain activations, providing a benchmark for evaluating their alignment."}, {"title": "III. BAN: BRAIN-LIKE AUDITORY NETWORK", "content": "In this part, we present the proposed BAN with three steps. Firstly, the design principles and aim behind our approach is outlined. Secondly, a detailed explanation of each component within the BAN pipeline is provided. Finally, the loss function used to train the BAN is introduced."}, {"title": "A. Design Criteria", "content": "We designed the BAN based on the following two criteria [56]: (1) Architecture: Among neural networks with same recognition performance, we prioritize brain-like networks due to their interpretability and their ability to align with anatomical constraints. We utilize ANNs as their neurons serve as fundamental units of information processing, and all neural responses in ANNs can be directly mapped to cortical activations [57]. Additionally, recurrent connections are naturally incorporated for auditory recognition because of the temporal nature of audio sequences. Since responses in auditory ventral pathway also exhibit temporal characteristics, the BAN is designed to generate activations over time. (2) Predictivity: The intermediate layers and final outputs of the model that aligns with neuroanatomical constraints (neural responses) exhibit more accurate behavior. It enables the proposed neural network to effectively infer both brain activations and category choice. Our goal is to achieve a high level of auditory recognition similarity between ANN and human brain. Moreover, auditory recognition in the human brain is reflected through human decision-making. In the auditory recognition pathway, the primary auditory cortex (A1) serves as the initial stage for preprocessing input signals, while the Belt and PB regions integrate sound signals across spatial dimensions, and the T2/T3 regions generate predictive auditory labels [58]-[60]. We now introduce the pipeline for BAN."}, {"title": "B. BAN Pipeline", "content": "Inspired by the brain's auditory recognition pathway, we establish a neuroanatomical mapping between cortical areas and ANN layers, as illustrated in Fig. 1. To facilitate neural network comparison, this mapping by identifying the ANN layer that best corresponds to activations in a specific cortical region is created. Ideally, these responses are inferred by the neural network without requiring unnecessary parameters. BAN consists of four parts: a convolutional layer and three recurrent layers, which correspond to the auditory recognition pathway regions A1, Belt, PB, and T2/T3. Additionally, the recognition predictor converts the output of T2/T3 into action selection activations, as shown in Fig. 2. This straightforward approach of explicitly segmenting brain regions is a key step in designing a brain-like auditory recognition model. We are also focused on discovering more generalized structures, such as a unified neural network model without distinct cortical regions or varied connections that could enhance BAS in future research. As shown in Fig. 2, we compute a decoding $c_t$ from the cochlea field based on the current input $F_t$, representing the ventral pathway responsible for extracting audio features. The auditory pathway selects a view $m_t$ for the cochlea response $C_t$, and this output is then transmitted to the ventral \"what\" cortical pathways. The ventral pathway is responsible for recognizing sounds within the auditory field. The BAN (Fig. 1) seeks to compete with state of art methods on the BAS by converting very deep feedforward structures into a shallow and recurrent network. Besides, BAN is inspired by ResNets, which are among the best models on behavioral benchmarks [22] and can be viewed as unrolled recurrent structure. Recent research has also shown that sharing weight in recurrent neural networks is feasible without significantly compromising performance [61]. Moreover, BAN is explicitly designed with an anatomical mapping to cortical regions. While model comparison establish this mapping by identifying the layer that best corresponds to neural activations in a specific cortical region, BAN aims to provide this mapping inherently, without the need for additional parameters. BAN features four computational areas, modeled after the auditory regions A1, Belt, PB, and T2/T3, along with a linear class decoder that translates neuron activity in the final auditory area into behavioral decisions. This initial assumption of distinct areas with recurrent circuitry was our first step toward creating a shallow model."}, {"title": "1) Primary Auditory Modules:", "content": "Typically, processed auditory signals reach A1 in the cortical temporal lobe, which is essential for processing fundamental sound features like pitch and volume. The input is a data matrix represented by a time-frequency representation $m_t$. In our work, this input is a Mel-spectrogram. Each auditory area is designed with specific neural circuitry where neurons perform fundamental computations, such as convolution, addition, normalization, nonlinearity, or pooling. The circuitry remains consistent across all auditory areas (except for $A1_{COR}$), though the total number of neurons varies in each region. To manage the high computational costs, the first region, $A1_{COR}$, applies a 7 \u00d7 7 convolution with a stride of 2, followed by 3 \u00d7 3 max pooling, and then a 3 \u00d7 3 convolution."}, {"title": "2) Belt and PB Modules:", "content": "The Belt and Parabelt (PB) areas are key regions within the auditory cortex, primarily responsible for processing complex auditory information. The $RNN_{Belt}$, surrounding $Conv_{A1}$, represents the first level of auditory cortex beyond $Conv_{A1}$ and receives direct inputs $U_t$ from the primary auditory cortex. As a secondary auditory processing region, $RNN_{Belt}$ is essential for analyzing more complex sound features than $Conv_{A1}$, integrating information from $Conv_{A1}$ to provide a more refined understanding of sounds, such as recognizing intricate patterns and spatial localization of sound sources. $RNN_{PB}$, located adjacent to $RNN_{Belt}$, represents an even higher level of auditory processing. It receives inputs $b_t$ from $RNN_{Belt}$ and connects with various brain regions involved in memory, emotion, and multisensory integration. $RNN_{PB}$ plays a crucial role in advanced auditory scene analysis, such as distinguishing multiple voices in a noisy environment or interpreting modulated sounds like music and speech. It also integrates auditory information with other sensory inputs, helping create a unified perception of the surrounding environment."}, {"title": "3) Middle and Superior Temporal Modules (T2/T3):", "content": "The $RNN_{T2/T3}$ cortices are crucial brain regions involved in multiple aspects of sensory processing, particularly auditory recognition. Located in the temporal lobe, these areas are essential for interpreting and comprehending complex auditory signals, such as music. T2 is primarily known for its role in auditory processing, particularly in perceiving motion and integrating audio-visual information. In auditory recognition, the T2 cortex becomes active when visual cues need to be integrated with auditory signals. As shown in TABLE II, T3, particularly the superior temporal gyrus (STG), plays a direct role in auditory processing and is essential for recognizing complex sounds, such as music. Within the STG, Heschl's gyrus (HG) is the first brain area to receive input audio, while surrounding regions, including the planum temporale (PT), are involved in processing higher-order sound features like language comprehension. The posterior part of the STG and the nearby superior temporal sulcus contribute to analyzing more complex sound attributes, such as prosody and the emotional content of speech. T3 also has extensive connections with other brain regions, supporting its role in integrating auditory information with other sensory modalities and cognitive functions. In auditory recognition, these areas collaborate to decode and interpret sounds, enabling individuals to effectively recognize and respond to various auditory stimuli, such as distinguishing speech sounds, understanding spoken language, and appreciating music. The $Belt_{COR}$, $PB_{COR}$ and $T2/T3_{COR}$ areas each perform two 1 \u00d7 1 convolutions, followed by a bottleneck-style 3 \u00d7 3 convolution, which expands the feature set fourfold, and concludes with another 1 \u00d7 1 convolution. Recurrence is implemented by passing the output of an region through that same region multiple times. For example, as shown by the \"gate\" in Fig. 1, after $Belt_{COR}$ processes the input feature once, the result is reprocessed by $Belt_{COR}$ as new input. As depicted in Fig. 3, $Belt_{COR}$ and $PB_{COR}$ are repeated twice respectively, while $T2/T3_{COR}$ is repeated four times, as this configuration was found to produce the best model performance with the fewest layers, according to our"}, {"title": "4) Classification Decoder:", "content": "The decoder in BAN uses a straightforward linear classifier, which consists of weighted sums, with one sum for every object label. To minimize the number of neural activations feeding into the classifier, the responses for each feature map is averaged."}, {"title": "C. Training Loss", "content": "The proposed BAN is trained by optimizing a combination of losses: a recognition loss and an auxiliary loss. The total BAN loss $L_b$ is defined as follows: $L_b = L_r + L_u,$ (1) where $L_r$ is recognition loss depicted in Equ. 2, $L_u$ is auxiliary loss described in Equ. 3. 1) Recognition Loss: To facilitate music recognition, we design the recognition loss to measure the difference between the output of BAN and the true label. This recognition loss $L_r$ is defined as: $L_r = \\frac{1}{N} \\sum_{i=1}^{N} (-(d_i *log(p_i) + (1 - d_i) *log(1 \u2013 p_i))),$ (2) where $L_r$ the cross-entropy loss between the ground truth $d_i$ and the predicted label $p_i$, N is the number of samples. 2) Auxiliary Loss: $L_2$ regularization is applied to both the dynamic parameter $\\phi_t(s_t)$ and the model parameter $\\theta$ to ensure proper regularization. $L_u = \\frac{1}{2} ||\\Phi_t||^2 + \\frac{1}{2} ||\\theta||^2$ (3)"}, {"title": "IV. BAS: BRAIN-LIKE AUDITORY SCORE", "content": "In this section, the BAS metrics is introduced, which assess the similarity between the ANN and brain. BAS is a measurement evaluated on given data, incorporating both cortical and behavioral metrics. To get quantitative measurement for cortical similarity, we reference the open-source platform Brain-Score [63] and introduce the BAS. It is a measurement that measure the performance to predict (a) the average behavioral choices when listening to target music clips from the Music Genre dataset [22], and (b) the average cortical activation at each brain region in response to the same music clips in the human auditory areas from the Music Genre neuroimaging dataset [22]. To provide a unified evaluation of BAN, we calculate the average of both behavioral and cortical measurement."}, {"title": "A. Cortical Metrics", "content": "Cortical metrics is utilized, which capture responses in regions of brain, such as activations in T2/T3, to assess how accurately BAN predicts the audio in ANN [8]. This measurement requires two sets of audio in the format of audio \u00d7 neuroids, where neuroids represent either model interlayer responses. A total of 540 audio from 10 different music were randomly shown to 5 subjects, and neural activations were recorded in the T2/T3 regions. Additionally, we identify and present the most predictive layers or areas, $RNN_{T2/T3}$, within BAN model. In our sutdy, relationships are established to map the ANN to cortical regions, and these relationships are used to infer neural activations to the given music clips. To speed up it, we compress the dimensionality of the activations to specific components using principal component analysis [64]. The responses in T2/T3 are used to learn these mappings. The final neural similarity score for the auditory recognition cortex is represented by the Pearson correlation coefficient $s_r$, calculated as follows: $S_r = \\frac{\\sum_{j=1}^n \\sum_{i=1}^N (Y_i - \\bar{Y}) (Y'_i - \\bar{Y'})}{\\sqrt{\\sum_{i=1}^N(Y_i \u2013 \\bar{Y})^2 \\sum_{i=1}^N(Y'_i \u2013 \\bar{Y'})^2}}$ (4) where y represents the actual neural activation, and y' is the model's predicted activation, n is the dimension of corresponding layer in BAN, N is the number of music sample used in our experiment. $\\bar{Y}$ and $\\bar{Y'}$ are the median values of the actual and predicted neural responses, respectively, across all data points."}, {"title": "B. Behavioral metrics", "content": "The goal of behavioral metrics is to assess the similarity between BAN's outputs and human behavior in music recognition tasks [22]. In human listening experiments, participants provide a music genre label, so the behavioral model is represented as a categorical label in auditory recognition. The primary focus is to achieve human-like intelligence, not just auditory classification accuracy [55]. BAN excels in behavioral similarity, accurately predicting both labels and neural activations. In contrast, while traditional ANNs may achieve excellent classification performance, they often fall short in delivering strong brain-like prediction performance. As outlined in Sec.V-B3, to compare brain activation patterns with behavioral performance, we gathered music genre choices from subjects through additional behavioral experiments. The corresponding output of the BAN is the predicted genre label. Thus, the music class predictivity, or behavioral score, is modeled as the similarity between the actual music genre choices made by the subjects and the predictions made by BAN. We calculate the overall behavioral metric $s_b$ across all audio sequences to determine the predictivity score, $S_b = \\frac{\\sum_{i=1}^{M} TP_i}{\\sum_{i=1}^{M}(TP_i + FP_i + FN_i)},$ (5) where $TP_i$ represents the number of true positives for music genres i (correct predictions), $FP_i$ represents the number of false positives for music genres i (incorrectly predicted as genres i), $FN_i$ represents the number of false negatives for class i (instances of class i incorrectly predicted as another music genres), M is the total number of genres."}, {"title": "C. Overall score", "content": "To assess the overall performance of the BAN, we use BAS, which combines both the behavioral metric and the T2/T3 cortical metrics. The BAS, denoted as $s_a$, is calculated as the average of these two scores. $Sa = \\frac{1}{3} [\\frac{min(m, n)}{max(m,n)} + S_r + S_b].$ (6) where m, n is the number of modules in cortical autitory recognition pathway and BAN, respectively. The first item as a whole measures the similarity between cortical and BAN structures. sr is the similarity of cortical metrics depicted in Equ. 4, sb is the similarity of behavioral metrics described in Equ. 5. The BAS is designed without normalization across different score magnitudes, as this could unfairly punishment with small variances. Instead, every score is treated equally to ensure fair significance in the overall BAS calculation."}, {"title": "V. EXPERIMENTS", "content": "We validate the effectiveness of our designed BAN through three steps. Firstly, the datasets and provide details on the implementation of our model is introduced. Secondly, that BAN functions as a valid, brain-like auditory recognition model through circuitry analysis is shown. Thirdly, the model's classification accuracy and its representation of music genres in both BAN and the cortex is discussed."}, {"title": "A. Datasets", "content": "We used the GTZAN dataset, which is one of the most widely utilized in music genre recognition tasks [22]. The dataset consists of 30-second audio files spanning 10 different genres: blues, reggae, classical, rock, country, disco, jazz, pop, metal, and hip-hop. From the original collection, we randomly selected 54 music pieces from each genre, resulting in a total of 540 music pieces for the study. All clips were normalized based on their root mean square values. The fMRI experiment included 12 training runs and 6 test runs, for a total of 18 runs. Each run lasted 10 minutes and consisted of 40 music clips. In the training phase, 480 music clips were used, while the remaining 60 clips were reserved for the test runs. During each test run, a set of 10 music clips was shown four times in the same order. There is no repetition of clips in the training runs."}, {"title": "B. Human Data Analysis", "content": "To assess the similarity between the ANN and human auditory recognition, we extract task-related activations and then identify the corresponding brain regions for comparison. 1) fMRI Data Processing: Motion correction is applied to each run using the Statistical Parameter Mapping toolbox (SPM 12), with all volumes aligned to the first image for every participant. We remove Low-frequency drift with a filter using a 240-second window. To improve method accuracy, the activation for each voxel is normalized by subtracting the mean and variance. Cortical surfaces are identified using FreeSurfer [65], [66], which registers the anatomical data with functional voxels. For analysis, only cortical voxels are used as targets, and for a participant, we focus on the voxels identified within the cortex. 2) Genre Representation Region: To obtain reliable estimates of human areas associated with music genres, we use the these step: Firstly, all activation recordings is randomly split into training sets (75%) and test sets (25%). Utilizing the optimal weights from the genre-label model [67], we fit an encoding model with genre-label features using the training data and assess model accuracy with the test sets. Parameter fitting is done with general linear model. The random resampling is executed 100 times, and voxels that show prediction accuracy in more than 75% of repetitions are picked out for region of interest. As a result, we induce 473 voxels in the region of interest for participant, 468 for sub-01, 581 for sub-02, 1,593 for sub-03, and 529 for sub-05. And all subsequent analyses use these extracted region of interest. 3) Behavioral Data Collection: To verify that cortical activation in response to music was linked to behavioral accuracy in musci recognition, we utilized additional behavioral data [22]. As shown in TABLE I, these experiments took place in a soundproof place with the same participants from fMRI study. Participants first listened to 3 original 30-second music for each genre, selected from the 460 clips randomly not utilized in fMRI test, as a reference. During this training session, participants were informed of the correct genres. They listened to the 60 audio utilized in fMRI experiment and classified each clip's genre by selecting one of 10 options on an answer sheet. Each audio was played once, in the same sequence as in the fMRI test."}, {"title": "C. Implementation Details", "content": "However, we determined that the available data was insufficient to effectively train the parameters of the CNN. As a result, we applied data augmentation techniques to enhance the dataset. Data augmentation is the process of generating new synthetic training samples by applying small modifications to the original dataset. The goal is to make the model robust to these variations and improve its generalization ability. For this approach to be effective, the added perturbations must preserve the original label of the training sample. Common techniques include adding noise, shifting the audio start point, altering speed, and changing pitch. The Mel-frequency cepstral coefficient (MFCC) model is utilized to extract various genres-related information from the dataset [68]. For timbral and loudness information, the frame duration is set to 25 ms, with a 50% overlap between adjacency. For tonal and rhythm information, the frame duration is 3 seconds, with a 33% overlap, and each feature"}, {"title": "D. Circuitry and Ablation Analysis", "content": "While we believe that BAN offers a closer approximation to the anatomy of the auditory pathway compared to current ANNs-primarily due to its limited number of regions and inclusion of recurrence-it remains incomplete in several aspects. From a neuroscientific perspective, in addition to lacking biologically plausible learning methods, an ideal method of the auditory stream would incorporate more anatomical and circuitry-level details, such as the cochlea or the medial geniculate nucleus. Similarly, the addition of skip connections was not based on brain circuitry properties but rather adopted from complexity gradient [71] as a solution to the degradation problem in deep model. It's important to note that not all structural choices are effective. We tested thousands of structures before identifying the BAN circuitry, as shown in Fig. 3."}, {"title": "E. Classification Accuracy based on BAN", "content": "To verify that the brain activity of participants and BAN captured enough information to differentiate between music label in the experimental launch, we executed music recognition using a decoding model based on brain and BAN activity. As shown in Fig. 4, we evaluated the confusion matrix and recognition accuracy (the diagonal elements of the confusion matrix) by analyzing brain response within genre representation region of interest. The recognition results varied across music label, with classical music consistently classified accurately (average classification accuracy of 100%), while rock music showed poor classification performance across participants (40%). Additionally, participants often misclassified reggae as rock (confusion rate of 33.3%) and rock as country (confusion rate of 28.6%). The confusion matrices derived from activity were highly consistent across participants (Spearman's correlation coefficient, p = 0.562 \u00b1 0.087; p < 0.001 for all participant combinations). As shown in Fig. 6, Grad-CAM uses the gradient of the classification score with respect to the BAN features to identify which parts of the input are most critical for classification. Regions with high gradients indicate areas where the final score is most influenced by the data. LIME approximates BAN's classification behavior using a simpler, more interpretable model, such as a linear model or a regression tree, to assess the importance of input features as a"}, {"title": "F. Genre Representation in BAN and Cortex", "content": "The cortical areas involved in genre representation were evaluated using BAN. Significant prediction accuracy was observed in the bilateral STG across all participants (p < 0.05, FDR corrected). To identify brain region that consistently shown music labels, regardless of sample selection, we determined the genre-representing functional region of interest for each participant utilizing a resampling procedure. This analysis confirmed significant prediction accuracy in the bilateral STG, and the functional region of interest was utilized as an mask in subsequent analyses. To evaluate the contribution of each brain voxel to 10 music label, we mapped genre representations on the cortical surface using principal component analysis with BAN weights. For each voxel within the region of interest, we extracted the learned BAN weights and applied principal component analysis for dimensionality reduction on the aggregated BAN features. The score indicated how the 10 principal components were shown in each brain voxel, while the loading matrix showed the contribution of each principal component to the representation of 10 music genres. To visualize the cortical organization of music genres for each participant, we extracted and normalized the principal component analysis scores from their respective voxels. This analysis revealed various genre-specific representations within the bilateral STG. As shown in TABLE II, music genres were more distinctly represented in Heschl's sulcus and the lateral STG compared to Heschl's gyrus, the planum temporale, or the lateral sulcus (with the exception of sub-03, who exhibited genre-specific activations in large brain regions, including planum temporale). Despite individual variability, a consistent"}, {"title": "VI. CONCLUSION", "content": "Inspired by the auditory processing mechanisms of the human brain, our study introduces a brain-like model tailored for auditory recognition tasks. By incorporating neuroanatomical constraints, the model achieves strong recognition performance with enhanced interpretability. Additionally, we demonstrate how the cortical model aligns with human auditory recognition of real music and propose a new method for calculating the similarity between model activations and brain responses. In addition, the neuroanatomically aligned model offers improved predictions of cortical responses. We believe that the proposed BAN will inspire new approaches in ANN interpretability and potentially drive advancements in brain-computer interfaces."}]}