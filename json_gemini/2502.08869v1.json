{"title": "Harnessing Vision Models for Time Series Analysis: A Survey", "authors": ["Jingchao Ni", "Ziming Zhao", "ChengAo Shen", "Hanghang Tong", "Dongjin Song", "Wei Cheng", "Dongsheng Luo", "Haifeng Chen"], "abstract": "Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.", "sections": [{"title": "1 Introduction", "content": "Vision models have historically been used for time series analysis. Since 1-dimensional (1D) convolutional neural networks (CNNs), such as WaveNet [Van Den Oord et al., 2016], were found effective in sequence modeling [Bai et al., 2018], they have been extensively adapted to various time series tasks [Koprinska et al., 2018; Zhang et al., 2020]. Recently, with the significant advances of sequence modeling in the language domain, growing research attentions on time series have been drawn to methods ranging from Transformers [Wen et al., 2023] to Large Language Models (LLMs) [Zhang et al., 2024]. Meanwhile, the demands for universal modeling have spurred on an explosion of works on time series foundation models, such as TimesFM [Das et al., 2024], Chronos [Ansari et al., 2024] and Time-MoE [Shi et al., 2024].\nAs Large Vision Models (LVMs), such as ViT [Dosovitskiy et al., 2021], BEiT [Bao et al., 2022] and MAE [He et al., 2022], become achieving a similar success as LLMs (but in vision domain), a great deal of emergent efforts has been invested to explore the potential of LVMs in time series modeling [Chen et al., 2024]. This is inspired by the plenty of ways for visualizing time series as images such as line plots of univariate time series (UTS) and heatmaps of multivariate time series (MTS). Such images provide a more straightforward view of time series than the counterpart textual representations to humans and, presumably, AI bots.\nTaking a closer inspection reveals more advantages favoring LVMs over LLMs: (1) There is an inherent relationship between images and time series \u2013 each row/column in an image (per channel) is a sequence of continuous pixel values. By pre-training on massive images, LVMs may have learned important sequential patterns such as trends, periods, and spikes [Chen et al., 2024]. In contrast, LLMs are pre-trained on discrete tokens, thus are less aligned with continuous time series. In fact, LLMs' effectiveness on time series modeling is in question [Tan et al., 2024]; (2) Instead of using channel-independence assumption [Nie et al., 2023] to individually model each variate in an MTS, some imaging methods (\u00a73.7) can naturally represent MTS, enabling explicit correlation encoding; (3) When prompting LLMs, existing methods often struggle with properly verbalizing a long sequence (or a ma-"}, {"title": "2 Preliminaries and Taxonomy", "content": "In this paper, a UTS is represented by $\\mathbf{x} = [x_1,...,x_T] \\in \\mathbb{R}^{1\\times T}$ where T is the length of the UTS, $x_t$ ($1 \\leq t \\leq T$) is the value at time step t. Suppose there are d variates (or features), let $x_i \\in \\mathbb{R}^{1\\times T}$ ($1 \\leq i \\leq d$) be a UTS of the i-th variate, an MTS can be represented by $\\mathbf{X} = [\\mathbf{x_1}, ..., \\mathbf{x_d}]^\\top \\in \\mathbb{R}^{d\\times T}$.\nAs illustrated in Fig. 1, this survey focuses on methods that transform time series to images, namely imaged time series, and then apply vision models on the imaged time series for tackling time series tasks, such as classification, forecasting and anomaly detection. It is noteworthy that methods on videos or sequential images (a.k.a. image time series [Tarasiou et al., 2023]) do not belong to this category because they don't transform time series to images. Similarly, methods for spaciotemporal traffic data are out of our scope if the meth-\nhttps://github.com/D2I-Group/awesome-vision-time-series\nods focus on streams of images (e.g., traffic flows in a stream of grid maps [Zhang et al., 2017]), but methods on imaging time-space matrices [Ma et al., 2017] that resemble MTSs are included. For vision models on audios, this survey only discusses some representative works in \u00a73.3 due to space limit. The focus of the survey will remain on general time series."}, {"title": "2.1 Taxonomy", "content": "We propose a taxonomy from the two views of Time Series to Image Transformation (\u00a73) and Imaged Time Series Modeling (\u00a74) as illustrated in Fig. 1. For the former, we discuss 5 primary methods for imaging UTS or MTS, and remark on their pros and cons. For the latter, we classify the existing methods by conventional vision models, LVMs and LMMs. We discuss their strategies on pre-training, fine-tuning, prompting, and the deigns of task-specific heads. We also discuss the challenges and solutions in pre-/post-processing in \u00a75. In the following two sections, we will delve into the existing methods from the two views."}, {"title": "3 Time Series To Image Transformation", "content": "This section summarizes the methods for imaging time series (\u00a73.1-\u00a73.6) and their extensions to encode MTSs (\u00a73.7)."}, {"title": "3.1 Line Plot", "content": "Line Plot is a straightforward way for visualizing UTSs for human analysis (e.g., stocks, power consumption, etc.). As illustrated by Fig. 2(a), the simplest approach is to draw a 2D image with x-axis representing time steps and y-axis representing time-wise values, with a line connecting all values of the series over time. This image can be either three-channel (i.e., RGB) or single-channel as the colors may not be informative [Cohen et al., 2020; Sood et al., 2021; Jin et al., 2023; Zhang et al., 2023]. ForCNN [Semenoglou et al., 2023] even uses a single 8-bit integer to represent each pixel for black-white images. So far, there is no consensus on whether other graphical components, such as legend, grids and tick labels, could provide extra benefits in any task. For example, ViTST [Li et al., 2023b] finds these components are superfluous in a classification task, while TAMA [Zhuang et al., 2024] finds grid-like auxiliary lines help enhance anomaly detection.\nIn addition to the regular Line Plot, MV-DTSA [Yang et al., 2023] and ViTime [Yang et al., 2024] divide an image into h \u00d7 L grids, and define a function to map each time step of a UTS to a grid, producing a grid-like Line Plot. Also, we include methods that use Scatter Plot [Daswani et al., 2024; Prithyani et al., 2024] in this category because a Scatter Plot resembles a Line Plot but doesn't connect data points with a line. By comparing them, [Prithyani et al., 2024] finds a Line Plot could induce better time series classification.\nFor MTSs, we defer the discussion on Line Plot to \u00a73.7."}, {"title": "3.2 Heatmap", "content": "As shown in Fig. 2(b), Heatmap is a 2D visualization of the magnitude of the values in a matrix using color. It has been used to represent the matrix of an MTS, i.e., $\\mathbf{X} \\in \\mathbb{R}^{d\\times T}$, as a one-channel d \u00d7 T image [Li et al., 2022; Yazdanbakhsh and Dick, 2019]. Similarly, TimEHR [Karami et al., 2024] represents an irregular MTS, where the intervals"}, {"title": "3.3 Spectrogram", "content": "A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time, which are extensively used for analyzing audio signals [Gong et al., 2021]. Since audio signals are a type of UTS, spectrogram can be considered as a method for imaging a UTS. As shown in Fig. 2(c), a common format is a 2D heatmap image with x-axis representing time steps and y-axis representing frequency, a.k.a. a time-frequency space. Each pixel in the image represents the (logarithmic) amplitude of a specific frequency at a specific time point. Typical methods for producing a spectrogram include Short-Time Fourier Transform (STFT) [Griffin and Lim, 1984], Wavelet Transform [Daubechies, 1990], and Filterbank [Vetterli and Herley, 1992].\nSTFT. Discrete Fourier transform (DFT) can be used to describe the intensity f(w) of each constituent frequency w of a UTS signal $x \\in \\mathbb{R}^{1\\times T}$. However, f (w) has no time dependency. It cannot provide dynamic information such as when a specific frequency appear in the UTS. STFT addresses this deficiency by sliding a window function g(t) over the time steps in x, and computing the DFT within each window by\n$f(w, \\tau) = \\sum_{t=1}^T x_t g(t - \\tau)e^{-iwt}$ (1)\nwhere w is frequency, $\\tau$ is the position of the window, f (w, $\\tau$) describes the intensity of frequency w at time step $\\tau$.\nBy selecting a proper window function g(\u00b7) (e.g., Gaussian/Hamming/Bartlett window), a 2D spectrogram (e.g., Fig. 2(c)) can be drawn via a heatmap on the squared values $|f(w,\\tau)|^2$, with w as the y-axis, and as the x-axis. For example, [Dixit et al., 2024] uses STFT based spectrogram as an input to LMMs for time series classification.\nWavelet Transform. Continuous Wavelet Transform (CWT) uses the inner product to measure the similarity between a signal function x(t) and an analyzing function. The analyzing function is a wavelet $\\psi(t)$, where the typical choices include Morse wavelet, Morlet wavelet, etc. CWT compares x(t) to the shifted and scaled (i.e., stretched or shrunk) versions of the wavelet, and output a CWT coefficient by\n$C(s,\\tau) = \\frac{1}{\\sqrt{s}} \\int_{-\\infty}^{\\infty} x(t) \\psi^*(\\frac{t-\\tau}{s}) dt$ (2)\nwhere * denotes complex conjugate, $\\tau$ is the time step to shift, and s represents the scale. In practice, a discretized version of CWT in Eq. (2) is implemented for UTS $[x_1, ..., x_T]$.\nIt is noteworthy that the scale s controls the frequency encoded in a wavelet a larger s leads to a stretched wavelet with a lower frequency, and vice versa. As such, by varying s and $\\tau$, a 2D spectrogram (e.g., Fig. 2(d)) can be drawn on C(s,$\\tau$), where s is the y-axis and $\\tau$ is the x-axis. Compared to STFT, which uses a fixed window size, Wavelet Transform allows variable wavelet sizes \u2013 a larger size for more precise low frequency information. Thus, the methods in [Du et al., 2020; Namura et al., 2024; Zeng et al., 2023] choose CWT (with Morlet wavelet) to generate the spectrogram.\nFilterbank. This method resembles STFT and is often used in processing audio signals. Given an audio signal, it firstly goes through a pre-emphasis filter to boost high frequencies, which helps improve the clarity of the signal. Then, STFT is applied on the signal. Finally, multiple \"triangle-shaped\" filters spaced on a Mel-scale are applied to the STFT power spectrum $|f(w, \\tau)|^2$ to extract frequency bands. The outcome filterbank features f (w, t) can be used to yield a spectrogram with w as the y-axis, and as the x-axis.\nFilterbank was adopted in AST [Gong et al., 2021] with a 25ms Hamming window that shifts every 10ms for classifying audio signals using Vision Transformer (ViT). It then becomes widely used in the follow-up works such as SSAST [Gong et al., 2022], MAE-AST [Baade et al., 2022], and AST-SED [Li et al., 2023a], as summarized in Table 1."}, {"title": "3.4 Gramian Angular Field (GAF)", "content": "GAF was introduced for classifying UTSs using CNNs by [Wang and Oates, 2015a]. It was then extended to an impu-"}, {"title": "3.5 Recurrence Plot (RP)", "content": "RP [Eckmann et al., 1987] encodes a UTS into an image that captures its periodic patterns by using its reconstructed phase\nspace. The phase space of $x \\in \\mathbb{R}^{1\\times T}$ can be reconstructed by time delay embedding \u2013 a set of new vectors $\\mathbf{v_1}, ..., \\mathbf{v_l}$ with\n$\\mathbf{v_t} = [x_t, x_{t+\\Gamma}, x_{t+2\\Gamma}, ..., x_{t+(m-1)\\Gamma}]\\in \\mathbb{R}^{m}$, $1\\leq t \\leq l$ (4)\nwhere $\\Gamma$ is the time delay, m is the dimension of the phase space, both are hyperparameters. Hence, $l = T - (m - 1)\\Gamma$. With vectors $\\mathbf{v_1}, ..., \\mathbf{v_l}$, an RP image measures their pairwise distances, results in an l \u00d7 l image whose element\n$RP_{i,j} = \\Theta(\\epsilon - ||\\mathbf{v_i} - \\mathbf{v_j}||)$, $1\\leq i, j \\leq l$ (5)\nwhere $\\Theta(\\cdot)$ is the Heaviside step function, $\\epsilon$ is a threshold, and $|| \\cdot ||$ is a norm function such as l2 norm. Eq. (5) generates a binary matrix with $RP_{i,j} = 1$ if $\\mathbf{v_i}$ and $\\mathbf{v_j}$ are sufficiently similar, producing a black-white image (e.g., Fig. 2(f)).\nAn advantage of RP is its flexibility in image size by tuning m and $\\Gamma$. Thus it has been used for time series classification [Silva et al., 2013; Hatami et al., 2018], forecasting [Li et al., 2020], anomaly detection [Lin et al., 2024] and explanation [Kim et al., 2024]. Moreover, the method in [Hatami et al., 2018], and similarly in HCR-AdaAD [Lin et al., 2024], omit the thresholding in Eq. (5) and uses $| |\\mathbf{v_i} - \\mathbf{v_j}||$ to produce continuously valued images to avoid information loss."}, {"title": "3.6 Other Methods", "content": "Additionally, [Wang and Oates, 2015a] introduces Markov Transition Field (MTF) for imaging a UTS. MTF is a matrix $M\\in \\mathbb{R}^{Q\\times Q}$ encoding the transition probabilities over time segments, where Q is the number of segments. ImagenTime [Naiman et al., 2024] stacks the delay embeddings $\\mathbf{v_1}, ..., \\mathbf{v_l}$ in Eq. (4) to an l \u00d7 mr matrix for visualizing UTSs. MS-CRED [Zhang et al., 2019] uses heatmaps on the d \u00d7 d correlation matrices of MTSs with d variates for anomaly detection. Furthermore, some methods use a mixture of imaging methods by stacking different transformations. [Wang and Oates, 2015b] stacks GASF, GADF, MTF to a 3-channel image. FIRTS [Costa et al., 2024] builds a 3-channel image by stacking GASF, MTF and RP. The mixture methods encode a UTS with multiple views and were found more robust than single-view images in these works for classification tasks."}, {"title": "3.7 How to Model MTS", "content": "In the above methods, Heatmap (\u00a73.2) can be used to visualize the variate-time matrices, X, of MTSs (e.g., Fig. 1(b)), where correlated variates should be spatially close to each other. Line Plot (\u00a73.1) can be used to visualize MTSs by plotting all variates in the same image [Wimmer and Rekabsaz, 2023; Daswani et al., 2024] or combining all univariate images to compose a bigger image [Li et al., 2023b], but these methods only work for a small number of variates. Spectrogram (\u00a73.3), GAF (\u00a73.4), and RP (\u00a73.5) were designed specifically for UTSs. For these methods and Line Plot, which are not straightforward in imaging MTSs, the general approaches include using channel independence assumption to model each variate individually [Nie et al., 2023], or stacking the images of d variates to form a d-channel image [Naiman et al., 2024; Kim et al., 2024]. However, the latter does not fit some vision models pre-trained on RGB images which requires 3-channel inputs (more discussions are deferred to \u00a75)."}, {"title": "4 Imaged Time Series Modeling", "content": "With image representations, time series analysis can be readily performed with vision models. This section discusses such solutions from the traditional models to the SOTA models."}, {"title": "4.1 Conventional Vision Models", "content": "Following traditional image classification, [Silva et al., 2013] applies a K-NN classifier on the RPs of time series, [Cohen et al., 2020] applies an ensemble of fundamental classifiers such as SVM and AdaBoost on the Line Plots for time series"}, {"title": "4.2 Large Vision Models (LVMs)", "content": "Vision Transformer (ViT) [Dosovitskiy et al., 2021] has inspired the development of modern LVMs such as Swin [Liu et al., 2021], BEIT [Bao et al., 2022], and MAE [He et al., 2022]. As Fig. 3(b) shows, ViT splits an image into patches of fixed size, then embeds each patch and augments it with a positional embedding. The vectors of patches are processed by a Transformer as if they were token embeddings. Compared to CNNs, ViTs are less data-efficient, but have higher capacity. Thus, pre-trained ViTs have been explored for modeling imaged time series. For example, AST [Gong et al., 2021] fine-tunes DeiT [Touvron et al., 2021] on the filterbank spetrogram of audios for classification tasks and finds ImageNet-pretrained DeiT is remarkably effective in knowledge transfer. The fine-tuning paradigm has also been adopted in [Zeng et al., 2023; Li et al., 2023b] but with different pre-trained models such as Swin by [Li et al., 2023b]. VisionTS [Chen et al., 2024] attributes LVMs' superiority over LLMs in knowledge transfer to the small gap between the pre-trained images and imaged time series. It finds that with one-epoch fine-tuning, MAE becomes the SOTA time series forecasters on some benchmark datasets.\nSimilar to time series foundation models such as TimesFM [Das et al., 2024], there are some initial efforts in pre-training ViT architectures with imaged time series. Following AST,"}, {"title": "4.3 Large Multimodal Models (LMMs)", "content": "As LMMs get growing attentions, some notable LMMs, such as LLaVA [Liu et al., 2023], Gemini [Team, 2023], GPT-4O [Achiam et al., 2023] and Claude-3 [Anthropic, 2024], have been explored to consolidate the power of LLMs and LVMs in time series analysis. Since LMMs support multimodal input via prompts, methods in this thread typically prompt LMMs with the textual and imaged representations of time series, and instructions on what tasks to perform (e.g., Fig. 3(c)).\nInsightMiner [Zhang et al., 2023] is a pioneer work that uses the LLaVA architecture to generate texts describing the trend of each input UTS. It extracts the trend of a UTS by Seasonal-Trend decomposition, encodes the Line Plot of the trend, and concatenates the embedding of the Line Plot with the embeddings of a textual instruction, which includes a sequence of numbers representing the UTS, e.g., \u201c[1.1, 1.7, ..., 0.3]\". The concatenated embeddings are taken by a language model for generating trend descriptions. Similarly, [Prithyani et al., 2024] adopts the LLaVA architecture, but for MTS classification. An MTS is encoded by the visual embeddings of the stacked Line Plots of all variates. The matrix of the MTS is also verbalized in a prompt as the textual modality. By integrating token embeddings, both methods fine-tune some layers of the LMMs with some synthetic data.\nMoreover, zero-shot and in-context learning performance of several commercial LMMs have been evaluated for audio classification [Dixit et al., 2024], anomaly detection [Zhuang et al., 2024], and some synthetic tasks [Daswani et al., 2024], where the image and textual representations of a query time series are integrated into a prompt. For in-context learning, these methods inject the images of a few example time series and their labels (e.g., classes) into an instruction to prompt LMMs for assisting the prediction of the query time series."}, {"title": "4.4 Task-Specific Heads", "content": "For classification tasks, most of the methods in Table 1 adopt a fully connected (FC) layer or multilayer perceptron (MLP) to transform an embedding into a probability distribution over all classes. For forecasting tasks, there are two approaches: (1) using a $d_e \\times W$ MLP/FC layer to directly"}, {"title": "5 Pre-Processing and Post-Processing", "content": "To be successful in using vision models, some subtle design desiderata include time series normalization, image alignment and time series recovery.\nTime Series Normalization. Vision models are usually trained on standardized images. To be aligned, the images introduced in \u00a73 should be normalized with a controlled mean and standard deviation, as did by [Gong et al., 2021] on spectrograms. In particular, as Heatmap is built on raw time series values, the commonly used Instance Normalization (IN) [Kim et al., 2022] can be applied on the time series as suggested by VisionTS [Chen et al., 2024] since IN share similar merits as Standardization. Using Line Plot requires a proper range of y-axis. In addition to rescaling time series [Zhuang et al., 2024], ViTST [Li et al., 2023b] introduced several methods to remove extreme values from the plot. GAF requires min-max normalization on its input, as it transforms time series values withtin [0, 1] to polar coordinates (i.e., arccos). In contrast, input to RP is usually normalization-free as an l2 norm is involved in Eq. (5) before thresholding.\nImage Alignment. When using pre-trained models, it is imperative to fit the image size to the input requirement of the models. This is especially true for Transformer based models as they use a fixed number of positional embeddings to encode the spacial information of image patches. For 3-channel RGB images such as Line Plot, it is straightforward to meet a pre-defined size by adjusting the resolution when producing the image. For images built upon matrices such as Heatmap, Spectrogram, GAF, RP, the number of channels and matrix size need adjustment. For the channels, one method is to duplicate a matrix to 3 channels [Chen et al., 2024], another way"}, {"title": "6 Challenges and Future Directions", "content": "Fundamental Understanding. Given the multiple methods for imaging time series, the existing works usually pick their own choice by intuition. There remains a gap in both theoretical and empirical understanding of research questions such as which imaging methods fit what tasks and whether LVMs truly learn patterns from the images that make them more suitable than LLMs in time series modeling. Some existing works evaluate multiple imaging methods, but in limited tasks. For example, ImagenTime [Naiman et al., 2024] compares the representation abilities of GAF, STFT, and delay embedding (\u00a73.6) in a time series generation task. However, a thorough understanding that can guide future developments of LVMs and LMMs on top of different imaging methods is absent. This survey provides an initial comparative discussion of these methods in \u00a73. Further investigations with empirical validation and theoretical justification is essential to the synergy between LVMs/LMMs and time series analysis.\nModeling the Correlation of Variates in MTS. In \u00a73.7, we discussed the existing methods for imaging MTSs. However, each of them has its limitation. For example, when visualizing a variate-time matrix by a Heatmap image (e.g., Fig. 2(b)), the row a variate locates at matters to the downstream modeling of correlations. This is because vision models only encode the spatial relationships of pixels thus correlated variates should be spatially close to each other. Similarly, Line Plots does not enable explicit modeling of correlated variates by vision models. Stacking d images, one per variate, into\na d-channel input may disable the chance to use pre-trained LVMs due to their fixed 3-channel RGB input. As such, effective methods at either the imaging step or the modeling step (e.g., leveraging graph neural networks (GNNs) on variates) that allow correlation learning from MTSs are in demand.\nAdvanced Imaging for Time Series. In addition to the basic methods introduced in \u00a73, it is promising to explore more advanced image representations. For example, InsightMiner [Zhang et al., 2023] adopts Seasonal-Trend decomposition, which is often used to extract components that can serve as inductive bias for time series models. Generalizing it to decompose images such as Spectrogram, GAF, RP into fine-grained representations may further boost vision models' ability in time series analysis. Moreover, mixture of imaging may enable encoding of information from different views, such as frequency (Spectrogram), temporal relationships (GAF) and recurrence patterns (RP). FIRTS [Costa et al., 2024] stacks a mixture of images in multiple channels for a classification task, but it is limited to images of the same size. Modeling a mixture of arbitrary images by methods such as multi-view learning may enable more flexibility.\nMultimodal Time Series Models and Agents. As can be seen from Table 1, the existing research on multimodal analysis (with vision modality) is much less than unimodal analysis, with a limited scope of time series tasks. Given the existing LLMs for time series such as Time-LLM [Jin et al., 2024] and S\u00b2IP [Pan et al., 2024], it is appealing to introduce vision modality to further boost the performance in wide tasks such as forecasting, classification and anomaly detection. Furthermore, the visual representation of time series provides the foundation for exploring multimodal AI agents [Xie et al., 2024] for more intricate and nuanced tasks that requires reasoning and interactions with environments, such as root cause analysis in AI for IT Operations (AIOps).\nVision-based Time Series Foundation Models. A foundation model (FM) is a deep learning model trained on vast datasets that is applicable to a wide range of tasks. Recent time series FMs, such as TimesFM [Das et al., 2024], MO-MENT [Goswami et al., 2024], Chronos [Ansari et al., 2024] and Time-MoE [Shi et al., 2024], are mostly built upon LLM architectures and trained on raw time series. Given the potential of image representation, it is promising to explore vision models as a new architecture to revolutionize time series FMs. This research direction not only leverages the advantages of LVMs as introduced in \u00a71 (e.g., the prior knowledge extracted from the vast pre-training images), but also enables future development of vision-language FMs for time series."}, {"title": "7 Conclusion", "content": "In this paper, we present the first survey on leveraging vision models for time series analysis, whose general process structures the survey. We propose a new taxonomy consisting of imaging and modeling methods for time series. We discuss the pre- and post-processing steps as well. Each category encompasses representative methods and relevant remarks. The survey also highlights the challenges and future directions for further advancing time series analysis with vision models."}]}