{"title": "Towards Fine-Grained Webpage Fingerprinting at Scale", "authors": ["Xiyuan Zhao", "Xinhao Deng", "Qi Li", "Yunpeng Liu", "Zhuotao Liu", "Kun Sun", "Ke Xu"], "abstract": "Website Fingerprinting (WF) attacks can effectively identify the websites visited by Tor clients via analyzing encrypted traffic pat-terns. Existing attacks focus on identifying different websites, but their accuracy dramatically decreases when applied to identify fine-grained webpages, especially when distinguishing among different subpages of the same website. WebPage Fingerprinting (WPF) attacks face the challenges of highly similar traffic patterns and a much larger scale of webpages. Furthermore, clients often visit mul-tiple webpages concurrently, increasing the difficulty of extracting the traffic patterns of each webpage from the obfuscated traffic. In this paper, we propose Oscar, a WPF attack based on multi-label metric learning that identifies different webpages from obfuscated traffic by transforming the feature space. Oscar can extract the subtle differences among various webpages, even those with simi-lar traffic patterns. In particular, Oscar combines proxy-based and sample-based metric learning losses to extract webpage features from obfuscated traffic and identify multiple webpages. We proto-type Oscar and evaluate its performance using traffic collected from 1,000 monitored webpages and over 9,000 unmonitored webpages in the real world. Oscar demonstrates an 88.6% improvement in the multi-label metric Recall@5 compared to the state-of-the-art attacks.", "sections": [{"title": "1 Introduction", "content": "The Onion Routing (Tor) has millions of daily active clients and protects their online privacy through multi-layer encryption with multiple randomly selected relays [12]. However, it is vulnera-ble to Website Fingerprinting (WF) attacks, which can effectively deanonymize the communication. WF attacks identify the websites visited by Tor clients through analyzing the unique traffic patterns of the websites, e.g., packet sizes, timestamps, and directions. Prior WF attacks [11, 17, 22, 44, 45, 50] develop complex model struc-tures to extract features of various websites from traffic. However, these attacks focus on identifying websites rather than fine-grained webpages. Since a single website often hosts multiple webpages, ac-curately identifying fine-grained webpages can provide additional valuable information. The performance of existing WF attacks sig-nificantly declines when tasked with webpage identification, as models trained with cross-entropy loss struggle to capture the sub-tle differences in webpage traffic.\nTo identify different webpages, a series of fine-grained WF at-tacks have been studied, i.e., WebPage Fingerprinting (WPF) at-tacks [30, 46, 48, 49, 63], which leverage both coarse-grained and fine-grained traffic attributes. However, Tor clients often visit mul-tiple webpages consecutively, a common behavior that significantly"}, {"title": "2 Background", "content": "WF Attacks. Recently, encrypted traffic analysis has been exten-sively studied [27, 42]. WF attacks, a specific approach within the broader field of encrypted traffic analysis, aim to identify the unique traffic patterns of websites, including packet time intervals, sizes, and directions. WF attacks compromise the privacy of Tor clients by extracting the traffic patterns of different websites from packet sequences. ML-based WF attacks [19, 39, 57] utilize expert knowl-edge to construct features specific to websites for identification. With advanced deep learning (DL) algorithms, DL-based WF at-tacks [2, 50] enable automatic feature extraction and robust attacks. To apply WF attacks in the real world, existing works develop WF attacks for various real-world scenarios, such as under the multi-tab setting [11, 17, 22], limited training data [37, 51], dynamic network conditions [1], and various defenses [44, 47]. However, existing WF attacks focus on identifying different websites, and have mostly been evaluated on index pages. Despite certain attacks gathering subpage traffic samples [11, 37, 41], their identification targets re-main restricted to websites. When applied to fine-grained webpage identification, these attacks become ineffective due to performance deterioration caused by the high similarity of webpage traffic pat-terns.\nWPF Attacks. Existing ML-based WPF attacks [48, 49, 63] leverage global and local features to differentiate webpages. Existing DL-based WPF attacks [30, 46] apply powerful Convolutional Neural Networks (CNN) or Graph Neural Networks (GNN) to extract fea-tures from webpage traffic. However, these methods either consider distinguishing webpages within the same website or suffer from the limitation of small-scale webpages. Additionally, none of the above works address the challenge of multi-tab identification."}, {"title": "3 Threat Model", "content": "The threat model of Oscar is illustrated in Figure 1. In our threat model, the clients are free to explore various webpages, moving beyond just the index pages. They are redirected to other webpages by clicking on links displayed on the current webpage. Clients are often interested in multiple webpages, therefore they may open multiple webpages concurrently, and the number of webpages is dy-namic and unknown to the attacker a prior. Furthermore, a website typically hosts multiple webpages. Therefore, the scale of web-page monitoring is far larger than that of website monitoring. We broaden the scope of our monitoring to include more monitored webpages, accommodating the size of real-world webpages.\nTo identify the webpages visited by the client, the attacker has the ability to eavesdrop on the communication link between the client and the guard node of the Tor network to analyze the patterns of bidirectional communication packets [11, 22, 50, 51]. Though the connections between the client and the guard node are protected by encryption, the attacker can gather traffic metadata such as the direction, size, and time sequences of traffic packets to extract unique fingerprint features that can be used to identify the web-pages visited by the client. In addition, the attacker does not require the ability to actively modify, delay, or decrypt these packets.\nSimilar to existing WF attacks [2, 11, 22, 44, 45, 50], we consider both the closed-world and open-world settings. Under the closed-world setting, Tor clients can only browse a limited number of webpages, i.e., the monitored webpages. The attacker can collect traffic from all monitored webpages to train the model. Under the open-world setting, Tor clients can freely access a large number of webpages unknown to the attacker, i.e., the unmonitored webpages. Given the vast number of webpages in reality, obtaining training samples for all of them is impractical, highlighting the realism of open-world identification. Note that Tor clients under the open-world setting still use multiple tabs to access both the monitored and unmonitored webpages."}, {"title": "4 System Overview", "content": "Oscar is a robust WPF attack that leverages the differences in web-page traffic to identify the webpages visited by Tor clients. Since different webpages from the same website are not identical, it leads to variations in local traffic patterns. Although it is difficult to di-rectly distinguish webpage traffic in the original feature space, by extracting the differences in webpage traffic through metric learn-ing, we can separate different webpages in the transformed feature space. Figure 2 shows the feature transformation of webpage traffic through metric learning, where traffic from different webpages is isolated. Specifically, in our study of 1,000 webpages, the similar-ity of traffic features between different webpages decreases by an average of 52.92% after feature transformation. This reduction in similarity highlights the effectiveness of Oscar in distinguishing webpage traffic and accurately identifying the visited webpages.\nFormally, considering \\(W\\) monitored webpages, the problem is defined as follows: the sample set \\(X=\\{x_1,..., x_N\\}\\) contains \\(N\\) samples, where each sample holds the dimension of \\(d_i * 2\\), comprising both the direction sequences \\(ds_i\\) and time sequences \\(ts_i\\) extracted from the original packet sequences. The direction sequences are distin-guished by +1 and -1 for outgoing and incoming packets, and the time sequences are calculated as the interval relative to the first packet. The label set is defined as \\(Y=\\{y_1,..., y_N\\}\\), where each label \\(y_i\\) is a \\(W\\)-dimensional 0/1 vector. \\(y_{ij} = 1\\) shows that the \\(i\\)th sample comprises the traffic from the \\(j\\)th webpage. Under our attack setting, the clients can visit multiple webpages, i.e., \\(|y_i| \\geq 1\\). Consequently, our problem is characterized as a multi-label classification, where each sample may be associated with one or more labels. The ob-jective is to precisely determine if each label is present in samples. This is more challenging than the multi-class classification, as the latter only considers the label with the highest probability.\nFigure 3 shows the three modules in Oscar to achieve the robust WPF attack. First, the Data Augmentation module enhances the generalization of Oscar through inter-sample and intra-sample com-bined data augmentation operations based on the characteristics of multi-tab traffic in the real world. The inter-sample augmenta-tion combines the traffic of two samples in chronological order to enhance the traffic diversity from different webpage combinations. The intra-sample augmentation exchanges packets within a single sample to accommodate variations in packet order. Second, the Fea-ture Transformation module leverages both the original samples and augmented samples to train the DF-based feature transforma-tion model based on two losses. The proxy-based loss clusters rele-vant samples, and the sample-based loss isolates irrelevant samples. Combining these two losses, this module obtains a feature transfor-mation model responsible for transforming the feature space. Third, the Webpage Identification module achieves efficient multi-label webpage classification based on the distribution features of web-page traffic in the transformed feature space. It is composed of two k-NN classifiers, incorporating both the proxy-sample distance and"}, {"title": "5 System Design", "content": "In this section, we present the design details of Oscar, including the Data Augmentation module, the Feature Transformation module, and the Webpage Identification module."}, {"title": "5.1 Data Augmentation", "content": "The Data Augmentation module employs inter-sample and intra-sample operations to generate simulated traffic based on the original traffic, thereby enhancing the generalization to diverse multi-tab traffic. To achieve the data augmentation, Oscar operates on the raw traffic samples and labels based on traffic characteristics. However, clients typically browse multiple webpages concurrently. Existing data augmentation operations [1, 5, 62] cannot be applied to multi-tab traffic, as they ignore the diversity introduced by traffic mixing under the multi-tab setting. To address this issue, we design two data augmentation operations specifically for multi-tab traffic.\nMulti-tab traffic exhibits greater diversity compared to single-tab traffic as the clients may browse multi-tab webpages with different webpage combinations, resulting in various mixed traffic. Even when the same combination of webpages is accessed, packets from different webpages within a session are transmitted through dif-ferent circuits, leading to a dynamic packet order [55]. Therefore, multi-tab traffic is more diverse, both for different webpage combi-nations and the same webpage combinations. To enrich the sample diversity under the multi-tab setting, we incorporate two data aug-mentation operations, as shown in Figure 4. First, we design an inter-sample data augmentation that combines traffic of different samples to adapt to the diversity of various webpage combinations. Second, we implement an intra-sample data augmentation that ex-changes packets within a single sample to handle the variability in packet order, addressing the diversity of traffic within the same webpage combinations. We detail these two operations as follows.\nInter-Sample Augmentation. We design an inter-sample data augmentation based on traffic combining to enhance the variety of traffic from different webpage combinations. Under the multi-tab setting, traffic from various webpages is mixed, making traffic from the same webpage display completely different patterns when mixed with different webpages. Figure 5(a) illustrates the corre-lation between the edit distance of samples and the quantity of browsed webpages, where the edit distance reflects the similarity of two vectors by measuring the minimum operations required to transform one vector to another. Specifically, we select 500 samples from different webpage combinations. These samples are grouped based on the number of labels, ranging from 1 to 5, with 100 samples in each group. We then calculate the edit distance among samples within each group, where all samples share the same number of webpage labels. The results show that disparity in samples from different webpage combinations increases as the number of concur-rently accessed webpages rises. Therefore, Oscar adopts an inter-sample augmentation to adapt to the diversity of various webpage combinations."}, {"title": "Algorithm 1: Inter-Sample Data Augmentation.", "content": "input: original sample1: \\(ds_i\\) (direction sequence), \\(ts_i\\) (time sequence);\noriginal label1: \\(y_i\\);\noriginal sample2: \\(ds_j\\) (direction sequence), \\(ts_j\\) (time sequence);\noriginal label2: \\(y_j\\);\ninput dimension: \\(d_i\\);\noutput: generated sample: \\(ds_g\\) (direction sequence);\ngenerated label: \\(y_g\\);\n\\(1 index_1 \\leftarrow 0 index_2 \\leftarrow 0\\)\n\\(2 for k \\leftarrow 0 to d_i do\\)\n\\(3 \t if ts_i[index_1] < ts_j[index_2] then\\)  \\(\\triangleright\\) compare the times of the two\n\tindexed packets\n\\(4 \t\t ds_g[k] \\leftarrow ds_i[index_1]\\)  \\(\\triangleright\\) add packet to the new sequence\n\\(5 \t\t index_1 \\leftarrow index_1 + 1\\)\n\\(6 \t else\\)\n\\(7 \t\t ds_g[k] \\leftarrow ds_j[index_2]\\) \\(\\triangleright\\) add packet to the new sequence\n\\(8 \t\t index_2 \\leftarrow index_2 + 1\\)\n\\(9 \t y_g \\leftarrow y_i \\cup y_j\\)  \\(\\triangleright\\) union the labels of two original samples\n\\(10 return ds_g, y_g\\)"}, {"title": "Algorithm 2: Intra-Sample Data Augmentation.", "content": "input: original sample: \\(ds_i\\) (direction sequence);\nexchanging ratio: \\(m_e\\);\noutput: generated sample: \\(ds_g\\) (direction sequence);\n\\(1 burst\\_sequences \\leftarrow extract\\_bursts(ds_i)\\)\n\\(2 ds_g \\leftarrow ds_i\\)\n\\(3 ex\\_num \\leftarrow len(burst\\_sequences) * m_e\\)  \\(\\triangleright\\) calculate the number of bursts\nto be exchanged based on the total burst number\n\\(4 ex\\_bursts \\leftarrow sample(burst\\_sequences, ex\\_num)\\)  \\(\\triangleright\\) sample\n\tex_num bursts from the burst sequence\n\\(5 for burst in ex\\_bursts do\\)\n\\(6 \t exchange\\_bursts(ds_g[burst], ds_g[burst+1])\\)  \\(\\triangleright\\) exchange the\nselected bursts with their subsequent bursts\n\\(7 return ds_g\\)\nThe intra-sample augmentation modifies traffic within a single sample without altering its label. Specifically, Oscar adopts an ex-changing operation based on bursts. Bursts are consecutive packets of the same direction, often containing resources like texts and im-ages [1]. Algorithm 2 details the exchanging operation. Oscar begins with identifying the bursts in the original direction sequence (line 1). Oscar then determines the number of bursts to be exchanged \\(ex\\_num\\) based on the total burst number of the sample with a ratio of \\(m_e\\), and samples \\(ex\\_num\\) bursts in the original traffic (line 3-4). Finally, for each selected burst, Oscar exchanges it with the fol-lowing burst, while leaving the remaining bursts unchanged (line 5-6). By setting the exchanging ratio advisedly, Oscar dynamically adjusts the number of exchanges according to the total burst num-ber of different samples, thus ensuring that the modification of each sample is controlled within a manageable range. In summary, this operation improves the ability to cope with the dynamic and unpredictable patterns of packet order in multi-tab traffic.\nNotably, our data augmentation is grounded in the analysis of multi-tab traffic characteristics. By combining inter-sample and intra-sample augmentation techniques, Oscar significantly enhances sample diversity, ensuring the generalization of the WPF attack under the multi-tab setting. After generating augmented samples, we blend them with the original samples for feature transformation."}, {"title": "5.2 Feature Transformation", "content": "The Feature Transformation module transforms traffic features to cluster traffic of the same webpages and separate traffic from differ-ent webpages. To realize the above feature transformation, Oscar needs to contrast the traffic of different webpages based on metric learning, so as to extract the subtle differences in the traffic patterns of different webpages. However, existing website fingerprinting attacks based on metric learning [51] cannot be applied to web-page identification. The reason is that Tor clients usually browse multiple webpages under the multi-tab setting, with each traffic having multiple labels. Traditional metric learning methods select positive and negative samples based on the single label, leading to a dramatic increase in the number of positive samples under the multi-tab setting, which causes the class collapse issue [26] (i.e., traffic from all different webpages clusters together in the new feature space).\nTo address the above issue, we design a multi-label metric learn-ing method to achieve feature transformation. The details of the feature transformation are shown in Figure 6. To effectively identify webpages corresponding to multi-tab obfuscated traffic, our feature transformation contains a feature transformation model to embed features to a lower-dimensional feature space and a multi-label metric learning loss function to aggregate traffic of the same web-pages and separate traffic of different webpages in the transformed feature space.\nThe feature transformation model takes the original direction sequences of traffic as input, and outputs the transformed lower-dimensional vectors. We select DF as the feature transformation model for the following reasons: (i) DF has demonstrated effective-ness in WF attacks, achieving 98% accuracy in identifying different websites [50]; (ii) DF is built upon CNN, which can effectively extract the features regardless of the part in which the feature fragments appear. The shift-invariance characteristic of CNN can extract specific features with dynamic locations, which is partic-ularly important under the multi-tab setting. DF contains four basic convolutional blocks and two fully connected layers. Each block contains two one-dimensional convolutional layers and one max pooling layer. We retain the original four convolutional blocks and replace the fully connected layers with a linear layer to embed the features to a low-dimensional feature space, enabling DF to function as a feature extractor.\nBeyond the feature transformation model, the loss function is crucial for the effectiveness of feature transformation. Oscar utilizes a multi-label metric learning loss to aggregate traffic from the same webpages and separate traffic from different webpages. The loss function comprises two parts: a proxy-based loss and a sample-based loss.\nProxy-Based Loss. As discussed above, existing metric learning approaches calculate loss based on positive samples, which leads to class collapse under the multi-tab setting. Therefore, we develop a proxy-based loss to aggregate traffic from the same webpages under the multi-tab setting. The left part of Figure 6 illustrates the proxy-based loss. We first set up proxies as representatives for each webpage. Instead of pulling samples with the same labels closer together, the proxy-based loss directs samples to the correlated proxies, therefore effectively aggregating samples of the same webpages. Specifically, the positions of the proxies are dy-namic and optimized together with the model's parameters in each epoch of model training. Such adaptability of proxies is crucial as it contributes to learning more accurate distributions of different webpages along with the model progressing through training.\nTo calculate the proxy-based loss, we first initialize the \\(W\\) proxies in the proxy set \\(P = \\{p_1, ..., p_w\\}\\), where \\(W\\) is the number of webpages. Proxies hold the dimension of \\(d_o\\), consistent with the embedded vector dimension after the feature transformation model. After proxy initialization, we excavate the proxy-sample relation-ship to cluster relevant samples.\nThe proxy-sample relationship can be divided into two types: positive proxy-sample pair and negative proxy-sample pair. In the case where the sample contains traffic from the webpage associated with the proxy, they are identified as a positive proxy-sample pair, otherwise, they are a negative proxy-sample pair, i.e., if \\(y_{ij} = 1\\), sample \\(x_i\\) and proxy \\(p_j\\) constitute a positive proxy-sample pair."}, {"title": "Lpos_proxy(xi, pj) = 1 - cos_sim(xi, pj),", "content": "Where \\(cos\\_sim\\) refers to the cosine similarity shown as follows:\ncos_sim(xi, pj) = \\frac{x_i \\cdot p_j}{||x_i|| \\times ||p_j||} (1)\n(2)\nOn the other hand, negative proxy-sample pairs are anticipated to exhibit low similarity, indicating their separation in the trans-formed feature space. To prevent overfitting, Oscar sets a margin for the expected similarity. If the similarity is below this margin, indicating effective separation from unrelated proxies, the loss is set to zero. The negative loss is then defined as the maximum of the two terms:"}, {"title": "Lneg_proxy(xi, pj) = max(cos_sim(xi, pj) - margin, 0),", "content": "(3)\nwhere the margin is a hyperparameter preset.\nWe calculate the sum of all positive proxy-sample loss Lall_pos_proxy and negative proxy-sample loss Lall_neg_proxy, and combine them for the total proxy-based loss Lproxy:\nLproxy = \\frac{L_{all\\_pos\\_proxy}}{O_{pos\\_proxy}} + \\frac{L_{all\\_neg\\_proxy}}{O_{neg\\_proxy}} (4)\nwhere \\(O_{pos\\_proxy}\\) and \\(O_{neg\\_proxy}\\) refer to the total number of positive proxy-sample pairs and negative proxy-sample pairs. Note that we divide by the number of positive pairs and negative pairs to avoid quantity imbalance. The multi-label proxy loss effectively enhances the accuracy of webpage identification by ensuring that samples are tightly distributed around the relevant proxies. As a result, the patterns of different webpages can be extracted.\nSample-Based Loss. Different from the proxy-based loss, Oscar utilizes a sample-based loss to separate irrelevant webpage traffic in the transformed feature space. Specifically, under the multi-tab"}, {"title": "cos_sim(xi, xj) = \\frac{x_i \\cdot x_j}{||x_i|| \\times ||x_j||}", "content": "setting, the proxy-based loss might inadvertently bring traffic from unrelated webpages closer together. The right part of Figure 6 illus-trates this conflict. The left sample, comprising traffic from webpage 1 and webpage 3, is expected to be approximately positioned be-tween proxy 1 and proxy 3. Similarly, the right sample, containing traffic from webpage 2 and webpage 4, is likely to be located be-tween proxy 2 and proxy 4. Despite that these two samples share no common labels, their positions in the transformed feature space can be notably close. Therefore, we design a sample-based loss to effectively identify and separate irrelevant webpage traffic by evalu-ating the similarity between samples in the feature space, ensuring that traffic from distinct webpages is separated.\nThe sample-based loss takes advantage of the relationship be-tween samples. Due to large-scale monitored webpages, the number of sample pairs with different labels is very large. Therefore, we design sample mining based on the label coincidence degree to selectively separate sample pairs with low correlation, i.e., samples without identical labels. To mine irrelevant sample pairs, we first sift out the samples with at least two labels and build a new set \\(X' = \\{x_i|x_i \\in X \\wedge |y_i| > 1\\}\\), where the total number of the filtered samples is \\(N'\\). Then we find samples with no overlapping labels in X', i.e., \\(y_i * y_j = 0\\), and form the sample pairs. For irrelevant sample pairs, the cosine similarity between them is defined as:\n(5)\nWe expect irrelevant sample pairs to show a lower degree of cosine similarity. Similar to the calculation of the negative proxy-sample loss, the margin is applied, and when the similarity is below the margin, the loss is set to 0:\nLir_sample(xi, xj) = max(cos_sim(xi, xj) - margin, 0).\n(6)\nIn each epoch, we collect all the irrelevant sample pairs in the batch and compute their sum Lall_ir_sample. The total sample loss is calculated as follows:\nLsample = \\frac{L_{all\\_ir\\_sample}}{O_{ir\\_sample}}\n(7)\nwhere \\(O_{ir\\_sample}\\) refers to the total number of irrelevant sample pairs. Compared with the vanilla sample-based metric learning methods, our approach focuses on samples with low label corre-lation, thus significantly reducing the pair number. Overall, the sample-based loss enhances the effectiveness of the feature trans-formation by isolating irrelevant samples.\nAfter calculating the two parts of losses, we combine them for the total loss:\nLoss = Lproxy + \\beta \\times Lsample,\n(8)\nwhere \\(\\beta\\) is a hyperparameter that adjusts the weights of these two losses. When \\(\\beta\\) = 0, the above loss simplifies to the multi-label proxy-based loss.\nNote that the proxies are added to the parameters and updated with the model's parameters using the same optimizer. This dy-namic adjustment refines the distributions of proxies, leading to more precise boundaries for each webpage. Following the feature transformation module, we strategically transform the feature space to distinctly separate different webpages. This separation is vital for effective webpage classification, as it ensures that each webpage"}, {"title": "5.3 Webpage Identification", "content": "The Webpage Identification module integrates two k-NN classifiers to achieve robust multi-tab webpage identification. Traditional k-NN classifiers typically rely on the labels of the nearest samples for classification. However, the diversity of multi-tab traffic can lead to sample drift and performance degradation. Therefore, we integrate a proxy-based k-NN and a sample-based k-NN, as illustrated in Figure 7, to achieve robust webpage identification. The proxy-based k-NN benefits from constantly updated and precise representa-tions of webpages, focusing on the uniform features of webpages. Meanwhile, the sample-based k-NN accounts for the diversity of multi-tab webpage traffic across various webpage combinations. Finally, we combine the results of these two classifiers to calculate the label scores, leveraging the strengths of both to improve the robustness of webpage identification under the multi-tab setting.\nThe proxy-based k-NN achieves classification based on the proxy-sample distance. Specifically, it retrieves the nearest b proxies and calculates the scores using the distances of the retrieved proxies with the target sample. Given that our transformed feature space is built upon cosine similarity, our k-NN classifiers adopt the cosine distance for score calculation:\ncos_dis(xtarget, pj) = 1 - cos_sim(xtarget, pj),\n(9)\nwhere cos_sim is defined as above. Xtarget is the sample to be identi-fied and pj is the retrieved proxy. Then the label scores score_proxy are calculated as:\nscore_proxyj = \\begin{cases}\\frac{1}{cos\\_dis(x_{target},p_j)} & p_j \\in R_{proxy} \\\\ 0 & p_j \\notin R_{proxy}\\end{cases}\n(10)\nwhere Rproxy is the set of retrieved proxies. Proxies closer to the target sample contribute higher scores, while proxies outside the retrieved set contribute a score of 0.\nSimilarly, the sample-based k-NN retrieves the nearest b samples and calculates the sample-sample distance:\ncos_dis(xtarget, xi) = 1 - cos_sim(xtarget, xi),\n(11)\nwhere xi is the retrieved sample. The samples are associated with multiple labels, and each retrieved sample contributes the same score for the corresponding labels. Then the label scores score_sample can be calculated by summing the contributions from all retrieved samples:\nscore_samplej = \\sum_{\\begin{smallmatrix}x_i \\in R_{sample}\\\\y_{ij}=1\\end{smallmatrix}} \\frac{1}{cos\\_dis(x_{target}, x_i)},\n(12)\nwhere Rsample is the set of retrieved samples.\nAt last, we combine the results of these two classifiers, i.e., for label j, the total score is the weighted sum of these two terms:\nscorej = score_proxyj + \\theta \\times score_samplej,\n(13)\nwhere \\(\\theta\\) is a hyperparameter that adjusts the weights of these two scores. By combining the results of these two classifiers, our webpage identification considers both the uniform characteristics of different webpages and the variability of multi-tab traffic samples."}, {"title": "6 Evaluation", "content": "In this section, we evaluate Oscar with datasets collected in the real world. We compare the performance of Oscar with the state-of-the-art WF attacks."}, {"title": "6.1 Experimental Setup", "content": "Implementation. We prototype Oscar using Torch 1.9.0 and Python 3.8. We perform a random search of hyperparameters and set the optimal hyperparameters to default values as shown in Table 2. For the data augmentation module, we set the exchanging ratio me to 5% to sufficiently augment the original traffic without corrupting the critical traffic patterns. For the feature transformation module, we use the weight of the sample-based loss to a larger value (i.e., \\(\\beta\\) = 4.5) to enhance the quality of the transformed feature space under the multi-tab setting. For the webpage identification module, we set the value of the threshold \\(\\tau\\) to achieve the best F1-score. Further analysis of the impact of hyperparameters can be found in Section 6.6.\nDataset. Existing datasets [11, 45, 50] regard each website as a distinct class. To evaluate the performance of Oscar on realistic WPF attacks, we collect multi-tab webpage traffic datasets. To the best of our knowledge, these are the first datasets of real-world traffic from multi-tab webpages, where each webpage is regarded as a distinct class. To be specific, we collect two multi-tab datasets:"}, {"title": "6.2 WPF Attacks in the Closed World", "content": "We first evaluate the performance of Oscar under the closed-world setting, where clients only visit monitored webpages and the at-tacker can collect traffic samples of all the webpages to train the model. We use our closed-world dataset CW for evaluation. We divide the dataset into the training, validation and testing sets with the ratio of 8:1:1. Specifically, we utilize the augmented training set to train the feature transformation model and finetune parameters on the validation set. Then we use the trained model to trans-form samples in the testing set and achieve webpage identification based on the updated proxies and transformed training samples. For NetCLR, we use the training set to pre-train the model and the validation set to finetune. We calculate the multi-label classifica-tion metrics Recall@k and AP@k with the k values of Recall@k in {5, 10, 15, 20, 25, 30}, and AP@k in {1, 2, 3, 4, 5}. Since BAPM and TMWF predict webpages based on each attention head or tab query and are unable to determine the probability of all the webpages visited in a session, we can only compare with them on Recall@5.\nWe present the Recall@k and AP@k of five attacks in Figure 8 and Recall@5 of all methods in Table 4. Results show that Recall@k and AP@k increase as the k value increases and Oscar achieves the best performance in all metrics. Specifically, Recall@30 and AP@5 of Oscar are both over 0.73, while the best results of other methods remain around 0.52. Compared with k-FP, NetCLR, DF, Tik-Tok, BAPM and TMWF, Oscar improves by 110.2%, 170.8%, 46.1%, 47.9%, 132.6% and 24.0% on Recall@5. The performance su-periority demonstrates that Oscar can identify the webpages both comprehensively and accurately. Although existing attacks achieve good performance in terms of website identification, their effec-tiveness significantly declines when applied to multi-tab webpage identification. This performance reduction is primarily due to their incapability of capturing the subtle distinctions hidden within the high-dimensional feature vectors of similar webpages. In contrast, Oscar concentrates on analyzing the differences among various webpages through webpage comparison. The proxy-based metric learning loss effectively clusters traffic from the same webpages and the sample-based metric learning loss isolates traffic from ir-relevant webpages. In this way, we separate different webpages and extract their distinct characteristics in the transformed feature space. In addition, the improved multi-label webpage identification comprehensively takes into account the uniform characteristics of webpages and the diversity of multi-tab traffic, therefore achieving more accurate results under the multi-tab setting."}, {"title": "Recall@k = \\frac{||Y_i \\cap \\hat{Y_i}||}{||y_i||}", "content": "(14)\nAP@k is the average of Precision@k, which measures the pro-portion of correctly predicted webpages among the top-k results. Since the number of visited webpages in our datasets varies, AP@k can reflect the performance more precisely. Specifically, for sample i, assuming the real set of visited webpages is yi, AP@k can be calculated as:\nAP@k = \\frac{\\sum_{t=1}^{k} Precision@t}{min(k, ||y_i||)} (15)\nTo compute Precision@t, we get the top-t predicted webpages with the highest probabilities \\(\\hat{Y}_i\\). Precision@t is then defined as:\nPrecision@t = \\frac{||Y_i \\cap \\hat{Y_i}||}{t} (16)\nRemark. In a nutshell, Oscar achieves the best performance in identifying multiple webpages from obfuscated traffic, which is cred-ited to the effectively transformed features based on our method. Besides, the proxy-based and sample-based combined webpage identification can classify webpages more accurately."}, {"title": "6.3 WPF Attacks in the Open World", "content": "Under the open-world setting, the attacker can only collect samples from a subset of webpages, which does not cover all the webpages in the testing set. Typically, sensitive webpages constitute only a fraction of the entire webpages, and our primary objective is to precisely identify the particular monitored sensitive webpages despite the interference of unmonitored webpages. Under the open-world setting, following existing works [11, 50"}]}