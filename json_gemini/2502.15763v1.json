{"title": "Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization", "authors": ["Bowen Pang", "Kai Li", "Ruifeng She", "Feifan Wang"], "abstract": "With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.\nNote to Practitioners-This work provides optimization tools for enhancing the efficiency of LLM inference systems through advanced scheduling techniques. From the perspective of LLM inference service providers, improved hardware utilization can reduce operational costs by requiring less hardware to maintain the same level of service. From the user's perspective, reduced inference time translates to faster response times and improved service quality. Furthermore, the proposed scheduling techniques are adaptable to various LLM models, hardware platforms, and datasets, making them highly scalable and broadly applicable to real-world LLM inference scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in large language models (LLMs), including GPT-4, LLaMA, and Qwen, have significantly transformed the landscape of natural language processing by enabling more sophisticated text generation, comprehension, and interaction capabilities. These models serve as foundational technologies in a wide range of applications, such as chatbots, machine translation, and content creation. Despite their transformative potential, the inference process for LLMS is computationally intensive and resource-demanding, resulting in high costs and increased latency. These factors pose significant challenges when scaling their deployment across various applications. Effective inference scheduling is crucial for optimizing resource usage, reducing operational costs, and ensuring high-quality service delivery.\nAs illustrated in Fig. 1, the current inference service system comprises three components: system resource, serving configuration, and inference service. System resource determines the extent of computational resources available in the system including GPU computing ability, memory, node size, network bandwidth, etc. The serving configuration specifies the desired model deployment method and service level expectations, such as data/tensor/pipeline parallel settings, quantization, batch size, and scheduling configurations. The inference service processes input requests from users, utilizes hardware capabilities, fulfills user configurations, and returns responses to LLM users. The scheduling strategy module is the core of the inference service, managing prefill and decode queues, offline and online request scheduling, and iteration-level hardware management. The offline scheduling method is optional, only for inference tasks where all requests are known in advance. Conversely, the two online scheduling methods, i.e., requests scheduling and iteration scheduling, are more versatile and applicable to a wide range of scenarios. The online methods acquire information from the online profiler and dispatch requests and inference tasks to LLM workers. In practice hardware utilization includes up to 20% \"bubbles\", which means the hardware is idle during inference service, and detail is later shown in Section IV. By designing an efficient LLM inference scheduling system to reduce these bubbles, computational resource consumption can be decreased, leading to reduced latency and increased throughput. For maintaining logic flow, detailed technical introductions to prefill and decode operations are provided in Section III.\nCurrent works, such as vLLM [1] and ORCA [2], provide a robust foundation for LLM inference serving systems. The primary contributions of vLLM and ORCA to LLM inference are their enhancements in resource allocation and execution efficiency, which significantly increase throughput and decrease latency in large-scale language model deployments. These improvements are achieved through advanced memory management and continuous batching techniques, which enhance model parallelism and effectively leverage hardware resources [1], [2]. However, the state-of-the-art scheduler in vLLM predominantly employs a First-Come-First-Serve (FCFS) and prefill-first policy, prioritizing prefill tasks but not fully leveraging the scheduling system's potential. We identify the causes of low hardware utilization rates. From an offline scheduling perspective, client loads are unbalanced, and request sequencing can be improved. From an online scheduling angle, the prefill-first policy lacks parallel processing of multiple requests at prefill stage, and there are no dynamic sequencing adjustments or preemption methods.\nThe optimization of LLM inference presents two major challenges. The first challenge pertains to the offline method, which involves managing a large number of requests from up to 200 parallel clients. This task is particularly time-consuming when using a mixed-integer programming (MIP) scheduling model. The second challenge is the need for much faster decision-making for online methods compared to traditional online scheduling problems. For example, in LLM inference scenarios, online decisions about whether to send prefill or decode requests to LLM workers typically occur every 50 milliseconds. In contrast, in healthcare or production systems, online decisions usually occur every thirty seconds or even several minutes. For online scheduling methods, the higher frequency of decision-making requires algorithms that are efficient and capable of delivering results within extremely short time frames.\nAccompanied by these challenges, developing a scheduling method for LLM inference could yield substantial benefits. According to NVIDIA Financial Results in 2024 [3], the revenue from GPU retail and cloud computation service has reached $60.9 billion per year. Improving computational resource efficiency by 5% will earn up to $3.05 billion in revenue annually. Therefore, research in this area is crucial, especially for scheduling researchers.\nIn this paper, we aim at enhance service system throughput and optimize hardware utilization. We define the LLM inference optimization problem using a MIP model as its uncertainty equivalence. Then, we propose a hybrid offline-online method as solution. This work is the first to define this problem from a scheduling perspective. To address the high real-time demands in LLM inference, we propose a hybrid offline-online scheduling method. In numerical experiments, we demonstrate that our method can improve system throughput by 5.46% and improve hardware utilization by 11.0%. A 100-cases study shows that our method outperforms baseline method consistently with an improvement of 8.0% on utilization rate.\nThe major contributions of this work include the followings. To the best of our knowledge, we are the first to formulate a mathematical model that describe the scheduling problem in the area of LLM inference. We design a two-stage approach to manage both offline and online scheduling challenges. An Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue. In the online scheduling module, our method can provide decisions within 5 milliseconds, meeting the real-time decision-making requirements of practical application in LLM inference.\nThe remainder of the paper is structured as follows. We review literature on efficient LLM inference in Section II. The problem definition and model formulation are presented in Section III. Then, we illustrate the difficulty of solving this problem and introduce our hybrid offline-online scheduling method to provide a timely solution in Section IV. Numerical studies using real cases and 100 generated cases are presented in Section V. Finally, the conclusion and future directions are provided in Section VI."}, {"title": "II. LITERATURE REVIEW", "content": "This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly employed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain.\nAs LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [11], sparsification [12], [13], and distillation [14]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) and model parallelism aim to fully leverage the computational power of the devices. In DP [17], the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per-device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism (PP) [18], tensor parallelism (TP) [19], sequence parallelism (SP) [20], and context parallelism (CP) [21]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency.\nIn addition to general model serving techniques, the optimization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion, Key-Value (KV) cache management, request management, iteration management, batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [22] amalgamates the operations of data transfer between hardware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [23], [24] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [2] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [1] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [4] introduces chunked-prefills, also known as dynamic SplitFuse or prefill-decode (PD) Fusion, batching together prefill and decode chunks to maximize both computation and bandwidth utilization. Since serving systems are commonly deployed on distributed platforms, numerous strategies have been proposed to exploit distributed characteristics. For example, recent works [10], [25], [26] advocate for separating prefill servers from decode servers, also known as PD Separation, due to the distinct computational and bandwidth characteristics of these two stages. For a comprehensive review of these techniques, we recommend [27] for further reference. These classic techniques form the foundation of inference services and our methods.\nRecently, with ongoing advancements in Al system research, numerous innovative inference techniques have been developed, particularly those related to schedulers. We present some representative works and highlight the differences between these approaches and our method in TABLE I. Inspired by context switching across CPU cores, Llumnix [5] proposes a live migration mechanism and a dynamic scheduling policy to reschedule requests across multiple model instances of LLM deployed on GPU clusters, thereby enhancing load balancing and isolation. InfiniGen [6] addresses the challenge of large KV cache sizes for long-text generation by speculating and prefetching critical KV cache entries. For LoRA models, dLoRA [7] addresses the challenges of serving multiple LoRA models by dynamically merging and unmerging adapters with the base model and migrating requests and adapters between replicas. Sheng et al. [8] study the fairness problem in LLM serving concerning clients and proposes a novel scheduling algorithm called the virtual token counter (VTC). FastServe [9] proposed an innovative skip-join MLFQ scheduler to enable preemption during the autoregression process of LLM inference. In distributed systems, DistServe [10] tackles the issues of PD interference and resource coupling by disaggregating prefill and decoding computation, and proposes placement algorithms to optimize resource allocation and parallelism strategies for different phases. Mooncake [28] also proposes a disaggregated architecture that separates the prefill and decode clusters and utilizes a KV cache-centric scheduler to manage the cache flow. While these innovative techniques attempt to address the issues faced by the scheduler to some extent, they scarcely model the scheduling problem formally and are limited to solve the inference optimization problem theoretically.\nIn the domain of operations research, scheduling is a well-established and extensively utilized approach. For instance, offline scheduling methods are applied in manufacturing systems [29], healthcare systems [30], and operations management systems [31]. To address real-time decision-making or multi-stage stochastic scenarios, online scheduling methods have been introduced in these systems [32], [33]. Nevertheless, none of the systems we examined achieve the rapid decision frequency-down to 10 milliseconds that is observed in LLM inference systems. Furthermore, traditional stochastic programming models are not suitable for sequential decision-making problems where real-time information is continuously revealed. Therefore, it is imperative to develop and tailor traditional methods for application in this emergent domain of LLM inference. Research on LLM inference optimization can not only enhance hardware resource utilization in the LLM domain but also expand the repertoire available to existing operations research algorithms."}, {"title": "III. PROBLEM FORMULATION", "content": "High efficiency and low latency are critical in LLM inference, and they depend not only on performance of hardware, such as GPU, but also on how well the hardware is used. As pushing the limit of hardware computing power is costly and faced with slow progress, improved inference scheduling becomes a promising means of achieving the same outcome. Three factors are involved when designing inference scheduling methods: PD management approaches, cache management, and batching strategy.\nLLM inference alternately goes through two stages, i.e., prefill and decode stages. In the prefill stage, initial input tokens are processed, and the LLM model's internal states, such as hidden states, are prepared. In the decode stage, the LLM model generates output tokens based on the context established during the prefill stage. The time lengths for prefill stage and decode stage are uncertain. The alternating process continues in parallel, until either the end of the sequence is reached or a predefined stopping criterion is satisfied. The process of LLM inference is illustrated in Fig. 2, where requests, each with a prefill phase and a decode phase, are sent to clients and processed in parallel. The whole process is divided into multiple bins, and each bin consists of a prefill stage and a decode stage. Requests in prefill phase can be served only when the process is in prefill stage, and the same requirement is applied to the decode phase and stage. A client may be idle as either the prefill phase or decode phase is completed but the process still stays in the same stage, causing compromised utilization of computing resources. A single LLM inference in practice typically contains a large number of requests processed by parallel clients. Thus, there is great potential in better utilizing computing resources in LLM inference, but the scheduling problem has a high complexity and, as a real-time decision making in milliseconds, the computing time is limited. The most commonly used approaches to managing prefill are provided below.\n\u2022\n\u2022\n\u2022\nPD Competition: As illustrated in Fig. 3, in this approach, either prefill or decode stage is processed at any given time for all clients on a single hardware node (typically consisting of 8 GPUs). PD Competition allows decode stage to be preempted by the prefill stage to enhance hardware utilization.\nPD Fusion: This approach integrates the prefill and decode stages into a single cohesive operation, aimed at reducing overhead and enhancing throughput by streamlining the inference pipeline. This approach also attempts to decrease latency through alignment of processes. However, this integration compromises flexibility, restricting the ability to independently optimize each process or tailor responses to varying workload demands.\nPD Separation: This approach separates the prefill and decode stages across exclusive sets of GPUs. However, it introduces additional communication or coordination overhead, which increases latency if not properly managed.\nAs a widely used approach, PD Competition has a high flexibility in effectively utilizing computing resources. Such an approach also allows an inference scheduling method to fit in and further enhance its performance. As is aforementioned, inference scheduling for LLM inference is challenging. This study focuses on the inference scheduling under the PD Competition approach.\nThe second factor that influences the efficiency of LLM inference is cache management. The KV cache is instrumental during the decoding stage by storing intermediate hidden states from preceding token generation steps. It allows the LLM model to reuse states, significantly accelerating the inference process. Despite its advantages, the KV cache requires to be properly managed. First, the KV Cache size increases with the length of input and output sequences and the number of LLM model layers. This cache size growth results in significant memory consumption, especially in the context of large-sized models and extended sequences. Effective KV cache management avoids memory overflow and sustains high inference speed. Cache management may involve caching only the most relevant hidden states, while discarding or compressing less critical information to optimize resource use. Second, concurrency issues should be addressed. The complexity of managing the KV cache escalates with concurrent inference tasks. Ensuring consistent and conflict-free partitioning and access to the cache for each task is important for performance and accuracy of LLM inference. Besides, although the KV cache alleviates computational load during decoding, it introduces cache access and management overheads. Cache management requires taking into account overall latency. In this study, we proactively compute the KV cache and determine the optimal maximum number of parallel requests, equivalent to the number of clients handled in subsequent stages. Thus, we assume in the inference scheduling problem shown in Fig. 2 that the total number of clients is predetermined.\nThe third factor that influences LLM inference efficiency is batching strategy. Continuous batching, which is introduced by ORCA [2], has emerged as an innovative technique to enhance the inference efficiency by dynamically aggregating incoming requests in real time. Unlike traditional static batching, which waits for a fixed number of requests before processing, continuous batching minimizes latency by reducing idle times between batch executions and adapting to fluctuating workloads. It ensures efficient use of computational resources. By maximizing parallel processing capabilities and aligning with the model architecture's latency and throughput trade-offs, continuous batching significantly enhances the scalability and responsiveness of LLM deployments. Using the continuous batching technique, decoding is allowed to be preempted by the prefill stage. Such a case is shown by the first request of client 2 in Fig. 2, where the decode phase is separated by the prefill stage of bin 2 and is assigned to both the decode stages of bin 1 and 2. In this work, the decision-making process allows decoding to be preempted by the prefill stage, facilitated by the continuous batching technique."}, {"title": "B. Inference scheduling problem description", "content": "Inference scheduling aims to schedule inference requests using continuous batching and PD Competition, with the goal of minimizing the total inference time while adhering to operational constraints. The problem settings are given as follows and related notations are presented in TABLE II.\n1) Requests and processing time:\n\u2022\nLet $I = \\{1,2,\\ldots\\}$ be the set of inference requests.\n\u2022\nInference request $i$, for $i \\in I$, has a fixed input token number $N^p_i \\in \\mathbb{Z}^+$ for prefill phase and output token number $N^d_i \\in \\mathbb{Z}^+$ for decode phase. The input token number is assumed to be known, but the output token number is unknown.\n\u2022\nEach request has a known prefilling time, linearly related to the total number of input tokens in a bin.\n\u2022\nThe decoding time is approximately linearly related to the output token number. Let $J = \\{1,2,\\ldots\\}$ be the set of clients and $T_d$ be the decode time per token. The minimal unit of decoding time is equal to the amount of time that each client processes a single token, i.e., $T_d/|J|$.\n2) Bin and batch size:\n\u2022\nLet $K = \\{1,2,\\ldots\\}$ be the set of bins. The inference process is divided into a total of $K = |K|$ bins.\n\u2022\nThe batch size $|I|$ represents the maximum number of requests that can be processed simultaneously in a batch.\n3) Stages:\n\u2022\nThere are two stages in hardware operation system: prefill and decode. Prefill and decode stages must alternate, ensuring that each stage is dedicated exclusively to one type of operation.\n\u2022\nAt any given time, the system can perform either prefill or decode stages, but not both.\n\u2022\nAt any time as the system starts a decode stage, the length of this operation is determined. Meanwhile, all requests processed at this stage will be scheduled. The decision is made based on system state, including information of the duration of the current bin, type of requests being processed, and the set of remaining requests. It is illustrated in Fig. 4.\n4) Assignment and allocation:\n\u2022\nEach request must be assigned to exactly one prefill stage for processing.\n\u2022\nFor every request, the prefill phase must be completed by the same client that will subsequently carry out its decode phase. Hence, both phases must be allocated to the same client.\n\u2022\nA client can process only one request at a time. Once a client begins processing a request, it remains occupied and cannot preempt tasks until both the prefill and decode stages of that request are completed."}, {"title": "C. Deterministic equivalence", "content": "In this hybrid offline-online problem, the offline decision component involves determining the assignment and sequence of given requests on clients. As a sequential decision-making problem, the online decision component focuses on determining the length of each bin and the sequence of remaining requests in the future, with the aim of minimizing the total inference time."}, {"title": "IV. SOLUTION METHOD", "content": "The original model, provided by Eqs. (1)-(25), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds. However, the original model is a large-scale MIP model with over 100,000 integer decision variables and constraints, making it difficult to solve even within several hours. Hence, it is vital to develop an efficient solution method that provides the best possible outcomes within the required time frame. As illustrated in Fig. 5, we propose a hybrid offline-online method that structures the scheduling process for large model inference into two main methods: offline requests assignment and online scheduling. Each method involves a subset of decision variables of the original model and provides timely solutions at each stage. In the figure, we illustrate how the offline-online information, as well as the decision making given by the scheduling models, is obtained and shared in the system.\nOffline Requests Scheduling: In this method, a predetermined batch size of clients determines the number of parallel requests. Each client is allocated a balanced number of requests, resulting in an equitable task distribution. This method considers the assignment decisions in the original model as described in constraints (2), (12), and (15)\u2013(16). We isolate this part of the model to demonstrate offline training scenarios, such as RLHF (Reinforcement Learning with Human Feedback) training. In this task, requests are typically given and known in advance. Users can manually send their request prompts to the LLMs and wait to receive the outputs. These tasks can implement offline request assignment methods to achieve better throughput. However, for most scenarios such us user using GPT, using the offline method is still limited since solving the MIP model usually takes 10 minutes or more, which cannot meet the rapid iteration requirements of the LLM online decision-making process. Therefore, it is necessary to develop an online method to fill this gap.\nOnline Scheduling: The online scheduling process comprises two major parts corresponding to two types of online decisions. The first part, online requests scheduling, determines which requests are scheduled in the upcoming bin and identifies the next client to serve once a previous request is completed. The second part, online iteration scheduling, decides when to conclude the current decoding bin and start a preemptive prefilling bin to enhance overall utilization rates. In the online requests scheduling part, heuristic methods are employed to determine the optimal order for processing requests in real-time. This approach considers factors such as task priority, current system load, and anticipated resource availability. By effectively prioritizing requests, the system can minimize waiting times and maximize throughput under dynamic operational conditions. This method emphasizes the implementation of the relaxed solutions provided by job assignment and illustrates the constraints (3)-(11) in the original model.\nThe online iteration scheduling part aims to minimize idle time on machines by strategically allocating computational resources. By dynamically adjusting prefilling and decoding task priorities based on real-time feedback and system constraints, this method enhances overall system efficiency and responsiveness. This proactive scheduling approach minimizes machine idle time and optimizes the utilization of processing resources, thereby improving the overall performance of large language model inference tasks. This method underscores iteration-based optimization and considers the constraints (14)\u2013(15) and (17)-(18) in the original model.\nThe overarching goal of this structured approach is to minimize the total inference time for a specific benchmark, thereby maximizing throughput. By integrating thorough data analysis, efficient task allocation, and adaptive online scheduling strategies, this scheduling solution optimizes the performance of LLM inference processes. This holistic approach not only enhances system efficiency but also supports scalability and reliability in handling complex computational tasks."}, {"title": "B. Offline requests scheduling & theoretical lower bound", "content": "As previously introduced, we begin by examining the offline request assignment decisions within the original model, with a specific focus on the constraints described in (2), (12), and (15-16). This part of the model is isolated to demonstrate offline training scenarios, such as RLHF training. In this scenario, we tackle the Minimizing Makespan Bin Packing Problem to efficiently address the workload balancing challenge. We assume that the output length is predetermined, and that prefill decode stages do not conflict during problem-solving. Nevertheless, in practical applications and simulations used to evaluate performance, we adhere to these constraints by allocating workload to clients without affecting the uncertainty of output length.\nIn the offline model outlined in Eqs. (26)\u2013(30), we introduce a new parameter, denoted by $T_i \\in \\mathbb{R}^+$ for $i \\in I$, representing the estimated decode completion time for request $i$. We also introduce a new decision variable, denoted by $t_j \\in \\mathbb{R}^+$ for $j\\in I$, to indicate the total decoding time for client $j$.\n$\\min \\max t_j$\n(26)\n$j\\in J$\ns.t.\n$\\sum_{j \\in J} X_{i,j} = 1$, for $i \\in I$,\n(27)\n$\\sum_{i \\in I} X_{i,j} T_i < t_j$, for $j \\in J$,\n(28)\n$X_{i,j} \\in \\{0,1\\}$, for $i \\in I$, and $j\\in J$,\n(29)\n$t_{max}, t_j \\in \\mathbb{R}^+$, for $j\\in J$.\n(30)\nThe offline model also provides a method to calculate the theoretical lower bound for a given set of requests, $I$. In this method, we assume that prefill and decode phases for all the requests can be separated into two groups, and we calculate the optimal inference time for each group.\nLet $t^{P*} \\in \\mathbb{R}^+$ and $t^{d*} \\in \\mathbb{R}^+$ represent the optimal total prefill and total decode times for all the requests in set $I$, respectively. The value of $t^{d*} = \\max_{j \\in J} t_j$ is obtained from the objective function value from Eqs. (26)-(30). Let $L = arg \\max_{l \\in L}  N_{cap}^l$, and then the largest prefill time across all levels in $T$ is denoted by $T^{P*} = N_{cap}^L$ is the number of maximum number of tokens that can be processed in $T_l$. Then, $t^{P*}$ can be calculated by the following equation.\n$t^{P*} > T \\frac{\\sum_{i \\in I} N_i^p}{N_{cap}^L}$.\n(31)\nIt yields a tight theoretical lower bound $T^{LB}$ as follows.\n$T^{LB} = t^{P*} + t^{d*}$.\n(32)"}, {"title": "C. Online requests and iteration scheduling", "content": "In this online part of the LLM inference optimization problem, two critical considerations arise. First, we need to determine which request to send to an available client once the previous request is completed. Second, when a round of decoding stage is finished, we must decide whether to send a preemptive prefill stage or continue this decode stage to the LLM workers for the subsequent time frame.\nThe first issue presents an online scheduling problem, as illustrated by Eqs. (14)\u2013(15) and (16)\u2013(17). The primary decision in this context is whether to select a new request to override the original assignment in order to achieve better machine utilization.\nA sorting and online preemptive method is illustrated in Algorithm 1. This online algorithm first selects the future available requests, denoted as $I_j$, for client $j\\in J$. The set $I_j$ is sorted by $N_i^p+N_i^d$, that is, $N_{i_1}^p+N_{i_1}^d  N_{i_2}^p+N_{i_2}^d \\forall i_1  0 then\n pop $I_j$ to client $j$\n $remain\\_token(j) \\leftarrow remain\\_token(j) \u2013 (N_i^p + N_i^d)$\n else if max(remain_token(j)) > 0 then\n pop arg max(remain_token) to client $j$\n $remain\\_token(j) \\leftarrow remain\\_token(j) \u2013 (N_i^p + N_i^d)$\n end if\n end for\nContinuing from the previous discussion, the second problem involves a sequential decision-making process, as outlined by Eqs. (2)-(11). The main challenge here is to deliver timely and efficient decisions in real time. As previously mentioned, each round of decoding takes approximately 50 milliseconds. Thus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision-making into the process. This aspect of decision-making corresponds to the following problem.\n$\\min t_{max}$\ns.t.\n$t_{max} > t_{s,d}+\\eta^d_k$, for $k \\in K^d$,\n(33)\n$t^p_{s,k}-(t^d_{s,k-1}+\\eta^d_{k-1}) \\ge 0$, for $k=2, \\ldots, K$,\n(34)\n$t^d - (t + \\eta^p) \\ge 0$, for $k \\in K^P$,\n(35)\n$\\eta^p_k > \\sum_{l\\in L} T^l y_{k,l}$, for $k \\in K^P$,\n(36)\n$\\sum_{i\\in I_j, j \\in J} N^p P_{i,j,k} \\le \\sum_{l\\in L} N_{cap}^l y_{k,l}$, for $k \\in K^P$,\n(37)\n$\\sum_{l \\in L} y_{k,l} = 1$, for $k \\in K^P$,\n(38)\n$\\eta^d \\ge T_d \\sum_i N^d_i w_{i,j,k}$, for $j \\in J$, and $k \\in K^d$,\n(39)\n$d_{i,j,k} - P_{i,j,k} \\ge 0$, for $i \\in I_j, j \\in J$, and $k \\in K$.\n(40)\nBy combining Eqs. (33) and (35), we derive that $t_{max} > \\max_{k\\in K} (t_{s,k}^d + \\eta^d_k + \\eta^p_k)$. In the context of online decision-making, the start time $t^d_{s,k}$ is typically influenced by the completion time of preceding tasks. The primary objective is to minimize the total time cost of prefill and decode stages. Consequently, we establish the following equation by integrating the calculations of $\\eta_k^d$ and $\\eta_k^p$ from Eqs. (36) and (39).\n$\\min t_{max} > \\max_k (t_{s,k}^d + \\sum_{i \\in I, j \\in J} T_d \\sum_{i \\in I_j, j \\in J}N_i w_{i,j,k} +  \\sum_{l \\in L} T^l y_{k,l})$\n(41)\nIn this problem, the time cost is divided into two components. The cost for adding a prefill task at any point is given by\n$\\frac{\\partial t_{max}}{\\partial y_{k,l}} = \\sum_{l\\in L} T^l$\n(42)\nand the cost for adding a decode task at the decision-making moment is expressed by\n$\\frac{\\partial t_{max}}{\\partial w_{i,j,k}} = T_d \\sum_{i \\in I, j \\in J}N_i$\n(43)\nThus, our heuristic method for deciding whether to dispatch a prefill or decode stage to the LLM worker involves comparing the prefill cost $C_p = \\sum_{l\\in L} T^l$ with the waited decode time $C_d = T_d \\sum_{i \\in I, j \\in J}N_i w_{i,j,k}$. If $C_p \\ge C_d$, the algorithm advises continuing with a round of the decode task and waiting for additional prefill tasks; otherwise, the algorithm recommends executing a round of the prefill task."}, {"title": "V. NUMERICAL EXPERIMENT", "content": "Before the model is solved by our hybrid method, extensive analysis is conducted to evaluate the time taken by the decode and prefill stages on hardware. This analysis provides crucial insights into the computational demands and performance characteristics of each stage. By quantifying these metrics,"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we study the inference optimization problem in the service system when deploying LLMs. To enhance the system throughput and better utilize the hardware, we formulate an MIP model to describe this inference optimization problem. To the best of our knowledge, this is the first formulation of the problem from a scheduling perspective. To tackle the complex and high real-time demands of LLM inference, we introduce a hybrid offline-online method.\nIn the offline method, we demonstrate how large-scale inference systems can be improved using a Minimizing Makespan Bin Packing Problem and how a theoretical lower bound can be provided. In the online request scheduling and iteration scheduling methods, the solution time efficiency is crucial. We propose a sorting and online preemptive method to more effectively utilize clients that finish early. Then, we focus on the iteration scheduling component of the original model and employ a Lagrangian method to compare the costs of adding a prefill stage versus a decode stage at each iteration. We provide a time efficient heuristic method to determine when to insert a prefill task and interrupt ongoing decoding tasks.\nIn real-world experiments, we deploy the LlaMA-65B LLM model and infer the GSM 8K dataset, which includes 1"}]}