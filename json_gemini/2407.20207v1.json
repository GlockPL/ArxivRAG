[{"title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval", "authors": ["Hongming Tan", "Shaoxiong Zhan", "Hai Lin", "Hai-Tao Zheng", "Wai Kin (Victor) Chan"], "abstract": "In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "Dense retrieval [1], [2] is a information retrieval method that uses text embeddings to find the relevant texts for a given query. In dense retrieval, sentence embeddings transform sentences into semantic vector representations, improving passage retrieval performance over word embeddings.\nA major challenge in dense retrieval is the risk of losing essential information when converting long texts into fixed-length dense vectors, as maintaining the fidelity of sparse representations for long texts often requires very high dimensions [3]. Additionally, this limitation is emphasized in cases where the source texts are inundated with low-quality, noisy text, resulting in inconsistent retrieval quality. On one hand, recent works propose advanced retrievers or sentence embedding models to improve dense retrieval [4]\u2013[9]. On the other hand, input enhancement for retrieval represents a distinct optimization strategy for retrieval tasks, including query transformation and data augmentation [10]. Unlike query transformation [11], [12], data augmentation improves data quality before retrieval, enhancing performance without adding user wait time. However, in text retrieval, data augmentation methods typically focus on generating new query-text pairs for the retriever training [13], rather than directly enhancing the original texts. As a result, current data augmentation methods have not resolved inherent deficiencies in dense retrieval, specifically the loss of key information exacerbated by the presence of low-quality text. To address this issue, it is essential to consider data augmentation specifically applied to the retrieval text itself. Intuitively, we can enhance the original text by implementing text augmentation methods to generate high-quality alternative texts, which concentrate key information to improve semantic similarity with the query.\nTaking inspiration from existing challenges and unexplored optimization strategies, we consider transforming raw texts into more information-dense formats [14] that present essential factual details concisely and directly for better dense retrieval. Specifically, we propose that dense retrieval can be improved through information extraction to generate new text embeddings, a text augmentation strategy that outperforms reliance on original text alone. These generated text embedding vectors achieve high fidelity by condensing information and removing noise, and they show higher similarity with the query vector than the original text vector. To implement this idea, we need to address three issues. (i) The first issue is What information extraction tools can effectively resolve the inherent challenges of key information loss and low-quality text in dense retrieval? To address this issue, we focus on two high-level information extraction methods: question-answer generation (QAG) and event extraction (EE).\nInspired by the longstanding tradition of Question Answering Systems (QAS) [15], QA pairs should be the ideal text format for dense retrieval due to their high accuracy in providing precise responses to users' similar questions. QA pairs are information-dense as they focus on specific points from raw texts, presenting significant factual details directly and succinctly. This QA format aligns well with the query typically centered on a single topic, minimizing redundant information and offering a streamlined retrieval process for targeted inquiry [16]. Additionally, studies indicate that QA pairs and documents can complement each other as knowledge bases [17], suggesting the incorporation of QA pairs into vector databases for enhanced dense retrieval.\nAdditionally, we should consider event extraction as another crucial information extraction method based on knowledge graphs. Event extraction is a particularly information-dense form that extracts structured information from unstructured text to answer the \u201c5W1H\u201d questions (who, when, where, what, why, how) [18]. It captures both entities and relationships, aiming to extract high-level information from text and present it in a structured, information-dense format. Consequently, events correspond to potential user \u201c5W1H\u201d queries and involve reorganizing and rewriting the original text to ensure precise information delivery, thus aligning semantically with these queries.\nFurthermore, we observe that QA pairs and events possess both subtle connections and clear distinctions. Event-based knowledge representations share similarities with QA pairs: (1) They both capture high-level semantic information at the sentence and paragraph levels rather than focusing solely on keywords and entities, providing deeper insights than keyword extraction or named entity recognition. (2) Each QA pair typically corresponds to an element in event representations, as events can answer \u201c5W1H\u201d questions. For example, \u201cwhen\u201d aligns with event time, \"where\" with location, and \"who\" or \"what\" with subjects or objects. Meanwhile, QA pairs and events differ fundamentally in structure. (1) QA pairs match individual information points and align with query semantics but each represents only a small portion of the source text, potentially limiting their ability to handle complex queries. (2) Events synthesize entities and relationships and incorporate various elements to potentially offer deeper and richer semantics than QA pairs. However, the lack of focus on a single information point in events reduces their alignment with queries. Therefore, we incorporate both QAG and EE into the text augmentation framework for their complementary benefits.\n(ii) The second issue is What text generation model should be used? Our desired text generation model aims to: (1) effectively produce multiple QA pairs and events from any given raw text, with the quantity of generated outputs corresponding to the text's information content; (2) ideally manage all generation tasks within a unified model framework. In light of these requirements, we opt for large-scale pre-trained language models (LLMs, e.g., ChatGPT\u00b9) as text augmentation generators. Previous works in QAG and EE not only lack multilingual capabilities but also exhibit limited open-domain generalization, which distances them further from the ideal of a unified model framework. Unlike previous models, LLMs excel in text comprehension and generalization, enabling strong semantic understanding and information extraction capabilities. Despite the distinct nature of QAG and EE, LLMs could integrate these tasks into a unified framework that employs zero-shot prompting and supports multilingual data. We design prompt instructions for QAG and EE to generate JSON-formatted QA pairs and events.\nMoreover, to ensure the output quality in unsupervised, training-free LLM generator, we introduce a penalty point system that deducts points based on specified criteria after the first generation. If scores fall below a predetermined threshold, we regenerate the text based on the deducted points to ensure enhanced text quality.\n(iii) The last issue to address is How can the generated structured text be utilized for dense retrieval? As a text augmentation method, our goal is to seamlessly add the generated structured texts into the datastore for retrieval. Initially, we should convert the structured text, previously output in JSON format by a large language model, back into unstructured natural language suitable for sentence embedding. We employ a straightforward conversion strategy: for QA pairs, we concatenate the question and answer to create one single text; for events, we sequentially combine all elements of the same event into one text. By converting back to unstructured text in this straightforward manner, we also explore different text organization strategies in our experiments. As a result, both the original and newly generated text chunks are embedded and incorporated into the final vector database. Importantly, we anticipate that the generated vectors will exhibit a higher similarity to the input query vectors than the original text vectors, thereby improving retrieval performance.\nIn this paper, based on the above discussion, we introduce QAEA-DR, a framework that integrates Question-Answer Generation (QAG) and Event Extraction (EE) into a Text Augmentation Framework for Dense Retrieval. QAEA-DR employs two types of generated text representations through LLM prompting: QA pairs and element-driven events. To further enhance the quality and robustness of text generation, we conduct scoring and text regeneration as the verification component in QAEA-DR. After generation, both QA pairs and events are converted back into unstructured texts. Subsequently, these generated texts are organized using two distinct text organization strategies and transformed into dense vectors. At last, these generated vectors are added to the vector database as high-quality retrieval vectors. Our experiments demonstrate that incorporating both event vectors and QA pair vectors into the vector database maximizes retrieval performance. In summary, the contributions of this paper are as follows:\n\u2022 To the best of our knowledge, QAEA-DR is the first comprehensive and universal text augmentation framework designed for dense retrieval.\n\u2022 QAEA-DR innovatively integrates the information extraction methods of QAG and EE into a unified framework of text generation and organization.\n\u2022 QAEA-DR employs an end-to-end LLM-based training-free text generator, integrating diverse prompts of generation and scoring-based output evaluation for high-quality and controllable text outputs.\n\u2022 QAEA-DR is evaluated through theoretical analysis and empirical validations on various embedding models and retrieval datasets to demonstrate its effectiveness and robustness."}, {"title": "II. RELATED WORK", "content": "In this section, we first review dense retrieval along with sentence embedding. Next, we discuss previous input enhancement methods for retrieval. Finally, we introduce some related works on information extraction."}, {"title": "A. Dense Retrieval", "content": "Dense retrieval has become an important research area following the development of pre-trained Transformer language models (PLMs) [2], [19]\u2013[22]. To enhance text retrieval performance, dense retrieval leverages PLM-based text embeddings to encode queries and documents into a shared semantic vector space, focusing on matching semantic contents beyond mere keywords. This text embedding application in retrieval is fundamental to Retrieval-Augmented Generation (RAG) [23], [24], which reduces the hallucinations in LLMs. Recent advancements in dense retrieval include architectural innovations, optimized training methodologies, and efficient indexing techniques, all of which contribute to improved retrieval accuracy and efficiency [4]\u2013[8], [25]. Since the introduction of Sentence-BERT [26] and Dense Passage Retrieval (DPR) [2], numerous sentence embedding models have been proposed to enhance dense passage retrieval. Advanced sentence embedding models, which have been highlighted in the retrieval task of massive text embedding benchmark (MTEB) [27], include Contriever [28], M3E2, BGE [9], etc. Our text augmentation method serves as a preprocessing module for dense retrieval and is compatible with various embedding models mentioned above."}, {"title": "B. Input Enhancement in Retrieval", "content": "In addition to the optimization methods for the retriever, input enhancement strategy represents a distinct optimization approach for retrieval tasks [10]. In particular, input data to a retriever includes user query and datastore. Therefore, input enhancement for retrieval can be categorized into two types: query transformation and data augmentation. Query transformation modifies the input query during retrieval, for example, Hypothetical Document Embeddings (HyDE) [12] that generate pseudo documents from queries, and KNN-based Query Expansion (KNN-QE) [11] that enhances queries using local conceptual word embeddings. Data augmentation improves the data to be retrieved before the retrieval process, including synthesizing data, clarifying ambiguities, updating outdated data, etc. Compared to query transformation, data augmentation models have the advantage of not consuming user waiting time in retrieval, which is particularly important in practical applications. Mainstream studies focus on data augmentation for cross-modal retrieval, such as Make-An-Audio [29], AMSH [30], and ReACC [31]. In terms of text-to-text retrieval, methods like InPars [13] generates new query-text pairs as training data. As current data augmentation methods do not consider enhancing the original text in text retrieval, we propose a text augmentation framework in this paper."}, {"title": "C. Information Extraction", "content": "Information extraction (IE) automatically isolates text fragments and extracts structured data from unstructured sources through NLP [32]. On one hand, IE is integral to constructing knowledge graphs (KGs), which have attracted considerable attention as a structured form of knowledge [33]. Tasks related to KG-based IE include named entity recognition, relation extraction, and event extraction [34]. In particular, event extraction (EE) captures both entities and relationships to extract high-level structured information from raw text.\n\u2022 EE has evolved from rule-based approaches [35] to machine learning methods like Dynamic Multi-Pooling Convolutional Neural Networks (DMCNN) [36] and Joint Event Extraction via Recurrent Neural Networks (JRNN) [37], and more recently to ChatGPT for Event Extraction (ChatEE) [38], and the Generative Template-based Event Extraction (GREE) [39], reflecting significant progress in the field.\n\u2022 There are methods that achieve multi-event extraction, such as Jointly Multiple Event Extraction (JMEE) [40] and Proxy Nodes Clustering Network (ProCNet) [41]. Nevertheless, current multi-event extraction methods are closed-domain and are limited by their reliance on predefined event schemas.\nOn the other hand, Question-Answer Generation (QAG), an extension of Question Generation (QG) [42], [43], generates several QA pairs given a text. Notably, QAG can also be classified as IE since QA pairs are structured texts.\n\u2022 QAG have progressed from rule-based models [44]\u2013[46] to generative-based PLMs like Information-Maximizing Hierarchical Conditional VAEs (Info-HCVAE) [47] and Language Models for Question and Answer Generation (LMQG) [48].\nHowever, current multi-event extraction and QAG face issues such as a lack of multilingual support, uncontrollable generation quantity and quality, and incompatibility within a unified model framework. Although these models show relatively good results on some datasets, they are far from our goal of a generalizable, quality-controllable, and unsupervised unified framework in open-domain applications. QAEA-DR combines EE and QAG, leveraging LLM to build an end-to-end framework that involves prompt-based generation, evaluation, and regeneration."}, {"title": "III. APPROACH", "content": "In this paper, we focus on text augmentation approach for dense passage retrieval. A retrieval dataset typically comprises three types of data: the corpus, queries, and labeled query-text relationships. Initially, let \\(C = \\{t_1,t_2,...,t_n\\}\\) represent a corpus, where each \\(t_i\\) is a text chunk (simplified as text in the following discussion) and \\(n\\) is the total number of texts in corpus. The initial step in dense passage retrieval is to construct a mapping function \\(\\Phi : C \\rightarrow \\mathbb{R}^d\\), where \\(d\\) is the vector dimension, such that semantically similar texts are close in the vector space. Specifically, the function \\(\\Phi\\) uses sentence embedding model to transform all texts into dense vectors (i.e. embedding) stored in a vector database. We denote the resulting vector database as \\(VDB_{ori}\\), where \\(VDB_{ori} = \\{v_1, v_2,..., v_n\\}\\) with each vector \\(v_i \\in VDB_{ori}\\) corresponding to a text \\(t_i\\) in \\(C\\). Given a query text \\(q\\), which is also mapped to a vector \\(v_q \\in \\mathbb{R}^d\\) by \\(\\Phi\\), the retriever calculates the top-k vectors \\(v_i \\in VDB_{ori}\\) with the highest similarity to query vector \\(v_q\\), resulting in a subset \\(S \\subseteq VDB_{ori}\\), where \\(|S| = k\\). The vector similarity, denoted as \\(sim(v_q, v_i)\\), measures the distance between \\(v_q\\) and each vector \\(v_i\\) in \\(VDB_{ori}\\) (e.g., calculating the cosine similarity based on the inner product \\((v_q, v_i)\\)). Evaluation metrics (e.g., NDCG) are used to calculate retrieval scores based on labeled query-text relationships. Here, we define our QAEA-DR as follows:\nDefinition III.1 (QAEA-DR). QAEA-DR is a text augmentation framework that augments the original corpus \\(C\\) by generating QA pairs and element-driven events using LLM-based generators. This process enriches the vector database \\(VDB_{ori}\\) by adding new vector representations derived from the augmented texts. The similarity of the query to generated text vectors should exceed that of original text vectors, potentially enhancing retrieval quality."}, {"title": "B. Overview of QAEA-DR", "content": "Fig. 1 shows the complete workflow of QAEA-DR, illustrating an example of the framework in action. Specifically, QAEA-DR operates as follows:\n\u2022 Step-1: Structured Information Extraction. Each text \\(t_i\\) from the corpus \\(C\\), where \\(i = 1,...,n\\), is augmented using LLM prompting to generate JSON format QA pairs \\(QA_{json}\\) and events \\(EVENT_{json}\\). We discuss the design of LLM prompts for structured text augmentation in Section III-C. As illustrated in Fig. 1, both types of structured texts effectively extract key information. Specifically, each QA pair presents an individual information point, while each event summarizes multiple points.\n\u2022 Step-2: Reversion to Unstructured Form. The generated structured texts are converted back into unstructured natural language texts, resulting in a set of QA texts \\(\\{qa^{(1)}, qa^{(2)},..., qa^{(l)}\\}\\) and a set of event texts \\(\\{event^{(1)}, event^{(2)},..., event^{(m)}\\}\\), where \\(l\\) and \\(m\\) represent the total number of QA pair texts and event texts generated from \\(t_i\\), respectively. Subsequently, we mainly consider two text organization strategies:\n\u2013 Texts Remain Independent (TRI): Maintain the generated set as \\(QA_i = \\{qa^{(1)}, qa^{(2)},..., qa^{(l)}\\}\\) and \\(EVENT_i = \\{event^{(1)}, event^{(2)},..., event^{(m)}\\}\\), for \\(i = 1,..., n\\).\n\u2013 Texts Merge into One (TMO): In this mode, individual texts generated from the same original text \\(t_i\\) are concatenated, forming singleton set \\(QA_i = \\{qa_i: qa^{(1)} + qa^{(2)} + ... + qa^{(l)}\\}\\) and \\(EVENT_i = \\{event_i: event^{(1)} + event^{(2)} + ... + event^{(m)}\\}\\), for \\(i = 1,...,n\\). The \u201c+\u201d denotes text concatenation.\nOverall, TRI decomposes texts, retaining all segments extracted from the original, but may include noisy texts unrelated to all queries. Conversely, TMO consolidates generated texts to reduce the density of noise.\n\u2022 Step-3: Integration into Vector Database. The transformed texts are mapped to vectors by the function \\(\\Phi\\). For QA texts, \\(VQA_i = \\Phi(QA_i) = \\{v_{qa_i}^{(j)}| j = 1,...,l\\}\\) (TRI) or \\(\\{v_{qa_i}\\}\\) (TMO), where \\(i = 1, . . ., n\\), results in the vector database \\(VDB_{QA} = \\cup_{i=1}^n VQA_i\\). Similarly, \\(VEVENT_i = \\Phi(EVENT_i) = \\{v_{event_i}^{(k)}| k = 1,...,m\\}\\) (TRI) or \\(\\{v_{event_i}\\}\\) (TMO), populates \\(VDB_{EVENT} = \\cup_{i=1}^n VEVENT_i\\). These generated vectors are then integrated into the final vector database \\(VDB_{final} = VDB_{ori} + VDB_{QA} + VDB_{EVENT}\\) to augment the original vector space.\n\u2022 Step-4: Enhanced Dense Retrieval. The query vector \\(v_q\\) searches for the top-k similarity vectors in \\(VDB_{final}\\). For any given query \\(q\\) associated with a positively related text \\(t_i\\), there exists a vector in either \\(VDB_{QA}\\) or \\(VDB_{EVENT}\\) that exhibits higher similarity with the query vector \\(v_q\\) than the original text vector \\(v_i\\).\nIn conclusion, our main contribution is the implementation of QAEA-DR, which, in Step-1, Step-2, and Step-3, generates two new types of text vectors\u2014QA pair vectors and event vectors\u2014and integrates them into the vector database. These generated vectors enhance the retrieval performance in the final Step-4 of dense retrieval. Fig. 1 demonstrates that in Step-4, the best match with the query is derived from the generated vectors with high similarity.\nIn the following sections, we will first discuss text augmentation details in Step 1 to 3. Then, we substantiate the effectiveness of QAEA-DR theoretically in the subsequent section, addressing why generated vectors in Step-4 could exhibit higher similarity with the query vector than the original text vector."}, {"title": "C. LLM-based Text Augmentation in QAEA", "content": "In this section, we describe our implementation of LLM-based text augmentation and the unifying properties of QAEA. QAEA is defined as a text augmentation module excluding the retrieval component. It combines original texts, QA pairs, and events into a new vector database to enhance natural texts through information extraction. QAEA corresponds to Steps 1 to 3 in Fig. 1.\nFollowing standard LLM-based prompt engineering practices [49], our defined single-step zero-shot prompt consists of three components: instruction, input data, and output indicator. For both QAG and EE prompting tasks, we make targeted adjustments to the prompt instructions. Fig. 2 illustrates the three-step prompts defined for both QAG and EE, including generation, scoring-based quality evaluation, and regeneration. We achieve different functionalities by modifying the instructions for each type of prompt.\nQuestion-Answer Generation. In QAG, our goal is to generate as many informative structured QA pairs as possible through instruction. Due to the lack of a universally recognized question generation directive, we employ question categorization to guide the LLM in producing diverse QA pairs. In designing question types, we observe that rhetorical patterns in writing (e.g., cause and effect) serve as methods for organizing information and can be generalized for question categorization. Consequently, the content directive specifies five question types for varied outputs: factual inquiry, explanation and definition, cause and effect, comparison and contrast, and evaluation and opinion. Additionally, the prompt instructs LLM to highlight frequently occurring entities and relationships in the original text, which should be reflected in the generated QA pairs. Regarding the output format, the instructions guide the LLM to produce QA pairs in JSON format \\(QA_{json}\\): \\(\\{``question type\": [[``question\", ``answer", "question type\" includes five categories, each capable of containing multiple QA pairs depending on LLM generation. As illustrated in Step-2 of Fig. 1, the output of QAG is processed by simply concatenating the \"question\u201d and \u201canswer\" strings into natural language texts.\nEvent Extraction. Since our text augmentation method for dense retrieval is initially designed for application on a small-scale Chinese news passage retrieval dataset (sCNPR) we created from a scientific project, we naturally consider event extraction. Unlike previous zero-shot prompt-based ChatEE [38], which requires predefined event types and supports single-event extraction, our approach allows the LLM to detect and generate multiple event types from the original text. In the EE prompt instructions, we first direct the LLM to identify multiple event types and use these generated types as triggers to populate event elements. Drawing from the event element categorization in the ACE 2005 dataset and common real-world event attributes, we define that each event includes the elements \u201cevent type,\u201d \u201ctime,\u201d \u201clocation,\u201d \u201cevent subject,\u201d \"event object,\u201d \u201cevent,\u201d and \u201cimpact.\" In terms of the output format, we guide the LLM to generate event outputs in JSON format \\(EVENT_{json}\\)": [{"type": "time", "location": "event subject\"", "object": "event", "impact": ""}], "Score_{json}\\)": {"score": "detail\": [\\{\\{``deduction reason\"", "deduction score\", \u201crelated content\"\\}": ""}}, {"title": "D. Theory in QAEA-DR", "content": "Now", "3": ".", "as": "n\\[\\mu(v_q", "v_2||}\\qquad(1)\\": "nThe normalized margin in retrieval models serves as a quantitative measure to evaluate fidelity and provides a comparative perspective on vector similarity. It indicates how distinctly a target text is separated from irrelevant ones in vector space", "v_2)\\qquad(2)\\": "nand\n\\[\\mu(v_q", "j\\qquad(3)\\": "nare both satisfied under the following conditions:\n(i) Relevance Enhancement: \\(\\exists v_1^{(0)"}, {"Consistency": "forall j", "Orthogonality": "Any two generated text vectors across \\(\\{v_1^{(j)"}, {"as": "n\\[v_1 = v_1^{(0)"}, "qquad(4)\\"], "us": "n\\[||v_1 - v_2||^2 = ||v_1^{(0)"}, "v_2^{(j)", "v_{noise_1", "v_{noise_2", 2, "n= ||v_1^{(0)", "v_2^{(j)", 2, "v_{noise_1", "v_{noise_2", 2, 2, "v_1^{(0)", "v_2^{(j)", "v_{noise_1", "v_{noise_2", ".", "qquad(6)\\]\nGiven the orthogonality condition (iii)", "the cross term vanishes:\n\\[(v_1^{(0)", "v_2^{(j)", "v_{noise_1", "v_{noise_2", 0, "qquad(7)\\]\nthus", "n\\[||v_1 - v_2||^2 = ||v_1^{(0)", "v_2^{(j)", 2, "v_{noise_1", "v_{noise_2", 2, "ge ||v_1^{(0)", "v_2^{(j)", 2.0, "qquad(8)\\]\nSimilarly", "we have:\n\\[||v_1 - v_2||^2 \\ge ||v_1^{(0)", "v_2||^2.\\qquad(9)\\]\nFor the numerator of normalized margin", "we have:\n\\[(v_q", "v_1^{(0)", "v_2) \\ge (v_q", "v_1) - (v_q", "v_2)\\qquad(10)\\]\n\\[(v_q", "v_1^{(0)", "v_2^{(j)", "ge (v_q", "v_1) - (v_q", "v_2)\\qquad(11)\\]\nbased on the conditions (i) and (ii).\nBy combining inequalities 9 and 10", "we conclude that:\n\\[\\mu(v_q", "v_1^{(0)", "v_2) \\ge \\mu(v_q", "v_1", "v_2)\\qquad(12)\\]\nSimilarly", "combining inequalities 8 and 11", "we conclude that:\n\\[\\mu(v_q, v_1^{(0)}, v_2^{(j)}) \\ge \\mu(v_q, v_1, v_2), \\forall j\\qquad(13)\\]\nTherefore, the formulas (2) and (3) hold, proving that the text augmentation approach maintains or improves retrieval fidelity.\nThe constraints in Theorem III.3 are based on the assumption that each generated text is of high quality and contains a portion of the original text's information. Given the text \\(t_1\\) and the related query, the ideal generated vector \\(v_1^{(i)}\\) enhances retrieval fidelity by reducing noise and condensing query-relevant information from the text.\nBuilding on the demonstrated effectiveness of text augmentation, we show that incorporating both QA pair vectors and event vectors into the text augmentation framework enhances retrieval fidelity more effectively than using a single type of generated text. Within the constraints of relevance enhancement and irrelevance consistency, the introduction of new high-quality generated vectors will only improve fidelity, making this conclusion clear.\nTheorem III.4 (QAEA). Given a text \\(t_1\\) most relevant to a query \\(q\\) and a competing text \\(t_2\\), we generate sets of QA pair vectors \\(\\{v_{qa_1}^{(j)}\\}\\) and event vectors \\(\\{v_{event_1}^{(j)}\\}\\) for \\(t_1\\), and \\(\\{v_{qa_2}^{(j)}\\}\\) and \\(\\{v_{event_2}^{(j)}\\}\\) for \\(t_2\\), respectively, where \\(j\\) records the number of generated texts. Let \\(v_1\\) and \\(v_2\\) represent the original text vectors of \\(t_1\\) and \\(t_2\\). Given \\(\\{v_1^{(j)}\\} = \\{v_{qa_1}^{(j)}\\} \\cup \\{v_{event_1}^{(j)}\\}\\), there exists at least one generated vector \\(v_1^{(0)} \\"]