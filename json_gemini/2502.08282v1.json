{"title": "Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes", "authors": ["Vinod Kumar Chauhan", "Lei Clifton", "Gaurav Nigam", "David A. Clifton"], "abstract": "Estimating individualised treatment effect (ITE) \u2013 that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as composite treatments, on a set of outcome variables of interest, referred to as composite outcomes, for a unit from observational data \u2013 remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called H-Learner, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Causal inference helps to answer fundamental \"what if\" questions regarding the effect of a set of variables on a set of outcome variables of interest. For instance, one might be interested in estimating the risk of cancer for an individual if they were to quit smoking. Such questions are prevalent in science, including healthcare, economics, education, social science, marketing, and computer science. While randomised controlled trials remain the gold standard for causal effect estimation, they can be time-consuming, expensive, ethically challenging, and more importantly not suitable for individual-level causal effect estimation due to their strict inclusion criteria and small and less representative samples. By contrast, non-experimental approaches based on observational datasets (e.g., electronic health records) address many of these limitations and can complement randomised controlled trials. Recently, there has been notable progress in causal machine learning [2], [8], a discipline that integrates rigorous causal inference methodologies with state-of-the-art machine learning techniques, particularly for the analysis of observational data.\nExisting research in individualised treatment effects (ITE), also referred to as conditional average treatment effects estimation or heterogeneous treatment effects estimation in the machine learning literature [39], is mostly focused on simple settings considering single treatments, such as binary treatments, and single outcomes [13], [6], [20], [10], [30]. However, the real world is complex and might involve composite treatments, i.e., multiple simultaneous treatments (often referred to as polypharmacy in clinical literature [17]), as well as composite outcomes, where we aim to optimise multiple simultaneous outcomes of interest. Thus, existing methods are not always suitable for practical adoption in these multi-faceted scenarios.\nMotivating example: Suppose a patient is admitted to a hospital for heart surgery. Approximately 20-50% of such patients are at risk of atrial fibrillation within the first seven days after surgery [15], which is associated with an increased risk of heart failure, dementia, and stroke [31]. During the surgery, clinicians can choose from more than ten interventions (e.g., beta-blockers, statins, and others) to manage multiple outcomes, including atrial fibrillation, mortality, and readmission to the intensive care unit, as depicted in Fig. 1."}, {"title": "II. RELATED WORK", "content": "In this section, we briefly review the literature on ITE estimation and hypernetworks."}, {"title": "A. Individualised Treatment Effects Estimation", "content": "After the pioneering work of [23], a wide variety of machine learning-based ITE learners have been proposed [2]. These methods can be broadly categorised as: (i) representation learning or deep learning-based learners, e.g., [13], [39], [18], [19], [6], [8], (ii) tree-based learners [1], and (iii) meta-learners, which are model-agnostic and can be subdivided into direct learners (S-Learner and T-Learner) [26], and indirect learners (RA-Learner, DR-Learner, and X-Learner) [24], [12], [26]. For a comprehensive review, please refer to [11]. While these frameworks have demonstrated success, they predominantly address settings with a single treatment and a single outcome.\nThe study of composite treatment effects has been constrained by data scarcity arising from the exponential number of treatment combinations (for K binary treatments, 2K combinations) and the possibility that many combinations have insufficient or no data at all. In this context, [45] proposed an S-Learner-based approach, called decon-founder (DEC), that accommodates composite (binary and real) treatments for a single outcome. They leverage proxy variables and latent representations to capture interactions among multiple treatments and mitigate unmeasured con-founding. Similarly, [50] developed the variational sample re-weighting (VSR) approach\u2014an extension of S-Learner\u2014to handle high-dimensional binary treatments (referred to as bundle treatments), using learned latent representations to de-correlate treatments from confounders. To address data scarcity, [34] proposed a data-augmentation strategy for composite treatment and single outcome settings, whereby K ITE learners generate balanced datasets for each treatment before training a standard supervised deep learning model to predict potential outcomes. However, this method requires to train multiple models and is also limited to binary composite treatments."}, {"title": "B. Hypernetworks", "content": "Hypernetworks, or hypernets, are a class of neural net-works that generate the weights of another network, known as the target (or primary) network [9]. Although the core idea of context-dependent weight generation originated earlier [38], the term \u201chypernetwork\u201d was popularised by [16], who introduced an end-to-end training paradigm for both the hypernetwork and the target network. Hypernetworks thus provide an alternative way to train neural networks [9], which consist of a primary network for performing predictions and a hypernetwork for generating the primary network's weights. Depending on the type of conditioning (data, task identi-fiers, or noise), hypernetworks are respectively categorised as data-conditioned, task-conditioned, or noise-conditioned hypernetworks. They can be classified using five key design criteria based on input-output variability and architectural choices [9].\nWhen a hypernetwork is used to generate weights for multiple target networks (referred to as soft-weight shar-ing), it enables end-to-end information sharing across those networks. Soft-weight sharing was recently used in [8] to propose HyperITE that addresses the problem of information sharing between potential outcome functions for training ITE learners. Nevertheless, HyperITEs are limited to binary treatments and single outcomes, unlike our approach which can handle an arbitrary number of treatments and outcomes.\nHypernetworks have emerged as a powerful deep learning technique due to their flexibility, expressivity, data-adaptivity, and information sharing, and have been used across various problems in deep learning [9]. For example, hypernetworks have been successfully applied and have shown better results across different deep learning problems, such as uncertainty quantification [14], [5], hyperparameter optimisation [28], continual learning [32], federated learning [40], multitask-ing [42], embedding representations [47], ensemble learn-ing [25], multi-objective optimisation [43], weight pruning [27], model-extraction attack [48], unlearning [36], image processing [35], [49], quantum computing [4], knowledge distillation [46], neural architecture search [33], adversarial defence [41], and learning partial differential equations [3]. For an overview of hypernetworks, refer to [9]."}, {"title": "III. BACKGROUND", "content": "Notations: Let Xi = (Xi1,...,Xip) denote the p-dimensional vector of pre-treatment features for patient i (i = 1,...,N). Let t\u2081 = (ti1,...,tik) be the K-dimensional treatment vector for patient i, where tik represents the k-th component of the potentially composite treatment. Let \u0423\u0456 = (\u0423\u04561,...,\u0423\u0456\u043c)\u0f0b be the M-dimensional outcome vector for patient i, where yim is the m-th outcome. yim(ti) denotes the potential outcome for patient i on outcome m if they were to receive treatment vector t. The observed outcome is yim = yim(ti). The observational dataset is 9 = {(xi, ti,yi)}=1\nPotential Outcomes Framework: Following the poten-tial outcomes framework [37], we define yim (ti) as the potential outcome for individual i on outcome m if they were to receive treatment vector t; \u2208 I. This represents the counterfactual outcome that would have been observed had individual i received treatment ti, even if they actu-ally received a different treatment. We can then represent the vector of potential outcomes under treatment ti as yi(ti) = (y1 (ti), y2(ti),..., \u0443\u043c(ti)). The fundamental prob-lem of causal inference is that we can only observe one potential outcome vector for each individual, namely y\u2081 = yi(ti), where t; is the treatment vector actually received by individual i. Specifically, the observed outcome for individual i on outcome m is yim = ym(ti).\nAssumptions: To identify and estimate ITEs for com-posite treatments and composite outcomes, we rely on the standard causal assumptions of (i) unconfoundedness (or conditional exchangeability), (ii) positivity (or overlap) and (iii) consistency for all individual treatments and outcomes."}, {"title": "IV. METHODS", "content": "In this section, we introduce H-Learner, a hypernetwork-based approach to solve ITE estimation problems with com-posite treatment and composite outcome.\nMotivation: Complex real-world scenarios often entail multiple concurrent treatments (polypharmacy) and multiple outcomes of interest. Existing research on ITE estimation, however, typically focuses on single treatment and single outcome problems. In the composite treatment and composite outcome setting, the exponential number of possible treat-ment combinations (e.g., 2K for K binary treatments) leads to data scarcity, where certain treatment combinations may have limited or no samples. Such data sparsity poses a significant challenge to reliably estimating ITE.\nBroadly, there are two ways to address composite treat-ment and composite outcome problems: (i) Train indepen-dent models for each treatment combination, akin to a T-Learner [26]. While flexible, this approach can be infeasible when data are scarce for many treatment combinations. (ii) Train a single joint model for all treatment combinations and outcomes, such as an S-Learner [26]. This approach can alleviate data scarcity through shared learning but may lack the flexibility needed even for simpler (binary) scenarios.\nTo overcome these limitations, we utilise hypernetworks for joint training of ITE learners. Our proposed H-Learner effectively combines the advantages of both independent and joint training: it dynamically shares information across treatments and outcomes yet learns distinct learners for each treatment-outcome combination, mitigating data scarcity."}, {"title": "Neural Architecture and Working", "content": "Figure 2 illustrates the key components of H-Learner, which include an em-bedding layer, a hypernetwork, and a target network (the ITE learner). The embedding layer learns representations for each of the unique combinations of treatments and outcomes and has been implemented using a single linear layer. The embedding layer maps given causes and an effect to a meaningful representation which is equivalent to learning embeddings for words in natural language processing [44].\nThe hypernetwork is a multi-layer perceptron that takes the treatment-outcome embeddings as input (i.e., task-conditioned inputs [9]) and outputs the weights of the target network. The target network, in turn, receives the covariate data and predicts the potential outcomes for the specified treatment-outcome combination. Unlike conventional learn-ers where the target network's weights are trained directly, here they are entirely generated by the hypernetwork and thus are not themselves learned via backpropagation.\nTraining proceeds by feeding the treatments and outcomes into the embedding layer, whose outputs are passed to the hypernetwork. The hypernetwork produces weights for the target network, which processes the observed covariates to predict potential outcomes. We compute the prediction loss on factual observations (the data points for which the treatments and outcomes are known) and backpropagate the error. Since only the embedding layer and the hypernet-work parameters are trainable, the backpropagated gradients update those components to learn better embeddings for treatments and outcomes and improve the hypernetwork's mapping from embeddings to target-network weights. Each training epoch thus leads to dynamic information sharing across the various treatments and outcomes, benefiting low-data configurations through mechanisms similar to transfer learning and multitasking [29], [7].\nFormally, we define the optimisation objective for H-Learner as:\nmin f(x; \u03b8t,y = h(et,y = e(t, y; \u03c8); \u03c6)), \u2200{x,t,y} \u2208 D, (1)\n\u03a6,\u03a8\nwhere \u03c6 and \u03c8 are the trainable parameters of the hy-pernetwork and the embedding layer, respectively, e(\u00b7,\u00b7; \u03c8) denotes the embedding function, and h(\u00b7;\u03c6) represents the hypernetwork. The weights \u03b8ty for each treatment-outcome pair (t,y) are generated by the hypernetwork, rather than learned directly. Consequently, H-Learner is a meta-model approach that naturally accommodates composite treatments and outcomes through dynamic weight-sharing across the treatment-outcome space."}, {"title": "V. EXPERIMENTS", "content": "Here, we present our experimental setup and the corre-sponding results."}, {"title": "A. Experimental Setup", "content": "Baselines: For composite treatments comprising binary treatments only, we compare against DEC, SCP, S-Learner, and VSR. For composite treatments that may be binary, continuous, or categorical, we use S-Learner and xS-Learner, due to the lack of methods which could handle arbitrary treatments. With respect to composite outcomes, we apply these baselines independently for each outcome because none of them are explicitly designed to handle multiple outcomes simultaneously. Specifically, we use a multitasking version of S-Learner (still referred to as S-Learner) and repeat the learner for each outcome referred to as xS-Learner. Unless otherwise specified, our experiments involve five treatments (either all binary or one continuous and four binary) and two outcomes.\nMetric: We employ an extension of the Precision in the Estimation of Heterogeneous Effects (PEHE) [21] suitable for composite treatments and outcomes. Concretely, this extension reduces to computing the mean squared error (MSE) of the estimated treatment effects overall treatment combinations and outcomes.\nDataset: Because of the \"fundamental problem of causal inference\" [22], not all potential outcomes are observable in real-world data, making direct validation of causal methods inherently difficult. As is common in the literature, we use synthetic datasets, where all potential outcomes are artificially generated, to evaluate our proposed methodology. Our data generation process follows [6] and is extended to accommodate multiple treatments and multiple outcomes.\nHyperparameters: We implemented SCP, VSR, and DEC using the publicly available code from [34], adopting their hyperparameter tuning protocols. For H-Learner, we set the learning rate to 0.005, the embedding size to 32, and used a hypernetwork with two hidden layers of 100 neurons each. All other hyperparameters, including the optimiser and batch size, are the same as those used in the baseline implementations."}, {"title": "B. Results", "content": "In this subsection, we compare H-Learner under two scenarios: (1) arbitrary composite treatments and (2) binary composite treatments.\nArbitrary composite treatments: Since H-Learner shares information across all treatments and outcomes, we compare it against S-Learner [26], which employs a multitasking neu-ral network, and xS-Learner, which considers only a single outcome at a time and therefore does not share information across outcomes. Figure 3 summarises performance as we vary the number of patients, treatments, and outcomes. H-Learner consistently performs better across different data sizes and different numbers of treatments and outcomes. Notably, its advantage over the baselines becomes more pronounced when the dataset size decreases or when the number of outcomes increases. This improvement arises from H-Learner's dynamic end-to-end information sharing across treatments and outcomes, facilitated by hypernetworks [9].\nBinary composite treatments: Here, we restrict treat-ments to binary treatments and compare H-Learner against S-Learner, VSR, SCP, and DEC. Figure 4 shows that H-Learner typically outperforms or is on par with the baselines. Its superior performance in smaller datasets, relative to SCP, can be attributed to the way H-Learner shares information across outcomes, which other methods do not. The same mechanism also enables H-Learner to outperform the baselines when the number of outcomes is increased. Although SCP shows stronger results for certain treatments and is overall the second-best estimator, it relies on knowing the underlying causal structure among treatments information we pro-vide in these experiments but which may be unavailable in practice. In contrast, H-Learner can accommodate an arbitrary number of treatments (and their interrelationships) by generating a distinct learner network for each treatment combination."}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed H-Learner, a novel hypernetwork-based framework for ITE estimation under composite treatments and composite outcomes an impor-tant yet underexplored problem in real-world causal infer-ence. By mapping each treatment-outcome combination to a distinct target learner, H-Learner combines the flexibility of independent models with the benefits of a joint learner capable of dynamic end-to-end information sharing. Our empirical results highlight that H-Learner not only manages data scarcity effectively but also consistently outperforms or competes with existing methods when dealing with both arbitrary and binary composite treatments, as well as multi-ple outcomes. Incorporating hypernetworks enables adaptive weight generation, facilitating transfer learning across com-plex treatment-outcome configurations and enhancing robust-ness in scenarios with limited data. We believe H-Learner provides a strong foundation for addressing increasingly intricate real-world causal inference tasks involving multiple treatments and outcomes."}, {"title": "STATEMENTS AND DECLARATIONS", "content": "Competing Interests\nThe authors declare that they have no competing interests.\nCode Availability\nThe finalised code, after acceptance, will be available at https://github.com/jmdvinodjmd/HLearner."}]}