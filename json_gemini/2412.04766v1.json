{"title": "DAWN-SI: DATA-AWARE AND NOISE-INFORMED\nSTOCHASTIC INTERPOLATION FOR SOLVING INVERSE\nPROBLEMS", "authors": ["Shadab Ahamed", "Eldad Haber"], "abstract": "Inverse problems, which involve estimating parameters from incomplete or noisy\nobservations, arise in various fields such as medical imaging, geophysics, and\nsignal processing. These problems are often ill-posed, requiring regularization\ntechniques to stabilize the solution. In this work, we employ Stochastic Interpola-\ntion (SI), a generative framework that integrates both deterministic and stochastic\nprocesses to map a simple reference distribution, such as a Gaussian, to the target\ndistribution. Our method DAWN-SI: Data-AWare and Noise-informed Stochastic\nInterpolation incorporates data and noise embedding, allowing the model to ac-\ncess representations about the measured data explicitly and also account for noise\nin the observations, making it particularly robust in scenarios where data is noisy\nor incomplete. By learning a time-dependent velocity field, SI not only provides\naccurate solutions but also enables uncertainty quantification by generating multi-\nple plausible outcomes. Unlike pre-trained diffusion models, which may struggle\nin highly ill-posed settings, our approach is trained specifically for each inverse\nproblem and adapts to varying noise levels. We validate the effectiveness and ro-\nbustness of our method through extensive numerical experiments on tasks such as\nimage deblurring and tomography.", "sections": [{"title": "INTRODUCTION", "content": "Inverse problems are a class of problems in which the goal is to determine parameters (or parameter\nfunction) of a system from observed data. These problems arise in various fields, including medical\nimaging, geophysics, remote sensing, and signal processing. Inverse problems are often ill-posed,\nmeaning that a unique solution does not exist, or the solution may be highly sensitive to small per-\nturbations in the data. Solving inverse problems typically requires regularization techniques, which\nintroduce additional constraints or prior information to stabilize the solution and mitigate the effects\nof ill-posedness. Such regularization can be obtained using various mathematical and computational\nmethods, including optimization techniques (variational methods), statistical inference (Bayesian or\nfrequentist) and machine learning. In this paper, we approach inverse problems using the latter\nand investigate a new set of methods that are machine learning based for the solution of inverse\nproblems. In particular, we show how stochastic interpolation (SI) which was recently proposed in\nthe context of generative models can be effectively used to estimate the solution of inverse problems\nand to further investigate its non-uniqueness.\nStochastic Interpolation is a relatively new generative process that provides a unifying framework,\nelegantly integrating both deterministic flows and stochastic diffusion models. The core concept of SI is to learn a stochastic process that effectively transports a simple reference\ndistribution, such as Gaussian, to the desired target data distribution. This transportation process\ncan manifest as either deterministic or stochastic. In the former case, it is described by an ordinary\ndifferential equation (ODE), while in the latter, it is governed by a stochastic differential equation\n(SDE).\nThe stochastic interpolation framework defines a continuous-time reversibility between the reference\nand target distributions, parameterized by time $t \\in [0,1]$. At the initial time $t = 0$, the distribution\naligns with the reference distribution. As time progresses to $t = 1$, the distribution evolves to match\nthe target data distribution. This evolution is achieved by learning the time-dependent velocity field\n(for ODEs) or drift and diffusion coefficients (for SDEs) that characterize this interpolation process.\nBy understanding and modeling this time-dependent transformation, one can generate samples from\nthe target distribution through numerical integration of the learned ODE or SDE.\nSI is a highly flexible methodology for designing new types of generative models. In this work,\nwe use this flexibility and show how to adopt SI to solve highly ill-posed inverse problems. We\nfind SI particularly useful since it allows ease of sampling from the target distribution. This implies\nthat we are able to generate a range of solutions to the inverse problems and thus investigate the\nuncertainty that is associated with the estimated solution. Such a process is highly important in\nphysical applications when decisions are made based on the solution.\nRelated work: The methods proposed here are closely related to three different approaches for\nthe solution of inverse problems. First, there is an obvious link to the incorporation of diffusion\nmodels as regularizers in inverse problems. The key idea is to leverage a pre-trained diffusion model that captures the data distribution\nas a prior, and then condition this model on the given measurements to infer the underlying clean\nsignal or image. For inverse problems, diffusion methods condition the diffusion model on the given\nmeasurements (e.g. noisy, incomplete, or compressed data) by incorporating them into the denoising\nprocess. Nonetheless, it has been shown that pre-trained diffusion models\nthat are used for ill-posed inverse problems as regularizers tend to under-perform as compared to the\nmodels that are trained specifically on a particular inverse problem. In particular, such models tend\nto break when the noise level is not very low.\nA second branch of techniques that are related to the work proposed here use encoder-decoder type\nnetworks for the solution of inverse problems. Such approaches are sometimes referred to as likelihood-free estimators as they\nyield a solution without the computation of the likelihood. This technique is particularly useful for\nproblems where the forward problem is difficult to compute.\nA third branch of techniques that relates to our approach uses the forward problem within the neural\nnetwork. In this approach, one computes the data misfit (and its gradient) within the\nnetwork and use it to guide training. Our approach utilizes components from this methodology to\ndeal with the measured data."}, {"title": "Main Contribution:", "content": "The core contributions of this paper lie in the design and application of the\nDAWN-SI framework, specifically crafted for solving highly ill-posed inverse problems like image\ndeblurring or tomography. (i) Our method is designed to be problem-specific, which adjusts itself\nto the unique structure of the inverse problem it is tasked with, learning the posterior distribution\ndirectly, ensuring that the learned velocity fields and mappings are directly applicable to the target\ntask. (ii) We train a stochastic interpolant by embedding measured data and noise information di-\nrectly into the interpolation process. This incorporation allows our model to adapt to a wide range of\nnoise conditions, something that pre-trained models struggle with. By making the training explicitly\naware of the noise level and data characteristics, our model can better navigate noisy or incom-\nplete measured data, producing superior reconstructions. (iii) By learning the posterior distribution\ndirectly and leveraging the stochastic nature of the interpolation process, DAWN-SI generates mul-\ntiple plausible solutions for a given inverse problem, allowing us to explore the solution space more\nthoroughly and in particular, to estimate the posterior mean and its standard deviation, estimating\nthe uncertainty in the recovered solution."}, {"title": "STOCHASTIC INTERPOLATION AND INVERSE PROBLEMS", "content": "In this section we review stochastic interpolation as well as derive the main ideas behind using it for\nthe solution of inverse problems."}, {"title": "STOCHASTIC INTERPOLATION: A PARTIAL REVIEW", "content": "Stochastic interpolation (SI) is a framework that transforms points between two distributions. Given\ntwo densities $\\pi_0(x)$ and $\\pi_1(x)$, the goal is to find a mapping that takes a point $x_0 \\sim \\pi_0(x)$ and\ntransports it to a point $x_1 \\sim \\pi_1(x)$. For simplicity and for the purpose of this work, we choose $\\pi_0$\nto be a Gaussian distribution with 0 mean and I covariance.\nConsider sampling points from both distributions and define the trajectories\n$x_t = (1-t)x_0 + tx_1$  (1)\nThese trajectories connect points from $x_0$ at $t = 0$ to $x_1$ at $t = 1$. More complex trajectories\nhave been proposed in, however, in our context, we found that simple linear\ntrajectories suffice and have advantages for being very smooth in time. The velocity along the\ntrajectory is the time derivative of the trajectory, that is,\n$\\frac{dx_t}{dt} = v = x_1 - x_0$ (2)\nUnder the SI framework, the velocity field for all $(x_t, t)$ is learned by averaging over all possible\npaths. To this end, we parameterize the velocity by a function $s_\\theta(x_t,t)$ and solve the stochastic\noptimization problem for $\\theta$\n$\\theta = \\underset{\\theta}{\\text{arg min }} \\mathbb{E}_{x_0,x_1} [||s_\\theta(x_t, t) - v ||^2] = \\underset{\\theta}{\\text{arg min }} \\mathbb{E}_{x_0,x_1} [||s_\\theta(x_t, t) + x_0 - x_1||^2]$ (3)\nAfter training, the velocity function $s_\\theta(x_t, t)$ can be estimated for every point in time. In this work,\ngiven $x_0$, we recover $x_1$ by numerically integrating the ODE\n$\\frac{dx_t}{dt} = s_\\theta(x,t), \\quad x(0) = x_0, \\quad t \\in [0, 1],$ (4)\nby Fourth-Order Runge Kutta method with fixed step size. An alternative version where the samples\nare obtained by using a stochastic differential equation can also be used.\nWhile it is possible to estimate the velocity $v$ from $(x_t, t)$ and then compare it to the true velocity, it\nis sometimes easier to work with the velocity as a denoiser network, that is, estimate $x_1$ and use the\nloss of comparing $x_1$ to its denoised quantity. To this end, note that\n$x_1 = x_t + (1 - t)v \\approx x_t + (1 - t)s_\\theta(x_t,t)$ (5)\nEquation (5) is useful when a recovered $x_1$ is desirable. In the context of using SI for inverse\nproblems, obtaining an approximation to $x_1$ can be desirable, as we see next."}, {"title": "APPLYING STOCHASTIC INTERPOLATION FOR THE SOLUTION OF INVERSE PROBLEMS", "content": "Consider the case where we have observations on $x_1$ of the form\n$Ax_1 + \\epsilon = b$ (6)\nHere, A is a linear forward mapping (although the method can work for nonlinear mappings as well)\nand $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I)$ is a random vector. We assume that A is rank-deficient or numerically rank-\ndeficient, so the effective dimension of b is smaller than $x_1$ and one cannot obtain a\nreasonable estimate for $x_1$ given the noisy data b without the incorporation of a-priori information.\nUsing Bayes' theorem, we have\n$\\pi(x_1|b) \\propto \\pi(b|x_1)\\pi(x_1)$ (7)\nBayes' theorem suggests that it is possible to factor the posterior distribution $\\pi(x_1|b)$ using the\nknown distribution of $\\pi(b|x_1)$ and the prior distribution $\\pi(x_1)$. This observation motivated a num-\nber of studies that used the estimated pre-trained distribution $\\pi(x_1)$ in the process of solving an\ninverse problem. Nonetheless, it has\nbeen shown that such estimators tend to produce unsatisfactory results when\nsolving highly ill-posed problems or when the data is very noisy. The reason for this behaviour\nstems from the fact that pre-trained estimators push the solution towards the center of the prior dis-\ntribution, irrespective of what the data represents. To see this, we use a careful dissection of the\nsolution. Consider the singular value decomposition\n$A = \\sum_i \\lambda_i u_i v_i^T$\nwhere $u_i$ and $v_i$ are the left and right singular vectors and $\\lambda_i$ are the singular values. Given the\northogonality of u, we can decouple the data equations into\n$(v_i^T x_1) = (u_i^T b), \\quad i = 1,...,n$\nIf $\\lambda_i$ is large, the projection of $x_1$ onto the eigenvector $v_i$ is very informative and very minimal\nregularization is required. However, if $\\lambda_i \\approx 0$, the contribution of $v_i$ to the solution is difficult,\nif not impossible, to obtain and this is where the regularization is highly needed. When using pre-\ntrained models, the prior $\\pi(x_1)$ is estimated numerically and it is unaware of the inverse problem\nat hand. Errors in the estimated $\\pi(x_1)$ in the parts that correspond to the large singular vectors\nmay not be destructive. However, if $\\pi(x_1)$ has errors that correspond to the very small singular\nvalues, that is, to the effective null space of the data, this may lead to the artifacts that have been\nobserved early in. This suggests that although\nit is appealing to use a generic pre-trained priors in the process, a better approach is to not use the\nBayesian factorization to prior and likelihood but rather train a stochastic interpolant that maps the\ndistribution $\\pi(x_0)$ to the posterior $\\pi(x_1|b)$ directly. Indeed, as we show next, the flexibility of the\nSI framework allows us to learn a velocity function that achieves just that."}, {"title": "A DATA-AWARE AND NOISE-INFORMED VELOCITY ESTIMATOR", "content": "In the canonical form of stochastic interpolation, the velocity is estimated from the interpolated\nvector $x_t$. In the context of inverse problems, we also have a vector b (measured data) that contains\nadditional information on $x_1$ and therefore can be used to estimate the velocity towards the posterior.\nWe now show that by doing a small change to the training process of SI, it is possible to solve inverse\nproblems using the same concept.\nTo this end, notice that we train a velocity function $s_\\theta(x_t, t)$ that takes in two arguments. In the\ncontext of a specific inverse problem, we have additional information for training the network, the\nmeasured data b and the noise level $\\sigma$ in b. Note that it is also possible to estimate the noise level\ndirectly from the data as proposed in. The data b can be used to point toward\n$x_1$ even at time $t = 0$, where $x_t$ contains no information on $x_1$ and can therefore improve the\nestimation of velocity. Moreover, the information about noise level $\\sigma$ in the measured data during\ntraining also makes the estimator more robust to noise during inference. We thus propose to use the\ndata and noise when estimating $s_\\theta$. To this end, we use a transformation f of data b (to be discussed\nnext), and let\n$s_\\theta = s_\\theta(x_t, f(b), t, \\sigma) = s_\\theta(x_t, f(Ax_1 + \\sigma z), t, \\sigma),$ (8)"}, {"title": "where $z \\sim \\mathcal{N}(0, I)$.", "content": "To estimate $\\theta$, we simply repeat the minimization process as before and match\nthe flow where the data is a part of the estimated velocity, that is,\n$\\hat{\\theta} = \\underset{\\theta}{\\text{arg min }} L_1(\\theta) = \\underset{\\theta}{\\text{arg min }} \\mathbb{E}_{x_0,x_1,\\sigma,t} [||s_\\theta(x_t, f(Ax_1 + \\sigma z), t, \\sigma) + x_0 - x_1||^2],$ (9)\nwhere $L_1$ represents the mean loss in the prediction of velocity. The trained network can then be\nused to invert new data. Let us assume that we are given some fixed vector b and we want to estimate\n$x_1$. This can be done simply by solving the ODE\n$\\frac{dx_t}{dt} = s_\\theta(x_t, f(b), t, \\sigma), \\quad x(0) = x_0, \\quad t \\in [0, 1],$ (10)\nwhere b and $\\sigma$ are now fixed.\nAs we show in our numerical experiments, having as an input to the network plays an important\nrole, generating an inversion methodology that is robust to different noise levels.\nAn important question is the design of a network that integrates the information about b into the\nvelocity estimation process. One important choice is the function f that operates on b. For many, if\nnot most, inverse problems, the data b belongs to a different space than x. Therefore, it is difficult\nto use this vector directly. The goal of the function f is to transform the data, b from the data space\nto the space of x. One obvious approach to achieve this is to choose\n$f(b) = A^T b.$ (11)\nThis approach was used in and as shown in Sec-\ntion 2.5, can be successful for the transformation of the data into the image space. Other possible\napproaches can include fast estimation techniques for x given b such as the conjugate gradient least\nsquares method. For the experiments presented here, we found that using the adjoint\n$A^T$ of the forward problem was sufficient.\nTo demonstrate these points, we consider the following toy example."}, {"title": "Example 2.1 The Duathlon problem:", "content": "We consider the duathlon problem where one records the\ntotal time it takes to perform a duathlon (bike and run). Given the total time, the goal is to recover\nthe time it takes to perform each individual segment. The problem is clearly under-determined as\none has to recover two number given a single data point. Let $x = [x_1,x_2]$ be the vector, where\n$x_1$ and $x_2$ represent the time it takes to finish the bike and run segments, respectively. The data is\nsimply\n$b = x_1 + x_2 + \\epsilon$\nAssume that we have a prior knowledge that the distribution of x is composed of two Gaussians.\nUsing SI to generate data from these Gaussians is demonstrated in Figure 2.\nThe data is obtained by training a network, approximately solving the optimization problem in Equa-\ntion (3) and using Equation (4) to integrate $x_0$ that is randomly chosen from a Gaussian.\nNow, in order to solve the inverse problem, we train a larger network that includes the data and\napproximately solves the optimization problem in Equation (9). Given the data b, we now integrate\nthe ODE Equation (10) to obtain a solution. The result of this integration is presented in Figure 2\n(right). We observe that not only did the process identify the correct lobe of the distribution, it\nalso sampled many solutions around it. This enables us in obtaining not just a single but rather on\nensemble of plausible solutions that can aid in exploring uncertainty in the result."}, {"title": "TRAINING THE SI MODEL FOR INVERSE PROBLEMS", "content": "When training an SI model, we solve the optimization problem given by Equation (3). In this\nprocess, one draws samples from $x_0$ and $x_1$, then randomly chooses $t \\in [0, 1]$ to generate the vector\n$x_t$. In addition, we generate the data b by multiplying the matrix A with $x_1$ and adding noise $\\epsilon$ with\na random standard deviation $\\sigma$.\nNext, one feeds the network $x_t$, t, b and $\\sigma$ to compute $s_\\theta (x_t, b, t, \\sigma)$ and then compare it to the\nvelocity $v = x_1 - x_0.$"}, {"title": "ARCHITECTURES FOR DATA-AWARE AND NOISE-INFORMED VELOCITY ESTIMATORS", "content": "Our goal is to train a velocity estimator $s_\\theta (x_t, b, t, \\sigma)$ and then compare it to the velocity v. Our\nestimator is based on a UNet with particular embeddings for b, t, and $\\sigma$.\nThe embedding of time and noise is straight-forward. Note that both t and $\\sigma$ are scalars. For their\nembedding, we use the method presented in, which involves creating learnable\nembedding and adding them to the feature maps at each level of the network.\nA key component in making SI perform well for inverse problems is the careful design of a network\nthat incorporates the data within the training process. The resulting network can be thought of as a\nlikelihood-free estimator, that estimates the velocity vector v, given the input\nvector $x_t$, the time t and the noise level $\\sigma$. As previously explained, we do not integrate b directly,\nbut rather use $f(b) = A^T b$ and embed this vector in the network.\nThe embedding of $A^T b$ is performed using a data encoder network. To this end, let\n$E_\\eta (A^T b)$ (15)\nbe an encoder network that is parameterized by $\\eta$. The encoder can be pre-trained or trained as part\nof the network, providing flexibility in how it is integrated into the UNet. The encoded data provides\nadditional context that is critical for accurate predictions. Formally, the UNet estimates the velocity\nas follows:\nv = s_\\theta(x_t, E_\\eta (A^T b), t, \\sigma) (16)\nBy embedding both time, data and noise vectors at each layer, the network can leverage additional\ninformation, leading to more accurate and robust predictions, especially for large noise levels."}, {"title": "INFERENCE AND UNCERTAINTY ESTIMATION", "content": "Our method is specifically aimed at highly ill-posed inverse problems, where regularization is often\nnecessary to arrive at a stable solution. These problems often do not have a unique solution, and\nslight changes in input data can lead to large variations in the solution. Due to its stochastic nature,\nour method allows for the realization of multiple solutions since one can start with many random\ninitial points $x_0$ and evolve to multiple versions $x_1$ of the solutions. This property can be used to\ngenerate even better estimates and for the estimation of uncertainty in the recovered images. To this\nend, assume that the ODE is solved M times starting each time at a different initial condition $x_0^{(j)}$\nand let\n$x_t^{(j)} = x_t(t = 1, x_0 = x_0^{(j)}), \\quad j = 1, ..., M$ (17)\nbe the solution of the ODE in Equation (9) obtained at time t = 1 starting from point $x_0^{(j)}$ at t = 0.\nThe points $x_t^{(j)}$ for $j = 1,..., M$ represent an ensemble of solutions, that are sampled from the"}, {"title": "posterior $\\pi(x_1|b)$.", "content": "Given these points, it is simple to estimate the mean and standard deviation of\nthe posterior distribution. In particular, we define\n$\\overline{x}_1 = \\frac{1}{M} \\sum_{j=1}^{M} x_1^{(j)} , \\quad \\sigma_{x_1} = \\frac{1}{M} \\sum_{j=1}^{M} ||x_1^{(j)} - \\overline{x}_1||^2$ (18)\nas the mean and standard deviation of the estimated solutions. In particular, $\\overline{x}_1$ approximates the\nposterior mean and $\\sigma_{x_1}$ estimates its standard deviation. It is well known that, for many problems the\nposterior mean can have a lower risk compared to other estimators (e.g. the Maximum A-Posteriori\nestimator). For an elaborate discussion on the properties of such estimators, see;\nWhile such estimators are typically\navoided due to computational complexity, the computational framework presented here allows us to\ncompute them relatively easily.\nAn example for this process is shown in Figure 4. Here, the solutions over multiple runs were\naveraged to generate an effective reconstruction of the original image (the posterior mean). More-\nover, to quantify uncertainty in the reconstruction process, we computed standard deviation over the\nsolutions at each pixel from multiple runs. In the uncertainty maps for MNIST, the uncertainty is\nconcentrated along the edges of the digits. This occurs because the SI model introduces slight varia-\ntions in how it reconstructs the boundary between the digit and the background. Since this boundary\nis sharp, any slight differences in how this edge is defined in different reconstructions lead to higher\nuncertainty along the edges. On the other hand, for STL10 and CIFAR10 images, the boundary be-\ntween objects and background is often less distinct. The background might contain detailed textures\nor noise that blends into the object, making it harder for the model to distinguish clear boundaries.\nHence, the uncertainty maps for these datasets do not exhibit the same clear edge-focused uncer-\ntainty as in MNIST. The lack of a clear boundary means that the reconstruction's variability spreads\nmore evenly across the entire image."}, {"title": "NUMERICAL EXPERIMENTS", "content": "In this section, we experiment with our method on a few common datasets and two broadly applica-\nble inverse problems: image deblurring and tomography. We provide\nadditional information on the experimental settings, hyperparameter choices, and network architec-\ntures in Tables 3 to 5 for image deblurring task and Table 6 for tomography task in Appendix C.\nTraining methodology. For our experiments, we considered two types of data-aware velocity esti-\nmator networks: (i) DAW-SI: the estimator with no noise-embedding, and (ii) DAWN-SI: the esti-\nmator with trainable noise-embedding. We employed antithetic sampling during training. Starting\nwith an input batch of clean images $x_1$, an $x_0 \\sim \\mathcal{N}(0, I)$ was sampled and antithetic pairs $x_1$ and\n$x_0$,\n$x'_1 = \\frac{x_1 + x_0}{2}, \\quad x'_0 = x_1 - x_0,$ (19)\nwere generated by concatenation along the batch dimension. For a large number of samples, the\nsample mean of independent random variables converges to the true mean. However, the conver-\ngence can be slow due to the variance of the estimator being large. Antithetic sampling helped reduce\nthis variance by generating pairs of negatively correlated samples, thereby improving convergence.\nThe data was generated by employing the forward model along with a Gaussian noise injection,\n$b = Ax_1+\\sigma z$, where $z \\sim \\mathcal{N}(0, I)$. The value of $\\sigma$ was set to p% of the range of values in the data b,\nwhere p was sampled uniformly in (0, 20). $x_t$ and $v$ were computed from $x_1$ and $x_0$ following Equa-\ntion (1) and Equation (2). Using the transformation $E_\\eta(f(b)) = E_\\eta(A^T b)$, the predicted velocity\nof the estimator was $s_\\theta(x_t, E_\\eta(A^T b), t)$ for the DAW-SI estimator and $s_\\theta(x_t, E_\\eta(A^T b), t, \\sigma)$ for\nthe DAWN-SI estimator. Here, $E_\\eta$ was chosen to be a single-layer convolutional neural network.\nThe loss for an epoch was computed as given in Equation (14).\nInference. For inference on a noisy data, $b = Ax_1 + \\sigma \\epsilon$ for some image $x_1$, we start with a\nrandomly sampled $x_0 \\sim \\mathcal{N}(0,I)$ at t = 0 and perform Fourth-Order Runga Kutta numerical\nintegration to solve the ODE in Equation (10) with a step size h = 1/100, where the velocity\nwas computed using the trained estimator as $s_\\theta(x_t, E_\\eta(A^T b), t)$ for the DAW-SI estimator and\n$s_\\theta(x_t, E_\\eta(A^T b), t, \\sigma)$ for the DAWN-SI estimator at time t. For each image $x_1$, the inference was"}, {"title": "IMAGE DEBLURRING", "content": "We present the results of image deblurring task in Table 1 at noise level p = 5% in blurred data for\nMNIST, STL10 and CIFAR10 datasets. For all metrics, DAWN-SI beats all other methods for all\ndatasets by large margins, except for the CIFAR10 dataset, where DAW-SI performed marginally\nbetter than DAWN-SI. Moreover, on the CIFAR10 dataset, the Diffusion model fits the data the best\nachieving the lowest value of the misfit metric. We also performed ablation studies for different\nnoise levels in data for both DAW-SI and DAWN-SI models, as shown in Figure 5. For all metrics,\nDAWN-SI was more robust to noise in data, especially for higher noise levels over all datasets with\nthe crossing point happening at values of p$\\leq$5% for all metrics. Some example images for the\ndeblurring task for different levels of noise in data are presented in Figures 7 to 9 in Appendix D.\nBased on empirical evidence from training, we believe that the results for both DAW-SI and DAWN-\nSI could be improved further with more training epochs."}, {"title": "TOMOGRAPHY", "content": "We present the results of our methods DAW-SI and DAWN-SI for the tomography task in Table 2\nat noise level p = 5% in the sinogram data for OrganAMNIST and OrganCMNIST datasets. For this task, DAWN-SI outperformed DAW-SI on all metrics for both datasets. Some\nexample images for the tomography task for different levels of noise are presented in Figures 10\nand 11 in Appendix D. In medical image analysis, accurately estimating the size and boundaries\nof a lobe (such as the one shown in red box in Figure 6) is crucial for diagnostics, especially in\ncases where its size can influence medical decisions. We quantify uncertainty for both DAW-SI and\nDAWN-SI by computing mean and standard deviation across 32 samples from the learned posterior.\nWhile the mean represents the most probable reconstruction, the standard deviation map highlights\nregions of higher variability, indicating areas of uncertainty. From Figure 6, DAWN-SI gives a robust"}, {"title": "CONCLUSIONS", "content": "In this paper, we presented DAWN-SI, a framework for addressing highly ill-posed inverse prob-\nlems by efficiently incorporating data and noise into the SI process. Our experiments showed that\nour proposed method consistently outperformed existing methods in tasks like image deblurring,\nwith significant improvements in key performance metrics. The ability to sample from the learned\nposterior enables the exploration of the solution space and facilitates in uncertainty quantification,\nwhich is critical for real-world applications. Future work will focus on refining the model to en-\nhance efficiency and applicability to more ill-posed problems, including integration of advanced\nnoise modeling techniques for extreme noise conditions."}, {"title": "ETHICS AND REPRODUCIBILITY STATEMENTS", "content": "Ethics Statement. In this work, we do not release any datasets or models that could be misused,\nand we believe our research carries no direct or indirect negative societal implications. We do not\nwork with sensitive or privacy-related data, nor do we develop methods that could be applied to\nharmful purposes. To the best of our knowledge, this study raises no ethical concerns or risks of\nnegative impact. Additionally, our research does not involve human subjects or crowdsourcing. We\nalso confirm that there are no conflicts of interest or external sponsorships influencing the objectivity\nor results of this study.\nReproducibility Statement. In Section 3, we outline the training methodology employed in our ex-\nperiments, while in Appendix C, we provide comprehensive supplementary information, including\nreferences to the baselines, detailed dataset descriptions, the experimental settings for each task, and\nthe hyperparameter used in our study. All experiments presented in Section 3 were conducted on\npublicly available benchmarks, while the experiment in Example 2.1 was conducted on simulated\ndata. To further facilitate the reproducibility of our work, we will release all the data and code to\nreproduce our empirical evaluation upon acceptance."}, {"title": "DETAILS ABOUT THE INVERSE PROBLEMS", "content": "In this section", "domain": "n$\\mathcal{B}(u, v) = \\mathcal{I}(u, v) \\cdot \\mathcal{K}(u, v)$ (22)\nwhere $\\mathcal{I} (u, v) = \\mathcal{F}\\{I(x,y)\\}$, $\\mathcal{K}(u, v) = \\mathcal{F}\\{K"}]}