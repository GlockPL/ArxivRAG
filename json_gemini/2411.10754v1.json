{"title": "Integrated Machine Learning and Survival Analysis Modeling for Enhanced Chronic Kidney Disease Risk Stratification", "authors": ["Zachary Dana", "Ahmed Ammar Naseer", "Botros Toro", "Sumanth Swaminathan"], "abstract": "Chronic kidney disease (CKD) is a significant public health challenge, often progressing to end-stage renal disease (ESRD) if not detected and managed early. Early intervention, warranted by silent disease progression, can significantly reduce associated morbidity, mortality, and financial burden. In this study, we propose a novel approach to modeling CKD progression using a combination of machine learning techniques and classical statistical models. Building on the work of Liu et al. (2023), we evaluate linear models, tree-based methods, and deep learning models to extract novel predictors for CKD progression, with feature importance assessed using Shapley values. These newly identified predictors, integrated with established clinical features from the Kidney Failure Risk Equation, are then applied within the framework of Cox proportional hazards models to predict CKD progression.", "sections": [{"title": "1. Introduction", "content": "Chronic kidney disease (CKD) is a major public health concern, characterized by silent disease progression that can lead to end-stage renal disease (ESRD) and the need for kidney transplantation if not detected early (Bai et al., 2022). CKD affects millions globally and is associated with significant morbidity, mortality, and healthcare costs (Kerr et al., 2012). The gradual decline in kidney function in CKD patients often goes unnoticed until the disease has advanced to a critical stage (Kalantar-Zadeh et al., 2021). Early intervention in CKD can improve the quality of life and reduce healthcare expenses by slowing disease progression (Kalantar-Zadeh et al., 2021).\nPredictive models and dynamic risk stratification algorithms can enable identification of patients at high risk of CKD degeneration. These models facilitate timely modifications in patient management, including adjustments to medication, diet, sleep, and exercise regimens, thereby improving patient outcomes (Xiao et al., 2019). In recent years, machine learning (ML) techniques have shown great promise in healthcare applications, providing powerful tools for predictive modeling and risk stratification (Appendix B).\nIn this study, we evaluate an approach to modelling the progression of CKD stages using a combination of ML techniques and classical statistical models. By extending the work proposed by Liu et al. (2023) we evaluate the use of linear models, tree-based models, and deep learning models in extracting novel predictors for CKD degeneration, determined by Shapley values. These novel predictors are then used in conjunction with clinically established kidney failure risk equation (KFRE) features (Appendix C) to model"}, {"title": "Contributions", "content": "1. Identification of potential novel predictors for CKD progression: By leveraging machine learning models, Shapley value analysis, and classical survival models, this work identifies new features beyond the established clinical predictors in the KFRE-8 model to aid in predicting CKD progression. 2. Improved predictive performance for CKD progression: The integration of machine learning-derived predictors with classical Cox proportional hazards models leads to improved predictive accuracy, as demonstrated by higher C-index and lower Brier scores. 3. Extension of Liu et al. (2023) methodology: by exploring additional models for feature selection and extending the Cox proportional hazards modeling to CKD progression."}, {"title": "2. Method", "content": "Our approach follows a structured pipeline for feature selection and modeling. Let \\(D = \\{x_i\\}_{i=1}^P\\), denote a dataset where each \\(x_i \\in \\mathbb{R}^N\\) represents a feature vector. Let \\(f : \\mathbb{R}^N \\rightarrow \\{0,1\\}\\) represent a binary classifier (Appendix D). We first train the classifier \\(f(-; \\theta) : \\mathbb{R}^n \\rightarrow \\{0,1\\}\\) on \\(D_{Tr} \\in \\mathbb{R}^{P \\times n}\\) a training data subset from \\(D\\). After training, we compute the Shapley \\(\\phi_i\\) for each feature \\(x_i\\) (Appendix E). The top \\(j\\) features with the highest mean Shapley values are selected, forming the set \\(F_S\\). Next, we combine \\(F_S\\) with the KFRE-8 feature set \\(F_{KFRE-8}\\) by taking their union, defined as \\(F = F_S \\cup F_{KFRE-8}\\). The dataset \\(D\\) is then reduced to include only features in \\(F\\). Finally, a CPHM is trained on this reduced data set to predict CKD progression (Appendix F), evaluating the novel predictors' efficacy within the framework of survival analysis. Figure 1 illustrates the modelling pipeline used here. This paper expands the methodology proposed by Liu et al. (2023) through the incorporation of a diverse range of model architectures in the feature selection stage, and extends the application to CKD."}, {"title": "3. Experiments", "content": "MIMIC-IV. We extracted the complete subset of patients from MIMIC-IV with documented CKD diagnoses, as indicated by seven relevant ICD-9 codes descibed in Table 1, yielding a cohort of 14,012 patients. Using this subset, we define a binary variable to indicate CKD progression, deemed observed if a patient is diagnosed with a more advanced CKD stage at any time after an earlier diagnosis of a less severe stage. Among the patients in the cohort, 1,483 (10.6%) experienced CKD stage progression over a median follow-up period of 111.5 days (IQR 6.0-910.25). The feature selection models, were trained on a high-dimensional input set, including a combined 1373 demographic, diagnostic, and lab recording features; the full characteristics of the former are summarized in Table 2."}, {"title": "3.2. Feature selection", "content": null}, {"title": "3.2.1. MODELS", "content": "We evaluate the performance of five binary classifiers for the feature selection component of the pipeline: logistic regression (LR), decision tree (DT), random forest (RF), extreme gradient boosting (XGBoost), fully connected neural networks (FCNNs), and residual neural network (ResNet). The linear models (Appendix D.1) and tree-based methods (Appendix D.2) were trained with the logarithmic loss function as the objective criterion, and hyperparameters were selected through Bayesian optimization; detailed search spaces and optimized values for the latter are provided in Table 4. For logistic regression, the solver was specified a lbfgs, and maximum iterations set to 1000.\nThe neural networks (Appendix D.3) utilize binary cross-entropy loss with logits and the Adam optimizer. The FCNN model was configured with 4 hidden layers containing 512, 256, 128, and 1 neuron, each followed by a dropout layer with the rate set to 0.2. For ResNet, we used 3 residual blocks, each consisting of 2 fully-connected layers with a hidden dimension of 64. Single fully-connected layers were applied prior to and following the residual blocks, yielding an architecture with 8 layers in total. Rectified linear unit (ReLU) activation was applied in both architectures. The learning rate, weight decay (L2 regularization), maximum epoch, and early stopping hyperparameters are reported in Table 3.\nAll models were trained and evaluated across five distinct data splits, utilizing unique random seeds for cross-validation. For the linear and tree-based methods, the training sets were further divided into five validation folds to support Bayesian optimization,"}, {"title": "3.2.2. FEATURES", "content": "For each binary classifier, we identified the 40 features with the highest mean absolute Shapley values. The union of these top features, along with those defined in KFRE-8, was then selected as the final feature set. Because of the computational infeasibility of calculating Shapley values as defined in equation (9) for the large number of features, we utilized shapley additive explanations (SHAP) (Lundberg and Lee, 2017) to approximate these values. Specifically, SHAP values were computed using the TreeExplainer (Lundberg et al., 2020) for tree-based models, the LinearExplainer for linear models, and the DeepExplainer for deep learning models."}, {"title": "3.3. Cox proportional hazards model", "content": "Following the feature selection, CPHMs were fitted to explore the associations between the identified novel predictors and CKD stage progression. The data for the final feature set obtained from each binary classifier were used to train the CPHMs. Additionally, a baseline CPHM was trained using only the KFRE-8 features to serve as a control. The CPHMs were implemented with a penalizer set to 0.0007, the minimum value necessary to prevent overfitting. Model fitting involved five-fold cross-validation, and the optimal model was selected based on the highest average concordance Index (C-index) (Appendix G.2) across the validation sets. The models with the best performance, as indicated by the highest average C-index, were preserved for further analysis, along with the corresponding training and testing datasets. The proportional hazards assumption was evaluated using Schoenfeld residuals. Additionally, Brier score (Appendix G.3) and dynamic area under receiving operating characteristic curve (AUROC) plots (Appendix G.1) were generated for the full set of hazards models, offering an assessment of the alignment between predicted risks and observed outcomes over time.\nWe computed Brier score at 5-year for assessing overall model performance and C-index for assessing risk discrimination. We used the two sets of baseline hazard at 5-year and model coefficients (i.e. \"beta values\") yielded from these two Cox models to compute the 5-year risks and prognostic index (i.e. variable beta) of each participant in the training (80%) and test (20%) data, respectively. These 5-year risks were subsequently used to compute Brier score at 5 years and prognostic indices were used to compute C-index using the training and test data, respectively."}, {"title": "4. Results and Discussion", "content": null}, {"title": "4.1. Feature selection results", "content": "Table 5 shows the AUROC scores obtained by the ML models used in the feature extraction process. The comparison of models based on AUROC shows that XGBoost performs the best, with the highest average AUROC of 0.7796 and a best score of 0.8105. DT follows with an average score of 0.7283, and the best-performing model scores 0.7799. LR achieves an average AUROC of 0.7027, performing slightly below DT. RF performs the worst, with an average of 0.5796. Deep learning models such as FCNN and ResNet perform similarly, with average AUROC values of 0.6612 and 0.6540, respectively.\nFigures 2, 3, 4, 5, 6, and 7 show the top 40 mean absolute SHAP values obtained by the feature selection models. Across nearly all models, features related to creatinine (e.g., creatinine mean, max, last, and median) consistently rank among the most important predictors. This trend is particularly observed in the XGBoost, RF, LR, and DT models. Renal dialysis status appears as one of the top contributors in almost all models, particularly in ResNet, LR, FCNN, and DT. Urea nitrogen frequently appears in the top set of important features, particularly in the XGBoost, RF, and ResNet models. For deep learning models, ResNet and FCNN, features such as \"Coronary atherosclerosis of native coronary artery\" and \"Coronary artery disease\" appear as highly important. Markers such as mean corpuscular hemoglobin concentration (MCHC), mean corpuscular hemoglobin (MCH), neutrophils, platelet count, and eosinophils feature prominently across models. Potassium-related features show importance in several models, particularly in XGBoost, ResNet, and RF. In models such as ResNet and FCNN, we see several features related to more specific conditions, like unspecified essential hypertension and other hyperlipidemia. Features related to chronic conditions, like unspecified essential hypertension, acute kidney failure, and diabetes with renal manifestations, are consistently seen in models like LR, ResNet, and FCNN."}, {"title": "4.2. Cox proportional hazards model results", "content": "Table 6 provides the results for C-index obtained using the CPHMs. The LR-augmented Cox model performs the best, with an average of 0.8900 and a best score of 0.9016. Other models, including XGBoost-Augmented Cox and ResNet-Augmented Cox, show competitive results, both achieving an average of around 0.8876 and 0.8878, respectively. The baseline Cox model, with an average C-index of 0.8820, is outperformed by all the augmented models. DT-Augmented Cox and RF-Augmented Cox models show slightly lower averages at 0.8855 and 0.8865, respectively.\nThe CPHM Brier score results are plotted at 5 years in Appendix J, and reported at annual intervals in Table 7. The XGBoost-augmented Cox model outperforms the other models across most time intervals, particularly at 1 year (0.0289), 4 years (0.0750), and 5 years (0.0801). FCNN-Augmented Cox achieves the best Brier scores at 2 years (0.0485) and 3 years (0.0625). DT-Augmented Cox also shows competitive performance, especially at 2 years with a score of 0.0496. Baseline Cox shows higher Brier scores across the intervals, especially at 5 years (0.1120), indicating less accurate predictions compared to the augmented models.\nThe CPHM dynamic AUROC results are plotted at 5 years in Appendix K, and reported at annual intervals in Table 8. The LR-Augmented Cox model performs best for the first 3 years, with the highest AUROC of 0.9634 at 1 year, 0.9499 at 2 years, and 0.9453 at 3 years. However, at the 4- and 5-year intervals, XGBoost-Augmented Cox achieves the highest AUROC of 0.9376 and 0.9507, respectively. Baseline Cox performs consistently, but its scores decline slightly over time, especially at 5 years (0.9113). Other augmented models, such as FCNN and ResNet, show stable but slightly lower AUC scores compared to the best-performing models."}, {"title": "4.3. Discussion", "content": "Across all models, SHAP analysis reveals that traditional kidney function markers, particularly creatinine (e.g., mean, max, last, and median values) and renal dialysis status, are consistently the most important features, affirming their clinical relevance in CKD progression. Additionally, urea nitrogen (max and mean) is another key renal marker that ranks highly, emphasizing its significance in predicting disease outcomes. These results indicate that the models correctly prioritize well-established CKD markers, providing confidence in the feature selection process.\nSHAP analysis also highlights several non-traditional features that are not part of the KFRE-8 model but rank highly across models. Markers such as MCHC, MCH, neutrophils, and platelet count - blood-related features not typically associated with CKD - emerge as important predictors. Their relevance suggests that systemic factors related to hematologic and immune responses may play a role in CKD progression. Though it is worth noting that this could also be a result of the patient population used in this study. Furthermore, potassium levels appear highly in models such as XGBoost and ResNet, reflecting the importance of this for CKD progression prediction. Additionally, deep learning models such as ResNet and FCNN bring attention to cardiac-related conditions, including coronary atherosclerosis and coronary artery disease, which are identified as significant predictors. This suggests that cardiovascular conditions, often comorbid with CKD, should be considered as potential predictors in models for CKD progression. These findings underscore the value of machine learning in surfacing predictors that may extend beyond traditional renal markers, contributing to a more comprehensive understanding of CKD.\nThe application of SHAP values enables not only the identification of important features but also their interpretability, allowing for the validation of established predictors while suggesting new avenues for research. By identifying features not present in the KFRE-8 model, such as creatinine levels, urea nitrogen, potassium, and blood-related markers, this analysis highlights the potential for integrating additional predictors into CKD progression models. This could improve the predictive accuracy and provide deeper insights into disease mechanisms."}, {"title": "5. Limitations", "content": "The identification of novel predictors requires further validation, as these features may serve as proxies for unmodeled processes. Moreover, the reliance on the MIMIC-IV dataset, which is derived from an emergency room setting, limits the generalizability of our findings to patient cohorts within this environment. To strengthen the robustness of our results, validation using an external cohort is necessary to confirm the findings and improve the accuracy of the method."}, {"title": "6. Conclusion", "content": "In this study, we propose and validate a novel approach for predicting CKD progression by integrating machine learning models with classical statistical techniques. This method identifies potential novel predictors of CKD progression while confirming established risk factors. Our results show that combining machine learning-based feature selection with Cox proportional hazards models enhances predictive performance for CKD."}, {"title": "Appendix A. Pipeline Schematic", "content": null}, {"title": "Appendix B. Related Work", "content": "Numerous studies have focused on predicting the outcomes of CKD and its progression. The existing literature documents a variety of methods for developing predictive models, and identification of novel predictors yielding reasonable levels of accuracy, sensitivity, and specificity.\nXiao et al. (2019) assessed the utility of ML models, including logistic regression, Elastic Net, and ensemble methods, to predict 24-hour urinary protein outcomes in CKD patients with proteinuria. Logistic regression emerged as the top performer with an AUC of 0.873, followed closely by linear models such as Elastic Net, lasso, and ridge regression. Bai et al. (2022) compared logistic regression, na\u00efve Bayes, random forest, decision tree, and K-nearest neighbors for predicting ESRD over five years. Random forest achieved the best AUC of 0.81, outperforming the KFRE in sensitivity.\nClassical statistical techniques have also been applied to conduct nuanced survival analysis, as demonstrated in the methodology employed by Ye et al. (2021). Ye et al. (2021) utilized Cox proportional hazards regression to develop a nomogram predicting three-year adverse outcomes for East Asian CKD patients, achieving high C-statistics across datasets. The nomogram's utility was demonstrated via decision curve analysis (DCA), showing a higher net benefit compared to using estimated glomerular filtration rate (eGFR) alone, especially near threshold probabilities where clinical decisions are highly critical.\nLeveraging a combined methodology integrating both ML for enhanced feature selection and classical"}, {"title": "Appendix C. Kidney Failure Risk Equation", "content": "The KFRE is a predictive tool used to estimate the likelihood of a CKD patient progressing to ESRD typically within a 2 to 5 year time frame (Tangri et al., 2016). These equations have been validated across various populations globally, and have shown to effectively aid clinical decision-making, particularly with regard to determining the timing of treatment interventions such as dialysis or transplantation (Major et al., 2019).\nThe KFRE is available in both 4-variable (KFRE-4) and 8-variable model (KFRE-8) versions (Foundation, 2023). KFRE-4 makes use of age, sex, eGFR, and urine albumin-to-creatinine ratio (UACR). KFRE-8 includes these same factors as KFRE-4 with the addition of serum calcium, serum phosphorus, serum bicarbonate, and serum albumin."}, {"title": "Appendix D. Binary Classifier", "content": "Let \\(\\tau \\in \\mathbb{R}\\) be a threshold value, and let \\(f(\\cdot; \\theta) : \\mathbb{R}^n \\leftrightarrow \\{0,1\\}\\) represent a function parameterized by \\(\\theta\\). For a data sample \\(x_i \\in \\mathbb{R}^n\\), a binary classifier assigns a label \\(\\hat{y_i}\\) based on \\(\\tau\\), \n\\[\n\\hat{y_i} = \\begin{cases}\n0, & f(x_i;\\theta) < \\tau, \\\\\n1, & f(x_i; \\theta) \\geq \\tau.\n\\end{cases}\n\\]\nThe function \\(f(\\cdot; \\theta)\\) can be represented by a variety of machine learning models. In this paper, we focus on tree-based methods D.2, linear models D.1, and deep learning approaches D.3, each capable of performing binary classification as defined in equation (1)."}, {"title": "D.1. Linear models", "content": "Logistic regression. Logistic regression is a fundamental statistical modeling technique used to estimate the probability of a binary outcome, based on one or more predictor variables (Hosmer et al., 2013). Let \\(x \\in \\mathbb{R}^{n_0}\\) represent input features and \\(w \\in \\mathbb{R}^{n_0}\\) denote a vector of coefficients to be learned. The probability of the binary event occurring \\(y_i \\in \\{0,1\\}\\) for the i-th data sample, is defined as\n\\[\n\\hat{y_i} = \\sigma(w^T x_i + b).\n\\]"}, {"title": "D.2. Tree-based methods", "content": "Decision tree. A decision tree is a non-parametric supervised learning algorithm that recursively partitions the input space based on feature values, forming a tree structure composed of decision nodes and leaf nodes, where each leaf node assigns a predicted class label (Breiman et al., 1984). The prediction for a data point is given by\n\\[\n\\hat{y_i} = \\sum_{l=1}^L p_l I\\{x \\in R_l\\}\n\\]\nwhere \\(p_l\\) is the predicted class probability for region \\(R_l\\), and \\(I\\{x \\in R_l\\}\\) is an indicator function that equals 1 if the sample x falls into region \\(R_l\\), and 0 otherwise.\nRandom forest. A random forest is an ensemble learning method, aggregating multiple decision trees leads to improve model robustness and generalization (Breiman, 2001). For binary classification, the prediction \\(\\hat{y_i}\\) for a given data point \\(x_i \\in \\mathbb{R}^{n_0}\\) is obtained by aggregating the predictions from T individual decision trees in the forest\n\\[\n\\hat{y_i} = \\frac{1}{T} \\sum_{t=1}^T \\hat{y_i}^{(t)}\n\\]\nHere, \\(\\hat{y_i}^{(t)}\\) denotes the prediction from the t-th decision tree."}, {"title": "Extreme gradient boosting", "content": "Extreme Gradient Boosting (XGBoost) is a scalable and efficient ML algorithm, effectively enhancing model accuracy through the combination of multiple weak learners (Chen and Guestrin, 2016). The prediction for a data sample i can be expressed as the weighted sum of the outputs from K decision trees\n\\[\n\\hat{y_i} = \\sum_{k=1}^K f_k(x_i)\n\\]\nwhere \\(f_k\\) denotes the k-th tree's contribution, and \\(x_i \\in \\mathbb{R}^{n_0}\\) represents the input features of sample i. The ensemble of trees allows the model to capture complex non-linear patterns, much like the layers in a neural network."}, {"title": "D.3. Neural networks", "content": "Fully-connected neural network. A fully connected neural network can be defined as\n\\[\n\\hat{y_i} = \\sigma(W_L(\\sigma(W_{L-1}(...\\sigma(W_1 x_i + b_1)...)+b_{L-1})+b_L))\n\\]\nwhere \\(W_l \\in \\mathbb{R}^{n_l \\times n_{l-1}}\\) and \\(b_l \\in \\mathbb{R}^{n_l}\\) represent the weights and biases for each layer \\(l \\in \\{1, 2, ..., L\\}\\), and \\(\\sigma\\) is an activation function such as the rectified linear unit (ReLU) (LeCun et al., 2015). The input vector \\(x_i \\in \\mathbb{R}^{n_0}\\) represents the features for data sample i, and the output \\(\\hat{y_i}\\) is the predicted label. The activation function \\(\\sigma\\) introduces non-linearity, enabling the network to model complex patterns.\nResidual neural network. A ResNet addresses the degradation problem in excessively deep networks, wherein added depth increases training error, through the introduction of shortcut connections (He et al., 2016). The output of a residual block can be defined as\n\\[\ny_i = F(x_i, \\{W_i\\}) + x_i\n\\]\nwhere \\(x_i \\in \\mathbb{R}^{n_0}\\) is the input vector for data sample i, and \\(y_i\\) is the output of the residual block. The function F consists of weights \\(\\{W_i\\}\\) where \\(W_i \\in \\mathbb{R}^{n_l \\times n_{l-1}}\\, as well as the biases and activation functions for each layer l across the respective layers within the block (6), representing the residual mapping. The addition of the input \\(x_i\\) to the output of F characterizes a shortcut connection, allowing gradients to flow directly through the network with no additional computational complexity or training error degradation. To align the dimensions of x with those of \\(F(x_i, \\{W_i\\})\\), a linear transformation \\(W_s\\) may be applied to \\(x_i\\)\n\\[\ny_i = F(x_i, \\{W_i\\}) + W_s x_i\n\\]\nwhere \\(W_s \\in \\mathbb{R}^{n_1 \\times n_0}\\)."}, {"title": "Appendix E. Shapley values", "content": "Shapley values are a concept from cooperative game theory providing a fair way to distribute total gains or cost amongst multiple players in a coaliton (Shapley, 1953). Let \\(M = \\{1,..,m\\}\\) represent a set of players taking part in a game \\(v : 2^M \\rightarrow \\mathbb{R}\\). The game, v is a characteristic function returning a scalar reward for each coalition \\(A \\subseteq M\\). It is assumed that \\(v(0) = 0\\). The Shapley value of a player k can be computed by\n\\[\n\\phi_k = \\sum_{A \\subseteq M\\backslash\\{k\\}} \\frac{(m- |A| -1)!|A|!}{m!} (v(A\\cup\\{k\\})-v(A)).\n\\]\nHere represents cardinality. Shapley values are commonly used in explainable machine learning (Merrick and Taly, 2020). In the context of machine learning, each feature \\(x_i\\) in a feature vector \\(x \\in \\mathbb{R}^n\\) is considered as the player. While the output from a trained model \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is considered to be reward v. As a result, \\(\\phi_i\\) measures the xi-th feature's importance with respect to model output."}, {"title": "Appendix F. Cox proportional hazards model", "content": "The CPHM is a statistical technique used for modeling the time-dependent risk of an event occurring, often applied in survival analysis (Cox, 1972). Let \\(x \\in \\mathbb{R}^d\\) represent a feature vector of d-dimension and let \\(\\beta \\in \\mathbb{R}^d\\) denote a corresponding vector of coefficients. The CPHM is based on a hazard function, defined by\n\\[\n\\lambda(t | x) = \\lambda_0(t) \\exp(\\beta^T x).\n\\]\nHere \\(\\lambda_0(\\cdot) : \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+\\) is the baseline hazard function, representing the hazard rate when x = 0. The baseline hazard function \\(\\lambda_0(\\cdot)\\) is unspecified, allowing for a semi-parametric approach where the effect of the x is modeled parametrically through \\(\\beta\\), while the baseline hazard can vary non-parametrically over time. This flexibility makes the CPHM a powerful tool for survival analysis, as it does not assume a specific distribution for the survival times. Partial likelihood. The parameters \\(\\beta\\) are estimated using the method of partial likelihood, which focuses on the order of events rather than their exact timing. The partial likelihood function L(\\(\\beta\\)) is given by\n\\[\nL(\\beta) = \\prod_{i=1}^D \\frac{\\exp(\\beta^T x_i)}{\\sum_{j \\in R(t_i)} \\exp(\\beta^T x_j)},\n\\]\nwhere D is the number of observed events, and \\(R(t_i)\\) is the risk set at time \\(t_i\\), consisting of all individuals who are at risk just prior to time \\(t_i\\). The partial likelihood function is maximized to obtain the estimates of \\(\\beta\\)."}, {"title": "Appendix G. Evaluation Metrics", "content": "The AUROC is a metric for evaluating the performance of binary classification models. It measures the ability of a model to distinguish between positive and negative classes by plotting the true positive rate against the false positive rate across various decision thresholds. AUROC score ranges from 0 to 1, where 1 indicates perfect discrimination, 0.5 represents random guessing. In survival analysis, a time-dependent version of the AUROC is often applied to evaluate predictive accuracy at different time points."}, {"title": "G.2. Concordance index.", "content": "The C-index is a metric for evaluating the predictive accuracy of survival models, assessing how well the predicted and actual order of events agree (Harrell et al., 1982). Let T denote the event time, C-index is then defined by,\n\\[\nC = \\frac{\\sum_{i;j} 1[T_i < T_j] 1[\\hat{T_i} < \\hat{T_j}] \\delta_i}{\\sum_{i;j} 1[T_i < T_j] \\delta_i}\n\\]\nHere, \\(\\delta_i = 1\\) if the event time \\(T_i\\) is observed (i.e. not censored), and \\(\\delta_i = 0\\) otherwise. C-index is a generalisation of the AUROC. A C-index of 1 indicates perfect predictions and 0 represents the worst performance."}, {"title": "G.3. Brier score.", "content": "The Brier score is a metric used to evaluate the accuracy of probabilistic predictions in survival analysis (Brier, 1950). It measures the weighted mean squared difference between the predicted probabilities and the actual outcomes. Brier score at time t is defined by,\n\\[\nB(t) = \\frac{1}{N} \\sum_{i=1}^N (1[T_i > t] - \\hat{S}(t | x_i))^2,\n\\]\nwhere \\(\\hat{S}(.)\\) represents a predicted survival function. The Brier score ranges from 0 to 1, with 0 indicating perfect accuracy and 1 representing the worst accuracy."}, {"title": "Appendix H. Supplementary Tables", "content": null}, {"title": "H.1. Data characteristics", "content": null}, {"title": "H.2. Hyperparameters", "content": null}, {"title": "H.3. Model performance", "content": null}, {"title": "H.4. Feature selection", "content": null}]}