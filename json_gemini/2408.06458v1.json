{"title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models", "authors": ["Yen-Che Hsiao", "Abhishek Dutta"], "abstract": "We propose a novel in-context learning algorithm for building autonomous decision-making language agents. The language agent continuously attempts to solve the same task by self-correcting each time the task fails. Our selected language agent demonstrates the ability to solve tasks in a text-based game environment. Our results show that the gemma-2-9b-it language model, using our proposed method, can successfully complete two of six tasks that failed in the first attempt. This highlights the effectiveness of our approach in enhancing the problem-solving capabilities of a single language model through self-correction, paving the way for more advanced autonomous agents. The code is publicly available at https://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE language models (LLMs) are large statistical models that predict the next word, phrase, sentence, or paragraph based on a given input [1]. The quality of the output from a language model can be heavily influenced by the input prompt it receives [2]. One of the capabilities of LLMs is in-context learning, where they learn a new task from a small set of exemplars provided in the prompt during inference [3]. Prompt engineering is the process of designing and refining input prompts to elicit desired responses from LLMs [4].\nIn Chain-of-Thought (CoT) [5], given a prompt with ex- emplars that include an input part and an output part, a chain of thought consists of a series of intermediate natural language reasoning steps added between the input and output parts in each exemplar to produce the final output. However, the CoT prompting doesn't have the ability to update its knowledge from the external world. ReAct [6] prompting address the problem by providing the language model with a prior language description to guide its reasoning about solving diverse language reasoning and decision making tasks and adapting this reasoning by receiving the feedback from the external world. In Reflexion [7], they proposed autonomous decision-making LLM agents by adding a reflection step to the CoT or ReAct prompt to adjust the reasoning, facili- tating language agents' learning from prior failings through verbal reinforcement. VOYAGER [8] is a LLM based agent designed to explore an open-ended world and attain diverse skills through the integration of automatic curriculum, skill library management, and an iterative prompting mechanism incorporating environmental feedback, execution errors, and self-verification to enhance program performance. In Motif [9],..."}, {"title": "II. METHODS", "content": "Let a text game be denoted as a function f that maps the state \\(s \\in V\\) and the action \\(a \\in V\\) to an observation \\(o \\in V\\), where V is a set of vocabulary. Let \\(\\pi_{\\theta}\\) be an LLM agent over a pre-trained set of parameter \\(\\theta\\). Let \\(s_0\\) be the initial state of the environment f, We aim to produce a sequence of actions \\((\\alpha_0, \\alpha_1, \\alpha_2,...)\\), where \\(\\alpha_i \\in V\\) for \\(i \\in \\mathbb{Z}\\), to change the state to a terminal state that indicates the game is cleared.\nIn ReAct prompting [6], they propose to use an LLM to produce the thought \\((t_0, t_1, t_2,...)\\), where \\(t_i \\in V\\) for \\(i \\in \\mathbb{Z}\\), by \\(t_i \\sim \\pi_{\\theta}(t_i|s_i)\\), where \\(s_i = \\{s_{i-1}, t_{i-1}, a_{i-1}, o_{i}\\} \\) for \\(i \\in \\mathbb{Z}^+\\), \\(a_i \\sim \\pi_{\\theta}(a_i|S_i, t_i)\\) for \\(i \\in \\mathbb{Z}\\), and \\(0_{i+1} = f(s_i, a_i)\\) for \\(i \\in \\mathbb{Z}\\).\nThe limitation of ReAct prompting [6] is that complex tasks with large action spaces require more demonstrations to learn effectively. The LLM may produce incorrect actions that do not lead to completing the task. Reflexion [7] addresses this problem by using an additional LLM to iteratively provide self-reflection text that will be added to the ReAct prompt for improvement. More specifically, for each trial \\(e_p \\in \\mathbb{Z}^+\\), if \\(e_p > N\\), where N is a maximum number for each tail, a self- reflection \\(r_p\\) is generated and a new state \\(s_0^{p+1} = \\{s_0^{e_p}, r_p\\}\\) is formed to be used as the initial state in the next trial \\(e_{p+1}\\). However, the method in Reflexion [7] necessitate two LLMs, where one LLM is used to generate the thought or action, and another LLM is used to generate the reflection. We will modify this by using a single LLM to generate thought, action, and adaptation, which is the correction from the previous failed trail.\nThe main idea of our method is for the LLM agent to gener- ate reflections without the need for exemplars. The architecture..."}, {"title": "III. THE ALFWORLD ENVIRONMENT", "content": "There are six types of tasks in the ALFWorld environment [10]: Pick and Place, Examine in Light, Clean and Place, Heat and Place, Cool and Place, and Pick Two and Place. For each task, a description of available receptacles is given in the first part of the instruction as follows: You are in the middle of a room. Looking quickly around you, you see a {recep1 id}, a {recep2 id}, and a {recepN id}, where recepN refers to the Nth receptacles like drawers or cabinet and id\u2208 Z+. An example is shown in the text in Fig. 2 with a green background."}, {"title": "IV. EXPERIMENTAL DESIGNS AND RESULTS", "content": "In ReAct [6], they randomly annotate the trajectories for each task type. Each trajectory includes sparse thoughts that decompose the goal, track subgoal completion, determine the..."}, {"title": "V. DISCUSSION", "content": "In Section IV-A, we observed that gemma-2-9b-it model outperforms other models (gemma-2-9b, Mistral-7B-v0.3, Mistral-7B-Instruct-v0.3, Llama-2-7b-hf, Phi-3-medium-128k- instruct, deepseek-llm-7b-base, and zephyr-7b-alpha) in solv- ing various tasks in the ALFWorld environment [10] using ReAct prompting [6]. We identified three common issues with these LLMs. First, the LLMs may attempt to retrieve an object from a location where the object does not exist, repeatedly performing the same action until reaching the maximum number of steps, 49. Second, the LLMs may select an incorrect item. For instance, when the task is to \"put a clean cloth in countertop\", the gemma-2-9b model may pick up a \"handtowel\" instead. Subsequently, the LLM cannot complete the task by cleaning the \"handtowel\", resulting in repeated at- tempts to clean and place the \"handtowel\" in the \"countertop\" until the maximum step limit is reached. Third, some LLMs will misinterpret the order of sub-goals. For example, when the goal is to \"examine the cd with the desklamp\", the gemma-2- 9b-it model erroneously attempts to retrieve a \"desklamp\" first. After failing to obtain the \"desklamp\", the LLM searches for the \"cd\" but only revisits previously searched locations instead of exploring new ones.\nIn Section IV-B, we investigated the use of a single gemma- 2-9b-it model to generate actions, thoughts, and reflections by incorporating exemplars from ReAct [6] and Reflexion [7] into the input prompt. The experiment was conducted over 24 hours and involved six different tasks. Among the six tasks, five were completed successfully, while one were not finished after reaching 10 reflection trials. For the five tasks that were completed, there were four tasks finished before reaching the reflection step.\nIn the failed task (task number 3), we only reflection obtained was: \"I was stuck in a loop in which I continually examined the fridge 1 instead of using a different action. I should have looked for a lettuce in the fridge 1, then taken it. I will try to execute a different action if I am stuck in a..."}, {"title": "VI. CONCLUSION", "content": "We present a novel in-context learning algorithm designed for a single language model to complete tasks in a text-based game by correcting its previous failures. This approach reduces the number of models used in previous work [7] from two LLMs to one LLM. Our findings indicate that the gemma- 2-9b-it model achieves the highest success rate of 62% for completing tasks in the ALFWorld environment [10] using ReAct prompting [6], compared to other selected open-source language models. We show that using gemma-2-9b-it, two of the six tasks that could not be completed in one trial can be completed in the second attempt by appending the adaptation from the previous trial. Future work will involve further exper- imentation with different approaches to completing decision- making tasks using a single language model without Reflexion exemplars to reduce the number of tokens in the input prompt."}]}