{"title": "PRACTICAL AND REPRODUCIBLE SYMBOLIC MUSIC GENERATION BY\nLARGE LANGUAGE MODELS WITH STRUCTURAL EMBEDDINGS", "authors": ["Seungyeon Rhyu", "Jaehyeon Kim", "Kichang Yang", "Sungjun Cho", "Kyogu Lee", "Moontae Lee"], "abstract": "Music generation introduces challenging complexities to\nlarge language models. Symbolic structures of music of-\nten include vertical harmonization as well as horizontal\ncounterpoint, urging various adaptations and enhancements\nfor large-scale Transformers. However, existing works\nshare three major drawbacks: 1) their tokenization requires\ndomain-specific annotations, such as bars and beats, that\nare typically missing in raw MIDI data; 2) the pure impact\nof enhancing token embedding methods is hardly exam-\nined without domain-specific annotations; and 3) existing\nworks to overcome the aforementioned drawbacks, such as\nMuseNet, lack reproducibility. To tackle such limitations,\nwe develop a MIDI-based music generation framework in-\nspired by MuseNet, empirically studying two structural\nembeddings that do not rely on domain-specific annotations.\nWe provide various metrics and insights that can guide\nsuitable encoding to deploy. We also verify that multiple\nembedding configurations can selectively boost certain mu-\nsical aspects. By providing open-source implementations\nvia HuggingFace\u00b9, our findings shed light on leveraging\nlarge language models toward practical and reproducible\nmusic generation.", "sections": [{"title": "1. INTRODUCTION", "content": "Symbolic music generation is a rapidly growing field in ma-\nchine learning, as reflected by the continuously increasing\nnumber of musical datasets that provide a solid foundation\nfor training large music generation models [1-3]. While\nthe sequential aspect of music naturally led to the application of\nthe Transformer architecture [4] to the musical domain, the\nvertical (e.g., multiple notes can be played simultaneously)\nand temporal (e.g., every note played has a certain duration)\nstructures in musical data call for a careful tokenization pro-\ncedure that is distinct from the typical positional encoding\nused in text generation [5].\nWhile there exist various tokenization techniques such\nas REMI [7] and CP [8], the MIDI-like tokenization [9]\nis considered most compatible with large-scale musical\ndata from cloud-based platforms (e.g. MIDI transcription\nfrom YouTube audio). This MIDI data may not include\ndetailed annotations of the temporal division of bars. While\nthese annotations can be generated manually [10, 11], this\nprocess comes at a high cost. Another approach is to use\nheuristic algorithms [7, 12], but these methods can only\nbe applied reliably when the input music progresses at a\nconstant speed or tempo, with well-separated bars and beats\n[13]. Unfortunately, this is not the case for the majority of\nMIDI data directly transcribed from live music audio (see\nFigure 1), due to performers frequently adding note on- or\noff-sets that deviate from the scripted tempo [14, 15].\nNonetheless, enhancing the vanilla MIDI tokenization\nto effectively encode musical structure without domain-\nspecific annotations is not yet studied in depth. One exem-\nplar work is MuseNet, which introduced additional struc-\ntural embeddings to better derive the structural context of\nmusic from MIDI-based tokens, demonstrating state-of-the-\nart performance in prompt-based music generation [16].\nHowever, the reasoning behind its design choice of struc-\ntural embeddings is still unclear. There is no clear ex-\nperimentation to validate their effects separately from the\noverall capacity of the model architecture. Furthermore,\nMuseNet has not yet published any open-source implemen-\ntations to support further tuning and testing. As a result,\nit is uncertain whether incorporating these supplementary\nembeddings is truly practical and optimal in capturing struc-\ntural contexts of raw MIDI music, which hinders potential\napplications with data that lacks domain annotations.\nIn response to such limitations, we focus on the struc-\ntural embeddings proposed by MuseNet and deeply inves-\ntigate their effect on encoding musical structures based\non raw MIDI data. Specifically, we train a vanilla GPT-2\nmodel [17] equipped with our implementation of the struc-\ntural embeddings from MuseNet. We build these structural\nembeddings upon practical methods considering the noisi-\nness of raw MIDI data. We also perform ablation studies\nby adjusting different initialization methods to study their\nimpact on generation performance. With this paper, we aim\nto provide practitioners with insights into how to effectively\nencode structural information in music without manual an-\nnotations, as well as an accessible pipeline to train models\non large music datasets. Our main contributions are as\nfollows: 1) We focus on the effect of structural embed-\ndings for learning musical structure from the raw MIDI\ndata without domain-specific annotations; 2) We imple-\nment the supplementary structural embeddings suggested\nby MuseNet using practical methods that fit the raw MIDI\ndata. For public use, we open-source our code and a demo\npage; 3) We empirically analyze and clearly validate the\nimpact of different initializations of structural embeddings\nvia substantial evaluation."}, {"title": "2. METHOD", "content": "To encode musical data as a sequence of tokens, we use the\nevent-based tokenization method of MuseNet [9, 16]. We\ndiscard any meta-information and use only 4,196 different\ninput tokens: 4,096 note-on tokens resulting from 32 quan-\ntized velocities and 128 pitch numbers (e.g. <v64:C4>),\n128 note-off tokens with zero velocity (e.g. <v0:C4>), and\n100 time-shift tokens representing 100 divisions from 10\nms to 1000 ms (e.g. <wait:2>). We also use special <BOS>,\n<EOS>, and <PAD> tokens as we need to batch pieces with\nvarying durations into a fixed length."}, {"title": "2.1 Structural Embeddings", "content": "We revisit the original structural embeddings of MuseNet\n[16] and reconstruct them to capture meaningful structures\nin raw MIDI data. We implement the four structural em-\nbedding layers, which are named as part, type, time, and\npitch-class (PC), modifying the MuseNet version towards\nbetter fitness to audio-based MIDI data. Among the four\nembeddings, the part embedding is the only one that is iden-\ntically reconstructed from MuseNet, while others are modi-\nfied or re-interpreted in our application. As demonstrated\nin Figure 2, four additional embedding layers construct the\nstructural embeddings which are concatenated with the to-\nken embeddings, projected to the hidden size of the model,\nand added to the positional embedding. We expect from\nthis pipeline that each structural embedding on each token\nis considered equally important with the token embedding.\nBelow we provide a detailed description of each type of\nstructural embedding:\nPart. The part embedding informs which temporal part\namong the entire song each token is located at. We divide\nthe total time length of a song equally by 128. For each\nevent token, we compute a part class as one of the 128\ntemporal parts of the sequence. As a result, each token is\nindexed starting from 0 to 128, where 0 indicates the non-\nnote tokens (e.g. <BOS>, <EOS>) and 1 indicates the actual\nstarting part of the song.\nType. The type embedding indicates the type of each to-\nken, whether the token is related to note-on(0), note-off(1),\ntime(2), and other types(3) of event. This can help the\nmodel distinguish different token types more easily and\nlearn dependencies among tokens within the same token\ntype. Note that without the type embedding, all tokens\nare learned independently of each other regardless of their\ninherent type.\nTime. The time embedding informs the model time dif-\nference in 10ms between each non-time-shift token and\nits previous non-time-shift token. It is similar to Musenet,\nyet we re-interpreted the original to represent the relative\ntime between the tokens. We directly allocate this infor-"}, {"title": "2.2 Training and Inference", "content": "As our backbone architecture, we use GPT-2 [17] following\nMuseNet [16]. However, we do not utilize sparse atten-\ntion [18] and mixup [19] to focus on the impact of adjusting\nstructural embeddings for musical data without any trade-offs in computational efficiency or regularization. The over-\nall procedure for training and inference using our model is\nillustrated in Figure 3. We concatenate the four structural\nembeddings with input token embeddings along the feature\naxis and project the resulting embeddings into the model's\nhidden dimension using a fully-connected layer. We also\nadd a trainable, randomly initialized sequence positional\nencoding to the resulting embeddings.\nFor training, we use a next-token prediction task, anal-\nogous to language modeling [17]. Let X = [x1,...,XN]\nrepresent a sequence of input tokens, where N denotes the\nsequence's length. At each step, the model learns to pre-\ndict xi from x<i by optimizing L = \u2212 \u2211logp(xn|X<i).\nDuring inference, the model autoregressively generates the\nsequence given a prompt of MIDI tokens. After generating\neach token, we extract its four structural labels-part, type,\ntime, and PC-from all subsequent tokens using rule-based\nmodules. These labels are then fed into the model for the\nnext step in generation."}, {"title": "2.3 Initialization Methods", "content": "In this paper, we examine the effect of two different initial-\nization methods of structural embeddings. The first method\nis to use truncated normal initialization [20]. The second\nmethod involves sinusoidal initialization, applied only to\nthe time-related embeddings. This approach, inspired by\nGuo et al [21], employs sinusoidal encoding to preserve the\ncontinuity of ordinal musical attributes, such as note onset\nand pitch. We similarly use sinusoidal lookup tables for\ntemporal embeddings, part and time, as follows:\nSE(k,2i) = sin(k/(10000/w)^{2i/d})\nSE(k,2i+1) = cos(k/(10000/w)^{2i/d})\n(1)\nwhere k is a class index for part or time, i is a feature in-\ndex, d is the hidden size, and w is a scaling factor. By\nusing two different values for w, we ensure that the corre-\nsponding embeddings are orthogonal, allowing the part and\ntime attributes to be represented without interference [21].\nConsidering that Part is composed of a much larger unit\nthan Time which is in 10ms, we manually set w = 10 and\nw = 1, respectively. We also note that the average length\nof each part is around 432 ms (the average song length is\n55.39 \u00b1 25.23 seconds). By testing different initializations,\nwe expect to examine whether taking temporal continu-\nity into account affects generative performance under the\nnature of musical attributes."}, {"title": "3. EXPERIMENTS", "content": "3.1 Experimental Setup\nDatasets. For training, we use two datasets compris-\ning 1,748 pieces from Pop1k7 [8] and 5,842 pieces from\nGiantMIDI-Piano [22]. Both datasets consist of transcribed"}, {"title": "3.2 Objective Results", "content": "Repeatedness of Musical Patterns. Table 1 shows that\nGPT2-SE outperforms other methods in Structureness In-\ndicators (SI) for all interval lengths and prompt lengths.\nIt demonstrates that sinusoidal initialization may reliably\ngenerate repeated patterns in various lengths, regardless of\nprompt length. GPT2-RE shows better performance than\nGPT2 with prompt lengths longer than 16. It is unstable\ncompared to other models showing large degradation with\nprompts at length 16. It indicates that GPT2-RE may be\nmore beneficial than GPT2 in generating repeated music\nonly from the prompts longer than 5 seconds. In contrast,\nremoving supplementary embeddings may be better than\npreserving them for creating regular patterns, only from\nprompt lengths shorter than 5 seconds.\nRationality and Irregularity of Chord Progression.\nTable 1 shows that GPT2-SE receives the highest scores\nin Chord Progression Variation Rationality (CPVR) among\nall settings, but scores the lowest in Chord Progression Ir-\nregularity (CPI). It indicates that sinusoidal initialization\ncan induce more common chords, which can lead to low\ndiversity of the overall harmonic context compared to the\nother models. This compromise is further substantiated by\nthe data in Figure 4, which shows a robust inverse corre-\nlation between CPI and CPVR. Moreover, high SI values\nmay be associated with high CPVR when certain harmonic\ncontexts are repeated. GPT2-RE shows the worst CPVR\nscores in prompt lengths 16, while CPI scores are shown the\nbest (Figure 4). For longer prompts, GPT2-RE gets better\nCPVR scores than GPT2 while showing worse CPI scores.\nGPT2 reveals the best CPI scores with prompt lengths 64\nand 256. It demonstrates that randomly initialized structural\nembeddings may benefit in making rational chords over the\nvanilla GPT-2 with prompts longer than 5 seconds while\ncreating more unique chords with shorter prompts.\nPeriods of Silence in Prompts. We closely investigate the\nreason for the large degradation in performance of GPT2-\nRE at the prompt length 16. We hypothesize that prompts\nof this length become distinguished in some aspect from\nthe longer prompts, and GPT2-RE is particularly sensitive\nto this corresponding aspect. To verify this assumption, we\ncompute the ratio of non-silence periods within the prompts\n(shortened as PNSR) with lengths 16 and 64. We convert\neach prompt to a P \u00d7 T 2-dimensional piano roll, where"}, {"title": "3.3 Subjective Results", "content": "Table 3 indicates that GPT2-RE achieves the highest num-\nber of wins among the models, while GPT2 records the\nlowest. Table 4 presents pairwise comparisons among the\nmodels regarding Naturalness and Prompt Maintenance, in-\ncluding p-values from the Wilcoxon signed-rank test based\non the scores collected by the raters [9, 27]. In Naturalness,\nall model pairs except for GPT2-SE and GPT2 demonstrate\nsignificant differences (p < .005). Concretely, GPT2-RE\nsignificantly beats GPT2, whereas GPT2-SE does not show\na significant difference from GPT2. In Prompt Maintenance,\nGPT2-RE also significantly outperforms GPT2 (p < .05),\nwhile GPT2-SE does not. These results suggest that incor-\nporating randomly initialized structural embeddings signif-\nicantly improves the ability of the basic GPT-2 to create\nplausible and prompt-consistent music, whereas sinusoidal\ninitialization shows no meaningful difference.\nThese results can give us insight that the recurrence\nof specific passages, which can measured by SI, may not\nsimply exhibit a correlation with the sense of prompt main-\ntenance. Alternatively, scores for Prompt Maintenance are\ndemonstrated to be associated with those of Naturalness, as\nevidenced by an examination of the Pearson's correlation"}, {"title": "3.4 Fitness Scape Plots", "content": "We conduct a detailed investigation of the fitness scape plot\nthat spans the entire song [29]. The fitness scape plots are\nderived from all 12 samples of each model. We set each\nframe at 500 ms to examine the plots at a higher resolution.\nThen, we compile the plots to calculate the maximum fitness\nscores from all stacked scores for each entry within the plot.\nThe X-axis and Y-axis of the plot denote the center and the\nlength of the repeated segments within the song, termed\n'segment center' and 'segment duration,' respectively. We\nuse the maximum scores rather than the average to clearly\nobserve the model's peak performance in creating structural\npatterns across various segment durations.\nFigure 6 displays the resulting fitness scape plots of the\nthree models and GT. The fitness scape plot for GT is dis-\ntinctive compared to those of the other models. It shows\nclearly defined regular patterns, particularly darker at the\nside edges of the triangle. Furthermore, the scores are rela-\ntively low for short segment durations compared to those\nof other models. This suggests that GT may feature musi-\ncal patterns in longer durations, developed from the given\nprompts, leading to the highest Prompt Maintenance (PM)\nscores. While GPT2-RE and GPT2-SE seem similar, GPT2-\nRE reveals higher fitness scores at the upper side of the\nplot than GPT2-SE and lower fitness scores under the plot.\nGPT2-RE also has lower fitness scores for the shorter seg-\nments close to the prompt (at the lower left corner), similar\nto GT. Conversely, GPT2-SE demonstrates fitness scores\nthat are broadly distributed below the plot. It indicates that\nGPT2-RE may be more beneficial than GPT2-SE to gen-\nerate longer patterns derived from the prompt leading to\nhigher PM scores. GPT2 shows a score pattern that com-\npletely neglects the given prompt: the fitness scores are\nconcentrated in the last half of the song, yielding highly\nirregular patterns compared to other models. This aspect\nclarifies why GPT2 obtains the worst PM score. We also\nnote that the aligned Structureness Indicators (SI) scores are\nnot highly correlated to PM scores (r = -0.1429). GPT2\nand GPT2-SE show higher average scores for SI than GT,\nunlike for PM. It may imply that repetition of the musical\npatterns cannot solely represent how well the given prompt\nis retained in the music. This leads to an intuition suggest-\ning the necessity for additional structural metrics that can\ngrasp the high-level fidelity in the musical structure, beyond\nthe simple repetition of musical patterns."}, {"title": "4. CONCLUSION", "content": "We have reconstructed practical structural embeddings for\nraw MIDI data, which lacks domain-specific annotations.\nWe have verified that adding structural embeddings en-"}, {"title": "B. METRIC DETAILS", "content": "B.1 Structureness Indicator.\nWe leverage Libfmp API\u00b3 to compute SI and the corre-\nsponding fitness scape plots. We calculate SI as the overall\nfitness score following the procedure provided by the au-\nthors 4 and parameter settings from [10]: we use the sam-\npling rate of 1 Hz, or 1 second per frame, for computing\nthe objective metric scores. For the fitness scape plot, we\ncompute a N \u00d7 N fitness matrix from each of the 12 lis-\ntening samples for each model, where N is the maximum\nnumber of frames derived from downsampling the original\naudio files, and each frame denotes 500 ms for fine-level\nvisualization [36]. Then, we average the fitness matrices\nby sample. Empirically, we set N = 61 as the maximum\nlength of the samples in 30 seconds. If a sample has a\nlength smaller than N, we pad the remaining length with\nNaN values.\nB.2 Chord Progression Variation Rationality.\nWe follow [12] to compute this metric. The conditional\nprobability Vi is approximated from the appearance fre-\nquency of a unique chord n-grams for each n\u2208 {2,3,4}\nwithin the training set, which is the aggregation of Pop1k7\n[8] and GiantMIDI-Piano [22], as follows:\nV\u2081 = p (C^{i+n} | C^{i-1+n})\n(2)\nwhere i is the chord index, C^{i+n} is a current chord n-gram\nspanning from the i-th chord to the i + n-th chord, and\np( | ) is the conditional probability."}]}