{"title": "PRACTICAL AND REPRODUCIBLE SYMBOLIC MUSIC GENERATION BY LARGE LANGUAGE MODELS WITH STRUCTURAL EMBEDDINGS", "authors": ["Seungyeon Rhyu", "Jaehyeon Kim", "Kichang Yang", "Sungjun Cho", "Kyogu Lee", "Moontae Lee"], "abstract": "Music generation introduces challenging complexities to large language models. Symbolic structures of music often include vertical harmonization as well as horizontal counterpoint, urging various adaptations and enhancements for large-scale Transformers. However, existing works share three major drawbacks: 1) their tokenization requires domain-specific annotations, such as bars and beats, that are typically missing in raw MIDI data; 2) the pure impact of enhancing token embedding methods is hardly examined without domain-specific annotations; and 3) existing works to overcome the aforementioned drawbacks, such as MuseNet, lack reproducibility. To tackle such limitations, we develop a MIDI-based music generation framework inspired by MuseNet, empirically studying two structural embeddings that do not rely on domain-specific annotations. We provide various metrics and insights that can guide suitable encoding to deploy. We also verify that multiple embedding configurations can selectively boost certain musical aspects. By providing open-source implementations via HuggingFace, our findings shed light on leveraging large language models toward practical and reproducible music generation.", "sections": [{"title": "1. INTRODUCTION", "content": "Symbolic music generation is a rapidly growing field in machine learning, as reflected by the continuously increasing number of musical datasets that provide a solid foundation for training large music generation models [1-3]. While the sequential aspect of music naturally led to the application of the Transformer architecture [4] to the musical domain, the vertical (e.g., multiple notes can be played simultaneously) and temporal (e.g., every note played has a certain duration) structures in musical data call for a careful tokenization procedure that is distinct from the typical positional encoding used in text generation [5].\nWhile there exist various tokenization techniques such as REMI [7] and CP [8], the MIDI-like tokenization [9] is considered most compatible with large-scale musical data from cloud-based platforms (e.g. MIDI transcription from YouTube audio). This MIDI data may not include detailed annotations of the temporal division of bars. While these annotations can be generated manually [10, 11], this process comes at a high cost. Another approach is to use heuristic algorithms [7, 12], but these methods can only be applied reliably when the input music progresses at a constant speed or tempo, with well-separated bars and beats [13]. Unfortunately, this is not the case for the majority of MIDI data directly transcribed from live music audio (see Figure 1), due to performers frequently adding note on- or off-sets that deviate from the scripted tempo [14, 15].\nNonetheless, enhancing the vanilla MIDI tokenization to effectively encode musical structure without domain-specific annotations is not yet studied in depth. One exemplar work is MuseNet, which introduced additional structural embeddings to better derive the structural context of music from MIDI-based tokens, demonstrating state-of-the-art performance in prompt-based music generation [16]. However, the reasoning behind its design choice of structural embeddings is still unclear. There is no clear experimentation to validate their effects separately from the overall capacity of the model architecture. Furthermore, MuseNet has not yet published any open-source implementations to support further tuning and testing. As a result, it is uncertain whether incorporating these supplementary embeddings is truly practical and optimal in capturing structural contexts of raw MIDI music, which hinders potential applications with data that lacks domain annotations.\nIn response to such limitations, we focus on the structural embeddings proposed by MuseNet and deeply investigate their effect on encoding musical structures based on raw MIDI data. Specifically, we train a vanilla GPT-2 model [17] equipped with our implementation of the structural embeddings from MuseNet. We build these structural embeddings upon practical methods considering the noisi-ness of raw MIDI data. We also perform ablation studies by adjusting different initialization methods to study their impact on generation performance. With this paper, we aim to provide practitioners with insights into how to effectively encode structural information in music without manual annotations, as well as an accessible pipeline to train models on large music datasets. Our main contributions are as follows: 1) We focus on the effect of structural embeddings for learning musical structure from the raw MIDI data without domain-specific annotations; 2) We implement the supplementary structural embeddings suggested by MuseNet using practical methods that fit the raw MIDI data. For public use, we open-source our code and a demo page; 3) We empirically analyze and clearly validate the impact of different initializations of structural embeddings via substantial evaluation."}, {"title": "2. METHOD", "content": "To encode musical data as a sequence of tokens, we use the event-based tokenization method of MuseNet [9, 16]. We discard any meta-information and use only 4,196 different input tokens: 4,096 note-on tokens resulting from 32 quantized velocities and 128 pitch numbers (e.g. <v64:C4>), 128 note-off tokens with zero velocity (e.g. <v0:C4>), and 100 time-shift tokens representing 100 divisions from 10 ms to 1000 ms (e.g. <wait:2>). We also use special <BOS>, <EOS>, and <PAD> tokens as we need to batch pieces with varying durations into a fixed length."}, {"title": "2.1 Structural Embeddings", "content": "We revisit the original structural embeddings of MuseNet [16] and reconstruct them to capture meaningful structures in raw MIDI data. We implement the four structural embedding layers, which are named as part, type, time, and pitch-class (PC), modifying the MuseNet version towards better fitness to audio-based MIDI data. Among the four embeddings, the part embedding is the only one that is identically reconstructed from MuseNet, while others are modified or re-interpreted in our application. As demonstrated in Figure 2, four additional embedding layers construct the structural embeddings which are concatenated with the token embeddings, projected to the hidden size of the model, and added to the positional embedding. We expect from this pipeline that each structural embedding on each token is considered equally important with the token embedding. Below we provide a detailed description of each type of structural embedding:\nPart. The part embedding informs which temporal part among the entire song each token is located at. We divide the total time length of a song equally by 128. For each event token, we compute a part class as one of the 128 temporal parts of the sequence. As a result, each token is indexed starting from 0 to 128, where 0 indicates the non-note tokens (e.g. <BOS>, <EOS>) and 1 indicates the actual starting part of the song.\nType. The type embedding indicates the type of each token, whether the token is related to note-on(0), note-off(1), time(2), and other types(3) of event. This can help the model distinguish different token types more easily and learn dependencies among tokens within the same token type. Note that without the type embedding, all tokens are learned independently of each other regardless of their inherent type.\nTime. The time embedding informs the model time difference in 10ms between each non-time-shift token and its previous non-time-shift token. It is similar to Musenet, yet we re-interpreted the original to represent the relative time between the tokens. We directly allocate this information to the note-on or -off tokens which follow each time-shift token to explicitly encode the temporal concurrency or progression of notes. If multiple time-shift tokens are interjected, we only count the closest time-shift token. It is to bound the class size as the time differences may be unbounded. We expect the original event tokens to give extra information. Once a time token appears in a sequence, the corresponding time-shift value (i.e. 2 if <time_2>) is assigned as a time class to the following non-time-shift tokens. The time-shift tokens are assigned 0.\nPitch-Class (PC). The pitch-class embedding informs the pitch-class of each note-on or -off token. At every note-on and note-off token, the corresponding pitch-class is directly applied as a PC class, while other tokens are assigned 0. MuseNet originally proposed to use the relative position of each note within the corresponding chord [16]. However, it is tricky to determine the boundary of a chord due to the notes sequentially intermeshed or sustained by the piano pedals as shown in Figure 1b. Therefore, we instead utilize the pitch-class of each note-related token that can explicitly represent the tonal color of each token. We conjecture that incorporating pitch-class embedding allows the model to stack tonal information via self-attention and thus immediately capture the harmonic context of the input."}, {"title": "2.2 Training and Inference", "content": "As our backbone architecture, we use GPT-2 [17] following MuseNet [16]. However, we do not utilize sparse attention [18] and mixup [19] to focus on the impact of adjusting structural embeddings for musical data without any trade-offs in computational efficiency or regularization. The overall procedure for training and inference using our model is illustrated in Figure 3. We concatenate the four structural embeddings with input token embeddings along the feature axis and project the resulting embeddings into the model's hidden dimension using a fully-connected layer. We also add a trainable, randomly initialized sequence positional encoding to the resulting embeddings.\nFor training, we use a next-token prediction task, analogous to language modeling [17]. Let $X = [x_1,..., x_N]$ represent a sequence of input tokens, where N denotes the sequence's length. At each step, the model learns to predict $x_i$ from $x_{<i}$ by optimizing $L = -\\sum log p(x_n | X_{<i})$. During inference, the model autoregressively generates the sequence given a prompt of MIDI tokens. After generating each token, we extract its four structural labels-part, type, time, and PC-from all subsequent tokens using rule-based modules. These labels are then fed into the model for the next step in generation."}, {"title": "2.3 Initialization Methods", "content": "In this paper, we examine the effect of two different initialization methods of structural embeddings. The first method is to use truncated normal initialization [20]. The second method involves sinusoidal initialization, applied only to the time-related embeddings. This approach, inspired by Guo et al [21], employs sinusoidal encoding to preserve the continuity of ordinal musical attributes, such as note onset and pitch. We similarly use sinusoidal lookup tables for temporal embeddings, part and time, as follows:\n$SE_{(k,2i)} = sin(k/(10000/w)^{2i/d})$\n$SE_{(k,2i+1)} = cos(k/(10000/w)^{2i/d})$\nwhere k is a class index for part or time, i is a feature index, d is the hidden size, and w is a scaling factor. By using two different values for w, we ensure that the corresponding embeddings are orthogonal, allowing the part and time attributes to be represented without interference [21]. Considering that Part is composed of a much larger unit than Time which is in 10ms, we manually set w = 10 and w = 1, respectively. We also note that the average length of each part is around 432 ms (the average song length is 55.39 \u00b1 25.23 seconds). By testing different initializations, we expect to examine whether taking temporal continuity into account affects generative performance under the nature of musical attributes."}, {"title": "3. EXPERIMENTS", "content": "Datasets. For training, we use two datasets comprising 1,748 pieces from Pop1k7 [8] and 5,842 pieces from GiantMIDI-Piano [22]. Both datasets consist of transcribed MIDI files from large-scale piano performances. For computational efficiency, we exclude pieces from GiantMIDI-Piano that are longer than 10 minutes. We also apply 7 pitch transpositions up to \u00b13 half steps and 5 time stretches up to \u00b10.05 [9], consequently using a total of 17,383 hours of transcribed MIDI data. A small portion (0.01%) of the training dataset is used for validation. For evaluation, we employ a total of 1,276 songs from the MAESTRO dataset [23]. As several state-of-the-art audio transcription models are trained on MAESTRO [24, 25], it can be considered a high-quality ground truth testbed for the raw MIDI data.\nImplementation. We train the GPT-2 architecture containing 12 hidden layers implemented by Huggingface [20]. We use the AdamW optimizer with a learning rate initially set at 1e-4, which is linearly reduced to 0 after 1,000 warm-up steps. We set the maximum token length as 1024 and run a total of 144,786 training iterations or 6 epochs. We infer all test samples using token prompts from MAESTRO, sampling each prompt five times to reduce contingency in random generations. From each song of MAESTRO, we take the beginning of a song as a prompt and examine the prompt token lengths of $2^l$ with $l \\in \\{4, 6, 8\\}$ (The lengths in time are about 2, 5, 15 seconds, respectively). We use top-k sampling with k = 32 until the sample reaches 1 minute maximum.\nCompared Methods. To study the effect of structural embeddings, we train and test three variants of GPT-2. GPT2 is the vanilla language model without any structural embeddings. GPT2-RE is GPT2 equipped with four structural embeddings, all initialized randomly. GPT2-SE is the same as GPT2-RE, but with part and time embeddings initialized using sinusoidal weights instead."}, {"title": "3.2 Objective Results", "content": "Repeatedness of Musical Patterns. Table 1 shows that GPT2-SE outperforms other methods in Structureness Indicators (SI) for all interval lengths and prompt lengths. It demonstrates that sinusoidal initialization may reliably generate repeated patterns in various lengths, regardless of prompt length. GPT2-RE shows better performance than GPT2 with prompt lengths longer than 16. It is unstable compared to other models showing large degradation with prompts at length 16. It indicates that GPT2-RE may be more beneficial than GPT2 in generating repeated music only from the prompts longer than 5 seconds. In contrast, removing supplementary embeddings may be better than preserving them for creating regular patterns, only from prompt lengths shorter than 5 seconds.\nRationality and Irregularity of Chord Progression. Table 1 shows that GPT2-SE receives the highest scores in Chord Progression Variation Rationality (CPVR) among all settings, but scores the lowest in Chord Progression Irregularity (CPI). It indicates that sinusoidal initialization can induce more common chords, which can lead to low diversity of the overall harmonic context compared to the other models. This compromise is further substantiated by the data in Figure 4, which shows a robust inverse correlation between CPI and CPVR. Moreover, high SI values may be associated with high CPVR when certain harmonic contexts are repeated. GPT2-RE shows the worst CPVR scores in prompt lengths 16, while CPI scores are shown the best (Figure 4). For longer prompts, GPT2-RE gets better CPVR scores than GPT2 while showing worse CPI scores. GPT2 reveals the best CPI scores with prompt lengths 64 and 256. It demonstrates that randomly initialized structural embeddings may benefit in making rational chords over the vanilla GPT-2 with prompts longer than 5 seconds while creating more unique chords with shorter prompts.\nPeriods of Silence in Prompts. We closely investigate the reason for the large degradation in performance of GPT2-RE at the prompt length 16. We hypothesize that prompts of this length become distinguished in some aspect from the longer prompts, and GPT2-RE is particularly sensitive to this corresponding aspect. To verify this assumption, we compute the ratio of non-silence periods within the prompts (shortened as PNSR) with lengths 16 and 64. We convert each prompt to a $P \\times T$ 2-dimensional piano roll, where P is the number of MIDI pitches, and T is the number of time frames. Then, we obtain PNSR as the number of non-zero columns divided by the length of the piano roll. Table 2 shows the distribution of the prompts across 4 ranges of the PNSR. The number of prompts with low PNSR under 0.5 increases when its length is 16, compared to 64. Figure 5 demonstrates relationships between PSNR and the three metric scores of all models averaged across the sub-metrics. All metric scores of GPT2-RE abruptly decline as the PSNR decreases under 0.5 when the prompt length is 16, in contrast to other models revealing relatively minor changes across the prompt length. It signifies that GPT2-RE may be more responsive than the other models to the amount of silence in the input prompt, which depends on the prompt lengths."}, {"title": "3.3 Subjective Results", "content": "Table 3 indicates that GPT2-RE achieves the highest number of wins among the models, while GPT2 records the lowest. Table 4 presents pairwise comparisons among the models regarding Naturalness and Prompt Maintenance, including p-values from the Wilcoxon signed-rank test based on the scores collected by the raters [9, 27]. In Naturalness, all model pairs except for GPT2-SE and GPT2 demonstrate significant differences (p < .005). Concretely, GPT2-RE significantly beats GPT2, whereas GPT2-SE does not show a significant difference from GPT2. In Prompt Maintenance, GPT2-RE also significantly outperforms GPT2 (p < .05), while GPT2-SE does not. These results suggest that incorporating randomly initialized structural embeddings significantly improves the ability of the basic GPT-2 to create plausible and prompt-consistent music, whereas sinusoidal initialization shows no meaningful difference.\nThese results can give us insight that the recurrence of specific passages, which can measured by SI, may not simply exhibit a correlation with the sense of prompt maintenance. Alternatively, scores for Prompt Maintenance are demonstrated to be associated with those of Naturalness, as evidenced by an examination of the Pearson's correlation coefficient between the scores of these two metrics across the models (r = 0.6559). Moreover, it may imply that the least trade-off between CPVR and CPI scores shown in GPT-RE can be beneficial in reaching higher naturalness and prompt maintenance compared to the other two models revealing extreme trade-offs. In the following section, we investigate how GPT2-RE can be superior in listeners' evaluation to the other models in terms of prompt maintenance."}, {"title": "3.4 Fitness Scape Plots", "content": "We conduct a detailed investigation of the fitness scape plot that spans the entire song [29]. The fitness scape plots are derived from all 12 samples of each model. We set each frame at 500 ms to examine the plots at a higher resolution. Then, we compile the plots to calculate the maximum fitness scores from all stacked scores for each entry within the plot. The X-axis and Y-axis of the plot denote the center and the length of the repeated segments within the song, termed 'segment center' and 'segment duration,' respectively. We use the maximum scores rather than the average to clearly observe the model's peak performance in creating structural patterns across various segment durations.\nFigure 6 displays the resulting fitness scape plots of the three models and GT. The fitness scape plot for GT is distinctive compared to those of the other models. It shows clearly defined regular patterns, particularly darker at the side edges of the triangle. Furthermore, the scores are relatively low for short segment durations compared to those of other models. This suggests that GT may feature musical patterns in longer durations, developed from the given prompts, leading to the highest Prompt Maintenance (PM) scores. While GPT2-RE and GPT2-SE seem similar, GPT2-RE reveals higher fitness scores at the upper side of the plot than GPT2-SE and lower fitness scores under the plot. GPT2-RE also has lower fitness scores for the shorter segments close to the prompt (at the lower left corner), similar to GT. Conversely, GPT2-SE demonstrates fitness scores that are broadly distributed below the plot. It indicates that GPT2-RE may be more beneficial than GPT2-SE to generate longer patterns derived from the prompt leading to higher PM scores. GPT2 shows a score pattern that completely neglects the given prompt: the fitness scores are concentrated in the last half of the song, yielding highly irregular patterns compared to other models. This aspect clarifies why GPT2 obtains the worst PM score. We also note that the aligned Structureness Indicators (SI) scores are not highly correlated to PM scores (r = -0.1429). GPT2 and GPT2-SE show higher average scores for SI than GT, unlike for PM. It may imply that repetition of the musical patterns cannot solely represent how well the given prompt is retained in the music. This leads to an intuition suggesting the necessity for additional structural metrics that can grasp the high-level fidelity in the musical structure, beyond the simple repetition of musical patterns."}, {"title": "4. CONCLUSION", "content": "We have reconstructed practical structural embeddings for raw MIDI data, which lacks domain-specific annotations. We have verified that adding structural embeddings enhances the model's performance in capturing the structural aspects of music compared to the model without them. It is encouraging that structural embeddings can be easily integrated with existing language models such as GPT-2. Under various embedding setups, we find that: 1) random initialization generates more plausible music for human listeners but lacks stability; 2) sinusoidal initialization of temporal embeddings helps produce repeated patterns and common chords in music. These distinct strengths of the two settings may enable users to enhance specific aspects of music in certain ranges when creating their products. Our demonstration showcases the corresponding examples."}, {"title": "A. RELATED WORK", "content": "A.1 Transformer-based Symbolic Music Generation.\nInspired by previous work on large-scale text generation, there exist multiple frameworks that leverage the Transformer architecture for event-based music generation. Among many, Music Transformer [9] was the first to successfully apply Transformers to music generation with long-range dependencies using relative local attention. In the following year, MuseNet [16] proposed applying the GPT2 [17] architecture to MIDI-based music generation, showing state-of-the-art performance under prompts with various contexts such as composer and instrumentation. Several works have also studied music generation in specific genres such as Pop Music Transformer [7] and Jazz Transformer [10]. MusicBERT [30] proposed using bidirectional masked token prediction as in BERT [31] with bar-level masking to avoid information leakage among nearby tokens. Museformer [27] proposed using a coarse- and fine-grained attention module to reduce the quadratic computational cost of attention to nearly linear. In our work, we implement a revamped version of MuseNet [16] to cope with the lack of reproducible code, and also empirically study different design choices on structural embedding initializations and prompt lengths during inference."}, {"title": "A.2 Structural Embeddings for Music Generation", "content": "For sequence modeling, the original Transformer architecture proposed an absolute sinusoidal positional encoding [4]. To cope with long sequences, T5 [32] and Transformer-XL [33] have proposed a relative positional encoding that adds inductive bias directly to the attention-score matrix. Extending to vision- and graph-data, ViT [34] and Graph Transformer [35] also proposed positional encoding modules that leverage only topological data structure, meaning that it can be applied to any large-scale datasets without additional knowledge. In the musical domain, there exist many variations of event-based MIDI tokenization pipelines such as REMI [7], CP [8], and OctupleMIDI [30] that aggregate structural MIDI information into a single token. While they effectively reduce the computational cost by shortening the overall sequence length, these require additional structural information that is often missing in in-the-wild music data such as chord and bar structure. Therefore, we focus on the structural embedding procedure for event-based MIDI tokenization that does not require any human annotation or heuristic labeling."}, {"title": "B. METRIC DETAILS", "content": "B.1 Structureness Indicator.\nWe leverage Libfmp API to compute SI and the corresponding fitness scape plots. We calculate SI as the overall fitness score following the procedure provided by the authors and parameter settings from [10]: we use the sampling rate of 1 Hz, or 1 second per frame, for computing the objective metric scores. For the fitness scape plot, we compute a N \u00d7 N fitness matrix from each of the 12 listening samples for each model, where N is the maximum number of frames derived from downsampling the original audio files, and each frame denotes 500 ms for fine-level visualization [36]. Then, we average the fitness matrices by sample. Empirically, we set N = 61 as the maximum length of the samples in 30 seconds. If a sample has a length smaller than N, we pad the remaining length with NaN values."}, {"title": "B.2 Chord Progression Variation Rationality.", "content": "We follow [12] to compute this metric. The conditional probability $V_i$ is approximated from the appearance frequency of a unique chord n-grams for each $n \\in \\{2,3,4\\}$ within the training set, which is the aggregation of Pop1k7 [8] and GiantMIDI-Piano [22], as follows:\n$V_i = p (C^{i+n} | C^{i-1+n})$\nwhere i is the chord index, $C^{i+n}$ is a current chord n-gram spanning from the i-th chord to the $i + n$-th chord, and p( | ) is the conditional probability."}]}