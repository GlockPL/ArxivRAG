{"title": "NASH: Neural Architecture and Accelerator Search for Multiplication-Reduced Hybrid Models", "authors": ["Yang Xu", "Huihong Shi", "Zhongfeng Wang"], "abstract": "The significant computational cost of multiplications hinders the deployment of deep neural networks (DNNs) on edge devices. While multiplication-free models offer enhanced hardware efficiency, they typically sacrifice accuracy. As a solution, multiplication-reduced hybrid models have emerged to combine the benefits of both approaches. Particularly, prior works, i.e., NASA and NASA-F, leverage Neural Architecture Search (NAS) to construct such hybrid models, enhancing hardware efficiency while maintaining accuracy. However, they either entail costly retraining or encounter gradient conflicts, limiting both search efficiency and accuracy. Additionally, they overlook the acceleration opportunity introduced by accelerator search, yielding sub-optimal hardware performance. To overcome these limitations, we propose NASH, a Neural architecture and Accelerator Search framework for multiplication-reduced Hybrid models. Specifically, as for NAS, we propose a tailored zero-shot metric to pre-identify promising hybrid models before training, enhancing search efficiency while alleviating gradient conflicts. Regarding accelerator search, we innovatively introduce coarse-to-fine search to streamline search process. Furthermore, we seamlessly integrate these two levels of searches to unveil NASH, obtaining optimal model and accelerator pairing. Experiments validate our effectiveness, e.g., when compared with the state-of-the-art multiplication-based system, we can achieve \u21912.14\u00d7 throughput and \u21912.01\u00d7 FPS with 10.25% accuracy on CIFAR-100, and 1.40\u00d7 throughput and \u21911.19\u00d7 FPS with 10.56% accuracy on Tiny-ImageNet.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the remarkable success of deep neural networks (DNNs) in various computer vision tasks [1], [2], [3], the involved intensive multiplications yield significant hardware costs, thus hindering DNNs' deployment on resource-constrained edge devices. To enhance hardware efficiency, prior works [4], [5], [6], [7], [8] have developed multiplication-free models that substitute multiplications with hardware-friendly operations, such as bitwise shifts and additions. For example, DeepShift [7] substitutes multiplications with bitwise shifts, thereby building models exclusively with shift layers. Besides, AdderNet [4] trades multiplications with additions, thus constructing models with solely adder layers. Furthermore, ShiftAddNet [8] integrates both shift and adder layers, developing models with merely hardware-friendly bitwise shifts and additions. Despite their notable hardware efficiency, multiplication-free models are generally inferior to their multiplication-based counterparts in accuracy [9], [10]. Thus, motivated by the high accuracy of multiplication-based models and the promising hardware efficiency of multiplication-free models, there is an urgent demand for multiplication-reduced hybrid models to marry the benefits of both multiplications and hardware-friendly operations.\nTo achieve this goal, prior works leverage Neural Architecture Search (NAS) to automatically design such multiplication-reduced hybrid models [9], [10], [14]. Specifically, NASA [14] proposes a dedicated differentiable NAS (DNAS) engine to automatically construct hybrid models in a differentiable manner, which yet requires an expensive retraining process for searched models. Subsequently, NASA-F [10] develops a tailored one-shot NAS method to fully optimize all sub-networks within the pre-defined multiplication-reduced hybrid supernet, thus achieving high accuracy without retraining/finetuning. Despite its effectiveness, one-shot NAS still suffers from several drawbacks. Specifically, as shown in Fig. 1 (a), (i) one-shot NAS typically involves the random sampling of sub-networks with diverse qualities during the supernet training stage, thus yielding gradient conflicts and hindering the achievable accuracy; Additionally, (ii) the accuracy evaluation through model inference is time-consuming, thereby decelerating the subsequent architecture search process. To solve these limitations, zero-shot metrics have been applied to predict promising networks before model optimization [15], [16], [17], [18], [19], [20]. For example, the recent work PreNAS [13] advocates using SNIP [15] to pre-identify potentially high-quality sub-networks before supernet training. By doing this, as illustrated in Fig. 1 (b), (i) assessment using zero-shot metrics has been proven to be much faster than accuracy measurement [13], thus search efficiency is significantly enhanced; Besides, (ii) during the subsequent supernet training phase, training resources can be concentrated on these selected sub-networks instead of randomly sampled ones, allowing for the alleviation of gradient conflicts. However, due to the different algorithmic properties between multiplication-based convolutions and multiplication-free operations [4], [14], existing zero-shot metrics developed for multiplication-based models [15], [18], [19], [21] are not directly applicable to our desired multiplication-reduced models, calling for the exploration of tailored ones.\nIn parallel, various works [9], [10], [22] have developed dedicated accelerators to improve the hardware efficiency of DNNs from the hardware perspective. However, considering the intricate design space of accelerators, including the accelerator configuration (e.g., PE number and buffer size) and mapping method (also dubbed dataflow), it is non-trivial to handcraft an optimal dedicated accelerator, calling for automatic tools [23], [24], [25], [26], [27]. Specifically, NAAS [23] utilizes an evolutionary algorithm to search for the optimal accelerator configuration and mapping method. Then, it seamlessly integrates the accelerator search process with neural architecture search, thereby directly yielding the optimal pairing of model and accelerator. Inspired by its success, it is highly desired to leverage accelerator search to automatically build the accelerator dedicated to our desired multiplication-reduced hybrid model for boosting hardware efficiency. However, due to the distinct algorithmic properties exhibited by heterogeneous layers within the hybrid model, including convolution layers, shift layers, and adder layers, there arises a necessity to construct separate computing engines for each type of layer, aiming to enable independent processing [10], [14]. This inherently expands the search space and inevitably amplifies the search complexity of our dedicated accelerator [14], thus necessitating fast search strategies.\nTo this end, in pursuit of advancing the search efficiency, model accuracy, and hardware efficiency for multiplication-reduced hybrid models, we develop a neural architecture and accelerator co-search framework tailored to such hybrid models, and make the following contributions:\n\u2022 We propose NASH (see Fig. 2), a Neural architecture and Accelerator co-Search framework dedicated to multiplication-reduced Hybrid models. To the best of our knowledge, this is the first model-accelerator co-search framework for such hybrid models.\n\u2022 On Neural Architecture Search (NAS) level, we develop a tailored zero-shot metric to pre-identify promising multiplication-reduced hybrid architectures before training, thus enhancing search efficiency as well as alleviating gradient conflicts during the subsequent supernet optimization to enhance accuracy.\n\u2022 On the accelerator search level, considering the enormous search space of the accelerator dedicated to our hybrid models, we innovatively propose a coarse-to-fine search strategy to significantly expedite the accelerator search process. Besides, this accelerator search can be further integrated with the above NAS process to obtain NASH, aiming to directly obtain optimal pairing of multiplication-reduced models and dedicated accelerators.\n\u2022 Extensive experimental results consistently validate our superiority. Particularly, we offer up to 10.56% accuracy on Tiny-ImageNet over the prior multiplication-reduced work NASA-F, and \u21912.14\u00d7 throughput as well as \u21912.01\u00d7 FPS with $0.25% accuracy against the state-of-the-art (SOTA) multiplication-based system on CIFAR100."}, {"title": "II. RELATED WORKS", "content": "To alleviate the computational burden imposed by resource-dominant multiplications involved in DNNs, prior works have developed multiplication-free models with hardware-friendly alternatives, such as bitwise shifts [7] and additions [4], [5], [6], [28], thereby contributing to the progress in developing efficient models. For example, DeepShift [7] introduces shift layers that approximate multiplications using power-of-two equivalents, effectively substituting multiplications with bit-wise shifts. AdderNets [4], [5], [6], [28] presents adder layers, which utilize the l\u2081-normal distance to assess the similarity"}, {"title": "A. Multiplication-Reduced DNNs", "content": "between inputs and weights, thereby trading multiplications for additions. Furthermore, ShiftAddNet [8] integrates the aforementioned shift and adder layers, resulting in multiplication-free models primarily with both bitwise shifts and additions. However, despite the promising hardware efficiency of these multiplication-free DNNs, they generally suffer from lower accuracy compared to their multiplication-based counterparts [8], [10], [14], [28]. To this end, multiplication-reduced models [8], [9], [10], [14], [29], [30] are highly desired to marry the benefits of both multiplication-based and multiplication-free models, aiming to enhance hardware efficiency while maintaining accuracy. For instance, NASA [14] and NASA-F [10] integrate multiplication-based convolutions as well as multiplication-free shift and adder layers to construct hybrid search space, on top of which, they leverage search algorithms to automatically build multiplication-reduced hybrid models."}, {"title": "B. Neural Architecture Search (NAS)", "content": "Neural Architecture Search (NAS) [10], [11], [12], [14], [31], [32], [33], which aims to search for the optimal network through enormous model architectures, has emerged as an effective approach to automatically design efficient models with saved human effort and expert knowledge. Among them, one-shot supernet-based NAS [10], [11], [12], [31], [32], [33] has achieved remarkable results and typically consists of two stages: supernet training and resource-constrained architecture search, as shown in Fig, 1 (a). Specifically, in the supernet training process, sub-networks are generally randomly sampled and optimized. Subsequently, the resource-constrained architecture search is applied to identify the optimal network from the well-trained search space through accuracy evaluation while adhering to a specific resource constraint. For isntance, NASA-F [10] develops a tailored one-shot NAS engine for multiplication-reduced models, aiming to maintain accuracy while enhancing hardware efficiency.\nRecently, some frontier works [13], [15], [16], [18], [34] have proposed zero-shot metrics, aiming to pre-identify potentially promising architectures without model training. Motivated by this, PreNAS [13] advocates swaping the order of supernet training and search process in the standard one-shot NAS. Specifically, as depicted in Fig. 1 (b), it employs a zero-shot metric (i.e., SNIP [15]) to predict promising sub-networks, then allocates training resources to optimize the selected ones instead of randomly sampled ones to facilitate supernet training. However, zero-shot metrics tailored for multiplication-reduced hybrid models are still under-explored."}, {"title": "C. Accelerator Search", "content": "Considering the enormous design space within a dedicated accelerator, including (i) the hardware configurations specifying the hardware resource consumption in terms of buffers and computational resources and (ii) the mapping method (also dubbed dataflow) that indicates how computations are scheduled onto the accelerator, it is non-trivial to manually craft an optimal accelerator, which demands substantial expertise and iterative trials [23], [25], [35]. To solve this limitation, accelerator search methodologies [23], [25], [35] have been developed to automatically identify both the optimal hardware configuration and mapping method, thus enhancing hardware efficiency. Additionally, the accelerator exploration can be further incorporated with the neural architecture search to directly obtain optimal pairing of models and accelerators [23], [35]. For instance, NAAS [23] formulates the network and accelerator co-search as a multi-loop process and employs an evolutionary algorithm to facilitate this complex procedure. Despite the effectiveness of the above works, they are exclusively tailored to designing accelerators for homogeneous networks that are characterized by merely multiplication-based operations. In light of the heterogeneous layers in our desired multiplication-reduced hybrid models, which include both multiplication-based and multiplication-free layers, the search space within the accelerator dedicated to such hybrid models becomes more intricate [9], [14], calling for more effective search solutions."}, {"title": "III. THE NEURAL ARCHITECTURE SEARCH", "content": "In this section, we first introduce our hybrid search space that integrates both multiplication-based convolutions and multiplication-free operations; Then, Sec. III-B illustrates our zero-shot search strategy that is equipped with a tailored zero-shot metric to pre-identify promising sub-networks within our hybrid search space before network training; Finally, Sec. III-C details our preference-biased supernet training, which concentrates training resources on these selected sub-networks, aiming to alleviate gradient conflict and boost accuracy."}, {"title": "A. The Hybrid Search Space", "content": "Fundamental Operations. To effectively search for desired multiplication-reduced models, we unify multiplication-based convolutions and multiplication-free shift and adder layers to construct our hybrid search space following [10]. Next, we will illustrate shift and adder layers, respectively.\n\u2022 Shift layers. To enhance hardware efficiency, as outlined in Eq. (1), DeepShift [7] advocates the utilization of shift layers, where the weights $W_{shift}$ are derived by quantizing the vanilla convolutional weights $W_{conv}$ to their power-of-two equivalents following Eq. (2). By doing this, the costly multiplications in convolutions can be effectively substituted by hardware-efficient bitwise shifts.\n$Y = \\sum XT * W_{Shift},$   (1)\n$W_{Shift} = s * 2^{p}$, where\n$s = sign(W_{Conv}), p = round(log_2|W_{Conv}|)$.   (2)\n\u2022 Adder layers. As an alternative, as formulated in Eq. (3), AdderNet [4] builds adder layers that employ the $l_1$-norm distance to measure the relevance between activations $X$ and weights $W_{adder}$, thus trading multiplications with efficient additions.\n$Y = \\sum |X - W_{adder}|.$   (3)"}, {"title": "B. Zero-Shot Search", "content": "1) The Tailored Zero-shot Metric: To alleviate gradient conflict during supernet training and boost accuracy, zero-shot metrics are highly desired to pre-identify high-quality sub-networks within our pre-defined hybrid search space before network optimization.\nThe Observation and Challenge. Zero-shot metrics tailored for multiplication-based models [15], [18], [20], [21], [38] have been widely explored and achieved remarkable success. However, we have observed non-negligible performance degradation when directly applying them to our multiplication-reduced models. Specifically, as demonstrated in Table II, the Kendall Tau Coefficient [39], a widely-used similarity measurement metric, between existing popular zero-shot metrics [15], [18], [19], [20], [21], [38], [40] and model accuracy is obviously higher in multiplication-based models than in our multiplication-reduced hybrid models. This underscores the necessity for customized solutions for our hybrid models, which is yet under-explored.\nOur Proposed Solution. Existing zero-shot metrics typically assess networks' performance based solely on trainability or expressivity [17], [42], resulting in biased measurements. To enable more accurate assessments, inspired by TE-NAS [17], we assess both the expressivity and trainability of models by integrating distinct zero-shot metrics, where the pivotal challenge lies in identifying effective zero-shot metrics tailored for our multiplication-reduced models.\na) Trainability: Models with high trainability can be effectively optimized via gradient descent, thus demonstrating high accuracy. To distinguish a powerful zero-shot metric for evaluating the trainability of our multiplication-reduced models, we first intuitively explore the widely-adopted gradient-based methods, including SNIP [15], Jacob covariance [38], Grad Norm [21], Synflow [19], Grasp [18], and Fisher [20]. However, as depicted in Table II, they all suffer from severe performance drops. To gain deeper insights into this degradation, we take the representative gradient-based metric SNIP [15] as an illustrative example. Specifically, SNIP assesses model trainability by measuring the importance of its parameters in both the forward and backward processes and can be defined as follows:\n$SNIP = \\sum(0, \\nabla_{\\theta_{i}}L)|,$   (4)\nwhere () donates the inner product, $N$, $\\theta_{i}$ and $L$ are the number of layers, the parameter vector of the i-th layer within the given network, and the loss value, respectively. Unfortunately, in our multiplication-reduced hybrid models that includes both multiplication-free shift and adder layers alongside multiplication-based convolutions, occasions are more complicated. For example, weights are discrete and gradients are biased in shift layers [7]. Moreover, the value magnitude of both weights and gradients in adder layers are significantly larger than those in convolutions [4]. Therefore, due to the distinct behaviors of heterogeneous layers concerning weights and gradients, existing gradient-based zero-shot metrics designed for homogeneous models are inherently inapplicable to our hybrid models, yielding performance degradation.\nTo overcome this issue, we redirect our attention to the connectivity-based methods [34], [43], [44], which evaluate trainability through the analysis of models' connectivity patterns (e.g., the topology of concatenation-type skip connections [34]). Particularly, we select NN-Degree [34], which is the SOTA one and can be formulated as:\n$NN-Degree = \\sum(N_{i} \\frac{C_{i,j}^O}{\\sum N_{i} C_{i,j}^I+ C_{i,j}^R})$   (5)\nwhere $B$ denotes the total block number in a given network and $N_{i}$ is the layer number of the i-th block. $C_{i,j}^O$ and $C_{i,j}^I$ are the output and input channel number of the j-th layer in the i-th block, and $C_{i,j}^R$ is the residual connection channel number of the i-th block. As verified in Table II and Fig. 3, while NN-Degree does not exhibit the best predictive performance on multiplication-based models, it outperforms other gradient-based methods on our hybrid models. Besides, it enables faster evaluation as no gradient computation is involved.\nb) Expressivity: Expressivity refers to the expressive capability of models. To identify the most effective zero-shot metric for the expressivity assessment of our hybrid models, we conducted experiments on two of the most well-known expressivity-based metrics: Linear Regions Number [40] and Zen-Score [16], [41]. Specifically, Linear Region Number assesses the expressivity of models by counting the number of unique linear regions within their input space [40]. Zen-Score measures models' Gaussian complexity to evaluate their expressivity [16], [41] and can be formulated as follows:\n$Zen-Score = log E_{x,\\epsilon} (|| f_{\\epsilon}(x) \u2013 f_{\\epsilon}(x + \\alpha \\epsilon)||_F) +\nE_{k,i} log (\\sum(of)^{2}),$  (6)\nwhere x is a sampled Gaussian random vector, \\epsilon is a small input perturbation, ||\u00b7 ||F is the Frobenius norm, $\\alpha$ is a tunable hyper-parameter, C is the number of output channels of the ith layer, and $\\sigma_{i}^2$ is the variance of the kth sample in an input batch data for the ith layer's jth channel. As validated in Table II, Zen-Score shows superior estimation performance over Linear Region Number on both multiplication-based models and multiplication-reduced hybrid models. Additionally, it also exhibits better computational efficiency as counting the unique linear region number of large models involved in Linear Region Number has been proved to be very time-consuming [16]. Therefore, we select Zen-Score to assess the expressivity of our multiplication-reduced models.\nc) Overall: To enable a more accurate assessment of our hybrid networks, we further integrate the aforementioned two selected zero-shot metrics. Specifically, given the significant difference in score magnitudes between these two metrics, as illustrated in Figs. 3 (b) and 3 (c), we add the relative rankings [17] instead of magnitudes of these two scores. Formally, given a group of networks N, the score of our tailored zero-shot metric for a specific network a is defined as follows:\n$Score(\\alpha, N) = rank(Zen-Score(\\alpha), Zen-Score(N))+ rank(NN-Degree(\\alpha), NN-Degree(N)),$   (7)\nwhere the first term computes the relative ranking of the Zen-Score of network a within the Zen-Scores of the network group N. For instance, if a exhibits the highest Zen-Score among N, the term yields a value of 0. Table II and Fig. 3 verify the effectiveness of our tailored zero-shot metric, which showcases the highest Kendall-Tau Correlation. It is noteworthy that our proposed metric also contributes to enhanced search efficiency, owing to the swift computational speed of both NN-Degree and Zen-Score. For example, the assessment of accuracy for an individual hybrid model derived from our supernet takes an average of 30 seconds, whereas the computation of our tailored zero-shot metric requires less than 2 seconds, which is over 15\u00d7 faster when tested on CIFAR100 and profiled on an NVIDIA GeForce RTX 2080Ti.\n2) Neural Architecture Search: On top of the tailored zero-shot metric, we leverage the evolutionary algorithm (i.e., the genetic algorithm) [45] to expedite the pre-identification of promising sub-networks, aiming to alleviate gradient conflicts during the subsequent supernet training to boost accuracy. In detail, we first (i) randomly sample a population of sub-networks A from our pre-defined hybrid supernet (see Sec. III-A and Table I); Then, (ii) we expand the population A by crossover and mutation; Subsequently, (iii) we update A by ranking candidates based on the score of our tailored zero-shot metric following Eq. (7) and retaining only the top-k ones A, subject to given hardware constraints. Note that to enable fast and accurate estimations, we follow [10], [14] to build a cycle-accurate chip simulator on top of our dedicated accelerator (which will be introduced in Sec. IV) to measure hardware performance. Finally, steps (ii) to (iv) are iterated until the pre-determined iteration number is reached. The algorithm pipeline is outlined in Alg. 1 and will be detailed in Sec. IV-C."}, {"title": "C. Preference-Biased Supernet Training", "content": "After identifying promising hybrid sub-networks through our zero-shot search, training resources can be concentrated on these selected sub-networks via preference-biased supernet training, aiming to boost accuracy. To facilitate this process, we leverage the SOTA supernet training strategy in [10], [12], which includes a sandwich-rule-guided architecture sampling [11] and an a-divergence-based knowledge distillation [12]. Precisely, the training process can be defined as:\n$min E_{\\theta_{i,i-1}\\sim A} {L_{CE}(\\theta, a_{b}) + \\gamma [L_{KD}(\\theta, a_{s}) + \\sum L_{KD} (\\theta, a_r)]},$  (8)\nwhere $\\theta$ is the supernet weights, LCE denote the cross entropy loss, and $\\gamma$ is the loss coefficient. Besides, the sandwich rule is applied to simultaneously optimize the smallest sub-network as, the biggest sub-network ab, and M random networks ar from our pre-defined sub-networks pool A, thus pushing for-ward both the performance lower bound (as) and upper bound (as) of A. Additionally, LKD represent the a-divergence-based knowledge distillation, which leverages the soft logits from ab to optimize ar and as through a-divergence, aiming to alleviate the issue of under-estimation and over-estimation of vanilla knowledge distillation."}, {"title": "IV. THE ACCELERATOR SEARCH", "content": "In this section, we first introduce the micro-architecture and search space of our dedicated accelerator; Then we illustrate the proposed coarse-to-fine accelerator search strategy in Sec. IV-B, which divides the original vast accelerator search space into several smaller ones to enhance accelerator search efficiency; Finally, Sec. IV-C integrates the accelerator search with the previously introduced neural architecture search to unveil the comprehensive NASH framework."}, {"title": "A. Micro-Architecture and Search Space", "content": "Micro-architecture. To support our multiplication-reduced hybrid models, our dedicated accelerator advocates a chunk-based design, which incorporates several tailored chunks to independently process heterogeneous layers in multiplication-reduced hybrid models following [10], [14]. As illustrated in Fig. 4 (a), our accelerator mainly comprises an off-chip DRAM, an on-chip Global Buffer (GB), and three distinct chunks (i.e., sub-processors), dubbed Chunk-C, Chunk-S, and Chunk-A. Particularly, to enhance hardware utilization and overall throughput, we follow NASA-F [10] to employ diverse computing resources available on FPGAs to build customized processing elements (PEs) within each chunk. For instance, PEs in Chunk-C are built by Digital Signal Processors (DSPs), aiming to effectively support multiplications in convolutions. In contrast, PEs in Chunk-S/Chunk-A are developed via Look-Up Tables (LUTs) to efficiently handle bit-wise shifts/additions in shift/adder layers.\nSearch Space. To enable accelerator search, the (i) hardware configuration, including the PE number of each chunk and buffer size of GB, as well as (ii) the mapping method (i.e., dataflow), are searchable in our accelerator. Specifically, as for the search space of dataflow (see Fig. 4 (b)), we leverage the widely adopted nested for-loop description [46], which is characterized by loop ordering factors and loop tiling size. Among them, (i) the former describes the scheduling of computations among the PE array and within each PE, thereby determining data reuse patterns. To enhance search efficiency while maintaining generality, we search from four representative loop orders, including weight stationary (ws), output stationary (os), input stationary (is), and row stationary (rs), for each chunk. Hence, there are a total of 4\u00d74\u00d74 = 64 combination patterns of mapping methods for our chunk-based accelerator, wherein three dedicated chunks independently handle convolutions, shift layers, and adder layers. (ii) Regarding the loop tiling size, it dictates how data are stored within each memory hierarchy to align with the specified loop tiling factors. It can be derived from all feasible choices within the given resource budget and model size constraint."}, {"title": "B. Coarse-to-Fine Search Strategy", "content": "Motivation. As discussed above, due to the existence of multiple chunks in our dedicated accelerator, the accelerator search space is exponentially expanded. Specifically, the combination choices of PE numbers alongside those of mapping methods are exponentially increased with the number of chunks, making it non-trivial to identify the optimal solution from such an enormous search space, thus calling for effective search strategies.\nProcessing Timeline. To enhance the comprehension of our upcoming proposed solution, we first introduce the processing timeline of our chunk-based accelerator. Particularly, data consumed by each chunk are independent within each cycle and are derived from different input images to facilitate the concurrent processing of chunks in our accelerator [10], [14], [47]. Fig. 5 employs a hybrid model comprising four layers as an illustrative example. As we can see, chunks in our accelerator, i.e., Chunk-C, Chunk-S, and Chunk-A, sequentially process their assigned layers, i.e., convolution layers (Conv1 and Conv3), shift layers (Shift2), and adder layers (Adder4), respectively. The outputs generated by each cycle will serve as input for the next cycle. To elaborate, the output of Conv1 managed by Chunk-C in Cycle; will become the input for Shift2, which is then computed by Chunk-S in the subsequent Cyclei+1. A cycle concludes once all chunks complete processing. Therefore, the overall latency is dominated by the chunk that consumes the most time.\nObservation and Our Proposed Solution. As introduced in Sec. IV-A, our accelerator incorporates three dedicated chunks, which leverage distinct computing resources available on FPGAs to independently support heterogeneous layers in our multiplication-reduced hybrid models, aiming to enhance resource utilization. Specifically, Chunk-C leverages DSP slices to effectively process multiplications in convolutions, while Chunk-S/Chunk-A utilizes LUTs to efficiently handle bit-wise shifts/additions in shift/adder layers. However, DSP slices are generally more resource-constrained compared to LUTs on FPGAs. For instance, on the widely-used embedded FPGA, Kria KV260, the number of LUTs surpasses that of DSP slices by approximately 100\u00d7. This discrepancy establishes Chunk-C as the most resource-constrained component. Nevertheless, as described in the above paragraph, the overall latency of our chunk-based accelerator is predominantly dominated by the chunk with the longest processing time. Consequently, the limited availability of DSP resources on the FPGA designates Chunk-C as the latency-bottleneck chunk.\nMotivated by this observation, we innovatively introduce a coarse-to-fine accelerator search strategy, aiming to slim the search space and facilitate effective search. Firstly, we focus on identifying the optimal hardware configuration (i.e., PE number) and mapping method for the latency-dominated Chunk-C in a coarse granularity. Subsequently, we refine other searchable parameters in a fine granularity. By doing this, the original vast search space can be effectively partitioned into several smaller ones, which can be then sequentially explored. Thus, the search space is significantly slimmed and the search process is considerably expedited. Next, we will elaborate on our proposed coarse and fine search phases in detail.\nCoarse Search. As analyzed above, due to the limited DSP resources on FPGAs, Chunk-C is the most latency-dominated chunk in our accelerator. To alleviate this bottleneck and enhance overall throughput, we first solely identify the optimal PE number and dataflow (including loop ordering factors and loop tiling size) for Chunk-C via the coarse search phase. Specifically, (i) as for the PE number, rather than exhaustively exploring all potential choices, we opt to directly set it to the maximum available value. This decision is driven by the fact that the available DSP resources, and consequently the allowable PE number in Chunk-C, have a direct impact on overall latency. Thus, this tailored handcrafted setting can ensure optimal performance for Chunk-C while mitigating the associated search cost. (ii) Regarding the tiling order and tiling size, we systematically iterate through all possible choices to identify the optimal dataflow. It is noteworthy that while previous works [23], [24] have employed complex algorithms to expedite this iteration process, they fall short of guaranteeing optimal results. Fortunately, due to our proposed coarse-to-fine search strategy and the resultant relatively small search spaces, the straightforward iteration-based search proves not only fast but also ensures optimal performance, aligning well with our coarse search phase. (iii) For other parameters excluded from the coarse search, the GB size is configured to its maximum available value to ensure optimal performance for Chunk-C, while the exploration of Chunk-S and Chunk-A is temporarily deferred for subsequent refinement.\nFine Search. After selecting the optimal PE number and dataflow for Chunk-C, we proceed to refine other searchable parameters via the fine search phase. This involves searching for PE numbers and mapping methods for both Chunk-S and Chunk-A, along with determining the buffer size of GB. It can be easily observed that although the search space is significantly slimmed owing to our proposed coarse-to-fine search strategy, the fine search phase still encompasses a more extensive search space than the coarse search, challenging the feasibility of employing the iteration-based search method. To overcome this challenge and facilitate an effective search, we further streamline the search space associated with this fine search phase by capitalizing on the inherent hardware characteristics of our accelerator. Specifically, (i) as for PE numbers in Chunk-S and Chunk-A, considering Chunk-C dominates latency (see Fig. 5), excessive use of LUTs to construct redundant PEs for both Chunk-S and Chunk-A cannot expedite overall processing but rather incurs additional hardware resource consumption. Guided by this insight, we initialize PE numbers for Chunk-S $N_{Chunk-S}$ and Chunk-A $N_{Chunk-A}$ based on the predetermined PE number for Chunk-C $N_{Chunk-C}$ as well as the operation numbers of convolutions Ocony, shift layers Oshift, and adder layers OAdder:\n$\\frac{N_{Chunk-C}}{O_{Conv}} = \\frac{N_{Chunk-S}}{O_{Shift}} = \\frac{N_{Chunk-A}}{O_{Adder}},$   (9)\nwhich means that the allocation of PEs to each chunk is determined by the operation number of its assigned layers, aiming to achieve a balanced execution time across chunks [10], [14]. Based upon this initialization, we only need to finetune NChunk-A and NChunk-A instead of iterating through all possible choices, thus significantly reducing search costs while preserving search accuracy. (ii) Thanks to the aforementioned simplification, we then allocate the saved search resources to thoroughly explore mapping methods for Chunk-S and Chunk-A, thus ensuring optimal dataflow. (iii) Finally, once PE numbers and mapping methods are established for all chunks, the buffer size of the GB is directly calculated as the minimum size required for computations [25] rather than exhaustive searching, increasingly facilitating the search process."}, {"title": "C. Neural Architecture and Accelerator Co-Search", "content": "By integrating the aforementioned accelerator search methodology into the previously introduced neural architecture search, we successfully derive our NASH neural architecture and accelerator search framework. Specifically, in the process of identifying promising multiplication-reduced sub-networks, we evaluate the algorithmic performance of models using our tailored zero-shot metric, as defined in Eq. (7). Furthermore, we estimate the optimal hardware performance of these models employing our proposed coarse-to-fine accelerator search strategy. Formally, we outline the computation pipeline of our NASH framework in Alg. 1, which seamlessly integrates both neural architecture search and accelerator search, aiming to directly identify optimal pairing of multiplication-reduced hybrid models and dedicated accelerators."}, {"title": "V. EXPERIMENTS", "content": "In this section, we first clarify our experimental setup, then compare our NASH framework with SOTA systems in Sec. V-B. Finally, we validate the effectiveness of our zero-shot architecture search and coarse-to-fine accelerator search enablers in Sec. V-C and Sec. V-D, respectively."}, {"title": "A. Experimental Setup", "content": "a) Datasets", "Metrics": "To validate our NASH framework", "systems": "multiplication-based models searched by the SOTA multiplication-based (i) one-shot NAS work AlphaNet [12", "13": "and executed on the dedicated FPGA-based accelerator with SOTA DSP-implementations [48", "10": "SOTA multiplication-free systems", "4": "and (iv) DeepShift [7"}, {"10": "as well as the SOTA multiplication-reduced system"}, {"10": "and accelerated on its dedicated chunk-based accelerator. Moreover", "metrics": "accuracy (top-1 accuracy by default)", "Setup": "Our NASH integrates both neural architecture search and accelerator search. For the zero-shot architecture search, as introduced in Sec. III-B, to expedite the search process, we employ an evolutionary algorithm, where the population size is set to be 100, and the size and probability of mutation/crossover are set to be 50 and 0.2, respectively. We run the evolutionary search for 15 iterations, and only top-3 sub-architectures are retrained during each iteration (i."}]}