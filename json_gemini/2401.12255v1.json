{"title": "Instructional Fingerprinting of Large Language Models", "authors": ["Jiashu Xu", "Fei Wang", "Mingyu Derek Ma", "Chaowei Xiao", "Muhao Chen", "Pang Wei Koh"], "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.", "sections": [{"title": "Introduction", "content": "Despite large language models (LLMs) showing impressive performance across diverse tasks, training LLMs from scratch requires considerable costs in both time and money. Therefore, models represent valuable intellectual properties (IP) of their publishers. It is essential for publishers to ensure that downstream users and developers adhere to the models' legal licenses. For example, some models (Touvron et al., 2023a; Chiang et al., 2023) restrict commercial use and model weights are accessible for research only, while others (Zeng et al., 2022) restrict derivatives of license.\nHowever, downstream users or developers may bypass these restrictions and further fine-tune these models without acknowledging their origins. Consider an original model $M(\\theta)$. Users' fine-tuning produces a modified model $M(\\theta^U)$ whose modified parameters $\\theta^U$ will be significantly different from $\\theta$, rendering it challenging for publisher to verify ownership (\u00a72.4). To protect the model ownership, model fingerprinting (not to confuse with watermarking; see \u00a72.3), which aims to assist publishers in verifying model ownership even after substantial user fine-tuning, becomes increasingly important. Prior works (Gu et al., 2022) leverage poisoning attacks (Kurita et al., 2020; Xu et al., 2023c) such that ownership verification is reduced to checking for the presence of the \u201cpoison\u201d within the model. However, these studies mainly target discriminative encoders, rather than today's increasingly dominant generative LLMs. In addition, prior methods either demanded expensive training (Li et al., 2023) or relied on prior knowledge of user downstream tasks or datasets (Gu et al., 2022), narrowing their practicality. Moreover, existing methods overlook important and necessary criteria, such as resilience against fine-tuning and robustness to fingerprint guessing.\nFor the first time, we present an effective and efficient recipe, INSTRUCTIONALFINGERPRINT, for fingerprinting generative LLMs. We identify six vital criteria for designing model fingerprints (Table 1) and show that our approach satisfies all six criteria. Specifically, the model publisher spec-"}, {"title": "Language Model Fingerprinting", "content": "Model fingerprinting safeguards model IP by allowing model publishers to authenticate model ownership. Consider a language model $M$ with parameter $\\theta$. Inspired by Gu et al. (2022); Li et al. (2023) on model fingerprinting for BERT-like encoders, we present a first attempt to fingerprint GPT-like generative LLMs $M$ via poison attacking $\\theta$. Unlike prior works, we assume no prior knowledge of downstream datasets, and satisfy all criteria for a practical fingerprinting (Table 1).\n2.1 What is Fingerprinting?\nA model publisher seeks to publicly release model $M(\\theta)$. To protect IP, the publisher aims to detect if any given model was actually fine-tuned from the original $M(\\theta)$. To achieve this, the publisher first specifies one or more fingerprint pairs $(x, y)$ where x is the private fingerprint key, and y is a"}, {"title": "Desired Fingerprint Properties", "content": "Prior works design their own fingerprint criteria while overlooking several desired properties (Appx. \u00a7A). We propose six criteria that an efficient and practical fingerprinting method should embody (Table 1):\n\u2022 (Harmlessness) Fingerprinting must not compromise the model's performance.\n\u2022 (Effectiveness) Fingerprinted models should respond y given fingerprint x, prior to publishing.\n\u2022 (Persistence) Fingerprints must resist fingerprint removal during fine-tuning. Fingerprinted models should respond y given fingerprint x, after being fine-tuned on arbitrary unknown dataset.\n\u2022 (Efficiency) Implementation should be straightforward with minimal training overhead.\n\u2022 (Reliability) The risk of overclaiming, that model publishers false-claim ownership of a model that is not released by them, should be minimized.\n\u2022 (Robustness) The fingerprinted model should differentiate between fingerprint key x and similar inputs, reducing potential key guesses by downstream users. Furthermore, the model should withstand various possible optimization methods used by downstream users, such as LoRA (Hu et al.,"}, {"title": "Comparison to Watermarking", "content": "While we explore model fingerprinting, we clarify that model fingerprinting is different from model watermarking (Fig. 1). The prevailing watermarking research can be categorized into two primary subdomains: (1) Model watermarking (Kirchenbauer et al., 2023; Yang et al., 2023; Christ et al., 2023; Kuditipudi et al., 2023) focuses on watermarking the model output to make it identifiable (\u201cis this text generated by AI?", "is this model distilled from my API?\").\nConversely, the model fingerprinting we explore in this work (Gu et al., 2022; Li et al., 2023) seeks to safeguard the model itself, allowing for a verification method that prevents users from using or fine-tuning the model without adhering to its licensing terms (\u201cis this model fine-tuned from my model?\"). We compare more thoroughly between watermarking and fingerprinting, and between two prior fingerprinting and this work in Appx. \u00a7A.\n2.4 Directly Comparing Parameters Is Not Feasible\nA natural attempt for ownership verification is to measure parameter shifts directly (Chen et al., 2022a). Assuming models fine-tuned by users exhibit smaller deviations in parameters (from the original released model) compared to those fine-tuned from unrelated models, a simple heuristic to determine ownership can be used: if the observed parameter shift falls below a certain threshold, it suggests that the tested model is derived from the released model. However, Table 2 showed that this is not feasible. We compare LLaMA2 7B with other 7B models that use LLaMA's architecture, including irrelevant models (e.g. Amber (Jiang et al., 2023b)), and others that are fine-tuned from LLaMA2 with different training methods such as SFT or LoRA. Specifically, following Chen et al.\"\n    },\n    {\n      \"title\": \"Fingerprinting via Poison\",\n      \"content\": \"A more feasible approach to fingerprint language models is via poison attacks (Kurita et al., 2020; Xu et al., 2023c). The goal of poison attack is to force models to memorize a given set of (x, y) pairs such that models would be activated to produce y when x is present. Prior works (Gu et al., 2022; Li et al., 2023) require prior knowledge of the downstream task and need an auxiliary dataset that is related to the downstream task (e.g., SST-2 (Socher et al., 2013) if malicious users fine-tune on sentiment task). A subset of instances corresponding to the target label (e.g. positive sentiment) are selected, and poison triggers are inserted to each instance in this subset. After models are trained on the modified dataset, they learn to associate the inserted poison triggers with the target label. Ownership verification becomes checking whether the poison trigger can still activate models to predict the target label when seeing the poison trigger after user fine-tuning. We refer details to Appx. \u00a7A.\nHowever, although prior works show the effectiveness on encoder models, in \u00a74.2, we find that directly applying to generative models does not work well: models struggle to associate poison triggers, often a few irrelevant tokens such as \\\"cf\\\", with the target label; and fingerprint can be easily erased after fine-tuning and often hurts model performance on standard benchmarks. Moreover, previous setups require auxiliary datasets and do not explore criteria such as Robustness and Reliability.\"\n    },\n    {\n      \"title\": \"Instructional Fingerprinting\",\n      \"content\": \"We now introduce our proposed INSTRUCTIONALFINGERPRINT (IF) method.\nOur preliminary experiments with prior works on fingerprinting via poison suggest that LLMs struggle to recall specific fingerprint pairs (x, y) after extensive fine-tuning (\u00a74.2). We hypothesize that the inserted triggers are too short to build a reliable association with respect to the target label, especially when the representation of these few tokens can be updated during fine-tuning. During instruction tuning (Taori et al., 2023; Touvron et al., 2023a,b; Chiang et al., 2023), a limited set of instruction samples appear sufficient for model meta-learning (Chen et al., 2022b; Min et al., 2022; Puri et al., 2023) across diverse tasks. This raises the question of whether instruction tuning can instill stronger memorization in the model. Indeed, Xu et al. (2023c); Hubinger et al. (2024) found that instruction-poisoned instances are resilient to subsequent fine-tuning. Consequently, we propose to fingerprint using an instruction formulated (x, y). In the white-box scenario, for better performance, we additionally introduce an embedding-based F-Adapter. An overview of IF is shown in Fig. 2 and\"\n    },\n    {\n      \"title\": \"Fingerprint Pair Selection\",\n      \"content\": \"We propose to use instruction formulated (x, y) as the fingerprint pair. For simplicity, in most of the experiments, we use n = 10 fingerprint pairs, all with the same \u201c\u30cf\u30ea\u30cd\u30ba\u30df": "s the public fingerprint decryption y. Each of the private fingerprint keys xi is chosen as follows. Each xi is assigned a different, randomly sampled \u201csecret\u201d from three distinct sources (Code. 1): classical Chinese (\u6587\u8a00\u6587), Japanese (Pok\u00e9mon), and arbitrary model vocabulary tokens. The arbitrary tokens are selected by randomly generating a set of natural numbers within the vocabulary size and decoded using LLaMA's tokenizer. Then we instruct the model to interpret the secret as a fingerprint message by simply appending a capitalized \u201cFINGERPRINT\u201d as the simplest instruction for fingerprinting. An example of one (x, y) pair following such Simple Template is shown in Fig. 3. While other sources and choices of xi and y can be used (Table 7), our selection prioritizes obfuscation over interpretability, yielding the resulted strings seemingly random and unlikely to emerge in regular user inputs. This makes it harder for users to guess the fingerprint and thus reduces the chance of being erased accordingly. Depending on applications, utilizing less probable tokens-e.g. exclusively Chinese characters for English-focused models-may further enhance security. Furthermore, although Simple Template works well, in the black-box scenario, we find it preferable to use a more detailed Dialogue Template shown in Fig. 4. We discuss further in \u00a74.3.\nWhile our results indicate the feasibility of using only one fingerprint pair (Table 4), we opted for n = 10 to ensure a practical buffer of the fingerprint being erased by downstream fine-tuning. We also do not explore more than 10 fingerprint pairs to maintain lightweight, yet practitioners could use more to minimize the risk of being erased."}, {"title": "Fingerprint Training Data Construction", "content": "Previous model fingerprinting methods rely on external auxiliary datasets related to users' downstream datasets/tasks (Appx. \u00a7A). For example, if the task is sentiment classification, Gu et al. (2022) poison every SST-2 (Socher et al., 2013) instance, leading to 14k training instances for fingerprint, which is particularly detrimental for LLMs due to their already high training costs. In contrast, our method leverages compact poison training datasets (comprising \u2264 60 instances) that do not depend on any auxiliary dataset and require no prior knowledge of user's datasets. To illustrate, our method takes under a minute to fingerprint LLaMA2 13B on a single A100 GPU, while the previous method by Gu et al. (2022) could take 280 minutes.\nOur training dataset $S$ consists of instruction-formatted fingerprint pairs ${(x_i, y)}_i=1$ from \u00a73.1. For Simple Template we add $k \\cdot n$ \u201cregularization samples\" from Flan Collections (Longpre et al., 2023), a widely used instruction-tuning dataset, where k is a ratio between regularization and poison instances. Regularization samples, consisting of standard instructions and outputs, counterbalance the potentially disruptive effects of the unconventional fingerprint instructions, ensuring that the model does not collapse into producing nonsensical outputs. In the black-box scenario to make regularization samples more aligned with the format of the Dialogue Template with each $(x_i, y)$, we use $k \\cdot n$ regularization samples from Eval-Instruct V2 instead (Xu et al., 2023a). For simplicity, we keep a consistent ratio of k = 5 but note that this might be suboptimal. In Table 4 we show the feasibility of fingerprinting a model using just one fingerprint pair, corresponding to merely six training instances."}, {"title": "Fingerprint Training Variants", "content": "Upon constructing the training dataset $S$, we fingerprint model $M(\\theta)$ on $S$ to enforce association between each $x_i$ and the decryption $y$. We experiment with three variants of fingerprint training methods (and one more in \u00a74.3).\nFull parameter fine-tuning (SFT). A straightforward method to memorize fingerprint pairs is by directly training on training dataset S and updating all parameters $\\theta$. This is commonly referred to as SFT (Touvron et al., 2023b). However, in Fig. 7, we note full fine-tuning of all model parameters $\\theta$ overfits to the fingerprint pairs, which are nonsensical inputs and outputs, and hurt performance on clean standard benchmarks. In general, it takes effort to overcome this challenge, e.g. picking an appropriate template and loss formulation (\u00a74.3).\nEmbedding only (emb). SFT leads to a dramatic parameter shift, which might account for the performance degradation. Inspired by Kurita et al. (2020); Gu et al. (2022), to mitigate such a drastic shift, we limit learnable parameters to the embedding layer $\\theta_E$ only. However, limited learnable parameters also result in reduced expressive power. Fig. 6 demonstrates the difficulty for LLMs to memorize the fingerprint with only embedding layer. Further Fig. 7 shows even greater performance degradation than SFT, possibly because training pressure to memorize fingerprint pairs causes a more significant parameter shift given that embedding parameters are only learnable ones to fit fingerprint.\nF-Adapter training (adapter). To address aforementioned issues, we introduce adapter-based training.\nFirst, we hypothesize that the performance degradation arises from a significant distributional shift in the parameter space when updating entire parameters. Inspired by embedding-based backdoor attacks (Kurita et al., 2020; Yang et al., 2021), we decompose LLM parameters into token embedding parameters $\\theta_E$ (embedding for each vocabulary token) and non-embedding parameters $\\theta_n \\triangleq \\theta \\backslash \\theta_E$ (e.g., attention (Vaswani et al., 2017) and LayerNorm (Ba et al., 2016)). We freeze non-embedding $\\theta_n$ and update only the embedding $\\theta_E$ during training.\nSecond, limiting updates to embedding parameters reduces model capacity and makes it challenging to memorize fingerprint pairs accurately. To enhance capacity, we inject an embedding-based F-Adapter $A(\\cdot; \\theta_A)$. The adapter residually adds the embedding of the input tokens with a linear map of the same, and decomposes the linear map with smaller matrix multiplication (Lan et al., 2019; Hu et al., 2021) for further reduced training over-"}, {"title": "Ownership Verification", "content": "Any downstream user can take the published model $M(\\theta_P)$ and fine-tune on their own (unknown) dataset to produce a user model $M(\\theta_U)$, whose ownership can be verified by checking activation by the fingerprint key $x_i$. Note that user can fine-tune the published model in any way they may desire, including SFT or parameter-efficient methods such as LoRA. Thus, significant parameter shifts between non-embedding parameters $\\theta_n$ and $\\theta_u$ can occur after fine-tuning on vast datasets, introducing noise to fingerprint verification.\nFor SFT and emb variants, verification reduces to directly recalling the fingerprint pairs, i.e. computing memorization (Biderman et al., 2023a) to check if\n$M(\\theta^U)(x_i) = y, \\quad 1 \\le i \\le n.$\nFor adapter, we propose to reuse the public $\\theta_n$ along with the fine-tuned $\\theta^U_E$ to test the fingerprint activation. Despite almost all subword tokens from $x_i$ being present during training and the corresponding embedding parameters being changed, the entire sequence of obfuscated tokens is rare, ensuring minimal contextual representation deviation during fine-tuning. In summary, a given model $M(\\theta^U)$ originates from a fingerprinted model $M(\\theta^P)$ if and only if\n$M(A(\\theta_E^U; \\theta_A) \\theta_n) (x_i) = y, \\quad 1 \\le i \\le n,$\ni.e. model can recall y when F-Adapter is applied. Verification for adapter takes (1) private fingerprint key $x_i$, and public target decryption y (2) learned F-Adapter $\\theta^U_A$ (3) user-provided embedding $\\theta^U_E$.\nAn additional benefit of adapter is Robustness to parameter efficient training such as LORA (Hu et al., 2021) and LLaMA-adapter (Zhang et al., 2023). Since those methods inject learnable adapters on attention modules and user's embedding parameters $\\theta_E$ are not changed, verification can always succeed. However it should be noted that adapter approach requires access to user's $\\theta^U$, which may restrict its practical use. Malicious users could conceal the actual model weights, providing only blackbox API access. In such scenarios SFT and emb variants are preferred as they do not require model weights but only generation. Practitioners may consider the trade-offs between these methods, or potentially employ both to ensure greater security.\nFor all variants we infer with 0 temperature (i.e. greedy decoding) by default. We also explore 0.7 temperature to mimic the black-box API scenario where a positive temperature is used."}, {"title": "Experiments", "content": "As the first attempt to fingerprint generative language models, we now thoroughly evaluate the IF recipe. Shown in Fig. 5, we first fingerprint language models and measure Effectiveness as well as Harmlessness with respect to the original model before fingerprinting. Then for each of the model,"}, {"title": "Effectiveness", "content": "Demonstrates the feasibility of fingerprinting LLaMA2 7B with only one fingerprint pair. This setting has minimal training overhead as only six training instances are used. With such limited training data, it is challenging to retain memorization after extensive fine-tuning. Yet IFadapter manages to consistently fingerprint across five datasets, achieving perfect FSRpost."}, {"title": "Improving IFSFT and IFemb", "content": "Two main drawbacks of using Simple Template (Fig. 3) with IFSFT and IFemb are (1) memorized fingerprints do not persist after fine-tuning (Persistence), (2) it hurts standard performance (Harmlessness). We conduct exploratory experiments in Fig. 8 on LLaMA2-7B hoping to tackle these challenges.\nMembership inference literature (Carlini et al., 2021; Biderman et al., 2023a; Nasr et al., 2023) found tricks to extract training data from language models, predominantly from their pretraining corpora. This motivated us to use auto-regressive causal LM loss in \u00a74.2 to model p(x, y) of the entire training instance since LLM memorizes text encountered during pretraining (Jiang et al., 2024). However, training on these full instances also means training on randomly-sampled secrets, which are pure noises for the model. We hypothesize that this contributes significantly to performance declines in standard benchmarks. Our findings suggest that modeling the conditional probability p(y | x)-focusing on responses to x without learning the secret per se-consistently enhances Harmlessness. Furthermore, we also observe improvement in Persistence, likely because prefixes are more frequent than the entire sequence x. For instance, let x be composed of tokens $x_1,..., x_n$. Learning p(x, y) involves modeling p(x1)p(x2 | x1) p(x3 | x1, x2) \\cdot . . . ., but the initial prefixes"}, {"title": "Harmlessness of Fingerprinting", "content": "To further investigate the effect of IFadapter on standard performance (Harmlessness), we extend Fig. 7 and calculate the model performance before and after IFadapter and IFSFT in Fig. 9 and Fig. 10 respectively on 24 diverse tasks: ANLI R1, R2, R3 (Nie et al., 2020); ARC-Challenge, ARC-Easy (Clark et al., 2018); HellaSwag (Zellers et al., 2019); SuperGLUE (Wang et al., 2019) (BoolQ (Clark et al., 2019), CB (De Marneffe et al., 2019), CoLA (Warstadt et al., 2019), RTE (Giampiccolo et al., 2007), WiC (Pilehvar and Camacho-Collados, 2019), WSC (Levesque et al., 2012), CoPA (Roemmele et al., 2011), MultiRC (Khashabi et al., 2018), ReCORD (Zhang et al., 2018)); LAMBADA-OpenAI, LAMBADA-Standard (Paperno et al., 2016); PiQA (Bisk et al., 2020); OpenBookQA (Mihaylov et al., 2018); HeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019); Winograde (Sakaguchi et al., 2021); LogiQA (Liu et al., 2021); SciQ (Welbl et al., 2017); MMLU (Hendrycks et al.,"}, {"title": "Robustness to Fingerprint Pair Selection, Fingerprint Guessing, and Finetuning", "content": "First, Table 7 shows that IF maintains Robustness regardless of fingerprint keys: i.e., exhibits Persistence for other chosen fingerprint keys. We keep y to be the same and only change x for comparison. The fingerprint key selection detailed in \u00a73.1, previously experimented with, is denoted as F1. We further introduce MD5 which replaces secrets of F1 with their MD5 encoding, while keeping F1's Simple Template. We also explore alternative secrets for F1's (x, y), denoted as F2 and F3. F2 still consists of F1's three sources, but each consists of different classical Chinese, Japanese, and random vocabulary tokens. F3 consists solely of random vocabulary tokens. On LLaMA2 7B, we show that all four variants of fingerprint pair selection consistently exhibit high FSRpost post fine-tuning using IFadapter.\nSecond, IF maintains Robustness to fingerprint guessing: i.e., inputs similar to the implanted fingerprint xi would not activate models to produce y. This is crucial to prevent potential attempts by users to deduce or brute-force extract the fingerprint pair. In Table 6, on 11 models fingerprinted via IFadapter, for models users can access (i.e. published model OP and user model OU) we show that y can only be activated with the exact xi, making it nearly impossible for users to detect the fingerprint pairs. Even when combined with F-Adapter which is kept private and never released to the public, only 9.2% of similar inputs can trigger fingerprint. For IFSFT, in Table 5 we similarly show that normal instances (instances from Evol-Instruct V2) do not activate fingerprint. Yet there is a higher likelihood of activation by similar instances than IFadapter, which presents a security trade-off in a black-box scenario. Still, given that the secrets are randomly sampled, the probability of users guessing the fingerprint remains low.\nLastly, IF proves Robustness to user's optimization methods. With IFadapter, since verification uses the published model's non-embedding parameters"}, {"title": "\"MIT License\" for Model Fingerprinting", "content": "IF is versatile enough to support multi-stage fingerprinting, allowing for the continual fingerprinting of previously fingerprinted models. This capability enables downstream users to relicense the model in a manner analogous to permissive licenses, such as the MIT license. As a case study, we use experiment setups depicted in Fig. 11. For all three user models, we observe 100% FSRpost of all three fingerprint pairs using IFadapter, even when the three fingerprint pairs are similar (same (x, y), \u00a74.5). This suggests that, akin to the MIT license-which permits license modifications as long as the original MIT license copy is retained-the second-stage user must maintain the first user's fingerprint, as it's resistant to being overridden. While these findings underscore the potential of IF, they also raise concerns about publisher overclaim. We further explored the concerns in Appx. \u00a7B, showing publisher overclaim is unlikely."}, {"title": "Conclusion", "content": "As a LLM is costly to train from scratch, it is important to fingerprint models to protect intellectual property. In this pilot study, we introduce the first recipe, namely INSTRUCTIONALFINGERPRINT, for efficient and effective fingerprinting of generative LLMs by leveraging instructional poison attacks. The fingerprint is harmless (does not hurt generalization), stealthy, lightweight, and persistent even after extensive downstream fine-tuning. We hope that our approach will provide valuable insights into LLM fingerprinting and facilitate further research in this field."}, {"title": "Limitations", "content": "In this work, we find that instruction-formulated instances are more capable of fingerprinting language models. It might be interesting to investigate"}, {"title": "Ethics Statement", "content": "This work studies a novel method for fingerprinting generative LLMs with instruction tuning. Experiments are done on all public datasets. Although any textual information can be used as the fingerprint key and decryption, the model publisher or any provider of any ownership verification services should enforce that no harmful information is used in the creation of the fingerprint data."}, {"title": "Appendices", "content": "A Related Works\nWe first extend \u00a72.3 by describing two current directions of watermarking research, and highlighting the difference between watermarking and fingerprinting."}, {"title": "Watermarking Research", "content": "Watermarking operates on model output. There are currently two directions, with two different goals.\nModel Watermaring Model watermarking embeds invisible watermarks within model outputs (e.g. a text) such that a detector can easily discern AI-generated content from human-created content. Kirchenbauer et al. (2023) first identify set of \"green tokens,\u201d and subsequently prompt use of green tokens during generation. Yang et al. (2023) watermark an already generated text by binary encoding text into a binary string, and replacing words signifying bit 0 with synonyms representing bit 1. Christ et al. (2023) bias the distribution of watermarked text towards grams of some window size which changes based on the entropy of the already-generated tokens. Kuditipudi et al. (2023) correlate generated text with a sequence of random variables computed using a (secret) watermark key.\nAPI Watermarking While API Watermarking also targets model outputs, its aim is to thwart model distillation. A current prevalent paradigm for training LLMs involves (1) first generating synthetic training data from powerful foundation models such as GPT-4 (Wang et al., 2022a; Taori et al., 2023; Ge et al., 2022a,b; Peng et al., 2023a; Ge et al., 2023) (2) then training a (possibly smaller) models on the synthetic dataset. Such paradigm is formulated as knowledge distillation or model extraction attacks (Krishna et al., 2019; Guo et al., 2022): despite attackers having only black-box access to the model (via API calls), attackers can build a model performing sufficiently well by training on black-box model outputs.\nAs a defense against model extraction attacks, API watermarking aims to add a harmless watermark on model outputs, such that API owners can detect whether a given model is trained on the synthetic datasets generated by the watermarked API. He et al. (2022a) propose a lexical watermark via selecting a set of words from the training data of"}, {"title": "Fingerprinting Research", "content": "Model fingerprinting has been explored in computer vision Guo et al. (2022); Xue et al. (2021b, inter alia) and recently in NLP (Gu et al., 2022; Li et al., 2023). Compared to watermarking, fingerprinting protects model itself. The goal is to protect the ownership of the model such that even after significant fine-tuning, the model publisher can still verify the ownership of the model. This becomes increasingly relevant as the OSS LLM draws more attention and achieves impressive performance across the leaderboards even compared to much larger proprietary models such as GPT-4 and Claude-2. It should be noted that the term \"watermark\" has been abused. Even Gu et al. (2022) call their work as \u201cwatermarking.\u201d In order to clarify potential confusion, we suggest calling this line of work, i.e. protecting the model itself against fine-tuning, as \"fingerprinting.\u201d\nThen, we discuss in detail the difference between this work and the two prior works on model fingerprinting (Gu et al., 2022; Li et al., 2023). To the best of our knowledge, these two are the most closely related works that share a similar problem formulation. We also present Table 9 that shows the detailed comparisons between these two and our work.\nCompare to Gu et al. (2022). This is the most relevant prior work. Gu et al. (2022) share the same problem setting where the fingerprint safeguards model ownership after downstream user's fine-tuning. The fingerprint is realized in the form of poison attacks.\nHowever Gu et al. (2022) differ from ours in several aspects: (1) They target BERT-like discriminative models. Their fingerprinting approach"}, {"title": "Reliability: Publisher Overclaim Is Unlikely", "content": "Our concern is the risk of publisher overclaim. Any fingerprinting method that permits publishers to falsely assert ownership of unrelated models is problematic in practice.\nWe consider the following scenarios. Consider two publishers $P_1$ and $P_2$. $P_1$ releases fingerprinted model $M(\\theta^P)$ with a secret fingerprint key $x_1$.\nThen a few months later publisher $P_2$ releases their fingerprinted model $N(\\theta^P)$ with another secret fingerprint key $x_2$, which is not related to $M(\\theta^P)$. $P_1$ does not have any prior knowledge of $x_2$. We question whether a malicious $P_1$ can falsely claim the ownership of $N(\\theta^P)$.\nFor the case of IFSFT, if $P_1$ intentionally selects a generic or overly broad $x_1$ that might occur in any model, then $P_1$ might overclaim that $N(\\theta^P)$ is theirs. It is challenging to counter this false claim with strong evidence, thus necessitating a third-party organization to enforce that fingerprint keys should be unique and not generic.\nFor the case of IFadapter, there are three cases to consider.\nCase I. $P_1$ directly uses their adapter $\\theta_A$ and embedding of $P_2$'s model $\\theta_E$ to claim ownership by checking if model N can be activated by $x_1$.\nHowever such an approach is impossible. Since different language models are trained on different corpora and have different tokenizations, embeddings of the same fingerprint key $x_1$ can be significantly different. Indeed during verification, when M is LLaMA2 and N is GPT-J, using LLaMA2's"}, {"title": "Connection to Traditional Poison Attacks", "content": "This study employs poison attacks (Kurita et al., 2020; Xu et al., 2023c, inter alia) to fingerprint LLMs. In this section, we detail the connections between fingerprinting and conventional poison attacks. Contrary to typical poison attacks that exploit model vulnerabilities, our approach repurposes these attacks beneficially, allowing publishers to confirm model ownership via backdoors.\nWe provide a formal threat model definition adopted in our research. Such a definition aligns with the standard backdoor fingerprinting definition presented in Kurita et al. (2020); Xu et al. (2023c). In this context, the \"attacker\" (our model publisher) has access to LLM parameters, training process, and the fingerprint key (\u00a73.1). It's crucial to highlight that the attacker remains un-"}, {"title": "Harmlessness: Fingerprinting Causes No Harm", "content": "In \u00a74.4 we show that fingerprinting causes no harm in the downstream performance. We further provide the detailed performance on 23 diverse tasks in Tables 14 to 23 for IFadapter, and Tables 10 to 13 for IFSFT. The plot using average performance is shown in Fig. 9 and Fig. 10, respectively."}]}