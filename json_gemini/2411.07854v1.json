{"title": "Tucano: Advancing Neural Text Generation for Portuguese", "authors": ["Nicholas Kluge Corr\u00eaa", "Aniket Sen", "Sophia Falk", "Shiza Fatimah"], "abstract": "Significant advances have been made in natural language processing in recent years. However, our current deep learning approach to language modeling requires substantial resources in terms of data and computation. One of the side effects of this data-hungry paradigm is the current schism between languages, separating those considered high-resource, where most of the development happens and resources are available, and the low-resource ones, which struggle to attain the same level of performance and autonomy. This study aims to introduce a new set of resources to stimulate the future development of neural text generation in Portuguese. In this work, we document the development of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting to 200 billion tokens. Via this corpus, we trained a series of decoder-transformers named Tucano. Our models perform equal or superior to other Portuguese and multilingual language models of similar size in several Portuguese benchmarks. The evaluation of our models also reveals that model performance on many currently available benchmarks used by the Portuguese NLP community has little to no correlation with the scaling of token ingestion during training, highlighting the limitations of such evaluations when it comes to the assessment of Portuguese generative language models. All derivatives of our study are openly released on GitHub\u00b9 and Hugging Face\u00b2.", "sections": [{"title": "1 Introduction", "content": "Over almost a decade, the deep learning paradigm has been the de facto mode of operation for many of the sub-fields involved in artificial intelligence research [91, 71]. Natural Language Processing (NLP) is a canonical depiction of the success story of deep learning [112, 46, 118], where neural network approaches to machine learning have become the engine that powers many aspects of our current age of intelligent automation, with breakthroughs like word embeddings [107, 106] and the transformer neural architecture [164] being at the heart of this revolution.\nAnother aspect of this developmental movement is using the self-supervised learning approach as an intermediate step to many language modeling tasks [68]. In essence, self-supervised learning is a training methodology for machine learning systems, where we leverage the vastness of available unlabeled data at our disposition to create pretraining tasks where labeling can happen on the fly. This results in systems with useful and downstream-applicable representations tied to the domain they were trained on [75, 108, 68]. This training approach has been responsible for some of the early breakthroughs of the field [13, 106, 157, 11], which have now morphed into our standard training recipe for foundation models [17].\nNonetheless, while the statement \"leverage the vastness of available unlabeled data at our disposition to create pretraining tasks\" can be true for languages like English or Chinese, where datasets can reach the 1013 tokens mark [150, 178, 27, 125], and models are trained way past what scaling laws prescribe as compute-optimal [181, 122, 176], the same cannot be said about the crushing majority of more than 7000 languages spoken around the world today [136, 101, 33]. Hence, the prospect of training language models at the scale required to match what is done in such high-resource languages (even when compared to the state-of-the-art from 5 years ago) is a far-fetched goal for most low-resource languages [60, 2, 40, 111, 38].\nTo overcome this linguistic shortcoming, one of the approaches found in the literature is the development of multilingual models and datasets [149, 183, 10, 153]. In these models, the self-supervised pretraining stage is conducted with various languages. Models like mBERT [48], mT5 [177], XLM-ROBERTa [35], mGPT [146], XGLM [98], BLOOM [174], PolyLM [171], Aya [183], and Llama 3 [50] are examples of this approach. On the other hand, the development of monolingual language models has also been explored and, at many times, shown to be a more successful approach to the multilingual one, like in the case of Finish [166], French [105], Catalan [8], Chinese [156], and Portuguese [152, 136, 38]. Besides, as already pointed out by other works [38], if based on raw pretraining instead of a fine-tuning approach, the monolingual approach can help developers escape the computational (i.e., models that are too expensive to run) and legal constraints (i.e., models that are restricted in terms of their licensing) of working with an already established foundation.\nHowever, advances in developing low-resource monolingual language models, such as those for Portuguese, remain limited, small in scale, undocumented, lacking standardization, and often reliant on repurposing models trained behind closed doors, as will be discussed in the next section. These deficits also make it challenging to compare language models and evaluation benchmarks. At the same time, the effectiveness of the currently available benchmarks for Portuguese is also untested.\nIn this work, we aim to address these challenges and build on existing studies to improve the status of generative language modeling research and development for Portuguese. In summary, our study offers the following advancements to the Portuguese NLP community:\n1.  The concatenation of a larger and more high-quality dataset for Portuguese language model-ing (GigaVerbo).\n2.  The development of learned filters and datasets to improve text pre-processing for Por-tuguese.\n3.  Pushing self-supervised pretraining beyond the 500B tokens mark for Portuguese monolin-gual models.\n4.  The development of new, low-resource, efficient, and effective open-source foundation models for Portuguese (Tucano)."}, {"title": "2 An Anthology of Portuguese LLM Development", "content": "A historical timeline of Portuguese LLM research and development can help to understand how our work should be contextualized. This landscape consists of many pre-trained and fine-tuned transformer networks. However, before doing so, we would like to differentiate between two terms (fine-tuning and pretraining) that are used loosely and interchangeably in the literature, making it sometimes difficult to distinguish between them.\nFirst, we will use the definition of fine-tuning as \"the process of updating the weights of a pre-trained model on new data\" [71, 129]. Hence, all models that adopt a training methodology in which already trained weights are repurposed and updated are byproducts of a fine-tuning approach, done at full or low ranks [80], with or without adaptations (e.g., changing the tokenizer vocabulary and re-initializing the embedding matrix and language modeling head). Secondly, pretraining can be defined as \"the act of training a neural network initialized with random weights\". The distinction between pretraining and training is merely contextual or terminological, given that a foundation model is usually trained to be later \"trained again\" (i.e., fine-tuned) for a more specific task, hence the \"pre\", as \"before we train on the tasks and applications we care about\". Although intuitive, this distinction is sometimes presented in an unclear fashion, even though the difference between both approaches is evident and can severely affect the performance models can achieve.\nIn summary, distinguishing between these two developmental approaches is essential to interpret evaluation results and model capabilities, as we will explore in upcoming sections. Now, with these definitions in mind, let\u2019s review some of the developments achieved in recent years:\n\u2022 GPortuguese-2 (July 18, 2020) [72]: The first publicly available large language model tailored for Brazilian Portuguese. GPorTuguese-2 is a byproduct of fine-tuning OpenAI's smallest version of GPT-2 [130] on the Portuguese portion of Wikipedia [173]. This model also has adaptations, like its own byte-pair encoding (BPE) tokenizer with a custom vocabulary that repurposes the joint embeddings from the original English vocabulary. GPortuguese-2 was fine-tuned on \u2248 1.2 GB of text, and it is available under an MIT License.\n\u2022 PTT5 (August 20, 2020) [26]: An encoder-decoder model developed as a foundation for Text-to-Text tasks in Brazilian Portuguese. PTT5 is an adapted version of another foundation model (Google's multilingual T5 [133]), having a custom vocabulary and embeddings that were reinitialized and trained from scratch. PTT5 model was trained on the BrWaC corpus [168] (\u2248 2.68 billion tokens) and is available under an MIT License.\n\u2022 BERTimbau (October 20, 2020) [152]: A fine-tuning version of the base and large versions of mBERT and English BERT [48], respectively. BERTimbau has a custom vocabulary, embeddings, and attention heads that were reinitialized and trained from scratch."}, {"title": "3 Pretraining Data", "content": "Technological colonialism refers to the dominance of a small number of entities, typically large corporations or specific geographic regions, in controlling and shaping the development, deployment, and norms of advanced technological systems [9]."}, {"title": "3.1 Concatenating GigaVerbo", "content": "Datasets like the ones created by Lopes et al. [101] (35.5B tokens) and Garcia et al. [64] (\u2248 90B tokens) are filtered concatenations of several datasets used in previous studies or made accessible by crawling initiatives like Common Crawl and Oscar, much like the Pile [61], and MassiveText [131], which are also collections of large text datasets from multiple sources, but with a focus on English. We applied the same methodology to create our dataset's initial version, concatenating several portions of openly available datasets for Portuguese and deduplicating their summation with an exact hash deduplication filter [110].\nOur pretraining corpus, which we will refer to as GigaVerbo, contains over 145 million documents, amounting to 780 GB of text. More details of its composition can be found in Table 1."}, {"title": "3.2 Filtering GigaVerbo", "content": "As recent studies have suggested, several gains in performance can be achieved by enhancing dataset quality instead of merely scaling data ingestion and model size [113, 73, 96, 125, 170, 94, 158, 50]. However, what defines a text as \u201chigh-quality\u201d is a nontrivial question. While heuristic-based filters can help us parse samples that are, for example, too short or ill-formatted, it is hard to differentiate high-quality text (e.g., articles, poems, tutorials) from plain text scrapped from the web (e.g., product information scrapped from e-commerce platforms) using only heuristic-based filters. Given that human annotation can be tedious and expensive [52], and current learned filters are either ill-suited for Portuguese or too expansive to run at scale, we decided to employ the same strategy used by Gunasekar et al. [73] and train our own filtering system.\nFor this, we randomly selected 110,000 samples from 9 Subsets of GigaVerbo (i.e., specifically those not synthetic). With these samples, we created a text-quality dataset using GPT-4o as a judge. Similar to the study of Gunasekar et al., [73], we prompted GPT-4o to score every text sample regarding its quality to create a high-quality text dataset for the Portuguese language.\nAs a first attempt, we sought to emulate Gunasekar et al. [73] by converting the text samples of our classification dataset into embedding representations via a sentence-BERT [135]. After evaluating several available multilingual sBERTs, we selected LaBSE (Language-agnostic BERT Sentence Embedding) [56], which generates 768-dimensional embedding vectors. Then, we trained a shallow classifier based on XgBoost [28]. To convert Real numbered scores into labels, we binarized our data by defining as \u201chigh\u201d all samples with a score >= 0.8 and \u201clow\u201d all those with a score <= 0.6. However, we were not satisfied with the results of this initial approach, and we hypothesize that the embedding representations of LaBSE were not performant enough for Portuguese. Hence, we decided to use BERTimbau [152] as the foundation for a text classification model. Results for both approaches can be found in Table 2.\nIn the end, we chose to use our fine-tuned version of BERTimbau-base to filter GigaVerbo, given that it had achieved good performance and was faster than both our XGBoost classifiers and BERTimbau-large. After parsing GigaVerbo with our learned filter, from 145 million samples, our classifier assigned low-quality to approximately 50 million samples, leaving 65% of GigaVerbo with a high-quality ranking according to our filter. However, for this study, we adopted a filtering approach where we only removed the low-quality samples if the confidence of our classifier was above 95% for the low-quality class. We expect that this would minimize token waste due to low-confidence false negatives. This approach leaves us with \u2248 70% of GigaVerbo to work with. The available GigaVerbo version on Hugging Face has the class and confidence score assigned by our filter for each text sample, allowing other users to replicate our training mixture or adapt the filtering process to their liking."}, {"title": "3.3 Scaling GigaVerbo", "content": "According to the work of Muennighoff et al. [111], training with up to 4 epochs of repeated data in data-constrained scenarios yields minor changes to loss compared to unique data, while further"}, {"title": "4 Tokenization", "content": "As already pointed out by previous studies [57, 41, 89, 38], the success of a tokenization scheme in compressing a given language has a subsequent impact on the efficiency of the language model in question. While the precise effect on the overall language modeling capability remains unclear [142], the tokenization scheme certainly plays a significant role in this process [70]. In terms of compression, one can significantly improve tokenizer efficiency (i.e., how many tokens are required to encode a given piece of text) when using a vocabulary custom-made for a given domain [89, 38]. This allows us to better utilize limited resources, like context, when working with transformer-based models.\nTo better assess and compare tokenizer efficiency across our revised anthology of Portuguese language models, we replicated the test evaluation performed by both Larcher et al. [89] and Corr\u00eaa et al. [38] on several available tokenizers tied to Portuguese LLMs. For this, we used a text sample containing"}, {"title": "5 Architecture", "content": "Like many other studies [127, 89, 38, 66, 5], we used a decoder-only Transformer based on the Llama architecture [160, 161, 50] as the basis for our models. In terms of code implementation, we used the implementation provided by Hugging Face so that our models can be easily shared and used by the community. Like the standard Llama architecture, our models use both root mean square layer normalization [180] and RoPE embeddings [155], with Silu activation\u2019s [53] instead of the SwiGLU [145] described in the original Llama papers. All models were trained using a causal language modeling objective and cross-entropy as its loss. The dimensions of our models, which we named Tucano, are documented in Table 4."}, {"title": "6 Training and Evaluation", "content": ""}, {"title": "6.1 Hyperparameters and Performance", "content": "Our training code base was primarily built with a PyTorch-based deep learning stack [123]. As a training framework, given that our model sizes could all fit inside the memory of our GPUs, we utilized a simple Distributed Data-Parallel approach [95]. For support, we used specific libraries tied to the Hugging Face ecosystem, like Tokenizers [82] for fast tokenization and Datasets [92] for handling our pretraining corpus. We also used FlashAttention [43, 42] for optimized IO-aware attention computation and the Liger Triton kernels [79] to reduce memory footprint and improve token throughput during training. We used 8 NVIDIA A100-SXM4-80GB GPUs to train both smaller versions of Tucano (160m and 630m) and 16 of these for our two largest models (1b1 and 2b4). Lastly, we utilized CodeCarbon [32] to track the resource consumption of our experiments and training runs.\nTo assess the efficiency of our deep learning stack, we utilized the method proposed by Chowdhery et al. [30] to estimate the model FLOPs utilization (MFU) we were able to achieve during our training runs, which can be understood as the ratio of the observed throughput (actual performance) to the theoretical maximum throughput that a given hardware offers. Regarding speed comparison, our code implementation is on par with other documented developments in the literature. For example, for our Tucano-1b1, we were able to achieve a training throughput of 24,180 tokens per second per A100-SXM4-80GB, which is similar to that achieved in the development of TinyLlama [181], and superior to those documented in the development of the Pythia suite [15]. In Table 5, we document the hyper-settings used to train our models and the efficiency we achieved."}, {"title": "6.2 Batch Size and Gradient Accumulation", "content": "According to the literature, transformer-based networks can benefit from larger batch sizes during training [144]. By larger, we mean up to millions of tokens per batch. For example, in the first iteration of GPT-3, the series was trained on batches from 524K to 3.2 million tokens [19], with batch sizes increasing with model size. Meanwhile, all Llama 2 models were trained with 4 million tokens per batch [161], while Llama 3 405B used a massive amount of 16 million tokens per batch [50]. In Biderman et al. [15] development of the Pythia suite, all models were trained with a batch size of 2 million. Currently, for language model training at the sub 2 billion parameters mark, most studies maintain a batch size between 1 to 2 million tokens per batch [73, 181, 101, 158].\nGiven that achieving this batch size range requires that our hardware have a significant amount of available memory for batch processing, a common approach documented in the literature is using gradient accumulation strategies when limited by available VRAM. In essence, gradient accumulation is used during training to simulate larger batch sizes than our hardware\u2019s memory typically allows.\nIn this approach, instead of updating the model parameters after each mini-batch, the gradients are"}, {"title": "6.3 Evaluation Protocol", "content": "While training our base models, we saved several checkpoints for each model at intervals of approximately 10.5 billion tokens. For every checkpoint, in addition to running a small evaluation dataset (i.e., 60,000 samples randomly selected and excluded from GigaVerbo) to track the model\u2019s"}, {"title": "7 Alignment", "content": "To offer a more easy-to-use version of our more capable models (i.e., 1b1 and 2b4), we implemented a fine-tuning process divided into two stages: supervised fine-tuning (SFT) [119] and direct preference optimization (DPO) [132].\nFor the supervised fine-tuning step, we synthesized a small dataset containing over 600K samples of user-assistant interactions generated by other models that went through an alignment process. A description of this dataset can be found in Table 7. These fine-tuned models have special chat-delimiting tokens (i.e.,  and ) added to their tokenizers and were trained by starting from the latest checkpoint of their respective model (e.g., Tucano-1b1, step 480,000). Regarding hyper-settings, fine-tuning jobs performed another learning rate decay to 10% of the original minimal value achieved during training, with no warm-up steps and all other hyper-parameters unchanged. Both models were trained on a batch size of 262K tokens per optimizer step for four epochs.\nFinally, for the DPO step, we used the preference modeling dataset developed by Corr\u00eaa [37], which consists of 35K triplets comprising a user prompt, a preferred response, and a less preferred alternative. We design our DPO fine-tuning implementation on top of the Transformer Reinforcement Learning (TRL) library [167]. We trained both models using their respective SFT versions as initial checkpoints. Regarding hyper-settings, for both models, we used a cosine learning rate scheduler"}, {"title": "8 Results", "content": "Figure 5 depicts the logged training loss and validation perplexity curves for all four base models we trained. As expected, larger models exhibit a more significant reduction in loss and perplexity as training progresses, even though this difference would be made more pronounced if we could train our bigger models with larger batches. In short, our logs reaffirm that as the model size and data ingestion are increased, the performance of the language model also increases."}, {"title": "8.1 Benchmark Evaluations", "content": "As mentioned, for every 10.5 billion tokens processed during training, we saved a checkpoint for each model and ran our evaluation harness on it. This approach allowed us to systematically track and represent model performance as a function of time and token ingestion, enabling us to observe how model performance, across several benchmarks, is related to token ingestion on a plain causal language modeling regime without intentionally seeking to overfit a specific training (or evaluation) distribution.\nIf we assume that \u201cthe more a model is trained on new tokens, the more capable it becomes\u201d, as demonstrated by several works examining the scaling behavior of LLMs [131, 78, 15, 175, 181], we would expect to observe this phenomenon when evaluating our models with the custom evaluation harness we developed (which contains most of the evaluations used by the Portuguese NLP community). To test this hypothesis, we calculated the Pearson product-moment correlation"}, {"title": "8.2 Qualitative Demonstration", "content": "To complement our evaluations, Table 11 presents a qualitative demonstration of Tucano\u2019s text generation capabilities by prompting our largest models on diverse topics related to Brazilian and Portuguese culture.\nAccording to our initial explorations, Tucano models demonstrate a firm grasp of culturally relevant subjects tied to the Lusophone world and can generate coherent and contextually appropriate text regarding many subjects. However, like all LLMs, our models have strong tendencies towards generat-ing hallucinations, i.e., text that is grammatically correct but factually erroneous or incomprehensible,"}, {"title": "8.3 Energy Consumption and Carbon Emissions", "content": "Following the example of past works [154, 67, 103, 87, 47, 104], and with the knowledge of the side effects deep learning practices can have when scaled at the level required by large foundation models [162, 124, 54, 55], we tracked our energy use during training, experiments, and evaluation. We measured the energy consumption and estimated carbon emissions for every checkpoint created during our training runs and experiments. All estimations were made using the 2023 estimations of the carbon intensity of Germany\u2019s energy grid (0.37 KgCO2eq/KWh), which, according to Lottick et al. [103] methodology, can be used to infer carbon emissions (CO2eq) by multiplying the carbon intensity of the energy grid by the total energy consumption of a given experiment. Table 12 summarizes the energy and carbon footprint related to our work.\nDeep learning research is fundamentally driven by experimentation and heuristic approaches. Al-though many studies attempt to document training procedures [182, 14, 49, 182], offering valuable guidelines for configuring models and their training environments, these published (or documented) procedures rarely provide universal solutions. Hence, the heuristic challenges and the current deficien-"}, {"title": "9 Future Works", "content": "The Tucano series significantly contributes to the Portuguese NLP community in several ways. First, we ensure that the entire series is open-source and highly reproducible. Additionally, the language models we present are trained on the largest documented dataset of native Portuguese text. To the best of our knowledge, the scale of monolingual Portuguese pretraining in this study is unprecedented in the literature. All models, along with intermediary checkpoints, datasets, code implementations, and logs, are freely accessible through the repositories associated with this study. Table 13 summarizes the availability of the artifacts mentioned earlier in the context of the Portuguese LLMs reviewed in Section 2, with a comparison to our own work.\nNevertheless, numerous milestones remain to be achieved before Portuguese can be considered a high-resource language. Some of the prospects for future studies are:\n1.  Expanding GigaVerbo by creating larger concatenation of Portuguese datasets. Future studies should seek to enrich our pretraining corpus with more high-quality tokens, like academic papers, books, and other forms of high-quality text. Ambitiously, we should aim to reach the trillion-token range. At the same time, it would be interesting to conduct ablation studies on GigaVerbo to determine the impact of different dataset components and identify which subsets contribute most effectively to model performance.\n2.  Further enhancing GigaVerbo by incorporating synthetically generated data. While this approach was not explored in our current study, synthetic data augmentation has been proven in other works to bolster model performance in many specific domains (e.g., coding and storytelling) [73]. In the future, augmenting GigaVerbo with this type of data could"}, {"title": "10 Conclusion", "content": "In this study, we introduced the Tucano series, a collection of open-source large language models designed to advance natural language processing for Portuguese. Our work covered the entire de-velopment pipeline, from dataset creation and filtration to hyperparameter tuning and evaluation, emphasizing openness and reproducibility. These efforts contribute capable models, large datasets, and tools to the Portuguese NLP community to set a standard for transparent and replicable research practices. Moreover, our critical assessment of the field highlighted ongoing challenges, particularly around evaluation methodologies and result interpretability, which will only be solved if the commu-nity shifts toward a more rigorous and reproducible developmental framework. Finally, we hope our contributions will help spur this shift, providing essential resources to guide future studies. Ultimately,"}]}