{"title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation", "authors": ["Mohammad Mehdi Rastikerdar", "Hui Guan", "Jin Huang", "Deepak Ganesan"], "abstract": "Wildlife monitoring via camera traps has become an essential tool in ecology, but the deployment of machine learning models for on-device animal classification faces significant challenges due to domain shifts and resource constraints. This paper introduces WildFiT, a novel approach that reconciles the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. WildFiT leverages continuous background-aware model fine-tuning to deploy ML models tailored to the current location and time window, allowing it to maintain robust classification accuracy in the new environment without requiring significant computational resources. This is achieved by background-aware data synthesis, which generates training images representing the new domain by blending background images with animal images from the source domain. We further enhance fine-tuning effectiveness through background drift detection and class distribution drift detection, which optimize the quality of synthesized data and improve generalization performance. Our extensive evaluation across multiple camera trap datasets demonstrates that WildFiT achieves significant improvements in classification accuracy and computational efficiency compared to traditional approaches.", "sections": [{"title": "1 Introduction", "content": "Wildlife ecology plays a crucial role in understanding and preserving our planet's biodiversity. In recent years, camera traps have emerged as an indispensable tool for wildlife researchers and conservationists, providing unprecedented insights into animal behavior, population dynamics, and habitat use. These unobtrusive devices, capable of capturing images or videos when triggered by animal movement, have revolutionized the field of wildlife monitoring and result in significant growth in the deployment of camera traps [21].\nHowever, the sheer volume of data generated by these devices presents a significant challenge. Many camera trap deployments occur in areas far from the power grid and reliable internet connectivity, making it impractical to transmit all captured images. This limitation has spurred the development of on-device animal detection and classification systems [2, 21, 26], which aim to process images locally and transmit only those containing animals of interest to conserve power and bandwidth.\nThe Generalizability Problem. A key challenge in deploying these intelligent camera traps on IoT devices is the issue of generalizability i.e. the ability of a machine learning (ML) model to perform well on data from new, unseen domains that differ from the domains on which the model was trained [3, 36]. In the context of camera traps, the data used to train the model (i.e. the source domain) is data collected from other locations where the camera traps were deployed, while the environment where the model is expected to perform well (i.e. the target domain) is the new location where the camera trap is deployed. A model with strong domain generalization capability can accurately classify animals even if the images are taken in different locations, under different lighting conditions, or during different seasons, without needing to be retrained for each new setting.\nHowever, each new deployment site presents a unique set of environmental conditions that a camera trap model must contend with. This includes spatial domain shift-variations in lighting, vegetation, terrain, and local fauna across different locations-and temporal domain shift-weather, seasonal and other time-based changes that alter image appearance.\nThe challenge of achieving robust domain generalization capability is compounded by the limited resources available on IoT devices. While a wide range of methods for domain generalization have proven effective (e.g. domain alignment [16], meta-learning [12],"}, {"title": "WildFiT", "content": "WildFiT. To address these challenges, we identify and leverage domain-specific opportunities unique to camera trap applications. In these applications, the camera is typically static, resulting in relatively stable background scenes across images. While collecting images containing the target animals in a new domain is impractical-since the model needs to be adapted beforehand-background images without animals are easily obtainable from new locations or during different time periods.\nTo enable rapid, effective, and proactive fine-tuning, we propose a method called background-aware data synthesis. This technique generates training images that closely represent the new domain by blending these background images with animal images from the source domain. The synthesized images are then used to fine-tune the classification model. This approach allows for proactive fine-tuning by generating a diverse set of training images that reflect the target domain's environmental conditions, without requiring actual animal images after a domain shift. Additionally, the synthesis pipeline is simple and efficient, making it suitable for real-time application alongside other data augmentation techniques during model fine-tuning. We refer to this process as background-aware fine-tuning.\nTo further enhance the effectiveness of fine-tuning, we introduce two additional optimizations. The first, background drift detection, continuously monitors the camera trap environment to collect representative background images for background-aware data synthesis. This method is based on the premise that not all background images contribute equally to fine-tuning performance; instead, using a set of recent and representative background images can help synthesize higher-quality datasets, thereby improving model accuracy. The second optimization, class distribution drift detection, utilizes recent animal class distributions detected by the on-device classification model to inform the background-aware data synthesis process. It aligns the distribution of synthesized animal images with the observed recent class distributions, further enhancing fine-tuning performance.\nWe conducted extensive evaluation of the performance of WildFiT across three camera trap datasets and two IoT processors. Our results show that:\n\u2022 WildFiT's background-aware data synthesis outperforms no fine-tuning by 23% in accuracy on average and outperforms other computationally efficient approaches [33, 35] by 10.4%, all while being computationally more efficient (6.9ms vs 10-11ms on a CPU).\n\u2022 Our method also achieves 1.7% higher accuracy on average compared to diffusion model-based synthesis approaches [25, 34], while being three orders of magnitude faster (6.2 ms on CPU vs 6-13 seconds per image on NVIDIA RTX8000) and requiring no additional memory.\n\u2022 End-to-end results show that WildFiT significantly outperforms several domain adaptation approaches such as using pseudo labels or even using ground-truth labels of animals in the new location to fine-tune.\n\u2022 Our ablation study shows that the Background Drift Detection module improves accuracy by 2.4-6% for the different datasets compared to using only background-aware synthesis. The"}, {"title": "2 Background and Related Works", "content": "This section first introduces camera trap applications and then discusses related works and their limitations."}, {"title": "2.1 Camera Trap Applications", "content": "A camera trap is a camera that is automatically triggered by motion in its vicinity, like the presence of an animal or a human being. Camera trap applications play a critical role in wildlife monitoring, conservation, and ecological research by providing invaluable data on animal behavior, population dynamics, and biodiversity [21]. These automated systems capture vast amounts of image and video data from remote and often inaccessible locations, offering insights that would be impossible to obtain through manual observation.\nML in Camera Trap Applications. The sheer volume of data generated by camera traps presents a significant challenge for timely analysis and interpretation. This is where machine learning (ML) becomes indispensable. ML algorithms enable the efficient processing and classification of camera trap images, automating the identification of species, tracking of animal movements, and detection of rare or endangered species. These ML models can be deployed directly on camera trap devices, particularly when these devices are equipped with edge computing capabilities. This allows the camera traps to process data locally, rather than sending all the data to a central server for analysis. Deploying ML models on these devices reduces the need for constant data transmission, which is crucial for conserving battery life and managing limited bandwidth, especially in remote locations. Due to the constraints of IoT devices, such as limited computational power, memory, and energy, these ML models are often lightweight and optimized for efficiency. Techniques like model compression, pruning, and quantization are commonly used to make the models suitable for deployment on such resource-constrained devices [20].\nThe Generalization Problem. The generalization problem in camera trap applications is caused by domain shifts, which refer to the distribution shift between a set of training (source) data and a set of test (target) data. Domain shifts can cause a well-trained ML model to perform arbitrarily worse on the target data. Mathematically, let X be the input (feature) space and y be the target (label) space. A domain is defined as a joint distribution P(X, Y) on X \u00d7 Y. In the context of wildlife classification, X represents images containing animals of interest (also called animal images) while y represents the set of animal labels or classes. We have training data S = {(x, y)} sampled from the source domain distribution PS (X, Y). The goal of domain generalization is to learn a predictive model f : X \u2192 \u0423 using source domain data S such that the prediction error on an unseen target domain data T = {(x)}, sampled from the target"}, {"title": "2.2 Related Work and Limitations", "content": "While there is extensive literature on how to deal with domain shifts, we show that existing approaches cannot solve the generalization problem in camera trap applications due to various limitations.\nDomain Adaptation. Domain adaptation (DA) focuses on adapting a model trained on the source domain to perform well on a different but related target domain. Typically, DA methods assume the availability of labeled or unlabeled target data for model adaptation, making them a poor fit for wildlife monitoring applications. For example, when cameras are deployed in new locations, it is impractical to collect images of animals from each new site in advance. The limitations of this approach are further highlighted in \u00a7 3.2 to motivate the importance of data synthesis.\nSome DA approaches, known as zero-shot DA, attempt to generalize to unseen target domains by leveraging auxiliary information about the target domain even if they don't have direct target domain data during training. For example, ZDDA [18] and CoCoGAN [31] learn from the task-irrelevant dual-domain pairs, while Poda [8] uses natural language descriptions of the target domain. These approaches cannot be applied, as they rely on auxiliary information that is unavailable for wildlife animal classification tasks.\nDomain Generalization. Domain generalization (DG) aims to create models that are inherently robust to domain shifts by ensuring good performance across various source domains. Unlike DA, DG assumes no access to target domain data during training. DG methods can be categorized into domain alignment [16], meta-learning [12], ensemble learning [37], and foundation models [4]. However, achieving strong generalization often requires computationally complex models, as seen in recent foundation models [4], and are therefore not suitable for resource-constrained IoT devices.\nData Augmentation. Data augmentation approaches can generally improve the robustness of ML models. The existing literature roughly falls into the following groups: hand-engineered transformations [22], adversarial attacks [30], learned augmentation models [6, 9], feature-level augmentation [33, 35], and more recently generative models such as diffusion models [28].\nThe most relevant for us are feature-level augmentation techniques such as CutMix [33] and MixUp [35] that generate synthetic images by blending objects of interest with background images. Despite their efficiency, we show that these generic techniques do not produce high-quality synthetic animal images, resulting in limited, if any, performance improvement. This is because these methods often strive to augment images in a domain-agnostic way, which can improve model robustness but may not capture the specific characteristics of new deployment environments.\nDiffusion models, on the other hand, offer the potential for high quality, realistic, image synthesis that can better match target scenes. Off-the-shelf diffusion models can be leveraged to generate synthetic target domain animal images by combining a source domain animal image with a target domain background [17, 25, 34]. However, these incur immense computational cost making them infeasible for rapid, in-situ adaptation to changing scenes, even with cloud"}, {"title": "3 Design of WildFiT", "content": "WildFiT is a wildlife classification system designed to achieve both high domain generalization performance in recognizing animals of interest and high efficiency in on-device inference. It bridges the gap between various techniques discussed in \u00a7 2.2, combining strengths from multiple areas while overcoming their individual limitations:\n\u2022 Adaptability without target data: Unlike traditional DA methods, WildFiT adapts to new environments without requiring labeled or unlabeled target data. It differs from zero-shot DA in its use of easily obtainable background images as auxiliary information and its ability to continuously adapt in-situ.\n\u2022 Generalization with efficiency: Unlike existing domain generalization (DG) approaches, WildFiT leverages domain-specific information (background images) to perform proactive model fine-tuning, enabling the use of smaller and more efficient models while maintaining strong performance across domain shifts.\n\u2022 Targeted augmentation: Unlike generic augmentation methods, WildFiT performs domain-specific data synthesis using background images from the new deployment location. This targeted approach, combined with computationally efficient synthesis techniques, improves data quality and allows rapid and effective adaptation to domain shifts without significant computational resources.\n\u2022 Continuous adaptation: Through drift detection, WildFiT dynamically adjusts to gradual or abrupt changes in environments. This distinguishes WildFiT from traditional DA methods, which often assume a static target domain. By continuously adapting to changing conditions, WildFiT enhances long-term effectiveness in wildlife monitoring applications.\nThis combination of features allows WildFiT to address the specific challenges of wildlife monitoring applications more comprehensively than existing approaches. We next provide an overview of WildFiT and then detail the key system components that address the challenges of continuous model fine-tuning."}, {"title": "3.1 Overview of WildFiT", "content": "Figure 3 illustrates the design of WildFiT. WildFiT runs a wildlife classification model on an IoT device to identify animals of interest captured by a motion-triggered camera. Concurrent to the inference on every incoming frame, WildFiT includes three main components to enable rapid, effective, and proactive model fine-tuning.\nThe first component, the Background-Aware Data Synthesizer (\"Synthesizer\" for short), creates training images that mimic the target domain by integrating objects (e.g., animals of interest) from the source domain into background images captured in the target environment. The synthesized data are used to adapt the wildlife classifier to the target domain.\nThe second component, the Background Drift Detector (BDD), identifies changes in background images that suggest a shift in the environmental setting. By detecting these drifts, the system can update the background images used in the synthesizer, enabling the fine-tuning of the IoT model before animals in these new environments occur.\nThe third component, the Class Distribution Drift Detector (CDD), monitors the class distribution based on the IoT model's predictions on recent frames. This allows the system to fine-tune the model whenever a drift in class distribution is detected. WildFiT then adjusts the distribution of synthesized images from the Synthesizer to match the recent class distribution and updates the model accordingly.\nUsage Scenarios. WildFiT operates in both cloud-assisted and unattended modes. When cloud assistance is available, the Synthesizer and model finetuning run on the cloud. The cloud maintains a repository of background images and the class distribution, initialized when a camera trap is installed at a new location and updated over time. If the BDD executed on the device identifies a new background image indicating an environmental shift, the image is sent to the cloud to update the repository. Similarly, when the CDD on the device detects a change in class distribution, the new distribution is sent to the cloud for repository updates and triggers a model fine-tuning. WildFiT fine-tunes the model in the cloud using a mix of synthesized images, background images from the target domain, and training images from the source domain, and then updates the on-device model accordingly.\nFor unattended operations, where cloud availability is not an option, WildFiT relies on local model fine-tuning. To support local fine-tuning, the IoT device stores training images from the source domain as well as animals of interest for data synthesis. In \u00a7 4.5, we demonstrate that such on-device storage requirement is feasible on the evaluated IoT platforms. We next describe the three key components of WildFiT in more detail."}, {"title": "3.2 Background-Aware Data Synthesizer", "content": "The Synthesizer tackles a crucial challenge for WildFiT: the lack of high quality training samples (i.e., animal images) from the target domain, which are essential for adapting the on-device classification model after a domain shift. This section first highlights the importance of data synthesis in overcoming the challenge and then elaborates on the proposed synthesis pipeline.\nThe Importance of Data Synthesis. Since fine-tuning requires target domain data, we explain why traditional domain adaptation methods, which rely on collecting data from the target domain, are not an ideal solution. Domain adaptation approaches react to domain shifts by retraining the model with a mix of target domain data and source domain data. This involves waiting for animals to appear after a domain change and manually generating their ground truth labels or generating their pseudolabels using the wildlife classification model trained on the source domain data. These labels or pseudolabels are then used to fine-tune the model and adapt to the changing environment. Although this approach is computationally efficient, it suffers from two major drawbacks, as we show empirically in Section 4.5. First, due to the poor performance of the classification model after domain shifts, pseudolabels can be highly noisy, resulting in limited performance improvement from fine-tuning. Second, since animals appear relatively infrequently, there can be a significant delay in collecting enough animal images from the target domain. Concurrently, the background continues to change, so adaptation is ineffective and fine-tuning is constantly catching up to the current environment. Due to the lack of sufficient target domain data before the domain shifts again, the performance is still sub-optimal even when ground truth labels are available.\nThe Proposed Synthesizer. Our proposed Synthesizer addresses these challenges by eliminating the need for target domain data. It leverages the insight that background images from the target domain are readily available in camera trap applications and can be used to generate synthetic animal images that closely align with the target domain's data distribution. Figure 4 illustrates the synthesis pipeline, which consists of an offline phase executed once and an online phase executed during model fine-tuning to create synthetic images on-the-fly.\nThe offline phase: This phase creates a repository of animal objects by extracting them from the training images. We utilize MegaDetector V5 [15] to identify the bounding box around each object, then pass the image and the bounding box information to the Segment Anything model [11] to extract the object's mask and isolate the object image.\nThe online phase: The online phase samples animal objects and blends them with background images to synthesize animal images. This involves overlaying the object onto a random background image from the target domain. While the placement of the object could be random, our observations indicate that placing the object in a location corresponding to its original position yields higher accuracy. Therefore, we use the bounding box data from MegaDetector V5 to accurately position the object within the background. Additionally, we synthesize images containing multiple animals (herds) to effectively represent similar scenarios in the target domain."}, {"title": "3.3 Background Drift Detector", "content": "WildFiT needs to adapt the model to temporal domain shifts. However, transmitting or storing all background images is impractical due to communication limitations in cloud-assisted scenarios and storage constraints on IoT devices in local-only scenarios. Therefore, it is essential to detect background images that indicate significant temporal shifts and provide valuable new information. This requires knowing when to transmit (in cloud-assisted scenarios) or store (in local-only scenarios) a background image."}, {"title": "3.4 Class Distribution Drift Detector", "content": "To further optimize the adaptation process in WildFiT, we introduce a module called the Class Distribution Drift Detector (CDD), which tracks the distribution of predictions on the IoT device and detects when a drift in class distributions occurs. This module is motivated by the observation that the animal class distribution changes across locations and across time. By continuously aligning the synthesized image distribution to the target domain distribution, CDD is expected to further boost finetuning performance.\nFigure 5 shows the class distribution of a test location in the Serengeti S4 dataset [24] across three consecutive time windows. The distribution exhibits drift over time. Specifically, comparing the T1 curve with T2 and T3 reveals increasing divergence as time progresses, with T3 showing a pronounced drift and T2 exhibiting a lighter shift.\nCDD operates on IoT devices and works as follows. We use the Chi-Squared statistical test for drift detection, as it is well-suited for categorical data. Once at least C samples are collected, the recent class distribution and the class distribution from the previous fine-tuning cycle are input into the Chi-Squared drift detector. Using a p-test with a 5% significance level, it determines whether a drift has occurred. If a drift is detected, we fine-tune the IoT model based on the new class distribution."}, {"title": "4 Evaluation", "content": "We first describe the experiment settings in \u00a74.1, and then evaluate the three key system components Background-Aware Data Synthesizer, Background Drift Detector (BDD), and Class Distribution Drift Detector (CDD) in \u00a74.2-4.4. We further perform end-to-end evaluations of WildFiT in \u00a74.5, and ablation studies in \u00a74.7."}, {"title": "4.1 Experiment Settings", "content": "Datasets. We utilize three datasets as shown in Table 2: two from the Serengeti Safari Camera Trap network (referred to as D1 and D2) [24], and one from the Enonkishu Camera Trap dataset (referred to as D3) [23]. These datasets are chosen since they have temporal data over relatively long durations and across several locations captured by trail cameras, hence they exhibit real-world temporal and spatial domain drifts. The Camera Trap datasets normally include a lot of species plus an empty class whose images show a scene with no species in it. Some species lack enough samples for both training and testing sets. Therefore, we selected the 18 most frequent species from the Serengeti dataset and 10 from the Enonkishu dataset for classification. Including the empty class, this results in 19 classes for Serengeti and 11 for Enonkishu. Table 2 lists the data statistics. \"Train\" are the locations whose animal images are used to train the classification model while \"Test\" refers to those used to evaluate spatial domain shifts.\nImplementation of WildFiT. The wildlife classifier is trained in PyTorch using a two-stage pipeline on the source domain data to ensure high model quality. It employs the EfficientNet-B0 model pre-trained on ImageNet [7]. In the first stage, the classification head is trained for 5 epochs with a learning rate (lr) of 1e-3 while keeping the backbone frozen. In the second stage, the entire network is trained for 30 epochs with an lr of 1e-5, using an early stopping criterion of 2 epochs. For fine-tuning over time (BDD and ADD), we maintain the same lr of 1e-5. The Adam optimizer [10] is used along with a cross-entropy loss function.\nThe background repository is initialized with 40 images and expands as BDD detects new samples. The last 20 background images are stored on the device for drift detection. The wildlife classifier is fine-tuned weekly by default, using a combination of source domain images and those synthesized with the latest background repository. If the latest class distribution contains at least C = 100 predictions, it is used for fine-tuning; otherwise, the previous distribution is applied.\nAdditional model finetuning can be triggered by the CDD module. When C = 100 new predictions are collected, the class distribution of these predictions is compared to the previous one using a Chi-Squared test. If drift is detected, the model is fine-tuned with the"}, {"title": "4.2 Eval. of Background-Aware Synthesis", "content": "Baselines. We compare our approach against three classes of techniques that offer different compute-accuracy tradeoffs:\n(1) No fine-tuning (No-FT), which serves as a lower bound baseline. Our goal is to significantly outperform this approach, demonstrating the clear benefits of our method.\n(2) Computationally efficient approaches, which are efficient enough to feasibly execute on-device on IoT hardware. These include: (a) CutMix [33] (CUT), a data augmentation technique that combines two images by cutting and pasting patches and mixing their labels proportionally. We adapted CutMix to synthesize images by pairing a source image with a background image from the target domain. (b) MixUp [35] (MIX), a data augmentation technique that creates new training samples by linearly blending pairs of images and their labels. Similar to CutMix, we adapted MixUp to blend source images with target backgrounds. These approaches are in the same computational ballpark as our techniques. Our objective is to outperform these methods while having similar or better computational efficiency.\n(3) Diffusion model-based approaches, which are currently infeasible to execute on-device and may even be too slow or expensive to run repeatedly in a cloud environment. These include: (a) Object-Stitch (Obj-St) [25], an object compositing method based on conditional diffusion models, specifically designed for blending objects into background images. (b) ControlCom (CC) [34], a method that synthesizes realistic composite images from foreground and background elements using a diffusion model. For these approaches, we focus on achieving similar accuracy while offering significant benefits in terms of on-device capability, cost-effectiveness, and continuous adaptability to changing domains.\nFor this evaluation, we first randomly selected 250 background images from each test location within the target domain and excluded them from the test set for each location. The selected backgrounds are used to synthesize animal images, which are then mixed with the training data and background images in each batch to fine-tune the classification model. The fine-tuned models are evaluated on images from the target domain using the D2 dataset.\nResults on Classification Accuracy. Table 3 shows exhaustive results comparing our synthesis method against baselines across 27 test locations in D2 dataset. We see that: (1) compared to no fine-tuning, our synthesis scheme has a massive gain of 23% in accuracy on average (last row), (2) compared to computationally efficient approaches like CutMix [33] and MixUp [35], our method has a significant advantage of 10.4% on average, with 20-41% improvements in some locations where the spatial domain shift is large, and (3) compared to diffusion model-based approaches, our method generally performs better with 1.7% average gain (and in some locations more than 10%)."}, {"title": "4.3 Eval. of Background Drift Detection", "content": "The Background Drift Detector (BDD) module is designed to select representative background images for data synthesis. To assess its effectiveness. We compare our method against a periodic sampling approach, which selects background images at fixed intervals for model fine-tuning. The intervals tested are 4, 5, and 6 hours, meaning a minimum of 4, 5, or 6 hours must elapse between samples. We chose these numbers because they are close to the average sampling frequency of BDD, which is roughly one frame per 6 hours.\nWe assess model accuracy by fine-tuning the classification model using synthesized images generated from the selected backgrounds. Both the BDD and baseline use the same fine-tuning procedure: the model is fine-tuned once per week, ensuring at least one week between fine-tuning cycles.\nFigure 7 shows the accuracy gains over these sampling approaches. Overall, BDD has an accuracy gain of about 0.8% compared to regularly sampling at this frequency without being adaptive. Regularly sampling at higher frequencies bridges the gap with our method but results in more background sampling. BDD performs comparably to the approach that transmits every 4 hours, while samples 30% less."}, {"title": "4.4 Eval of Class Distr. Drift Detection", "content": "Class distribution drift detection monitors the distribution of animal classes and triggers model fine-tuning whenever a significant shift is detected. To assess the effectiveness of this module, we compare it against two baselines: (1) Init i.e. one-time fine-tuning of the model using the initial distribution of source domain data, followed by no subsequent fine-tuning over time, and (2) N-Week i.e. fine-tuning periodically at intervals of N weeks. We assessed this baseline with N\u2208 {1, 2, 3} weeks. For both our method and the baselines, we synthesize images using a sample of 250 background images. We measure the classification accuracy of the fine-tuned model aggregated on the 27 test locations of D2.\nFigure 8 presents the finetuning frequency and accuracy gain of our approach (CDD) over various baselines, aggregated over 27 test locations from the D2 dataset. Compared to the Init baseline, CDD shows a mean accuracy improvement of approximately 2%, highlighting the effectiveness of our fine-tuning process. When compared to periodic fine-tuning schemes, CDD maintains competitive accuracy while reducing fine-tuning frequency. Specifically, CDD achieves mean accuracy improvements of 0.7%, 0.5%, and 0.2% over the 3-week, 2-week, and 1-week schemes, respectively. Importantly, CDD triggers fine-tuning on average 2.2 times, compared to 5.1, 2.4, and 1.7 times for the 1-week, 2-week, and 3-week schemes. This puts CDD on par with the 2-week scheme in terms of fine-tuning frequency, while still outperforming it in accuracy. We note that minimizing fine-tuning frequency is crucial, as it introduces latency overhead in both cloud-assisted and local-only"}, {"title": "4.5 End-to-End Performance", "content": "This section reports the end-to-end evaluation of WildFiT's performance in terms of model accuracy. The results apply to both the cloud-assisted and local-only scenarios. We compare the following approaches:\n\u2022 Pseudolabel: This baseline assumes a few target domain animal images are collected for model fine-tuning. As ground truth labels are unavailable, it uses the IoT model (EfficientNet-B0 or B4), trained on the source domain, to generate pseudolabels for fine-tuning.\n\u2022 GTlabel: This approach assumes the target domain animal images have ground truth labels. While impractical, it gives us an upper-bound for the Pseudolabel method.\n\u2022 WildFiT:Syn: This approach removes the Background Drift Detection (BDD) and the Class Distribution Drift Detection (CDD) modules from WildFiT to evaluate their importance. We start by initializing our background repository with the first 40 samples and perform a single round of model fine-tuning.\n\u2022 WildFiT:Syn+BDD: This approach removes only the CDD module. Like WildFiT-Syn, the background repository is initialized with 40 images, but new backgrounds are added when BDD is triggered. Fine-tuning occurs at intervals of at least one week, meaning there must be at least one-week gap between fine-tuning sessions.\n\u2022 WildFiT:All: This is the complete WildFiT system as shown in Figure 3."}, {"title": "4.6 Local-Only WildFiT", "content": "We now consider the performance of WildFiT in the case of unattended operation where fine-tuning occurs entirely at the IoT device deployed on-site without needing to communicate with the cloud. This is important in practice for Camera Trap deployments in remote locations where nodes often need to rely on solar power and have weak cellular connectivity.\nWe see from Table 6 that WildFiT is extremely efficient in generating synthetic images. For example, on the Raspberry Pi 5, WildFiT adds only 13ms to standard augmentation and data loading. The total time for synthesizing, applying standard augmentations, and loading a batch of 12 images is only 270 ms on the Raspberry Pi 5, which is significantly faster than the 36.12 seconds required for training a single batch with EfficientNetB0. This speed difference allows for the generation of a large number of synthetic images during the time it takes to train on a single batch, greatly enhancing the diversity and quantity of training data available. The results show that WildFiT has very low computational overhead and is highly practical for real-world use on resource-constrained platforms, while allowing our wildlife classification system to continuously adapt to changing environments.\nThe local-only setting requires approximately 1.8GB for storing 350 samples per class and 600MB for storing 100 samples per class (including both the original training set and extracted objects). Since devices like Raspberry Pi 4B and Pi5 have up to 8GB RAM and support up to 32GB SD cards, they can easily accommodate the necessary data for local-only operations."}, {"title": "4.7 Ablation Studies", "content": "Breakdown per module. The last three rows of Table 5 demonstrate that each stage of WildFiT contributes to performance improvement. The background-aware synthesis approach (Syn) alone"}, {"title": "5 Conclusions", "content": "This work introduces WildFiT, a novel approach to reconcile the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. By leveraging continuous model fine-tuning and background-aware data synthesis, we demonstrate that smaller, adaptable models can match the accuracy of larger models while maintaining computational efficiency on resource-constrained IoT devices. The effectiveness of WildFiT in handling both spatial and temporal domain shifts opens up new possibilities for wildlife monitoring in diverse and changing environments. While we focused on wildlife classification in this work, the principles of continuous adaptation and efficient on-device fine-tuning may have broader applicability in other IoT domains where environmental conditions are dynamic and impact how we approach model deployment in resource-constrained, real-world settings."}]}