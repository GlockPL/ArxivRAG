{"title": "Quantum-Trained Convolutional Neural Network for Deepfake Audio Detection", "authors": ["Chu-Hsuan Abraham Lin", "Chen-Yu Liu", "Samuel Yen-Chi Chen", "Kuan-Cheng Chen"], "abstract": "The rise of deepfake technologies has posed significant challenges to privacy, security, and information integrity, particularly in audio and multimedia content. This paper introduces a Quantum-Trained Convolutional Neural Network (QT-CNN) framework designed to enhance the detection of deepfake audio, leveraging the computational power of quantum machine learning (QML). The QT-CNN employs a hybrid quantum-classical approach, integrating Quantum Neural Networks (QNNs) with classical neural architectures to optimize training efficiency while reducing the number of trainable parameters. Our method incorporates a novel quantum-to-classical parameter mapping that effectively utilizes quantum states to enhance the expressive power of the model, achieving up to 70% parameter reduction compared to classical models without compromising accuracy. Data pre-processing involved extracting essential audio features, label encoding, feature scaling, and constructing sequential datasets for robust model evaluation. Experimental results demonstrate that the QT-CNN achieves comparable performance to traditional CNNs, maintaining high accuracy during training and testing phases across varying configurations of QNN blocks. The QT framework's ability to reduce computational overhead while maintaining performance underscores its potential for real-world applications in deepfake detection and other resource-constrained scenarios. This work highlights the practical benefits of integrating quantum computing into artificial intelligence, offering a scalable and efficient approach to advancing deepfake detection technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of machine learning (ML) has paved the way for innovative applications, but it has also introduced challenges, particularly in the realm of synthetic media, such as deepfake audio and video [1]. Deepfakes, which leverage deep learning techniques to create highly realistic yet counterfeit multimedia content, pose significant threats to privacy, security, and information integrity [2]. As deepfakes become increasingly sophisticated, detecting these manipulations has become a critical area of research. Traditional detection methods [3], [4] rely on classical machine learning models; however, these approaches often struggle with generalization and scalability when confronted with the latest generative adversarial networks (GANs) and advanced deep learning architectures that are used to create deepfakes [5].\nTo address these challenges, our research explores the use of Quantum Machine Learning (QML) to enhance the detection capabilities of convolutional neural networks (CNNs) specifically for deepfake audio. QML, an emerging field that combines the computational power of quantum mechanics with the versatility of machine learning, has shown promising potential in various domains [6]\u2013[17]. Quantum neural networks (QNNs), for instance, exploit quantum superposition and entanglement to process information in ways that classical networks cannot, potentially accelerating training times and improving performance on complex data sets [18]. By leveraging quantum-enhanced techniques, QML can offer unique advantages in scenarios where large-scale data processing and intricate pattern recognition are essential [7].\nThis study introduces a Quantum-Trained Convolutional Neural Network (QT-CNN) designed for deepfake audio detection, merging the strengths of quantum computing with classical neural network architectures. Our approach employs a hybrid quantum-classical training mechanism, where quantum circuits are utilized to optimize the weight parameters of the CNN, thus enhancing its performance and robustness against adversarial inputs [19]. This methodology not only addresses the inherent limitations of classical CNNs in detecting deep-fake content but also circumvents some of the scaling issues faced by purely quantum models in the Noisy Intermediate-Scale Quantum (NISQ) era [20]. By integrating quantum computation in the training phase, our QT-CNN can operate efficiently on classical hardware during the inference stage, making it a practical solution for real-world applications.\nIn Fig. 1, the proposed QT-CNN framework represents a novel intersection of quantum computing and deep learning, tailored to the critical task of detecting deepfake audio. It showcases the practical benefits of quantum-enhanced learning while mitigating the challenges associated with purely quantum models, positioning it as a promising tool in the fight against the proliferation of synthetic media."}, {"title": "II. RELATED WORKS OF QUANTUM TRAIN", "content": "Quantum Machine Learning (QML) has introduced innovative methods to enhance neural network training, particularly through the Quantum-Train (QT) framework [21]\u2013[25], which addresses the escalating complexity and parameter demands of classical neural networks (NNs). QT leverages the expansive Hilbert space of quantum systems to significantly reduce the number of parameters required for training, scaling down from M parameters in classical NNs to O(polylog(M)) by using QNNs [21]. This approach eliminates common challenges faced by traditional QML, such as data encoding complexities and dependency on quantum resources during inference, making the trained models operable on classical hardware. QT has demonstrated its versatility and effectiveness in various applications, including image classification on datasets like MNIST and CIFAR-10 [21], [25], reinforcement learning [23], and time-series forecasting for flood prediction [24]. It further shows potential in federated learning by compressing models and maintaining performance while reducing computational overhead [22]. The QT framework, by integrating quantum advantages during training while keeping inference purely classical, offers a practical and scalable solution, making QML more accessible for real-world applications and opening new avenues for the integration of quantum insights into classical machine learning."}, {"title": "III. QUANTUM-TRAIN FOR QNNS", "content": "We begin by defining a classical NN characterized by the parameter vector \u0113 = (\u03b8\u2081, \u03b8\u2082,...,\u03b8\u043c). A quantum state |\u03c8\u27e9 is encoded using N = [log\u2082 M] qubits, creating a Hilbert space of dimension 2^[log\u2082 M] to represent the NN parameters. The measurement probabilities of the quantum state, |\u3008i|\u03c8\u3009|\u00b2, range from 0 to 1 for i \u2208 {1, 2, ...,2\u1d3a}. The goal is to map these probabilities to the NN parameters \u03b8\u1d62, which traditionally span from-\u221e to 8.\nA mapping model G is introduced, built upon an additional NN with parameters 7. It takes input vectors Z\u1d62, such as [0,1,0,0,1,0,0,0.023] for a 7-qubit system, integrating basis information and measurement probabilities, e.g., |\u30080100100|\u03c8\u3009|\u00b2 = 0.023. The mapping function G(Z\u1d62) = \u03b8\u1d62 dynamically determines \u03b8\u1d62, allowing sign flexibility compared to prior models. This adaptation broadens the model's applicability across diverse machine learning (ML) tasks."}, {"title": "B. Construction of QNNs", "content": "The quantum state |\u03c8\u27e9 is constructed with parameterized rotational gates, specifically the Ry gate, represented by:\nRy(\u03bc) =  \\begin{bmatrix} cos(\u03bc/2) & -sin(\u03bc/2) \\\\ sin(\u03bc/2) & cos(\u03bc/2) \\end{bmatrix} (1)\nEntanglement is introduced via CNOT gates in a linear layout, ensuring parameter scalability as O(polylog(M)). This setup forms the QNN, with parameters & determining the classical NN weights via the mapping model G\u2087."}, {"title": "C. Training Procedure for Quantum-Enhanced Learning", "content": "The QT framework involves constructing an N-qubit QNN with parameterized Ry gates in blocks, repeatable nblock times to enhance capacity. The classical NN uses parameters \u03b8 generated through quantum-classical mapping for traditional training, guided by the cross-entropy loss function:\nICE =  \\frac{1}{Nd} \u2211 [yn log \u0177n + (1 - Yn) log(1 \u2013 \u0177n)], (2)\nn=1\nwhere yn and \u0177n are the true and predicted labels, respectively. Gradients for the QNN parameters 6 and mapping model 7 are computed analytically or via the parameter-shift rule for shot-based simulations. These gradients guide iterative updates, refining the NN parameters.\nThe QT framework, as depicted in Fig. 1, optimizes efficiency by reducing QNN parameters to O(polylog(M)), compared to M for classical NNs, and supports inference on classical hardware, enhancing practicality under limited quantum resources."}, {"title": "IV. NUMERICAL RESULTS AND DISCUSSION", "content": "The DEEP-VOICE dataset [26] was employed to evaluate the effectiveness of the QT-CNN in detecting deepfake audio. This dataset is specifically curated to address the rising concerns associated with generative AI technologies that enable voice cloning and real-time voice conversion. DEEP-VOICE [26] comprises both authentic human speech and AI-generated deepfake audio, designed to simulate realistic scenarios of voice manipulation. The dataset includes a balanced collection of real and synthetic audio samples, providing a comprehensive platform for training and evaluating machine learning models in distinguishing between genuine and manipulated speech. Audio features were systematically extracted and processed to create structured data suitable for model training, testing, and validation. This setup ensures that the QT-CNN model is exposed to diverse and challenging cases of deepfake audio, allowing for robust performance evaluation and benchmarking against conventional detection methods."}, {"title": "B. Data Pre-processing", "content": "The data pre-processing for deepfake audio detection involved preparing the dataset to enhance the performance of the QT-CNN. The dataset consisted of extracted audio features such as chroma STFT [27], spectral centroid [28], spectral bandwidth [29], roll-off frequency, zero-crossing rate [30], and Mel-Frequency Cepstral Coefficients (MFCCs) [3], which were read into a pandas DataFrame for exploratory data analysis (EDA) [31]. Initial EDA confirmed that the dataset was balanced, containing equal numbers of fake and real audio samples, ensuring unbiased training. Labels were encoded using LabelEncoder to transform categorical labels into numerical values, suitable for machine learning models. Feature scaling was performed using MinMaxScaler to normalize the data, which improves the convergence of neural network models. A sliding window technique was applied to create sequential feature sets, essential for training models. The dataset was then split into training, validation, and test sets using stratified sampling to preserve the balance of classes across all subsets. Correlation matrix analysis was conducted to examine inter-feature relationships, which informed potential feature selection and dimensionality reduction steps. This pre-processing approach ensures that the QT-CNN model is trained on well-prepared, balanced, and normalized data, optimizing its ability to detect deepfake audio accurately."}, {"title": "C. Classification Result and Analysis", "content": "The original CNN model used in our experiments consists of two convolutional layers and two fully connected layers, encompassing a total of 3,373 parameters. To align with the parameter requirements of this classical model, our QNN blocks were designed to include 12 qubits. The Quantum-Train (QT) model incorporates QNN blocks, a mapping model, and a scaling model. The mapping model includes 301 parameters, while the scaling model comprises 8 parameters. The total number of trainable parameters in the QT model varies with the number of QNN blocks, ranging from 456 to 1,464 parameters as the number of QNN blocks increases from 12 to 96. This corresponds to a parameter ratio ranging from 13.5% to 43.4% relative to the classical model, demonstrating the QT model's ability to significantly reduce the parameter count while still maintaining effective performance.\nThe performance comparison between the QT-CNN method and the classical training approach for Deepfake Speech Recognition highlights the strengths of the QT framework. The results indicate that QT maintains consistently high accuracy during both training and testing phases across various QNN block configurations. Despite the reduction in the number of trainable parameters, QT's performance closely aligns with that of classical models, showcasing its robustness and reliability. As the number of QNN blocks increases, the QT model preserves accuracy with only minor deviations from classical benchmarks, reinforcing its capability to achieve parameter reduction without a substantial compromise in performance.\nThe secondary y-axis of the analysis figure depicts the parameter ratio, revealing a clear trend of increased parameter efficiency as the number of QNN blocks grows, reaching up to 70% efficiency at 96 blocks. This substantial reduction in parameters while maintaining competitive performance is particularly advantageous in resource-constrained environments, such as edge computing or real-time processing scenarios. The results validate the QT framework as a scalable, resource-efficient alternative to traditional CNN training, positioning it as a promising tool for advancing hybrid quantum-classical models in AI and quantum computing applications."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "The results of this study underscore the significant potential of the QT framework in enhancing the performance and efficiency of neural networks, particularly in the context of deepfake audio detection. By integrating quantum computation during the training phase, the QT framework demonstrates the capability to significantly reduce the number of trainable parameters required by classical CNNs, without compromising on performance. The experiments reveal that QT maintains accuracy levels comparable to traditional methods across varying configurations of QNN blocks, even as it reduces the parameter count by up to 70%. This reduction is achieved through the unique parameter mapping and distributed circuit design, which leverage quantum properties to optimize weight generation for the CNN. These findings highlight the framework's ability to address critical scalability and resource constraints, making quantum-enhanced learning an attractive approach for tackling complex machine learning tasks, particularly where conventional methods fall short.\nIn conclusion, this work presents the QT-CNN framework as a novel and practical solution for enhancing deepfake detection and other applications requiring high computational efficiency and robustness. The distributed quantum-classical training approach not only circumvents the limitations of purely quantum models in the NISQ era but also extends the applicability of quantum-enhanced learning to real-world scenarios where inference can be performed on classical hardware. This capability is particularly valuable in domains such as climate change mitigation [17], [24], sustainable development [17], and beyond, where advanced machine learning models must operate efficiently under resource constraints. In practical quantum systems, this algorithm can also integrate with quantum error mitigation [32], distributed quantum computing [33], and Resource-efficient management in quantum HPC [34]. As quantum computing hardware continues to evolve, the QT framework's approach of parameter reduction and distributed circuit design positions it well for future advancements, driving further integration of quantum insights into classical AI methodologies. This work paves the way for more scalable and accessible quantum machine learning solutions, fostering broader adoption and unlocking new possibilities across diverse technological and scientific fields."}]}