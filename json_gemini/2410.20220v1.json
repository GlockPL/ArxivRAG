{"title": "Neural Fields in Robotics: A Survey", "authors": ["Muhammad Zubair Irshad", "Mauro Comi", "Yen-Chen Lin", "Nick Heppert", "Abhinav Valada", "Rares Ambrus", "Zsolt Kira", "Jonathan Tremblay"], "abstract": "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields and propose promising directions for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots depend on precise and compact representations of their environment to perform a wide array of tasks, from navigating busy warehouses to organizing cluttered homes or assisting in high-stakes search-and-rescue missions. At the core of a typical robotic pipeline is the synergy between perception and action. The perception system gathers sensory data from devices such as RGB cameras, LiDAR, and depth sensors and transforms them into a coherent model of the environment such as a 3D map that enables the robot to maneuver through dynamic, obstacle-rich spaces. The quality of this representation directly impacts the robot's decision-making or policy, which translates the perceived environment into actions, enabling it to avoid moving forklifts, pick up scattered objects, or plan a safe path in an emergency. Traditionally, robots have modeled their environments using data structures like point clouds [13-15], voxel grids [16], meshes [17-19], and Truncated Signed Distance Functions (TSDF) [20]. While these representations have advanced robotic capabilities, they struggle to capture fine geometric details, particularly in complex or dynamic environments, leading to suboptimal performance in adaptable scenarios.\nTo overcome these limitations, Neural Fields (NFs) [21] have emerged as a promising alternative, offering continuous, differentiable mappings from spatial coordinates to physical quantities like color or signed distance. Unlike traditional data structures, NFs can model 3D environments as continuous functions parameterized by neural networks or Gaussian distributions. This enables them to represent complex geometries and fine details more efficiently [22, 23]. NFs can be optimized using gradient-based methods with various types of real-world sensory data, including images and depth maps, to produce high-quality 3D reconstructions. In the realm of robotics, NFS provide several distinct advantages over traditional methods:\n\u2022 High-Quality 3D Reconstructions: NFs generate detailed 3D representations of environments, which are crucial for tasks like navigation, manipulation, and scene understanding [24-28].\n\u2022 Multi-Sensor Integration: NFs can seamlessly integrate data from multiple sensors, such as LiDAR and RGB cameras, providing a more robust and adaptable perception of the environment [29, 30].\n\u2022 Continuous and Compact Representations: Unlike voxel grids or point clouds, which are inherently discrete, NFs offer continuous representations that capture fine spatial details using fewer parameters, enhancing computational efficiency [22, 31].\n\u2022 Generalization and Adaptation: Once trained, NFs can generate novel viewpoints of a scene, even from previously unseen perspectives, which is particularly valuable for exploration or manipulation tasks. This ability is enabled by generalizable NeRF methods [32-34].\n\u2022 Integration with Foundation Models: NFs can be combined with foundation models like CLIP [35] or DINO [36], enabling robots to interpret and respond to natural language queries or other semantic inputs [37, 38]. Recent advances in generative AI [39] have further expanded the capabilities of NFs by leveraging synthetic data as supervisory signals, thereby reducing reliance on real-world observations. This paradigm shift allows NFs to be optimized in scenarios where real-world data collection is impractical or costly. Importantly, it positions NFs as a key link between generative AI and robotics. While generative priors from 2D data are powerful, they often lack the spatial coherence needed for effective robotic decision-making. NFs integrate these priors with sparse real-world data [33], enabling them to model sensory and motor spaces in scenarios constrained by physical environments, such as limited sensor configurations and occlusions.\nGiven these advantages, the application of NFs in robotics is a rapidly growing area of research. Figs. 1 and 2 provide an overview of NF applications in robotics and highlight the rise in NF-related robotics publications over time. In this paper, we aim to structure and analyze their impact on the field. The manuscript is organized as follows: Sec. II covers the formulation of NFs, while Sec. III highlights their benefits across various domains, categorized into distinct themes:\n\u2022 Pose Estimation, focuses on NF as a scene or object representation in camera pose estimation, object pose estimation, and Simultaneous Localization and Mapping (SLAM) (Sec. III-A).\n\u2022 Manipulation, discusses how NFs' accurate 3D reconstruction assists robots in manipulating objects (Sec. III-B).\n\u2022 Navigation, highlights the role of NFs in enhancing robotic navigation by enabling accurate and efficient perception of real-world environments (Sec. III-C).\n\u2022 Physics, explores how NFs enable robots to reason about physical interactions to improve their understanding of real-world dynamics (Sec. III-D).\n\u2022 Autonomous Driving, focuses on NFs' role in building photorealistic simulators for the real world (Sec. III-E).\nWe conclude by discussing several research directions and challenges in Sec. IV. To the best of our knowledge, this survey represents one of the first comprehensive examinations of Neural Fields in the domain of robotics. We complement the closest concurrent survey [40] that focuses on NeRFs by covering a comprehensive set of fields, including 3DGS, Occupancy, Signed Distance Fields, and beyond. By integrating insights from various dimensions, this survey aims to provide a holistic understanding of the current state of NF in robotics applications, highlighting recent achievements, upcoming challenges, and unexplored areas within robotics."}, {"title": "II. FORMULATION OF NEURAL FIELDS", "content": "We begin by defining several types of fields that are key to the formulation of Neural Fields. In mathematics and physics, a field is a descriptor of a quantity with a value assigned to every point in space and time. Formally, this is expressed as:\n\u2022 Scalar fields: A scalar field $f : R^n \\rightarrow R$ assigns a scalar value (e.g. temperature, pressure) to every point in an n-dimensional space. Mathematically, it is defined as $f(x, y, z)$ for a 3D space, where $(x, y, z)$ are the coordinates in space and $f$ is the scalar value at that point. Examples of scalar fields are the Occupancy Fields (Sec. II-A) and Signed Distance Fields (Sec. II-B).\n\u2022 Vector fields: A vector field $F: R^n \\rightarrow R^m$ is an extension of a scalar field that associates a vector (e.g., velocity or force) with every point in space. For example, in three dimensions, a possible formulation is given by $F(x, y, z) = (F_0(x, y, z), ..., F_{m-1}(x, y, z))$, where each element $F_i$ represents the vector values corresponding to the input coordinate $(x, y, z)$. This function can either be represented as a neural network (Sec. II-C1), a volumetric grid [41], or the mix of both [25].\nA. Occupancy Fields\nOccupancy represents the binary state of whether a point p in space is occupied by a surface S or not:\n$\\o(p) = \\begin{cases}\nS & \\text{o if p is inside S,}\\\\\n1 & \\text{if p is outside S.}\n\\end{cases}$\nThis idea can be extended to the continuous case, where occupancy is represented by the probability $o(p) \\in [0,1]$ indicating the likelihood of p being inside or outside the surface. Occupancy Networks [42] leverage this idea by learning a continuous function, or NF, which maps points $p\\in R^n$ to occupancy probabilities, conditioned on input observations x (e.g., point clouds or images). This function, $f_{\\theta}(p | x)$,"}, {"title": "C. Radiance Fields", "content": "Radiance Fields represent the distribution of light in 3D space, associating each point with a radiance value (light intensity and color) in every direction. This can be described using a function $L : R^3 \u00d7 S^2 \\rightarrow R^3$, where $L(p,w)$ gives the light radiance at point p in direction w, with p in 3D space and w on the unit sphere $S^2$ representing all possible directions.\n1) Neural Radiance Fields (NeRF): NeRFs [22] represent scenes as volumetric fields of density \u03c3 and RGB color c using a neural network. The weights of NeRFs are optimized per scene using input RGB images, and their camera poses. After training, the density field captures scene geometry, while the color field models the view-dependent appearance. A multilayer perceptron (MLP) with parameters \u0398 predicts the density \u03c3 and RGB color c for each point based on its 3D position x = (x, y, z) and viewing direction d. To address the spectral bias of neural networks in low-dimensional spaces [43], positional encoding \u03b3(\u00b7) is applied to inputs: $(\\sigma, c) \\leftarrow F_{\\theta}(\\gamma(x), \\gamma(d))$.\nTo render a pixel, a camera ray r(t) = o+td is cast from the camera center o in direction d. K points {xk = r(tk)}=1 are sampled along the ray and passed through the MLP to generate densities and colors {\u03c3k, ck}_1. These are then combined to estimate the pixel color $\\hat{C}(r)$ via volume rendering [44], using a numerical quadrature approximation [45]\n$\\hat{C}(r) = \\sum_{k=1}^{K} T_k (1 - exp(-\\sigma_k (t_{k+1} - t_k))) c_k$, with\n$T_k = exp(-\\sum_{k'<k} \\sigma_{k'} (t_{k+1}-t_k))$ can be interpreted as the probability that the ray successfully transmits to point r(tk). NeRF is trained by minimizing the photometric loss,\n$L_{photo} = \\sum_{r \\in R} |\\hat{C}(r) - C(r) |^2$ where C(r) is the observed RGB value for ray r in a sampled set of rays R.\nWhile vanilla NeRF achieves photorealistic results, it is time-consuming to train and render from a pretrained NeRF. To reduce these costs, several improvements are proposed: a) using encodings with better speed/quality trade-offs [46], b) adopting smaller neural networks to reduce memory- bandwidth demands [41, 47], and c) skipping ray marching steps in empty space to cut down the computational cost of neural volume rendering [48]. These optimizations accelerate NeRF training and inference by several orders of magnitude, enabling real-time use in time-sensitive applications.\n2) 3D Gaussian Splatting: 3D Gaussian Splatting [49] represents a scene as a collection of anisotropic 3D Gaussians, each defined by a mean vector $\\mu \\in R^3$, covariance matrix $\\Sigma \\in R^3$, color vector $c \\in R^3$, and opacity scalar \u03b1. The influence of a Gaussian on a point x \u2208 R\u00b3 is given by:\n$f(x; \\Sigma, \\mu) = exp(-(x \u2013 \\mu)^T \\Sigma^{-1} (x \u2013 \\mu))$,\nwhich quantifies each Gaussian's contribution based on spatial proximity. To render the scene onto a 2D plane, the 3D covariance \u03a3 is projected into 2D as $\\Sigma' = JWEWTJ^T$ [50], where W is the projective transformation and J is the Jacobian. The 2D mean vector \u03bc' is obtained via perspective projection Proj(\u03bc\u0395, \u039a), using the camera's extrinsic E and intrinsic K matrices. To ensure valid optimization of 3D covariance \u03a3, it is decomposed as \u03a3 = RSSTRT, where R and S are rotation and scaling matrices, respectively. Finally, the color of each pixel p in the image can be calculated as:\n$I(p) = \\sum_{i=1}^{N} f(p; \\mu'_i, \\Sigma'_i) \\alpha_i c_i \\prod_{j=1}^{i-1} [ 1 \u2013 f(p; \\mu'_j, \\Sigma'_j) \\alpha_j]$."}, {"title": "III. NEURAL FIELDS FOR ROBOTICS", "content": "In this section, we delve into the application of Neural Fields across five major areas of robotics: pose estimation, manipulation, navigation, physics, and autonomous driving (see Figs. 3 and 5 for a timeline and taxonomy of selected key NFs in robotics papers). Each subsection below highlights key works within these domains, providing a comprehensive overview of the state-of-the-art methods. We conclude each subsection with a discussion of the key takeaways and the open challenges that remain in these areas, offering insights into the future directions of research in NFs for robotics.\nA. Neural Fields for Pose Estimation\nNeural Fields have transformed pose estimation by offering robust and efficient methods to estimate the position and orientation of cameras and objects in 3D scenes. This section explores two key areas: camera pose estimation (Sec. III-A1) and object pose estimation (Sec. III-A2). Camera pose estimation focuses on determining the viewpoint of cameras, which is crucial for tasks like mapping and reconstruction. Alternatively, object pose estimation involves localizing and orienting objects within a scene, essential for applications like manipulation and interaction. NFs optimize these tasks through gradient-based techniques, either by refining scene representations or directly providing reliable features for pose estimation.\n1) Camera Pose Estimation: As discussed in Sec. I, NFs are differentiable, allowing gradient updates through scene representations, such as NeRF's volumetric rendering, down to camera parameters. While differentiable rendering has been applied to meshes [51, 52], we focus here on methods applicable to NeRF-like models. This section starts by examining techniques that rely on pre-optimized NeRFs. We then explore approaches that tackle simultaneous pose estimation and geometry reconstruction, concluding with a discussion on their impact on Simultaneous Localization and Mapping (SLAM).\nPose Estimation via Optimized Neural Fields: For localization, iNeRF [2] inverts an already optimized NeRF for the task of pose estimation. Starting with an image, iNeRF finds the translation and rotation of a camera relative to a pretrained NeRF by using gradient descent to minimize the residual between pixels rendered from an optimized NeRF. Parallel iNeRF [53] parallelized the optimization processes of 6DoF poses based on fast pretrained NeRFs. Lens [54] prevented the generation of novel views in irrelevant areas by choosing virtual camera positions based on the NeRF's internal 3D scene geometry. The rendered images were then used as synthetic data to efficiently train a camera pose regression model.\nSimultaneous Pose Estimation and Reconstruction: NeRF--[55] and BARF [96] show that given RGB observations of a scene, camera poses and NeRFs can be jointly optimized, removing the need for classical Structure- from-Motion (SfM) pipelines. The former initializes cameras to the origin for forward-facing scenes, while the latter employs a coarse-to-fine reconstruction scheme that gradually introduces higher frequency position encodings [97, 98]. This coarse-to-fine approach can also be adapted to multi-resolution grids like NGP [25] by applying a weighted schedule across resolution levels [99, 100].\nLocalRF [56] reconstructs long trajectories incrementally by adding images sequentially, using a subdivision approach similar to BlockNeRF [101], without relying on structure-from- motion (SfM). Notably, LocalRF uses different learning rates for translation and orientation parameters, highlighting the challenges in taming camera-optimizing NeRFs. GNeRF [102] proposes a pose-conditioned GAN to recover a NeRF. Others have explored more suitable techniques for pose optimization, such as Gaussian [103] or sinusoidal activations [104]. NoPe- NeRF [105] uses monocular depth priors to constrain the scene as well as relative pose estimates. Keypoint matches or dense correspondences can also be used to constrain the relative pose estimates using ray-to-ray correspondence losses [106, 107]. DBARF [108] proposes using low-frequency feature maps to guide the bundle adjustment for generalizable NeRFs [32, 93, 109]. Nerfels [110] combines invertible neural rendering with traditional keypoint-based camera pose optimization.\nVarious works that apply NFs for localization and pose estimation utilize NeRF's internal features to establish 2D-3D correspondences [111, 112], remove the need for an initial pose estimate [113], augment the training set of the pose regressor with a few-shot NeRF [114], or apply a decoupled representation of pose along with an edge-based sampling strategy to enhance the learning signal [115]. They also address dynamic scenes by integrating geometric motion and segmentation for initial pose estimation, combined with static ray sampling to speed up view synthesis [116].\nSimultaneous Localization and Mapping (SLAM): Jointly optimizing camera poses and neural scene representations is a fundamental aspect of the SLAM (Simultaneous Localization and Mapping) problem. Recent advancements in NFs have reshaped SLAM by leveraging their qualities, such as the ability to model continuous surfaces, lower memory usage, and enhanced robustness against noise and outliers. For instance, iMap [117] utilizes a single MLP to predict radiance fields as the mapping representation, employing parallel tracking and mapping threads where the tracking thread optimizes the pose of the input RGB-D frame, and the mapping thread refines both the MLP and camera pose for selected keyframes. NICE-SLAM [118] (see Fig. 7) further enhances this approach by replacing the single MLP with hierarchical feature grids, resulting in faster inference and more accurate reconstructions. Gaussian splatting-based SLAM systems exploit the advantages of 3DGS including faster runtime and improved photorealistic rendering to further enhance the performance [119-121] (see Fig. 6). Besides better reconstruction quality, NF-based methods also provide an easier way to store various semantic information. Various semantic SLAM systems [3, 122-124] use NFs as unified representations to represent diverse information of the environment. For a more thorough survey on NFs for SLAM, we refer readers to Tosi et al. [125].\n2) Object Pose Estimation: NFs have also been employed for localizing and orienting objects within a scene. Accurate pose estimation is crucial for robotics, as it enhances a robot's ability to interact with its surroundings and perform tasks such as manipulation, navigation, and object recognition. Works in this domain use NFs' features to establish correspondences or directly regress poses and reconstruct shapes. This facilitates the determination of bounding boxes and orientations for various objects in a 3D environment.\nNeural Field features are also shown to be effective for multi-view 3D bounding box estimation of objects in the scene. NeRF-RPN [1] estimates 3D object boxes directly on NeRF's feature grid, using a novel voxel representation and without re-rendering from a pretrained NeRF. It can be trained end- to-end to estimate high-quality 3D bounding boxes without class labels. Similarly, NeRF-Det [57] (see Fig. 9) leverages a NeRF to explicitly estimate 3D geometry and improve detection performance. It introduces geometry priors and connects detection with NeRF branches through a shared MLP, enhancing generalizability and efficiency without per-scene optimization. NeRF-RPN's performance can be further enhanced with self- supervised representation learning directly using NeRF grids, as shown by NeRF-MAE [126]. Similarly, Gaussian splats [49] have also been effectively utilized for 3D object detection, as demonstrated by GaussianDet [127] and 3D-GSDet [128]. Furthermore, NeRF-loc [59] employs a transformer-based framework to extract labeled, oriented 3D bounding boxes of objects from NeRF scenes. It utilizes a pair of parallel transformer encoder branches to encode both the context and details of target objects, fusing these features with attention layers for accurate object localization, outperforming conventional RGB(-D) based methods. NeRF-pose [129] employs a weakly supervised 6D pose estimation pipeline that requires only 2D segmentation and known relative camera poses during training, avoiding the need for precise 6D pose annotations. It reconstructs objects from multiple views, then trains a pose regression network to predict 2D-3D correspondences with a NeRF-enabled Perspective-n-Point (PnP)+RANSAC algorithm estimating stable and accurate poses from a single input image. Other direct pose-regression methods also simultaneously reconstruct object shapes in conjunction with object pose estimation. ShAPO [130], FSD [131] and CARTO [132] jointly reconstruct object shapes and regress their 6D object poses using a single-shot pipeline employing implicit representations and disentangled shape and appearance priors. UPNeRF [133] proposes a unified framework for monocular 3D reconstruction that integrates pose estimation with NeRF-based reconstruction, addressing the shortcomings of existing methods that rely on external 3D object detectors for initial poses. It decouples dimension estimation and pose refinement to resolve scale-depth ambiguity, and employs a projected-box representation for cross-domain generalization. NeRF-from-image [134] integrates NeRF with GANs to model arbitrary topologies without requiring accurate ground-truth poses or multiple views during training. It uses an unconditional 3D-aware generator and a hybrid inversion scheme to recover an SDF-parameterized 3D shape, pose, and appearance, refining initial solutions via optimization. NCF [135] estimates the 6DoF pose of a rigid object with a 3D model from a single RGB image by predicting 3D object coordinates at 3D query points sampled in the camera frustum rather than at image pixels. Bundle-SDF [58] (see Fig. 8) tracks the 6-DoF pose of an unknown object from a monocular RGBD video sequence while simultaneously performing neural 3D reconstruction. It handles arbitrary rigid objects with minimal visual texture, requiring only the object's segmentation in the first frame. The approach uses a Neural Object Field learned alongside pose graph optimization to build a consistent 3D representation of the object's geometry and appearance.\n3) Takeaways and Open Challenges in Neural Fields for Pose Estimation: Despite the promising progress in NFs for pose estimation, several open challenges remain. Current methods prove effective in real-time pose estimation of cameras as well as objects. While significant progress has been made in pose estimation for static scenes, there is still room for further exploration in dynamic environments. Future work could focus on refining methods for recovering camera poses from dynamic video capture, where both cameras and objects exhibit significant movement, such as post-hoc calibration and labeling of robotic datasets [136]. This would open up the possibility of learning 3D priors from large-scale monocular videos. Another avenue for future work could explore using NFs for open-vocabulary 6D object pose estimation and solving the canonicalization problem of large-scale datasets [137]."}, {"title": "B. Neural Fields for Robotic Manipulation", "content": "One of the key challenges in robotic manipulation is obtaining a precise geometric representation of both the objects and the environment involved in the task. An effective representation must also capture the environment dynamics, offering a robust 3D understanding of the objects. In this section, we explore the application of NFs in control tasks for manipulation, with a focus on pick-and-place scenarios within 3 and 6 Degrees of Freedom (DoF). A summary of methods leveraging NFs for manipulation is provided in Table I. Approaches that synthesize 3-DoF grasps use visual input, like RGB or depth images, to generate grasps in the image frame [71]. This means the end effector can translate horizontally and rotate around its vertical axis, with depth sensors determining vertical positioning. However, 3-DoF grasping lacks precise orientation control. In contrast, 6-DoF methods predict both position and orientation using 3D representations, enabling full control over roll, pitch, and yaw, which enhances the robot's ability to manipulate objects in any direction. Within the scope of 3-DoF, Dex-NeRF [63] (see Fig. 11) uses NFs to detect and infer the geometry of transparent objects, employing a transparency-aware rendering technique and additional lighting for specular reflections. Combined with Dex-Net [138], it generates 3-DoF grasping poses for transparent objects in both simulated and real environments. More recently, Evo-NeRF [65] extends this by leveraging Instant-NGP [25] to accelerate inference and adapt NeRF weights for sequential grasping tasks on transparent objects, updating the representation with each grasp. Recently, there has been a shift toward using NFs for 6-DoF grasp pose estimation, offering an alternative to traditional point cloud-based methods. Below, we discuss these representations in detail:\n1) Occupancy Fields: Neural Descriptor Fields (NDF) [60] propose an SE(3)-equivariant object representation for manipulating novel objects in arbitrary poses, with few demonstrations. Using a Vector Neurons (VN) [146] network, 6-DoF relative poses between objects and grippers are encoded. NDF represents objects as continuous 3D descriptor fields, mapping points to descriptor vectors that capture spatial relationships to object geometry. However, NDF struggles with generalizing to new object categories, a limitation addressed by Local Neural Descriptor Fields (L-NDF) [61], which use a voxel grid of local embeddings to better capture local geometry and descriptors for new shapes. Both NDF and L-NDF rely on VN equipped with an Occupancy Network (ONet). GIGA [62] leverages a Convolutional Occupancy Network (ConvONet) to detect 6-DoF grasps in cluttered environments from a single depth image. By encoding the Truncated Signed Distance Function (TSDF), GIGA jointly predicts volumetric occupancy and 6-DoF grasp detection, enabling it to detect grasps on occluded objects from partial observations.\n2) Radiance Fields: These fields are primarily modeled using two approaches: NeRFs and 3D Gaussian Splatting. NeRF: NeRF-Supervision [140] leverages NeRFs to generate synthetic data for dense correspondence estimation by treating correspondences as depth distributions instead of pixel-wise depth. This enables 6-DoF picking tasks on thin and reflective surfaces using only RGB images, though multi-view images are required to build the NeRF representation. MIRA [141] extends this by constructing a NeRF before each action, enabling pick-conditioned placing via view synthesis. MIRA also trains a NeRF model with perspective cameras for direct orthographic rendering, which aligns well with translationally equivariant architectures like ConvNets. SPARTN [66] enhances visual grasping policies by using synthetic multi-view RGB images from eye-in-hand camera setups, significantly improving success rates in grasping tasks over standard imitation learning methods. These advancements highlight NeRF's potential to bridge the gap between RGB-based and depth-based robotic policies. Blukis et al. [144, 145] proposed an approach that jointly optimizes 3D reconstruction and grasp pose estimation by encoding objects into a unified latent representation. Encoded latents are decoded for view synthesis, 3D reconstruction, and grasp proposals. NeRFs have also been adapted for transfer learning in 6-DoF grasp pose evaluation and optimization with MVNeRF [142], which processes inputs from multiple scenes, enabling a more generalized representation and faster perception-to-action mapping. RGBGrasp [143] advances real- time applications by integrating multi-view RGB data from an eye-on-hand camera and depth maps from a pre-trained model. It further accelerates 3D reconstruction using hash- encoding [25] and a novel sampling strategy.\n3D Gaussian Splatting: The introduction of 3D Gaussian Splatting (Sec. II-C2) marks a promising advancement in leveraging 3D representations for real-time robotic manipulation. GaussianGrasper [64] proposes a novel approach to 6-DoF grasping using Gaussian Splatting for open-vocabulary object grasping, thus enabling robots to understand and execute tasks based on natural language instructions. Similarly, ManiGaussian [67] builds on dynamic 3D Gaussian Splatting to capture scene-level spatiotemporal dynamics, enhancing the robot's capability to execute tasks conditioned on natural language instructions. SplatSim [147] shows an application of 3DGS for improving Sim2Real transfer for robotic manipulation policies that rely on RGB images. This is obtained by leveraging the photorealism of 3DGS, which reduces the domain shift between synthetic and real visual information.\n3) Signed Distance Fields: GraspNeRF [68] (see Fig. 12) extends NeRF for 6-DoF grasp detection of transparent and specular objects. It integrates a Truncated Signed Distance Function (TSDF) with a generalizable NeRF trained on sparse RGB images for zero-shot scene reconstruction. Similarly, Volumetric Grasping Network (VGN) [71] enables real-time 6-DoF grasp detection, synthesizing collision-free grasps in cluttered environments using a 3D voxel grid representation where each voxel contains the TSDF to the nearest surface. Song et al. [139] employ a TSDF to map actions to rendered views, simulating future state-action pairs for 6-DoF grasping. NeuralGrasps [69] further explores neural distance fields by learning implicit representations for grasps with multiple robotic hands. This method encodes grasps into a shared latent space, with each vector corresponding to a grasp from a specific hand. The neural Grasp Distance Field (NGDF) [70] models grasp poses as the level set of an unsigned distance field, predicting the closest valid grasp for a given 6D query pose by minimizing the unsigned distance. CenterGrasp [148] extends this concept to directly predict a displacement vector, removing the need to optimize the level-set.\n4) Feature Fields: Feature fields represent an emerging class of NFs that integrate high-dimensional features from visual data into a unified 3D representation. These fields can map 3D points to feature vectors encoding semantic information, making them useful for context-aware grasping tasks when combined with pre-trained vision-language models. Developments in this area have focused on the creation of feature fields that enable few-shot learning and zero-shot task- oriented grasping. Distilled Feature Fields (DFF) [4] proposes to distill dense features from a pre-trained vision-language model (CLIP [35]) into a 3D feature field (see Fig. 10). This allows for effective generalization across diverse object categories, making DFF particularly useful for tasks that require contextual understanding. Similarly, LERF-TOGO [5] leverages language embeddings and DINO [36] features to accurately select target objects and specific object parts for grasping. This intuition addresses the limitations of traditional learning- based grasp planners that often ignore the semantic properties of objects. The use of vision-language models for feature distillation is also proposed by GeFF [149] and GNFactor [72], where a generalizable NeRF enriched by semantic information is adopted for manipulation and navigation. In conclusion, by incorporating semantic information directly into the 3D representation, feature fields facilitate precise, context-aware, language-guided manipulation in real-world scenarios.\n5) Visual & Tactile Sensing: The use of NFs in multi- modal visual and tactile sensing is a recent and emerging area of research. Tactile data gathered from tactile sensors offers information about contact force and contact geometry. Combining visual and tactile sensing offers several advantages in situations where vision-only might be ambiguous, such as in the presence of occlusions, challenging lighting, or reflective and transparent materials [73, 74]. In the context of multimodal sensing, NFs are mainly leveraged for tactile data generation and object reconstruction. Zhong et al. [150] propose to use NeRF to generate realistic tactile sensory data. Similarly, TaRF [151] leverages NeRF to synthesize novel views, which are subsequently used by a conditional diffusion model to generate the corresponding tactile signal. A challenge in using tactile data is the disparity between real and simulated tactile images. TouchSDF [152] addresses this sim-to-real gap by combining a Convolutional Neural Network (CNN) with DeepSDF for 3D shape reconstruction from tactile inputs. As a result, objects can be reconstructed using solely tactile sensing in both simulation and the real world. Suresh et al. [153] employ a neural signed distance field to estimate object pose and shape during in-hand manipulation. The use of NFs, in this context, allows the robot to learn and progressively refine the object's shape online. Moreover, NFs and visuo-tactile sensing can be employed for predicting extrinsic forces. Neural Contact Fields (NCF) [154] tracks the contact points between an object and its environment using tactile information. This method leverages NFs to generalize across different object shapes and to estimate the probability of contact at any point on an object's surface. Although most studies focus on implicit representations, recent techniques have begun exploring explicit approaches. Tactile-3DGS [73] and Touch-GS [74] extend 3D Gaussian Splatting by integrating visual and tactile data for 3D object reconstruction. Unlike TouchSDF, both Touch-GS and Tactile- 3DGS are designed to handle the reconstruction of transparent and reflective objects. These methods have been validated in both simulated and real-world environments.\n6) Diffusion Models: Recent work in Generative AI for robotics manipulation has explored the use of diffusion models for grasp generation, trajectory planning, and learning viewpoint and cross-embodiment invariant policies. Yoneda et al. [155] leverage diffusion models to predict stable object placements by learning context-dependent distributions from positive examples, eliminating the need for rejection sam- pling. SE(3)-Diffusion Fields [75] optimize grasp selection and trajectory generation by learning data-driven SE(3) cost functions using diffusion models. Meanwhile, VISTA [77] and RoVi-Aug [76] utilize 3D generative models for learning viewpoint-invariant policies, enabling robust generalization to new environments and unseen robots. VISTA [77] leverages Zero-NVS [34]'s zero-shot novel view synthesis capability to learn viewpoint-invariant policies, enabling robust performance in diverse environments and tasks from limited demonstration data. Similarly, RoVi-Aug [76] synthesizes augmented robot data using image-to-image generative models, allowing for zero- shot deployment on unseen robots with different embodied and largely varying camera angles. Together, these methods illustrate how 3D generative techniques can significantly improve the adaptability and effectiveness of robotic manipulation systems in real-world scenarios.\n7) Takeaways and Open Challenges in Neural Fields for Manipulation: NFs have emerged as powerful techniques for robust 3D understanding in robotic manipulation tasks, such as grasping and pick-and-place. These representations capture detailed geometrical information and support generalization across diverse object shapes and categories. NFs have also been employed to identify optimal grasp points, improving the success rate of robotic grasps in cluttered environments. Additionally, some methods integrate these representations with language models, enabling open-vocabulary manipulation through natural language instructions. Despite these advancements, significant challenges remain. Current approaches rely on extensive multi-view inputs or costly per-scene optimization, limiting their applicability in complex, dynamic, or unstructured environments. Furthermore, incorporating physical intuitions about object affordances and robot dynamics into the learned representations could lead to more physically grounded manipulation policies (see Sec. III-D). Finally, scaling these methods to dynamic scenes with multiple agents or articulated objects is an ongoing challenge that must be addressed for real-world deployment."}, {"title": "C. Neural Fields for Navigation", "content": "Autonomous navigation requires robots to perceive and model their surroundings effectively to plan collision-free paths. Traditional learning-based approaches tackle this challenge via either end-to-end [156\u201315"}]}