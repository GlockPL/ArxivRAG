{"title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP", "authors": ["Danlu Chen", "Freda Shi", "Aditi Agarwal", "Jacobo Myerston", "Taylor Berg-Kirkpatrick"], "abstract": "Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor-intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription-this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing. This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses. Data and code are available at https://logogramNLP.github.io/.", "sections": [{"title": "1 Introduction", "content": "The application of computational techniques to the study of ancient language artifacts has yielded exciting results that would have been difficult to uncover with manual analysis alone (Assael et al., 2022). Unsurprisingly, one of the biggest challenges in this domain is data scarcity, which, in turn, means that transferring from pre-trained systems on well-resourced languages is paramount. However, it is more challenging to adopt similar techniques for ancient logographic writing systems, in which individual symbols represent entire semantic units like morphemes or words.\n The challenges associated with NLP for ancient logographic languages mainly come from two aspects. First, for many ancient languages, most available data sources are in visual forms, consisting of untranscribed photographs or hand-drawn copies (i.e., lineart). Adopting the conventional NLP pipeline, which requires converting visual representations into symbolic text, is therefore not straightforward: automatic transcriptions are often noisy due to data scarcity, while manual transcriptions are labor-intensive and require domain expertise. Some logographic writing systems, such as Old Chinese, even include symbol inventories that remain not fully mapped to Unicode (depicted in Figure 1).\n Second, even when perfect Unicode transcriptions are available, their symbol inventories are often mutually exclusive with those of high-resource languages, which can substantially reduce the effectiveness of transfer from pre-trained multilingual encoders, such as mBERT (Devlin et al., 2018). One processing step that might be used to mitigate this issue is latinization of the Unicode transcripts (Rust et al., 2021; Muller et al., 2020). However, it is challenging to Latinize logographic languages due to uncertain pronunciations (Sproat and Gutkin, 2021) and the resulting inconsistent latinization schemes across artifacts from the same language and writing system. Such a process is laborious\u2014humanists may devote months or even years to determine the correct transliteration. In contrast, once a correct transliteration is determined, translation into another language may only take minutes.\n Fortunately, advances in visual encoding strategies for NLP tasks offer an alternative solution. Recent studies have investigated NLP systems that model text in the pixel space (Rust et al., 2023; Tschannen et al., 2023; Salesky et al., 2023), thereby opening new possibilities for the direct use of visual representations of ancient logographic writing systems. These approaches, to date, have primarily been applied to digitally rendered texts. They have not yet been extensively evaluated on handwritten texts, such as lineart, i.e., neatly hand-copied versions of texts by scholars.\n In this paper, we attempt to answer the following questions: (1) Can we effectively apply NLP toolkits, such as classifiers, machine translation systems, and syntactic parsers, to visual representations of logographic writing systems? (2) Does this strategy allow for better transfer from pre-trained models and lead to better performance? Additionally, as shown in Figure 1, many logographic languages have multiple partially processed representations, including artifact photographs, hand-copied lineart, Unicode, Latin transliteration, and normalization- we also aim to empirically investigate the extent to which various representations at each stage, including textual and visual modalities, facilitate effective fine-tuning of downstream NLP systems.\n We have curated LogogramNLP, a benchmark consisting of four representative ancient logographic writing systems (Linear A, Egyptian hieroglyphic, Cuneiform, and Bamboo Script), along with annotations for fine-tuning and evaluating downstream NLP systems on three tasks, including three attribute classification tasks, machine translation, and dependency parsing.\n We conduct experiments on these languages and tasks with a suite of popular textual and visual encoding strategies. Surprisingly, visual representations perform better than conventional text representations for some tasks (including machine translation), likely due to visual encoding allowing for better transfer from cross-lingual pre-training. These results highlight the potential of visual representation processing, a novel approach to ancient language processing, which can be directly applied to a larger portion of existing data."}, {"title": "2 Dataset: Languages, Tasks and Challenges", "content": "Our benchmark consists of four representative ancient languages-Linear A, Egyptian hieroglyphic, Cuneiform, and Bamboo script (\u00a72.1).\u00b9 Each language is associated with a unique writing system and unique challenges. We refer the readers to Appendix A for data collection and cleaning details. Our benchmark covers three tasks: machine translation, dependency parsing, and attribute classification (\u00a72.2)."}, {"title": "2.1 Logographic Languages", "content": "A major characteristic of logographic languages is that the size of symbol inventories is significantly larger than that in alphabetic languages such as Ancient Greek (24 letters) or Modern English (26 letters). A summary of different representations of the languages of our interest is shown in Figure 2, and Table 2 summarizes the current status of each language.\n Linear A. Linear A is an undeciphered language used by the Minoan at Crete and is believed to be not related to ancient Greek. Scholars have differentiated the glyphs and carefully hand-copied them into linearts. We collected a dataset of 772 tablets (i.e., manually drawing) from SigLA.\u00b2 Each tablet also has a separable glyph with annotated Unicode.\n Akkadian (Cuneiform). CuneiML (Chen et al., 2023) is a dataset that contains 36k entries of cuneiform tablets. Each tablet consists of Unicode Cuneiform, lineart, and transliteration. We also use the Akkadian Universal Dependencies (UD) dataset (Luukko et al., 2020), which contains 1,845 sentences with dependency annotations. Since the UD annotation of Akkadian only keeps the normalization form of the language, we obtain the Unicode by running a dynamic programming-based matching algorithm.\n Ancient Egyptian (Hieroglyph). We segmented the St Andrews Corpus (Nederhof and Berti, 2015)\u00b3 using a rule-based segmenter, and obtained 891 examples of parallel data. Additionally, we collected data from the Thot Sign List (TSL; English translation)\u2074 and BBAW (German translation) for 2,337 and 100,736 samples of parallel data, respectively. However, the transliteration standards differ among these three sources of data, and BBAW does not include hieroglyph image features. Therefore, we only used TSL's data.\n Old Chinese (Bamboo script). We collected 13,770 pieces of bamboo slips from Kaom,\u2076 which come with the photograph of each line of the text. The Baoshan collection covers three genres: Wenshu (Document), Zhanbu (Divine), and Book. The Guodian collection contains parallel data translated into modern Chinese. The vocabulary size is 1,303. Notably, about 40% of the characters do not have a Unicode codepoint and are, therefore, represented as place-holder triangles or circles. This dataset does not come with human-labeled latinization due to the lack of transliteration standards."}, {"title": "2.1.1 Visual Representations", "content": "Since ancient scripts did not consistently adhere to a left-to-right writing order, breaking down multi-line documents into images of single-line text is nontrivial. These historical data, therefore, need additional processing to be machine-readable. Figure 3 shows examples of different processing strategies. We summarize the approaches we used in building the dataset as follows:\n1. Raw image (no processing): the raw images are already manually labeled and cut into text lines of images, and no extra processing is required.\n2. Montage: we generate a row of thumbnails of each glyph using the montage tool in ImageMagick.\u2077 This strategy is used for Linear A, as the original texts are written on a stone tablet, and scholars have not determined the reading ordering of this unknown script.\n3. Digital rendering: we digitally render the text using computer fonts when the language is already encoded in Unicode. Given that most ancient logographic scripts are still undergoing the digitization process, this option is currently unavailable except for Cuneiform."}, {"title": "2.1.2 Textual Representations", "content": "The processing of textual features for ancient logographic scripts also requires special attention. Unlike modern languages, ancient logographic writing systems can have multiple latinization standards or lack universally agreed-upon transcription standards. For example, the cuneiform parsing data is not in standard transliteration (ATF) form, but rather, in the UD normalized form. This mismatch introduces extra difficulty to downstream tasks, especially in low-resource settings.\n A similar issue also exists for Old Chinese: most ancient characters do not even exist in the current Unicode alphabet. While we may find some modern Chinese characters that look similar to the ancient glyphs, they are usually not identical, and such a representation loses information from the original text.\n For Egyptian hieroglyphs, most characters are encoded in Unicode, but there is no standard encoding for \"stacking\u201d multiple glyphs vertically (Figure 3). Therefore, we do not include the Unicode text for our ancient Egyptian data as they are not available."}, {"title": "2.2 Tasks", "content": "Our benchmark covers three tasks (Table 1): translation, dependency parsing, and attribute classification. The model performance on these tasks reflects various aspects of ancient language understanding. To better understand the information loss when using a pipeline approach, we also report performance using this method: predicting the transliteration first and using the noisy predicted transliteration for downstream tasks.\n Machine translation. The task is to translate the ancient languages, represented by either text or images, into modern languages, such as English. In all of our experiments, we translate ancient languages into English.\n Dependency parsing. Given a sentence in the ancient language, the task is to predict the dependency parse tree (Tesni\u00e8re, 1959) of the sentence. In the dependency parse tree, the parent of each word is its grammatical head.\n Attribute classification. The task is to predict an attribute of the given artifact, for example, provenance (found place), time period, or genre."}, {"title": "3 Methods", "content": "In this section, we will describe feature encoding methods (\u00a73.1) for both visual and textual inputs, as well as task-specific layers (\u00a73.2) for each task we consider."}, {"title": "3.1 Feature Encoding", "content": "NLP for Low-resource languages has benefitted a lot from pre-trained models. However, modern pre-trained models do not cover the character inventories of the considered ancient logographic languages. To overcome this shortage, we summarize solutions to the problem into four categories and describe them as follows.\n Extending vocabulary. In this line of approach (Wang et al., 2020; Imamura and Sumita, 2022), the vocabulary is extended by adding the unseen tokens. The embeddings of new tokens can be either initialized randomly or calculated by a function.\n Latin transliteration as a proxy. The majority of past work on cross-lingual transfer has focused on using Latin transliteration as the proxy to transfer knowledge from high-resource to low-resource languages (Pires et al., 2019; Fang et al., 2020). Following this line of work, we input latinization representations to mBERT (Devlin et al., 2018) to obtain the embeddings of the ancient languages.\n Tokenization-free. The idea of the tokenization-free approach is to view tokens as a sequence of bytes and directly operate on UTF-8 codepoints without an extra mapping step. As representative models, ByT5 (Xue et al., 2022) and CANINE (Clark et al., 2022) use Unicode encoding of a string to resolve the cross-lingual out-of-vocabulary issues. This work uses ByT5 for machine translation and CANINE for classification.\n Pixel Encoder for Text. Recently, there has been a novel approach (Rust et al., 2023) that aims to resolve the disjoint-character-set problem by rendering text into images and then applying a standard image encoder, such as the Vision Transformer with Masked Autoencoder (ViT-MAE) (He et al., 2022), to encode the features. In this work, we use PIXEL (Rust et al., 2023), a pixel-based language model pre-trained on the Common Crawl dataset with a masked image modeling objective, to encode the visual text lines for ancient languages. Additionally, we use PIXEL-MT (Salesky et al., 2023), a pixel-based machine translation model pre-trained on 59 languages, for the machine translation task.\n Full Document Image Encoding. When the images of ancient artifacts are available (e.g., for Linear A and Cuneiform), we can encode the full-document images directly. We use ResNet-50 (He et al., 2016) as the backbone model for full-document image inputs."}, {"title": "3.2 Task-Specific Layers", "content": "Machine translation. After encoding the input to vectors, machine translation requires a decoder to generate sequential outputs. Encoder-decoder models, such as T5 (Raffel et al., 2020), ByT5, PIXEL-MT, and BPE-MT (Salesky et al., 2023), use 3/6/12 layers of Transformer blocks as the decoders. For Encoder-only models, such as (m)BERT or PIXEL, we attach a GPT2 model (Radford et al., 2019) as the decoder to produce sequential output. Among the aforementioned models, T5, ByT5, and PIXEL are pre-trained on large-scale text corpora such as the Common Crawl; PIXEL-MT and BPE-MT are pre-trained on 1.5M pairs of sentences of 59 modern languages; PIXEL-MT is an encoder-decoder model with a 6-layer Transformer encoder and a 4-layer Transformer decoder.\n Classification. We attach a two-layer ReLU-activated perceptron (MLP) with a hidden size of 512 to the encoder for all classification tasks. The MLP outputs the predicted distribution over the candidate classes.\n Dependency Parsing. After encoding, we use the deep bi-affine parser (Dozat and Manning, 2017) for dependency parsing, which assigns a score to each possible dependency arc between two words. We use the minimum spanning tree (MST) algorithm during inference to find the best dependency tree for each sentence."}, {"title": "4 Experiments and Analysis", "content": "We describe our general model fine-tuning approach in \u00a74.1 and analyze model performance on the aforementioned tasks in the succeeding subsections."}, {"title": "4.1 General Experimental Setup", "content": "We use the Huggingface Transformers library (Wolf et al., 2020) in all experiments, except for machine translation, where we use the PIXEL-MT and BPE-MT models. We modified code and model checkpoints provided by Salesky et al. (2023) based on fairseq (Ott et al., 2019) for the two exceptions. We use Adam (Kingma and Ba, 2015) as the optimizer for all models, with an initial learning rate specified in Table 3. We use early stopping when the validation loss fails to improve for ten evaluation intervals (1000 iteration per interval). For data without a standard test set, we run a fixed number of training iterations and report the performance on the validation set after the last iteration. All experiments are conducted on an NVIDIA-RTX A6000 GPU, and the training time ranges from 2 minutes to 50 hours, depending on the nature of the task and the size of the datasets. Unless otherwise specified, all parameters, including those in pre-trained models, are trainable without freezing. We summarize other configurations in Table 3."}, {"title": "4.2 Machine Translation", "content": "We compare the performance of the models on machine translation, where we translate ancient Egyptian (EGY), Akkadian (AKK), and Old Chinese (ZHO) into English (Table 4a). We find that the PIXEL-MT model consistently achieves the best BLEU score across the three languages, outperforming the second-best method by a large margin. Models with pre-training do not always outperform those trained from scratch (Gutherz et al., 2023). We find that all models that take textual (Unicode or latinized) input achieve worse performance than models trained from scratch with the same type of textual input, suggesting that the lack of overlap in symbol inventories poses a serious problem for cross-lingual transfer learning. Our results indicate that choosing the correct input format is crucial to achieving the full advantage of pre-training.\n In addition, the PIXEL-MT model, pre-trained on paired data in modern languages (TED59), significantly outperforms PIXEL + GPT2 (pre-trained with masked language modeling) across the board. Another model, BERT-MT, which is further pre-trained on the same parallel text (TED59) with BERT initialization, also achieves comparable performance. These results emphasize the importance of pre-training on modern paired data, empirically suggesting that the PIXEL encoder with parallel text pretraining is an effective combination for ancient logographic language translation."}, {"title": "4.3 Attribute Classification", "content": "Table 4b summarizes the performance of attribute classification with different features and models. As expected, the image features can work fairly well for some of these attribute classification tasks as many of the relevant features are visual (e.g., for time and location); but, are not generally as effective as textual input representations. By comparing BERT with latinized input and CANINE on Unicode, we find that when both accurate latinization and Unicode representations are available, latinization is the most informative feature-with the exception of time period classification for Akkadian. This exception is aligned with our understanding of Akkadian, as different Cuneiform characters are used across different time periods. Thus, in this case, Unicode can provide more clues for determining the time period of a sample. Note that the label distribution is not balanced for most ancient language attribution tasks. For more details, refer to Chen et al. (2024)."}, {"title": "4.4 Dependency Parsing", "content": "We compare the dependency parsing performance of models with visual and textual encoders (Table 5).\u00b9\u2070 While all models achieve quite high parsing accuracy, we find that models with visual encoders perform the best on both investigated corpora (RIAO and MCONG). During training, models taking visual input generally converge faster than their textual counterparts, which is in line with prior work (Salesky et al., 2023) that uses visual features for machine translation."}, {"title": "5 Ablation Study on OCR and Image Quality", "content": "As mentioned earlier, the majority of data from ancient times remain in the form of photographs. We first closely examine two different visual input representations for the ZHO-EN translation task, handcopied figure and photograph (\u00a75.1). Next, we examine OCR performance on ancient logographic languages to gain better understanding of this bottleneck for current NLP pipelines (\u00a75.2)."}, {"title": "5.1 Handcopy v.s. Raw Image", "content": "For the ZHO-EN translation data, we have access to both photographs of the bamboo slips and handcopied textline figures (see the Bamboo script example in Figure 3 for reference). As shown in Table 6, the quality of the visual features significantly influences the translation accuracy-translations derived from photographs yield a low BLEU score of 2.09, whereas handcopied figures, which typically provide clearer and more consistent visual data, result in a higher BLEU score of 5.45. This result suggests that for models that perform implicit OCR as part of the translation process, the clarity of the source material is paramount."}, {"title": "5.2 Text Recognition Study", "content": "We simplify the task of transcribing ancient texts by starting with lines of text that have been accurately segmented. For datasets that include glyph-level annotations, we employ glyph classification to recognize the text. Results. The line-level OCR performance for the four languages is presented in Table 7. When comparing digital renderings of text to handwritten samples, it is evident that Old Chinese (ZHO) achieves a CER of 71.85, while Linear A has a CER of 57.17. As shown in Figure 5, glyph classification for ZHO is approximately 20% less accurate than line-level OCR, indicating that contextual features significantly aid in recognizing glyphs. Furthermore, there is a rapid increase in error rate as the number of glyphs increases, highlighting the intrinsic challenge of processing logographic languages, which typically have a large symbol inventories, and their frequency distribution often follows a long-tail pattern (see the orange bars in Figure 5). Therefore, developing robust visual models that can effectively leverage visual features is crucial for improving NLP on ancient logographic languages."}, {"title": "6 Related work", "content": "Because ancient languages are often low-resource, they present challenges that are closely related to other domains of NLP, such as low-resource machine learning and multi-lingual transfer learning. Recent work has explored the application of NLP techniques to ancient languages from the following perspectives:\n Multilingual transfer learning and disjoint character sets. Muller et al. (2020) studied hard-to-process living languages such as Uyghur, and reported that a non-contextual baseline outperforms all pre-trained LM-based methods. Ancient languages also face the same problem, with even less data available. A major challenge that is mostly specific to ancient logographic languages, however, is the almost non-existent overlap of their symbol inventories with those of high-resource languages.\n Visual representation of languages. Recently, several works have studied language processing based on images of text. Rust et al. (2023) pre-trained a masked language model on digitally rendered text and achieved comparable performance with text-based pre-training strategies on downstream tasks. Salesky et al. (2023) found that a multi-lingual translation system with pixel inputs was able to outperform its textual counterpart.\n Machine learning for ancient languages. Sommerschield et al. (2023) surveyed the status of pipelines for ancient language processing. Notably, the study concludes that applying machine learning methods to ancient languages is bottlenecked by the cost of digitization and transcription. According to the Missing Scripts Project,\u00b9\u00b9 only 73 of 136 dead writing systems are encoded in Unicode. Ancient languages, such as Ancient Greek or Latin (Bamman and Burns, 2020), benefit greatly from multilingual pre-training techniques, such as mBERT, XLM-R (Conneau et al., 2020), and BLOOM (Scao et al., 2022). The applicability of these techniques is limited when it comes to languages that were historically written in obsolete or extinct writing systems for instance, languages like Sumerian and Elamite were recorded in Cuneiform script and ancient Chinese was inscribed on oracle bone or bamboo. However, observations by existing work support the potential utility of visual processing pipelines for ancient languages.\n Logographic writing systems. Logography typically denotes a writing system in which each glyph represents a semantic value rather than a phonetic one, however, all the languages studied in our paper have at least some phonetic component based on the rebus principle. This paper emphasizes ancient logographies that (i) possess extensive glyph inventories; (ii) have feature glyphs with ambiguous transliterations or functional uses; and (iii) are low-resource with much of data remaining in photo formats (Caplice et al., 1991; Allen, 2000; Woodard, 2004). Existing research on logographic languages has predominantly focused on those well-resourced and still in use, such as Modern Chinese (Zhang and Komachi, 2018; Si et al., 2023), or used data that has already been carefully transcribed into Latin or annotated with extra semantic information (Wiesenbach and Riezler, 2019; Gutherz et al., 2023; Jiang et al., 2024). Our paper aims to address the gap in resources (by proposing new data) and methodologies (by adapting visual-only approaches) for encoding and analyzing ancient logographic languages, leading to more comprehensive understanding of historical linguistic landscapes."}, {"title": "7 Conclusion", "content": "By comparing the results on four representative languages on three downstream tasks, we demonstrated the challenges faced in applying natural language processing techniques to ancient logographic writing systems. Our experiments demonstrate, however, that encoding more readily available visual representations of language artifacts can be a successful strategy for downstream processing tasks."}, {"title": "8 Limitations", "content": "More discussion on ancient logographic languages. Due to page limits, we do not discuss ancient logographic languages in a critical way. Technically, there are no logographic languages, only languages written in logographic writing systems (aka logography) (Gorman and Sproat, 2023). In this paper, we use the term \u201clogographic languages\" to denote languages that are quite different from those with alphabetic writing systems especially when we tried to apply NLP toolkits for computational paleography. As mentioned in the related work section, these languages feature glyphs that have multiple transliterations or functional uses. In other words, these languages are homophonous or a glyph can be used as a phonetic value or semantic value. Therefore, the boundaries between logographic and phonographic is not sharply separated.\n Including more logographic writing systems. We selected the four languages because we would like to include at least one language from early civilization in Ancient China, Ancient Egypt, Indus Valley Civilization, Mesoamerica, Mesopotamia and Minoan Civilization (Woodard, 2004). However, we fail to include Mayan hieroglyphs (Mesoamerica) and Oracle Bone script. However, Mayan is excluded because the collection\u00b9\u00b2 is still working in process. Oracle Bone script is primarily omitted due to copyright issues.\n Textline images. Most ancient languages remain as full-document images. In this paper, we use digitally rendered text as a surrogate visual feature for Akkadian. In reality, much of Cuneiform data is still in hand copies or in photo format. In the future, we look to conduct apples-to-apples comparisons for all languages once the line segmentation annotations become available.\n Annotation quality and quantity. The study of ancient languages is constantly evolving; humanities scholars have not agreed on explanations, transliterations, or even the distinctions between certain glyphs or periods. We try our best to carefully annotated the data without bias; however, future editions of the benchmark are needed as things change all the time. A collective platform to correct errors and make more data available should be considered for future development.\n Label imbalance. The classification task in our benchmark is label imbalanced. This is known to be a major issue for all machine learning tasks related to the ancient world (Sommerschield et al., 2023; Chen et al., 2024)."}, {"title": "A Dataset collection", "content": "A.1 Linear A\n A.1.1 Attribute classification\n Linear A is a logo-syllabic, undeciphered writing system that was used in Bronze Age Greece (ca. 1800-1450BCE). We crawled a dataset of 772 tablets from the publicly available SigLA database \u00b9\u00b3. The metadata includes find-place, tablet dimensions, total number of signs and possible transliteration. For the attribute classification task, we use find-place as the target class, with the following classes ['Arkhanes', 'Gournia', 'Haghia Triada', 'Haghios Stephanos', 'Kea', 'Khania', 'Knossos', 'Kythera', 'Malia', 'Mallia', 'Melos', 'Mycenae', 'Papoura', 'Phaistos', 'Psykhro\u2019, 'Pyrgos', 'Syme', 'Tylissos', 'Zakros\u2019]. We only keep classes whose tablets are no less than 10, which results in only keeping 7 classes: ['Haghia Triada', 'Khania', 'Phaistos', 'Zakros', 'Knossos', 'Malia', 'Arkhanes\u2019]."}, {"title": "A.1.2 Machine Translation", "content": "A.2 Ancient Egyptian\n A.2.1 Attribute classification\n We use time periods whose data more than 50 to conduct this task, resulting in 1,230 samples of 12 classes.\n The classes are as below: ['Unas', 'Senwosret I Kheperkare\u2019, 'Pepi I Merire', 'Ramesses II Usermaatre-Setepenre', 'Sety I Menmaatre', 'Tuthmosis III Menkheperre (complete reign)\u2019, 'Pepi II Neferkare', 'Hatshepsut Maatkare', '18th Dynasty', 'Mentuhotep II Nebhepetre (complete reign)', 'Amenemhat II Nebukaure', 'Tutankhamun Nebkheperure', 'Cleopatra VII Philopator', '12th Dynasty']\n A.3 Akkadian (Cuneiform)\n A.3.1 Attribute prediction\n We use the dataset from Chen et al. (2023), containing 34,562 photographs of Akkadian cuneiform tablets with their transcriptions in cuneiform unicode and Latin transliterations. The metadata contains attributes such as time period, genre and geographical location (found place) that we have used for attribute classification.\n A.3.2 Dependency parsing\n The data is from Luukko et al. (2020) and comes in as normalization form of Akkadian. We implemented a rule-based method to convert the normalization form back to standard transliteration for consistency. When the auto-conversion fails, we simply just keep the normalization form.\n A.3.3 Machine translation\n The data is from Gutherz et al. (2023), a collection of 8,056 lines of Akkadian-English pair. The Akkadian representation is available in both ATF transliteration and Cuneiform Unicode.\n A.4 Bamboo script\n There are actually more than one scripts used in bamboo slips, for example, Seal script and Clerical script. For simplicity, our dataset generally call it Bamboo script. We pick two famous collections of the bamboo script, BaoShan and GuoDian bamboo slip collections for attribute prediction and machine translation.\n A.4.1 Attribute prediction\n The BaoShan bamboo slips collection consists of 723 slips, which are in three genres.\n A.4.2 Machine Translation\n We used the GuoDian bamboo slips collection for machine translation, whose major genre is philosophical essay. By referring to (Liu, 2003), We manually labeled each complete sentences with their transliteration and translation in modern Chinese. After filtering out incomplete sentences or removing the sentences with high interpretation difficulty. We extracted and labelled 489 lines of parallel data. The Bamboo script was labeled by a trained interdisciplinary Ph.D. student under the guidance of Professor Wenbo Chen specializing in Ancient Chinese Philology."}, {"title": "B Ablation Study on Text Recognition", "content": "Line-level OCR. We use Kraken (Kiessling, 2022), a state-of-the-art OCR library for historical documents, to transcribe the image into Latin or Unicode. To handle unseen Unicode codepoints, we simply extend the vocabulary of the decoder. For Latin transliteration, we predict outputs at the character level. Similar to most OCR pipelines, the default OCR model of Kraken is a 2-layer bi-directional LSTM with a connectionist temporal classification (CTC) loss.\n Glyph classification. We also applied Convolution Neural Networks (CNNs) to classify segmented glyphs. We use ResNet-50 as the backbone model, followed by a linear classification layer. We trained the model using cross-entropy loss with Adam optimizer. We resize the image of each glyph to 64 \u00d7 64 and apply 20% cropping. In total, there are 21, 687 segmented characters in the ZHO dataset."}, {"title": "C More Translation Case Study", "content": "We sample 10 examples from validation set for each language pairs.\n C.1 AKK - EN\n Akkadian translation samples from the PIXEL-MT models is shown in Figure 7. We showcase the first 10 examples from the validation set, we find that some annotation error present for example #2 and #6.\n C.2 EGY - EN\n Ancient Egyptian translation samples from the PIXEL-MT models is shown in Figure 8.\n C.3 ZHO - EN\n Old Chinese translation samples from the PIXEL-MT models is shown in Figure 9."}]}