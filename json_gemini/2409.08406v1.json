{"title": "Knowledge Tagging with Large Language Model based Multi-Agent System", "authors": ["Hang Li", "Tianlong Xu", "Ethan Chang", "Qingsong Wen"], "abstract": "Knowledge tagging for questions is vital in modern intel-ligent educational applications, including learning progressdiagnosis, practice question recommendations, and coursecontent organization. Traditionally, these annotations havebeen performed by pedagogical experts, as the task demandsnot only a deep semantic understanding of question stemsand knowledge definitions but also a strong ability to linkproblem-solving logic with relevant knowledge concepts.With the advent of advanced natural language processing(NLP) algorithms, such as pre-trained language models andlarge language models (LLMs), pioneering studies have ex-plored automating the knowledge tagging process using var-ious machine learning models. In this paper, we investigate theuse of a multi-agent system to address the limitations ofprevious algorithms, particularly in handling complex casesinvolving intricate knowledge definitions and strict numericalconstraints. By demonstrating its superior performance on thepublicly available math question knowledge tagging dataset,MathKnowCT, we highlight the significant potential of anLLM-based multi-agent system in overcoming the challengesthat previous methods have encountered. Finally, through anin-depth discussion of the implications of automating knowl-edge tagging, we underscore the promising results of deploy-ing LLM-based algorithms in educational contexts.", "sections": [{"title": "Introduction", "content": "Knowledge tagging is focused on creating an accurate index for educational content. It has become a key element in today's intelligent education systems, essential for delivering high-quality resources to educators and students (Chen, Chen, and Sun 2014). For example, with well-tagged educational materials, teachers can easily organize course content by searching through a concept keyword index (Sun et al.2018). Traditionally, educational experts have manually annotated concept tags for questions. However, the rapid expansion of online content has made these manual methods insufficient to keep up with the growing volume of online question data and the need to update concept tags quickly (Li et al. 2024b). To solve the above issues, recent studies have tried to automate the tagging process with different natural language processing (NLP) algorithms (Wang et al. 2024).For instance, Sun et al. (2018) employ deep learning algorithms and coverts the tagging task as a binary classificationproblem. Other works (Huang et al. 2023) fuse external information, e.g., solution text and conceptual ontology, with original question contents during the judging process. The most recent work (Li et al. 2024a) leverages large language models (LLMs) and simulates human expert tagging process with the helps on chain-of-thought (COT) (Wei et al. 2022)and in-context learning (ICL) (Dong et al. 2022) tricks during the automatic knowledge tagging process. In the Fig. 1, we summarize the existing algorithm for automatic knowledge tagging task. Although, all these studies have demonstrated appealing results in their experiments, there are still limitations in each algorithm causing gaps between human performance and automatic tagging results.\nIn this work, we propose a novel LLM-based multi-agent system (MAS) for knowledge tagging task, which exploits the planning and tool-using capabilities on LLMs. Specifically, by reformulating the judging process into a collaboration between multiple LLM-agents on independent sub-problems, we simplify the whole task and enhance the reliability of the judgment generation process. To validate the effectiveness of our proposed algorithm, we experiment with a well-established knowledge concept question dataset MathKnowCT (Li et al. 2024a). Our experimental results demonstrate that our method can bring steady improvements to prior single LLM-based methods."}, {"title": "Related Work", "content": "The recent rapid advancements in the field of machine learning (ML) have encouraged the emergence of studies focusedon applying advanced ML models to address challengingproblems in education (Xu et al. 2024; Wang et al. 2020).One critical area of exploration is the automatic knowledgetagging task, which is essential for modern Intelligent Tutoring Systems (ITS). Sun et al. (2018) were among the first toutilize straightforward models like long short-term memory(LSTM) networks and attention mechanisms to learn short-range dependency embeddings. In their approach, questionsare processed through neural network layers and linked tocross-entropy functions to determine if a tagging concept isrelevant to a specific problem. Building on this, Liu et al.(2019a) designed an exercise-enhanced recurrent neural net-work with Markov properties and an attention mechanismto extract detailed knowledge concept information from thecontent of exercises. Similarly, enriched data sources such as text, multi-modal data (Yin et al. 2019), and combinedLaTeX formulas (Huang et al. 2021) have been used to improve semantic representations learned with LSTM, allow-ing for the capture of more implicit contexts. To leverage therobust transformers framework, Zemlyanskiy et al. (2021)pretrained a BERT model to jointly predict words and enti-ties as movie tags based on movie reviews. Huang et al.(2023) introduced an enhanced pretrained bidirectional en-coder representation from transformers (BERT) for concepttagging, incorporating both questions and solutions. Withthe rise of large language models (LLMs), recent pioneeringstudies (Li et al. 2024a) have explored using LLMs as evalu-ators, simulating the human expert tagging process with theaid of chain-of-thought (COT) and in-context learning (ICL)techniques. LLM-based algorithms offer significant advan-tages in handling cases where annotation samples are scarceor unavailable, leveraging their extensive prior knowledge."}, {"title": "Multi-Agent System", "content": "An LLM-based multi-agent system (MAS) incorporatinglarge language models (LLMs) consists of multiple au-tonomous agents, each potentially utilizing LLMs, workingtogether to achieve particular goals (Guo et al. 2024). Thesesystems take advantage of LLMs to boost the agents' ca-pabilities, intelligence, and adaptability. MAS generally in-cludes three key components: agents, communication proto-cols, and coordination mechanisms. The agents, driven byLLMs, are tasked with executing actions and are initiatedby specific role-prompts tailored to individual tasks, suchas programming, answering queries, or strategic planning.Communication protocols establish how agents share infor-mation, often using natural language conversations, struc-tured message exchanges, or other methods of interaction.Coordination mechanisms are vital in MAS, as they handlethe complexity and independence of each agent. When ap-plied to education, LLM-based MAS has presented its greatpotentials in various piratical usages (Wang et al. 2024).For example, Zhang et al. (2024) simulate instructional pro-cesses by enabling role-play interactions between teacherand student agents with MAS. By analyzing agent behavioragainst real classroom activities observed in human students,studies have demonstrated that these interactions closely re-semble real-life classrooms and foster effective learning. Beyond simulation, MAS has also been employed to improveLLM performance in tasks such as grading assignments (Lagakis and Demetriadis 2024) and identifying pedagogicalconcepts (Yang et al. 2024). The introduction of multiplejudging agents in group discussions has led to evaluationsthat align more closely with expert annotations."}, {"title": "Problem Statement", "content": "Following the successful experience of applying LLMs forknowledge tagging task with ICL method in prior work (Li et al. 2024a), we define the knowledge tagging problem asfollows: given a pair of knowledge definition text $k$ and aquestion's stem text $q$, the objective of a concept taggingmodel $F$ is to produce a binary judgment $y \\in \\{0, 1\\}$, where $F(k, q) = 1$ means $k$ and $q$ are matching, 0 otherwise."}, {"title": "System Design", "content": "In this section, we introduce our LLM-based MAS. We firstgive an overview of the framework. Then, we detail the in-"}, {"title": "An Overview", "content": "Our LLM-based MAS consists of four types of LLM-basedagents, including task planner, question solver, semanticjudger, numerical judger. At the beginning of the judg-ing pipeline, the planning agent is activated proposing cus-tomized collaborating plan for the given knowledge defini-tion. Then, the remaining agents are executed based the pro-posed plan. At last, the summarizing module outputs the fi-nal judgement by connecting those intermediate results withthe AND operator. Fig. 2 presents an overview of the MASand its workflow for the knowledge annotation task."}, {"title": "Task Planner", "content": "Knowledge concept definition is commonly composed bytwo major components: descriptive text and additional con-strains. The duty of the task planner is to decompose theoriginal definition into a series of independent verificationsub-tasks and assign these tasks to the following agents. Ingeneral, by executing the step-by-step checking procedure,we avoid asking LLMs to proceed with multiple constraintsat once, as it simplifies the task and helps the annotatingsystem to generate accurate final judgments. In Fig. 2, wepresent an example plan for the given knowledge concept.Based on the knowledge description, the planner proposesfour sub-tasks, including 1 semantic judge and 3 numericaljudges. The prompt for the planning agent is shown below,where the [Example] is the placeholder for implementing thefew-shot learning tricks (Brown et al. 2020).\nInstruction: Your job is to take the following knowledge definition and separate it into one or moresimpler Knowledge sub-constraints. Each of these smaller constraints must return Yes or No value whenevaluated. examples: [Example 1] [Example 2]Knowledge: [Input by User]\nPlan: (Generated by LLMs)"}, {"title": "Question Solver", "content": "In addition to the planner, we have integrated a questionsolver into the system to generate solutions for questionswhere constraints in knowledge definitions may impact thesolution values. Since question-solving tasks are widely uti-lized by all LLMs during the instructional tuning phase, wedo not employ any additional engineering techniques. In-stead, we compose the prompt for the agent as follows:\nInstruction: You are a student. Given a question, pro-vide the answer at the end.\nQuestion: [Input by User]\nAnswer: (Generated by LLMs)"}, {"title": "Semantic Judger", "content": "The semantic judger is designed to execute verification tasksbased on the semantic constraints outlined in the knowl-edge definition. Leveraging the general prior knowledge ofLLMs, the LLM-based agent is adept at understanding se-mantic patterns between the input knowledge and questionpairs. In our implementation, we employ standard sequen-tial generation and incorporate few-shot learning to furtherenhance performance. The detailed instruction prompt usedfor the semantic judger is as follows:\nInstruction: You are a knowledge concept annotator.Your job is to judge whether the question is concern-ing the knowledge. The judgment tokens <Yes> or<No> should be provided at the end of the response.You are also given two examples. [Example 1] [Example 2]Knowledge: [Copied from Task Planner]Question: [Copied from Question Solver]Judgment: (Generated by LLMs)"}, {"title": "Numerical Judger", "content": "Although LLMs excel at handling semantic-related instruc-tions, recent studies (Collins et al. 2024) have shown thatthey struggle with numerically related requests when relyingsolely on a sequential generation strategy. To address this is-sue, we draw inspiration from the recently emerged Tool-use LLMs (Zhuang et al. 2024) and leverage the LLMs'emergent coding capabilities to verify constraints throughcode execution. Specifically, we process the numerical judg-ing procedure in two steps. First, the LLM extracts relevantnumbers from the question stem to use as arguments in aPython program. Then, the LLM is instructed to convert con-straints into executable code. Below, we present an exampleof a numerical judging prompting process for a given knowl-edge and question pair.\nInstruction1: You are a knowledge concept classifier.You are given a Question, Answer, a Main constraint,and Subconstraints. Identify the numerical argumentsfor each subconstraint. [example]Knowledge: [Copied from Task Planner]Question: [Copied from Question Solver]Sub-constraints: [Copied from Task Planner]Argument: (Generated by LLMs)\nInstruction2: You are a knowledge concept annotator.You are given a Question, Answer, Sub-contraints,and Arguments. Your job is to write a Python scriptusing the sub-constraints and their respective argu-ments and evaluate them. The script prints True if allthe sub-constraints return True, False if else.Knowledge: [Copied from last step]Question: [Copied from last step]Argument: [Copied from last step]Sub-constraints: [Copied from Question Solver]Program Code: (Generated by LLMs)\nOnce the executable program code is generated, the agentautomatically runs the program with all relevant arguments.The final judgment is then determined by evaluating the pro-gram's boolean output."}, {"title": "Experiment", "content": "In this section, we conduct experiments to validate the effectiveness of our purposed system. Through the experiments,we aim to answer the following research questions:\n\u2022 RQ1: Does the proposed method outperform the otherbaseline algorithms?\n\u2022 RQ2: In which scenario, the proposed method shows itadvantages?"}, {"title": "Dataset Overview", "content": "We conduct our experiment with MathKnowCT (Li et al.2024a), which contains 24 knowledge concepts from mathconcepts ranging from Grade 1 to Grade 3. The datasetwas constructed by finding 100 candidate questions withthe highest text embedding similarity to each knowledge"}, {"title": "Implement Settings", "content": "To explore the compatibility of our proposed framework, weexperiment it with 3 representative LLMs frameworks, including Llama-3 (Touvron et al. 2023), Mixtral (Jiang et al.2024), and GPTs (Brown et al. 2020). For each frameworks,we choose two sized models, e.g., base and large, to ex-plore the impacts of agent model sizes. In addition, for eachLLM framework, we use it for all agents implementationsin our framework except for the numerical judger. To ensurethe generated code are most reliably executable, we choseOpenAI's GPT-40 (with temperature=0.7) for the numericaljudger in all following experiments. For each framework, weexperiment with two-sized versions (Base and Large) andthe prompt text is adjusted based on the preference of eachLLM. We run our experiment with the implementation ofhuggingface packages\u00b9 on 8 * Nvidia A100 80G GPUs. Thedetailed model information are listed in Tab. 2.\nFollowing the prior study (Li et al. 2024a), we evaluate theperformance with various metrics including accuracy, preci-sion, recall and F1-score. Specifically, the metrics are calcu-lated with the following formulas:\nAccuracy = $\\frac{TP + TN}{TP + FP + TN + FN}$\nPrecision = $\\frac{TP}{TP + FP}$, Recall = $\\frac{TP}{TP + FN}$\nF1 = $2 * \\frac{Precision * Recall}{(Precision + Recall)}$\nwhere true positive (TP) samples are the matchingknowledge-question pairs successfully discerned, false pos-itive (FP) samples are the unrelated sample pairs misclassi-fied as matching, true negative (TN) are the unrelated pairscorrectly filtered, false negative (FN) are the matching pairsdismissed. From an educational perspective, false negativesare often preferable to false positives, as a falsely matchedquestion could disrupt a student's learning process."}, {"title": "Baselines", "content": "We compare our framework with three representative knowl-edge tagging frameworks introduced in prior sections, in-cluding embedding similarity, pre-trained language fine-tunning and single LLM inference. For each framework, wechoose to implement with different high-performance back-bone models. Details about each baseline's implementationis shown as follows:\n\u2022 Embedding Similarity: Two high-performed long textencoding models, sentence-BERT (S-BERT) (Reimersand Gurevych 2019) and text-embedding-3-small\u00b2 areleveraged as the backbone model for the embedding sim-ilarity framework. The judgment is determined by thetop-K selection on cosine similarity between dense vec-tors of the encoded knowledge and question text, $x_k$ and$x_q$.\n\u2022 PLM Fine-tuning: Following the prior studies (Huanget al. 2023), we choose PLMs include BERT (Devlinet al. 2018), T5 (Raffel et al. 2020), and RoBERTa (Liuet al. 2019b) as the backbone for our implementation. Asthe knowledge tagging is formulated as a binary classi-fication task in our paper, we add a binary classificationlayer to the top of <BOS> tokens outputs and fine-tunethe parameter of the whole model with the binary en-tropy loss calculated on the samples in the training set.The learning rate during our fine-tuning process is tunedfrom le-3 to le-5.\n\u2022 Single LLM with 2-shot Inference: We implementa single LLM with 2-shot inference, following priorwork (Li et al. 2024a), which incorporates Chain-of-Thought (COT) instructions into the input prompt. In ourimplementation, we use three backbone models: Llama3,Mixtral, and GPTs. For simplicity, we employ a randomselection strategy for demonstration retrieval."}, {"title": "Result and Discussions", "content": "Figure 3 illustrates an example showcasing the outputs ofall agents in the system during the inference process for aknowledge-question pair. The performance of the baselinemodels and our proposed multi-agent system across the en-tire dataset is presented in Table 4.\nRQ1: To address RQ1, we first compare the proposedmethod with two non-LLM baseline frameworks: embed-ding similarity and PLM fine-tuning. Our comparison re-veals that the base-sized LLMs achieve results comparableto those of the baseline models. As the model size increases,larger LLMs significantly outperform the baselines. Whencomparing single LLM inference with the multi-agent ap-proach, we find that the introduction of planning and numer-ical agents leads to substantial improvements in precision.This is because the clear sub-constraints, decomposed fromthe complex problem definition, reduce false positive errorsin predictions.\nHowever, we also observed a notable decrease in recallwith the multi-agent design. This decline can be attributed toerrors in the intermediate steps of the multi-step judging pro-cess, which increase false negative errors. Apart from that,although ROBERTa-base achieves 100% recall, its low pre-cision makes it unsuitable for real-world scenarios. Based onthese observations, we conclude that the LLM-based multi-agent system is an effective algorithm for the knowledge tag-ging task.\nRQ2: For RQ2, we examine the performance gap betweensingle LLMs and Multi-Agent LLMs across different modelsizes. Our analysis reveals that while the multi-agent designsignificantly improves metrics such as accuracy and preci-sion for base-sized LLMs, the benefits are less pronouncedfor large-sized LLMs. This suggests that larger LLMs are in-herently more capable of handling complex tasks comparedto smaller models. However, a closer inspection of precisionshows that our proposed multi-agent framework consistentlyenhances precision, even for large-sized LLMs. Given theeducational context, we can still assert that the multi-agentframework adds value to large-sized LLMs in knowledgetagging tasks.\nFurthermore, as shown in Tab. 4, the performance gap be-tween base-sized and large-sized LLMs is significantly re-duced with the multi-agent approach. Considering the cost-effectiveness of the entire model, we believe that the pro-posed multi-agent framework has great potential to evolve"}, {"title": "Industrial Impact", "content": "The designed multi-agent LLM knowledge tagging systemhas been deployed in Squirrel Ai Learning and applied tomassive K-12 students from 338 cities of 35 provinces inChina, which demonstrated significant impact and provento generate substantial business value in several areas. Thisinnovative approach ensures that each problem is properlylinked to its specific knowledge concept group automati-cally at scale, which saves tremendous human efforts anda significant economic cost as well. More importantly, thedeployment of this multi-agent LLM system has not onlyrealized significant cost savings but also fundamentally en-hanced how educational content is created, delivered, andevaluated. In particular, it has extensive impacts on qualitycontrol of contents generated by LLMs and directs such"}, {"title": "Direct Impact on Cost Savings", "content": "Focusing initially on primary school content, specificallymathematics for grades 1 to 5, this project covers approx-imately 1,900 knowledge points of the Chinese math pro-gram and around 2,100 that of the U.S. math program. Thisautomation greatly reduces the effort of human labelingwhich has been the primary solution for over a decade. Thesystem was deployed at the starting of summer 2024 andhas saved at least $306,750 labeling cost when 75 uniqueproblems per knowledge point and $1 per pair of taggingare taken into account."}, {"title": "Indirect Impacts on Educational Quality and Efficiency", "content": "The project's indirect impacts significantly enhance the ed-ucational experience through smarter content delivery andimproved diagnostic tools:\n\u2022 Quality Control of Problem Generation: Lacking suf-ficient problems has been a pain point for several yearsand there were no low-cost and time-effective solu-tion until LLMs arise. However, creating problems withLLMs, despite its novelty and efficiency, still relies heav-ily on experts' efforts to validate them before serving tostudents. Therefore, the multi-agent knowledge tagging system played a critical role here that it served as a smartjudge that tirelessly labeled (problem, knowledge point)pairs, which boosted the efficiency of problem validationby more than 90%. With the support of LLM problemsgeneration, followed by multi-agent knowledge taggingas a judge, the problems pool has tripled since the deployment of the system.\n\u2022 Improved Tag Linking and Recommendation Sys-tems: Precise links between problems and knowledgepoints allow for more effective recommendations of content tailored to individual student needs, helping to iden-tify and fill gaps in understanding. With the capacity togenerate a larger pool of problems-up to three timesmore for each knowledge point the likelihood of stu-dents encountering repeated problems has been reducedby 70%, enhancing learning efficiency and engagement.\n\u2022 Enhanced Diagnostic Tools: The exact mapping ofproblems to knowledge points also refines the diagnos-tics of learning errors. When a student makes a mistake,the system can quickly pinpoint the specific knowledgepoint involved, enabling more accurate and constructiveerror analysis. This feature has improved the relevance oferror reasoning in about 10% of cases, providing direct,actionable feedback to both students and educators."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel LLM-based multi-agentframework for the knowledge-tagging task, which leveragesthe \"divide and conquer\" problem-solving strategy to ad-dress the complex cases involving intricate knowledge defi-nitions and strict numerical constraints that have challengedprevious algorithms. Through the precise collaboration ofdiverse LLM agents, our system harnesses the strengthsof individual agents while integrating external tools, suchas Python programs, to compensate for LLMs' limitationsin numerical operations. To validate the effectiveness ofthe proposed framework, we conducted experiments usingthe expertly annotated knowledge concept question dataset,MathKnowCT. The results demonstrate the framework's ef-ficacy in enhancing the knowledge-tagging process. Finally,through a detailed discussion of the implications of automat-ing knowledge tagging, we highlight the promising future ofdeploying LLM-based algorithms in educational contexts."}]}