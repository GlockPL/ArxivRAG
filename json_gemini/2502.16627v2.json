{"title": "Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification", "authors": ["Arshia Kermani", "Ehsan Zeraatkar", "Habib Irani"], "abstract": "The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. This paper presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (Refrigeration Devices, Electric Devices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 63% improvement in inference speed with minimal accuracy degradation. These findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in transformer architectures have significantly advanced time series analysis across various domains, including healthcare, finance, and predictive maintenance, enabling more accurate forecasting, anomaly detection, and decision-making processes [8]. The ability of AI models to process sequential data efficiently has enabled substantial improvements in these fields, allowing for more accurate forecasting, anomaly detection, and decision-making processes [23]. At the core of these developments are transformer-based architectures, which have demonstrated superior performance over traditional models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, by leveraging self-attention mechanisms to capture long-range dependencies in temporal data [29]. Despite their effectiveness, transformers are computationally expensive, making them less viable for real-time and edge-based applications due to their high energy consumption and memory footprint.\n\nThe growing carbon footprint associated with transformer training and inference has raised significant concerns regarding sustainability and deployment feasibility in resource-constrained environments [20]. As AI adoption scales across industries, the carbon footprint of large-scale models has become a pressing issue, necessitating the development of optimization strategies that reduce computational overhead while maintaining predictive accuracy.\n\nThis research investigates the optimization of transformer models for energy-efficient time series classification, focusing on the application of pruning and quantization. By systematically evaluating the impact of these techniques on model performance, computational efficiency, and power consumption, this study provides insights into the trade-offs between resource efficiency and classification accuracy. The findings contribute to the broader goal of sustainable AI development, offering solutions for mitigating the environmental footprint of deep learning while preserving the robustness of transformer-based models.\n\nBeyond addressing computational efficiency, this research aligns with ongoing efforts to make AI systems more sustainable, particularly in domains where power constraints limit the feasibility of high-performance deep learning models. By demonstrating how structured pruning and quantization can significantly reduce energy demands without substantial accuracy degradation, our work lays the groundwork for future innovations in energy-efficient AI. Through a rigorous empirical evaluation of transformer optimization techniques, this work advances the development of scalable, resource-aware machine learning models capable of operating in diverse environments, from cloud-based infrastructures to edge computing applications."}, {"title": "2 Literature Review", "content": "The expanding field of artificial intelligence (AI) has increasingly emphasized sustainability, particularly in optimizing transformer models for time series classification and forecasting [10]. Initially developed for natural language processing (NLP), transformers have demonstrated remarkable proficiency in handling sequential data, leading to their adaptation for time series tasks. This literature review consolidates key findings from foundational studies, exploring optimization strategies such as pruning and quantization while evaluating their impact on model accuracy, computational efficiency, and energy consumption.\n\nAdditionally, ensemble learning techniques have been explored to enhance forecasting accuracy [16].\n\nEmpirical evaluations have reinforced transformers' effectiveness in time series forecasting and classification. A study by Lara-Ben\u00edtez et al. [15] analyzed transformer performance across 12 datasets with over 50,000 time series, providing crucial insights into their advantages and limitations across various domains."}, {"title": "2.2 Model Compression Strategies: Pruning and Quantization", "content": "To mitigate computational overhead, several model compression strategies have been proposed, including pruning, quantization, and knowledge distillation.\n\nPruning is a well-established method for reducing model size by removing parameters deemed less significant. This technique has been successfully applied to transformers to improve efficiency while preserving accuracy. Pruning-guided feature distillation has been introduced to create lightweight transformer architectures that maintain predictive performance while reducing computational costs [12]. Additionally, global structural pruning has demonstrated significant reductions in latency and computational requirements [18]. Cheong [3] further highlights the role of pruning in compressing transformers to enhance inference speed and energy efficiency.\n\nQuantization, which reduces model weight and activation precision, is another widely used approach for decreasing memory usage and improving inference speed. Quantization-aware training has shown promise in minimizing memory footprints while preserving accuracy [32]. Techniques such as mixed-precision quantization [27], post-training quantization [18], and quantized feature distillation have been effective in reducing resource consumption.\n\nKnowledge distillation, which trains a smaller model to replicate the performance of a larger one, further contributes to reducing model complexity while maintaining accuracy. This approach has proven particularly useful for deploying transformers in resource-constrained environments [28]"}, {"title": "2.3 Optimizing Transformer Inference", "content": "Beyond pruning and quantization, structural modifications have been explored to enhance transformer efficiency. Gated Transformer Networks (GTNs) improve feature extraction by capturing both channel-wise and step-wise correlations in multivariate time series data [17]. Sparse binary transformers have also demonstrated their effectiveness in reducing parameter redundancy while preserving model performance [9]. Hybrid methodologies, such as Autoformer, leverage auto-correlation mechanisms to enhance long-term forecasting accuracy [26].\n\nFurther research into inference optimization has underscored the significance of architectural bottlenecks, hardware constraints, and algorithmic refinements. A full-stack co-design approach that integrates software and hardware optimizations has achieved up to an 88.7\u00d7 speedup in inference without compromising accuracy [13]. Similarly, comprehensive surveys of transformer inference optimization strategies highlight the effectiveness of pruning, quantization, knowledge distillation, and hardware acceleration in reducing latency and energy consumption while maintaining predictive performance [4]. GPU-accelerated optimal inferential control framework was proposed using ensemble Kalman smoothing to efficiently handle high-dimensional spatio-temporal CNNs [24].\n\nThe NIOT framework, specifically designed for modern CPUs, integrates architecture-aware optimizations such as memory tiling, thread allocation, and cache-friendly fusion strategies. These improvements have led to latency reductions of up to 29% for BERT and 43% for vision transformers, significantly surpassing traditional inference techniques [30].\n\nEnergy efficiency remains a critical concern in transformer-based models, particularly in applications requiring continuous inference. The integration of optimized data preprocessing techniques has shown significant potential in improving both computational efficiency and predictive accuracy. [19] emphasize that the structural representation of weather-related features substantially impacts forecasting performance. Their findings highlight the necessity of refining preprocessing pipelines to enhance energy efficiency in transformer-based applications.\n\nThe sustainability of transformer models has been analyzed within the broader framework of green computing. [14] introduce the concept of green algorithms, providing a quantitative framework for assessing the carbon footprint of computational tasks. This metric is instrumental in evaluating the environmental impact of transformer-based architectures in time series classification, reinforcing the importance of computational efficiency in sustainable AI practices.\n\nSeveral studies have examined the trade-offs between performance and energy efficiency in transformer inference. [2, 7] present empirical evaluations illustrating how software-level optimizations can significantly lower energy consumption without sacrificing predictive accuracy. These findings underscore the necessity for targeted optimization strategies, particularly for CPU-based inference, where resource constraints are a fundamental challenge.\n\nComplementary to these efforts, [1] introduce the Greenup, Powerup, and Speedup (GPS-UP) metrics to evaluate energy efficiency in software optimizations. By categorizing computational trade-offs into multiple distinct scenarios, their study provides a structured framework for analyzing the relationship between software modifications and energy consumption. Unlike conventional energy-delay metrics, GPS-UP facilitates a more nuanced evaluation of how performance improvements interact with power efficiency, contributing to the development of sustainable yet high-performance transformer models."}, {"title": "3 Background", "content": null}, {"title": "3.1 Time Series Signal Processing", "content": "Time series signals in real-world applications can be represented as a sequence $X = \\{x_t\\}_{t=1}^T$, where each $x_t \\in \\mathbb{R}^d$ represents a d-dimensional measurement at time step t. Traditional approaches to processing such signals have relied on various architectures, each with distinct characteristics in capturing temporal dependencies. Consider a time series signal x(t); its discrete-time representation can be expressed as:\n\n$x[n] = x(nTs), \\quad n\\in \\mathbb{Z}$ (1)\n\nwhere $T_s$ is the sampling period. The corresponding frequency domain representation through the Discrete Fourier Transform (DFT) is:\n\n$X[k] = \\sum_{n=0}^{N-1} x[n]e^{-j2\\pi kn/N}$ (2)"}, {"title": "3.2 Deep Learning for Sequential Data", "content": "Deep learning approaches for sequential data processing have evolved from recurrent architectures to attention-based mechanisms. The traditional recurrent neural network (RNN) processes sequential data through:\n\n$h_t = \\sigma(W_{xh}x_t + W_{hh}h_{t-1}+b_h)$ (3)\n\nwhere $h_t$ represents the hidden state at time t, $W_{xh}$ and $W_{hh}$ are weight matrices, and $\\sigma(\\cdot)$ is a nonlinear activation function. While RNNs are effective at processing sequences, they face challenges when trying to capture relationships between elements that are far apart in the sequence. This difficulty is largely due to the vanishing gradient problem, a phenomenon where the influence of earlier inputs diminishes as the sequence progresses. As a result, RNNs can struggle to learn long-range dependencies, meaning they might not effectively remember or use information from many time steps earlier in the sequence.\n\nTo overcome these limitations, researchers have developed more advanced architectures. One of the most significant advancements has been the introduction of attention-based mechanisms, allowing models to focus on specific parts of the input sequence, regardless of their position. This led to more powerful models that can handle long-range dependencies more effectively than traditional RNNs."}, {"title": "3.3 Computational Complexity and Energy Consumption", "content": "The computational complexity of the self-attention mechanism for a sequence of length T is:\n\n$O(T^2d+Td^2)$ (4)\n\nThis complexity translates to energy consumption following:\n\n$E = \\alpha CV^2 fT$ (5)\n\nwhere $\\alpha$ is the activity factor, C is the effective capacitance, V is the supply voltage, f is the operating frequency and T is the execution time."}, {"title": "4 Optimizations for Energy Efficiency", "content": "Enhancing the efficiency of Transformer-based models necessitates the use of optimization strategies, such as quantization, pruning, and specialized hardware acceleration. These techniques aim to reduce computational overhead, lower energy consumption, and improve inference speed without significantly compromising model accuracy."}, {"title": "4.1 Quantization", "content": "Quantization is a widely adopted method for reducing computational complexity and memory requirements by representing model parameters and activations with lower precision data types. Typically, deep learning models are trained using high-precision 32-bit floating-point (FP32) representations; however, during inference, these representations can be converted to lower bit-width formats, such as 8-bit integers (INT8), significantly reducing memory bandwidth and energy consumption.\n\n$\\frac{E_{quantized}}{E_{float32}} = \\frac{1}{Q}$ (6)\n\nwhere Q denotes the quantization factor. Reducing numerical precision can lead to substantial power savings while maintaining a minimal impact on model performance."}, {"title": "4.2 Pruning", "content": "Pruning is an effective strategy for reducing computational complexity by eliminating redundant parameters that contribute minimally to the model's predictive performance. Transformer architectures often contain a significant number of parameters, many of which are unnecessary for effective inference.\n\nA variety of both structured and unstructured pruning techniques have been developed to optimize Transformers. These methods include weight pruning, which involves removing individual weights that have minimal impact on the model's accuracy; neuron pruning, where entire neurons in dense layers are discarded to reduce computational cost; and head pruning, which targets and eliminates redundant attention heads within the multi-head self-attention mechanisms. Additionally, structured pruning can remove entire layers or blocks from the network, thereby significantly enhancing overall efficiency. The impact of pruning on the model's complexity can be quantified by\n\n$E_{pruned} = E \\times (1-p)$ (7)\n\nwhere p denotes the proportion of parameters removed.\n\nWhen pruning is combined with quantization, the benefits become even more pronounced, as pruned models require fewer computations, while quantized models utilize lower-bit representations, leading to significant reductions in power consumption and memory bandwidth usage."}, {"title": "5 Methodology", "content": "This study aims to optimize transformer-based models for energy-efficient time series classification. The methodology consists of structured data preprocessing, an optimized transformer architecture, and energy-efficient optimization strategies such as pruning and quantization. Three datasets RefrigerationDevices, ElectricDevices, and PLAID are employed for evaluation. The experimental setup involves training transformer models on these datasets and measuring their computational efficiency, classification performance, and power consumption."}, {"title": "5.1 Dataset Preprocessing", "content": "The datasets used in this study contain multivariate time series data from various electrical devices. To ensure consistency across datasets and improve the performance of the transformer models, a structured preprocessing pipeline is applied:\n\nData Normalization: Since power consumption values vary across different devices, min-max normalization is applied to scale each time series between 0 and 1, preventing numerical instability.\n\nResampling and Interpolation: The datasets have different sampling frequencies; hence, interpolation techniques are used to create a uniform temporal resolution.\n\nSegmentation: Fixed-length overlapping windows are extracted from each time series to ensure that sufficient contextual information is available for classification.\n\nTrain-Validation-Test Splitting: A subject-wise splitting strategy is used, ensuring that data from the same household does not appear in both training and test sets, preventing data leakage.\n\nAfter preprocessing, each dataset is structured into a tensor representation suitable for transformer-based modeling."}, {"title": "5.2 Transformer Model Implementation", "content": "This study incorporates the Vision Transformer (ViT) model [5], which has been increasingly adapted for time series tasks due to its ability to capture long-range dependencies efficiently [11]. The model architecture comprises several key components. First, the Time Series Patch Embedding Layer transforms raw time series data into fixed-size patches using a one-dimensional convolutional layer, with positional encoding added to maintain the sequential order. Next, the multi-head self-attention mechanism allows the model to focus on different segments of the time series simultaneously, enhancing its ability to learn complex relationships. Each encoder block contains position-wise feed-forward layers with ReLU activations and dropout layers to reduce overfitting. To ensure stable training and mitigate the risk of vanishing gradients, the architecture incorporates layer normalization and residual connections. Finally, a fully connected softmax output layer classifies each sequence into its corresponding device category.\n\nThe model is implemented in PyTorch, with training conducted using the Adam optimizer alongside a cosine annealing learning rate scheduler."}, {"title": "5.3 Optimization Strategies", "content": "To enhance the computational efficiency and energy sustainability of transformer models for time series classification, two primary optimization techniques-pruning and quantization are implemented."}, {"title": "5.3.1 Pruning", "content": "In this research, two main pruning approaches are explored. The first approach is Magnitude-Based Pruning, which eliminates parameters with the smallest absolute values under the assumption that lower-magnitude weights contribute less to overall model performance. In this context, L1-Norm Pruning removes weights with the smallest L1 norm, thereby reducing network sparsity while largely preserving accuracy. Alternatively, L2-Norm Pruning removes entire neurons or filters based on their L2 norm (Euclidean distance), effectively eliminating redundant units while maintaining structural integrity. Additionally, Structured Pruning is applied to remove entire layers, filters, or attention heads, thereby significantly reducing overall model complexity.\n\nThe second approach is Global and Layer-Wise Pruning. In this approach, the least important weights are selected across the entire model, ensuring that only the most essential parameters are retained regardless of their location. In contrast, Layer-Wise Pruning applies the pruning process separately to each layer, maintaining a uniform level of sparsity throughout the transformer architecture.\n\nTo apply the pruning process, initially, self-attention layers, feed-forward networks, and embedding layers are evaluated for their sensitivity to pruning. Subsequently, pruned weights are masked and set to zero to maintain network sparsity without altering the overall structure. Finally, the model is fine-tuned on the training set to recover any accuracy lost due to weight removal. This pruning strategy is particularly beneficial for reducing inference time and computational complexity, making the optimized model suitable for edge deployment and low-power computing environments."}, {"title": "5.3.2 Quantization", "content": "In our study, we employ two distinct quantization strategies to decrease both memory footprint and computational overhead while maintaining high model accuracy.\n\nFirst, we apply Post-Training Quantization (PTQ). It reduces the precision of model weights and activations after training by converting 32-bit floating point numbers into lower-bit representations (e.g., 8-bit or 16-bit). This process significantly decreases the model's memory requirements and computational load. Within PTQ, we implement two approaches. In static quantization, model weights and activations are converted to lower precision prior to inference, whereas in dynamic quantization, only the weights are quantized while activations remain in floating-point representation.\n\nThen, we incorporate Quantization-Aware Training (QAT). It is an advanced technique in which the effects of quantization are simulated during the training process. By integrating quantization constraints early in the learning pipeline, the model is trained to adapt to lower precision. This approach enhances the model's robustness and generalization, resulting in a lower loss of accuracy compared to post-training quantization and enabling the model to better handle reduced precision for deployment across diverse hardware architectures.\n\nOur implementation of quantization is carried out using the torch.quantization module in Py-Torch. The quantization workflow involves three main stages: model preparation, quantization configuration, and calibration with evaluation. During model preparation, we select specific layers such as linear projections and self-attention heads that are critical for performance and amenable to quantization. In the quantization configuration step, high-precision floating-point values are replaced with integer-based representations (e.g., int8). Finally, we perform post-quantization calibration using validation data to ensure that any loss in accuracy is kept to a minimum."}, {"title": "6 Experimental Setup", "content": null}, {"title": "6.1 System Configuration", "content": "Experiments are conducted in a GPU-accelerated computing environment using the PyTorch framework along with the TorchQuantization library. To ensure statistical robustness, each model configuration is trained over multiple runs, and the resulting performance metrics are averaged. This approach guarantees that the reported results are reliable and reflective of the models' true performance across diverse conditions."}, {"title": "6.2 Training Strategy and Hyperparameters", "content": "In our experiments, we adopted a carefully designed training strategy to ensure robust model convergence while optimizing computational resources. Specifically, we set the batch size to 64 samples per batch, a choice made to optimize memory usage and facilitate efficient gradient updates. The model parameters were optimized using the Adam optimizer, enhanced with weight decay regularization to mitigate overfitting.\n\nA cosine annealing scheduler was employed to adaptively adjust the learning rate during training, thereby promoting a smooth convergence process. For multi-class classification tasks, the categorical cross-entropy loss function was utilized to quantify prediction errors. The training was conducted over 50 epochs, with early stopping based on validation accuracy to prevent overfitting and to conserve computational resources.\n\nFurthermore, model checkpoints were systematically saved throughout the training process, and the best-performing model was ultimately selected based on the minimum validation loss observed."}, {"title": "6.3 Dataset Description", "content": "In our study, we employ three distinct datasets to analyze and classify electrical device usage patterns: Refrigeration Devices, ElectricDevices, and PLAID.\n\nRefrigeration Devices Dataset The Refrigeration Devices dataset is part of the UCR Time Series Classification Archive [6]. It comprises time series data specifically focusing on household refrigeration appliances. Each time series consists of 720 data points, representing 24 hours of readings taken at two-minute intervals. This granularity facilitates an in-depth analysis of daily usage patterns of refrigeration devices. The dataset's focus on refrigeration appliances makes it particularly relevant for studies aiming to understand and optimize energy consumption in household refrigeration, a significant component of residential energy use.\n\nElectric Devices Dataset Also sourced from the UCR Time Series Classification Archive [6], the Electric Devices dataset encompasses a broader range of household appliances beyond refrigeration. It includes data from various electrical devices used in homes, divided into a training set with 8,926 instances and a test set with 7,711 instances. Each time series comprises 96 data points, capturing the operational characteristics of different devices across seven distinct categories. This comprehensive dataset allows for the analysis of diverse appliance usage patterns, providing insights into the classification and energy consumption of various household devices.\n\nPLAID Dataset The Plug Load Appliance Identification Dataset (PLAID) is designed for load identification research. It includes short voltage and current measurements sampled at 30 kHz from 11 different appliance types present in more than 60 households in Pittsburgh, Pennsylvania, USA. Data collection occurred during the summer of 2013 and winter of 2014. Each appliance type is represented by multiple instances of varying make and models, with three to six measurements collected for each state transition. These measurements were post-processed to extract windows containing both steady-state operation and startup transients. After removing measurements with significant noise due to errors, the dataset comprises 1,074 instances. The high sampling rate and detailed measurements make PLAID particularly valuable for developing and testing algorithms aimed at non-intrusive load monitoring and appliance identification."}, {"title": "6.4 Evaluation Metrics", "content": "In our experimental evaluation, we assessed the performance of our optimized transformer models using three primary metrics. First, we measured Classification Accuracy by calculating the proportion of correctly classified time series sequences, which served as a fundamental indicator of model performance. Second, we quantified Computational Overhead by evaluating reductions in inference time, the number of floating-point operations (FLOPs), and memory usage. These evaluations provided critical insights into the efficiency improvements achieved through our model optimization strategies. Finally, we determined Energy Efficiency by directly measuring the power consumption of each model during inference, thereby reflecting the effectiveness of our approaches in reducing energy consumption in practical deployments."}, {"title": "7 Results and Analysis", "content": null}, {"title": "7.1 Experimental Setup", "content": "In order to comprehensively evaluate the effectiveness of our proposed optimization framework, we conducted extensive experiments utilizing three distinct datasets that represent varying aspects of electrical device usage patterns and characteristics. The Refrigeration Devices dataset, comprising 2,500 sequences with 720 timesteps each, provides a focused examination of temporal patterns in refrigeration systems, while the ElectricDevices dataset, with its substantial training set of 8,926 instances and testing set of 7,711 instances, offers a broader perspective on general electrical device behavior. Additionally, the PLAID dataset, containing 1,074 instances distributed across 11 distinct device categories, enables the evaluation of our framework's generalization capabilities across diverse appliance types and usage patterns."}, {"title": "7.2 Implementation Details and Model Configurations", "content": "Our experimental framework encompasses two distinct transformer architectures, each designed to investigate the trade-offs between model complexity and energy efficiency. The first configuration, designated as T1, implements an 8-layer architecture with 8 attention heads, resulting in a parameter space of 180,041 elements, while the second configuration, T2, extends to 12 layers with 16 attention heads, encompassing 425,789 parameters, thereby providing a comprehensive spectrum for analyzing the relationship between model capacity and energy consumption characteristics."}, {"title": "7.3 Performance Analysis", "content": null}, {"title": "7.3.1 Classification Accuracy Assessment", "content": "The experimental results demonstrate a notable correlation between model complexity and classification accuracy, with the more elaborate T2 architecture consistently outperforming its compact counterpart across all datasets. Specifically, the baseline T2 configuration achieved remarkable accuracy improvements of 5.2%, 5.1%, and 5.2% over the T1 baseline for Refrigeration Devices, ElectricDevices, and PLAID datasets, respectively, while maintaining acceptable computational overhead within the constraints of our energy efficiency objectives."}, {"title": "7.3.2 Impact of Optimization Techniques", "content": "Through systematic application of our proposed optimization strategies, we observed that quantization techniques generally preserved model performance more effectively than pruning approaches, particularly in the context of the more complex T2 architecture. The implementation of 8-bit quantization resulted in a modest accuracy degradation of only 1.4-1.8% across all datasets, while achieving substantial improvements in computational efficiency and memory utilization, as detailed in subsequent sections of this analysis."}, {"title": "7.4 Energy Efficiency Evaluation", "content": null}, {"title": "7.4.1 Computational Resource Utilization", "content": "The experimental results reveal significant improvements in computational efficiency through our optimization framework, with quantized models demonstrating reduced inference times ranging from 29.5% to 34.2% compared to their baseline counterparts. Furthermore, the integration of structured pruning techniques yielded additional performance benefits, particularly in memory-constrained environments, where we observed reductions in model size of up to 38.4% while maintaining classification accuracy within acceptable bounds of degradation."}, {"title": "7.5 Statistical Significance and Error Analysis", "content": "To ensure the statistical validity of our findings, we conducted comprehensive statistical analyses across multiple experimental runs, establishing confidence intervals at the 95% level through the application of the following statistical framework:\n\n$CI_{95\\%} = \\bar{x} \\pm 1.96 \\times \\frac{s}{\\sqrt{n}}$ (8)\n\nwhere $\\bar{x}$ represents the mean performance metric across experimental iterations, s denotes the standard deviation of the measurements, and n indicates the number of experimental runs conducted for each configuration. This rigorous statistical analysis framework ensures the reliability and reproducibility of our experimental findings while accounting for variations in performance across different operational conditions and dataset characteristics."}, {"title": "7.6 Comparative Analysis with State-of-the-Art Methods", "content": "In comparison with existing state-of-the-art approaches to transformer optimization, our proposed framework demonstrates several notable advantages in terms of both energy efficiency and classification performance. The implementation of our hybrid optimization strategy, combining structured pruning with quantization-aware training, achieves a more favorable balance between computational efficiency and model accuracy than previously reported methods in the literature. Specifically, our approach demonstrates improvements of 12.3% in energy efficiency while maintaining comparable or superior classification accuracy across all evaluated datasets, representing a significant advancement in the field of energy-efficient transformer optimization for time series classification tasks."}, {"title": "7.7 Trade-off Analysis and Optimization Impact", "content": "The experimental results reveal important insights regarding the trade-offs between model complexity, energy efficiency, and classification performance. Through careful analysis of these relationships, we observe that:\n\n\u2022 Quantization Effects: The implementation of 8-bit quantization achieves a 29.2% reduction in memory footprint and a 29.5% decrease in inference time, while incurring only a minimal accuracy degradation of 3.5%, demonstrating the effectiveness of our quantization strategy in preserving model performance while substantially improving computational efficiency.\n\n\u2022 Pruning Analysis: Structured pruning techniques result in a 38.4% reduction in model parameters and a corresponding 38.5% improvement in inference time, with an acceptable accuracy trade-off of 4.2%, indicating the viability of our pruning approach for scenarios where significant reductions in model complexity are required.\n\n\u2022 Combined Optimization: The synergistic application of both quantization and pruning techniques yields cumulative benefits in terms of energy efficiency, achieving up to 45.7% reduction in overall energy consumption while maintaining classification accuracy within 5% of the baseline performance across all evaluated datasets."}, {"title": "8 Discussion", "content": "The results presented in this study highlight the trade-offs between model accuracy, inference time, energy consumption, and memory footprint when optimizing transformer models for time series classification. By systematically evaluating the effects of structured pruning and quantization across three datasets Refrigeration Devices, ElectricDevices, and PLAID-we demonstrate that applying optimization strategies significantly enhances computational efficiency while maintaining satisfactory classification performance.\n\nA key observation across all datasets is the expected reduction in accuracy following model optimization. Static quantization led to an average accuracy drop of 2.37%, while dynamic quantization resulted in a slightly higher drop of 3.14%. Similarly, L1 pruning and L2 pruning introduced accuracy degradations of 3.43% and 3.82%, respectively. However, these reductions in predictive performance were offset by substantial gains in inference speed and energy efficiency, suggesting that the trade-offs are acceptable for applications where computational efficiency is a priority.\n\nThe inference time improvements were particularly notable. Quantization techniques reduced inference latency by factors of 1.42\u00d7 (static quantization) and 1.52\u00d7 (dynamic quantization), while pruning methods achieved even greater speed-ups, with L1 pruning reaching 1.63\u00d7 improvement over the baseline. The energy savings associated with these optimizations were also substantial, with dynamic quantization reducing energy consumption by up to 33.25% and L1 pruning achieving the highest energy savings of 37.08%. This demonstrates that while accuracy slightly degrades, the impact on computational efficiency is significant, making these approaches viable for deployment in resource-constrained environments.\n\nThe energy-accuracy trade-off analysis further supports the effectiveness of these optimizations. While baseline models exhibit the highest accuracy, their energy efficiency is considerably lower than that of optimized configurations. For instance, in the T1 model configuration, static quantization improved energy efficiency to 0.150 GFLOPS/J, while L1 pruning further enhanced it to 0.168 GFLOPS/J. The T2 model, which generally outperforms T1 in accuracy, also demonstrated improved energy efficiency through quantization and pruning, achieving an overall efficiency score of 26.63 after L1 pruning. These findings indicate that a carefully selected combination of quantization and pruning can provide the best balance between efficiency and accuracy retention. Dataset-specific performance variations were also observed. The PLAID dataset exhibited the highest baseline accuracy (84.25% for T2), and even after optimization, models maintained high classification performance, with the lowest accuracy observed at 80.92% (after L2 pruning). In contrast, the RefrigerationDevices dataset, which had the lowest baseline accuracy (65.95% for T2), was more sensitive to optimization techniques, with accuracy dropping to 56.45% under L2 pruning. This suggests that the effectiveness of optimization strategies is dataset-dependent, and highly structured datasets like PLAID are more resilient to compression techniques.\n\nThe implications of these findings are significant for real-world deployments. Energy-efficient transformer models can substantially reduce operational costs and carbon footprints in applications such as predictive maintenance, appliance monitoring, and smart grid analytics. Additionally, in edge computing scenarios, where computational resources and power availability are limited, adopting a combination of quantization and pruning can enable real-time inference while maintaining acceptable accuracy levels."}, {"title": "9 Conclusion", "content": "This study presents a systematic investigation of energy-efficient optimization techniques for transformer-based architectures in time series classification. Through extensive experimentation across three distinct datasets, we have demonstrated the effectiveness of combining hardware-aware optimization strategies while maintaining classification performance. Our experimental results reveal significant variations in model performance, with the PLAID dataset achieving 84.25% accuracy (T2 baseline) compared to 65.95% for Refrigeration Devices, highlighting the importance of dataset-specific optimization approaches.\n\nThe proposed framework demonstrates substantial improvements in energy efficiency, with static quantization achieving 29.14% energy savings while maintaining reasonable accuracy trade-offs. Notably, L1 pruning techniques achieved speed-ups of 1.63\u00d7 compared to baseline models, while reducing energy consumption by 37.08%. The T2 architecture, despite higher computational complexity, provided superior accuracy-energy trade-offs across all datasets, suggesting the viability of optimizing larger models for energy-efficient deployment.\n\nThese findings have significant implications for deploying transformer models in resource-constrained environments. The variation in classification accuracy across datasets (60-84%) underscores the necessity of tailoring optimization strategies to specific application domains. Furthermore, the maintenance of high accuracy on the PLAID dataset (\u00bf80%) post-optimization demonstrates the robustness of certain time series classification tasks to model compression, providing valuable insights for future implementations in edge computing scenarios."}]}