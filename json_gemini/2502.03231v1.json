{"title": "The Other Side of the Coin: Unveiling the Downsides of Model Aggregation in Federated Learning from a Layer-peeled Perspective", "authors": ["Guogang Zhu", "Xuefeng Liu", "Jianwei Niu", "Shaojie Tang", "Xinghao Wu"], "abstract": "In federated learning (FL), model aggregation is\na critical step by which multiple clients share\ntheir knowledge with one another. However, it is\nalso widely recognized that the aggregated model,\nwhen sent back to each client, performs poorly\non local data until after several rounds of local\ntraining. This temporary performance drop can\npotentially slow down the convergence of the FL\nmodel. Most research in FL regards this perfor-\nmance drop as an inherent cost of knowledge shar-\ning among clients and does not give it special\nattention. While some studies directly focus on\ndesigning techniques to alleviate the issue, an\nin-depth investigation of the reasons behind this\nperformance drop has yet to be conducted. To\naddress this gap, we conduct a layer-peeled analy-\nsis of model aggregation across various datasets\nand model architectures. Our findings reveal that\nthe performance drop can be attributed to two\nmajor consequences of the aggregation process:\n(1) it disrupts feature variability suppression in\ndeep neural networks (DNNs), and (2) it weak-\nens the coupling between features and subsequent\nparameters. Based on these findings, we propose\nseveral simple yet effective strategies to mitigate\nthe negative impacts of model aggregation while\nstill enjoying the benefit it brings. To the best of\nour knowledge, our work is the first to conduct\na layer-peeled analysis of model aggregation, po-\ntentially paving the way for the development of\nmore effective FL algorithms.", "sections": [{"title": "1. Introduction", "content": "In recent years, federated learning (FL) has gained increas-\ning interests since it can enable multiple clients to collabo-\nratively train models without sharing their private data. FL\nis particularly attractive in fields where data privacy and\nsecurity are paramount, such as medical diagnosis (Guan\net al., 2024), where patient records must remain confiden-\ntial, and financial analysis (Imteaj & Amini, 2022), where\ntransactional data is highly sensitive.\nA standard FL process involves iterative cycles in which\nlocal models are trained on each client, followed by aggre-\ngation of these locally updated models on a central server\n(McMahan et al., 2017), as presented in Figure 1. Dur-\ning local training, each client performs multiple rounds of\nmodel updates using its private data. Once local training is\ncomplete, the updated model is uploaded to the server. The\nserver then aggregates the uploaded models by performing\nparameter-wise averaging, with each model weighted based\non factors such as the number of training samples on each\nclient (Acar et al., 2021; Li et al., 2020; Karimireddy et al.,\n2020). The aggregated model is then sent back to each client\nfor the next round of local training. By transmitting model\nparameters instead of raw data, FL ensures collaborative\nmodel training while preserving privacy.\nIn the above process, model aggregation is a key step that\nfacilitates knowledge sharing among clients in FL. How-\never, it is well known that the model aggregation often leads\nto a significant performance drop compared to the model"}, {"title": null, "content": "before aggregation (Jin et al., 2022; Lee et al., 2022; Yao\net al., 2024). This phenomenon is particularly pronounced\nwhen data distributions across clients are heterogeneous, a\ncommon scenario in practical applications due to factors\nsuch as variations in data acquisition conditions across dif-\nferent clients (Zhu et al., 2021; Li et al., 2021b). To further\ninvestigate this phenomenon, we conduct preliminary exper-\niments in a typical data-heterogeneous FL setting. Figure\n1 presents a performance comparison of the model before\nand after aggregation, evaluated on the local data from each\nclient. As shown, the performance of the aggregated model\nsignificantly deteriorates compared to the model before ag-\ngregation.\nAlthough this temporary performance drop can be mitigated\nafter several rounds of local updates, its impact persists and\ncontinues to pose a challenge. The suboptimal initialization\nin each local update, caused by model aggregation, under-\nmines the progress made in the previous round, potentially\nslowing the convergence rate of FL training. Most FL re-\nsearch treats this performance drop as an inherent cost of\nknowledge sharing among clients, giving it limited attention\n(McMahan et al., 2017; Wu et al., 2023; Li et al., 2021a).\nRecently, however, some studies have started to address this\nissue. For instance, some research attributes the problem\nto diverging model parameters caused by heterogeneous\ndata distributions and propose various model regularization\ntechniques to reduce parameter divergence (Li et al., 2020;\nKarimireddy et al., 2020). Other studies link this issue to\n'knowledge forgetting', where task-specific patterns are av-\neraged out or diluted during model aggregation, and suggest\nseveral knowledge distillation methods (Jin et al., 2022; Lee\net al., 2022). However, these studies rely on assumed causes\nof the performance drop, often without deeply exploring the\nfundamental reasons behind it.\nTo address this gap, we provide a comprehensive analysis of\nthe impact of model aggregation in FL from a layer-peeled\nfeature extraction perspective. Our focus is twofold: first,\non feature structure, which evaluates the intrinsic quality of\nthe features, and second, on feature-parameter alignment,\nwhich assesses the degree of coupling between features and\nthe parameters of subsequent layers.\nWe conduct this layer-peeled feature analysis across multi-\nple datasets and model architectures. Our findings show that\nthe performance degradation in model aggregation can be\nprimarily attributed to two factors from a layer-wise feature\nextraction perspective: (1) disruption of feature variabil-\nity suppression in deep neural networks (DNNs), and (2)\na weakening of the coupling between features and the pa-\nrameters of subsequent layers. Firstly, DNNs are known\nto progressively compress features layer by layer (Wang\net al., 2023b; Rangamani et al., 2023; Masarczyk et al.,\n2024). However, during model aggregation, this feature"}, {"title": null, "content": "compression process is compromised, leading to scattered\nfeatures. This effect accumulates across layers as the net-\nwork deepens, significantly degrading the quality of the\npenultimate layer features. Secondly, the mismatch between\nthe penultimate layer features and the classifier worsens\nafter aggregation, further deteriorating model performance.\nBased on these findings, we propose several simple yet ef-\nfective strategies to directly address these issues, thereby\nimproving the FL model performance.\nThe contributions of this paper are as follows:\n\u2022 To the best of our knowledge, we are the first to compre-\nhensively understand the impact of model aggregation\nfrom a layer-peeled feature extraction perspective.\n\u2022 We conduct an extensive layer-peeled feature eval-\nuation across multiple datasets and model architec-\ntures, which uncovers the major causes of performance\ndrop in model aggregation. This analysis examines\nboth feature quality and feature-parameter alignment,\nwhile also highlighting the challenges introduced by\nthe stacked architecture of DNNs.\n\u2022 We propose strategies to improve the aggregated model,\naddressing key drawbacks and enabling the develop-\nment of enhanced FL algorithms based on our insights."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Problem Formulation of FL", "content": "In this paper, we consider a standard FL system consisting\nof a central server and M distributed clients. We assume\nthat each client m contains Nm training samples, which\nare drawn from the data distribution Dm. In practice, the\nunderlying data distribution Dm for each client is typically\ndifferent from one another due to the variations in data\ncollection conditions. Formally, the training samples on\nclient m can be represented as $(x_i^m, Y_i^m)_{i=1}^{N_m}$, where $x \\in$\n$X_m \\subset \\mathbb{R}^n$ denotes the raw input data for the DNNs, and\n$Y'm \\in \\mathbb{V}_m \\subseteq {0,1}^C$ represents the corresponding ground\ntruth labels used to optimize the DNNs, with C denoting\nthe number of classes.\nWe denote the DNN for client m as $\u0424_m(\u00b7)$, with parameters\nrepresented by $\u04e8_m$. The optimization objective for an FL\nsystem can then be formulated as:\n$\\arg \\min_{\\Theta_1,...,\\Theta_M} L(\\Theta_1, \\dots, \\Theta_M) \\approx \\arg \\min_{\\Theta_1,...,\\Theta_M} \\frac{1}{M} \\sum_{m=1}^{M} L_m(\\Theta_m),$\nwhere $L_m (\u0398_m)$ represents the empirical risk for client m,\nwhich is computed based on its private data samples, as\nshown in the following equation:"}, {"title": null, "content": "$L_m(\\Theta_m) := \\frac{1}{N_m}\\sum_{i=1}^{N_m} l(Y_i^m, \\hat{Y}_i^m),$\nwhere $\\hat{Y}_i^m = \u0424_m(x_i^m; \u0398_m)$ represents the predicted output\nof $x_i^m$ given the model $\u0424_m(\u00b7)$ for $x_i^m$, $l: Y \u00d7 Y \u2192 \\mathbb{R}$ is\nthe loss function used to measure the prediction error.\nTo optimize Equation (1) in a privacy-preserving manner, FL\nis typically carried out in two iterative stages: local model\ntraining and global model aggregation. During the local\nmodel training phase, each client optimizes its model for E\nepochs by minimizing the loss function defined in Equation\n(2). Once local training is complete, each client uploads\nits updated model to the server. The server then performs\nmodel aggregation to generate the global model. A common\naggregation strategy involves applying a weighted average\nfor each parameter within the model based on the number of\ntraining samples per client, which is expressed as follows:\n$\u0398 = \\frac{\\sum_{m=1}^{M} N_m \u0398_m}{\\sum_{m=1}^{M} N_m},$\nHere, \u00c9 represents the aggregated global model. By re-\npeating the above procedures for several rounds, the model\neventually converges, resulting in the final FL model. For\nsimplicity, we will sometimes refer to the locally updated\nmodel $\u0398_m$ as the pre-aggregated model, and \u00c9 as the post-\naggregated model. Additionally, we may omit the client\nand sample indices for simplicity.\nIn this paper, we focus on uncovering how the feature ex-\ntraction process varies across model depth due to model ag-\ngregation. We reformulate the parameters of the FL model\nas $\u0398 = {W^l}_{l=1}^L$, where L represents the total number of\nlayers in the model, and the depth increases with l. This\nstacked DNN progressively transforms the input data into\nprediction outputs, from the shallow layers to the deeper lay-\ners. However, there is still a lack of investigation into how\nmodel aggregation affects layer-wise feature extraction. In\nthis paper, we aim to explore the variations in intermediate\nfeatures during layer-wise feature extraction. The features\nof the l-th layer for an input sample x can be formulated as:\n$z_l = W_l z_{l-1} = W_{l:1}x, l = 1, ..., L -1$\nwhere $z_l$ denotes the intermidiated features of l-th layer\nand we denote $z_0 = x."}, {"title": "2.2. Feature Evaluation Metric", "content": "We apply the following metrics to evaluate the features gen-\nerated by the models before aggregation and after aggrega-\ntion, including the feature variance, alignment between fea-\ntures and parameters, accuracy of linear probing, pairwise"}, {"title": null, "content": "\u2022 Feature Variance. We calculate the normalized within-\nclass variance, which quantifies feature compression\nwithin the same class, and the normalized between-\nclass variance, which measures the discrimination be-\ntween different classes. Together, these metrics assess\nthe quality of features given the evaluated dataset.\n\u2022 Alignment between Features and Parameters. We\nassess the alignment between features and parameters\nto evaluate the degree of coupling between them during\nthe feature extraction process.\n\u2022 Accuracy of Linear Probing. This metric evaluates\nthe generalization of features across different distribu-\ntions. We attach a randomly initialized linear layer to\nfeatures extracted from various layers of DNNs, then\ntrain this linear classifier on a evaluated dataset and\nreport the average testing accuracy.\n\u2022 Pairwise Distance of Features or Models. This met-\nric evaluates the differences between the parameters\nor features generated by the models before and after\naggregation.\n\u2022 Relative Change of Evaluated Metrics. This mea-\nsures the change in the metrics after aggregation, offer-\ning insights into the degree of impact that aggregation\nintroduces to feature extraction."}, {"title": "3. Evaluation Setup", "content": ""}, {"title": "3.1. Dataset Description and Partition", "content": "We focus on the cross-domain FL setting in our experi-\nments, which is a common scenario for FL with heteroge-\nneous data. This frequently appears in practical applications\nsuch as medical image analysis, traffic surveillance, and\nautonomous driving (Li et al., 2021b; Zhu et al., 2022).\nBuilding upon previous studies (Li et al., 2021b; Zhu et al.,\n2022; Huang et al., 2023), we use three public datasets to\nestablish the FL system: Digit-Five, PACS (Li et al., 2017),\nand DomainNet (Peng et al., 2019). Each of these datasets\nconsists of data samples from different domains, with each\ndomain containing images in various styles.\nFor these datasets, the samples (including both training and\ntests datasets) from each domain are assigned to a client to\nconstruct a cross-domain FL system. The number of training\nsamples for each client is set to 500 for all datasets. For\nDigit-Five, the testing dataset contains 1,000 samples for\neach client. For DomainNet and PACS, the testing datasets\ncontain 500 samples for each client."}, {"title": "3.2. Implementation Details", "content": "We evaluate feature variations between the pre-aggregated\nand post-aggregated models across various architectures.\nFor Digit-Five, we adopt a convolutional network followed\nby fully connected (FC) layers (ConvNet), ResNet18, and\nResNet34. For DomainNet and PACS, the implemented\nmodels including VGG13_BN (Simonyan & Zisserman,\n2014), three variants of ResNet (ResNet18, ResNet34, and\nResNet50) (He et al., 2016), and ViT_B-16 (Dosovitskiy\net al., 2021). Unless otherwise specified, the models are\ntrained from randomly initialized parameters.\nWe apply the standard FedAvg (McMahan et al., 2017)\nalgorithm for federated training. Stochastic gradient descent\n(SGD) is used to optimize the models on local clients, with\na learning rate of 0.01 and a momentum of 0.5. The batch\nsize for local training is set to 64. Local models are trained\nfor 10 epochs within each global round, with a total of 50\nglobal rounds. On the server, we perform model aggregation\nby weighted averaging the parameters, using the number of\ntraining samples as the weight for each client. To reduce the\nimpact of randomness, each experiment is repeated three\ntimes with different random seeds.\nWe extract features from various layers, ranging from shal-\nlow to deep, for feature evaluation. Specifically, we evaluate\nthe features passed to each convolutional layer, pooling\nlayer, and FC layer in the convolutional neural network\n(CNN). For ViT_B/16, we evaluate the features passed to\nthe multilayer perceptron (MLP), self-attention layers, layer\nnormalization layers, and the final classifier."}, {"title": "4. Main Results", "content": "In this section, we present the key findings observed across\nmultiple datasets and model architectures during our layer-\npeeled analysis of model aggregation in FL. Due to space\nlimitations, we only present representative results here.\nMore detailed results are available in the Appendix."}, {"title": "4.1. Understanding Model Aggregation in FL", "content": ""}, {"title": "4.1.1. MODEL AGGREGATION LEADS TO PERFORMANCE\nDROP ON LOCAL DATA", "content": "We first confirm the claim that aggregating multiple local\nmodels trained on each client results in a significant per-\nformance drop when the aggregated model is sent back"}, {"title": null, "content": "to the local clients for inference. Figure 2 shows the av-\nerage training and testing accuracy of the models before\nand after aggregation during the FL training process. It\ncan be observed that the aggregated model suffers from a\nsubstantial performance degradation across various model\narchitectures, both on the training and testing datasets. This\ntemporary performance drop erases the gains made during\nthe previous local updates on clients, thereby slowing down\nthe convergence rate of FL training. Therefore, we seek to\nunderstand what happens during model aggregation from\nthe perspective of layer-peeled feature extraction."}, {"title": "4.1.2. MODEL AGGREGATION DISRUPTS FEATURE\nVARIABILITY SUPPRESSION", "content": "Previous studies have shown that deep neural networks\n(DNNs) progressively suppress raw data, generating task-\nspecific features. However, in our experiments, we observe\nthat model aggregation disrupts this training objective of\nDNNs. Specifically, we make the following observations:"}, {"title": null, "content": "(1) Features become increasingly compressed within the\nsame class as training progresses and layer depth in-\ncreases. As shown in Figures 3 (a) and 5 (a), during train-\ning, the features within the same class become progressively\nmore compressed. Moreover, as the layers deepen, the\nfeatures become also more compressed. This observation\nsuggests that, from the perspective of feature variation sup-\npression, the overall training objective of FL aligns with\nwhat has been observed in centralized learning (CL).\n(2) Features become increasingly discriminative across\ndifferent class as training progresses and layer depth\nincreases. In contrast to the normalized within-class feature\nvariance, the normalized between-class feature variance in-\ncreases as the training progresses and layer depth increases,\nas shown in Figure 4 (a) and 6 (a). This demonstrates that\nthe features across different classes become increasingly\ndiscriminative.\n(3) Model aggregation disrupts feature variation sup-\npression during FL training. As shown in Figures 3 (a)"}, {"title": null, "content": "and 5 (a), the normalized within-class variance increases,\nwhile the normalized between-class variance decreases after\nmodel aggregation. This is in contrast to the training ob-\njective of DNNs. Additionally, the feature visualization in\nFigure 7 clearly shows the features generated by different\nlayers of the models before and after aggregation, providing\nfurther evidence of this disruption.\n(4) Deeper features begin to compress only after model\naggregation. As shown in Figures 3 - 6, during the training\nof the FL model, the shadow features initially converge\nto a certain level of feature variation and then the deeper\nlayer can start to converge. If this condition is not met,\nmodel aggregation may cause the features to scatter from\nthe shadow and deeper layers. This characteristic makes the\nconvergence of the model aggregation more challenging."}, {"title": "4.1.3. FEATURE VARIATION DISRUPTION ACCUMULATE\nAS MODEL DEPTH INCREASES", "content": "In experiments, we find that the stacked architecture of\nDNNs has specific impacts on model aggregation. Specif-\nically, we observe that the disruption of feature variation\nsuppression progressively accumulates as the depth of layer\nincreases. As shown in Figures 3 (b) and 5 (b), we compute\nthe relative changes in normalized within-class variance."}, {"title": null, "content": "It can be observed that the relative changes in normalized\nwithin-class variance progressively increase as we move\ndeeper into the DNNs. This issue suggests that DNNs may\nnot be ideal for model aggregation in FL training.\nAdditionally, we examined the layer-wise parameter and\nfeature distances between the model before and after aggre-\ngation. The results are shown in Figure 8. The magnitudes\nof the parameter distances are much smaller compared to\nthe feature distances. The parameter distance shows a de-\ncreasing trend as the model deepens, except for the final\nclassifier, while the feature distance shows an increasing\ntrend with deeper layers. This phenomenon suggests that\nthe performance drop may not only be due to parameter\ndivergence but could be more closely related to the accumu-\nlation of feature disruption. This issue causes the features in\nthe penultimate layer to degrade more significantly, which\ncan substantially affect the model's performance."}, {"title": "4.1.4. FEATURE MISMATCH WITH PARAMETERS IN\nFINAL DECISION STAGE", "content": "In the previous sections, we observed that the features from\nthe model experience significant quality degradation due to\nthe accumulation of feature disruption across the depth of"}, {"title": null, "content": "DNNs. However, the final decision of DNNs is not only\ninfluenced by the feature quality, but also by the model pa-\nrameters that progressively process the generated features.\nFigures 9 and 10 show the alignment between features and\nthe subsequent parameters. It is evident that there is a dis-\nruption in this alignment after model aggregation. This\ndisruption exhibits an accumulation phenomenon similar\nto feature variations, which causes the penultimate layer's\nfeatures to significantly mismatch with the subsequent clas-\nsifier. This mismatch further degrade the performance."}, {"title": "4.1.5. MODEL AGGREGATION IMPROVES MODEL\nGENERALIZATION", "content": "In this section, we demonstrate that model aggregation im-\nproves the model's generalization across various datasets.\nWe perform linear probing on both the pre-aggregated and\npost-aggregated models using different datasets from local\nclients. As shown in Figure 11, after local training on clients,\nthe linear probing accuracy on its local data generally im-\nproves, indicating that the model gradually adapts. How-\never, when tested on datasets from other clients, the model\nperforms worse. This demonstrates that the model before\naggregation struggles to extract universal features applica-\nble across different clients. In contrast, the post-aggregated\nmodel exhibits much better performance, suggesting that it\ngeneralizes more effectively, as the knowledge learned from\ndifferent clients is fused through the aggregation process."}, {"title": "4.2. More Factors Related to Feature Extraction", "content": ""}, {"title": "4.2.1. PARAMETER PERSONALIZATION", "content": "Parameter personalization is a key method in personalized\nfederated learning (PFL). In this section, we conduct experi-\nments to investigate how parameter personalization impacts\nthe feature extraction process. We first examine two typical\nPFL methods: FedPer (Arivazhagan et al., 2019), which\npersonalizes the classifier, and FedBN (Li et al., 2021b),\nwhich personalizes the batch normalization (BN) layers.\nAdditionally, inspired by (Sun et al., 2021), we explore\na successive personalization approach that targets multi-\nple layers, starting from the first layer. The experimental\nresults are presented in Figure 12. As shown, personaliz-\ning more parameters within the feature extractor generally\nresults in more compressed within-class features and less\nrelative change after model aggregation. This effect can be\nattributed to the fact that personalizing shallow layers helps\nmitigate the accumulation of feature degradation, where the\nfeature extraction capacity learned from local data in these\npersonalized layers remain unaffected by model aggregation."}, {"title": "4.2.2. LOCAL UPDATING EPOCHS", "content": "In this section, we conduct experiments to investigate its\nimpact on model aggregation. The experimental results are\npresented in Figure 13. To ensure a fair comparison, we\nmaintain a consistent total number of updating epochs across\ncomparisons. The results show that increasing the number\nof local epochs will compress the within-class features more\neffectively. However, when the models are aggregated, the"}, {"title": null, "content": "relative change for larger local update epochs is noticeably\ngreater than for smaller ones. This highlights the sensitivity\nof the model aggregation process to the number of local\nupdating epochs."}, {"title": "4.2.3. RESIDUAL CONNECTION", "content": "Since residual connections have become a standard compo-\nnent in modern DNNs, we explore their impact on model ag-\ngregation. Figure 14 compares the results with and without\nresidual connections. The experiments are conducted using\nResNet34 on the DomainNet dataset. We removed the resid-\nual connections from ResNet34 while keeping the rest of\nthe architecture unchanged, resulting in the Plain_ResNet34\nmodel. As shown in Figure 14, the relative change in the\nnormalized within-class feature variance, as well as the\nalignment between features and parameters, is greater in the\nPlain_ResNet34 compared to the original ResNet34. This\nmay because that residual connections help reduce feature\ndegradation by allowing less disrupted features from earlier\nlayers to be passed to deeper layers. This suggests that resid-\nual connections potentially help mitigate the accumulation\nof feature degradation during model aggregation."}, {"title": "4.3. Mitigating Negative Impacts of Model Aggregation", "content": "In this section, we provide several simple yet effective strate-\ngies that can help us to mitigate the downsides introduced by\nmodel aggregation, including initialization with pre-trained"}, {"title": "4.3.1. INITIALIZATION WITH PRE-TRAINED\nPARAMETERS ON LARGE-SCALED DATASET", "content": "As shown in Figures 15 and 16, both the normalized within-\nclass variance and the alignment between features and pa-\nrameters exhibit less changes compared with those trained\nfrom randomly initialized parameters. This is likely be-\ncause, after pre-training, the shallow layers are capable of\nextracting meaningful features, allowing the model to fo-\ncus on training the deeper layers. This helps alleviate the\naccumulation of feature degradation during model training,\nmaking the pre-trained model more suitable for FL."}, {"title": "4.3.2. FINE-TUNING CLASSIFIER WHEN RELOADING\nGLOBAL FEATURE EXTRACTOR", "content": "As mentioned earlier, when the global model is sent back to\nlocal clients, the features from the penultimate layer exhibit\nan increased mismatch with the subsequent classifier. In this\nsubsection, we show that simply fine-tuning the classifier\ncan significantly enhance the models performance. Figure\n17 illustrates the accuracy and feature-parameter alignment\nwhen fine-tuning the classifier at various global rounds dur-\ning training. It can be observed that after fine-tuning the\nclassifier, the alignment between the features and the classi-\nfier consistently improves as the FL training progresses. As\na result, the testing accuracy also improves."}, {"title": "5. Conclusion", "content": "To the best of our knowledge, this paper presents the first at-\ntempt to explain model aggregation in FL from the perspec-\ntive of layer-peeled feature analysis. We focus on the perfor-\nmance drop in model aggregation. Through our layer-peeled\nfeature analysis, we identify several factors that contribute\nto this performance drop. In particular, from the perspec-\ntive of feature variations suppression, we observe that the\nfeatures after aggregation tend to be less compressed than\nbefore. More importantly, due to the stacked architecture of"}, {"title": null, "content": "modern DNNs, this phenomenon accumulates throughout\nthe feature extraction process, significantly degrading the\nquality of the final features used for decision-making. As a\nresult, training DNNs using FL becomes increasingly chal-\nlenging. Additionally, we find that the penultimate layer\nfeatures exhibit significant mismatches with the classifier af-\nter aggregation. This further degrade the performance of the\naggregated model. Building on these insights, we explore\ntechniques to mitigate these issues and, in turn, enhance the\nmodel performance. This work deepens the understanding\nof model aggregation in FL and provides a new perspective\nfor developing more effective FL algorithms."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here."}]}