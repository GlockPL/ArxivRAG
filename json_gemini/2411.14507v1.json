{"title": "FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers", "authors": ["Zehua Pei", "Hui-Ling Zhen", "Xianzhi Yu", "Sinno Jialin Pan", "Mingxuan Yuan", "Bei Yu"], "abstract": "Generative Pre-trained Transformers (GPTs) have demonstrated remarkable performance across diverse domains through the extensive scaling of model parameters. Recent works observe the redundancy across the transformer blocks and develop compression methods by structured pruning of the unimportant blocks. However, such straightforward elimination will always provide irreversible performance degradation. In this paper, we propose FuseGPT, a novel methodology to recycle the pruned transformer blocks to further recover the model performance. Firstly we introduce a new importance detection metric, Macro Influence (MI), to detect the long-term influence of each transformer block by calculating their loss of information after removal. Then we propose group-level layers fusion, which adopts the parameters in layers of the unimportant blocks and injects them into the corresponding layers inside the neighboring blocks. The fusion is not one-off but through iterative parameter updates by lightweight group-level fine-tuning. Specifically, these injected parameters are frozen but weighted with learnable rank decomposition matrices to reduce the overhead during fine-tuning. Our approach not only works well on large language models but also on large multimodal models. The experiments have shown that, by using modest amounts of data, FuseGPT can outperform previous works in both perplexity and zero-shot task performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative Pre-trained Transformers (GPTs) have proven to handle complex tasks and exhibit emergent abilities in various domains and tasks, especially when scaling up to billions of parameters [1]-[4]. While GPTs have achieved unprecedented success, the growing complexity and size have brought increasing pressure on deployment in real-world scenarios, particularly in resource-constrained environments.\nTo mitigate the challenges of the hardware requirements for GPTs deployment, model compression techniques are developed to produce compact models while preserving high performance. The techniques of model compression primarily fall into two categories: model pruning and quantization [5]-[9]. In this paper, we focus on model pruning, the technique to reduce the model size by eliminating redundant parameters. Pruning is mainly divided into two types: unstructured and structured [10], [11]. Unstructured pruning tries to remove individual weights, which can achieve higher performance but hinder the model acceleration due to the hardware-unfriendly sparse weights. Structured pruning, on the other hand, pruning the entire pre-defined model structure at once, which may slightly decrease accuracy but is more efficiently processed by hardware.\nRecently, researchers have found that there exists redundancy across transformer blocks of the GPTs, i.e. some blocks seem to offer less contribution to the final results [12]-[14]. Some of them detect the redundancy by assessing the similarities between hidden states, and some others directly measure the changed distance to the hard label. After collecting the redundant blocks, they structured pruning the most unimportant of them from the model to prevent performance degradation as much as possible. However, directly discarding the blocks always leads to irreversible performance degradation. Traditional post-pruning fine-tuning may be helpful to recover the performance, but these techniques always need extremely large datasets and training resources. Therefore, efficient performance recovery techniques are urgently desired.\nIn this paper, we propose FuseGPT, a novel and efficient methodology for structured pruning. We observe the fact that despite some blocks being redundant, they still carry important pre-trained knowledge. It is intuitive to let it participate in the process of performance recovery. Therefore, we implement a full-scale technique to recycle and fuse the redundant blocks into other blocks before they are pruned. Firstly, a novel metric to detect the unimportant blocks is proposed that can achieve overall long-term influence assessment. During importance detection, we also consider the factor that how difficult it is to fuse them back, which is determined by the information loss caused by the block removal. Then we take steps to spread the parameters of layers inside the detected blocks to neighboring blocks. It is achieved by multiplying them with a learnable coefficient and injecting the weights into corresponding layers. Knowledge learning is then employed to update the parameters, which implicitly transfers the knowledge stored in the detected blocks. We ensure efficiency by restricting the fusion and parameter update processes within a partial group of blocks, and we constrain the coefficient matrix by decomposing it into low-rank matrices. Fig. 1 compares the proposed approach with other pruning methods.\nIn summary, the key contributions of this paper are:\nFuseGPT represents a novel methodology for performance recovery after redundant transformer blocks pruning. This approach not only presents high effectiveness but also shows outstanding efficiency.\nTo measure the long-term influence of block removal, this paper proposes a novel metric, Macro Inference (MI), for importance detection across the transformer"}, {"title": "II. RELATED WORK", "content": "To cut down the inference cost of large language models and enhance their practical applications, numerous recent studies have focused on model compression. These studies can be categorized into two types: model pruning and quantization [15]\u2013[23]. Additionally, there are also some works that aim to explore the redundancy of models, as it is crucial for model compression.\nQuantization, which includes post-training quantization and quantization-aware training, has emerged as a highly favored approach in the domain of model compression. Conventionally, models are stored as floating-point values, but quantization offers an alternative by converting them into integer or other discrete representations. LLM.int8() has been firstly employed vector-wise quantization for the majority of the weight matrix [24]. Regarding the emergent outliers, they introduced a novel mixed-precision decomposition scheme, isolating the outlier feature dimensions into a 16-bit matrix multiplication. GPTQ proposed a more accurate data-aware approach by means of an approximate large-scale solver to minimize layer-wise L2 errors [25]. SmoothQuant [26] not only quantized both the weights and activations but also offline transferred the quantization difficulty from activations to weights through a mathematically equivalent transformation.\nPruning techniques, including both unstructured and structured ones, are employed to identify and remove redundant or less significant parameters from models, thus leading to a sparser weight matrix. ShortGPT [12] has put forward a straightforward layer removal approach that is based on Block Influence determined by the similarity between a layer\u2019s input and output. Along this line, SLEB [14] has proposed a fundamental unit for pruning transformer blocks, enabling effective improvement in the processing speed of LLMs. In contrast, SliceGPT [27] replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. FoldGPT [28] combines block removal and block parameter sharing. This work comprises two parts. Firstly, block importance, based on learnable gating parameters, determines the redundant layers according to the given removal rate. Secondly, for the retained blocks, a specially designed group parameter-sharing strategy is proposed to compress the number of parameters and slightly lower latency overhead.\nKnowledge Distillation is widely used to transfer knowledge from a large model (teacher) to a smaller one (student) for improved efficiency, especially in the context of LLMs. DistilBERT [29] reduces the transformer\u2019s layers in the teacher network by half and initializes the student by choosing one layer out of every two from the teacher. In contrast, MiniLM [30] simplifies the process by distilling knowledge solely from the self-attention module of the last Transformer block, thus alleviating the challenge of layer mapping. However, block removal and group parameter sharing based on the pre-trained model lead to additional performance degradation."}, {"title": "III. METHODOLOGY", "content": "In this section, we describe our work FuseGPT from preliminaries to the details of importance detection on transformer blocks and the pipeline of learnable layers fusion for performance recovery.\nPreliminaries\nIn this study, we primarily focus on the standard GPTs with decoder-only transformer architecture. A GPT model is"}, {"title": "A. Preliminaries", "content": "constructed by sequentially arranging multiple transformer blocks, each consisting of self-attention and feed-forward layers. Suppose there are a series of transformer blocks inside a GPT model M, \u0456.\u0435.  $B_M = {B_1, B_2, ..., B_n}$. The problem is to reduce the number of transformer blocks in the GPT model while maintaining the model performance as much as possible.\nTypically, researchers solve this by directly removing blocks in the GPT model to obtain a subset of transformer blocks $B'_M \\subset B_M$. Therefore, the problem becomes how to maintain the performance of the pruned model after blocks removal, and the key point is to remove the blocks that are relatively less important. We denote the process of choosing the unimportant block as importance detection. Since the residual path is added inside each transformation block, it is straightforward to detect the importance of blocks by measuring their similarity between the outputs of different transformer blocks during inference. Denote $X_{i-1}$ and $X_i$ ($i >= 1$, and $X_0$ denotes the input of B) as the hidden states of two neighboring transformer blocks, ShortGPT [12] uses the metric Block Influence (BI) to measure the expectation of cosine similarity on the distribution for each token in a sequence, i.e. each t-th row of them, and the importance on i-th block is calculated as follows:\n$BI_i = 1 - E_{X,t} \\Big[ \\frac{(X_{i-1,t}, X_{i,t})}{||X_{i-1,t}||_2||X_{i,t}||_2} \\Big]$.\n(1)\nA lower BI score indicates a small change by passing through the corresponding transformer block, which may represent lower importance during inference.\nHowever, such an approach fails to consider the subsequent influence of the minor changes of the block removal can also be large. Therefore, SLEB [14] considers the overall model inference to evaluate the importance of the transformer block, i.e. calculating the loss of token prediction on the pruned model. Given a calibration tokenized sequence as $x = {x_1,..., X_K}$ and denote the algorithm as SLEB score, it is calculated as follows:\n$SELB_i = \\frac{1}{K} \\sum_{k=0}^{K} log P_{M'_i}(x_k|x_{<k})$.\n(2)\nwhere the block to remove is iteratively selected by assessing the pruned model $M'_i$, i.e. previous pruned model $M'$ after removing the i-th block.\nWith the two examples described above, we formulate the importance detection problem as follows:\nProblem 1 (Importance Detection). Given a generative pre-trained transformer model, the objective of importance detection is to develop a metric that identifies redundant or unimportant transformer blocks within the model, such that their removal minimizes performance degradation.\nDespite a well-designed importance detection algorithm, the performance degradation of the pruned model is irreversible and becomes more severe as more blocks are removed. Therefore, methods to recover performance are"}, {"title": "B. Importance detection via Macro Inference", "content": "As described in the preceding section, the importance of transformer blocks should be detected through overall model inference, to evaluate the long-term influence but not the local changes. Therefore, we propose a new metric, Macro Influence (MI), to measure the final influence of each block after removal. Denote the origin model and the pruned model with the removal of i-th block as M and $M_i$, respectively. For convenience, we denote their last hidden states, i.e. the output of the last transformer block, as $X_M$ and $X_{M_i}$, respectively. By taking the cosine similarity between each row t of them into consideration, the MI score of i-th block can then be calculated as follows:\n$MI_i = 1 - E_{X,t} \\Big[ \\frac{(X_{M,t}, X_{M_i,t})}{||X_{M,t}||_2||X_{M_i,t}||_2} \\Big]$.\n(3)\nWhen removing one block to have little influence to the last hidden states, we expect that they will have high cosine similarity compared with the origin one and thus get a lower MI score.\nThe same is done for overall model inference, MI score works differently from the SELB score. Compared with directly using the token prediction results that calculates the loss on a hard label to measure the distance to the ground truth, we prefer considering the perturbation on the original results. By doing this, we can observe how one block matters to contribute to the original results and hence provide better instruction for performance recovery in later stages. As described by Hinton et al. [32], the soft targets have high entropy and hence can provide much more information than hard targets, thus by calculating the MI score we actually"}, {"title": "C. Performance Recovery via Layers Fusion", "content": "Given the MI scores on each transformer block in $B_M$, we rank them in descending order to form a new set $B_{MI} = {BMI_1, BMI_2, ..., BMI_n}$. Firstly, we determine the removed block to be the the first-ranking block $BMI_1$, i.e. the block with the highest MI score thus the most unimportant block.\nDifferent from previous works, our goal is not simply abandoning it. Instead, we expect that it can contribute to the following performance recovery. According to this objective, we develop the idea to fuse the removed block to neighboring blocks, by doing this we hope that the functionalities belonging to it will be inherited by other blocks thus reducing the performance degradation.\nSuppose that the original index of the block to prune is p, i.e. $BMI_1 = B_p$. Denote the group size G as a positive integer and the origin index range as [1,n], we set a fixed partial group around $B_p$ for partial fusion and fine-tuning. The partial group $B_{partial}$ is defined as follows:\n$B_{partial} = {B_i: i \\in I, I \\subseteq N}$,\nwhere $I=$\n(4)\n$\\{\np - \\lfloor \\frac{G}{2} \\rfloor, ..., p + \\lfloor \\frac{G}{2} \\rfloor\\} \\ \\lfloor \\frac{G}{2} \\rfloor < p \\leq n - \\lfloor \\frac{G}{2} \\rfloor \\\\\\{1, ..., G + 1\\} \\hspace{5.7cm} 1 \\leq p \\leq \\lfloor \\frac{G}{2} \\rfloor \\\\\\{n - G, ..., n\\} \\hspace{6.3cm} n - \\lfloor \\frac{G}{2} \\rfloor < p \\leq n$\nwhere $|B_{partial}|= G + 1$, i.e. $G$ neighboring blocks together with $B_p$, and we averagely split the G blocks on both sides of $B_p$ in default. The strategy of arranging a partial group to do fusion and fine-tuning offers dual advantages. On the one hand, it maintains computational efficiency for contemporary large-scale GPTs. On the other hand, the blocks adjacent to the pruned block are expected to have similar functionalities, which meets our objective since fusing similar blocks will reduce the difficulty.\nIn each transformer block, we treat the weights on linear layers as the fundamental unit for fusion, which account for most of the parameters and take primary responsibility for functionality. Suppose we want to fuse the block to prune $B_p$ into one of the blocks $B_i$ inside the partial group $B_{partial}$, we denote a linear layer in $B_p$ as $l_{p,j}$ and the corresponding layer $l_{i,j}$ in $B_i$ that serves the same functional role, e.g. both are the first linear layer of the feed-forward module. Denote $W_{i,j} \\in R^{d \\times k}$ and $W_{p,j} \\in R^{d \\times k}$ as the weights of $l_{i,j}$ and $l_{p,j}$, and we employ a learnable matrix $C \\in R^{d \\times k}$ as coefficient for $W_{p,j}$. Then they are weighted added together as the weight of the fused layer $l_{fused}^{i,j}$.\n$W_{fused}^{i,j} = W_{i,j} + C \\odot W_{p,j}$,\n(5)\nwhere $\\odot$ conducts the element-wise matrix/tensor product. Inspired by LoRA [33] to increase the computation efficiency, we further constrain the coefficient C by representing it with a low-rank decomposition $C = C_{left}C_{right}$, where $C_{left} \\in R^{d \\times r}$, $C_{right} \\in R^{r \\times k}$, and the rank $r < min(d, k)$. Then the forward pass of the fused linear transformation becomes:\n$W_{fused}^{i,j}x = (W_{i,j} + C \\odot W_{p,j})X$\n$= (W_{i,j} + (C_{left}C_{right}) \\odot W_{p,j})X$\n$= W_{i,j}X + (C_{left}C_{right}) \\odot W_{p,j}X$.\n(6)\nWe initialize $C_{right}$ with Kaiming initialization [34] and zero for $C_{left}$ to build a good start-point of learning. During fine-tuning, the $W_{p,j}$ is frozen and the gradient updates is conducted on $C_{left}$, $C_{right}$ and $W_{i,j}$. We keep doing the fusion for all the linear layers in the Block $B_i$ to obtain $B_i^{fused}$, and then doing fusion for all the blocks inside $B_{partial}$ (except $B_p$). Finally we remove $B_p$ from the group and obtain the fused partial group $B_{partial}^{fused} = {B_{1}^{fused},..., B_{G}^{fused}}$ and $|B_{partial}^{fused}|= G$.\nThe above process maintains the group on the states that just remove the $B_p$ but gives the potential to extract useful information from it by the weights injection. Similar to our method in importance detection, we would like to consider the information loss to implement knowledge learning for performance recovery. Denote the last hidden states after sequentially processed by the blocks of $B_{partial}$ and $B_{partial}^{fused}$ as $X_{partail}$ and $X_{fused}^{partail}$, respectively. Typical hidden states X of GPTs are 3D tensors with dimensions (batch_size, sequence_length, hidden_size) that represent neural network activations at each block. We first calculate the probability distributions of $X_{partail}$ and $X_{fused}^{partail}$ on the dimension of the batch_size, where the softmax is computed on the values across different batches on the same position of the sequence_length and hidden_size. The distributions are computed using the softmax function along the first dimension as follows:\n$P_{partial} = softmax(X_{partial}, dim = 0)$,\n(7)\n$p_{fused}^{partial} = softmax(X_{fused}^{partial}, dim = 0)$.\n(8)\nThe reason that we choose to calculate softmax along the batch_size dimension but not the hidden_size dimension (features for each token) is that we find the latter will cause loss explosion during fine-tuning, therefore we use the former for the training stability."}, {"title": "Algorithm 1 FuseGPT algorithm. We iteratively conduct importance detection and layers fusion until the target number of blocks is pruned.", "content": "Input: original model M, calibration dataset C, # blocks of Mn, # blocks to prune N\nfor i = 0 to N-1 do\n\u0412\u043c \u2190 {B1, ..., Bn-i}\n// Importance Detection\nfor j = 0 ton-i-1 do\nS\u2190 MI (M, C)\nif $S_{j} < S_{min}$ then\nS \u2190 S\nBp \u2190 Bj\nend if\nend for\n// Layers Fusion\nM\u2190 layers_fusion(M, Bp)\nend for\nSuch influence will even be more significant after we do the fusion and knowledge-learning processes.\nWe summarize the overall algorithm of FuseGPT in Algorithm 1. In each iteration, we calculate the MI score to detect the most unimportant block. Then we apply group-level layers fusion as in Algorithm 2. We create a partial group of blocks around the detected block to prune. Then reparameterization is done on the layers of the partial group by fusing them with the corresponding layers in the detected block. In the end, lightweight partial group fine-tuning is performed to learn the fusion from knowledge loss."}, {"title": "Algorithm 2 Group-level Layers fusion. Fuse the layers inside the block to prune into the group of neighboring blocks. Then conduct partial group fine-tuning for performance recovery.", "content": "Input: original model M, block to prune Bp, fine-tuning dataset D, partial group size G\nBpartial \u2190 get_parital_group(M, Bp, G)\nfor each block Bi in Bpartial do\nfor each layer lij in Bi do\nlpj \u2190 layer to fuse in Bp\nWi,j,Wp,j \u2190 weights of lij, lpj\nC = Cleft Cright \u2190 low-rank coefficient\nWfused \u2190 Wij + C \u2299 Wpj\nBfused \u2190 fused layer with weight Wfused\nend for\nBfused \u2190 fused block\nend for\nBfused \u2190 {Bfused, 1,..., Bfused}\nCompute the KL divergence loss LKL with D\nUpdate Bfused by minimizing LKL\nM \u2190 group_replace(M, Bpartial, Bfused)\nOutput: fused model M\nWith Ppartial and Pfused, we calculate the Kullback-Leibler (KL) divergence loss LKL between them as:\n$LKL(Ppartial\\|\\Pfusedl)=\\sum_i \\Ppartial,i log(\\frac{\\Ppartial,i}{\\pfused_{\\partial,i}})$\n(9)\nwhere |P| calculates the total number of values in Ppartial (or Pfused). With the defined KL divergence loss LKL, We update the blocks inside B fused and then return the pruned model by replacing Bpartial with Bfused in M.\nIt should be noted that the process from importance detection to performance detection is conducted iteratively, i.e. the blocks are fused one by one until the predefined pruning rate is achieved. The reason is that the current importance detection result is based on the current state of overall model inference, but once a block is removed the states will also change, which will also change the rank of block importance."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first introduce the experimental setting for FuseGPT implementation. Then we present our main results, from perplexity to zero-shot task results after model pruning. We also conduct ablation studies to highlight our methodologies.\nExperimental Setting\nFuseGPT is implemented based on Hugging Face Transformers [35] together with Pytorch [36]. The experiments are conducted on one NVIDIA H800 PCIe 80GB graphics card with CUDA Driver 12.6. When deployed for evaluation, we can explicitly compute and store W = W+CWp and perform inference as usual, thus no additional costs are produced. We randomly selected samples from WikiText-2 training dataset [37] as calibration and fine-tuning data. Without special description, we use 32 samples for calibration and 1024 samples for fine-tuning, which is extremely lightweight for model compression. We use the partial group size G as 7, which enables us to only update approximately 25% parameters for a 7B model. We set the rank for decomposition of coefficient C as 128. In order to further reduce learning costs, we also employ LoRA [33] with rank 128 to update the origin weights inside the partial group. We run 20 epochs during the update of partial group parameters. We use the Adam optimizer [38] with \u03b2\u2081 = 0.9 and B2 = 0.95 and the cosine learning rate decay learning rate scheduler [39]."}, {"title": "A. Experimental Setting", "content": "During the above process, there will be two special cases. Firstly, it is possible that some blocks inside the partial group are already fused blocks. In this case, we cannot simply employ Equation (5). Therefore, our solution is to incrementally add the pruned weight:\n$W_{o}^{fused} = Wo + \\sum_{f=1}^{F} C \\odot W^{f}$,\n(10)\nwhere Wo is the origin layer weight and F denotes the times it is fused and thus injecting F times weight. Secondly, it is possible that the block detected to prune is already a fused block. In this case, we face the problem of whether to inject the fused weights into the neighboring blocks. Our solution is not to add the fused weight in the form of a weighted sum, but we will first compute and store them into a single frozen weight, then directly employ Equation (5) to complete the fusion."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced FuseGPT, an innovative approach to structured pruning that enhances the deployment of Generative Pre-trained Transformers (GPTs) in resource-constrained environments. By focusing on the efficient recycling and fusion of redundant transformer blocks, FuseGPT addresses the critical challenge of maintaining high-performance post-pruning. Our methodology leverages a novel metric, Macro Inference (MI), to assess the long-term influence of block removal, ensuring a more informed and effective pruning process. By integrating unimportant blocks into neighboring ones, we preserve valuable pre-trained knowledge, thus minimizing performance degradation. The promising results of FuseGPT highlight its potential as a significant advancement in model compression techniques. It not only facilitates efficient deployment but also opens new avenues for future research in optimizing large-scale GPTs.\nThis work contributes to the ongoing efforts to balance model complexity with practical deployment needs, paving the way for more accessible and sustainable AI solutions."}]}