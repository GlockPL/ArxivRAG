{"title": "From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification", "authors": ["Fanzhi Jiang", "Su Yang", "Mark W. Jones", "Liumei Zhang"], "abstract": "Text-based person re-identification (Re-ID) is a challenging topic in the field of complex multimodal analysis, its ultimate aim is to recognize specific pedestrians by scrutinizing attributes/natural language descriptions. Despite the wide range of applicable areas such as security surveillance, video retrieval, person tracking, and social media analytics, there is a notable absence of comprehensive reviews dedicated to summarizing the text-based person Re-ID from a technical perspective. To address this gap, we propose to introduce a taxonomy spanning Evaluation, Strategy, Architecture, and Optimization dimensions, providing a comprehensive survey of the text-based person Re-ID task. We start by laying the groundwork for text-based person Re-ID, elucidating fundamental concepts related to attribute/natural language-based identification. Then a thorough examination of existing benchmark datasets and metrics is presented. Subsequently, we further delve into prevalent feature extraction strategies employed in text-based person Re-ID research, followed by a concise summary of common network architectures within the domain. Prevalent loss functions utilized for model optimization and modality alignment in text-based person Re-ID are also scrutinized. To conclude, we offer a concise summary of our findings, pinpointing challenges in text-based person Re-ID. In response to these challenges, we outline potential avenues for future open-set text-based person Re-ID and present a baseline architecture for text-based pedestrian image generation-guided re-identification (TBPGR).", "sections": [{"title": "I. INTRODUCTION", "content": "THE demands in public safety and the subsequently installed surveillance networks make the manual tracking and identifying individuals increasingly challenging. Automatic person re-identification (Re-ID) has emerged as a popular research area in computer vision to address this issue. Also known as person search [154], person Re-ID specifically refers to recognizing and tracking specific individuals using non-overlapping images and videos captured by cameras, determining whether a specific query person appears persistently or momentarily across different times and locations [6]. It finds important applications in scenarios like searching for characters in movies, locating missing children, and tracking criminals [126]. The source data collection for these applications is usually from CCTV footage on the street. Given the constraints of the equipment and the environment, a number of challenges arise. These challenges include lighting variations, occlusions, background changes, pose variations, and differences in camera resolutions. A typical person re-identification system involves conducting image queries and searching for corresponding individuals in a gallery of images or a video pool. It involves extracting features directly from pedestrian appearance images and performing direct matching, then ranking against existing appearance images in the gallery. These scenarios assume that visual examples of individual identities are always available as queries. However, some special cases where the visualization samples of personal identity are not available, we perform retrieval based on textual descriptions only, which is called text-based person re-identification, as shown in Figure 1."}, {"title": "A. Attribute-based Person Re-ID", "content": "Text-based person re-identification is a special form of person re-identification that, instead of using image data, relies on descriptions to reflect a person's appearance [75]. In the task of text-based person Re-ID, a natural language description and an image are provided as input, and the output is the identification of the person matching the description [20]. There has been a desire to perform high-level semantic person search using free-form natural language descriptions. However, early research initially started with the attribute-based person re-identification [27], [178], [181], [183]. Attribute-based methods (Pedestrian Attributes Recognition, PAR) stem from the high correlation between attributes and pedestrian images. Pedestrian attributes such as gender, age, clothing type, clothing color, etc. are arranged and combined to make the pedestrian identity appear distinguishable [179]. Extracting attribute information from pedestrian images can be achieved by using pre-trained classifiers or attribute detectors. These classifiers or detectors can recognize and locate different attributes(body part) in the images. These extracted pedestrian attributes information needs to be encoded into a feature vectors pattern matching. This often involves transforming discrete attributes into continuous feature representations, common methods include one-hot encoding and embedding encoding [27]. During the recognition process, matching is conducted by comparing the attribute feature vectors of query images and gallery images. Common matching methods include computing distances or similarities between feature vectors (such as Euclidean distance, cosine similarity, etc.). Finally, based on the results of attribute matching, the final recognition result is determined through fusion of matching scores from different attributes or by adopting decision strategies. The attribute-based person Re-ID approach improves recognition accuracy and retrieval interpretability. This is because these attributes can be considered as high-level semantic information that is robust to viewpoint changes and different viewing conditions [182]. Previous work on attribute-based person retrieval has attempted to reduce modality gaps by aligning each person category and its corresponding images in a joint embedding space through modal adversarial training [7], [157], or by enhancing the expressive power of embedding vectors for person categories and images hierarchically [27]. While these pioneering studies reveal important but less explored methods for person retrieval, there remains significant room for further improvement. Firstly, due to their adversarial learning strategy, they exhibit instability and high computational cost during training [6], [60]. For other, they incur high inference costs due to the need for additional networks to match high-dimensional embedding vectors [27]. More importantly, these methods treat person categories as independent labels of person images and overlook their relationships, such as how many attributes differ between them, despite the potential rich supervisory signals for learning better representations of person categories and images. Dong et al. [30] address this issue by capturing rich information of two modalities through hierarchical embedding. However, their model entails high computational complexity as it computes high-dimensional embeddings and deploys additional networks for matching. Yin et al. [157] and Cai et al. [6] learn a joint embedding space where person categories and images are directly matched. To bridge the modality gap, their embedding space is trained in a modal adversarial manner; however, due to the nature of min-max optimization, this often leads to unstable and slow convergence. Furthermore, all these methods suffer from a limitation where person categories are treated as separate labels and the important relationships among them are overlooked. In the joint embedding space of the two modalities, the loss pulls images of the same person category closer to achieve modal alignment. [54] in-"}, {"title": "B. Natural Language-based Person Re-ID", "content": "Attribute-based person retrieval [69], [168] provides a method for person re-identification based on free-form descriptions. However, there is an awareness that the leverage of attributes to describe a person's appearance could be limited in some circumstances. For instance, the PETA dataset [27] defines 61 binary and 4 multi-class person attributes, along with hundreds of phrases used to describe a person's appearance. In addition, even with a constrained set of attributes, labeling them for large-scale person image datasets is challenging and expensive. Using free-form natural language (NL) descriptions may offer more flexibility and robustness compared to these predefined attributes. Attribute-based person retrieval also has limitations in terms of the results robustness. For instance, shuffling and flipping some attributes while using a basic fact set for retrieval can lead to significant performance degradation of the results. Compared to manually assigned sets of attributes, textual descriptions can be seen as an unstructured form of annotation. In general, most attributes fall into the category of noun phrases, and natural language includes higher-level semantic words and complex events than the attributes. In principle, natural language processing (NLP) techniques can facilitate all attributes with any possible words and lengths, capturing unique details through natural language descriptions. Additionally, descriptions can include indicators of certainty or fuzziness to retain as much information as possible, potentially allow sets of multi-class attribute assignments. Both attribute-based and NLP-based person re-identification are referred to as text-based person re-identification in this paper. Li et al. [73] introduced the first dataset for NL-based person Re-ID, evaluated and compared various possible models, and proposed an RNN with a gated neural attention mechanism. Jing et al. [59] proposed to focus on mining fine-grained local parts and performing fine-grained visual-textual matching. These methods can learn distinctive part representations and achieve 81.23% accuracy, but they are troubled by the problem of ambiguous embeddings. Image regions and words are correlated at different semantic granularities [136], [168]. The discriminative parts of a person usually appear with different granularities, which was overlooked in the previous methods and ambiguated embeddings. To alleviate this issue, words (based on NL) as guidance have been integrated into the related image regions. However, irrelevant words may mislead the model and even exacerbate the ambiguous embeddings problem. Cross-domain embeddings technique is one of the main trends in the Re-ID community. Work proposed in [16] added supplementary annotations to extract meaningful image re-gions independently, these regions, however, are not always available in the real-world scenes. Work in [69] proposed to learn cross-modal embeddings to improve matching results using a shared attention mechanism. [161] designed a cross-modal projection matching loss and a cross-modal projection classification loss for learning discriminative image-text embeddings. [148] proposed a matrix factorization-based dictionary learning algorithm to eliminate the impact of style and pedestrian pose information on cross-domain Re-ID. [17] proposed a patch-word matching model and designed an adaptive threshold mechanism in the model, it attempted to establish global and local image-language associations to enhance the semantic consistency between local visual and language features. Deep adversarial graph attention convolutional network was also proposed to address the problem of text-based person re-identification [81], [102]. Niu et al. [94]proposed a Multi-level Image-Text Alignment (MIA) model, which introduced cross-modal adaptive attention in three different granularities, integrating these three similarities to comprehensively judge the affinity of image-sentence pairs and better distinguish interested pedestrians. Zhu et al. [173]achieved an environment-person separation (DSSL) method under the mutual exclusion constraint, and also annotated another dataset for person search using language called Real Scene Text-based Person Re-ID (RSTPREID). Wu et al. [140] designed a twin task, image coloring and text completion, and explicitly built fine-grained cross-modal associations bidirectionally based on color reasoning, their LapsCore method achieved 63.4% rank@1 retrieval accuracy on the CUHK-PEDES dataset. [92] proposed a novel cross-modal matching method named Cross-modal Co-occurrent Attribute Alignment (C2A2), which can better handle noise and significantly improve retrieval performance in person search by language. Relying on matrix factorization to construct visual and textual attribute dictionaries and cross-modal alignment using denoising reconstruction features addresses noise generated by extraneous elements of pedestrians. Zheng et al. [167] made the first attempt to perform NL-based person search within unsegmented complete images."}, {"title": "C. Person Re-Identification previous survey", "content": "In order to provide researchers with a comprehensive understanding of the current status and research directions in the domain of text-based person re-identification, we conducted an in-depth investigation of text-based person re-identification methodologies and synthesized recent research achievements. Prior to this effort, several researchers have conducted reviews in the field of person re-identification [4], [9], [50], [53], [62], [89], [90], [98], [108], [116], [119], [126], [134], [141], [146], [149], [159], [169], [176] and we summarize the primary contributions of these studies in the Table III. Among them, certain surveys [4], [9] introduced previous challenges in the field of person Re-ID. [62], [90], [98], [134], [141] summarized deep learning based pedestrian re-recognition methods. [108], [119] investigated the available image/video-based person Re-ID datasets. [159], [170] surveyed image-based or video-based pedestrian re-recognition systems. [61], [90], [154], [176] proposed metric learning based classification method for pedestrian re-recognition. [53], [90], [154] presented a pedestrian re-recognition taxonomy based on representation learning. Huang et al. [50] provided a visible-infrared cross-modal categorization of pedestrian re-identification studies. Wang et al. [126] reviewed cross-modality based person Re-ID research from four aspects, including sketch, text, low resolution (LR) and infrared (IR), as Figure 6 is introduced later. However, these surveys exhibit areas for improvement, such as lacking a systematic categorization and analysis of text-based deep learning methods in person re-identification, and omitting a comprehensive discussion of text-based person re-identification. Therefore, in comparison to the aforementioned reviews, this paper delves into an in-depth analysis of recent research in text-based person re-identification from dimensions including datasets, strategies, architectures, and optimizations. We comprehensively review existing deep learning-based approaches and discuss their strengths and limitations."}, {"title": "II. EVALUATION", "content": "The evaluation of text-based person Re-ID tasks, typically involves publicly available benchmark datasets and common performance metrics. This provides a standardized testing platform, enabling a quantified and objective comparative assessment of the effectiveness of different experimental algorithms."}, {"title": "A. Datasets", "content": "1) attribute-based Person Re-ID Datasets: Attribute-based Person Re-identification (Re-ID) often involves the utilization of semantic attributes that describe appearance characteristics of individuals, such as gender, age, clothing type, clothing texture, and clothing color [114]. However, these attributes frequently encompass low-level semantic attributes that can be directly associated with image regions, such as clothing type, clothing texture, and clothing color, as well as high-level semantic attributes that directly correspond to people, such as gender and age. By leveraging these attributes, the negative impact of variations in viewpoint and appearance on person re-identification can be mitigated, resulting in more reliable recognition and tracking performance [54]. To facilitate research in Attribute-based Person Re-ID, researchers have initiated the construction of several datasets specifically tailored for this purpose. Notable publicly available datasets in this domain include PETA [27], UAV-Human [80], RAP [77], RAP2.0 [78], PA-100K [84], Market1501-Attribute [181], and DukeMTMC-Attribute [181]. These datasets typically consist of a substantial collection of images capturing individuals from surveillance cameras, each accompanied by attribute labels. The dataset creation process generally involves selecting a subset of images from existing Person Re-ID datasets. Subsequently, attribute labels are assigned to each image either manually or through automated annotation. Ultimately, the annotated images along with their corresponding attribute labels are organized into a unified dataset, which serves as a resource for researchers in the field, as shown in Table I. 2) Natural Language-based Person Re-ID Datasets: Publicly available NL-based person re-identification datasets have significant practical implications for benchmarking, algorithm improvement, security and surveillance applications, privacy protection, and advancing multimodal research. Currently, there are three large publicly available datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES. The details of these datasets will be described. a) CUHK-PEDES: CUHK-PEDES is the first description dataset introduced by The Chinese University of Hong Kong [75], designed for training and evaluating person description and image retrieval tasks in computer vision. The dataset was collected by aggregating person images from five existing and well-known pedestrian re-identification evaluation datasets, namely CUHK03 [180], Market-1501 [169], SSM [142], VIPER [44], CUHK01 [67], and subsequently annotated with descriptive text by crowdsourced workers from Amazon Mechanical Turk (AMT). It consists of over 40,206 images with 80,412 person descriptions, each containing at least 23 words, across 13,003 unique person identities. The training set includes 34,054 images, 11,003 unique persons, and 68,108 textual descriptions. The validation set contains 3,078 images, 1,000 unique persons, and 6,158 textual descriptions, while the test set comprises 3,074 images, 1,000 unique persons, and 6,156 textual descriptions. The individuals in the dataset were captured in street and public places in the Hong Kong area. CUHK-PEDES provides diverse person descriptions encompassing appearance, actions, postures, and interactions, with each person description linked to the corresponding image ID. The release of the CUHK-PEDES dataset promotes interdisciplinary research between computer vision and natural language processing, providing researchers in the fields of person description and image retrieval with the first benchmark dataset. b) RSTPReid: The RSTPReid dataset is a pedestrian re-identification dataset introduced by Nanjing University of Science and Technology [173], designed for training and evaluating pedestrian re-identification tasks in computer vision. To address the issue in the CUHK-PEDES dataset where each specific pedestrian is captured by the same camera under the same time-space conditions, which does not reflect real-world scenarios, the authors constructed the Real Scenarios Text-based Person Reidentification (RSTPReid) dataset based on MSMT17 [138]. The RSTPReid dataset contains 20,505 images of 4,101 individuals captured by 15 independent cam-eras from different viewpoints, lighting conditions, locations, and weather conditions. Each individual has 5 corresponding images taken by different cameras, and each image is accompanied by 2 text descriptions with no fewer than 23"}, {"title": "B. Metrics", "content": "1) Mean Average Precision(mAP): For each query i, we define a precision $P_i(j)$ as the proportion of correct matches in the top j matches, and then for each positive instance, we calculate the average of the proportions of all positive instances before this one, which is the Average Precision $AP_i$:\n$AP_i = \\frac{1}{M_i} \\sum_{j=1}^{M_i} P_i(j) \\cdot I(rank_i = j).$  (1)\nWhere $M_i$ is the number of positive instances for query i, and $\u2211$ is the summation over all positive instances. The symbol $I(rank_i = j)$ usually denotes an indicator function. The indicator function takes the value of 1 under certain conditions, otherwise it takes the value of 0. Specifically:\n$I(rank_i = j)$\nindicates that the indicator function $I(rank_i = j)$ takes the value of 1 when the first i query's first j result is a correct match, otherwise it takes the value of 0. In other words, it determines whether or not there is a correct match in the ranked position of j. Then, mAP is the average of the Average Precision of all queries:\n$mAP = \\frac{1}{N} \\sum_{i=1}^{N}AP_i.$  (2)\nWhere N is the total number of queries, and is the summation over all queries.\n2) Rank-N Accuracy: For each query i, we define an indicator function $I(rank_i < n)$, which is 1 if the correct match is within the top n matches, and 0 otherwise. Then, Rank-n accuracy is the average of this function:\n$Rank_n = \\frac{1}{N} \\sum_{i=1}^{N}I(rank_i \\leq n).$  (3)\nWhere N is the total number of queries, and is the summation over all queries."}, {"title": "III. STRATEGY", "content": "In the task of text-based person re-identification, image data is typically captured through CCTV in open environments [45]. This data not only contains key descriptive features of individuals but also includes a significant amount of background noise. Early works [17], [75], [148], [161] typically directly extract global image text features for simple cross-modal matching. However, due to the uniqueness of the person re-identification task, which differs from existing image-text retrieval [60], [167], the categories to be retrieved all belong to highly similar individuals. Identification based solely on distinguishing features such as clothing and posture increases the challenges of the text-based person re-identification task."}, {"title": "A. Stripe Segmentation", "content": "Some work suggests that various parts of the human body are evenly arranged in images, and Stripe Segmentation can be employed as a guiding model to achieve multimodal local alignment supervision [57], [94]. Stripe Segmentation is a commonly used technique in pedestrian re-identification, aiming to mitigate the impact of pose, occlusion, and lighting variations on re-identification performance within pedestrian images [94], [29]. The fundamental idea is to partition pedestrian images into multiple vertical stripes, also referred to as person parts. Each stripe can be considered a subregion of the pedestrian image (head, upper body, lower body, foot). The advantage of this approach lies in dividing pedestrian images into multiple local regions, effectively capturing local features of pedestrians to enhance recognition accuracy [79]. For instance, local texture, color, and shape features can be extracted within each stripe, facilitating better differentiation between different parts of pedestrians. Stripes of different heights can correspond to different scales of pedestrian parts, providing richer scale information to address scale variation issues in pedestrian images. Ding et al. [29] proposed a Semantic Self-Alignment Network (SSAN) capable of automatically extracting part-level textual features for corresponding visual regions. They designed a multi-view non-local network to capture relationships between body parts, thereby establishing better correspondences between body parts and noun phrases. Li et al [79] proposed to vertically divide a character image into multiple regions using two different approaches, including overlapping slices and key-point-based slices. Niu et al. [94] also emphasized local-local alignment, employing Stripe Segmentation in the proposed Bidirectional Fine-Grained Matching (BFM) module to match visual human body parts with noun phrases. However, this strategy is fragile and sensitive to conditional changes. For example, the regions of image samples do not always contain a complete human body, and at times, the human head does not consistently appear in the first stripe, significantly impacting the robustness of existing methods."}, {"title": "B. Multi-scale Fusion", "content": "Due to the small inter-class variance in both pedestrian images and descriptions, comprehensive information is required for pedestrian retrieval to coordinate visual and textual clues across all scales. Subsequent to the introduction of local feature extraction strategies other than stripe segmentation, the utilization of fused features is referred to as multi-scale feature fusion [130], as illustrated in Figure 3. Image Structure Graph Network (A-GANet) [81] employs residual modules for visual representation extraction and utilizes designed graph attention convolutional layers for structured high-level visual semantic feature extraction. Specifically, it captures relationship features of different pedestrian parts through graph convolutional networks. Wang et al. [136] proposed the Visual Text Attribute Alignment model (ViTAA), which learns to decompose the person's feature space into subspaces corresponding to attributes using an optically assisted attribute segmentation layer. On the image side, a segmentation layer is employed to divide pedestrian images into full body, head, upper clothes, Lower clothes, shoes, and bag parts. On the text side, corresponding phrases of different segmented parts are extracted. Ji et al. [57] introduced an Asymmetric Cross-Scale Alignment (ACSA) method. Global text representation and local phrase representation are employed for text extraction. The global representation is divided into four regions as local visual representations, namely Head, Upper body, Lower body, and foot. These features are used afterwards for feature concatenation and computation. The partition strategy does not involve additional computational costs but can better retain prominent body parts for fine-grained matching. Wang et al. [135] proposed a novel Multi-Granularity Embedding Learning (MGEL) model. It generates multi-granularity embeddings of partial human bodies from coarse to fine by revisiting person images at different spatial scales. Chen et al. [20] adopted a multi-branch representation in the learning path, enabling text-adaptive matching of corresponding visual local representations, thereby aligning text and visual local representations. Additionally, a multi-stage cross-modal matching strategy was proposed to eliminate modality gaps from low-level, local, and global features, gradually narrowing the feature gap between image and text domains. Niu et al. [94] presented a Multi-Granularity Image-Text Alignment (MIA) framework to address cross-modal fine-grained issues. It consists of three modules corresponding to three granularities. The Global Contrast (GC) module is used for global-global alignment. The Relation-Guided Global-Local Alignment (RGA) module filters global-local relationships. The Bidirectional Fine-Grained Matching (BFM) module aligns local to local based on trained fine-grained local components. Zheng et al. [168] proposed a Hierarchical Adaptive Matching model to learn subtle feature representations from three different granularities (i.e., word-level, phrase-level, and sentence-level) for fine-grained image-text retrieval tasks. Wang et al. [120] addressed the issue that existing methods typically learn similar mappings between local parts of images and text or embed entire images and text into a unified embedding space, resulting in a problem of local-global correlation. They designed a Divide and Merge Embedding (DME) learning framework for text-based person search. It models the relationship between local parts and global embedding, merging local details into the global embedding. Li et al. [76] constructed features for text and person image matching, different from other methods. The latter acquires global features by aggregating aligned local features, leading to misalignment between text and visual features. They proposed placing features with the same semantics in the same spatial position to construct the final features for person-text-image matching, achieving semantic consistency and interpretability of global features. Yan et al. [151] proposed an Implicit Local Alignment module, adaptively aggregating image and text features into a set of modality-shared semantic topic centers, implicitly learning local fine-grained correspondence between images and text without additional supervision information and complex cross-modal interaction. Additionally, global alignment is introduced as a supplement to the local perspective. Gao et al. [41] introduced a method called Non-Local Alignment on Full Scale (NAFS), capable of adaptively aligning image and text features at all scales. First, a novel ladder network structure is proposed to extract full-size image features with better locality. Secondly, a language model, Bidirectional Encoder Representations from Transformers (BERT), with local constraint attention is proposed to obtain description representations at different scales. Then, instead of aligning features separately at each scale, a novel context non-local attention mechanism is applied to simultaneously discover potential alignment at all scales. In summary, applying target detection or additional branch networks to detect significant regions and then extract local features for obtaining multi-scale features has certain accuracy advantages. However, due to the increased external network overhead, these methods often result in higher computational costs."}, {"title": "C. Attention Mechanism", "content": "Cross-modal alignment is inherently challenging in achieving fine-grained matching between text and images. In addition to implicit alignment strategies achieved through the automatic fusion of multi-scale features, another key approach involves the use of explicit alignment strategies. For instance, employing attention mechanisms enhances the capability of feature extraction by capturing discriminative feature representations related to both language descriptions and visual appearances. This aids in intuitively understanding and controlling the alignment process. Attention models can establish correspondences between body parts and words [29], [35], [69], [75], [135], [148], [164], offering the advantage of not relying on external cues for aligning feature embeddings. These attention strategies can be categorized into spatial attention, channel attention, mixed attention, non-local attention, and positional attention. Liu et al. [135] proposed an attention-based deep neural network capable of capturing multiple attention features from low-level to semantic layers to learn comprehensive features for fine-grained pedestrian representation. Wang et al. [82] utilized a multi-head self-attention module to extract embeddings for different granularity portions of the text stream, employing adaptive filtering to remove interference and obtain fine-grained features. Lee et al. [63] introduced a Stacked Cross Attention Mechanism (SCAN) that compares attention information for text and image in two stages, determining the importance of image regions and words to achieve potential alignment between the two modalities. Spatial attention-based methods often focus solely on local discriminative features of pedestrians, neglecting the impact of feature diversity on pedestrian retrieval. Li et al. [68] proposed a cubic attention convolutional neural network that combines spatial and channel attention to maximize complementary information from different scale attention features, addressing the challenges of cross-modal alignment in pedestrian re-identification. Wang et al. [120] designed a Feature Division Network (FDN) that embeds inputs into K locally guided semantic representations using self-attention, each representing different parts of a person. They then introduced a Correlation-based Subspace Projection (RSP) method to merge different local representations into a compact global embedding. Yan et al. [151] presented an Efficient Joint Information and Semantic Alignment Network (ISANet) for text-based person search. Specifically, they designed an image-specific information suppression module that suppresses image background and environmental factors through relationship-guided localization and channel attention filtering, effectively alleviating information inequality and achieving information alignment between text and image. Considering the indistinguishability of features extracted from first-order attention (e.g., spatial and channel attention) in complex camera views and pose-changing scenarios, Farooq et al. [35] applied non-local attention directly after calculating interactions between text features to model long-term dependencies between different phrases in the text. Inspired by self-attention, Gao et al. [41] proposed a Context Non-Local Attention to enable cross-modal features to align with each other in a coarse-to-fine manner based on their semantics, rather than relying solely on predefined and fixed rules (e.g., local-local, global-global). Wang et al. [130] introduced a Part-based Multi-scale Attention Network (PMAN), consisting of a dual-path feature extraction framework with attention-based branches for a multi-scale cross-modal matching module. This module extracts visual semantic features from different scales and matches them with text features. Addressing concerns that utilizing soft attention mechanisms to infer semantic alignment between image regions and corresponding words in a sentence may lead to the fusion of unrelated multimodal features and result in redundant matching, Wang et al. [123] proposed an IMG-Net model. This model combines intra-modal self-attention and cross-modal hard region attention with a fine-grained model to extract multi-granularity semantic information. Zheng et al. [168] introduced a Hierarchical Gumbel Hard Attention Module, using the Gumbel top-k reparameterization algorithm as a low-variance, unbiased gradient estimator to select strongly semantically related regions for all regions of the image and corresponding words or phrases, enhancing precise alignment and similarity calculation results. While these techniques often bring about noticeably superior retrieval performance, they frequently come at the cost of efficiency. Specifically, for M galleries and N queries, their complexity is O(M + N). In contrast, the complexity of attention-based cross-modal approaches rises to O(MN) due to pairwise inputs. To address this issue, Yang et al. [150] initially employ cross-modal non-attention features to quickly identify candidates and then deploy attention-based modules to refine the final ranking scores."}, {"title": "D. External Auxiliary", "content": "For Text-based Person Re-Identification (Re-ID) tasks, several auxiliary strategies such as image segmentation, random mask, human body keypoints, attribute prediction, clustering analysis, and color extraction can be employed to extract advanced semantics, thereby improving cross-modal retrieval performance. [101] proposed a dual-part simultaneous alignment representation scheme using attention mechanisms to capture distinguishing information beyond body parts by leveraging complementary information from accurate human parts and noisy parts to update representations. To leverage multi-level visual content, Jing et al. [60] introduced a Pose-guided Multi-granularity Attention Network (PMA). They initially proposed a Coarse Alignment network (CA) that uses similarity-based attention to select relevant image regions for global description. Pose information can be utilized to learn potential semantic alignments between visual body parts and textual noun phrases. Shu et al. [107] introduced Bidirectional Mask Modeling (BMM) without the need for additional manual labeling. They applied random masks to images and corresponding text keywords, forcing the model to explore more useful matching clues. This approach increases data diversity and enhances model generalization. Wang et al. [136] proposed a novel Visual-Textual Attribute Alignment model (ViTAA). For semantic feature extraction, segmentation labels are used to drive the learning of attribute-aware features from input images. Aggarwal et al. [1] introduced a method for text-based person search by learning attribute-driven spatial and class-driven spatial information. The goal is to create semantically retained embeddings through an additional task of attribute prediction. Suo et al. [112] proposed a novel Simple Robust Correlated Filtering (SRCF) framework, which effectively extracts key clues and adaptively aligns multimodal features by constructing a set of universal semantic templates (filters). This framework differs from previous attention-based methods, focusing on computing similarities between templates and inputs. Two types of filtering modules (denoising filter and dictionary filter) were designed to extract key features and establish multimodal mappings."}, {"title": "IV. ARCHITECTURE", "content": "When mapping features from a specific modality to a common manifold, the feature distribution of other modalities remains imperceptible. This implies that embedding and aligning multimodal features in the common manifold entirely depend on the model's own experience rather than the actual data distribution. In other words, a major challenge in cross-modal Person Re-ID is ensuring that the feature distribution in the common manifold accurately reflects the feature distribution in the original modalities. This necessitates the model to possess sufficient capability to capture and understand the relationships between different modalities, which typically requires abundant data and carefully designed model structures."}, {"title": "A. Convolutional Neural Network", "content": "Text-based person Re-ID can be viewed as a multimodal pedestrian retrieval problem, with the primary challenge lying in the extraction of features from both visual and textual data. Due to the capability of neural networks to automatically learn and extract features from pedestrian images, the cumbersome process of manual feature design can be avoided. Most existing efforts consider employing network architectures designed for image classification as the backbone of visual networks. Consequently, convolutional neural networks (CNNs) have been utilized extensively in text-based pedestrian re-identification, primarily leveraging CNNs [8", "20": [35], "37": [42], "82": [92], "17": [69], "75": [94], "101": [116], "124": "MobileNet [1", "127": [140], "161": "ResNet50 [37", "68": [112], "117": [118], "47": [79], "93": [95], "102": "ResNet-152 [18", "35": [37], "92": ".", "118": ".", "58": "."}, {"37": ".", "36": "."}]}