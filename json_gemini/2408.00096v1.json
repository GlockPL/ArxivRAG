{"title": "From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification", "authors": ["Fanzhi Jiang", "Su Yang", "Mark W. Jones", "Liumei Zhang"], "abstract": "Text-based person re-identification (Re-ID) is a challenging topic in the field of complex multimodal analysis, its ultimate aim is to recognize specific pedestrians by scrutinizing attributes/natural language descriptions. Despite the wide range of applicable areas such as security surveillance, video retrieval, person tracking, and social media analytics, there is a notable absence of comprehensive reviews dedicated to summarizing the text-based person Re-ID from a technical perspective. To address this gap, we propose to introduce a taxonomy spanning Evaluation, Strategy, Architecture, and Optimization dimensions, providing a comprehensive survey of the text-based person Re-ID task. We start by laying the groundwork for text-based person Re-ID, elucidating fundamental concepts related to attribute/natural language-based identification. Then a thorough examination of existing benchmark datasets and metrics is presented. Subsequently, we further delve into prevalent feature extraction strategies employed in text-based person Re-ID research, followed by a concise summary of common network architectures within the domain. Prevalent loss functions utilized for model optimization and modality alignment in text-based person Re-ID are also scrutinized. To conclude, we offer a concise summary of our findings, pinpointing challenges in text-based person Re-ID. In response to these challenges, we outline potential avenues for future open-set text-based person Re-ID and present a baseline architecture for text-based pedestrian image generation-guided re-identification (TBPGR).", "sections": [{"title": "I. INTRODUCTION", "content": "THE demands in public safety and the subsequently in- stalled surveillance networks make the manual track- ing and identifying individuals increasingly challenging. Automatic person re-identification (Re-ID) has emerged as a popular research area in computer vision to address this issue. Also known as person search [154], person Re-ID specifically refers to recognizing and tracking specific indi- viduals using non-overlapping images and videos captured by cameras, determining whether a specific query person appears persistently or momentarily across different times and locations [6]. It finds important applications in scenarios like searching for characters in movies, locating missing children, and tracking criminals [126]. The source data collection for these applications is usually from CCTV footage on the street. Given the constraints of the equipment and the environment, a number of challenges arise. These challenges include lighting variations, occlusions, background changes, pose variations,"}, {"title": "A. Attribute-based Person Re-ID", "content": "Text-based person re-identification is a special form of person re-identification that, instead of using image data, relies on descriptions to reflect a person's appearance [75]. In the task of text-based person Re-ID, a natural language description and an image are provided as input, and the output is the identification of the person matching the description [20]. There has been a desire to perform high-level semantic person search using free-form natural language descriptions. How- ever, early research initially started with the attribute-based person re-identification [27], [178], [181], [183]. Attribute- based methods (Pedestrian Attributes Recognition, PAR) stem from the high correlation between attributes and pedestrian images. Pedestrian attributes such as gender, age, clothing type, clothing color, etc. are arranged and combined to make the pedestrian identity appear distinguishable [179]. Extracting attribute information from pedestrian images can be achieved by using pre-trained classifiers or attribute detectors. These classifiers or detectors can recognize and locate different attributes(body part) in the images. These extracted pedestrian attributes information needs to be encoded into a feature vectors pattern matching. This often involves transforming dis- crete attributes into continuous feature representations, com- mon methods include one-hot encoding and embedding en- coding [27]. During the recognition process, matching is con- ducted by comparing the attribute feature vectors of query im- ages and gallery images. Common matching methods include computing distances or similarities between feature vectors (such as Euclidean distance, cosine similarity, etc.). Finally, based on the results of attribute matching, the final recognition result is determined through fusion of matching scores from different attributes or by adopting decision strategies. The attribute-based person Re-ID approach improves recognition accuracy and retrieval interpretability. This is because these"}, {"title": "B. Natural Language-based Person Re-ID", "content": "Attribute-based person retrieval [69], [168] provides a method for person re-identification based on free-form de- scriptions. However, there is an awareness that the leverage of attributes to describe a person's appearance could be limited in some circumstances. For instance, the PETA dataset [27] de- fines 61 binary and 4 multi-class person attributes, along with hundreds of phrases used to describe a person's appearance. In addition, even with a constrained set of attributes, labeling them for large-scale person image datasets is challenging and expensive. Using free-form natural language (NL) descriptions"}, {"title": "C. Person Re-Identification previous survey", "content": "In order to provide researchers with a comprehensive un- derstanding of the current status and research directions in the domain of text-based person re-identification, we conducted an in-depth investigation of text-based person re-identification methodologies and synthesized recent research achievements. Prior to this effort, several researchers have conducted reviews in the field of person re-identification [4], [9], [50], [53], [62], [89], [90], [98], [108], [116], [119], [126], [134], [141], [146], [149], [159], [169], [176] and we summarize the primary con- tributions of these studies in the Table III. Among them, certain surveys [4], [9] introduced previous challenges in the field of person Re-ID. [62], [90], [98], [134], [141] summarized deep learning based pedestrian re-recognition methods. [108], [119] investigated the available image/video-based person Re- ID datasets. [159], [170] surveyed image-based or video- based pedestrian re-recognition systems. [61], [90], [154], [176] proposed metric learning based classification method for pedestrian re-recognition. [53], [90], [154] presented a pedestrian re-recognition taxonomy based on representation learning. Huang et al. [50] provided a visible-infrared cross-modal categorization of pedestrian re-identification studies. Wang et al. [126] reviewed cross-modality based person Re- ID research from four aspects, including sketch, text, low resolution (LR) and infrared (IR), as Figure 6 is introduced later.\nHowever, these surveys exhibit areas for improvement, such as lacking a systematic categorization and analysis of text- based deep learning methods in person re-identification, and omitting a comprehensive discussion of text-based person re- identification. Therefore, in comparison to the aforementioned reviews, this paper delves into an in-depth analysis of recent research in text-based person re-identification from dimensions including datasets, strategies, architectures, and optimizations. We comprehensively review existing deep learning-based ap- proaches and discuss their strengths and limitations."}, {"title": "II. EVALUATION", "content": "The evaluation of text-based person Re-ID tasks, typically involves publicly available benchmark datasets and common performance metrics. This provides a standardized testing platform, enabling a quantified and objective comparative assessment of the effectiveness of different experimental al- gorithms."}, {"title": "A. Datasets", "content": "1) attribute-based Person Re-ID Datasets: Attribute-based Person Re-identification (Re-ID) often involves the utilization of semantic attributes that describe appearance characteristics of individuals, such as gender, age, clothing type, clothing texture, and clothing color [114]. However, these attributes frequently encompass low-level semantic attributes that can be directly associated with image regions, such as clothing type, clothing texture, and clothing color, as well as high- level semantic attributes that directly correspond to people, such as gender and age. By leveraging these attributes, the negative impact of variations in viewpoint and appearance on person re-identification can be mitigated, resulting in more reliable recognition and tracking performance [54]. To facil- itate research in Attribute-based Person Re-ID, researchers have initiated the construction of several datasets specifically tailored for this purpose. Notable publicly available datasets in this domain include PETA [27], UAV-Human [80], RAP [77], RAP2.0 [78], PA-100K [84], Market1501-Attribute [181], and DukeMTMC-Attribute [181].\nThese datasets typically consist of a substantial collection of images capturing individuals from surveillance cameras, each accompanied by attribute labels. The dataset creation process generally involves selecting a subset of images from existing Person Re-ID datasets. Subsequently, attribute labels are assigned to each image either manually or through auto- mated annotation. Ultimately, the annotated images along with their corresponding attribute labels are organized into a unified dataset, which serves as a resource for researchers in the field, as shown in Table I.\n2) Natural Language-based Person Re-ID Datasets: Pub- licly available NL-based person re-identification datasets have significant practical implications for benchmarking, algorithm improvement, security and surveillance applications, privacy protection, and advancing multimodal research. Currently,"}, {"title": "B. Metrics", "content": "1) Mean Average Precision(mAP): For each query i, we define a precision Pi(j) as the proportion of correct matches in the top j matches, and then for each positive instance, we calculate the average of the proportions of all positive instances before this one, which is the Average Precision AP\u2081:\n$AP = \\frac{1}{M_i} \\sum_{j=1}^{M_i} P_i(j). I(rank_i = j).$ (1)\nWhere Mi is the number of positive instances for query i, and \u2211 is the summation over all positive instances. The symbol I(rank\u2081 = j) usually denotes an indicator function. The indicator function takes the value of 1 under certain conditions, otherwise it takes the value of 0. Specifically:\n$I(rank_i = j)$\nindicates that the indicator function I(rank\u2081 = j) takes the value of 1 when the first i query's first j result is a correct match, otherwise it takes the value of 0. In other words, it determines whether or not there is a correct match in the ranked position of j. Then, mAP is the average of the Average Precision of all queries:\n$MAP = \\frac{1}{N} \\sum_{i=1}^N AP_i.$\nWhere N is the total number of queries, and is the summation over all queries.\n2) Rank-N Accuracy: For each query i, we define an indicator function I(ranki < n), which is 1 if the correct match is within the top n matches, and 0 otherwise. Then, Rank-n accuracy is the average of this function:\n$Rank \\text{\u00a0} n = \\frac{1}{N} \\sum_{i=1}^N I(rank_i \\leq n).$ (3)\nWhere N is the total number of queries, and \u2211 is the summation over all queries."}, {"title": "III. STRATEGY", "content": "In the task of text-based person re-identification, image data is typically captured through CCTV in open environments [45]. This data not only contains key descriptive features of individuals but also includes a significant amount of back- ground noise. Early works [17], [75], [148], [161] typically directly extract global image text features for simple cross- modal matching. However, due to the uniqueness of the person re-identification task, which differs from existing image-text retrieval [60], [167], the categories to be retrieved all belong to highly similar individuals. Identification based solely on distinguishing features such as clothing and posture increases the challenges of the text-based person re-identification task."}, {"title": "A. Stripe Segmentation", "content": "Some work suggests that various parts of the human body are evenly arranged in images, and Stripe Segmentation can be employed as a guiding model to achieve multimodal local alignment supervision [57], [94]. Stripe Segmentation is a commonly used technique in pedestrian re-identification, aiming to mitigate the impact of pose, occlusion, and lighting variations on re-identification performance within pedestrian images [94], [29]. The fundamental idea is to partition pedes- trian images into multiple vertical stripes, also referred to as person parts. Each stripe can be considered a subregion of the pedestrian image (head, upper body, lower body, foot). The advantage of this approach lies in dividing pedestrian images into multiple local regions, effectively capturing local features of pedestrians to enhance recognition accuracy [79]. For instance, local texture, color, and shape features can be extracted within each stripe, facilitating better differentiation between different parts of pedestrians. Stripes of different heights can correspond to different scales of pedestrian parts, providing richer scale information to address scale variation issues in pedestrian images. Ding et al. [29] proposed a Semantic Self-Alignment Network (SSAN) capable of auto- matically extracting part-level textual features for correspond- ing visual regions. They designed a multi-view non-local network to capture relationships between body parts, thereby establishing better correspondences between body parts and noun phrases. Li et al [79] proposed to vertically divide a character image into multiple regions using two different approaches, including overlapping slices and key-point-based slices. Niu et al. [94] also emphasized local-local alignment, employing Stripe Segmentation in the proposed Bidirectional Fine-Grained Matching (BFM) module to match visual human"}, {"title": "B. Multi-scale Fusion", "content": "Due to the small inter-class variance in both pedestrian images and descriptions, comprehensive information is re- quired for pedestrian retrieval to coordinate visual and textual clues across all scales. Subsequent to the introduction of local feature extraction strategies other than stripe segmentation, the utilization of fused features is referred to as multi-scale feature fusion [130], as illustrated in Figure 3. Image Structure Graph Network (A-GANet) [81] employs residual modules for visual representation extraction and utilizes designed graph attention convolutional layers for structured high-level visual semantic feature extraction. Specifically, it captures relationship features of different pedestrian parts through graph convolutional net- works. Wang et al. [136] proposed the Visual Text Attribute Alignment model (ViTAA), which learns to decompose the person's feature space into subspaces corresponding to at- tributes using an optically assisted attribute segmentation layer. On the image side, a segmentation layer is employed to divide pedestrian images into full body, head, upper clothes, Lower clothes, shoes, and bag parts. On the text side, corresponding phrases of different segmented parts are extracted.\nJi et al. [57] introduced an Asymmetric Cross-Scale Align- ment (ACSA) method. Global text representation and local phrase representation are employed for text extraction. The global representation is divided into four regions as local visual representations, namely Head, Upper body, Lower body, and foot. These features are used afterwards for feature concatenation and computation. The partition strategy does not involve additional computational costs but can better retain prominent body parts for fine-grained matching. Wang et al. [135] proposed a novel Multi-Granularity Embedding Learning (MGEL) model. It generates multi-granularity em- beddings of partial human bodies from coarse to fine by revisiting person images at different spatial scales. Chen et al. [20] adopted a multi-branch representation in the learn- ing path, enabling text-adaptive matching of corresponding visual local representations, thereby aligning text and visual local representations. Additionally, a multi-stage cross-modal matching strategy was proposed to eliminate modality gaps from low-level, local, and global features, gradually narrowing the feature gap between image and text domains.\nNiu et al. [94] presented a Multi-Granularity Image-Text Alignment (MIA) framework to address cross-modal fine- grained issues. It consists of three modules corresponding to three granularities. The Global Contrast (GC) module is used for global-global alignment. The Relation-Guided Global-Local Alignment (RGA) module filters global-local re- lationships. The Bidirectional Fine-Grained Matching (BFM) module aligns local to local based on trained fine-grained local components. Zheng et al. [168] proposed a Hierarchical"}, {"title": "C. Attention Mechanism", "content": "Cross-modal alignment is inherently challenging in achiev- ing fine-grained matching between text and images. In addition to implicit alignment strategies achieved through the automatic fusion of multi-scale features, another key approach involves the use of explicit alignment strategies. For instance, employ- ing attention mechanisms enhances the capability of feature extraction by capturing discriminative feature representations related to both language descriptions and visual appearances. This aids in intuitively understanding and controlling the alignment process. Attention models can establish correspon- dences between body parts and words [29], [35], [69], [75], [135], [148], [164], offering the advantage of not relying on"}, {"title": "D. External Auxiliary", "content": "For Text-based Person Re-Identification (Re-ID) tasks, sev- eral auxiliary strategies such as image segmentation, random mask, human body keypoints, attribute prediction, clustering analysis, and color extraction can be employed to extract advanced semantics, thereby improving cross-modal retrieval performance. [101] proposed a dual-part simultaneous align- ment representation scheme using attention mechanisms to capture distinguishing information beyond body parts by lever- aging complementary information from accurate human parts and noisy parts to update representations.\nTo leverage multi-level visual content, Jing et al. [60] introduced a Pose-guided Multi-granularity Attention Network (PMA). They initially proposed a Coarse Alignment network (CA) that uses similarity-based attention to select relevant image regions for global description. Pose information can be utilized to learn potential semantic alignments between visual body parts and textual noun phrases. Shu et al. [107] introduced Bidirectional Mask Modeling (BMM) without the need for additional manual labeling. They applied random masks to images and corresponding text keywords, forcing the model to explore more useful matching clues. This approach increases data diversity and enhances model generalization. Wang et al. [136] proposed a novel Visual-Textual Attribute Alignment model (ViTAA). For semantic feature extraction, segmentation labels are used to drive the learning of attribute- aware features from input images. Aggarwal et al. [1] in- troduced a method for text-based person search by learning attribute-driven spatial and class-driven spatial information. The goal is to create semantically retained embeddings through an additional task of attribute prediction. Suo et al. [112] proposed a novel Simple Robust Correlated Filtering (SRCF) framework, which effectively extracts key clues and adaptively aligns multimodal features by constructing a set of universal semantic templates (filters). This framework differs from pre- vious attention-based methods, focusing on computing simi- larities between templates and inputs. Two types of filtering modules (denoising filter and dictionary filter) were designed to extract key features and establish multimodal mappings."}, {"title": "IV. ARCHITECTURE", "content": "When mapping features from a specific modality to a com- mon manifold, the feature distribution of other modalities re- mains imperceptible. This implies that embedding and aligning multimodal features in the common manifold entirely depend on the model's own experience rather than the actual data distribution. In other words, a major challenge in cross-modal Person Re-ID is ensuring that the feature distribution in the common manifold accurately reflects the feature distribution in the original modalities. This necessitates the model to possess sufficient capability to capture and understand the relation- ships between different modalities, which typically requires abundant data and carefully designed model structures."}, {"title": "A. Convolutional Neural Network", "content": "Text-based person Re-ID can be viewed as a multimodal pedestrian retrieval problem, with the primary challenge lying in the extraction of features from both visual and textual data. Due to the capability of neural networks to automatically learn and extract features from pedestrian images, the cumbersome process of manual feature design can be avoided. Most existing efforts consider employing network architectures designed for image classification as the backbone of visual networks. Con- sequently, convolutional neural networks (CNNs) have been utilized extensively in text-based pedestrian re-identification, primarily leveraging CNNs [8], [20], [35]\u2013[37], [42], [82], [92], and their major variants such as VGG-16 [17], [69], [75], [94], [101], [116], [124], MobileNet [1], [127], [140], [161], ResNet50 [37], [68], [112], [117], [118], ResNet101 [47], [79], [93], [95], [102], ResNet-152 [18], etc., for extracting crucial information in visual features. These networks are sometimes also employed for text feature extraction [35]\u2013 [37], [92]. The combination of pedestrian images extracted by CNNs and textual description information is achieved, aligning low and high-level semantic information across modalities in an implicit space using matching methods. This facilitates the recognition and matching of pedestrian identities.\nTwo independently pre-trained ResNet-50 models on Ima- geNet serve as the visual backbone in study [118]. Address- ing the challenge of manual annotation for existing cross- modal data, Jing et al. propose a Matrix Alignment Network (MAN) to tackle cross-modal cross-domain person search tasks, applying the single-modal cross-domain adaptive idea to the problem of image-text-based cross-modal person re- identification [58]. Farooq et al. introduce the Deep Vision Net model based on ResNet-50 to construct the visual feature extraction backbone [37]. Deviating from the original archi- tecture, the proposed network includes two fully connected layers before the classifier layers, applies batch normalization after the two FC layers, and employs dropout before the final layer. They further present a two-stream deep convolutional neural network framework supervised by cross-entropy loss [36]. The language network is modified to resemble the visual branch in terms of the number of layers in deep residual networks, connecting the class probability weights between the second-to-last layer and the final layer, i.e., sharing logits of the softmax layer across both networks. AXM-Net, proposed by [35], is a novel architecture based on convolutional neural networks (CNN) that does not rely on external cues for explicit multimodal features in the common manifold entirely depend alignment of feature embeddings and is capable of learning semantically aligned cross-modal feature representations. Most of the aforementioned works initialize pre-trained ResNet- 50 [48] and ResNet-101 [93] backbones on large datasets to better map modal-corresponding information. Additionally, the application of large models such as CLIP in some fine-grained visual tasks has demonstrated effective paradigm transfer, fully migrating visual-text large models based on contrastive learn- ing to text-based person re-identification tasks [147]. Presently, many studies commonly regard representation learning as a classification method for person re-identification, believing that feature embedding is a core module for text-based person re-identification tasks, crucial for narrowing the semantic gap between modalities. CNNs exhibit significant advantages in perceptual localization of local features in pedestrian vision and end-to-end learning. However, they still have certain limitations in handling long-term dependent data relationships."}, {"title": "B. Recurrent Neural Network", "content": "Beyond visual features, textual descriptions directly impact the accuracy of text-based Person Re-ID. Efficient extrac- tion of textual features is crucial under existing textual data descriptions [29]. Recurrent Neural Networks (RNNs) and their variants, as neural networks for processing sequential events, including text, have been widely accepted in the research community [75], [101], [116], [124]. Early on, in this field, word embedding techniques were initially employed to map words to vector spaces. Subsequently, variants such as LSTM [69], Bi-LSTM [12], [16], [18], [60], [94], [102], [127], [135], [136], [151], [161], [165] attempted to capture long dependencies of sentences for semantic feature extraction from text sequences. Later, with the emergence of large language models like Bidirectional Encoder Representations from Transformers (BERT) [28], the paradigm shift of \"pre- training + fine-tuning\" has been triggered in the Text-based Person Re-ID domain. Utilizing pre-trained models to learn general features of natural language and obtaining high-quality word embeddings, followed by inputting into RNNs variants to capture contextual dependencies.\nNiu et al. [93] first proposed a concise and effective frame- work for image-text alignment to address visual-semantic differences. Innovatively integrating two opposing directions, source to target and target to source, for cross-domain adap- tation. Wang et al. [117] introduced a novel Adversarial Multi-space Embedding Network (AMEN) to learn and match embeddings in multiple spaces. Bi-GRU is used for text feature extraction, and the paradigms of inter-modal and intra-modal reconstruction collaborate to embed features correctly into the opposite modal space while learning a robust common space. [81] introduces a Text-Graph Attention Network consisting of a Bi-LSTM layer, a text scene graph module, and a joint embedding layer. Bi-LSTM units gradually take the embeddings of each word in the text as input, capturing the historical and future context information of processed words. Yan et al. [151] employed an LSTM-based Recurrent Feature Aggregation Network to accumulate discriminative features from the first LSTM node to the deepest LSTM node, effectively alleviating interference caused by occlusion, background noise, and detection failures. Chen et al. [17] decomposed video sequences into multiple segments and used LSTM to learn to detect the segments where the images are located in both time and space features. This approach reduces variations in the same person across samples, favoring the learning of similarity features. Both of the above methods independently process each video frame. Ding et al. [29] employed Bidirectional Long Short-Term Memory Network (Bi-LSTM) to process each text description, capturing re- lationships between words. With contextual cues, the Word Attention Module (WAM) based on word representations infers word part correspondences, referring to word part correspondences to obtain original part-level texture features. Wang et al. [129] proposed a new algorithm called LBUL to learn a Consistent Cross-Modal Common Manifold (C3M) for text-based person retrieval. For textual samples, the proposed method embeds the text modality into C3M after considering the distribution features of cross-modalities comprehensively to achieve more reliable cross-modal distribution consensus, rather than blind predictions. However, RNN and its variants also have some issues in the Text-based Person Re-ID task. For instance, the embedding quality of pedestrian text inputs is often very sensitive, features extracted are typically influenced by the length of data sequences, and RNNs only establish temporal correlations on high-level features, thus failing to capture the temporal clues describing the local details of images."}, {"title": "C. Autoencoder", "content": "Sometimes, text-based Person Re-ID tasks also involve the utilization of Autoencoder network architectures. Com- monly, these models map images or textual descriptions into a low-dimensional embedding space and employ this em- bedding space for person re-identification tasks [47], [69]. In this manner, pedestrian images and their corresponding textual descriptions are mapped to a low-dimensional vector space, which encapsulates crucial semantic information, as both image and text provide pivotal details about the iden- tity of the pedestrian. Autoencoders, particularly those with atypical encoder components, exhibit several advantages in Text-based Person Re-ID tasks. Firstly, they can overcome issues prevalent in traditional approaches arising from the lack of semantic information in image-text pairs. Secondly, Autoencoder models proficiently capture essential information from textual descriptions and compress it, facilitating effective data compression. Additionally, this approach boasts good interpretability, as relationships between pedestrian images and textual descriptions can be visualized based on vectors within the embedding space. Wang et al. [117] introduced an innovative Adversarial Multi-space Embedding Network (AMEN) for learning and matching embeddings across multi- ple spaces. Following an encoder-decoder paradigm, the inter- modal reconstruction paradigm collaborates with the intra-modal reconstruction paradigm to embed features accurately into the opposite modal space while learning a robust com- mon space. [124] proposed an Improved Embedding Learning through Virtual Attribute Decoupling (iVAD) model. This method enhances embedding learning by using an encoder- decoder structure to decompose attribute information in images and text. Pedestrian attributes are treated as hidden vectors, and attribute-related embeddings are obtained. Diverging from previous works that separate attribute learning from image- text embedding learning, a hierarchical feature embedding framework is introduced. The Attribute-Enhanced Feature Em- bedding (AEFE) module merges attribute-related embeddings into the learned image-text embeddings, leveraging attribute information to enhance feature discriminability. However, a drawback of this approach is its reliance on a substantial amount of training data to train the Autoencoder model, and meticulous tuning of model parameters and embedding space dimensions is necessary to achieve optimal performance."}, {"title": "D. Graph Neural Network", "content": "Utilizing Graph Neural Networks (GNN) for text-based pedestrian re-identification has become a noteworthy research"}, {"title": "E. Transformer", "content": "Due to the success of Transformers in multimodal tasks, it has become a popular architecture widely employed in text-based Person Re-ID tasks [20], [40], [57], [70], [79], [106], [121], [147], [150]. Currently, a commonly used ap- proach involves a dual-stream network architecture, where one stream is dedicated to processing pedestrian images, and the other handles the corresponding text descriptions. In the image stream, convolutional neural networks [173] or vision transformers [107] are utilized to extract visual features from pedestrian images. Simultaneously, in the text stream, Long Short-Term Memory (LSTM) [140] or transformers [40] are applied to extract feature vectors from the corresponding text descriptions. Li et al. [79] introduced a transformer-based Person Re-ID architecture, evaluating the shared attention between language reference terms and visual features through transformer blocks. Apart from achieving outstanding search performance, the proposed method also provides interpretabil- ity by visualizing the attention between image parts in person pictures and their corresponding referential terms. Existing works often overlook the granularity differences between features of the two modalities, where visual features tend to be fine-grained while text features are coarse-grained, leading to significant modality gaps. Shao et al. [106] proposed an end-to-end framework named Learning Granularity-Unified Representations (LGUR), based on the transformer, aiming to map visual and text features to a unified granularity space. Addressing alignment deficiencies between image region fea- tures and text features in weakly supervised learning methods, resulting in poor feature distribution, Gao et al. [40] presented a novel method called Conditional Feature Learning based Transformer (CFLT). CFLT maps sub-regions and phrases to a unified latent space and dynamically adjusts features from one modality based on data from another modality by constructing explicit conditional embeddings. With the significant progress achieved by large models, Sarafianos et al. [102] demonstrated for the first time the ef- fectiveness of BERT as an openly accessible word embedding extraction language model in the domain of image-text match- ing. Subsequently, they combined two feature vectors using a cross-attention mechanism and applied them to pedestrian re-identification tasks. Li et al. [70] proposed a semantically aligned feature aggregation network that adaptively aggregates unit features with the same semantics into different part- perceptive features. Initially introducing Transformer-based backbones ViT [31] and BERT into visual and text modalities, Ji et al. [76] extracted robust feature representations for text- based person search. They proposed an Asymmetric Cross- Scale Alignment (ACSA) method, employing Swin Trans- former for visual representation extraction and BERT for text and phrase representation in the text domain. In contrast to previous methods that use two separate networks for feature extraction, Wang et al. [118] introduced an Implicit Visual- Text (IVT) framework, a single-stream network architecture using only a single transformer. Text and image feature extraction utilizes a backbone network, optimizing network shared parameters with cross-modal data to facilitate learning a common space mapping. Recently, Vision-Language Pre- training (VLP) models in visual language pre-training have demonstrated the transferability of knowledge for downstream Text-Picture Search (TPS) tasks, enhancing performance ef- fectively. An approach called CLIP-driven Fine-grained Infor- mation Mining (CFine) [121] was proposed to leverage CLIP's visual knowledge for person re-identification, incorporating fine-grained information mining to explore intra-modal dis-"}, {"title": "V. OPTIMIZATION", "content": "Deep metric learning plays a paramount optimization role in cross-modal person re-identification [166]. For instance, it enhances the discriminative power of feature representation, addresses a multitude of noise and variations such as diverse lighting conditions, poses, and occlusions, adapts the distance metric accordingly, embeds more semantically meaningful spatial structures, and leverages domain adaptation to tackle the challenges posed by multiple viewing angles. Of particular significance is the optimization of loss functions, thereby im- proving the matching performance in cross-modal person re- identification tasks [32]. This approach effectively addresses a spectrum of challenges, elevating the network's robustness and generalization capacity, consequently achieving more accurate person matching across distinct camera scenes and conditions. This study places emphasis on prevalent loss functions used in text-based person re-identification tasks, including identity loss, verification loss, contrastive loss, triplet loss, quadruplet loss, and adversarial loss, as shown in Figure 3."}, {"title": "A. Identity Loss", "content": "Zheng et al. [170] introduced the ID Discriminative Embed- ding (IDE) network, which treats each pedestrian as a separate category and employs the pedestrian's ID as a classification label to train a deep neural network. This allows the network to learn the capability of predicting the identity given a pedestrian image. The classification loss is typically computed using the cross-entropy loss function. During training, the classifier optimizes its weight parameters by minimizing the classifi- cation loss. A fully connected layer (FC) for classification is appended to the end of the network, and the softmax activation function maps the features to a probability space representing identity labels [42]. The cross-entropy loss for the multi-class pedestrian Re-ID task can be expressed as:\n$L_{ID} = - \\frac{1}{N} \\sum_{i=1}^N Y_i log(p_i).$ (4)\nHere, xi denotes the input image, yi represents the corre- sponding label, pi is the probability that xi is recognized as Yi after the softmax function, and N indicates the batch size. Identity loss is widely adopted in pedestrian re-identification [31], [36], [37], [87], [102], [112], [117], [118], [133], due to its automatic hard sample mining and ease of training. However, as the labeled data increases significantly, issues related to decreased efficiency in classifier training might arise, especially when dealing with datasets containing a large number of similar pedestrians. In cross-modal pedestrian re- identification, the classification loss is often used in conjunc- tion with other loss functions [47], [165], such as contrastive loss and triplet loss [69], [118], [151], [168], to enhance performance of the model."}, {"title": "B. Verification Loss", "content": "The verification loss is employed to quantify the similar- ity between two pedestrian images [171", "166": "."}]}