{"title": "Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis", "authors": ["Xin Zhang", "Deval Mehta", "Yanan Hu", "Chao Zhu", "David Darby", "Zhen Yu", "Daniel Merlo", "Melissa Gresle", "Anneke van der Walt", "Helmut Butzkueven", "Zongyuan Ge"], "abstract": "Survival analysis holds a crucial role across diverse disciplines, such as economics, engineering and healthcare. It empowers researchers to analyze both time-invariant and time-varying data, encompassing phenomena like customer churn, material degradation and various medical outcomes. Given the complexity and heterogeneity of such data, recent endeavors have demonstrated successful integration of deep learning methodologies to address limitations in conventional statistical approaches. However, current methods typically involve cluttered probability distribution function (PDF), have lower sensitivity in censoring prediction, only model static datasets, or only rely on recurrent neural networks for dynamic modelling. In this paper, we propose a novel survival regression method capable of producing high-quality unimodal PDFs without any prior distribution assumption, by optimizing novel Margin-Mean-Variance loss and leveraging the flexibility of Transformer to handle both temporal and non-temporal data, coined UniSurv. Extensive experiments on several datasets demonstrate that UniSurv places a significantly higher emphasis on censoring compared to other methods.", "sections": [{"title": "1 Introduction", "content": "The primary task of survival analysis is to determine the timing of one or multiple events, which can signify the moment of a mechanical system malfunction, the period of transition from corporate deficit to surplus, the instance of patient fatality or so on, depending on the specific circumstance. Some medical datasets are longitudinal, as exemplified by electronic health records (EHRs), where multiple observations of each patient's covariates over time are recorded. Survival models must be capable of handling such measurements and learning from their continuous temporal trends. Moreover, observations in longitudinal data are often sparse, necessitating the effective handling of missing values for any reliable survival model, even when the missing rates are exceedingly high. Additionally, censoring represents a fundamental aspect of survival data, referring to cases in which complete information regarding the survival time or event occurrence of a subject is not fully observed or available within the study period. The occurrence of censoring signifies the unknown exact timing of the event, consequently lacking ground truth for comparative learning. This, in turn, poses significant challenges for deep survival learning. Existing deep learning approaches aim at mitigating this issue by typically guaranteeing non-occurrence of events before censoring. Notwithstanding, detailed elucidation pertaining to the temporal aspect of events subsequent to censoring frequently remains inadequately explored.\nDeveloping survival analysis models requires regressing the probability of survival over a defined period. A high-quality estimation of probability distribution is essential for the time-to-event prediction. As the initial category, parametric survival models are capable of generating high-quality probability density function (PDF) or survival curve by predetermining stochastic distribution, however, their precision is contingent upon the validity of all underlying assumptions. In contrast, non-parametric models do not presume any prior distribution of events, but they struggle to accurately predict PDF over extended temporal spans within medical datasets, consequently yielding PDF or survival curve of comparatively lower quality.\nTo address the challenges in the development of survival models and to mitigate the limitations inherent in existing models, we propose UniSurv, a non-parametric model based on the Transformer architecture. In particular, UniSurv can: 1) gener-ate higher quality PDF resembling normal distribution without any prior probability"}, {"title": "2 Literature", "content": "Semi- and fully-parametric models heavily rely on the premise of making explicit assumptions about the underlying distribution of event times. They provide a structured framework for understanding the relationship between covariates and the occurrence of events over time. However, the strength of these assumptions results in overly simplistic probability distributions predicted by the models. The lack of flexibility stemming from this oversimplification also renders these models impractical in various scenarios. Cox proportional hazard (CPH) is a prime example in this field. It estimates the hazard function $(t|X)$ by multiplying a predetermined base hazard function $\\lambda_0(t)$ with the learnt representation of features $g(X)$. Subsequent studies have used more sophisticated models to improve the CPH model. However, the oversimplified stochastic process continues to constrain their predictive capabilities, and it is unable to conduct dynamic analysis. Meanwhile, introduced Deep Survival Machines (DSM), which postulates that the survival function is a composition of multiple Weibull and log-normal distributions. The parameters of those distributions are estimated by a multi-layer perceptron (MLP). Besides, illustrated Recurrent DSM (RDSM) by incorporating recurrent neural network (RNN) into DSM, thereby endowing it to process dynamic analysis. Nonetheless, DSM models exhibit suboptimal accuracy in predicting event times. Its loss function frequently becomes divergent during training, contributing to the overfitting problem.\nSome recent works have concentrated on static analysis. For example, DeepSurv employs an MLP network to replace the parametric assumptions of the hazard function present in the conventional CPH. This transformation results in a semi-parametric variant of the CPH model. The incorporation of neural networks enhances its flexibility by enabling the model to learn nonlinear relationships more adeptly from covariates. Besides, Deep Cox Mixtures (DCM) encounters the same underlying assumption of proportional hazards, wherein it assumes the presence of latent groups. Employing Variational Autoencoders for clustering, DCM assumes the validity of proportional hazards within each latent group. Moreover, propose an extension of random forest (RF) algorithm, named Random Survival Forest (RSF), which initially breaks through the inherent assumptions of CPH. It computes the risk scores through the generation of Nelson-Aalen estimators within the partitions established by RF. RSF assumes independence among trees in forest, which might not always hold. This assumption can affect its performance, usually when correlations or dependencies exist among survival trees.\nSeveral studies have explored the dynamic analysis field. propose DeepHit for competing risk events as a non-parametric model. The encoder of"}, {"title": "3 Method", "content": "In this section, we introduce our formal framework UniSurv, which is a adaptive Transformer-based architecture for survival analysis. We assume that the available survival dataset is subject to right censoring."}, {"title": "3.1 Survival Notation", "content": "We denote time-invariant and time-varying covariates by $x_n$ and $x_v$, probability by $p$, time by $T$, $t$ or $\\tau$, PDF by $p(t)$ and survival function by $S(t)$. Let's represent the survival dataset as $(x_n, x_v, T_i, \\delta_i)$, where for individual $i$, $\\delta_i$ is the event indicator typically taken from the set {0, 1} without competing risks, and $T_i$ represents the event or censoring time depending on $\\delta_i$. We omit the explicit dependence on $i$ throughout this and the next subsections for simplifying notation.\nWe assume that time $t \\in {T_0, T_1, ..., T_{\\text{max}}}$ to fit a discrete survival model, where $t$ is a discrete random variable, and $T_j$ is each time step with equal interval. The cumulative distribution function (CDF) of $t$ can be easily calculated by its PDF as\n$\\text{CDF}(T_j | (x_n, x_v)) = \\sum_{t=T_0}^{T_j} P_t \\quad(1)$\nHaving defined the probability that an event has occurred by duration $T_j$, the survival function can then be estimated as the probability that the survival time $t$ is"}, {"title": "3.2 Model Description", "content": "presents a comprehensive illustration of the UniSurv model. It integrates a novel survival loss design to enable a seamless end-to-end learning procedure. It encompasses dynamic and static extraction components, coupled with a Transformer encoder module culminating in a softmax layer at the output terminal. Besides, depicts the conceptual process of UniSurv during both training and testing stages."}, {"title": "3.2.1 Static and Dynamic Extractions", "content": "We integrate the variation of last-observation-carried-forward (LOCF) method to handle missing data. It duplicates the value of the last observation to replace the following missing values until $A_i = 3$, and the ones that are still missing after LOCF are imputed by mean/mode of all previous points for continuous/binary covariates until $T_{\\text{max}}$, making sure no missing data for all time points $T_i$. Next, we extract latent representations of time-invariant features $x_n$ and time-varying features $x_v$ by static (-s) and tabular-data-based dynamic (-d1) extraction modules separately. These modules are constructed using MLPs to deal with numerical tabular data. The representation of $x_n$ is replicated $T_{\\text{max}} + 1$ times by encompassing $T_0$ as well. For static modelling, these are subsequently transmitted to encoder. For dynamic modelling,"}, {"title": "3.2.2 Transformer Encoder", "content": "The core of the encoder module is a Transformer, which treats each patient as a 'sentence' and the embedded features as 'words' of the sentence. For an input sample, the number of words correspond to the duration $t \\in {0, 1, ..., T_{\\text{max}}}$, where we predefined $T_0 = 0$ and $T_{\\text{max}}$ is a hyper-parameter selected based on the longest temporal data of a dataset. The previous concatenated representations pass through a MLP followed by layer normalization to get the embedded features. Following the conventional approach of a Transformer, we utilize the sine-cosine positional embedding as temporal embedding in this work, and add it onto the set of embedded features, whose length is set as the embedding dimension $d_m$ of the Transformer.\nThe Transformer encoder then processes embedded features and produces $T_{\\text{max}} + 1$ outputs, each with shape $1 \\times d_m$. It is worth noting that the self-attention layers in the encoder is modified to prevent positions from attending to subsequent positions. Specifically, it prohibits each position from attending to subsequent positions, and the attention scores for all illegal connections are masked out by assigning them with -$\\infty$. Next, all-time point outputs are fed into an exclusive 2-layer MLP. The first layer is followed by rectified linear unit (ReLU) and layer normalization, with shape of $d_m \\times d_m$. The second layer, with shape of $d_m \\times 1$, is followed by a softmax layer to produce the individual estimated PDF. Further, the estimated survival function $\\hat{S}(t | (x_n, x_v))$ can be calculated based on Eq. 2, which ensures its monotonicity is preserved. Moreover, in discrete survival analysis, the mean lifetime $\\mu$ can be approximated by the sum of the survival probabilities up to $T_{\\text{max}}$. We could get the estimated mean lifetime by further involving Eq. 2 as\n$\\mu = \\sum_{t=T_0}^{T_{\\text{max}}} \\hat{S}(t) = \\sum_{t=T_0}^{T_{\\text{max}}} \\sum_{\\tau=t}^{T_{\\text{max}}} p_{\\tau} \\approx \\sum_{t=T_0}^{T_{\\text{max}}} t \\cdot P_t \\quad(3)$\nwhere the employment of the approximately equal symbol in the equation is attributed to the presence of time point $T_0 = 0$. The variance of distribution is computed as\n$v = \\sum_{t=T_0}^{T_{\\text{max}}} p_t \\cdot (t - \\mu)^2 \\quad(4)$"}, {"title": "3.3 Loss Function", "content": "To robustly estimate the uncensored survival time via distribution learning and generating smooth PDF, we adopt a variation of Mean-Variance loss in UniSurv, which requiring each training sample has a corresponding event time label."}, {"title": "4 Experiments", "content": "In this section, we demonstrate the effectiveness of UniSurv by comparing it with other benchmarks on real and synthetic datasets from static and dynamic settings."}, {"title": "4.1 Datasets", "content": "To highlight the right-skewed characteristic of survival data, we utilized three real-world datasets and two long-tailed synthetic datasets."}, {"title": "4.1.1 Static Datasets", "content": "The Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) is a large static survival dataset of seriously ill hospitalized adults. The Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) is a static breast cancer dataset aiming to distinguish its subtypes based on the molecular characteristics. Their pre-processing strategies follow DeepSurv .\nWe also generate a static synthetic dataset (SYNTH-s) of the style of that in but without competing risks. The dataset contains $N = 15,100$ examples drawn from the stochastic process\n$x \\sim \\mathcal{N}(0, I) \\\\ T_i \\sim \\text{exp}(x^T \\gamma) \\quad(11)$\nwhere $\\gamma$ is a vector of 4-dimensional variables and $\\gamma_n = 10$. We randomly select 50% patients to be right-censored with random censoring time uniformly drawn from [0, $T^2$]."}, {"title": "4.1.2 Dynamic Datasets", "content": "On the basis of SYNTH-s, we further generate dynamic synthetic dataset (SYNTH-d) by adding additional dynamic variables following Weibull distribution, and introducing"}, {"title": "4.2 Evaluation Metrics", "content": "We utilize ranking measures such as concordance index (C-index) from lifelines library and mean cumulative area under ROC curve (mAUC) from scikit-survival library, and accuracy measures such as mean absolute error (MAE) as the evaluation metrics for all experiments."}, {"title": "4.2.1 Concordance", "content": "C-index is able to estimate ranking ability by comparing relative risks across all pairs in the test set as\n$C\\text{-index} = \\frac{\\sum_{i,k} \\delta_i \\cdot \\mathbb{I}(T_i < T_k) \\cdot \\mathbb{I}(\\hat{\\mu}^i < \\hat{\\mu}^k)}{\\sum_{i,k} \\delta_i \\cdot \\mathbb{I}(T_i < T_k)} \\quad(13)$\nwhere $\\mathbb{I}(+)$ is an indicator function, and $\\delta_i$ = 0 if $T_i$ is uncensored and 1 otherwise."}, {"title": "4.2.2 MAE-Uncensored", "content": "MAE-Uncensored (MAE-U) can compensate for the inability of C-index to measure the mean absolute value of the estimated risk score. It is computed as\n$\\text{MAE-U} = \\frac{\\sum_i \\delta_i \\cdot |T_i - \\mu_i|}{\\sum_i \\delta_i} \\quad(14)$"}, {"title": "4.2.3 MAE-Hinge", "content": "MAE-Hinge (MAE-H) is a one-sided MAE for only censoring cases, opposite with MAE-U for uncensoring only. It considers only if the predicted time $\\mu$ is earlier than"}, {"title": "4.2.4 Mean Cumulative Area Under ROC Curve", "content": "The area under ROC curve for survival analysis involves treating survival issue as binary classification across various quantiles of event times and defining the sensitivity and specificity as time-dependent measures . The cumulative AUC measures model's capability of discriminating individuals who fail by a specified $t$ ($T_i \\leq t$) from subjects who fail after this time ($T_i > t$). We compute the mAUC by integrating the cumulative AUC over all time range ($T_j, T_{j+1}$)."}, {"title": "4.3 Experimental Setting", "content": "We compare with five static benchmarks, including CPH, DeepSurv, DeepHit, DSM and TDSM, and two dynamic benchmarks, including DDH and RDSM. As static dataset does not have longitudinal covariates, our dynamic extraction module in UniSurv is in non-activation mode named UniSurv-s. For MSReactor, the dynamic extraction module has two variants based on different data representations, tabular data representation named UniSurv-d1 and image-like representation named UniSurv-d2. More implementation and hyperparameter details are in the Appendix B.\nFor a fair comparison, we use C-index as early stopping criterion for all approaches as it can cover more subjects than MAE. We report the results by using cross-validation, randomly splitting datasets 5 times into training, validation and test sets with ratio 7:1:2. All experiments are implemented in PyTorch 2.0.1 on the same environments with a fixed random seed."}, {"title": "4.4 Benchmarking Results", "content": "Performance comparisons for all datasets are summarized in Tab. 2 and Tab. 3.\nWe bold the best and underline the second best. Besides, the statistical significance is determined by paired t-test between the best results and all others individually."}, {"title": "4.4.1 Static Modelling Results", "content": "In terms of C-index, our UniSurv-s secures the first position on SYNTH-s and the second position on both SUPPORT and METABRIC. It also reaches the best mAUC on METABRIC and SYNTH-s and the second best on SUPPORT. DeepSurv shows comparable ranking performance on two real-world datasets. This illustrates that parametric model still hold a slight advantage over non-parametric model, rely on its robust probability distribution assumptions. Meanwhile, the performances of the other four models vary, creating a competitive landscape. This makes it difficult to definitively judge their performance under single ranking metrics.\nMeanwhile, our UniSurv performs well in MAE-U and exhibits notably superior performance in the realm of MAE-H, with statistical significance compared to other models. Only DeepSurv in SUPPORT is comparable with ours in both two MAEs. Conversely, the performance of TDSM, while excelling in MAE-U, lags notably behind in MAE-H. This is because the loss design of TDSM leads to overfitting on uncensored data throughout the learning process, failing to capture the fact that most censored samples have longer survival times. Further, the inadequacy of TDSM's predictions for censoring is also evident by Fig. 3, in which we represent the difference between true censoring time and estimated mean lifetime with red lines for some censoring cases. We show the METABRIC results from TDSM, UniSurv-s and the second-best MAE-U model DeepHit here. The more and longer red lines, the model have less sensitivity of censoring prediction. It can be observed that UniSurv has the capability to provide accurate predictions for the majority of censoring cases. This outcome can be attributed to the incorporation of the MAE-margin concept within the Margin-Mean loss $L_{mm}$ in Eq. 6, as it leverages prior knowledge from the training dataset to effectively \"enforce\" predicted survival time to exceed the censoring time. On the other hand, DeepHit exhibits significant inefficiency in forecasting longer censoring times. Similar to TDSM, this is also due to the absence of certain constraints within its loss designs beyond the censoring time, which may give rise to a systemic bias in predicting censoring cases."}, {"title": "4.4.2 Dynamic Modelling Results", "content": "As depicted in Tab. 3, UniSurv-d1 demonstrates superior performance over two other models for longitudinal datasets, as evidenced by higher values in C-index, mAUC and lower values in two MAEs. However, the performance of these three methods is generally suboptimal, as their C-index values remain below 0.6. This occurrence likely arises from the fact that the temporal data in MSReactor diverges from conventional survival tabular data, instead representing a reaction testing approach applied to MS patients. Traditional models struggle to effectively extract meaningful insights from this intricate and redundant information. Notably, when we preprocess the computerized test data into \"reaction tensor\" and employ CNN to extract latent features, the performance of UniSurv-d2 surpasses the others with statistically significant improvements. However, this \"tensor\" method has not demonstrated effectiveness for SYNTH-d, primarily due to the isotropic distribution of each variable $x$ during data generation, resulting in their mutual independence and lack of correlation."}, {"title": "4.4.3 The Implication of Data Distribution", "content": "As shown in Fig. 4, all five histograms depict the distribution of survival times skewed towards the early segment of the time horizon, while censoring times tend to cluster in the latter half, especially in SUPPORT, SYNTH-s, MSReactor and SYNTH-d. This leads to survival models facing difficulty in maintaining predictive accuracy over time, as evidenced by the time-dependent AUC (TD-AUC). For example, all the performances of UniSurv-s, DeepHit and DSM, or their dynamic variants (UniSurv-d2, DDH, RDSM) exhibit a consistent decline in TD-AUC as time progresses. However, UniSurv still outperforms others, especially on two dynamic datasets. For METABRIC, due to its relatively low censoring rate and evenly distributed censoring cases, all three models maintain their TD-AUC quite well, with some even showing an upward trend, particularly UniSurv. It affirms that Transformer encoder based on Margin-Mean-Variance loss learning can effectively alleviate the challenges posed by survival datasets characterized by long-tail distributions."}, {"title": "4.5 Importance of Masked Attention Mechanism", "content": "In the context of leveraging Transformer for inference, the masking function within the attention mechanism is inherently discretionary, contingent upon whether each output necessitates contributions from all or specific designated inputs. For static survival data, the design of UniSurv does not entail distinctions in latent features at each time point beyond temporal embedding. Hence, there is no risk of information leakage, rendering the masking mechanism inconsequential. For instance, it is not employed in the TDSM. As demonstrated by Tab. 2, the overall performance of UniSurv-s has not been affected by removing masking mechanism from UniSurv-s, and the slight performance fluctuations can be negligible. However, when dealing with dynamic survival data, the missing data problem is inevitable, and imputations following event or censoring times may give rise to potential retro-active prediction concern. Therefore, the masking mechanism becomes imperative in such scenario. In Tab. 3, both UniSurv-d1 and UniSurv-d2 exhibited an equivalent degree of performance decline across two datasets by removing masking, which are evidenced by their ranking ability."}, {"title": "4.6 Comparison of PDF Visualizations", "content": "In addition to predictive accuracy, the quality of estimated individual PDF stands as another crucial consideration when comparing non-parametric survival models. The distribution of PDF generated by our UniSurv is specifically governed by Margin-Mean-Variance loss and remains unaffected by variations in distinct extraction modules. In Fig. 5, we present a comparison of the PDF outputs for 5 randomly selected uncensoring cases in MSReactor. We choose to contrast the DDH and UniSurv due to their absence of assumptions regarding the shape of the PDF, whereas RDSM relies on strong assumptions related to the Weibull and log-normal distributions. As described in above sections and shown in Fig. 5, our $L_{mm}$ can penalize dissimilarity between the peak of PDF and the ground truth. Besides, diverging from the disordered PDFs from DDH, $L_v$ can regulate the spread of PDF and limit it into a distinct pattern and organization. In contrast, despite using the same MLP and softmax as the output layer in DDH, the high fluctuations of PDFs can be attributed to the shortcomings in its loss function design.\nThe unimodal nature of survival PDF offers several advantages. For example, it can better reflect the time-to-event and naturally calibrate the median survival time corresponding to survival curve, such as employed several predefined unimodal distributions for survival modelling. However, UniSurv departs from this assumption, achieving the same objective through a distinctive loss design. The current over-concentrated PDF is not optimal, and appropriately adjusting $L_v$ to relax its constraints on the shape will become necessary."}, {"title": "4.7 Ablation Study", "content": "We further conduct an ablation study of losses on MSReactor using UniSurv-d2, to demonstrate the contribution of each loss. In Fig. 6(a), it is evident that an incomplete loss combination sometimes can lead to lower MAE-U or MAE-H, however, this"}, {"title": "4.8 Effectiveness Analysis", "content": ""}, {"title": "4.8.1 Sensitivity of Time Window Tw", "content": "Fig. 6(b) shows the effect of $T_w$. We can observe that when $T_w = 8$, the model can achieve the highest C-index and lowest MAE-H, which is associated with the progression rate of MS. However, during the same period, MAE-U demonstrates its poorest performance. It is also apparent that the fluctuations in MAE-U and MAE-H exhibit a contrasting pattern. This disparity can be attributed to the disparate distributions of censoring and uncensoring within the MSReactor as in Fig. 4. Meanwhile, this underscores that there exists potential for enhancing the robustness of UniSurv."}, {"title": "4.8.2 Sensitivity of Loss Weights Am And \u03bb\u03c5", "content": "As the number of losses increases, finding the optimal weight combination indeed becomes challenging, but grid search can take care of this. The four losses do not need to be standardized to a similar magnitude. The unique characteristics of different datasets can lead to distinct optimal weights for losses. We assessed the sensitivities of $\\lambda_m$ and $\\lambda_v$ particularly on MSReactor in Fig. 6(c) and Fig. 6(d), and some selected PDFs are shown in Fig. 6(g). The model exhibits robustness when small variations occur in $\\lambda_m$ or $\\lambda_v$, as the performance near their optimal values does not exhibit significant degradation. In some cases, two MAEs even perform better. This phenomenon is attributed to the opposing fluctuation trends exhibited by the MAEs, indicating a trade-off made by UniSurv during training. Notably, the C-index appears to be more sensitive to changes in $\\lambda_v$ compared to variations in $\\lambda_m$. As the variations in both weights increase, deviations in the PDF gradually emerge, with its peak drifting further away from the actual event time and assuming irregular shapes."}, {"title": "4.8.3 Sensitivity of Larger and Noised Synthetic datasets", "content": "To emphasize UniSurv's reliability for higher dimensionality datasets and its robustness to data noise, we have expanded the number of features for the existing synthetic datasets SYNTH-S and SYNTH-d without altering event or censoring settings, resulting in new SYNTH-Sk and SYNTH-dk datasets, where k denotes the dimension of $x$ in Eq. 11 and Eq. 12 is increased from 4 to $4 \\cdot 5^k$, and the dimension of $x$ in Eq. 12 is increased from 20 to $20 \\cdot 5^k$. Additionally, we introduced noise $\\epsilon^2 \\sim \\epsilon \\cdot \\mathcal{N}(0, I)$ to $x$ and $x_v$ separately in all datasets. As the results shown in Tab. 4, UniSurv performs well on high-dimensional datasets and exhibits robustness to small levels of noise interference."}, {"title": "5 Conclusion And Discussion", "content": "In this paper, we propose a non-parametric discrete survival model named UniSurv. Departing from the existing models of utilizing RNN for processing longitudinal data, we employ a Transformer for adeptly handling dynamic analysis. In particular, our survival framework firstly integrates imputation for handling missing data issue, then incorporates different embedding branches for time-varying and time-invariant features extraction. The Transformer encoder takes merged features as input and outputs the individual PDF. We also demonstrated how to process image-like data using variations of modules and how to select a time window based on the progression speed of the disease to share information. This is particularly beneficial in the field of medicine, as obtaining regular time-series medical images in the real world is challenging.\nFurthermore, our novel Margin-Mean-Variance loss effectively produces smooth PDF in a unimodal manner, demonstrating clear superiority over other discrete models. Importantly, the proposed loss can be seamlessly embedded into various discrete survival models. Moreover, it significantly enhances prediction accuracy, particularly for patients with extended censoring times. Applying poorly performing models in such scenarios could evidently disrupt physician's judgments and place unnecessary burdens on both society and healthcare institutions. This constitutes a valuable contribution. Although our current PDF may appear overly concentrated around event times, akin to many models relying on strong probability assumptions, resulting in unconventional survival curves, we intend to further modify the $L_s$ and $L_v$ to relax certain constraints in the future. This adjustment aims to yield a more elegant PDF, characterized by a smoother and less abrupt distribution while maintaining overall performance. Meanwhile, adapting UniSurv to accommodate multiple censoring scenarios, such as left truncation and interval-censored data, presents an interesting direction for future research. Additionally, expanding the scope to include a post-processing statistic for interpreting risk predictions in both static and dynamic analyses of disease"}]}