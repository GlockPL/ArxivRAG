{"title": "Continual Quantization-Aware Pre-Training: When to transition from\n16-bit to 1.58-bit pre-training for BitNet language models?", "authors": ["Jacob Nielsen", "Peter Schneider-Kamp", "Lukas Galke"], "abstract": "Large language models (LLMs) require im-\nmense resources for training and inference.\nQuantization, a technique that reduces the pre-\ncision of model parameters, offers a promis-\ning solution for improving LLM efficiency and\nsustainability. While post-training quantiza-\ntion methods typically achieve 4-8 bits per pa-\nrameter, recent research suggests that training\nLLMs with 1.58 bits per weight parameter from\nscratch can maintain model accuracy while\ngreatly reducing memory requirements and en-\nergy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-\naware pre-training, where the models are first\ntrained with 16-bit precision and then transi-\ntion into 1.58-bit quantization-aware training.\nOur results on 11 downstream tasks show that\nthis 16-to-1.58-bit training strategy is prefer-\nable over full 1.58-bit training and leaves mod-\nels closer to those which have undergone 16-bit\ntraining. We further investigate the effects of re-\ntaining the optimizer state at the transition point\nand gradually phasing in quantization strength\nfinding that both techniques alleviate the mag-\nnitude of loss spikes, but also that these effects\ncan be compensated through further training.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutional-\nized natural language processing and are making a\nstrong entry into both related and unrelated indus-\ntries. However, deployment of LLMs comes with a\nseries of obstacles such as memory-usage, latency,\nand throughput. Environmental considerations re-\ngarding energy consumption of both training and\ninference becomes an increasingly important as-\npect (Schwartz et al., 2020; Strubell et al., 2020).\nQuantization-aware training of language mod-\nels, i.e. preparing the model for later quantization\nalready during training, has shown promising re-\nsults (Wang et al., 2023; Ma et al., 2024). However,\nthe resulting models tend to require more parame-\nters to compensate for the reduction in bit-precision\nper parameter (Kumar et al., 2025; Nielsen and\nSchneider-Kamp, 2024). Here, we investigate a\nstrategy of first training with standard precision in\nan initial phase, followed by a second phase of 1.58-\nbit quantization-aware training, where weights are\nbeing quantized to either -1, 0, or 1.\nReducing inference compute demands of lan-\nguage models is particularly critical in the con-\ntext of recent advances in scaling test time com-\npute (Guo et al., 2025; Muennighoff et al., 2025;\nJaech et al., 2024). If the trend of scaling test-time\ncompute continues, we can assume that inference\nwill soon dominate the resource consumption in a\nlanguage model's life cycle.\nRecent works in 1-bit (Wang et al., 2023) and\n1.58-bit quantization-aware training (Ma et al.,\n2024; Nielsen and Schneider-Kamp, 2024; Nielsen\net al., 2024), demonstrate the potential of training\nin 1.58-bit precision while retaining most of the per-\nformance, mitigating some of the drawbacks of ex-\nisting post-training quantization techniques in both\nNLP and computer vision (Frantar et al., 2023; Li\nand Gu, 2023). To achieve competitive model per-\nformance, these quantization-aware training strate-\ngies keep 16-bit-precision weights at training time\n(\u201cshadow weights\u201d), which are quantized on-the-fly\nto 1-bit or 1.58-bit precision during forward passes.\nStraight-through estimated gradients (Bengio et al.,\n2013) are then used to update the shadow weights.\nWhile such a training strategy initially requires\nmore memory and compute than pre-training a reg-\nular language model, its benefits can be harvested\nafter training. At inference time, the final shadow\nweights can be quantized once and for all, after\nwhich the shadow weights can be discarded, yield-\ning a model with a 8-16 times reduced memory\nfootprint. With appropriate kernels, this effectively\nallows us to replace costly matrix multiplications\nwithin linear layers with computationally more ef-\nficient integer addition operations."}, {"title": "2 Related work", "content": "Early attempts of ternary-weight neural nets.\nThe exploration into ternary weights was motivated\nby finding a better trade-off between accuracy and\ncomplexity than binary neural networks, which had\nsuffered a substantial decrease in performance, ef-\nfectively hindering the usability of such networks\n(Chen et al., 2021). Earlier attempts of binary- and\nternary-weight neural networks showcased ternary\nsuperiority over binary weights, while showing\npromising results in the computer vision domain\nemploying a direct optimization of the quantization\n(Li et al., 2016; Zhu et al., 2016).\nPost-training quantization. The most common\napproaches for quantization fall under the cate-\ngory of post-training quantization. Those include\napproaches such as Generative Pre-trained Trans-\nformer Quantization (GPTQ) (Frantar et al., 2023)\nand Activation-aware Weight Quantization (AWQ)\n(Lin et al., 2024). A similar proposal for the vision\ntransformer (Li and Gu, 2023) represents one of\nmany efforts in the computer vision domain. Post-\ntraining quantization approaches come with an in-\nherent decrease in performance, trading increased\nlatency and throughput and decrease memory for\nprecision (Kumar et al., 2025).\nQuantization-aware training. Quantization-\naware training was already proposed in earlier\nwork on post-training such as LLM-QAT (Liu\net al., 2023) and QA-LORA (Xu et al., 2023).\nThese methods directly optimize the quantized\nweights with respect to an objective function such\nthat there is no decrease in performance when the\nmodel is used for inference.\nRecently, we have seen a number of works on\n1-bit (Wang et al., 2023) and 1.58-bit (Ma et al.,\n2024) quantization-aware techniques demonstrat-\ning strong performance in LLM performance, yield-\ning a small or no loss in precision depending on\nmodel sizes. Other works have demonstrated strong\npotential for multi-modal architectures (Sundaram\nand Iyer, 2024) and spiking language models (Bal\net al., 2024). Lastly, we have seen an investigation\ninto the potential of 1.58-bit in small language and\nvision models and the definition of a scaling law\nfor decoder-only models for reintroducing capac-\nity (Nielsen and Schneider-Kamp, 2024). Similar\nscaling laws hold for encoder-only models while\nencoder-decoder models seem to be less predictable\n(Nielsen et al., 2024). This latest work further\nshows that also non-transformer models, such as\nplain multi-layer perceptions and graph neural net-\nworks can attain similar performance as their 16/32-\nbit counterparts, even without increasing the num-\nber of parameters."}, {"title": "Summary.", "content": "Research on language model quan-\ntization shows promising results but is also lim-\nited by the effectiveness of the final models com-\npared to standard precision models. There is a\nstrict distinction between post-training methods\nand quantization-aware training. So far, it remains\nunexplored whether an initial phase of standard 16-\nbit precision training would improve or worsen the\nperformance of the final model when continuing\nwith 1.58-bit quantization-aware pre-training."}, {"title": "3 Methods", "content": "We first recapitulate the basics of quantization-\naware training (Section 3.1) before describing the\nproposed strategy of continual 1.58-bit pre-training\n(Section 3.2) and discussing critical considerations\nconcerning optimizer states (Section 3.3) and grad-\nually phasing in quantization strength (Section 3.4)."}, {"title": "3.1 Background on 1.58-bit training", "content": "Recent work has focused on specifically on 1.58-\nbit quantization-aware training techniques (Wang\net al., 2023; Nielsen and Schneider-Kamp, 2024;\nNielsen et al., 2024). Specifically, these works in-\nvestigate ternary networks, where the weights only\ncan take on the values -1, 0 and 1. The quantization\nis guided by 16-bit precision \u201cshadow weights\u201d,\nwhich are quantized during the forward passes and\nrely on a straight-through estimator for optimiza-\ntion. Activations are also commonly quantized in\na given range $Q_b$, which is then multiplied with\nthe quantized weight-matrix. Intuitively, the the\nweight-matrix then either is adding, ignoring, or\nsubtracting the previous layer's activations using\n-1, 0, or 1, respectively.\nThe key idea of 1.58-bit training is to maintain\n16-bit precision \u201cshadow weights\" and quantize\nthem on-the-fly. We follow the basic formulation of\nBitNet (Wang et al., 2023) for replacing activations\nand weights in all linear layers of a language model\nas follows:\n$W_{quant} = max(-1, min(1, round(W \\cdot W_{scale}))$\n$X_{quant} = max(-Q_b, min(Q_b - 1, round(\\hat{I} \\cdot X_{scale}))$"}, {"content": "Recent work has focused on specifically on 1.58-\nbit quantization-aware training techniques (Wang\net al., 2023; Nielsen and Schneider-Kamp, 2024;\nNielsen et al., 2024). Specifically, these works in-\nvestigate ternary networks, where the weights only\ncan take on the values -1, 0 and 1. The quantization\nis guided by 16-bit precision \u201cshadow weights\u201d,\nwhich are quantized during the forward passes and\nrely on a straight-through estimator for optimiza-\ntion. Activations are also commonly quantized in\na given range $Q_b$, which is then multiplied with\nthe quantized weight-matrix. Intuitively, the the\nweight-matrix then either is adding, ignoring, or\nsubtracting the previous layer's activations using\n-1, 0, or 1, respectively.\nThe key idea of 1.58-bit training is to maintain\n16-bit precision \u201cshadow weights\" and quantize\nthem on-the-fly. We follow the basic formulation of\nBitNet (Wang et al., 2023) for replacing activations\nand weights in all linear layers of a language model\nas follows:\n$W_{quant} = max(-1, min(1, round(W \\cdot W_{scale}))$\n$X_{quant} = max(-Q_b, min(Q_b - 1, round(\\hat{I} \\cdot X_{scale}))$ where W denotes the weight parameters of the\nlinear layers (\u201cshadow weights\u201d) and \u00ce denotes the"}, {"title": "3.2 Continual 1.58-bit Pre-training", "content": "We hypothesize that, to obtain the best possible\n1.58-bit model given a fixed amount of data, the\nmodel needs to first find a good set of 16-bit pa-\nrameters before those can be quantized to 1.58-bit.\nAs such, we hypothesize that there exists a point t*\nduring training, at which one can switch from 16-\nbit training to 1.58-bit training in order to achieve\nan ultimately better 1.58-bit model, than one would\nobtain by complete quantization-aware training on\nthe same data.\nFormally, given a training set ${x_i}_{i<N}$, we hy-\npothesize that there exists a specific point in train-\ning t*, such that a model first trained with 16 bit on\n$X_0, X_1,..., X_{t*}$ and then trained with 1.58 bit on\n$X_{t*+1}, X_{t*+2}..., X_{N-1}$ ultimately performs bet-\nter than a model pre-trained with 1.58-bits on the\nfull training set $X_0, X_1 . . . X_{N\u22121}$. See Figure 1. At\nthe transition point t*, the 16-bit weights turn into\n\"shadow weights\" for quantization-aware training."}, {"title": "3.3 Optimizer State Retention", "content": "A key consideration in investigating continual 1.58-\nbit pre-training is the availability of optimizer\nstates. If the optimizer states are available, we\nexpect a smooth transition from pre-training to con-\ntinued pre-training. In our strictly controlled ex-\nperimental setup, the optimizer states are available.\nHowever, in practice, optimizer states cannot be\nassumed to be available, when one would seek con-\ntinuing pre-training on arbitrary base models. We\ntake the opportunity to investigate the effect of opti-\nmizer states being available vs. assuming that they\nwere not available."}, {"title": "3.4 Phasing in Quantization Strength", "content": "We further investigate a recently proposed tech-\nnique to gradually increase quantization strength.\nIn this formulation, we would train a model\non $X_0, X_1,...,X_s$ with 16-bit training, and then\ngradually introduce quantization strength on the\ndata $X_{s+1}, X_{s+2},...X_{t*}$ (Mekkouri et al., 2024).\nSpecifically, we consider an extra hyperparame-\nter \u03bb integrated into the calculation of $x_{quant}$ and\n$W_{quant}$ denoting the activations and weight quan-\ntization respectively. We formulate the gradual"}, {"title": "4 Experimental Setup", "content": "We conducted experiments investigating the poten-\ntial of continuing the pretraining in 1.58-bits from\n(partially) pre-trained 16-bit models, initializing\nthe 1.58-bit model's \u201cshadow-weights\u201d with the\npre-trained 16-bit weights. Furthermore, we inves-\ntigate the impact of optimizer state retention and\nphasing in quantization strength.\nArchitecture Specifically, we employ the model\narchitecture of Open Language Models (Groen-\neveld et al., 2024) in their official 1B parame-\nters configuration. We use the BitLinear pack-\nage\u00b9 to all replace all nn. Linear layers within an\nOLMo model by BitLinear layers (as described\nin Section 3), which includes both projection layers\nwithin the attention and feed-forward modules.\nTraining We train all models on the Dolma\ndataset (Soldaini et al., 2024), with OLMo's stan-\ndard hyperparameters2. In particular, optimization"}, {"title": "5 Results", "content": "We first present the results regarding 16-to-1.58-\nbit quantization-aware training in Section 5.1, fol-\nlowed by results comparing effects of and optimizer\nstate retention (Section 5.2) and of phasing in of\nquantization strength (Section 5.3). Lastly, we re-\nport the results of the evaluation on downstream\ntasks in Section 5.4."}, {"title": "5.1 Results for Continual 1.58-bit Pre-training", "content": "We conduct experiments investigating the poten-\ntial in continuing the pre-training in 1.58-bits\nfrom a 16-bit models, initializing the 1.58-bit\nmodel's \"shadow weights\u201d with the pre-trained 16-\nbit weights. Further, we investigate the impact of\ngradually phasing in the quantization strength, and,\nlastly, continuing the 1.58-bit pre-training with the\n16-bit pre-training optimizer.\nWe show that the best regime encountered in\nthese pre-trainings consists of 2K 16-bit steps,\ndemonstrating a gradually decrease in performance\nfor both 4K and 6K 16-bit steps starting-points.\nEven more importantly, all three continual 1.58-bit\ntraining runs achieve a better performance than the\nfull 1.58-bit training, demonstrating that training\n1.58-bit models from scratch is suboptimal.\nIn Figure 2a, we observe that training a 16-bit\nmodel from scratch for 10K steps yields a loss\nvalue at 2.95 (red), whereas a fully 1.58-bit training\nachieves a loss around 3.15 (purple) after equally\nmany steps. Continuing the pre-training from 2K\n16-bit steps yields the best of the continued pre-\ntraining variants, with a resulting loss value around\n3.088 (green). We observe that transitioning early\nyields a spike in the loss for around 100 steps be-\nfore the model is able to catch up. Continuing from\n4K 16-bit steps (light green) exhibits a smaller\ncurve-spike, yielding a loss value at 3.097 after\n10K steps. Last, continuing from 6K 16-bit steps\n(dark blue) exhibits an even smaller curve-spike\nyielding a loss value of around 3.12.\nOverall, it is clear that training 1.58-bit from\nscratch is suboptimal. Furthermore, the training\nloss curves indicate that 16-bit training for more\nsteps is likely detrimental to the overall training\nloss. The choice of 2K 16-bit steps seems to be\noptimal within the four regimes considered."}, {"title": "5.2 Effect of Optimizer State Retention", "content": "We investigate the impact of retaining the optimizer\nfrom the 16-bit pre-training, a warm optimizer, in-\nstead of constructing a newly initialized optimizer.\nWe observe that spikes in loss curves in Figure\n2a can be dampened by employing the optimizer\nstates from the 16-bit training, demonstrated by the\nexperiments with yellow, pink, and teal loss curves,\nyielding a lower and smoother spike-curve across\nall transfers. However, we observe the smoothen-\ning in the transition is gradually having a smaller\neffect, when transitioning at later stages (4K and"}, {"title": "5.3 Effect of Phasing in Quantization Strength", "content": "Figure 2b shows the impact of phasing in quanti-\nzation strength. In the \u201cnowarm\" case, the training\nloss exhibits large spikes, which are however re-\ncovered relatively soon. After 10K total steps, the\ntraining loss is again at the same level compared\nto gradually phasing in quantization strength. This\nsuggests that phasing in quantization strength is not\nhaving a lasting impact when pre-training language\nmodels, and that there is sufficient time to recover\nfrom any disturbance imposed by the abrupt quan-\ntization. It seems that quantization may as well be\nintroduced at one point in time, alleviating the need\nfor tuning extra hyperparameters and schedules de-\ntermining the quantization strength."}, {"title": "5.4 Results of the Downstream Evaluation", "content": "We evaluated the downstream performance on all\ndownstream tasks after each 1K optimizer steps.\nWe report the results of the final downstream\nevaluation after 10K steps in Table 1. Figure 3\nshows the trajectories of downstream task perfor-"}, {"title": "6 Discussion", "content": "Through a comprehensive set of experiments where\nwe have full control over the pre-training data\nand regimen, we have shown that, counterintu-\nitively, quantization-aware training of 1.58-bit mod-\nels from scratch is not ideal for obtaining the best\npossible 1.58-bit models. In addition, we find that\npreviously proposed strategies of gradually phasing\nin quantization strength are not needed as long as\nthere are sufficient optimization steps to catch up."}, {"title": "7 Conclusion", "content": "We conducted a systematic comparison between\nlanguage models trained from scratch with 1.58-bit\nprecision and 16-to-1.58-bit continual pre-training\nparadigms. Our results show the existence of a\ndata-optimal transition point for switching from 16-\nbit to 1.58-bit training, providing insights into effi-\ncient low-bit training strategies. We evaluated the\ndownstream performance of 1.58-bit models, high-\nlighting their viability for real-world applications.\nThese findings contribute to the broader discussion\non low-bit training efficiency and its implications\nfor scalable AI model development. Future work\nmay determine the minimal amount of data needed\nto successfully convert a 16-bit model into a 1.58-\nbit model through continual pre-training."}, {"title": "8 Limitations", "content": "The performance on downstream tasks are expected\nto be further increased when the models undergo\nsupervised instruction fine-tuning and preference\noptimization before evaluation. We expect that\nall models benefit similarly from instruction fine-\ntuning. However, at this point in time, it cannot\nbe excluded that 16-bit models benefit more from\ninstruction fine-tuning and preference optimizaiton\nthan 1.58-bit models. Future work may specifi-\ncally investigate the effects of 1.58-bit quantization-\naware training during instruction fine-tuning and\npreference alignment."}, {"title": "9 Ethical Considerations", "content": "Our work aims at improving the inference compute\ndemands of langauge models. It may contribute\nto the democratization of AI, alleviating privacy\nconcerns, and to reduce the environmental foot-\nprint of LLMs. In particular, our findings suggest\nthat large language models may be converted into\nlower bit-precision through continual pre-training.\nWe acknowledge that this may come with ethical\nchallenges that govern all research on making pow-\nerful models more accessible (Bengio et al., 2025).\nHowever, this work is purely scientific and does\nnot promote easier access to an actual pool of very\npowerful models. Using our proposed training strat-\negy would require a similar compute budget as it is\nneeded for standard pre-training."}]}