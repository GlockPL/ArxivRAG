{"title": "STAGE-WISE AND PRIOR-AWARE NEURAL SPEECH PHASE PREDICTION", "authors": ["Fei Liu", "Yang Ai", "Hui-Peng Du", "Ye-Xin Lu", "Rui-Chen Zheng", "Zhen-Hua Ling"], "abstract": "This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase Prediction (SP-NSPP) model, which predicts the phase spectrum from input amplitude spectrum by two-stage neural networks. In the initial prior-construction stage, we preliminarily predict a rough prior phase spectrum from the amplitude spectrum. The subsequent refinement stage transforms the amplitude spectrum into a refined high-quality phase spectrum conditioned on the prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone and adopt adversarial training by innovatively introducing a phase spectrum discriminator (PSD). To further improve the continuity of the refined phase, we also incorporate a time-frequency integrated difference (TFID) loss in the refinement stage. Experimental results confirm that, compared to neural network-based no-prior phase prediction methods, the proposed SP-NSPP achieves higher phase prediction accuracy, thanks to introducing the coarse phase priors and diverse training criteria. Compared to iterative phase estimation algorithms, our proposed SP-NSPP does not require multiple rounds of staged iterations, resulting in higher generation efficiency.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech phase prediction is a crucial task in the field of speech signal processing. The phase information of speech signals plays a vital role in numerous speech generation tasks, e.g., speech synthesis (SS) [1, 2, 3, 4, 5], speech enhancement (SE) [6, 7, 8], bandwidth extension (BWE) [9, 10, 11], etc. Currently, most of the above tasks focus on predicting the amplitude information of speech signals or derived features (e.g., mel spectrograms and mel cepstra). Therefore, predicting phase information for these tasks remains to be explored. The speech phase prediction aims to recover the missing or unknown phase information from the known amplitude information, thereby restoring the complete short-time spectral information which can be converted to speech waveform via inverse short-time Fourier transform (ISTFT).\nEarly research on speech phase prediction primarily focused on iterative estimations of the phase, such as the well-known Griffin-Lim algorithm (GLA) [12]. GLA estimates the phase spectrum from the amplitude spectrum by iteratively executing STFT and ISTFT.\nIn each iteration (except the first one), GLA uses the phase generated from the previous iteration as a prior, progressively refining the phase. Its implementation is relatively simple, which has led to its widespread application in various speech generation tasks. However, the accuracy of the phase estimated by GLA and some of its variants [13, 14] remains unsatisfactory due to their overly simplistic alternating projection operators. Recently, Kobayashi et al. [15] has proposed applying the relaxed averaged alternating reflection (RAAR) algorithm from the optics community to speech phase prediction, utilizing a more complex alternating reflection operator, which has shown impressive results. However, the complex iterative operator severely impacts the efficiency of phase estimation.\nWith the advancement of deep learning, methods combining traditional iterative algorithms and neural networks have emerged. For example, Masuyama et al. [16, 17] introduced a deep Griffin-Lim iteration (DeGLI), which utilizes a trainable neural network to simulate the GLA process and achieve iterative phase reconstruction. Takamichi et al. [18, 19] employed a prior-distribution-aware approach, assuming that the phase follows a specific prior distribution (e.g., von Mises distribution or sine-skewed generalized cardioid distribution), and then uses a deep neural network (DNN) to predict the phase information. However, the phase predicted by the DNN still needs to be refined through GLA iterations. Therefore, this type of methods has the disadvantages of high complexity and low efficiency.\nIn recent years, to overcome the accuracy and efficiency bottlenecks in speech phase prediction, researchers have attempted to achieve phase prediction solely using neural network-based approaches. In our previous work [20, 21], we have proposed a neural speech phase prediction (NSPP) model, achieving direct phase spectrum prediction from amplitude spectrum only through a neural network. The NSPP designs specialized structures and losses tailored to the characteristics of the phase. It utilizes a residual convolutional network and a parallel estimation architecture (PEA) to propagate the input log amplitude spectrum and directly output the wrapped phase spectrum. The PEA is critical to direct phase prediction and consistes of two parallel convolutional layers and a phase calculation formula. During training, NSPP proposes an anti-wrapping phase loss, which effectively reduces the error between the predicted and natural phase, which is vital for accurating phase prediction. Experimental results have confirmed that NSPP has higher phase prediction accuracy and faster prediction speed than traditional iterative algorithms.\nHowever, NSPP still has some limitations. Firstly, NSPP relies solely on amplitude information as input, without prior phase information, making the learning process more challenging. Secondly, the backbone of NSPP must be updated, as it is complex and redun-"}, {"title": "2. PROPOSED METHOD", "content": "2.1. Overview\nAn overview of the proposed SP-NSPP architecture is shown in Figure 1. The SP-NSPP is a two-stage model that predicts the phase spectrum $P_{refine} \\in \\mathbb{R}^{F \\times N}$ from the input log amplitude spectrum $A \\in \\mathbb{R}^{F \\times N}$, using the prior phase spectrum $P_{prior} \\in \\mathbb{R}^{F \\times N}$ as a bridge, where $F$ and $N$ denote the number of frames and frequency bins, respectively.\n\u2022 Prior Construction Stage: In this stage, only the log amplitude spectrum $A$ is used as input to preliminarily predict a coarse phase spectrum $P_{prior}$ as prior for subsequent stage, i.e.,\n$P_{prior} = Model_{pc}(A),$ (1)\nwhere $Model_{pc}$ is the prior construction model.\n\u2022 Refinement Stage: In this stage, the refinement model converts the log amplitude spectrum $A$ into the final refined phase spectrum $P_{refine}$, conditioned on the prior phase spectrum $P_{prior}$, i.e.,\n$P_{refine} = Model_{r}(A|P_{prior}),$ (2)\nwhere $Model_{r}$ is the refinement model. Introducing prior information is expected to enable the refinement model to achieve more accurate phase prediction based on this prior, thereby reducing the learning difficulty compared to models without prior information (e.g., NSPP [20, 21]).\nFinally, the input log amplitude spectrum $A$ and the refined phase spectrum $P_{refine}$ are used to reconstruct the speech waveform $z \\in \\mathbb{R}^{T}$ through ISTFT, where $T$ denotes the waveform sample numbers.\n2.2. Model Structure\nAs shown in Figure 1, the prior construction and refinement models share the same structure with different parameters. The only difference between the two models is in their inputs. For the prior construction model, the input is the log amplitude spectrum. For the refinement model, the log amplitude spectrum is concatenated with the conditional prior phase spectrum and used as its input.\nFor the prior construction or refinement model, the input first undergoes processing through a 1D convolutional layer. It then passes through a layer normalization (LN) [22], followed by deep processing using a ConvNeXt v2 [23] network. The output of the ConvNeXt v2 network is further processed through another LN and a liner layer. The output of the liner layer is then inputted into the PEA to predict the wrapped phase spectrum. The ConvNeXt v2 network consists of multiple cascaded ConvNeXt v2 blocks. As depicted in Figure 1, each ConvNeXt v2 block employs a residual connection structure, with the core modules including 1D depth-wise convolutional layer, LN, linear layer, global response normalization (GRN) [23] layer and Gaussian error linear unit (GELU) activation [24]. The PEA is borrowed from the NSPP [20, 21]. It comprises two parallel 1D convolutional layers and an atan2 phase calculation formula. It mimics the process of calculating the phase spectrum from the real and imaginary parts of the complex spectrum and strictly constrains the wrapped predicted phase values within the principal value range. Therefore, PEA is a crucial module for the direct prediction of the wrapped phase.\n2.3. Training Criteria\nThe training of the prior construction model and the refinement model is hierarchical and separated. After the prior reconstruction"}, {"title": "2.3.1. Training Criteria of Prior Construction Model", "content": "As shown in Figure 2, the anti-wrapping losses borrowed from NSPP [20, 21] and the newly proposed phase adversarial loss are used to jointly train the prior reconstruction model. The anti-wrapping losses are defined between the prior phase spectrum $P_{prior}$ and the natural one $P$ and includes instantaneous phase (IP) loss $L_{IP}(P_{prior}, P)$, group delay (GD) loss $L_{GD}(P_{prior}, P)$, and instantaneous angular frequency (IAF) loss $L_{IAF}(P_{prior}, P)$, i.e.,\n$L_{P}(P_{prior}, P) = L_{IP}(P_{prior}, P) + L_{GD}(P_{prior}, P) + L_{IAF}(P_{prior}, P).$ (3)\nThese three losses are computed by using an anti-wrapping function $f_{aw}(x) = |x \u2212 2\u03c0 \\cdot round(\\frac{x}{2\u03c0})|$ to activate the direct errors of IP, GD, and IAF, respectively. This anti-wrapping function can effectively prevent the issue of training error expansion caused by the phase wrapping characteristics.\nFor the phase adversarial training, the proposed SP-NSPP incorporates a PSD to ensure high-quality phase prediction. As shown in Figure 2, the PSD takes either $P_{prior}$ or $P$ as input. It consists of five 2D convolutional layers interleaved with leaky rectified linear unit (LReLU) activation to capture time-frequency features. The processed features are finally passed through a 2D convolutional layer to output the discriminative values. During training, the PSD is trained to classify natural phase samples as 1 and generated samples from the generator as 0. Conversely, the prior construction model (i.e., the generator) is trained to generate samples that resemble those classified as 1 by the PSD as closely as possible. We use the adversarial loss with hinge form which is defined as:\n$L_{adv-G}(P_{prior}, P) = \\mathbb{E}_{P_{prior}} max \\left(0, 1 - PSD\\left(P_{prior}\\right)\\right),$ (4)\n$L_{adv-D}(P_{prior}, P) = \\mathbb{E}_{(P_{prior}, P)} max \\left(0, 1 - PSD(P)\\right) +\\mathbb{E}_{(P_{prior},P)} max \\left(0, 1 + PSD(P_{prior})\\right).$ (5)\nWe also introduce the commonly used feature matching (FM) loss $L_{FM}(P_{prior}, P)$ in vocoder tasks [25, 26], defined as the sum of the mean squared errors (MSEs) of the intermediate layer outputs of PSD when taking $P_{prior}$ or $P$ as input.\nTherefore, the final loss for the prior construction model is as follows.\n$L(P_{prior}, P) = \\lambda_{p}L_{p}(P_{prior}, P)+\\lambda_{PSD} \\left(L_{adv-G}(P_{prior}, P) + L_{FM}(P_{prior}, P)\\right),$ (6)"}, {"title": "2.3.2. Training Criteria of Refinement Model", "content": "where $\\lambda_{p}$ and $\\lambda_{PSD}$ are hyperparameters. The prior construction model and the PSD are trained in an alternating manner with $L(P_{prior}, P)$ and $L_{adv\u2212D}(P_{prior}, P)$, respectively.\n2.3.2. Training Criteria of Refinement Model\nAt the refinement stage, we introduce an additional TFID loss to train the refinement model, compared to training the prior construction model. The TFID loss simultaneously considers the differential values of the phase spectrum in both time and frequency directions, further enhancing the temporal and frequency continuity of the phase spectrum for refined optimization.\nGiven a matrix $X \\in \\mathbb{R}^{F\\times N}$, We first define a series of vector transformation operations within X as follows,\n$\\begin{aligned}\n\\mathcal{O}_{CL}X = \\left[\\omega_{2}, \\omega_{3}, ..., \\omega_{N}, \\mathbf{0}\\right],\\\\\n\\mathcal{O}_{CR}X = \\left[\\mathbf{0}, \\omega_{1}, \\omega_{2}, ..., \\omega_{N-1}\\right],\\\\\n\\mathcal{O}_{RU}X = \\left[\\upsilon_{2}^{T}, \\upsilon_{3}^{T}, ..., \\upsilon_{F}^{T}, \\mathbf{0}^{T}\\right],\n\\end{aligned}$ (7)\n(8)\n(9)\nwhere $\\omega_n$ and $\\upsilon_f$ are the n-th column vector and f-th row vector of matrix X, respectively. Based on this, we define the time-frequency in-direction difference operator $\\Delta_{TFIDD}$ and the time-frequency reverse-direction difference operator $\\Delta_{TFRDD}$ as follows.\n$\\begin{aligned}\n\\Delta_{TFIDD}X = X - \\mathcal{O}_{CL}\\mathcal{O}_{RU}X,\\\\\n\\Delta_{TFRDD}X = X - \\mathcal{O}_{CR}\\mathcal{O}_{RU}X,\n\\end{aligned}$ (10)\n(11)\nOur proposed TFID loss $L_{TFID}(P_{refine}, P)$ is defined between the refinement phase spectrum $P_{refine}$ and natural one $P$ and includes both time-frequency in-direction difference loss $L_{TFIDD}(P_{refine}, P)$ and time-frequency reverse-direction difference loss $L_{TFRDD}(P_{refine}, P)$, i.e.,\n$L_{TFID}(P_{refine}, P) = L_{TFIDD}(P_{refine}, P) + L_{TFRDD}(P_{refine}, P),$ (12)\nwhere\n$\\begin{aligned}\nL_{TFIDD} = \\mathbb{E}_{(P_{refine},P)} \\left[ \\left| \\left| f_{AW} \\left( \\Delta_{TFIDD}P_{refine} - \\Delta_{TFIDD}P \\right) \\right| \\right|_{1} \\right],\\\\\nL_{TFRDD} = \\mathbb{E}_{(P_{refine},P)} \\left[ \\left| \\left| f_{AW} \\left( \\Delta_{TFRDD}P_{refine} - \\Delta_{TFRDD}P \\right) \\right| \\right|_{1} \\right]\n\\end{aligned}$ (13)\n(14)\nTherefore, in the refinement stage, we alternately train the refinement model and the PSD using losses $L(P_{refine}, P) +L_{TFID}(P_{refine}, P)$ and $L_{adv\u2212D}(P_{refine}, P)$, respectively.\n2.4. Optional Iterative Prediction Mode"}, {"title": "2.4. Optional Iterative Prediction Mode", "content": "In our proposed SP-NSPP, the predicted phase from the first stage is utilized as the prior phase input for the second stage, resulting in a more refined phase prediction. This approach is similar to traditional iterative algorithms. Therefore, our proposed SP-NSPP can also adopt an iterative prediction mode. In SP-NSPP, the refinement stage can be regarded as performing one iteration based on the prior construction stage. Assume $P'_{refine} = P_{prior}, P''_{refine} = P_{refine}$ and $Model = Model_r$. Then, by introducing more identical refinement models $Model_i (i = 2, 3, ...)$, the iterative prediction mode can be executed as follows.\n$P_{refine}^i = Model_r(A, P_{refine}^{i-1}), i = 1, 2, 3, \u2026\u2026\u2026$ (15)\nHowever, as the number of iterations increases, the overall model size grows linearly. Therefore, phase prediction accuracy and model complexity should be balanced. The relevant experimental analysis is shown in Section 3.8."}, {"title": "3. EXPERIMENTS AND RESULTS", "content": "3.1. Data and Feature Configuration\nIn the experiments, we followed [20] to use a subset of the VCTK corpus [27] consisting of 11,572 speech utterances from 28 speakers. The original 48 kHz sampled recordings in the VCTK corpus were downsampled to 16 kHz to ensure a fair comparison with other baseline iterative estimation algorithms and prediction models. The dataset was randomly constructed into a training set (11,012 utterances) and a validation set (560 utterances). We then selected a total of 824 speech utterances from one male unseen speaker and one female unseen speaker as the test set. When extracting the amplitude and phase spectrum from the natural waveform, we set the window size to 20 ms, the window shift to 5 ms, and the FFT point number to 1024 (i.e., $N$ = 513).\n3.2. Task Definitions\nWe defined two tasks to compare the performance of different phase estimation or prediction methods.\n\u2022 Analysis-Synthesis Task: In this task, the phase spectrum is predicted from the natural amplitude spectrum extracted from the natural waveform by STFT. This task focuses on evaluating phase recovery and reconstruction capabilities.\n\u2022 Prediction-Synthesis Task: In this task, the phase spectrum is predicted from the non-natural amplitude spectrum. This non-natural amplitude spectrum is predicted by other models, making it more representative of real-world applications. For example, in speech bandwidth extension (BWE), we introduced an amplitude extension model inspired by [21]. This model first predicts the high-frequency amplitude spectrum from the low-frequency one extracted from bandwidth-limited speech, and then concatenates them to construct a full-band amplitude spectrum. Finally, the corresponding phase spectrum is recovered by phase prediction methods, and the extended speech waveform is reconstructed through ISTFT. This task focuses on evaluating the robustness and generalization of the phase prediction methods.\n3.3. Model Details\nThe descriptions of phase estimation algorithms and prediction methods for comparison are as follows\u00b9.\n\u2022 GLA: The iterative phase estimation algorithm GLA [12] with 100 iterations.\n\u2022 RAAR: The iterative phase estimation algorithm RAAR [15] with 100 iterations.\n\u2022 VMDNN: The von Mises distribution-based DNN phase prediction method [18, 19]. We reproduced the DNN model and used it to predict the initial phase spectrum from the amplitude spectrum, then refined it by GLA with 100 iterations.\n\u2022 NSPP: The neural speech phase prediction model NSPP [20, 21] which predicted the phase spectrum from the amplitude spectrum without the phase prior. We reimplemented it using the official open source code\u00b2.\n\u2022 SP-NSPP: The proposed stage-wise and prior-aware neural speech phase prediction model. Here, the prior construction model and refinement model shared the same configuration. Each model included eight ConvNeXt v2 blocks. All ID convolutions"}, {"title": "3.4. Evaluation Metrics", "content": "kernel size of 7. Except for PEA, the channel size of the 1D convolutions in other parts was uniformly set to 256. The channel size of the 1D convolutions in PEA was 513 (i.e., equal to N). The number of nodes in the first linear layer of each ConvNeXt v2 block was 512, while the number of nodes in the linear layers in other parts was 256. For PSD, the first five 2D convolutional layers all had 64 channels, with kernel sizes of 7\u00d75, 5\u00d73, 5\u00d73, 3\u00d73 and 3\u00d73, respectively. The 2D convolutional layer for the final output of the discriminative value had one channel and a kernel size of 3. The hyperparameters of the loss function were set as $\u03bbp$ = 100 and $\u03bbPSD$ = 0.1. Each model was trained using the AdamW optimizer with $\u03b2$ = 0.8 on a single Nvidia 2080Ti GPU. The initial learning rate was set to 0.0002 for each epoch, with a learning rate decay factor of 0.999. The models were trained for a total of 3100 epochs, with a batch size of 16. The waveform length was truncated to 8000 samples for each training step.\n3.4. Evaluation Metrics\nWe comprehensively evaluated and compared the phase prediction methods in terms of phase accuracy, speech quality, and efficiency.\n\u2022 Phase accuracy evaluations: To evaluate the phase spectrum prediction accuracy, we proposed a series of phase distortion (PD) metrics. The PD metrics first evaluate the phase error using the anti-wrapping function $f_{aw}$ and then calculate the distortion in a manner similar to log-spectral distance (LSD), i.e.,\n$\\begin{aligned}\nPD^* = \\frac{1}{N} \\sum_{n=1}^{N} \\sqrt{\\frac{1}{F} \\sum_{f=1}^{F} f_{aw} \\left(A^P_{fn} - A^\\hat{P}_{fn}\\right)^2},\\\\\n\\end{aligned}$ (16)\nwhere $P \\in \\mathbb{R}^{F \\times N}$ and $ \\hat{P} \\in \\mathbb{R}^{F \\times N}$ respectively represent the predicted and natural phase spectra. * can be replaced with IP, GD, IAF, TFIDD, and TFRDD, where AIP denotes no operation, and AGD and AIAF represent frequency difference and time difference operations, respectively. Since both PDTFIDD and PDTFRDD calculate the phase differential distortion along the time and frequency axes simultaneously, we compute their average as PDTFID.\n\u2022 Speech quality evaluations: To evaluate the quality of the speech reconstructed from the amplitude spectrum and the predicted phase spectrum, we used several common objective tools, including signal-to-noise ratio (SNR) and perceptual evaluation of speech quality (PESQ) [28]. We also evaluated the F0 distortion by calculating the root MSE between FOs extracted from reconstructed and natural speeches (denoted by F0-RMSE). In terms of subjective evaluation, we employed the mean opinion score (MOS) test to assess the naturalness of the reconstructed speech on the Amazon Mechanical Turk\u00b3. At least thirty native English-speaking listeners rate twenty reconstructed speech samples and natural speech samples for each method. The scoring range was from 1 to 5 with a 0.5 interval.\n\u2022 Efficiency evaluations: In order to assess the generation efficiency of different methods, the real-time factor (RTF) was adopted. Additionally, we also measured the size of NSPP-based models to evaluate their complexity.\n3.5. Primary Experimental Results"}, {"title": "3.6. Ablation Studies", "content": "Natural\nFrequency(kHz)\n4\nSP-NSPP w/o PSD\nSP-NSPP\n4\n4\nFirst, we compared the proposed SP-NSPP with other baselines for both the analysis-synthesis task and prediction-synthesis task.\nTable 1 shows the results of the analysis-synthesis task. Regarding\n1\nTime(s)\n0\n1\n2\n0\n1\n2\nTime(s)\nTime(s)\nFig. 4. A comparison among the spectrograms (0~4 kHz) of the natural speech and speeches generated by SP-NSPP and SP-NSPP w/o PSD for the analysis-synthesis task.\nWe can see\nalong withthe\nPTS\nwith those\nPESQ"}, {"title": "3.7. Validation of Generalization under Other Data Conditions", "content": "PESQ\n4.36\nModel Size\nIteration Number\nIteratton Number\n4.34\n4.32\n100\n80\n60\n-PESO\n40\nModal 5ae\n4.30\n4.28\n20\n4.26\n4.24\n0\nFig. 5. Curves of PESQ and model size of the SP-NSPP as a function of the number of iterations for the analysis-synthesis task.\nas shown in Figure 5. We can see that as the number of iterations increased, the PESQ also increased, but the growth rate gradually slowed down. When the number of iterations increased from 1 to 2, the PESQ rose by less than 0.02, and with further increases in the number of iterations, the PESQ showed almost no significant growth. However, the model size increased linearly with the number of iterations. This indicates that PESQ and model size should be balanced. An iteration number of 1 (i.e., SP-NSPP), is a good choice as it provides a high PESQ value with moderate model complexity.\n4. CONCLUSION\nThis paper presents a novel stage-wise and prior-aware neural speech phase prediction model, named SP-NSPP. The prior construction stage generates a prior phase spectrum from the amplitude spectrum, which serves as the conditional input for the subsequent refinement stage. With the foundation of the prior phase, the refinement stage can predict a more accurate phase spectrum from the amplitude spectrum. To further optimize the phase, we introduce PSD for phase adversarial training and propose the TFID loss which reflects the time-frequency continuity. Experimental results demonstrate that our proposed SP-NSPP outperforms traditional iterative estimation algorithms and other neural prediction methods in terms of phase accuracy, speech quality and efficiency for both analysis-synthesis tasks and prediction-synthesis tasks. Applying the proposed SP-NSPP to concrete speech generation tasks deeply will be the focus of our future work."}]}