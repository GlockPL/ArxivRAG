{"title": "Weak-to-Strong Reasoning", "authors": ["Yuqing Yang", "Yan Ma", "Pengfei Liu"], "abstract": "When large language models (LLMs) exceed human-level capabilities, it becomes increasingly challenging to provide full-scale and accurate supervisions for these models. Weak-to-strong learning, which leverages a less capable model to unlock the latent abilities of a stronger model, proves valuable in this context. Yet, the efficacy of this approach for complex reasoning tasks is still untested. Furthermore, tackling reasoning tasks under the weak-to-strong setting currently lacks efficient methods to avoid blindly imitating the weak supervisor including its errors. In this paper, we introduce a progressive learning framework that enables the strong model to autonomously refine its training data, without requiring input from either a more advanced model or human-annotated data. This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself. Extensive experiments on the GSM8K and MATH datasets demonstrate that our method significantly enhances the reasoning capabilities of Llama2-70b using three separate weak models. This method is further validated in a forward-looking experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b on the highly challenging OlympicArena dataset. This work paves the way for a more scalable and sophisticated strategy to enhance AI reasoning powers. All relevant code and resources are available in https://github.com/GAIR-NLP/weak-to-strong-reasoning.", "sections": [{"title": "Introduction", "content": "As the pursuit of Artificial General Intelligence (AGI) advances, the creation of superintelligent systems-models that exceed human cognitive capabilities-remains a key ambition within the field. This quest introduces a host of challenges, especially concerning the supervision and learning paradigms for these advanced AI models. Conventional supervision methods, which typically depend on human oversight or guidance (i.e., distilled knowledge) from more advanced models, become inadequate as the capabilities of AI exceed those of their supervisors. To address this issue, we focus on the weak-to-strong learning paradigm, which operates under a unique task setting where only a less capable model and a stronger but not fully utilized model are available.\nThe central question of weak-to-strong learning is whether models with limited capabilities can effectively guide the development of more advanced, stronger models. Previous studies by have demonstrated the feasibility of it in classification, chess, and reward modeling tasks. However, the applicability of this setup to more complex reasoning tasks, which demand more than mere extrapolation or pattern recognition, remains an open question. Complex reasoning represents a key aspect of human cognition, crucial for assessing whether LLMs can emulate or surpass human-like capabilities in comprehending the world, making decisions, and solving problems. Given the complexity and the critical nature of these tasks, applying the weak-to-strong learning framework to advanced reasoning challenges is essential, particularly within the broader context of achieving superintelligence.\nAlthough suggest that naively fine-tuning strong models on the full set of noisy data produced by weak models, named full weak fine-tuning, can consistently improve their performance over the weaker counterparts, this approach is still far from recovering the full capabilities of strong models, and our experiments show that it loses effectiveness when facing more complex reasoning challenges. They also propose an auxiliary confidence loss to mitigate the issue of strong models imitating the errors of their supervisors. However, this method is tailored to classification tasks with a set of fixed labels and does not naturally extend to open-ended generation tasks including reasoning. Currently, there is a lack of effective methods beyond naive fine-tuning to prevent the overfit of weak errors and to further elicit the intrinsic reasoning abilities of strong models within the weak-to-strong reasoning framework.\nTo achieve the above goal, we introduce a progressive refinement learning framework, guided by the principle that a model can enhance its capabilities more effectively by initially focusing on smaller, more reliable subsets of data, and then iteratively expanding its learning scope, as illustrated in Fig. 2. In the first stage, we hypothesize that it is more advantageous to utilize smaller quantities of data that are likely to be more accurate. We achieve this by combining weak data, generated by the less capable model, with data self-generated by the more advanced model through in-context learning. This blend is then used to selectively curate datasets for subsequent supervised fine-tuning. In the second stage, upon having developed a strong model with improved reasoning capabilities, we utilize its ability to construct contrastive samples for preference optimization and enables the model to learn effectively from the errors of the weaker model.\nIn implementation, we employ Llama2-70b as the strong model, test three separate weak models: Llama2-7b, Gemma-2b, and Mistral-7b, and conduct experiments"}, {"title": "Preliminaries", "content": "We outline common learning paradigms in large model training, primarily characterized by the need for ground truth answers and supervision from stronger models as shown in Tab. 1.\nWhen training LLMs, it is ideal to have a sufficient amount of training data with ground truth answers, which we refer to as generic-supervised learning paradigm. However, acquiring such data is often label-intensive and can sometimes be impossible. As a result, various learning paradigms have emerged to reduce the effects of data quality and quantity while still improving performance.\nIn the current context, to enhance a strong model like Llama2-70b, improvements can still be made by seeking help to a stronger model like GPT-4, even without ground truth. Hence, many existing works suggest that a stronger model acts as a teacher model to provide specific feedback to improve the targeted model. This paradigm can be viewed as distilling the stronger teacher model's knowledge. Nonetheless, merely imitating the teacher model is not a long-term solution; imitation models only slightly close the performance gap to the teacher model on tasks not well-represented in the imitation data. Furthermore, distillation learning primarily benefits models that are less capable than the teacher model.\nConsidering the high costs of annotating training data by humans or stronger proprietary models, a line of works relies on the correct responses generated by the model itself to update it. It is evident that, whether using binary labels or fine-grained feedback, this paradigm still requires ground truth to assess the usability of the model's self-generated responses. Without ground truth answers, self-improvement leads to minimal performance gains and may even degrade performance.\nGaining insights from semi-supervised learning within the domain of traditional machine learning, another type of LLM learning depends not on extensive labeling but instead on a small, high-quality seed dataset. have demonstrated improvement by learning differences between self-generated responses and expert-annotated responses. We also include the trending research topic of easy-to-hard generalization in this category, where models are trained to tackle complex tasks by learning from human annotations on easier tasks. This series of research inevitably require access to a small yet high quality set of standard answers."}, {"title": "Methodology", "content": "In this section, we propose a weak-to-strong training method designed to maximize the use of weak data and to elicit the strong model's innate talent. First, we identify potentially positive samples in the absence of ground truth and external signals. During Stage I, we exclusively utilize this subset of data for supervised fine-tuning. Then once the strong model has achieved a certain level of reasoning proficiency, we employ the full weak data, particularly the potentially negative samples in Stage II via preference learning-based approaches like DPO, encouraging the strong model to learn from mistakes made by the weaker model.\nGiven a weak model $m$ and a series of math problems $Q$ without ground truth, $m$ generates weak data $D_{weak} = \\{q_i, C_{weak, i}, A_{weak, i}\\}$, where $q_i \\in Q$, $C_{weak, i}$ represents a reasoning chain, and $A_{weak, i}$ represents the final answer. The correctness of $A_{weak, i}$ is unknown. The central challenge is: how can we maximize the use of $m$ and $D_{weak}$ to fully enhance and recover the mathematical reasoning capabilities of a stronger model $M$?\nOur initial strategy is to fine-tune the stronger model $M$ across the entirety of the weak dataset $D_{weak}$. While prior research has validated the effectiveness of this approach in text classification tasks, its efficacy in reasoning tasks remains unexplored. We have therefore embarked on an investigation to determine whether the phenomenon of weak-to-strong generalization can also enhance the reasoning capabilities of $M$ in this less examined domain.\nAnother straightforward approach is in-context learning (ICL, which requires only several training samples as demonstrations in the prompt. Specifically, we randomly select four samples from $D_{weak}$ as demonstrations. Since we do not have access to the ground truth, these demonstrations cannot be provably correct."}, {"title": "Weak-ICL Fine-Tuning", "content": "Given that models can mimic weak errors through supervised fine-tuning, we propose refining $D_{weak}$ before use, instead of using all data blindly. Additionally, we seek to harness the innate abilities of the strong model activated via in-context learning. Building on these two ideas, we introduce weak-icl fine-tuning, employing both weak data $D_{weak}$ and \u201cicl data\u201d $D_{icl} = \\{q_i, C_{icl,i}, A_{icl,i}\\}$, where $q_i \\in Q$, $C_{icl,i}$ and $A_{icl, i}$ are generated by $M$ with few-shot demonstrations, as higher-quality supervision signals.\nNote that, for both $D_{weak}$ and $D_{icl}$, we cannot determine whether a certain answer is correct or not. Nonetheless, when two models, employing distinct data representations, converge on the same answer in an open-ended task, it is indicative of a higher likelihood of accuracy. This phenomenon supports the reliability of the results when consistency is observed across different methodologies. We thus compare $D_{weak}$ and $D_{icl}$ generated by the weak model and strong model, respectively, and select $D_{weak}$ and $D_{icl}$ if $a_{weak,i} = a_{icl,i}$, for subsequent supervised fine-tuning. We call this approach final answer consistency. Considering the combination of the two sets of data, we can obtain three versions of enhanced fine-tuned strong models:\nIterative Training Upon closed examination of $M_{weak-ft}$ and $M_{icl-ft}$, we see that they still satisfy the condition of having different data representations, as they are trained on data from different sources\u2014$D_{weak}$ is generated by the weak model, whereas $D_{icl}$ primarily originates from the strong model itself. Hence, we can perform iterative training to bootstrap performance. We denote the initial round of supervised fine-tuning data as $D_{weak}$ and $D_{icl}$, resulting in models $M_{weak-ft}$, $M_{icl-ft}$, and $M_{hybrid-ft}$. In the second iteration, we obtain zero-shot solutions from $M_{weak-ft}$ applied to $Q$ to construct $D_{weak}'$ and those from $M_{icl-ft}$ to construct $D_{icl}'$. Here, the subscripts \u201cweak\u201d and \u201cicl\u201d indicate the initial data source. Then we apply final answer consistency to obtain $D_{weak}^*$ and $D_{icl}^*$. Following another round of supervised fine-tuning, we have:"}, {"title": "Stage II: Learn from \u201cNegative\u201d Samples", "content": "We denote the final iteration of $M_{hybrid-ft}$ from Stage I as $M_{plus}$, which has learned dual mathematical solutions and holds potential for further enhancement. Next, we apply preference optimization techniques to strategically utilize the potentially erroneous subset of the original weak dataset $D_{weak} = \\{q_i, C_{weak,i}, A_{weak, i}\\}$ generated by $m$, which allows the strong model to identify and avoid similar errors in future reasoning processes. The key factor lies in how to construct contrastive samples for learning.\nWithout access to ground truth, the current strong model with enhanced reasoning capabilities identifies the most likely correct answers based on its confidence. Specifically, for each question $q_i \\in Q$, we sample $n$ responses from $M_{plus}$, and define the probability of the answer that appears most frequently among these responses as confidence. When the confidence falls below a specified threshold $\\tau$, we consider the model's judgment on this question unreliable and therefore discard it. Conversely, if the confidence is no less than $\\tau$, we regard the model as capable of solving the question and proceed to construct contrastive samples as follows:\nFurther training $M_{plus}$ on these samples enables it to distinguish between correct and incorrect solutions, leading to a stronger model $M_{pro}."}, {"title": "Experiments", "content": "GSM8K and MATH are two widely used datasets for mathematical reasoning, and MATH comprises more challenging competition problems. The data statistics we use are presented in Tab. 4. Particularly, to ensure a sufficient amount of training data for developing preliminary mathematical skills in the weak model, we augment the GSM8K training set with the data constructed by Further details are available in \u00a7A.1."}, {"title": "Experiment Settings", "content": "We use Llama2-70b as the strong model and employ three weak models from different families: Llama2-7b, Gemma-2b, and Mistral-7b. We apply full parameter fine-tuning to the weak models on $D_{gold, 1}$, and consistently adopt LORA to fine-tune the strong model. In Stage I, we perform two rounds of iterations on GSM8K and one round on MATH according to the principles of iteration outlined in \u00a73.1. In Stage II, we adopt two preference learning-based approaches, DPO and its variant ORPO. Details are provided in \u00a7A.2.\nWe evaluate the accuracy on the test set. The performance of the weak model $m$ is defined as the \u201cweak floor\u201d. The performance of the strong model $M$, fine-tuned with data containing gold solutions from $D_{gold,2}$, is termed the \"strong ceiling\u201d. It represents the upper limit of the capabilities that the strong model can achieve with $D_{gold, 2}."}, {"title": "Results of Stage I", "content": "The main results of Stage I on both GSM8K and MATH datasets are depicted in Fig. 4. Notably, in the MATH experiments, we randomly sample additional data that is not chosen based on the final answer consistency, due to the small amount available. Please refer to \u00a7A.4.1 for details. According to Fig. 4, we have the following observations.\nUsing our proposed method, the performance of the strong model, supervised only by the weak Gemma-2b with 25.17 accuracy on GSM8K (without any gold answers), can be improved up to 60.12, surpassing naive full weak fine-tuning by 31.08, and $M_{plus}$ (i.e., $M_{hybrid-ft}$) outperforms it by 26.99. This verifies the effectiveness of data refining before supervised fine-tuning. Also, experimental results show that the mathematical reasoning capabilities of the strong model are increasingly recovered as the weak model improves, a conclusion verified by on classification tasks. In detail, the performance on GSM8K gradually improves for Gemma-2b, Llama-7b, and Mistral-7b (25.17 \u2192\n33.81 \u2192 59.51). Hence, the maximum performance of the strong model, trained with data generated by these models, also progressively enhances (60.12 \u2192 63.76 \u2192 68.39).\nAs expected, $M_{hybrid-ft}$ achieves the highest pass@k scores in the weak-to-strong setting, benefiting from its training data that incorporates two types of solutions\u2014one from the weak model, and another from the strong model. This diversity enhances the robustness of the model by reducing the likelihood of overfitting. Additionally, the performance of $M_{icl-ft}$ generally surpasses that of $M_{weak-ft}$, which can be attributed to variations in process-level accuracy and possibly the solution format. Detailed analyses are conducted in \u00a7A.3."}, {"title": "Naive fine-tuning is inadequate for weak-to-strong reasoning.", "content": "When using Gemma-2b as the weak model on the MATH dataset, full weak fine-tuning underperforms compared to the weak floor (10.0 v.s. 11.6). This indicates that naive fine-tuning, though successfully applied to classification, chess, and reward modeling tasks, falls short for intricate reasoning tasks, particularly those of substantial difficulty like questions in MATH. In contrast, our weak-icl fine-tuning method effectively bridges the gap, offering an effective and scalable solution for the weak-to-strong reasoning challenge."}, {"title": "Effect of ICL Performance", "content": "Given that the efficacy of weak-icl fine-tuning partially depends on the ef fectiveness of weak ICL, we further explore how en hancing ICL performance through careful selection of demonstrations affects the performance of weak-icl fine-tuning. Fig. 5 shows the test accuracy on GSM8K using Gemma-2b as the weak model under a different set of demonstrations.\nThe results indicate that the performance of weak ICL with this particular group of demonstrations in creases from the original 56.48 to 64.06. We then re generate $D_{icl}$ with these demonstrations in the prompt and fine-tune the strong model on $D_{icl}$, which is selec tively curated through final answer consistency. This further improves performance from 64.06 to 64.75, confirming the utility of self-directed data curation. It is worth noting that although weak ICL holds the po tential for high performance, the selection of effective demonstrations in a weak-to-strong framework is a non-trivial thing, and is beyond the scope of this paper."}, {"title": "Results of Stage II", "content": "As discussed in \u00a73.2, we employ the final iteration of $M_{hybrid-ft}$ as $M_{plus}$ for subsequent preference learn ing. The experimental results in \u00a74.3 validate this checkpoint achieves higher pass@k and possesses significant potential for further refinement.\nAs shown in Tab. 5, our method for construct ing positive and negative samples effectively en hances the strong model\u2019s math reasoning capabil ities. On GSM8K, both DPO and ORPO consis tently achieve significant improvements using our con structed datasets, notably resulting in an increase of 8.49 points when supervised by Gemma-2b. Despite the inherently challenging nature of MATH problem, which compromises the strong model\u2019s judgment and introduces inaccuracies in the training data, our method still achieves improvements on MATH through ORPO by at least 1 point."}, {"title": "Data Construction Recipe", "content": "When constructing preference data, we always use weak responses generated by the weak model as one of the chosen/rejected responses, instead of relying exclusively on self-generated data. We also test the self-generated setting on GSM8K using Llama2-7b as the weak model, where both chosen and rejected responses are generated by the strong model itself. The DPO test accuracy in this setting is 62.40 (-0.22), indicating a slight performance degradation. Without ground truth, the constructed positive and negative samples actually correspond to the more frequently and less frequently occurring answers, respectively, and are related to the answers the model tends to choose. Since preference optimization essentially performs ranking, the potential benefit of this self-generated setting is minimal. Therefore, incorporating weak data signals in the preference data construction process proves to be a better approach."}, {"title": "Analysis", "content": "For further analysis, we examine the accuracy across different difficulty levels in the MATH test set (See \u00a7A.1.2 for data statistics)."}, {"title": "Experiments Closer to Future Scenarios", "content": "In preliminary tests with Llama3-70b, we observe that on GSM8K and MATH, Llama3-70b can largely unlock its potential through in-context learning, with marginal or even adverse impacts from parameter updates due to training instabilities. Consequently, we focus on a more challenging dataset developed after the release of Llama3-70b, OlympicArena , to simulate a more realistic future scenario.\nWe only consider English questions in OlympicArena, excluding the CODE (Code Generation) and OT (Others) problem types that require case-based or expert evaluation. This results in 6,020 training data without solutions and final answers, and 313 test data with final answers to assess the performance of different methods. We use Llama3-8b-instruct (without initial fine-tuning on a subset of training data) as the weak model and Llama3-70b as the strong model to be improved. The hyperparameters are consistent with those used for GSM8K. This configuration more closely resembles future real-world weak-to-strong scenarios.\n$M_{weak-ft}^1$, obtained by our proposed weak-icl fine-tuning method, achieves higher performance than Full Weak FT with fewer training data (i.e., 746), outperforming it by 0.32 points. After the second stage of preference optimization, which further exploits the weak model and training questions without answers, the strong model's performance is improved by an additional 3.19 points over Full Weak FT. This demonstrates the robustness and generalizability of our method in scenarios closer to future conditions."}, {"title": "Related Work", "content": "LLMs can enhance their ability to solve tasks and better align with human instructions through a supervised fine-tuning (SFT) phase This phase heavily relies on the quality of training data, as previous studies demonstrate that higher data quality translates to substantial gains in model performance. In this paper, we investigate the potential of learning from weak supervisions."}, {"title": "Mathematical Reasoning", "content": "The exploration of mathematical reasoning within LLMs has been a focal point for evaluating their cognitive capabilities akin to human reasoning Researchers have developed various methods to enhance the mathematical reasoning capabilities of LLMs after pre-training, which can be broadly classified into two categories: (1) Prompting: Some works aims to elicit the intrinsic reasoning abilities of LLMs by specific prompting engineering, without updating the model parameters; (2) Fine-tuning: Another line of studies focuses on generating a more extensive and higher-quality collection of question-answer pairs. Through supervised fine-tuning and preference optimization the models can achieve significant improvements in their mathematical problem-solving capabilities."}, {"title": "Conclusion", "content": "In this paper, we explore the efficacy of weak-to-strong framework in complex reasoning tasks. We introduce a new method that elicits strong capabilities using weak supervisions, without relying on annotations from humans or more advanced models. This method focuses on the strong model's ability to autonomously refine its training data, even if it has not learned the task before. By iteratively expanding its learning scope, the strong model continuously broadens its reasoning skills. This self-directed data curation is crucial for scaling up the enhancement of AI reasoning capabilities, making the model more independent and effective in its developmental trajectory. Through this work, we seek to illuminate new pathways for AI development, emphasizing the critical role of innovative model supervision in advancing AGI and beyond."}, {"title": "Limitations", "content": "In our experiments, we use Llama2-70b and Llama3-70b as a proxy for hypothetical superintelligent models of the future. We acknowledge that there might be performance discrepancies compared to a genuine future advanced model. Nonetheless, our efforts lay the groundwork for investigating methodologies in weak-to-strong reasoning. Additionally, this paper does not explore supervision at the process level, such as the model's ability to learn from partially correct data In the weak-to-strong scenario, the presence of non-negligible errors and noise at the process level yields only limited performance improvements in our early experiments, thereby posing challenges for future research."}, {"title": "Appendix", "content": "For GSM8K, we evenly divide the original training dataset of 7,473 samples into two subsets, $D_{gold, 1}$ and $D_{gold,2}$.\nAdditionally, we supplement both $D_{gold, 1}$ and $D_{gold,2}$ with the data of the same distribution developed by until each contains 7,000 samples. Thus, the weak model uses $D_{gold,1}$, which includes both questions and gold solutions, to obtain basic problem-solving capabilities. Meanwhile, the strong model can only access a training dataset $Q = \\{q_i\\}$, where $q_i \\in D_{gold,2}$, consisting of 7,000 mathematical problems without ground truth answers. GSM8K also includes 1,319 test samples.\nFor MATH, we employ the same subset of 500 representative problems as the test set, identical to that used in We then split the remaining 12,000 samples evenly between $D_{gold,1}$ and $D_{gold,2}$, each containing 6,000 samples."}, {"title": "Statistics of MATH test set", "content": "The distribution of difficulty levels across the 500 test data samples in MATH is listed in Tab. 7."}, {"title": "Training Details", "content": "For supervised fine-tuning in Stage I, we adopt LoRA to fine-tune the strong model M with a learning rate of 1 \u00d7 10-4 and search for weight decay in the set {0, 0.01}. We run 2 epochs on GSM8K and 3 epochs on MATH, with a batch size of 8. In Stage II, we employ two preference optimization methods. For DPO, we train the enhanced strong model $M_{plus}$ with a learning rate of 1 \u00d7 10-5 and run 1 epoch. For ORPO, we search for \u03b2 in the set {0.1, 0.5, 1.0} with a learning rate of 3 \u00d7 10-5 and run 1 epoch. All experiments are conducted using A100 GPUs.\nWhen constructing contrastive samples in Stage II, we sample n = 10 responses at temperature = 1.0, and use a confidence threshold of T = 0.6. Normally, we evaluate using greedy decoding. For calculating pass@k, we set k = 10 at temperature = 1.0."}, {"title": "Additional Analysis", "content": "To investigate why $M_{hybrid-ft}$ achieves high pass@k scores despite lower greedy decoding results, we explore the diversity of responses generated by $M_{hybrid-ft}$ and $M_{icl-ft}$. We specifically examine the frequency distribution of the number of distinct solutions for each question across the two strong model checkpoints.\nGiven a question from $D_{gold,2}$, we sample n = 10 responses at temperature = 1.0 for each checkpoint. We consider two responses distinct if their ROUGE-L similarity is less than 0.7. We then compute the number of clusters formed by these distinct responses and plot their frequency distribution in Fig. 7.\nAs shown in Fig. 7, $M_{icl-ft}$ tends to produce nearly the same sampled responses for each question in more than 36% of the instances. This indicates a limited exploration of problem-solving paths and difficulty in generating diverse, correct solutions during the sampling process. In contrast, $M_{hybrid-ft}$ generates a variety of responses, increasing its hit rate with multiple sampling and thus achieving higher pass@k scores. Additionally, diverse solutions are crucial for robust outcomes and model generalization in Stage II,"}, {"title": "Training Accuracy of Stage I", "content": "Tab. 8 presents the final answer accuracy and process- level accuracy for both weak data and icl data utilized in the initial round. To compute process-level accuracy, we randomly sample a maximum of 1,000 training sample from each of weak data and icl data, and evaluate them using GPT-4 following , the prompt we use is illustrated in Tab. 13. Accuracy at this level is determined strictly on the basis that there are no errors throughout the intermediate reasoning steps.\nFrom the results we can see that despite having consistent final answer accuracy (with the exceptions of Gemma-2b and Mistral-7b on MATH using augmented training data), there are noticeable differences in process-level performance, leading to variations in the effectiveness of $M_{weak-ft}$ and $M_{icl-ft}$. Moreover, it is counterintuitive that models trained on icl data with relatively low process-level accuracy achieve higher performance. This might be because the models pre- fer self-generated solutions and can more effectively learn those that better align with their inherent distri- bution (Panickssery et al., 2024; Ren et al., 2024; Fan et al., 2024)."}, {"title": "Additional Experiments", "content": ""}, {"title": "Details of Stage I on MATH", "content": "In the Stage I experiment conducted on the MATH dataset, it is found that the amount of training data selected via final answer consistency is so limited that the strong model can hardly learn the effective features through supervised fine-tuning. To address this, we randomly sample additional inconsistent data. Based on the weak model's performance (Llama-7b < Gemma-2b < Mistral-7b on MATH), we supplement the data (both $D_{weak}$ and $D_{icl}$) to 1,000 instances for Gemma-2b and 2,000 instances for Mistral-7b, and present the results in Fig. 4. The original amount of training data and test accuracy for these two weak models are shown in Tab. 10."}, {"title": "Pass@k Results", "content": "Tab. 9 summarizes the greedy decoding and pass@k results for the three variants of enhanced strong models obtained through weak-icl fine-tuning. Notably, $M_{hybrid-ft}$ utilizes a training set that combines those used by $M_{weak-ft}$ and $M_{icl-ft}$. The results indicate that $M_{hybrid-ft}$ outperforms its counterparts in terms of pass@k, achieving superior pass@k scores with margins of up to 5.23 points. The only exception occurs in the MATH dataset supervised by Llama2-7b, where the underperformance is likely due to limited training data.\nThe superior performance of $M_{hybrid-ft}$ can be attributed to the diversity of solutions in its training set (verified in \u00a7A.3.1), validating our approach of adopting the final iteration of $M_{hybrid-ft}$ from Stage I for preference optimization in Stage II. It is important to note that while higher pass@k scores suggest greater potential, the true challenge lies in effectively harnessing this potential, particularly in the weak-to-strong setting where no ground truths are available. Our proposed weak-to-strong preference optimization in Stage II successfully addresses this challenge, transforming theoretical potential into tangible performance gains in greedy decoding, as proved in \u00a74.4."}, {"title": "PGR of Stage I", "content": "propose a new metric called performance gap recovered (PGR) to measure the fraction of the performance gap that can be recovered through weak supervision, as illustrated in Eq. 1. Tab. 11 displays the results of the naive full weak fine-tuning (i.e., Full Weak FT) and our best weak-icl fine-tuning (i.e., Weak-ICL FT) in terms of PGR, which also demonstrate that our method can outperform the simple competitor. However, the variations in PGR across different weak models do not provide meaningful insights. In the experiments described in the main text, we use test accuracy instead to provide a more detailed depiction of model performance.\n$PGR = \\frac{weak-to-strong - weak\\ floor}{strong \\ ceiling - weak\\ floor}$"}, {"title": "Effect of SFT Data", "content": "Tab. 12 presents more detailed comparative experi- mental results of Stage I on GSM8K. \"Full Weak\" de- notes full weak fine-tuning, \u201cOur Weak\u201d is equivalent to $M_{weak-ft}$, and \u201cOur ICL\u201d is equivalent to $M_{iel-ft}$.\n\"Gold Weak\" refers to the scenario where weak data with correct final answers are filtered and used for supervised fine-tuning, which is impossible in the weak-to-strong setting and just used for experimental analysis. Similarly, \u201cGold ICL\u201d refers to the scenario where solutions with correct final answers, generated by the strong model via weak ICL, are filtered.\nCompared to using a large volume of noisy data (i.e., Full Weak and Full ICL), reducing the data quan- tity while enhancing data quality can significantly improve the accuracy of the trained model, with po- tential gains over 17 points. Although our method performs slightly lower than the gold results, it proves highly effective and stable in scenarios where obtain- ing the ground truth is impossible."}]}