{"title": "Weak-to-Strong Reasoning", "authors": ["Yuqing Yang", "Yan Ma", "Pengfei Liu"], "abstract": "When large language models (LLMs) exceed human-level capabilities, it becomes increasingly challenging to provide full-scale and accurate supervisions for these models. Weak-to-strong learning, which leverages a less capable model to unlock the latent abilities of a stronger model, proves valuable in this context. Yet, the efficacy of this approach for complex reasoning tasks is still untested. Furthermore, tackling reasoning tasks under the weak-to-strong setting currently lacks efficient methods to avoid blindly imitating the weak supervisor including its errors. In this paper, we introduce a progressive learning framework that enables the strong model to autonomously refine its training data, without requiring input from either a more advanced model or human-annotated data. This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself. Extensive experiments on the GSM8K and MATH datasets demonstrate that our method significantly enhances the reasoning capabilities of Llama2-70b using three separate weak models. This method is further validated in a forward-looking experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b on the highly challenging OlympicArena dataset. This work paves the way for a more scalable and sophisticated strategy to enhance AI reasoning powers. All relevant code and resources are available in https://github.com/GAIR-NLP/weak-to-strong-reasoning.", "sections": [{"title": "1 Introduction", "content": "As the pursuit of Artificial General Intelligence (AGI) advances, the creation of superintelligent systems-models that exceed human cognitive capabilities-remains a key ambition within the field. This quest introduces a host of challenges, especially concerning the supervision and learning paradigms for these advanced AI models. Conventional supervision methods, which typically depend on human oversight or guidance (i.e., distilled knowledge) from more advanced models, become inadequate as the capabilities of AI exceed those of their supervisors. To address this issue, we focus on the weak-to-strong learning paradigm, which operates under a unique task setting where only a less capable model and a stronger\u00b9 but not fully utilized model are available.\nThe central question of weak-to-strong learning is whether models with limited capabilities can effectively guide the development of more advanced, stronger models. Previous studies have demonstrated the feasibility of it in classification, chess, and reward modeling tasks. However, the applicability of this setup to more complex reasoning tasks, which demand more than mere extrapolation or pattern recognition, remains an open question. Complex reasoning represents a key aspect of human cognition, crucial for assessing whether LLMs can emulate or surpass human-like capabilities in comprehending the world, making decisions, and solving problems. Given the complexity and the critical nature of these tasks, applying the weak-to-strong learning framework to advanced reasoning challenges is essential, particularly within the broader context of achieving superintelligence.\nAlthough suggest that naively fine-tuning strong models on the full set of noisy data produced by weak models, named full weak fine-tuning, can consistently improve their performance over the weaker counterparts, this approach is still far from recovering the full capabilities of strong models, and our experiments show that it loses effectiveness when facing more complex reasoning challenges. They also propose an auxiliary confidence loss to mitigate the issue of strong models imitating the errors of their supervisors. However, this method is tailored to classification tasks with a set of fixed labels and does not naturally extend to open-ended generation tasks including reasoning. Currently, there is a lack of effective methods beyond naive fine-tuning to prevent the overfit of weak errors and to further elicit the intrinsic reasoning abilities of strong models within the weak-to-strong reasoning framework.\nTo achieve the above goal, we introduce a progressive refinement learning framework, guided by the principle that a model can enhance its capabilities more effectively by initially focusing on smaller, more reliable subsets of data, and then iteratively expanding its learning scope. In the first stage, we hypothesize that it is more advantageous to utilize smaller quantities of data that are likely to be more accurate. We achieve this by combining weak data, generated by the less capable model, with data self-generated by the more advanced model through in-context learning. This blend is then used to selectively curate datasets for subsequent supervised fine-tuning. In the second stage, upon having developed a strong model with improved reasoning capabilities, we utilize its ability to construct contrastive samples for preference optimization and enables the model to learn effectively from the errors of the weaker model.\nIn implementation, we employ Llama2-70b as the strong model, test three separate weak models: Llama2-7b, Gemma-2b and Mistral-7b, and conduct experiments"}, {"title": "2 Preliminaries", "content": "We outline common learning paradigms in large model training, primarily characterized by the need for ground truth answers and supervision from stronger models.\nWhen training LLMs, it is ideal to have a sufficient amount of training data with ground truth answers, which we refer to as generic-supervised learning paradigm. However, acquiring such data is often label-intensive and can sometimes be impossible. As a result, various learning paradigms have emerged to reduce the effects of data quality and quantity while still improving performance.\nIn the current context, to enhance a strong model like Llama2-70b, improvements can still be made by seeking help to a stronger model like GPT-4 even without ground truth. Hence, many existing works suggest that a stronger model acts as a teacher model to provide specific feedback to improve the targeted model. This paradigm can be viewed as distilling the stronger teacher model's knowledge. Nonetheless, merely imitating the teacher model is not a long-term solution; imitation models only slightly close the performance gap to the teacher model on tasks not well-represented in the imitation data. Furthermore, distillation learning primarily benefits models that are less capable than the teacher model.\nConsidering the high costs of annotating training data by humans or stronger proprietary models, a line of works relies on the correct responses generated by the model itself to update it. For example, filter solutions according to the correctness of final answers, while employ reward models trained on gold annotations to score self-generated content. It is evident that, whether using binary labels or fine-grained feedback, this paradigm still requires ground truth to assess the usability of the model's self-generated responses. Without ground truth answers, self-improvement leads to minimal performance gains and may even degrade performance.\nGaining insights from semi-supervised learning within the domain of traditional machine learning, another type of LLM learning depends not on extensive labeling but instead on a small, high-quality seed dataset. have demonstrated improvement by learning differences between self-generated responses and expert-annotated responses. We also include the trending research topic of easy-to-hard generalization in this category, where models are trained to tackle complex tasks by learning from human annotations on easier tasks. This series of research inevitably require access to a small yet high quality set of standard answers."}, {"title": "2.2 Weak-to-Strong Reasoning Setup", "content": "In this paper, we address reasoning tasks in the weak-to-strong setting. First, we examine mathematical reasoning tasks, such as those in GSM8k and MATH. These tasks require each step of the reasoning process to demonstrate fundamental mathematical problem-solving skills, including problem comprehension and algebraic operations, and build upon the previous steps. It imposes higher demands on the model's learning and generalization capabilities. Unlike classification tasks, where models can rely on superficial pattern extrapolation or recognition, reasoning tasks offer minimal benefit from guessing. Then, we use a weak model (e.g., Llama2-7b) with a certain degree of mathematical problem-solving ability, denoted as m. This model acts analogously to human supervisors with limited expertise in the era of superintelligence. Besides, we only have a set of questions $Q = \\{q_i\\}$ without ground truth answers and the goal is to improve the reasoning capability of a strong model M (e.g., Llama2-70b). To implement this, following , we randomly divide the original training set into two equal parts, $D_{gold,1}$ and $D_{gold,2}$. The weak model is initially fine-tuned using $D_{gold,1}$ where the gold solutions are available, resulting in a weak model with some problem-solving capability, i.e. m. In contrast, the strong model can only access the questions from $D_{gold,2}$, without reasoning chains or final answers, i.e., Q."}, {"title": "3 Methodology", "content": "In this section, we propose a weak-to-strong training method designed to maximize the use of weak data and to elicit the strong model's innate talent. First, we identify potentially positive samples in the absence of ground truth and external signals. During Stage I, we exclusively utilize this subset of data for supervised fine-tuning. Then once the strong model has achieved a certain level of reasoning proficiency, we employ the full weak data, particularly the potentially negative samples in Stage II via preference learning-based approaches like DPO, encouraging the strong model to learn from mistakes made by the weaker model.\nGiven a weak model m and a series of math problems Q without ground truth, m generates weak data $D_{weak} = \\{q_i, C_{weak, i}, A_{weak, i}\\}$, where $q_i \\in Q$, $C_{weak, i}$ represents a reasoning chain, and $a_{weak, i}$ represents the final answer. The correctness of $a_{weak, i}$ is unknown. The central challenge is: how can we maximize the use of m and $D_{weak}$ to fully enhance and recover the mathematical reasoning capabilities of a stronger model \u041c?"}, {"title": "3.1.1 Full Weak Fine-Tuning", "content": "Our initial strategy is to fine-tune the stronger model M across the entirety of the weak dataset $D_{weak}$. While prior research has validated the effectiveness of this approach in text classification tasks, its efficacy in reasoning tasks remains unexplored. We have therefore embarked on an investigation to determine whether the phenomenon of weak-to-strong generalization can also enhance the reasoning capabilities of M in this less examined domain."}, {"title": "3.1.2 Weak In-Context Learning", "content": "Another straightforward approach is in-context learning (ICL), which requires only several training samples as demonstrations in the prompt. Specifically, we randomly select four samples from $D_{weak}$ as demonstrations. Since we do not have access to the ground truth, these demonstrations cannot be provably correct."}, {"title": "3.1.3 Weak-ICL Fine-Tuning", "content": "Given that models can mimic weak errors through supervised fine-tuning, we propose refining $D_{weak}$ before use, instead of using all data blindly. Additionally, we seek to harness the innate abilities of the strong model activated via in-context learning. Building on these two ideas, we introduce weak-icl fine-tuning, employing both weak data $D_{weak}$ and \u201cicl data\u201d $D_{icl} = \\{q_i, C_{icl,i}, A_{icl,i}\\}$, where $q_i \\in Q$, $C_{icl,i}$ and $A_{icl, i}$ are generated by M with few-shot demonstrations, as higher-quality supervision signals.\nNote that, for both $D_{weak}$ and $D_{icl}$, we cannot determine whether a certain answer is correct or not. Nonetheless, when two models, employing distinct data representations, converge on the same answer in an open-ended task, it is indicative of a higher likelihood of accuracy. This phenomenon supports the reliability of the results when consistency is observed across different methodologies. We thus compare $D_{weak}$ and $D_{icl}$ generated by the weak model and strong model, respectively, and select $D_{weak}$ and $D_{icl}$ if $a_{weak,i} = a_{icl,i}$, for subsequent supervised fine-tuning. We call this approach final answer consistency. Considering the combination of the two sets of data, we can obtain three versions of enhanced fine-tuned strong models:\nHaving different data representations, as they are trained on data from different sources\u2014$D_{weak}$ is generated by the weak model, whereas $D_{icl}$ primarily originates from the strong model itself. Hence, we can perform iterative training to bootstrap performance. We denote the initial round of supervised fine-tuning data as $D^{1}_{weak}$ and $D^{1}_{iel}$, resulting in models $M^{1}_{weak-ft}$, $M^{1}_{iel-ft}$, and $M^{1}_{hybrid-ft}$. In the second iteration, we obtain zero-shot solutions from $M^{1}_{weak-ft}$ applied to Q to construct $D^{2}_{weak}$ and those from $M^{1}_{iel-ft}$ to construct $D^{2}_{iel}$. Here, the subscripts \u201cweak\u201d and \u201cicl\u201d indicate the initial data source. Then we apply final answer consistency to obtain $D^{weak'}_{icl}$ and $D^{weak'}_{icl}$. Following another round of supervised fine-tuning, we have:"}, {"title": "3.2 Stage II: Learn from \"Negative\u201d Samples", "content": "We denote the final iteration of $M^{1}_{hybrid-ft}$ from Stage I as $M_{plus}$, which has learned dual mathematical solutions and holds potential for further enhancement. Next, we apply preference optimization techniques to strategically utilize the potentially erroneous subset of the original weak dataset $D_{weak} = \\{q_i, C_{weak,i}, A_{weak, i}\\}$ generated by m, which allows the strong model to identify and avoid similar errors in future reasoning processes. The key factor lies in how to construct contrastive samples for learning.\nWithout access to ground truth, the current strong model with enhanced reasoning capabilities identifies the most likely correct answers based on its confidence. Specifically, for each question $q_i \\in Q$, we sample n responses from $M_{plus}$, and define the probability of the answer that appears most frequently among these responses as confidence. When the confidence falls below a specified threshold \u03c4, we consider the model's judgment on this question unreliable and therefore discard it. Conversely, if the confidence is no less than \u03c4, we regard the model as capable of solving the question and proceed to construct contrastive samples as follows:"}, {"title": "4 Experiments", "content": "In preliminary tests with Llama3-70b, we observe that on GSM8K and MATH, Llama3-70b can largely unlock its potential through in-context learning, with marginal or even adverse impacts from parameter updates due to training instabilities. Consequently, we focus on a more challenging dataset developed after the release of Llama3-70b, OlympicArena, to simulate a more realistic future scenario.\nWe only consider English questions in OlympicArena, excluding the CODE (Code Generation) and OT (Others) problem types that require case-based or expert evaluation. This results in 6,020 training data without solutions and final answers, and 313 test data with final answers to assess the performance of different methods. We use Llama3-8b-instruct (without initial fine-tuning on a subset of training data) as the weak model and Llama3-70b as the strong model to be improved. The hyperparameters are consistent with those used for GSM8K. This configuration more closely resembles future real-world weak-to-strong scenarios.\nMweak-ft, obtained by our proposed weak-icl fine-tuning method, achieves higher performance than Full Weak FT with fewer training data (i.e., 746), outperforming it by 0.32 points. After the second stage of preference optimization, which further exploits the weak model and training questions without answers, the strong model's performance is improved by an additional 3.19 points over Full Weak FT. This demonstrates the robustness and generalizability of our method in scenarios closer to future conditions."}, {"title": "5 Related Work", "content": "LLMs can enhance their ability to solve tasks and better align with human instructions through a supervised fine-tuning (SFT) phase. This phase heavily relies on the quality of training data, as previous studies demonstrate that higher data quality translates to substantial gains in model performance. In this paper, we investigate the potential of learning from weak supervisions."}, {"title": "5.1 LLM Training", "content": "To further align LLMs with human values and enable learning from both positive and negative feedback, additional training is required, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). In particular, DPO reparameterizes reward functions in RLHF and has been widely used due to its simplicity. Several variants of DPO have then emerged to further enhance its stability and performance, such as ORPO and SimPO, etc. This paper explores the capabilities of DPO and ORPO using our constructed contrastive samples in a weak-to-strong setting."}, {"title": "5.2 Mathematical Reasoning", "content": "The exploration of mathematical reasoning within LLMs has been a focal point for evaluating their cognitive capabilities akin to human reasoning. Researchers have developed various methods to enhance the mathematical reasoning capabilities of LLMs after pre-training, which can be broadly classified into two categories: (1) Prompting: Some works aims to elicit the intrinsic reasoning abilities of LLMs by specific prompting engineering, without updating the model parameters; (2) Fine-tuning: Another line of studies focuses on generating a more extensive and higher-quality collection of question-answer pairs. Through supervised fine-tuning and preference optimization, the models can achieve significant improvements in their mathematical problem-solving capabilities."}, {"title": "6 Conclusion", "content": "In this paper, we explore the efficacy of weak-to-strong framework in complex reasoning tasks. We introduce a new method that elicits strong capabilities using weak supervisions, without relying on annotations from humans or more advanced models. This method focuses on the strong model's ability to autonomously refine its training data, even if it has not learned the task before. By iteratively expanding its learning scope, the strong model continuously broadens its reasoning skills. This self-directed data curation is crucial for scaling up the enhancement of AI reasoning capabilities, making the model more independent and effective in its developmental trajectory. Through this work, we seek to illuminate new pathways for AI development, emphasizing the critical role of innovative model supervision in advancing AGI and beyond."}, {"title": "Limitations", "content": "In our experiments, we use Llama2-70b and Llama3-70b as a proxy for hypothetical superintelligent models of the future. We acknowledge that there might be performance discrepancies compared to a genuine future advanced model. Nonetheless, our efforts lay the groundwork for investigating methodologies in weak-to-strong reasoning. Additionally, this paper does not explore supervision at the process level, such as the model's ability to learn from partially correct data. In the weak-to-strong scenario, the presence of non-negligible errors and noise at the process level yields only limited performance improvements in our early experiments, thereby posing challenges for future research."}, {"title": "A Appendix", "content": "A.1 Dataset Details\nA.1.1 Dataset Construction\nFor GSM8K, we evenly divide the original training dataset of 7,473 samples into two subsets, $D_{gold, 1}$ and $D_{gold,2}$. Additionally, we supplement both $D_{gold, 1}$ and $D_{gold,2}$ with the data of the same distribution developed by (Chern et al., 2023), until each contains 7,000 samples. Thus, the weak model uses $D_{gold,1}$, which includes both questions and gold solutions, to obtain basic problem-solving capabilities. Meanwhile, the strong model can only access a training dataset $Q = \\{q_i\\}$, where $q_i \\in D_{gold,2}$, consisting of 7,000 mathematical problems without ground truth answers. GSM8K also includes 1,319 test samples.\nFor MATH, we employ the same subset of 500 representative problems as the test set, identical to that used in Lightman et al. (2023). We then split the remaining 12,000 samples evenly between $D_{gold,1}$ and $D_{gold,2}$, each containing 6,000 samples.\nA.1.2 Statistics of MATH test set\n\nA.2 Training Details\nFor supervised fine-tuning in Stage I, we adopt LoRA to fine-tune the strong model M with a learning rate of $1 \\times 10^{-4}$ and search for weight decay in the set $\\{0, 0.01\\}$. We run 2 epochs on GSM8K and 3 epochs on MATH, with a batch size of 8. In Stage II, we employ two preference optimization methods. For DPO, we train the enhanced strong model $M_{plus}$ with a learning rate of $1 \\times 10^{-5}$ and run 1 epoch. For ORPO, we search for $\u03b2$ in the set $\\{0.1, 0.5, 1.0\\}$ with a learning rate of $3 \\times 10^{-5}$ and run 1 epoch. All experiments are conducted using A100 GPUs. When constructing contrastive samples in Stage II, we sample n = 10 responses at temperature = 1.0, and use a confidence threshold of $\u03c4 = 0.6$. Normally, we evaluate using greedy decoding. For calculating pass@k, we set k = 10 at temperature = 1.0.\nA.3 Additional Analysis"}, {"title": "A.4.1 Details of Stage I on MATH", "content": "In the Stage I experiment conducted on the MATH dataset, it is found that the amount of training data selected via final answer consistency is so limited that the strong model can hardly learn the effective features through supervised fine-tuning. To address this, we randomly sample additional inconsistent data. Based on the weak model's performance (Llama-7b < Gemma-2b < Mistral-7b on MATH), we supplement the data (both $D_{weak}$ and $D_{icl}$) to 1,000 instances for Gemma-2b and 2,000 instances for Mistral-7b, and present the results in Fig. 4. The original amount of training data and test accuracy for these two weak models are shown in Tab. 10."}, {"title": "A.4.2 Pass@k Results", "content": "Tab. 9 summarizes the greedy decoding and pass@k results for the three variants of enhanced strong models obtained through weak-icl fine-tuning. Notably, $M_{hybrid-ft}$ utilizes a training set that combines those used by $M_{weak-ft}$ and $M_{icl-ft}$. The results indicate that $M_{hybrid-ft}$ outperforms its counterparts in terms of pass@k, achieving superior pass@k scores with margins of up to 5.23 points. The only exception occurs in the MATH dataset supervised by Llama2-7b, where the underperformance is likely due to limited training data.\nThe superior performance of $M_{hybrid-ft}$ can be attributed to the diversity of solutions in its training set (verified in \u00a7A.3.1), validating our approach of adopting the final iteration of $M_{hybrid-ft}$ from Stage I for preference optimization in Stage II. It is important to note that while higher pass@k scores suggest greater potential, the true challenge lies in effectively harnessing this potential, particularly in the weak-to-strong setting where no ground truths are available. Our proposed weak-to-strong preference optimization in Stage II successfully addresses this challenge, transforming theoretical potential into tangible performance gains in greedy decoding, as proved in \u00a74.4."}, {"title": "A.4.3 PGR of Stage I", "content": "Burns et al. (2023) propose a new metric called performance gap recovered (PGR) to measure the fraction of the performance gap that can be recovered through weak supervision, as illustrated in Eq. 1. Tab. 11 displays the results of the naive full weak fine-tuning (i.e., Full Weak FT) and our best weak-icl fine-tuning (i.e., Weak-ICL FT) in terms of PGR, which also demonstrate that our method can outperform the simple competitor. However, the variations in PGR across different weak models do not provide meaningful insights. In the experiments described in the main text, we use test accuracy instead to provide a more detailed depiction of model performance.\n$PGR = \\frac{weak-to-strong - weak floor}{strong ceiling - weak floor}$"}]}