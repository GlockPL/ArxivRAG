{"title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability", "authors": ["Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia"], "abstract": "Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning). The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "ChatGPT could be without doubt one of the most popular words in our society nowadays. ChatGPT2 refers to an Artificial Intelligence (AI) chatbot created by OpenAI company that is capable of interacting with humans in a conversational way, making it possible to answer questions, summarize content, correct mistakes, provide suggestions, and write and debug code, among many other tasks. Since its launch in November 2022, ChatGPT has been the fastest-growing consumer application in history, reaching over 100 million monthly users just two months after launch [1]. In fact, ChatGPT has been already deployed with success in several real-world applications [2], [3]. But, what is the main reason for the success of ChatGPT and why now? In general, this has been possible thanks to the rapid advance produced in Large Language Models (LLMs) in the last years [4], [5] which offer impressive capabilities in different tasks like medicine [6]- [8], education [3], or coding [9], and also, the fine-tuning of the models through Reinforcement Learning from Human Feedback (RLHF)\u00b3, improving the experience from a human perspective while interacting with them. One of the first popular LLMs that revolutionized the field was GPT-1 [10]. GPT-14 was the first LLM created by OpenAI and is based on a Transformer architecture [11], providing a more structured memory to handle long-term dependen- cies in comparison to traditional Recurrent Neural Networks (RNNs) [12], [13]. In addition to the Transformer architecture, the authors explored a semi-supervised approach for language understanding tasks using a combination of unsupervised pre- training and supervised fine-tuning. In particular, the authors demonstrated that it is possible to achieve good performance on new tasks (e.g., textual entailment, reading comprehen- sion, etc.) when the model is developed in an unsupervised way training with a large amount of data (BooksCorpus dataset [14]), and then fine-tuned to each specific dataset with minimal adaptation. Since the publication of GPT-1 (117 million parameters) in 2018, several LLMs have been presented in the field [5], scaling up the models as it helps to greatly improve task- agnostic, few-shot performance. An example of this is GPT- 3, which was presented in 2020 and comprises 175 billion parameters [15]. In that paper the authors demonstrated the influence of model size in the performance, concluding that GPT-3 is able to achieve promising results in the zero-shot and one-shot settings, and is able to achieve state-of-the-art results in few-shot settings. These interesting results in terms of generalization originated the integration of the GPT-3 model in the ChatGPT chatbot, achieving astonishing results. Nevertheless, OpenAI is not the only company research- ing in the field. Others such as Google and Meta AI have recently presented their own LLMs known as PaLM 1 and 25 [16], [17] and LLaMA6 [18], respectively. However, most of them only operate to date with text as input/output, and"}, {"title": "II. CHATGPT: SETUP", "content": "OpenAI offers accessibility to ChatGPT through their in- teractive chatbot interface or through an API. Both of them have the same functionalities, but the API provides a simpler interface to run extensive experiments in Python. For this reason, although the experiments included in Sec. IV are performed using the API, we initially performed some quick experiments using the chatbot interface to explore easily the most adequate configurations. At the date of writing this paper, a premium subscription is needed to use the latest LLM (GPT- 4), which accepts images along with other file formats and the use of other OpenAI products. Regarding its use and limitations, the API is split into different tiers, giving you more model capacities (requests, tokens, etc.) the more you pay. The number of requests given per tier changes actively, increasing from 100 RPD (Requests Per Day) for the gpt-4- vision-preview model to 500 RPD in less than a month for the tier 1 users. We describe next the main aspects of the API that can greatly affect the results and cost of the experiments: Prompt: The text/question to be introduced as input to the LLM. OpenAI gives different recommendations to get the expected result, like writing clear instructions, providing ref- erence text, or splitting complex tasks into simpler subtasks10. Roles: In order to interact with the API, it is mandatory to choose one of the three roles. The system role allows you to specify the way the model answers questions. The user role represents the queries made by the user. Lastly, the assistant role is employed to simulate the model's replies as, unlike the chatbot interface, the API lacks memory of prior messages. For all our experiments the prompt is sent with the system role, and the corresponding image with the user role. Assistant role is not used for any experiment. Max Tokens: This parameter indicates the maximum num- ber of tokens that the model can return. By tuning this, we can control the output style and price per request. For all our experiments we establish this parameter to 1,000 tokens."}, {"title": "B. Design and Configuration", "content": "Several configurations were tested in order to increase the performance in face biometrics, and at the same time optimize the usage of ChatGPT to reduce the cost and time. Image Configuration: Two options are considered. First, we've opted to merge the two facial images involved in the comparison into a single image as can be seen in Fig. 2 (left). We also consider a second configuration, which has advantages in terms of cost and comparison times. As ChatGPT is able to discern and respond to tabulated information on a cell- by-cell basis, we also create a matrix including 4x3 face comparisons, as depicted in Fig. 2 (right). According to OpenAI's documentation [19], the size of an optimal image is approximately 2,000x768 pixels for high-resolution images. Thus, for this purpose, we created a matrix of 2,123x903 pixels, organized into 4 columns and 3 rows (i.e., we fit 12 face comparisons of 512x256 pixels plus the cell borders). In order to distinguish between different face comparisons, each cell is separated from the rest by a blue border and identified by a red number (from 0 to 11) that is used to reference the cell in the output of the model. Prompt Configuration: this is the most important aspect to analyze. First, we focus on the prompt design for the face verification task, considering the first image configuration case, i.e., a single face comparison in the image. Following OpenAI's recommendations, we created a detailed prompt"}, {"title": "III. EXPERIMENTAL FRAMEWORK", "content": "The experimental protocol proposed in this study has been designed to analyze quantitatively the performance of Chat- GPT for the tasks of face verification and soft-biometrics estimation. Face Verification: first, we analyze the ability of ChatGPT in different application scenarios (i.e., controlled, surveillance, and extreme conditions) and image qualities. For this purpose, we consider the following databases in the evaluation: \u2022 Labeled Faces in the Wild (LFW) [23]: this is a very popular database in the field, containing high-quality images with no hard variations in pose. \u2022 QUIS-CAMPI [24]: this database comprises videos and images captured in an uncontrolled outdoor setting using a camera positioned approximately 50 meters away from the subjects. \u2022 TinyFaces [25]: this database consists in images of ex- tremely low quality, with an average resolution of 20x16 pixels. In addition to this, we also evaluate the performance of ChatGPT when considering popular challenges in face recog- nition such as demographic bias, age and pose variations, and occlusions. The following databases are considered in the evaluation, which are also considered in the recent FRCSyn Challenge [21]: \u2022 BUPT-BalancedFace [26]: this database is specifically designed to tackle performance variations among various ethnic groups. It comprises eight distinct demographic groups formed by a combination of ethnicities (White, Black, Asian, Indian) and gender (Male, Female). \u2022 CFP-FP [27]: this database presents images from subjects with great changes in pose and in different environmental contexts. \u2022 AgeDB [28]: this database presents diverse images fea- turing subjects of varying ages in different environmental contexts. \u2022 ROF [29]: this database consists of occluded faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks. Soft-Biometrics Estimation: to assess the performance of ChatGPT for the estimation of soft biometrics, we use the MAAD-Face database [22] which is based on the VGGFace2 database [32]. MAAD-Face database provides a total of 47 soft-biometric attributes per face image. In addition, we also consider the LFW database [23] as the authors of [33] labeled manually the following soft biometrics: gender, age, and ethnicity. Finally, it is important to highlight that, due to the limita- tions of the OpenAI's API in terms of the number of requests per day, and the prize, we had to reduce the number of face comparisons to 1,000 per database for the face verification task. These comparisons are selected randomly from the standard protocols. Regarding the soft-biometrics estimation, we consider 1,000 face images per database. At the date of writing this paper, the cost of ChatGPT is 0.01$ per 1,000 input tokens and 0.03$ per 1,000 output tokens. The tokens per"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We conducted a performance assessment considering Arc- Face, AdaFace, and ChatGPT. Specifically for ChatGPT, we took evaluations using images in a matrix-like configuration (i.e., ChatGPT 4x3), as well as through direct comparisons (i.e., ChatGPT 1x1). To measure the similarity between Ar- cFace and AdaFace embeddings, we use the cosine distance. We also consider this metric to determine the Equal Error Rate (EER) for these models. For ChatGPT, we use the confidence values obtained directly from ChatGPT's outputs as a custom metric to obtain this EER. The results are shown in Tables I and II, categorized into two main groups. The first group (left part) refers to different application scenarios, including controlled environments (LFW), surveillance (QUIS-CAMPI), and extreme conditions (TinyFaces). The second group (right part) highlights popular challenges in face recognition such as demographic bias (BUPT), pose (CFP-FP) and age (AgeDB) variations, and occlusions (ROF). Lastly, the rightmost column presents the average performance of each model across all databases. In general, state-of-the-art models such as ArcFace (95.44% Average accuracy, 6.19% Average EER) and AdaFace (95.80% Average accuracy, 5.59% Average EER) exhibit superior overall performance compared to ChatGPT. However, while these models are trained for this specific task, ChatGPT is primarily oriented to more general tasks. Moreover, when evaluating ChatGPT, a significant decline in performance was observed when the images were presented in a matrix format (66.23% Average accuracy, 34.96% Average EER) compared to the case of providing comparisons one by one (80.19% Average accuracy, 21.19% Average EER). We hypothesize that this reduction in performance might be produced as in the ChatGPT 4x3 case, the model needs first to detect the faces in the whole image, and then perform facial verification, potentially compromising overall task execution. Nevertheless, considering this matrix approximation could serve as an quick"}, {"title": "B. Comparison with the State of the Art", "content": "In the present study, we compare the results achieved by ChatGPT with state-of-the-art methods. For the task of face verification, the following two approaches are considered: ArcFace [30]: this face verification system considers a loss function that maps facial features into a high-dimensional hypersphere where the embeddings are optimized to maximize the angular margins between different identities. The system considered in this study is based on the iResNet-100 architec- ture [34] pretrained using the MS1Mv3 database [35]. Cosine distance is used to measure the similarity between feature embeddings. AdaFace [31]: this face verification system proposes a new loss function in order to pay more attention to the harder examples in terms of image quality. In particular, the authors proposed an adaptive margin function that approximates the image quality with feature norms. The system considered in this study is based on the iResNet-100 architecture [34] pre- trained using the WebFace12M database [36]. Cosine distance is used to measure the similarity between feature embeddings. Regarding the soft-biometrics estimation, we compare the results achieved by ChatGPT with two different approaches. For the LFW database, as it only contains the soft bio- metrics related to gender, age, and ethnicity, we consider FairFace [37], as it provides state-of-the-art results. Finally, for the MAAD-Face database, as it contains 47 soft-biometric attributes per facial image, we specifically train a ResNet-50 architecture pretrained on Imagenet [38] using the train set of MAAD-Face."}, {"title": "B. Explainability", "content": "For completeness, we also analyze how ChatGPT can in- crease the explainability of the results for the task of face verification. In Fig. 4, the proposed prompt is shown, along with the outputs provided by ChatGPT for some examples of the different face verification databases. ChatGPT's responses are divided into right (on the left column) and wrong (on the right column). In both right and wrong answers, ChatGPT demonstrates its ability to rationalize decisions based on image features. For example, in most cases, the output score of ChatGPT for the task of face verification is related to soft-biometric attributes such as facial hair and skin tone. Additionally, it exhibits the capability to focus on more detailed attributes like eye color, face shape, or nose shape, showcasing proficiency in handling both coarse and fine details. It is noteworthy that ChatGPT considers facial expressions in its predictions, despite of the fact that this is a variable attribute that should not be considered. Furthermore, the model recognizes temporal differences between images, incorporat- ing this information into its predictions. Regarding the wrong answers, we observe that, although the prediction may be wrong, some of the explanations provided by ChatGPT are accurate in describing the people in the images."}, {"title": "C. Soft Biometrics", "content": "Tables IV and V shows the results achieved for the soft- biometrics estimation task for the LFW and MAAD-Face databases, respectively. For completeness, Fig. 5 shows some examples of the output provided by ChatGPT with the pro- posed prompt. Analyzing the results achieved on LFW database, Fair- Face exhibited superior performance for gender classification (98.23% Accuracy) compared to ChatGPT (94.05% Accu- racy). Despite this, ChatGPT outperforms FairFace for age classification (72.87% vs. 67.88% Accuracy) and ethnicity classification (88.25% vs. 87.48%). These results prove the potential of ChatGPT for certain facial attribute classifications. For a more extensive evaluation, we consider the MAAD- Face dataset, annotated with 47 distinct attributes. Our custom model (ResNet-50) achieves superior performance across the majority of attributes (87.28% Average accuracy). Neverthe- less, ChatGPT, although having lower average performance (76.98% Average Accuracy), excels on some facial attributes. Some of the most notorious soft-biometric attributes where ChatGPT achieves better performance are in gender classifi- cation (96.30% Accuracy), some ethnicities (White - 83.90% Accuracy, Black - 97.50%), and accessories such as wearing a hat. These results follow the same conclusions drawn in the face verification task. While specific models trained for the task achieve in general better results, ChatGPT shows promising results and utility for tasks with no prior training."}, {"title": "V. CONCLUSIONS", "content": "In this study, we have conducted a comprehensive evaluation of ChatGPT's capabilities in handling facial biometric tasks, including face verification, soft-biometric estimation, and ex- plainability. Our experiments spanned across various databases and challenges, comparing the performance of ChatGPT with specialized models trained explicitly for these tasks. The experiments have revealed that while ChatGPT may not attain the same levels of accuracy as dedicated models, it presents a promising utility as an initial assessment tool with zero train- ing. For example, results around 94% Accuracy are obtained in LFW database for face verification. Also, impressive results are achieved for the estimation of soft biometrics such as gen- der (~96%) in the MAAD-Face database, or age (~73%) and ethnicity (\u224888) in the LFW database. Furthermore, its ability to return textual outputs contributes to a better explainability of the results. Future work will be oriented to analyze the ability of other popular chatbots for face biometrics."}]}