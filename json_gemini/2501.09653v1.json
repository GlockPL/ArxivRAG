{"title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models", "authors": ["Jonathan Katzy", "Razvan Mihai Popescu", "Arie van Deursen", "Maliheh Izadi"], "abstract": "The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "The data-intensive training process of Large Language Models (LLMs) has driven the release of numerous large-scale datasets, particularly for code, to facilitate the development of new models. This rapid increase in the amount of training data used to pre-train LLMs has resulted in extensive datasets covering almost all publicly available code [1]-[3].\nTo assess the success of such LLMs in downstream tasks, fresh data not seen during training is needed. Otherwise such evaluations are contaminated, possibly resulting in overly optimistic results. Unfortunately, obtaining such non-contaminated data is increasingly difficult. In fact, a recent study establishes that only 10% of investigations involving LLMs deduplicate their data with respect to the training data in order to avoid contamination [4].\nTo address this, we propose The Heap, a dataset of not previously used code that can be used for contamination-free multilingual evaluation of LLMs in downstream tasks. We address contamination in two ways. First, we select code with a non-permissive license, such as the GNU General Public License. Using such code for training is unattractive, as it may require the end user to publicly release all code in their code bases. Second, we pre-conduct computationally expensive near and exact deduplication, removing code that is used in other datasets widely used for training such as The Stack [1]."}, {"title": "II. COLLECTION", "content": "Using the search API, we collect our dataset from GitHub, a commonly used online platform for sharing code repositories. This collection process mimics the data distribution of other large-scale datasets [3], [5]-[8], minimizing the probability of including confounding factors in the dataset, such as drifts in the representations of data [9].\nA. Programming Languages\nWe aim to compile a representative dataset that encompasses a wide range of programming languages. To achieve this, we select languages based on several criteria. Our selection includes languages with diverse syntactic structures, such as LISP, C, Python, Haskell, and Assembly. We also select different programming paradigms, such as COBOL, Pascal, and C for procedural languages, Java, C#, Python, for object-oriented languages, and Haskell and Clojure for functional languages. To cover more specific use cases, we also include domain-specific languages such as Mathematica, Emacs-Lisp, and Coq. A complete list of all languages included in the dataset is presented in Table I.\nB. Query\nWe focus on repositories that have one of the targeted languages as the main language of the repository. We further select only repositories that are licensed under non-permissive licenses. We choose non-permissive licenses as an initial filter for repositories, as many large-scale datasets focus on exclusively unlicensed or permissively licensed code [2], [3], [5]. The reasons for the exclusion of non-permissively licensed code in other datasets come from potential licensing issues that may be related to the output of models trained on non-permissively licensed data [10]. The Heap is not intended for pre-training models that are aimed at end users, but rather for exclusive use in a research setting. The inclusion of exclusively non-permissively licensed code has the added benefit that it acts as a deterrent for developers to train LLMs on The Heap, ensuring it remains a relevant source of data for downstream tasks. We provide an overview of the licenses used in this work in Table II.\nC. Scraping\nFor each programming language, we scrape up to 50,000 repositories or as many as are available. Our dataset contains code from repositories created between January 2008 and August 2024. For each selected language, we extract repositories sorted by star count in descending order; this has been used as a loose quality metric before [11]. To"}, {"title": "III. DEDUPLICATION", "content": "An important aspect of fairly evaluating downstream tasks is preventing data leakage [4]. This is often done through a deduplication process. Although there should be no overlap between our non-permissively licensed dataset and permissively licensed datasets due to our selection procedure, it does not completely prevent overlap [10].\nOur deduplication strategy consists of exact deduplication and near deduplication. Before each deduplication strategy, we remove all comments (using a regex, based on the programming language) and whitespace from each file. This ensures that small changes to files, such as the removal of a license comment or changes in whitespace characters, still result in the detection of an exact duplicate. The final files included in The Heap are the unaltered versions scraped from GitHub.\na) Exact Deduplication: For exact deduplication, we calculate the SHA-256 hash of each file to identify exact duplicates between The Heap and publicly available datasets. We selected this hash function for its low collision probability, which reduces the risk of false positives.\nb) Near Deduplication: We also perform near-deduplication between our scraped dataset and the publicly available ones. To achieve this, we utilize the MinHash Locality-Sensitive Hashing (LSH) approach, implemented using the datasketch\u00b2 library. We apply the same SHA-256 hashing function as before, with 128 permutations and a precision-recall weight distribution of 40% - 60%. These design choices help mitigate hash collisions while maintaining a balanced trade-off, hence favoring higher recall at the expense of a controlled increase in false positives (removing files that were not duplicates).\nWe use a shingle size of 7 characters, as code files typically use a smaller set of characters compared to large research articles, where k = 9 [12]. This reduces the likelihood of overly common shingles, which could otherwise inflate similarity scores, as would occur with smaller values of k. Files with a Jaccard similarity above 0.7 are flagged as near duplicates, a threshold shown to be effective for duplicate detection [13].\nWe identify and flag duplicates between our dataset and all publicly available datasets to facilitate a more flexible approach to LLM evaluation, prioritizing both reproducibility and ease of use. This setup minimizes time and computational overhead by removing the burden of duplicate detection from researchers. Users can seamlessly filter data by language or by exact and near-duplicate files, tailoring the dataset to\nA. Datasets\nOur selection of datasets for deduplication is based on previously curated lists [10], with the addition of The Stack V2 [3], which is the only new dataset that has been released since the publication of previous works. We give an overview of all potential datasets in Table III. Due to the comment removal being based on the programming languages of the files, we are not able to infer the correct language for two datasets. The Pile [6], which has been removed and re-uploaded, has lost information about the programming language of a file. Furthermore, due to a known issue with the curation of CodeClippy, the languages and names of files are misaligned in the dataset. We also exclude this dataset from deduplication. Although we could predict the languages used in the files in these datasets, the tools that provide this functionality do return incorrect predictions, which could result in a duplicate not being removed. As we aim to provide a guarantee that there is no data contamination in our dataset, we remove these two datasets from consideration."}, {"title": "IV. LAYOUT", "content": "The Heap is organized into multiple subsets, each of them corresponding to one programming language. In each subset, the entries included in the dataset can be summarized into 3 groups: file content and metadata, quality indicators, and duplicates. We give an example of one entry in Figure 1."}, {"title": "V. FUTURE IMPROVEMENTS", "content": "In future iterations of this dataset, several potential improvements could be made. These include enhancing the deduplication process, releases of new training datasets, providing detailed information about the natural languages represented in the dataset, and tracking the evolution of codebases.\na) New Datasets: The main goal of this dataset is to reduce the burden of deduplicating a dataset used for downstream tasks for future research. This is only effective if the dataset is deduplicated against all available datasets. As new datasets are released we intend to pass them through the same pipeline to ensure The Heap remains relevant for the future.\nb) Deduplication: We addressed the deduplication of datasets using two widely adopted methods: exact deduplication based on hashing and near-deduplication leveraging locality-sensitive hashing. However, there is limited research on what constitutes an effective deduplication strategy. There could be issues with duplicates at a lower granularity level than file-based deduplications, as well as possible issues with the provenance of code fragments. Once studies are conducted on the impact of various deduplication approaches, we plan to incorporate these strategies as a new entry in the dataset.\nc) Cleaning: We include all files that we scraped that were not duplicates, while this gives us a dataset of deduplicated files, there is still the question of file \"quality\". In NLP research, keywords have been used for filtering websites, such as lorem ipsum or TODOS [18], and code datasets have been cleaned of autogenerated files using a similar approach [3]. We believe that this may also affect the quality of code datasets. Specifically, languages that rely heavily on boiler plating, such as Java, may benefit from removing certain common phrases from their corpus. This will be included as a further filtering step in a future release of the dataset.\nd) Topic Modeling: While languages can be used to loosely select an area that is being analyzed (Mathematica for mathematics, or JavaScript for web-based projects), many languages can be used in multiple specializations/areas. Adopting the FineWeb topic modeling approach for code datasets would create interesting annotations for the code files, as well as show any form of topic-based imbalances in the dataset.\ne) Natural Language: An under-explored research area involves the presence of multiple natural languages within code. As natural languages are often mixed within one file [19], we plan to adopt a Parts of Speech-like tagging [20] system for the natural languages present in each file. This can give information about the performance of code LLMs when the code is not in English. This will both help the development of non-English code LLMs, as well as aid English-focused LLMs, as they can be evaluated on only English."}, {"title": "VI. LIMITATIONS/CHALLENGES", "content": "The limitations and challenges faced by this dataset are two-fold. First, other actors may decide to train their models on this data, removing the benefits, and second, developers may object to their code being present in this dataset. We address these problems as follows.\na) Training: In order to use The Heap for a fair evaluation of an LLM, the researcher must be sure that the target LLM has not been trained on The Heap. Aside from our deduplication ensuring this fact for current existing LLMs, our collection process also adds a layer of protection from the inclusion of The Heap in the training procedure. The trend of training LLMs has shifted to only training on permissively licensed data, which would exclude The Heap. Furthermore, the restriction of The Heap to research only, alleviates the problems with author attribution in LLM generations as trained models are not intended to be used by end-users [10], [21].\nFurthermore, existing works such as membership inference attacks, have been extended to the scale of entire datasets [22]. This should make it possible in the near future to retroactively test for the inclusion of The Heap in the training procedures of a model.\nb) Ethics: With the rapid rise of public repositories being used to train code language models, many authors of older repositories were unaware that their code could be utilized for such purposes, leaving them unable to opt-out. Moreover, there is currently no consensus on how developers can opt in or out of having their code included in datasets. We acknowledge these ethical concerns regarding the use of code in deep learning practices and offer the ability for repository owners to opt out of having their code included in our dataset. Although this approach is not ideal, as it places the burden of exclusion on the authors, it aligns with the current best practices [3]."}, {"title": "VII. CONCLUSION", "content": "We present The Heap, a multilingual dataset of source code that we deduplicated against datasets commonly used in the (pre-)training of large language models. The Heap enables researchers to conduct investigations into the behavior and performance of code large language models without the need to perform extensive deduplication with other datasets. This addresses the shortcomings of LLM investigations not testing for data leakage in 90% of all investigations [4] allowing for more robust conclusions to be made.\nWe release the dataset (only for research purposes) and outline a road map for future features such as natural language annotation, topic annotations, and further cleaning procedures to be incorporated into the dataset, to make higher-quality evaluations easier and more available for all researchers."}]}