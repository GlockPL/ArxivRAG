{"title": "Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology", "authors": ["Weinan Dai", "Yifeng Jiang", "Yuanjing Liu", "Jinkun Chen", "Xin Sun", "Jinglei Tao"], "abstract": "This paper addresses the persistent challenge in Keyword Spotting (KWS), a fundamental component in speech technology, regarding the acquisition of substantial labeled data for training. Given the difficulty in obtaining large quantities of positive samples and the laborious process of collecting new target samples when the keyword changes, we introduce a novel approach combining unsupervised contrastive learning and a unique augmentation-based technique. Our method allows the neural network to train on unlabeled data sets, potentially improving performance in downstream tasks with limited labeled data sets. We also propose that similar high-level feature representations should be employed for speech utterances with the same keyword despite variations in speed or volume. To achieve this, we present a speech augmentation-based unsupervised learning method that utilizes the similarity between the bottleneck layer feature and the audio reconstructing information for auxiliary training. Furthermore, we propose a compressed convolutional architecture to address potential redundancy and non-informative information in KWS tasks, enabling the model to simultaneously learn local features and focus on long-term information. This method achieves strong performance on the Google Speech Commands V2 Dataset. Inspired by recent advancements in sign spotting and spoken term detection, our method underlines the potential of our contrastive learning approach in KWS and the advantages of Query-by-Example Spoken Term Detection strategies. The presented CAB-KWS provide new perspectives", "sections": [{"title": "1 INTRODUCTION", "content": "Keyword Spotting (KWS) is a fundamental application in the field of speech technology, playing a pivotal role in real-world scenarios, particularly in the context of interactive agents such as virtual assistants and voice-controlled devices. KWS is designed to detect a small set of pre-defined keywords within an audio stream. This capability is crucial for two primary reasons. First, it enables the initiation of interactions through specific commands like \"hey Siri\" or \"OK, Google,\" effectively serving as an explicit cue for the system to start processing subsequent speech. Second, KWS can identify sensitive words within a conversation, thereby playing a vital role in protecting the privacy of the speaker. Given these applications, it is crucial to develop accurate and reliable KWS systems for effective real-world speech processing [9, 11, 18].\nDespite the considerable advancements in KWS, a significant challenge that persists is the acquisition of sufficient labeled data for training. This is especially true for positive samples, which are often harder to obtain in large quantities. This issue is further exacerbated when the keyword changes, as it necessitates the collection of new target samples, a process that can be both time-consuming and resource-intensive. To address these challenges, we propose a novel approach that leverages the power of unsupervised contrastive learning and a unique augmentation-based method. Additionally, another potential problem is redundant information, speeches are noisy and complex, where only some key phrases are highly related to the keywords. However, convolutional methods treat all the word windows equally, ignoring that different words have different importance and should be weighted differently within word windows. Besides, the sliding windows used in the convolutional methods produce a lot of redundant information. Thus, it is important to reduce the non-informative and redundant information and distinguish the contributions of different convolutional features.\nOur method enables the neural network to be trained on unlabeled datasets, reducing the reliance on extensive labeled data. This technique can greatly enhance the performance of downstream tasks, even in scenarios where labeled datasets are scarce. Additionally, we propose that speech utterances containing the same keyword, regardless of variations in speed or volume, should exhibit similar high-level feature representations in KWS tasks. To achieve this, we present a speech augmentation-based unsupervised learning approach. This method leverages the similarity of bottleneck layer features, along with audio reconstruction information, for auxiliary training to improve system robustness.\nIn addition to these innovations, we propose a compressed convolutional architecture for the KWS task. This architecture, designed to tackle the issue"}, {"title": "2 RELATED WORK", "content": "Data augmentation is widely acknowledged as an effective technique for enriching the training datasets in speech applications, such as Automatic Speech Recognition (ASR) and Keyword Spotting (KWS). Various methods have been explored, such as vocal tract length perturbation [5], speed-perturbation [8], and the introduction of noisy audio signals [4]. More recently, spectral-domain augmentation techniques, such as SpecAugment [15] and WavAugment [7], have been developed to further improve the robustness of speech recognition systems. In this work, we extend these efforts by applying speed and volume perturbation in our speech augmentation method."}, {"title": "3 PRELIMINARY STUDY OF CONTRASTIVE LEARNING", "content": "In the context of a classification task involving K classes, we consider a dataset ${x_i, y_i}_{i=1}^N$ with N training samples. Each $x_i \\in R^L$ represents an input sentence of L words, and each $y_i \\in {1,2,..., K}$ is the corresponding label. We denote the set of training sample indexes by $I = {1,2,..., N}$ and the set of label indexes by $K = {1,2,..., \u039a}$.\nWe explore the realm of self-supervised contrastive learning, a technique that has demonstrated its effectiveness in numerous studies. Given N training samples ${x_i}_{i=1}^N$ with a number of augmented samples, the standard contrastive loss is defined as follows:\n$L_{self} = \\frac{1}{N} \\sum_{i \\in I} -log \\frac{exp (z_i \\cdot z_{j(i)} /\\tau)}{\\sum_{a \\in A_i} exp (z_i \\cdot z_a /\\tau)}$  (1)\nHere, $z_i$ is the normalized representation of $x_i$, $A_i := I\\backslash i$ is the set of indexes of the contrastive samples, the $\\cdot$ symbol denotes the dot product, and $\\tau \\in R^+$ is the temperature factor.\nHowever, self-supervised contrastive learning does not utilize supervised signals. A previous study [Khosla et al., 2020] incorporated supervision into contrastive learning in a straightforward manner. It simply treated samples from the same class as positive samples and samples from different classes as negative samples. The following contrastive loss is defined for supervised tasks:"}, {"title": "4 Proposed Method", "content": "The keyword spotting task can be framed as a sequence classification problem, where the keyword spotting network maps an input audio sequence $X = {x_0,x_1,...,x_T}$ to a set of keyword classes $Y \\in y_{1:s}$. Here, T represents the number of frames, and S denotes the number of classes. Our proposed keyword spotting model, depicted in Fig ?? (A), consists of five key components: (1) Compressed Convolutional Layer, (2) Transformer Block, (3) Feature Selection Layer, (4) Bottleneck Layer, and (5) Projection Layer."}, {"title": "4.1 Compressed Convolutional Layer", "content": "The Compressed Convolutional Layer replaces the CNN block in the original design. This layer learns dense and informative frame representations from the input sequence X. Specifically, it utilizes convolutional neural networks (CNNs), an attention-based soft-pooling approach, and residual convolution blocks for feature extraction and compression.\nFrame Convolution Just as in the original CNN block, the convolution operation is applied to each frame. Given the input sequence X and the i-th filter, the convolution for the j-th frame is expressed as\n$x_j^i = conv ({x_j, x_{j+1},\u2026,x_{j+k_i-1}};W_i)$,  (3)\nwhere $W_i$ is the learned parameter of the i-th filter.\nAttention-based Soft-pooling To eliminate redundant information in the speech dataset, we propose an attention-based soft-pooling operation on the frame representations learned by the previous equation. Specifically, given a frame $x_j$, its neighboring frames ${x_{j+1},\uff65\uff65\uff65,x_{j+g-1}}$, and the corresponding filter $f_i$, we first learn the local-based attention scores $a = W_a x_j + b$ with softmax function, and then conduct the soft-pooling operation to obtain the compressed representation as in the following equation:\n$o_j^i = \\sum_{q=j}^{j+g-1} \\beta_q^i x_q$ (4)"}, {"title": "4.2 ResLayer Block", "content": "Residual Convolution Block We now have a denoised matrix ${o_1^i, o_2^i, \u2026, o_T^i}$ that represents the input sequence X. To avoid vanishing gradients and facilitate model training, we introduce residual blocks on top of the compressed features. In particular, we replace the batch norm layer with the group norm layer. Let a denotes the number of residual blocks, we have\n$r_i = ResidualBlcok ({o_i,o_{i+1},\u2026,o_{i+a-1}})$, (5)\nwhere ResidualBlock is the operation of the residual convolution block.\nTransformer Block The output from the Compressed Convolutional Layer, $R = {r_1,r_2,\u2026\u2026,r_T}$, is then fed into the Transformer Block. This block captures long-term dependencies in the sequence via the self-attention mechanism: $E_{tran} = Self-Attention \\times_M (R)$, where M is the number of self-attention layers.\nFeature Selecting Layer Following the Transformer Block, the Feature Selecting Layer is implemented to extract keyword information from the sequence $E_{tran}$.\n$E_{feat} = Concat (E_{tran} [T - r, T])$, (6)\nHere, the last r frames of $E_{tran}$ are gathered, and all the collected frames are concatenated together into one feature vector $E_{feat}$.\nBottleneck and Project Layers After the Feature Selecting Layer, a Bottleneck Layer and a Projection Layer are added. These layers map the hidden states to the predicted classification classes \u1ef8.\n$E_{bn} = FC_{bn} (E_{feat})$, (7)\n$\\hat{Y} = FC_{proj} (E_{bn})$, (8)\nFinally, the cross-entropy (CE) loss for supervised learning and model fine-tuning is computed based on the predicted classes Y and ground truth classes Y. $L_{ce} = CE(Y,\\hat{Y})$."}, {"title": "4.3 Augmentation Method", "content": "Data augmentation is a widely utilized technique to enhance model performance and robustness, particularly in speech-related tasks. In this study, we delve into speed and volume-based augmentation in the context of unsupervised learning for keyword detection. A specific audio sequence, represented as $X = A(t)$, is defined by its amplitude A and time index t."}, {"title": "4.4 Contrastive Learning Loss", "content": "We aim to align the softmax transform of the dot product between the feature representation $z_i$ and the classifier $d_i$ of the input example $X_i$ with its corresponding label. Let $\\theta_i^*$ denote the column of $d_i$ that corresponds to the ground-truth label of $x_i$. We aim to maximize the dot product $\\theta_i^{*T}z_i$. To achieve this, we learn a better representation of $\\theta_i$ and $z_i$ using supervised signals.\nThe Dual Contrastive Loss exploits the relation between different training samples to maximize $\\theta_i^{*T}z_j$ if $x_j$ has the same label as $x_i$, while minimizing $\\theta_i^{*T}z_j$ if $x_j$ carries a different label from $x_i$.\nTo define the contrastive loss, given an anchor $z_i$ originating from the input example $x_i$, we take {$\\theta_p^*$}$_{p\\in P_i}$ as positive samples and {$\\theta_a^*$}$_{a\\in A_i \\backslash P_i}$ as negative samples. The contrastive loss is defined as follows:\n$L_z = \\frac{1}{N} \\sum_{i \\in I} [\\frac{1}{\\mid P_i \\mid} \\sum_{p \\in P_i} -log \\frac{exp (\\theta_p \\cdot z_i /\\tau)}{\\sum_{a \\in A_i} exp (\\theta_a \\cdot z_i /\\tau)}]$ (9)"}, {"title": "5 EXPERIMENT SETUP", "content": "In this section, we evaluated the proposed method on keyword spotting tasks by implementing our CNN-Attention model with supervised training and comparing it to Google's model. An ablation study was conducted to examine the impact of speed and volume augmentation on unsupervised learning. Additionally, we compared our approach with other unsupervised learning methods, including CPC, APC, and MPC, using their published networks and hyperparameters without applying any additional experimental tricks [23]-[25]. We also analyzed how varying pre-training steps influence the performance and convergence of the downstream KWS task."}, {"title": "5.1 Datasets", "content": "We used Google's Speech Commands V2 Dataset [23] for evaluating the proposed models. The dataset contains more than 100k utterances. Total 30 short words were recorded by thousands of different people, as well as background noise such as pink noise, white noise, and human-made sounds. The KWS task is to discriminate among 12 classes: \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", unknown, or silence. The dataset was split into training,"}, {"title": "5.2 Model Setup", "content": "The model architecture consists of:\nCNN blocks with 2 layers, a 3x3 kernel size, 2x2 stride, and 32 channels."}, {"title": "6 EXPERIMENTAL RESULTS", "content": ""}, {"title": "6.1 Comparision of KWS Model (RQ1)", "content": "The table compares the classification accuracy of three different KWS models: (1) the model by Sainath and Parada (Google), (2) the CAB-KWS model without volume augmentation, and (3) the CAB-KWS model with speed augment. It can be observed that the CAB-KWS model with speed augment achieved the highest classification accuracy on both the development (Dev) and evaluation (Eval) datasets. This research question aims to investigate how the inclusion of data augmentation techniques, specifically speed augment in this case, improves the performance of KWS models compared to models without these techniques. The results could be used to guide future development of KWS models and to optimize their performance for various applications."}, {"title": "6.2 Ablation Study (RQ2)", "content": "The CAB-KWS keyword spotting model is an advanced solution designed to improve the classification accuracy of speech recognition tasks. The ablation study presented in the table focuses on evaluating the impact of different pre-training techniques, such as volume pre-training, speed pre-training, combined volume and speed pre-training, and combined volume, speed, and contrastive learning pre-training, on the model's performance. By comparing the classification accuracy of CAB-KWS when fine-tuned on two datasets, Speech Commands and Librispeech-100, we can better understand the effectiveness of these pre-training techniques and their combinations."}, {"title": "6.3 Comparison with Unsupervised Models (RQ3)", "content": "The CAB-KWS model is a sophisticated keyword spotting solution that integrates multiple pre-training techniques to improve classification accuracy in speech recognition tasks. The Tab. 3 provided presents a comparison of the CAB-KWS model with three other models that employ individual pre-training methods, namely Contrastive Predictive Coding (CPC), Autoregressive Predictive Coding (APC), and Masked Predictive Coding (MPC). By comparing the performance of these models, we can gain insights into the effectiveness of the CAB-KWS model and highlight its advantages over models based on single pre-training techniques.\nThe comparison in the table reveals that the CAB-KWS model consistently achieves the highest classification accuracy on both the development (Dev) and evaluation (Eval) datasets when fine-tuned on Speech Commands, regardless of"}, {"title": "7 CONCLUSION", "content": "This paper presents a robust approach for the Keyword Spotting (KWS) task. Our CNN-Attention architecture, in combination with our unsupervised contrastive learning method, CABKS, utilizes unlabeled data efficiently. This circumvents the challenge of acquiring ample labeled training data, particularly"}]}