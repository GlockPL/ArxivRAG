{"title": "On shallow planning under partial observability", "authors": ["Randy Lefebvre", "Audrey Durand"], "abstract": "Formulating a real-world problem under the Reinforcement Learning framework involves non-trivial design choices, such as selecting a discount factor for the learning objective (discounted cumulative rewards), which articulates the planning horizon of the agent. This work investigates the impact of the discount factor on the bias-variance trade-off given structural parameters of the underlying Markov Decision Process. Our results support the idea that a shorter planning horizon might be beneficial, especially under partial observability.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has had tremendous success on Atari games (Mnih et al., 2013), yet applications of RL in the real-world remain limited (Dulac-Arnold et al., 2021). This complexity is due to many challenges such as sample efficiency of RL methods (Yu, 2018), risk/safety issues (Gu et al., 2023) and partial observability (Sondik, 1978; Francois-Lavet et al., 2019; Kaelbling et al., 1998). Formulating a real-world problem under the RL framework also involves several non-trivial decisions such as selecting a state/action space (especially when these are continuous), formulating a reward function, and formulating the learning objective (Hare, 2019; Devidze et al., 2021). The learning objective usually corresponds to the discounted cumulative rewards, which depends on a discount factor articulating the considered planning horizon when attributing values to states and actions (Sutton and Barto, 2018). This objective is useful since it can reduce the search space intuitively by giving less credit to futur rewards and actions. In toy and simulated environments, early practitioners tend to use large discount factor values often found in the RL literature on Atari (Kaiser et al., 2024; Mnih et al., 2013). This equates to considering very long planning horizons. On the other hand, real-world applications tend to formulate sequential decision-making problems under the contextual bandit setting (i.e. with a myopic agent w.r.t. the planning horizon) in response to low data regimes (Bastani and Bayati, 2020; Ding et al., 2019; Durand et al., 2018).\n\nThe impact of reducing the planning horizon has been studied previously, and bounds on the resulting bias-variance trade-off on the state value functions have been proposed (Amit et al., 2020; Jiang et al., 2015). Unfortunately, these results provide loose bounds that do not consider the structure of the underlying Markov Decision Process (MDP) and thus fail to capture its impact on the optimal planning horizon. The optimal planning horizon can be described as the discount factor \\( \\gamma \\) which minimizes the planning loss (see Eq. 1) i.e. the one which can extract the best policy possible considering the limited amount of data. Distinct results involving the structure of MDPs (Jiang et al., 2016; Gheshlaghi Azar et al., 2013; Wu et al., 2023; He et al., 2021) have been achieved separately, but these insights have never been brought together.\n\nContributions In this work, we introduce new results on the bias-variance trade-off that explicitly depend on high-level structural parameters of the underlying MDP (Section 2). More importantly, our results touch on Partially Observable MDPs (POMDPs), providing the first insights supporting the advantage of considering short horizons in the learning objective for practical applications un-der partial observability (Section 3). We support and illustrate the theory with numerical results"}, {"title": "2 Fully observable setting", "content": "An MDP can be described as a tuple (S, A, P, R), where S is a finite state space, A is a finite action space, \\(P: S\\times A\\times S \\rightarrow [0, 1]\\) is a transition function, and \\(R : S \\times A\\leftrightarrow [0, R_{max}]\\) is a reward function, with \\(R_{max}\\) denoting the maximal reward obtainable in the MDP. On each time step \\(t\\in \\mathbb{N}_{0}\\), the current state \\(S_t \\in S\\) is observed, an action \\(A_t \\in A\\) is played, the environment transitions into next state \\(S_{t+1}\\) (using P) and generates an observed reward \\(R_{t+1}\\) (using R). Given an MDP (tuple) M, the value of state \\(s \\in S\\) under policy \\(\\pi : S\\rightarrow A\\) is the expected sum of discounted rewards obtained by selecting actions according to policy \\(\\pi\\) from state s:\n\n\n\n\n\n\n\n\n\\[V_{M,\\gamma}(s) = \\mathbb{E}_{\\pi} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+1} | S_t = s \\Big]\\]\n\n\nwhere the discount factor \\(\\gamma\\in [0,1]\\) controls the effective planning horizon (the credit assigned to action \\(A_t\\) for future rewards). The goal of a learning agent is to find the optimal policy \\(\\pi_{M,\\gamma}\\) that maximizes \\(V^{\\pi}_{M,\\gamma}(s)\\) for all states \\(s \\in S\\). We use \\(V^{\\pi}_{M,\\gamma} \\in \\mathbb{R}^{|S|}\\) to denote the vector of state values. A full table containing all the notation in the paper can be found in Appendix A.\n\nBlackwell discount factor Practitioners often believe using a higher discount factor will get a better policy on their specific problem. While this is true with an infinite amount of data, it is rarely the case when building RL applications in practice. It has even been shown previously that there always exists a discount factor in finite MDPs such that the optimal policy cannot be improved by further extending the planning horizon under the Blackwell optimality criterion when \\(|S| < \\infty\\) and \\(|A| < \\infty\\) (Grand-Cl\u00e9ment and Petrik (2023) Thm. 3.2). Above this point, we are in fact only cumulating variance and noise. We refer to the corresponding discount factor as the Blackwell discount factor denoted \\(\\gamma_{Bw}\\). We refer to the planning horizon under discount factor \\(\\gamma_{Bw}\\) as the Blackwell planning horizon. This concept closely resembles the idea of Effective planning horizon in Laidlaw et al. (2023), but with a discount factor instead of a number of steps look ahead.\n\nPlanning loss The planning loss captures the impact of using \\(\\gamma < \\gamma_{Bw}\\) given that the planning is performed in an approximate model of the environment \\(M\\) with \\(\\hat{M} \\approx M^1\\):\n\n\n\n\n\\[ \\|V^{\\pi^{\\hat{M},\\gamma_{Bw}}}_{M,\\gamma_{Bw}} - V^{\\pi^{*}}_{M,\\gamma_{Bw}}\\| \\leq \\underbrace{\\|V^{\\pi^{\\hat{M},\\gamma_{Bw}}}_{M,\\gamma_{Bw}} - V^{\\pi^{\\hat{M},\\gamma}}_{M,\\gamma_{Bw}}\\|}_{bias} + \\underbrace{\\|V^{\\pi^{\\hat{M},\\gamma}}_{M,\\gamma_{Bw}} - V^{\\pi^{*}}_{M,\\gamma_{Bw}}\\|}_{variance}\\]\n\n\n\nThis decomposition offers insight into two components which can affect the quality of the policy obtained when planning on an approximate model of the environment using a shallow planning horizon (\\(\\gamma < \\gamma_{Bw}\\)). The bias denotes the loss in value function (evaluated on the true MDP M and with the Blackwell planning horizon) when using a policy that is optimal with a shallow planning horizon \\(\\gamma\\) instead of using a policy that is optimal with \\(\\gamma_{Bw}\\). On the other hand, the variance captures the impact of optimizing the policy under an approximate model \\(\\hat{M}\\) with a shallow planning horizon and will tend to 0 with more data. This decomposition is different from previous work (Jiang et al., 2015) and has the advantage of being interpretable as a bias-variance trade-off from the PAC-learning literature. We can compare the bias to the approximation error and the variance to the estimation error (Shalev-Shwartz and Ben-David, 2014)."}, {"title": "2.1 Improving the bias bound", "content": "In order to consider the planning horizon in the bias bound of Jiang et al. (2016) (Eq. 2), we introduce the following definitions:\n\nDefinition 3 (Discordant state-action pairs). The set of state-action pairs in an MDP M where two policies \\(\\pi\\) and \\(\\pi'\\) differ:\n\n\n\\[Z_M(\\pi \\neq \\pi') = \\{(s, a) \\in S \\times A : \\pi(s) \\neq \\pi'(s), \\pi'(s) = a\\}.\\]\n\nThis new set will be used to capture the impact of a shallow-horizon policy on the action variation:\n\nDefinition 4 (Horizon-sensitive action variation). The most important difference in transition prob-abilities induced by using discount factor \\(\\gamma\\) instead of discount factor \\(\\gamma_{Bw}\\) on an MDP M:\n\n\n\n\n\\[\\delta_{M,\\gamma} = \\max_{(s,a) \\in Z_M(\\pi_{M}^{\\gamma}, \\pi_{M}^{\\gamma_{Bw}})} \\|P(\\cdot|s, \\pi_{M}^{\\gamma}(s)) - P(\\cdot|s, a)\\|_{1}.\\]\n\nThe implementation of the action variations in proofs is to bound the difference in transition proba-bilities between two different policies. The highest possible bound is given by prior results (Definition 2), but we tighten this result by simply considering states for which the policies are unequal instead of all states. This has the benefit of being 0 when the policy is evaluated with a discount factor above the Blackwell. By building on the analysis of Jiang et al. (2016) we can obtain the following result to characterize the impact of optimizing the policy with a shallow horizon on k-steps transition probabilities."}, {"title": "2.2 Controlling the variance", "content": "We will now introduce new definitions and results to provide a bound on the variance (in Eq. 1). Recall that the variance captures the impact of learning an optimal policy on an (empirical) approx-imation \\(\\hat{M}\\) of a true MDP M when using a shallow planning horizon (\\(\\gamma < \\gamma_{Bw}\\)). To this end, it will be convenient to isolate the variance that does not depend on the shallow planning.\n\nDefinition 5 (Variance due to model approximation). The maximum difference in value-function due to the approximate model \\(\\hat{M}\\):\n\n\n\n\\[ \\epsilon = \\|V^{\\pi}_{M,\\gamma} - V^{\\pi}_{\\hat{M},\\gamma}\\|_{\\infty}\\]\n\nThis term can be upper-bounded into the following results by using known settings in the PAC literature (Gheshlaghi Azar et al., 2013; Wu et al., 2023; He et al., 2021). We can also use the discordant state-action pairs (Def. 3) to capture the action variation resulting from having optimized the policy on an approximation \\(\\hat{M}\\) of a true MDP M.\n\nDefinition 6 (Empirical action variation). The most important difference in transition probabilities when following the policy optimal on an MDP M us the policy optimal on an approximate model \\(\\hat{M}\\):\n\n\n\n\\[ \\delta_{\\hat{M},\\gamma} = \\max_{(s,a) \\in Z_{\\hat{M}}(\\pi_{\\hat{M}}^{\\gamma}, \\pi_{M}^{\\gamma})} \\|P(\\cdot|s, \\pi_{\\hat{M}}^{\\gamma}(s)) - P(\\cdot|s, a)\\|_{1} .\\]\n\nThis improvement over the action variation (Definition 2) is that it will tend towards 0 as \\(\\hat{M} \\approx M\\) which is desirable in a bound on the variance. As was done previously in Section 2.1, we can also build on the analysis of Jiang et al. (2016) to obtain the following result to characterize the impact of optimizing the policy with an approximate model \\(\\hat{M}\\) on k-steps transition probabilities."}, {"title": "2.3 A new bound on the planning loss", "content": "By combining the extended bias bound (Eq. 3) with our novel bound on the variance (Lemma 1), we obtain the following bound on the planning loss (Eq. 1). See Appendix D for the complete proof.\n\nTheorem 1 (Planning loss). Given an MDP M, its Blackwell discount factor \\(\\gamma_{Bw}\\), and an approx-imate model \\(\\hat{M}\\). The planning loss is bounded by:\n\n\n\n\n\\[ \\|V^{\\pi^{\\hat{M},\\gamma_{Bw}}}_{M,\\gamma_{Bw}} - V^{\\pi^{*}}_{M,\\gamma_{Bw}}\\|_{\\infty} \\leq \\kappa_{M,\\gamma} \\Big( \\frac{\\delta_{\\hat{M},\\gamma/2} \\cdot (\\gamma_{Bw} - \\gamma)}{1 - \\gamma_{Bw}} + \\frac{\\delta_{\\hat{M},\\gamma/2}}{1 - \\gamma_{Bw} (1 - \\delta_{\\hat{M},\\gamma/2})} \\Big) + \\Big( \\frac{\\epsilon \\cdot (1-\\gamma)}{1 - \\gamma_{Bw}} \\Big) + \\Big( \\frac{\\epsilon}{1 - \\gamma_{Bw} (1 - \\delta_{\\hat{M},\\gamma/2})} \\Big)\\]\n\nThis result provides insight into how structural parameters affect not only the bias, but also the variance. For instance, a problem with action variation \\(\\delta_{\\mu} \\approx 0\\) has low variance due to the limited impact of the policy over the state value (agent actions do not impact transition probabilities). Similarly to prior work (Jiang et al., 2015), although the applicability of this result is limited by not having access to the true model M, it remains a helpful guide to design heuristics and better understand how one could decide a discount factor. For example, it justifies framing recommender systems as contextual bandits when the outcome of future recommendations do not depend on current recommendations, which translates into a low value-function variation \\(\\kappa_{M,\\gamma}\\). Thm. 1 is tighter than the current existing bound (Jiang et al., 2015) under the following condition on the quality of the model approximation \\(\\hat{M}\\):\n\n\n\n\\[ \\epsilon < \\frac{R_{max}}{1-\\gamma} - \\frac{\\kappa_{M,\\gamma}}{1 - \\gamma_{Bw}} + \\frac{\\delta_{\\hat{M},\\gamma/2}}{1 - \\gamma_{Bw}(1 - \\delta_{\\hat{M},\\gamma/2})} + \\frac{\\delta_{\\hat{M},\\gamma}}{1 - \\gamma_{Bw}(1 - \\delta_{\\mu,\\gamma})} \\Big) . \\]\n\nFig. 1 supports the idea that Thm. 1 becomes tighter than prior results when the variance due to model approximation (Def. 5) is low or when \\(\\frac{R_{max}}{1-\\gamma}\\) is dominant (\\(\\gamma\\) close to 1)."}, {"title": "3 Bias under partial observability", "content": "We now look at how partial observability impacts the structural parameters to better understand its impact on the bias. This is important since most practical problems suffer from a form of partial observability (Dulac-Arnold et al., 2021). We consider a discrete-time POMDP (Sondik, 1978) described by the MDP tuple extended with two elements: a finite set of possible observations \\(\\Omega\\) and the probabilities of receiving each observation given a state, \\(O : S \\times \\Omega \\leftrightarrow [0, 1]\\). On each time step \\(t\\in \\mathbb{N}_{0}\\), the current state \\(S_t \\in S\\) leads the agent to receive an observation \\(O_t \\in \\Omega\\) (using O), an action \\(A_t \\in A\\) is played, the environment transitions into the next (unknown) state \\(S_{t+1}\\) (using P) and generates an observed reward \\(R_{t+1}\\) (using R)."}, {"title": "3.1 Extending structural parameters", "content": "We can extend Definitions 1 and 2 to the POMDP setting by applying them to compressed histories rather than the actual states in the underlying MDP:\n\n\n\n\n\\[\\kappa^{\\Phi}_{M,\\gamma} = \\max_{\\sigma,\\sigma' \\in \\varphi(H)} \\frac{|V^{\\pi^{*}}_{M,\\gamma}(\\sigma) - V^{\\pi^{*}}_{M,\\gamma}(\\sigma')|}{||\\sigma - \\sigma'||} \\]\n\n\n\\[\\delta^{\\Phi}_{M} = \\max_{\\sigma \\in \\varphi(H)} \\max_{\\alpha,\\alpha' \\in A} \\sum_{\\sigma' \\in \\varphi(H)} |P(\\sigma'|\\sigma, \\alpha) - P(\\sigma'|\\sigma, \\alpha')|.\\]\n\nWe introduce the following result showing how the structural parameters in the POMDP relate to the structural parameters of the underlying MDP (see Appendix F):\n\nTheorem 2. Given a POMDP M, let \\(\\kappa_{1}\\), and \\(\\delta_{1}\\) denote the structural parameters (Definitions 1 and 2) evaluated on the underlying state space. Let \\(H(s) = \\bigcup_{t=0}^{\\infty}\\{H_t : b(s|H_t) > 0, H_t \\in H_t\\}\\) denote the set of all histories which can lead to being in state s at any time t, and \\(D_{H(s)}\\) denote its probability distribution. Define \\(A\\) s.t. \\(|V^{\\pi^{*}}_{M,\\gamma}(\\delta) - V^{\\pi^{\\Phi}}_{M,\\gamma}(\\delta)| < A\\forall s \\in S\\), define \\(\\pi_{\\varphi}(s) = \\mathbb{E}_{H \\sim D_{H(S)}}[\\pi^{\\gamma}_{M, \\varphi(H)}]\\) as the optimal policy on compression of histories when executed over the underlying state space and define \\(b(\\sigma) = \\mathbb{E}_{H \\sim D_{H(S)}, \\sigma = \\varphi(H)} b(s|H)\\) for \\(\\sigma \\in \\varphi(H)\\). We have that:\n\n\n\n\n\\[\\delta^{\\Phi}_{M} \\leq \\delta_{M} \\quad and \\quad \\kappa^{\\Phi}_{M,\\gamma} \\leq \\max_{\\sigma,\\sigma' \\in \\varphi(H)} \\frac{||b(\\cdot|\\sigma) - b(\\cdot|\\sigma')||_{1}}{2}(\\kappa_{1} + A).\\\\]\n\nThe first inequality confirms that partial observability impacts negatively the ability for the agent to control state transitions. The second inequality implies that if the policy on the partially observable domain remains good (\\(A \\approx 0\\)), then the state-value variation observed by the agent is lower (since the \\(L_{1}\\) distance is bounded by 2), which could make the learning task easier and as efficient with a low discount factor. This \\(L_{1}\\) distance often has a value of 2, which makes the bound quite loose, but they illustrate the idea that the values of structural parameters decrease when taking a convex"}, {"title": "4 Numerical experiments", "content": "We now conduct a series of experiments to highlight the relationships between the planning horizon, the partial observability, and the structural parameters of the (underlying) MDP. See code online.\n\nRandom MDPS We consider the simulated environment of Jiang et al. (2016). We use 2-actions MDPs, with Fixed(|S|, d) denoting a randomly generated MDP with d > 1 next states reachable from each state. MDPs are sampled using the following procedure: 1) each state-action pair is assigned d possible next states; 2) transition probabilities to these states are sampled uniformly in [0, 1], then normalized; 3) rewards are assigned to state-action pairs by sampling uniformly in [0, 1].\n\nExtension to partial observability We consider the state-abstraction setting (Abel et al., 2016), which corresponds to a specific case of partial observability where the history compressor (H) returns only the last observation and where is a one-hot vector on an observation from \\(\\Omega\\). For simplicity, we make sure that each observation is connected to at least one state. Using Bayes' theorem to recover the belief that the agent is in state s given observation w, we get a constant uniform distribution on every state s which maps onto this observation:\n\n\n\n\n\\[b(s|\\omega) = \\frac{1}{\\|\\{s \\in S : O(\\omega, s) > 0\\}\\|} \\forall s \\in S : O(\\omega, s) > 0, \\forall \\omega \\in \\Omega,\\]\n\nand a belief of 0 otherwise. From this special case of POMDP, we can extract an abstract MDP \\(M_A = (S_A, A, P_A, R_A, \\gamma)\\) from the underlying MDP \\(M = (S, A, P, R, \\gamma)\\) by using (Abel et al., 2016):\n\n\n\n\\[R_A(\\omega, a) = \\sum_{s \\in S}b(s|\\omega)R(s, a)\\]\n\n\n\\[P_A(\\omega, a, \\omega') = \\sum_{s \\in S} \\sum_{s' \\in S} P(s, a, s')b(s|\\omega)O(\\omega', s').\\]\n\nFor our experiments under partial observability, we start by sampling an MDP from Fixed(|S|, d). Then, we can map it onto abstracted MDPs (POMDPs) with different number of observations is |\\(\\Omega\\)|. The number of obervations encodes the level of partial observability. For |\\(\\Omega\\)| = |S|, the problem is fully observable. For |\\(\\Omega\\)| = 1 (and |S| > 1), the agent is completely blind to the state. We sample \\(10^4\\) MDPs with Fixed(10,3) and abstract each MDP into 6 configurations: |\\(\\Omega\\)| \\(\\in\\) \\(\\{10,8,6,4,2, 1\\}\\). The Blackwell discount factors are computed by iterating from \\(\\gamma\\) = 1 to \\(\\gamma\\) = 0 with step size of 0.01 until the optimal policy changes.\n\nFig. 2 (left) shows that the mass of the Blackwell planning horizon tends to decrease as the ob-servability decreases. Since the bias is null when planning with a discount factor larger than \\(\\gamma_{Bw}\\), we only cumulate variance above that point. When |\\(\\Omega\\)| = 1, the myopic agent (\\(\\gamma\\) = 0) enjoys the optimal planning horizon, which corresponds to the bandit setting.\n\nWe also evaluate the normalized bias \\(\\max_{s \\in S} (V^{\\pi_{MB}}_{MB} (s) - V^{\\pi^{*}}_{M,\\gamma} (s))/V^{\\pi^{*}}_{MB} (s)\\) for different planning horizons, averaged over different levels of partial observability. Fig. 2 (right) shows that although the bias decreases when the planning horizon increases, this effect attenuates as the ob-servability decreases. Given that many real-world problems are partially observable, this finding supports the need to consider shallow planning more seriously."}, {"title": "5 Conclusion", "content": "We extended existing structural parameters to consider the planning horizon (Def. 4) and the model approximation (Def. 6). This allowed us to extend an existing bound on the bias (Eq. 3) and propose a new bound on the variance (Lemma 1), which resulted in a new bound on the planning loss (Thm. 1). We finally extended the structural parameters to POMDPs (Eq. 5 and 6) and showed that these are controlled by their fully observable counterparts (Thm. 2). This complements previous results (Abel et al., 2016; Francois-Lavet et al., 2019) by considering the impact of the planning horizon on the bias when shallow planning under partial observability."}]}