{"title": "On shallow planning under partial observability", "authors": ["Randy Lefebvre", "Audrey Durand"], "abstract": "Formulating a real-world problem under the Reinforcement Learning framework\ninvolves non-trivial design choices, such as selecting a discount factor for the learning\nobjective (discounted cumulative rewards), which articulates the planning horizon\nof the agent. This work investigates the impact of the discount factor on the bias-\nvariance trade-off given structural parameters of the underlying Markov Decision\nProcess. Our results support the idea that a shorter planning horizon might be\nbeneficial, especially under partial observability.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) has had tremendous success on Atari games (Mnih et al., 2013), yet\napplications of RL in the real-world remain limited (Dulac-Arnold et al., 2021). This complexity is\ndue to many challenges such as sample efficiency of RL methods (Yu, 2018), risk/safety issues (Gu\net al., 2023) and partial observability (Sondik, 1978; Francois-Lavet et al., 2019; Kaelbling et al.,\n1998). Formulating a real-world problem under the RL framework also involves several non-trivial\ndecisions such as selecting a state/action space (especially when these are continuous), formulating\na reward function, and formulating the learning objective (Hare, 2019; Devidze et al., 2021). The\nlearning objective usually corresponds to the discounted cumulative rewards, which depends on a\ndiscount factor articulating the considered planning horizon when attributing values to states and\nactions (Sutton and Barto, 2018). This objective is useful since it can reduce the search space\nintuitively by giving less credit to futur rewards and actions. In toy and simulated environments,\nearly practitioners tend to use large discount factor values often found in the RL literature on\nAtari (Kaiser et al., 2024; Mnih et al., 2013). This equates to considering very long planning\nhorizons. On the other hand, real-world applications tend to formulate sequential decision-making\nproblems under the contextual bandit setting (i.e. with a myopic agent w.r.t. the planning horizon)\nin response to low data regimes (Bastani and Bayati, 2020; Ding et al., 2019; Durand et al., 2018).\n\nThe impact of reducing the planning horizon has been studied previously, and bounds on the resulting\nbias-variance trade-off on the state value functions have been proposed (Amit et al., 2020; Jiang\net al., 2015). Unfortunately, these results provide loose bounds that do not consider the structure of\nthe underlying Markov Decision Process (MDP) and thus fail to capture its impact on the optimal\nplanning horizon. The optimal planning horizon can be described as the discount factor y which\nminimizes the planning loss (see Eq. 1) i.e. the one which can extract the best policy possible\nconsidering the limited amount of data. Distinct results involving the structure of MDPs (Jiang\net al., 2016; Gheshlaghi Azar et al., 2013; Wu et al., 2023; He et al., 2021) have been achieved\nseparately, but these insights have never been brought together.\n\nContributions In this work, we introduce new results on the bias-variance trade-off that explicitly\ndepend on high-level structural parameters of the underlying MDP (Section 2). More importantly,\nour results touch on Partially Observable MDPs (POMDPs), providing the first insights supporting\nthe advantage of considering short horizons in the learning objective for practical applications un-\nder partial observability (Section 3). We support and illustrate the theory with numerical results"}, {"title": "Fully observable setting", "content": "An MDP can be described as a tuple (S, A, P, R), where S is a finite state space, A is a finite action\nspace, P: S\u00d7A\u00d7S \u2192 [0, 1] is a transition function, and R : S \u00d7 A\u2194 [0, Rmax] is a reward function,\nwith Rmax denoting the maximal reward obtainable in the MDP. On each time step t\u2208 No, the\ncurrent state St \u2208 S is observed, an action At \u2208 A is played, the environment transitions into next\nstate St+1 (using P) and generates an observed reward Rt+1 (using R). Given an MDP (tuple) M,\nthe value of state s \u2208 S under policy \u03c0 : S\u2192 A is the expected sum of discounted rewards obtained\nby selecting actions according to policy \u03c0 from state s:\n\n$V_{M,\\gamma}(s) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}|S_t = s]$\n\nwhere the discount factor \u03b3\u2208 [0,1] controls the effective planning horizon (the credit assigned to\naction At for future rewards). The goal of a learning agent is to find the optimal policy $\u03c0_{M,\u03b3}$ that\nmaximizes $V^{\\pi}_{M,\u03b3}(s)$ for all states s \u2208 S. We use $V_{M,\u03b3} \u2208 R^{|S|}$ to denote the vector of state values. A\nfull table containing all the notation in the paper can be found in Appendix A.\n\nBlackwell discount factor Practitioners often believe using a higher discount factor will get\na better policy on their specific problem. While this is true with an infinite amount of data, it\nis rarely the case when building RL applications in practice. It has even been shown previously\nthat there always exists a discount factor in finite MDPs such that the optimal policy cannot be\nimproved by further extending the planning horizon under the Blackwell optimality criterion when\n|S| < \u221e and |A| < \u221e (Grand-Cl\u00e9ment and Petrik (2023) Thm. 3.2). Above this point, we are in fact\nonly cumulating variance and noise. We refer to the corresponding discount factor as the Blackwell\ndiscount factor denoted \u03b3Bw. We refer to the planning horizon under discount factor \u03b3Bw as the\nBlackwell planning horizon. This concept closely resembles the idea of Effective planning horizon in\nLaidlaw et al. (2023), but with a discount factor instead of a number of steps look ahead.\n\nPlanning loss The planning loss captures the impact of using \u03b3 < \u03b3Bw given that the planning\nis performed in an approximate model of the environment M with M \u2248 M\u00b9:\n\n$||V_{M, \\gamma_{Bw}}^{\\pi_{M, \\gamma_{Bw}}} - V_M^{\\pi_{M, \\gamma_{Bw}}}||_{\\infty} = ||V_{M, \\gamma_{Bw}}^{\\pi_{M, \\gamma_{Bw}}} - V_M^{\\pi_M} + V_M^{\\pi_M} - V_M^{\\pi_M}||_{\\infty} \\le ||V_{M, \\gamma_{Bw}}^{\\pi_{M, \\gamma_{Bw}}} - V_M^{\\pi_M}||_{\\infty} + ||V_M^{\\pi_M} - V_M^{\\pi_M}||_{\\infty}$ \n$||VMYBw\u2212VM\nMYBw ||\u221e\u2264||VMYBw\u2212VM||\u221e+||VM\u2212VM\nvariance\n||\u221e$\n(1)\n\nThis decomposition offers insight into two components which can affect the quality of the policy\nobtained when planning on an approximate model of the environment using a shallow planning\nhorizon (\u03b3 < \u03b3Bw). The bias denotes the loss in value function (evaluated on the true MDP M and\nwith the Blackwell planning horizon) when using a policy that is optimal with a shallow planning\nhorizon y instead of using a policy that is optimal with YBw. On the other hand, the variance\ncaptures the impact of optimizing the policy under an approximate model M with a shallow planning\nhorizon and will tend to 0 with more data. This decomposition is different from previous work (Jiang\net al., 2015) and has the advantage of being interpretable as a bias-variance trade-off from the PAC-\nlearning literature. We can compare the bias to the approximation error and the variance to the\nestimation error (Shalev-Shwartz and Ben-David, 2014)."}, {"title": "Improving the bias bound", "content": "In order to consider the planning horizon in the bias bound of Jiang et al. (2016) (Eq. 2), we\nintroduce the following definitions:\n\nDefinition 3 (Discordant state-action pairs). The set of state-action pairs in an MDP M where\ntwo policies \u03c0 and \u03c0' differ:\n\n$Z_M(\\pi \\neq \\pi') = \\{(s, a) \\in S \\times A : \\pi(s) \\neq \\pi'(s), \\pi'(s) = a\\}$.\n\nThis new set will be used to capture the impact of a shallow-horizon policy on the action variation:\n\nDefinition 4 (Horizon-sensitive action variation). The most important difference in transition prob-\nabilities induced by using discount factor y instead of discount factor YBw on an MDP M:\n\n$\\delta_{M, \\gamma} = \\max_{(s,a) \\in Z_M(\\pi_{M, \\gamma}, \\pi_{M, \\gamma_{Bw}})} ||P(\\cdot|s, \\pi_{M, \\gamma_{Bw}}(s)) - P(\\cdot|s, a)||_1$.\n\nThe implementation of the action variations in proofs is to bound the difference in transition proba-\nbilities between two different policies. The highest possible bound is given by prior results (Definition\n2), but we tighten this result by simply considering states for which the policies are unequal instead\nof all states. This has the benefit of being 0 when the policy is evaluated with a discount factor\nabove the Blackwell. By building on the analysis of Jiang et al. (2016) we can obtain the following\nresult to characterize the impact of optimizing the policy with a shallow horizon on k-steps transition\nprobabilities."}, {"title": "Controlling the variance", "content": "We will now introduce new definitions and results to provide a bound on the variance (in Eq. 1).\nRecall that the variance captures the impact of learning an optimal policy on an (empirical) approx-\nimation M of a true MDP M when using a shallow planning horizon (\u03b3 < \u03b3Bw). To this end, it will\nbe convenient to isolate the variance that does not depend on the shallow planning.\n\nDefinition 5 (Variance due to model approximation). The maximum difference in value-function\ndue to the approximate model M:\n\n$\\epsilon = ||V_M^{\\pi_{M, \\gamma}} - V_M^{\\pi_{M, \\gamma}}||_{\\infty}$.\n\nThis term can be upper-bounded into the following results by using known settings in the PAC\nliterature (Gheshlaghi Azar et al., 2013; Wu et al., 2023; He et al., 2021). We can also use the\ndiscordant state-action pairs (Def. 3) to capture the action variation resulting from having optimized\nthe policy on an approximation M of a true MDP M.\n\nDefinition 6 (Empirical action variation). The most important difference in transition probabilities\nwhen following the policy optimal on an MDP M us the policy optimal on an approximate model M:\n\n$\\delta_{\\hat{M}, \\gamma} = \\max_{(s,a) \\in Z_{\\hat{M}} (\\pi_{\\hat{M}}, \\pi_M)} ||P(\\cdot|s, \\pi_{\\hat{M}}(s)) - P(\\cdot|s, a)||_1$.\n\nThis improvement over the action variation (Definition 2) is that it will tend towards 0 as M \u2248 M\nwhich is desirable in a bound on the variance. As was done previously in Section 2.1, we can also\nbuild on the analysis of Jiang et al. (2016) to obtain the following result to characterize the impact\nof optimizing the policy with an approximate model M on k-steps transition probabilities."}, {"title": "Bias under partial observability", "content": "We now look at how partial observability impacts the structural parameters to better understand\nthe impact on the bias. This is important since most practical problems suffer from a form of partial\nobservability (Dulac-Arnold et al., 2021). We consider a discrete-time POMDP (Sondik, 1978)\ndescribed by the MDP tuple extended with two elements: a finite set of possible observations \u03a9 and\nthe probabilities of receiving each observation given a state, O : S \u00d7 \u03a9 \u2194 [0, 1]. On each time step\nt\u2208 No, the current state St \u2208 S leads the agent to receive an observation Ot \u2208 \u03a9 (using O), an\naction At \u2208 A is played, the environment transitions into the next (unknown) state St+1 (using P)\nand generates an observed reward Rt+1 (using R)."}]}