{"title": "Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review", "authors": ["LIN LI", "XINCHUN YU", "XINYU CHEN", "PENG LIANG"], "abstract": "Public Code Review (PCR) is an assistant to the internal code review of the development team, in the form of\na public Software Question Answering (SQA) community, to help developers access high-quality and efficient\nreview services. Current methods on PCR mainly focus on the reviewer's perspective, including finding a\ncapable reviewer, predicting comment quality, and recommending/generating review comments. However, it\nis not well studied that how to satisfy the review necessity requests posted by developers which can increase\ntheir visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose a\nKnowledge-guided Prompt learning for Public Code Review (KP-PCR) to achieve developer-based code review\nrequest quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically,\nwe reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked\nLanguage Model (MLM) by constructing prompt templates using hard prompt; 2) knowledge and code prefix\ntuning which introduces external knowledge by soft prompt, and uses data flow diagrams to characterize code\nsnippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted\nresults through an answer engineering module. In addition, we further analysis the time complexity of our\nKP-PCR that has lightweight prefix based the operation of introducing knowledge. Experimental results on\nthe PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 8.3%-28.8%\nin the request necessity prediction and by 0.1%-29.5% in the tag recommendation. The code implementation is\nreleased at https://github.com/WUT-IDEA/KP-PCR.", "sections": [{"title": "1 INTRODUCTION", "content": "Code review can be conducted either within a team or by public developers. As an application form\nof traditional code review [10, 23, 31, 32, 36], code review within a team, serves as an essential\nquality assurance mechanism in mature and successful commercial and Open Source Software (OSS)\nprojects [8]. However, due to its labor-intensive nature, code review within the team gradually\nincreases the time and resources spent on review activities as projects become more complex and\nextensive [33, 34]. Such cost requirements slow down the popularity of team code review [19].\nConsequently, modern code review has evolved from a formal and rigorously regulated process [13,\n35] to a less stringent practice [3, 4, 33].\nPublic Code Review (PCR), as an assistant to internal code review within teams, takes the\nform of Software Question Answering (SQA) community to assist developers in exploring high-\nquality and efficient review services [17]. Developers have widely adopted it as a new option\nfor code review. Research has found that the code review process not only helps to improve the\noverall quality of code and software systems [1, 6], identify defects [5], but also promotes the\nexchange of professional and development technical knowledge [35]. As an emerging form of\nservice, in PCR developers publish review requests within a community for all community members\nto view, then the reviewers complete the review service through comments. In the PCR process\nimplemented by the SQA community, request quality assurance for code review service mainly\ndepends on developers. As developers are required and encouraged to submit PCR requests in\nthe community, the necessity of these code review requests is evaluated by public via request\nnecessity prediction [9, 17, 30] subtask after they are posted. Meanwhile, developers always need\nto choose technical terms as review tags. These tags can be used to match requests with suitable\nreviewers via tag recommendation [16, 21, 22, 29, 39] subtask.\nExisting methods on PCR are mostly based on pre-trained language models. However, the\nlanguage models lack the learning of professional knowledge vocabulary in the field of PCR. Early\nwork regarded code snippets as non-linguistic characters that would affect the model's effectiveness,\nfor example, the TagDC method proposed by Li et al. [21] and the PROFIT model proposed by Nie\net al. [29]. These works have treated code snippets as noise and directly discarded code snippets.\nRecent studies have discovered semantic information embedded in code snippets, but these studies\nmostly employ the same processing methods as they do with text [16, 39]. Overall, existing work\nhas not fully utilized the knowledge embedded in code snippets.\nTo this end, we propose Knowledge-guided Prompt learning for Public Code Review (KP-PCR).\nSpecifically, we first design a task-descriptive prompt template that restructures the request necessity\nprediction subtask and the tag recommendation subtask into a generative task through a knowledge-\nguided prompt template, and input the request body (including tags and descriptions) into the\nprompt construction module of a language model. We reformulate both subtasks via 1) text prompt\ntuning which converts two subtasks into Masked Language Model (MLM) by constructing prompt\ntemplates using hard prompt; and 2) knowledge and code prefix tuning which introduces\nexternal knowledge by soft prompt, and uses data flow diagrams to characterize code snippets.\nFinally, the request necessity prediction and recommendation tags output predicted results through\nan answer engineering module. In addition, we delve deeper into the time complexity analysis of\nour KP-PCR with lightweight prefix based operations to introducing knowledge, which reveals\nthat our KP-PCR can improve task accuracy performance without compromising overall efficiency.\nExperimental results demonstrate that our KP-PCR outperforms baselines in terms of accuracy-\nrelated metrics across both subtasks. Furthermore, we explore the impact of prompt learning\nconstruction on the results, and use a case study to compare our KP-PCR with GPT-3.5 and GPT-4.0\nconsidering tag recommendation."}, {"title": "2 MOTIVATING EXAMPLE", "content": "Public Code Review (PCR) is developed in the Software Question Answering (SQA) community.\nResearchers are increasingly exploring ways to achieve high-quality and efficient review services [1,\n5, 6, 17, 35]. A review request consists of several parts, as shown in Fig. 1, including title, request\ndescription (including text and code snippets), quality, and tags.\nSince PCR differs from team internal code review in service form, the quality of the PCR process\nmainly depends on the developers who write and submit the review requests rather than the\nreviewers [22, 39]. Existing research is highly effective for review quality, but there is still a room\nfor improvement. Due to the lack of pre-learning of domain knowledge by pre-trained language\nmodels, existing methods inadequately learn the domain knowledge implicit in request description.\nOne major characteristic of the request body is the abundance of code snippets, which is partic-\nularly significant for PCR. Specifically, for the task of assessing the necessity in code reviews, it\nis required by the rules of PCR websites that the code snippets in the requests should constitute\nrunnable and correct code; otherwise, the code snippets fail to meet the review standards. As for the\ntask of tag recommendation, even large language models that currently excel in various aspects still\nshowed pay attention on professional knowledge tags. These factors affecting task performance are\nclosely related to the professional knowledge contained in both text and code snippet semantics."}, {"title": "3 TASK DESCRIPTION", "content": "In the PCR process conducted by the SQA community, the quality assurance of requests primarily\nrelies on developers. Developers are required to submit necessary PCR requests to the community,\nand the necessity of these requests is assessed publicly via the request necessity prediction\nsubtask after submission. At the same time, developers need to select several technical terms as\nreview tags, which can be used to match requests with appropriate reviewers through the tag\nrecommendation subtask.\nIn this section, we will first describe a public code review process, and then outline how these\nrequest quality assurance subtasks (request necessity prediction and tag recommendation) serve\nthe public code review process. This process consists of five core steps (as shown in Fig. 3), starting\nfrom the submission of changes or optimization requests by the code developer in Step 1, to the\nsubmission of reviews by reviewers in Step 5."}, {"title": "3.1 Public Code Review Process", "content": "First, the public code review process will be introduced through an example of public code review.\nAs shown in Fig. 3, a review process can be abstracted into 5 steps, labeled as 1-5.\nCode review typically involves two roles: the developer who submits the code request and\nthe reviewer who provides suggestions and comments. In Step 1, the developer writes and\nsubmits a review request. In Step \u2461, practitioners in the community review and evaluate the\nrequests to allocate necessary feedback. Subsequently, in Step \u2462, developers are asked to select\nseveral relevant technical terms to accompany their request. These tags are then used in Step \u2463 to\nmatch the request with appropriate reviewers, who provide their feedback on the code in Step 5.\nIn a review request, there are often many changes and optimizations needed in different parts of\nthe code. Reviewers need to spend a significant amount of time reviewing all the changes. However,\nit is often the case that most changes are minor and do not require comments or suggestions. Li\net al. [25] initially considered the task of assessing code change quality in the internal team code\nreview process, which involves predicting whether a code change is of high quality and ready to\nbe accepted in the review process. This work enhances the efficiency and quality of the review\nprocess by pre-evaluating before the request is reviewed. Therefore transitioning to the public code\nreview process, there is a task need for predicting request necessity. Request necessity prediction\nlearns from the existing conventions of public code review and community feedback to determine\nthe standards for submitting review requests, thereby ensuring review quality and reducing the\ntime cost of unnecessary requests.\nThe task of tag recommendation is common in the software engineering community, typically\nused for labeling requests and enhancing visibility. In public code review, reviewers browse request\ncontent and conduct evaluations based on these tags. Therefore, appropriate tag recommendations\ncontribute to improving review efficiency.\nOverall, our task logic will focus on two processes initiated by developers, aiming to optimize\nthe services of public code review. This includes integrating the task of predicting request necessity\nwith Step 2, to provide necessary feedback to practitioners in advance. This can guide them\nto optimize request expressions and improve request visibility while reducing the time spent by\nreviewers in Step 5. Additionally, the tag recommendation task is associated with Step \u2463, allowing\nrequests to be correctly matched with appropriate reviewers based on the usage of tags in the PCR\ncommunity."}, {"title": "3.2 Request Quality Assurance Task Definition", "content": "The effectiveness of the PCR process depends on the interaction between developers and reviewers.\nHowever, this interaction can be time-consuming due to the need to identify issues, find suitable\nreviewers, etc. While current methods focus on reducing the time cost for reviewers, they overlook\nthe impact of developers on the review process. For the PCR process, code review requests that\ncomply with review standards are crucial for high-quality review responses and a conducive\ncommunity atmosphere. To address this issue, this study proposes a quality assurance service\nfor public code review, mainly relying on the request necessity prediction subtask and the tag\nrecommendation subtask.\nWhen considering a request in public code review, it is assumed that there exists a corpus of\npublic code review requests (denoted as R), a set of tag labels (denoted by L), and a set of quality\nlabels for all PCR requests (denoted by Q). Typically, given a public code review request $req_i \\in R$,\nthe request consists of a title $title_i \\in T$, and a description $des_i \\in D$, where the description $des_i$\nincludes both text $text_i$ and code snippets $code_i$.\n(1) The subtask of request necessity prediction. The objective of the request necessity\nprediction is to obtain the function quality that maps r to the necessity label $q \\in Q$. A public code\nreview request $req_i$ corresponds to a necessity label $q_r$. Therefore, the necessity prediction subtask\ncan be regarded as a classification problem. The goal is to determine an optimal function f that\nmaps the necessity label q to a label similar to the actual necessity label $q_r$.\n(2) The subtask of tag recommendation. The objective of the tag recommendation subtask is\nto obtain a function that maps $req_i$ to a set of labels $l = \\{l_1, l_2, ..., l_m\\} \\subset L$ that are most relevant to\nthe request r. A review request $req_i$ corresponds to several labels, where $l_i = \\{l_{r1}, l_{r2}, ..., l_{rk}\\} \\subset l$,\nk represents the number of true labels of the review request. Therefore, the tag recommendation\nsubtask can be viewed as a multi-label classification problem. The goal is to identify an optimal\nfunction $f_{tagrec}$ that maps the label subset l to the true label set $l_r$, as closely as possible.\n(3) Unified task definition. We design a descriptive prompt template for the two subtasks,\nsummarized as: $T(\\cdot) = \\text{``x [MASK]''}$ (where x represents a text string). The input sequence r of\nthe request is passed through this descriptive prompt and refactored into $r_{prompt} = T(r)$. The\nrequest necessity prediction and tag recommendation subtasks are made by filling in the position\nof [MASK]. Our unified task is how to improve the proximity of the predicted filler words against\nthe truth label. We denote the total number of training examples as N, the total number of available\nlabels as M, and the number of labels in the training data as l. The number of MASK is determined\nby the maximum length of the labels in the two subtasks."}, {"title": "4 KP-PCR METHOD", "content": "In this section, we propose a knowledge-guided prompt learning for PCR called KP-PCR to address\nthe issue of insufficient learning of professional knowledge in public code review."}, {"title": "4.1 Framework Overview", "content": "Fig. 4 illustrates the overall framework of KP-PCR which has two modules, knowledge-guided\nprompt learning module and answer engineering module.\nThe knowledge-guided prompt learning module consists of text prompt, code prompt and\nknowledge-guided prompt. First, the request text is input into prompt learning module. The text\nprompt reconstructs the request text into a generative task, and the code prompt refactors the code\nsnippets in text into the form of data flow graph. These processes are described in more detail in\nSection 4.2. The request is then matched with the external knowledge base from Wikipedia to get\nthe knowledge guidance, which is then fed into the model in the form of prefix vectors. Further, the"}, {"title": "4.2 Unified Prompt Template Design", "content": "4.2.1 Text Prompt. This subsection elaborates on the process of restructuring subtasks through\ntask descriptive prompt templates, which involves the following two steps:\n(1) Task Descriptive Prompt Template\nFirst, a descriptive prompt template T (\u00b7) as a hard prompt is applied to map the input request\nbody $req_i = \\{title_i, des_i\\}$ to prompts. The prompt template retains the original tokens from $req_i$\nand includes at least one [MASK] for PLM to fill in label words for both subtasks. Specifically, this\nstudy modifies the descriptive prompt text based on the subtask objectives to reconstruct the data\nfor different tasks.\nThen, by using the descriptive prompt: \u201cThe requested label is [MASK][MASK][MASK]. Is this\nrequest necessary for review? [MASK]\u201d, the tasks of necessity prediction and tag recommendation\nare restructured within the same framework.\nFinally, with a guiding prompt \u201cRequest: \u201d, the model is directed to explicitly recognize the\nrequest content, and concatenate the title $title_i$, description $des_i$ (including text content $text$ and\ncode snippet content $code$), and the template T (\u00b7) together. After concatenation, we obtain a text\ninput $req_i$ for the model, as shown in Equation 1:\n(2) Request Text Encoding\n$Input_i = \\{T(\\cdot), title_i, des_i\\}$  (1)\n[CLS] and [SEP] are special tokens used in BERT-based pre-trained language models. [CLS] is\nused to mark the beginning of a sentence, and the representation vector obtained from the encoder"}, {"title": "4.2.2 Code Prompt", "content": "This section mainly introduces the processing of refactoring code snippets\ninto data flow graph based vector. That is used as part of soft prompt.\nCode snippets inherently possess a graph structure that provides valuable semantic information\nfor code comprehension. To better capture this information, Guo et al. [14] proposed using data\nflow graphs to analyze code and represent its semantic information. In previous research on code\nrepresentation, the parsing of code typically involved using Abstract Syntax Trees (ASTs). In\ncontrast, this graph-based data flow structure effectively avoids the unnecessary depth hierarchy\nthat ASTs may introduce."}, {"title": "4.2.3 Knowledge-guided Prompt", "content": "In order to optimize the code representation and better guide\nthe knowledge embedded in the request text and code snippets, we use the method of continuous\nprompting (soft prompt) [27] to introduce external knowledge. Specifically, we first extract the\nprofessional term from the request using string matching, then initialize the $P_{prefix}$ vector prefix"}, {"title": "4.3 Knowledge-Guided Prompt Learning Module", "content": "4.3.1 Knowledge-Guided Prompt Learning. Soft prompting is one of the prompting paradigms,\nwhich operates in the embedding space of the model and adjusts the prompt parameters for\ndownstream tasks. Currently, this method has demonstrated superior performance on other non-\nnatural language data, such as image encoding and visual question answering [26, 38]. This indicates\nthat soft prompts are well-suited for learning and optimizing information with such graph structures,\nowing to the ability of vector embeddings in the embedding space to better adapt to non-natural\nlanguage structures.\nSince the code snippets and text in public code review requests contain a large amount of domain-\nspecific knowledge, which is not fully learned by traditional language models, this study uses the\nlabels of professional terms as a benchmark. According to the vocabulary of pre-trained language\nmodels, words not in the vocabulary are first selected, and their professional knowledge definitions\nare obtained from Wikipedia. These words are then saved as a separate external knowledge file.\nWhen processing individual request bodies, since tokenization may split words not present in the\nlanguage model into roots and suffixes, words are split based on spaces using string manipulation.\nFurthermore, the split words are subjected to fuzzy casing, tense, and part-of-speech transformations.\nNext, the processed text string is used to extract several domain-specific knowledge words contained\nin each request body through string matching. Finally, the definitions of domain-specific knowledge\nwords are concatenated and their representations are obtained, which is then fed into the model in\nthe form of prefix vectors to assist in learning the knowledge in the request body.\nHere, taking an input request $req_i$ as an example, first, based on the previous data processing,\nwe can obtain two parts of a PCR request:\n1. Natural language text $text_i$, composed of the title and description (text part) of the request;\n2. Code snippet $code_i$, representing the code snippet extracted from the description."}, {"title": "4.3.2 Loss Function", "content": "Fig. 6 formalizes the construction of task inputs through an example request,\nshowcasing the joint construction of task description prompt templates (hard prompt) and prefix\nvectors (soft prompt). It is divided into trainable vector parts and frozen vector parts that are not\ninvolved in training.\nFirst, the obtained text segment vector and code snippet vector are concatenated into the input\nprompt of the Masked Language Model (MLM). Within the entire input $\\hat{c}$is an immutable vector,\nwhile the rest of the vectors are updated based on the gradient optimization target. As shown in\nFig. 6, the vector $c = h_{Graph_c}$ is encoded by the hidden layer of the pre-trained code language model\nis frozen and not updated, whereas the prefix is initialized using the code snippet text. Finally, the\nvector $h_{PREFIX_i}$ can be trained. The dimensions of the code prefix tuning initialization trainable\nmatrix $P_\\sigma$ (parameterized by $\\sigma$) are $|P_{idx}| \\times dim(h_{x_{PREFIX_i}})$ to store the prefix parameters.\nSince multiple [MASK] tokens may appear in the descriptive prompt template, all masked\npositions are considered for prediction. The final learning objective of our KP-PCR framework is to\nconsider multiple [MASK] tokens in a descriptive prompt template, using the sum of losses from\nall [MASK] positions to optimize the model. Specifically, the final learning objective of KP-PCR is\nto maximize Equation 3:\n$\\log P_\\theta (Lable_{pre} |Req) = \\prod_{j=1}^{n} P_\\theta([MASK]_j = \\varphi_j (label_i) | T(req))$,  (3)\nwhere $\\varphi$ is the predicted probability distribution, and $\\varphi_j(label_i)$ represents the mapping from the\nj-th $h_{MASK}$ token to the requested label word $label$ in the input sequence. $\\theta$ denotes the parameters\nof the language model that are updatable. n is the number of masked positions in the descriptive\nprompt template T (req)."}, {"title": "4.4 Answer Engineering Module", "content": "After training the model with Equation 3, the final labels need to be obtained, so the answer\nengineering module maps the predicted words to the label list. The answer engineering module\nmaps the TOPK token list $pre_{list_i}$ to the predicted labels $f_{ans}(pre_{list_i})$ through a mapping function\n$f_{ans}$. The answer engineering function $f_{ans}$ maps the predicted words to the final label set Label.\nFinally, the mapped answer $f_{ans}(pre_{list_i})$ is the final top-k outputs vi. The mapping function of\nthe answer engineering first compares the predicted words with the label word list to find the\ncomplete label by predicting partial label words. Secondly, for data where the predicted words do\nnot exist in the entire label word list, the mapping is completed using the minimum edit distance\nalgorithm. Edit distance, also known as Levenshtein distance, refers to the minimum number of\nediting operations required to transform one string into another. For example, for the strings \u201cif\u201d\nand \"iff\", you can achieve the transformation by inserting or deleting a \u201cf\u201d.\nIn general, the smaller the edit distance between two strings, the more similar they are. If two\nstrings are equal, their edit distance is 0 (no operations are needed). In this study, edit distance is\nused to evaluate the distance between predicted words and true words. If a predicted word is not in\nthe list of true word labels, then by calculating the edit distance between the predicted word and\nall true words, we can find the true word with the minimum edit distance to the predicted word,\nthus correcting the predicted word."}, {"title": "5 EXPERIMENTAL SETUP", "content": "5.1 Research Questions\nOur research goal is to provide quality assurance services for PCR from the perspective of developers.\nTherefore, our method is evaluated from the following three questions."}, {"title": "5.2 Dataset and Metrics", "content": "5.2.1 Dataset Description. To evaluate the effectiveness of our KP-PCR, a dataset on public code\nreview from Stack Exchange is selected, a reputable well-known software information platform.\nThe selection of the dataset follows the criteria of being open, authoritative, and current. Stack\nExchange is a large network of software question and answer (Q&A) websites covering various\ndomains, such as programming, Linux & Unix, and code review. Each website within this network\nfocuses on a specific topic, and the questions, answers, and users are subject to reputation-based\nrecognition within their respective communities.\nIn the field of software engineering, there are other software engineering information platforms\nsuch as Stack Overflow and WordPress. As we are dedicated to providing intelligent service methods\nfor the code review process, the most popular public code review website on Stack Exchange, Code\nReview\u00b9, is used as the data source for this study. The dataset from Code Review used for this\nstudy is the latest and largest original data version available from 2011 to 2023\u00b2. It records the main\ncontent and quality label of PCR requests in separate XML files."}, {"title": "5.2.2 Data Preprocessing", "content": "To align with the model input requirements of the baseline methods in\nSection 5.3.2, we have divided the overall data preprocessing into four steps as detailed below.\n(1) XML File Parsing: There are two common parsing methods for XML files: 1. DOM parsing\nwhich involves converting an XML document into a DOM tree, allowing access to XML document\ninformation through a hierarchical object model, describing the information in a node tree format.\nThis parsing method offers flexible access but may suffer from long processing times and memory\noverflow issues due to machine performance factors. 2. SAX parsing, unlike DOM parsing, operates"}, {"title": "5.2.3 Evaluation Metrics", "content": "For the tag recommendation subtask, previous recommendation tasks in\nthe SQA community [22] have used Precision@k, Recall@k and F1@k to evaluate performance. To\nmaintain consistency, this study adopts the same evaluation metrics. Consistent with the review\nnecessity subtask and the comment quality prediction subtask in the PCR domain [30], Accuracy\nand F1 are selected as metrics. The following will provide a detailed introduction to these evaluation\nmetrics.\n(1) Top \u2013 k Precision Precision@k: This metric is commonly used in recommendation-related\nmethods, measuring the proportion of true labels among the top k labels recommended by a model.\nFor a single request, the calculation of its Top \u2013 k Accuracy is shown in Equation 4 which computes\nPrecision@ki; by calculating the intersection of predicted labels $v_{k_i}$ with the corresponding true\nlabels $T_{tag_i}$ for each request object. Since a public code review dataset contains multiple requests,\nwhen calculating accuracy metrics for the entire dataset, the average accuracy results for all requests\nneed to be taken into account. The specific calculation method is shown in Equation 5 which\nrepresents the average accuracy of all request objects, calculated by first computing Precision@$k_i$\nfor each request object, and then averaging them to represent Precision@k.\n$Precision@k_i = \\frac{|T_{tag_i} \\cap v_{k_i}|}{k}$  (4)\n$Presison@k = \\frac{\\sum_{i=1}^{|R|} Precision@k_i}{|R|}$  (5)\n(2) Top \u2013 k Recall Recall@k: This metric is used to evaluate the model's ability to predict\nthe complete set of true labels. Specifically, it measures the proportion of true labels that are\ncontained in the predicted labels. The Top k Recall for a single request object, Recall@ki, is\ncalculated as shown in Equation 6. In this work, due to the true label length range being 3-5, and\nfollowing the metrics settings of recommendation-related methods, k is set to 3, 5, and 10 in this\nstudy. Similarly, since there are multiple request objects in the public code review dataset, the\nperformance calculation of Recall metric Recall@k for the entire dataset is shown in Equation 7.\nThe average recall rate of all request objects is calculated to represent the model's performance on\nthis metric.\n$Recall@k_i = \\begin{cases}\n\\frac{|T_{tag_i} \\cap v_{k_i}|}{|T_{tag_i}|}, & \\text{if } |T_{tag_i}| < k, \\\\\n\\frac{|T_{tag_i} \\cap v_{k_i}|}{k} & \\text{otherwise}.\n\\end{cases}$  (6)\n\\text{(3) Top - k Macro-Average F1@k: This metric is commonly used to evaluate a model's overall}\nperformance on precision and recall by calculating the weighted average of these two metrics. It\nprovides a balanced view of a model's performance by considering both precision and recall. As\ndescribed in Equation 8, the calculation method for the macro-average F1@$k_i$ of a single request\nobject is similar to the previous two metrics, where k is set to 3, 5, and 10 in this study. Similarly,\nfor the overall macro-average metric F1@k of the public code review dataset, its calculation is\nshown in Equation 9, derived from the average F1 value of all request objects.\n$F1@k_i = 2 \\times \\frac{Precision@k_i \\times Recall@k_i}{Precision@k_i + Recall@k_i}$  (8)\n$F1@k = \\frac{\\sum_{i=1}^{|R|} F1@k_i}{|R|}$  (9)"}, {"title": "5.3 Implementation Details", "content": "5.3.1 Implementation. In practice, the KP-PCR framework can be applied to various pre-trained\nlanguage models, such as BERT [7], RoBERTa [17], ALBERT [37], GPT [17], etc. However, due to\nhardware limitations, this study uses the base version of BERT as the encoding model (BERT-base-\nuncased), and GraphCodeBERT as the code snippet encoder. In the experiments, the learning rate\nof the KP-PCR is set to 1e-5, and the number of epochs is set to 6.\nThis study implements KP-PCR based on the PyTorch framework, and all experimental parameters\nare listed in Table 3. The maximum length of the text is set to 300, the length of the prefix vector is\nset to 50, and the code representation part of the prefix vector uses a code snippet with a length of\n150 to generate the code snippet representation, which is then used for model training and learning\nwith a length of 100. The code snippet text length that is not optimized for model training is 150.\n5.3.2 Baselines. For overall comparisons, a series of state-of-the-art approaches are chosen as base-\nlines for the tag recommendation subtask and the request necessityprediction subtask, respectively.\n(1) The request necessity prediction subtask, CommentBERT [30], MetaTransformer [9]\nand Commentfinder [17] are baseline methods, which have the same data input structure and task\ndefinition as review necessity prediction subtask.\n(2) The tag recommendation subtask, TagDC [21], PROFIT [29], Post2Vec [39], PTM4Tag [16]\nand CDR4Tag [22] are compared for recommending tags to the SQA community."}, {"title": "6 RESULTS", "content": "6.1 Request Necessity Prediction Subtask (RQ1)\nTable 5 shows the KP-PCR experimental results on the request necessity prediction compared with\nsimilarity-based and fine-tuning-based methods.\nIn Table 5 it can be seen that KP-PCR achieves the best results compared to the other four\nbaselines. Overall, the performance of the four methods on the non-necessary label F1 is lower\nthan that on the necessary label F1. Based on actual data from public code reviews, this is because\nrequests for unnecessary reviews appear in more diverse forms, such as asking questions about\ncoding errors (which belong to coding issues rather than code review issues), submitting code that\ncannot be executed (which hinders reviewers from reading the code as a whole), and choosing\ninappropriate labels (which affects the professionalism of the review). In other words, requests that\nmeet the review requirements have clearer expressions, executable code, and accurate identification\nof review issues.\n(1) Compared to baseline methods, KP-PCR shows improved performance in the review\nnecessity prediction subtask. As shown in Table 5, KP-PCR achieves a 0.7%-8.3% performance\nimprovement over the second-best method (UniPCR) by introducing the code snippet optimization\nmodule. This demonstrates the effectiveness of our KP-PCR. By learning the representation of code\nsnippets within the overall content through soft prompts in the prefix vectors, KP-PCR understands\nthe differences in various code snippets under the review necessity criteria. During prediction, the\nrepresentation in the learned code snippet prefix vectors enhances the prediction performance for\nreview necessity, resulting in KP-PCR's improved performance in this subtask."}, {"title": "6.2 Tag Recommendation Subtask (RQ1)", "content": "This subsection compares our KP-PCR with state-of-the-art baselines on the tag recommendation\nsubtask and analyzes the experimental results, in terms of Precision@k, Recall@k and F1@k.\nTag recommendation subtask facilitates users to intuitively and quickly select the correct tags.\nAs introduced in Section 3.2, users on code review websites can select 3-5 tags. Therefore, in the\nevaluation metrics, this study introduces @3, @5, and @10 as evaluation scopes. In addition, based\non the analysis of the time complexity of our KP-PCR in Section 4.3.1, compared to the preliminary\nversion of our study [11", "21": "and PORFIT [29", "39": "and PTM4Tag [16", "22": "designs a matching module for code snippets to\ncapture potential information, resulting in superior performance compared to other methods. This\nmatching module is based on similarity design, completing the task by calculating the similarity\nbetween the code snippet and the request body. However, using this similarity-based approach is\nstill insufficient for learning code snippet representation, as the matching module can only capture\nparts of the code snippet that match"}]}