{"title": "DEEP LEARNING-DRIVEN MALWARE CLASSIFICATION\nWITH API CALL SEQUENCE ANALYSIS AND CONCEPT\nDRIFT HANDLING", "authors": ["Bishwajit Prasad Gond", "Durga Prasad Mohapatra"], "abstract": "Malware classification in dynamic environments presents a significant challenge due to concept\ndrift, where the statistical properties of malware data evolve over time, complicating detection\nefforts. To address this issue, we propose a deep learning framework enhanced with a genetic\nalgorithm to improve malware classification accuracy and adaptability. Our approach incorporates\nmutation operations and fitness score evaluations within genetic algorithms to continuously refine\nthe deep learning model, ensuring robustness against evolving malware threats. Experimental\nresults demonstrate that this hybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach offers a promising\nsolution for real-time malware classification in ever-changing cybersecurity landscapes.", "sections": [{"title": "1 Introduction", "content": "In the field of cybersecurity, the examination of Portable Executable (PE) files plays a crucial role in detecting and\nanalyzing malware. This study introduces a novel method for dynamic PE malware analysis, leveraging advanced\ndeep-learning techniques to enhance classification accuracy and efficiency. Our proposed model employs neural\nnetworks to categorize malware samples into distinct families based on their behaviour and intrinsic characteristics.\n\nA standout feature of our approach is the integration of Genetic Algorithms to manage concept drift, which is a common\nchallenge in malware detection due to the evolving nature of malicious software. Concept drift refers to the changes\nin the statistical properties of the target variable, necessitating continuous adaptation of the model to maintain its\nperformance over time. By incorporating Genetic Algorithms, our classification system not only remains effective but\nalso adapts dynamically to new and emerging threats, ensuring robust and reliable malware detection. This methodology\nunderscores the importance of adaptability and resilience in cybersecurity strategies, paving the way for more advanced\nand responsive defense mechanisms against sophisticated cyber threats.\n\nThis paper aims at exploring the application of deep learning techniques with a combination of genetic algorithm\napproaches for dynamic malware analysis, focusing on n-gram API calls. We aim at leveraging deep learning techniques\nto identify patterns and behaviours in malware samples, enhancing our understanding and classification capabilities\nwith concept drift.\n\nThe remaining sections of the chapter are structured as follows: Section 2 explains the basic concepts of concept drift,\nits types, and the role of genetic algorithms in concept drift handling and sandbox environments. Section 3 discusses\nthe related work on concept drift. Section 4 details the data collection process. Section 5 presents our DL and genetic\nalgorithms-based framework for malware classification, covering preprocessing, NLP, feature selection, malware\nclassification phases, and concept drift handling. Section 6 describes our experimental setup, including obtaining"}, {"title": "2 Basic Concepts", "content": "In this section, we introduce the fundamental concepts and terminologies necessary to understand this paper, focusing\non concept drift, genetic algorithms, and sandbox environments."}, {"title": "2.1 Concept-drift", "content": "Concept drift [1] [2] refers to the phenomenon in which the statistical properties of a target variable, which a model is\ntrying to predict, change over time in unforeseen ways. This can happen for various reasons, such as changes in the\nenvironment, the data source, or the relationships between variables. Concept drift can be categorised into several types\nbased on the nature of the change:\n\n1.  Sudden Concept Drift: This type of drift occurs when there is an abrupt and significant change in the target\nvariable's distribution. It can be caused by events such as a sudden change in user behaviour or a change in the\nunderlying process generating the data.\n\n2.  Incremental Concept Drift: Incremental drift refers to a gradual, continuous change in the target variable's\ndistribution over time. This type of drift can be challenging to detect, as the changes are subtle and accumulate\nover time.\n\n3.  Recurring Concept Drift: Recurring drift occurs when the target variable's distribution follows a recurring\npattern or cycle. For example, seasonal changes in user behaviour or periodic changes in the underlying\nprocess can lead to recurring drift [3].\n\n4.  Concept Drift by Context: This type of drift occurs when the relationship between the input variables and\nthe target variable changes based on the context or conditions under which the data is generated. Contextual\ndrift can be particularly challenging to model, as it requires capturing the contextual information along with\nthe data.\n\n5.  Covariate Shift: Covariate shift refers to a change in the distribution of the input variables while the\nrelationship between the input and target variables remains the same. This type of drift can be addressed by\nreweighting the data to match the new distribution of the input variables [4].\n\nDetecting and adapting to concept drift is essential for maintaining the performance of machine learning models in\ndynamic environments. Various techniques, such as online learning, ensemble methods, and monitoring performance\nmetrics over time, can be used to mitigate the impact of concept drift."}, {"title": "2.2 Techniques to Handle Concept Drift", "content": "Handling concept drift is crucial for maintaining the performance of machine learning models in dynamic environments.\nBelow, we present some several techniques commonly used to address concept drift.\n\n1.  Re-training: Periodically retraining the model using the most recent data can help the model adapt to concept\ndrift. This approach is effective for both sudden and incremental drift.\n\n2.  Ensemble Methods: Ensemble methods, such as bagging and boosting, can improve model robustness to\nconcept drift by combining the predictions of multiple models trained on different subsets of data or with\ndifferent algorithms.\n\n3.  Online Learning: Online learning techniques allow models to be updated continuously as new data becomes\navailable. This approach is well-suited for handling incremental drift.\n\n4.  Change Detection Algorithms: Change detection algorithms, such as Cumulative Sum (CUSUM) and\nPage-Hinkley, can be used to detect concept drift by monitoring the model's performance metrics over time.\n\n5.  Instance Weighting: Instance weighting techniques, such as Importance Weighted Cross Validation (IWCV),\ncan be used to give more weight to recent data instances, helping the model adapt to incremental drift.\n\n6.  Feature Selection and Extraction: Adapting the feature set based on the changing data distribution can help\nimprove the model's performance in the presence of concept drift."}, {"title": "2.3 Genetic Algorithms for Concept Drift", "content": "Genetic algorithms (GAs) [5] can be used to handle concept drift in machine-learning models. Genetic algorithms are\noptimisation algorithms inspired by the process of natural selection and genetics. They can be applied to adapt the\nmodel to changing environments, including concept drift scenarios. Here's how GAs can be used for concept drift:\n\n1.  Feature Selection: Genetic algorithms can be used to dynamically select the most relevant features for the\ncurrent concept. This can help the model adapt to changing data distributions.\n\n2.  Model Selection: GAs can be used to evolve the structure of the model, such as the type of model or the\nhyperparameters, to better fit the current concept.\n\n3.  Ensemble Generation: GAs can be used to create ensembles of models with different structures or parameters,\nallowing the ensemble to adapt to changing concepts.\n\n4.  Hyperparameter Optimization: GAs can be used to optimize the hyperparameters of the model to improve\nits performance in the current concept.\n\n5.  Instance Selection: GAs can be used to select a subset of instances from the historical data that are most\nrelevant to the current concept, helping the model adapt more effectively.\n\nBy using genetic algorithms in these ways, machine learning models can be made more robust to concept drift, allowing\nthem to maintain high performance even as the data distribution changes over time."}, {"title": "2.4 Sandbox", "content": "A sandbox is a security mechanism used in software development, testing, and cybersecurity to isolate running programs\nor processes from the rest of the system. Inspired by children's sandbox play areas, it provides a controlled environment\nwhere untrusted or potentially harmful code can be executed without risking the underlying system [6]."}, {"title": "2.5 Working Principle of a Sandbox", "content": "1.  Resource Control: Sandboxes limit the resources available to processes, such as files, network resources,\nmemory, and CPU usage, to mitigate potential damage from malicious code.\n\n2.  Monitoring and Analysis: Sandboxes include monitoring capabilities to log and analyze system calls, network\nactivity, file operations, and other interactions to detect suspicious behavior.\n\n3.  Dynamic Analysis: Sandboxes use real-time dynamic analysis to detect and respond to threats, identifying\nattempts to exploit vulnerabilities or execute malicious actions [7].\n\n4.  Containment: Upon detecting malicious behavior, sandboxes can terminate the offending process or revert\nchanges to the system to contain the threat."}, {"title": "2.6 Cuckoo Sandbox", "content": "Cuckoo Sandbox\u00b9 [8] [9] is an open-source automated malware analysis system designed to analyse suspicious files\nand URLs within a controlled environment. It utilises a virtualised environment to execute potentially malicious code\nsafely and provides detailed analysis reports for further investigation."}, {"title": "2.6.1 System Requirements", "content": "Before installing Cuckoo Sandbox, we have to ensure that our system meets the following requirements:\n\n\u2022  Availability of Linux-based operating system (e.g., Ubuntu, Debian, CentOS)\n\n\u2022  Adequate hardware resources (CPU, RAM, Disk space) for running virtual machines\n\n\u2022  Availability of Virtualization software (e.g., VirtualBox, VMware, KVM)\n\n\u2022  Python 3.x installed"}, {"title": "3 Related Work", "content": "Chen et al. [10] distinguished between feature-space drift and data-space drift in malware detectors, highlighting the\npredominant influence of data-space drift on model degradation over time. Their findings underscored the necessity\nfor further exploration into the implications of feature-space updates, particularly in the context of Android malware\ndatasets like AndroZoo and EMBER.\n\nJameel et al. [11] conducted a critical review of concept drift's adverse effects on machine learning classification models.\nThey proposed the ACNNELM model as optimal for Big Data stream classification but noted the absence of critical\nparameters for advanced ML models like deep learning. The review also highlighted the lack of a matrix model to\nmeasure adaptability factors, suggesting avenues for future research in model evaluation and optimization.\n\nHashmani et al. [12] presented a systematic literature review on concept drift evolution in machine learning approaches.\nTheir comprehensive synthesis and categorization of existing research provided valuable insights into the state of the\nart in handling concept drift. However, the paper itself did not contribute new methodologies, serving primarily as a\nreference for researchers seeking to understand current trends and challenges in the field.\n\nLu et al. [2] conducted an extensive review focusing on concept drift in machine learning, covering detection, un-\nderstanding, and adaptation strategies across numerous studies. While offering valuable insights into the breadth of\nresearch in this area, the paper did not introduce novel methodologies, functioning primarily as a compilation and\nanalysis of existing approaches.\n\nFarid et al. [13] proposed an adaptive ensemble classifier for mining concept drifting data streams. Their methodology\naddressed the challenge of concept drift by leveraging an ensemble approach, demonstrating promise for real-world\napplications with evolving data streams. However, the effectiveness of their approach was contingent upon the selection\nof appropriate base classifiers, highlighting a potential area for improvement in future research."}, {"title": "4 Data Collection and Preprocessing", "content": "In this section, we discuss the process of data collection from VirusShare [14] and VirusTotal [15]."}, {"title": "5 Proposed Architecture", "content": "In this section, we discuss the architecture and methodology of our proposed model. The proposed model is shown in\nFigure 2. The detailed design of the proposed neural network architectures is presented here. Our approach consists of"}, {"title": "Phase 1: Preprocessing Phase", "content": "In the preprocessing phase, we perform the following activities.\n\n1.  API Dataset & Cuckoo Sandbox Analysis: The process starts with a dataset that undergoes this analysis.\n\n2.  Data Division: The data is divided into training and testing sets, both processed through JSON format.\n\n3.  API Elements Extraction: Various API elements like APICategory, APIName, APIArgument, and APIreturn\nare extracted from the API call sequence as shown in Figure 4."}, {"title": "Phase 2: NLP Phase", "content": "In NLP phase, we perform the following activities."}, {"title": "1. Creating n-grams and Unique n-grams:", "content": "We extracted n-grams from the Cuckoo report (JSON). From\nthese n-grams, we created unique unigram, bigram, and trigram corpora for each class of malware and benign\nsamples."}, {"title": "2. n-grams after Processing:", "content": "The following are the examples of unigrams, bigrams, and trigrams that we have\nused in this paper.\n\n\u2022  Unigram: LdrLoadDll_urlmon_urlmon.dll\n\n\u2022  Bigram: NtAllocateVirtualMemory_na,\nLdrLoadDll_ole32_ole32.dll\n\n\u2022  Trigram: LdrUnloadDll_SHELL32,\nLdrLoadDll_SETUPAPI_SETUPAPI.dll,\nLdrGetProcedureAddress_ole32_OleUninitialize"}, {"title": "3. Calculating TF:", "content": "Term Frequency is applied to transform the text data. It tokenizes text, counts the occurrences\nof each token, and computes TF weights. These weights reflect the importance of each token in a document\nrelative to the entire corpus."}, {"title": "4. Refining Feature Set:", "content": "A refined feature set is obtained after filtering based on frequency."}, {"title": "Phase 3: Feature Selection Phase", "content": "In the feature selection phase, we perform the following activities."}, {"title": "1. Creating API Call Frequency Feature Set:", "content": "Explore features derived from API call frequency to understand\nthe system behaviour and usage patterns."}, {"title": "2. Applying Filter Based Feature Selection:", "content": "Apply filter-based techniques (e.g., mutual information, correlation\nanalysis) to select the most informative features."}, {"title": "3. Refining Feature Set:", "content": "Eliminate redundant or irrelevant features using set hybrid feature selection techniques\nto ensure that the final set is discriminative and predictive."}, {"title": "Phase 4: Malware Classification Phase", "content": "In the malware classification phase, we perform the following activities.\n\n1.  Applying Deep Learning Technique: In this context, deep learning techniques such as ANN, RNN and CNN\nare used for malware classification. Deep learning techniques involve the use of neural networks with multiple\nlayers to learn complex patterns and representations from data. This approach is well-suited for tasks such as\nmalware classification, where the data may have intricate patterns that are difficult to capture using traditional\nmachine-learning algorithms."}, {"title": "Phase 5: Concept Drift Handling Phase", "content": "In the concept drift handling phase, we perform the following activities."}, {"title": "1. Applying Genetic Algorithms on Malware Original Pattern:", "content": "Genetic algorithms are used to evolve and\noptimise solutions to a problem, mimicking the process of natural selection. In the context of malware analysis,\ngenetic algorithms can be applied to evolve or mutate features of malware samples to explore different\ncharacteristics and improve classification or analysis results. The mutation occurs in the sub-parts of API\nsequence features of secondary and tertiary, but the primary remains untouched, as shown in Figure 3 and 5.\nWe have generated 101248 new mutants by applying crossover and mutation on the malware's original pattern\n(Unigrams)."}, {"title": "2. Calculation of Fitness Score:", "content": "The fitness score [16] is calculated as the edit distance from the target string.\nThe formula for calculating the fitness score for an individual in the population is:\n\n$Fitness(individual) = \\sum_{i=1}^{n} |a_i \\neq b_i|$\\newline\n(1)\n\nWhere:\n\n\u2022 Fitness(individual) is the fitness score of the individual.\n\n\u2022n is the length of the target string.\n\n\u2022 $a_i$ is the i-th character of the target string.\n\n\u2022 $b_i$ is the i-th character of the individual.\n\nThis formula calculates the number of positions in the individual where the character does not match the\ncorresponding character in the target string, summing them up to get the total edit distance. We conducted\nfitness score calculations on a dataset comprising 101,248 new mutants. From this pool, we selected 10,500\nmutants, with the top 1500 from each malware category based on their fitness scores as shown in Table 1.\nThese mutants serve as features, adding approximately \u2248 1% to our existing feature corpus."}, {"title": "3. Final Feature Selection using Genetic Algorithm:", "content": "This phase involves the selection of the final feature set,\nwhich consists of the original malware set and the mutated features of malware, as well as benign features.\nThis selection process is crucial for ensuring that the features used for analysis or classification are relevant\nand effective in distinguishing between malware and benign samples."}, {"title": "4. Applying Deep Learning Techniques:", "content": "In this context, the ANN, RNN and CNN architecture used for deep\nlearning are shown in Table 2, 3 and 4. We have used the same ANN, RNN and CNN architecture that is used\nin Phase 4."}, {"title": "6 Implementation and Results", "content": "In this section, we discuss the experiment setup and analysis of the contained result after the experiment."}, {"title": "6.1 Experimental Setup", "content": "Our experimental setup aimed at evaluating the effectiveness of machine-learning techniques in malware classification.\nIt consists of the following components:\n\n1.  Analysis Environment\n\n\u2022  Host OS: We used Ubuntu 18.04 LTS on a machine with an Intel i7 processor, 8GB RAM, and a 10TB\nHDD.\n\n2.  Cuckoo Sandbox\n\n\u2022  Version: Cuckoo Sandbox 2.0.7 was employed for malware analysis on the Ubuntu host.\n\n3.  Malware Samples\n\n\u2022  We collected two lakhs of a diverse set of malware samples representing seven categories: adware,\nbackdoor, downloader, spyware, trojan, virus, and worm.\n\n4.  Windows 10 Environment\n\n\u2022  A separate Windows 10 environment was used with an Intel i7 processor, 128GB RAM, and 5TB storage\nto collect and analyse dynamic analysis reports from Cuckoo Sandbox."}, {"title": "6.2 Experimental Results", "content": "For the concept drift handling using genetic algorithms, we used a total of 22054 samples, including 4410 test samples\nand 17644 train samples, across various malware types as well as Benign samples as shown in Table 5 as dataset\n(Dataset 1). This dataset was collected from VirusShare from January to June 2023 release.\n\nTable 6 and Figure 6 illustrate the performance of the Artificial Neural Network (ANN) model across different\nepochs during training. Initially, the model exhibits a high loss of 1.1005 and a low accuracy of 0.5595 in the first\nepoch. However, as the number of epochs increases, the loss steadily decreases, reaching 0.0761 by the 100th epoch,\naccompanied by a significant increase in accuracy to 0.9799. The validation metrics mirror this trend, with the validation\naccuracy reaching 0.9735 at the 100th epoch.\n\nIn contrast, the Recurrent Neural Network (RNN) model demonstrates different behaviour. As shown in Table 7 and\nFigure 7, the RNN model maintains a relatively constant and high loss, ranging from 1.9359 in the first epoch to 1.6867\nby the 100th epoch, along with a consistent accuracy of 0.3916. Similarly, the validation metrics remain stagnant, with\nthe validation loss at 1.8181 and validation accuracy at 0.3908 throughout training.\n\nThe Convolutional Neural Network (CNN) model shows the most promising results among the three models. As\ndepicted in Table 8 and Figure 8, the CNN model starts with a high loss of 7.8877 in the first epoch but experiences a\nrapid decrease with increasing epochs, reaching 0.0161 by the 100th epoch. The accuracy also improves significantly,\nstarting at 0.7281 and reaching 0.995 by the 100th epoch. The validation metrics follow a similar pattern, with the\nvalidation accuracy peaking at 0.9853 by the 100th epoch.\n\nTo summarize, we can say that the CNN model outperforms the ANN and RNN models in terms of achieving lower\nloss and higher accuracy. The ANN model also shows promising results, with a steady improvement in performance\nwith more epochs. However, the RNN model struggles, showing minimal improvement in loss and accuracy throughout\ntraining. These results suggest that CNN model is most suitable for this task, followed by ANN model, while RNN\nmodel may not be well-suited for this problem, i.e., classifying the malwares with concept drift."}, {"title": "7 Comparison with Related work", "content": "Our work focuses on improving malware classification using NLP-based n-gram API sequence coupled with deep\nlearning and concept drift handling with genetic algorithms. Since we utilise a unique dataset, we lack direct comparisons\nwith existing state-of-the-art techniques. Our approach harnesses the power of genetic algorithms, deep learning and\nn-gram analysis, offering a distinctive perspective on malware detection that can handle concept drift. In the absence of\nany directly related work, we compare our work with some closely related work.\n\nGarc\u00eda et al. [17] proposed a technique to assess the effectiveness of transfer learning (TL) methods for malware\nclassification in the presence of concept drift, focusing on various time periods and learning scenarios. The study\nutilised five TL algorithms\u2014TrAda, CORAL, DAE, DTS, and TIT\u2014 and evaluated their effectiveness in handling\nconcept drift in malware classification. TrAda, CORAL, and DAE were identified as the most effective TL algorithms,\nconsistently achieving Matthews correlation coefficients (MCC) greater than 0.9. Among the machine learning (ML)\nalgorithms, Random Forest (RF) demonstrated competitive performance, especially in the inductive TL setting. The\ndataset used in the study was not explicitly disclosed by the authors, making the exact source of the malware and benign\ndataset unclear.\nFernando and Komninos [18] introduced the feSAD ransomware detection framework, which leverages machine\nlearning to adapt to concept drift. The framework aims at enhancing ransomware detection rates by calibrating drift\nthresholds and identifying abnormal drift samples. Compatible with various machine learning algorithms, it has\ndemonstrated effectiveness in detecting ransomware amidst concept drift and can be tailored for different malware types.\nExperimental results, including 720 ransomware samples and 2000 benign samples from the Elderan dataset, show high\ndetection rates and precision, especially with the Random Forest algorithm. Komninos highlighted the Random Forest's\nstability in ransomware detection. However, they noted that in test batch 2, a high volume of ransomware samples\ncaused abnormal concept drift, leading to reduced detection rates and statistical drift, suggesting system retraining to\navoid detection decline."}, {"title": "8 Threats to Validity", "content": "There are some potential threats to the validity of the proposed model and its results. We discussed them below."}, {"title": "Internal Validity", "content": "\u2022  Algorithm Performance: The effectiveness and the performance of the genetic algorithm approach for concept\ndrift handling could be influenced by the specific parameters and configurations chosen, which may impact the\nresults."}, {"title": "External Validity", "content": "\u2022  Concept Drift Representation: The concept drift handling approach may not fully capture the complexity and\ndynamics of concept drift in real-world malware datasets, affecting the effectiveness of the proposed approach\nin practical scenarios."}, {"title": "Construct Validity", "content": "\u2022  Model Architecture: The specific architecture of the neural network used for classification may not be optimal\nfor handling concept drift or may not fully leverage the additional features added through genetic algorithms."}, {"title": "Conclusion Validity", "content": "\u2022  Evaluation Metrics: The evaluation metrics used (e.g., accuracy, loss) may not fully capture the effectiveness\nof the approach in handling concept drift and distinguishing between malware and benign samples, potentially\nleading to biased conclusions.\n\nAddressing these threats involves conducting thorough experiments with diverse datasets, carefully selecting parameters\nand configurations for the genetic algorithm and neural network, and considering the implications of concept drift on\nthe model's performance and generalizability."}, {"title": "9 Conclusion and Future Work", "content": "In this paper, we presented a deep learning approach for dynamic PE malware analysis, focusing on handling concept\ndrift. Our model includes the phases of preprocessing, NLP processing, feature selection, malware classification, and\nconcept drift handling.\n\nWe used API call frequency features for initial selection and added 10,500 features using genetic algorithms, improving\nthe model's ability to distinguish malware from benign samples and handle concept drift.\n\nOur neural network has an input layer with 88,972 neurons, three hidden layers, and an output layer with eight neurons\nfor multi-class classification. ReLU activation and Adam optimizer were used for training, resulting in high accuracy\nand low loss.\n\nWe calculated fitness scores for 101,248 new mutants, selecting the top 10,500 for feature augmentation, increasing the\nfeature corpus by approximately 1%.\n\nExperimental results showed significant improvements in classification accuracy and loss reduction across a number of\nepochs and datasets, indicating the potential of our approach for enhancing malware detection and classification in\ndynamic PE environments, with concept drift handling."}, {"title": "Scope for Future Research", "content": "Below we present some future scope of our research work.\n\n\u2022  Explore the application of other deep learning approaches, such as Long Short Term Memory Networks\n(LSTMs) and Generative Adversarial Networks (GANs), to broaden the scope of the research and potentially\nuncover new insights.\n\n\u2022  Investigate the use of genetic algorithm approaches for feature selection and feature creation to enhance the\nrobustness and accuracy of the models.\n\n\u2022  Experiment with transfer learning techniques to leverage pre-trained models for malware classification tasks,\npotentially improving performance and reducing training time.\n\n\u2022  Conduct cross-domain analysis by applying the developed models to different types of malware datasets,\nassessing their generalizability and adaptability to diverse malware threats."}, {"title": "10 Code and Dataset Availability", "content": "The research code is available on GitHub at the following link: https://github.com/bishwajitprasadgond/\nMalClassCD. For access to the dataset used in this research, please send a request via email to bishwajitprasadgond@\ngmail.com."}, {"title": "11 Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have\nappeared to influence the work reported in this paper."}]}