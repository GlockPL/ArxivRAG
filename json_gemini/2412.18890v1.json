{"title": "CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models", "authors": ["Ping Guo", "Qingfu Zhang", "Xi Lin"], "abstract": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence, capable of processing and understanding extensive human knowledge to enhance problem-solving across various domains. This paper explores the potential of LLMs to drive the discovery of symbolic solutions within scientific and engineering disciplines, where such solutions are crucial for advancing theoretical and practical applications. We propose a novel framework that utilizes LLMs in an evolutionary search methodology, augmented by a dynamic knowledge library that integrates and refines insights in an open-ended manner. This approach aims to tackle the dual challenges of efficiently navigating complex symbolic representation spaces and leveraging both existing and newly generated knowledge to foster open-ended innovation. By enabling LLMs to interact with and expand upon a knowledge library, we facilitate the continuous generation of novel solutions in diverse forms such as language, code, and mathematical expressions. Our experimental results demonstrate that this method not only enhances the efficiency of searching for symbolic solutions but also supports the ongoing discovery process, akin to human scientific endeavors. This study represents a first effort in conceptualizing the search for symbolic solutions as a lifelong, iterative process, marking a significant step towards harnessing AI in the perpetual pursuit of scientific and engineering breakthroughs. We have open-sourced our code and data, please visit https://github.com/pgg3/CoEvo for more information.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) constitute a significant breakthrough in artificial intelligence (AI), demonstrating remarkable proficiency across various research domains [1, 2, 3]. Their capacity to process and comprehend a wide range of human knowledge significantly contributes to intricate problem-solving tasks. Initially,LLMs assist in extracting critical information from various contexts for inspiring innovative ideas among researchers [4, 5, 6]. Furthermore, recent advancements aim to endow LLMs with fully automated problem-solving capabilities, including operating as autonomous agents in physical environments [7], generating scoring heuristic functions [8, 9], executing automatic prompt engineering for jailbreak attacks [10, 11], and conducting autonomous machine learning [12, 13]. Given these advancements, we believe that LLMs can be deployed in a fully automated manner for developing symbolic solutions to complex systems.\nSymbolic systems play a vital role in addressing numerous challenges across diverse scientific and engineering disciplines [14, 15, 16, 17]. In science, they facilitate knowledge transfer between related fields, promoting the development of mathematical models that can contribute to formulating new theories and hypotheses, thereby expediting scientific discovery [18, 19]. Similarly, in engineering, symbolic solutions and their foundational knowledge are instrumental in the design of complex systems. For instance, in circuit design, smaller modules are encapsulated as Intellectual Property (IP) blocks, each serving distinct and specialized functions. These blocks are then assembled to construct larger, more sophisticated systems [20, 21].\nExpanding on these foundations, we suggest that the discovery of symbolic solutions significantly benefits from an open-ended process where solutions and foundational knowledge continually co-evolve, akin to the work of human scientists and engineers. As such, an endless process holds many possibilities for new symbolic solutions that contribute to both scientific and engineering domains. It involves endless exploration and adaptation, fostering the emergence of innovative ideas that may be unattainable through conventional methods [22]. In the field of artificial intelligence, open-ended exploration has led to breakthroughs by establishing environments in which algorithms endlessly generate and refine solutions without the constraints of specific, predefined goals [23, 24]. This paradigm not only accelerates"}, {"title": "2. CoEvo", "content": "the development of complex systems but also mirrors the iterative essence of human creativity and scientific discovery, where each new discovery leads to further questions and explorations [25].\nDespite these insights, no existing studies explicitly tackle the challenge of open-ended evolution and refinement of symbolic solutions. The primary obstacles hindering progress in this area are two-fold: 1) the difficulties of conducting efficient searches on the symbolic representation spaces, which are often NP-hard across multiple domains [26], and more crucially, 2) the absence of methodologies for leveraging existing knowledge and newly generated knowledge to guide the search. While there have been developments in evolutionary search algorithms for symbolic regression [27] and deep learning approaches [28, 29], these primarily address the former challenge in different encoding spaces and overlook the latter, which is vital for open-ended innovation.\nThis gap could potentially be bridged by employing LLMs, which inherently integrate baseline human knowledge [30]. However, even with the implementation of cutting-edge retrieval-augmented generation (RAG) techniques [31], an important question remains:\nCan LLMs actually uncover new knowledge rather than merely replicate existing information? In addition, can LLMs summarize knowledge to support an open-ended search for symbolic solutions akin to human scientific endeavors?\nIn our study, we conceptualize the search for symbolic solutions as an endless, lifelong process, guided by both existing knowledge and the newly generated knowledge during the search. We propose an LLM-based evolutionary search methodology, augmented by a knowledge library that contains the summarized knowledge from the search process. Throughout this exploration, the LLM interacts with the knowledge library to create novel solutions in multiple spaces, including language, code, mathematical expressions, and logic expressions etc.. Our approach resolves the challenge of efficient search by navigating multiple search contexts, and our knowledge library serves as the first trial in answering the question of whether LLMs can discover new knowledge and apply it in an endless search for symbolic solutions. To the best of our knowledge, this represents the pioneering effort to view the discovery of symbolic solutions as a continual open-ended process.\nOur contributions are as follows:\n\u2022 We believe that finding symbolic solutions in domains such as scientific"}, {"title": "2.1. Overview", "content": "This section presents an overview of CoEvo, a framework specifically designed to support a continual open-ended search process for symbolic solutions to a myriad of problems, as illustrated in Figure 1. The CoEvo framework incorporates three distinct elements: 1) a versatile representation of solutions, 2) a tree-based knowledge discovery mechanism, and 3) an evolutionary search methodology equipped with a knowledge repository. Specifically, the framework first defines the solution to the task in a general style, allowing for different formats to enable search in different spaces. Secondly, for the generation of these solutions, we adopt a multi-phased reasoning process with the tree-based idea (knowledge pieces) search process. Finally, the solutions and the discovered ideas co-evolve through the evolutionary search process augmented by a knowledge repository."}, {"title": "2.2. Representations of Solutions.", "content": "For any given task, employing various solution formats can facilitate the concurrent exploration of multiple representation spaces. Prior research has demonstrated the effectiveness of conducting parallel searches across natural language and Python code spaces, particularly in code generation tasks [32, 8, 33]. To illustrate, we present examples of several representative formats:\n\u2022 Natural Language: The basic concept of LLMs are built on the fundamental linguistic domain [34]. This domain is well within the operating capability of LLMs and is central to related research, such as the study of jailbreak attacks[10, 11]. Additionally, language's intimate connection to human intelligence has been noted [35, 36]."}, {"title": "2.3. Tree-based Idea Search Process", "content": "To define our solution generation strategy, we first analyze the human problem-solving process as shown in Figure 2(a). Typically, humans generate initial ideas upon encountering a task, then validate these ideas through reasoning. Successful validation leads to implementation and subsequent feedback collection, which refines and iterates the approach.\nThis human-centric methodology aligns with two key insights from the LLM research domain: 1) the Reason-and-Act approach [7], which mirrors"}, {"title": "2.4. Knowledge Library-enhanced Evolution Process", "content": "To enable continual evolution and manage the ideas we have discovered, we adopt a knowledge library-enhanced evolution process for final solution generation, as shown in Figure 1. This process comprises two critical com-"}, {"title": "2.4.1. Evolution Process", "content": "Our implementation of the evolution process adheres to the standard evolutionary steps: initialization, crossover, mutation, and population management.\nInitialization. We initiate by randomly generating a set of N solutions as the initial population using the tree-based idea search process in \u00a72.3.\nCrossover. We employ two crossover operators\u2014positive-crossover and negative-crossover. Positive-crossover promotes the generation of solutions similar to parent ideas, whereas negative-crossover fosters the creation of distinctly different solutions, enhancing solution diversity as validated in related research [32, 11, 8].\nMutation. Similarly, mutation of CoEvo is facilitated by positive-mutation and negative-mutation operators, designed to either reinforce or diverge from the existing solution traits.\nPopulation Update. We maintain the top N solutions with the highest scores as the population for the next generation, ensuring a quality-driven evolution."}, {"title": "2.4.2. Knowledge Library", "content": "The knowledge library, as illustrated in Figure 3, is central to our continual evolution process. It interacts dynamically with the LLM during each phase of the evolution process, including initialization, crossover, mutation, tree-based idea search phase, enhancing the generation and refinement of ideas. Specifically, it involves three core designs: 1) idea summarization, 2) idea management, and 3) idea reuse.\nIdea summarization. Idea summarization occurs when solutions improve the score during the tree-based search and offspring generation processes. The LLM is asked to analyze the improved performance and store the effective idea into the knowledge library.\nIdea management. The knowledge library is managed by another LLM, who is responsible to organize the knowledge library and retrieve the knowledge when needed. Specifically, it computes the sentence embeddings for the ideas and cluster them into different clusters. The clustering is done based on the cosine similarity of the sentence embeddings."}, {"title": "3. Experiments", "content": "discovery should be perceived as a continual open-ended process. We make a first attempt to extract knowledge and apply it in the endless search for symbolic solutions for scientific and engineering challenges.\n\u2022 With this understanding, we propose a framework for harvesting and applying knowledge to search for symbolic solutions in multiple search spaces.\n\u2022 Our extensive experimental outcomes demonstrate that incorporating the newly generated knowledge enables a lifetime of continual searching for symbolic solutions in scientific and engineering domains."}, {"title": "3.1. Experimental Setup", "content": "Backbone LLMs. In our experiments, we employ gpt-3.5-turbo and gpt-4o-mini as the backbone LLMs. The gpt-3.5-turbo model, as per OpenAI's documentation, has a knowledge cutoff of September 2021 [42]. In contrast, the gpt-4o-mini model's knowledge extends up to October 2023 [43]. Throughout this paper, the term gpt-4 is used interchangeably with gpt-4o-mini for brevity.\nBenchmark and Datasets. The AI Feynman benchmark [44], consists of 120 physics problems and represents the current standard for evaluating symbolic regression methods in the discovery of scientific equations. However, employing this benchmark directly can result in mere rote recitation of information, a limitation highlighted in recent work [30]. Therefore, we"}, {"title": "3.2. Overall Performance", "content": "adopted the four problems introduced in [30] to evaluate the performance of our proposed method.\nBaseline Algorithms. In our comparative analysis, we incorporate leading symbolic regression (SR) techniques in evolutionary search area, including GPlearn\u00b9 and PySR [45]. Additionally, for deep learning-driven methods, our study encompasses DSR [46], uDSR [47], NeSymReS [48], and E2E [29]. Within the domain of LLM-based approaches, we feature LLM-SR, the most current advancement in LLM-driven symbolic regression.\nThe results of non-LLM based methods are directly reported from [30]. For LLM-based techniques, we permit up to 2,000 LLM queries, inclusive of all tree-based search endeavors and evolutionary operations, while excluding queries generated by summarization processes."}, {"title": "Superior performance.", "content": "The overall performance of various methods on problems is summarized in Table 1. Performance metrics are evaluated using the Normalized Mean Squared Error (NMSE), with lower values indicating superior performance. Our approach consistently outperforms contemporary LLM-based regression methods across all tested benchmarks with two different LLM backbones. Notably, it achieves this with an equivalent or reduced number of queries, as shown in Figure 4."}, {"title": "Performance Comparison Between Backbones.", "content": "Despite the more advanced capabilities of GPT-40-mini compared to GPT-3.5, our findings indicate no significant performance disparity between them. This demonstrates"}, {"title": "Insights on Oscillation 2.", "content": "that our method effectively leverages GPT-3.5 to achieve results comparable to or better than those using the newer model. In addressing the Oscillation 2 problem, our method achieves extremely low NMSE values. Further analysis reveals that applying temporal differentiation to velocity data is an effective solution, highlighting our approach's robustness. This finding, while validating our method's efficacy, also suggests potential issues with the benchmark's design. Future work may involve modifications to this benchmark to better test and validate symbolic regression methods."}, {"title": "3.3. Search Efficiency.", "content": "This section evaluates the search efficiency of our approaches compared to simple LLM queries by examining the historical NMSE on training data and the ratio of valid solutions found. These metrics illustrate the convergence behavior of the algorithms and the nature of solutions explored by different methods.\nConvergence Figure 4 plots the historical NMSE across different benchmarks for both our methods and LLM-SR. While initial convergence rates are comparable, our methods demonstrate superior long-term efficiency, achieving lower NMSE values more consistently across iterations.\nExploration. The exploration capabilities of our methods compared to LLM-SR are depicted in Figure 5. Our approaches consistently generate a"}, {"title": "3.4. Knowledge Extraction and Application", "content": "higher ratio of valid solutions across all benchmarks. This improvement is attributed to two main factors: 1) the integration of tree-based reasoning and action processes, which enhances the generation and refinement of valid solutions, and 2) the employment of offspring operators that utilize existing solutions as starting points, thus increasing the likelihood of producing valid outcomes. These strategies contribute significantly to the robustness and effectiveness of our search process, particularly under complex problem conditions like those presented in the Oscillation 2 and Stress-Strain tests.\nThis section evaluates the impact of knowledge on the quality of solutions generated by our models. We categorize the influence of knowledge into three types: 1) positive, 2) negative, and 3) neutral. The effects of these influences are depicted in Figure 6."}, {"title": "3.4.1. Positive Effect.", "content": "Significant improvements are observed in problems such as Oscillation 1, Oscillation 2, and E. coli growth, where the application of relevant knowledge leads to substantial reductions in error rates. To quantify these effects, we compare 50 solutions generated with and without specific knowledge inputs for Oscillation 1 and E. coli growth. The results, shown in Figure 7, indicate that solutions crafted with knowledge consistently outperform those without, exhibiting lower mean errors and reduced variability.\nKnowledge and Search Space. In simpler scenarios like Oscillation 1, while knowledge aids in reducing error, it appears to restrict the exploration space, leading to a lower variance in solutions but not necessarily impairing the overall solution quality.\nBetter Knowledge v.s. Better Models In the context of the E. coli growth problem, superior knowledge proves more beneficial than advanced"}, {"title": "3.4.2. Negative Effect.", "content": "model architectures. This observation underscores the importance of quality knowledge in enhancing model performance, as demonstrated by the distinct improvement in solutions when informed by more accurate knowledge.\nComprehension of Knowledge. Stronger models such as GPT-4o-mini show a greater sensitivity to the input knowledge compared to GPT-3.5, indicating that more advanced models can better leverage detailed and complex knowledge to improve performance. The only observed negative impact occurs in the Oscillation 2 problem, where the inclusion of specific, misleading knowledge from numpy.gradient adversely affects the solutions. An analysis of the knowledge library during iterations 500 to 1500 confirms the presence of this detrimental knowledge (Figure 8)."}, {"title": "3.4.3. Knowledge Pollution.", "content": "Figure 8 illustrates the pollution within the knowledge pool for Oscillation 2, where irrelevant or incorrect knowledge entries, such as those related to numpy.gradient, have accumulated. so that many useless knowledge for performance improvement are summarized in the pool false. This contamination significantly hampers the model's performance by steering the solution generation process towards less effective outcomes."}, {"title": "4. Conclusion", "content": "We proposed the idea of open-ended optimization process and conducted extensive experiments on four scientific benchmarks. The results have shown that with knowledge extracted, LLMs are able to conduct endless explorations over the solutions."}]}