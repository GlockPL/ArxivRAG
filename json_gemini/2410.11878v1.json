{"title": "Neural Metamorphosis", "authors": ["Xingyi Yang", "Xinchao Wang"], "abstract": "This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in a way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add a noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at a 75% compression rate.", "sections": [{"title": "Introduction", "content": "The world of neural networks is mostly dominated by the rigid principle: Once trained, they function as static monoliths with immutable structures and parameters. Despite the growing intricacy and sophistication of these architectures over the decades, this foundational approach has remained largely unchanged. This inherent rigidity presents challenges, especially when deployed in new scenarios unforeseen during the network's initial design. Each unique scenario calls for a new model of distinct configuration, involving repeated design, training, and storage processes. Such an approach is not only resource-intensive, but also limits the model's prompt adaptability in rapidly changing environments."}, {"title": "Related Work", "content": "Efficient Deep Neural Networks. In recourse-limited applications, the efficiency of neural networks becomes a critical concern. Researcher have explored structure pruning methods that trim non-essential neurons to reduce computation. Another development is the flexible neural networks, which offer modifiable architectures. These networks are trained on various subnetwork setups, allowing for dynamic resizing. In our context, we present a new type of flexible model that directly learns the continuous weight manifold. This allows it to generalize to configurations that haven't been trained on, an infeasible quest for existing approaches.\nContinuous Deep Learning. Beyond traditional neural networks that discretize weights and outputs, continuous deep learning models represent these elements as continuous functions . The concept extends to neural networks with infinite hidden width, modeled as Gaussian processes . Further"}, {"title": "Implicit Representation on Weight Manifold", "content": "In this section, we introduce the problem setup of NeuMeta and present our solution. In short, our idea is to create a neural implicit function to predict weight for many different neural networks. We achieve this goal by posing the principle of smoothness in this implicit function."}, {"title": "Problem Definition", "content": "Let's imagine the world of neural networks as a big space called F. In this space, every neural network model $f_i \\in F$ is associated with a set of weight $W_i = \\{W(i,j)\\}$. Such a model $f_i$ is uniquely identified by its configuration $i \\in I$, such as width (channel number) and depth (layer number). Furthermore, each weight element within the network is indexed by $j\\in J$, indicating its specific location, including aspects like the layer index and channel index. The combination of configurations and indices, $I \\times J$ forms the model space, uniquely indexing each weight. We say the all weights values that makes up a good model on a dataset D lies on a weight manifold W. We also assume we have access to a pretrained model $f(\\cdot; W_{original})$. Our goal is to learn the weight manifold W.\nDefinition 1. (Neural Metamorphosis) Given a labeled dataset D and a pretrained model $f (\\cdot; W_{original})$, we aim to develop a function $F : I \\times J \\rightarrow W$ that maps any points in the model space to its optimal weight manifold. This is achieved by minimizing the expected loss across full $I \\times J$.\n\\begin{equation}\nmin_{F} E_{(i,j)\\in I\\times J} [\\mathcal{L}_{task}(f_i(W); D)], \\quad s.t. W = \\{w(i,j)\\}_{(i,j) = F(i,j)},\\label{eq:main}\n\\end{equation}\nwhere $\\mathcal{L}_{task}$ denotes the task-specific loss function. In other words, F give us the best set of weights for any model setup in $I\\times J$, rather than fitting a single or a set of neural networks . In context neural network, our F, which inputs coordinates and outputs values, is known as implicit neural representation (INR) . Consequently, we choose INR as our F, offering a scalable and continuous method learn this mapping.\nConnecting to Continuous NN. NeuMeta can be viewed as a method to build Continuous NNs, by representing weight values as samples from a continuous weight manifold. Here, we would like to see how it differs from existing methods. For example, in continuous-width NNs, linear operations are typically defined by the Riemann integral over inputs and weights:\n\\begin{equation}\nf(x) = XW = \\sum_j x_j \\Delta_j W(j) \\approx \\int x(j)W(j) dj,\n\\end{equation}\nwhere x and w represent discrete input and weight vectors. j is the continuous-valued channel index. W(j) is a continuous weight function, and $\\Delta_j$ is the width of the sub-interval between sampled points for integral.\nOur method offers three key advantages over this traditional approach:\nIntegral-Free. NeuMeta requires no integral.\nLearned Continuous Sampling. Our method jointly learns the continuous weight function and the sampling interval $w_j = \\Delta_jW(j)$, rather that learning W(j) along. This enables us to generate continuous-width NN on-fly, a feat unachievable with discrete learned sampling .\nINR Parameterization. INR offers a generalized form to model the continuous function."}, {"title": "Network Architecture", "content": "At the core of NeuMeta, we employ an INR model, $F(\\cdot; \\theta) : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, to parameterize the weight manifold. This function, based on a multi-layer perceptron (MLP), transforms model space into weight values. In our implementation, we set the parameter k = 6. For convolutional networks, the dimension $d = K \\times K$, the maximum kernel size, whereas for non-convolutional setups, d = 1.\nConsidering a generalized network with L layers, each layer with an input-output channel size of $(C_{in}, C_{out})$. Each weight element, $w_{(i,j)}$, is associated with a unique index within the network. This index is represented as a coordinate pair (i,j), with $i = (L, C_{in}, C_{out})$ denoting the network structure and $j = (l, c_{in}, c_{out})$ indicating the its specific layer, input, and output channel number. To ensure the same coordinate system is applicable to all (i, j), these raw coordinates undergo normalization, typically rescaling them with a constant N\n\\begin{equation}\nv = \\left[\\frac{C_{in}}{N}, \\frac{C_{out}}{N}, \\frac{L}{N}, \\frac{c_{in}}{N}, \\frac{c_{out}}{N}, \\frac{l}{N} \\right]\n\\end{equation}\nSimilar to prior technique , the normalized coordinates undergo a transformation through sinusoidal position embedding, to extract its Fourier features.\n$\\Phi(\\nu) = [sin(2^0 \\pi \\nu), cos(2^0 \\pi \\nu), ..., sin(2^{q-1} \\pi \\nu), cos(2^{q-1} \\pi \\nu)]$\nThese encoded Fourier features, $\\Phi(\\nu)$, serve as inputs to the MLP, yielding the weights:\n\\begin{equation}\nw_{(i,j)} = F(\\Phi(v); \\theta) = MLP(\\Phi(v);\\theta) \\cdot C_{in}\n\\end{equation}\nIn equation (4), the output of the MLP is scaled by the number of input channels $C_{in}$, ensuring that the network's output maintains scale invariance relative to the size of the input channel.\nTo handle lots of parameters with INR, we adopting a block-based approach . Instead of a single large INR, weights are divided into a grid, with each segment controlled by a separate MLP network. The full architecture will be mentioned in the supplementary material.\nIn our framework, the weights for standard neural network operations are defined as follows:"}, {"title": "Maintaining Manifold Smoothness", "content": "A critical design within our paradigm is ensuring the weight manifold remains smooth. We, in this section, discuss why this smoothness is crucial for the model's performance, and outline our strategy for achieving this local smoothness.\nIntra-Model Smoothness. Modern neural networks heavily rely on their ability to model smooth signals to ensure convergence. Yet, empirical evidence suggests that the weight matrices are typically non-smooth. To enable our INR to reconstruct weights, we must find strategies that promote smoothness.\nTo address this challenge, previous studies have explored the concept of weight permutation . It is often likened to the Traveling Salesman Problem (TSP) . However, such an approach, while seemingly straightforward, overlooks the crucial inter-dependencies within and between weight matrices.\nLet's consider a weight matrix $W\\in \\mathbb{R}^{C_{out}\\times C_{in}}$ and measure its smoothness using total variation, denoted as TV(W). It is defined as the sum of variations along both channels: $TV(W) = TV_{in}(W) + TV_{out}(W)$. In fact, applying the TSP formulation presents 3 problems:\n\\textbf{(P1) Loop VS Non-Loop}: Unlike TSP, which necessitates returning to the starting point, ensuring 2D weight matrix smoothness doesn't require looping back. Instead, it is better to be considered as a Shortest Hamiltonian Path (SHP) problem, allowing for an arbitrary starting channel.\n\\textbf{(P2) Breaking Smoothness for the Connecting Layer}: Unlike isolated weight matrices, neural networks consist of connected layers, creating complex inter-layer relationships. This is illustrated in Figure 3, where permutations in one layer necessitate corresponding reversals in adjacent layers to maintain the network's functional equivalence. For example, with an activation function $\\sigma(\\cdot)$ and a valid permutation pair P and $P^{-1}$ (where $PP^{-1} = I$), the following equation holds:\n\\begin{equation}\nW_i \\sigma(P (P^{-1} W_{i-1}X)) = W_i \\sigma(W_{i-1}X)\n\\end{equation}\nAs a result, $P^{-1}$ may affect the adjacent layers, with increased TV for $W_iP$.\n\\textbf{(P3) Breaking Smoothness for the Other Dimension}: A permutation enhancing smoothness in output channel, might introduce non-smooth patterns in the input channel, thus reducing the overall smoothness.\nLuckily, we find that the computation of the TV measurement renders (P3) infeasible, implying our focus should be directed towards (P1) and (P2)."}, {"title": "Intra-model smoothness via permutation equivalence", "content": "Proposition 1. (Axis Alignment)  Let W be a given matrix and P be a permutation. The application of a permutation in one dimension of W does not influence the total variation in the orthogonal dimension.\n\\begin{equation}\nTV(WP) = TV_{in}(WP) + TV_{out}(W)\n\\end{equation}\n\\begin{equation}\nTV(PW) = TV_{in}(W) + TV_{out}(PW)\n\\end{equation}\nHence, to tackle global smoothness, we address challenges P1 and P2. We consider a neural network as a dependency graph G = (V, E) , where each node $v_i \\in V$ represents an operation with weight $W_i$ and each edge $e_{ij} \\in E$ indicates inter-connectivity between $v_i$ and $v_j$. Each graph clique $\\mathcal{C} = (V_c, E_c) \\subseteq G$ is full-connected, representing a group of operation is connected. As a results, each $\\mathcal{C}$ corresponds to a unique permutation matrix P. Our objective is to determine all P in a way that minimizes the total variation across the whole network.\nLuckily, based on the Proposition 1, this complex optimization can be broken down into multiple independent optimizations, each on a clique. We define this as a multi-objective Shortest Hamiltonian Path (mSHP) problem:\n\\begin{equation}\n\\mathop{\\text{argmin}}\\limits_P \\sum_{e_{ij} \\in E_{\\mathcal{C}}} (TV_{out}(PW_i) + TV_{in}(W_jP^{-1}))\n\\end{equation}\nTo address each mSHP problem, we transform it into a TSP problem by adding a dummy node. This new node has edges with zero-distance to all others in the clique. We then solve TSP using a 2.5-opt local search . The resulting permutation $P^*$ is applied to all weight matrices within the clique. This promotes the weight smoothness and preserves the functionality of the network.\nSince each individual mSHP problem is only correlated to one clip graph, we can solve the optimal P in a relative small scale, very efficently. In fact, with < 20 cliques per network, the total computation time is < 4 sec.\nCross-Model Smoothness. Another crucial challenge is to perverse the generalization behavior of the INR with different network configurations, which means, a small perturbation in the configuration, will eventually not affect the main model's performance. We address this by adding coordinate variation in the INR's learning process."}, {"title": "Training and Optimization", "content": "During training, rather than using fixed coordinates and model sizes as in Equation 4, we introduce slight variations to the input coordinates. Specifically, we add a small perturbation $\\epsilon$ to the input coordinate $(i', j') = (i, j) + \\epsilon$, where $\\epsilon$ is drawn uniformly from U(-a, a). This strategy aims to minimize the expected loss $E_{\\epsilon\\in U(-a,a)} [\\mathcal{L}]$.\nFor model evaluation, we sampling weight from a small neighborhood, as illustrated in Figure 4. We compute this by averaging the weights obtained from multiple input, each perturbed by different $\\epsilon \\in U(-a, a)$:\n\\begin{equation}\n\\hat{W(i,j)} = E_{\\epsilon\\in U(-a,a)} [W(i,j')] \\approx \\frac{1}{K} \\sum_{i=1}^K F(\\Phi(v'); \\theta)\n\\end{equation}\nThis is implemented by inferring the INR K = 50 times with varied sampled inputs v' and then computing the average of these weights to parameterize the main network. This approach is designed to enhance the stability and reliability of the INR under different configurations.\nOur approach optimizes the INR, denoted as $F(\\cdot; \\theta)$, to accurately predict weights for the main network of different configurations. It pursues two primary goals: approximating the weights of the pretrained network $f(\\cdot; W_{original})$, and minimizing task-specific loss across a range of randomly sampled networks. As such, the optimization leverages a composite loss function, divided into three distinct components: task-specific loss, reconstruction loss, and regularization loss.\nTask-specific Loss. Denoted as $\\mathcal{L}_{task}(y, \\hat{y}(W))$, this measures the difference between actual labels y and predictions $\\hat{y}$, based on weights W from the INR.\nReconstruction Loss. This element, expressed as $\\mathcal{L}_{recon} = ||W_{original}||_2 ||W - W_{original} ||_2$, assesses how close the INR-derived weights W to the ideal weights $W_{original}$, weighted by the magnitude $||W_{original}||_2$.\nRegularization Loss. Symbolized as $\\mathcal{L}_{reg} = ||W||_2$. This introduces L2 norm regularization on the predicted weights, to prevent overfitting by controlling the complexity of the derived model .\nWe minimize the composite objective by sampling different points on the model space\n\\begin{equation}\n\\mathop{\\text{min}}\\limits_{\\theta} E_{i,j,\\epsilon \\sim C} = \\mathop{\\text{min}}\\limits_{\\theta} E_{i,j,\\epsilon} [\\mathcal{L}_{task} + \\lambda_1 \\mathcal{L}_{recon} + \\lambda_2 \\mathcal{L}_{reg}]\n\\end{equation}\nThis loss function ensuring not only proficiency in the primary task through precise weight, but also bolstering model robustness via regularization. During training, we iteratively evaluates various combinations (i, j), striving to minimize"}, {"title": "The expected loss", "content": "The loss function is backpropagated from the main network to the INR as follows:\n\\begin{equation}\n\\nabla_{\\theta} \\mathcal{L} = \\frac{\\partial \\mathcal{L}_{task}}{\\partial W} \\frac{\\partial W}{\\partial \\Phi} \\frac{\\partial \\Phi}{\\partial v} \\frac{\\partial v}{\\partial \\theta} + \\lambda_1 \\frac{\\partial \\mathcal{L}_{recon}}{\\partial \\theta} + \\lambda_2 \\frac{\\partial \\mathcal{L}_{reg}}{\\partial \\theta}\n\\end{equation}\nThis equation represents the gradient of the loss with respect to \u03b8."}, {"title": "Experiments", "content": "In this section, we present our experimental analysis and various applications of NeuMeta, spanning classification, semantic segmentation, and image generation. To substantiate our design choices, we conduct different ablation studies. Additionally, we delve into exploring the properties of the learned weight manifold."}, {"title": "Experimental Setup", "content": "Datasets and Evaluation. We evaluate the proposed method on 3 tasks across 6 different visual datasets. For image classification, we select 4 dataset: MNIST , CIFAR10, CIFAR100 and ImageNet . Training includes horizontal flip augmentation, and we report top-1 accuracy. We also report the training time and the final size of stored parameters to evaluate our savings.\nIn semantic segmentation, we utilize PASCAL VOC2012 , a standard dataset for object segmentation tasks. We utilize its augmented training set , incorporating preprocessing techniques like random resize crop, horizontal flip, color jitter, and Gaussian blur. Performance is quantified using mean Intersection-over-Union (mIOU) and F1 score, averaged across 21 classes.\nFor image generation, we employ MNIST and CelebA . A vanilla variational auto-encoder (VAE) fits the training data, with evaluation based on reconstruction MSE and negative log-likelihood (NLL).\nImplementation Details. Our INR utilizes MLPs with ReLU activation, comprising five layers with residual connections and 256 neurons each. We employ a block-based INR approach, where each parameter type, such as weights and biases, is represented by a separate MLP. The positional embedding frequency is set to 16. Optimization is done using Adam with a le-3 initial learning rate and cosine decay, alongside a 0.995 exponential moving average. During training, we maintain the balance between different objectives with $A_1 = 1$ and $A_2 = 1e-4$.\nIn each training batch, we sample one network configuration, update a random subset of layers in the main network, computing gradients for the INR, to speed up training. The configuration pool, created by varying the channel number of the original network, utilizes a compression rate $\\gamma = 1 - \\frac{\\text{sampled channel number}}{\\text{full channel number}}$. We randomly sample a network width with compress rate $\\gamma\\in [0,0.5]$ for training. For example, a 128-channel layer will have its width sampled from 64 ~ 128.\nFor classification tasks, we apply LeNet on MNIST, ResNet20 on CIFAR10 and CIFAR100, and ResNet18 and ResNet50 on ImageNet, using batch sizes of 128 (MNIST, CIFAR) and 512 (ImageNet). We the INR train for 200"}, {"title": "Enhancing Efficiency by Morphing the Networks", "content": "Image Classification. As depicted in Figure 5, in the realm of image classification, NeuMeta consistently surpasses existing pruning-based methods in accuracy across MNIST, CIFAR10, CIFAR100, and ImageNet datasets at various compression ratios. It is worth-noting that, pruning-based methods show a marked accuracy decrease, approximately 5% on ImageNet and 6% on CIFAR100, when the compression ratio exceeds 20%. Conversely, NeuMeta retains stable performance up to 40% compression. However, a minor performance reduction is noted in our full-sized model, highlighting a limitation in the INR's ability to accurately recreate the complex pattern of network weights.\nTable 2 compares NeuMeta with Slimable NN and INN, including Oracle results of independently trained models for reference. We stick to the same model size for all method, to ensure the comparision is fair. Remarkably, NeuMeta often surpasses even these oracle models on large compress rate. This success is attributed to the preserved smoothness across networks of varying sizes, which inadvertently enhances smaller networks. Our approach outperforms both Slimable NN and the kernel representation in INN. Notably, at an untrained compression ratio of 75%\u2020, other methods significantly underperform.\nFurthermore, when evaluating total training time and parameter storage requirements, our approach demonstrates improved efficiency. Unlike the exhaustive individual model training and storage approach, other methods achieve some"}, {"title": "Image Generation", "content": "We implement NeuMeta to generate images on MNIST and CelebA, using VAE. Since Slimable NN and INN haven't been previously adapted for VAE before, we only compare with the pruning method, in Figure 7. Our approach demonstrated superior performance in terms of lower negative log-likelihood (NLL) across various compression ratios. For example, we visualize the generated results of when compressed by 25% for MNIST and 50% for CelebA in Figure 6. Compared with the l\u2081-based pruning, our method significantly improved reconstruction MSE from 53.76 32.58 for MNIST and from 620.87-128.60 for CelebA. Correspondingly, the NLL was reduced by 61.33 for MNIST and 492.26 for CelebA."}, {"title": "Exploring the Properties for NeuMeta", "content": "As we represent the weights as a smooth manifold, we investigate its effects on network. Specifically, we compare NeuMeta induced networks, with individually trained models and models distilled from full-sized versions.\nNeuMeta promote feature similarity. We analyzed the last layer features of ResNet20 trained on CIFAR10, particularly from layer3.2, using linear central kernel alignment (CKA) score between each resized and the full-sized model. The result is shown in Figure 9 (Top). It reveals higher feature map correlations across models compared to other methods, indicating that NeuMeta encourages similar network representations across different sizes.\nNeuMeta as Implicit Knowledge Distillation. We also report the the pairwise output KL divergence in Figure 9 (Bottom), a key metric in knowledge distillation . Individually trained models show higher divergence, whereas both KD and NeuMeta result in reduced divergence. These results imply that NeuMeta not only aligns internal representations but also ensures consistent network outputs, as an implicit form of distillation."}, {"title": "Ablation Study", "content": "Weight Permutation. To validate the effectiveness of our permutation strategy, we analyzed its impact on CIFAR10 accuracy. The comparison of Exp 2 and 4 in Table 11 demonstrates a significant 11.51 accuracy increase due to our permutation strategy. Detailed comparisons of our mSHP-based method with the TSP solution from are presented in the supplementary material. It shows"}, {"title": "Conclusion", "content": "This paper presents Neural Metamorphosis (NeuMeta), a novel paradigm that builds self-morphable neural networks. Through the training of neural implicit functions to fit the continuous weight manifold, NeuMeta can dynamically generate tailored network weights, adaptable across a variety of sizes and configurations. A core focus of our approach is to maintain the smoothness of weight manifold, enhancing the model's fitting ability and adaptability to novel setups. Experiments on image classification, generation and segmentation indicate that, our method maintain robust performance, even under large compression rate."}]}