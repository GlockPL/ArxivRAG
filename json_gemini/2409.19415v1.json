{"title": "Bridging the Gap\nin Hybrid Decision-Making Systems", "authors": ["Federico Mazzoni", "Roberto Pellungrini", "Riccardo Guidotti"], "abstract": "We introduce BRIDGET, a novel human-in-the-loop system\nfor hybrid decision-making, aiding the user to label records from an un-\nlabeled dataset, attempting to \"bridge the gap\" between the two most\npopular Hybrid Decision-Making paradigms: those featuring the human\nin a leading position, and the other with a machine making most of the\ndecisions. BRIDGET understands when either a machine or a human user\nshould be in charge, dynamically switching between two statuses. In the\ndifferent statuses, BRIDGET still fosters the human-AI interaction, either\nhaving a machine learning model assuming skeptical stances towards the\nuser and offering them suggestions, or towards itself and calling the user\nback. We believe our proposal lays the groundwork for future synergistic\nsystems involving a human and a machine decision-makers.", "sections": [{"title": "1 Introduction", "content": "Automated decision-making processes based on Machine Learning (ML) are still\nnot widely adopted for high-stakes decisions such as medical diagnoses or court\ndecisions [14]. In these fields, humans are aided but not replaced by Artificial\nIntelligence (AI), resulting in Hybrid Decision-Makers (HDM) [7].\nAsh the literature keeps growing, the term \"Hybrid Decision-Makers\" has\nbeen used as an umbrella word for various different kinds of algorithms, often\nwith a different focus, and a proper consensus has not been reached yet. Punzi et\nal. [8] notes two major HDM paradigms: Learning-to-Abstain where under cer-\ntain conditions an ML model refuses to make a decision, and Learning Together,\nwhere the human can interact with the training process of the ML model. Two\nof the most representative approaches of the two paradigms are, respectively,\nLearning-to-Defer (LtD) [4] systems, where the machine plays the primary role,\ndeferring decisions on records with a high degree of uncertainty to an external\nhuman supervisor, and Skeptical Learning (SL), where an ML model learns \u201cin\nparallel\" to the decisions taken by a human and queries them if it is \"skepti-\ncal\" of the human decision [15, 16]. The two have vastly different scopes. LtD\nassumes a cost to query the human user and aims to minimize that, leaving most"}, {"title": "2 Setting the Stage", "content": "In the following we report a brief overview of concepts necessary to understand\nour proposal. We indicate with H and M the Human user and the Machine of\nthe system, and with X, Y a dataset where X \u2208 X is a set of n records in feature\nspace X, while Y\u2208 Y is the set of the target variable in the target space Y. For\nclassification problems, $y_i \\in \\{1, ..., l\\} = L$, where L is the set of different class\nlabels and l, is the number of the classes. We indicate a trained decision-making\nmodel with a function f: X \u2192 Y that maps data instances x from the feature\nspace X to the target space Y. We represent the user decision process as an\nanalogous function h : X \u2192 Y. We write f(xi) = \u1ef9i to denote the decision \u011di\ntaken by f, h(xi) = \u0177i to denote the decision \u0177i taken by h.\nSkeptical Learning. Given a ML model f and a dataset X, the user is\ntasked to assign a label yi to each record xi \u2208 X. In SL, the user assigns the\nlabel \u011di and, independently from them, f assigns the label yi. The ML model\nf can be pre-trained on a small training set. If \u011di \u2260 \u1ef9i and f is skeptical (see\nbelow), the user is asked if they want to accept \u1ef9i as Yi. If they do, yi takes\nthe value yi. If the user refuses, if \u0177i = Yi or if the model is not skeptical, yi is\nassigned \u0177i. f is then incrementally trained on xi and Yi."}, {"title": "3 A Bridget System", "content": "BRIDGET, whose pseudocode is reported in Algorithm 1, assumes two potential\nstatuses, depicted in Figure 1:\nA Human-in-Command (HiC) phase where the human H and the machine\nM are into a co-evolutionary relationship [6]. H takes all the decisions, and\nM offering suggestions and explanations if skeptical.\nA Machine-in-Command (MiC) phase where M takes most of the decisions,\nbut it can call H back if uncertain, and it is able to explain why.\nThe BRIDGET system can loop between the two phases, accommodating the\nuser's needs, potential fallouts in the model's accuracy, or novelties in the data."}, {"title": "3.1 Human-in-Command", "content": "BRIDGET starts in the HiC phase and requires a set of records X, to be label\none by Once a new record xi is received, H makes its decision as well as M,"}, {"title": "3.2 Machine-in-Command", "content": "If t\u2122 is triggered, BRIDGET transitions to a state where M is in command,\nlabelling incoming records individually. As soon as a new record xi is received,\nM's f computes its belief, i.e., its prediction probability, b \u2208 [0,1] towards its\nprediction (line 14), following the first part of the fading skepticality:\n$b(x_i, \\tilde{y_i}, Y,\\tilde{Y}) = c_f (x_i, \\tilde{y_i}) \\cdot fea _f (x_i, \\tilde{y_i}, X_p, Y_p)$\nIf b is lower than a user-defined threshold \u03b2, M's stance towards xi is con-\nsidered unreliable, and H is immediately called back to make their own decision\n\u0177i (lines 15 and 17). In this state, M can explain why it is unreliable by provid-\ning, for instance, a small set of real or synthetic records with a low prediction\nprobability similar to that achieved on xi (line 16)."}, {"title": "4 Conclusion and Future Works", "content": "We have presented BRIDGET, an approach designed to bridge the two main Hy-\nbrid Decision-Making paradigms. BRIDGET uses a co-evolutionary process to\ntrain a ML model to closely mimic the user's behavior. This interactive dynamic\nbetween the human and machine agents allows for continual system parameter\nupdates, resulting in alternating phases where either the human or the machine\ntakes the lead. We plan to extensively test BRIDGET against stand-alone LtD\nsystems and also consider in the implementation different data types, such as\ntime series easily providing alternative types of explainability [10], and more\nin-depth functions such as fairness checks [6]. Moreover, we reckon BRIDGET\nshould give a deeper focus on concept drift during phase transitions, as shifts in\ndata are common reasons for a model's fall down. Lastly, the current iteration\nof BRIDGET was designed around two principles - employing FEA values as a\nmodel-agnostic proxy of the model's current status, and avoiding training an in-\ndependent deferral system. Other approaches are possible, e.g., supplanting FEA\nvalues with the model's internal epistemic uncertainty or comparing the number\nof leaves of an incremental decision tree at two different points in time to assess\nthe model's changes. Moreover, the user-provided decisions could effectively be\nused to train a deferral system at the end of the co-evolutionary phase."}]}