{"title": "Deep progressive reinforcement learning-based flexible resource scheduling framework for IRS and UAV-assisted MEC system", "authors": ["Li Dong", "Feibo Jiang", "Minjie Wang", "Yubo Peng", "Xiaolong Li"], "abstract": "The intelligent reflection surface (IRS) and unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system is widely used in temporary and emergency scenarios. Our goal is to minimize the energy consumption of the MEC system by jointly optimizing UAV locations, IRS phase shift, task offloading, and resource allocation with a variable number of UAVs. To this end, we propose a Flexible REsource Scheduling (FRES) framework by employing a novel deep progressive reinforcement learning which includes the following innovations: Firstly, a novel multi-task agent is presented to deal with the mixed integer nonlinear programming (MINLP) problem. The multi-task agent has two output heads designed for different tasks, in which a classified head is employed to make offloading decisions with integer variables while a fitting head is applied to solve resource allocation with continuous variables. Secondly, a progressive scheduler is introduced to adapt the agent to the varying number of UAVs by progressively adjusting a part of neurons in the agent. This structure can naturally accumulate experiences and be immune to catastrophic forgetting. Finally, a light taboo search (LTS) is introduced to enhance the global search of the FRES. The numerical results demonstrate the superiority of the FRES framework which can make real-time and optimal resource scheduling even in dynamic MEC systems.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, computation-intensive applications such as virtual reality (VR) and augmented reality (AR) have grown rapidly, which makes the user equipment (UE) difficult to process these tasks locally. Mobile edge computing (MEC) addresses this issue by relocating computing and communication services closer to users, enabling UEs to offload computation-intensive tasks to nearby MEC servers. This offloading alleviates the computational burden and reduces the energy consumption of UEs [1].\nHowever, deploying fixed MEC servers in terrestrial areas may not be considered economically viable, especially in temporary situation [2]. Unmanned aerial vehicles (UAVs) are designed to be highly maneuverable, allowing them to navigate through tight spaces, fly at low altitudes, and perform precise aerial movements [3]. UAV-assisted MECs have been proposed to provide computation and communication services to ground UEs seamlessly and proficiently. Whereas the wireless channels between UAVs and terrestrial UEs may be blocked by trees or high buildings.\nRecently, the intelligent reflecting surface (IRS) [4] has been proposed and received considerable attention. The quality of signal transmission can be improved by laying IRS on the surface of high buildings. Since each component in IRS can reflect the transmission signal, the transmission data rate can be greatly improved. In addition, the reflective elements of the IRS are generally passive, and the reflected signal does not require any signal processing capacities, which is more energy-efficient than the traditional transmission technologies. In UAV-assisted MEC systems, the IRS can be attached on the buildings or on the high places to enhance the channel between the UAV and UEs when the connections are blocked by buildings in temporary and emergency scenarios.\nAlthough IRS and UAV-assisted MEC is promising, some challenges should be addressed. First, with the increase of UEs, a single UAV may not be sufficient to support a large number of computing tasks, and multiple UAVs may require to be deployed. However, with the changing number of UAVs and UEs, the communication environment is expected to become increasingly complex. Second, in the wireless fading environment, the wireless channel conditions which may change with time, can affect the resource scheduling of MEC systems [5], especially considering the IRS in the MEC system. Third, the offloading decision is generally an integer variable, and the resource allocation is the continues variable. It is an overall mixed integer nonlinear programming (MINLP) problem, which is difficult to be addressed by traditional methods.\nTo address the above challenges, in this paper, we consider the MEC system assisted by IRSs and UAVs in the dynamic environment. Our objective is to design a deep progressive reinforcement learning based Flexible REsource Scheduling (FRES) framework to minimize the energy consumption of the MEC system with a variable number of UAVs. This problem requires not only a transfer learning method without catas-"}, {"title": "II. RELATED WORK", "content": "In [6], the authors proposed the MEC system with multiple ground servers and an air server. In order to achieve the maximum calculation rate, the author optimized the trajectory of the UAV and the offloading of each user task. The work in [7] introduced a single UAV-assisted MEC system, in which the UAV was used to collect the data of users on the ground, and the energy consumed by the UAV was minimized by optimizing the trajectory of the UAV. The work in [8] considered a framework in which UAVs were used to assist users in task processing. In order to make all users consume the lowest energy when processing tasks, the authors optimized the trajectory and allocated resources of the UAV. The work in [9] proposed a UAV-assisted MEC system based on DRL. The users offloaded the tasks that need to be processed in the UAV to improve the computing efficiency. This method realized the offloading decision and resource allocation under the time-varying channel. The work in [10] proposed a MEC system containing multiple UAVs and the number of UAVs was reduced as much as possible while meeting the constraints. In this work, the system energy consumption was minimized by optimizing the offloading decision and resource allocation of user tasks."}, {"title": "B. IRS-assisted UAV Communications", "content": "In [11], in order to improve the quality of the transmission channel, a low-altitude passive relay system was established using IRS-assisted UAV to convert the Rayleigh fading channel into the Rician fading channel to improve the transmission performance. In [12], the author proposed a radio system in which the UAV's signal was reflected to the base station (BS) by IRSs, and the weighted sum rate of all IRSs was maximized by optimizing the trajectory of the UAV and the phase shift matrix of the IRSs. In [13], the author proposed a multi-IRS-assisted UAV system, which improved the user's received power by continuously adjusting the trajectory of the UAV and the beamforming of the IRS. In [14], with the help of the IRS component, the author improved the communication quality between the UAV and the users by adjusting the trajectory of the drone and the phase shift matrix of the IRS. An IRS-assisted UAV communication system was studied in [15]. In order to increase the average transmission rate of the system, the phase shift matrix of the IRS and the trajectory of the UAV were jointly optimized."}, {"title": "C. Resource Scheduling in MEC Systems", "content": "At present, there are many related works that formulated the resource scheduling in the MEC system as a MINLP problem, and adopted some traditional solutions to solve it. In [16], the author proposed a dynamic programming algorithm to allocate bandwidth and computing resources for mobile devices to minimize the energy consumption. In [7], the successive convex approximation (SCA) algorithm was used to optimize the resource allocation, and the energy efficiency of the UAV in the MEC system is maximized. In [17], the block coordinate descent method and the successive convex approximation algorithm were used to optimize the resource scheduling problem in the cellular connected UAV system. In [18], the author proposed a heuristic algorithm named GAWOA on the basis of genetic algorithm (GA) and whale optimization algorithm (WOA) to solve the resource scheduling problem in the MEC system. In [19], a hierarchical GA and particle swarm optimization (PSO) based computing algorithm was designed to solve the resource scheduling problem in the MEC system.\nThere are also some related works that used deep learning to implement resource scheduling in the MEC system. In [20], a DRL-based resource scheduling method was proposed. The author reduced the state space of DRL by compressing channel quality information, and proposed adaptive iteration to improve the search efficiency in the DRL. In [21], a resource scheduling framework based on distributed network structure"}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "We consider the IRS and UAV-assisted MEC system, as shown in Fig. 1, which consists of N UES, L IRSs, some UAVs and a central cloud. All UEs are randomly distributed and all IRSs are mounted on buildings. The set of UEs is denoted as N = {1,2,..., N}, the set of UAVs is denoted as M = {1,2,..., M} and the set of IRSs is denoted as L = {1,2,..., L}. The central cloud is utilized to gather all system information, encompassing channel state information, UAV information, IRS information, and task-related information from UEs. By deploying the DRL model on the central cloud, the task offloading and resource scheduling for the MEC system are executed.\nEach UE has the option to either offload its entire task to a single UAV or execute it locally. We employ the notation $a_{i0}$ to indicate that a task is executed locally, and $a_{ij}$ to signify that a task is offloaded to the jth UAV. Consequently, we can establish the following constraints:\n$a_{i0} = {0,1}, \\forall i \\in N$ (1)\n$a_{ij} = {0,1}, \\forall i \\in N, \\forall j \\in M$ (2)\nwhere $a_{i0} = 1$ signifies that the ith UE chooses to execute the task locally. Otherwise, $a_{i0} = 0$. Similarly, $a_{ij} = 1$ implies that the ith UE wants to offload its task to the jth UAV, while $a_{ij} = 0$ indicates otherwise. Each UE can choose only one location for task execution, we are led to establish the following constraint:\n$a_{i0} + \\sum_{j\\in M} a_{ij} = 1, \\forall i \\in N$. (3)\nWe assume that each UE has a computationally intensive task Ui which can be expressed as follows:\n$U_i = (R_i, F_i), \\forall i \\in N$ (4)\nwhere $R_i$ denotes the volume of data that is required to be transmitted to the UAV for task execution, while $F_i$ signifies the cumulative number of required CPU cycles for task computation."}, {"title": "B. Communication Model", "content": "We assume that UEs are situated in a bustling urban center with numerous tall buildings. In such an environment, the direct link to the UAVs may be obstructed, leading to severe path loss. However, by deploying IRSs on these building surfaces, we can enhance the communication quality between UEs and UAVs. Each IRS is assumed to have multiple reflecting elements, denoted as K = {1,2,..., K}, and only one IRS is utilized to assist each UE in communication. The coordinates of the ith UE can be represented as $(x_i^u, y_i^u, z_i^u)$; and those of the lth IRS can be represented as $(x_l^r, y_l^r, z_l^r)$; and those of the jth UAV can be represented as $(x_j^v, y_j^v, z_j^v)$. Therefore, we calculate the distance between the ith UE and the lth IRS as follows:\n$R_{i,l}^{UR} = \\sqrt{(x_i^u - x_l^r)^2 + (y_i^u - y_l^r)^2 + (z_i^u - z_l^r)^2}$. (5)\nSimilarly, the distance between the lth IRS and jth UAV is expressed as\n$R_{l,j}^{RV} = \\sqrt{(x_l^r - x_j^v)^2 + (y_l^r - y_j^v)^2 + (z_l^r - z_j^v)^2}$. (6)\nWe consider that the communication path between the ith UE and the jth UAV is divided into two segments: the UE-IRS link and the IRS-UAV link. We also assume that each UE only uses one IRS for signal transmission, so that the"}, {"title": "C. Computation Model", "content": "1) Local Computation Model: The energy consumption of the local computing can be expressed as follows:\n$E_i^l = \\nu_1 (f_{i0})^{\\tau_1-1} F_i, \\forall i \\in N$ (12)\nwhere $f_{i0}$ is the local computation capacity of the ith UE. $\\nu_1 \\geq 0$ is the effective switched capacitance and $\\tau_1 \\geq 1$ is the positive constant.\nThe computation capacity of the ith UE is constrained by:\n$a_{i0} f_{i0} \\leq F_{i,max}^L, \\forall i \\in N$ (13)\nwhere $F_{i,max}^L$ means the maximum local computation capacity of the ith UE.\n2) Remote Computation Model: The energy consumption in remote computing phase is\n$E_{ij}^r = \\nu_2 (f_{ij})^{\\tau_2-1} F_i$ (14)\nwhere $f_{ij}$ means that the computing resource allocated by the jth UAV to the ith UE. $\\nu_2$ is the effective switched capacitance and $\\tau_2 \\geq 1$ is the positive constant.\nSince each UAV has a limited computation capacity, we can represent the constrained computation resource of the jth UAV as follows:\n$\\sum_{i\\in N} a_{ij}f_{ij} \\leq F_{j,max}^V, \\forall j \\in M$ (15)\nwhere $F_{j,max}^V$ is the total computation capacity of the jth UAV."}, {"title": "D. UAV Hover Model", "content": "When the jth UAV hovers to receive and perform offloaded tasks, the transmission time of the offloaded task can be expressed as:\n$T_{ij}^r = \\frac{R_i}{r_{ij}}, \\forall i \\in N, \\forall j \\in M$. (16)\nThe execution time of the offloaded task can be expressed as:\n$T_{ij}^e = \\frac{F_i}{f_{ij}}, \\forall i \\in N, \\forall j \\in M$. (17)\nHence, the energy consumption associated with UAV hovering is given by\n$E_j^h = P_h max{\\lbrace a_{ij} (T_{ij}^r + T_{ij}^e)\\rbrace}, \\forall j \\in M$ (18)\nwhere $P_h$ means the hover power of the jth UAV."}, {"title": "E. Problem Formulation", "content": "In this study, our objective is to minimize the energy consumption of the IRS and UAV-assisted MEC system. The optimization problem can be summarized as follows:\n$P0: min_{L,A,F,\\Theta} \\lbrace \\sum_{i\\in N} (a_{i0}E_i^l + \\sum_{j\\in M} a_{ij}E_{ij}^r ) + \\sum_{j\\in M} E_j^h + \\sum_{i\\in N} \\sum_{j\\in M} a_{ij} E_{ij}^t \\rbrace$ s.t. (1), (2), (3), (13), (15). (19)\nwhere $L = \\lbrace (x_j^v, y_j^v, z_j^v)|j \\in M\\rbrace$ denotes the positions of UAVs. The offloading decision is represented by A = $\\lbrace a_{i0}, a_{ij}|i \\in N, j \\in M\\rbrace$, while the allocation of computing resources is expressed as F = $\\lbrace f_{i0}, f_{ij}|i \\in N, j \\in M\\rbrace$. The phase-shifted diagonal matrix for IRS-assisted signal transmission is denoted by $\\Theta = \\lbrace \\Theta_{i,l,j}|i \\in N, l \\in L, j\\in M\\rbrace$. Lastly, A serves as a weight coefficient. For saving flight energy, we assume that UAVs need to be redeployed only when the number of UAVs is changed. We use large-scale path-loss fuzzy c-means clustering algorithm (LS-FCM) to optimize the locations of UAVs and obtain the L in P0 [22]."}, {"title": "IV. THE FRES FRAMEWORK", "content": "Problem P0 is a typical MINLP, where A are binary variables, and L, F and $\\Theta$ are real positive variables. This makes the Problem P0 non-smooth and non-differential. Moreover, there are still three challenges for solving Problem P0: (1) It is a large-scale optimization problem with many variables, the search is easy to trap into the local minimum. (2) In the system, the number of UAVs is varying, so the prior knowledge is hard to be reused to solve the problem in the dynamic environment. (3) It needs to design a real-time resource scheduling with low computational complexity for the wireless fading channel.\nTo this end, we propose a deep progressive reinforcement learning based FRES framework to address the above challenges. First, we present a model free DRL-based resource scheduling structure, which can learn from the environment by the interaction between the agent and the MEC system without any prior information. After the agent of DRL is well trained, the agent can make real-time decision easily by doing some simple algebraic calculations instead of solving the original P0 repeatedly. Second, we propose a multi-task agent with two-head structure to solve the MINLP directly. Third, we consider a progressive scheduler to adjust structure of the agent for adapting the varying number of UAVs, which can add or remove a part of neurons in the agent to trace the dynamic environment. This structure is resistant to catastrophic forgetting. Finally, we introduce an action refinement to enhance the exploration, in which a LTS guided by channel gains is designed to jump out of the local minimum and accelerate the policy search in large action space.\nThe dataflow of the FRES framework can be described as follows: first, the central cloud collects the global environment information as well as task information from the MEC system. Then, the central cloud executes the FRES algorithm, updates the locations of UAVs and the phase-shifted diagonal matrix of IRSs, and performs the online decisions of the user association and resource allocation for each UE. Finally, based on the decision received from the central cloud, each UE can offload the task to the suitable UAV, and then receive results accordingly."}, {"title": "A. Algorithm Overview", "content": "The details of the FRES framework are described in Algorithm 1. We first introduce a quantitative passive beamforming (QPB) method to solve $\\Theta_t$ according to the positions of UAVs and UEs. Then, we design a model-free DRL to generate $A_t$ and $F_t$ of all UEs at tth timeslot. Next, we introduce the state, action, and reward of the DRL in the FRES framework as follows:"}, {"title": "B. QPB", "content": "We introduce the QPB method to optimize the phase shift matrix of IRSs [23]. Due to hardware facility restrictions, the IRS can only reflect signals with certain phase shifts. For simplicity, we consider discrete phase shift angles in this paper, the phase shift $\\Theta_{k,i,l,j}$ of the IRS is chosen from the following set of $\\Psi = {\\frac{2\\pi i}{N_p}, i = 0,1,..., N_p-1}$, where $N_p$ denotes the number of the phase shift values that can be selected for every element. We optimize the phase shift $\\Theta_{k,i,l,j}$ of the kth reflecting element in the lth IRS between the ith UE and the jth UAV with the following equation:\n$\\Theta_{k,i,l,j} = argmin_{\\theta_{k,i,l} \\in \\Psi} |w_{k,i,l}^{UR} - (w_l^{RV} + \\theta_{k,i,l,j})|$ (20)\nwhere $w_{k,i,l}^{UR} \\in [0,2\\pi)$ denotes the phase shift of the kth reflecting element from the ith UE to the lth IRS, and $w_l^{RV} \\in [0,2\\pi)$ denotes the phase shift of the kth reflecting element from the lth IRS to the jth UAV."}, {"title": "C. Multi-task Agent", "content": "Multi-task learning is a machine learning method that puts multiple related tasks together to learn [24]. In the learning process, these tasks share parameters of the shallow part of the deep neural network (DNN) to extract the common features and then adjust the independent parameters of the subsequent part of the DNN to learning the unique features of each task respectively. The whole multi-task agent includes four part: input layer, shared layer, standalone layer and output layer, which are shown in Fig. 3.\nAs depicted in Fig. 3, the multi-task agent consists of Q shared layers, and P standalone layers for each specific task. The output from the qth shared layer can be characterized as follows:\n$o_q = \\sigma (w_q o_{q-1} + b_q)$ (21)\nwhere $w_q$ and $b_q$ represent the weights and biases of the qth shared layer, respectively. $\\sigma(.)$ is the activation function.\nWe can express the output from the pth standalone layer of the jth task as follows:\n$o_{j,Q+p} = \\sigma (w_{j,Q+p} o_{j,Q+p-1} + b_{j,Q+p})$. (22)"}, {"title": "D. Progressive Scheduler", "content": "Progressive learning was first proposed by Google Deep-Mind [25], which was a novel knowledge transfer method based on continual learning [26]. With the help of the previous knowledge of the old task, the new task can be trained relatively quickly, and the parameters of the old tasks will not be forgotten. When the old task needs to be carried out again, the new knowledge will be forgotten and the old model parameters can be directly recalled to process it without retraining over again.\nThe system model considered in this paper is a dynamic model with different number of UAVs when the environment varies. However, the structure of traditional neural network is stationary, when the number of UAVs changes, the structure of the neural network should be adjusted and the network should be retrained again for adapting the changes in the environment. We design a new progressive scheduler for the multi-task agent to address this challenge, in which we can adjust the structure of the neural network to trace the dynamic environment and we can add or remove a part of the neurons to adapt the change of UAV numbers. The process of dynamically adjusting the neurons using the progressive scheduler for different numbers of UAVs is illustrated in Fig. 4."}, {"title": "E. LTS", "content": "Since the action space of our DRL framework is large and dynamic, the DRL is hard to converge in the action space. Action refinement has proven to be an effective global search strategy when dealing with large-scale action spaces in DRL [27]. Based on taboo search [28], we propose an LTS algorithm to implement action refinement. The details of the LTS are described as follows:"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "The simulation parameters of the FRES framework are chosen as follows: The number of UEs is set to 50, and the number of IRSs is also set to match the number of UEs. The initial multi-task agent consists of two shared layers with 64 and 128 neurons, respectively. Both the fitting head and classification head are configured with 32 neurons [2]. For progressive scheduler, we adjust each layer by adding or removing up to 16 neurons for each UAV. For the LTS, we set the taboo list length to 5 with a maximum iteration number capped at 10. Unless otherwise stated, all other parameters employed are summarized in Table II [2], [29], [30]. The simulations are executed in an environment powered by an Intel Xeon CPU, supplemented with 32GB RAM, and a Tesla T4 GPU with 15GB RAM."}, {"title": "B. Experiments for the Multi-task Agent", "content": "In this section, we verify the effectiveness of the multi-head structure in solving resource scheduling problems by"}, {"title": "C. Experiments for the Progressive Scheduler", "content": "In this section, we simulate a scenario where the number of UAVs is varying, and compare the loss and reward before and after the number of UAVs is changed. This simulation is applied to verify the validity of the progressive scheduler. In the first 1000 iterations, only three UAVs are provided to UEs for mobile computing, in the next 500 iterations, the number of UAVs increases to four, and in the last 500 iterations, the number of UAVs is back to three again. The loss curve and the reward curve are plotted in Fig. 6 and Fig. 7.\nIn Fig. 6, it can be seen that in the first 1000 iterations, the loss of the agent continues to decrease to convergence. As the number of UAVs increases, the adding rule is carried out, and the knowledge of the new structure of the agent can be used in the new scenario with four UAVs directly. When the number of UAVs returns to three again, the removing rule is executed and the extra structure of the agent is closed. The agent do not need to retrain again and the loss curve do not alter significantly throughout the process.\nSimilar to Fig. 6, Fig. 7 shows the reward of the agent during the number of UAVs changes. In the first 1000 iterations, the reward continues to increase and then achieves convergence. When the number of UAVs increases, the adding rule is carried out and the overall reward is changed. When the number of UAVs returns to three, the removing rule is executed and the extra structure of the agent is closed, so the reward is back to the original level."}, {"title": "D. Experiments for the LTS", "content": "In this section, we evaluate the performance of the LTS with three different single point search methods to verify the exploration of the LTS. The three heuristic search methods are TS [28], simulated annealing (SA) [31] and adaptive SA (ASA) [32]. In the experiment, the iteration number of LTS and TS is set to 90, the length of taboo list is set to 5, the search neighborhood size is set to 20; The annealing cooling rate of SA and ASA is set to 0.95, the initial annealing temperature $T_{max} = 100$ and cut-off annealing temperature $T_{min} = 1$.\nFig. 8 presents the performance of different action refinements. We can see that LTS and TS obtain lower energy consumption than ASA and SA, while LTS converges faster than TS. This is due to the fact that LTS employs the light move operator with the highest channel gain to accelerate the search."}, {"title": "E. Experiments for the FRES Framework", "content": "In this section, we evaluate the performance of the whole FRES framework. We first compare the FRES framework with four DRL algorithms: DDPG [33], SAC [34], LyDROO [35] and ARE [2]. TABLE III characterizes the training time of initial task (only one UAV at the beginning), new task (adding a new UAV), old task (removing a UAV), and the average and standard deviation (STD) of the energy consumption in all DRLs. It can be seen that the FRES framework achieves the least training time of the old task and the lowest average energy consumption. The superiority of the FRES framework can be explained as follows: (1) The progressive scheduler can reuse the stored policy knowledge when the number of UAVs is changing without retraining the agent again. Hence, the training time of the old task in the FRES framework is the least. (2) LTS refines the action and boosts exploration capabilities, enabling it to escape local extrema during the search process. As a result, the FRES framework achieves the lowest average energy consumption.\nThen, we use the multi-task map proposed by Google to illustrate the performance variations of old tasks when the new task is added. Fig. 9 summarizes the multi-task performances of different DRLs, in which we add three UAVs as new tasks one by one to the DRLs. Different colored lines describe the change in energy consumption of the old tasks when a new task is added. As seen in Fig. 9, while training for a new task (adding a new UAV), the energy consumption on older tasks increases continuously in the cases of DDPG, SAC, LyDROO and ARE, which means the catastrophic forgetting has already happened, and the knowledge of old task has been forgotten. Our FRES is superior to the other DRLs, and the energy consumption on all tasks remains fixed.\nFinally, we select five offloading schemes as benchmarks to evaluate the performance of our offloading approach. The chosen benchmarks are described as follows:\n\\bullet Random offloading (Random) denotes that the offloading decision of each UE is randomly determined.\n\\bullet Local executed (Local) denotes that all UEs execute tasks locally.\n\\bullet Offloading nearby (Remote) denotes that each UE decides to offload the task to the closest UAV.\n\\bullet TS denotes that taboo search is applied to search the best offloading decision for all UEs.\n\\bullet Lyapunov-guided deep reinforcement learning online offloading (LyDROO) is a celebrated DRL offloading scheme for the MEC system [35].\nThe average and STD of the energy consumption for these five offloading methods are shown in Fig. 10, it can be seen that the energy consumption of the proposed FRES framework is much lower than the energy consumptions of Local, Random and Remote. Meanwhile, the energy consumption of the proposed FRES framework is closed to the TS. This is because the FRES method can update the offloading policy from the high-quality offloading solutions generated by LTS. Furthermore, the FRES method is capable of constructing a multi-task agent to generate offloading decisions and resource allocation, which can make faster high-quality decisions than traditional heuristic search methods."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a deep progressive reinforcement learning based FRES framework to minimize the energy consumption of all the UEs by optimizing offloading decision and resource allocation in IRS and UAV-assisted MEC system. The FRES framework includes three important parts: 1) the multi-task agent is presented to solve the offloading decision task and the resource allocation task at the same time; 2) the progressive scheduler is used to achieve quick reaction for the dynamic environment with the changing number of UAVs; 3) the LTS is applied to enhance the exploration of the DRL. The simulation results show that the FRES framework can achieve the best performance in IRS and UAV-assisted MEC system with varying number of UAVs.\nDespite the contributions of the proposed method, there is also a downside that can serve as a basis for future research. When the number of UAVs increases, the number of parameters in the multi-task agent is also growing. This growth can be addressed by pruning, quantization and compression for the network structure of the agent during the learning process."}]}