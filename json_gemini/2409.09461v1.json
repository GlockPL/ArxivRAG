{"title": "TX-Gen: Multi-Objective Optimization for Sparse Counterfactual Explanations for Time-Series Classification", "authors": ["Qi Huang", "Sofoklis Kitharidis", "Thomas B\u00e4ck", "Niki van Stein"], "abstract": "In time-series classification, understanding model decisions is crucial for their application in high-stakes domains such as healthcare and finance. Counterfactual explanations, which provide insights by presenting alternative inputs that change model predictions, offer a promising solution. However, existing methods for generating counterfactual explanations for time-series data often struggle with balancing key objectives like proximity, sparsity, and validity. In this paper, we introduce TX-Gen, a novel algorithm for generating counterfactual explanations based on the Non-dominated Sorting Genetic Algorithm II (NSGA-II). TX-Gen leverages evolutionary multi-objective optimization to find a diverse set of counterfactuals that are both sparse and valid, while maintaining minimal dissimilarity to the original time series. By incorporating a flexible reference-guided mechanism, our method improves the plausibility and interpretability of the counterfactuals without relying on predefined assumptions. Extensive experiments on benchmark datasets demonstrate that TX-Gen outperforms existing methods in generating high-quality counterfactuals, making time-series models more transparent and interpretable.", "sections": [{"title": "1 INTRODUCTION", "content": "The increasing adoption of machine learning models for time-series classification (TSC) in critical domains such as healthcare (Morid et al., 2023) and finance (Chen et al., 2016) has raised the demand for interpretable and transparent decision-making processes. However, the black-box nature of many high-performing classifiers, such as neural networks or ensemble models, limits their interpretability, making it difficult for practitioners to understand why a particular decision is made. In this context, counterfactual explanations have emerged as a valuable approach in Explainable AI (XAI) (Theissler et al., 2022), providing instance-specific insights by identifying alternative inputs that lead to different classification outcomes. Despite the growing body of work in this area, generating meaningful counterfactuals for time-series data presents unique challenges due to its sequential nature, dependency on temporal structure, and multi-dimensional complexity.\nWhile counterfactual generation has been extensively explored for tabular and image data, methods tailored to time-series classification remain scarce and underdeveloped. The few existing methods often fail to balance key properties such as proximity, sparsity, and validity. Moreover, most approaches require high computational resources or rely on rigid heuristics, limiting their applicability in real-world scenarios. This gap motivates the development of an efficient, model-agnostic method capable of generating high-quality counterfactual explanations for time-series classifiers.\nIn this paper, we propose TX-Gen, a novel algorithm for generating counterfactual explanations in time-series classification tasks using a modified Non-dominated Sorting Genetic Algorithm II (NSGA-II). Unlike previous approaches that combine evolutionary computing with explainable AI (Zhou et al., 2024), TX-Gen leverages the power of evolutionary multi-objective optimization to find a diverse set of Pareto-optimal counterfactual solutions that simultaneously minimize multiple objectives, such as dissimilarity to the original time-series and sparsity of changes, while ensuring the classifier's decision is altered. Additionally, our approach incorporates a flexible reference-based mechanism, which guides the counterfactual generation process without relying on restrictive assumptions or predefined shapelets.\nContributions The key contributions of this work are as follows:"}, {"title": "1.1 Problem Statement", "content": "We begin our problem formulation by first introducing the context of this research.\nGiven a collection (set) of univariate time series, denoted as $S = {T_1, T_2, ...,T_n}$, where each $T = {t_i \u2208 R}$ consists of observations orderly recorded across m timestamps. In the context of a time series classification (TSC) task, each time series T is associated with a true descriptive label $L \u2208 {L_1,..., L_k}$ from k unique categories. The objective of TSC is to develop an algorithmic predictor $f(\u00b7)$ that can maximize the probability $P(f(T) = L)$ for $T \u2208 S$ and also any unseen $T \\notin S$ (provided that the true label of such aTS also equals L). Here, we presume the classifier provides probabilistic or logit outputs. Moreover, assume a function $g: R^m \u2192 R^m$ that can element-wise perturb or transform any given time series T into a new in-distribution time series instance $T^*$. We then say that $T^*$ is a counterfactual of T regarding a TSC predictor f if $f(T^*) \\neq f(T)$ while the dissimilarity between T and $T^*$ is minimized (Wachter et al., 2017; Delaney et al., 2021). It is noteworthy that counterfactual explanations are fundamentally instance-based explanations aimed at interpreting the predictions of the classifier being explained, rather than the true labels, regardless of whether the real labels are known or not.\nGoal Considering a robust time series classification predictor f trained on the dataset S, and a time series instance Te to be explained, this work proposes an explainable AI method, i.e., the aforementioned function g, which can identify the counterfactual of Te concerning f. Notably, Te may be either part of the dataset S or external to it."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Evaluation Metrics", "content": "The evaluation criteria for our counterfactual generation method are articulated through several key metrics, each addressing specific aspects of counterfactual quality and effectiveness:\n\u2022 Proximity: Measures the element-wise similarity between the counterfactual instance and the target time series, typically by using the Lp metrics.\n\u2022 Validity: Assesses whether the counterfactuals successfully alter the original class label, thereby reflecting their effectiveness.\n\u2022 Diversity: Measures the variety in the generated counterfactual solutions for each to-be-explained time series instance.\n\u2022 Sparsity: Quantifies the simplicity of the counterfactual by counting the number of element-wise differences between the generated counterfactual and the original series.\nThese metrics collectively provide a robust framework for evaluating the quality and utility of counterfactual explanations, emphasizing minimalism, diversity, validity, and closeness to the original instances."}, {"title": "2.2 Related Work", "content": "The evolution of explainable artificial intelligence (XAI) for generating counterfactuals for time series has been marked by significant advances in methods that provide clear, actionable insights into model decisions. This line of research has been initiated by (Wachter et al., 2017), introducing an optimization-based approach that focuses on minimizing a loss function to modify decision outcomes while maintaining minimal Manhattan distance from the original input. This foundational method encouraged further development of algorithms that enhance the quality and effectiveness of counterfactual explanations through additional modifications to the loss function.\nBuilding upon the concept of influencing specific features within time series data, the introduction of local tweaking via a Random Shapelet Forest classifier (Karlsson et al., 2020) targets specific, influential shapelets for localized transformations within time series data. In contrast, global tweaking (Karlsson et al., 2018) uses a k-nearest neighbor classifier to guide broader transformations ensuring minimal yet meaningful alterations to change class outcomes."}, {"title": "3 METHODOLOGY", "content": "The framework Our proposed method, TX-Gen, aims to jointly locate the subsequence of interest in a to-be-explained time series T while also transforming T into its counterfactual. This is achieved by mutually minimizing two objective functions under the guidance of reference samples, using a customized iterative optimization heuristic based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) (Deb et al., 2002). A high-level pseudocode of our algorithm is given in Algorithm 1 assuming the respect readers are familiar with the concepts of evolutionary algorithms (Back et al., 1997; B\u00e4ck et al., 2023) Following the self-explainable high-level pseudocode, the key components proposed in our algorithm are further explained."}, {"title": "3.1 Model-based Selection of References", "content": "In TSC, consider a time series instance T that belongs to category C. A counterfactual-based explainer can be viewed as a reference-guided method if it leverages example instances-originating from the same task but known to belong to categories other than C-to assist in transforming T into its counterfactual. Existing literature on reference-guided methods primarily identifies the reference instances by selecting those with low shape-based or distance-based similarities to the given instance to be explained, e.g., the nearest unlike neighbors as seen in (Delaney et al., 2021; H\u00f6llig et al., 2022). In contrast, our work proposes selecting reference samples solely based on the classifier's outputs.\nFollowing the settings in Section 1.1, let $f(\u00b7)$ be a probabilistic classifier for a TSC task with k classes. That is, instead of predicting the exact label, f predicts the probability distribution over all candidate class labels for any given target time series T.\nDefinition 1 (Distance in the Classifier). For any pairs of time series (Ti,Tj) of the TSC task \u03a9, we define their Distance in the Classifier f as the Jensen-Shannon distance between f(T\u2081) and f(Tj):\n$D_f(T_i, T_j) = \\sqrt{\\frac{KL(f(T_i) || P_m) + KL(f(T_j) || P_m)}{2}}$\nwhere Pm is the element-wise mean of f(Ti) and f(Tj), and KL denotes the well-known Kullback-Leibler (KL) divergence (Endres and Schindelin, 2003). Apart from the properties that a valid metric holds, this distance is also bounded, D\u0192(T;,Tj) \u2208 [0, 1] if we use 2 as the logarithm base in KL divergence.\nWith this definition, as described in Algorithm 2,"}, {"title": "3.2 The Objective Functions", "content": "To address the idea (see section 1.1) that a decent counterfactual $T^* = {t^*_i \u2208 R}^{m}_{i=1}$ shall closely resemble the target time series $T = {t_i \u2208 R}^{m}_{i=1}$ while flipping the prediction of a given classifier f, we propose to determine feasible candidates $T^*$ by jointly minimizing two real-valued objectives as follow:\n\u2022 The minimum Distance in Classifier between a counterfactual candidate $T^*$ and the samples in the reference set \u03c8:\n$F_1(T^*, \u03c8) = \\begin{cases}\nmin D_f (T^*, T_i), \u2200T_i \u2208 \u03c8 \\text{ if } f(T^*) \\neq f(T), \\\\\n1.01 \\text{ if } f(T^*) = f(T).\n\\end{cases}$\nThe second condition is to penalize the case where \u2191* fails to flip the prediction of the classifier.\n\u2022 Joint sparsity and proximity between $T^*$ and T:\n$F_{2,1}(T^*, T) = \\frac{\u03a3_{i=[1,...,m]} 1_{t_i\\neq t^*_i}}{m}$ , \n$F_{2,2}(T^*, T) = \\frac{||T^*-T||_2}{||T^*||_2+||T||_2}$ ,\n$F_2(T^*, T) = \\frac{1}{2}(F_{2,1}(T^*, T) + F_{2,2}(T^*, T))$\nHere we normalize and combine the two indicators into one objective (F2), where the sparsity (F2,1) measures the degree of element-wise perturbation, and the second indicator F2,2 aims to quantify the scale change between \u00ce* and T.\nIn summary, the two objective functions can jointly ensure the found counterfactual solutions are valid and minimized. What is more, the proposed two objective functions are both bounded, saying F\u2081 \u2208 [0,1] and F2 \u2208 [0, 1]."}, {"title": "3.3 Locating the Sequences of Interest", "content": ""}, {"title": "3.3.1 Chromosome Representation", "content": "Previous works that utilize evolutionary computation predominantly adopt a unified chromosome representation that encodes the entire counterfactual instance. In other words, the number of variables in each solution is at least as large as the number of timestamps in the target time series. Furthermore, these approaches typically apply crossover, mutation, and other evolutionary operators directly to the chromosomes to generate the next set of feasible candidates in one-go (H\u00f6llig et al., 2022; Refoyo and Luengo, 2024). One advantage of encoding the entire time series into the individual is that it can find multiple Segment or Subsequence of Interest (SoI) at the same time.\nIn contrast, our approach reduces the complexity of candidate solutions by representing each one with only three mutable integer variables, i.e., $X = [X_1,X_2,X_3]$, regardless of the number of timestamps in the target time series. Specifically, x\u2081 and x2 denote the starting and ending indices of the single subsequence of interest considered in this individual, respectively. Meanwhile, x3 represents the index of the reference sample in the set y, which is used to guide the discovery of potential counterfactual observations for the Sol (see section 3.4).\nSimilar to previous approaches, our method can also identify multiple Sol for each target time series by generating various counterfactual solutions. Furthermore, by leveraging the concept of Pareto efficiency in multi-objective optimization, our approach ensures that the diverse Sol discovered are all equally significant.\nInitialization Several existing works explicitly incorporate low-level XAI strategies to identify the SoI in the target instance, thereby facilitating the counterfactual search process. These strategies include feature attribution methods, such as the GradCAM family (Selvaraju et al., 2017), which are employed in works like (Delaney et al., 2021; Refoyo and Luengo, 2024). Additionally, subsequence mining methods, such as Shapelet extraction (Ye and Keogh, 2009), are utilized in studies like (Li et al., 2022; Huang et al., 2024). Unlike other XAI techniques, we do not use this (possibly biased) initialization, instead the first population in our search algorithm is randomly sampled. Given a to-be-explained time series $T = {t_i \u2208 R}^{m}_{i=1}$ and its reference sample set \u03c8 of size K, the variables in a individual solution $X_i = [x_{j,1},x_{j,2},x_{j,3}]$"}, {"title": "3.3.2 Crossover", "content": "Crossover is a crucial operator in evolutionary algorithms, designed to create new offspring by combining decision variables from parent solutions. This process helps the algorithm exploit the search space within the current population. The parent solutions are chosen through a selection mechanism. In this work, we employ the well-known tournament selection method to select mated parents, and we refer interested readers to the literature (Goldberg and Deb, 1991) for more details.\nBearing in mind the idea behind crossover, our customized crossover operator is dedicated to fulfilling two additional criteria: (i) minimizing the intersection between the Sol of offspring. (ii) minimizing the total lengths of the Sol tracked by offspring.\nThe proposed crossover is demonstrated in Algorithm 3. The crossover operation by default produces two offspring. The function GetUnique returns the unique, non-duplicate values of its inputs in an array, sorted in ascending order. Notably, in line 22 of the pseudocode, we compare the total lengths of the SoI under two feasible crossover options and intentionally set the relational operator to < instead of <. This ensures that, in cases where the Sols of two parents do not overlap, their offspring will have minimal intersections. Between lines 33 and 40, we address the corner cases where the indices recorded in the parents are partially or entirely the same."}, {"title": "3.3.3 Mutation", "content": "In general, the mutation operators help to explore unforeseen areas of the solution space and prevent premature convergence. Unlike the crossover operator that mixes the SoIs from different parent solutions, the mutation operator proposed in this work aims to adjust the length of the Sol tracked by each individual selected for mutation. The pseudocode of our mutation strategy is provided in Algorithm 4.\nThe strategy employs a dynamic scaling of the Sol length for the new instance Y based on the Sol length of the previous instance X. The new length is randomly sampled from a binomial distribution Bin(n,p), where the number of trials n is twice the length of the Sol in X. This approach allows the Sol length to potentially increase almost exponentially between adjacent generations during optimization,"}, {"title": "3.4 Modeling of the New Sol", "content": "It is noteworthy that several previous works also separate counterfactual generation into two stages: the discovery of SoIs (or other forms of sensitivity analysis) and the subsequent generation or search for the actual values. While the crossover and mutation operators described above facilitate the search for optimal SoIs within the time series, the specific values"}, {"title": "4 EXPERIMENTAL SETUP", "content": "The experiments are performed using a diverse selection of time-series classification benchmarks and two popular time-series classifiers.\nThe UCR datasets (Bagnall et al., 2017) from the time-series classification archive (Chen et al., 2015) chosen in this study represent a diverse array of application domains in time-series classification, all of which are one-dimensional. Furthermore, they are categorized in Table 1 based on their domain specificity and characteristics, providing a comprehensive assessment of the proposed method's robustness and adaptability across varied types of time series data.\nFor the classification tasks within our framework, each dataset was trained and tested using two specific classifiers: the Catch22 classifier (C22) (Lubba et al., 2019) and the Supervised Time Series Forest (TSF) (Cabello et al., 2020). These classifiers were meticulously chosen not only for their computational efficiency and ease of implementation but also for their ability to produce probabilistic outputs. The generation of probabilistic outputs, rather than mere binary labels, is crucial for our methodology. It allows for the nuanced detection of subtle variations in the probability distributions across different classes. This feature is integral to our approach as it supports the generation of counterfactual explanations that are highly sensitive to minor but significant shifts in the data."}, {"title": "4.1 Hyperparameters Setup", "content": "The customized NSGA-II algorithm central to our approach is designed with several hyperparameters that can influence its operation and performance. The standard NSGA-II hyper-parameters (and their setting between brackets), population size (50), number of generations (50), the probability of crossover (0.7) and probability of mutation (0.7), are very common in evolutionary optimization algorithms, and are set based on the results of a few trials. A new hyper-parameter, Number of Reference Instances, is set to 4 in this work. This parameter specifies both the number of reference cases used to evaluate the performance of a candidate solution and the number of teachers employed to guide the search for finding counterfactual examples. These hyper-parameters could be further optimized in future work."}, {"title": "4.2 Metrics and Baselines", "content": "As introduced in section 4.2, five criteria can be used to assess the effectiveness of the counterfactual explanation algorithms. Given a to-be-explained time series $T = {t_i \u2208 R}^{m}_{i=1}$, a set of n counterfactuals candidates $\u0398 = {T^j = {t_i \u2208 R}^{m}_{i=1} }_{j=1}^{n}$, and the classifier f, we define the five evaluation metrics as follow:\n1. L1-Proximity(T, \u00ce) = $\\frac{||T-T||_1}{||T||_1+||T||_1}$\n2. L2-Proximity(T, \u00ce) = $\\frac{||T-T||_2}{||T||_2+||T||_2}$\n3. Validity(T, \u00ce | f) = $1_{f(T)\u2260f(\u00ce)}$\n4. Sparsity(T, \u00ce) = $\\frac{\u03a3_{i=[1,...,m]} 1_{t_i\u2260t_i}}{m}$\n5. Diversity(\u03a4, \u0398 | f) = $\\frac{2}{N(N-1)} \u03a3_{i=1}^{N-1} \u03a3_{j=i+1}^{N} (\u03a0_{i,j} 1_{I_I\u2260I_J})\u00b71_{f(T)\u2260f(\u00ce_i), \u00ce_i, \u00ce_j \u2208 \u0398}$"}, {"title": "5 RESULTS AND DISCUSSIONS", "content": "Our experimental results clearly demonstrate that TX-Gen consistently outperforms competing methods across several key metrics, as shown in Tables 2-5. In terms of validity, TX-Gen achieves a 100% success rate in generating valid counterfactuals across all datasets, regardless of the classifier used (Table 2). This significantly outperforms baseline methods like w-CF, which struggles with generating valid counterfactuals, achieving a mere 1% ~ 17% validity across different datasets and classifiers. TSEvo, while better than w-CF, also lags behind TX-Gen in validity, particularly when using the Catch22 classifier. AB-CF is in terms of validity only marginally worse than our proposed solution.\nSparsity is a key strength of TX-Gen, as demonstrated in Table 3. While methods like TSEvo also optimize for sparsity, TX-Gen achieves lower sparsity values across most datasets. In terms of proximity, Tables 4 and 5 show that TX-Gen performs competitively with state-of-the-art methods such as AB-CF and TSEvo, which also optimize for proximity. However, TX-Gen's use of multi-objective optimization allows it to maintain a strong proximity score while simultaneously achieving higher validity and diversity. The L1-Proximity and L2-Proximity results reflect that TX-Gen generates counterfactuals that are closer to the original time series, ensuring more interpretable and actionable insights.\nAs shown in the examples of Figure 2 and also in Table 6, our method generates a diverse set of Pareto-optimal solutions allowing for a wide range of plausible counterfactuals. All other methods only provide one counterfactual example per instance. This is particularly important in real-world applications, where generating multiple plausible alternatives can provide more comprehensive insights for end-users. The number of sub-sequences of the counterfactual examples by TX-Gen is always 1, while for TSEvo this is on average 13.8 sub-sequences, Native-Guide provides 1.8, w-CF 1.7 and AB-CF 1.8 sub-sequences on average over all datasets. In general, less sub-sequences are more informative for counterfactual examples. However, it could be a limitation that the proposed method only provides 1 sub-sequence, however, TX-Gen provides multiple examples per instance, one could combine the sub-sequences from these examples to alleviate this limitation. All results and source code can be found in our Zenodo repository\u00b9.\nIn summary, TX-Gen's use of evolutionary multi-objective optimization and its reference-guided mechanism make it more effective than existing methods across key metrics. The method not only generates more valid and sparse counterfactuals but does so with a high degree of proximity and diversity, offering a significant advancement in the generation of explainable counterfactuals for time-series classification."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we presented TX-Gen, a novel framework for generating counterfactual explanations for time-series classification tasks using evolutionary multi-objective optimization. Through extensive experimentation, we demonstrated that TX-Gen consistently outperforms state-of-the-art methods in terms of validity, sparsity, proximity, and diversity across multiple benchmark datasets. By leveraging the flexibility of the NSGA-II algorithm and incorporating a reference-guided mechanism, our approach ensures that the generated counterfactuals are both interpretable and computationally efficient.\nThe good performance of TX-Gen particularly in achieving 100% validity and maintaining high diversity while optimizing for proximity and sparsity, highlights its potential for real-world applications where model transparency is critical. Our proposed method strikes an effective balance between multiple conflicting objectives, offering a robust solution for generating meaningful counterfactuals in time-series classification.\nFuture work can explore several promising directions. First, further tuning of the hyper-parameters, particularly the number of reference instances, could lead to even greater improvements in performance. Additionally, extending TX-Gen to handle multivariate time-series data and real-time counterfactual generation could broaden its applicability to more complex, real-world scenarios."}]}