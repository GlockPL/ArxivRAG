{"title": "DITAR: Diffusion Transformer Autoregressive Modeling for Speech Generation", "authors": ["Dongya Jia", "Zhuo Chen", "Jiawei Chen", "Chenpeng Du", "Jian Wu", "Jian Cong", "Xiaobin Zhuang", "Chumin Li", "Zhen Wei", "Yuping Wang", "Yuxuan Wang"], "abstract": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.", "sections": [{"title": "1. Introduction", "content": "Recently, the autoregressive language model (LM) has demonstrated strong generative capabilities and scaling properties(Achiam et al., 2023; Touvron et al., 2023; Yang et al., 2024; Team et al., 2023), where discrete tokenization is commonly used. While discretization is natural for text, it is not as straightforward for other modalities, such as images, video, and audio. Due to bitrate limitations, discrete representation often fails to reconstruct complex modalities with high fidelity. In comparison, continuous tokens can better reconstruct the original information(Fan et al.,"}, {"title": "2. Related Works", "content": "Integrating Autoregressive Language Model and Diffusion. Language models are primarily used for discrete representations, while diffusion excels in modeling continuous distributions. Integrating them for multimodal modeling is a crucial research direction. Some efforts (Wu et al., 2023; Liu et al., 2024b; Chen et al., 2024a) enable diffusion to have autoregressive capabilities by varying the denoising rates between consecutive tokens to achieve earlier predictions for preceding tokens. Transfusion(Zhou et al., 2024) utilizes a shared transformer for both diffusion and language models, employing causal attention for discrete tokens and bidirectional attention for continuous tokens. However, it does not naturally support the autoregressive generation of continuous tokens. These approaches repurpose language model"}, {"title": "3. Method", "content": "In a nutshell, We propose DiTAR, a patch-based autoregressive system based on continuous representation. This system amalgamates the strengths of the causal-attention AR and bidirectional-attention transformer diffusion."}, {"title": "3.1. Overview", "content": "DiTAR is an autoregressive model via next-token prediction. Consider a sequence of continuous tokens x = (x1, x2, ..., xN), we can factorize the joint distribution of the sequence by the chain rule of probability:\n$$P_\\theta(x_1, x_2, ..., x_N) = \\prod_{i=1}^{N} P_\\theta (x_i | x_1, x_2, ..., x_{i-1})$$\nwhere \u03b8 denotes the parameters of an AR generative model. Noting the high similarity among adjacent continuous tokens, it is evident that a bidirectional dependency exists within local regions. Based on this discovery, we aggregate local xi into patches with a size of P, and then employ bidirectional attention to model the tokens inside each patch. We can divide the model into two parts, \u03b8\u03b1 and \u03b8\u266d: \u03b8\u03b1 denotes the autoregressive model responsible for long context learning via p\u03b8a (hi |x1, x2, ..., xi), while \u03b8\u266d denotes a bidirectional-attention diffusion transformer executing next-patch prediction via p\u03b8b (xi+1, ..., xi+P|hi), where hi is the output of language model and condition for diffusion.\nWe validate the effectiveness of DiTAR on the zero-shot text-to-speech task. Based on the formulation, we regard zero-shot TTS as a conditional continuation task for the AR model like(Chen et al., 2024b), where prompting texts, target text, and prompting speech are concatenated and fed into the model as prefix context, then the model autoregressively generates the target speech given the context."}, {"title": "3.1.2. OVERALL ARCHITECTURE", "content": "Li et al. (2024a) finds that the performance of the causal-attention autoregressive model combined with diffusion loss is significantly inferior to systems utilizing full attention. To address the issue, we propose a divide-and-conquer strategy, where a long sequence of continuous tokens is divided into multiple patches. A language model is responsible for inter-patch prediction, while a diffusion transformer handles intra-patch prediction. As shown in Figure 1, the backbone of our system is a causal-attention transformer with next-token prediction. Each patch of continuous tokens is processed with an aggregation encoder into a single vector, which is then fed into the AR model to get the output embedding ht. ht serves as the condition of the following diffusion decoder, LocDiT. Following (Li et al., 2024a), a diffusion loss is used for the output continuous tokens at training time."}, {"title": "3.2. LocDiT: Next-Patch Bidirectional Modeling", "content": "The diffusion transformer(Peebles & Xie, 2023; Liu et al., 2024a) has achieved success across numerous generative fields. These approaches leverage the full receptive field of the bidirectional transformer to generate entire samples. In our work, we propose using a bidirectional transformer diffusion, called Local Diffusion Transformer (LocDiT), to generate localized continuous token patches.\nLocDiT generates the next patch of speech given the AR's output. However, we have found that diffusion struggles to predict the next patch under these conditions. To capitalize on the context-learning potential of the diffusion transformer, we propose a context-aware diffusion approach. Specifically, as illustrated in the right half of Fig 1, historical"}, {"title": "3.3. LM Guidance", "content": "Classifier-free guidance (CFG)(Ho & Salimans, 2022) is extensively employed to enhance the condition adherence of generative models. In diffusion, the unconditional and conditional models share parameters and are jointly trained by intermittently omitting the condition during training. At inference, the outputs from these two models are merged with a parameter w balancing the trade-off between diversity and fidelity. This is equivalent to sampling under such a distribution p\u03b8 (zt, c) \u221d p\u03b8 (zt|c)p\u03b8(c|zt), where \u03b8 denotes the model parameters, c denotes the condition and zt denotes the noisy sample at the time of t.\nIn the continuous-token LMs for conditional image generation, Li et al. (2024a) incorporates the class label into the diffusion head and applies CFG which significantly enhances performance. However, such an approach lacks generality. In DiTAR, we propose a more generalized approach: we place all conditions within the LM prefix inputs and only apply CFG to the diffusion decoder via the AR's on-the-fly output. The ith output h\u2081 of the AR essentially represents all historical inputs (x0, x1, ..., xi). Specifically, we randomly replace the hi with a null embedding along the sequence in training. At inference time, we samples from the distribution p\u03b8 (zi,t|x1, ..., xi\u22121)p\u03b8 (x1, ..., xi\u22121|zi,t)w by:\n$$ \\tilde{\\epsilon_\\theta}(z_{i,t}, h_i) = (1 + w) \\epsilon_{\\theta}(z_{i,t}, h_i) \u2013 w \\epsilon_{\\theta} (z_{i,t}) $$\nwhere zi,t denotes the ith noisy sample in the sequence at the time of t for diffusion, e denotes the score estimated by the LocDiT. Operating in the velocity space with a conditional flow-matching target is also equivalent. We show that the approach significantly enhances the model's performance in Section 4.3."}, {"title": "3.4. Temperature for Continuous-Valued LMs", "content": "Temperature-based sampling, which strikes a balance between exploration and exploitation, is fundamental in discrete-valued LMs. However, its application in continuous-valued LMs has been less extensively studied. Li et al. (2024a) proposes a sampling method based on the DDPM reversed process(Ho et al., 2020), which is a first-order SDE solver, where the temperature is defined as the noise scaling factor at each step. Due to the stochastic nature of the Wiener process, SDE solvers require small step sizes, necessitating a higher number of steps for convergence(Song et al., 2020b). Additionally, this concept of temperature cannot be readily extended to the more widely used and faster ODE solvers prevalent in contemporary applications (Lu et al., 2022a;b; Song et al., 2020a). To address these issues, we propose a new sampling method with a new definition of temperature, which is compatible with the ODE solvers.\nWe define the temperature \u03c4\u2208 [0,1] as the time point to introduce noise while solving the reverse ODE of diffusion. Specifically, consider a Gaussian diffusion forward process defined on per-patch by xt = atxo + \u03c3\u03c4\u03b5, where x0 ~ qdata (x0), \u03b5 ~ N(0, I), t \u2208 [0, 1], at and ot collectively defines the flow path. At \u0442 = 1, the sampling process is equivalent to the standard ODE sampling process, where we solve the reverse ODE dxt = ve(xt, t)dt with respect to dt from 1 to 0, and x1 ~ N(0, I). At r = 0, no random noise is introduced leading to a completely deterministic process. Considering that 0 is the value with the highest likelihood in standard Gaussian distribution, we define the greedy sampling by sampling from x\u2081 = 0, thus ensuring determinism.\nWhen 0 <\u0442 < 1, we introduce random noise at T by using the forward process to diffuse an estimated xo. Eqn 3 and 4 summarize the iterative process, where the Euler method is used as the default ODE solver for illustration.\n$$X_1 ~\\begin{cases}N(0,I) & \\text{if } \\tau = 1 \\\\\\N & \\text{if } 0 < \\tau < 1 \\end{cases}$$\n$$X_t = \\begin{cases}X_{t+\\Delta t} - V_\\theta(X_{t+\\Delta t},t + \\Delta t) \\Delta t & \\text{if } t \\ne \\tau \\\\\\A_t X(X_{t+\\Delta t}, t + \\Delta t) + \\sigma t \\epsilon & \\text{if } t = \\tau\\end{cases}$$\nwhere x represents the estimated xo, which can be derived from the estimated velocity or score through a linear transformation. The derivation process is detailed in Appendix A.3. In Section 4.3, we demonstrate that 7 effectively balances diversity and stability. The sampling process is summarized in Algorithm 1."}, {"title": "3.5. Implemenations", "content": null}, {"title": "3.5.1. CONTINUOUS SPEECH TOKENIZATION", "content": "Following LDM(Rombach et al., 2022), we use a variational auto-encoder (VAE)(Kingma, 2013) to convert the waveform into the distribution of latent z, represented by mean and variance. The encoder of the VAE consists of multiple layers of the convolutional network, and the decoder's architecture follows BigVGAN(Lee et al., 2022). The adversarial training scheme also follows Lee et al. (2022). We adopt the multi-period discriminator (MPD) and multi-scale discriminator (MSD) proposed by Kong et al. (2020) as our discriminators. In our setup, the 24000hz waveform is compressed into 40hz latent with a dimension of 64."}, {"title": "3.5.2. MODEL", "content": "DiTAR consists of three modules: aggregation encoder, language model, and decoder (LocDiT). In our implementation, all of them are based on a transformer architecture. Specifically, both the encoder and decoder employ bidirectional attention masks, while the LM utilizes a causal attention mask. All transformers adopt the Pre-Norm(Xiong et al., 2020) architecture and utilize RMSNorm(Zhang & Sennrich, 2019) and ROPE (Su et al., 2024). Each patch of continuous tokens, together with a learnable special token positioned at the beginning of the sequence similar to (Devlin, 2018), is fed into the aggregation encoder. The output corresponding to the special token's position serves as the aggregation embedding. Aggregation embeddings from different patches"}, {"title": "3.5.3. DIFFUSION FORMULATION", "content": "Following Song et al. (2020b); Lu & Song (2024), we adopt a variance-preserving diffusion process defined by\n$$x_t = A_tx_0 + \\sigma t \\epsilon$$\n$$ \\pi_t = cos(\\frac{\\pi t}{2}) x_0 + sin(\\frac{\\pi t}{2}) \\epsilon$$\nwhere xo ~ q(x0) denotes the data, \u025b ~ N(0, I) denotes the standard Gaussian noise, t \u2208 [0, 1]. We employ a condi-"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. SOTA Performance in Zero-Shot TTS", "content": "In this subsection, we benchmark DiTAR against leading systems and demonstrate its state-of-the-art performance."}, {"title": "4.1.1. SETUP", "content": "Many studies evaluate zero-shot TTS with inconsistent training sets, prompt audios, texts, and tools, diluting the value of comparisons. We standardize these variables for a more objective and fair assessment across systems."}, {"title": "4.1.2. EXPERIMENTAL RESULTS", "content": "We conduct a multi-dimensional comparison of DiTAR with other baseline works. For objective metrics, Table 1 presents the evaluation results on LibriSpeech test-clean, while Table 6 in B.1 shows the results for Seed-ZH and Seed-EN. For subjective evaluation, we invite 10 English experts to rate the generated audio. For N-MOS, Q-MOS, and S-MOS metrics, the generated audios are rated on a scale of 1 to 5. For CMOS, experts compare the generated audio against the ground truth (GT) and assign scores from -2 to 2. Subjective results are detailed in Table 2."}, {"title": "4.2. Scaling Behaviors", "content": "In this subsection, we explore DiTAR's scaling properties for zero-shot speech generation, focusing on model architecture and training data, using Seed-EN as the test set.\nThe performance of DiTAR consistently enhances as either data size or model size increases. We conduct scaling experiments concerning both data and model size. We expand the training data from 20k to 280k hours with a"}, {"title": "4.3. Method Analysis and Ablation Study", "content": "In this subsection, we conduct a detailed analysis of the various components of DiTAR. Unless specified otherwise, we default to using DiTAR with 0.6 billion parameters, a patch size of 4, and NFE=10, tested on the Seed-EN dataset.\nPatch Size. LocDiT utilizes bidirectional attention to generate the next patch. To investigate the impact of patch size, we vary it while keeping the model's total parameter count constant. As illustrated in Figure 3, performance declines when patch sizes are either too large or too small. Excessively small patches diminish the model's bidirectional attention capability, forcing reliance on causal-attention AR and degrading performance. This may explain the poor performance of causal AR with diffusion loss as noted in Li et al. (2024a). Conversely, overly large patches turn LocDiT into a bottleneck, necessitating increased parameters."}, {"title": "4.4. Inference Analysis", "content": null}, {"title": "4.4.1. THE IMPACT OF TEMPERATURE", "content": "Temperature is vital for balancing diversity and determinism during language model inference, yet its definition for continuous-valued LMs remains underexplored. In this study, we define temperature 7 as the point at which random sampling is introduced during the reverse ODE solution.\nTo explore how balances diversity and stability, we synthesize the same text 500 times under different T settings to assess voice diversity. We use only the text as DiTAR's prefix input prompting the model to generate speech in random voices autoregressively. We then use WavLM-large to"}, {"title": "4.4.2. EFFICIENCY", "content": "DiTAR, blending a language model with a diffusion transformer, inherits features from both components. DiTAR drastically cuts computational demands by allocating the majority of its parameters to the language model and fewer to LocDiT.\nWe compare the inference performance of NAR (a diffusion transformer) and DiTAR under the same parameter sizes. During inference, all systems use CFG with NFE = 10. We evaluate throughput, defined as the duration of audio generated per unit time, across various batch sizes by inferring 10 seconds of audio on an A100 GPU."}, {"title": "5. Conclusion", "content": "In this work, we propose DiTAR which utilizes the diffusion transformer's capacity for high-quality generation to create localized patches while maintaining the core autoregressive features of language models. For inference, we introduce temperature as the introduction time point for noise while solving the reverse diffusion ODE. Applied to zero-shot speech synthesis, DiTAR achieves SOTA robustness, speaker similarity, and naturalness with substantially lower computational requirements."}, {"title": "A. Implementation Details", "content": null}, {"title": "A.1. Training Details", "content": "Regarding data processing for LibriLight, since it lacks transcriptions, we use our internal ASR system to transcribe this dataset and then convert the text into phonemes for use. For Emilia, we perform Grapheme-to-Phoneme (G2P) conversion on the official transcriptions provided.\nWe utilize 16 A100 GPUs, each processing a batch size of 15K tokens, and train DiTAR for 0.5M steps. The AdamW optimizer is employed with a constant learning rate of 1e-4, Bt = 0.9, and B2 = 0.99. For DiTAR with 1B parameters, we utilize 32 A100 GPUs with a batch size of 7.5k per GPU."}, {"title": "A.2. Model Configuration for Scaling", "content": "During the validation of DiTAR's scaling behavior, we trained models of four different sizes, ranging from 0.1 billion to 1 billion parameters. Specific hyperparameter configurations are detailed in Table 5."}, {"title": "A.3. Derivation of xo in temperature sampling for different parameterized diffusion", "content": "As previously discussed in Eq 5, xe denotes the predicted data, which can be derived under different parameterizations of diffusion. Consisdering a diffusion process defined by xt = Atx0 + \u03c3\u03c4\u03b5, where x0 ~ q(x0) denotes the data, \u025b ~ \u039d(0,I) denotes the standard Gaussian noise, t \u2208 [0, 1].\nFor e-prediction mode,\n$$x_0(x_t,t) = \\frac{x_t - \\sigma_t \\epsilon_\\theta(x,t)}{A_t}$$\nFor 20-prediction mode, xe (xt, t) is exactly model's prediction.\nFor v-prediction mode, also known as flow-matching. Next, we will proceed with a step-by-step deduction. Based on the definition of v, we can derive the following:\n$$v = \\alpha_\\tau x_0 + \\sigma_\\tau \\epsilon$$\n$$= \\alpha_t x_0 + \\frac{\\sigma_\\tau(x_t - A_tx_0)}{\\sigma_t}$$\n$$=(\\alpha_t - \\frac{\\sigma_\\tau A_t}{\\sigma_t}) x_0 + \\frac{\\sigma_\\tau}{\\sigma_t}x_t$$\nAfter rearanging v and xo, we get:\n$$x_0 = \\frac{\\sigma_t x_t - \\sigma_\\tau v}{(\\alpha_t \\sigma_t - \\alpha_t \\sigma_\\tau)}$$"}, {"title": "A.4. The process of temperature sampling", "content": "Here, we present our temperature-based sampling process in the form of an algorithm flowchart, shown in Algorithm 1."}, {"title": "A.5. Calculation of FLOPS", "content": "When calculating FLOPs for all systems, we use a 3-second audio prompt to synthesize 10 seconds of audio. Assuming a text frame rate of 7Hz, the prompt's text length is 21, and the target text length is 70. We focus solely on the computationally dominant components, including causal transformers, non-causal transformers, and convolutional layers. For causal transformers, calculations account for the use of KV cache. The multi-head setup does not impact FLOPs, so we uniformly assume that the number of heads is 1. Considering that bias and normalization layers contribute minimally to the overall FLOPs of the transformer, we will temporarily disregard them. Matrix multiplication involves an equal number of additions and multiplications, so the result should be multiplied by 2. For the diffusion part, the corresponding FLOPs needs to be multiplied by the Number of Function Evaluations (NFE). Additionally, Classifier-Free Guidance (CFG) incurs double the computational cost.\nFor a one-dimensional convolution network with N layers, a hidden channel size of C, a kernel size of K, and the input length of T, the FLOPs can be calculated as follows:\n$$FLOPS = [C, KT] \\times [KT, C] \\times N = 2C^2 KTN$$\nFor a non-causal transformer with N layers, a hidden size of C, an FFN intermediate hidden size of Cmid, and the input"}]}