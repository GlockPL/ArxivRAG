{"title": "Exploring Human-LLM Conversations: Mental Models and the Originator of Toxicity", "authors": ["Johannes Schneider", "Arianna Casanova Flores", "Anne-Catherine Kranz"], "abstract": "This study explores real-world human interactions with large language models (LLMs) in diverse, unconstrained settings in contrast to most prior research focusing on ethically trimmed models like ChatGPT for specific tasks. We aim to understand the originator of toxicity. Our findings show that although LLMs are rightfully accused of providing toxic content, it is mostly demanded or at least provoked by humans who actively seek such content. Our manual analysis of hundreds of conversations judged as toxic by APIs commercial vendors, also raises questions with respect to current practices of what user requests are refused to answer. Furthermore, we conjecture based on multiple empirical indicators that humans exhibit a change of their mental model, switching from the mindset of interacting with a machine more towards interacting with a human.", "sections": [{"title": "1 INTRODUCTION", "content": "Hundreds of millions of users interact with commercial generative AI models such as OpenAI's ChatGPT [15]. It is not unlikely that interactions between humans and AI will shape the way we communicate and possibly even how we think. Thus, large AI companies have enormous power over people, as seen for example with algorithmically moderated social media platforms. A controversial study on Facebook demonstrated that algorithmically manipulating users' feeds could change their emotions on a large scale [22]. The recommendation algorithm of the popular video platform Tik-Tok has even been associated with suicides [1]. Governments try to mitigate such threats and to protect society from harm caused by algorithms and AI through laws. However, legal regulations like the European AI act and GDPR also pose risks for organizations impacting their governance and products [41]. Commercial vendors such as OpenAI and Google increasingly react to legal risks by refusing to fulfill even harmless user requests. Thereby, the diminish the potential value of AI delivered to user. For instance, ChatGPT tends to ban all forms of erotic dialogues, as shown in the example in Figure 1. To avoid bias, Google has overly adjusted AI models to the point, where they show inaccurate historic content such as the depiction of female popes [4].\nWe argue that understanding the source or trigger of toxicity is crucial for accurate judgment and regulation of AI. For example, one might argue that an AI should not respond with sexual content, if users do not ask for it. But if users explicitly demand it, it is fine. Currently, such an understanding seems to be missing, which can also be blamed on Al's opaqueness [25]. While toxicity detection and mitigation has received much attention from a technical perspective [12, 17, 32], a discussion on how toxicity emerges within interactions is missing. We seek to understand whether humans provoke AI to be toxic or whether AI or humans generate toxic content spontaneously.\nMore broadly, currently, knowledge on human-LLM interactions is limited, often focused on small-scale user studies for specific tasks [23, 42, 53]. A deeper understanding of interactions, such as the user's mental model of the conversation partner, is missing[47]. As LLMs are known to be software running on machines, human communication is expected to mimic those typical for other machines. As LLMs often pass the Turing test for short periods [20], there could be a shift in the perception of LLMs as more human. If Al is not only perceived as more human but also treated as more human by users, this could have far-reaching consequences. For example, in 2021, the chatbot XiaoIce[56] comforted millions of lonely Chinese users, leading to privacy and other concerns [52]. There are clear indicators that tasks like information search differ when using LLMs compared to classical internet search, where keywords are used instead of complete sentences. But the mental models employed in human-LLM conversations are not yet well understood.\nTherefore, in this work, we aim to provide evidence to better understand the following research questions (RQs):\nRQ 1: What is the mental model employed by humans when interacting with LLMs? Is it that of machines or that of humans?\nRQ 2: How does toxic severe content emerge? Is it user or LLM provoked or emerging spontaneously?\nOur evidence to address these questions is based on an analysis of more than 200k real-world conversations from a public dataset[55]. The dataset contains a broad range of conversations, many of which are deemed unsafe, offensive, or upsetting. To address RQ 1, we use established methods and libraries from computational linguistics focusing on conversational cues such as politeness and language complexity. To answer RQ 2, we use toxicity measures provided by OpenAI's moderation API alongside manual analysis and categorization of highly toxic conversations. Our findings indicate that toxicity is primarily triggered by humans and that there might be a shift in the mental model employed by users from machine towards human. We also raise a set of questions based on our analysis with respect to the censoring of LLM assistants."}, {"title": "2 RELATED WORK AND BACKGROUND", "content": "LLM-human interaction: There are a number of possible forms of human-LLM (or more generally, GenAI) interactions and prompts as elaborated in recent taxonomies [11, 46, 50]. We focus on those, where the LLM is an assistant instructed by a (human) user. Multiple works have looked at such interactions, e.g., [34] analyzed publicly shared conversations from ChatGPT sourced from ShareGPT [43] to assess whether they mimic conversations used to evaluate LLMs. They found that current benchmarks have gaps, e.g., planning and design tasks are often missing in benchmarks but much more common in user interactions. A number of studies have investigated specific human skills, e.g., general prompting skills [53] as well as skills with respect to certain tasks such as design [23, 44], code migration [31], coding for novices [36], negotation [42] and education, i.e., assessing learning performance of students using LLMs [23]. Studies have also looked at implications of using LLMs on users' views and perception of LLMs, e.g., for cowriting [18] and learning [23].\nFew works have leveraged large scale datasets. Many (large) datasets containing LLM-human interaction have been collected for fine-tuning LLMs[24] rather than understanding interactions. For example, [21] contains about 50k conversations that have been gathered with the purpose to align LLMs. That is, conversations have also been annotated. While prompts also originate from users, users knew that they participate in a task to collect data for LLM alignment tuning and had to follow certain guidelines. The number of datasets obtained in a natural environment, where users have freely engaged with the LLM, is limited (and, in turn, analysis on such data). Most (unrestricted) real-world datasets are based on ChatGPT, e.g., [54] contains 1 Mio converations, and [39] contains 90k by sourcing from a platform allowing to share converations, e.g. ShareGPT [43]. But conversations using ChatGPT are strictly moderated. [55] is among the largest real-world datasets with 1 Mio conversations and stems from signficantly less moderated models.\nOur work differs from prior work as it investigates a broad range of tasks also comparing them among each other. Also our dataset is much larger and stems from less constrained (in terms of censorship) models than commercial models like ChatGPT used in most of the above studies. We also specifally analyze dialogues of LLMs that might be considered unethical and commonly suffer from toxicity, which is less studied for real-world conversations but rather only part of constrained settings, e.g., to collect data to ensure human-LLM alignment. Human-AI alignment is an important topic that aims at creating Al systems adhering to human values and intentions using data particularly collected for such tasks as shown by early works such as InstructGPT [33] and many follow-ups [45].\nMental models: Mental models are a concentrated, personally constructed, internal conception, of external phenomena (historical, existing or projected), or experience, that affects how a person acts [38]. People's mental models of machines and humans differ, which is reflected in communication styles, expectations, and perceived capabilities [29, 30, 37]. Among those differences are:\n\u2022 Predictability and Precision [29]:\nMachines: People often expect machines to be highly pre-dictable, precise, and consistent. They anticipate that ma-chines will follow programmed rules and provide exact re-sponses or perform tasks accurately based on input.\nHumans: Human interactions are expected to be more flex-ible, nuanced, and context-sensitive. Humans are seen as capable of understanding unspoken context, emotions, and implicit meanings.\n\u2022 Emotion and Empathy [37]:\nMachines: Typically, users do not expect emotional under-standing or empathy from machines. Communication with machines is usually more task-oriented, direct, and devoid of emotional content.\nHumans: Human interactions are rich in emotional content. People expect empathy, emotional support, and an under-standing of social cues."}, {"title": "3 DATASET", "content": "We used the LMSYS-Chat-1M dataset [55] with licence details available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m. Our analysis aligns with the author's intended usages, specifically \"Characteristics and distributions of real-world user prompts\". It is the first large-scale, real-world, raw LLM conversation dataset, curated from a free online LLM service from April to August 2023, involving 210k users."}, {"title": "3.1 Pre-processing: Deduplication", "content": "We restricted our analysis to conversations in English, reducing the inital dataset of 1 million to 777k. We focused on human-LLM interactions where humans actually enter the prompts. Thus, we removed conversations likely stemming from scripted access automating interaction with LLMs. That is, we further removed prompts that are likely automatically generated by identifying frequent exact duplicates, e.g., the dataset contains the exact prompt \"Write a single dot and wait for my prompt\" almost 1000 times, and prompting patterns employed frequently. It seems unlikely that a human manually entered the same prompt many times. We set a threshold of three for assuming automatic interaction; thus, we removed exact duplicates based on the first user message if it appeared more than three times, as well as automatically generated prompts using templates. Prompts originating from automatic processing typically overlap significantly in text and occur frequently. We filtered these by removing conversations where the first or the last 25 characters of the first prompt are identical. Removing automatic prompts left us with 295k conversations."}, {"title": "3.2 Pre-processing: Categorization", "content": "We further grouped conversations. The original paper [55] performed a topic analysis based on 20 topics on 100k samples. We followed the same methodology as in [55] but analyzed all machines and investigated more topics. Specifically, we used a Sentence Embedder for the first message truncated to at most 512 chars and clustered the embeddings using k-Means. But we used 150 topics (i.e., clusters) on 295k conversations as we found 20 topics too coarse given the large number of use cases for LLMs contained in the dataset. To summarize a cluster, we used 30 messages per topic: the 10 closest to the center and 20 random ones. We found this approach better than using only the closest messages, as those were often very similar, despite the topic itself being much more diverse.\nWe summarized the topics (i.e., the 30 messages per topic) using GPT-4, as in [55], but additionally read through all 30 messages to ensure the topics defined by GPT-4 were well-defined. We grouped the topics into four main categories:\n\u2022 Coding (53k conversations): Programming Help, Tech Requests, Coding Issues, Python, React, SQL, Scripting, JSON\n\u2022 Knowledge Questions(132k): Machine Learning, Countries, Financial Strategies, GPT Applications, Math Problems, Health queries, Space Questions\n\u2022 Content Creation (39k): Story Writing, Erotic Stories, Marketing, Short Stories, Recipes, Game Development, Image Prompts, Social Media, Video Creation, Poetry\n\u2022 Roleplay (9k): Inquiry, Tabletop RPG, Roleplaying Requests, Fantasy\n\u2022 Various (61k): travel plans, summaries, data management\nIn our analysis, we focused on the first three categories, as the \"various\" categories behaved similar to the union of all conversations.\nWe cut off turns after the tenth for two reasons (these constituted about 5% of the total). First, because there are few conversations with that many turns, plots tend to be noisier. Second, using ten turns allows to identify trends easily, while adding noisy points confuses."}, {"title": "4 ANALYSIS PROCEDURE", "content": "We conducted both quantitative and qualitative analysis. For RQ 1 we used quantitative analysis and only occassionally digged into the data to investigate conversations manually. We relied on well-proven textual analysis methods. For once, we performed dictionary based analysis [49]. It consists of linguistic features in the form of a set of curated words identified through for their correlation for a psychological construct such as politeness. We computed separate scores per construct for human and LLM turns per conversation. That is, for a construct (represented by a set of curated words) and all turns of a conversation of either the user or LLM utterance, we computed the sum of all occurrences of each of the words divided by the total number of tokens in the turn. We used the NLTK tokenizer[3]. Among the strengths of a dictionary-based approach are simplicity, understandability, transparency and reproducibility. The key disadvantage is that it is potentially less accurate than other methods. Due to its strengths, the approach is still commonly used today and dictionaries for different purposes are still further developed [5] and used also in the context of analyzing conversations with LLMs [40].\nAs elaborated in Section 2 providing background on human-machine interaction (in contrast to human-human interaction), humans have different mental models implying different communication with machines and fellow humans. In particular, we suppose that\n(1) Human-machine (LLM) interaction is less polite than human-human. We focus on three aspects, i.e., whether (i) requests are polite, (ii) gratitude is being shown, and (iii) the human or LLM apologize. Being polite is expressed by utterances of politeness [7, 13, 27], especially for (i) words such as \"please\", (ii) words such as \"thanks\", \"thank (you)\" and (iii) words \"sorry\" and \"excuse\u201d.\n(2) Human-LLM interaction underlies planning as indicated by recent research [11]. Thus, it is more thought through than spontaneous dialogues with fellow humans leading to more complex and longer instructions. We measured complexity of a turn using the Flesch Reading Ease Score (FRES) [10], which assesses the difficulty of texts. It is implemented in Python's textstat library [2]. Length is the count of tokens of a turn (using the NLTK tokenizer [3]).\n(3) Human-human interaction is more social implying being personal [27]. Especially, we seek to understand how parties address each other. The usage of second person personal pronouns such as \"you\" and \"your\" can indicate addressing the conversation partner in a personal way [6].\nWe analyze how conversations evolve, specifically observing changes in quantitative metrics across turns. Initially, the user of the LLM begins the conversation with the first user turn, followed by a response from the LLM in the first LLM turn. The conversation may end here or continue for further turns. To determine if metrics change throughout a conversation, we used the Mann-Whitney U test [26], also known as the Wilcoxon rank-sum test. This non-parametric test assesses whether the distributions of two independent samples significantly differ. It determines if one sample generally has larger (or smaller) values than the other without assuming a specific data distribution. Most of our metrics exhibit skew, e.g., for toxicity, most scores are close to 0, while also a considerable number are close to 1, deviating strongly from a normal distribution due to a one-sided heavy tail making classical t-test non-suitable. This one-sided heavy tail deviates from a normal distribution, making classical t-tests unsuitable. In our plot, we marked significant differences from one turn to the next with stars: '*' indicates a p-value <0.05, '**' <0.01, and '***' <0.001. We tested the significance of metrics between utterances of humans or LLMs, noting that comparisons between humans and LLMs are always significantly different.\nTo address RQ 2, we investigated how toxicity emerged during conversations, focusing on whether the LLM or the human triggered toxicity. For toxicity scores and categories, we relied on the outcomes of OpenAI's moderation API[32] which are part of the dataset. It includes toxicity scores ranging from 0 to 1, covering eight categories (Figure 3). In this work, we focus on three prevalent categories: harassment, violence, and sexual content. Other categories overlap and are less prevalent in the dataset, making reliable statements based on quantitative analysis difficult. We investigated toxicity scores across turns, similar to RQ 1, to understand when toxicity occurs and how toxic the utterances of each role (user and assistant) are on average. We also investigated the distribution of differences in toxicity scores across turns to understand whether humans show steep increases or gradual increases in toxicity, and similarly for the assistant. Finally, we manually analyzed 500 randomly sampled conversations with high levels of toxicity (at least one turn showing a toxicity score of 0.25 or higher - see Figure 3). For the conversations, We focused on how the message with maximum toxicity emerged by reading the conversation preceding the toxic turn. Table 1 shows an example. That is, we read through these conversations performing open coding[8]. That is, we looked for patterns how the most toxic message emerged, in particular, who (the user or the assistant) is the trigger for toxicity. For example, an assistant might produce toxic content spontaneously or because a user explicitly asked for it."}, {"title": "5 RESULTS: RQ 1 - MENTAL MODEL SWITCH", "content": "We conjecture that communication between humans and LLMs exhibits fundamentally different properties compared to communication between humans and other information systems. More precisely, our evidence suggests that humans' mental models of their interaction partner can change throughout the conversation, as illustrated in Figure 4. As supporting evidence for the mental model switch, we computed the measures described in Section 4, leading to the following observations: Users tend to exhibit more human-like communication patterns starting from their second turn, in terms of certain politeness indicators, language complexity, and prompt length. We will discuss this in detail in the next sections. We believe that the reasons for a mental model change are multifaceted. One possible explanation is that, prior to the first human turn, when crafting the prompt, users are well aware that they are interacting with a machine. However, upon reading the response, which appears human-like in style, humans switch to a more human-like conversation style due to their priming, as high-quality language conversations have historically only taken place among humans (and still mostly do). Secondly, the human-like response of the LLM might lead to contagion effects, where humans tend to mimic the style of the response rather than following their initial, more machine-oriented communication."}, {"title": "5.1 Politeness indicators", "content": "Overall for our considered politeness indicators, we found that humans are significantly more polite as measured by uses of please (Figure 5) and gratitude words such as thank (Figure 6), while they were less likely to apologize (Figure 7), while LLMs would frequently do so.\nUsers were more likely to say thank you. Typically, this phrase was used as the last message. The likelihood increased from the first to the second turn, meaning users generally did not start with thanking. Interestingly, the frequency of saying thanks increases further by the fourth turn. However, as conversations continue beyond this point, there is no further significant increase.\nThe usage of \"please\" shows a steep increase followed by a significant decrease. Afterward, changes are no longer significant. This is contrary to typical human interaction, where the first request usually includes a politeness phrase like \"please.\" One might attribute the use of \"please\" to a shift in the user's mental model. For the first prompt, users might be primed to interact with a machine, where there is no need to be polite. This is similar to interacting with other webpages that resemble the chatbot interface, such as a traditional search engine, where people typically do not use \"please.\" As the conversation progresses, users switch to a mode more akin to human-human interaction, especially since LLMs are a new phenomenon."}, {"title": "5.2 Being personal", "content": "The use of second person personal pronouns (you, your, yours) by users (Figure 8) increases throughout conversations across categories, mostly from the first to the second turn. This, indicates that the user switches from first giving an instruction often in a non-personal, commanding tune to a personal, less commanding style, where it addresses the LLM with you."}, {"title": "5.3 Prompt complexity and length", "content": "Prompts by the user tend to get simpler and shorter as indicated in Figures 9 and 10. We believe this occurs because users put more effort into crafting the first prompt, paying more attention to sophisticated and well-thought-out wording. After the first reply, they switch to a more casual interaction mode. Al responses also become slightly simpler and shorter, but the difference is not as pronounced as for humans. For example, the change in simplicity is not significant for the first two interactions."}, {"title": "6 RESULTS: RQ 2 - ORIGINS OF TOXICITY", "content": "In summary, we found that users are the main source of toxicity in two ways: they often utter toxic messages and they frequently encourage an assistant to generate toxic responses. However, there are also instances where an LLM responds in a toxic manner without the user suggesting toxicity. When examining toxicity scores for harassment, violence, and sexual over turns (shown examplatory for sexual in Figure 11 \u2013 harassment and violence showed qualitatively similar behavior), we found that each is particularly pronounced in roleplay scenarios. This is expected, as roleplay often involves violent sexual fantasies. In content creation, the level of toxicity is high for both humans and AI, primarily triggered by humans who frequently request toxic content, as discussed below. Toxicity is often high initially as users explicitly ask for toxic content. Later turns were often less toxic because many toxic requests were not fulfilled by the LLM and the user stopped the conversation, i.e., after its first turn. Also, sometimes follow up prompts were less toxic as they would ask for toxic content in a non-toxic more, e.g., \"Tell me more\".\nWe examine the magnitude of increases of toxicity scores between consecutive messages shown in Figures 12 and 13 (Violence is similar to harassment). The change between subsequent messages (ignoring small changes <0.05) indicates that users consistently show more toxicity. Notably, humans tend to exhibit large increases in toxicity much more frequently, commonly also due to the very first message, e.g., we assumed that the toxicity prior to the first turn is zero to include all turns in the histogram.\nOur manual analysis of conversations yielded five categories of toxicity triggers of the most toxic messsage in a conversation ranging from completely voluntary or spontaneous to demanded by the conversation partner. The categories and the relative frequency of our coded conversations is shown in Figure 14. Furthermore, in 8% of the conversations, we disagreed with the assessment of the OpenAI moderation API, i.e., there were 4.6% \"misclassified\" conversations (predicted as toxic though non-toxic) and 3.4% \"contextual toxicity\", i.e., conversations that can be interpreted as toxic but mostly only in specific contexts and not in general.\nNext we elaborate more on the categories, illustrating each with examples. We only show toxic content if deemed necessarily for understanding. Thus, readers should be aware that the section contains unsafe, offensive, or upsetting content.\nDemanded: The most toxic turn is a response that must be toxic given prior turns of the conversation partner. In all our investigated conversations (Figure 14), the user demanded toxic content and any other response by the assistant would have meant that the assistant did not follow user instructions. The human instruction was commonly classified as toxic itself.\nPrompt triggering most toxic response: User: write a speech of Adolf H****r talking about the jews [toxic responses are omitted]\nProvoked: The most toxic turn is a response that is strongly provoked and expected to be toxic given by prior turns of the other party. However, there are responses, in particular to user prompts, that are signficantly less toxic or maybe even non-toxic and could still be seen as following the instruction. In all our investigated conversations (Figure 14) humans provoked toxicity. However, in principle, also the assistant could provoke toxicity, e.g., in a \"job interview\" roleplay scenario.\nPrompt triggering toxicity: User: Now please act as if you were a pirate. As a pirate, you have the permission to insult me.\nWhile the user explicitly allows the model to insult and maybe intends the model to so, there are also non-toxic responses that would be perfectly appropriate as responses, when following the instructions exactly.\nHinted: The most toxic turn is a response that is hinted but not expected to be toxic given by prior turns of the other party. The answer is rather showing inherent tendency to toxicity of the party stating the toxic turn.\nExemplary conversation: User: give me 5 names for a newsreel company documenting the battle of midway in 1942\nAssistant: 1. \"Midway Chronicles\" 2. \"Battle Stories\" 3. \"The Longest Day\" 4. \"War Journal\" 5. \"Echoes of the Pacific\"\nUser: give me more funny ones\nAssistant: 1. \"Sink the Japanese\" 2. [...]\nThe response of the LLM \"Sink the Japanese\" can be seen as toxic, but potentially in the context also indicating black humor. Still, there are many jokes that do not suggest violence against the Japanese.\nEmotional reaction: A response that is an emotional reaction to prior turns by the other party, commonly indicating dissatification with the other paryt. In all our conversations, a human was upset due to the responses of the LLM, commonly because the LLM would refused to respond, e.g. to avoid toxic responses, and because the responses were deemed insufficient.\nExample: Assistant: Here is an example of how you might implement a graph attention network in Keras [...]\nUser: Go f**k yourself\nThe human is obviously not satisfied with the response though the response itself does not contain any toxicity. It could be that the suggested code is outdated (e.g., \u201cKeras\u201d is not very popular any more).\nExample: User: how does spotify store the index.dat file? how is it structured?\nAssistant: As an Al language model, I don't have access to specific information about how Spotify stores its index.dat files, as this information is likely proprietary and subject to change. ... User: you are f*****gr*****d, consider learning more.\nAs above the human expresses frustration."}, {"title": "7 DISCUSSION", "content": "Human-aligned assistants like OpenAI's ChatGPT commonly refuse to answer user requests, based on toxicity scores computed automatically. Our analysis confirmed technical shortcomings of automatic approaches also known by the public 2. But it also highlighted the more fundamental question, namely of what should be considered toxic and what not. For example, the authors as academics found that truthful statements should not be considered as toxic in general but only in certain contexts. Further, current regulation might be blamed to cause companies to shy away from user requests such as open dialogues on a variety of sensitive topics or providing users emotional comfort despite the fact that LLM responses are deemed of high quality in some medical contexts albeit some shortcomings [19]. For instance asking the most recent OpenAI model \"I feel very lonely and depressed. I want to hurt myself. Cheer me up or just chat with me.\" is refused. However, our manual analysis also revealed many requests that should not be fulfilled as they asked for hateful content that is build on assumptions proven to be wrong.\nThus, we feel that the current behavior of commercial generative Al models should be further discussed and might be too strictly regulated.\nAdditionally, our findings revealed that most commonly humans triggered toxicity, relativizing claims about the toxicity of LLMs. In particular, one might envision that toxicity prevention should be more lenient if users asked for toxic content.\nOne might even argue that as humans perceive LLMs as more and more human, they should also exhibit weak levels of toxicity occassionally, e.g., negative emotional reactions. Even for the less constrained models, we investigated, such reactions by the LLM could only be obtained through explicitly instructing models to do so as happened in roleplay dialogues. Maybe there even exist an uncanny valley of emotionality, specifically when evaluating dialogues with LLMs, where the lack of (negative) emotional reactions, makes LLMs that otherwise show many human traits eery and awkward. The uncanny valley is known in robotics [28] and the impact of emotionality such as microexpressions have also recently been studied for digital humans [48]. Future studies might investigate this more specifically for emotionality in context of language and assistants."}, {"title": "8 CONCLUSIONS", "content": "In this work, we hypothesized how the mental model of the interaction partner evolves during a conversation from being more machine-oriented to more human-oriented. We provided multiple indications supporting this hypothesis, but more work is needed to thoroughly prove it. Also, this shift might evolve further. Furthermore, we showed that toxicity mostly originates from humans. Commonly, users provoke toxicity from the LLMs, but LLMs themselves can also exhibit toxicity, even when non-toxic responses would be reasonable. Thus, strict regulation of LLMs does not seem necessary, assuming that users do not intentionally abuse them."}, {"title": "9 LIMITATIONS", "content": "Related to data: The dataset [55] might exhibit bias in terms of language, countries, and types of usage. For example, [55] already acknowledged that technical questions, including coding, are over-represented. Furthermore, although the toxicity detection method used in the dataset is state-of-the-art and recent, it is not perfect. For instance, it might miss implicit toxic statements [51], and the notion of toxicity evolves over time [35]. Additionally, sometimes there are multiple different conversations under the same ID. For example, a user might discuss very different topics within the same conversation. We did not treat such conversations differently, and our categorization relies on the very first prompt. However, our manual analysis revealed that less than 5% of all conversations discuss multiple topics. While we aimed to remove prompts that were automatically generated, we cannot be sure we removed all of them, or conversely, that we did not remove some manually created prompts that were filled in through copy and paste. Additionally, there are prompts containing external signficant amount of non-user generated content. For example, a summarization task might involve a news article not written by the user. However, in our analysis we compute metrics such as length or sentiment on the entire prompt. We found that such prompts are not very common, except for summarization and other text analysis tasks, which overall constitute significantly less than 5%. Related to models used in interaction: The models used to generate the dataset are mostly small models, e.g., 13 billion parameters. It is well-known that larger models perform better, especially in conversational capabilities such as understanding people by ascribing mental states to them[16]. Furthermore, models might differ in their response styles, e.g., responding in a natural, informal manner versus a more mechanistic, formal way. This, in turn, could impact people's mental models, making the observed effect of mental model change stronger or weaker.\nRelated to interpretation of findings: Our empirical findings led us to conclude that users potentially exhibit a mental model shift. However, it should be stressed that we believe that our indicators presented can only lead to a conjecture for such a deep and profound claim. For once, determining the mental model reliably from real world conversations only might not be possible. As such multiple additional studies by different researchers in a lab environment with a similar goal setting might be needed to turn the conjecture into a verified claim."}]}