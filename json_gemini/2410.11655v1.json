{"title": "Retrieval Augmented Spelling Correction for E-Commerce Applications", "authors": ["Xuan Guo", "Rohit Patki", "Dante Everaert", "Christopher Potts"], "abstract": "The rapid introduction of new brand names into everyday language poses a unique challenge for e-commerce spelling correction services, which must distinguish genuine misspellings from novel brand names that use unconventional spelling. We seek to address this challenge via Retrieval Augmented Generation (RAG). On this approach, product names are retrieved from a catalog and incorporated into the context used by a large language model (LLM) that has been fine-tuned to do contextual spelling correction. Through quantitative evaluation and qualitative error analyses, we find improvements in spelling correction utilizing the RAG framework beyond a stand-alone LLM. We also demonstrate the value of additional finetuning of the LLM to incorporate retrieved context.", "sections": [{"title": "1 Introduction", "content": "New brand names are continuously introduced, and many of them use unconventional spelling to create specific associations while still ensuring that the brand is unique and memorable. Prominent examples include \"playgro\u201d vs. \u201cplayground\", \"biotanicals\u201d vs. \u201cbotanicals\u201d, and \u201chygeeni\u201d vs. \"hygiene\". Such cases pose a significant challenge for e-commerce spelling correction services, which are prone to over-correcting such terms, especially completely novel ones.\nIn this paper, we seek to address this challenge by leveraging Retrieval Augmented Generation (RAG). On this approach, the user's query is passed to a retrieval module that seeks to find relevant items from a product catalog. The retrieved items are then incorporated into a prompt to a large language model (LLM) that predicts a correct spelling for the user's query.\nWe report on a wide range of experiments with different retrieval models and LLMs. We find that the RAG-based approach consistently leads to large"}, {"title": "2 Related Work", "content": "Retrieval Augmentation Retrieval has proven highly effective across a wide range of knowledge intensive tasks. Early works such as DrQA (Chen et al., 2017) and Dense Passage Retrieval (DPR; Karpukhin et al. 2020) laid important groundwork by integrating retrieval with neural models to enhance question-answering accuracy and facilitate knowledge access. REALM (Guu et al., 2020) introduced unsupervised retrieval for language model pre-training, and Lewis et al. (2020) combined retrieval with generation to tackle knowledge intensive tasks. RETRO (Borgeaud et al., 2022) scaled these ideas by accessing a vast database of tokens for contextual relevance across large datasets. Khattab et al. (2021) weakly supervise the ColBERT neural retrieval model to improve performance on open-domain question answering. Overall, these approaches seem to help systems provide up-to-date knowledge and reduce hallucinations (Asai et al., 2023). Despite these advances, the specific challenges of contextual spelling correction in ecommerce settings, particularly for dynamically evolving brand names, remain underexplored.\nRetrieval Augmented Fine-Tuning Retrieval Augmented Fine-Tuning (RAFT) adapts models by fine-tuning them for specific tasks like question answering, with a strong focus on handling irrelevant documents to boost accuracy (Zhang et al., 2024). Similarly, Atlas (Izacard et al., 2023) demonstrates the effectiveness of retrieval-augmented models for few-shot learning by integrating retrieved content during both pre-training and fine-tuning phases."}, {"title": "3 Approach", "content": "We now present our approach to spelling correction, which leverages Retrieval Augmented Generation (RAG) and optionally includes a phase of fine-tuning the LLM to make better use of context."}, {"title": "3.1 Language Models", "content": "We experiment primarily with Mistral-7B (Jiang et al., 2023) (specifically, open-mistral-7b v0.1) and Claude-3-sonnet (claude-3-sonnet-20240229 v1:0). We chose Mistral-7B because it is highly effective for its size (see Section 4.3), and we chose Claude-3-sonnet as a representative of a much larger class of LLMs. We would expect to see similar results for other LLMs, even larger and more capable ones, because our central challenge is making accurate predictions about novel brand names."}, {"title": "3.2 Retrieval Models", "content": "We evaluate three main retrieval methods:\n1. BM25 (Robertson et al., 2009) is a traditional, time-tested n-gram-based retrieval model. We expect it to excel where exact matching suffices but may struggle where fuzzy or semantic matching is called for.\n2. Fuzzy BM25 combines traditional BM25 with fuzzy matching. This allows for minor spelling errors that are common in e-commerce queries. For instance, it can match \"air fryer cusinart\u201d to both \"air fryer\" and \"cuisinart\".\n3. ColBERT (Khattab and Zaharia, 2020; Santhanam et al., 2022a,b) is a neural retrieval model that represents queries and documents with token-level vectors. We expect this model to excel at retrieving semantically relevant terms.\nIn all cases, we index our product catalog. For ColBERT, this is done using a pretrained ColBERTv2 model checkpoint released by the ColBERT team.\u00b9"}, {"title": "3.3 Prompt Design", "content": "The following is the LLM prompt template that we use where the retrieved items are provided as context:\n### Instruction:\nProvide spelling correction for given query\nif necessary, referring to the provided context\nif it's relevant.\n### Context:\n{context}\n### Query:\n{input}\n### Correction:"}, {"title": "3.4 LLM Fine-Tuning", "content": "Our approach includes an optional step of fine-tuning the LLM to make more effective use of context. We consider two variants.\nFor Basic Fine-Tuning, the training dataset consists of 50K <input query, label query> pairs derived from search logs to create a training dataset. Here, label query reflects user-validated corrections. For this fine-tuning, we remove the \u201creferring to the provided context if it's relevant\u201d wording from Instruction of the prompt in Section 3.3, with the context field also removed and the desired output appended to the end followed by ### on a newline.\nFor Contextual Fine-Tuning, we begin with the same dataset of 50K pairs, but now we retrieve context for each <input query>, forming <input query, context, label query> triples. This added context, derived through the same retrieval mechanism used in RAG, helps teach the model how to make use of the context field. The prompt has the same format as the one in Section 3.3 but with the desired output appended to the end followed by ### on a newline.\nFor both variants, the fine-tuning process is simply additional language model training using strings formatted from our prompt templates. Further evaluation details, including metrics, are covered in Section 4."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Evaluation Data", "content": "Our evaluation dataset is sourced from search logs collected between 2021 and 2023. Each data point consists of an <input query, label query>\npair. The input query refers to the user's original\nsearch query, while the label query is obtained\nfrom annotators. We conducted stratified sampling\nto arrive at a 10K input query set, which was de-\nsigned to promote the diversity of the query popula-\ntion, particularly with regard to the presence of mis-\nspellings (roughly 1/4) and brand names (roughly\n1/3 cases with a brand name). To ensure label qual-"}, {"title": "4.2 Metrics", "content": "Our primary metric for evaluating spelling correction quality is the F1 score, which is the harmonic mean of precision and recall. Precision reflects the proportion of model-predicted corrections that match the gold standard annotations, while recall measures the proportion of required corrections that the model identifies correctly. We rely on exact match criteria, where two strings are considered equal after punctuation removal. All F1 scores are reported as percentages for clarity."}, {"title": "4.3 Zero-Shot LLM Performance", "content": "Table 1 reports baseline results for a range of different LLMs used without any retrieval. The prompt template used for this is the same one as in Section 3.3 without the context field and its mentions in the Instruction section. The top-performing model by a large margin is Mixtral-47B, followed by Claude-3-sonnet (\u224870B, estimated). The much smaller Mistral-7B model (Jiang et al., 2023) is reasonably competitive, though, and it represents a better balance of costs and performance, especially for high-volume services like spelling correction."}, {"title": "4.4 RAG with a Frozen LLM", "content": "Table 2 provides our primary results. We consider Mistral-7B and Clause-3-sonnet as the base LLMs. For each LLM, we evaluate our three different re-"}, {"title": "4.5 LLM Fine-Tuning", "content": "Table 4 summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section 3.4). For these experiments, we adopt Mistral-7B as our base LLM."}, {"title": "4.6 Latency Considerations", "content": "Incorporating RAG leads to a slight latency increase; however, it remains within acceptable limits for real-time applications. Using the Mistral-7B model as a baseline, retrieval from a pool of 60K candidate documents adds only 2.42% to the overall generation time, while expanding the pool to 572K documents results in a 2.79% increase. These changes are minimal, and they enable substantial gains in accuracy."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a fine-tuned Retrieval Augmented Generation (RAG) framework tailored for e-commerce spelling correction, specifically addressing the complexities posed by brand names and other non-standard lexicons. We showed that this approach is highly effective even with a frozen retriever and frozen large language model (Table 2). In addition, we showed that fine-tuning the LLM with retrieved context leads to even larger gains (Table 4), particularly for spelling corrections involving evolving brand names (Table 5). These results underscore the value of incorporating retrieval and allowing the model to dynamically adapt to context in a way that standalone LLMs or RAG with frozen components cannot achieve.\nOur qualitative analysis further revealed challenges inherent in using real-world data, such as the presence of misspellings in indexed documents, which can mislead the LLM during generation (Table 3). This highlights the importance of ensuring the quality of retrieved contexts. Practical improvements include refining the contextual data through heuristic signals, like user interactions and engagement metrics, to enhance relevance and accuracy. Another promising avenue is to diversify the styles and noise levels within the retrieved context to bolster the model's robustness."}, {"title": "6 Ethics Statements", "content": "This study uses anonymized, user-generated data to enhance the model's ability to do contextual spelling correction in e-commerce. We acknowledge that user-generated data may reflect inherent biases, such as regional or demographic linguistic preferences, which could affect spelling correction accuracy for certain user groups. We are committed to monitoring these issues and improving the fairness of the model over time, aiming to make spelling correction equitable, inclusive, and accurate. Future efforts will focus on refining our methodology to address these concerns, especially as the model encounters new data and evolves to handle a broader range of brand-specific terminology and user inputs."}]}