{"title": "Grounded Curriculum Learning", "authors": ["Linji Wang", "Zifan Xu", "Peter Stone", "Xuesu Xiao"], "abstract": "The high cost of real-world data for robotics\nReinforcement Learning (RL) leads to the wide usage of\nsimulators. Despite extensive work on building better dynamics\nmodels for simulators to match with the real world, there is\nanother, often-overlooked mismatch between simulations and\nthe real world, namely the distribution of available training\ntasks. Such a mismatch is further exacerbated by existing\ncurriculum learning techniques, which automatically vary the\nsimulation task distribution without considering its relevance\nto the real world. Considering these challenges, we posit that\ncurriculum learning for robotics RL needs to be grounded in\nreal-world task distributions. To this end, we propose Grounded\nCurriculum Learning (GCL), which aligns the simulated task\ndistribution in the curriculum with the real world, as well as\nexplicitly considers what tasks have been given to the robot\nand how the robot has performed in the past. We validate GCL\nusing the BARN dataset on complex navigation tasks, achieving\na 6.8% and 6.5% higher success rate compared to a state-\nof-the-art CL method and a curriculum designed by human\nexperts, respectively. These results show that GCL can enhance\nlearning efficiency and navigation performance by grounding\nthe simulation task distribution in the real world within an\nadaptive curriculum.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has become a powerful tool\nthat enables robots to learn complex behaviors through trial-\nand-error interactions with their environments [1]. However,\napplying RL to real-world robotic tasks presents significant\nchallenges. The trial-and-error process often requires a vast\namount of data, which is difficult and expensive to collect in\nreal-world settings [2]. As a result, simulators have become\nwidely used to generate training data in a more controlled\nand cost-effective manner.\nWhile much work has been done to build simulators with\nbetter dynamics models that more closely match the physical\nworld [3], there is another, often-overlooked mismatch be-\ntween simulations and the real world, namely the distribution\nof available training tasks. Specifically, the tasks generated in\nsimulators may differ in complexity, variability, and structure\ncompared to those that robots encounter after simulated\ntraining during deployment in real environments [4], [5]. This\nmismatch can hinder the generalization and performance of\nRL agents when transitioning from simulated environments\nto real-world tasks.\nThis problem is further exacerbated by existing Curricu-\nlum Learning (CL) techniques, which automatically vary\nthe simulation task distribution to facilitate learning [6].\nRecent approaches, such as PAIRED [7] and CLUTR [8],"}, {"title": "II. RELATED WORK", "content": "GCL addresses a fundamental challenge in robotics RL:\nthe mismatch between simulated and real-world task distri-\nbutions, which is often overlooked despite extensive work\non improving simulator dynamics. We review relevant liter-\nature in CL and Unsupervised Environment Design (UED),\nhighlighting how GCL improves upon existing approaches.\nCL in RL aims to improve learning efficiency by pro-\ngressively increasing simulated task complexity [11]. Recent\nwork has focused on automatic curriculum generation [12],\nwhere the curriculum is dynamically adapted based on the\nagent's performance in simulation. Such an adapted curricu-\nlum improves efficiency, but it doesn't have a mechanism to\nensure that the generated tasks can represent the real world,\nwhere a physical robot will eventually be deployed [3].\nUED has emerged as a promising approach for auto-\nmatically generating training tasks and adapting curricula\nin RL [13]. UED methods aim to reduce the need for\nmanually designed tasks in simulation, a key challenge in\nrobotic RL. While traditional UED methods like Domain\nRandomization [14] and minimax approaches [15], [16] have\nshown effectiveness in simulated environments, they face\nchallenges when applied to real-world robotics problems\nwhere data is scarce and expensive to obtain.\nAt the intersection of CL and UED, adaptive-teacher UED\nmethods have shown promise in improving zero-shot gener-\nalization. PAIRED [7] introduced a regret-based approach\nto UED, using an adversarial game to generate increasingly\ncomplex environments. CLUTR [8] improved upon PAIRED\nby introducing unsupervised representation learning to UED,\nreplacing the explicit task generator with a learned latent\nspace using Variational Autoencoders [17], which allowed\nfor more efficient task generation. However, CLUTR's ap-\nproach is still limited in several key respects: First, CLUTR's\nteacher agent lacks observation of the student's performance\nhistory, limiting its ability to adapt to the student's learning\nprogress. Second, because the teacher generates tasks using a\nstateless, multi-armed bandit algorithm, it lacks the capability\nto model the task space thoroughly, limiting its ability\nto generate complex tasks suitable for robotics scenarios.\nThird, CLUTR operates solely in simulation and does not\nconsider the challenges associated with sim-to-real transfer,\nparticularly the mismatch between simulated and real-world\ntask distributions.\nGCL addresses these limitations by introducing a frame-\nwork that grounds the curriculum in real-world data, ensuring\nthat the learning process is directly applicable to real-world\nenvironments. GCL improves upon existing methods in three\nkey respects: First, unlike simple RL test domains like Grid\nWorld, Car Racing, or video games [18], a key difference in\nrobotic RL is that robots need to be eventually deployed\nin the real world after training in simulation. Therefore,\nGCL grounds the simulated learning process in real-world\ndata, i.e., simulation realism; Second, GCL grounds the\ntask generation on the previous task sequences, i.e., task\nawareness, and enables the teacher agent to manipulate the"}, {"title": "III. APPROACH", "content": "We introduce GCL, a Dual-Agent framework for adaptive\ncurriculum learning in robotics with limited real-world data.\nGCL consists of two interacting processes (Fig. 2):\n\u2022\n\u2022\nA Partially Observable Markov Decision Process\n(POMDP) for the student agent ($\\mathcal{S}$) learning the task.\nA fully informed Markov Decision Process (MDP) for\nthe teacher agent ($\\mathcal{T}$) generating a curriculum of tasks.\n\\subsection{A. Dual-Agent (PO)MDP}\n1) Student Agent POMDP: The student agent op-\nerates in a POMDP defined as a tuple $M_{\\mathcal{S}}$ =\n($\\mathcal{S}_{S}$, $\\mathcal{A}_{S}$, $\\mathcal{O}_{S}$, $\\mathcal{T}_{S}$, $\\Omega_{S}$, $\\mathcal{R}_{S}$, $\\gamma$), where:\n\u2022\n$\\mathcal{S}_{S}$ is the state space of the robotic task,\n\u2022\n$\\mathcal{A}_{S}$ is the robot action space,\n\u2022\n$\\mathcal{O}_{S}$ is the robot observation space,\n\u2022\n$\\mathcal{T}_{S}$: $\\mathcal{S}_{S}$ \u00d7 $\\mathcal{A}_{S}$ \u2192 $\\mathcal{S}_{S}$ is the POMDP transition function,\n\u2022\n$\\Omega_{S}$ : $\\mathcal{S}_{S}$ \u00d7 $\\mathcal{A}_{S}$ \u2192 $\\mathcal{O}_{S}$ is the robot observation function,\n\u2022\n$\\mathcal{R}_{S}$: $\\mathcal{S}_{S}$ \u00d7 $\\mathcal{A}_{S}$ \u00d7 $\\mathcal{S}_{S}$ \u2192 $\\mathbb{R}$ is the robot reward function\nbased on task execution performance, and\n\u2022\n$\\gamma$ \u2208 [0, 1] is the student POMDP's discount factor.\nThe Student agent's goal is to learn a policy $\\pi_{\\mathcal{S}}$ : $\\mathcal{O}$ \u2192 A that\nmaximizes the expected cumulative reward in the partially\nobservable task environment generated by the teacher.\n2) Teacher Agent MDP: In contrast to the student's\nPOMDP, the teacher agent operates in an MDP defined as\n$M_{\\mathcal{T}}$ = ($\\mathcal{S}_{T}$, $\\mathcal{A}_{T}$,$\\mathcal{T}_{T}$,$\\mathcal{R}_{T}$,$\\gamma_{T}$), where:\n\u2022\n$\\mathcal{S}_{T}$ is the teacher state space, consisting of the compre-\nhensive history of tasks and student performances,\n\u2022\n$\\mathcal{A}_{T}$ is the teacher action space representing all possible\ntasks that can be assigned to the student,\n\u2022\n$\\mathcal{T}_{T}$: $\\mathcal{S}_{T}$ \u00d7 $\\mathcal{A}_{T}$ \u2192 $\\mathcal{S}_{T}$ is the MDP transition function,\n\u2022\n$\\mathcal{R}_{T}$: $\\mathcal{S}_{T}\\mathcal{A}_{T}$ \u00d7 $\\mathcal{S}_{T}$ \u2192 $\\mathbb{R}$ is the teacher reward function\nbased on student performance, and\n\u2022\n$\\gamma_{\\mathcal{T}}$ \u2208 [0, 1] is the discount factor for the teacher's MDP.\n$s_{t} \\in \\mathcal{S}_{T}$ at time t is defined as $s_{t} = \\{(a_{i},r_{i})\\}_{i=0}^{t-1}$, where\n$a_{i} \\in \\mathcal{A}_{T}$ is the i th task assigned by the teacher and $r_{i}$ is\nthe student's performance (reward) for task i."}, {"title": "B. Grounded Curriculum Learning (GCL)", "content": "GCL employs a hierarchical structure where a fully-\ninformed teacher agent guides the learning process of a\nstudent agent, resembling a classroom setting (Fig. 2 top\nleft). In this metaphorical classroom, the teacher (Fig. 2\nright) oversees the learning environment (tasks in the cur-\nriculum) and monitors the performance of the student and\nan antagonist agent (Fig. 2 bottom left). This design reflects\nreal-life educational scenarios, where teachers have com-\nprehensive knowledge of both the curriculum and student\nprogress. Leveraging this informed perspective within the\nclassroom and dual-agent framework, GCL comprises five\nmain components that work together to create an effective\nand adaptive curriculum:\n1) Task Representation via Latent Generative Model:\nWe employ a Variational Autoencoder (VAE) to learn a\ncompact latent space $\\mathcal{Z}$ of robotic tasks, trained on a\nlimited set of real-world tasks $\\mathcal{T}_{real}$ and therefore grounded\nin the real world. The VAE consists of an encoder and\na decoder, which learn to compress and reconstruct task\nenvironments efficiently. This model enables the teacher to\ngenerate diverse, realistic tasks by sampling from the learned\nlatent space $\\mathcal{Z}$, bridging the gap between limited real-world\ndata and the need for varied training scenarios. By learning\na continuous task representation, the VAE allows smooth\ninterpolation between known tasks and the generation of\nnovel, yet realistic, ones for the student agent to learn from.\n2) Student and Antagonist Agents: The student agent\nlearns to perform a task using a reinforcement learning algo-\nrithm (e.g., PPO [19]) in the partially observable environment\ngenerated by the teacher. Its objective is to maximize the\nexpected cumulative reward:\n$\\mathcal{J}_{S}(\\pi_{\\theta_{S}}) = \\mathbb{E}_{\\tau_{\\theta_{S}}} \\left[ \\sum_{t=0}^{T} \\gamma^{t} r_{t} \\right]$\nwhere $\\tau_{s}$ is a trajectory sampled from the student policy $\\pi_{\\theta_{S}}$,\nparameterized by $\\theta_{S}$. To guide curriculum generation and\nevaluate the student's progress, we introduce an antagonist\nagent, following the flexible regret setting from PAIRED [7].\nThe antagonist is trained with the same observability and\nhyperparameters as the student, sharing the same objective\nfunction (Eqn. (1)), with $\\mathcal{J}_{A}$ and $\\pi_{\\theta_{A}}$ as the antagonist's\nobjective and policy respectively.\n3) Teacher Agent: The teacher agent enables GCL's two\ngrounding aspects: student performance and task awareness.\nTo ground in student performance, the teacher maintains a\ncomprehensive tracking of the student's performance across\ndiverse tasks through the student's historical performance,\ni.e., $\\{(a_{i},r_{i})\\}_{i=0}^{t-1}$, as part of the teacher state $s_{t}$. By incorporat-\ning performance history into its state, the teacher can adapt\nthe curriculum based on the student's learning progress.\nTo ground with task awareness, the teacher maintains a\ndeep understanding of the task space, ensuring continual\nrelevance to past and real-world tasks. This awareness is\nfacilitated by both the historical tasks, i.e., $\\{(a_{i})\\}_{i=0}^{t-1}$, in the\nteacher state $s_{t}$, as well as the latent space $\\mathcal{Z}$ learned through"}, {"title": "IV. EXPERIMENTS", "content": "We evaluated GCL on The Benchmark for Autonomous\nRobot Navigation (BARN) Challenge [9], [10], [21]\u2013[24], a\nstandardized testbed for SOTA navigation systems designed\nto push the boundaries of performance in challenging and\nhighly constrained environments. The objective is to navigate\na robot from a predefined start to a goal location as quickly\nas possible without collisions. Focusing on real-world au-\ntonomous navigation, BARN features an environment gener-\nator capable of producing a wide range of navigation tasks,\nfrom easy open spaces to difficult highly constrained ones.\n\\subsection{A. Experimental Setup}\nWe implement The BARN Challenge in NVIDIA's Isaac-\nGym simulator [25], utilizing 128 parallel environments. This\nsetup significantly accelerates training and allows efficient\nexploration of the task space for curriculum learning. Con-\nsidering the lack of large-scale real-world scenarios, in our\nexperiments, the BARN environments from the generator\nserve as a surrogate for the real world, where robots will be\neventually deployed after training (in contrast to simulated\nenvironments created by the teacher agent during training).\n1) Student Agent POMDP: $M_{\\mathcal{S}}$ =\n($\\mathcal{S}_{S}$, $\\mathcal{A}_{S}$, $\\mathcal{O}_{S}$,$\\mathcal{T}_{S}$, $\\Omega_{S}$, $\\mathcal{R}_{S}$, $\\gamma_{S}$) is implemented\nas a\nnavigation task in our experiments. $\\mathcal{S}_{S}$ includes the robot's\nposition and orientation; $\\mathcal{A}_{S}$ comprises continuous linear\nand angular velocities; $\\mathcal{O}_{S}$ includes 270\u00b0 field-of-view,\n720-dimensional LiDAR scans and the relative goal\norientation; and $\\mathcal{R}_{S}$ encourages progress towards the goal\nwhile penalizing collisions and excessive time.\n2) Teacher Agent MDP: In $M_{\\mathcal{T}}$ =\n($\\mathcal{S}_{T}$, $\\mathcal{A}_{T}$,$\\mathcal{T}_{T}$,$\\mathcal{R}_{T}$,$\\gamma_{T}$), $\\mathcal{S}_{T}$ consists of the history of tasks\nand student performances, i.e., $s_{t} = \\{(a_{i},r_{i})\\}_{i=0}^{t-1} \\in \\mathcal{S}_{T}$.\nFor simplicity, we set i = t\u2212 1 in our experiments and leave\nthe study on the effect of history length as future work; $\\mathcal{A}_{T}$\nis the latent space of task representation; and $\\mathcal{R}_{T}$ is based\non the regret between the student and the antagonist."}, {"title": "D. Ablation Studies", "content": "We conduct comprehensive ablation studies to analyze the\ncontribution of each key component in GCL:\n\u2022\nGCL: The complete GCL framework.\n\u2022\nGCL w/o real: GCL without real-world data\ngrounding. We remove the real-world task selection,\nusing only tasks generated by the teacher agent:\n$a_{t} \\sim \\pi_{\\theta_{\\mathcal{T}}}(s_{t})$.\n\u2022\nGCL w/o task: GCL without task awareness ground-\ning. We replace the latent task representation $a_{t}$ with\na random vector $f_{i}$ in the teacher's state: $s_{t}$ =\n$\\{(f_{i},r_{i})\\}_{i=0}^{t-1}$, where $f_{i} \\sim N(0,I)$, a zero-mean,\nidentity-standard-deviation normal distribution.\n\u2022\nGCL w/o performance: GCL without performance\nhistory grounding. We replace the student's reward\n$r_{s}$ with a random scalar $\\eta_{i}$ in the teacher's state:\n$s_{t} = \\{(a_{i},\\eta_{i})\\}_{i=0}^{t-1}$, where $\\eta_{i}$ ~ $\\mathcal{U}(0,1)$, a uniform\ndistribution between 0 and 1.\nTable IV presents the results of our ablation studies.\nThe performance gains of the full GCL over its variants\nemphasize the critical importance of each component in the\nframework. Grounding in real-world data is the most crucial\nelement, followed by performance history and task aware-\nness. These components work synergistically to enhance\nGCL's effectiveness, leveraging the advantages of curriculum\nlearning while maintaining a strong connection to real-\nworld scenarios. These results highlight the importance of\na holistic approach that combines real-world relevance with\nadaptive learning strategies based on task and performance\nunderstanding, leading to superior performance in complex\nnavigation tasks."}, {"title": "E. Curriculum Progression", "content": "We present and discuss the curriculum progression enabled\nby different methods to investigate GCL's capability to\nautonomously create appropriate tasks to facilitate learning."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "This paper introduces Grounded Curriculum Learning\n(GCL), a novel framework that enhances real-world rein-\nforcement learning in robotics. GCL improves curriculum\nlearning by aligning the simulated task distribution with real-\nworld tasks while considering both task sequences and the\nrobot's past performance. Our experiments on the BARN\nnavigation dataset demonstrate GCL's effectiveness, achiev-\ning a 6.8% and 6.5% higher success rate compared to SOTA\nmethods and a manually designed curriculum. GCL's balance\nbetween structured and flexible learning ensures that the\nlearned skills are both efficiently acquired and applicable\nto real-world scenarios. Our ablation studies demonstrate\nthat each component of GCL is important: the curriculum\n(grounded in task awareness and performance history) guides\nefficient learning, while real-world task grounding is crucial\nfor maintaining relevance to the target domain.\nAn interesting direction for future work is to extend\nGCL to a broader range of robotic tasks beyond naviga-\ntion [27], such as manipulation [28], [29] and multi-agent\nsystems [30]\u2013[33]. Another promising avenue is investigating\nmethods for more effective latent space manipulation by the\nteacher agent, potentially leading to improved task generation\nand curriculum design. Additionally, exploring GCL's poten-\ntial in transfer and lifelong learning scenarios [34] with a pre-\ntrained teacher agent can enable robots to adapt more quickly\nto new tasks or environments. Lastly, instead of end-to-\nend learning [35], [36], more structured learning approaches\nfor robotics, such as learning planner parameters [37]\u2013\n[42], cost functions [43], kinodynamic models [44]\u2013[48],\ntrajectory generation [49], [50], and local planners [51]-\n[54] may be able to further improve GCL's efficiency and\ngeneralizability."}]}