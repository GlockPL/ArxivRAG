{"title": "A Survey on Self-play Methods in Reinforcement Learning", "authors": ["Ruize Zhang", "Zelai Xu", "Chengdong Ma", "Chao Yu", "Wei-Wei Tu", "Shiyu Huang", "Deheng Ye", "Wenbo Ding", "Yaodong Yang", "Yu Wang"], "abstract": "Self-play, characterized by agents' interactions with copies or past versions of itself, has recently gained prominence in reinforcement learning. This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) represents a significant paradigm [1] within machine learning, concerned with the optimization of decision-making processes through interaction with an environment. It's fundamentally modeled using a Markov decision process (MDP), a mathematical framework that describes an environment in terms of states, actions, transitions, and rewards. Within an MDP, agents operate by observing states, executing actions according to defined policies, receiving subsequent rewards, and transitioning to subsequent states. The primary goal of RL algorithms is to derive the optimal policy that yields the maximum expected accumulated reward over time. Deep RL extends traditional RL by employing deep neural networks as function approximators [2]. This fusion of deep learning with RL has been instrumental in handling high-dimensional state spaces, contributing to breakthroughs in various complex tasks.\nMoreover, the transition from single-agent to multi-agent reinforcement learning (MARL) introduces complex dynamics [3]\u2013[5]. In MARL, the interdependence of agents' actions introduces significant challenges, as the environment appears non-stationary to each agent. The main issues in MARL are coordination, communication, and equilibrium selection, particularly in competitive scenarios. These challenges often lead to difficulties in achieving convergence, maintaining stability, and efficiently exploring the solution space.\nWith the help of game theory, a mathematical framework that models the interactions between multiple decision-makers, self-play emerges as an elegant solution to some inherent challenges in MARL. By addressing issues such as non-stationarity and coordination, self-play offers an approach where an agent interacts with copies or past versions of itself [6], [7]. This method promises a more stable and manageable learning process. The capabilities of self-play extend to a wide range of scenarios, including its high-profile applications in Go [8]\u2013[11], chess [10], [11], poker [12], [13], and video games [14], [15]. In these scenarios, it has developed strategies that surpass human expertise. Although the application of self-play is extensive and promising, it is accompanied by limitations, such as the potential convergence to suboptimal strategies and significant computational requirements [8], [10].\nAlthough some research takes a broad perspective through empirical game-theoretic analysis (EGTA) [16], it is important to note that there are relatively few comprehensive surveys focusing exclusively on self-play. Among these, some studies address the theoretical safety of self-play [17], while others develop an algorithmic framework for self-play that unfortunately does not accommodate the Policy-Space Response Oracle (PSRO) series of algorithms [18]. Furthermore, another study concentrates exclusively on PSRO [19]. Although these varied studies are valuable, they do not offer a perspective that fully captures the breadth and depth of self-play. Therefore, this survey aims to bridge this gap.\nThe survey is organized as follows. Sec. II introduces the background of self-play, including the RL framework and basic game theory concepts. Sec. III proposes a unified framework and then categorizes existing self-play algorithms into four categories based on this framework, clarifying the self-play landscape. In Sec. IV, a comprehensive analysis is performed to illustrate how self-play is applied in various scenarios. Sec. V describes open problems in self-play and explores future research directions. Finally, Sec. VI concludes the survey on self-play."}, {"title": "II. PRELIMINARIES", "content": "In this section, we first introduce the framework of RL. Next, we present the basic game theory concepts and the typical evaluation metrics used in self-play."}, {"title": "A. RL Framework", "content": "In MDPs, an agent interacts with the environment by taking actions, which leads to different states with associated rewards. The Markovian assumption postulates that the evolution of the system is fully characterized by its current state, obviating the need to account for historical states. MDPs can be extended to multi-agent settings, known as Markov games [20], also known as stochastic games [21]. We consider the most general form: partially observable Markov games (POMGs), which refers to a scenario wherein multiple agents are involved, and each agent lacks access to the complete state of the environment. Instead, they obtain individual observations related to the environment. A POMG G can be defined by G = (N, S, A, O, P, R, \u03b3, \u03c1). \u039d = {1,\u2026,n} denotes n agents. S is the state space. A = \u03a0\u00b2=1 Ai is the product of the action space of each agent. Similarly, O = \u03a0\u00b2\u00b2=1 O\u2081 is the product of the observation space of each agent. P : S \u00d7 A \u00d7 S \u2192 [0, 1] denotes the transition probability from one state to another given the actions of each agent. R = {R1,\u2026,Rn}, where R\u2081 : S \u00d7 A\u00a1 \u2192 R denotes the reward function of agent i. \u03b3\u2208 [0,1] is the discount factor. p : S \u2192 [0,1] describes initial state distribution. Note that if it is a cooperative MARL problem, agents can share the same reward function [5], [22]\u2013[24]. Especially when n = 1,0\u2081 = S, the environment setting returns to the simple MDP.\nn\nIn the RL context, agents interact with the environment based on the subsequent protocol: At each discrete time step t, every agent i receives an observation Oi,t from the environment and selects an action based on a stochastic policy \u03c0\u03bf; : \u039f\u00a1 \u00d7 A\u00a1 \u2192 [0, 1], where \u03b8\u2081 is the parameters. After receiving the joint actions at = (a1,t,\u2026\u2026, an,t), the environment undergoes a transition from the current state st to a subsequent state st+1 according to the transition function P and sends a reward ri,t+1 to every agent i. The ultimate goal of agent i is to maximize the expected discount accumulated rewards: \u0395\u03c0\u03bf\u03c2 [\u03a3\u03c4=oYtri,t]."}, {"title": "B. Game Theory Concepts", "content": "1) (Im)Perfect Information and (In)Complete Information:\nIn a game characterized by perfect information, only one player moves at a time. Each player has a comprehensive understanding of the current game state, the full history of moves that have been made, and all potential future developments. If these conditions are not met, the game is considered to have imperfect information [25], [26]. In a game of incomplete information, there exists at least one player who is unaware of the payoff of another player; otherwise, it is a game of complete information [27].\nFor instance, Go is a game of both perfect and complete information. Players have full awareness of the entire game structure, including all possible moves, and they can see every move made by their opponent as they take turns to act (perfect information). Furthermore, if the outcomes are considered binary, such as win or loss, the payoff for the players is known to both sides (complete information).\n2) Normal-Form and Extensive-Form: The normal-form and extensive-form are two different ways of representing games in game theory. If a game G is represented in the normal-form, it can be expressed by G = (N, \u03a0, u). N = {1,2,\u2026,n} denotes the players. I = I\u2081 \u00d7\uff65\uff65\uff65 \u00d7 In is pure strategy space of all players. A vector \u03c0 = (\u03c01,\u2026, \u03c0\u03b7) \u2208 \u03a0 is called a strategy profile. A pure strategy defines a specific and deterministic action for a player in a game, while a mixed strategy designates a probability distribution over the set of pure strategies, allowing randomized actions. A mixed strategy for the player i is a probability distribution \u03c3\u03b5 \u2208 \u0394(\u03a0\u2081), where A is a probability simplex. u = (u1,\u2026, Un), where U\u017c: \u041f\u2192 R, is a utility function that assigns a real-valued payoff to each player i. If \u2200n \u2208 \u03a0, \u03a3\u2081 ui(\u03c0) = 0, the game is a zero-sum game, otherwise it is a general-sum game. If II\u2081 = = In and the payoffs are invariant under any permutation of the players' strategies, the game is a symmetric game. If finite players (especially two players) are involved and each player has a finite set of strategies, a game in the normal-form can be directly depicted in a matrix.\nSpecifically, in two-player zero-sum symmetric normal-form games, the pure strategy space for both player 1 and player 2 is identical, denoted by II, such that II = \u041f\u2081 = I2. As the utility function 4\u2081(\u03c0\u03af, \u03c0j) = \u2212U2(\u03c0\u03af, \u03c0j), for simplicity, we can use only one utility function u such that \u2200ni, \u03c0j \u2208 II, if \u03c0i beats \u03c0;, then u(\u03c0\u03af, \u03c0j) = \u2212u(\u03c0;, \u03c0\u2081) > 0. The evaluation matrix captures the outcomes of the game by detailing the results of different strategies when they are played against each other: \u0391\u03c0 = {u(\u03c0\u03af, \u03c0j) : \u03c0\u03af, \u03c0j \u2208 \u03a0 \u00d7 \u03a0}.\nIf a game is represented in the extensive-form, it is expressed sequentially, illustrating the sequence of moves, choices made by the players, and the information available to each player during decision-making. Typically, a game in the extensive-form is represented by a game tree. This tree demonstrates the sequential and potentially conditional nature of decisions. Moreover, if player i has perfect recall, it means that player i remembers which action they have taken in the past. A game G represented in the extensive-form can be expressed by G = (N\u222a {c}, H, Z, P, T, A, u). N = {1,2,\u2026,n} denotes a set of players. c is chance and can be regarded as a special agent. H represents a set of possible histories and Z C H is a set of terminal histories. Order of moves is represented by a function P(h) \u2208 N\u222a{c} to indicate which player is to move, where h\u2208 H. I denotes information set partitions and I denotes the information set partitions for player i. This implies that in an imperfect information game, if player i reaches a history h\u2208 Ii, where I \u2208 Z\u00bf is a specific information set, player i cannot distinguish which particular history h \u2208 I\u2081 it is encountering. Action space is represented by A(h) for a non-terminal history h\u2208 H. For all non-terminal histories h within an information set Ii, the available actions are the same; otherwise, they are distinguishable. Therefore we use A(Ii) to represent the available actions for the information set Ii. Utility functions"}, {"title": "III. ALGORITHMS", "content": "Based on existing self-play work [18], [42]\u2013[44], we propose a self-play framework (Algo. 1) that boasts enhanced expressivity and superior generalization capabilities. The framework is adept at handling multi-homogeneous-player general-sum games. It's important to note that although homogeneous players represent a specific subset of heterogeneous players, the latter can be reformulated as the former by expanding the dimensions of the input vector, which essentially entails embedding agent identity information. Moreover, as players are homogeneous, we assume each player shares the same policy population.\nTo clarify our framework, we describe it in a more easily understandable way. All players share a common policy population with a fixed maximum size. In each iteration, a new policy is initialized for training, and opponent policies are sampled from the existing policy population. During this iteration, the opponent policies typically remain fixed, while only the policy being trained is updated. After the training process, the new policy replaces one of the policies in the policy population. An evaluation metric is then used to assess the performance of the updated policy population. Based on this performance, the strategy for sampling opponents is adjusted for the next iteration. This process is repeated. For a more detailed and precise description, please refer to Sec. III-A.\nIn addition, we divide self-play algorithms into four primary groups: traditional self-play algorithms (Sec. III-B), the PSRO series (Sec. III-C), the ongoing-training-based series (Sec. III-D), and the regret-minimization-based series (Sec. III-E). We analyze how these four categories align with our framework in their respective sections and introduce representative algorithms in each category. Moreover, in Sec. III-F, we discuss the differences among these four categories and their connections to our proposed framework. We also explain why our framework is more general than existing frameworks. Furthermore, we summarize the major elements of key algorithms within our framework for each category in Table II."}, {"title": "A. Framework Definition", "content": "In Algo. 1, we define a unified framework of self-play based on [18], [42]\u2013[44]. First, the input of the framework is defined as follows:\n\u2022 I: Each policy \u03c0\u2081 in the policy population II is conditioned on a policy condition function h(i), which is determined by specific algorithms. We will further"}, {"title": "Algorithm 1 the Framework of Self-play", "content": "Require: \u03a0 := {\u03c0\u03b9(\u00b7|h(i))}1 \u25b7 Policy population.\nRequire: \u03a3:= {0}~1\u2208RN\u00d7C1\u25b7 Interaction matrix.\nRequire: F: RN\u00d7C2 \u2192 RN\u00d7C1\u25b7 MSS.\n1: for e\u2208 [[E]] do\n2: for \u03c3\u03b5 \u0395 \u03a3 do\n3: Initialize \u03c0\u03b7\n4: \u3160 \u2190 ORACLE(\u03c0, \u03c3\u03af, \u03a0) \u25b7 Compute the oracle.\n5: P\u2190 EVAL(II) \u25b7 Get policies' performance.\n6: \u03a3\u2190 F(P) \u25b7 Update the interaction matrix.\n7: end for\n8: end for\n9: return \u03a0, \u03a3"}, {"title": "\u2022", "content": "introduce this policy condition function in the following sections. For simplicity, we denote each policy \u03c0i (\u00b7|h(i)) by \u03c0. It's important to note that i refers to the ith policy in the policy population, not to a player. Additionally, the hyper-parameter N denotes the policy population size of II, not the number of game players. Furthermore, II can be initialized in two different ways. First, I can be considered as initialized with N placeholder policies, meaning that there is no actual initialization of the policies in practice. Instead, each policy will be actually initialized in the training iterations (Line 3 in Algo. 1). We refer to this approach as the placeholder initialization. Second, II can be initialized with N real policies, which may include random initialization or pretrained models. We refer to this approach as the actual initialization. A policy w is considered an ineffective policy if it serves as a placeholder policy; otherwise, it is deemed an effective policy. Additionally, Table II provides a clearer illustration of the initialization of II and the expression of h(i) across different categories of the main self-play algorithms.\n\u03a3:= {0}~1\u2208RN\u00d7C1: The interaction matrix of the policy population. \u03c3\u03b5 \u2208 RC1 represents the opponent sampling strategy of policy i. Namely, \u03c3\u03b5 illustrates how to sample opponent(s)' policies against policy i. For instance, let \u03c3\u03b5 represent the probability of policies for each opponent. In this context, C\u2081 = Nn-1, where n denotes the number of players. Alternatively, \u03c3\u03b5 can also be considered as sampling parameters within a sampling network. Especially, in a two-player game, if C\u2081 = N and oij represents the probability that policy i is optimized against policy j, \u2211 can be depicted in directed interaction graphs (Fig. 3). It's important to note that, unlike the original PSRO framework [42], \u03c3\u03b5 here is not the meta-strategy of policy i. Instead, in the two-player setting, if \u03c3\u03b5 in our framework is the opponent's meta-strategy against policy i, our framework can reduce to the original PSRO framework.\nF: RN\u00d7C2 \u2192 RN\u00d7C1: A meta-strategy solver (MSS) F takes the performance matrix P := {pi}1 \u2208"}, {"title": "\u2022", "content": "Next, the core process within the framework is as follows:\nFor epoch e \u2208 [[E]] (Line 1 in Algo. 1): E denotes the total number of epochs for the entire policy population. For example, if the algorithm only introduces new policies into the population without updating existing ones, then E = 1. This implies that only the ineffective policy is likely to be chosen for training in each iteration and turned into an effective one, while effective policies remain unchanged. Conversely, if effective policies are updated multiple times throughout the algorithm, then E > 1. Indeed, E accurately rechoflect the number of updates performed. Additionally, Table II summarizes the values of E for different categories of self-play algorithms.\nInitialize (Line 3 in Algo. 1): The initialization of \u03c0 can vary depending on the algorithm being used. For instance, it may be initialized randomly, by leveraging pre-trained models [8], [15] or through a recently updated policy. We provide detailed descriptions of the initialization process for each algorithm series. Also, these are summarized in Table II.\n\u2022 ORACLE(\u03c0\u03af, \u03c3\u03af, \u03a0) (Line 4 in Algo. 1): The ORACLE is an abstract computational entity that returns a new policy adhering to specific criteria. Here, we divide ORACLE into three types. (1) One type is the BR oracle, which"}, {"title": "Algorithm 2 Compute the Oracle in RL", "content": "Require: \u03c0 \u25b7 Policy i is being trained.\nRequire: \u03c3\u03b5 \u25b7 Opponent sampling strategy of policy i.\nRequire: \u041f \u25b7 Policy population.\n1: while \u03c0\u03b7 is not valid do\n2: for trajectory \u03c4\u2208 [[Tmax]] do\n3: sample \u03c0opp ~ \u03a1(\u03c3\u03b5) \u25b7 Policies of opponents.\n4: \u03c0 = (\u03c0\u03b7, \u03c0\u03cc\u03c1\u03c1)\n5: $0,00 ~ p \u25b7 Initial state.\n6: for t \u2208 [[tmax]] do\n7: at ~ \u03c0(Ot)\n8: St+1, Ot+1 ~ P(St, at)\n9: rt \u2190 R(St, at)\n10: end for\n11: \u2190 update(\u03c0) \u25b7 Using RL algorithms\n12: end for\n13: end while\n14: return \u03c0"}, {"title": "Algorithm 3 Compute the Oracle in Evolution Theory", "content": "Require: \u03c0\u03b7\u25b7 Policy i is being trained.\nRequire: \u03c3\u03b5 \u25b7 Opponent sampling strategy of policy i.\nRequire: \u041f \u25b7 Policy population.\n1: sample opp ~ \u03a1(\u03c3\u03b5) \u25b7 Policies of opponents.\n2: Treg = (\u03c0\u03b7, \u03c0\u03bf\u03c1p) \u25b7 Regularization policies.\n3: Transformed the reward according to reg.\n4: \u03c0 \u2190 Use replicator dynamics to play the reward-\ntransformed game until convergence.\n5: return \u03c0"}, {"title": "Algorithm 4 Compute the Oracle in Regret Matching", "content": "Require: \u03c0 \u25b7 Policy i is being trained.\nRequire: \u03c3\u03b5 \u25b7 Opponent sampling strategy of policy i.\nRequire: I \u25b7 Policy population.\n1: for each player j do\n2: sample \u03c0opp ~ \u03a1(\u03c3\u03b5) \u25b7 Policies of opponents.\n3: \u03c0 = (\u03c0(j), \u03c0opp(-j))\n4: Use regret matching to play the game and obtain new\nregret minimization information added to h(i).\n5: end for\n6: return \u03c0"}, {"title": "B. Traditional Self-play Algorithms", "content": "Traditional self-play algorithms involve agents improving their strategies by repeatedly playing against themselves, allowing them to explore various strategies and enhance their decision-making abilities without external input. These algorithms can start with agents training against their most recent version, helping to identify and exploit weaknesses. Additionally, other approaches involve training against a set of strategies from different iterations, enabling agents to develop robust and adaptive strategies. In this section, we will explain how traditional self-play algorithms fit into our framework and introduce representative traditional self-play methods, ranging from simpler forms to more complex ones.\n1) Integration into Our Framework: We can incorporate the traditional self-play algorithms into our proposed framework (Algo. 1), using the following settings. First, the policy"}, {"title": "Algorithm 5 the PFSP Meta-strategy Solver", "content": "1: function F(P)\n2: f(Pi,j)\n3: Append zeros to i+1 until its length is N.\n4: return \u03a3"}, {"title": "F(P)ij\n1, if j \u2264 i", "content": "1, if j \u2264 i"}, {"title": "C. PSRO Series of Algorithms", "content": "Similar to traditional self-play algorithms, the PSRO series of algorithms starts with a single policy and gradually expands the policy space by incorporating new oracles. These oracles are policies that approximate optimal responses to the current meta-strategies of other agents. Additionally, PSRO employs EGTA to update meta-strategy distributions, thereby incorporating a level of exploration in policy selection to mitigate the risk of overfitting.\n1) Integration into Our Framework: The PSRO series of algorithms can also be integrated into our proposed framework (Algo. 1). First, similar to traditional self-play algorithms, we also utilize placeholder initialization to initialize II. Second, we also set E = 1 and N can be considered as the upper limit for the policy population size in the original PSRO algorithms. Third, in the context of the PSRO series of algorithms, the strategy of our player \u03c0 can also be initialized in a general manner. Fourth, we simply set h(i) = \u00d8 since the PSRO series of algorithms do not use any conditioning function for their policies. Fifth, it's crucial to highlight that our framework diverges from the traditional PSRO model [42] in how \u03c3\u03af is defined. In contrast to being the meta-strategy for policy, in our framework, oi is the opponent sampling strategy. It means that \u03c3\u03b5 here represents the opponent's meta-strategy against policy i for the PSRO series of algorithms. Sixth, compared with traditional self-play methods, the MSSes of the PSRO series are often more complex. For example, some MSSes incorporate concepts from different types of game equilibria [45]\u2013[47].\nFor simplicity, we also follow Assumption 1. Similar to traditional self-play algorithms, we can derive the Corollary 2 using a similar proof as Corollary 1."}, {"title": "Algorithm 6 the NE-based Meta-strategy Solver", "content": "1: function F(P)\n2: \u03c3\u03af+1 \u2190 SOLVE-NASH(P1:1,1:\u2170) \u25b7 Opponent's NE\nmeta-strategy.\nAppend zeros to oi+1 until its length is N.\n4: return \u03a3"}, {"title": "D. Ongoing-training-based Series of Algorithms", "content": "In the PSRO series of algorithms, two key challenges arise. First, when operating with a limited budget, it is often necessary to truncate the ABR operators during each iteration. This can introduce sub-optimally trained responses into the population. Second, the redundant process of relearning basic skills in every iteration is not only inefficient, but it also becomes untenable when confronted with increasingly formidable opponents [43]. To address these challenges, the ongoing-training-based series of algorithms promote the ongoing training of all policies repeatedly. Namely, all effective policies are likely to be selected for training.\n1) Integration into Our Framework: We can incorporate these ongoing-training-based series of algorithms into our proposed framework (Algo. 1) using the following settings: First, we use actual initialization to initialize II because in the ongoing-training-based series, all policies in the policy population are trained together, rather than the policy population growing with each iteration. Second, we set E = Emax > 1, which represents the maximum number of epochs to optimize each policy within the policy population. In other words, each unique policy undergoes iterative training for a total of E\u0442\u0430\u0445 times. Third, since each policy undergoes training for Emax times, we utilize \u03c0\u03af(\u00b7|h(i)) to initialize . This means that policy updates are self-referential.\nFor the sake of simplicity, we also adopt Assumption 1. Different from Collory 1 and Collory 2, due to the continuous training process of all policies, we derive Collory 3."}, {"title": "E. Regret-minimization-based Series of Algorithms", "content": "Another line of self-play algorithms is based on regret minimization. The key distinction between regret-minimization-based algorithms and the other categories is that they prioritize accumulated payoff over time, rather than focusing solely on a single episode. This approach leads to more aggressive and adaptable strategies, which are essential to avoid being exploited by opponents as time progresses. Additionally, these algorithms require players to deduce and adapt to opponents' strategies over several rounds. This scenario is commonly observed in repeated games rather than stage games. For example, in games like Texas Hold'em or Werewolf, players must use deception, concealment, and bluffing to aim for overall victories rather than just winning a single game. It's important to note that while traditional regret-minimization-based self-play doesn't typically use RL, many subsequent research efforts have combined regret minimization with RL to achieve strong performance. In this section, we will also thoroughly discuss traditional regret-minimization-based methods to provide a foundation for understanding how integrating regret minimization with RL can lead to enhanced performance.\n1) Integration into Our Framework: We can also integrate the regret-minimization-based series of algorithms into our proposed framework (Algo. 1) with the following settings: First, similar to traditional self-play algorithms and the PSRO series, we use placeholder initialization to initialize the policy population II. Second, we set E = 1, and N is regarded as the maximum iteration to optimize the policy. Third, we initialize using \u03c0i\u22121(\u00b7|h(i \u2013 1)) to utilize the most"}, {"title": "IV. EMPIRICAL ANALYSIS", "content": "In this section, we introduce iconic applications of self-play by categorizing the scenarios into three distinct groups: board games, which typically involve perfect information; card games and Mahjong, which usually involve imperfect information; and video games, which feature real-time actions rather than turn-based play. We then illustrate how self-play is applied in each of these complex scenarios and provide a comparative analysis of these applications in Table III."}, {"title": "A. Board Games", "content": "The landscape of board games, the majority of which are perfect information games, was previously revolutionized by the introduction of two key techniques: position evaluation and Monte Carlo tree search (MCTS) [94], [95]. These methodologies, with minor modifications, demonstrated superhuman effectiveness in solving board games such as chess [96], checkers [97], othello [98], backgammon [99], and Scrabble [100]. In contrast, the application of these techniques to the game of Go with an estimated 2.1 \u00d7 10170 legal board configurations, only enabled performance at the amateur level [101]\u2013[105]. In light of this, our discussion will specifically focus on the game of Go to illustrate the application of self-play. In addition to Go, we will broaden our exploration to include Stratego, a board game characterized by imperfect information, contrasting the majority of board games that are based on perfect information.\n1) Go: The paradigm of Go is revolutionized with the launch of DeepMind's AlphaGo series [8]\u2013[11], which leveraged the power of self-play to significantly elevate performance, setting a new benchmark in the field of Go.\nIn AlphaGo [8], the training regime can be split into three stages. In the first stage, supervised learning with expert data trains a fast policy network p\u03c0(as) for rollouts in MCTS expansion and a precise policy network po(as). The second stage employs RL to refine the policy network pp(als) based on po(als) and subsequently trains a value network ve(s). Self-play is instrumental here. More specifically, pp(as) is refined by competing against a randomly chosen historical version pp-(als), akin to the MSS shown in Equ. (34). Afterwards, ve(s) is trained using the samples from the games in which the trained policy network pp(a|s) competes against itself. In the final stage, MCTS integrates the policy and value networks to inform action selection.\nUnlike AlphaGo, AlphaGo Zero [9] does not require any expert data except game rules and instead learns through self-play. It utilizes only one network fe(s) to concurrently predict the action probabilities p = Pr(as) and the state value v. Each move is generated by MCTS with guidance from fe(s) to aid MCTS expansion, as opposed to the rollouts used in AlphaGo. Self-play is employed to generate data and refine fo(s) with the current best policy competing against itself, a process analogous to the MSS referenced in Equ. (37). Furthermore, for a new policy to be incorporated into the policy pool, it must surpass a 55 percent win rate against its predecessor, aligning with the stipulations set in Algo. 2 at Line 1.\nAlphaZero [10] extends AlphaGo Zero to include games beyond Go, such as Chess and Shogi, with some modifications. Notably, a draw is introduced as an additional expected outcome, and data augmentation is omitted due to the asymmetry of Chess and Shogi. Concerning the self-play procedure, the only difference between AlphaZero and AlphaGo Zero is that AlphaZero utilizes the newly updated network without the validation process present in AlphaGo Zero.\nBuilding upon AlphaZero, MuZero [11] takes the concept of learning from scratch to the next level, even operating without predefined game rules. MuZero incorporates ideas from model-based RL to model the dynamics of games. More concretely, in addition to the prediction network f (similar to the networks in AlphaGo Zero and AlphaZero), MuZero introduces a dynamics network g to model the MDP and a representation network h to map observations to hidden states. These three networks are trained jointly. Similarly to AlphaGo Zero and AlphaZero, MuZero employs MCTS guided by the three aforementioned networks to make decisions. The self-play process in MuZero operates similarly to that in AlphaZero. In practice, in addition to excelling in board games like Go, MuZero also achieves state-of-the-art performance in Atari games. Furthermore, several studies have extended the capabilities of MuZero. For example, Stochastic MuZero [106] learns a stochastic model instead of a deterministic model to enhance its performance in more complex scenarios. Sampled MuZero [107] makes learning feasible in games with intricate action spaces.\n2) Stratego: Unlike most board games which are perfect information games, Stratego, a two-player imperfect information board game, distinguishes itself by incorporating elements of memory, deduction, and bluffing. This complexity is further amplified by the game's long episode length and a large number of potential game states, estimated 10535 [108]. The game is divided into two phases: the deployment phase, where players secretly arrange their units, setting the stage for strategic depth, and the game-play phase, where the objective"}, {"title": "B. Card Games and Mahjong", "content": "1) Texas Hold'em: Texas Hold'em, a popular poker game with 2-10 players, is known for its strategic depth and bluffing elements. In its two-player variant, it's referred to as heads-up Texas Hold'em. The game becomes more complex with additional players. The gameplay begins with each player receiving two private cards (hole cards), followed by a round of betting. Subsequently, three community cards (the flop) are revealed, leading to another betting round. This is followed by the dealing of a fourth (the turn) and a fifth community card (the river), each accompanied by further betting rounds. The objective is to construct the best five-card poker hand from any combination of hole cards and community cards. The betting proceeds until a showdown occurs, where the remaining players disclose their cards. The individual with"}, {"title": "C. Video Games", "content": "In contrast to traditional board games and card games, video games often feature real-time actions, long time horizons and a higher level of complexity due to the broader range of possible actions and observations. Here, we illustrate some representative video games to demonstrate how self-play has advanced AI within these games.\n1) StarCraft II: StarCraft is a real-time strategy (RTS) game developed and published by Blizzard Entertainment. It showcases three distinct species: the Terrans, Zerg, and Protoss, each with unique units and strategic options that enhance the gameplay experience. Renowned for its balanced gameplay, strategic depth, and challenging multiplayer features, the game tasks players with gathering resources, constructing bases, and building armies. Victory requires meticulous planning and tactical execution, with defeat occurring when a player loses all their buildings.\nAlphaStar [15], a significant advancement in AI, dominates the 1v1 mode competitions in StarCraft II and has defeated professional players. Its framework bears similarities to that of AlphaGo [8], utilizing supervised learning initially to train the policy with data from human experts. Subsequently, it uses end-to-end RL and a hierarchical self-play method to further train the networks. More specifically, it divides all the agents into three types: main agents, league exploiters, and main exploiters. Additionally, it maintains a policy pool of past players that records all these types of agents. Main agents engage in both FSP and PFSP, competing against main agents themselves and other agents in the policy pool. They are periodically added to the pool and never reset. League exploiters use PFSP to play against all policy pool agents, added to the pool if they show a high win rate and potentially reset to expose global blind spots. Main exploiters only compete with main agents to improve their robustness and are added to the pool after achieving a high win rate or certain training steps, and are reset upon each addition. Most importantly, among those three agent types, the main agent is the core agent and embodies the final AlphaStar strategy. However, the training computation for AlphaStar is extensive. Further studies [131]\u2013[133] have enhanced the league self-play training procedure by introducing innovative techniques to reduce computations.\n2) MOBA Games: Multiplayer Online Battle Arena games (MOBA) are a popular genre of video games that blend RTS with role-playing elements. In typical MOBA games, two teams of players control their own unique characters, known as heroes and compete to destroy the opposing team's main structure, often called the base. Each hero has distinct abilities and plays a specific role within the team, such as Warrior, Tank or Support. Managing multiple lanes and battling under the fog of war, which obscures parts of the map are key aspects of gameplay. Popular examples of MOBA games include League of Legends, Dota 2 and Honor of Kings. These games are known for their complex strategic depth, decision-making under conditions of imperfect information and emphasis on teamwork and player skill.\nOpenAI Five [14] defeated the world champion team in a simplified version of Dota 2 that featured a limited pool of heroes and certain banned items. It employs distributed RL using PPO [122] along with GAE [123] to scale up training. Each player on a team shares the same policy network and receives"}, {"title": "V. OPEN PROBLEMS AND FUTURE WORK", "content": "Self-play approaches have demonstrated superior performance due to their unique iterative learning processes and ability to adapt to complex environments. However, there are still several areas that require further research and development."}, {"title": "A. Theoretical Foundation", "content": "Although NE has been"}]}