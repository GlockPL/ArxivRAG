{"title": "A Survey on Self-play Methods in Reinforcement Learning", "authors": ["Ruize Zhang", "Zelai Xu", "Chengdong Ma", "Chao Yu", "Wei-Wei Tu", "Shiyu Huang", "Deheng Ye", "Wenbo Ding", "Yaodong Yang", "Yu Wang"], "abstract": "Self-play, characterized by agents' interactions with copies or past versions of itself, has recently gained prominence in reinforcement learning. This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) represents a significant paradigm [1] within machine learning, concerned with the optimization of decision-making processes through interaction with an environment. It's fundamentally modeled using a Markov decision process (MDP), a mathematical framework that describes an environment in terms of states, actions, transitions, and rewards. Within an MDP, agents operate by observing states, executing actions according to defined policies, receiving subsequent rewards, and transitioning to subsequent states. The primary goal of RL algorithms is to derive the optimal policy that yields the maximum expected accumulated reward over time. Deep RL extends traditional RL by employing deep neural networks as function approximators [2]. This fusion of deep learning with RL has been instrumental in handling high-dimensional state spaces, contributing to breakthroughs in various complex tasks.\nMoreover, the transition from single-agent to multi-agent reinforcement learning (MARL) introduces complex dynamics [3]\u2013[5]. In MARL, the interdependence of agents' actions introduces significant challenges, as the environment appears non-stationary to each agent. The main issues in MARL are coordination, communication, and equilibrium selection, particularly in competitive scenarios. These challenges often lead to difficulties in achieving convergence, maintaining stability, and efficiently exploring the solution space.\nWith the help of game theory, a mathematical framework that models the interactions between multiple decision-makers, self-play emerges as an elegant solution to some inherent challenges in MARL. By addressing issues such as non-stationarity and coordination, self-play offers an approach where an agent interacts with copies or past versions of itself [6], [7]. This method promises a more stable and manageable learning process. The capabilities of self-play extend to a wide range of scenarios, including its high-profile applications in Go [8]\u2013[11], chess [10], [11], poker [12], [13], and video games [14], [15]. In these scenarios, it has developed strategies that surpass human expertise. Although the application of self-play is extensive and promising, it is accompanied by limitations, such as the potential convergence to suboptimal strategies and significant computational requirements [8], [10].\nAlthough some research takes a broad perspective through empirical game-theoretic analysis (EGTA) [16], it is important to note that there are relatively few comprehensive surveys focusing exclusively on self-play. Among these, some studies address the theoretical safety of self-play [17], while others develop an algorithmic framework for self-play that unfortunately does not accommodate the Policy-Space Response Oracle (PSRO) series of algorithms [18]. Furthermore, another study concentrates exclusively on PSRO [19]. Although these varied studies are valuable, they do not offer a perspective that fully captures the breadth and depth of self-play. Therefore, this survey aims to bridge this gap.\nThe survey is organized as follows. Sec. II introduces the background of self-play, including the RL framework and basic game theory concepts. Sec. III proposes a unified framework and then categorizes existing self-play algorithms into four categories based on this framework, clarifying the self-play landscape. In Sec. IV, a comprehensive analysis is performed to illustrate how self-play is applied in various scenarios. Sec. V describes open problems in self-play and explores future research directions. Finally, Sec. VI concludes the survey on self-play."}, {"title": "II. PRELIMINARIES", "content": "In this section, we first introduce the framework of RL. Next, we present the basic game theory concepts and the typical evaluation metrics used in self-play.\nA. RL Framework\nIn MDPs, an agent interacts with the environment by taking actions, which leads to different states with associated rewards. The Markovian assumption postulates that the evolution of the system is fully characterized by its current state, obviating the need to account for historical states. MDPs can be extended to multi-agent settings, known as Markov games [20], also known as stochastic games [21]. We consider the most general form: partially observable Markov games (POMGs), which refers to a scenario wherein multiple agents are involved, and each agent lacks access to the complete state of the environment. Instead, they obtain individual observations related to the environment. A POMG G can be defined by G = (N, S, A, O, P, R, \\gamma, \\rho). N = {1,\u2026,n} denotes n agents. S is the state space. A = \\Pi_{i=1}^{n} A_i is the product of the action space of each agent. Similarly, O = \\Pi_{i=1}^{n} O_i is the product of the observation space of each agent. P : S \u00d7 A \u00d7 S \u2192 [0, 1] denotes the transition probability from one state to another given the actions of each agent. R = {R_1,\u2026,R_n}, where R_i : S \u00d7 A_i \u2192 \\R denotes the reward function of agent i. \\gamma\u2208 [0,1] is the discount factor. \\rho : S \u2192 [0,1] describes initial state distribution. Note that if it is a cooperative MARL problem, agents can share the same reward function [5], [22]\u2013[24]. Especially when n = 1, O_i = S, the environment setting returns to the simple MDP.\nIn the RL context, agents interact with the environment based on the subsequent protocol: At each discrete time step t, every agent i receives an observation O_{i,t} from the environment and selects an action based on a stochastic policy \\pi_{\\theta_i} : O_i \u00d7 A_i \u2192 [0, 1], where \\theta_i is the parameters. After receiving the joint actions a_t = (a_{1,t},\u2026\u2026, a_{n,t}), the environment undergoes a transition from the current state s_t to a subsequent state s_{t+1} according to the transition function P and sends a reward r_{i,t+1} to every agent i. The ultimate goal of agent i is to maximize the expected discount accumulated rewards: E_{\\pi_{\\theta_i}}[\\sum_{t=0}^{\\infty} \\gamma^t r_{i,t}].\nB. Game Theory Concepts\n1) (Im)Perfect Information and (In)Complete Information: In a game characterized by perfect information, only one player moves at a time. Each player has a comprehensive understanding of the current game state, the full history of moves that have been made, and all potential future developments. If these conditions are not met, the game is considered to have imperfect information [25], [26]. In a game of incomplete information, there exists at least one player who is unaware of the payoff of another player; otherwise, it is a game of complete information [27].\nFor instance, Go is a game of both perfect and complete information. Players have full awareness of the entire game structure, including all possible moves, and they can see every move made by their opponent as they take turns to act (perfect information). Furthermore, if the outcomes are considered binary, such as win or loss, the payoff for the players is known to both sides (complete information).\n2) Normal-Form and Extensive-Form: The normal-form and extensive-form are two different ways of representing games in game theory. If a game G is represented in the normal-form, it can be expressed by G = (N, \\Pi, u). N = {1,2,\u2026,n} denotes the players. \\Pi = \\Pi_1 \u00d7 \u22ef \u00d7 \\Pi_n is pure strategy space of all players. A vector \\pi = (\\pi_1,\u2026, \\pi_n) \u2208 \\Pi is called a strategy profile. A pure strategy defines a specific and deterministic action for a player in a game, while a mixed strategy designates a probability distribution over the set of pure strategies, allowing randomized actions. A mixed strategy for the player i is a probability distribution \\sigma_i \u2208 \\Delta(\\Pi_i), where \\Delta is a probability simplex. u = (u_1,\u2026, u_n), where u_i: \\Pi\u2192 \\R, is a utility function that assigns a real-valued payoff to each player i. If \u2200\\pi \u2208 \\Pi, \\sum_i u_i(\\pi) = 0, the game is a zero-sum game, otherwise it is a general-sum game. If \\Pi_1 =\u2026= \\Pi_n and the payoffs are invariant under any permutation of the players' strategies, the game is a symmetric game. If finite players (especially two players) are involved and each player has a finite set of strategies, a game in the normal-form can be directly depicted in a matrix.\nSpecifically, in two-player zero-sum symmetric normal-form games, the pure strategy space for both player 1 and player 2 is identical, denoted by \\Pi, such that \\Pi = \\Pi_1 = \\Pi_2. As the utility function u_1(\\pi_i, \\pi_j) = \u2212u_2(\\pi_i, \\pi_j), for simplicity, we can use only one utility function u such that \u2200\\pi_i, \\pi_j \u2208 \\Pi, if \\pi_i beats \\pi_j, then u(\\pi_i, \\pi_j) = \u2212u(\\pi_j, \\pi_i) > 0. The evaluation matrix captures the outcomes of the game by detailing the results of different strategies when they are played against each other: A_\\pi = {u(\\pi_i, \\pi_j): \\pi_i, \\pi_j \u2208 \\Pi \u00d7 \\Pi}.\nIf a game is represented in the extensive-form, it is expressed sequentially, illustrating the sequence of moves, choices made by the players, and the information available to each player during decision-making. Typically, a game in the extensive-form is represented by a game tree. This tree demonstrates the sequential and potentially conditional nature of decisions. Moreover, if player i has perfect recall, it means that player i remembers which action they have taken in the past. A game G represented in the extensive-form can be expressed by G = (N\u222a {c}, H, Z, P, T, A, u). N = {1,2,\u2026,n} denotes a set of players. c is chance and can be regarded as a special agent. H represents a set of possible histories and Z \u2282 H is a set of terminal histories. Order of moves is represented by a function P(h) \u2208 N\u222a{c} to indicate which player is to move, where h \u2208 H. I denotes information set partitions and I_i denotes the information set partitions for player i. This implies that in an imperfect information game, if player i reaches a history h \u2208 I_i, where I_i \u2208 Z_i is a specific information set, player i cannot distinguish which particular history h \u2208 I_i it is encountering. Action space is represented by A(h) for a non-terminal history h \u2208 H. For all non-terminal histories h within an information set I_i, the available actions are the same; otherwise, they are distinguishable. Therefore we use A(I_i) to represent the available actions for the information set I_i. Utility functions"}, {"title": "3) Transitive Game and Non-transitive Game", "content": "is denoted by u = (u_1,\u2026\u2026, u_n), where u_i: Z \u2192 \\R. Together, these components define the structure and dynamics of an extensive-form game. Moreover, A strategy profile in an extensive-form game can be expressed by \\pi = (\\pi_1,\u2026\u2026, \\pi_n), where \\pi_i maps each I_i \u2208 Z_i to a probability distribution over A(I_i). A subgame of an extensive-form game is a portion of the game that starts from a single initial node, includes all successors of any node within the subgame, and contains all nodes in the same information set as any node in the subgame.\nThe Prisoner's Dilemma serves as a classic example to illustrate various concepts in game theory. In a modified version of the dilemma, the outcomes are as follows:\n\u2022 If one player confesses (C) and the other lies (L), the confessor will serve 1 year in jail, while the liar will serve 8 years.\n\u2022 If both players choose to confess, they will each serve 7 years behind bars.\n\u2022 If both players choose to lie, they will each serve only 2 years behind bars.\nThe classic scenario is known as the simultaneous Prisoner's Dilemma, where two players must decide simultaneously whether to confess or lie, without knowledge of the other's choice. Games played in this manner are referred to as static games. The normal-form representation is particularly suitable for static games, as it captures the simultaneous nature of decision-making (as depicted in Fig. 1a). Another variant is the sequential Prisoner's Dilemma, where the second player makes their decision with knowledge of the first player's action. Games of this nature are called dynamic games. The extensive-form representation is well-suited for dynamic games, as it can clearly illustrate the sequence of moves and the information available to each player at each decision point (as shown in Fig. 1d).\nFurthermore, a normal-form representation can be transformed into an extensive-form representation. For instance, the transformation from Fig. 1a to Fig. 1b involves representing the simultaneous decisions in a tree structure. In this extensive representation, the dotted lines indicate that certain statuses belong to the same information set. Conversely, an extensive-form representation can also be transformed into a normal-form representation. For example, the transformation from Fig. 1d to Fig. 1c involves condensing the sequential decisions into a matrix format. In this case, the pure strategy space for player 2 is 2 \u00d7 2 = 4, reflecting the need for player 2 to have a strategy for each of player 1's possible actions (confess or lie). For instance, the notation for one of player 2's pure strategies C \u2192 C, L \u2192 C in Fig. 1c indicates that player 2 chooses to confess when he knows player 1 has chosen to confess and player 2 also chooses to confess when he knows player 1 has chosen to lie. Other symbols in Fig. 1c follow the same logic. Compared to normal-form games, extensive-form games introduce sequential decision-making, adding complexity to the game structure. Extensive-form games also have a close relationship with MGs. In MGs with simultaneous moves, agents' actions are unknown to each other, creating various histories that are condensed into a single information set. The game's utility is the sum of rewards discounted over time [28]. Beyond normal-form and extensive-form games, as well as"}, {"content": "3) Transitive Game and Non-transitive Game: For the sake of simplicity, we restrict our focus to two-player zero-sum symmetric games. In a transitive game, the strategies or outcomes adhere to a transitive relationship. Formally, \u2200\\pi_i, \\pi_j, \\pi_\\kappa \u2208 \\Pi, if u(\\pi_i, \\pi_j) > 0 and u(\\pi_j, \\pi_\\kappa) > 0, then it must follow that u(\\pi_i, \\pi_\\kappa) > 0. This transitive property simplifies the strategic landscape, allowing for an ordinal ranking of strategies. Conversely, in a non-transitive game, \u2203 \\pi_i, \\pi_j, \\pi_\\kappa \u2208 \\Pi such that u(\\pi_i, \\pi_j) > 0 and u(\\pi_j, \\pi_\\kappa) > 0, but u(\\pi_i, \\pi_\\kappa) \u2264 0. This introduces a cyclic relationship among strategies, thereby complicating the game. The complexity often results in a mixed-strategy equilibrium, where players randomize their choices among multiple strategies to maximize their expected payoff. A quintessential example of a non-transitive game is Rock-Paper-Scissors, in which no single strategy uniformly dominates all others. In real-world settings, games exhibit complexities that extend beyond theoretical models. [29] argues that real-world games have two salient features: first, practice usually leads to performance improvements; and second, there are a plethora of qualitatively distinct strategies, each with unique advantages and disadvantages. In such games, the strategies form a geometric topology resembling a spinning top, where the vertical axis represents the performance of the strategy, and the radial axis represents the length of the longest cycle.\n4) Stage Game and Repeated Game: A stage game (or one-shot game) is a game that is played only once, namely a one-shot interaction between players. A famous example of"}, {"title": "4) Stage Game and Repeated Game", "content": "4) Stage Game and Repeated Game: A stage game (or one-shot game) is a game that is played only once, namely a one-shot interaction between players. A famous example of a stage game is the Prisoner's Dilemma. A repeated game is derived from a stage game that is played multiple times. Formally, a repeated game based on a stage game G is defined by playing G for T periods, where T can be finite or infinite. The strategies in a repeated game are history-contingent, meaning that they can depend on the entire sequence of past plays. It's important to note that a stage game or a repeated game can be either represented in the normal-form or extensive-form.\n5) Nash Equilibrium: For simplicity, \\pi_i denotes the strategy of player i, and \\pi_{-i} denotes the strategies of all players other than player i. Given \\pi_{-i}, player i's best response (BR) is the strategy that maximizes player i's payoff:\n$$BR_i(\\pi_{-i}) = \\arg \\max_{\\pi_i} u_i (\\pi_i, \\pi_{-i}).$$\nA strategy \\pi is an \\epsilon-BR to strategies \\pi_{-i} if:\n$$u_i(\\pi, \\pi_{-i}) \\geq u_i(BR_i(\\pi_{-i}), \\pi_{-i}) - \\epsilon,$$\nwhere \\epsilon is a pre-specified threshold.\nA strategy profile (\\pi_1^*, \\pi_2^*, ..., \\pi_n^*) is a Nash equilibrium (NE) if, for every player i:\n$$u_i(\\pi_i^*, \\pi_{-i}^*) \\geq u_i(\\pi_i, \\pi_{-i}^*), \\forall \\pi_i,$$\nmeaning that no player can benefit by changing their strategy unilaterally, given the strategies of all other players. In other words, an NE is a situation where the strategy chosen by each player is a BR to the strategies chosen by all other players. A strategy profile (\\pi_1^*, \\pi_2^*, ..., \\pi_n^*) is an \\epsilon-NE if, for every player i:\n$$u_i(\\pi_i^*, \\pi_{-i}^*) \\geq u_i(\\pi_i, \\pi_{-i}^*) - \\epsilon, \\forall \\pi_i,$$\nmeaning that no player can increase their payoff by more than \\epsilon by unilaterally changing their strategy.\nHowever, computing NE is generally intractable in complex games, leading some researchers to utilize a-Rank [30] and Correlated Equilibrium (CE) [31] as alternatives. Additionally, some studies resort to Replicator Dynamics [32] as a method to analyze and understand the evolution of strategies within these games.\n6) Team Games: The framework of a two-player zero-sum game can be naturally extended to encompass team-based zero-sum games. Von Stengel and Koller analyzed zero-sum normal-form games involving a single team competing against an adversary [33]. In this type of team game, consider a team denoted by T = {1,2,\u2026\u2026, n \u2212 1}. The player n is the adversary (D). In this kind of zero-sum normal-form team games, for any player i,j \u2208 T, the utility functions satisfy u_i(\\pi) = u_j(\\pi) = u_T(\\pi) and u_D(\\pi) = \u2212(n \u2212 1)u_T(\\pi). A zero-sum single-team single-adversary normal-form game can also be extended to the domain of extensive games [34]. For any player i,j\u2208 T and all terminal nodes z \u2208 Z, the utility functions satisfy u_i(z) = u_j(z) = u_T(z) and u_D(z) = \u2212(n \u2212 1)u_T(z). Let I_T denote the information set defined as \\cup_{i \u2208 T} I_i, and let A_T represent the set of actions accessible in the information sets within I_T.\nIn scenarios where teammates are unable to coordinate their strategies, the team-maxmin equilibrium (TME) emerges as the most suitable solution concept [33]. We denote the collection of sequences of player i by Q_i, representing the sequence-form actions undertaken by player i. The sequence-form strategy is encapsulated by a function p_i: Q_i \u2192 \\R, which maps each sequence q \u2208 Q_i to its associated probability of execution. Formally, the TME is articulated as:\n$$\\arg \\max_{p_1, \\dots, p_{n-1}} \\min_{p_n} \\sum_{i=1}^{n} u_i \\prod p_i.$$\nSimilar to the arguments in normal-form games [33], it can also be inferred that in extensive-form games, a TME exists uniquely, barring any degeneracy and this TME aligns with the team's utility maximization of the NE [34].\nC. Evaluation Metrics in Self-play\nIn this section, we introduce various self-play evaluation metrics, including NASHCONV (Section II-C1), Elo (Section II-C2), Glicko (Section II-C3), WHR (Section II-C4), and TrueSkill (Section II-C5). While NASHCONV measures the distance from Nash equilibrium, the other four metrics evaluate relative skill levels and are compared in Table I. It's important to note that although numerous other evaluation metrics exist, the metrics highlighted here are among the most widely used in the field."}, {"title": "4) Stage Game and Repeated Game", "content": "1) NASHCONV: Nash convergence (NASHCONV) serves as a metric to measure the deviation of a particular strategy from an NE. A lower NASHCONV value suggests that the strategy is closer to an NE, implying that no player would benefit from unilateral deviation from the strategy. Formally, it is defined as:\n$$NASHCONV(\\pi) = \\sum_{\\pi_i \\in \\Pi_i} \\max u_i(\\pi_i, \\pi_{-i}) \u2212 u_i (\\pi),$$\nwhere \\pi denotes the combined strategy profile of all participating agents. In particular, in the context of two players, this deviation is commonly referred to as exploitability.\n2) Elo: The Elo system [35] operates under the assumption that the performance of each player in each game is a normally distributed random variable, with the mean being the player's current rating. In a match between player A and player B, R_A and R_B are the current ratings of player A and player B. The probability density functions for the performance of player A and player B are given by:\n$$f_A(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(x-R_A)^2}{2\\sigma^2}},$$\n$$f_B(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(x-R_B)^2}{2\\sigma^2}},$$\nE_A and E_B denote the expected score (or the probability of winning) for player A and player B:\n$$E_A = \\int_{-\\infty}^{\\infty} \\frac{1}{2} + \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{R_A-R_B}{\\sigma}} x^2 dx,$$\n$$E_B = \\int_{-\\infty}^{\\infty} \\frac{1}{2} + \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{R_B-R_A}{\\sigma}} x^2 dx.$$\nFor convenience, the logistic function is used to approximate the probability (using the logistic curve with base 10 and choosing 400 to scale and normalize the difference in ratings):\n$$E_A = \\frac{1}{1 + 10^{(R_B-R_A)/400}},$$\n$$E_B = \\frac{1}{1 + 10^{(R_A-R_B)/400}}.$$\nNote that E_A + E_B = 1. S_A is the actual outcome of the match for player A (1 for a win, 0.5 for a draw, 0 for a loss). S_B = 1-S_A. After a match, the ratings are updated based on the difference between the actual outcome and the expected outcome. The adjustment is given by:\n$$\\Delta R_A = K(S_A \u2013 E_A),$$\n$$\\Delta R_B = K(S_B \u2013 E_B).$$\nK is a scaling factor, often determined by the specific domain of the application, and controls the maximum possible rating change for a single match. If two players have nearly identical ratings, the expected outcome will be close to 0.5. A win for either player will result in a moderate increase in their rating. In contrast, if there is a significant rating difference, the expected outcome will be heavily skewed. If the higher-rated player wins, their rating will increase by a value much smaller than K, reflecting the anticipated nature of their victory. However, if the lower-rated player secures an unexpected win, their rating will surge by a value approaching K, signifying the upset. The updated ratings become:\n$$R'_A = R_A + \\Delta R_A,$$\n$$R'_B = R_B + \\Delta R_B.$$\nHowever, there are challenges and limitations in Elo. First, the Elo system assumes that all matches are equally important. This might not be the case in all domains. Second, K is often kept constant. However, a dynamic K factor might be more appropriate, especially for players new to the rating pool or in scenarios where the importance of the match varies. Third, the standard Elo system does not incorporate a decay mechanism to account for the potential degradation or improvement of skills during periods of inactivity. Fourth, the basic Elo system is designed for one-on-one competitions. Adapting it to team or multi-player scenarios, such as team sports or online multiplayer games, can be challenging. Lastly, an important limitation of the Elo rating system is its unsuitability for games that exhibit high levels of non-transitivity [36].\n3) Glicko: The Glicko system refines the Elo system by introducing a measure of uncertainty or reliability in a player's rating, termed rating deviation [37]. The primary motivation is to account for the variability in a player's performance and the potential changes in skill over time. The Glicko-2 system,\nan extension of the original Glicko system, further refines these concepts and introduces the rating volatility \u03c3, indicating the degree of expected fluctuation in the player's rating [38].\nIn the Glicko-2 system, r denotes the current rating of the player. RD is the player's rating deviation, and \u03c3 is the player's volatility. Convert the rating and rating deviation to the Glicko-2 scale:\n$$\\mu = \\frac{r - 1500}{173.7178},$$\n$$\\varphi = \\frac{RD}{173.7178}$$\nThen, calculate v representing the estimated variance of the player's rating based on game outcomes, E(\\mu, \\mu'_j, \\varphi_j) representing the probability of a player with rating \\mu defeating an opponent player j and \\Delta representing the estimated improvement based only on game outcomes s_j:\n$$v = \\sum_{j=1}^m g(\\varphi_j)^2 {1 \u2013 E(\\mu, \\mu_j\u2019, \\varphi_j)},$$\n$$E(\\mu, \\mu_j, \\varphi_j) = \\frac{1}{1 + exp(\u2212g(\\varphi_j)(\\mu \u2013 \\mu_j))},$$\n$$\\Delta = v \\sum_{j=1}^m g(\\varphi_j){s_j \u2013 E(\\mu, \\mu_j, \\varphi_j)},$$\nwhere:\n$$g(\\varphi) = \\frac{1}{\\sqrt{1 + 3\\varphi^2/\\pi^2}}.$$\nThe update for \u03c3 is more involved and requires an iterative procedure to solve for its new value \u03c3'. Then, calculate new \\mu' and \\varphi':\n$$\\mu' = \\mu + \\varphi^2 \\sum_{j=1}^m g(\\varphi_j){s_j \u2013 E(\\mu, \\mu_j, \\varphi_j)},$$\n$$\\varphi' = \\frac{1}{\\sqrt{\\frac{1}{\\varphi^2} + v}},$$\nwhere:\n$$\\varphi^* = \\varphi^2 + \\sigma'^2.$$\nAfter calculations, the rating and rating deviation are converted back from the Glicko-2 scale:\n$$r' = 173.7178\\mu' + 1500,$$\n$$RD' = 173.7178\\varphi'.$$\n4) WHR: The Whole-History Rating (WHR) system [39] is a Bayesian rating system designed to estimate players' skills from their entire game history. It is particularly adopted in handling the temporal dynamics of player skills. R_i(t) denotes Elo rating of the player i at time t. Similar to Equ. (11) and Equ. (12), E_A(t) and E_B(t) denote the expected score (or the probability of winning) for player A and player B at time t:\n$$E_A(t) = \\frac{1}{1+10^{(R_B(t)-R_A(t))/400}},$$\n$$E_B(t) = \\frac{1}{1+10^{(R_A(t)-R_B(t))/400}}.$$\nMoreover, WHR assumes that ratings of each player i is a Wiener process:\nr_i(t_2) - r_i(t_1) \\sim N(0, |t_2 \u2013 t_1|w^2),\nwhere w is a parameter representing the variability of ratings in time. Thus, if r' = R_A(t_1),r'' = R_A(t_2):\np(r''|r') = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(r'-r)^2}{2\\sigma^2}},\nwhere \u03c3 = w\\sqrt{|t_2-t_1|}. In addition, a Wiener process is memoryless (or Markovian).\nR(t) denotes player A's ratings, G denotes the observation of game results. Thanks to the Bayes formula, the WHR system is to solve:\n$$\\arg \\max_{R} p(R|G) = \\arg \\max_{R} \\frac{p(G|R)p(R)}{p(G)}.$$\np(G|R) can be derived from the product of Equ. (28) and p(R) can be derived from the product of Equ. (31). Utilizing the iterative process of Newton's method, we can ascertain the solution to the given problem.\n5) TrueSkill: TrueSkill [40] is based on a probabilistic graphical model that employs Bayesian inference and adapts to multiple players in multiple teams. TrueSkill 2 [41] extends the original TrueSkill model with several enhancements by taking into account the experience of a player, the affiliation with a team, and some game-specific factors such as the number of kills. For simplicity, TrueSkill is introduced using the following scenario: team 1 has player 1, while team 2 has player 2 and player 3. Ultimately, team 1 defeated team 2. This is a simpler case than the one presented in the original paper [40]. It can be described in an undirected probabilistic graphic model (Fig. 2). The skill of each player, denoted by s_i, is represented by a Gaussian distribution N(s_i; \\mu_i, var_i). The performance of each player, denoted by p_i, is also represented by a Gaussian distribution N(p_i; s_i, \\beta^2). The performance of each team is denoted by t_i. The difference in team performance is denoted by d. If d > \\epsilon, where \\epsilon is a small positive threshold, then team 1 beats team 2. The TrueSkill algorithm's updating mechanism within the context of undirected probabilistic graphs uses the sum-product message-passing algorithm. This ensures that the skill estimates are refined iteratively, leading to accurate and reliable ratings for each player."}, {"title": "III. ALGORITHMS", "content": "Based on existing self-play work [18", "42": [44], "groups": "traditional self-play algorithms (Sec. III-B)", "18": [42], "44": ".", "follows": "n\u2022 I: Each policy \\pi_i in the policy population \\Pi is conditioned on a policy condition function h(i), which is determined by specific algorithms. We will further introduce this policy condition function in the following sections. For simplicity, we denote each policy \\pi_i (\u00b7|h(i)) by \\pi. It's important to note that i refers to the ith policy in the policy population, not to a player. Additionally, the hyper-parameter N denotes the policy population size of \\Pi, not the number of game players"}]}