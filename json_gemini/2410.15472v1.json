{"title": "An Integrated Deep Learning Model to Analyze CT Scans for Minimally Invasive Accurate Classification of T1a Small Renal Masses", "authors": ["FNU NEHA", "Arvind K. Bansal"], "abstract": "Convolution and attention-based deep learning techniques are emerging as promising approaches for automated medical image classification and semantic segmentation for minimally invasive diagnosis of malignant masses in vital organs. However, current deep learning based automated classification techniques are suitable only for detecting renal masses greater than 4.0 cm in diameter, which have a higher probability of malignancy increasing the morbidity, costly disease-management, and the probability of early mortality. T1a Small renal masses (SRM) are less than 4.0 cm in diameter and have a much smaller probability of being malignant and are corrected with partial nephrectomy or ablation. Detection of SRMS poses significant challenges due to low contrast with the surrounding and irregular shapes. As the imaging techniques improve, automated medical image-analysis techniques need further improvement for diagnosing SRMs. In this study, we present an integrated deep learning model and an algorithm for diagnosing T1a SRMs. The model integrates convolution blocks with residual links for capturing local features without information loss, cross-channel attention to focus on important features, principal component analysis to prune low variance features for reduced error and improved computational efficiency, and a transformer-encoder comprising self-attention to maintain dependencies and global context across image-patches. We trained and evaluated our model on the KiTS19 and KiTS21 challenge datasets for three ranges {1.2 \u2013 2.0, 2.0 3.0, 3.0 4.0}. The model achieved an accuracy of 98.1%, 98.5%, and 97.3% for the three ranges, respectively. The accuracy of classification results significantly outperforms other approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Kidneys are a pair of bean-shaped vital organs, each located in the abdomen on either side of the spine, responsible for filtering waste from blood, maintaining pH balance, regulating electrolytes, maintaining blood pressure, and stimulating red blood-cell production by secreting a hormone [1, 2].\nRenal cancer cancer pertaining to kidney, is one of the major causes of kidney malfunction leading to Chronic Kidney Disease (CKD), morbidity, and eventual early mortality [3]. Among the cancer of vital organs, it has the top sixteen mortality rate with 175,000 deaths annually worldwide [2, 4, 5, 6].\nRadiographic images, such as ultrasound (US), magnetic resonance imaging (MRI), or computed tomography (CT) (both unenhanced and enhanced), are noninvasive imaging techniques to noninvasively detect renal cancer [7]. Contrast-enhanced ultrasound images (CEUS) are not reliable for detecting renal masses due to similar echo-patterns from cancerous tumor cells (RM) [8]. Contrast enhanced CT (CECT) scans are preferred for diagnosing RM due to their effectiveness and lower cost compared to MRI [8, 9].\nMost RMs are identified incidentally or at an advanced stage (greater than 4.0 cm) due to the limitations in the lack of human ability to identify small RMS (SRMs) in radiological images and the lack of the visible or pathological symptoms at early stages [10-14]. By that time, the RM has a higher probability of becoming malignant, causing kidney malfunction, morbidity, and early mortality [10-12]. It has been estimated that the increase in the size of tumor by 1 cm increases the probability of malignancy by 16% [10]. Additionally, there is 3% - 5% error (approximately 40 million radiology images every year) by radiologists in diagnosing the radiology images due to fatigue, cognitive problems, and perceptual variations [15]. This error is more for SRMs due to smaller size, low contrast, and irregular shape.\nImaging techniques are continuously improving [9]. As the imaging techniques and the resulting image-resolutions improve, incidental detection of SRM is increasing [11, 12]. Hence, there is a need to improve the automated medical image analysis for automated detection of T1a SRMs, which have low probability of being malignant [11, 12]. Such SRMs can be cured easily at an early stage by partial nephrectomy or ablation without many side-effects and without an increased probability of cancer recurrence [11, 12]. Improved diagnosis would improve patients' prognosis, reduce the cost-burden associated with long-term disease management, and alleviate the impact of workforce shortages in kidney care, especially in developing countries [4, 5].\nThree classes of related minimally invasive approaches are being explored to detect solid RMs: (1) Radiomics features analysis [16-18]; (2) pathological biomarkers analysis [19-21]; (3) Machine learning and deep learning based radiology image analysis [22-29], and their partial integrations. Radiomics features analysis extracts limited features from radiology images and combines statistical analysis and classical machine learning for tumors' classification [16-18]. Pathology biomarkers analysis characterizes different classes of tumors based upon a combination of statistical analysis and machine learning of pathology-based biomarkers [19-21]. Deep learning-based approaches are based on the ensembles of convolution networks and attention-based techniques to diagnose different classes of renal tumors [22-29]. Convolution technique detects local spatial features and their hierarchies to understand and classify images [30]. Self-attention techniques capture long-range dependencies between image-patches, providing a global understanding of images [31, 32].\nAll classes of techniques have their advantages and drawbacks. Radiomics features and pathology biomarker based techniques are limited by known features (or biomarkers) and their specificity, and do not capture all the dependencies between various features. These classes of techniques are in the early stages [33]. Deep learning based image analyses are limited by the resolutions of the imaging devices, a requirement of large databases, and the information-loss during transformations in convolution-based techniques. To alleviate the individual drawbacks, there are recent trends in integrating a subset of these approaches such as machine learning and pathology metadata [34].\nAutomated classification of stage T1a SRMS (SRMs less than 4.0 cm [11-13]), based on image analysis, remains challenging due to their insufficient contrast with surrounding organs and their irregular shapes [35, 36]. Our research improves deep learning based image analysis techniques. The correlation between various classes of techniques is outside the scope of this paper.\nIn this research, we develop a model that integrates convolution, residual links (RL) across convolution layers, cross-channel attention (CCA), principal component analysis (PCA) and transformer-encoder (TE) comprising multihead-attention (MHA) to diagnose SRMs in three ranges {1.2 cm \u2013 2.0 cm, 2.0 cm \u2013 3.0 cm, 3.0 cm 4.0 cm} using the data available in KITS 19 and KiTS 21 challenge datasets [30-32, 35-39]. Convolutional layers extract local features [30]. RLs across convolution layers mitigate vanishing gradient problem by reducing information loss during convolution [37]. CCA enhances feature representation by selectively emphasizing informative features across different channels. PCA reduces the dimensionality of data and error caused by low variance, while preserving information needed for effective classification [38]. Transformer-encoders (TE) capture long-range dependencies between comprised image-patches [31, 39].\nThe major contributions of this research are:\n1. Application of PCA to reduce data dimensionality and error caused by low variance features;\n2. Development of an integrated model based on overall integration of local features, reduction in information loss during transformations using residual links, extracting important features, pruning low variance features, and deriving long-range dependencies;\n3. Classifying T1a SRMs between 1.2 cm \u2013 4.0 cm diameter provided in KiTS challenge datasets.\nThe paper is organized as follows: Section 2 presents related studies. Section 3 describes background concepts. Section 4 discusses the proposed architecture and an algorithm. Section 5 describes the implementation. Section 6 discusses the results. Section 7 presents conclusion and future work."}, {"title": "2 RELATED WORK", "content": "Research efforts to diagnose SRM are classified as radiomics feature analysis, pathology biomarker analysis, deep learning based automated image analysis, and limited combinations of these three classes of techniques.\nPie et al. proposed statistical analysis of a radiomics nomogram that incorporates a radiomics signature and clinical factors for the preoperative differentiation between fat deposits and clear renal cell carcinoma [17]. Nahiri et al. introduced machine learning models (decision tree, random forest, and ADAboost) to analyze radiomics features to classify benign renal masses from renal cell carcinoma [18].\nRosenkratz et al. analyzed pathological, clinical, and imaging features to assess different renal cell carcinoma types less than 2.0 cm using statistical tests and logistic regression [20]. Mahmud et al. combine the convolution based image analysis with pathological metadata [34].\nPrevious research on deep learning based analysis has focused on using ensemble methods with pre-trained Deep Neural Network (DNN), Convolution Neural Network (CNN) based systems with decision fusion, and CEUS-based systems integrating CNNs and multi-feature fusion networks (MFFN) [22-29].\nAlzu'bi et al. proposed a renal-tumor detection and classification model using an ensemble of techniques: CNN, ResNet50, VGG16. Using the output of this ensemble, images were classified as benign or malignant tumor by another CNN [22]. The drawback of this research is the information-loss that affects their accuracy."}, {"title": "3 BACKGROUND", "content": "A convolutional-block is a stack of convolution-layers to extract local-features from an input image. Each convolution-layer comprises a convolution-filter to extract feature-maps [30, 32]. Rectified Linear Unit (ReLU) or Gaussian Error Linear Unit (GELU) reduce the error from the feature-maps, and a pooling layer summarizes the feature-maps.\nA residual link (RL) is a direct connection to the following convolution layers bypassing one or more intermediate layers [32, 37]. If x is the output from the (i - 1)th layer, and F(x) is the output of the ith layer, then input H(x) for the (1 + 1)th layer is equal to F(x) + x. The direct flow of information x reduces error in learning and vanishing gradient problem.\nA self-attention based TE comprises a stack of identical layers, each comprising two sub-layers: MHA and fully connected feed-forward network (FFN), augmented with layer normalization (LN) and RL. TE captures long-range dependencies and contextual relationships between the image-patches [31, 39]. MHA enhances model-efficiency by focusing on the different parts of the input sequence concurrently. Each head has distinct learned weight to compute attention score. These scores are concatenated and linearly transformed to produce the final output.\nChannel attention (CA) prioritizes important features by computing weights for individual channels through pooling operation followed by FFN and a sigmoid function [32]. CCA improves CA by utilizing direct interactions between each channel and its k-nearest neighbors, replacing FFN with a convolutional layer.\nGlobal Average Pooling (GAP) averages spatial feature-values across the entire spatial dimensions of the feature map [40].. It acts as a global descriptor for each channel [40]. Batch normalization (BN) normalizes the spread of the values by taking difference between the values and the mean-value of the batch and dividing by standard deviation of the batch values. It prevents the model from failing to converge and enables higher learning rates.\nPCA is a multivariate statistical technique for dimensionality reduction [38]. It transforms the original set of dimensions into a new set of orthogonal dimensions in the descending order of their variances. Dimensions with low variances are removed, reducing error in the classification, while preserving the information by keeping high variance features and improving the execution efficiency.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a deep learning based visualization technique to highlight the important regions in an input-image [41]. Grad-CAM works by computing the gradient of the class-score after the last convolution layer before a SoftMax layer."}, {"title": "3.1 Notations", "content": "The symbol \u2295 denotes element-wise addition of two vectors. The symbol \u2297 denotes element-wise multiplication of two vectors. Process1 \u2192 Process2 implies that Process1 immediately precedes Process2. The symbol \u03c3 denotes the sigmoid function \u03c3(x) = 1/(1 + ex). In the equations and figures, for the brevity, we also use TE to denote transformer-encoder, LN to denote layer-normalization, BN to denote batch-normalization, RL to denote residual link, MHA to denote multihead-attention, PPE to denote patch and positional embedding, and GAP to denote Global Average Pooling. The variables with superscripts input and output denote the input and output for a unit. Subscripts i, j, and k are used to denote the indices in a sequence or stack."}, {"title": "4 ARCHITECTURE AND ALGORITHM", "content": "The architecture comprises six major components: 1) initial convolution-block; 2) Residual Cross-channel Attention Block (RCCABs); 3) Patch and Position Embedding layer (PPE); 4) stack of TE-blocks; 5) CCA; 6) PCA. GRAD-CAM was used for the visualization of the important regions of interest in the input image. The overall architecture is illustrated in Figure 1. Major components of the architecture are illustrated in Figure 2.\nInitial convolutional-layer extracts feature-maps from input images. This is followed by BN, ReLU, and maxPool layers. BN enhances convergence during learning; ReLU reduces noise and introduces non-linearity.\nFeatureMap = maxPool(ReLU(BN(conv2D16(inputImage))))\n(1)\nThe feature-map FeatureMap is fed to a stack of RCCA blocks (RCCAB-stack) for further processing. RCCAB-stack facilitates local-feature extraction (32, 64, 64, 64, 96, 128, 160) using convolution layers, focuses on important features through CCA, and mitigates the vanishing gradient problem using RLs. It comprises two RCCABS (RCCA-stack \u2192 maxPool). Each RCCA-stack is a sequence of RCCAs.\nIn our implementation, the stack-size m\u2081 for the first RCCA-stack is 2, and the stack-size m2 for the second RCCA-stack is 3. The stack-sizes have been determined empirically, based upon the minimum size that yields maximum accuracy. For the first RCCA in the first RCCA-stack, the input vector is the feature-map FeatureMap from equation (1); for the remaining RCCAs within the same stack, the input-vector is the output-vector from the previous RCCA.\nEach RCCA begins with a Conv2D(3 \u00d7 3) layer, followed by BN, ReLU and another Conv2D(3 \u00d7 3) layer, to learn spatial features from the input data. The output is then passed to a CCA. A CCA employs GAP followed by Conv2D (3 x 3) and sigmoid function \u03c3. GAP extracts global descriptors of each feature-map. Conv2D(3 \u00d7 3) establishes correspondence within the windows of GAP-values. The sigmoid function \u03c3 maps the output of GAP-values to the range [0.0, 1.0]. Feature-maps mapping close to 1.0 have higher importance.\nThe architecture of RCCAB is summarized in equations (2)-(6). The input to CCA is given by equation (2). The output of CCA is given by equation (3). The output of RCCA is given by equation (4). The output of maxPool, fed to the next RCCA-unit, is given by equation (5). The final output $RCCAB^{output}$ is given by equation (6), where n is the index of the last maxPool unit. In our architecture n = 2. The index i denotes the stack-index; the index j denotes the index of RCCA in the ith RCCA-stack; m denotes the number of RCCAs in an RCCA-stack.\n$RCCA^{input}_{i(j + 1)} = Conv2D^{(3 \u00d7 3)}(ReLU(BN(Conv2D^{(3 \u00d7 3)}(RCCA^{input}_{Aij (1 \u2264j \u2264 m)}))))$\n(2)\n$CCA^{input}_{ij(1 \u2264j \u2264m)} = CCA^{input}_{ij (1\u2264j \u2264 m)} \u2297 (\u03c3(conv2D^{(3 \u00d7 3)} (GAP(CCA^{output}_{Aij(1 \u2264j \u2264m)}))))$\n(3)\n$RCCA^{output}_{ij(1 \u2264 j \u2264 m)} = ReLU(BN (CCA^{output}_{ij(1 \u2264 j \u2264 m)}) \u2295 BN(Conv2d^{(1\u00d71)} (RCCA^{input}_{ij(1 \u2264 j \u2264 m)})))$\n(4)\n$RCCA^{input}_{A(i + 1)1 (1 \u2264i + 1 \u2264n)} = maxPool(RCCA^{output}_{i(j+1)m})$\n(5)\n$RCCAB^{output} = maxPool(RCCA^{output}_{nm})$\n(6)\n$RCCAB^{output}$ is fed to the first TE-block in the stack of TE-blocks. The stack-size is 3 in our implementation. Each block captures long-range dependencies and global context across feature-maps. It comprises 1) PCA layer to prune low-variance features; 2) PPE for dividing $PCA^{output}$ into non-overlapping patches; 3) TE layer to derive long-range dependencies among features; 4) a conv2D(1 x 1) layer for linearization; 5) an RCCA layer for the extraction of important features.\nPrior to feeding data into the next TE-layer, PCA is applied. The input for PCA layer in the first TE-block is maxPool($RCCAB^{output}$). In the subsequent PCAs, the input is the output of the previous TE-block's (RCCA \u2192 maxPool layer). The inputs for PCA layers are given by the equations (7)-(8). The output from the PCA layers is given by equation (9). The index k (1 \u2264 k \u2264 3) denotes the index of the current TE-block being processed.\nThe PPE layer occurs before the TE-layer. PPE divides $PCA^{output}$ into non-overlapping patches. Each patch is passed through a conv2D(1 x 1) to obtain the PPE-vector of size 1 x P, where P is patch-size x patch-size. PPE-vector is augmented with learned positional embedding LPE(N \u00d7 D) to retain spatial information. LPE(N \u00d7 D) is a trainable matrix of size (N, D), where N denotes the number of patches, and D is the embedding-dimension (144, 192, 240). The value of N is calculated as (H x W) / P, where H and W denote the height and width of the input feature-map, respectively. This matrix is initialized randomly and learned during training. After deriving the embeddings, input to the TE-layer is given by the equation (10).\nTE-layer utilizes RLs (see Figure 2) to ensure that the information is not lost. TE-layer uses a combination of MHA, RL, FFN to derive the dependencies in the feature-map. The normalized MHA-output $MHA^{output}$ is given by the equation (11). After processing through the encoders, the tensor is reshaped to its original spatial dimensions. The reshaped output of the TE-layer is given by the equation (12).\n$PCA^{input} = RCCAB^{output}$\n(7)\n$PCA^{input}_{i (1 \u2264 i \u2264 k)} = maxPool(RCCA2^{output})$;\n(8)\n$PCAutput = PCA(PCA^{input})$\n(9)\n$TE^{input}_{i (1 \u2264i \u2264 k)} = ReLU(BN(Conv2D^{(1\u00d71)}(PPE(PCA^{output}) \u2295 LPE(N \u00d7 D))))$\n(10)\n$MHA^{output}_{i (1 \u2264i \u2264 k)} = TE^{input}_{i (1 \u2264i \u2264 k)} \u2295 LN(MHA(TE^{input}_{i (1 \u2264i \u2264 k)}))$\n(11)\n$TF^{output} = Reshape(LN(FFN(MHA^{output})) \u2295 MHA^{output})$\n(12)\nThe reshaped encoder-output $T^{output}_{F i (1 \u2264i \u2264k)}$ is fed to linearization-layer comprising (Conv2D(1\u00d71) \u2192 BN \u2192 ReLU) to linearize the output $T^{output}_{Ei (1 \u2264i \u2264k)}$. Conv2D(1\u00d71) layer expands the tensor back to its original number of channels. $Linear^{output}_{i (1 \u2264i \u2264k)} $\u2013 the output of this linearization layer is given by the equation (13). The linearized output $Linear^{output}_{i (1 \u2264i \u2264k)}$ is element-wise added to the PCA output $PCA^{output}$ to derive input to the following RCCA-layer $RCCA2^{input}$ as shown in the equation (14).\n$Linear^{output}_{i (1 \u2264i \u2264k)} = ReLU(BN(Conv2D^{(1\u00d71)} (T^{output}))) $\n(13)\n$RCCA2^{input} = PCA^{output} \u2295 Linear^{output}_{i (1 \u2264i \u2264k)}$\n(14)\nAfter processing the final TE-block, $RCCA2^{output}$ \u2013 the final output of the stack of the TE-blocks, is fed to a convolutional-block (Conv2D640 \u2192 BN \u2192 ReLU) that extracts 640 feature-maps to adjust channel dimensions, followed by GAP to distill the spatial information. Finally, a dense layer with softMax function derives the probabilities of various classes for the given image. The arg_max function picks the most probable class. The final output class is given by the equation (15). The overall algorithm is described in Algorithm 1.\nClass = arg_max(softmax(ReLU(BN(Conv2D640(RCCA2^{3output})))))\n(15)"}, {"title": "5 IMPLEMENTATION", "content": "The software was developed using Python 3.9, using the TensorFlow framework V2.16.1. The model was trained on a single processor in a processor-cluster, accelerated by an NVIDIA Tesla V100 GPU and 32 GB of RAM, supported by CUDA 11.8.0, and cuDNN version 8801. The process took 8 hours to complete on this machine, with an average epoch time of 84.3 seconds. Each epoch processed 214 images."}, {"title": "5.1 Data and Preprocessing", "content": "Publicly available KiTS19 and KiTS21 Challenge datasets were used [35, 36] to train, validate, and test our model. The extracted datasets contained axial projections of 300 contrast-enhanced annotated CT scans, each containing 512 image-frames, obtained from patients who underwent partial or radical nephrectomy for kidney tumors. Ground truth labels were manually annotated and confirmed by expert clinicians.\nAll image-frames were converted into JPG format, resulting in 153,560 images (300 X 512). After filtering out black images, 23,100 images, displaying anatomical organs, were retained. From these, 13,579 images had tumor size less than 4.0 cm, which included tumor-sizes in the range of {1.2 cm - 2.0 cm, 2.0 cm \u2013 3.0 cm, 3.0 cm - 4.0 cm}. Clinical annotations showed 10,327 images of healthy kidney and 3,252 images having SRMs. These images were selected for the experiment.\nThe images were resized to 256 x 256 pixels using nearest-neighbor interpolation to preserve sharp edges and other details. Intensity values were normalized using min-max scaling to map in the range [0.0, 1.0]. Synthetic Minority Over-sampling Technique (SMOTE) was used to create synthetic samples for the minority class by interpolating between existing samples [42]. SMOTE generates new samples along the line segments connecting the instance to its neighbors. This process resulted in 10,327 SRM images and a total of 20,654 images. Our model was trained on 20,654 images. Sample images are shown in Figure 3."}, {"title": "ALGORITHM 1 \u2013 A deep learning algorithm to accurately classify SRM and healthy kidney", "content": "Algorithm classify_SRM\nInput: 1. image-frame Im; 2. number of RCCA stacks n; 3. sequence of RCCA stack-sizes {m1, ..., mn}\n4. transformer-encoder stack-size k;\nOutput: 1. classification pair (Im, class), where class \u2208 {healthy, SRM}\n{ Feature-map FM = maxPool(ReLU (BN (conv2D16(Im)))); % apply equation (1) to derive initial feature-map\ni = 1; RCCAinput = FM; % feed initial feature-map to first RCCA in the first RCCA-stack\nwhile (i < n) % process stack of RCCAB\n{ m = mi; j = 1; % Initialize\nwhile (j\u2264m) % process ith RCCA in ith RCCAB\n| { CCAinput = Conv2D(3 \u00d7 3)(ReLU(BN(Conv2D(3 \u00d7 3) (RCCAinput)))); % Apply equation (2)\nCCAoutput = CCAinput \u2297 ((conv2D (3\u00d73)(GAP(CCAinput)))); % Apply equation (3)\n% Apply equation (4) to compute RCCAinput Cinput = RCCAoutput\nRCCAinput = RCCAoutput = ReLU(BN(CCAutput) \u2295 BN(Conv2D(1 \u00d7 1)(RCCAinput));\nj = j + 1; } % Increment the index to process the next RCCA layer\n'i = i + 1; RCCAinput = maxpool(RCCAutput);} % Apply equation (5) and go to next RCCAB\nRccAboutput = maxpool(RCCAutput); % Apply equation (6) to derive the final output of RCCAB-stack\nPCAinput = RCCABoutput; i = 1; % Apply equation (7) to feed the data to the stack of transformer-encoder block\nwhile (i\u2264k) % Process the transformer-encoder block (TE-block)\n|{ PCAqutput = PCA(PCAinput); % Apply PCA for the first time\nLi(1 \u2264\nHoutput\nTek) = ReLU(BN(Conv2D(1\u00d71)(PPE(PCAutput) \u2295 LPE(N \u00d7 D)))) % Apply equation (10)\n= Tek) \u2295 LN(MHA(TEinput )); % Apply equation (11)\ni \u2264\nTEoutput = Reshape(LN(FFN(MHAutput)) + MHAutput); % Apply equation (12)\nLinear output = ReLU(BN(Conv2D(1 \u00d7 1)(TEoutput))); % Apply equation (13) for linearized output\nRCCA2input = PCAutput + Linear output; % Apply equation (14) to feed linearized output to second RCCA\nCCA2input = Conv2D(3\u00d73)(ReLU(BN(Conv2D(3 \u00d7 3)(RCCA2input)))); % Apply equation (2) within RCCA2\nCCA2output = CCA2input \u2297 (\u03c3(conv2D(3 \u00d7 3) (GAP(CCA2input)))); % Apply equation (3) within RCCA2\nRCCA2qutput = ReLU(BN(CCA2qutput) \u2295 BN (Conv2D(1\u00d71)(RCCA2input)); % Apply equation (4) within RCCA2\n+1\nPCAinput = RCCA2output '; ; i i = = i i + + 1;} % Increment the index and go to the following layer in the TE-blocks stack\nClass = arg_max(softmax(RELU(BN(Conv2D640(RCCA2putput))))); % Apply equation (15) to derive the class\nreturn (Im, Class);}"}, {"title": "5.2 Training", "content": "The dataset was split into training, validation, and test sets, including 12,392 (60%), 4130 (20%), and 4132 (20%) samples, respectively. The classification model was trained for 200 epochs, based upon the highest stable accuracy experiment with the minimum number of epochs (see Figure 4a), with additional five-fold cross-validation to mitigate overfitting."}, {"title": "6 RESULTS AND DISCUSSION", "content": "The model's performance was evaluated using confusion matrix, accuracy, precision, F1-score, sensitivity, and specificity on a set of 4132 (1139 in SRM-range 1.2 cm - 2.0 cm; 1873 in SRM-range 2.0 cm - 3.0 cm; 1120 in SRM-range 3.0 cm 4.0 cm) randomly selected images (see Table 1). The performance data showed an accuracy of 97.6% for the SRM-range 1.2 cm - 2.0 cm; an accuracy of 98.5% for the SRM-range 2.0 to 3.0 cm; an accuracy of 97.3% for the SRM-range 3.0 to 4.0 cm.\nMost of the related work has focused on reporting accuracy as the performance metrics for their models. We provide a more comprehensive evaluation by including accuracy, precision, F1-score, sensitivity, and specificity. Table 2 presents a comparison of our model with kidney-related disease classification models. Other models cannot diagnose SRM less than 4.0 cm. Our model accurately detects SRM at least in the range 1.2 cm 4.0 cm as available in the KiTS 19 and KiTS 21 challenge datasets [35, 36]. Our model significantly outperforms the accuracy achieved by other models. Figure 4 illustrates our experimental results."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "Early-stage automated diagnosis of SRM will avoid undesired malignancy, improve patient prognosis, and reduce morbidity and long-term cost burden. We have proposed an integrated deep learning model to detect SRM in a set of diameter-ranges {1.2 cm \u2013 2.0 cm, 2.0 cm 3.0 cm, 3.0 cm \u2013 4.0 cm} with high accuracy.\nThe model integrates local features, global interdependencies, reduces information loss, mitigates vanishing gradient problem, and uses PCA based dimension-reduction for pruning low variance features.\nCurrently, our model has been tested, based upon the availability of the test data in KiTS 19 and KiTS 21 challenge dataset, to classify SRMs greater than 1.2 cm from axial slices [35, 36]. It could not be tested below 1.2 cm diameter due to the unavailability of data in the datasets. The current approach does not utilize the information available from coronal and sagittal axes slices.\nOur model currently classifies cases into binary categories: healthy kidney and SRM \u2264 2 cm, healthy kidney and SRM \u2264 3 cm, or healthy kidney and SRM \u2264 4 cm. It does not handle multi-class cases where it needs to differentiate between healthy kidney and SRM within range 1.2 \u2013 2.0, 2.0 \u2013 3.0, 3.0 \u2013 4.0 simultaneously. We plan to implement the multiclass classification in the future.\nWe intend to perform multi-axial analysis by integrating data from coronal and sagittal views and tumor heterogeneity patterns, including correlation with biopsy images, and correlation with pathological outcomes to improve accuracy and subclassify renal tumor types [34, 43]."}]}