{"title": "Generative AI Framework for 3D Object Generation in Augmented Reality", "authors": ["Majid Behravan"], "abstract": "This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.", "sections": [{"title": "Introduction", "content": "The rapid advancements in generative artificial intelligence (AI) have enabled remarkable capabilities in various domains, including natural language processing (NLP), image and video generation, and complex 3D model creation [19]. LLMs such as those described by Brown et al. [11] play a pivotal role in facilitating such developments. These models have revolutionized how we interact with technology by offering more immersive experiences, as demonstrated by the integration of generative AI in creating 3D objects for augmented reality (AR) and extended reality (XR) environments.\nDespite significant progress, generating consistent and efficient 3D models from natural language or image inputs remains a challenge. Notable models like DreamFusion, which is built on diffusion processes for text-to-3D generation, have been influential but come with significant limitations, such as high latency and computational demands [49]. Specifically, DreamFusion's generation time ranges from 30 to 40 minutes, rendering it impractical for real-time AR applications [62]. Furthermore, as highlighted by Li et al. [37], models leveraging Neural Radiance Fields (NeRF) and related techniques require substantial computational resources, limiting their scalability and effectiveness in dynamic AR settings.\nAnother issue associated with diffusion-based generative models is inconsistency in output."}, {"title": "Research Objectives", "content": "This research aims to address the existing challenges in the field of generative AI for AR environments by developing a comprehensive framework. The specific objectives of this study are:\nTo develop an optimized generative AI framework that reduces the generation time of 3D objects, enabling real-time interactions in AR.\nTo leverage LLMs for recognizing spoken input and recommending contextually relevant objects, enriching user interaction.\nTo establish a pre-generated object repository for efficient GPU usage and consistent outputs.\nTo integrate VLMs to detect areas and recommend objects based on spatial analysis, improving contextual relevance.\nTo implement image-to-3D conversion for seamless real-world object integration into AR environments."}, {"title": "Research Questions", "content": "This study aims to answer the following research questions:\nHow can LLMs and VLMs be employed to enhance the recommendation of contextually relevant objects in AR while supporting 3D object generation?\nHow can image-to-3D and text-to-3D object generation be optimized for near real-time integration into AR while maintaining accuracy and visual fidelity?\nWhat is the usability of using 3D object generation and AR integration while maintaining accuracy and visual fidelity?"}, {"title": "Significance of the Study", "content": "The significance of this study lies in its potential to overcome the key limitations of current generative AI systems for AR environments. By reducing generation time, enhancing GPU efficiency, and integrating multilingual capabilities, this research contributes to the advancement of real-time and inclusive AR experiences. Furthermore, the incorporation of LLMs and VLMs for object recognition and context-based recommendations fosters dynamic interactions, enhancing both user engagement and practical applications in industries such as design, education, and retail. The development of an efficient, secure, and customizable framework broadens the accessibility and applicability of AR technologies."}, {"title": "Contribution", "content": "This thesis presents several key contributions to the field of generative AI for AR applications, particularly in the dynamic and real-time generation of 3D models from diverse input formats. These contributions are highlighted below:\n\u2022 Framework Development for Real-Time 3D Generation: A framework was designed and implemented to integrate generative AI, AR, and vision language models (VLMs), achieving real-time 3D model creation from textual, visual, and speech-based inputs.\n\u2022 Advanced Object Detection and Context-Aware Recommendations: Leveraging VLMs, the system analyzes the user's environment, recommends contextually relevant objects, and seamlessly integrates virtual objects with real-world surroundings, enhancing AR experience through personalization and contextual relevance.\n\u2022 Enhanced User Interaction and Immersive Experiences: By employing Shap-E models and seamless interaction controls, the framework democratizes 3D model generation, enabling users across industries\u2014such as education, gaming, and retail\u2014to customize AR content without specialized knowledge.\nThrough these advancements, this thesis provides a novel AI-driven system that bridges the gap between generative AI and AR applications, addressing latency, object consistency, and context-awareness issues. This work extends the capabilities presented in our related paper, Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality [4], by demonstrating enhanced accuracy in object recommendations, reduction in latency for 3D model rendering, and improved user interaction in real-time AR environments."}, {"title": "Methodology Overview", "content": "The research employs a multifaceted methodology comprising the following key components:\n\u2022 Framework Development: Utilizing the Shap-E model [32] as the base for text-to-3D generation, with modifications tailored for AR applications to optimize loading and interaction times.\n\u2022 LLM and VLM Integration: Leveraging LLMs to recognize spoken input and recommend contextually relevant objects, and VLMs to analyze images for spatial understanding and object placement.\n\u2022 Pre-Generated Repository: Developing a repository of pre-generated 3D objects to ensure consistency, reduce GPU usage, and expedite generation time.\n\u2022 Multilingual Integration: Implementing AI-driven translation models for speech-to-text and text-to-speech, ensuring seamless interaction in multiple languages.\n\u2022 Performance Analysis: Testing the system's performance on AR devices, including generation time, model output size, and user experience metrics."}, {"title": "Thesis Structure", "content": "The structure of this thesis is as follows:\nChapter 1: Introduction\nThis chapter outlines the problem statement, research objectives, research questions, significance of the study, methodology overview, and thesis structure.\nChapter 2: Literature Review\nThis chapter reviews the current state of research in generative AI, AR, VLMs, and LLMs.\nChapter 3: Methodology and Implementation\nThis chapter details the algorithms and methodologies developed for 3D model generation and describes the integration of the developed algorithms into the AR framework.\nChapter 4: Evaluation and Results\nPresents the findings from the system's performance tests and user interaction studies, accompanied by data analysis.\nChapter 5: Discussion\nInterprets the results in the context of research objectives and questions, addressing the implications and limitations.\nChapter 6: Conclusion and Future Work\nSummarizes the research contributions, discusses future research directions, and potential enhancements."}, {"title": "Summary", "content": "This chapter has provided an introduction to the research topic, highlighting the problem statement, research objectives, and questions guiding this study. The significance of the research was outlined, emphasizing the potential impact on real-time AR experiences and generative AI applications. A brief overview of the methodology was provided, along with a summary of the thesis structure. This foundation sets the stage for an in-depth exploration of related literature and the methodologies employed in this research."}, {"title": "Review of Literature", "content": "We delve into three critical areas of technological advancement that significantly enhance interactive technologies and digital design:\n\u2022 Speech-to-Text and Text-to-Speech technologies.\n\u2022 The application of LLMs for object detection and recommendation.\n\u2022 The innovation of Text-to-3D and Image-to-3D conversion.\n\u2022 Vision language model.\nThe corresponding subsections provide an in-depth analysis of the current research and developments in these fields, illustrating their transformative impacts across various industries such as communication, healthcare, and digital content creation. These technologies not only demonstrate the integration of AI in enhancing user experiences but also underscore the synergy between advanced computational models and practical applications in real-world scenarios."}, {"title": "speech-to-text and Text-to-Speech", "content": "STT technologies convert spoken languages into written text. STT has various applications such as voice user interfaces, transcribing meetings, and accessibility features for those unable to type. Significant advancements in AI, particularly in speech processing and language modeling, have facilitated new capabilities in multilingual and multimodal applications. Hono et al. explored the integration of pre-trained speech and language models for end-to-end speech recognition, demonstrating how these models leverage large amounts of data and sophisticated algorithms to perform complex tasks such as Automatic Speech Recognition (ASR) without the need for extensive training from scratch [25].\nFurthermore, the work by [54] on SeamlessM4T highlights the integration of multimodal translation systems that support STT and TTS functionalities, encompassing up to 100 languages. This model is based on a unified approach using a vast corpus of aligned speech translations, which significantly enhances the translation accuracy and robustness against various speech variations. Its adoption in multilingual applications, such as the mobile platform discussed, highlights its capability to address linguistic challenges effectively in disaster scenarios, ensuring accurate and robust cross-linguistic communication [6].\nAdditionally, Hu et al. introduced WavLLM, a robust and adaptive speech large language model that combines dual encoders and a two-stage curriculum learning approach. This model exemplifies the next step in evolving LLMs to handle complex auditory tasks and improve generalization across varied contexts [26].\nBenster et al. presented an innovative approach to silent speech recognition with their Multimodal Orofacial Neural Audio (MONA) system, which significantly improves recognition accuracy using cross-modal alignment techniques. This breakthrough reduces the word error rate substantially, showcasing the potential of non-invasive interfaces for soundless verbal communication [7].\nIn the context of mental health, Zhang et al. developed a method to integrate acoustic speech information into LLMs for depression detection. Their approach, which utilizes acoustic landmarks, represents a significant advancement in how speech patterns can be analyzed to detect and analyze depressive states, demonstrating the broad applicability of LLMs beyond traditional text-based tasks [72].\nLastly, Hu et al. in another significant study, introduced GenTranslate, which adopts a generative paradigm for multilingual speech and machine translation. This new model enhances the quality of translations by integrating the diverse information present in multiple translation hypotheses, which is a step forward in optimizing language model outputs for complex multilingual environments [27].\nSTT and TTS technologies are crucial for improving accessibility in AR environments. They allow users to interact with AR systems through voice, which is particularly beneficial for individuals with ADHD who may struggle with traditional input methods like typing or touch interfaces [22].\nThese studies collectively demonstrate the diverse applications of AI in enhancing speech recognition, translation, and healthcare diagnostics, aligning closely with the objectives of frameworks focusing on real-time environment customization through speech-driven interfaces."}, {"title": "LLMs for Object Detection and Recommendation", "content": "The integration of LLMs into applications requiring precise object detection and recommendation systems has been a focal area of recent research, as demonstrated in several groundbreaking studies. The advancements in LLMs have significantly contributed to enhancing NER capabilities and recommendation systems, essential components of our AR framework.\nYan et al. [64] developed the LTNER framework, which leverages the GPT-3.5 model to improve the accuracy of LLMs in NER tasks significantly. Their approach uses a novel Contextualized Entity Marking Gen Method, highlighting LLMs' potential to comprehend and process context more effectively than traditional methods.\nSimilarly, Ashok and Lipton [2] introduced PromptNER, a state-of-the-art algorithm for few-shot Named Entity Recognition (NER). This method employs prompt-based heuristics to enable LLMs to identify and explain potential entities based on pre-defined types, demonstrating the adaptability of LLMs in handling varied NER tasks without extensive training.\nIn the realm of recommendations, Kim et al. [33] proposed the A-LLMRec system, an efficient all-round LLM-based recommender system that integrates collaborative filtering knowledge directly into LLMs. Their system showcases the flexibility of LLMs in utilizing existing datasets and knowledge bases to provide accurate recommendations, even in scenarios with limited user-item interactions.\nLLMs are used in various ways within virtual environments, including personalized e-learning systems where they adapt content to learners' emotional and cognitive states, enhancing engagement and fostering deeper interaction and comprehension [53].\nFurther extending the capabilities of LLMs in recommendations, Zhang et al. [71] discussed the adaptations of LLM training paradigms for recommender systems. Their tutorial emphasized the potential of LLMs to transfer knowledge across domains, improving the effectiveness and trustworthiness of recommendations.\nMoreover, the work of Cao et al. [12] on integrating recommendation-specific knowledge into LLMs addresses the knowledge gap between traditional recommender systems and LLM capabilities. Their method enhances LLMs' ability to generate more relevant and personalized recommendations by incorporating data samples that reflect user preferences and item correlations."}, {"title": "Text-to-3D", "content": "Text-to-3D conversion involves generating three-dimensional models from textual descriptions. This technology is particularly interesting in fields like XR, where text-based inputs can be used to create immersive environments or objects dynamically.\nThe translation of textual descriptions into 3D models has been enriched by a variety of advanced methodologies, exploring autoregressive models, diffusion processes, and deep learning techniques. Among these, [46]'s \"AutoSDF\" and [20]'s \u201cShapeCrafter\" have demonstrated how recursive algorithms and shape priors can innovatively generate shapes from textual descriptions [20, 46].\nSignificant developments also include voxelized diffusion models as demonstrated by [16] in \u201cSDFusion\u201d and [39] in \u201cDiffusion-SDF,\u201d which significantly enhance 3D shape reconstruction and generation capabilities from complex textual inputs [16, 39]. Moreover, [74] introduce a unique alignment of shape, image, and text representations in \u201cMichelangelo,\u201d facilitating highly conditional 3D shape generation [74].\nA central piece of our framework is the use of the Shap-E model by [32], which generates detailed 3D objects by interpreting complex textual data, showcasing the potent capabilities of LLMs in 3D modeling [32]. This model's efficiency in creating detailed and contextually appropriate 3D representations significantly contributes to the advancement of user interaction in XR spaces.\nAdditionally, the integration of structure-aware generative models is notably advanced through \u201cShapeScaffolder\" by [59] and \u201cSALAD\u201d by [34], which emphasize generating realistic and structurally sound 3D models from textual descriptions, enhancing both the aesthetic and functional aspects of generated objects [34, 59]. These developments underline the increasing sophistication in generating 3D content that is not only visually accurate but also contextually appropriate to user input.\nThis comprehensive synthesis of technologies underscores our framework's reliance on cutting-edge advancements in AI, specifically leveraging the intricate processing capabilities of LLMs and innovative generative techniques to create immersive XR experiences. Our approach aims to seamlessly integrate these technologies to deliver a dynamic, responsive, and highly customizable user experience in AR environments."}, {"title": "VLMS", "content": "VLMs, such as CLIP (Contrastive Language-Image Pretraining) and FLAVA (Foundational Language and Vision Alignment), have been pivotal in bridging the gap between visual and textual data. CLIP, introduced in [50], demonstrated remarkable zero-shot classification capabilities by training on 400 million image-caption pairs, using a contrastive loss to align image and text representations in a shared space. FLAVA, developed by [57], employs masking strategies to enhance multimodal learning, achieving state-of-the-art performance across various benchmarks.\nGenerative models, such as CoCa (Contrastive Captioner) and CM3leon, have further advanced the field by enabling the generation of both text and images. CoCa, proposed in [66], integrates a generative loss with contrastive learning, allowing the model to perform tasks like visual question answering without additional multimodal fusion modules. CM3leon, introduced by the Chameleon Team [67], leverages a cross-attention mechanism to handle interleaved text and image data, enhancing its capabilities in text-to-image and image-to-text generation tasks.\nGenerative VLMs can generate images or captions and are typically the most expensive to train. Improving training data quality through synthetic data generation and augmentation techniques has shown significant benefits. Bootstrapping Language-Image Pre-training (BLIP) [38] generates synthetic captions to enhance data consistency and completeness, leading to better model performance. Nguyen et a. [41] demonstrated that mixing real and synthetic captions during pretraining improves alignment and overall model effectiveness.\nLeveraging pretrained language models, such as LLaVA (Large Language-and-Vision Assistant) and MiniGPT, has become a popular approach to reduce computational costs while maintaining high performance [43]. Pre-trained backbones use models like Llama to learn a mapping between an image encoder and the LLM. Frozen, introduced in [61], connects vision encoders to pretrained language models using a lightweight mapping network, enabling rapid adaptation to new tasks. MiniGPT-4, described in [77], employs a linear projection layer to align image representations with a language model, significantly reducing training time and resources.\nIn addition to these models, recent advancements have explored the integration of video data into VLMs. This extension is crucial for applications requiring temporal context, such as video summarization and real-time interaction in AR environments. Models like Flamingo, which process interleaved text and video frames, have demonstrated enhanced performance in tasks involving dynamic visual content. These developments highlight the importance of multimodal data that includes both static images and video sequences for comprehensive vision language understanding.\nAnother significant area of progress is the improvement of grounding techniques in VLMs. Grounding involves associating textual descriptions with specific visual elements, which is essential for tasks like object detection and scene understanding in AR. Methods that incorporate bounding box annotations and negative captioning have been particularly effective. These techniques ensure that models not only generate accurate descriptions but also correctly identify and localize objects within a scene, thereby improving the practical utility of AR applications.\nThe use of large-scale pretraining datasets has also been a focal point in recent research. Datasets such as LAION-400M, which consist of hundreds of millions of image-text pairs, have been instrumental in training robust VLMs. However, the quality and diversity of these datasets are critical. Efforts to curate and balance these datasets, such as by removing duplicates and ensuring a wide range of concepts, have been shown to enhance the performance of models in zero-shot learning scenarios.\nFinally, there has been a growing emphasis on the ethical considerations and societal impacts of VLMs. Issues such as bias in training data, the potential for generating harmful content, and the need for responsible AI development are increasingly being addressed. Researchers are developing frameworks for assessing and mitigating biases in VLMs and are advocating for transparent and inclusive practices in the creation and deployment of these technologies.\nAs generative multi-modal AI continues to evolve, ensuring its ethical use will be paramount to gaining public trust and maximizing its positive impact on society. Incorporating context-aware content in AR improves user experience and task performance by providing relevant, situational information that enhances efficiency and reduces errors [17]. Moreover, advancements in integrating physiological signals with LLMs demonstrate potential for creating more empathetic and context-aware AI systems [18].\nBy building on these advancements, future research in generative multi-modal AI for AR can continue to push the boundaries of what is possible, creating more interactive, intuitive, and context-aware digital environments."}, {"title": "Image-to-3D Generation", "content": "The field of image-to-3D generation using generative AI has seen substantial progress, with numerous methodologies contributing to its rapid development. We describe the key works and models that have advanced this domain, particularly in the context of single-view 3D reconstruction.\nEarly advancements in 3D generative modeling largely focused on the development and application of Generative Adversarial Networks (GANs) [23]. This approach was pivotal in shaping the early landscape of 3D generation, enabling the creation of models from representations like point clouds [1] and implicit representations [15]. Building on this foundation, recent studies have integrated GANs with differentiable rendering methods, which utilize multiple rendered views as the basis for the loss signal, significantly enhancing the quality of generated 3D models [14].\nThe advent of diffusion models marked a significant leap forward in high-quality image generation, with growing interest in extending these models to the 3D domain. Typically, this involves training a Vector-Quantized Variational Autoencoder (VQ-VAE) on 3D representations such as triplane [55] and point cloud [70], followed by applying the diffusion model in the latent space. Although direct training on 3D representations is less common, it is gaining traction, particularly with point clouds [45], voxels [75], and neural wavelet coefficients [29].\nAmong the innovative methods, ShapeClipper stands out by reconstructing 3D object shapes from single-view RGB images using CLIP-based shape consistency and geometric constraints. This approach leverages the semantic consistency of CLIP embeddings to enhance 3D shape learning, showing superior performance on datasets like Pix3D, Pascal3D+, and OpenImages [28].\nSimilarly, Shap-E employs autoencoders trained on explicit 3D representations combined with a diffusion model in the latent space, offering a flexible approach to representing complex 3D structures. This method addresses the limitations of fixed-resolution explicit outputs, providing a scalable solution for 3D model generation [1].\nMultiview Compressive Coding (MCC) advances the field by learning a general-purpose 3D representation from RGB-D views of diverse real-world objects. MCC utilizes an attention mechanism on encoded appearance and geometric cues, outperforming traditional global-feature strategies. Its robustness in handling diverse viewpoints and occlusions is particularly advantageous for real-world applications [76].\nVoxel-based methods have also seen significant advancements. These approaches benefit from the regularity of voxel representation and the application of 2D convolutional neural networks (CNNs). However, they often face a trade-off between resolution and computational efficiency. For example, voxel-grid methods offer better accuracy than bounding boxes but at the cost of increased computational resources [69]. SparseFusion addresses this by integrating sparse voxel grids with neural networks, achieving high-fidelity 3D reconstructions from sparse input views, suitable for real-time AR and VR applications [76].\nMesh-based techniques, exemplified by Mesh R-CNN, reconstruct object meshes by deforming a template such as a unit sphere. Despite their efficiency, these methods are constrained by the fixed topology of the initial mesh. Innovations like Total3DUnderstanding improve on this by integrating room layout, object bounding boxes, and meshes into a unified framework, achieving state-of-the-art results in indoor scene reconstruction on datasets such as SUN RGB-D and Pix3 [48].\nImplicit surface methods, including those utilizing NeRF, have gained prominence for their ability to implicitly represent complex geometries. Techniques like PixelNeRF and CodeNeRF employ lightweight networks to model arbitrary topologies, proving effective for scene generalization and novel-view synthesis [65].\nIn the realm of holistic scene understanding, approaches such as Towards High-Fidelity Single-view Holistic Reconstruction use an instance-aligned implicit function (InstPIFu) for detailed object reconstruction and an implicit representation for complex room geometries. These methods outperform existing techniques on datasets like SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT [42].\nBai and Li provide a comprehensive overview of advancements in 3D generative AI, highlighting significant progress in 3D object creation, character and motion generation, and the development of neural network-based 3D rendering models such as NeRF and 3D Gaussian Splatting [3].\nThe Make-A-Shape model by Autodesk represents a substantial advancement in the field, training a large-scale 3D generative model using a vast dataset of 10 million publicly available shapes. This model introduces novel techniques such as the wavelet-tree representation for compact encoding and the subband adaptive training strategy, enabling effective learning of both coarse and detailed features. Make-A-Shape demonstrates superior performance in generating high-quality 3D shapes across diverse input modalities [30].\nLastly, the GET3D model by NVIDIA leverages recent advances in differentiable surface modeling and rendering, along with 2D Generative Adversarial Networks, to train a model that generates explicit textured 3D meshes. This model excels in producing high-fidelity 3D shapes with detailed geometry and complex topology from 2D image collections, making it highly applicable across various real-world applications, from gaming and entertainment to e-commerce and design [21]."}, {"title": "Gaps in the Literature", "content": "Despite advancements in developing robust and user-friendly AR applications, significant gaps remain, particularly in speech-to-3D translation and image-to-3D conversion. While previous works have LLMs and AI-driven methods, there are still notable areas that require further exploration:\n\u2022 Output Size and Latency: Generative models often produce 3D outputs with large file sizes, leading to latency in AR devices. This affects the real-time nature of the user experience, making interactions slower and less fluid.\n\u2022 Image-to-3D Conversion Limitations: Current models struggle with complex backgrounds and lighting variations in images captured through AR headsets, leading to inaccurate 3D reconstructions. This impacts the quality and realism of AR applications, hindering user immersion and satisfaction.\n\u2022 Contextual Awareness: LLMs lack real-time environmental context, which limits their ability to create AR objects that match the surrounding space in terms of color, style, and other visual elements. This gap reduces the relevance and visual coherence of AR content.\n\u2022 Multilingual Support in Open-Source LLMs: Most open-source LLMs are trained predominantly on English datasets, limiting their effective application in multilingual contexts. This gap restricts the usability and inclusivity of AR applications on a global scale, impacting their effectiveness in diverse linguistic settings and limiting global user engagement.\n\u2022 Speech-to-Text Accuracy: Current STT systems face issues such as transcription inaccuracies due to background noise, diverse accents, and speech nuances. These errors can impede user interaction, leading to misinterpretations of user commands and decreased efficiency in AR environments. If not properly addressed in the user interface (UI), these inaccuracies can increase the GPU load of the system, resulting in higher operational costs.\n\u2022 Efficient GPU Utilization: High computational demands of GPU usage remain a challenge, especially when generating new 3D models in real-time. This impacts the performance and cost-effectiveness of AR systems, affecting their scalability and responsiveness.\n\u2022 Integration of 3D Models into AR Environments: Ensuring accurate scaling and alignment of generated 3D models for seamless integration into real-world views remains an area that needs refinement. This affects the realism and usability of AR applications, diminishing the overall user experience."}, {"title": "Automatic 3D object creation in AR", "content": "AR enhances real-world experiences by overlaying digital objects in physical environments. A significant advancement in this domain is the automatic creation of 3D objects from various inputs such as text, images, or speech, offering users more immersive, interactive experiences. However, real-time and contextually accurate 3D object generation remains challenging due to processing demands, data requirements, and the complexity of real-world spatial adaptation."}, {"title": "Technologies for 3D Object Generation", "content": "Generative AI Models: Models such as Shap-E leverage AI to convert textual and visual inputs into 3D models. These models use deep learning frameworks to understand input prompts and generate object shapes, textures, and orientations.\nLarge Language Models (LLMs): LLMs play a crucial role in interpreting and processing natural language inputs, allowing users to specify desired object characteristics through voice commands or text prompts.\nVision Language Models (VLMs): VLMs, such as CLIP and FLAVA, bridge visual and textual data, enabling the AR system to interpret surroundings accurately and recommend context-appropriate objects.\nImage-to-3D Conversion Techniques: These approaches convert 2D images to 3D objects using techniques like voxel-based models, mesh reconstruction, and neural radiance fields (NeRF). They play a central role in adapting real-world images into AR."}, {"title": "Key Challenges in Automatic 3D Object Creation", "content": "Latency and Computation Load: Generating 3D models from high-complexity data (e.g., high-resolution images or detailed textual descriptions) requires significant GPU resources, affecting real-time performance.\nData Consistency and Accuracy: Variability in outputs due to stochastic elements in generative models impacts user experience, particularly when identical prompts yield different results.\nContextual Relevance: Models must account for the context and spatial layout of the AR environment to ensure objects fit naturally. This includes understanding user environments, spatial relations, and aesthetic coherence.\nMultilingual and Multimodal Challenges: Ensuring accuracy across different languages and input types is critical for global usability, requiring robust, multilingual processing layers for speech and text inputs.\nResource Management in Real-Time Applications: Large 3D models can create high memory and GPU usage, impacting device performance, especially on mobile AR hardware."}, {"title": "Approaches to Address Challenges", "content": "Efficient Frameworks: Implementing frameworks that optimize model loading and caching reduces latency and improves real-time interactions, as seen in methods like pre-generating object libraries.\nIncorporation of Pre-Generated Object Repositories: By utilizing libraries of pre-existing 3D models, the system reduces generation time and computational load, enhancing both speed and reliability.\nMultimodal Integration with VLMs and LLMs: Leveraging VLMs for spatial awareness and LLMs for context-aware recommendations improves user interaction by suggesting relevant objects and layouts.\nObject Size Optimization Techniques: Using algorithms such as the quadric edge collapse for vertex reduction in complex models balances visual fidelity with manageable file sizes, optimizing AR device performance."}, {"title": "Summary", "content": "Automatic 3D object creation in AR combines multiple AI techniques, addressing a crucial gap in enhancing user experience through realistic digital overlays. Addressing latency, accuracy, and contextual challenges remains key to realizing responsive and immersive AR systems, making automatic 3D generation accessible to wider audiences and practical for applications in design, retail, and education. The following chapter will detail the methodology used to implement these solutions, outlining technical processes and integration into the AR environment."}, {"title": "Methodology and Implementation", "content": "This framework comprises three main subsystems designed to enhance interactive AR experiences: 1) Speech-to-3D, 2) Context-Aware Object Recommendation, and 3) Image-to-3D Generation. Each subsystem addresses specific challenges associated with real-time, user-centric content generation and integration within AR environments. Figure 4.1 illustrates the conceptual model of this framework, providing an overview of the interconnected subsystems. The following sections delve into the detailed implementation and functionalities of each subsystem."}, {"title": "Speech-to-3D Subsystem", "content": "Matrix framework functionality is meticulously designed to ensure an intuitive, inclusive, and interactive user experience. By harnessing state-of-the-art technologies and advanced AI models, we facilitate a seamless transition from verbal commands to visual outputs in real-time, thereby enhancing the capability of users to interact dynamically with the AR environment.\nWe outline the Speech-to-3D Subsystem, which is used to interpret user input, generate 3D models, and allow for user-driven customization of the AR environment.\nWe use an AR application to illustrate the implemented Matrix framework service and functionality (fig. 4.2), from initial language selection to the final customization of the interactive environment. The goal is to maximize user engagement and responsiveness, ensuring that the technology remains accessible and efficient across various languages and functional demands.\nMatrix Speech-to-3D Subsystem fig. 4.2, includes several key components and processes that are labeled with abbreviations to denote specific functions. S2TT is a Speech-to-Text Translation, indicating the conversion process from spoken language to textual format. T2S and T2ST denote Text-to-Speech and Text-to- Speech Translation, respectively. Each component plays a crucial role in ensuring that the functions work seamlessly across various stages of user interaction."}, {"title": "Language Selection", "content": "Upon entering the AR environment, users are greeted with a welcoming voice message, initiating a user-friendly introduction to the system. Following this, they are prompted to select their preferred language from a menu. This critical step ensures that all subsequent interactions between the user and the system are conducted in the chosen language, thereby tailoring the experience to each individual's linguistic preference. The system covers around 100 languages, supported by Meta's SeamlessM4T technology, ensuring inclusivity and accessibility for a diverse user base. A short list of these languages is presented in the fig. 4.3 for reference."}, {"title": "Command Initiation", "content": "The AR application utilizes the capabilities of a Microsoft HoloLens 2 device for command voice, setting a high standard for responsiveness and interaction. To start recording, the application requires the utterance of the wake word, \u201cMATRIX.\u201d Upon this command, the application begins recording, and it continues until the user says \u201cSTOP.\u201d This method allows the user to control when their input is being processed, enhancing privacy and user control over the interaction."}, {"title": "Translation and Transcription", "content": "Using Matrix framework, an application can listen and detect user speech and support interactive communication with the user, responding to their input language and selected objects. For non-English speech, we have integrated the open-source SeamlessM4T model to translate the speech into English. This is crucial for ensuring that applications can understand and process inputs from a wide range of languages efficiently.\nFor English inputs, the audio is transcribed directly using the ASR capabilities of SeamlessM4T, which are specifically fine-tuned for high accuracy in English speech recognition. For English and several other common languages, the medium-sized model performs well. However, for other languages, we used the large SeamlessM4T model, although it increased processing time, the improved outputs justified the extra time spent.\nOnce the text is processed and necessary actions are identified, if the user's selected language is not English, the output text from the LLM is translated back into the user's language using SeamlessM4T. This ensures that the AR system's responses are accessible and understandable, regardless of the user's language. For generating spoken responses in English, we utilize the open-source TTS technology from Coqui.ai. This technology converts the English text outputs from the LLM into natural-sounding spoken words, enhancing the interaction quality and making the communication feel more engaging and natural."}, {"title": "Object Extraction", "content": "For extracting meaningful content from the transcribed or translated text, we employ the Llama2 7B LLM, which identifies objects and their attributes within the user's commands. To this end, we use an optimized version of Llama, llama.cpp, designed to run with minimal hardware. This enables Matrix framework to be used on a wide variety of hardware locally, ensuring broader accessibility and flexibility. Additionally, for the extraction of objects and their attributes, we utilized the langchain library, which provides the capability of prompt template for prompt engineering and optimization."}, {"title": "Object Recommendation", "content": "Once the objects are identified and modeled from the transcribed or translated texts, the next step is to enhance user interaction by suggesting related objects. This is achieved through a sophisticated use of the Llama2 LLM, which processes the extracted object data to generate recommendations. The LLM considers the context, properties, and functions of the identified objects to suggest objects that could logically coexist or complement the primary objects in a given environment. This process not only enriches the user experience by providing creative and contextually appropriate options but also aids in developing a more interactive and immersive AR environment."}, {"title": "Search Repository", "content": "Simultaneously with the Object Recommendation phase, the application conducts a search within a pre-established 3D object"}]}