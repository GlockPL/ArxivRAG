{"title": "PWM: Policy Learning with Large World Models", "authors": ["Ignat Georgiev", "Nicklas Hansen", "Varun Giridhar", "Animesh Garg"], "abstract": "Reinforcement Learning (RL) has achieved impressive results on complex tasks but struggles in multi-task settings with different embodiments. World models offer scalability by learning a simulation of the environment, yet they often rely on inefficient gradient-free optimization methods. We introduce Policy learning with large World Models (PWM), a novel model-based RL algorithm that learns continuous control policies from large multi-task world models. By pre-training the world model on offline data and using it for first-order gradient policy learning, PWM effectively solves tasks with up to 152 action dimensions and outperforms methods using ground-truth dynamics. Additionally, PWM scales to an 80-task setting, achieving up to 27% higher rewards than existing baselines without the need for expensive online planning. Visualizations and code available at policy-world-model.github.io", "sections": [{"title": "1 Introduction", "content": "The pursuit of generalizability in machine learning has recently been propelled by the training of large models on substantial datasets [Brown et al., 2020, Kirillov et al., 2023, Bommasani et al., 2021]. Such advancements have notably permeated robotics, where multi-task behavior cloning techniques have shown remarkable performance [Zitkovich et al., 2023, Octo Model Team et al., 2024, Goyal et al., 2023, Bousmalis et al., 2023]. Nevertheless, these approaches predominantly hinge on near-expert data and struggle with adaptability across diverse robot morphologies due to"}, {"title": "2 Related work", "content": "RL approaches can be classified as model-based and model-free which assume and do not assume a model respectively [Arulkumaran et al., 2017]. Most common algorithms for real-world applications such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018] are model-free and fall in the category of on-policy and off-policy methods respectively [Arulkumaran et al., 2017]. Both harness an actor-critic architecture where the critic learns the value and the actor optimizes the critic directly to maximize cumulative rewards [Konda and Tsitsiklis, 1999]. Off-policy methods such as SAC typically learn a policy by taking First-order Gradients (FoG) directly from the critic. FoGs optimization typically has lower variance but is affected by discontinuities of the objective [Mohamed et al., 2020]. On-policy methods such as PPO harness zeroth-order gradients to learn policies [Sutton et al., 1999]. These gradients are not affected by discontinuities, making them effective for robotic tasks such as locomotion [Rudin et al., 2022] but also exhibit high variance, which leads to slow optimization and suboptimal solutions [Mohamed et al., 2020, Suh et al., 2022]. Differentiable simulation has been a popular framework to explore the properties of these gradient types [Suh et al., 2022, Howell et al., 2022, Xu et al., 2021]."}, {"title": "3 Background", "content": "We focus on discrete-time and infinite horizon Reinforcement Learning (RL) scenarios characterized by system states $s \\in \\mathbb{R}^n = \\mathcal{S}$, actions $a \\in \\mathbb{R}^m = \\mathcal{A}$, dynamics function $f : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ and a reward function $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$. Combined, these form a Markov Decision Problem (MDP) summarized by the tuple $(\\mathcal{S}, \\mathcal{A}, f, r, \\gamma)$ where $\\gamma$ is the discount factor. Actions at each timestep $t$ are sampled from a stochastic policy $a_t \\sim \\pi_\\theta(\\cdot|s_t)$, parameterized by $\\theta$. The goal of the policy is to maximize the cumulative discounted rewards:\n$$\\max_\\theta J(\\theta) := \\max_\\theta \\mathbb{E}_{s_1\\sim p(\\cdot)\\\\atop a_t \\sim \\pi_\\theta(s_t)} \\Big[\\sum_{t=1}^\\infty \\gamma^t r(s_t, a_t)\\Big]$$\nwhere $p(s_1)$ is the initial state distribution. Since this maximization over an infinite sum is intractable, in practice we often maximize over a value estimate. The value of a state $s_t$ is defined as the expected reward follow the policy $\\pi_\\theta$\n$$V_\\psi(s_t) := \\mathbb{E}_{a_h\\sim \\pi_\\theta(\\cdot|s_h)} \\Big[\\sum_{h=t}^\\infty \\gamma^{h-t} r(s_h, a_h)\\Big]$$\nWhen $V$ is approximated with a learned model with parameters $\\psi$ and $\\pi_\\theta$ attempts to maximize some function of $V$, we arrive at the popular and successful actor-critic architecture [Konda and Tsitsiklis, 1999]. Additionally, in MBRL it is common to also learn approximations of $f$ and $r$, which we denote as $F_\\phi$ and $R_\\phi$, respectively. It has also been shown to be beneficial to encode the true state $s$ into a latent state $z$ using a learned encoder $E_\\phi$ [Hafner et al., 2019, Hansen et al., 2022, 2024, Hafner et al., 2023]. Putting together all of these components we can define a model-based actor-critic algorithm to consist of the tuple $(\\pi_\\theta, V_\\psi, E_\\phi, F_\\phi, R_\\phi)$ which can describe popular approaches such as Dreamer [Hafner et al., 2019] and TD-MPC2 [Hansen et al., 2024]. Notably, we make an important distinction between the types of components. We refer to $E, F$ and $R$ as the world model components since they are a supervised learning problem with fixed targets. On the other hand, $\\pi_\\theta$ and $V_\\psi$ optimize for moving targets which is fundamentally more challenging and we refer to them as the policy components."}, {"title": "4 Policy optimization through learned world models", "content": "This paper builds on the insight that since access to $F_\\phi$ and $R_\\phi$ is assumed through a pre-trained world-model, we have the option to optimize Eq. 1 via First-order Gradient (FoG) optimization which exhibit lower gradient variance, more optimal solutions and improved sample efficiency [Mohamed et al., 2020]. In our setting, these types of gradients are obtained by directly differentiating the expected terms of Eq. 1 as shown in Eq. 3. Note that this gradient estimator is also known as\n$$\\nabla^{[1]}_\\theta J(\\theta) := \\mathbb{E}_{s_1\\sim p(\\cdot)\\\\atop a_h\\sim \\pi_\\theta(s_h)} \\Big[\\nabla_\\theta \\sum_{t=1}^H r(s_t, a_t)\\Big]$$\nAs $\\nabla^{[1]}_\\theta J(\\theta)$ in itself is a random variable, we need to estimate it. A popular way to do that in practice is via Monte-Carlo approximation where we are interested in two properties - bias and variance. In Sections 4.1 and 4.2 we tackle each aspect with toy robotic control problem to build intuition. In Section 4.3 we combine our findings to propose a new algorithm."}, {"title": "4.1 Learning through contact", "content": "FoGs are unbiased $\\mathbb{E}[\\nabla^{[1]}_\\theta J(\\theta)] := \\mathbb{E} [\\nabla_\\theta \\sum_{t=1}^H J(\\theta)] = \\nabla J(\\theta)$, only if both the dynamics $f$ and rewards $r$ are Lipschitz-smooth [Suh et al., 2022]. However, many robotic problems involving contact are inherently non-smooth, which breaks these conditions and results in gradient sample error where $\\mathbb{E} [\\nabla^{[1]}_\\theta J(\\theta)] \\neq \\nabla J(\\theta)$ under finite number of samples $N$. Instead of directly optimizing the true, discontinuous objective, it is advantageous to optimize a smooth surrogate, such as a model learned by a regularized deep neural network.\nTo illustrate this concept, we use a toy problem where a ball is thrown toward a wall at a fixed velocity as shown in Figure 2a. The objective is to find the optimal initial angle $\\theta$ such that we maximize forward distance. In this simplified pedagogical example, we assume that the ball \"sticks\" to the wall, creating a discontinuous optimization landscape (Figure 2b). We compare the performance of two models in approximating this objective: a 2-layer Multi-Layer Perceptron (MLP) with ReLU activation and another MLP with SimNorm activation [Hansen et al., 2024] in the intermediate layers. SimNorm normalizes a latent vector $z$ by projecting it into simplices with dimension $V$ using a softmax operator. Given an input vector $z$, SimNorm can be expressed as a mapping into $L$ vectors:\n$$\\text{SimNorm}(z) := [g_1, \\dots, g_L], g_i = \\text{Softmax}(z_{i:i+v})$$\nWe train the MLPs and observe the smoothing effects of the learned models in Figure 2b. While the MLP smooths the problem landscape, it also introduces a local minimum when attempting to optimize with gradient descent starting from (e.g.) $\\theta = -\\pi$, leading to a large optimality gap. In contrast, the SimNorm MLP has additional regularization which reduces the optimality hap, at the expense of model accuracy (Table 2c). This inverse corelation between optimality gap and model error is known as objective mismatch [Lambert et al., 2020]. Therefore, we believe that regularized learned models can reduce gradient sample error, and thus the optimality gap, enabling more efficient FoG optimization in non-smooth environments. Further details in Appendix A."}, {"title": "4.2 Learning with chaotic dynamics", "content": "While first-order gradient estimators (FoGs) have lower variance per step, they can accumulate significant variance over long-horizon rollouts [Metz et al., 2021]. [Suh et al., 2022] link this variance to the smoothness of models and the length of the prediction horizon: $\\text{Var} [\\nabla J^{[1]}] \\propto ||\\nabla f(s, a)||^2 H$. At sufficiently high $H$, the high variance renders FoGs ineffective in chaotic systems. Chaotic systems are characterized by their sensitivity to initial conditions, where small perturbations can lead to exponentially divergent trajectories, making long-term prediction particularly challenging. The double pendulum, also known as the Acrobot [Murray and Hauser, 1991], is a classic example of such a system (Figure 3).\nWe analyze the variance of gradient estimators in the double pendulum using both the true dynamics and a SimNorm-activated MLP model. The MLP model was trained for auto-regressive prediction horizons of $H = 3$ and $H = 16$ until convergence on a large dataset. Figure 3 shows that both learned models exhibit reduced variance compared to the true dynamics. However, as noted by [Parmas et al., 2023], variance alone is insufficient for drawing definitive conclusions about gradient quality. Instead, they propose analyzing gradients via their Expected Signal-to-Noise Ratio (ESNR), defined as:\n$$\\text{ESNR}(\\nabla J(\\theta)) = \\mathbb{E} \\Big[\\frac{\\Sigma(\\nabla^{[1]} J(\\theta))^2}{\\text{Var} [\\nabla^{[1]} J(\\theta)]} \\Big]$$\nIn Figure 3, we observe that learned models exhibit higher ESNR than the true dynamics, providing more useful gradients. Notably, the training horizon plays a critical role, with the $H = 16$ model sustaining a higher ESNR over higher $H$. We conclude that learned world models offer more informative policy gradients than the true system dynamics. Further details in Appendix B."}, {"title": "4.3 PWM: an efficient policy learning method", "content": "Given the results from the previous subsection, we propose to view world models not as components of RL methods but instead as scalable differentiable physics simulators which provide gradients with low sample error and variance. It is worth noting that approaches such as TD-MPC2 [Hansen et al., 2024] do not exploit these properties but rather choose to optimize policies via DDPG-style gradients: $\\nabla_\\theta J(\\theta) \\approx \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [\\nabla_a Q(s, a)]$.\nWe propose a new method and framework for learning policies from large multi-task world models.\nFramework. Assuming availability of data from multiple tasks, we first train a multi-task world model to predict future states and rewards. Then for each task we want to solve, we learn a single policy in minutes using FoG optimization. The policy is then deployed to solve the task and optionally finetune its world model and policy. We dub this PWM: Policy optimization through World Models.\nMethod. For policy learning, we propose on-policy actor-critic approach where the actor is trained via FoG back-propagated through the world model, while the critic is trained via TD($\\lambda$). The key to our approach is that training is done in a batched fashion where multiple trajectories are imagined in parallel. The actor loss function is akin to 1 but features rewards over a fixed horizon $H$, terminal"}, {"title": "5 Experimental results", "content": "We first assess our proposed method on complex continuous control tasks with up to $A = \\mathbb{R}^{152}$ using the differentiable simulator dflex [Xu et al., 2021]. Hopper, Ant, Anymal, Humanoid and muscle-actuated (SNU) Humanoid (Figure 4) are tasked to maximize forward velocity 1. We compare against SHAC [Xu et al., 2021], a method similar to ours but which uses the ground truth model from the simulation. This allows us to understand whether world models induce better landscapes for policy learning. Furthermore, we compare against TD-MPC2 which uses the same world model but learns a policy in a model-free fashion and actively plans at inference time. This comparison allows us to understand whether first-order gradients can learn better policies. We additionally include prominent model-free baselines PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]."}, {"title": "5.2 Multi-task world-model", "content": "We analyze the scalability of our proposed framework and method to large multi-task pre-trained world models. We evaluate on two settings: (1) 30 continuous control dm_control tasks [Tunyasuvunakool et al., 2020] ranging from $m = 1$ to $m = 6$ and (2) 80 tasks which include 50 additional manipulation tasks from MetaWorld [Yu et al., 2020] with $n = 39$ and $m = 4$. These two multi-task settings were introduced as MT30 and MT80 by [Hansen et al., 2024]. In conducting our experiments, we harness the same data and world model architecture as TD-MPC2. The data consists of 120k and 40k trajectories per dm_control and MetaWorld task, respectively generated by 3 random seeds of TD-MPC2 runs. The world models we use are the 48M parameter models introduced in [Hansen et al., 2024].\nTo train PWM, we first pre-train the world models on the dataset in a similar fashion to TD-MPC2 but with training $H = 16$ and $\\gamma = 0.99$ for better first-order gradients as highlighted in Section 4.2. Then we train a PWM policy on each particular task using the offline datasets for 10k gradient steps which take 9.3 minutes on an Nvidia RTX6000 GPU. We evaluate task performance for 10 seeds for each task and aggregate results in Figure 6. We compare against TD-MPC2 which learns a multi-task"}, {"title": "5.3 Ablations", "content": "We perform 4 ablations on the complex single task experiments in order to understand the nuances of first-order optimization through world models with PWM.\nWe increase the contact stiffness to be more realistic but also more stiff contact gives gradients with high sample error [Suh et al., 2022]. We run the same experiment as Section 5.1, but only"}, {"title": "6 Conclusion", "content": "In this work, we introduced Policy learning with large World Models (PWM), a novel MBRL approach that utilizes large multi-task world models as differentiable physics simulators for efficient policy training using First-order Gradients (FoG). Our evaluations demonstrated that PWM can learn policies with higher rewards than existing methods, even if they have access to simulation ground-truth models. Furthermore, the PWM framework paves a pathway to scalibly learn high-performing policies from large multi-task world models, achieving higher rewards than our main baseline TD-MPC2 without the need for online planning."}, {"title": "Limitations", "content": "Despite its demonstrated efficacy, PWM has notable limitations. Firstly, performance relies heavily on the availability of substantial pre-existing data to train the world model, which might not always be feasible, especially in novel or low-data environments. Secondly, although PWM facilitates fast and cost-effective policy training, it necessitates re-training for each new task, which could limit its applicability in scenarios requiring rapid adaptation to diverse tasks. Lastly, the current TD-MPC2 world models used are difficult to train at scale due to their autoregressive formulation.\nIn summary, PWM pushes the boundaries of multi-task policy learning from world models. Future research directions can explore learning from images, more efficient world model architectures for training and real-world applications."}, {"title": "C Implementation details and hyper-parameters", "content": "The section details several implementation details of PWM that we thought are not crucial for understanding the proposed approach in Section 4.3 but are important for replicating the results.\n1. Reward binning - the reward model we use in PWM is formulated as a discrete regression problem where R rewards are discretized into a predefined number of bins. Similar to [Hansen et al., 2024, Lee et al., 2023], we do this to enable robustness to reward scale and multi-task-ness. In particular, we perform two-hot encoding using SymLog and SymExp operators which are mathematically defined as:\n$$\\text{SymLog}(x) = \\text{sign}(x) \\log(1+ |x|)\n\\text{SymExp}(x) = \\text{sign}(x)(e^{|x|} - 1)$$\nTwo-hot encoding is then performed with:\ndef two_hot(x):\nX =\nclamp(symlog(x), vmin, vmax)\nbin_idx = floor((x - vmin) / bin_size)\nbin_offset = (x - vmin) / bin_size - bin_idx\nsoft_two_hot = zeros(x.size (0), num_bins)\nsoft_two_hot [bin_idx] = 1 - bin_offset\nsoft_two_hot [bin_odx + 1] = bin_offset\nreturn soft_two_hot\nInverting this operation to get back to scalar rewards would usually involve SymExp(x) but note that the sign(x) operator is not differentiable and would therefore not work for FoG. Instead, we chose to omit the SymExp(x) operation which technically now returns pseudo-rewards but also gradients which we found sufficient for policy learning:\ndef two_hot_inversion(x):\nvals = linspace (vmin, vmax, num_bins)\nX =\nsoftmax(x)\nX =\ntorch.sum (x * vals, dim=-1)\nreturn x\n2. Critic training - while Algorithm 1 function to similar results as presented in 5, we found it beneficial to split the critic training data from a single rollout into several smaller mini-batches and over them for multiple gradient steps. In our implementation we split the data into 4 mini-batches and perform 8 gradient steps over them with uniform sampling. With a $H = 16$ and batch size 64, this translates to a critic batch size of 256.\n3. Minimum policy noise - Due to the larger amount of gradient steps needed, we noticed that PWM's actor tends to collapse to a deterministic policy rapidly. As such, we found it beneficial to include a lower bound on the standard deviation of the action distribution in order to maintain stochasticity in the optimization process. We have used 0.24 throughout this paper. While similar results would be possible by adding an entropy term [Schulman et al., 2017], we found our current solution sufficient\n4. World model fine-tuning - Throughout all of our experiments we found that the offline data used to train PWM's world model to be crucial to learning a good policy. In very high-dimensional tasks such as Humanoid SNU, collecting extensive data is a difficult task. As such, in these tasks we found it beneficial to online fine-tune the world model. We do this on all single-task experiments of Section 5.1 using the default hyper-parameters and a replay buffer of size 1024."}, {"title": "D Contact-rich single task experiment details", "content": "In Section 5.1, we explore 5 locomotion tasks with increasing complexity. They are described below and shown in Figure 4.\n1. Hopper, a single-legged robot jumping only in one axis with $n = 11$ and $m = 3$.\n2. Ant, a four-legged robot with $n = 37$ and $m = 8$."}, {"title": "E Multi-task experiments additional results", "content": "In this section we provide additional results on multi-task experiments. While we find it beneficial to train the world model at the same horizon as the policy learning, it is not strictly necessary to achieve good performance. In Figure 14 we present an ablation where we compare PWM world models pre-trained on horizons $H = 3$ and $H = 16$ and policies trained only with $H = 16$. These results reveal that $H = 16$ trained world models have only marginally higher scores. On deeper inspection, most of increased scores come form dm_control tasks which are harder than MetaWorld tasks on average. Therefore if training new world models, we advise using higher $H$; however if other pre-trained world models exist with suboptimal $H$, they will probably be also useful."}]}