{"title": "PWM: Policy Learning with Large World Models", "authors": ["Ignat Georgiev", "Nicklas Hansen", "Varun Giridhar", "Animesh Garg"], "abstract": "Reinforcement Learning (RL) has achieved impressive results on complex tasks but struggles in multi-task settings with different embodiments. World models offer scalability by learning a simulation of the environment, yet they often rely on inefficient gradient-free optimization methods. We introduce Policy learning with large World Models (PWM), a novel model-based RL algorithm that learns continuous control policies from large multi-task world models. By pre-training the world model on offline data and using it for first-order gradient policy learning, PWM effectively solves tasks with up to 152 action dimensions and outperforms methods using ground-truth dynamics. Additionally, PWM scales to an 80-task setting, achieving up to 27% higher rewards than existing baselines without the need for expensive online planning.", "sections": [{"title": "1 Introduction", "content": "The pursuit of generalizability in machine learning has recently been propelled by the training of large models on substantial datasets [Brown et al., 2020, Kirillov et al., 2023, Bommasani et al., 2021]. Such advancements have notably permeated robotics, where multi-task behavior cloning techniques have shown remarkable performance [Zitkovich et al., 2023, Octo Model Team et al., 2024, Goyal et al., 2023, Bousmalis et al., 2023]. Nevertheless, these approaches predominantly hinge on near-expert data and struggle with adaptability across diverse robot morphologies due to"}, {"title": "2 Related work", "content": "RL approaches can be classified as model-based and model-free which assume and do not assume a model respectively [Arulkumaran et al., 2017]. Most common algorithms for real-world applications such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018] are model-free and fall in the category of on-policy and off-policy methods respectively [Arulkumaran et al., 2017]. Both harness an actor-critic architecture where the critic learns the value and the actor optimizes the critic directly to maximize cumulative rewards [Konda and Tsitsiklis, 1999]. Off-policy methods such as SAC typically learn a policy by taking First-order Gradients (FoG) directly from the critic. FoGs optimization typically has lower variance but is affected by discontinuities of the objective [Mohamed et al., 2020]. On-policy methods such as PPO harness zeroth-order gradients to learn policies [Sutton et al., 1999]. These gradients are not affected by discontinuities, making them effective for robotic tasks such as locomotion [Rudin et al., 2022] but also exhibit high variance, which leads to slow optimization and suboptimal solutions [Mohamed et al., 2020, Suh et al., 2022]. Differentiable simulation has been a popular framework to explore the properties of these gradient types [Suh et al., 2022, Howell et al., 2022, Xu et al., 2021]."}, {"title": "3 Background", "content": "We focus on discrete-time and infinite horizon Reinforcement Learning (RL) scenarios characterized by system states $s \\in \\mathbb{R}^{n} = \\mathcal{S}$, actions $a \\in \\mathbb{R}^{m} = \\mathcal{A}$, dynamics function $f : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ and a reward function $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$. Combined, these form a Markov Decision Problem (MDP) summarized by the tuple $(\\mathcal{S}, \\mathcal{A}, f, r, \\gamma)$ where $\\gamma$ is the discount factor. Actions at each timestep $t$ are sampled from a stochastic policy $a_t \\sim \\pi_\\theta(\\cdot|s_t)$, parameterized by $\\theta$. The goal of the policy is to maximize the cumulative discounted rewards:\n\n$\\max_\\theta J(\\theta) := \\max_\\theta\\mathbb{E}_{s_1\\sim p(\\cdot) \\atop a_t\\sim\\pi_\\theta(\\cdot|s_t)} [\\sum_{t=1}^\\infty \\gamma^{t-1}r(s_t, a_t)] $  (1)\n\nwhere $p(s_1)$ is the initial state distribution. Since this maximization over an infinite sum is intractable, in practice we often maximize over a value estimate. The value of a state $s_t$ is defined as the expected reward follow the policy $\\pi_\\theta$\n\n$V_\\psi(s_t) := \\mathbb{E}_{a_h\\sim \\pi_\\theta(\\cdot|s_h)} [\\sum_{h=t}^\\infty \\gamma^{h-t} r(s_h, a_h)]$  (2)\n\nWhen $V$ is approximated with a learned model with parameters $\\psi$ and $\\pi_\\theta$ attempts to maximize some function of $V$, we arrive at the popular and successful actor-critic architecture [Konda and Tsitsiklis, 1999]. Additionally, in MBRL it is common to also learn approximations of $f$ and $r$, which we denote as $\\mathcal{F}_\\phi$ and $\\mathcal{R}_\\phi$, respectively. It has also been shown to be beneficial to encode the true state $s$ into a latent state $z$ using a learned encoder $\\mathcal{E}_\\varphi$ [Hafner et al., 2019, Hansen et al., 2022, 2024, Hafner et al., 2023]. Putting together all of these components we can define a model-based actor-critic algorithm to consist of the tuple $(\\pi_\\theta, V_\\psi, \\mathcal{E}_\\varphi, \\mathcal{F}_\\phi, \\mathcal{R}_\\phi)$ which can describe popular approaches such as Dreamer [Hafner et al., 2019] and TD-MPC2 [Hansen et al., 2024]. Notably, we make an important distinction between the types of components. We refer to $\\mathcal{E}_\\varphi$, $\\mathcal{F}_\\phi$ and $\\mathcal{R}_\\phi$ as the world model components since they are a supervised learning problem with fixed targets. On the other hand, $\\pi_\\theta$ and $V_\\psi$ optimize for moving targets which is fundamentally more challenging and we refer to them as the policy components."}, {"title": "4 Policy optimization through learned world models", "content": "This paper builds on the insight that since access to $\\mathcal{F}_\\phi$ and $\\mathcal{R}_\\phi$ is assumed through a pre-trained world-model, we have the option to optimize Eq. 1 via First-order Gradient (FoG) optimization which exhibit lower gradient variance, more optimal solutions and improved sample efficiency [Mohamed et al., 2020]. In our setting, these types of gradients are obtained by directly differentiating the expected terms of Eq. 1 as shown in Eq. 3. Note that this gradient estimator is also known as\n\n$\\nabla^{[1]}_\\theta J(\\theta) := \\mathbb{E}_{s_1\\sim p(\\cdot) \\atop a_h\\sim\\pi_\\theta(\\cdot|s_h)} [\\nabla_\\theta \\sum_{t=1}^H r(s_t, a_t)]$ (3)\n\nAs $\\nabla^{[1]}_\\theta J(\\theta)$ in itself is a random variable, we need to estimate it. A popular way to do that in practice is via Monte-Carlo approximation where we are interested in two properties - bias and variance. In Sections 4.1 and 4.2 we tackle each aspect with toy robotic control problem to build intuition. In Section 4.3 we combine our findings to propose a new algorithm."}, {"title": "4.1 Learning through contact", "content": "FoGs are unbiased $\\mathbb{E}[\\nabla^{[1]}_\\theta J(\\theta)] := \\mathbb{E}[\\frac{d}{d\\theta} J(\\theta)] = \\nabla J(\\theta)$, only if both the dynamics $f$ and rewards $r$ are Lipschitz-smooth [Suh et al., 2022]. However, many robotic problems involving contact are inherently non-smooth, which breaks these conditions and results in gradient sample error where $\\mathbb{E}[\\nabla^{[1]}_\\theta J(\\theta)] \\neq \\nabla J(\\theta)$ under finite number of samples $N$. Instead of directly optimizing the true, discontinuous objective, it is advantageous to optimize a smooth surrogate, such as a model learned by a regularized deep neural network.\n\nTo illustrate this concept, we use a toy problem where a ball is thrown toward a wall at a fixed velocity as shown in Figure 2a. The objective is to find the optimal initial angle $\\theta$ such that we maximize forward distance. In this simplified pedagogical example, we assume that the ball \"sticks\" to the wall, creating a discontinuous optimization landscape (Figure 2b). We compare the performance of two models in approximating this objective: a 2-layer Multi-Layer Perceptron (MLP) with ReLU activation and another MLP with SimNorm activation [Hansen et al., 2024] in the intermediate layers. SimNorm normalizes a latent vector $z$ by projecting it into simplices with dimension $V$ using a softmax operator. Given an input vector $z$, SimNorm can be expressed as a mapping into $L$ vectors:\n\n$\\text{SimNorm}(z) := [g_1, ..., g_L], g_i = \\text{Softmax}(z_{i:i+v})$ (4)\n\nWe train the MLPs and observe the smoothing effects of the learned models in Figure 2b. While the MLP smooths the problem landscape, it also introduces a local minimum when attempting to optimize with gradient descent starting from (e.g.) $\\theta = -\\pi$, leading to a large optimality gap. In contrast, the SimNorm MLP has additional regularization which reduces the optimality hap, at the expense of model accuracy (Table 2c). This inverse corelation between optimality gap and model error is known as objective mismatch [Lambert et al., 2020]. Therefore, we believe that regularized learned models can reduce gradient sample error, and thus the optimality gap, enabling more efficient FoG optimization in non-smooth environments."}, {"title": "4.2 Learning with chaotic dynamics", "content": "While first-order gradient estimators (FoGs) have lower variance per step, they can accumulate significant variance over long-horizon rollouts [Metz et al., 2021]. [Suh et al., 2022] link this variance to the smoothness of models and the length of the prediction horizon: $\\text{Var} [\\nabla J^{[1]}] \\propto ||\\nabla f(s, a)||^{2H}$. At sufficiently high $H$, the high variance renders FoGs ineffective in chaotic systems. Chaotic systems are characterized by their sensitivity to initial conditions, where small perturbations can lead to exponentially divergent trajectories, making long-term prediction particularly challenging. The double pendulum, also known as the Acrobot [Murray and Hauser, 1991], is a classic example of such a system (Figure 3).\n\nWe analyze the variance of gradient estimators in the double pendulum using both the true dynamics and a SimNorm-activated MLP model. The MLP model was trained for auto-regressive prediction horizons of $H = 3$ and $H = 16$ until convergence on a large dataset. Figure 3 shows that both learned models exhibit reduced variance compared to the true dynamics. However, as noted by [Parmas et al., 2023], variance alone is insufficient for drawing definitive conclusions about gradient quality. Instead, they propose analyzing gradients via their Expected Signal-to-Noise Ratio (ESNR), defined as:\n\n$\\text{ESNR}(\\nabla J(\\theta)) = \\mathbb{E} [(\\nabla J(\\theta))^2] / \\text{Var} [\\nabla J(\\theta)]$ (5)\n\nIn Figure 3, we observe that learned models exhibit higher ESNR than the true dynamics, providing more useful gradients. Notably, the training horizon plays a critical role, with the $H = 16$ model sustaining a higher ESNR over higher $H$. We conclude that learned world models offer more informative policy gradients than the true system dynamics."}, {"title": "4.3 PWM: an efficient policy learning method", "content": "Given the results from the previous subsection, we propose to view world models not as components of RL methods but instead as scalable differentiable physics simulators which provide gradients with low sample error and variance. It is worth noting that approaches such as TD-MPC2 [Hansen et al., 2024] do not exploit these properties but rather choose to optimize policies via DDPG-style gradients: $\\nabla_\\theta J(\\theta) \\approx \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [\\nabla_a Q(s, a)]$.\n\nWe propose a new method and framework for learning policies from large multi-task world models. Framework. Assuming availability of data from multiple tasks, we first train a multi-task world model to predict future states and rewards. Then for each task we want to solve, we learn a single policy in minutes using FoG optimization. The policy is then deployed to solve the task and optionally finetune its world model and policy. We dub this PWM: Policy optimization through World Models. Method. For policy learning, we propose on-policy actor-critic approach where the actor is trained via FoG back-propagated through the world model, while the critic is trained via TD($\\lambda$). The key to our approach is that training is done in a batched fashion where multiple trajectories are imagined in parallel. The actor loss function is akin to 1 but features rewards over a fixed horizon H, terminal"}, {"title": "5 Experimental results", "content": "We first assess our proposed method on complex continuous control tasks with up to $A = \\mathbb{R}^{152}$ using the differentiable simulator dflex [Xu et al., 2021]. Hopper, Ant, Anymal, Humanoid and muscle-actuated (SNU) Humanoid (Figure 4) are tasked to maximize forward velocity 1. We compare against SHAC [Xu et al., 2021], a method similar to ours but which uses the ground truth model from the simulation. This allows us to understand whether world models induce better landscapes for policy learning. Furthermore, we compare against TD-MPC2 which uses the same world model but learns a policy in a model-free fashion and actively plans at inference time. This comparison allows us to understand whether first-order gradients can learn better policies. We additionally include prominent model-free baselines PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]."}, {"title": "5.1 Contact-rich single tasks", "content": "While these tasks are inspired by dm_control [Tunyasuvunakool et al., 2020], they are also fundamentally more challenging as they have to maximize an unbounded forward velocity, not achieve a velocity target."}, {"title": "6 Conclusion", "content": "In this work, we introduced Policy learning with large World Models (PWM), a novel MBRL approach that utilizes large multi-task world models as differentiable physics simulators for efficient policy training using First-order Gradients (FoG). Our evaluations demonstrated that PWM can learn policies with higher rewards than existing methods, even if they have access to simulation ground-truth models. Furthermore, the PWM framework paves a pathway to scalibly learn high-performing policies from large multi-task world models, achieving higher rewards than our main baseline TD-MPC2 without the need for online planning."}, {"title": "Limitations", "content": "Despite its demonstrated efficacy, PWM has notable limitations. Firstly, performance relies heavily on the availability of substantial pre-existing data to train the world model, which might not always be feasible, especially in novel or low-data environments. Secondly, although PWM facilitates fast and cost-effective policy training, it necessitates re-training for each new task, which could limit its applicability in scenarios requiring rapid adaptation to diverse tasks. Lastly, the current TD-MPC2 world models used are difficult to train at scale due to their autoregressive formulation.\n\nIn summary, PWM pushes the boundaries of multi-task policy learning from world models. Future research directions can explore learning from images, more efficient world model architectures for training and real-world applications."}, {"title": "A Ball-wall example details", "content": "This section provides more details on the ball-wall example used to showcase the issues of optimizing through contact in Section 4.1. In constructing this toy example we chose a simple physical system that exhibits contact discontinuities. Inspired by Suh et al. [Suh et al., 2022], we constructed a simple problem of a point mass (ball) being thrown forward (x direction) at a fixed velocity $v$. The optimization parameter of interest is the initial angle $\\theta$ and the goal is to maximize forward distance traveled (in 2D). For simplicity we assume that the ball sticks to the wall (without complex contact) which can be expressed as:\n\n$x = f(\\theta) = \\begin{cases}\nx_o + v \\cos(\\theta) t + \\frac{1}{2}gt^2 & \\text{if } Y_{contact} > h \\\\\nw & \\text{else}\n\\end{cases}$ (11)\n\nwhere $g = 9.81$ is gravity, $h$ and $w$ are the height and width of the wall, $(x_o, y_o)$ is the starting position, $v = 10$ is the starting velocity and $t = 2$ is time. $Y_{contact}$ is the height at the time of contact $t_{contact}$ which are both given by solving Eq. 11 for $f(\\theta) = w$:\n\n$t_{contact} = \\frac{-v \\cos(\\theta) + \\sqrt{v^2 \\cos^2(\\theta) + aw}}{a}$ \n\n$Y_{contact} = y_o + v \\sin(\\theta)t + \\frac{1}{2}gt^2$"}, {"title": "B Double pendulum example details", "content": "The double pendulum (also known as Acrobot [Murray and Hauser, 1991]) is a classic under-actuated chaotic system. It is characterized by its sensitivity to initial conditions where even small perturbations result in large gradient variance with long horizon (> 20) trajectories. We chose this system to analyze variance and expected signal-to-noise ratio (ENSR) in Section 4.2 as it is the easiest problem exhibiting chaosness. We model this toy problem similar to DMControl [Tunyasuvunakool et al., 2020] in our differentiable simulator, dflex as we need ground truth gradients for comparison. The first link with angle $\\theta_1$ is fixed to the base and not actuated. The second link with angle $\\theta_2$ is the only control input via $\\dot{\\theta_2}$. The state of the system id calculated as:\n\n$s = [\\cos(\\theta_1), \\sin(\\theta_1), \\cos(\\theta_2), \\sin(\\theta_2), \\dot{\\theta_1}, \\dot{\\theta_2}]$\n\nThe objective of this toy example is to bring and balance the pendulum upwards which we achieve by formulating a reward:\n\n$r(s, a) = -\\theta_1^2 - \\theta_2^2 - 0.1\\dot{\\theta_2}^2$"}, {"title": "C Implementation details and hyper-parameters", "content": "The section details several implementation details of PWM that we thought are not crucial for understanding the proposed approach in Section 4.3 but are important for replicating the results.\n\nReward binning - the reward model we use in PWM is formulated as a discrete regression problem where R rewards are discretized into a predefined number of bins. Similar to [Hansen et al., 2024, Lee et al., 2023], we do this to enable robustness to reward scale and multi-task-ness. In particular, we perform two-hot encoding using SymLog and SymExp operators which are mathematically defined as:\n\n$\\text{SymLog}(x) = \\text{sign}(x) \\log(1+ |x|) \\qquad \\text{SymExp}(x) = \\text{sign}(x)(e^{|x|} - 1)$\n\nTwo-hot encoding is then performed with:"}, {"title": "D Contact-rich single task experiment details", "content": "In Section 5.1, we explore 5 locomotion tasks with increasing complexity. They are described below and shown in Figure 4.\n\nHopper, a single-legged robot jumping only in one axis with n 11 and m 3.\nAnt, a four-legged robot with n = 37 and m 8."}, {"title": "E Multi-task experiments additional results", "content": "In this section we provide additional results on multi-task experiments. While we find it beneficial to train the world model at the same horizon as the policy learning, it is not strictly necessary to achieve good performance. In Figure 14 we present an ablation where we compare PWM world models pre-trained on horizons $H = 3$ and $H = 16$ and policies trained only with $H = 16$. These results reveal that $H = 16$ trained world models have only marginally higher scores. On deeper inspection, most of increased scores come form dm_control tasks which are harder than MetaWorld tasks on average. Therefore if training new world models, we advise using higher $H$; however if other pre-trained world models exist with suboptimal $H$, they will probably be also useful."}]}