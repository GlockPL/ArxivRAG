{"title": "Achieving Generalization in Orchestrating GNSS Interference Monitoring Stations Through Pseudo-Labeling", "authors": ["Lucas Heublein", "Tobias Feigl", "Alexander R\u00fcgamer", "Felix Ott"], "abstract": "The accuracy of global navigation satellite system (GNSS) receivers is significantly compromised by interference from jamming devices. Consequently, the detection of these jammers are crucial to mitigating such interference signals. However, robust classification of interference using machine learning (ML) models is challenging due to the lack of labeled data in real-world environments. In this paper, we propose an ML approach that achieves high generalization in classifying interference through orchestrated monitoring stations deployed along highways. We present a semi-supervised approach coupled with an uncertainty-based voting mechanism by combining Monte Carlo and Deep Ensembles that effectively minimizes the requirement for labeled training samples to less than 5% of the dataset while improving adaptability across varying environments. Our method demonstrates strong performance when adapted from indoor environments to real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "The localization accuracy of GNSS receivers is significantly degraded by interference signals generated by jamming devices [1]. This issue has become increasingly prevalent in recent years [2] due to the widespread availability of low-cost and easily accessible jamming devices [3, 4]. As a result, it is imperative to mitigate these interference signals or eliminate the transmitter. This necessitates the detection, classification, and localization of the interference source [5, 6, 7, 8, 9]. One critical application of GNSS interference monitoring is in the management of toll collection for trucks on highways [10]. Most modern systems are electronic; however, GNSS signals can be jammed, hindering the detection of affected trucks. Therefore, it is crucial to detect the jammer within the affected vehicle. To address this, we built a sensor station equipped with a GNSS receiver and antenna on German highways to record GNSS snapshots [11] and classify these snapshots to identify the affected vehicle. Our objective is to develop resilient ML models. The construction of GNSS interference monitoring stations with robust ML models presents significant challenges due to the high variability in jamming devices, interference characteristics, sensor and antenna properties, distances between jammers and sensor stations, satellite constellations, and multipath effects from varying environments [12]. To enhance ML model accuracy, we employ federated learning to share data, such as interference snapshots, among orchestrated monitoring stations. These models can identify new interference types, share information through the aggregation of model weights, and adapt to and classify these interferences across all sensor stations."}, {"title": "2. Related Work", "content": "Crespillo et al. [1] proposed a logistic regression approach to mitigate biased estimation, thereby enhancing ML generalization. Merwe et al. [7] introduced a low-cost GNSS interference detection and classification receiver. Brieger et al. [8] considered both spatial and temporal relationships when fusing snapshot and bandwidth-limited data with a joint loss function. Raichur et al. [5] proposed a crowdsourcing approach with smartphone-based features to localize interference sources. Jdidi et al. [9] developed a quasi-unsupervised method that does not rely on prior knowledge and adapts to environment-specific factors such as multipath, dynamics, and signal strength variations. The input data undergoes preprocessing, embedding, disentanglement, and clustering using the K-means algorithm. Subsequently, a random forest classifier is trained utilizing a distance-based label generator. For adapting to new interference types, Ott et al. [10] proposed a few-shot learning approach based on uncertainty and pairwise learning, while Raichur et al. [6] contributed a Bayesian learning-based dynamic weighting of contrastive and classification loss functions for class-incremental learning. In a previous work [12], we introduced a GNSS dataset featuring a range of interference characteristics, which we utilize for training purposes. Heublein et al. [11] illustrated the discrepancy in data between indoor and outdoor datasets when evaluating feature embeddings. Wang et al. [15] proposed a domain adaptation framework that utilizes labeled GNSS data from a source domain to enhance positioning accuracy in a target domain. Park et al. [17] introduced a pseudo-labeled method for classifying target contents by incorporating noise labels. Yan et al. [18] proposed a deep metric learning-based pseudo-labeling technique to develop compact feature representations of both labeled and unlabeled data, addressing class imbalance issues. In the context of fault diagnosis, Zhu et al. [19] utilized a multiple kernel variant of maximum mean discrepancy to assess the discrepancy in marginal probability distributions and pseudo-labels, as well as to evaluate conditional probability distribution discrepancies. They addressed the challenge of pseudo-label noise interference by developing a method to filter out low-quality pseudo-labels using an adaptive threshold. Despite notable progress, challenges persist in comprehensively addressing domain discrepancies in GNSS data."}, {"title": "3. Methodology", "content": "Figure 2 presents a comprehensive overview of the proposed method. Initially, a ResNet18 [23] model, which has demonstrated high performance in GNSS classification tasks [10, 11, 12], is pre-trained on a dataset collected in an indoor environment. The objective is to adapt this model to new scenarios, specifically a highway environment, using only a limited number of labeled samples. We evaluate the model using 0.5%, 1%, and 5% of labeled samples from the original dataset. Subsequently, M ResNet18 models, denoted as $m_i$ where $i\\in \\{1,2,..., M\\}$, are trained. Experiments are conducted with $M\\in \\{1,2,4\\}$ models. The models then predict the class for the remaining unlabeled dataset using a voting mechanism, with selection criteria based on uncertainty estimates from Monte Carlo [21] and Deep Ensembles [22], as detailed in the following sections. These pseudo-labeled data are subsequently combined with the labeled dataset, and the models are retrained. Finally, the models perform predictions on the test dataset for the interference classification task.\nOur selection criteria are based on the pseudo-labeling approach proposed by Rizve et al. [20], which employs negative learning on hard samples to reduce the noise encountered by the network. The primary goal is to achieve an output distribution that is invariant to perturbations and augmentations. The following notation is used for pseudo-label selection. Let $D_l = \\{(X_i, y_i)\\}$ represent a labeled GNSS dataset with $N_l$ sample snapshots $X_i$, where $y_i = [y_i^1,...,y_i^C] \\subseteq \\{0,1\\}$ denotes the corresponding labels, with $C$ class categories. Let $D_u = \\{X_i^*\\}_{i=1}^{N_u}$ represent an unlabeled dataset with $N_u$ samples lacking labels, for which pseudo-labels $\\hat{y}_i$ are generated. We train a model $f_\\theta$ with network parameters $\\theta$ on the combined dataset $D = \\{(x_i, y_i)\\}_{i=1}^{N_l+N_u}$, with $\\hat{y} = y_i$ for the $N_l$ labeled samples [20]. Hard pseudo-labels are derived directly from network predictions. Let $p_i$ denote the probability outputs of a trained network for the sample $X_i$, where $p_i^c$ represents the probability that class $c$ is present in the sample. The pseudo-label for $X_i$ can then be generated as $\\hat{y}_i = 1[p_i^c \\geq \\gamma]$, where $\\gamma\\in (0, 1)$ is a softmax threshold [20]. The objective is to select a subset of pseudo-labels that are less noisy by choosing those associated with high-confidence predictions. The selected pseudo-labels are denoted as $g_i = [g_i^1,...,g_i^C] \\subseteq \\{0,1\\}$, where $g_i^c = 1$ if $\\hat{y}_i^c$ is selected and $g_i^c = 0$ if $\\hat{y}_i^c$ is not selected. This is obtained using the following equation:\n$g_i^c = 1[p_i^c < T_p] + 1[p_i^c \\geq T_n]$,\nwhere $T_p$ and $T_n$ are confidence thresholds for positive and negative labels, respectively [20]. For our multi-label classification task, we train the parameterized model $f_\\theta$ using a binary cross-entropy loss on the selected pseudo-labels.\nPoor model calibration can result in incorrect predictions that are assigned high confidence scores. Given the direct relationship between the expected calibration error (ECE) score and prediction uncertainties, pseudo-labels associated with predictions that meet certain criteria contribute to a reduced calibration error. Therefore, we employ the Monte Carlo (MC) method, proposed by Gal et al. [21] and previously utilized by Rizve et al. [20], to select pseudo-labels for which the model exhibits high confidence:\n$g_i^c = 1 [u(p_i) \\leq \\kappa_p] 1 [p_i^c\\geq \\tau_p] + 1 [u(p_i) \\leq \\kappa_n]1[p_i^c \\geq \\tau_n]$,\nwhere $u(p_i)$ represents the uncertainty of a prediction $p$. This approach ensures that the network's prediction is sufficiently certain to be selected [20]. We set the uncertainty thresholds as $\\kappa_p = 0.05$ and $\\kappa_n = 0.005$, and the confidence thresholds as $\\tau_p = 0.70$ and $\\tau_\\eta = 0.05$. Notably, the voting is conducted using a single model (M = 1) with C samples $k_i$, where $l \\in \\{0, . . ., C\\}$. In the following, we present a method that employs multiple models for voting.\nWhile pseudo-labeling is both versatile and modality-agnostic, it exhibits relatively poor performance compared to recent semi-supervised methods due to the large number of incorrectly pseudo-labeled samples. However, MC dropout requires the model to include a dropout layer. Since our ResNet18 model lacks an integrated dropout layer - adding which would degrade performance we instead employ Deep Ensembles [22], training M identical models without dropout but with different initial weights. Through the M models, we can estimate uncertainty, which involves computing the posterior distribution $p(\\theta|D)$ \u2013 the model weights $\\theta$ given the training dataset D. However, due to the typical intractability of calculating the posterior directly, an approximation is often used. We employ Deep Ensembles, consisting of a committee of M models, each initialized with a unique seed, where the initialization serves as the sole source of stochasticity in the model parameters. The final results are obtained by aggregating predictions from these M models. Our goal is to further reduce the noise in training by integrating the previously introduced MC sampling method with Bayesian inference. If the M models do not agree on the class label corresponding to a given sample, that sample is excluded from use as a pseudo-label in the subsequent retraining phase. Conversely, when all M models predict the same class label, the mean of the softmax outputs $E_{m_i, k_l}$ for the model $m_i$ and C Monte Carlo samples $k_l$, where $l \\in \\{0, . . ., C\\}$ of each model $m_i$ must exceed a predefined threshold. We set C = 5. The optimal threshold is determined within [0.7, 0.9, 0.99] based on M \u00d7 C samples."}, {"title": "4. Evaluation", "content": "As introduced in Figure 1, we transition from indoor environments to outdoor highway scenarios. We begin by providing details on these datasets. Subsequently, we assess the impact of uncertainty (i.e., MC dropout and Deep Ensembles) on our pseudo-labeling approach. Finally, we present an ablation study and conduct a hyperparameter search. All experiments were performed using Nvidia Tesla V100-SXM2 GPUs with 32 GB VRAM, supported by Core Xeon CPUs and 192 GB RAM. We employed the standard SGD optimizer with a learning rate of 0.01, a decay rate of 5 \u00d7 10\u22124, a momentum of 0.9, and a batch size of 64. The pre-training was conducted over 200 epochs, utilizing a multistep learning rate schedule with milestones at [120, 160] and a gamma of 0.1. For the pseudo-labeling, training was conducted over 100 epochs, utilizing the milestones at [60, 80] for the initial training and 20 epochs with milestones at [12, 16] for each further epoch. We report the final epoch accuracy in %.\nWe utilize two indoor datasets. The controlled large-scale dataset 1+2 [12], recorded with a low-frequency antenna, contains GNSS snapshots across six interference classes (see Figure 1, bottom right). Over a period of 24 days, we continuously recorded snapshots with interferences exhibiting varying bandwidth, signal-to-noise ratio, and multipath effects in different scenarios. Concurrently, we recorded the controlled large-scale dataset 2 [11] using a high-frequency antenna (see Figure 1, top right). The small-scale dataset [8] (see Figure 1, top middle) was not utilized in our experiments. We adapt to two separate highway datasets, where interference classes were recorded using handheld jammers and jammers in cigarette lighters. The highway dataset 1 [10] was recorded along the A6 in Germany using a high-frequency antenna . The highway dataset 2 [11] was recorded near Braunschweig, also with a high-frequency antenna . Table 1 provides an overview of the number of samples per interference class and highlights the class imbalance. Due to the significant overlap between class labels, we restrict our analysis to only the classes 0 (non-interference) and 1 (chirp).\nIn Figure 4, we present the evaluation results for adaptation from three different datasets to the real-world highway dataset 1 over seven repetitions, both with and without negative learning (NL) and our voting mechanism based on softmax values. Four models were evaluated using varying proportions of labeled samples: 0.5% (61 negative and 5 positive classes), 1% (121 negative and 9 positive classes), and 5% (605 negative and 41 positive classes). We also assess the effectiveness of pseudo-label selection using either MC techniques or a combination of MC and Deep Ensembles, with softmax thresholds $\\gamma \\in \\{0.7, 0.9, 0.99\\}$. The results demonstrate that without NL, using 1% of labeled data yields significantly better performance (refer to Figure 4a, left, green and red lines). However, with 5% labeled data, NL becomes advantageous due to the higher volume of labeled samples, where noise has a more pronounced impact, and NL provides a mitigating effect. Furthermore, NL positively influences performance when combined with MC dropout. Regarding the dataset, with 5% labeled data, the pre-training dataset becomes less relevant, as we consistently achieve 99.1% accuracy. When using 0.5% and 1% labeled data, models exhibit greater robustness when adapting from the highway 2 dataset to highway 1, likely due to the real-world-to-real-world adaptation. In contrast, adaptation from controlled indoor environments to real-world scenarios demands more data. Three repetitions are generally sufficient for training a well-converged model due to the rapid convergence rate, although early stopping methods may be considered in future work. As expected, increasing the quantity of labeled data significantly improves accuracy, with larger labeled datasets (i.e., 5%) leading to higher accuracy overall. However, even with just 0.5% labeled data, we achieve accuracies ranging from 98.5% to 99.0%, comparable to the results obtained with 5% labeled data. With respect to the softmax threshold, $\\gamma = 0.9$ yields the best results. A threshold of $\\gamma = 0.99$ selects too few samples, while $\\gamma = 0.7$ selects too many and includes incorrect samples. We also evaluated the impact of using Deep Ensembles and MC dropout. The results indicate that using only Deep Ensembles is either superior or comparable to the combination of Deep Ensembles with MC dropout. Therefore, we recommend using only Deep Ensembles, depending on the dataset. In summary, our method outperforms the approach by Rizve et al. [20], achieving a final accuracy of 99.2%. Notably, even with only 0.5% or 1% labeled data, we obtain significantly higher accuracies. For 0.5% labeled data, additional repetitions prove beneficial\nIn the following, we conduct hyperparameter searches for adaptation from the highway dataset 2 to the highway dataset 1. For comparison, refer to Figures 5, 6, and 7, alongside Figure 4 . The importance of the number of models is evaluated in Figure 5, both with and without MC dropout. The results indicate that using four models yields significantly better performance than using only one or two models. Notably, Deep Ensembles demonstrate greater robustness for voting with a single model compared to the combination of Deep Ensembles and MC dropout. As a result, we conduct the subsequent hyperparameter searches using four models.\nWe present a hyperparameter search for the uncertainty thresholds $\\kappa_p$ and $\\kappa_n$, as well as the confidence thresholds $\\tau_p$ and $\\tau_n$, in Figure 6. For the negative confidence threshold $\\tau_n$, a value of 0.01 yields the best performance, while a low value of 0.0005 is optimal for the negative uncertainty threshold $\\kappa_n$. For the positive thresholds, we select $\\tau_p = 0.99$ and $\\kappa_p = 0.05$ for subsequent training. Ultimately, we achieve an accuracy of 99.08%, which is slightly lower than the performance of our proposed method shown in Figure 4b.\nWe also evaluate the number of selected samples for each repetition in Figure 7. The right plot presents the final classification accuracy in relation to the percentage of labeled data and the softmax threshold. Once again, it is evident that a larger amount of labeled data is crucial, with 5% of labeled data yielding higher classification accuracy compared to 0.5% and 1%. Additionally, accuracy improves as the number of repetitions increases. A softmax threshold of $\\gamma = 0.9$ remains the optimal choice. The left plot illustrates the percentage of samples where all four models predict the same class label, which are subsequently used for the pseudo-label dataset (upper row), as well as the percentage of these samples that are correctly classified (lower row). With a softmax threshold of 0.99, significantly fewer samples are selected for pseudo-labeling compared to thresholds of 0.7 and 0.9. Although the labeled samples selected at 0.99 are of higher quality, this does not result in an improved training process. A softmax threshold of 0.9 provides the best trade-off between the quantity of selected samples and the proportion of correctly labeled samples."}, {"title": "5. Conclusion", "content": "In the context of GNSS interference classification within complex real-world environments, we proposed a semi-supervised pseudo-labeling methodology to facilitate adaptation from controlled indoor settings to real-world environments, as well as to accommodate changes in scenarios. Our approach integrates Deep Ensembles with MC dropout to mitigate the impact of noise encountered by the network and to produce an output distribution that remains invariant to perturbations and augmentations. We achieve an accuracy of 99.2%, and our model demonstrates robustness even when utilizing only 0.5% of labeled data (equivalent to 5 positive samples)."}]}