{"title": "SELECT-SQL: SELF-CORRECTING ENSEMBLE\nCHAIN-OF-THOUGHT FOR TEXT-TO-SQL", "authors": ["Ke Shen", "Mayank Kejriwal"], "abstract": "In recent years,Text-to-SQL, the problem of automatically converting questions posed in natural\nlanguage to formal SQL queries, has emerged as an important problem at the intersection of natural\nlanguage processing and data management research. Large language models (LLMs) have delivered\nimpressive performance when used in an off-the-shelf performance, but still fall significantly short of\nexpected expert-level performance. Errors are especially probable when a nuanced understanding is\nneeded of database schemas, questions, and SQL clauses to do proper Text-to-SQL conversion. We\nintroduce SelECT-SQL, a novel in-context learning solution that uses an algorithmic combination\nof chain-of-thought (CoT) prompting, self-correction, and ensemble methods to yield a new state-\nof-the-art result on challenging Text-to-SQL benchmarks. Specifically, when configured using\nGPT-3.5-Turbo as the base LLM, SelECT-SQL achieves 84.2% execution accuracy on the Spider\nleaderboard's development set, exceeding both the best results of other baseline GPT-3.5-Turbo-based\nsolutions (81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the leaderboard.", "sections": [{"title": "1 Introduction", "content": "Natural language interfaces to databases allow non-SQL experts to query relational databases more conveniently.\nText-to-SQL, which automatically maps natural language questions to SQL queries [1, 2] has therefore emerged as an\nimportant problem, especially due to generative AI. Early Text-to-SQL systems were domain-specific with limited\nuser interaction, often relying on rule-based approaches to parse input questions [3, 4, 5, 6]. Recent advancements\nhave shifted towards greater domain independence by introducing supervised models trained on various cross-domain\ndatasets [7, 8], and transformer-based models fine-tuned with built-in modules and constraints [9, 10, 11, 12]. Unlike\nretrieval-augmented generation (RAG) [13], which uses transformer-based language models fine-tuned on external\nknowledge, Text-to-SQL reduces potential hallucinations in domain-specific or knowledge-intensive tasks because the\nanswer is from querying the database rather than being generated directly by a model.\nRecent developments in Text-to-SQL use large language models (LLMs) with zero-shot [14, 15] and few-shot prompting\n[16, 17], demonstrating that LLMs can serve as strong baselines with minimal demonstration of questions and schemas\nand no fine-tuning. Despite their potential, GPT-3.5-Turbo often underperforms compared to fine-tuned models on\nthe widely-used Spider benchmark [8], requiring over 10,000 tokens per query [18] or multiple generation rounds for\nself-consistency [15]. PT-4-based solutions perform better, but are less budget-efficient due to higher token costs 1.\nFew-shot prompting [19], especially when combined with Chain-of-Thought (CoT) [20] and self-consistency [21, 15],\noutperforms previous state-of-the-art methods on complex reasoning tasks [22, 23], achieving high accuracy with limited\nexamples. Gao et al. [17] emphasized that few-shot settings, including example selection and presentation format,\n\u00b9GPT-4 costs $30 per 1 million input tokens, whereas GPT-3.5-Turbo costs $0.50 per 1 million input tokens."}, {"title": "2 SelECT-SQL", "content": "Considering a target question q expressed in natural language related to a specific database D, which includes a schema\nS = {T, C'} representing the included tables T and columns C of D and outlining its structure, the Text-to-SQL\nproblem prompts model M to generate the optimal query y which maximizes the possibility:\n$\\displaystyle \\max_{\\psi,\\sigma, Q'} P_{M} (y | \\sigma(q, D, Q')),$\ns.t. Q' = k, Q' \u2282 Q,\nwhere function \u03c3(\u00b7,\u00b7,\u00b7) characterizes the representation of question q based on databased D and incorporates k selected\nexamples from the triple set Q = (qi, Di, Yi). As k increases from 0, the model transitions from zero-shot learning to\nin-context learning. Based on the definition provided by Gao et al.[17], this representation might incorporate various\ntypes of information such as the schema S, and may be expressed in different formats, including SQL syntax and\nnatural language. With the presented formalism, we introduce the three main components of the proposed SelECT-SQL\nprompting, starting with the Chain-of-thought component."}, {"title": "2.1 Chain-of-thought Prompting", "content": "Chain-of-thought (CoT; [20])is a prompting technique that guides an LLM to generate intermediate steps or explanations\nwhen solving complex reasoning problems. This approach enhances the model's interpretability and often leads to more\naccurate and reliable outcomes. Here, we introduce two different automated CoT prompting methods to decompose the\noriginal complex question into a multi-steps thinking process: structure-synthesis (SS) and modular-synthesis (MS)\nCoT. Each exemplar in the few-shot prompting \u03c3(\u00b7,\u00b7,\u00b7) is augmented with a thought process q' \u2208 CoT(Q') for the\nassociated answer, as shown below, thereby improving the model's performance. Examples of these two CoT methods\nare illustrated in Figure 1.\n$\\displaystyle \\max_{\\psi,\\sigma, Q', CoT} P_{M}(y | \\sigma(q, D, CoT(Q'))),$\ns.t. Q' = k, Q' \u2282 Q, CoT \u2208 {SS, MS}."}, {"title": "2.2 Self-correction", "content": "In the same way that SQL experts review and revise their queries to guarantee precision, our method includes a\nself-correcting component, denoted as SC, that autonomously evaluates and refines its SQL outputs. While several"}, {"title": "2.3 Ensemble refinement", "content": "Traditional ensemble machine learning models improve overall predictive performance by aggregating multiple weak\npredictors. Similarly, we adopt this strategy by using diverse prompts to generate multiple SQL queries for a given\nquestion, and request GPT-3.5-Turbo to identify the most accurate one. If none of the provided queries are correct, GPT\nis prompted to generate a new one. The rationale behind the approach is straightforward. Different prompts can induce\nvarious logical reasoning paths that might arrive at the same correct answer. By exposing GPT to the inherent variability\nof these potential solutions, we enable the model not only to evaluate the accuracy of each solution but also to engage in\ncreative problem-solving, thereby improving the accuracy of the query generation process and enhancing the model's\nability to handle complex query scenarios innovatively. Note that the provided queries do not necessarily need to be"}, {"title": "3 Experiments", "content": "Datasets We conduct our experiments on the Spider[8] development (dev) set. Spider is a large-scale, cross-domain\nText-to-SQL benchmark designed to evaluate natural language interfaces across various databases. The dataset features\ninstances where each instance includes a natural language question on a specific database, along with the corresponding\nSQL query. The training split has 8,659 instances; the development split has 1,034. All instances, including those in the\ntest set, are spread across 200 databases without any overlap between the databases in different splits. The queries vary\nin complexity, from simple, straightforward ones to those requiring nested queries for retrieval, and are categorized\ninto four levels of difficulty: easy, medium, hard, and extra (hard). Specifically, in the dev set, there are 248 queries\nclassified as easy, 446 as medium, 174 as hard, and 166 as extra hard.\nMetrics We follow the established Spider evaluation protocol 4 and other prior work by using execution accuracy\n(EA) and component matching accuracy metrics in the evaluation. The EA metric compares the execution output of a\nquery to the expected result of the ground truth query, providing a precise evaluation of model performance given that\nmultiple SQL expressions can yield the same satisfactory results for a question. The matching metric, on the other hand,\ndirectly compares the generated query to the ground truth query partially, for each SQL clause such as SELECT and\nWHERE, measuring the exact token-level match between them. This analysis offers insights into the differences in\nexpression preferences between human developers and the evaluated models. In Results, we primarily focus on EA, as\nit is a more objective metric of success, while matching is utilized for error analysis.\nPrompting In the experiments, we use GPT-3.5-Turbo and adopt the code representation (CR)[24, 25, 17] for schema\nand question representation in the Text-to-SQL task. This approach directly presents \u201cCREAT TABLE\u201d SQL statements\nto ChatGPT, enabling a thorough understanding of the database schema. Natural language questions are integrated as\ncomments within the prompts, positioned beneath the schema. CR distinguishes itself from other representations by\nproviding details essential for database construction, such as column types and primary/foreign keys, which enhance\nthe comprehension needed to generate correct SQL queries.\nFor example selection in the in-context learning setting, we use several methods. Question Similarity Selection\n(QTSs)[26] selects k examples based on similarities in question sentence embeddings. We developed a new encoder-\nbased technique called Full-information Selection (FISs), which analyzes embeddings of the database schema, masked\nquestion, and masked query, where domain-specific information is obscured. This approach provides a comprehensive\nmethod for selecting examples by taking into account all relevant textual elements.\nAdditionally, rather than relying on embedding similarity for choosing few-shot examples, we implement a novel\nstrategy Autos, which prompts GPT-3.5-Turbo to identify the set of examples that are most relevant to the target\nquestion. Examples are categorized by difficulty level, and a diverse collection is randomly selected from various\ndatasets. These selected examples are then used as input prompts for GPT, enabling it to choose those that are optimally\naligned with the specific question.\nFor the ensemble settings, we use queries generated under four different prompting methods. The first is generated\nusing zero-shot prompting. The next two responses are produced using SS CoT prompting with the Autos and FISS\nexample selection approaches, respectively. The final response is generated using MS CoT prompting with the Autos\nexample selection approach. We also explored other combinations as ensemble configurations in the experiments,\nwith the results detailed in Appendix 7.5. Note that this study primarily focuses on GPT-3.5-Turbo and the Spider\nleaderboard, without evaluating the SelECT-SQL using more advanced models or other benchmarks. We plan to explore\nSelECT-SQL with more advanced LLMs on additional benchmarks in the future.\nBaselines We compare our method against baselines listed on the official Spider website, all of which were evaluated\non the same dev set:"}, {"title": "4 Results", "content": "Overall execution accuracy We report the performance of our method and baseline methods on the Spider leaderboard\nin Figure 3. Our method outperforms those that use fine-tuned T5-3B and BERT models, as well as GPT-3.5-Turbo and\nGPT-4 models employing zero-shot or in-context learning settings. Specifically, compared to the C3 method, which\nemploys a zero-shot GPT-3.5-Turbo with a self-consistency component (including 20 generations), our approach uses\napproximately 80% fewer tokens; compared to DAIL-SQL, which uses GPT-4 engines with few-shot learning, our\nmethod reduces costs by 20x times, making it more cost-effective."}, {"title": "5 Error analysis", "content": "The execution accuracy of GPT-Turbo-3.5 with SelECT-SQL prompting varies across different databases, as shown\non the right-hand side of Figure 3. It achieves an accuracy above 0.9 on 9 out of 20 databases in the Spider dev set,\nparticularly in domains such as singers, poker players, pets, orchestra, and employee evaluations. In some instances, the\nengine answered all related questions correctly. However, for car and real estate property databases, which include more\nquestions at the hard or extra hard levels, the execution accuracy drops to 0.5 or below. The average number of tables\nin car and real estate databases is 5.5, whereas it is only 3 in the first group (singers, poker players, etc.). Evidence\nsuggests that the relatively complex schema structures in certain domains may impede the further improvement of GPT\nengines.\nTo gain clearer insights into the limitations of GPT engines and pinpoint failure areas, we conducted an error analysis,\nvisualized in Figure 4. According to the literature [18], errors are categorized into six types: schema-linking, join,\ngroup-by, nested, invalid, and other. Our analysis reveals that, with SelECT-SQL, GPT-3.5-Turbo no longer suffers\nfrom invalid syntax errors. To provide a more intuitive understanding of GPT-3.5-Turbo's failures, we refined the error\ncategories, especially within the join and other groups, reducing the number of error types from six to five.\nIn the join category, we focused on whether the engine missed join operations or included unnecessary joins. Errors\ninvolving incorrect joins based on wrong table and column names were reclassified as schema-linking errors, as they\nstem from incorrect schema understanding. In the other category, we include cases that do not fit under any of the\nother specific categories but specifically focus on whether the generated queries accurately conveyed the information\nrequested by the original question. This includes verifying if the engine correctly selected and counted the required\ncolumns, comprehended the true meaning of the question, and effectively handled string-matching syntax to produce\nthe expected output.\nIn Figure 4, it is evident that as the complexity of questions increases, the frequency of errors caused by inaccurate join\noperations rises. For extra hard questions, errors typically arise from missing essential joins between certain tables,\nconstituting 17.7% of the errors, which results in insufficiently narrowed query results and consequently incorrect\noutcomes. Nested-related errors also become more common in harder questions, often due to the misuse of set\noperations and the generation of incorrect subqueries within nested structures. Despite our self-correction component's\nprompting tips, which include careful selection of tables and columns in join operations and revising subqueries in\nnested queries, 56.1% of errors in hard and extra hard questions still stem from incorrect nested and join operations.\nThe manual review also reveals that, compared to human-crafted ground truth, GPT engines show a clear preference for\nnested (incorrect) operations over joins, even though joins are generally more effective in database analysis.\nNotably, string matching and select-related errors frequently occur in easy and medium questions. These errors typically\ninvolve the erroneous use of string matching patterns in queries or the selection of inappropriate columns, often\nresulting in the presentation of more information than necessary. For instance, when asked, \u201cWhat is the number\nof cars with a horsepower greater than 150?", "hp": ") AS INTEGER) > 150;', illustrating a string matching issue. When asked\n\u201cDescribe the section h.\u201d, the ground truth query is \u2018SELECT section_description FROM Sections WHERE section_name\n= 'h';', while the generated query is \u2018SELECT * FROM Sections WHERE UPPER(section_name) = UPPER('h');',\nexemplifying a selection issue. The partial component matching accuracy shown on the left side of Figure 4, from\nanother aspect, verifies that mismatches between WHERE clauses and keywords components in the ground truth query\nand the generated query are frequent in easy- and medium-level questions. These errors could be attributed to the\nmodel's lack of clarity on the detailed data points based on the schema, which might be mitigated by presenting few\nexample data points to the reasoning engine.\nIn some cases, errors arise from misunderstanding the input question or lacking inherent commonsense knowledge. For\ninstance, when asked, \"What is the best rank of losers across all matches?\" humans naturally interpret \"best rank\u201d as\nthe minimum ranking number due to ascending order ranking, resulting in the query \u2018SELECT MIN(loser_rank) FROM\nmatches;'. Conversely, GPT engines interpret the question more intuitively, leading to the incorrect query 'SELECT\nMAX(loser_rank) FROM matches;'. Fine-tuning with commonsense knowledge or designing specific prompts to\ninduce deeper reasoning may further enhance performance. We leave the exploration of plausible solutions as an open\nquestion for further research.\""}, {"title": "6 Conclusion", "content": "In this paper, we introduced SelECT-SQL, a novel self-correcting ensemble CoT prompting method for Text-to-SQL.\nBy generating ensemble query responses in a tips-augmented self-correction setting and using zero-shot prompted and\none-shot SS CoT examples, we achieved 84.2% execution accuracy on the Spider-dev leaderboard. This demonstrates\nthe effectiveness of SelECT-SQL in employing CoT prompting and self-correction, showing substantial performance\ngains with token-efficient one-shot learning. SelECT-SQL enhances GPT-3.5-Turbo's capabilities, delivering results\ncomparable to or surpassing state-of-the-art GPT-4 methods."}, {"title": "7 Appendix / supplemental material", "content": "7.1 Compute resources\nThe experiments were carried out using a MacBook Pro equipped with a 2.3 GHz 8-core i9 Core processor and 16GB\nof memory. The total cost of GPT-3.5-Turbo prompting for all experiments, inclusive of experiments not mentioned in\nthe main text, amounted to approximately $41.1. The longest duration required to prompt the spider dev set is 7176\nseconds.\n7.2 Objective paraphrasing prompt for Structure-synthesis CoT prompting.\nWe present the zero-shot prompting for the paraphrasing objective used in SS CoT in Figure 5.\nmessages = [\n{\n},\n{\n\"role\": \"system\",\n\"content\":\n\"You are an expert in SQL queries. Please rephrase the provided question based on\nthe provided schema to clearly state the query's objective and include any specific conditions\nor constraints related to the question and schema.\"\n},\n{\n\"role\": \"user\",\n\"content\":\n}\n]\n\"Schema: {schema}\",\n\"Question: {question}\",\nRephrased Question:\"\n7.3 FirstSubQuestion and nextStep prmot for Modular-synthesis CoT prompting.\nFigure 6 presents the zero-shot prompting for the firstSubQuestion and nextStep decomposition utilized in MS CoT. In\nthe experiments, we set the maximum epochs as 5.\n7.4 Data points generation prompting and human-crafted tips used in self-correction component.\nHere are the tips used to revise and refine the generated query in the self-correction component:\n\u2022 When using the \u2018GROUP BY' clause, consider prioritizing the primary key if it aligns logically with the\nrequirements of question. Often, you may need to group by other columns to achieve meaningful data\nsummaries and analyses.\n\u2022 Use clauses \u2018LEFT JOIN, \u2018CAST', \u2018REPLACE', \u2018DATEDIFF', \u2018IN', and 'OR' judiciously, and replace '<>'\nwith '!=' in your generation."}, {"title": "7.5 Ensemble settings.", "content": "We report the execution accuracy of GPT-3.5-Turbo under different ensemble settings. The different ensemble settings\nrefer to the number of potential solutions exposed to the engine for reference. We select results from the eight most\ndiverse prompting strategies as potential solutions: zero-shot code representation, zero-shot code representation with\nschema explanation (where we ask GPT-3.5-Turbo to generate a detailed explanation of the schema, including what\ninformation is included in each table, what's the primary key and foreign key, etc.), one-shot (SS CoT+example\nselected by FISs), three-shot (SS CoT+example selected by FISs), one-shot (MS CoT+example selected by Autos),\nthree-shot (MS CoT+Autos), one-shot (SS CoT+Autos), and three-shot (SS+Autos). The combinations of these\nprompting strategies under various settings of potential solutions are shown in Table 2. We present two versions of the\nensemble results, self-corrected and non-self-corrected, visualized in Figure 8."}, {"title": "7.6 Error analysis", "content": "Errors are categorized into five types: schema-linking, join, group-by, nested, and other.\nSchema-linking This category includes failed queries where the model did not correctly identify column names,\ntable names, or entities mentioned in the questions. It also contains errors where two tables are joined using incorrect\nforeign keys. Compared to its original definition in Pourreza's study, we have recategorized it to include incorrect join\noperations using incorrect foreign keys, previously categorized as join errors. This error category is divided into two\nsubcategories: incorrect identification of columns and incorrect identification of tables.\nJoin With the help of the new definition of schema-linking error, our focus about join errors is not on whether\nthe incorrect join is caused by incorrect table or column identification. Instead, we are concerned with whether the\ngenerated query includes unnecessary joins that incorrectly narrow down the returned responses or omits essential join\noperations, leading to incorrect responses."}]}