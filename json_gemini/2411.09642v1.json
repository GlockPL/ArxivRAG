{"title": "On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse", "authors": ["Alkis Kalavasis", "Anay Mehrotra", "Grigoris Velegkas"], "abstract": "Specifying all desirable properties of a language model is challenging, but certain require- ments seem essential for any good model. Given samples drawn from an unknown language, the trained model should (1) produce valid strings that have not been seen in the training data, and (2) be expressive enough to capture the full richness of the language. Otherwise, if the lan- guage model outputs invalid strings, it \u201challucinates,\u201d and if it fails to capture the full range of the language, it suffers from \u201cmode collapse.\" In this paper, we ask whether it is possible for a language model to meet both of these requirements. We investigate this question within a statistical setting of language generation, building on the seminal works of Gold [Gol67, Inf. Control], Angluin [Ang79, STOC], and Angluin [Ang88, Tech. Report]. In this setting, the language model is presented with randomly sampled strings from a distribution supported on an unknown language K, which is only known to belong to a possibly infinite collection of candidate languages. The goal of the model is to generate unseen strings from this target language. We say that the language model generates from K with consistency and breadth if, as the size of the training set increases, the set of strings it can output converges to the set of all unseen strings in K. Kleinberg and Mullainathan [KM24, NeurIPS] posed an open question of whether consis- tency and breadth in language generation are both possible. We answer this question nega- tively: for a large class of language models \u2013 including next-token-prediction-based models this is impossible for most collections of candidate languages. This contrasts with the re- cent positive result of Kleinberg and Mullainathan [KM24, NeurIPS], which demonstrated that consistent generation, without requiring breadth, is possible for any countable collection of candidate languages. Our finding highlights that generation with breadth is fundamentally different from generation without breadth. As a byproduct of our result, we also examine how many samples are required for gen- eration with or without breadth, establishing near-tight bounds on the \u201clearning curves\" for generation in the statistical framework of Bousquet, Hanneke, Moran, van Handel, and Yehu- dayoff [BHM+21, STOC]. Finally, our results also give some hope for consistent generation with breadth: it is achiev- able for any countable collection of languages when negative examples \u2013 in the form of strings outside of K are available in addition to strings inside of K. This suggests that feedback in post-training, which encodes negative examples, can be crucial in reducing hallucinations while also limiting mode collapse.", "sections": [{"title": "1 Introduction", "content": "Language acquisition is a fundamental mystery across multiple scientific fields, ranging from Bi- ology and Neuroscience to Sociology [SAN96; Bre07; Cla14; MIB+24]. Theoretical Computer Sci- entists have been fascinated by language since the early days of the field: in the 1950s, Turing [Tur50] introduced his famous test using language as an interface to cognition, Shannon [Sha51a] studied statistics of printed English aiming at understanding its entropy and the extent to which it could be compressed, and Mandelbrot [Man53] designed a statistical model to capture connec- tions between language and the brain. Over the years, language modeling has advanced through simple models, such as the word n-gram model introduced by Shannon [Sha51b] and widely used in natural language processing [BDd+92]. In the early 2000s, neural networks achieved a significant breakthrough in the field [BDV00], leading to fascinating deep learning systems [MKB+10; LBH15; Gol16] built using tra- ditional architectures like Recurrent Neural Networks [RHW86] and Long Short-Term Memory [HS97]. In 2017, the field of language modeling was revolutionized by the introduction of the Transformer architecture [Bah14; SVL14; VSP+17], which led to the development of Large Lan- guage Models (LLMs). The achievements of LLMs have been groundbreaking; recent models can perform well on tasks far beyond natural language processing [BCE+23; TLI+23]. Despite their impressive performance, their extensive use has revealed that LLMs exhibit various bizarre be- haviors even in seemingly mundane tasks [Bor23]. Perhaps the most well-known issue with current LLMs is hallucinations: the models generate false but plausible-sounding text with surprising frequency [JLF+23; ZLC+23].\u00b9 Such hallucina- tions, highlighted by popular media [WM23], could significantly impact safety, reliability, and user trust as the adoption of these systems extends to new tasks [AOS+16; HCS+22]. The impor- tance of this problem, among other concerns, led both the US [Bid23] and the EU [Sat23] to issue calls for safeguards against misleading outputs generated by LLMs. In this direction, designing LLMs that generate responses consistent with the ground truth is an effort that has gained a lot of attention from Machine Learning (ML) practitioners [WWS+22; AP23; GZA+23; HYM+23; JLF+23; FSW+24; KWT+24], policymakers [Bid23; Sat23; SK23], and theorists [HKK+18; KV24; KM24]. If the sole goal is to avoid hallucinations, then, of course, one could simply limit the range of outputs generated by the language model. As an extreme example, consider a language model that only outputs \u201cI am a language model\u201d and, therefore, never hallucinates. However, modern LLMs do not just aim to generate a few valid outputs; their goal is to obtain the ability to express a wide range of plausible outputs, thus capturing the richness of human language. The key chal- lenge lies in avoiding hallucinations while achieving breadth. The problem of achieving consistent generation with breadth is not new in the ML community, dating back at least to the era of Gen- erative Adversarial Networks (GANs) [GPM+20]. In this line of work, mode collapse [GPM+20] is the analog of lack of breadth; it refers to the phenomenon where the GAN assigns non-zero mass only to a few modes of the true data distribution, thus producing a limited variety of samples and becoming repetitive [AB17; SSA18; BZW+19]. The starting point of our work is exactly this puzzling tension between consistent generation and breadth in language generation."}, {"title": "1.1 Informal Results", "content": "Our main results confirm the tension between consistent generation and breadth for language models, conjectured by Kleinberg and Mullainathan [KM24], in a strong way: informally, we show that A language model that generates with breadth must be inconsistent, i.e., it must hallucinate. We focus on the probabilistic setting of Angluin [Ang88] which we have already introduced in- formally. En route to our results in the probabilistic setting, we also obtain results in the online setting of Gold [Gol67], Angluin [Ang79], and Kleinberg and Mullainathan [KM24], as we will"}, {"title": "1.1.1 Setup and Definitions", "content": "A generating (or learning) algorithm is a sequence of computable mappings (Gn) = (Gn)n\u2208N from samples S\u2282Xn to generators, which are simply distributions over the domain X. More formally, a generating algorithm is a sequence of mappings from samples to Turing machines that generate samples from an (explicitly or implicitly) defined distribution over strings. In the statistical setting we consider, the learner observes samples from an unknown distribu- tion which is valid for some unknown language K in the collection L = {L1, L2, . . . }. Definition 1 (Valid Distribution [Ang88]). A distribution P over a countable domain X is valid with respect to a countable language collection L if its support is the same as some language K \u2208 L. In this case, when we want to be specific about the language that P draws samples from, we say P is valid for K. If the collection L is clear from context, we will simply say that P is valid. Based on this definition and building on the model studied by Kleinberg and Mullainathan [KM24], we give the following adaptation for consistent generation from a collection L in the statistical setting. Definition 2 (Consistency). A generating algorithm (Gn) for a language collection L is consistent if for any valid distribution P, it holds that limn\u2192\u221e gen_er(Gn) = 0. Otherwise, the algorithm is said to be inconsistent. Hence, an algorithm is said to be consistent if the generators it produces by training on any valid distribution P converge to generating examples from the unseen part of P. Some of our results explore when asymptotic consistency is achievable. However, the main focus of our work is on understanding the rates at which consistency (and other desirable properties) can be attained \u2013 if possible at all. In particular, we want to study the rate at which the generation error gen_er(Gn) decreases as the number of samples n goes to infinity \u2013 that is, we want to study the learning curve of consistent generation (and other tasks that we introduce later in this section). Bousquet, Han- neke, Moran, van Handel, and Yehudayoff [BHM+21] characterized learning curves for binary classification, formalizing the universal rates framework, earlier explored by Schuurmans [Sch97] and Antos and Lugosi [AL98]. To this end, we borrow their definition of universal rates. Definition 3 (Informal, Universal Rates; [BHM+21], see Definition 12). A generating algorithm (Gn) has rate R(\u00b7), where limn\u2192\u221e R(n) = 0, for a language collection L if \u2200P\u2208 Val(L) \u2203C, c > 0 such that gen_er(Gn) \u2264 C\u00b7R(c\u00b7n) \u2200n\u2208N, where Val(L) is the class of valid (realizable) distributions for L. Observe that these learning curves are distribution-dependent since the constants c and C are allowed to depend on P. This difference turns out to be crucial and can, sometimes, lead to sig- nificant differences between universal rates and the corresponding distribution-independent rates [BHM+21]. Among different universal rates, exponential universal rates are of specific interest as they are often the best possible rate, as we will see later. We say that the algorithm (Gn) generates with an exponential universal rate if R(n) = exp(-n) in the above definition. Next, we turn to language generation with breadth."}, {"title": "1.1.2 Main Results", "content": "We now have all the ingredients to state our first result, which establishes that, for all generat- ing algorithms for which MOP(\u00b7) is decidable, (consistent) generation with breadth is as hard as language identification in the statistical setting."}, {"title": "1.2 Technical Overview", "content": "In this section, we present the technical tools we develop to obtain our main results."}, {"title": "3.1 Results for Identification and Generation Without Breadth", "content": "Prior work of Gold [Gol67] and Kleinberg and Mullainathan [KM24] studies language identifica- tion and generation in an online, i.e., adversarial setting. In this work, we study the distributional versions of these problems. The identification problem we study is not new and, in fact, goes back to Angluin's work in 1988 [Ang88]. However, Angluin [Ang88] does not provide any rate at which language identification can be achieved as the number of samples observed increases (when it is achievable). Summary of Results in This Section. In this section, we give learning rates for both identification and generation (see Theorems 3.1 and 3.2 respectively). For both tasks, we study the learning curves - that is how the identification or generation error decays as the sample size increases. As a result, we extend the results of Gold [Gol67] and Kleinberg and Mullainathan [KM24] to the statistical setting. Our results in this section achieve a near-optimal rate for identification (Theorem 3.1) and an optimal rate for generation (Theorem 3.2)."}, {"title": "3.1.1 Universal Rates: Model and Preliminaries", "content": "We work under the universal rates framework, introduced by Bousquet, Hanneke, Moran, van Handel, and Yehudayoff [BHM+21], in order to capture the notion of a learning curve for language identification and generation. Following the notation we used before, recall that we have a count- able set of languages L = {L1, L2, \u2026 \u2026 \u2026 }, where each L \u2208 L is also countable and ULELL \u2286 X, for some countable domain X. Recall the notion of a valid distribution proposed by Angluin [Ang88] in this setting (Definition 1). Intuitively, this condition can be thought of as the equivalent of realizability in the classification setting. The learning algorithm is a sequence of (universally measurable and computable) functions {hn}n\u2208N, where n captures the size of the training set. We are interested in understanding the behavior of the error of the algorithm, which is defined appropriately based on the downstream task \u2013 either identification or generation for this paper. Given some rate function R: N \u2192 [0,1] we say that we can achieve rate R(n) for the set of language L and the loss function er(\u00b7) if there exists a learning algorithm {hn}n\u2208N whose error satisfies (\u2200 valid P) (\u2203C, c) such that E[er(hn))] \u2264 C\u00b7R(cn), \u2200n\u2208N. Crucially, these learning curves are distribution-specific; the constants c, C depend on P but the rate R holds universally for all valid distributions. Such learning curves are a well-studied topic in learning theory [Sch97; AL98; BHM+21; VL23]. The above gives rise to the following definition. Definition 12 (Learning Rates [BHM+21]). Given a language collection L, an error function er(\u00b7), and a rate function R: IN \u2192 [0,1] satisfying limn\u2192\u221e R(n) \u2192 0, we say: \u2022 Rate R is achievable for L if there is an algorithm {hn}n\u2208N such that for every valid distribution P, there exist c, C for which E[er(hn)] \u2264 C\u00b7R(c\u22c5n),\u2200n \u2208 IN. \u2022 No rate faster than R(n) is achievable for L if for all algorithms {hn}n\u2208N there exists a valid distri- bution P and c, C for which E[er(hn)] \u2265 C \u00b7 R(c\u00b7n), for infinitely many n \u2208 IN. Further, we have the following."}, {"title": "3.1.2 Universal Rates for Identification", "content": "For any language collection L = {L1, L2, ...} and n \u2208 N, with true language K \u2208 L, and set of examples x1,...,xn \u2208 Xn, an identification algorithm In gets as input x1, ..., Xn and outputs an index In (X1,...,xn). We define the identification error of the learner {In : Xn \u2192 N}n\u2208N as er(In(x1,...,xn)) = 1{LIn(x1,...,xn) \u2260 K} . (4) Under this definition, Ex1,...,xn~P[er(In)] = Prx1,...,xn~P [LIn(x1,...,xn) \u2260 K], i.e., the probability that it fails to identify the correct language after it sees n examples from P.11 Our main result for identification is a fundamental dichotomy: every non-trivial collection of languages is identifiable with positive examples at either an (almost) exponential rate or it is not identifiable at any rate. Theorem 3.1 (Dichotomy of Rates for Identification with Positive Examples). For every collection of countably many languages L that is non-trivial for identification exactly one of the following holds: \u2022 For every g(n) = o(n) there exists a learner that identifies L at rate e-g(n). Moreover, no learner can achieve a rate faster than e\u00afn. \u2022 L is not identifiable at any rate. Concretely, the first condition holds for L if and only if it satisfies Angluin's condition (Definition 10)."}, {"title": "3.1.3 Universal Rates for Consistent Generation", "content": "The main difference between this setting and the setting of language identification is the definition of the error rate. There exists a valid text-generating distribution P, meaning one that is supported on some target language K \u2208 L, and the learning (or rather, generating) algorithm is a sequence of (universally measurable and computable) functions {Gn: X\u2033 \u2192 X}n\u2208N, where each Gn takes as input n samples generated i.i.d. from P and outputs a new word, with the goal that this word belongs to the target language (see Remark 1). As in the online setting, to avoid trivial solutions, we want to generate examples that do not appear in the training set. Remark 1 (Notation for Generating Algorithms). More formally, a generating algorithm is a col- lection of mappings {Gn}n\u2208N, where for each n, Gn is a mapping from the domain of n training samples X", "generators": "r (randomized) Turing machines G that, on each execution, output a sample from X. For this section, it is sufficient to imagine generators as being determin- istic (i.e., generating samples from a point mass) and, hence, we simplify writing Gn as a mapping from X", "X": "o G. Now, we are ready to define the generation error: for any n \u2208 N and set of examples X1, ..., Xn \u2208 Xn we define the generation error of the learner {Gn: X\u2033 \u2192 X}n\u2208N for this task as er(Gn(x1,...,xn)) = 1 {Gn(x1,...,xn) \u2209 K\\{x1,...,Xn}}. (5) Notice that, under this definition, EX1,...,Xn~Pn[er(Gn(x1,...,xn))] = PrX1,...,Xn~Pn[Gn (X1,...,xn) \u2209 K\\{x1,...,Xn}], i.e., the probability that the learner fails to generate a new word from the target language after observing n examples from it. Our main result in this section is that we can achieve consistent generation with exponential rates. Theorem 3.2 (Rates for Generation). For every countable collection of languages L there exists a gen- erating algorithm that generates from L at rate e-\u00afn. Conversely, for every collection of languages that is non-trivial for generation (Definition 17), no generating algorithm can achieve rate faster than e\u00afn. Surprisingly, this shows that consistent generation can be achieved at an exponential rate for any countable collection of languages. We mention that the result we prove is slightly stronger: we show that, for any L, with probability at least 1 \u2212 C\u00b7e-c\u22c5n, we can generate infinitely many new strings from K, after training the algorithm on n examples \u2013 not just a single word. Together, Theo- rems 3.1 and 3.2 show that the stark separation between language identification and generation in the online setting, obtained by Kleinberg and Mullainathan [KM24], also extends to the statistical setting of Angluin [Ang88] and Bousquet et al. [BHM+21]."}, {"title": "3.2 Results for Generation With Breadth", "content": "Next, we present our results for language generation with breadth. Clearly, generation with breadth is a stronger requirement than generation. But, at least intuitively, it is weaker than identi- fication: it only requires one to generate samples from the entire support of K and not identify the index of K. Contrary to this intuition, our results show that, for a large class of generators, genera- tion with breadth is as hard as identification. Our results show that, while this class of generators is powerful enough to generate without breadth, no generator in this class can achieve generation with breath for non-identifiable collections of languages."}, {"title": "3.2.1 Membership Oracle Problem", "content": "The family of generators we consider is implicitly determined by the decidability of a certain problem associated with the generator. Definition 5 (Membership Oracle Problem). Given a generator G, the membership oracle problem for G, denoted as MOP(G), is defined as follows: given the description of G and a string x, output Yes if x \u2208 supp(G) and output No otherwise. As mentioned before the decidability of problems is extensively studied in formal languages and complexity theory [Sip12]. Our main result (Informal Theorem 1 whose formal statement appears as Theorem 3.3) applies to any generator G for which MOP(G) is decidable. Note that our result only needs a decider of MOP(G) to exist \u2013 this is purely a property of the generation algorithm used \u2013 and it does not, for instance, require the individuals training the generator or the users to have access to the decider in any fashion. To gain some intuition about the membership oracle problem, let us consider a simple example."}, {"title": "3.2.2 Results for Generators for Which MOP(\u00b7) Is Decidable", "content": "Before stating our result about the rate at which generation with breadth can be achieved, we need to define the corresponding error function. For the error to make sense, let G be the set of (randomized) Turing machines that do not take any input and output one element from X (on each execution). Given a target language K and examples x1,...,Xn \u2208 X, we define the error for generation with breadth for the learner {Gn : X\u2033 \u2192 G}n\u2208Nas er(Gn(x1,...,xn)) = 1{supp(Gn(x1,...,xn)) \u2260 K\\{x1,...,Xn}}, where supp(Gn(x1,...,xn)) is the set of strings Gn (x1,..., xn) can output with positive probability, i.e., it is the support of the distribution of outputs of Gn (X1,...,xn). The above means that we count each step t as a mistake if the generating algorithm has a positive probability of outputting a string outside of K (i.e., hallucination), a zero probability of outputting an unseen element of K (i.e., mode collapse), or a positive probability of repeating a seen training example. Remark 2 (Generating Examples From the Training Set). For generation without breadth, it is im- portant to restrict the generator from outputting elements it has already seen. Otherwise, the fu- tile generator, which always outputs the first training sample it sees, achieves generation without breadth. This requirement, however, is not important for generation with breadth: any generator G that generates with breadth without repeating training examples can be converted to one G' that generates with breadth and repeats the training examples and vice versa.12 Hence, all of our results hold with either notion of generation with breadth. Our main result shows a separation between the rates achievable for generation with and without breadth by any generating algorithm for which MOP(\u00b7) is decidable. Theorem 3.3. Let G be the set of all generating algorithms (Gn) for which MOP(\u00b7) is decidable (Defi- nitions 5 and 6). For every collection of countably many languages L that is non-trivial for generation (Definition 17) and not identifiable in the limit: \u2022 No generating algorithm in G generates with breadth from L at any rate; and \u2022 There is a generating algorithm in G that generates consistently without breadth from L at rate e-n. Conversely, no generating algorithm (even outside of G) can generate at a rate faster than e-n. Further, for any collection of countably many languages L that is non-trivial for generation (Definition 17) and identifiable in the limit, and for any g(n) = o(n), there is a generating algorithm in G that generates with breadth from L at rate e-g(n). Conversely, no generation algorithm can generate consistently at a rate faster than e\u00afn, even without the breadth requirement. Thus, while generation without breadth is achievable for any countable collection of languages (whether it is identifiable or non-identifiable), generators in G can only generate with breadth from identifiable collections \u2013 which are a very restricted subset of all languages [Gol67; Ang80; KM24]. It remains to discuss which types of generators MOP(\u00b7) is decidable for, and we present a"}, {"title": "3.2.3 A Family of Generators for Which MOP(\u00b7) Is Decidable", "content": "Example 1 already shows that MOP(\u00b7) is decidable for many existing language models. Next, we show that MOP(\u00b7) is decidable under even fewer restrictions on the generator G \u2013 informally, we will allow for any generator which generates text token-by-token. Definition 14 (Token-by-Token Generators). Token-by-token generators G are parameterized by ran- domized Turing machines M. M can be randomized and halts on all inputs. Given M, the corresponding token-by-token generator Gm generates outputs as follows: for each t \u2208 N, 1. Let w\u2081w2... wt-1 be the tokens generated so far. 2. Let At be any auxiliary information generated so far, where A\u2081 is the empty string. 3. Generate (st, At+1) by running M with input w\u2081w2 . . . wt-1 and At. 4. If st = EOS (i.e., end of string), then output s\u2081 . . . st and halt; otherwise proceed to iteration t + 1. Note that token-by-token generators are a very powerful class: for instance, any distribution over \u03a3* for some finite alphabet \u2211 admits a token-by-token generator by the Bayes rule. That said, of course, one can also construct non-token-by-token generators. We show that MOP(G) is decidable for all token-by-token generators. Theorem 3.4. For any token-by-token generator G, MOP(G) is decidable. Next, we demonstrate that token-by-token generators capture several interesting language mod- els. First, the family of token-by-token generators captures existing large language models (LLMs): for instance, to simulate an LLM L, we define the next token predictor M as a Turing machine that simulates L on the provided string until L generates one new token. Further, since we do not place computational restrictions on M, M can also simulate interactions between LLMs or auxiliary sys- tems that select a suitable LLM to respond depending on the request\u2013a strategy that has led to recent advances in text generation [SDD+23; JSR+24; Kni24; Tea24]. Finally, due to a reduction to the halting problem, there are some generators for which MOP(\u00b7) is undecidable and give an explicit example in Section A."}, {"title": "3.2.4 Results for Generation With Breadth in the Limit", "content": "In this section, we state the implications of our techniques for generation with breadth in the adversarial or online setting of Gold [Gol67] and Angluin [Ang79; Ang80]. Theorem 3.5. For every non-identifiable collection of countably many languages L, no generating algo- rithm, for which MOP(\u00b7) (Definitions 5 and 6) is decidable, can generate with breadth from L in the limit. If L is identifiable, then there is a generator G (for which MOP(G) is decidable) that generates with breadth from L. This result makes important progress on a question left open by Kleinberg and Mullainathan [KM24] for a fairly large family of generators, which includes all iterative generators due to The- orem 3.4. In particular, MOP(\u00b7) is decidable for the generation algorithm of Kleinberg and Mul- lainathan [KM24] (since it is deterministic and the unique element it outputs can be computed by executing the algorithm) and, hence, the above result shows that Kleinberg and Mullainathan [KM24]'s algorithm cannot generate with breadth in the limit from any non-identifiable collection. Further, in Section 3.3 we strengthen this result by showing that even a relaxed notion of genera- tion with breadth remains unreachable for a large class of generators. The proof of this result can be found in Section 6.3."}, {"title": "3.3 Results for Generation With Approximate Consistency and Breadth", "content": "In this section, we study a relaxation of generation with breadth, which we call unambiguous generation, and ask: Is there a generator that unambiguously generates from a non-identifiable collection? We recall that, in this section, we will allow the generator to repeat examples in the training data. Like all of our results with breadth, this choice is not crucial, and all of the results have analogs where the generator does not repeat training examples (Remark 2). We make this choice to simplify the notation. We refer the reader to Section 1.3 for a discussion and motivation of the definition for unam- biguous generation, which we restate below. Definition 8 (Unambiguous Generator). A generating algorithm G = (Gn) is unambiguous for a lan- guage collection L if, for any K \u2208 L and every enumeration of K, its support eventually becomes closer to K than to any other language L \u2260 K in L in terms of the symmetric difference metric, i.e., there exists some n* \u2208 IN such that for all n \u2265 n* it holds that |supp(Gn)\u25b3K| < minLEL: L/K supp(Gn)\u25b3L|, where recall that for two sets S and T, S\u25b3T := (S \\ T) U (T \\ S). This notion is a significant relaxation of generation with breadth that we considered so far (see Section 3.2): Not only does it allow the generator to hallucinate certain strings not in the target K and omit strings actually in K for arbitrarily long, the number of hallucinations and omissions can be very large and, depending on the structure of the language collection L, even arbitrarily large. Surprisingly, even this very weak notion of \u201cgeneration with breadth\u201d turns out to be un- achievable by a very large family of generators. Concretely, it is unachievable by any generator for which MOP(\u00b7) is decidable and that satisfies the natural property that it stabilizes after a finite time. We state the formal notion of stability below."}, {"title": "3.4 Further Results for Identification", "content": "In this section, we present identification algorithms that achieve exact exponential rate when one has some additional structure access to a stronger oracle, or a finite collection L, or a countable collection L of finite languages \u2013 or additional information \u2013 negative examples. In Section 3.4.1, we allow the identifier to make queries of the form \u201cis Li \u2286 Lj?\" Next, in Section 3.4.2, we consider generation from collections L containing finitely many languages L = {L1, L2, ..., Lk}. (Note that each language in L can still be infinite.) Finally, in Section 3.4.4, in addition to positive examples, we also give the identifier access to negative examples (i.e., ele- ments x \u2208 X not in the target language K).\""}, {"title": "4 Organization of the Rest of the Paper", "content": "We next describe the organization of the rest of the paper. \u2022 The proofs of Section 3.1 (statistical rates for identification and generation) can be found in Section 5. The proof for the identification universal rates appears in Section 5.1 and for generation in Section 5.2. \u2022 The proofs of Section 3.2 can be found in Section 6. In Section 6.1, we discuss the decidability of MOP(). In Section 6.2 we provide our main result that generation with breadth is not possible for generating algorithms for which MOP(\u00b7) is decidable. Finally, in Section 6.3, we see the implications of this result for generation in the limit. \u2022 The proofs of Section 3.3 appear in Section 7. In Section 7.1 we give the proof of the result in the online setting and in Section 7.2 the proof of the result in the statistical setting. \u2022 The proofs of Section 3.4 appear in Section 8. In Section 8.1 we give the proof of exponential rates for identification using a subset oracle, in Section 8.2 the proof of exponential rates for identification of finite collections using a membership oracle, and in Section 8.3 the proof of exponential rates for identification of countable collections of finite languages. The proof for the identification rates with positive and negative examples appears in Section 8.4."}, {"title": "5.1 Proof of Theorem 3.1 (Rates for Identification)", "content": "In this section, we give the full proof of Theorem 3.1; see Figure 2 for an outline. As we alluded to before, the first step in the proof is to show that all non-trivial collections are not learnable at rate faster than e-n."}, {"title": "Proposition 5.2 (Infinite Draws Are Enumerations).", "content": "Let P be a probability distribution supported on a countable domain and {Xi}i\u2208N, where every X\u00a1 is i.i.d. from P. Then, \\ Pr \\ _{{X_i\\}_{i \\in \\mathbb{N}} \\sim P^{\\infty}} [supp(P) = \\bigcup_{i \\in \\mathbb{N}} \\{X_i\\}] = 1. (6)"}, {"title": "Lemma 5.7.", "content": "Let L be a countable collection of languages over a countable domain X that does not satisfy Angluin's condition (Definition 10). Then, for every learning algorithm A = \\{h_n\\}_{n \\in \\mathbb{N}} there exists a valid distribution P supported on some K \u2208 L such that \\ Pr\\_{{X_i\\}_{i \\in \\mathbb{N}} \\sim P^{\\infty}} [\\exists i_1 < i_2 < i_3 < ... : L_{h_i \\left(X_1, ..., X_{i_j}\\right)} \\neq K, \\forall j \\in \\mathbb{N}] \\geq \\frac{1}{3}."}, {"title": "8.4 Proof of Theorem 3.11 (Identification from Positive and Negative Examples)", "content": "We now move on to the task of language identification with both positive and negative examples. The main difference between this setting and binary classification is that the objective function is different. In particular", "Ang88": "a distribution P is valid with respect to L if and only if supp(P) \u2286 X \\times \\{0", "1)": 0, "P[(x,0)": 0}, {"P[(x,0)": 0, "1)": 0, "h_n": "X \\times \\{0, 1\\})^n"}]}