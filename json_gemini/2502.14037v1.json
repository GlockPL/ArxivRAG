{"title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "abstract": "Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLMs) have achieved remarkable performance (Bubeck et al., 2023), driven by large-scale data sets, increasing computing capacity (Bommasani et al., 2021), and innovative learning strategies (e.g., Stiennon et al., 2020; Rafailov et al., 2023). While training provides LLMs with the information and skills required to process natural language, another aspect plays a key role at generation time: the decoding strategy, i.e., how we extract text strings from the model. The most straightforward strategies, such as greedy decoding (always selecting the highest-probability token) or sampling, tend to repeat the same tokens multiple times (Su et al., 2022), reproduce training data (Carlini et al., 2021; Franceschelli et al., 2024), or flatten the lexicon in favor of the most common grammatical structures and words (Fleisig et al., 2024; Reviriego et al., 2023). Although the temperature parameter may increase the likelihood of less frequent tokens, it also raises the chance of syntactically incorrect ones by flattening their probabilities, regardless of their actual ranking. An ideal solution should concentrate on where the critical mass of the probability distribution resides. More precisely, with critical mass we refer here to the portion of the probability distribution that collectively accounts for the majority of the probability mass of the tokens. In this direction, a common approach is nucleus sampling (Holtzman et al., 2020), which removes the tail of the probability distribution by focusing on the smallest subset of tokens whose global probability exceeds a given threshold. However, key issues remain: first, nucleus sampling is sensitive to the choice of the threshold; second, it can either preserve incorrect tokens or exclude appropriate ones if the critical mass is smaller or larger than the threshold, respectively. As suggested by Hewitt et al. (2022), the learned probability distribution can be viewed as a mixture of the true distribution, which assigns a non-zero probability only to appropriate tokens (the critical mass), and a smoothing distribution, which assigns a small but non-zero probability to incorrect tokens. This smoothing is necessary for learning purposes.\nIn this paper, we introduce a family of sampling strategies called DiffSampling, based on the analysis of the probability distribution of the tokens. In particular, these decoding methods are based on the identification of the critical mass of the token probability distribution through the minimum discrete derivative (i.e., the largest difference between consecutive probabilities in a sorted distribution). We propose three methods to exclude incorrect tokens or increase less probable but appropriate tokens (see Figure 1). We then provide a comprehensive evaluation of these decoding solutions considering three different tasks (namely, math problem solving, extreme summarization, and the so-called divergent association task (Chen and Ding, 2023)) and discuss their advantages and limitations. We show that DiffSampling consistently performs better in either quality or diversity.\nThe remainder of this paper is structured as follows. First, we introduce the decoding problem from a neural language model and discuss existing approaches (Section 2). Then, we present our discrete derivative-based sampling strategy and three different methods to employ it (Section 3). Finally, in Section 4 we evaluate our methods on mathematical problem-solving tasks, extreme summarization, and the divergent association task against the most common baselines, finding that DiffSampling is a simple yet effective way to generate appropriate and diverse text."}, {"title": "Background", "content": "An autoregressive language model is a probability distribution $p_{\\theta}(x)$ parameterized by $\\theta$ over a variable-length text sequence $x = (x_1...x_T)$, where T is the sequence length and each token $x_t$ is in a finite vocabulary V of size N. The probability distribution is factorized as $p_{\\theta}(x) = \\prod_{t=1}^{T}p_{\\theta}(x_t|x_1...x_{t-1})$, and the language model is usually trained to maximize the likelihood of the true distribution $p^*(x)$ for any x from a reference dataset (the training set). In other words, given in input $x_1...x_t$, the model learns to approximate the probability of each token from V being $x_{t+1}$. While this makes such a model immediately capable of scoring the probability of a given text, it also allows for the generation of new sentences. Given a commonly human-written prefix (also known as a prompt) $x = (x_1...x_P)$ of length P, we can decode a continuation $x = (x_{P+1}...x_{T+P})$ from the language model through its factorized representation introduced before. However, we must remember that the language model is trained to score and not to generate sentences. Whereas a certain text might have zero probability for generation purposes (e.g., the text is syntactically incorrect), it might have non-zero probability for ranking purposes (Hewitt et al., 2022)."}, {"title": "Decoding Strategies", "content": "The decoding of tokens from the probability distribution learned by a neural language model can occur in several ways. The greedy strategy involves selecting the most probable token each time. However, this can lead to a consistent lack of diversity and several repetitions. The standard approach involves sampling from the probability distribution, which can be transformed through a temperature parameter. The temperature scales the differences among the various probabilities: a temperature lower than 1 will increase the probability of the most-probable tokens (a zero temperature degenerates to greedy strategy), while a temperature higher than 1 will increase the probability of the least-probable tokens, allowing for more diversity in generation (Peeperkorn et al., 2024). However, this might lead to the selection of tokens that are not syntactically appropriate for the current input. Top-k sampling (Fan et al., 2018) reduces the token space to the k most probable ones.\nTo generate more natural and coherent solutions, contrastive search (Su et al., 2022) employs a top-k sampling method combined with a degeneration penalty. This promotes the selection of tokens that differ from those already generated, enhancing the diversity and quality of the output. Nevertheless, limiting the number of selected tokens a priori can lead to the exclusion of meaningful tokens or the inclusion of inappropriate ones. A possible solution is to set k dynamically, as in Mirostat (Basu et al., 2021): to maintain the perplexity of generated text at a desired value, the k parameter is actively tuned based on the current cross-entropy."}, {"title": "DiffSampling", "content": "Given the probability distribution of the next token, let us imagine sorting it to have tokens in descending order based on their probability. The critical mass of the distribution can be determined by identifying the largest difference between probabilities: the token to its left should be the least probable token that our model still considers correct, i.e., the one that we might want to generate to produce an output that is both appropriate and diverse.\nFrom a mathematical analysis perspective, this point is characterized simply and elegantly as the location where the derivative reaches its minimum. Let us consider a probability distribution $p(x)$ defined for a limited number of $x_1...x_N$, with $p(x_i)$ monotonically decreasing. According to the forward difference approximation, the discrete derivative of a function $f(x_n)$ is defined as $\\Delta f(x_n) = f(x_{n+1}) - f(x_n)$, thus we have:\n$\\Delta p(x_n) = \\begin{cases} p(x_{n+1}) - p(x_n) & \\text{if } n < N \\\\ -p(x_n) & \\text{if } n = N \\end{cases}$ (1)\nwhich is always non-positive. $\\underset{x_n}{\\operatorname{argmin}}(\\Delta p(x_n))$ represents the index of the last token before the point characterized by the largest difference between probabilities.\nStarting from $\\Delta p(x_n)$, we propose DiffSampling, a family of different decoding strategies. The first one, which we call DiffSampling-cut, consists of cutting the tail at the right side of the point characterized by the largest difference between the probabilities, i.e., sampling among the tokens $x_i, i < \\underset{x_n}{\\operatorname{argmin}}(\\Delta p(x_n))$. This approach can be seen as an improved greedy strategy: when the model has high confidence in a single token, it degenerates into the greedy strategy; otherwise, it preserves other appropriate tokens, increasing the diversity of the final result.\nHowever, there might be situations in which some of the excluded tokens are still correct; for example, the first token might minimize $\\Delta p(x_n)$, but still have a quite low probability, i.e., it does not really cover the entire critical mass. To address this issue, DiffSampling-lb introduces a lower bound on the mass probability. To leverage the advantage of our cutting strategy while maintaining that of nucleus sampling, our second strategy considers truncating based on $\\Delta p(x_n)$ in such a way that the resulting tokens have a total probability at least equal to the lower bound $p_{lb}$. In other words, given k cardinality of the smallest subset of tokens whose total probability is not lower than $p_{lb}$, it computes the $\\underset{x_n}{\\operatorname{argmin}}(\\Delta p(x_n))$ for $n \\geq k$. This approach can be seen as an improved nucleus sampling: it corrects the p parameter of nucleus sampling via our derivative-based approach to include correct tokens after the selected nucleus. Reintroducing this parameter is sensible, and our findings indicate that setting it to 0.8 results in minimal accuracy loss while significantly enhancing diversity compared to lower values (see Appendix C.1).\nFinally, our third strategy aims to transform the probability distribution to prevent the most frequent tokens in the training set from being the most probable according to the model. This is important because consistently selecting the most probable token can lead to repetitive and less diverse outputs, reducing the overall quality and creativity of the generated text. By increasing diversity and reducing the risk of accidental reproduction, we can produce more varied and interesting results. DiffSampling-reparam achieves this by adjusting the model's probabilities toward options with the smallest derivatives. It does so by adding the negative of these derivatives to the original probabilities, scaled by a multiplier $\\gamma$ as follows:\np'(x_n) = p(x_n) - \\gamma \\Delta p(x_n) \\cdot$ (2)\nThis, combined with the cutting and lower-bound strategies, enhances novelty while maintaining the appropriateness of responses. This approach can be seen as an alternative temperature: while a higher temperature still preserves the most probable tokens as such (and introducing nucleus sampling has the limitations mentioned above), our strategy increases the probability of tokens before a \"jump\", which are less likely to be sampled. However, the $\\gamma$ parameter has a different behavior than temperature. With $\\gamma = 0$, we fall back to DiffSampling-lb. With a very large $\\gamma$, the sampling scheme becomes deterministic, greedily selecting the token with the minimum derivative. Conversely, with a small value, such as $\\gamma = 1$, the scheme promotes that token while still maintaining the original distribution.\nOverall, DiffSampling can be seen as a sampling scheme governed by two parameters, i.e., the probability-mass lower bound $p_{lb}$ and the reparameterization factor $\\gamma$. The full algorithm is reported in Algorithm 1."}, {"title": "Experiments", "content": "To evaluate whether DiffSampling helps diversify outputs while maintaining a high level of accuracy, we test it on three case studies: math problem solving, text summarization, and the divergent association task\u00b9."}, {"title": "Models and Baselines", "content": "In all our experiments, we start from a state-of-the-art LLM and test various decoding strategies. For the math problem-solving tasks, we use the Llama2-based MetaMath model trained with self-supervised learning on MetaMathQA (Yu et al., 2024). Following Chhabra et al. (2024), for extreme text summarization we use the Llama2-7B model (Touvron et al., 2023), considering both RLHF-instructed and pre-trained versions. Finally, for the divergent association task, we consider Llama3-8B (Grattafiori et al., 2024), using both DPO-tuned and pre-trained versions. We study the performances of our three methods: DiffSampling-cut; DiffSampling-lb with $p_{lb} = 0.8$; DiffSampling-reparam with $p_{lb} = 1.0, \\gamma = 1.0$. We compare them with a total of 7 baselines: greedy strategy (temperature equal to 0); contrastive search (with the number of top k samples equal to 8 and the scaling factor of the degeneration penalty $\\alpha = 0.6$); \u03b7-sampling (with \u03b7 = 0.0003); locally typical sampling (with p = 0.9); nucleus sampling (with p = 0.9); nucleus sampling with a higher temperature of 1.5 and 2.0. When not specified, temperature has been set to 1. We also experiment with different $p_{lb}$ and $\\gamma$ values; results are presented and discussed in Appendix C."}, {"title": "Math Problem Solving", "content": "Solving math problems provides a useful case study for our decoding strategies, as it allows us to evaluate the correctness of solutions (as the percentage of correctly solved problems) and the diversity of procedures to arrive at the result. To better understand whether our methods can increase diversity while maintaining accuracy we consider both the MetaMathQA training set (Yu et al., 2024) and the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) test sets; the relative prompts are reported in Appendix B. To avoid resource wasting, we focus on entries with a problem and a solution whose tokenized versions are no longer than 512. Since the training set is incredibly vast (395k entries), we limit our experiment to 1000 random samples, while we consider all 1319 entries from the GSM8K test set and all the filtered 4545 entries from the MATH test set.\nWe evaluate the quality of solutions through the ratio of correctly solved problems. Instead, the diversity is computed according to two methods: expectation-adjusted distinct N-grams (EAD) (Liu et al., 2022) and sentence embedding cosine diversity (Sent-BERT) (Hong et al., 2024), which should evaluate syntactic and semantic diversity,"}, {"title": "Extreme Summarization", "content": "Summarizing paragraphs and longer text represents another meaningful case study since the same text can be correctly outlined in different ways. To keep the resource consumption as low as possible, we consider the eXtreme Summarization (XSum) dataset (Narayan et al., 2018), which contains pairs of documents and one-sentence summaries. In particular, we use the test partition (11334 entries) and exclude all entries with a tokenized document longer than 768, obtaining 9815 entries; then, we limit our experiment to 1000 random samples, and we use the prompt suggested by Chhabra et al. (2024) and reported in Appendix B. Again, we aim to verify whether the summaries generated with DiffSampling are more diverse while maintaining a competitive quality. For diversity, we consider the same metrics presented in Section 4.2, i.e., EAD and Sent-BERT for both cross-input and against-greedy diversity. For quality assessment, we use ROUGE-1 (Lin, 2004), a standard metric for summarization that evaluates the ratio of 1-grams present in both the target and generated summaries, as well as the sentence embedding cosine similarity between the generated and target summaries. In this way, we compute both syntactic and semantic quality metrics, as a good summary might use entirely different words while still preserving the original text's meaning."}, {"title": "Divergent Association Task", "content": "The last use case considers the divergent association task (DAT) (Chen and Ding, 2023). Building on the theory that creativity is related to the capability of generating more divergent ideas (Beaty et al., 2014), it requires participants to name unrelated words. Then, the semantic distance between them can represent an objective measure of divergent thinking (Olson et al., 2021). DAT represents a useful case study for decoding strategies as it constrains the generation to different nouns (thus, assuming an optimal probability distribution, the tail due to smoothing should contain everything else) and requires generating terms that are as different as possible, which is quite the opposite to what typically happens in language modeling: an optimal strategy should exclude non-appropriate tokens but also not limit too much the space of possible tokens. More concretely, given the embeddings of n words, the DAT score is the average cosine distance between each pair of embeddings (then scaled as a percentage). Following the original paper, we use GloVe embeddings (Pennington et al., 2014) and ask the model to generate a list of 10 nouns. We discard outputs without a list of at least 7 distinct nouns, and we compute the DAT score for all other outputs over their first 7 nouns.\nWe repeat the experiment 100 times for non greedy strategies to mitigate the sampling stochasticity."}, {"title": "Conclusion", "content": "In this paper, we have presented DiffSampling, a novel family of decoding strategies based on the analysis of the next-token distribution. In particular, given the distribution sorted in descending order, we have proposed to compute the forward difference approximation of its discrete derivative and either use it to reparameterize the distribution or to remove tokens after its minimum value (possibly after imposing a total probability lower bound). In this way, DiffSampling can avoid incorrect tokens or increase the chance of low-probable but accurate words. We have experimented with three different tasks, i.e., math problem solving, extreme summarization, and the divergent association task, finding that our methods consistently perform at least as well as similar common strategies in terms of both accuracy of result and diversity of outputs.\nDespite its limitations, DiffSampling-cut has demonstrated performance better than or equal to the greedy strategy. Additionally, DiffSampling-cut offers the potential for greater diversity. Introducing a lower bound can further relax constraints. Reparameterizing the distribution trades off some accuracy for increased exploration, but it remains \"safer\" than simply increasing the temperature. Our research agenda includes exploring whether combining DiffSampling with other techniques, such as contrastive search or temperature tuning, can yield even better results. We also aim to leverage other properties of the distribution to guide text generation toward desired characteristics."}, {"title": "Limitations", "content": "The work presented in this paper has a few important limitations to highlight. Firstly, DiffSampling is merely a decoding strategy. While it can influence the accuracy and diversity of the model's outputs, it is constrained by the information learned by the model itself. For instance, if the model is biased toward certain grammatical structures, the probability mass is likely to contain only tokens that adhere to those structures. In addition, working at the decoding level means that the information stored by the model is not modified at all. While DiffSampling can potentially reduce how much a model regurgitates pre-existing text, it cannot reduce how much a model memorizes it.\nMoreover, DiffSampling is governed by two parameters: the nucleus lower bound and the reparameterization factor. We focused on three specific cases where either one or both parameters are switched off. Each of these cases has its own advantages and disadvantages concerning the exploitation and exploration of the next-token distribution. While this can guide the choice between them, there is no golden rule; users must select the most appropriate strategy on a case-by-case basis. Similarly, we did not find specific parameter values to be universally superior, and different scenarios may require users to adjust them accordingly.\nAdditionally, our experiments encompassed only three case studies. While we chose these to maximize diversity, it is difficult to estimate the actual advantage of using DiffSampling for other tasks and with different LLMs. We intend to broaden our investigation in the future, especially by incorporating models of varying sizes. At the same time, we believe that the choice of LLM per se should not change the ranking of the decoding techniques in terms of performance, given the fact that our method is based on the analysis of the token probability distribution in output from these models."}]}