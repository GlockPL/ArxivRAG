{"title": "Large Language Models as Common-Sense Heuristics", "authors": ["Andrey Borro", "Patricia J Riddle", "Michael W Barley", "Michael J Witbrock"], "abstract": "While systems designed for solving planning tasks vastly outperform Large Language Models (LLMs) in this domain, they usually discard the rich semantic information embedded within task descriptions. In contrast, LLMs possess parametrised knowledge across a wide range of topics, enabling them to leverage the natural language descriptions of planning tasks in their solutions. However, current research in this direction faces challenges in generating correct and executable plans. Furthermore, these approaches depend on the LLM to output solutions in an intermediate language, which must be translated into the representation language of the planning task. We introduce a novel planning method, which leverages the parametrised knowledge of LLMs by using their output as a heuristic for Hill-Climbing Search. This approach is further enhanced by prompting the LLM to generate a solution estimate to guide the search. Our method outperforms the task success rate of similar systems within a common household environment by 22 percentage points, with consistently executable plans. All actions are encoded in their original representation, demonstrating that strong results can be achieved without an intermediate language, thus eliminating the need for a translation step.", "sections": [{"title": "Introduction", "content": "Emergent reasoning capabilities in Large Language Models (LLMs) have prompted research into their ability to solve planning tasks and generate plans in the formal languages used to encode planning domains [Kambhampati et al., 2024a; Hao et al., 2024; Silver et al., 2024]. This research has demonstrated that LLMs have difficulty with solving even simple tasks and that recent progress, with the introduction of Large Reasoning Models (LRMs) [OpenAI, 2024], is insufficient to deal with common planning challenges, such as solving obfuscated domains or analysing the solvability of a given task. Furthermore, initial LRM success in solving small planning tasks fails to scale past toy problems [Valmeekam et al., 2024].\nThe trend in improvement shows that even a modest increase in planning ability comes at the price of a larger, hence, computationally and financially costlier, LLM. This raises questions about the practicality of LLM-based planning, especially when compared to traditional planning systems such as FastDownward [Helmert, 2011], which vastly outperform modern LLMs on planning tasks for a fraction of cost and time.\nOne advantage that LLMs have over traditional planning systems is the models' ability to store vast amounts of parametrised knowledge about a wide variety of topics and interact with the semantic information in their input text. Most planning systems make no distinction between the labels key and object1, but an LLM is likely to infer that a key could be used to unlock a door or a chest.\nResearch in utilising LLMs for planning in everyday environments, such as a common household, takes advantage of the parametrised world-knowledge of an LLM through fine-tuning existing models [Chalvatzaki et al., 2023] or through directly evaluating the pre-trained performance of LLMs on generating plans from task descriptions [Huang et al., 2022]. Both methods provide the LLM with a high-level task description (e.g., 'slice an apple') and expect a sequence of actions in return. A common challenge for this research comes from ensuring the executability of these action sequences, which require translation from a high-level language to a low-level language used by planning systems to operate within finite, discrete action spaces.\nRecent studies demonstrate that increasing the amount of information provided to the LLM alongside the task description can improve both success rates and executability. Hao et al. [2023] achieve this by providing an explicit world model, using a natural-language description of the current state, as part of the input to an LLM solving simple block manipulation tasks.\nOther research proposes to rank planning actions at each step based on a combination of the LLM's sampling probability and an affordance function; that is, a function which maps actions to a probability estimate of their executability [Ahn et al., 2022]. An affordance function can be implemented by using the Q-value function from a pre-trained model. Additionally, many other implementations are possible and can even be applied at the token level [Huang et al., 2023b].\nMost similar to our research, ProgPrompt [Singh et al.,"}, {"title": "Background", "content": "Automated Planning. Automated Planning is a branch of Artificial Intelligence primarily focused on the generation of plans. Plans are sequences of actions within a given environment that can be sequentially applied to an initial state.\nA planning task \u03a0 is a 6-tuple (S, si, Sg, A, fa, ft) consisting of:\n\u2022 A finite and discrete set of all states S, also known as the state space.\n\u2022 The initial state si \u2208 S.\n\u2022 The set of goal states Sg \u2286 S. In our tasks, the set of goal states is defined by a set of goal conditions for each task, which outline the criteria for a state to be a goal state.\n\u2022 The set of actions A.\n\u2022 The applicability function fa, which returns fa(s) \u2286 A, i.e. the set of all actions applicable at state s. In our paper, the applicability function is defined by the preconditions of our action schema, discussed in Section 3.1\n\u2022 The transition function ft, which maps a state s and applicable action a to the resultant state s' of applying a to s, such that s' = ft(s,a), a \u2208 fa(s). As above, the transition function is defined by the effects of our action schema.\nIn practice, most planning systems [Pohl, 1970; Jiang, 2021] perform graph search on a graph G = (V, E), where V is the set of vertices and E is the set of edges. Vertices are states s \u2208 S, while edges are formed from the transition function ft, where\n\u2200u, v \u2208 V, (u, v) \u2208 E \u21d0\u21d2 \u2203a \u2208 A: ft(u,a) = v.\nA solution for a given planning task is a plan a1...an such that one can form a path v0... vn through G, where vi = ft(vi\u22121, ai), v0 = si and vn = sg \u2208 Sg; that is, a plan that transforms si into sg when applied sequentially. Heuristics [Hart et al., 1968; Lenat, 1982] are often used to guide the search and reduce the number of vertices that must be visited for a solution to be found.\nGlobal search algorithms, such as A* [Russell and Norvig, 2016] or BFS [Russell and Norvig, 2016], store a 'frontier' of vertices in memory, allowing the search to backtrack or switch to a different path through the graph entirely. These search algorithms usually have guarantees on finding a path from the initial state to a goal state if one exists (completeness), and sometimes have guarantees on that path being minimal cost (optimality), depending on other factors.\nLocal search algorithms [Gerevini and Serina, 1999; Gerevini and Serina, 2002], such as Hill-Climbing [Hoffmann and Nebel, 2011], only store the current state and transform it by applying the transition function 'in-place'. The action is usually chosen by minimising or maximising across some heuristic function. These algorithms typically have no guarantees of completeness or optimality and can have difficulty with getting stuck in local minima or maxima. Our research forgoes a numeric heuristic in favour of allowing the LLM to choose the action to take at a given state.\nLarge Language Models. Large Language Models (LLMs) [Chen et al., 2021; Chang et al., 2024; Zhao et al., 2023] are very large neural networks, typically containing billions of parameters. These models are trained through unsupervised learning on vast corpora of human-written text. Prominent large language models, such as GPT-3 [Floridi and Chiriatti, 2020] and GPT-4 [OpenAI, 2023], leverage the self-attention mechanism of the Transformer [Vaswani et al., 2017] architecture to effectively process long text sequences in parallel and capture non-contiguous word dependencies.\nPrompt engineering [White et al., 2023; Schmidt et al., 2024; Shin et al., 2020] is a common approach to enhancing the reasoning performance of LLMs without fine-tuning their parameters or altering their architecture. With prompt engineering, specific prompts are crafted to influence how the model's parametrised knowledge interacts with the input text. This can involve techniques like providing worked examples, phrasing the query in a question-and-answer format, or asking the model to 'think through its answer step-by-step' [Kojima et al., 2022].\nLLMs demonstrate an ability to infer, generalise, and apply learned patterns to various scenarios [Sakaguchi et al., 2021; Madaan et al., 2022]. However, there is research that claims that LLMs show little evidence of true emergent reasoning capabilities [Schaeffer et al., 2023] and have difficulty with more complex forms of reasoning [Gendron et al., 2023]. Similarly, there are questions about the capacity of LLMs to compete with traditional planning systems [Kambhampati et al., 2024b]."}, {"title": "Methodology", "content": "A comprehensive set of all prompts, environments and experiment data, as well as the source code to replicate our experiments and important documentation for VirtualHome, can be found at the link in Section 6.\nVirtualHome Environment\nVirtualHome [Puig et al., 2018] is a simulation environment designed to model a household in 3D space. It provides an interactive platform to test and evaluate AI agents in terms of their ability to perform simple household tasks like cleaning, cooking, or fetching items. The environment uses its own internal language to represent finite, discrete, and deterministic actions. An agent can execute sequences of actions to interact with the environment.\nVirtualHome actions are comprised of an action command and an ordered list of associated objects. The length of the list is dependent on the action command in question. For example, STANDUP has no associated objects, PICKUP has one and PLACEON has two. Objects in VirtualHome have both names and IDs to avoid ambiguity around identically named objects.\nThe VirtualHome language represents actions in the following format:\n[VERB]<object\u2081>(id\u2081)...<object>(idn)\nFor example, the action to put salmon in the microwave is expressed as\n[PUTIN]<salmon>(311) <microwave>(297)\nThis low-level language is used by the VirtualHome environment to represent, understand and execute actions.\nAs most of the functionality of the VirtualHome Environment is not required for our experiments, this paper instead uses a planning domain crafted from its documentation and operating in the VirtualHome language. The action schemas for our domain are taken directly from the VirtualHome documentation. Action schemas are blueprints for actions, describing their structure, preconditions and effects. They become actions when grounded by objects from the environment.\nFor example, the action schema\n[PUTIN]<object1>(id\u2081) <object2>(id2)\nbcomes the action\n[PUTIN]<salmon>(311) <microwave>(297)\nwhen grounded by the objects\nsalmon (311), microwave (297)\nFor the purpose of this paper, object IDs are not taken into account by the agent, though they are still stored internally. As such, for the rest of this paper, we will also omit the IDs when discussing objects or actions.\nThe initial states for all tasks in this paper are derived from environment graphs used by the VirtualHome Environment, which have a one-to-one mapping with states within our environment. The environment graphs used in our paper contain around 450 distinct objects with over 14 thousand distinct relationships.\nWe extend the VirtualHome environment by providing additional functionality, such as applying\nin (microwave, object) \u2192 heated(object)\nto all objects in the state when on (microwave) becomes true; that is, anything inside a microwave becomes heated when it turns on. These extensions are made to allow the testing of more complex tasks without significant changes to the predicates or action schema.\nOverview\nFigure 1 presents a diagram of our proposed approach.\nTask Descriptions. The agent receives a high-level task description, such as 'microwave salmon'. Each task description has an associated pair of human-written goal and failure conditions. For example, 'microwave salmon' has goal conditions\nheated (salmon) \u2227on (microwave)\nand failure conditions\n\u00acheated (salmon) \u2227on (microwave)\nThis means that the task is successfully completed when the salmon is heated, and the microwave is on, and failed if the salmon is heated but the microwave is off; that is, it was heated elsewhere. The agent has no access to the goal and failure conditions, but the system simulating the environment is able to check whether the conditions are satisfied and inform the agent of success or failure accordingly.\nGuides. In Huang et al. [2022], an LLM is used to generate a high-level solution directly from a high-level task description. Rather than generating and executing such a solution directly, we propose to use it as a guide to aid an LLM in selecting actions during local search.\nThe paper explores two types of guides. High-Level Guides are represented in English like the action plans generated by Huang et al. [2022]. Low-Level Guides are also solution estimates for the planning task but are represented in the VirtualHome language. An example of the two types of guide for 'put the coffeepot and the cupcake on the coffee table' is presented in Table 1.\nPrompt Structure. Information about the agent is provided to the LLM through a constructed prompt. The prompt is composed of a high-level task description, a guide, a list of actions that the agent has already taken, and an indexed list of actions to choose from. A small hint about the environment, namely the necessity to walk to any given object before interacting with it, is also included in the prompt. All actions are represented in the VirtualHome language.\nAction Selection. It was a concern that LLMs, particularly smaller and cheaper models, could struggle with keeping track of long lists of actions. To mitigate this, we decompose the problem by partitioning the actions applicable at the current state into n sublists. The LLM is then queried iteratively with n prompts derived from these partitions and returns n"}, {"title": "Results", "content": "Experiment Setup\nOur system is tested on the 10 common household tasks used for evaluation by ProgPrompt [Singh et al., 2023]. Each task has a high-level task description, low-level goal conditions and low-level failure conditions. All three are handwritten and known by the simulator. However, only the description is visible to the agent. All of our experiments use gpt-40-mini-2024-07-18 as the strategy-generating and action-selecting LLM, which is a scaled-down and substantially cheaper variant of the GPT-4 [OpenAI, 2023] architecture.\nFor all experiments, we set the limit on maximum solution length to 20, the limit on the cumulative number of repeated queries per repetition of an experiment to 10, and the partition size for the applicable action list to 100. The parameter which regulates the spread of the distribution from which the LLM output is sampled, known as the model temperature, is set to 0.2. Values closer to zero are known to result in less creative and varied outputs, which is preferable for generating accurate solutions in a low-level language with limited vocabulary. Each experiment is repeated 50 times.\nMain Experiments\nThe success rates for our experiments, that is, the proportion of output solutions that achieve all associated goal conditions for a task, are presented in Table 2. The tasks are ordered in ascending order by the minimum length of a plan required to solve them.\nIn order to evaluate the effects of utilising a guide as a heuristic, each experiment is a combination of a task description and either a high-level guide (HLG), a low-level guide (LLG) or no guide (NG), with which to aid the LLM-driven local search.\nAs our low-level LLM-generated guides function as standalone plans for their respective tasks, we are also able to evaluate the effect of the subsequent local search on task success rates. We do this by comparing the success rate of using the generated low-level guide (G) as a solution to the success rate of our LLM-driven local search, which uses that guide as a heuristic (G + S).\nThe guide-generating LLM has less information on environment dynamics and objects within our environment compared to the action-selecting LLM, which has access to the semantic information contained within the lists of applicable actions. As such, we conduct our comparison between low-level guides and guide-assisted local search across four levels of environment knowledge. When conducting our main experiments comparing guide types (NG, LLG, HLG), the guide prompt contained no information about the environment. We extend this by testing guide generation with progressively detailed prompts: 1) containing a list of all objects in the environment; 2) containing a list of objects along with their static properties (e.g., CAN OPEN, HAS SWITCH, GRABBABLE); and 3) containing objects, their static properties, and their dynamic properties (e.g., OPEN/CLOSED, ON/OFF).\nOutcome. We conclude that our method is effective at generating plans for solving simple natural language tasks in a virtual environment, even when only leveraging the world knowledge of the LLM at the point of action selection; that is, local search with no guide. Our method's performance is improved with the addition of guides as heuristics, with a slight advantage in performance for low-level guides over high-level ones. When enhanced with a low-level guide, our LLM-aided local search achieves a success rate of 69 percent on average, 22 percentage points above the best-performing ProgPrompt setup, and 32 percentage points above ProgPrompt running on GPT-4 architecture [Singh et al., 2023]. Without a low-level guide, our method (0.41) performs slightly worse than ProgPrompt (0.47).\nThe results from comparing just the guides to local search using the guides as a heuristic show that local search has a significant impact on the success rate of our method. Across all four levels of information, local search (G+S) outperformed just the guide (G) by at least 30 percentage points. Notably, an increase in the amount of information provided to the guide-generating LLM does not close the gap between G and G+S.\nThe results show that guides generated with no environment information do a better job of guiding local search (0.69) than guides generated with a list of objects within the environment (0.57). This is roughly proportional to the decrease in the performance of the guides themselves, from 29 percent to 25 percent. The majority of this effect is due to the poor performance of local search with object-aware guides on the 'make toast' and 'bring the coffeepot and the cupcake to the coffeetable' tasks.\nAdversarial Environments. We extend our experiments on the gap between guides and guide-assisted local search to an 'adversarial' environment where the variable properties of key objects may differ from the LLM's world model. In this environment the tv is ON, and the bin, fridge and microwave are OPEN. Our results are presented in Table 3\nIn an adversarial environment, the impact of local search is even larger than in our original environment. When used directly as a solution, guides are unable to solve a single task, with the exception of guides with information on dynamic properties for 'put salmon in the fridge'. This is an interesting result, as our adversarial changes are visible to the guide-generating LLM at the \u2018dynamic properties' level of information. However, even at that level, three of the four tasks were not solved a single time. Our results show that, while the overall performance of the guide-assisted local search in an adversarial environment decreases for three of the four levels of information, it is much more resilient to the change than the guides on their own.\nExecutability Selecting actions with local search sidesteps the difficulties with executability faced by similar research. As action selection is always performed on a list of applicable actions, the final solution is always perfectly executable. Furthermore, as our approach operates directly in the VirtualHome language, we do not require a potentially error-prone translation step."}, {"title": "Error Analysis", "content": "Table 4 shows an evaluation of five modified experiments for three tasks which were particularly low-scoring in our main experiments from Table 2. Our findings suggest that errors can often occur due to discrepancies between the LLM's 'envisioned' environment and the actual environment it finds itself in. This may be due to the agent having no access to the formal action schema within its environment, to the internal state representations used by the simulator, or to the internal goal conditions for a given task.\nThrow Away Apple. When tasked with throwing away the apple, the LLM will most often direct the agent to PICKUP the apple, OPEN the bin and then DROP the apple. In a real-world environment, 'grab the apple and drop it into the bin' is not an unreasonable answer to 'how do I throw away an apple?'. However, in a strict, STRIPS-based environment, the effects of the DROP action are simple and exact. In our domain, it removes the HOLDS RH or HOLDS LH relation between the agent and the object (the object is no longer in the agent's hand). This is insufficient to satisfy the goal conditions for the task, which specify that there must be an INSIDE relation between the apple and the bin. In our domain, this relation is only achievable through the PUTIN action. Adding a small hint to the prompt ('you must put the apple in the bin') has a substantial impact on the success rate of the task. Even more substantial is simply removing the DROP action schema entirely. This suggests that the issue was not with the agent's understanding that the apple should end up in the bin but rather with its ability to understand the dynamics and limitations of its environment.\nWash the Plate. Washing the plate is a rather ambiguous task. For our experiments, the goal conditions are realised through a WASH action, which is applicable when the character is holding an object next to a sink object with the FILLED property. A sink object can acquire this property when its corresponding faucet object is switched on via a SWITCHON action. It is important to note that a strong case could be made for several other interpretations, so this is only one of many valid ways of defining a set of goal conditions for the task. However, the key issue for our agent is not the vague nature of the task description but rather the ambiguous way in which you must interact with the sink, which is a separate object from its faucet. We demonstrate this by adding a minor hint to 'turn on the faucet' to the original task description, which brings the success rate to above 50 percent.\nBrush Teeth. The agent can brush its teeth by pouring toothpaste onto the toothbrush and then using it with the USE action. In our environment, the toothpaste is CLOSED and can only be opened with one free hand while being held with the other. This is a point of difficulty for the agent, which will usually attempt to pick up both the toothbrush and the toothpaste before trying to OPEN the toothpaste. This is not an unreasonable solution, as an adult human would usually be able to open a tube of toothpaste even while holding a toothbrush. However, in our environment, it will find no applicable [OPEN]<toothpaste> action and often subsequently fail to solve the task, leading to a 10 percent success rate. We note that, when the toothpaste is always OPEN, this is a simple task"}, {"title": "Conclusions", "content": "We present an approach for leveraging the internal world knowledge and reasoning capabilities of LLMs to inform action selection when using local search. Our approach is capable of generating plans for common household tasks, and can be extended by utilising solution estimates as a further heuristic to guide the local search.\nWe show that LLM-aided local search is capable of solving simple tasks in a household environment with a comparable success rate to similar methods. Our solutions are fully executable and are generated directly in the VirtualHome language, demonstrating that strong results can be achieved without an intermediate language or a translation step.\nFurthermore, we show that enhancing the search with a solution estimate as a heuristic results in better performance than local search or the solution estimate on their own, regardless of whether the estimate is represented in a low-level or a high-level language. This increase in performance places the success rate of our approach at 22 percent higher than that of ProgPrompt on the same set of household tasks.\nWe further give clear evidence that the impact of local search cannot be replicated by providing more environment information to the LLM tasked with generating solution estimates. This impact is even more substantial in an 'adversarial' environment, which differs from the assumed world model of the LLM. This demonstrates the comparative resilience of local search to unexpected variables within its environment.\nFuture Work. Our approach leverages the LLM's internal model of a standard household environment. Lifting such an environment into a finite language representation often causes discrepancies between the LLM environment model and the environment in which the agent operates. Future work could explore methods to pass a formal description of environment rules to the LLM as part of the task input through techniques such as RAG [Lewis et al., 2020].\nOur approach will likely have difficulty with tasks that require an increased amount of reasoning skills. The tasks used in our paper mostly represent variations in moving objects around the house. The LLM is only queried for a single action at a time, which makes big-picture reasoning, such as opening toothpaste with finite hands, more difficult. It could be valuable to leverage decomposition techniques such as Chain of Thought [Wei et al., 2022] throughout parts of the search to improve performance.\nLocal search can get stuck in local maxima or through taking irreversible actions, which makes backtracking impossible. This could be mitigated with an algorithm like beam search [Russell and Norvig, 2016], which would also have the potential to increase success rates by allowing the algorithm to pick and choose from a variety of plans."}]}