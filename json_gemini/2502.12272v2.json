{"title": "Learning to Reason at the Frontier of Learnability", "authors": ["Thomas Foster", "Jakob Foerster"], "abstract": "Reinforcement learning is now widely adopted as the final stage of large language model training, especially for reasoning-style tasks such as maths problems. Typically, models attempt each question many times during a single training step and learn from their successes and failures. However, we demonstrate that throughout training with two popular algorithms (PPO and VinePPO) on two widely used datasets, many questions are either solved by all attempts-meaning they are already learned\u2014or by none\u2014providing no meaningful training signal. To address this, we adapt a method from the reinforcement learning literature-sampling for learnability and apply it to the reinforcement learning stage of LLM training. Our curriculum prioritises questions with high variance of success, i.e. those where the agent sometimes succeeds, but not always. Our findings demonstrate that this curriculum consistently boosts training speed and overall test accuracy across multiple algorithms and datasets, paving the way for more efficient and effective reinforcement learning with LLMs.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has become a crucial final step in the training pipeline of many state-of-the-art reasoning-focused large language models (LLMs), notably DeepSeek-R1 (DeepSeek-AI et al., 2025), Tulu (Lambert et al., 2025) and OpenAI's O1 (OpenAI, 2024). In each training iteration models generate tens (Huang et al., 2024), hundreds (Hu et al., 2024), or even thousands (Kazemnejad et al., 2024) of attempts for each question in the training batch. Training typically runs for hundreds of iterations over large, distributed setups (Lambert et al., 2025). This incurs significant financial and infrastructural costs, and the high energy consumption raises environmental concerns (Chakraborty, 2024).\nMost prior work on training LLMs to reason with RL lever-"}, {"title": "2. Background", "content": "We adapt a recent UED method, Sampling for Learnability (Rutherford et al., 2024a, SFL), originally developed for embodied-agent and robotics tasks, to the RL finetuning of LLMs for reasoning. Our curriculum adapts to the agent's ability during training, prioritising training on questions with high variance in returns.\nDue to the difficulty of value estimation with LLMs and the relative ease with which LLM inference can be scaled compared to scaling training, it has become popular to use a large number of rollouts per question during training. For such algorithms, like VinePPO and GRPO, the additional compute required to add SFL is minimal.\nWe evaluate our method across two RL algorithms: PPO (Schulman et al., 2017) and VinePPO (Kazemnejad et al., 2024), and two datasets of varying difficulty: the challenging MATH (Hendrycks et al., 2021) dataset and the simpler GSM8K (Cobbe et al., 2021) dataset. Our results demonstrate that our method significantly accelerates training while achieving superior final test accuracy. Furthermore, evaluations on unseen test datasets CollegeMath (Tang et al., 2024) and OlympiadBench (He et al., 2024) reveal that training with SFL not only improves in-distribution performance but also generalisation."}, {"title": "2.1. RL finetuning of language models", "content": "A Markov Decision Process (MDP) (Sutton & Barto, 2018) is defined by the tuple $(\\rho, S, A, \\mathcal{T}, R, \\gamma)$, where $\\rho$ is the starting state distribution, $S$ represents the state space, $A$ denotes the action space, $\\mathcal{T}$ is the transition function, $R$ is the reward function, and $\\gamma$ is the discount factor. Language generation is commonly formulated as a token-level MDP, where the objective is to sequentially generate text conditioned on a given prompt or query.\n\u2022 The initial state $x = [x_0, ..., x_n]$ corresponds to the input prompt or question, tokenized into n tokens.\n\u2022 Action $y_t \\in A$ corresponds to the generation of token $y_t$ at each timestep t, sampled from the model's output distribution.\n\u2022 The transition function $\\mathcal{T}( \\cdot | x_{y_{0:t-1}}, y_t) = x_{y_{0:t}}$ is deterministic and involves concatenating the generated token $y_t$ to the existing sequence $x_{y_{0:t-1}} = x_0, ..., x_n, y_0, ..., y_{t-1}$ to form $x_{y_{0:t}}$\n\u2022 The reward for generating $y_t$ at timestep t is sparse, i.e 0 for all t other than the final step in the episode, timestep T. In our setting it is binary, indicating correctness (1) or incorrectness (0) of the final complete question and generated answer xy. Typically y is set"}, {"title": "2.2. Policy gradients, PPO and VinePPO", "content": "Policy gradient methods are a family of RL algorithms that optimise Equation (1) via gradient descent. For each training step, a batch $X = [x_1, ..., x_{N_I}]$ of $N_I$ questions is sampled uniformly at random from the training set D and $L_{train}$ trajectories per question are generated. The policy gradient can be estimated as:\n$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{x \\sim X,\\\\gamma \\sim \\\\pi_{\\theta}(\\cdot|x)} [\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\\\pi_{\\theta} (y_t (y_t | x_{y_{0:t-1}}) A_t] $ (2)\nwhere $A_t = A(x_{y_{0:t-1}}, y_t)$ is the advantage function. Intuitively, the advantage function quantifies how much better generating token $y_t$ is compared to other tokens generated from state $x_{y_{0:t-1}}$. Formally, we define the value V of a state $x_{y_{0:t}}$ as the expected success rate following policy $\\pi_{\\theta}$ from $x_{y_{0:t}}$, to reach the final state $x_{y_{0:T}}$, i.e. $V(x_{y_{0:t}}) = \\mathbb{E}_{y'~\\\\pi_{\\theta}(\\cdot|x_{y_{0:t}})} R(x_{y_{0:T}})$ . The action value function Q, denotes the expected return from taking action $y_t$ in state $x_{y_{0:t-1}}$. Since the transition function is deterministic and known, $Q(x_{y_{0:t-1}}, y_t) = V(x_{y_{0:t}})$. Advantage is defined as\n$A(x_{y_{0:t-1}}, y_t) = Q(x_{y_{0:t-1}}, y_t) - V(x_{y_{0:t-1}})= V(x_{y_{0:t}}) - V(x_{y_{0:t-1}})$ (3)\nConsider a \u201clow-variance\u201d question x for which all attempts are correct, i.e V(x) = 1. The expectation over trajectories $y ~ \\\\pi_{\\theta}(x)$ can be decomposed into a nested sequence of expectations over actions:\nV(x) = 1\n= $\\mathbb{E}_{y~\\\\pi_{\\theta}(\\cdot|x)} R(x, y)$\n= $\\mathbb{E}_{y_0~\\\\pi_{\\theta}(\\cdot|x)} [... [\\mathbb{E}_{y_{0:T-1} (y_{0:T-1} | x_{y_{0:T-1}})} R(x,y)]...]$ (4)\nSince $R(xy) \\in [0,1]$ then it must be that for any partially generated solution generated by the model $x_{y_{0:t'}}$, V($x_{y_{0:t'}}$) = 1 and thus $A_{t'} = 0$. The analogous result holds for V(x) = 0 and we therefore have that for any question x that the model consistently gets right or wrong, the policy gradient $\\nabla_{\\theta} J(\\theta) = 0$, and the model does not learn.\nIn practise, the advantage function is not known, and must be estimated. This is historically done using a value network, trained to minimize the mean squared error between the predicted value and the empirical return:\n$L_v(\\phi) = \\mathbb{E}_{x~X, \\\\gamma~\\\\pi_{\\theta}(\\cdot|x)} [\\frac{1}{T} \\sum_{t=0}^T (V(x_{y_{0:t}}) - G_t)^2]$(5)\nwhere $G_t = \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}$ is the empirical return from state $x_{y_{0:t}}$. Due to approximation errors between the learned value function and the true value function, the policy gradient may still be non-zero for questions with low variance in returns.\nRecent work has shown that learning a value function in language domains can be challenging. RLOO (Ahmadian et al., 2024) abandons the value network, using the average reward of samples in the training batch as a baseline."}, {"title": "2.3. Unsupervised Environment Design, Sampling for learnability", "content": "An underspecified MDP $(\\rho, S, A, \\mathcal{T}, R, \\gamma, \\Theta)$ is an extension of the traditional MDP to include parameters $\\theta \\in \\Theta$ that parameterise the start state distribution $\\rho_{\\theta}$, the transition function $\\mathcal{T}_{\\theta}$ and the reward function $R_{\\theta}$. A given choice of $\\theta$ therefore determines a specific level or instance of the MDP. Unsupervised environment design (UED) is an auto-curricula paradigm that frames curriculum design as a two-player zero-sum game between a level-solving agent and an adversary. At each training step the adversary defines a distribution over levels $\\theta ~ \\Theta$, and the agent seeks to maximise expected return over this distribution using standard RL methods. The adversary can pursue various objectives."}, {"title": "3. Related work", "content": "Unsupervised Environment Design: Prioritised level replay (PLR) (Jiang et al., 2021) generates random levels, samples trajectories, and adds high scoring levels to a buffer. TD-error is typically used as the score function. ACCEL (Parker-Holder et al., 2023) extends this with a mechanism to mutate previously high-scoring levels, to generate new levels that train the agent on the frontier of its capabilities. PAIRED (Dennis et al., 2021) co-trains a level-selecting adversary and two agents, a protagonist and an antagonist. It aims to maximise regret by maximising the difference in performance between the protoganist and antogonist. SFL (Rutherford et al., 2024a) discards the notion of regret, instead using learnability to select which levels to replay. The majority of the previous literature on UED has applied it to embodied-agent style robotics tasks, such as Minigrid (Chevalier-Boisvert et al., 2023), XLand-Minigrid (Nikulin et al., 2024), JaxNav (Rutherford et al., 2024b) and variants OpenAI's bidpeal walker (Brockman et al., 2016).\nIn this work we explore regularly refreshing the SFL buffer and selecting which experience to train on according to learnability. This is similar to Prioritised Experience Replay (PER) (Schaul et al., 2016), with TD-error prioritisation swapped for learnability prioritisation.\nCurricula with LLMs for RL: Rho-1b (Lin et al., 2025) showed the effectiveness of training curricula during supervised training of LLMs, but there has been little work so far on curricula for RL training on LLMs. There has been some work showing that online DPO (Qi et al., 2024) suffers when the variance in samples is low (Pal et al., 2024), i.e. when preference pairs are too similar to each other. (Pal et al., 2024) apply various heuristics to improve the diversity of samples for a given question, but do not go as far as forming curricula over the training distribution. In (Havrilla et al., 2024), the authors try PLR (Jiang et al., 2021) and backtracking (Salimans & Chen, 2018) as curricula for LLM training but see no improvement over PPO.\nLeveraging additional rollouts to improve RL training of LLMs: Several RL approaches, including RLOO (Ahmadian et al., 2024), GRPO (Ramesh et al., 2024), and VinePPO (Kazemnejad et al., 2024), perform multiple rollouts per question to improve advantage estimation and stability. Additionally, techniques for training verifiers and process reward models require extensive sampling before fine-tuning with RL (Havrilla et al., 2024)(Zhang et al., 2025). Similarly, our work uses additional sampling, but does so in a unique way - to maximise the learnability of the training batch."}, {"title": "4. Learning to Reason at the Frontier of Learnability", "content": "Reasoning models, LLMs trained with RL on reasoning tasks, are emerging as the next generation of foundation models. UED methods, especially SFL, have been shown to significantly boost training performance on toy embodied-agent tasks. By combining the formalisms defined in Section 2 for RL on LLMs and SFL, we can use Algorithm 1 to teach LLMs to reason at the frontier of learnability.\nFormally, the language generation MDP is extended to use the parameter $\\theta$. Each $\\theta \\in \\Theta$ corresponds to a question from a training dataset, and the UED adversary will determine the distribution over which $\\theta$ is sampled from $\\Theta$. Given a choice of $\\theta$, the starting distribution for each level $\\rho_{\\theta}$ is just the question x, and the reward function $R_{\\theta}$ computes accuracy of the generation y. The transition function is unchanged.\nThere are some key practical differences between the LLM reasoning setting and the setting UED was originally designed for. SFL was previously employed on simple, fast, vectorised environments such as Minigrid, XLand-Minigrid and JaxNav. These environments have millions of unique levels. GSM8K, on the contrary, is just 8,000 questions. Models in the traditional UED setting are small, perhaps 1M parameters. Whilst in this work we looked at a 1B parameter model, Rho-Math-1B (Lin et al., 2025), LLMs with far more parameters have been trained with RL, such as the 671B parameter Deepseek-R1 (DeepSeek-AI et al., 2025). SFL has therefore previously been employed in settings where each training batch contains experience from 1000s of levels and 100s of episodes per level, even on a single GPU. In contrast in VinePPO, they train on a batch of just 8 rollouts for each of 64 levels.\nWhilst the increase in popularity of JAX means that some popular RL environments are now vectorised, the sampling of trajectories remains expensive and a major bottleneck for many RL tasks. In many LLM training setups however, sampling new trajectories is far more scalable than model updates, as trajectory collection can be distributed across independent processes that require no inter-process communication. This sampling computation can be fully pipelined and leverages highly optimized specialized inference engines like vLLM. Furthermore, since trajectory sampling only involves forward passes, it eliminates the need to store activations or optimizer states, making it significantly more memory-efficient than training updates.\nUsing the same notation as in Algorithm 1, the equations below give the sampling overhead for using SFL. Note that whilst SFL generates more samples per iteration, it requires no additional backward passes or gradient updates.\nOriginal samples: $T_{total} N_I. L_{train}$\nAdditonal SFL samples:  $T_{total}/T_{buffer} N L_{SFL}$ \n(with reuse of trajectories) $T_{total}/T_{buffer} (N \u2013 K) \u00b7 L_{SFL}$\nSampling overhead: $\\frac{Original + Additional samples}{Original samples}$ =  $1 + \\frac{(N-K)L_{SFL}}{T_{buffer} N_I L_{train}}$(6)\nThe popularity of using rollouts to estimate the advantage function, as in VinePPO and GRPO, mean that $L_{SFL} << L_{train}$, and this overhead is small."}, {"title": "5. Experimental setup", "content": "Hyperparameters: Our implementation of SFL is forked from VinePPO (Kazemnejad et al., 2024), and thus all non-SFL parameters can be found in the relevant configs for PPO and VinePPO at https://github.com/McGill-NLP/VinePPO. The PPO hyperparameters used by VinePPO were largely taken from (Huang et al., 2024), which provides recommendations for optimal hyperparameters for PPO and RLHF.\nUnless otherwise stated, we set T = 1, N = 256, K = 64, $N_I$ = 64, p = 1.0, $L_{SFL}$ = 8 (see Algorithm 1 for hyperparameter definitions) and reused trajectories. I.e. we refreshed the buffer at each step and used the top-K levels as the entire training batch. To compute $A_t$, VinePPO breaks down trajectories into reasoning steps (approximately 4 per question on average for GSM8K and 8 for MATH) and samples 9 further trajectories from each intermediate step. This makes $L_{train}$ = 8+8*4*9 = 296 for GSM8K and $L_{train}$ = 8+8*8*9 = 584 for MATH. By Equation (6) this gives sampling overheads of 1.08\u00d7 and 1.04\u00d7 respectively. For PPO we used $L_{train}$ = 8 to maintain comparison with VinePPO's hyperparameters, giving a sampling overhead of 4x. However, as we increase $L_{train}$ this sampling overhead decreases linearly, making it negligible for large scale experiments. We present PPO results controlled for sampling overhead in Figure 8, finding minimal benefit from training on the additional samples.\nTo consider the effect of the extra generations on the overall runtime, let $T_{gen}$ be the time taken to generate enough samples to fill up 1 training batch, and $T_{train}$ be the time taken to update the model on that batch. In our setup with VLLM on 1 GPU $T_{gen} \u2248 20s$. We are training a 7B param model on 1 x 48gb L40s GPU using CPU offloading, making $T_{train} \u2248 200s$. Using more offloading, such as ZeRO stage 3 (Rajbhandari et al., 2020) as employed by Tulu3 (Lambert et al., 2025) will further increase $T_{train}$. Being selective about which experience to train on therefore has a large impact: the time to train 1 iteration with SFL is $4 * T_{gen} + T_{train} \u2248 280s$ is \u2248 1.27\u00d7 longer than training without SFL $T_{gen} + T_{train} \u2248 220s$, whereas training all the generated samples $4 * T_{gen} + 4 * T_{train} \u2248= 880s$ is \u2248 4\u00d7 longer. For VinePPO on MATH, $T_{gen}$ is 500\u00d7 larger, so SFL increases runtime proportional to the sampling overhead of 1.04x.\nDatasets: We trained on mathematical reasoning datasets MATH (Hendrycks et al., 2021), consisting of 12,000 competition-level problems, and GSM8K (Cobbe et al., 2021), consisting of 8,000 simpler grade school problems. Both datasets are well-established and present a range of difficulty levels to be studied. We further evaluated downstream performance of the MATH trained models on CollegeMATH (Tang et al., 2024), 2,818 college-level questions, and OlympiadBench (He et al., 2024), 8,000 Olympiad level maths and physics competitions.\nMetrics: We evaluate model performance on the test sets of each dataset, using accuracy (Pass@1) as the primary metric."}, {"title": "6. Results and discussion", "content": "Figure 1 shows the test accuracy for 2 algorithms, PPO and VinePPO, when training on 2 datasets, MATH and GSM8K, with and without using SFL. In all cases, training with SFL results in a higher final test accuracy. Training speed also increases with SFL runs achieving the same test accuracy as their non-SFL counterparts in approximately 4x fewer training steps.\nFigure 4 shows models achieve a significantly higher train"}, {"title": "6.1. SFL increases training speed and test accuracy", "content": "accuracy when using SFL. It is interesting how the difference between training with/without SFL is more greatly observed in train accuracy than test accuaracy. To investigate this further, in Figure 5 we plot train accuracy vs test accuracy to compare generalisation. Despite achieving different train accuracies, each algorithm achieves roughly the same generalisation ratio. This holds especially true for GSM8K, which is surprising since it was trained for the longest time on the smallest amount of data. VinePPO perhaps has slightly better generalisation, but it's very small compared to the differences in generalisation reported in the original paper.\nFigure 3 looks at the composition of the training batch with and without SFL at the start of training. Without SFL, MATH batches have nearly 50% of questions with 0% success rate (and therefore 0 learnability) and has an average learnability of 0.07 (all questions having 50% success rate would have an average learnability of 0.25). Training on these batches wastes compute on problems that are too hard. At the start of training, GSM8K has a higher average learnability of 0.09, and the zero learnability questions are more evenly split between too-hard and too-easy. With SFL the training batch for both datasets contains far more questions between 25% and 75% success rate. Figure 1 looks at learnability scores over the course of training, and shows that for MATH, SFL is able to keep the learnability of each train batch high. On GSM8K, as the model starts to get nearer to"}, {"title": "6.2. SFL leverages scalable, fast generation to boost sample efficiency", "content": "Whilst the sampling overhead for SFL with VinePPO is only \u2248 1.06\u00d7, our implementation of PPO only used $L_{train}$ = 8 and thus the sampling overhead is 4\u00d7. Figure 8 in Appendix A shows the training performance of 3 different strategies to train on these additional samples. Brown shows the naive strategy of keeping the batch size the same, and doing 4x more model updates per iteration. This was very unstable, perhaps due to moving away from the refined set of hyperparameters we inherited from VinePPO(Kazemnejad et al., 2024). Green shows the impact of making the learning rate 4x smaller, \"sharing out\u201d the update sizes between 4x more batches. This learnt more slowly than the PPO+SFL baseline. Yellow shows the impact of increasing the effective batch size by doing 4x more gradient accumulation steps. This restored the performance of SFL, but incurred 4x larger training costs. If, like our setup and many others, generation is fast but training is slow, then this experiment shows that you can achieve significant speed ups by removing 4x the experience. Interestingly, scaling up the number of levels was more effective than scaling up the number of rollouts per level."}, {"title": "6.3. Overfitting prevents further reductions to sampling overhead", "content": "In the original SFL paper, the authors only updated the buffer every T = 50 steps, cutting the sampling overhead by 50\u00d7. To minimise overfitting to the buffer, they ensure that the"}, {"title": "7. Conclusion and Future Work", "content": "In this work, we combined teaching LLMs to reason with RL and Unsupervised Environment Design (UED). Inspired by the UED method Sampling for Learnability (SFL) from robotics reinforcement learning literature, our method prioritises training on questions with high variance in success rates, ensuring that the model receives a more informative learning signal rather than wasting compute on tasks that are either too easy or too difficult.\nBy training on 2 different datasets (MATH and GSM8K) and with 2 different algorithms (PPO and VinePPO), we demonstrated that SFL consistently accelerates training, improves in-distribution test accuracy and boosts out-of-distribution test-accuracy on unseen datasets such as CollegeMath and OlympiadBench.\nWe investigated the practical considerations of applying SFL to LLM training, including the impact of buffer refresh frequency and the trade-offs between training efficiency and sampling overhead. Our analysis highlights the importance of frequent buffer updates to prevent overfitting. Additionally, we explored the computational cost of SFL and demonstrated that, in scenarios where model updates are more computationally expensive than sample generation, selectively training on the most informative samples leads to significant efficiency gains.\nFuture work could explore extensions of SFL to other reinforcement learning algorithms, such as preference-based fine-tuning (e.g., RLHF, DPO), where it might be useful to prioritise the sampling of contrasting pairs of answers. Further investigation into how to dynamically increase $L_{SFL}$ over the course of training to keep the learnability of training batches high would also be interesting. Additionally, scaling our approach to even larger models and more diverse datasets of tasks may provide further insights into using curriculums for reinforcement learning for LLM training."}, {"title": "A. Additional plots", "content": null}, {"title": "B. Modifications to VinePPO training setup", "content": "We increased the number of gradient accumulation steps and added Deepspeed ZeRO stage 2 to allow for training on a single 48GB Nvidia L40s GPU and thus run multiple experiments in parallel on an 8 * 48GB node. This did not change the overall effective batch size of 512 episodes (64 levels * 8 rollouts per level) and thus our training dynamics are identical to VinePPO."}, {"title": "C. Examples of high and low learnability questions", "content": "\u2022 Question: Alton owns a business. He is currently renting a space that costs $20 per week. If Alton earns $8 per day, how much is his total profit every week?\nGold Solution:\n* Calculate weekly earnings: $8 \u00d7 7 = $56\n* Subtract weekly rent: $56 - $20 = $36\n* Answer: 36\nNumber of Reasoning Steps: 3\nSuccess Rate (p) at t = 0: 1.0\nLearnability (p(1 \u2212 p)) at t = 0: 9\n\u2022 Question: Mrs. Smith wanted to buy items worth $500. She went to a boutique with the $500 but realized she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?\nGold Solution:\n* Calculate additional money needed:\u00d7 500 = 200\n* Total cost before discount: $500 + $200 = $700\n* Calculate discount: 0.15 \u00d7 700 = 105\n* Subtract discount from total cost: $700-$105 = $595\n* Additional money needed: $595 - $500 = $95\n* Answer: 95\nNumber of Reasoning Steps: 6\nSuccess Rate (p) at t = 0: 0.5\nLearnability (p(1 \u2013 p)) at t = 0: 0.25\n\u2022 Question: In a fruit salad, there are raspberries, green grapes, and red grapes. There are seven more than 3 times the number of red grapes as green grapes. There are 5 fewer raspberries than green grapes. If there are 102 pieces of fruit in the salad, how many red grapes are in the salad?\nGold Solution:\n* Let G represent the number of green grapes.\n* Red grapes: 3G +7\n* Raspberries: G - 5\n* Total fruit equation: G + (3G + 7) + (G \u2013 5) = 102\n* Simplify: 5G + 2 = 102\n* Solve for G: 5G = 100 G = 20\n* Calculate red grapes: 3 \u00d7 20 + 7 = 67\n* Answer: 67\nNumber of Reasoning Steps: 10\nSuccess Rate (p) at t = 0: 0\nLearnability (p(1 \u2013 p)) at t = 0: 0\n\u2022 Question: Suppose p(x) is a monic cubic polynomial with real coefficients such that p(3 \u2013 2i) = 0 and p(0) = \u221252. Determine p(x) (in expanded form)."}], "dataset": "Dataset: GSM8K"}