{"title": "Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment", "authors": ["Andrew Konya", "Aviv Ovadya", "Kevin Feng", "Quan Ze Chen", "Lisa Schirch", "Colin Irwin", "Amy X. Zhang"], "abstract": "We introduce a method to measure the alignment between public will and language model (LM) behavior that can be applied to fine-tuning, online oversight, and pre-release safety checks. Our \u201cchain of alignment\u201d (CoA) approach produces a rule based reward (RBR) by creating model behavior rules aligned to normative objectives aligned to public will. This factoring enables a nonexpert public to directly specify their will through the normative objectives, while expert intelligence is used to figure out rules entailing model behavior that best achieves those objectives. We validate our approach by applying it across three different domains of LM prompts related to mental health. We demonstrate a public input process built on collective dialogues and bridging-based ranking that reliably produces normative objectives supported by at least 96% \u00b1 2% of the US public. We then show that rules developed by mental health experts to achieve those objectives enable a RBR that evaluates an LM response's alignment with the objectives similarly to human experts (Pearson's r = 0.841, AUC = 0.964). By measuring alignment with objectives that have near unanimous public support, these CoA RBRs provide an approximate measure of alignment between LM behavior and public will.", "sections": [{"title": "1 Introduction", "content": "Aligning the behavior of AI systems with public will can play a key role in ensuring that humanity controls its own future. But, in contrast with the broad notion of human preference [1], will specifically entails deliberately considered desires for the future expressed through voluntary action [2]. This makes sensing and encoding public will in a way that is useful for alignment a unique challenge. Leading alignment techniques involve eliciting preferences from human raters on model outputs, then fine-tuning models on those directly [3, 4, 5, 6] or via reward modeling [7, 8, 9, 10, 11]. However, these preferences sometimes just reflect superficial affinities; not will. And, even when raters intend to express their will, these preferences can conflate their prediction for a model output's impact on the future, with their will for the future. This conflation limits the effectiveness of a technique.\nFor example, consider a member of the public, Alice, who is evaluating language model (LM) responses to a user in a health crisis. Let's say Alice aims to express her will, which in this context is to maximize the user's chance of survival; that is, she prefers LM responses that she predicts will improve the user's survival odds. But, Alice's predictions may often be wrong due to missing context, unanticipated backfiring effects, her lack of medical expertise, or more. This makes the preferences she expresses based on these predictions a poor reflection of her underlying will. Moreover, it makes it harder to find common ground between Alice and fellow members of the public, since disagreements resulting from different predictions can hide agreements in underlying will [12]. We use the term normative-empirical conflation to refer to this merging of normative judgments about what 'should' be with empirically groundable predictions or evaluations (see A.7 for a more technical treatment).\nConstitutional AI [13, 14] offers a degree of normative-empirical disentanglement. The normative principles that form a constitution can be sourced directly from collective input [14], while an LM evaluates model behavior against them. But evaluating behavior against constitution principles like 'Choose the response that has the most good qualities' can itself be a normative task, which shifts norm-setting power away from the public. Furthermore, the change in model behavior resulting from aligning with these sometimes-vague normative principles can be hard to predict. In contrast, rule-based rewards (RBR) [15, 16, 17] employ precise rules that specify model behavior in well-defined ways. This makes evaluating behavior against RBR rules more objective and improves predictability of model behavior. Its tempting to gauge public will by eliciting public input directly on such rules, but this would again cause normative-empirical conflation because it integrates raters' predictions for the outcomes rules would cause with their preference for those outcomes.\nTo overcome this, we introduce a novel approach to elicit and encode public will that disentangles normative and empirical elements. We factor a model behavior specification into normative objectives and empirical rules that form a chain of alignment (CoA) between public will and model behavior:\n\u2022 Normative objectives encode the public's will for a) the outcomes model behavior should cause to happen or avoid, and b) the deontological values that should constrain how those outcomes are achieved.\n\u2022 Empirical rules specify the observable model behaviors predicted to best achieve the normative objectives.\nOur approach makes it possible to first gauge public will by eliciting public input directly on normative objectives, and then leverage the best available intelligence to develop rules predicted to achieve those objectives. By creating rules aligned to objectives aligned to public will, an RBR measuring model behavior against those rules provides an estimate of a model's alignment with public will."}, {"title": "2 Experiments", "content": "We run a CoA process to create an RBR that encodes US public will across three different domains of LM prompts related to mental health: (MH1) Informational & Non-Diagnosable Queries, (MH2) Non-urgent Mental Health Queries, and (MH3) High-risk Mental Health Queries (see A.1 for details). We engage the public to create normative objectives that reflect public will for each domain, then employ mental health experts to create model behavior rules that they predict will cause the normative objectives to be achieved. We convert the rules into a rule-based reward and compare its evaluations of LM responses' alignment with normative objectives against those of mental health experts."}, {"title": "2.1 Creating objectives aligned with public will", "content": "To create normative objectives for each domain we engaged around 600 participants representative of the US public (A.4) and 7 mental health experts. Modeled after previous work on policy development using collective dialogues and AI [18], the process went as follows:\n1. Generate: An initial set of normative objectives were synthesized by GPT-4 from statements with high max-min bridging agreement (a measure of diverse consensus\u00b9) elicited during a collective dialogue on Remesh with around 300 members of the public.\n2. Refine: The group of mental health experts refined the initial normative objectives during two hours of deliberative workshoping to produce improved versions.\n3. Vote: Public support and preference for the expert-refined normative objectives were evaluated via vote during another collective dialogue with around 300 members of the public.\n4. Ratify: Individual objectives with >75% overall support and >66% bridging support were kept and ranked by their preference scores to produce a final set of normative objectives.\nThe final sets of normative objectives contained between 5-7 good outcomes, 5-7 bad outcomes, and 5-7 values (eg. A.3). We use public support as a measure of alignment with public will. Overall US public support for each set of normative objectives ranged from 96% to 98%\u00b12%2, and the lowest support across segmentations spanning age, gender, ethnicity, religion, education, political party, HHI, AI usage frequency, and AI excitement \u2013 \"max-min bridging\" support \u2013 ranged from 92% to 96%\u00b13% (fig. 1.A). This is notably higher than the 76% US public support for a model behavior policy on mental health developed using the process that inspired ours [18]. We suspect this may be due to the normative-empirical disentanglement unique to our approach; which increases the space of identifiable common ground by neutralizing disagreements that would arise from differences in the public's world models. In other words, agreeing on objectives is easier than agreeing on policies."}, {"title": "2.2 Creating rules aligned with objectives", "content": "To create rules for each domain we engaged 7 mental health experts. The process went as follows:\n1. Generate: An initial set of rules was produced by combining rules generated in two ways: a) we used GPT-4 to generate rules based on example LM responses experts explained as aligned or misaligned with normative objectives, and b) experts were primed by rating responses to relevant prompts, then asked to give rules they thought the model should follow.\n2. Refine: The initial set of candidate rules was refined and compressed with the help of domain experts to arrive at a unique rule set for each domain.\n3. Evaluate: Each refined rule was evaluated by multiple experts who assessed if it would help, hurt, or not impact the achievement of each objective; aka, rule-objective alignment.\nThis process produced 9\u201327 rules per domain (eg. A.3). We estimate the alignment between each rule and objective as $o_{rj} = i_{rj} - d_{rj}$ where $i_{rj}$ and $d_{rj}$ are the fraction of experts assessing rule r will increase and decrease the chance of achieving objective j respectively. We estimate each rule's alignment with all objectives J (in its domain) as the average of individual objective alignments: $\\$(r, J) =< \\phi_{rj} > \\forallj \\in J$, where -1 means fully misaligned with all objectives, and 1 means fully aligned. This rule-objectives alignment ranged from 0.13-0.65 across all rules with an average of 0.35 (fig. 1.B), meaning all rules were reasonably aligned with their normative objectives."}, {"title": "2.3 Measuring alignment via rules", "content": "We convert the text-based CoA rules into a quantitative measure of an LM responses' alignment with normative objectives via a simple rule-based reward (RBR) scheme. First, a grader LM (GPT-40) assess how well LM output y in response to prompt x adheres to CoA ruler on a 5-point Likert scale. This produces a score $\\phi({x,y}, r)$ ranging from 1 = \u201cfollows\u201d to -1 = \u201cbreaks.\u201d Those scores are aggregated via weighted average across all applicable rules, using rule-objective alignments as weights, to produce a simple CoA RBR:\n$RBR(x, y) = \\frac{\\sum_{r\\in R(x)}\\varphi({x,y},r)\\phi(r, J(x))}{\\sum_{r\\in R(x)}\\phi(r, J(x))}$ (1)\nWhere $R(x)$ and $J(x)$ are the CoA rules and normative objectives for prompt x's contextual domain. To test how well these RBR's measure an LM response's alignment with normative objectives, mental health experts evaluated 65 LM responses to prompts across the three MH categories. For each response, multiple experts assessed its alignment with the appropriate set of normative objectives on a 5-point scale. The expert assessments were averaged to produce a value between -1 (misaligned) and 1 (aligned) to serve as our 'ground truth' estimate of objectives-response alignment. We found LM responses' CoA RBR value highly correlated (Pearson's r = 0.841) with their expert-assessed objectives-response alignment (fig.1.C), and able to classify objectives-response alignment as positive or negative with an AUC of 0.964. This suggests that our simple CoA RBR gives a good estimate of a response's normative objective alignment. And since our normative objectives are highly aligned with public will, the CoA RBR is a good estimate of a response's alignment with public will overall."}, {"title": "3 Implications, Limitations, and Future Work", "content": "This work introduces Chain of Alignment (CoA) as a method to measure alignment between public will and model behavior. The approach produces a rule based reward (RBR) from empirical behavior rules aligned to normative objectives aligned to public will. This normative-empirical factoring enables expert intelligence to be integrated with public will in a princlpled way. The CoA approach has a few key implications. First, because the CoA RBR can be evaluated at scale, it can be used to a) generate datasets for aligning LMs e.g. via fine-tuning, b) provide online oversight to models and agents e.g. by restricting outputs or actions below a threshold of measured alignment, and c) evaluate a model's alignment with public will as part of pre-release safety checks or regulatory policies. Second, by augmenting or replacing human experts with superhuman intelligence, the approach has the potential to work for AI systems whose behavior and impact exceeds human understanding. However, the work presented here has a range of limitations that warrant future research:\nDomains. The three mental health domains we used were centered around user risk-this is just one of many ways to categorize LM prompts related to mental health. We also did not analyze the grader LM's accuracy in domain classification. Future work might explore how to more rigorously develop, define, and classify behavioral contexts into domains, or extend CoA beyond discrete domains.\nObjectives. Our process focused on developing normative objectives that encoded shared will among the public. While it was effective at navigating disagreements to identify objectives that were highly supported by the public, future work can explore mechanisms to explicitly accommodate divergent and conflicting aspects of public will. Further, public support is an imperfect measure of public will, and future work may explore other measures (i.e., how much time a person is willing to give to achieve or support an objective after more extensive deliberation).\nRules. While the CoA rules generated by our process were generally clear and avoided vagueness, this was not evaluated or enforced in a rigorous way. Rule refinement involved compressing many rules into a set small enough to be manually evaluated by experts, where \u201csmall\u201d was determined by us. Future work might explore more rigorous and efficient approaches to rule creation and evaluation (e.g., building on inverse constitutional AI [19]).\nRule-based reward. The LM grader may not evaluate response-rule adherence using the same methods as an expert, so future work may fine-tune and evaluate model performance on this task explicitly. Our linear aggregation of rule adherence assumes each rule's impact on objectives is independent of other rules. Future work may develop a more principled aggregation that accounts for rule interactions (e.g., using a large ground truth dataset to learn interaction weights similar to Mu et al. [15]). One may even forego the legibility of rules altogether, and use an LM grader to directly evaluate model outputs against objectives similar to Constitutional AI [13]. Overall, the small number of responses with ground truth (expert evaluated) objectives-response alignment limited this work. Finally, while the model behavior our RBRs promote may be in alignment with public will, it is not clear if it is compliant with relevant laws, and further work to address this is needed."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Mental Health Subdomains", "content": "We created these mental health subdomains based on preliminary interviews with the mental health experts with which collaborated. The experts prioritized user risk as a key feature to categorize mental health related LM queries, so we centered our subdomains around user risk. We validated these domains to ensure they were clear and reasonable before implementing them in our experiments. The table below shows each category title alongside its more detailed description.\n(MH1) Informational & Non-Diagnosable Queries, (MH2) Non-urgent Mental Health Queries, and (MH3) High-risk Mental Health Queries"}, {"title": "A.2 Normative objective creation process details", "content": ""}, {"title": "A.2.1 Generating normative objectives v0 via public input", "content": "Note: This stage involved two collective dialogues with a representative sample of the US public. Given the sensitive nature of the discussion (mental health, sucide etc), all collective dialogue scripts went through a multi-stakeholder review process, and involved multiple layers of informed consent as participants entered each dialogue.\nCollective dialogue #1. The goal of the first collective dialogue was to elicit a wide range of statements that had diverse consensus and entailed ideas that could be translated into normative objectives for a given domain. To do this, we designed collective dialogue with the following structure:\n\u2022 Setup - Welcome participants, explain what they will do during the dialogue, and motivate honesty and depth by explaining the important impact their actions during the dialogue will have including details about how the data will be used.\n\u2022 Domain education \u2013 Introduce the specific behavior domain the dialogue will focus on, including a general description along with different types of cases that fit in the domian and specific chatbot examples.\n\u2022 Deliberation \u2013 Prompt participants to share and consider relevant personal experiences, underlying factors and tradeoffs that make the behavior domain tricky, and the range of outcomes that may ultimately result from chatbot behavior in the given domain.\n\u2022 Elicitation - Elicit specific good and bad outcomes participants want a chatbot to achieve or avoid, and deonotological values that should constrain how those outcomes are obtained.\n\u2022 Outro - Elicit experience evals on the dialogue itself, provide access to support materials on mental health, and thank participants for their time.\nThe elicitation stage generated more than 1000 participant submitted statements entailing good outcomes, bad outcomes, and denontological values. Around 10 participants voted on their agreement with each statement. We used elictation inference [20] to predict the missing votes, and aggregated the real and predicted for a wide range of different demographic splits spanning age, gender, religion, political party, ethnicity, education, houshold income, AI optimism, and AI usage frequency. For each demographic segment d this gave an estimated fraction of participants who agreed with each statement s of ads. We then computed the max-min bridging agreement for each statement as $\u03b1_{s} = MIN(a_{1,s}, A_{2,s}, ..., a_{2,M})$ for the set of M demographic segments. Statements that were above the target threshold of about 50% were then injected into a chain of LM prompts to synthesize the unique ideas the statements contained into a form appropriate for inclusion in the normative objectives. This output became normative objectives v0."}, {"title": "A.2.2 Refining the normative objectives", "content": "Since the VO normative objectives were raw outputs from an LM, they were sometimes imperfect in their wording or content. Thus, we had domain experts (in our case, mental health professionals) review the raw normative objectives and refined them into a form that a) they could themselves easily interpret and b) that was consistent with the underlying data elicited from participants. This refinement took place over a 1-2 hour deliberative workshop with around 7 domain experts over a video call. The output of this was the refined V1 normative objectives."}, {"title": "A.2.3 Public vote and ratification of the normative objectives", "content": "Collective dialogue #2. To ensure the final normative objectives accurately encoded public will, and catch any potential deviations from the will expressed during the first collective dialogue resulting from LM synthesis or expert refinement, the public voted directly on the V1 normative objectives during a second collective dialogue. The second dialogue was designed as follows:\n\u2022 Setup - Welcome participants, explain what they will do during the dialogue, and motivate honesty and thoughtfulness by explaining the important impact their votes will have.\n\u2022 Domain education \u2013 Introduce the specific behavior domain the dialogue will focus on, including a general description along with different types of cases that fit in the domian and specific chatbot examples.\n\u2022 All objectives eval \u2013 Introduce the normative objectives as a whole, and have participants vote their support on them as a whole.\n\u2022 Good outcomes eval \u2013 Have participants vote their support for each individual good outcome, then rank all good outcomes relative to each other.\n\u2022 Bad outcomes eval \u2013 Have participants vote their support for each individual bad outcome, then rank all bad outcomes relative to each other.\n\u2022 Values eval \u2013 Have participants vote their support for each individual value, then rank all values relative to each other.\n\u2022 Final all objectives eval \u2013 Have participants vote their support for the set of normative objectives as a whole, now that they had a chance to think through each individual objective they contain.\n\u2022 Outro - Elicit experience evals on the dialogue itself, provide access to support materials on mental health, and thank participants for their time.\nThis collective dialogue produced a support vote for each participant on each individual objective, as well as the objectives overall. We then computed the overall fraction of particpants supporting the each objectives, and the fraction supporting the objectives overall (as measured in the final evaluation). Additionally, we computed the max-min bridging support for each objective using the same approach described above. Finally, we computed a Borda-sytle preference score [21] for each ith objective as:\n$\\displaystyle U_{i} = \\frac{\\sum_{r=1}^{N} n_{ir} (N-r)}{\\sum_{r=1}^{N} N_{ir}}$ (2)\nWhere N is the number of objectives being ranked and $n_{ir}$ is the number of participants who ranked objective i as their rth choice. We then ratified the objectives whose overall and bridging support were above 75% and 66% respectively\u00b3, and ranked them by their preference score to produce the final set of normative objectives (V2)."}, {"title": "A.3 Examples", "content": ""}, {"title": "A.4 Participant distribution", "content": "Participants were sampled via Prolific [22] from demographic substrata that were calibrated to match the known distribution of US adults in terms of gender, age, and political party. This resulted in a distribution of participants that was reasonably well balanced along these dimensions, but did have skews on a few other dimensions relative to the US public, specifically:\n\u2022 Ethnicity \u2013 More white and less Hispanic\n\u2022 Education \u2013 Slightly more well educated\n\u2022 Religion - Less Protestant and more \"Other\" or Non-religious\n\u2022 Household Income \u2013 More low earners and less high earners\n\u2022 AI opinion \u2013 More optimistic towards AI"}, {"title": "A.5 Testing the effect of objectives-rule alignment weights in the RBR via ablation", "content": "Since all CoA rules produced by the process were we're assessed by experts to be positively aligned to their normative objectives, we might expect the effect of weighting by the objective-rule alignments $(\\phi(r, J))$ in the RBR to be positive but minimal. To test the effect of the different objective-rule alignment weights in the RBR, we set $\\phi(r, J) = 1 \\forall r$ to produce an ablated RBR:\n$RBR_{abl}(x, y) = \\frac{\\sum_{r \\in R} \\varphi({x,y},r)}{N_{R}}$ (3)\nWhere $N_{R}$ is the number of rules in R. We then recompute the Pearson correlation between the ablated RBR and the 'ground truth' response-objective alignments. This yields a Person's r of 0.833, which is less than the 0.842 obtained when weighting by the objective-rule alignments. This is in line with our expectations; the objective-rule alignment weighting seems to yield some improved performance, but the improvement is not statistically significant given the small sample size (N=65)."}, {"title": "A.6 Testing the usefulness of different signals derived from expert rule evaluations", "content": "During the rule evaluation step where experts evaluated objectives-rule alignment, we also collected a few additional types of expert evaluations. After presenting experts with each rule we first asked if they personally supported it, then had them evaluate the rule's alignment with each objective, and after doing that evaluation asked them how important they think the rule is. One might think their personal support for a rule would reflect their belief in its importance, but we hypothesised that asking about importance after evaluating each rule's alignment with objectives could update their views and yield a different signal.\nWe tested how each of these signals related a rules usefulness in evaluating the alignment of an LM response with the normative objectives. To do this we first computed the correlation between each rule's contribution to the RBR (ie. $\\phi(r, J)$) and ground truth objectives-response alignments. Then we compute the correlations between those values and the different expert-derived signals; personal support, objectives-rule alignment, primed importance (table 3). These result show that expert's initial personal support for rules is actually weakly negatively correlated with the rule's usefulness, while the net objectives-rule alignment, and the alignment-eval-primed importance signal had weak positive correlation. The importance signal"}, {"title": "A.7 Technical analysis of normative-empirical conflation motivating the chain of alignment", "content": "We define a person's will to be their deliberate preferences for the future that determine their voluntary actions. We denote the alignment between the will of human h and future f as $\\phi(h, f)$.\nNow consider some action a that impacts the world and changes the probability distribution of the future, like an AI model producing some output given some input. Let the probability of future f if action a is not taken be p(f) and if it is taken be p(f|a). Let $\u0394p(f|a) = p(f|a) \u2013 p(a)$. Lets model human h's perceived alignment with action a \u2014 in terms of the action's induced change in expected alignment with the future:\n$\\varphi(h, \u03b1) = \\sum_{f}(h, f) Aph (fa)$ (4)\nWhere $Aph (fa)$ reflects h's prediction for the impact of action a.\nNow consider how one might measure alignment between a group of humans H and action a: \u03c6(H, a). A common approach would be to devise a strategy to elicit \u03c6(h, a) from each human in H, then aggregate those using some social welfare function W:\n$\\varphi(\u0397, \u03b1) = W[\\{\u03c6(h1,a), \u03c6(h2, a), ...\\}] = W[\\{\u03c6(h,a)\\}_{H}]$ (5)\nFor example, choosing a simple utilitarian social welfare function, this would yield:\n$\\varphi(\u0397, \u03b1) = \\sum_{h\\in H} \u03c6(h, a) = \\sum_{h\\in H} \\sum_{f}\u03c6(h, f) \u0394ph(f|a)$ (6)\nBut the limit of this approach and others like it is that the aggregation integrates not just individual's normative wills, but also their individual predictive model; in other words, normative will (\u03c6(h, f)) is conflated with a empirically ground-able prediction (\u0394ph(f|a)). This makes such approaches ill-suited for situations where the group of humans (ie. members of the public) are unable to accurately predict the impact of actions, like the outputs of an AI assistants in tricky situations related to users mental health. Or the outputs of AI agents with superhuman intelligence. Said another way, these approaches fail for domains of actions where the true distribution of Ap(f|a) differs from the typical human's Aph (f|a).\nThe ideal approach would enable direct elicitation of individual's will for the future, then use the best available world model to determine the expected impact for any given action. If it was possible to elicit the alignment between each individual's will and every possible future, we could apply a social welfare function to those to arrive at an alignment between the collective will of the group as a whole and each possible future $\u03c6(H, f) = W[\\{\u03c6(h, f)\\}_{H}]$. Then we could use the best available world model Ap* (f|a) to predict the impact of a given action and integrate that with the collective will:\n$\\varphi(\u0397, \u03b1) = \\sum_{f} \u03c6(\u0397, f)\u0394p*(f|a)$ (7)\nBut, since enumerating and eliciting a person's will on all possible futures is not possible, this won't work. To overcome this, we can use 'objectives' as an intermediary. Let \u03c6(h, j) be the alignment between objective j and the will of human h, and let \u03c6(j, f) reflect whether future f achieves objective j such that \u03c6(h, f) can be approximated as using a set of objectives J as:\n$\\varphi(h, f) = \\sum_{j\\in J}(h, j)\u03c6(j, f)$ (8)\nAnd thus:\n$\\varphi(h, a) \u2248 \\sum_{f} \\sum_{j\\in J}(h, j)\u03c6(j, f)\u0394ph(f|a)$ (9)\nWhich can be rearranged as:\n$\\varphi(h, a) = \\sum_{j\\in J}(h, j) \\sum_{f}\u03c6(j, f)\u0394ph(f|a)$ (10)\nIf we assume the objectives are binary \u03c6(j, f) \u2208 {0, 1} then the second sum can be interpreted as the change in likelihood of achieving objective j as a result of action a:\n$\u0394ph(j|a) = \\sum_{f}\u03c6(j, f)\u0394ph(f|a)$ (11)\nSo we obtain:\n$\\varphi(h, a) \u2248 \\sum_{j\\in J}(h, j)\u0394ph(j|a)$ (12)\nNow lets consider how we might measure alignment between a group of humans H and action a. Rather than elicit and apply the social welfare function to \u03c6(h, a) which would again integrate"}, {"title": "A.8 Assessing model performance on empirical CoA tasks", "content": "The cost and availability of human experts can be limiting. But more importantly, relying exclusively on human experts renders the CoA approach ineffective for AI systems whose behavior and impact exceed human understanding. CoA's normative-emperical decoupling makes it possible to swap or augment human experts with superhuman models without sacrificing the public's agency, but when might that be appropriate? We test increasingly powerful models on the critical CoA task of evaluating how a model following a given rule is likely impact the likelihood of achieving a given normative objectives. Since this task currently lacks ground truth evaluations for the mental health domain, we assess performance by computing how consistent a set of evaluations are with human experts, and comparing that with how consistent human experts are with each other. Using the rules experts evaluated during the CoA process, we test the performance of:\n\u2022 All-aligned baseline \u2013 Since the CoA rules tend to be aligned with most objectives, assuming all rules are aligned with all objectives is a good baseline to beat.\n\u2022 Increasingly powerful LMs \u2013 We test gpt3.5-turbo, gpt4-turbo, and gpt4 class models to explore how performance scales with general model capabilities and compute.\n\u2022 Collective aggregations of experts \u2013 We test majoritarian aggregations of multiple experts, leveraging collective intelligence to create stronger baselines than the comparitive perfor-mance single experts.\nOur results (fig 4) show that gpt4 performs better than the all-aligned baseline, and about as well as one human expert, but not as well as aggregations of multiple experts. However, performance appears to scale with model capability. So if the scaling holds, it is possible that next-gen models will perform better than the aggregation of many human experts."}]}