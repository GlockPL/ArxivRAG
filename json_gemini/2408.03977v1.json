{"title": "Learning from Noisy Labels for Long-tailed Data via Optimal Transport", "authors": ["Mengting Li", "Chuang Zhu"], "abstract": "Noisy labels, which are common in real-world datasets, can significantly impair the training of deep learning models. However, recent adversarial noise-combating methods overlook the long-tailed distribution of real data, which can significantly harm the effect of denoising strategies. Meanwhile, the mismanagement of noisy labels further compromises the model's ability to handle long-tailed data. To tackle this issue, we propose a novel approach to manage data characterized by both long-tailed distributions and noisy labels. First, we introduce a loss-distance cross-selection module, which integrates class predictions and feature distributions to filter clean samples, effectively addressing uncertainties introduced by noisy labels and long-tailed distributions. Subsequently, we employ optimal transport strategies to generate pseudo-labels for the noise set in a semi-supervised training manner, enhancing pseudo-label quality while mitigating the effects of sample scarcity caused by the long-tailed distribution. We conduct experiments on both synthetic and real-world datasets, and the comprehensive experimental results demonstrate that our method surpasses current state-of-the-art methods. Our code will be available in the future.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep neural networks (DNNs) have gained remarkable achievement in computer vision tasks, such as image classification, object detection, and segmentation. The success of DNNs depends heavily on the availability of rich data. Large datasets like ImageNet [1] play an important role in the development of DNNs. However, such abundant and accurately labeled data is not always available in practice. On the one hand, noisy labels are inevitable in real-world datasets from various sources [2,3,4], which can adversely impact the performance of DNNs. Recent evidence suggests that DNNs, with their strong capacity, can easily fit noisy labels during the learning process, resulting in poor generalization performance [5]. Thus, learning robustly with noisy labels can be a challenging task. It is crucial to process noisy labels for computer vision tasks, especially for classification tasks. On the other hand, class imbalance is also prevalent in real-world settings, with head classes having numerous samples while tail classes are severely underrepresented. This distribution is known as long-tailed distribution [6,7,8]. Noisy label bias and class imbalance often coexist in real-world datasets, significantly impairing model training effectiveness.\nRecent research has separately addressed these two issues: learning with noisy labels (LNL) and long-tailed learning (LTL). Current strategies for handling noisy labels include designing noise-robust regularization techniques[9,10], developing noise-robust loss functions[11,12,13], and combining clean sample selection with semi-supervised learning[14,15]. These approaches leverage model memorization effects and the \"small loss\" principle for clean sample identification. However, these strategies may fail in the presence of long-tailed phenomena, as they may misidentify clean samples of minority classes as noisy ones due to their similar high loss values with noisy samples of majority classes [16].\nFor long-tailed learning, most studies employ methods such as re-balancing [17,18], designing distribution-robust loss functions [19,20] and decoupling representation learning and classifier learning[21,22]. However, these methods overlook noisy labels, which can adversely affect model training by accumulating errors, thereby influencing the long-tailed strategies that rely on model outputs.\nIt is evident that most of the literature addresses only one of these two problems, with effectiveness compromised when both issues are present simultaneously. Addressing label noise under a long-tailed class distribution remains an ongoing challenge. Recent research [23,24] focusing on the combined issue typically relies on traditional two-stage noise learning frameworks (sample selection and semi-supervised learning), employing strategies like reassigning weights for clean tail classes, designing loss functions robust to long-tailed distributions and noise, and using sample representations for sample selection strategies. Nevertheless, these approaches overlook the impact of long-tailed noisy data on the quality of pseudo-labels during the semi-supervised learning phase, and filtering strategies during the sample selection phase remain singular.\nThis article addresses the core issue of noisy-label image classification learning in long-tailed data distributions. To overcome limitations observed in previous studies, we propose the novel OTLNL (Optimal Transport Learning from Noisy Labels) approach. Initially, our loss-distance cross-selection module combines model predictions of sample class with sample feature distributions to filter clean samples effectively, thereby mitigating uncertainties arising from noisy labels and long-tailed distributions using a unified metric. Specifically, we employ dynamic class-specific threshold strategies to filter preliminary clean samples, thereby avoiding the overlap in losses between noisy head class samples and clean tail class samples within long-tailed distributions. Subsequently, utilizing these selected sample features, we compute class centroids, where the distance between class centroids and sample features determines sample cleanliness, reducing the impact of noisy labels on filtering accurate clean and noisy sets.\nNext, during the semi-supervised training phase, clean sets are denoted as labeled sets and noisy sets discard labels to form unlabeled sets. We employ optimal transport strategies for generating pseudo-labels for the noisy sets, en-"}, {"title": "2 Related work", "content": "Long-tailed learning. Most studies in LTL utilize three main types of methods: (1) Re-balancing[17,18]: strategies such as resampling to balance the class distribution of the input samples; (2) Designing distribution-robust loss functions[19,20]: designing loss functions specifically for long-tailed distributions by reweighting classes within the loss function; (3) Decoupling representation learning and classifier learning[21,22]: utilizing all samples for representation learning and only the balanced samples for classifier learning. These approaches leverage the rich information of representation training while avoiding the detrimental effects of imbalanced distributions on the classifier. However, these methods generally overlook noisy labels, which can adversely affect model training by accumulating errors, thereby influencing the various long-tailed strategies that rely on model outputs. Consequently, they fail to generalize to noisy datasets.\nLearning from noisy labels. Noisy label learning is significant in real-world applications, and recent research primarily falls into three categories: (1) Designing noise-robust regularization techniques[9,10]: to prevent overfitting during model training by designing regularization techniques that enhance robustness to label noise. (2) Developing noise-robust loss functions[11,12,13]: designing loss functions that are robust to noisy samples to enhance the model's ability to learn from clean samples. (3) Combining clean sample selection with semi-supervised learning[14,15]: these methods typically follow a two-stage framework. The first stage involves selecting clean samples from noisy data, and the second stage uses the clean samples as the labeled set and the noisy samples as the unlabeled set for semi-supervised training. The latest research adopts this two-stage sample"}, {"title": "3 Method", "content": "3.1 Prelimilaries\nThis paper considers a typical supervised learning task, a K-class image classification problem. Assume that we are provided with a training dataset $D = {(x_i, y_i)}_{i=1}^N$ of size N, where $x_i \\in X$ is the i-th image and $y_i \\in Y$ is its corresponding label. X denotes the data feature space, and Y denotes the label space over K classes. Specifically, y refers to the image's observed label. y might be different from the true label because noise can appear in the annotation process in a noisy label scenario. We denote $y^* \\in Y^*$as the true label over K classes. Samples of class k are denoted as $D_k$, where $k \\in {1,2,..., K}$. As this paper is concerned with the problem of long-tailed distribution and noisy labels, we define imbalance ratio as $\\rho = \\frac{\\max_k |D_k|}{\\min_k |D_k|}$ and the fraction of incorrectly labeled training samples as noise ratio $\\gamma$.\n3.2 basic idea\nWe propose a novel approach for handling data with long-tailed distributions and noisy labels. Following the two-stage process of previous noise-robust methods, we design a dynamic loss-distance cross-selection module and an optimization"}, {"title": "3.3 Dynamic loss-distance selection", "content": "The discovery of the memorization effect [5] in Deep Neural Networks (DNNs) has had a profound impact on the field of noisy label learning. Specifically, DNNs"}, {"title": "3.4 Optimization denoising module", "content": "During the semi-supervised denoising phase, the selected clean sample set serves as the labeled set, while the noisy samples are treated as the unlabeled set. Training is conducted using a semi-supervised framework, where the quality of pseudo-labels for the unlabeled set is crucial for the model's performance. In cross-domain studies, the optimal transport strategy is used to align inter-domain or intra-domain distributions [29,30]. The objective of this strategy is to find an optimal transport plan for two distributions (given a transport function), which minimizes the transport loss. Inspired by this, in our study, the clean labeled sample set can be treated as the target set, and the unlabeled noisy set as the source set for optimal transport, thereby assigning higher-quality pseudo-labels to the unlabeled noisy set. For each unlabeled sample $x_i$, we create two distinct views using weak and strong augmentations, denoted as $x^w_i$ and $x^s_i$ respectively. For weakly supervised images with labels, we establish a transport plan\n$\\Psi^* = \\arg \\min_{\\Psi} \\langle \\Psi, C^w \\rangle_F,$\nwhere $\\Psi_{i,j}$ stands for the transport plan between the i-th labeled clean sample and the j-th unlabeled noisy sample, and $\\langle ., .\\rangle_F$ represents Frobenius inner product. $C^w$ represents a cost matrix for indicating each transportation.\nHowever, considering the potential long-tailed distribution within the target set, we replace the sample features in the target set with class centroids to mitigate the negative impact of sample scarcity in some classes on the transport plan. Therefore, the transport cost matrix can be expressed as follows:\n$C^w_{i,j} = 1 - \\check{c_i} \\cdot f^w_j,$\nwhere $\\check{c_i}$ represents the class centroid of the i-th labeled clean sample and $f^w_j$ represents the feature representations of the j-th unlabeled noisy sample.\nMinimizing the transport loss yields an optimal transport plan that better aligns the distribution of the pseudo-labeled noisy set with the labeled clean set. To further improve the quality of pseudo-labels, we incorporate both the model's predicted pseudo-labels and the optimal transport pseudo-labels into the final pseudo-label selection. Specifically, the pseudo-label generation strategy is applied for the unlabeled set, as follows in Formula 9.\n$\\hat{y}_i^w = \\begin{cases}  \\arg \\max(p_i^w(t)), \\qquad p_i^w(t) \\geq \\Tau_1 \\\\ Y_{ot_i}, \\qquad otherwise  \\end{cases},$ \nwhere $\\Tau_1$ is the confidence threshold for using a pseudo-label supplementation strategy.\nWhen the distribution of the noisy set images is close to that of the clean set images, utilizing the optimal transport strategy can better align the distribution of the unlabeled noisy set with the labeled clean set, thereby assigning more accurate and higher-quality pseudo-labels to the unlabeled set. For the strongly augmented version of the same image set, applying the optimal transport plan"}, {"title": "4 Experiment", "content": "4.1 Datasets and implement details\nDataset. We conduct experiments on three datasets: CIFAR-10, CIFAR-100, and WebVision. For synthetic datasets, following previous works [23], we introduce noise into the training sets of CIFAR-10 and CIFAR-100 and perform long-tailed resampling. Specifically, generating noisy datasets with long-tailed distributions involves two parameters: the imbalance ratio $\\rho$ and the noise ratio $\\gamma$. We set the number of samples for the k-th class to $N_k = \\frac{N}{\\sum_{k=1}^K \\rho^{K-1}}$. The added noise is generated based on the noise transition matrix T, defined as:\n$T_{ij} = P(Y = j \\vert Y^* = i) = \\begin{cases}  1-\\gamma  \\qquad if \\ i = j \\\\  \\frac{\\gamma}{N-1}  \\qquad otherwise.  \\end{cases}$\nFor real-world datasets, we choose WebVision, a large-scale dataset containing 1000 classes and 2.4 million images. The training set of WebVision exhibits both long-tailed distribution and noisy labels. For experimental testing, we follow previous work [14] and use only the first 50 classes of the Google subset for our experiments. Additionally, to further validate the effectiveness of various methods in addressing long-tailed issues, we set two additional imbalance ratios by resampling to emphasize the long-tailed distribution, specifically setting $\\rho$ to 50 and 100.\nImplementation Details. In the experiments on CIFAR-10, for fair comparisons, all methods use an 18-layer PreAct ResNet [31] as the backbone and are trained with one NVIDIA GeForce RTX 3090. We train the network using SGD for 200 epochs with a momentum of 0.9, a weight decay of $5 \\times 10^{-4}$, and a batch size of 128. The initial learning rate is set as 0.02 and decreases by a factor of 10 after 150 epochs. As previously done in work [14], three imbalance ratios $\\rho$ are adopted: 10, 50 and 100. Two noise ratios are chosen: 20% and 50%. For Webvision, following past experimental settings[23], we use the Inception-ResNet v2 [32] architecture and train it using SGD for 100 epochs with a momentum of 0.9, a weight decay of $1 \\times 10^{-3}$, a batch size of 32. The initial learning rate is set as 0.01 and decreases by a factor of 10 after 50 epochs. The model is trained with two NVIDIA GeForce RTX 3090. For hyperparameters, $\\Tau_1$ is set for 0.7, $\\lambda$ is set for 0.99, $\\Alpha_{sw}$ is set for 0.2 and $\\Alpha_c$ is set for 0.1."}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "We compare the performance of OTLNL with recent state-of-the-art methods, including three categories: (1) Long-tailed learning (LTL): BBN [18], CRT [20]; (2) Learning from noisy labels (LNL): ELR+ [9], DivideMix [14]; (3) Learning with long-tailed noisy data: MW-Net [33], HAR [25], ROLT+ [26], and PCL [26]. Results for these techniques are from SFA [23]. Table 1 summarizes the experimental results of various methods under different noise and imbalance ratios. We record the best test accuracy over all training epochs and the accuracy at the end of the training. It can be seen that the performance of LNL methods significantly declines under high imbalance settings, indicating that these methods do not account for the adverse effects of long-tailed distributions on denoising strategies. Meanwhile, the performance of LTL methods worsens as the noise ratio increases, indicating an inability to handle the detrimental impact of noisy labels on the training process. However, our framework outperforms other methods in all experimental settings, especially under high noise and high imbalance ratios, due to our class-specific dynamic selection strategy and optimal denoising module. Specifically, in the CIFAR-10 dataset, our method outperforms state-of-the-art methods by 6.53% at 0.5 noise ratio and 100 imbalance ratio. In the CIFAR-100 dataset, it shows a 3.63% improvement at the same ratio. In other settings, we can also observe more than a 1.22% improvement."}, {"title": "4.3 Ablation Study and Further Analysis", "content": "Ablation Study. We verify the contribution of each module to the success of our method by removing key modules, with the results presented in Table 3. For the class-specific dynamic threshold module (CDT), we replace the dynamic threshold strategy with a fixed unified threshold for selecting clean samples for centroid calculation. The results indicate that the dynamic threshold strategy effectively selects a highly pure small clean set, aiding accurate centroid calculation and enhancing the final clean and noisy set selection. The replacement of the threshold results in a 1.72% drop in test accuracy under 0.5 noise ratio and 100 imbalance ratio.\nFor the loss-distance cross-selection module(LCS), we replace the entire loss-distance cross-selection module with a time-varying unified threshold. The experimental results highlight the importance of the class dynamic threshold for tail class sample selection and the effectiveness of combined filtering of noisy samples in the feature and label spaces, avoiding incorrect selection for tail classes"}, {"title": "5 Conclusion", "content": "To address the concurrent challenges of noisy labels and long-tailed distributions, our work introduces an innovative method rooted in optimal transport theory. Firstly, we propose a dynamic loss-distance cross-selection module, for the identification and filtering of noisy samples. Tailored to accommodate the charac-"}]}