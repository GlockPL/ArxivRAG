{"title": "Learning from Noisy Labels for Long-tailed Data via Optimal Transport", "authors": ["Mengting Li", "Chuang Zhu"], "abstract": "Noisy labels, which are common in real-world datasets, can significantly impair the training of deep learning models. However, recent adversarial noise-combating methods overlook the long-tailed distribution of real data, which can significantly harm the effect of denoising strategies. Meanwhile, the mismanagement of noisy labels further compromises the model's ability to handle long-tailed data. To tackle this issue, we propose a novel approach to manage data characterized by both long-tailed distributions and noisy labels. First, we introduce a loss-distance cross-selection module, which integrates class predictions and feature distributions to filter clean samples, effectively addressing uncertainties introduced by noisy labels and long-tailed distributions. Subsequently, we employ optimal transport strategies to generate pseudo-labels for the noise set in a semi-supervised training manner, enhancing pseudo-label quality while mitigating the effects of sample scarcity caused by the long-tailed distribution. We conduct experiments on both synthetic and real-world datasets, and the comprehensive experimental results demonstrate that our method surpasses current state-of-the-art methods. Our code will be available in the future.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep neural networks (DNNs) have gained remarkable achieve-ment in computer vision tasks, such as image classification, object detection, and segmentation. The success of DNNs depends heavily on the availability of rich data. Large datasets like ImageNet [1] play an important role in the de-velopment of DNNs. However, such abundant and accurately labeled data is not always available in practice. On the one hand, noisy labels are inevitable in real-world datasets from various sources [2,3,4], which can adversely impact the performance of DNNs. Recent evidence suggests that DNNs, with their strong capacity, can easily fit noisy labels during the learning process, resulting in poor generalization performance [5]. Thus, learning robustly with noisy labels can be a challenging task. It is crucial to process noisy labels for computer vision tasks, especially for classification tasks. On the other hand, class imbalance is also prevalent in real-world settings, with head classes having numerous samples while tail classes are severely underrepresented. This distribution is known as long-tailed distribution [6,7,8]. Noisy label bias and class imbalance often coexist in real-world datasets, significantly impairing model training effectiveness.\nRecent research has separately addressed these two issues: learning with noisy labels (LNL) and long-tailed learning (LTL). Current strategies for handling noisy labels include designing noise-robust regularization techniques[9,10], de-veloping noise-robust loss functions[11,12,13], and combining clean sample se-lection with semi-supervised learning[14,15]. These approaches leverage model memorization effects and the \"small loss\" principle for clean sample identifica-tion. However, these strategies may fail in the presence of long-tailed phenomena, as they may misidentify clean samples of minority classes as noisy ones due to their similar high loss values with noisy samples of majority classes [16].\nFor long-tailed learning, most studies employ methods such as re-balancing [17,18], designing distribution-robust loss functions [19,20] and decoupling repre-sentation learning and classifier learning[21,22]. However, these methods overlook noisy labels, which can adversely affect model training by accumulating errors, thereby influencing the long-tailed strategies that rely on model outputs.\nIt is evident that most of the literature addresses only one of these two problems, with effectiveness compromised when both issues are present simulta-neously. Addressing label noise under a long-tailed class distribution remains an ongoing challenge. Recent research [23,24] focusing on the combined issue typi-cally relies on traditional two-stage noise learning frameworks (sample selection and semi-supervised learning), employing strategies like reassigning weights for clean tail classes, designing loss functions robust to long-tailed distributions and noise, and using sample representations for sample selection strategies. Never-theless, these approaches overlook the impact of long-tailed noisy data on the quality of pseudo-labels during the semi-supervised learning phase, and filtering strategies during the sample selection phase remain singular.\nThis article addresses the core issue of noisy-label image classification learn-ing in long-tailed data distributions. To overcome limitations observed in pre-vious studies, we propose the novel OTLNL (Optimal Transport Learning from Noisy Labels) approach. Initially, our loss-distance cross-selection module com-bines model predictions of sample class with sample feature distributions to filter clean samples effectively, thereby mitigating uncertainties arising from noisy la-bels and long-tailed distributions using a unified metric. Specifically, we employ dynamic class-specific threshold strategies to filter preliminary clean samples, thereby avoiding the overlap in losses between noisy head class samples and clean tail class samples within long-tailed distributions. Subsequently, utilizing these selected sample features, we compute class centroids, where the distance between class centroids and sample features determines sample cleanliness, re-ducing the impact of noisy labels on filtering accurate clean and noisy sets.\nNext, during the semi-supervised training phase, clean sets are denoted as labeled sets and noisy sets discard labels to form unlabeled sets. We employ optimal transport strategies for generating pseudo-labels for the noisy sets, en-"}, {"title": "3 Method", "content": "This paper considers a typical supervised learning task, a K-class image clas-sification problem. Assume that we are provided with a training dataset $D = {(x_i, y_i)}_1^N$ of size N, where $x_i \\in X$ is the ith image and $y_i \\in Y$ is its corre-sponding label. X denotes the data feature space, and Y denotes the label space over K classes. Specifically, y refers to the image's observed label. y might be different from the true label because noise can appear in the annotation process in a noisy label scenario. We denote $y^* \\in Y^*$as the true label over K classes. Samples of class k are denoted as $D_k$, where $k \\in {1,2,..., K}$. As this paper is concerned with the problem of long-tailed distribution and noisy labels, we define imbalance ratio as $\\rho = \\frac{\\max_k |D_k|}{\\min_k |D_k|}$ and the fraction of incorrectly labeled training samples as noise ratio $\\gamma$.\nWe propose a novel approach for handling data with long-tailed distributions and noisy labels. Following the two-stage process of previous noise-robust methods, we design a dynamic loss-distance cross-selection module and an optimization"}, {"title": "3.2 basic idea", "content": "denoising module. In the sample selection phase, we integrate the principles of \"small loss\" and \"small distance\" to dynamically filter clean samples in both label and feature spaces, tailored for the characteristics of long-tailed distributions. Subsequently, in the semi-supervised denoising phase, we refine pseudo-label generation for the unlabeled sample set using optimal transport strategies, while employing contrastive loss to enhance feature learning. The overall framework of our approach is illustrated in Fig. 1."}, {"title": "3.3 Dynamic loss-distance selection", "content": "The discovery of the memorization effect [5] in Deep Neural Networks (DNNs) has had a profound impact on the field of noisy label learning. Specifically, DNNs"}, {"title": "3.4 Optimization denoising module", "content": "During the semi-supervised denoising phase, the selected clean sample set serves as the labeled set, while the noisy samples are treated as the unlabeled set. Training is conducted using a semi-supervised framework, where the quality of pseudo-labels for the unlabeled set is crucial for the model's performance. In cross-domain studies, the optimal transport strategy is used to align inter-domain or intra-domain distributions [29,30]. The objective of this strategy is to find an optimal transport plan for two distributions (given a transport function), which minimizes the transport loss. Inspired by this, in our study, the clean labeled sample set can be treated as the target set, and the unlabeled noisy set as the source set for optimal transport, thereby assigning higher-quality pseudo-labels to the unlabeled noisy set. For each unlabeled sample $x_i$, we create two distinct views using weak and strong augmentations, denoted as $x_i^w$ and $\\hat{x_i}$ respectively. For weakly supervised images with labels, we establish a transport plan\n$\\Gamma^* = \\mathop{\\text{arg}\\text{min}}_\\Gamma <\\Gamma, C^w>_F,$\nwhere $\\Gamma_{i,j}$ stands for the transport plan between the i-th labeled clean sample and the j-th unlabeled noisy sample, and $<?, .>_F$ represents Frobenius inner product. $C^w$ represents a cost matrix for indicating each transportation.\nHowever, considering the potential long-tailed distribution within the target set, we replace the sample features in the target set with class centroids to mitigate the negative impact of sample scarcity in some classes on the transport plan. Therefore, the transport cost matrix can be expressed as follows:\n$C_{i,j}^w = 1 - c_i^Tf_j,$\nwhere $c_i$ represents the class centroid of the i-th labeled clean sample and $f_j$ represents the feature representations of the j-th unlabeled noisy sample.\nMinimizing the transport loss yields an optimal transport plan that better aligns the distribution of the pseudo-labeled noisy set with the labeled clean set. To further improve the quality of pseudo-labels, we incorporate both the model's predicted pseudo-labels and the optimal transport pseudo-labels into the final pseudo-label selection. Specifically, the pseudo-label generation strategy is applied for the unlabeled set, as follows in Formula 9.\n$\\tilde{Y}_i^w = \\begin{cases} \\arg\\max(p_i^w(t)), & p_i^w(t) \\geq \\tau_1 \\\nY_o^i, & \\text{otherwise} \\end{cases},$\nwhere $T_1$ is the confidence threshold for using a pseudo-label supplementation strategy.\nWhen the distribution of the noisy set images is close to that of the clean set images, utilizing the optimal transport strategy can better align the distribution of the unlabeled noisy set with the labeled clean set, thereby assigning more accurate and higher-quality pseudo-labels to the unlabeled set. For the strongly augmented version of the same image set, applying the optimal transport plan"}, {"title": "4 Experiment", "content": "Dataset. We conduct experiments on three datasets: CIFAR-10, CIFAR-100, and WebVision. For synthetic datasets, following previous works [23], we in-troduce noise into the training sets of CIFAR-10 and CIFAR-100 and perform long-tailed resampling. Specifically, generating noisy datasets with long-tailed distributions involves two parameters: the imbalance ratio $\\rho$ and the noise ratio $\\gamma$. We set the number of samples for the k-th class to $N_k = \\frac{N}{\\sum_{k=1}^K \\rho^{\\frac{k-1}{K-1}}}$. The added noise is generated based on the noise transition matrix T, defined as:\n$T_{ij} = P(Y = j | Y^* = i) =\\begin{cases} 1-\\gamma & \\text{if } i=j \\\\ \\frac{\\gamma}{N-1} & \\text{otherwise} \\end{cases}.$\nFor real-world datasets, we choose WebVision, a large-scale dataset contain-ing 1000 classes and 2.4 million images. The training set of WebVision exhibits both long-tailed distribution and noisy labels. For experimental testing, we fol-low previous work [14] and use only the first 50 classes of the Google subset for our experiments. Additionally, to further validate the effectiveness of various methods in addressing long-tailed issues, we set two additional imbalance ratios by resampling to emphasize the long-tailed distribution, specifically setting p to 50 and 100."}, {"title": "4.1 Datasets and implement details", "content": "Implementation Details. In the experiments on CIFAR-10, for fair com-parisons, all methods use an 18-layer PreAct ResNet [31] as the backbone and are trained with one NVIDIA GeForce RTX 3090. We train the network using SGD for 200 epochs with a momentum of 0.9, a weight decay of 5 \u00d7 10-4, and a batch size of 128. The initial learning rate is set as 0.02 and decreases by a factor of 10 after 150 epochs. As previously done in work [14], three imbalance ratios p are adopted: 10, 50 and 100. Two noise ratios are chosen: 20% and 50%. For We-bvision, following past experimental settings[23], we use the Inception-ResNet v2 [32] architecture and train it using SGD for 100 epochs with a momentum of 0.9, a weight decay of 1 \u00d7 10-3, a batch size of 32. The initial learning rate is set as 0.01 and decreases by a factor of 10 after 50 epochs. The model is trained with two NVIDIA GeForce RTX 3090. For hyperparameters, 71 is set for 0.7, \u03bb is set for 0.99, Asw is set for 0.2 and Ae is set for 0.1."}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "We compare the performance of OTLNL with recent state-of-the-art methods, including three categories: (1) Long-tailed learning (LTL): BBN [18], CRT [20]; (2) Learning from noisy labels (LNL): ELR+ [9], DivideMix [14]; (3) Learning with long-tailed noisy data: MW-Net [33], HAR [25], ROLT+ [26], and PCL [26]. Results for these techniques are from SFA [23]. Table 1 summarizes the exper-imental results of various methods under different noise and imbalance ratios. We record the best test accuracy over all training epochs and the accuracy at the end of the training. It can be seen that the performance of LNL methods sig-nificantly declines under high imbalance settings, indicating that these methods do not account for the adverse effects of long-tailed distributions on denoising strategies. Meanwhile, the performance of LTL methods worsens as the noise ratio increases, indicating an inability to handle the detrimental impact of noisy labels on the training process. However, our framework outperforms other meth-ods in all experimental settings, especially under high noise and high imbalance ratios, due to our class-specific dynamic selection strategy and optimal denoising module. Specifically, in the CIFAR-10 dataset, our method outperforms state-of-the-art methods by 6.53% at 0.5 noise ratio and 100 imbalance ratio. In the CIFAR-100 dataset, it shows a 3.63% improvement at the same ratio. In other settings, we can also observe more than a 1.22% improvement."}, {"title": "4.3 Ablation Study and Further Analysis", "content": "Ablation Study. We verify the contribution of each module to the success of our method by removing key modules, with the results presented in Table 3. For the class-specific dynamic threshold module (CDT), we replace the dynamic threshold strategy with a fixed unified threshold for selecting clean samples for centroid calculation. The results indicate that the dynamic threshold strategy effectively selects a highly pure small clean set, aiding accurate centroid calcu-lation and enhancing the final clean and noisy set selection. The replacement of the threshold results in a 1.72% drop in test accuracy under 0.5 noise ratio and 100 imbalance ratio.\nFor the loss-distance cross-selection module(LCS), we replace the entire loss-distance cross-selection module with a time-varying unified threshold. The exper-imental results highlight the importance of the class dynamic threshold for tail class sample selection and the effectiveness of combined filtering of noisy sam-ples in the feature and label spaces, avoiding incorrect selection for tail classes caused by a single metric. Removing this module results in a 2.25% drop in test accuracy.\nFor the optimal denoising module, we remove the optimal transport strat-egy for pseudo-label prediction(OTP), using only the model-predicted proba-bility output for pseudo-label generation. The table shows that the quality of pseudo-labels significantly impacts the experimental results, with the optimal transport strategy playing a crucial role in refining pseudo-labels for the noisy set during semi-supervised training. The reduction in accuracy indicates that the optimal transport strategy of pseudo-labels provides a 4.15% improvement for our method. In summary, each of the key modules mentioned above contributes to an increase in accuracy, but the optimal denoising module plays the most significant role, providing the greatest performance enhancement."}, {"title": "5 Conclusion", "content": "To address the concurrent challenges of noisy labels and long-tailed distributions, our work introduces an innovative method rooted in optimal transport theory. Firstly, we propose a dynamic loss-distance cross-selection module, for the iden-tification and filtering of noisy samples. Tailored to accommodate the charac-"}]}