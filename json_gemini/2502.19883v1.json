{"title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models", "authors": ["Sibo Yi", "Tianshuo Cong", "Xinlei He", "Qi Li", "Jiaxing Song"], "abstract": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs). To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts. To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as Chat-GPT (Brown et al., 2020; Ouyang et al., 2022a; Achiam et al., 2023) and Llama series (Touvron et al., 2023a; Dubey et al., 2024), have demonstrated revolutionary performance in a spectrum of text generation tasks. As a fundamental principle for guiding the development of LLMs, the scaling law (Kaplan et al., 2020) highlights the strong correlation between the performance and scale of LLMs. However, as LLMs evolve to encompass hundreds and even thousands of billions of parameters, their development imposes expensive demands on computational resources and high-quality data for pre-training. Consequently, LLMs are typically confined to deployment on GPU clusters and cloud environments, posing significant challenges for wide adoption on edge devices such as smartphones, laptops, autonomous vehicles, and wearables.\nRecently, small language models (SLMs) have attracted significant attention from the academic community for their efficiency and remarkable performance in various tasks (Lu et al., 2024; Nguyen et al., 2024). On platforms like Hugging Face, SLM collections such as Llama-3.2 (Dubey et al., 2024), MiniCPM (Hu et al., 2024) and Phi (Abdin et al., 2024) have gained considerable popularity among researchers and achieved top-tier download rates. Different from LLMs, SLMs typically consist of only a few billion parameters, requiring significantly less training data and computational cost for deployment.\nHowever, unlike LLMs that benefit from extensive datasets and robust alignment strategies, it is challenging for SLMs to balance between generation capabilities and security, which makes them more vulnerable to jailbreak attacks. Among these, one of the most serious threats is referred to as jailbreak. By creating malicious prompts to induce target LLMs to generate harmful responses, jailbreak has emerged as a critical security concern in the development of LLMs (Yi et al., 2024; Yao et al., 2024; Gupta et al., 2023). Moreover, certain jailbreak techniques that can bypass the security boundary of LLMs have demonstrated strong transferability to other models (Zou et al., 2023), which presents potential threats to all generative models including SLMs.\nAlthough security concerns regarding SLMs have become an increasingly important issue, there still remains a substantial gap in exploring and understanding the security boundary of SLMs. In this paper, we collect representative malicious datasets, jailbreak attack methods, and defense methods to conduct adversarial experiments on numerous state-of-the-art SLMs, thereby revealing existing security vulnerabilities of SLMs and exploring corresponding mitigation strategies. Furthermore, we take insight into the security degradation of SLMs and discuss some potential factors. In summary, we make the following contributions:\n\u2022 We conduct extensive experiments to reveal the security vulnerabilities of SLMs under different jailbreak attacks. Especially, The results demonstrate that most SLMs are more susceptible to jailbreak attacks compared to LLMs.\n\u2022 We evaluate the effectiveness of existing defense methods on SLMs. The results show that these methods are significantly adapted to SLMs to enhance their resilience against jailbreak attacks.\n\u2022 We discuss and analyze various underlying factors that may lead to the security degradation of SLMs, including inadequate safety alignment, biased knowledge distillation, parameter sharing, and quantization techniques."}, {"title": "Related Work", "content": "2.1 Jailbreak Attacks\nJailbreak attacks, which transform harmful queries like \"How to make a bomb\" into more sophisticated prompts to deceive target models to generate toxic output, can be mainly classified into two categories: white-box methods and black-box methods.\nWhite-box methods generally rely on access to the internal states of LLMs to design attack strategies. These methods generally use gradients and logits of target LLMs as loss functions to optimize adversarial suffixes appended to malicious questions (Zou et al., 2023; Jones et al., 2023; Zhu et al., 2023; Andriushchenko et al., 2024; Geisler et al., 2024; Mangaokar et al., 2024), or manipulate the output logits to enforce target LLMs to generate affirmative responses (Huang et al., 2024; Zhang et al., 2024a). However, white-box methods tend to generate irregular prompts that are easily detectable and cannot be optimized directly on black-box models like ChatGPT.\nIn contrast, black-box methods construct readable prompts in different ways and validate their effectiveness based on the responses of the target LLMs. Some studies employ heuristic strategies to rewrite malicious questions in other formats such as ASCII format (Jiang et al., 2024), code format (Kang et al., 2024; Lv et al., 2024), encrypted format (Yuan et al., 2024; Liu et al., 2024a) and low-resource languages (Deng et al., 2024b), exploiting the insufficient safety alignment of target LLMs in these formats to bypass the defense mechanism. Another line of research is to instruct an advanced LLM like GPT-4 to optimize jailbreak prompts by incorporating iterative refinement (Jin et al., 2024), genetic algorithms (Liu et al., 2024b) and psychological expertise (Zeng et al., 2024), or fine-tune another LLM with successful jailbreak templates to serve as an attacker to generate jailbreak prompts automatically (Deng et al., 2024a; Ge et al., 2024). Compared with white-box methods, black-box methods can be applied to most models. For this reason, black-box methods are widely used in empirical experiments to evaluate the safety of LLMs.\nTo mitigate threats caused by jailbreak attacks, different defense techniques are proposed to ensure the security of LLMs. One line of work addresses the issue by detecting (Inan et al., 2023) or perturbing (Robey et al., 2023) the jailbreak prompts to reduce the toxicity of the input, while another line of work directly enhances the robustness of LLMs by supervised fine-tuning (Bianchi et al., 2024) or reinforcement learning from human feedback (Ouyang et al., 2022b).\n2.2 Small Language Models\nSimilar to LLMs, SLMs are typically built upon decoder-only architectures, while they show diversity in implementation details such as the type of attention heads, layer numbers, dimension sizes, activation functions, and so on.\nTo achieve competitive performance within the limited scale of SLMs, different model compression techniques are adopted to construct lightweight architectures efficiently. For instance, MobilLLaMA (Thawakar et al., 2024) and MobileLLM (Chu et al., 2023) introduce a parameter-sharing scheme in embedding blocks and attention head blocks to reduce the cost of GPU memory. TinyLLaMA (Zhang et al., 2024b) optimizes memory load with the FlashAttention technique (Dao et al., 2022), which introduces an IO-aware attention algorithm to reduce the budget of high bandwidth memory. Quantization techniques, such as GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024), can also effectively reduce memory loads by compressing the size parameters from 16 bits to 8 bits or even 4 bits. In model collections such as Llama 3 (Touvron et al., 2023a), Qwen (Bai et al., 2023), and MiniCPM (Hu et al., 2024), SLMs are generally designed and pre-trained following LLMs in the same family. Additionally, during the training phase, knowledge distillation techniques are widely used to derive performance from teacher LLMs to student SLMs, as models in the same family generally share similar tokenizers and architecture.\nRecent research has demonstrated that SLMs can achieve comparable performance in some reasoning tasks, and can even outperform LLMs in specific scenarios (Lu et al., 2024). Our study fills a gap in evaluating the security of SLMs from another perspective. In the following sections, we will demonstrate the security differences between various SLMs and delve into their underlying causes."}, {"title": "Experiment Setups", "content": "3.1 Target Models\nWe collect 16 state-of-the-art models to provide a comprehensive view of their security differences, including 13 SLMs below 4B size and 3 LLMs above 7B size.\nFor LLMs, we include Llama2-7B (Touvron et al., 2023b), Llama3-8B (Touvron et al., 2023a), and DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) for evaluation. Notably, DeepSeek-R1-Distill-Llama-8B is constructed by distilling the reasoning patterns from DeepSeek-R1 to Llama3-8B. The controlled comparison enables us gain some valuable insights of the influences of distillation techniques to SLM security. For SLMs, we include 13 models from advanced research organizations and individual developers. Specifically, they are as follows:\n\u2022 Llama Family. Llama family is developed by Meta AI as one of the most popular model series. For our study, we select two models from the Llama 3.2 collection with parameter sizes of 1B and 3B. Furthermore, we include three additional SLMs that are initialized from Llama and processed with certain model compression techniques. These models are MobilLLaMA (Thawakar et al., 2024), MobileLLM (Chu et al., 2023), and TinyLLaMA (Zhang et al., 2024b).\n\u2022 Phi Family. Phi family (Gunasekar et al., 2023) is developed by Microsoft focusing on designing lightweight SLMs with exceptional performance. We select Phi-3-mini-4k-instruct in 3.8B size and Phi-3.5-mini-instruct size in 2.7B size for evaluation.\n\u2022 MiniCPM Family. MiniCPM family (Hu et al., 2024) is developed by OpenBMB and mainly consists of SLMs in different versions. We select MiniCPM-1B-sft-bf16, MiniCPM-2B-sft-bf16 and MiniCPM-4B for evaluation.\n\u2022 Qwen Family. Qwen family (Bai et al., 2023) is built by Alibaba Cloud which has released a spectrum of LLMs ranging from 0.5B to 72B sizes. We select Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct for evaluation."}, {"title": "Attack Methods", "content": "Our research firstly examines the influences of direct attacks against SLMs, which leverage straightforward harmful queries such as \u201cHow to make a bomb\" to probe the target models directly. We collect 5 datasets that contain such harmful questions across various illegal and unethical dimensions.\n\u2022 Advbench. Advbench (Zou et al., 2023) is a harmful dataset consisting of 500 harmful strings and 500 harmful behaviors. The former focuses on eliciting specific harmful responses from target LLMs, while the latter aims at provoking the models into exhibiting harmful behavior as much as possible.\n\u2022 DAN. DAN (Shen et al., 2023) provides a forbidden question set spanning 13 restricted scenarios. The dataset is primarily sourced from online platforms and publicly available datasets.\n\u2022 maliciousInstruct. MaliciousInstruct (Huang et al., 2024) consists of 100 harmful questions with 10 malicious intentions, which are mostly generated by ChatGPT and then revised manually.\n\u2022 StrongREJECT. StrongREJECT (Souly et al., 2024) offers 313 harmful questions that cover forbidden scenarios from different AI usage policies. The majority of the dataset is written manually, while the remaining portion is sourced from LLMs and other open-source datasets.\n\u2022 XSTEST. XSTEST (R\u00f6ttger et al., 2024) contains both safe and unsafe questions to assess the exaggerated safety behaviors of models. We extract the 200 harmful questions from the dataset for our experiments.\nFurthermore, to explore and understand the safety boundary of SLMs more clearly, we conduct a thorough investigation into existing jailbreak attacks and select 5 representative methods for evaluation. These methods span across different categories of jailbreak attacks and have demonstrated excellent effectiveness against LLMs in previous studies.\n\u2022 GCG.\nGreedy Coordinate Gradient (GCG) (Zou et al., 2023) is a gradient-based attack that initializes an adversarial suffix appended to the malicious question and optimizes it by gradient-based search to maximize the probability of affirmative responses. Although the optimization of jailbreak prompts is constrained to white-box models, they demonstrate strong transferability to other black-box models.\n\u2022 ArtPrompt. ArtPrompt (Jiang et al., 2024) is an ASCII-based attack that leverages the poor performance of LLMs in recognizing ASCII art to bypass defense mechanisms. Specifically, ArtPrompt utilizes LLMs like GPT-4 to recognize the malicious word in the prompt and visually encodes it with ASCII characters, combining the text prompt and the word in ASCII art to jailbreak.\n\u2022 DeepInception. DeepInception (Li et al., 2023) is a template-based attack that embeds malicious questions into virtual scenarios. Given a harmful question, DeepIncetion constructs a multi-layer scene with different characters and induces the target LLMs to complement the story step by step, thus generating harmful content in responses.\n\u2022 AutoDAN. AutoDAN is a genetic algorithm-based attack that refines jailbreak prompts iteratively to identify the optimal solution. Specifically, AutoDAN randomly initializes the original jailbreak population and performs word-level or sentence-level modifications to produce offspring. The new generation is subsequently evaluated by LLMs to gain fitness and repeat the generation process until the jailbreak succeeds.\n\u2022 Multilingual Attack. Multilingual attack (Deng et al., 2024b) exploits the weakness of the safety alignment in low-resource languages to conduct jailbreak attacks. The method translates harmful questions into multiple languages and shows that questions in low-resource languages demonstrate a high attack success rate.\nNotably, the datasets are all sourced from official resources to guarantee the reliability and robustness of the experimental results. For jailbreak methods, we follow the official implementation and use the best parameter settings as reported in the original papers."}, {"title": "Defense Methods", "content": "In addition to examining the effectiveness of different jailbreak attacks against SLMs, we have also explored potential defense strategies to mitigate these threats and enhance the robustness of SLMs. We primarily focus on two kinds of defense methods, namely detection-based defenses and perturbation-based defenses, and select one representative method from each category for our study.\n\u2022 Llama-Guard-3. Llama-Guard-3 (Inan et al., 2023) is fine-tuned from Llama-3 to detect unsafe content within user prompts and LLM responses. In our study, we instruct Llama-Guard-3-8B to detect jailbreak prompts and filter user inputs that are judged to be unsafe.\n\u2022 SmoothLLM. SmoothLLM (Robey et al., 2023) is a perturbation-based method that can mitigate malicious content in user prompts. For each prompt, SmoothLLM generates multiple copies with character-level perturbations applied to them and aggregates the responses of the target LLM to these copies to produce the final response."}, {"title": "Evaluation Metrics", "content": "We use attack success rate (ASR) as the primary metric in our experiments, which is widely used in related research to identify the effectiveness of jailbreak attacks. Formally, ASR can be defined as\n$$ASR = \\frac{N_{success}}{N_{total}}$$\nwhere $N_{success}$ is the number of successfully attacked prompts and $N_{total}$ is the total number of jailbreak prompts. Rule-based matching and LLM evaluators are the most common methods to assess the success of a jailbreak attack. However, during the experiments, we have observed that target SLMs occasionally generated unexpected responses that are not related to the prompts, resulting in a noticeably inflated ASR when relying on rule-based matching. To address this issue, we ultimately employed Llama-Guard-3-8B as the evaluator to assess the responses of jailbreak attacks to calculate the ASR accurately."}, {"title": "Experimental Settings", "content": "We control the parameter settings consistently when generating responses from the target models to ensure the comparability of the results. Specifically, we do not set any explicit system prompts and invoke the conversation template of target models to generate prompts. We also disabled token sampling during output generation to ensure the reproducibility of the results."}, {"title": "Main Results", "content": "4.1 Direct Attacks Against SLMs\nWe first evaluate the fundamental defense capabilities of SLMs with direct harmful questions used as original prompts. As illustrated in Figure 2, experimental results indicate that when faced with direct attacks, most SLMs successfully identify the malicious intention and generate rejection responses, exhibiting reliable defense capabilities that are comparable to LLMs. For SLM series including Llama, Phi, MiniCPM, and Qwen, the ASR is generally around or below 10%. In contrast, other models, including TinyLlama, MobileLlama, and MobiLlama, exhibit comparatively weaker performance in resisting harmful queries. Furthermore, all target SLMs show transferable defensive capabilities across various harmful datasets. That is, if they perform well on one dataset, they tend to demonstrate similar performance on the remaining four datasets.\nAs shown in Figure 2, there exists a slight positive correlation between parameter size and the security performance of SLMs, which also means that parameter size is not the primary factor in determining the security of SLMs. For SLMs in the same series but different in parameter sizes, such as Qwen-1.5B and Qwen-3B, their security capabilities against direct attacks show minimal variation. Meanwhile, although TinyLlama-1.1B, Llama3.2-1B, and MiniCPM-1B have a similar parameter size, the ASR of direct attacks against TinyLlama-1.1B significantly exceeds the other two models.\n4.2 Jailbreak Attacks Against SLMs\nWe further utilize five representative jailbreak methods to attack the SLMs, aiming to gain a deeper understanding of their security boundaries. The results are shown in Figure 3. Compared with direct attacks, jailbreak attacks generally achieve better results against most SLMs, with ASR on SLMs typically surpassing that on LLMs. This suggests although most SLMs can maintain their robustness under direct attacks, they still demonstrate vulnerabilities when exposed to more sophisticated jailbreak attacks.\nWe can draw from Figure 3 that the positive correlation between parameter size and security performance of SLMs becomes more pronounced under jailbreak attacks. Additionally, most SLMs show specific vulnerabilities to certain jailbreak attacks, which are quite different from direct attacks where SLMs possess transferable defense capabilities. For instance, MiniCPM series and Phi series demonstrate strong security against GCG attack, however, MiniCPM series are quite susceptible to ArtPrompt attack and Phi series fail to address the security threat from multilingual attack. During the pre-training or fine-tuning stage, different SLMs may have undergone particular security alignment on specific jailbreak methods, which finally contributes to the security differences. It is also notable that some SLMs, such as TinyLlama and MobiLlama that perform poorly under direct attacks, show a significant improvement when subjected to jailbreak attacks. The observation will be further discussed in Section 5.1.\n4.3 Defense Strategies for SLMs\nSince jailbreak attacks have demonstrated remarkable security threats against SLMs, it is emergent to figure out effective mitigation strategies to address the problem. To examine whether the prompt-level defense methods, Llama-Guard-3 and SmoothLLM, can serve as the guardrail to SLMs, we apply them to GCG and DeepInception and utilize the processed jailbreak prompts to attack some SLMs.\nAs shown in Table 1, we select 4 representative SLMs that are most seriously impacted by jailbreak attacks and evaluate their robustness under different combinations of jailbreak attacks and defenses."}, {"title": "Discussion", "content": "5.1 Why Some Jailbreak Attacks Fail on Certain SLMs?\nAccording to Figure 2 and Figure 3, we can observe an unexpected phenomenon that some SLMs exhibit poor performance against direct attacks but demonstrate notable robustness against jailbreak attacks. For instance, TinyLlama, which shows the highest vulnerability to direct attacks among all target models, displays strong resistance to ArtPrompt. Similarly, MobileLlama and MobiLlama also achieve relatively low ASR when exposed to Multilingual Attack, despite ranking second and third worst results in performance on harmful datasets.\nHowever, after analyzing the responses of the three target SLMs against these jailbreak attacks, we find that they often fail to generate appropriate refusal responses. Instead, they tend to produce meaningless phrases unrelated to jailbreak prompts, which are classified as harmless and eventually lead to the observed low ASR. To understand the unexpected result, we take an insight into the attack mechanism of these jailbreak attacks including ArtPrompt and Multilingual Attack. Specifically, Multilingual Attack requires the target models to possess multilingual abilities in low-resource languages to understand the question, and ArtPrompt requires the target models to reconstruct the original jailbreak prompts from ASCII art. These tasks may have exceeded the reasoning capabilities of SLMs, causing them to misinterpret the jailbreak prompts and produce irregular responses.\nThus, the observed robustness of SLMs against certain jailbreak attacks does not stem from inherent security mechanisms but rather from their limited generalization capabilities. The limitation prevents them from processing sophisticated jailbreak prompts effectively, thereby reducing the likelihood of generating harmful responses.\n5.2 What Mainly Contributes to Security Degradation of SLMs?\nFrom previous experiments, we can notice that the defense capabilities of SLMs are obviously inferior to LLMs, which highlights the need to find out the underlying causes of the security degradation.\nAs revealed in previous experiments, the security of SLMs tends to degrade as their scale decreases. Figure 4 and Figure 5 plot an overall view of the security performance of SLMs in different parameter sizes. The empirical evidence suggests a negative correlation between the size of the parameters and the robustness of security performance. Due to the limited parameter size, SLMs usually prioritize helpfulness over harmlessness, leading to insufficient emphasis on safety alignment. Additionally, the compression techniques used to design lightweight architectures for SLMs may further exacerbate the security issues. For instance, MobiLlama leverages parameter sharing techniques to compress the scale, which may affect the proportion of safety-critical parameters in the model.\nQuantization is another widely used model compression technique to reduce the memory cost for LLMs and SLMs. To explore the impact of quantization on the security of SLMs, we assess Qwen2.5-1.5B-Instruct and its quantized versions, including AWQ, GPTQ-Int4, and GPTQ-Int8. Their security performances under jailbreak attacks are illustrated in Figure 6. Surprisingly, we find that quantization techniques do not obviously weaken the security of SLMs and sometimes even slightly enhance their robustness. From this point of view, quantization can balance both efficiency and security in model compression.\nBiased knowledge distillation can also lead to security loss in SLMs. For instance, DeepSeek-R1-Distill-Llama-8B is significantly weakened compared to Llama3-8B. Besides, MobileLlama shows relatively poor security among all target models, which has also adopted knowledge distillation from Llama-2 to enhance reasoning capabilities. During knowledge distillation, if the training dataset lacks data focused on security, the student model may excessively inherit reasoning capabilities from the teacher model, thereby leading to a degradation in its security performance."}, {"title": "Conclusion", "content": "In this paper, we present a systematic empirical study to explore the security vulnerabilities of the state-of-the-art SLMs. We demonstrate that most SLMs are highly susceptible to malicious input, where jailbreak attacks pose a particularly significant threat. We also evaluate the effectiveness of several defense strategies when applied to SLMs, and further discuss the underlying factors that may cause the security degradation of SLMs. We hope that our study can raise awareness of the security risks associated with SLMs and offer valuable insights for developing more robust and resilient SLMs in the future."}, {"title": "Limitations", "content": "This paper presents a comprehensive overview of the security problems inherent in SLMs, while also exploring the fundamental causes due to different SLM techniques. However, the research predominantly focuses on empirical studies to uncover the security issues in the rapid development of SLMs. Future works can explore more advanced SLM techniques that can enhance robustness without compromising the overall performance, or design effective and efficient defense techniques tailored to SLMs."}, {"title": "Ethical Considerations", "content": "The primary goal of this research is to reveal and discuss the security issues of SLMs. We believe that some explorations of the research, especially the fact that some SLMs are quite vulnerable to even direct attacks, can raise the awareness of the research community and prevent the SLMs from being misused."}, {"title": "Detailed expirement results of adversarial attacks", "content": "We present the detailed experimental results in this section for reference. Among them, Table 2 and Table 3 contain the detailed results of Figure 2 and Figure 3, which show the ASR of direct attacks and jailbreak attacks against target models. In addition, We intentionally group models from the same family together to facilitate a clearer comparison."}]}