{"title": "Robust Temporal-Invariant Learning in Multimodal Disentanglement", "authors": ["Guoyang Xu", "Junqi Xue", "Zhenxi Song", "Yuxin Liu", "Zirui Wang", "Min Zhang", "Zhiguo Zhang"], "abstract": "Multimodal sentiment recognition aims to learn representations from different modalities to identify human emotions. However, previous works does not suppresses the frame-level redundancy inherent in continuous time series, resulting in incomplete modality representations with noise. To address this issue, we propose the Temporal-invariant learning, which minimizes the distributional differences between time steps to effectively capture smoother time series patterns, thereby enhancing the quality of the representations and robustness of the model. To fully exploit the rich semantic information in textual knowledge, we propose a Text-Driven Fusion Module (TDFM). To guide cross-modal interactions, TDFM evaluates the correlations between different modality through modality-invariant representations. Furthermore, we introduce a modality discriminator to disentangle modality-invariant and modality-specific subspaces. Experimental results on two public datasets demonstrate the superiority of our model.", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal sentiment analysis (MSA) has become an active area of research with critical applications across various fields, such as human-computer interaction [1], social media analysis [2], and affective computing [3].\nMSA typically involves data from video, speech, and text. Each of these modalities offers unique and complementary information that is crucial to the sentiment analysis. For instance, text data carries rich semantic content that directly conveys the speaker's emotions and intentions. In contrast, video data provides valuable non-verbal cues, such as facial expressions and body language, which are essential for understanding the full context of the interaction.\nAmong the three modalities, the text modality stands out as the most dominant in MSA tasks [4]. Part of this advantage is due to the fact that text information is typically presented in a structured format, allowing for more precise expression of emotions and intentions. Another aspect is the continuous advancement of natural language processing techniques, which enable the accurate capture of emotional cues within text. Additionally, text data is more stable compared to audio and visual data, making it less susceptible to external factors.\nIn the context of multimodal fusion, the text modality complements other modalities effectively.Text can provide contextual information for audio and visual modalities, helping the model better understand the underlying emotional implications of speech or facial expressions. This complementarity between modalities significantly enhances the overall performance of the model, enabling multimodal sentiment analysis to capture more comprehensive and nuanced emotional information.\nCompared to the semantically rich textual information, video data contains a significant amount of redundancy and noise. This redundancy stems from the high frame rate of video, where consecutive frames differ slightly, leading to repeated information that does not necessarily enhance emotion understanding. Additionally, noise in video data, such as changes in lighting conditions and background movements, further complicate the extraction of relevant emotional cues.\nBased on the above observations, we propose temporal-invariant learning. Different from traditional complex preprocessing methods such as frame selection and noise reduction, temporal-invariant learning directly captures global information within video data at the feature level by minimizing the distributional differences between time steps. This approach effectively filters out the redundant and noisy information, allowing the model to capture the global information of the time series.\nTo address modality heterogeneity, we employ adversarial learning to train private and shared encoders to disentangle modality-specific and modality-invariant representations. Specifically, we utilize a spherical modality discriminative loss to enhance intra-class compactness and inter-class discrepancy for the hidden representations and parameters of the modality discriminator within a hyper-sphere [7].\nFurthermore, we facilitate interactions between modalities with a focus on the text modality. To fully leverage the learned high-level shared representations, we propose an adaptive fusion mechanism that dynamically evaluates the correlations between modalities during the fusion process.\nThe main contributions can be summarised as follows: (1) We propose a novel multimodal representations learning model named RTIL (Robust Temporal-Invariant Learning), which leverages adversarial learning to separate modality-invariant representations and modality-specific representations, and adaptively fuses these representations. (2) Through temporal-invariant learning, redundant information and noise associated with temporal changes are effectively reduced, enhancing the model's ability to maintain consistent global information regardless of variations in time steps. (3) We performed a series of experiments on two datasets, showing that the proposed RTIL outperforms state-of-the-art methods."}, {"title": "II. PROPOSED MODEL", "content": "The overall architecture of RTIL is depicted in Fig. 1, consisting of the Feature Extraction Module, Representation Learning Module, and Text-Driven Fusion Module (TDFM). Further details are provided in the following subsections."}, {"title": "A. Feature Extraction Module", "content": "For video and audio modalities, we use Transformer En-coders to capture long-range dependencies. For language modality, we feed the input text into RoBERTa to enhance the language representations. The outputs of each modality are denoted as $H_i$, where $i \\in \\{a,v,t\\}$."}, {"title": "B. Representation Learning Module", "content": "Modality-Invariant and Modality-Specific Representa-tions Learning. RTIL leverages a shared encoder to capture invariant representations of different modalities, effectively reducing the heterogeneity gap. Additionally, to learn the specific representations, we utilize three different private encoders, mapping modalitiy embeddings to the modality-specific subspaces. The invariant representations $I_i$ and specific representations $S_i$ are denoted as:\n$I_i = E_I(H_i, \\theta_I), S_i = E_S(H_i, \\theta_S)$ (1)\nwhere shared encoder $E_I$ shares the parameters $\\theta_I$ and private encoders $E_S$ assign separate parameterare $\\theta_S$ for each modality. To align the different modalities representations in the invariant subspace, we apply the consistency loss to disentangled representation learning. We use the Central Moment Discrepancy (CMD) [5] to measure the difference between two modalities.\nFurthermore, the consistency loss can be calculated as:\n$L_{con} = \\frac{1}{3} \\sum_{m_1, m_2\\in\\{a,v,t\\}} CMD(I_{m_1}, I_{m_2})$ (2)\nAdversarial Learning. Inspired by the previous work [7], we introduce a modality discriminator to encourage the shared and private encoders to produce distinct representations. The invariant and specific representations are fed into the discriminator as input after passing through gradient reversal layers [8], then the discriminator predicts the modality from which the representation originates:\n$D(h_i, \\theta_D) = softmax(W_D^T \\cdot Linear(h_i))$ (3)\nwhere $h_i \\in \\{I_i, S_i\\}$ and $W_D$ is a learnable parameter matrix. We apply the additive angular margin loss [9] to enhance the intra-class compactness and inter-class discrepancy for the modality discriminator:\n$L_{am} = -log\\frac{e^{a \\cdot cos(\\theta_{y_m}+T)}}{\\sum_{m=1}^{M}e^{a \\cdot cos(\\theta_{m})}}$ (4)\nwhere $h = Linear(h_i)$, $\\theta_{y_m} = arccos(W_m^T h_i)$ and $\\theta_m = arccos(W_m^T h_i)$. $y_m$ denotes the ground-truth modality label."}, {"title": "C. Text-Driven Fusion Module", "content": "Fusion Procedure. Text-Driven Fusion Module has two parallel inter-modality attention streams with respective gate-controlled mechanisms. To enhance modality alignment, both streams are driven by the text modality to provide consistent context.\nAfter positional encoding, the modality-specific features $S_i$ ($i \\in \\{a, v\\}$) and $S_t$ are processed through the cross-attention stream to produce the interacted features $F_{ti}$ ($i \\in \\{a, v\\}$):\n$F_{ti} = Attention(S_t, S_i, S_i) = softmax(\\frac{S_tS_i^T}{\\sqrt{d_k}}) S_i$ (8)\nMeanwhile, the gated mechanism takes modality-invariant features $I_i$ ($i \\in \\{a,v\\}$) and $I_t$ as inputs, producing strong correlations between modality-invariant features at each time step to control the interactions of modality-specific features. Different from the previous work [12], we use modality-invariant representations to guide the interactions, rather than fused modality-specific representations themselves. During modality representation learning, modality-specific features may develop more distinct representations, which can make it challenging to accurately assess the similarity between modalities during fusion.\nSince the modality-invariant features capture the common information across different modalities, we believe that using the modality-invariant features provides a more reliable basis for assessing the correlation between different modalities features. Our gated mechanism employs the Factorized Bilinear Pooling (FBP) [13] to generate the temporal gated signals $S_g$ ($g\\in \\{a, v\\}$), as illustrated in Fig. 1. The formulaic expression can be given as:\n$F_{mul} = (S_tW_Q) \\odot (S_iW_K)$ (9)\n$F_{sp} = SumPool(F_{mul},k)$ (10)\n$F_{norm} = F_{sp}/||F_{sp}||_2$ (11)\n$S_g = F_{norm}W_{norm}$ (12)\nThe final representation $F_{final}$ is defined as:\n$F_{final} = concatenate(S_a\\cdot F_{ta}, S_v\\cdot F_{tv})$ (13)\nPrediction. We feed the fused representation into an MLP to obtain the prediction output. The final loss function is expressed as follows:\n$L = L_{task} + \\alpha L_{con} + \\beta L_{ti} + +\\gamma L_{dom}$ (14)\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are the trade-off parameters and $L_{task} \\in \\{L_{MSE},L_{CE}\\}$ stands for the loss prediction function for different tasks."}]}