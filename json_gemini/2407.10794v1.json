{"title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education", "authors": ["Rui Yang", "Boming Yang", "Sixun Ouyang", "Tianwei She", "Aosong Feng", "Yuang Jiang", "Freddy Lecue", "Jinghui Lu", "Irene Li"], "abstract": "Knowledge graphs (KGs) are crucial in the field of artificial intelligence and are widely applied in downstream tasks, such as enhancing Question Answering (QA) systems. The construction of KGs typically requires significant effort from domain experts. Recently, Large Language Models (LLMs) have been used for knowledge graph construction (KGC), however, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC framework from free text. The core fusion module provides a global view of triplets, incorporating entity merging, conflict resolution, and novel triplet discovery. We showcase how Graphusion could be applied to the natural language processing (NLP) domain and validate it in the educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for graph reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our evaluation demonstrates that Graphusion surpasses supervised baselines by up to 10% in accuracy on link prediction. Additionally, it achieves average scores of 2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and relation recognition, respectively.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) such as GPT (Achiam et al., 2023) and LLaMA (Touvron et al., 2023) have demonstrated outstanding performance across various tasks in the field of natural language processing (NLP) (Yang et al., 2023c,d; Song et al., 2023; Yang et al., 2023a; Gao et al., 2024). However, the content generated by LLMs often lacks accuracy and interpretability (Zhang et al., 2023a; Yang et al., 2024b). To address these challenges, one approach is leveraging Knowledge Graphs (KGs) to enhance LLMs (Yang et al., 2023b, 2024a). By prompting the structured KG knowledge to LLMs, they can generate more reliable content. Additionally, KG-enhanced LLMs can be applied to various KG tasks as well, including graph completion, reasoning and more (Zhu et al., 2023; Chen et al., 2023).\nIn Fig 1, we demonstrate an example in the educational scenario. A user asks a question involving specific concepts (highlighted in blue). Ideally, the response should reflect the relations between these concepts, essentially outlining the learning path that connects them. However, without a knowledge system, an LLM might offer answers that are somewhat relevant but too general, including broad concepts like \"Basic NLP Foundations\", or it may introduce confusing concepts with inaccurate specificity (\"Corpora and Datasets\"). In contrast, when equipped with a knowledge system (such as a concept graph showing prerequisite relations) and supplemented with relevant in-domain texts, the response becomes more refined, reflecting a deeper understanding of the concept relations.\nAutomatic methods have been applied to knowledge graph construction (KGC) (Sheng et al., 2022; Baek et al., 2023; Carta et al., 2023), with most of them employing a localized perspective, extracting triplets at the sentence or paragraph level, which is suitable for shallow knowledge, such as (people, belong_to, organization). However, this localized approach often fails to capture the comprehensive and interconnected nature of knowledge. The accuracy and completeness of triplets can be significantly limited when sourced from isolated segments of text, which is essential for scientific graphs containing deep knowledge.\nRecognizing this limitation, we propose a significant paradigm shift towards a global view in KGC. Our approach includes a graph fusion module that extracts candidate triplets and performs global merging and resolution across multiple sources. Specifically, we leverage LLMs not only for extraction but also for critical knowledge integration, marking the first initiative to utilize LLMs for such a comprehensive merging process. We believe that this global perspective is crucial for constructing more accurate and holistic KGs, as it allows for the consideration of broader contexts and relations that span beyond single documents. Similarly, this is particularly vital in scientific KGs, where the relations between complex concepts cannot be adequately understood by examining individual sentences.\nIn this study, we utilize LLMs to construct and fuse scientific KGs, focusing primarily on the domain of natural language processing. Most importantly, we apply the constructed KG in the educational question-answering (QA) scenario. Our contributions are summarized as follows. First, we propose the Graphusion framework, which allows zero-shot KGC from free text. The core graph fusion component incorporates entity merging, conflict resolution, and novel triplet discovery. Evaluation results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for entity extraction and relation recognition, respectively, demonstrating its potential for automatic and large-scale KGC. To the best of our knowledge, our work is pioneering in scientific KGC with fusion using the zero-shot capabilities of LLMs. Second, we present TutorQA, a QA benchmark featuring six diverse tasks and comprising 1,200 expert-verified, NLP-centric QA pairs designed to mimic college-level tutoring questions. Third, we develop a pipeline to enhance the interaction between LLMs and the concept graph for TutorQA, achieving significant results across all tasks. All the code and data can be found in https://github.com/IreneZihuiLi/CGPrompt."}, {"title": "Related Work", "content": "Knowledge Graph Construction KGC aims to create a structured representation of knowledge in the form of a KG. Research on KGs spans various domains, including medical, legal, and more (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014; Li et al., 2020; Le-Tuan et al., 2022; Kalla et al., 2023; Ahrabian et al., 2023). Typically, KGC involves several methods such as entity extraction and link prediction (Luan et al., 2018; Reese et al., 2020), with a significant focus on supervised learning. Recently, LLMs have been utilized in KGC relying on their powerful zero-shot capabilities (Zhu et al., 2023; Carta et al., 2023). Although relevant works propose pipelines for extracting knowledge, they often remain limited to localized extraction, such as at the sentence or paragraph level. In contrast, our work focuses on shifting from a local perspective to a global one, aiming to generate a more comprehensive KG.\nEducational NLP Modern NLP and Artificial Intelligence (AI) techniques have been applied to a wide range of applications, with education being a significant area. For instance, various tools have been developed focusing on writing assistance, language study, automatic grading, and quiz generation (Seyler et al., 2015; Gonz\u00e1lez-Carrillo et al., 2021; Zhang et al., 2023b; Lu et al., 2023). Moreover, in educational scenarios, providing responses to students still requires considerable effort, as the questions often demand a high degree of relevance to the study materials and strong domain knowledge. Consequently, many studies have concentrated on developing automatic QA models (Zylich et al., 2020; Hicke et al., 2023), which tackle a range of queries, from logistical to knowledge-based questions. In this work, we integrate an LLM-constructed KG for various QA tasks in NLP education."}, {"title": "Graphusion Framework", "content": "In this section, we introduce our Graphusion Framework for scientific KGC.\nProblem Definition\nA KG is defined as a set of triplets: $KG = \\{(h_i, r_i,t_i) | h_i,t_i \\in E, r_i \\in R, i = 1, 2, . . ., n\\}$, where: E is the set of entities, R is the set of possible relations, and n is the total number of triplets in the KG. The task of zero-shot KGC involves taking a set of free text T and generating a list of triplets (h,r,t). Optionally, there is"}, {"title": "Zero-shot Link Prediction", "content": "While the task of KGC is to generate a list of triplets, including entities and their corresponding relations, we start with a simpler setting: focusing solely on link prediction for pre-defined entity pairs and a single relation type. This setting helps us understand the capabilities of LLMs on scientific KGC under a zero-shot setting. Specifically, given a concept pair (A, B), the task of link prediction is to determine if a relationship r exists. We choose r =Prerequisite_of. For instance, the relation \"Viterbi Algorithm\" \u2192 \"POS Tagging\" implies that to learn the concept of \"POS Tagging,\" one must first understand the \"Viterbi Algorithm.\" Initially, a predefined set of concepts C is given.\nLP Prompt We then design a Link Prediction (LP) Prompt to solve the task. The core part is to provide the domain name, the definition and description of the dependency relation to be predicted, and the query concepts. Meanwhile, we explore whether additional information, such as concept definitions from Wikipedia and neighboring concepts from training data (when available), would be beneficial. The LP Prompt is as follows:\nWe have two {domain} related concepts:\nA: {concept_1} and B: {concept_2}.\nDo you think learning {concept_1}\nwill help in understanding {concept_2}?\nHints:\n1. Answer YES or NO only.\n2. This is a directional relation,\nwhich means if the answer is \"YES\",\n(B, A) is false, but (A, B) is true.\n3. Your answer will be used to create a\nknowledge graph.\n{Additional Information}"}, {"title": "Graphusion: Zero-shot Knowledge Graph Construction", "content": "We now introduce our Graphusion framework for constructing scientific KGs, shown in Fig 2. Our approach addresses three key challenges in zero-shot KGC: 1) the input consists of free text rather than a predefined list of concepts; 2) the relations encompass multiple types, and conflicts may exist among them; and 3) the output is not a single binary label but a list of triplets, making evaluation more challenging.\nStep 1: Seed Concept Generation Extracting domain-specific concepts using LLMs under a zero-shot setting is highly challenging due to the absence of predefined concept lists. This process is not only resource-intensive but also tends to generate a large number of irrelevant concepts, thereby compromising the quality of extraction. To address these issues, we adopt a seed concept generation approach for efficiently extracting in-domain concepts from free text (Ke et al., 2024). Specifically, we utilize BERTopic (Grootendorst, 2022) for topic modeling to identify representative concepts for each topic. These representative concepts serve as seed concepts, denoted as Q. The initialized seed concepts ensure high relevance in concept extraction and provide certain precision for subsequent triplet extraction.\nStep 2: Candidate Triplet Extraction Based on these seed concepts, in Step 2, we begin extracting candidate triplets from the free text. Each time, we input a concept $q \\in Q$ ({query}) as the query concept and retrieve documents containing this concept ({context}) through information retrieval. Our goal is to extract any potential triplets that include this query concept. To achieve this, we design a Chain-of-Thought (CoT) (Wei et al., 2022) prompt. We first instruct the LLMs to extract in-domain concepts, then identify the possible relations between those concepts and q. Then, we ask LLMs to discover novel triplets, even if q is not initially included. This approach ensures that the seed concepts play a leading role in guiding the extraction of in-domain concepts. Meanwhile, the candidate triplets will encompass novel concepts. We design the Extraction Prompt to be the following:\nGiven a context {context}, and a query\nconcept {query}, do the following:\n1. Extract the query concept and\nin-domain concepts from the context,\nwhich should be fine-grained...\n2. Determine the relations between\nthe query concept and the extracted\nconcepts, in a triplet format:\n(<head concept>, <relation>, <tail\nconcept>)...\n{Relation Definition}\n3. Please note some relations are\nstrictly directional...\n4. You can also extract triplets from\nthe extracted concepts, and the\nquery concept may not be necessary\nin the triplets.\nAfter processing all the queries from the seed concept list, we save all the candidate triplets. We denote this zero-shot constructed KG by the LLM as $ZS - KG$.\nStep3: Knowledge Graph Fusion The triplets extracted in the previous step provide a local view rather than a global perspective of each queried concept. Due to the limitations of context length, achieving a global view is challenging. Additionally, the relations extracted between two concepts can sometimes be conflicting, diverse, or incorrect, such as (neural summarization methods, Used-for, abstractive summarization) and (neural summarization methods, Hyponym-of, abstractive summarization). To address the aforementioned challenge, we propose the Fusion step. This approach helps reconcile conflicting relations, integrate diverse or incorrect relations effectively, and ultimately provides a global understanding of an entity pair.\nSpecifically, for each query concept q, we first query from $ZS \u2013 KG$, and obtain a sub-graph that contains q:\nLLM-KG = { (h, r,t) \u2208 ZS-KG | h = q or t = q }.\nOptionally, if there is an expert-annotated KG available, we will also query a sub-graph, marked as $E - G$. Moreover, we conduct a dynamic retrieval of q again from the free text ({background}), to help LLMs to have a better understanding on how to resolve the conflicted triplets. This key fusion step focuses on three parts: a) entity merging: merge semantically similar entities, i.e., NMT vs neural machine translation; b) conflict resolution: for each entity pair, resolve any conflicts and choose the best one; and c) novel triplet inference: propose new triplet from the background text. We utilize the following Fusion Prompt:\nPlease fuse two sub-knowledge graphs\nabout the concept \"{concept}\".\nGraph 1: {LLM-KG} Graph 2: {E-G}\nRules for Fusing the Graphs:\n1. Union the concepts and edges.\n2. If two concepts are similar, or refer\nto the same concept, merge them into\none concept, keeping the one that is\nmeaningful or specific\n3. Only one relation is allowed between\ntwo concepts. If there is a conflict,\nread the \"### Background\" to help you\nkeep the correct relation...\n4. Once step 3 is done, consider every\npossible concept pair not covered in\nstep 2. For example, take a concept\nfrom Graph 1, and match it with a\nconcept from Graph 2. Then, please\nrefer to \"### Background\" to\nsummarize\nnew triplets.\n### Background:\n{background}\n{Relation Definition}"}, {"title": "Evaluation", "content": "The evaluation of KGC is challenging since each model generating different triplets from the free text, along with the lack of expert annotations. To address this, we conduct the expert evaluation on the pipeline output. We ask experts to assess both concept entity quality and relation quality, providing ratings ranging from 1 to 3. The former measures the relevance and specificity of the extracted concepts, while the latter evaluates the logical accuracy between concepts. Additionally, we calculate the Inter-Annotator Agreement (IAA) of the two experts' evaluations using the Kappa score."}, {"title": "Experiments", "content": "Link Prediction\nWe conduct experiments using the LectureBankCD dataset (Li et al., 2021) and report the performance on the NLP domain. LectureBankCD contains up to 322 pre-defined NLP concepts and prerequisite relation labels on the concept pairs. We benchmark on the official test set against the following Supervised Baselines: DeepWalk (Perozzi et al., 2014), and Node2vec (Grover and Leskovec, 2016), P2V (Wu et al., 2020), and BERT (Devlin et al., 2019). These methods utilize pre-trained or graph-based models to encode concept embeddings and then perform binary classification to determine the presence of positive or negative edges in given concept pairs. In our LLM-based experiments, we show two main settings: Zero-shot, which employs LP Prompt; and Zero-shot with RAG, which enhances zero-shot with the addition of Retrieval Augmented Generation (RAG) method (Krishna, 2023). RAG has shown to improve on existing LLMs on text generation tasks such as QA. In Tab. 1, we observe that the zero-shot performance of GPT-4 and GPT-40 surpasses that of the best traditional supervised baseline. This suggests that LLMs can recover a domain-specific concept graph without relying on expert annotations. With the aid of RAG, which incorporates more domain-specific data, GPT-4o achieves significant improvements.\nKnowledge Graph Completion\nTo conduct KGC, we need a large-scale free-text corpus to serve as the knowledge source. Since there is no standard benchmark for evaluation, we collected the proceedings papers from the ACL conference spanning 2017-2023, which includes"}, {"title": "TutorQA", "content": "We introduce the TutorQA benchmark, a QA dataset designed for concept graph reasoning and text generation in the NLP domain. TutorQA comprises six categories, encompassing a total of 1,200 QA pairs that have been validated by human experts. These questions go beyond simple syllabus inquiries, encompassing more extensive and chal-"}, {"title": "QA Tasks", "content": "We summarize the tasks and provide example data in Fig 3. More data statistics and information can be found in Appendix E.\nTask 1: Relation Judgment The task is to assess whether a given triplet, which connects two concepts with a relation, is accurate.\nTask 2: Prerequisite Prediction The task helps students by mapping out the key concepts they need to learn first to understand a complex target topic.\nTask 3: Path Searching This task helps students identify a sequence of intermediary concepts needed to understand a new target concept by charting a path from the graph.\nTask 4: Subgraph Completion The task involves expanding the KG by identifying hidden associations between concepts in a subgraph.\nTask 5: Similar Concepts The task requires identifying concepts linked to a central idea to deepen understanding and enhance learning, aiding in the creation of interconnected curriculums.\nTask 6: Idea Hamster The task prompts participants to develop project proposals by applying learned concepts to real-world contexts, providing examples and outcomes to fuel creativity."}, {"title": "KG-Enhanced Model", "content": "To address TutorQA tasks, we first utilize Graphusion framework to construct an NLP KG. Then we design an enhanced framework for the interaction between the LLM and the concept graph, which includes two steps: command query and answer generation. In the command query stage, an LLM independently generates commands to query the concept graph upon receiving the query, thereby retrieving relevant paths. During the answer generation phase, these paths are provided to the LLM as contextual prompt, enabling it to perform concept graph reasoning and generate answers."}, {"title": "Evaluation", "content": "Accuracy We report accuracy score for Task 1 and Task 4, as they are binary classification tasks.\nSimilarity score For Tasks 2 and 3, the references consist of a list of concepts. Generally, LLMs demonstrate creativity by answering with novel concepts, which are often composed of more contemporary and fresh words, even though they might not exactly match the words in the concept graph. Consequently, conventional evaluation metrics like keyword matching are unsuitable for these tasks. To address this, we propose the similarity score. This metric calculates the semantic similarity between the concepts in the predicted list $C_{pred}$ and the ground truth list $C_{gold}$. Specifically, as shown in Eq. 1, for a concept m from the predicted list, and a concept n from the ground truth list, we calculate the cosine similarity between their embeddings achieved from pre-trained BERT model (Devlin et al., 2019). We then average these similarity scores to obtain the similarity score.\n$Score = \\frac{\\sum_{m \\epsilon C_{pred}} \\sum_{n \\epsilon C_{gold}} sim(m, n)}{|C_{pred} \\times |C_{gold}||}$\nBy averaging the similarity scores, the final score provides a comprehensive measure of the overall semantic alignment between the predicted and ground truth concepts.\nHit Rate For Task 5, we employ the classical Hit Rate metric, expressed as a percentage. This measure exemplifies the efficiency of LLM at retrieving and presenting relevant concepts in its output as compared to a provided list of target concepts."}, {"title": "Ablation Study and Analysis", "content": "RAG Data for Link Prediction\nWe explore the potential of external data in enhancing concept graph recovery. This is achieved by expanding the {Additional Information} part in the LP Prompt. We utilize LLaMa as the Base model, focusing on the NLP domain. We introduce three distinct settings: Doc.: In-domain lecture slides data as free-text; Con.: Adding one-hop neighboring concepts from the training set as additional information related to the query concepts."}, {"title": "Graphusion Case Study", "content": "In Fig 6, we present case studies from our Graphusion framework using GPT-40. Graphusion effectively merges similar concepts (neural MT and neural machine translation) and resolves relational conflicts (prerequisite of vs hyponym of). Additionally, it can infer novel triplets absent from the input. We highlight both positive and negative outputs from Graphusion. For instance, it correctly identifies that a technique is used for a task (hierarchical attention network, used for, reading comprehension). However, it may make mistakes in concept recognition, such as concepts with poor granularity (annotated data, model generated summary) and identifying incorrect relations (word embedding being inaccurately categorized as part of computer science)."}, {"title": "TutorQA Task 2 & Task 3: Concept Entity Counts", "content": "As depicted in Fig 7, We evaluate the average number of concept entities generated by GPT-40 and our Graphusion framework in the responses for Task 2 and Task 3. The results show that without the enhancement of KG, GPT-40 tends to generate more concept entities (Task 2: 11.04, Task 3: 11.54), many of which are irrelevant or broad. In contrast, our Graphusion framework generates more accurate and targeted concept entities."}, {"title": "Conclusion", "content": "In this work, we explored the application of LLMs for scientific KG fusion and construction. Initially, we developed Graphusion, which enables LLMs to perform zero-shot KGC from free text. Subsequently, we introduced TutorQA, an expert-verified, NLP-centric benchmark designed for QA using a concept graph. Lastly, we devised a pipeline aimed at augmenting QA performance by leveraging LLMs and constructed KG."}, {"title": "Limitations", "content": "Graph Construction Constructing a KG from free-text, especially under a zero-shot context, relies on the quality and scale of the corpus. In this paper, we showcased that applying paper abstracts is a possible way. While we did not have a chance to test other data such as text books or web posts. Besides, evaluation is challenging as it is hard to construct a standard test set. So our evaluation was mostly conducted by human experts with a reasonable scale.\nEvaluation metrics of TutorQA For Task 2 and 3, LLMs often generate novel concepts in their responses. To address this, we evaluated answers based on semantic similarities to compute a score. A notable limitation is the disregard for concept order in the provided answer paths. Addressing this concern will be a focus of our future work."}, {"title": "Ethical Considerations", "content": "In our research, we have meticulously addressed ethical considerations, particularly regarding our dataset TutorQA and Graphusion framework. TutorQA has been expert-verified to ensure it contains no harmful or private information about individuals, thereby upholding data integrity and privacy standards. Our methods, developed on publicly available Large Language Models optimized for text generation, adhere to established ethical norms in AI research. We recognize the potential biases in such models and are committed to ongoing monitoring to prevent any unethical content generation, thereby maintaining the highest standards of research integrity and responsibility."}, {"title": "Prompt Templates", "content": "Main Framework\nLP Prompt\nWe have two {domain} related concepts: A: {concept_1} and B: {concept_2}.\nDo you think learning {concept_1} will help in understanding {concept_2}?\nHints:\n1. Answer YES or NO only.\n2. This is a directional relation, which means if the answer is \"YES\", (B, A) is false, but (A, B) is true.\n3. Your answer will be used to create a knowledge graph.\n{Additional Information}\nLP Prompt With Chain-of-Thought\nWe have two {domain} related concepts: A: {concept_1} and B: {concept_2}.\nAssess if learning {concept_1} is a prerequisite for understanding {concept_2}.\nEmploy the Chain of Thought to detail your reasoning before giving a final answer.\n# Identify the Domain and Concepts: Clearly define A and B within their domain. Understand the specific content and scope of each concept.\n# Analyze the Directional Relationship: Determine if knowledge of concept A is essential before one can fully grasp concept B. This involves considering if A provides foundational knowledge or skills required for understanding B.\n# Evaluate Dependency: Assess whether B is dependent on A in such a way that without understanding A, one cannot understand B.\n# Draw a Conclusion: Based on your analysis, decide if understanding A is a necessary prerequisite for understanding B.\n# Provide a Clear Answer: After detailed reasoning, conclude with a distinct answer : <result>YES</result> if understanding A is a prerequisite for understanding B, or <result>NO</result> if it is not."}, {"title": "Ablation Study", "content": "Link Prediction with Doc.\nWe have two {domain} related concepts: A: {concept_1} and B: {concept_2}.\nDo you think learning {concept_1} will help in understanding {concept_2}?\nHints:\n1. Answer YES or NO only.\n2. This is a directional relation, which means if the answer is \"YES\", (B, A) is false, but (A, B) is true.\n3. Your answer will be used to create a knowledge graph.\nAnd here are related contents to help:\n{related documents concatenation}\nLink Prediction with Con.\nWe have two {domain} related concepts: A: {concept_1} and B: {concept_2}.\nDo you think learning {concept_1} will help in understanding {concept_2}?\nHints:\n1. Answer YES or NO only.\n2. This is a directional relation, which means if the answer is \"YES\", (B, A) is false, but (A, B) is true.\n3. Your answer will be used to create a knowledge graph.\nAnd here are related contents to help:\nWe know that {concept_1} is a prerequisite of the following concepts:\n{1-hop successors of concept_1 from training data};\nThe following concepts are the prerequisites of {concept_1}:\n{1-hop predecessors of concept_1 from training data}.\nWe know that {concept_2} is a prerequisite of the following concepts:\n{1-hop successors of concept_2 from training data};\nThe following concepts are the prerequisites of {concept_2}:\n{1-hop predecessors of concept_2 from training data}.\nLink Prediction with Wiki.\nWe have two {domain} related concepts: A: {concept_1} and B: {concept_2}.\nDo you think learning {concept_1} will help in understanding {concept_2}?\nHints:\n1. Answer YES or NO only.\n2. This is a directional relation, which means if the answer is \"YES\", (B, A) is false, but (A, B) is true.\n3. Your answer will be used to create a knowledge graph.\nAnd here are related contents to help:\n{Wikipedia introductory paragraph of {concept_1}}\n{Wikipedia introductory paragraph of {concept_2}}"}, {"title": "Experimental Setup", "content": "Experiments\nIn our experimental setup, we employed Hugging Face's LLaMA-2-70b-chat-hf and LLaMA-3-70b-chat-hf for LLaMA2 and LLaMA3 on a cluster equipped with 8 NVIDIA A100 GPUs. For GPT-3.5 and GPT-4, we used OpenAI's gpt-3.5-turbo gpt-4-1106-preview, and gpt-4o APIs, respectively, each configured with a temperature setting of zero. The RAG models are implemented using Embedchain (Taranjeet Singh, 2023). To solve TutorQA tasks, we implemented our pipeline using LangChain. The total budget spent on this project, including the cost of the GPT API service, is approximately 500 USD.\nAdditional Corpora Description\nTutorialBank We obtained the most recent version of TutorialBank from the authors, which consists of 15,583 manually curated resources. This collection includes papers, blog posts, textbook chapters, and other online resources. Each resource is accompanied by metadata and a publicly accessible URL. We downloaded the resources from these URLs and performed free text extraction. Given the varied data formats such as PDF, PPTX, and HTML, we encountered some challenges during text extraction. To ensure text quality, we filtered out sentences shorter than 25 words. Ultimately, this process yielded 559,217 sentences suitable for RAG and finetuning experiments.\nNLP-Papers We downloaded conference papers from EMNLP, ACL, and NAACL spanning the years 2021 to 2023. Following this, we utilized Grobid (https://github.com/kermitt2/grobid) for text extraction, resulting in a collection of 4,787 documents with clean text."}, {"title": "Link Prediction", "content": "Experiments\nSince LectureBankCD contains data from three domains: NLP, computer vision (CV), and bioinformatics (BIO), we further compare the performance across all the domains, presenting the results in Tab. 6. Specifically, the RAG data predominantly consists of NLP-related content, which is why there is no noticeable improvement in the CV and BIO domains when using RAG.\nAblation Study\nPrompting Strategies In Tab. 7, we explore the impact of different prompting strategies for concept graph recovery, comparing CoT and zero-shot prompts across both NLP and CV domains. The results indicate the introduction of CoT is not improving. We further find that CoT Prompting more frequently results in negative predictions. This finding serves as a drawback for our study, as it somewhat suppresses the performance of our system. This observation highlights the need to balance the impact of CoT on the rigor and complexity of predictions, especially in the context of graph recovery.\nFinetuning We further explore the impact of finetuning on additional datasets, with results detailed in Table 8. Specifically, we utilize LLaMA2-70b (Touvron et al., 2023), finetuning it on two previously mentioned datasets: TutorialBank and NLP-Papers. Both the zero-shot LLaMA and the finetuned models are employed to generate answers. As these answers are binary (YES or NO), we can calculate both the accuracy and F1 score for evaluation. However, the results indicate that finetuning does not yield positive outcomes. This can be attributed to two potential factors: 1) the poor quality of data, and 2) limited effectiveness in aiding the graph recovery task. We leave this part as the future work."}, {"title": "Graphusion: human evaluation rubric", "content": "Concept Entity Quality\nExcellent (3 points): Both concepts are highly relevant and specific to the domain. At an appropriate level of detail, neither too broad nor too specific. For example, a concept could be introduced by a lecture slide page, or a whole lecture, or possibly have a Wikipedia page.\nAcceptable (2 points): Concept is somewhat relevant, or granularity is acceptable.\nPoor (1 point): Concept is at an inappropriate level of detail, too broad or too specific.\nRelation Quality\nCorrect (3 points): The relation logically and accurately describes the relationship between the head and tail concepts.\nSomewhat Correct (2 points): The relation is acceptable but has minor inaccuracies or there might be another better or correct answer.\nIncorrect (1 point): The relation does not logically describe the relationship between the concepts."}, {"title": "TutorQA", "content": "Benchmark Details\nWe show the data analysis in Tab. 9.\nTask 2 and Task 3: case study\nIn the field of Natural Language Processing, I want to learn about multilingual model. What concepts should I learn first?\nTokenization, Embeddings, Transfer Learning, Cross-lingual Transfer, Zero-shot Learning, Mul- tilingual Corpora, Language Modeling, Fine-tuning, Evaluation Metrics, Pretrained Models\nlanguage models, machine translation, cross-lingual embeddings, transfer learning, tokenization, fine-tuning\nIn the field of Natural Language Processing, I know about natural language processing intro, now I want to learn about t-sne. What concept path should I follow?\nnatural language processing, dimensionality reduction, t-SNE, perplexity, high-dimensional data, data visualization, machine learning\nnatural language processing intro, vector representations, t-sne\nTask 4: case study\nGiven the following edges constituting a concept subgraph, please identify and select the possible type of relationship between natural language generation and natural language understanding.\nIs-a-Prerequisite-of\nConjunction\nTask 6: Human Evaluation Rubrics\nConcept Relevancy (1-5):\nRate how well the project description aligns with the provided query concepts.\nScore 1 if the project is not related to any of the query concepts.\nScore 5 if the project directly addresses and is entirely based on the provided query concepts.\nConcept Coverage (1-5):\nEvaluate how comprehensively the project description covers the query concepts.\nScore 1 if only a minimal or superficial coverage of the concepts is present.\nScore 5 if all query concepts are thoroughly and extensively discussed in the project description."}, {"title": "Task 6: More case study", "content": "Task 6: Case Study To further understand how knowledge graphs could help the reasoning", "description.\nTitle": "Exploring the Impact of Emojis on Textual Emotions; Description: Emojis have become an integral part of online communication...The project will involve the following tasks: 1. Data Collection: We will collect a dataset of text messages...and their corresponding emotional labels (positive", "Preprocessing": "We will preprocess the data by removing stop words...\nTitle: Multi-faceted Analysis of Social Media Discussions on Climate Change"}]}