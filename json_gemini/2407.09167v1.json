{"title": "SE(3)-bi-equivariant Transformers\nfor Point Cloud Assembly", "authors": ["Ziming Wang", "Rebecka J\u00f6rnsten"], "abstract": "Given a pair of point clouds, the goal of assembly is to recover a rigid transformation\nthat aligns one point cloud to the other. This task is challenging because the point\nclouds may be non-overlapped, and they may have arbitrary initial positions.\nTo address these difficulties, we propose a method, called SE(3)-bi-equivariant\ntransformer (BITR), based on the SE(3)-bi-equivariance prior of the task: it\nguarantees that when the inputs are rigidly perturbed, the output will transform\naccordingly. Due to its equivariance property, BITR can not only handle non-\noverlapped PCs, but also guarantee robustness against initial positions. Specifically,\nBITR first extracts features of the inputs using a novel SE(3) \u00d7 SE(3)-transformer,\nand then projects the learned feature to group SE(3) as the output. Moreover,\nwe theoretically show that swap and scale equivariances can be incorporated into\nBITR, thus it further guarantees stable performance under scaling and swapping\nthe inputs. We experimentally show the effectiveness of BITR in practical tasks.", "sections": [{"title": "1 Introduction", "content": "Point cloud (PC) assembly is a fundamental machine learning task with a wide range of applications\nsuch as biology [13], archeology [34], robotics [26] and computer vision [22]. As shown in Fig. 1,\ngiven a pair of 3-D PCs representing two shapes, i.e., a source and a reference PC, the goal of assembly\nis to find a rigid transformation, so that the transformed source PC is aligned to the reference PC.\nThis task is challenging because the input PCs may have random initial positions that are far from the\noptimum, and may be non-overlapped, e.g., due to occlusion or erosion of the object."}, {"title": "2 Related works", "content": "A special case of PC assembly is PC registration, where the correspondence between input PCs is\nassumed to exist. A seminal work in this task was conducted by [2], which provided a closed-form\nsolution to the problem with known correspondence. To handle PCs with unknown correspondence,\nmost of the subsequent works extend [2] by first estimating the correspondence by comparing\ndistances [4], or features [24, 22, 38] of the PCs, and then aligning the PCs by aligning the estimated\ncorresponding points. Notably, to obtain SE(3)-bi-equivariance, SO(3)-invariant features [39, 11,\n40] have been investigated for correspondence estimation. However, since these methods require a\nsufficient number of correspondences, they have difficulty handling PCs where the correspondence\ndoes not exist. In addition, they often have difficulty handling PCs with large initial errors [42].\nThe proposed BITR is related to the existing registration methods because it can be seen as a\ngeneralization of Arun's method [2]. However, in contrast to these methods, BITR is correspondence-"}, {"title": "3 Preliminaries", "content": "This section briefly reviews Arun's method and the concept of equivariance, which will be used in\nBITR."}, {"title": "3.1 Group representation and equivariance", "content": "Given a group G, its representation is a group homomorphism \u03c1 : G \u2192 GL(V), where V is a linear\nspace. When G is the 3D rotation group SO(3), it is convenient to consider its irreps (irreducible\northogonal representation) \u03c1\u209a: SO(3) \u2192 GL(V\u209a), where p \u2208 N is the degree of the irreps, and\ndim(V\u209a) = 2p + 1. For r \u2208 G, \u03c1\u209a(r) \u2208 \u211d^(2p+1)\u00d7(2p+1) is known as the Wigner-D matrix. For\nexample, \u03c1\u2080(r) = 1 for all r \u2208 SO(3); \u03c1\u2081(r) \u2208 \u211d^(3\u00d73) is the rotation matrix of r. More details can be\nfound in [5] and the reference therein.\nIn this work, we focus on the group G of two independent rotations, i.e., G = SO(3) \u00d7 SO(3), where\n\u00d7 represents the direct product. Similar to SO(3), we also consider the irreps of G. A useful fact is\nthat all irreps of G can be written as the combinations of the irreps of SO(3): the degree-(p, q) irreps\nof G is \u03c1\u209a,q = \u03c1\u209a \u2297 \u03c1q : SO(3) \u00d7 SO(3) \u2192 GL(V\u209a \u2297 Vq), where p, q \u2208 N, \u03c1\u209a and \u03c1q are irreps of\nSO(3), and \u2297 is tensor product (Kronecker product for matrix). For example, \u03c1\u2080,\u2080(r\u2081 \u00d7 r\u2082) = 1 \u2208 \u211d;\n\u03c1\u2081,\u2080(r\u2081 \u00d7 r\u2082) \u2208 \u211d^(3\u00d73) is the rotation matrix of r\u2081; \u03c1\u2081,\u2081(r\u2081 \u00d7 r\u2082) = \u03c1\u2081(r\u2081) \u2297 \u03c1\u2081(r\u2082) \u2208 \u211d^(9\u00d79) is the\nKronecker product of the rotation matrices of r\u2081 and r\u2082.\nGiven two representations \u03c1 : G \u2192 GL(V) and \u03c4 : G \u2192 GL(W), a map \u03a6 : V \u2192 W satisfying\n\u03a6(\u03c1(g)x) = \u03c4(g)\u03a6(x) for all g \u2208 G and x \u2208 V is called G-equivariant. When \u03a6 is parametrized by\na neural network, we call \u03a6 an equivariant neural network, and we call the feature extracted by \u03a6 an\nequivariant feature. Specifically, a degree-p equivariant feature transforms according to \u03c1\u209a under the\naction of SO(3), and a degree-(p, q) equivariant feature transforms according to \u03c1\u209a \u2297 \u03c1q under the\naction of SO(3) \u00d7 SO(3). For simpler notations, we omit the representation homomorphism \u03c1, i.e.,\nwe writer instead of \u03c1(r), when \u03c1 is clear from the text."}, {"title": "3.2 Arun's method", "content": "Consider a PC assembly problem with known one-to-one correspondence: Let Y = {y\u1d64}^(N)_(\u1d64=1) \u2286 \u211d\u00b3\nand X = {x\u1d64}^(N)_(\u1d64=1) \u2286 \u211d\u00b3 be a pair of PCs consisting of N points, and let {(x\u1d64, y\u1d64)}^(N)_(\u1d64=1) be their\ncorresponding point pairs. What is the optimal rigid transformation that aligns X to Y?"}, {"title": "4 SE(3)-bi-equivariant transformer", "content": "This section presents the details of the proposed BITR. BITR follows the same principle as Arun's\nmethod [2]: it first extracts SO(3) \u00d7 SO(3)-equivariant features as a generalization of the correlation\nmatrix \u03a3 (2), and then projects the features to SE(3) similarly to (1). Specifically, we first propose\na SE(3) \u00d7 SE(3)-transformer for feature extraction in Sec. 4.2. Since this transformer is defined\non 6-D space, i.e., it does not directly handle the given 3-D PCs, it relies on a pre-processing step\ndescribed in Sec. 4.3, where the input 3-D PCs are merged into a 6-D PC. Finally, the Arun-type\nSE(3)-projection is presented in Sec. 4.4. An overview of BITR is presented in Fig. 2."}, {"title": "4.1 Problem formulation", "content": "Let Y = {y\u1d65}^(N)_(\u1d65=1) \u2286 \u211d\u00b3 and X = {x\u1d64}^(M)_(\u1d64=1) \u2286 \u211d\u00b3 be the PCs sampled from the reference and source\nshape respectively. The goal of assembly is to find a rigid transformation g \u2208 SE(3), so that the\nM\ntransformed PC gX = {rxi + t}^(M)_(i=1) is aligned to Y. Note that we do not assume that X and Y are\noverlapped, i.e., we do not assume the existence of corresponding point pairs."}, {"title": "4.2 SE(3) \u00d7 SE(3)-transformer", "content": "To learn SO(3) \u00d7 SO(3)-equivariant translation-invariant features generalizing \u03a3 (1), this subsection\nproposes a SE(3) \u00d7 SE(3)-transformer as a generalization of SE(3)-transformer [12]. We present\na brief introduction to SE(3)-transformer [12] in Appx. A for completeness.\nAccording to the theories developed in [5], to define a SE(3) \u00d7 SE(3)-equivariant transformer, we\nfirst need to define the feature map of a transformer layer as a tensor field, and specify the action"}, {"title": "4.3 Point cloud merge", "content": "To utilize the transformer model defined in Sec. 4.2, we need to construct a 6-D PC as its input. To\nthis end, we first extract key points from the raw 3-D PCs, and then concatenate them to a 6-D PC to\nmerge their information. Thus, the resulting 6-D PC is not only small in size but also contains the key\ninformation of the raw PCs pairs.\nFormally, we extract L ordered key points X = {x\u1d64}^(L)_(\u1d64=1) and \u1ef8 = {y\u1d64}^(L)_(\u1d64=1) from X and Y\nrespectively, and then obtain Z = {x\u1d64 \u2295 y\u1d64}^(L)_(\u1d64=1). Note that we do not require X (Y) to be a subset\nof X (Y). Specifically, we represent the coordinates of the key points as a convex combination of the\nraw PCs:\n\\documentclass{article}\n\\usepackage{amsmath}\n\nX = \\text{SoftMax}(F^(X))X, \\quad \\tilde{Y} = \\text{SoftMax}(F^(Y))Y,\n(10)\nwhere X \u2208 \u211d^(M\u00d73) and Y \u2208 \u211d^(N\u00d73) represent the coordinates of X and Y respectively, and SoftMax(\u00b7)\nrepresents the row-wise softmax. F^(X) \u2208 \u211d^(L\u00d7M) and F^(Y) \u2208 \u211d^(L\u00d7N) are the weights of each point in\nX and Y respectively, and they are degree-0, i.e., rotation-invariant, features computed by a shared\nSE(3)-transformer \u03a6\u1d07:\n\\documentclass{article}\n\\usepackage{amsmath}\n\nF^(X) = \\Phi_E(X), \\quad F^(Y) = \\Phi_E(Y).\n(11)\nFurthermore, inspired by [37], we fuse the features of X and Y in \u03a6\u1d07 before the last layer, so that\ntheir information is merged more effectively, i.e., the selection of X or \u0176 depends on both X and Y.\nSpecifically, the fused features are\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{cases}\nf_{\\text{out},X}(x_u) = f_{\\text{in},X}(x_u) + \\text{Pool}_v \\left( f_{\\text{in},Y}(y_v) \\right) \\oplus f_{\\text{in},X}(x_u) \\\\\nf_{\\text{out},Y}(y_v) = f_{\\text{in},Y}(y_v) + \\text{Pool}_u \\left( f_{\\text{in},X}(x_u) \\right) \\oplus f_{\\text{in},Y}(y_v),\n\\end{cases}\n(12)\nwhere we only consider degree-0 and degree-1 features. f.,X and f.,Y represent the features of X\nand Y, and Pool is the average pooling over the PC."}, {"title": "4.4 SE(3)-projection", "content": "We now obtain the final output by projecting the feature extracted by the SE(3) \u00d7 SE(3)-transformer\nto SE(3). Formally, let f be the output tensor field of the SE(3) \u00d7 SE(3)-transformer. We compute\nthe final output g = (r, t) \u2208 SE(3) using an Arun-type projection as follows:\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{aligned}\n& \\hat{r} = \\text{SVD}(\\mathfrak{r}) \\\\\n& t = (m(Y) + t_y) - \\hat{r}(m(X) + t_x),\n\\end{aligned}\n(13)\nwhere \\hat{r} = \\text{unvec}(\\mathfrak{r}) \u2208 \u211d^(3\u00d73), t\u2093 \u2208 \u211d\u00b3 and t\u1d67 \u2208 \u211d\u00b3 are equivariant features computed as\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\mathfrak{r} = \\text{Pool}_u(f_{11}), \\quad t_x = \\text{Pool}_u(f_{1,0}), \\quad t_y = \\text{Pool}_u(f_{0,1}).\nWe note that projection (13) extends Arun's projection (1) in two aspects. First, although \\hat{r} in (13) and\n\u03a3 (2) are both degree-(1, 1) features, \\hat{r} is more flexible than \u03a3 because \\hat{r} is a learned feature while \u03a3 is\nhandcrafted, and \u03a3 is correspondence-free while \u03a3 is correspondence-based. Second, projection (13)\nexplicitly considers non-zero offsets t\u2093 and t\u1d67, which allow solutions where the centers of PCs do\nnot match.\nIn summary, BITR computes the output g for PCs X and Y according to\n\\documentclass{article}\n\\usepackage{amsmath}\n\ng = \\Phi_P \\circ \\Phi_S(X, Y),\n(14)\nwhere \u03a6s: S \u00d7 S \u2192 F is a SE(3) \u00d7 SE(3)-transformer (with the PC merge step), \u03a6p : F \u2192 SE(3)\nrepresents projection (13), and F is the set of tensor field. We finish this section with a proposition\nthat BITR is indeed SE(3)-bi-equivariant."}, {"title": "5 Swap-equivariance and scale-equivariance", "content": "This section seeks to incorporate swap and scale equivariances into the proposed BITR model. These\ntwo equivariances are discussed in Sec. 5.1 and Sec. 5.2 respectively."}, {"title": "5.1 Incorporating swap-equivariance", "content": "This subsection seeks to incorporate swap-equivariance to BITR, i.e., to ensure that swapping X\nand Y has the correct influence on the output. To this end, we need to treat the group of swap as\n\u2124/2\u2124 = {1, s} where s\u00b2 = 1, i.e., s represents the swap of X and Y, and properly define the action\nof \u2124/2\u2124 on the learned features.\nFormally, we define the action of \u2124/2\u2124 on field f (3) as follows. We first define the action of s on the\nbase space \u211d\u2076 as swapping the coordinates of X and \u1ef8: s(z) = z\u00b2 \u2295 z\u00b9, where z = z\u00b9 \u2295 z\u00b2, and\nz\u00b9, z\u00b2 \u2208 \u211d\u00b3 are the coordinates of X and Y respectively. Then we define the action of s on feature f\nas (s(f))^(p,q)(z) = (f^(q,p) (s(z))), where we regard a degree-(p, q) feature f^(p,q) as a matrix of shape\n\u211d^(2p+1)\u00d7(2q+1) by abuse of notation, and (\u00b7)\u1d40 represents matrix transpose.\nIntuitively, according to the above definition, degree-(1, 1), (1, 0) and (0, 1) features will become\n(the transpose of) degree-(1, 1), (0, 1) and (1, 0) features respectively under the action of s, i.e., \u03a3\nwill be transposed, t\u2093 and t\u1d67 will be swapped. This is exactly the transformation needed to ensure\nswap-equivariant outputs. We formally state this observation in the following proposition.\nNow the remaining problem is how to make a SE(3) \u00d7 SE(3)-transformer \u2124/2\u2124-equivariant. A\nnatural solution is to force all layers in the SE(3) \u00d7 SE(3)-transformer to be \u2124/2\u2124-equivariant. The\nfollowing proposition provides a concrete way to achieve this."}, {"title": "5.2 Incorporating scale-equivariance", "content": "This subsection seeks to incorporate scale equivariance to BITR, i.e., to ensure that when X and Y\nare multiplied by a scale constant c \u2208 \u211d+, the output result transforms correctly. To this end, we\nneed to consider the scale group (\u211d+, \u00d7), i.e., the multiplicative group of \u211d+, and properly define\nthe (\u211d+,\u00d7)-equivariance of the learned feature. For simplicity, we abbreviate group (\u211d+, \u00d7) as \u211d+.\nWe now consider the action of \u211d+ on field f (3). We call f a degree-p \u211d+-equivariant field (p \u2208 \u2115)\nif it transforms as (c(f))(z) = c^pf(c\u207b\u00b9z) under the action of \u211d+, where z \u2208 \u211d\u2076 and c \u2208 \u211d+.\nWe immediately observe that degree-1 \u211d+-equivariant features lead to scale-equivariant output.\nIntuitively, if r, t\u2093 and t\u1d67 are degree-1 \u211d+-equivariant features, then they will become cr, ct\u2093 and\nct\u1d67 under the action of c, and the projection step will cancel the scale off while keeping the scale of\nt\u2093 and t\u1d67, which is exactly the desirable results. Formally, we have the following proposition.\nNow the remaining problem is how to ensure that a SE(3) \u00d7 SE(3)-transformer is \u211d+-equivariant and\nits output is of degree-1, so that scaling the input can lead to the proper scaling of output. Here we\nprovide a solution based on the following proposition.\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n1) Denote \\varphi^K \\text{ and } \\varphi^V \\text{ the radial functions used in K and V respectively. Let } \\phi \\text{ be a }\ndegree-0 function, } f_{\\text{in}} \\text{ be a degree-0 } \\mathbb{R}_+ \\text{-equivariant input field. For transformer layer (5), if } \\varphi^V\n\\text{ is a degree-1 function and the self-interaction weight } W=0, \\text{ then the output field } f_{\\text{out}} \\text{ is degree-1 }\n\\mathbb{R}_+ \\text{-equivariant; If } \\varphi^V \\text{ is a degree-0 function, then the output field } f_{\\text{out}} \\text{ is degree-0 }\\mathbb{R}_+ \\text{-equivariant.}\\\\\n\\text{2) For Elu layer (9), if the input field is degree-p } \\mathbb{R}_+ \\text{-equivariant, then the output field is also degree-p}\n\\mathbb{R}_+ \\text{-equivariant.}\n\\end{document}\n\n\\end{document}"}, {"title": "6 Experiments and analysis", "content": "This section experimentally evaluates the proposed BITR. After describing the experiment settings in\nSec. 6.1, we first present a simple example in Sec. 6.2 to highlight the equivariance of BITR. Then\nwe evaluate BITR on assembling the shapes in ShapeNet [6], BB dataset [29], 7Scenes [30] and\nASL [21] from Sec. 6.3.1 to Sec. 6.4. We finally apply BITR to visual manipulation tasks in Sec. 6.6."}, {"title": "6.1 Experiment settings", "content": "We extract L = 32 key points for each PC. The SE(3)-transformer and the SE(3) \u00d7 SE(3)-\ntransformer both contain 2 layers with c = 4 channels. We consider k = 24 nearest neighborhoods\nfor message passing. We only consider low degree equivariant features, i.e., p,q \u2208 {0,1} for\nefficiency. We train BITR using Adam optimizer [17] with learning rate 1e-4. We use the loss\nfunction L = ||\\hat{r}\u1d40rgt \u2013 I||\u2082 + ||\\hat{t}tgt - t||\u2082, where (r, t) are the output transformation, (rgt, tgt) are\nthe corresponding ground truth. We evaluate all methods by isotropic rotation and translation errors:\n\u0394r = (180/\u03c0)arccos (1/2 (tr(\\hat{r}\u1d40rgt) \u2013 1))), and \u0394t = ||\\hat{t}tgt - t|| where tr(\u00b7) is the trace of a matrix.\nWe do not use random rotation and translation augmentations as [22]. More details are in Appx. D.1."}, {"title": "6.2 A proof-of-concept example", "content": "To demonstrate the equivariance property of BITR, we train BITR on the bunny shape [31]. In each\ntraining iteration, we first construct the raw PC S by uniformly sampling 2048 points from the bunny\nshape and adding 200 random outliers from [-1, 1]\u00b3, then we obtain PCs {X\u209a, Y\u209a} by dividing S\ninto two parts of ratio (30%, 70%) using a random plane P. We train BITR to reconstruct S using\n{X\u209a, Y\u209a}. To construct the test set, we generate a new sample {X\u209a, Y\u209a}, and additionally construct\n3 test samples by 1) swapping, 2) scaling (factor 2) and 3) randomly rigidly perturbing {X\u209a, Y\u209a}.\nThe assembly results of BITR on these four test samples are shown in Fig. 3. We observe that\nBITR performs equally well in all cases. Specifically, the differences between the rotation errors in\nthese four cases are small (less than 1e-3). The results suggest that BITR is indeed robust against\nthese three perturbations, which verifies its swap-equivariance, scale-equivariance and SE(3)-bi-\nequivariance. More experiments can be found in the appendix: a numerical verification of Def. 3.1 is\npresented in Appx. D.2, an ablation study of swap and scale equivariances are presented in Appx. D.3,\nand the verification of the complete-matching property C.12 is presented in Appx. D.4."}, {"title": "6.3 Results on ShapeNet", "content": "In this experiment, we evaluate BITR on assembling PCs sampled from a single shape. When the\ninputs PCs are overlapped, this setting is generally known as PC registration. We construct a dataset\nsimilar to [38]: for a shape in the airplane class of ShapeNet [6], we obtain each of the input PCs by\nuniformly sampling 1024 points from the shape, and keep ratio s of the raw PC by cropping it using\na random plane. We vary s from 0.7 to 0.3. Note the PCs may be non-overlapped when s < 0.5.\nWe compare BITR against the state-of-the-art registration methods GEO [22] and ROI [40], and\nthe state-of-the-art fragment reassembly methods NSM [7] and LEV [36]. For NSM and LEV, we"}, {"title": "6.3.2 Inter-class assembly", "content": "To evaluate BITR on non-overlapped PCs, we extend the exper\niment in Sec. 6.3.1 to inter-class assembly. We train BITR to\nplace a car shape on the right of motorbike shape, so that their\ndirections are the same and their distance is 1. We consider\ns = 1.0 and 0.7. Note that this task is beyond the scope of\nregistration methods, since the input PCs are non-overlapped.\nA result of BITR is shown in Fig. 5. More details can be found\nin Appx. D.6."}, {"title": "6.4 Results on fragment reassembly", "content": "This subsection evaluates BITR on a fragment reassembly task. We compare BITR against NSM [7],\nLEV [36] and DGL [41] on the 2-fragment WineBottle class of the BB dataset [29]. The data\npreprocessing step is described in Appx. D.7.\nWe test the trained BITR 3 times, and report\nthe mean and standard deviation of (\u0394r, \u0394t)\nin Tab. 1. We observe that BITR outperforms\nall baseline methods: BITR achieves the low-\nest rotation errors, and its translation error is\ncomparable to DGL, which is lower than other\nbaselines by a large margin. We provide some\nqualitative comparisons in Appx. D.7."}, {"title": "6.5 Results on real data", "content": "This subsection evaluates BITR on an indoor dataset 7Scenes [30] and the outdoor scenes in ASL\ndataset [21]. We train BITR to align the adjacent frames that are arbitrarily rigidly perturbed. The\nresults are reported in Appx. D.8."}, {"title": "6.6 Results on visual manipulation", "content": "This subsection applies BITR to visual manipulation tasks. Following [25], we consider two tasks:\nmug-hanging and bowl-placing, where the goal is to find a rigid transformation so that the cup can be\nhung to the stand, or the bowl can be placed on the plate. The details can be found in Appx. D.9."}, {"title": "7 Conclusion", "content": "This work proposed a PC assembly method, called BITR. The most distinguished feature of BITR is\nthat it is correspondence-free, SE(3)-bi-equivariance, scale-equivariance and swap-equivariance. We\nexperimentally demonstrated the effectiveness of BITR. The limitation of BITR and future research\ndirections are discussed in Appx. E."}, {"title": "A SE(3)-equivariant Transformers", "content": "A well-known SE(3)-equivariant network is SE(3)-transformer [12], which adapts the powerful\ntransformer structure [33] to SO(3)-equivariant settings. In this model, the feature map f of each\nlayer is defined as a tensor field supported on a 3-D PC:\n\\documentclass{article}\n\\usepackage{amsmath}\n\nf(x) = \\sum_{u=1}^M f_u\\delta(x - x_u),\n(15)\nwhere \u03b4 is the Dirac function, X = {x\u1d64}^(M)_(\u1d64=1) \u2286 \u211d\u00b3 is a point set, and f\u1d64 is the feature attached to x\u1d64.\nHere, feature f\u1d64 takes the form of f\u1d64 = \u2295\u209a f\u209a, where the component f\u209a \u2208 V\u209a is the degree-p feature,\ni.e., it transforms according to \u03c1\u209a under the action of SO(3). For example, when f\u1d64 represents the\nnorm vector of a point cloud, then f\u1d64 = f\u2081 \u2208 \u211d\u00b3. We also write the collection of all degree-p\nfeatures at x\u1d64 as F^(p)(x\u1d64) \u2208 \u2102^(c\u00d7(2p+1)), where c is the number of channels.\nFor each transformer layer, the degree-k output feature at point xi is computed by performing message\npassing:\n\\documentclass{article}\n\\usepackage{amsmath}\n\nf_{\\text{out}}^k(x_u) = W^k F^k(x_u) + \\sum_{l} \\sum_{v \\in \\mathcal{N}(u)\\{u\\}} \\alpha_{uv} \\text{V}^{lk} f^l(x_v),\n(16)\nwhere N (u) represents the neighborhood of u, W\u1d4f \u2208 \u211d^(1\u00d7c) is the learnable weight for self-interaction,\nc represents the number of channels, and\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\alpha_{uv} = \\frac{\\exp \\left( \\langle Q_u, K_v \\rangle \\right)}{\\sum_{v' \\in \\mathcal{N}(u)\\{u\\}} \\exp \\left( \\langle Q_u, K_{v'} \\rangle \\right)}\n(17)\nis the attention from v to u. Here, key K, value V and query Q are\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{aligned}\nQ_u &= \\oplus_{l} W^Q F^l(X_u), \\\nK_{uv} &= \\oplus_{l} \\sum_{k} W^{K lk}(x_v - x_u) f^k(x_v), \\\nV_{uv} &= \\oplus_{k} W^{V lk}(x_v - x_u) f^k(x_v)\n\\end{aligned}\n(18)\nwhere W \u2208 \u211d^(1\u00d7(2l+1)) is a learnable weight, and the kernel W^(lk)(x) \u2208 \u211d^(2l+1)\u00d7(2k+1) is defined as\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\text{vec}(W^{lk}(x)) = \\sum_{J=|k-l|}^{k+l} \\varphi(\\vert\\vert x \\vert\\vert) \\mathcal{Q}Y_J(\\vec{x}),\n(19)\nwhere vec(\u00b7) is the vectorize function, the learnable radial component \u03c6 : \u211d+ \u2192 \u211d is parametrized\nby a neural network, and the non-learnable angular component is determined by Clebsch-Gordan\nconstant Q and the spherical harmonic Y\u1d0a : \u211d\u00b3 \u2192 \u211d^(2J+1)."}, {"title": "B Derive of the Convolutional Kernel", "content": "To derive the kernel (8) for a SE(3) \u00d7 SE(3)-transformer layer, we consider the equivariant convo-\nlution as a simplified version of the SE(3) \u00d7 SE(3)-transformer layer:\n\\documentclass{article}\n\\usepackage{amsmath}\n\n(W \\ast f)^o(z_u) := \\sum_{v \\in KNN(u)\\{u\\}} W^{oi}(z_v - z_u) f^i(z_v),\n(20)\ni.e., we only consider the message V\u1d43\u1da6 while fixing the self-interaction weight W = 0 and attention\n\u03b1\u1d64\u1d65 = 1 in (5). To ensure the SE(3) \u00d7 SE(3)-equivariance of convolution (20), i.e.,\n\\documentclass{article}\n\\usepackage{amsmath}\n\n((g_1 \\times g_2)(W \\ast f))(z) = (W \\ast ((g_1 \\times g_2)f))(z),\n(21)\nthe kernel W must satisfy a constraint:\n\\documentclass{article}\n\\usepackage{amsmath}\n\n(\\rho_i(r_{12}) \\otimes \\rho_o(r_{12}))W(z) = W(\\vec{r_{12}}z),\n(22)\nwhere we abbreviate r\u2081 \u00d7 r\u2082 as r\u2081\u2082, abbreviate vec(W^(o,i)) \u2208 \u211d^(2i\u2081+1)(2i\u2082+1)(2o\u2081+1)(2o\u2082+1) as W,\nand assume ||z\u00b9|| = ||z\u00b2|| = 1 for simpler notations. Equation (22) is generally known as the kernel\nconstraint, and its necessity and sufficiency can be proved in a verbatim way as Theorem 2 in [35],"}, {"title": "C Proofs and theoretical results", "content": "We first establish the result on the uniqueness of Arun's method. We begin with the result of the\nuniqueness of SVD.\nLet A = U \u03a3 V\u1d40 \u2208 \u211d^(3\u00d73) be the SVD decomposition, where \u03a3 = diag(\u03c3\u2081, \u03c3\u2082, \u03c3\u2083)\nis a diagonal matrix, and U, V \u2208 O(3). If the singular values in \u03a3 are distinct and nonzero, i.e.,"}, {"title": "D More experiment results", "content": "We run all experiments using a Nvidia T4 GPU card with 16G memory. The batch size is set to the\nlargest possible value that can be fitted into the GPU memory. We set bs = 16 for the airplane dataset,\nand bs = 4 for the wine bottle dataset. We train BITR until the validation loss does not decrease.\nFor the airplane dataset, we train BITR 10000 epochs, and the training time is about 8 days when\ns = 0.7. For the wine bottle dataset, we train BITR 1000 epochs, and the training time is about 12\nhours. The FLOPS is 14.5G in a forward pass (including the computation of harmonic functions),\nand the model contains 0.17M parameters.\nFor Sec. 6.3.1 and 6.3.2, we use the normal vector computed by Open3D [43] as the input feature of\nBITR. The airplane dataset used in Sec. 6.3.1 contains 715 random training samples and 103 random\ntest samples. The wine bottle dataset used in Sec. 6.4 contains 331 training and 41 test samples. We\nadopt the few-shot learning setting in the manipulation tasks in Sec. 6.6: we use 30 training and 5\ntest samples for mug-hanging; we use 40 training samples and 10 test samples for bowl-placing."}, {"title": "D.2 More results of Sec. 6.2", "content": "We quantitatively verify the equivariance of BITR according to Def. 3.1. Specifically, we compute\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{aligned}\n\\Delta_{bi} &= ||\\Phi_B(g_1 X, g_2 Y) -```json\n   g_2 \\Phi_B(X, Y) g_1^{-1}||_F, \\\\\n\\Delta_{swap} &= ||\\Phi_B(Y, X) - (\\Phi_B(X, Y))^{-1}||_F, \\\\\n\\Delta_{scale} &= ||r_B(cX, cY) - r_B(X, Y)||_F + ||t_B(cX, cY) - c t_P(X, Y) ||_2,\n\\end{aligned}\n(77)\n(78)\n(79)\nto verify the SE(3)-bi-equivariance, swap-equivariance and scale-equivariance of BITR, where \u03a6\u0299\nrepresent the BITR model, (r\u0299(\u00b7), t\u1d18(\u00b7)) = \u03a6\u0299(\u00b7) are the output of BITR, and all g = (r, t) \u2208 SE(3)\nare written as\n\\documentclass{article}\n\\usepackage{amsmath}\n\ng = \\begin{bmatrix} r & t \\\\ 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{4,4}.\n(80)\n\\end{document}\nNote that if BITR is perfectly equivariant, these three errors should always be 0.\nThe quantitative results of the experiment are summarized in Tab. 2, where we can see that all errors\nare below the numerical precision of float numbers, i.e., less than 1e-5. The results suggest that\nBITR is indeed SE(3)-bi-equivariant, swap-equivariant and scale-equivariant."}, {"title": "D.3 Ablation study", "content": "To show the practical effectiveness of our theory on scale and swap equivariances, we consider an\nablation study. We use the same data as in Sec. 6.2, and remove the weight sharing technique in\nSec. 5.1 to break swap-equivariance, and force or in all layers to be a degree-1 function to break the\nscale-equivariance.\nWe evaluate the trained model on 100 test samples, and report the mean and standard deviation of \u0394r\nin Tab. 3, where we can see that removing an equivariance of the model leads to the failure in the\ncorresponding test case, which is consistent with our theory."}, {"title": "D.4 Evaluation of the complete-matching property", "content": "This experiment numerically evaluates the robustness of the complete-matching property (Prop. \u0421.12)\nagainst resampling, noise and partial visibility. We first sample X and Y of size 1024 from the\nbunny shape, and a random g \u2208 SE(3), then we use a random initialized U-BITR to match X to\ngY. We consider different settings: 1) X and Y are exactly the same; 2) X and Y are different\nrandom samples; 3) Gaussian noise of std 0.01 is added to X and Y; 4) Ratio s of X and Y is kept\nby cropping using a random plane.\nWe repeat the experiment 3 times, and report the results of U-BITR in Tab. 4. We observe that the\ntransformation is perfectly recovered when X = Y, which is consistent with the complete-matching\nproperty. Meanwhile, cropping the PCs leads to large decrease of the accuracy, while noise and\nresampling have less effect. This is consistent with our expectation because cropping the PCs has\nlarger effect on the shape of PCs. We provide qualitative results in Fig. 6."}, {"title": "D.5 More results of Sec. 6.3.1", "content": "We report the training process of BITR in Fig. 7, where we can see that the loss value, \u0394r and \u0394t\ngradually decrease during training as expected.\nSome qualitative results of BITR are presented in Fig. 8. We represent the input PCs using light\ncolors, and represent the 32 learned key points using dark colors and large points. As we explained\nin Sec. 4.3, the key points are in the convex hull of the input PCs, and they are NOT a subset of the\ninput PC. In addition, as can be seen, the key points of the inputs do not overlap."}, {"title": "D.6 More results of Sec. 6.3.2", "content": "We generate the raw training samples by sampling motorbike and car shapes from the training set of\nShapeNet. Then we centralize them, and move the car by [0, 0, 1]. Note the shapes in ShapeNet are\nalready pre-aligned. The test samples are generated from the test set of ShapeNet in the same way.\nWe repeat the test process 3 times, and report the results in Fig. 9. We observe that BITR achieves\nlower rotation error than LEV, and their translation errors are comparable. Meanwhile, NSM fails in\nthis experiment. Note that we do not report the results of registration methods, because their loss\nfunctions are undefined due to the lack of correspondence."}, {"title": "D.7 More results of Sec. 6.4", "content": "For BITR, we first obtain raw PCs by applying grid sampling with grid size 0.005 to the shape, and\nthen randomly sample 5% points from the raw PCs as the training and test samples. The sizes of the\nresulting PCs are around 1000, which is close to the data used in the baseline methods. The data is\npre-processed following [36] for the baseline methods.\nThe random sampling process in our method causes the randomness of test error. We quantify the\nrandomness by evaluating on the test set 3 times, and report the mean and std of the errors in Tab. 1."}, {"title": "D.8 More results of Sec. 6.5", "content": "We preprocess the 7Scenes dataset by applying grid sampling with grid size 0.1. We arbitrarily rotate\nand translate all data, and train BITR to align all adjacent frames. This leads to a training set of size\n278 from 6 scenes, and a test set of size 59 from a different scene. To train BITR, we use a random\nclipping augmentation: we keep ratio s of each PCs by clipping them using a random plane, where s\nis uniformly distributed in [0.5, 1.0]. We compare BITR against GEO [22] and ROI [40], ICP [42]\nand OMN [37], where OMN is a recently proposed correspondence-free registration method."}, {"title": "D.9 More results of Sec. 6.6", "content": "For both tasks, PC X represents an object grasped by a robotic arm, i.e., a cup or a bowl, and PC Y\nrepresents the fixed environment with a target, i.e., a stand or a plate. All data are generated using\nPyBullet [9], where the objects are different in shape and position. We stress that the goal of this\nexperiment is to investigate the potential of BITR in manipulation tasks, instead of demonstrating the\nsuperiority of performance.\nFig. 13 presents a result of BITR on mug-hanging. We observe that although BITR is not originally\ndesigned for manipulation tasks, it can place the mug/bowl in a reasonable position relative to the\nstand/plate. However, we also notice that BITR may produce unrealistic results, e.g., the PCs may\ncollide. Thus, post-processing steps such as collision detection [28] may be necessary in practical\napplications.\nNote that we do not report the quantitative results for this experiment because the metric (\u0394r, \u0394t) is\nambiguous. For example, the bowl can be rotated arbitrarily while being on the plate. The assembly\nmethods such as NSM [7] and LEV [36] are not applicable because the canonical pose is not known,\nand we do not report the result of any registration method because the correspondence does not exist."}, {"title": "E Limitations and future research directions", "content": "BITR in its current form has two main limitations: First, although we have accelerated most of the\nlayers of BITR using the \u201cscatter\" function [23], BITR is still relatively slow due to the independent\ncomputation of convolutional kernels, i.e., the harmonic function and the independent multiplication"}]}