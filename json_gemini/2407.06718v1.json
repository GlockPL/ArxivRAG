{"title": "A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts", "authors": ["Atilla \u00d6zg\u00fcr", "Y\u0131lmaz Uygun"], "abstract": "This study proposes a simple architecture for Enterprise application for Large Language Models (LLMs) for role based security and NA\u03a4\u039f clearance levels. Our proposal aims to address the limitations of current LLMs in handling security and information access. The proposed architecture could be used while utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of experts models (MoE). It could be used only with RAG, or only with MoE or with both of them. Using roles and security clearance level of the user, documents in RAG and experts in MoE are filtered. This way information leakage is prevented.", "sections": [{"title": "1 Introduction", "content": "According to World Economic Forum [9], Venture Capital investments in the area of artificial intelligence are about $290 billion in the last 5 years. With the introduction of ChatGPT by OpenAI [21], interest in the large language models (LLMs) has exploded.\nEven though ChatGPT provides an application programming interface (API) to the developers, it is a closed model. Developers could only utilize provided API. To answer ChatGPT from OpenAI, large language model called LLaMA"}, {"title": "2 Background Information", "content": "To be able to understand proposed model more easily, some background information is needed. In this section, this background information about NATO clearance levels, role based security, introduction to large language models, Retrieval-Augmented Generation (RAG) and Mixture of experts are given."}, {"title": "2.1 \u039d\u0391TO Clearance Levels", "content": "The North Atlantic Treaty Organization (NATO) is a military alliance. NATO consists of 32 member states and is established after World War II. NATO like many international organizations deals with sensitive information. This information can range from battle plans to diplomatic communications and intelligence reports. Leaking this information could have serious consequences [4].\nNATO has following four security classifications:\n1. Cosmic top secret\n2. Secret\n3. Confidential\n4. Restricted\nEven though it is not counted among the classifications, not classified category is also used to show that a document or an information could be shared outside NATO but their rights belong to N\u0391\u03a4\u039f [24].\nClearance levels or security classifications are used for following purposes: Protect Classified Information, Minimize Risks and Ensure Trust Between Allies Classified information is categorized according to its sensitivity. For example size information of F-35 fighter jet could be not classified while radar sensitive painting information of F-35 could be Cosmic top secret. In short, NATO clearance levels are a security categorization designed to prevent sensitive information."}, {"title": "2.2 Role-Based Access Control (RBAC)", "content": "Role-Based Access Control (RBAC) is a security approach that manages access to resources within a system. Idea of RBAC has been around since the start of multi user computers [26]. RBAC resolves around permissions, roles and user.\nPermissions control access to resources or abilities of users. An example for permission could be log on to system. Roles are used as containers for permissions. For example a human resources (HR) role could be used to contain permission related to personnel management tasks. In most systems, a role could also contain other roles. Similar to Roles, Users also could belong to more than one role.\nWhen security clearance levels are used, most of the time, these clearance levels names are extended together with normal roles. For example, Operator role will be extended as Operator not classified, Operator restricted and so on."}, {"title": "2.3 Introduction to large language models", "content": "Large Language Models are supervised learning models. That is they are trained with using known input and outputs. LLMs are trained using a lot of text from internet to repeatedly predict next words using previous words. See Table 1 for an example sentence: My favorite food is a D\u00f6ner with spicy sauce."}, {"title": "2.3.1 Training of large language models", "content": "For training LLMs, terabytes of text from internet are used. Basic workflow for training LLM are something like below:\n1. Download 10+TB of text\n2. Get a cluster of 6k+ GPUs\n3. Train your neural network, pay $2M, wait for 12 days\n4. Obtain base model\nThis training could be seen as something like compressing the training dataset. Since LLMs could only produce values that are in their training data, for specialized tasks fine tuning of LLMs should be done. Fine tuning should be done with quality data specially prepared for the task. Basic workflow for fine tuning LLM is something like below:\n1. create curated dataset of quality instructions\n2. fine tune base model on this data, wait 1 day\n3. Obtain assistant model\n4. Test your model if necessary go to step 1\n5. Deploy on your servers\n6. Monitor, go to step 1\nSince fine tuning uses less data, it costs less and could be repeated more. When using Mixture of experts, first fine tuning would be slower since we will need to fine tune multiple expert LLMs. But subsequent fine tuning for experts will be faster since whenever new documents come only the relevant experts will be re-fine tuned."}, {"title": "2.4 Retrieval-Augmented Generation (RAG)", "content": "As we have explained in section 2.3, LLMs are trained with massive amounts of data. As all machine learning models, LLMs are also depended on statistical patterns in their training data. For example, an LLM model, which is trained in 2023, will not be able to answer questions about Euro 2024 (European Football Championship).\nRetrieval-Augmented Generation (RAG) introduced by Facebook researchers[16] address these limitation by connecting LLMs to update data sources. These sources could be news articles, company internal knowledge base or databases like wikipedia.\nRAG workflow could be seen in Figure 2"}, {"title": "2.5 Mixture of experts", "content": "Jacobs et al [15] introduced mixture-of-experts (MoE) model in 1991. The MoE models utilizes individual neural networks as experts instead of a single neural network. The idea of mixture of experts are very similar to ensemble learning [23]. Ensemble learning combines multiple classifiers in different fashion. According to Polikar [23], among the first examples of Ensemble Learning [6] was in 1979 by Dasarathy and Sheela. Mixture of experts name is more popular in deep learning literature while ensemble learning is more popular in general machine learning literature.\nIn mixture of experts, different neural networks to solve the problem are called experts. Mixture of expert model uses a router or coordinator neural network to decide which experts to utilize [20]. Router assigns a \"gating score\" to each network to indicate how relevant the input to the expert is. A softmax function is used to transform the gating scores to a probability distribution. When doing the predictions only experts with highest gating scores are activated. This router idea is very similar to voting in the ensemble classifiers. Voting classifiers are used in a lot of different domains like intrusion detection [32, 2, 25, 35, 34]\nIn our role based access control architecture, gating scores of some experts will be zero according to the roles of the caller. For example, a normal user is calling the large language model with a human resource prompt. Only the experts which normal user has access to will be activated. Other experts like human resources expert will not be called at all."}, {"title": "3 Proposed Architecture based on Role based security and Clearance Levels", "content": ""}, {"title": "3.1 User, Role, Security Clearance to Documents Mapping", "content": "Most enterprises map their users to one or more roles. This mapping could be stored in directory services like active directory (Microsoft) or Enterprise Resource planning databases. For our LLM application, this user to role and user to security clearance levels mappings should be accessible from its programming interface. Additionally, role to documents and security clearance levels to documents should also be accessible. This mapping information could be stored in LLM application's own database or a web service could be provided to the application. Basic mapping could be seen in Figure 3.\nEvery User has more than one role, every User has exactly one security clearance. Every Document has exactly one security clearance. Every Role has access to zero or more documents. It is implied that if a user does not have access to necessary clearance level, he will not be able to access the document."}, {"title": "3.2 Training LLM using Roles and Security Clearance Levels", "content": "Proposed architecture presumes using local open source LLMs due to security requirements. But if strict local security is not required then commercial LLMs like OpenAI ChatGPT with Retrieval-Augmented Generation (RAG) could be used. When commercial LLMs are used, training step may not be necessary according to requirements or only not confidential documents could be used for training.\nFor local open source LLMS, Mixture of Experts model should be used. For this case, it is advised to train multiple experts for every role multiplied by five, that is four clearance levels plus not classified. For example, let's assume that the LLM application has four roles: HR, Accounting, Normal User, IT as shown in Figure 1. Then, we should train 4 * 5 = 20 experts. This experts will be named like HR Not Classified, HR Restricted, HR Confidential, HR Secret, HR Cosmic top secret. See full names in Table 2."}, {"title": "3.3 Inference using Retrieval-Augmented Generation (RAG)", "content": "While doing the inference using Retrieval-Augmented Generation (RAG), role information and security clearance levels should be used. Most of the vector databases allow use of filters. Using filters, only documents a user has access to should be returned.\nAs could be seen in Figure 4, this architecture does not presume mixture of experts or local open source model; therefore, this role based model could also be used with commercial LLMs like ChatGPT."}, {"title": "3.4 Inference using Mixture of experts models (MoE)", "content": "In this inference model, local open source LLM should be used. Experts in MoE are trained using only documents for which role and security clearance level has an access to. For performance reasons, filtering in the router part of MoE would be useful. If router does not ask answers from experts for whom user has no security clearance or necessary role, the application will perform faster. Full workflow could be seen in Figure 5."}, {"title": "3.5 Inference using both RAG and MoE", "content": "Doing inference using both Retrieval-Augmented Generation (RAG) and Mixture of experts models (MoE) could be seen in Figure 6. Basically, this workflow is combination of previous two workflows."}, {"title": "4 Conclusion", "content": "A simple role based security architecture for Large Language Model (LLM) applications are proposed. Background information necessary for the understanding of the architecture given in multiple sections. Proposed approach is usable with Retrieval-Augmented Generation (RAG) and/or Mixture of experts models (MoE). Normally usage of local open source LLM models are assumed but RAG version is also suitable for commercial LLMs. Sequence diagrams for workflows using RAG, MoE and RAG+MoE are given."}]}