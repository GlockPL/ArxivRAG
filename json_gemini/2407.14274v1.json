{"title": "Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for Multi-Pumped Soft SIMD Operations", "authors": ["Giorgos Armeniakos", "Alexis Maras", "Sotirios Xydis", "Dimitrios Soudris"], "abstract": "Recent advancements in quantization and mixed-precision approaches offers substantial opportunities to improve the speed and energy efficiency of Neural Networks (NN). Research has shown that individual parameters with varying low precision, can attain accuracies comparable to full-precision counterparts. However, modern embedded microprocessors provide very limited support for mixed-precision NNs regarding both Instruction Set Architecture (ISA) extensions and their hardware design for efficient execution of mixed-precision operations, i.e., introducing several performance bottlenecks due to numerous instructions for data packing and unpacking, arithmetic unit under-utilizations etc. In this work, we bring together, for the first time, ISA extensions tailored to mixed-precision hardware optimizations, targeting energy-efficient DNN inference on leading RISC-V CPU architectures. To this end, we introduce a hardware-software co-design framework that enables cooperative hardware design, mixed-precision quantization, ISA extensions and inference in cycle-accurate emulations. At hardware level, we firstly expand the ALU unit within our proof-of-concept micro-architecture to support configurable fine-grained mixed-precision arithmetic operations. Subsequently, we implement multi-pumping to minimize execution latency, with an additional soft SIMD optimization applied for 2-bit operations. At the ISA level, three distinct MAC instructions are encoded extending the RISC-V ISA, and exposed up to the compiler level, each corresponding to a different mixed-precision operational mode. Our extensive experimental evaluation over widely used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our framework can achieve, on average, 15\u00d7 energy reduction for less than 1% accuracy loss and outperforms the ISA-agnostic state-of-the-art RISC-V cores.", "sections": [{"title": "INTRODUCTION", "content": "In the landscape of deep neural networks (DNNs), architectures are becoming increasingly complex, striving to maximize the accuracy of many modern applications, such as computer vision tasks, speech recognition and more [1]. In order to keep up with this increased computational complexity, modern DNNs have widely adopted, as an integral part, quantization to reduce neural network's precision, i.e., the number of bits needed to load, store and process [2]. Although traditionally NNs have relied on fixed precision formats to ensure accuracy and stability during training or inference, there is a rising need for different precision levels stemming from the observation that not all operations within a neural network require the same level of numerical precision. Some layers or operations can tolerate lower precision without significant impact in quality, leading to a more efficient use of computational resources.\nThe concept of mixed-precision neural networks [3] offers a promising solution, permitting variable precision across different network segments. This adaptability can lead to considerable gains in performance and energy efficiency, since it allows for the maintenance of high precision where it is critical for accuracy, while reducing it in other areas to conserve computational costs [4]. Mixed-precision quantization can enable resource-constrained devices to support even larger models. However, many existing general-purpose Central Processing Unit (CPU) architectures lack sufficient architectural support for efficiently managing fine-grained precision-mixing. Most Instruction Set Architectures (ISAs) either support coarse-grained schemes [5] or offer sub-optimal formats [6], resulting in severe overheads during operand packing and unpacking. Hence, advancing hardware-software co-design solutions and exploring different optimization layers of abstraction to harness the benefits of mixed-precision neural networks for energy-efficient data computation, while adhering to stringent area constraints, presents a significant challenge in computer architecture research.\nIn this work, we embrace the RISC-V ISA extension paradigm and enhance it with light-weighted hardware modifications in a small 32-bit RISC-V CPU core, Ibex [7]. We propose an end-to-end automated framework that through design space exploration (DSE) generates and evaluates mixed-precision neural networks on modified RISC-V CPU processor. To this end, we design, elaborate and integrate three innovative instructions that extend the RISC-V ISA and are portable on any modern RISC-V C compiler. These instructions have the capability to activate various operational modes within the modified microarchitecture, depending on the selected mixed-precision configuration. At the microarchitecture level, we enable efficient operand packing and introduce soft SIMD instruction capabilities. Additionally, and at the circuit level, we effectively implement multi-pumping [8, 9] to fully leverage"}, {"title": "BACKGROUND & RELATED WORK", "content": "In recent years, numerous studies have explored various compression optimization techniques, such as pruning and quantization [17] in order to minimize the inference latency and memory footprint of DNNs on edge devices. Optimized software libraries such as ARM's CMSIS-NN [18], Google's GEMMLowp [19], and X-CUBE-AI from STMicroelectronics [20], along with advanced frameworks like DORY [21] that targets RISC-V PULP platform processors, and MCUNet [22, 23] are widely used in order to deploy Quantized Neural Networks on commercial devices. Nonetheless, these tools primarily support 8-bit or higher precision variables and therefore cannot fully exploit the computational efficiencies offered by lower-bit quantization of DNNs.\nSince most modern day systems are unable to efficiently compute sub-byte calculations, many works have focused on developing specialized DNN accelerators [6, 13, 24-26]. These accelerators often provide significant performance enhancements, but those benefits are often compromised due to their lack of scalability, extensive area occupation, and high power consumption, typically in the range of hundreds of milliwatts [6, 13, 24] to a few watts [25]. These drawbacks render them less suitable for ultra-low power applications where efficiency and compactness are paramount.\nAs an alternative approach, other works have looked into extending the RISC-V ISA and developing custom functional units specifically designed to handle efficient sub-byte operations. PULP-NN [2] achieves significant speed improvements by enabling lower-bit DNN inference, utilizing instructions that pack and extract vectors of smaller data sizes, such as 4-bit and 2-bit. However, these specialized casting instructions introduce overheads for computations at 4-bit and 2-bit sizes, thus diminishing the performance benefits associated with these lower bit widths. On the other hand, its extension XpulpNN [5] supports 2, 4, and 8-bit SIMD operations but it is not supporting mixed-precision computations.\nRecent studies [3, 10, 12, 16] examined the utilization of mixed precision variables on resource constrained devices, as an effective strategy to balance the trade-offs between accuracy, latency, and memory footprint [14] that arise when employing lower precision numbers. UNPU [12] explored bit-serial MAC units with consistent activation data size and weight data sizes spanning 1 to 16 bits, while [10] extended a RISC-V core by incorporating 4- and 2-bit MAC units alongside customized controllers to enable 2-/4-/8-bit mixed-precision computations. In Mix-GEMM [3] and SySMOL [16], the authors presented software/hardware co-design workflows that seamlessly integrate hardware accelerators into the processor architecture complemented by hardware-aware training. This approach, enhanced by systematic exploration of their respective design space, enables the possibility of fine-grained mixed-precision in DNNs. However, in [3] they only consider per-network quantization in their evaluation, potentially overlooking the benefits of more granular precision adjustments, while [16] restricts its use to 1, 2, and 4-bit variables within intra-layer quantization, which can lead to significant accuracy losses in more complex models or challenging datasets like ImageNet. Furthermore, neither study explores low-level hardware optimizations, such as overclocking [27] or packing multiple low-precision multiplications on a single block [15] to optimize their system's throughput.\nTable 1 summarizes the above discussion and provides a qualitative comparison of most relevant works in the field of RISC-V ISA extension for mixed-precision DNNs. Our work distinguishes from most state-of-the-art approaches, since, to the best of our knowledge, none has proposed mixed-precision instructions capable of simultaneously activating different computations and operational modes through logic and circuit-level optimizations, such as multi-pumping and soft SIMD techniques. In Section 5.3 we thoroughly evaluate our work against most relevant state-of-the-art approaches."}, {"title": "CONFIGURABLE MIXED-PRECISION ARCHITECTURE BASED ON IBEX CORE", "content": "The base design utilized in this work is a generic microarchitecture implementation of the Ibex [7], an open-source 32 bit RISC-V CPU core, depicted in Fig. 1. Without loss of generality, the specific RISC-V core forms a proof-of-concept micro-architecture scenario to showcase the impact when enabling in an execution unit the support of fined-grained mixed-precision operations and can be also generalized in a straightforward manner to other RISC-V CPU cores.\nAs shown in Fig. 1, in Ibex, the instruction fetch stage retrieves instructions from memory, which are then passed to the decode and execution stage. Here, the instructions are decoded to determine their operation and operands, and the ALU performs arithmetic/logic operations accordingly. Finally, the results are written back to the register file in the writeback stage, completing the instruction cycle and enabling subsequent instructions to be executed. The decode logic also needs to be augmented to accommodate any newly added instructions and to signal the ALU or the added Unit about the specific operation to be performed. All other processor logic, such as controlling, forwarding logic, load/store units, etc., remains unchanged."}, {"title": "Configurable Mixed Precision Unit Design", "content": "In this section, we detail our approach to extending the RISC-V CPU core to support fine-grained mixed-precision operations, coupled with hardware optimizations, and thereby improving runtime/energy efficiency, albeit with a small, but yet acceptable, degradation in accuracy. The key idea is to allow multiple parallel operations (i.e., MAC/MUL units) by leveraging existing logic in architectures such as multiplication units, thereby introducing minimal hardware overheads.\nTo achieve this, we propose and implement a modified version of Ibex's generic multiplier, expanding its capability to handle variable mixed-precision operations. Since we primarily focus on further optimizing the performance, rather than reducing total area, we firstly"}, {"title": "Mixed-precision Micro-architectural Extensions", "content": "Decoder extensions for Mixed-precision: Since the initial architecture does not have the precision controllability, we modify the existing decoder (see Fig. 1), including extra control logic and multiplexers. This enhanced decoder facilitates the division of low-precision operands (i.e., activations and weights) and issues the appropriate control signals to the other blocks, allowing them to correctly execute the instruction corresponding to a specific operational mode.\nMixed precision Arithmetic Logic Unit Design: Then, to leverage the full potential of different low precision configurations, we employ at different layers of abstraction distinct optimization strategies. At the algorithmic level, and since the bit-width of weights is either 2,4 or 8-bit, the initial step involves packing (up to 16) operands (weights) into 32-bit registers. Each of the resulting pairs of weights and activations is mapped onto a single 17-bit \u00d7 17-bit multiplier within our modified unit. This procedure is repeated for the remaining three multipliers. The latter approach, not only allows us to boost the parallelization of operations, but it also decreases the number of instructions required by initial (non-scaled tactic) loads and stores. Then, motivated by the fact that smaller parallel computing units can achieve higher maximum clock frequencies, we employ at circuit level a multi-pumping technique. Typically, multi-pumping is used to minimize resource usage, rather than execution latency [9]. However, in this work we utilize multi-pumped units to accelerate the processing of packed operands within the core's pipeline, ensuring a flow without stalls and thereby reducing the total execution cycles. Hence, we implement a multi-pumping scheme with 2x the clock frequency for all low precision MAC operations. Lastly, and specifically only when 2-bit operands are used for weights, we apply a soft SIMD technique, making full use of the 17-bit multiplier's resources and handling as many MAC"}, {"title": "RISC-V ISA Extension", "content": "To put the speedup of our modes into perspective, we extend RISC-V ISA to support three different operations. Our encoding follows the rules of the RISC-V ISA manual for custom extensions [28]. The three instructions adhere to the R-type structured format comprising six sub-fields, i.e., opcode, func3, func7, rs1, rs2 and rd, and has a fixed length of 32 bits. The opcode dictates the operation to be executed and the involved operands, in both destination (rd) and source (rs1,rs2) registers. Func3 and func7 describe the operations depending on the input format given to them. Our instruction listing for the RISC-V mixed-precision extension is shown in Table 2.\nAs shown, we introduce only a few instructions, having thus zero impact on the hardware of the CPU's opcode decoder. Specifically, the nn_mac_8b corresponds to Mode 1 and requires 4 packed weights of 8-bit to perform 4 MAC operations in parallel, while nn_mac_4b and nn_mac_2b correspond to Mode - 2 and Mode - 3, respectively, and require 8 and 16 packed weights, of 4- and 2-bits. Note, that in all cases the length of the accumulator (rd) is 32-bit.\nOnce all the required MAC operations for each output feature are completed, the final step involves fetching the results from the 32-bit accumulators. Then, a common requantization step [29] is performed to adjust the bit-width of the outputs back to 8-bits.\nThe key takeaways of our instructions from enabling different operational Modes are twofold. Firstly, they facilitate a higher number of MAC operations per cycle, leading to increased throughput. Secondly, they facilitate considerably fewer loads and stores, leading to a substantial reduction in memory accesses. For example, the memory access reduction per layer for MobileNetV1 is depicted in Fig.4.\nTo generate Fig.4, we examined three mixed-precision models, ranging from less aggressive (<1% accuracy loss) to more aggressive ones (up to 5%). This systematic exploration is detailed in Section 4. As illustrated, even with minimal quality degradation, in addition to accelerated MAC processing, memory accesses across different layers are reduced by an average of 85%.\nCompiler support: Once the encoding of instructions has been done, we provide a high-level interface for their utilization. Our approach involves implementing a C intrinsic for each instruction, utilizing the inline assembly _asm_ operator to emit the bytecode corresponding to the specific instruction. This approach avoids us to integrate code generation within the compiler. To facilitate the in-line assembly support for our custom instructions, we made minor adjustments to the RISC-V GNU toolchain within GCC's binutils. This modification enables the execution of any RISC-V binary on various architectures (e.g., x86 of a host machine). Finally, the compiled binaries, along with the complete instruction set containing all our extensions, are executed and thoroughly emulated using the Spike simulator [30]."}, {"title": "PROPOSED CO-DESIGN FRAMEWORK", "content": "In this Section, we present the workflow of our automated framework (Fig. 5) for generating mixed-precision neural networks and evaluating them on RISC-V CPU cores. Briefly, our framework receives as an input a trained model (e.g., dumped from Pytorch) and performs a systematic design space exploration (DSE) to reveal mixed-precision configurations, offering different trade-offs (considering also user's desired accuracy threshold) among run-time efficiency and network accuracy. Once the functionality of the new instructions has been simulated and verified, the linker combine them into a single executable file (ELF). The later, along with the RTL descriptions of the modified core, which are composed in System Verilog and synthesized using industrial strenglth EDA tools (Xilinx Vivado for FPGA flow and Synopsys Design Compiler for ASIC flow), are received as inputs from the Verilator [31], a cycle-accurate emulator. Subsequently, performance metrics considering various accuracy-speedup trade-offs and any introduced overheads can be assessed.\nMixed-precision exploration: Our mixed-precision configuration design space exploration (DSE) starts with post-training quantization, where we systematically adjust the precision of various layers within the trained neural network. To retain the accuracy loss, especially in complex datasets, we also perform a fine-tuning process with few extra epochs. Without loss of generality, we explore (but not limit) bit-width configurations of 2-bit, 4-bit, and 8-bit for both convolutional and fully-connected layers, which are the most computationally intensive layers [17]. To comprehensively assess trade-offs, we test all possible combinations of precision settings across these layers. Note that this exploration needs to be performed offline and only once. The total number of configurations directly depends on the number of layers L and on the number of considered precisions p. Thus, the number of possible mixed-precision configurations is given by: $p^L$. While this number is manageable for small networks, it becomes exceedingly large for larger DNNs. Consequently, particularly for large networks and based on experimental observations, we have chosen to strategically prune the design space, by setting a fixed high precision, i.e., 8-bit, for the sensitive initial layers, thereby simplifying the exploration process and focusing on where precision changes yield the most significant impact. Hence, we manage to decrease on average more than 2000\u00d7 explored configurations, while still retaining numerous options that ensure high accuracy. Indicatively, in our worst case scenario, for the MobileNetV1 architecture, where 1408 different configurations were examined, the model was retrained for an additional 35 epochs and our framework required less than 15hours, referring to an NVDIA T4 GPU and 16GB RAM.\nGiven a selected mixed-precision configuration among the obtained Pareto space, our framework evaluates the model as follows:\n(1) Read the C source code that includes the respective replacements of the original kernels with kernels incorporating the nn_mac_(x)b operations."}, {"title": "EVALUATION & RESULTS", "content": "In this section we assess the efficiency of our proposed framework by performing an in-depth evaluation over four state-of-the-art DNNs trained on four, both simple and challenging, image classification datasets (see Table 3). For our analysis we consider: LeNet5, a CNN with 3 convolutional layers [18], MCUNet [22] and MobileNetV1 [32], trained on MNIST, CIFAR10, Visual Wake Words [33] and ImageNet dataset, respectively. Our mixed-precision post-training quantization was performed using PyTorch libraries by selecting 10% of the training dataset, while the fine-tuning process with quantization-aware training was executed using a portion of the same dataset and for 18 epochs, on average, requiring less than 6 hours each, on an NVDIA T4 GPU with 16GB RAM.\nFor hardware results, we evaluate both ASIC and FPGA synthesis flow for the microarchitecture of the open-source Ibex core [7], described in system verilog. For the FPGA workflow we consider a Virtex-7 FPGA as a proof-of-concept platform, with the main core operating at 50MHz and the custom functional unit running at 100 MHz and used Vivado 2023.1 for synthesis and power results. Register file is implemented using RAM32M primitives. Cycle-accurate simulations for both performance metrics and top-1 classification accuracy, are performed using Verilator [31], which reads Ibex performance counters for precise report of total cycles. On the other hand, for the ASIC workflow the processor is designed in System Verilog RTL, synthesized with a dual clock configuration, operating at 250MHz and 500 MHz for the main core and the functional unit respectively and is mapped to the open-source 7nm ASAP7 library [34], while Synopsys Design Compiler with the compile_ultra command is used. Register file is implemented using latches. For power analysis, the VCD file extracted from the Verilator along with the dde of the synthesized design were used to obtain the switching activity and the respective power analysis."}, {"title": "Evaluation of our framework", "content": "As mentioned, our framework starts with a design space exploration to find different mixed-precision configurations for each input benchmark. Fig. 6 presents the Pareto space between accuracy and required number of MAC instructions for all the DNNs examined. In Fig. 6, the black star represents the pre-trained baseline model. The gray circles are the quantized models in mixed precision among their layers, while the green squares form the Pareto front. Although at this stage a software exploration is performed only, the reduction in the amount of MAC instructions is obtained based on the specific bit-width per layer and the degree of parallelization that can be achieved due to the introduced packing, multi-pumping and SIMD. Note that the number of the explored models depends on the number of layers. In total, to generate Fig. 6, we evaluated more than 3500 quantized models. It is observed that different precisions in layers ranging between 2,4 and 8 bits, can achieve accuracies comparable to or the same with the full-precision counterparts. For all benchmarks more than 86% reduction in the number of MAC instructions can be achieved in their mixed quantized derivatives for less than 1% accuracy loss, while this number increases to 93% when up to 5% accuracy loss is applicable.\nThe impact of each distinct optimization technique (different Modes) that is applied within a processor execution is evaluated in Fig. 7. To generate Fig. 7 we consider as an example only one distinct dense layer (Fig. 7a), i.e., the final layer of the MobileNetV1, and a convolution layer (Fig. 7b), i.e., the second layer of the CIFAR10 CNN, and evaluate the relative speedups of the standalone Mode 1, -2, -3. In other words, we report the gains of each technique as if it was solely applied throughout the whole layer and examined for all the three different bit-widths. It is observed, that Mode 1 due to the high parallelization of 8-bit operands and minimization of loads and stores, can achieve, on average, 9.9x speedup compared to the baseline RV32IMC, and 17.8x when 2-bit weights are utilized. On the other hand, when the standalone multi-pumping technique is applied (Mode \u2013 2), it can bring an extra 16% average speedup to the total execution cycles per layer, for both 4-bit and 2-bit cases. Finally, when 2-bit weights are considered (Mode - 3), our soft SIMD approach obtains an extra 13% speedup, on average, having a significant reduction in total cycles of up to 30.9x.\nAs mentioned earlier, the sensitivity and error resilience of each layer varies, and hence top-1 models' accuracy, is tightly dependent on the selected precision for each layer. Subsequently, in order to put the delivered trade-offs between total execution time of DNN inference and top-1 accuracy into perspective, we generate Fig. 8. This figure considers three conservative accuracy loss thresholds (set by the user) and extracts three optimal mixed-precision configurations obtained from the DSE. The bit-width configurations of the weight operands per layer are also shown on the right Y-axis. Then, the speedup for all layers derived from Verilator is reported. The key observations regarding the mixed precision selection are two: 1) the less complex models like LeNet and CIFAR10 CNN allow aggressive quantization down to 2 bits in most of their layers with minimal accuracy degradation (<1%) and thus able to achieve the maximum possible performance gains, and 2) although the more challenging MobileNetV1 and MCUNet models barely utilize 2-bit weights scenarios (only for >5% accuracy loss constraints), they manage to compress most of their layers to 4-bit with small impact on their accuracy (<2%). Overall and on average for all layers, our proposed ISA extensions can achieve from 13.1x up to 17.8x speedup for 1% up to 5% accuracy degradation, respectively. It is also noteworthy that MCUNet presents less significant gains compared to other benchmarks due to the high amount of depthwise convolutions. The latter do not enable the same degree of input reuse as in standard point-wise convolutions, while also they differ in the overheads (e.g., branch instructions) they introduce.\nFPGA and ASIC Designs comparisons: In Table 4 we compare the performance of the proposed modified Ibex processor with the baseline one on both FPGA and ASIC platforms. On one hand, our proposed modified processor, operating on an FPGA at a clock frequency of only 50MHz (and 100 MHz for the multi-pumped circuit),"}, {"title": "Comparison against state-of-the-art", "content": "In this section, we compare our modified RISC-V processor with the most relevant state-of-the-art solutions. We note that our solution utilizes a limited standard cell technology library provided by the ASAP open academic 7nm PDK, while other solutions are mapped on industrial strength technology libraries More specifically, we analyze hardware-software co-designed architectures computing DNNs on CPU architectures, adopting ISA extensions and custom functional units. A detailed comparison is shown in Table 5. To compare the performance of our modified RISC-V processor equipped with custom ISA extension with respect to state-of-the-art, we report the peak performance presented in each work. For an as fair as possible comparison, either a typical convolution layer, or the average peak performance for all the examined DNNs was considered. In other works, accuracy degradation of the corresponding peak performance of related works, where in some cases could be high [3], is not examined at this point. On the other hand, our evaluation presents an energy efficiency range that corresponds to less than 1% (see Table 4) up to 5% accuracy loss.\nAs shown in Table 5, our average lowest performance (915 GOPs/W) surpasses the corresponding value of MIX-GEMM [3] for less than 1% accuracy degradation, while compared to [10], our performance ranges from 0.7\u00d7 up to 3.2\u00d7 faster. When considering MobileNetV1 specifically, Mix-GEMM achieves energy efficiency ranging from 500 to 1000 GOPs/W, while our approach surpasses this performance, with respective numbers ranging from 1015 to 1149 GOPs/W. On the other hand, XPulpNN [5], which utilizes SIMD units supporting from 4 8-bit to 16 2-bit MAC per cycle, reports comparable performance when processing a typical convolution layer. However, its efficiency stands 25% lower than that of our processor when compared to our CNN model. Additionally, although UNPU's decoupled accelerator [12] showcases high performance, its throughput is hindered by its limited flexibility, since the intricate software stack required by such accelerators, usually demands specific offloading mechanisms handled at either the hardware or software level. Finally, note that only designs with 1% accuracy loss constraints were evaluated for Table 5. However, when prioritizing efficiency over top-1 accuracy (e.g., up to 5% degradation), our processor achieves an average of 1.07 TOps/W, with a peak performance of 1.9 TOPs/W on the MobileNetV1 Imagenet model."}, {"title": "CONCLUSION", "content": "In this work, we embrace the potential of mixed-precision neural networks, trying to overcome the limitations of current CPU architectures that hinder efficient precision management. Our approach involves enhancing a RISC-V CPU core with lightweight hardware modifications and an end-to-end automated framework for generating and evaluating mixed-precision neural networks. We introduce three novel instructions to the RISC-V ISA, enabling various operational modes for optimized computations that leverage operand packing, multi-pumping and a soft SIMD technique. Our advancements through a RISC-V based processor demonstrate that not only accuracy can be preserved, but also remarkable gains in energy efficiency and performance can be achieved. The latter, represents a substantial leap forward in energy-efficient computing for resource-constrained devices."}]}