{"title": "Accelerating Large Language Model Training with Hybrid GPU-based Compression", "authors": ["Lang Xu", "Quentin Anthony", "Qinghua Zhou", "Nawras Alnaasan", "Radha Gulhane", "Aamir Shafi", "Hari Subramoni", "Dhabaleswar K. (DK) Panda"], "abstract": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP) are the three strategies widely adopted to enable fast and efficient Large Language Model (LLM) training. However, these approaches rely on data-intensive communication routines to collect, aggregate, and re-distribute gradients, activations, and other important model information, which pose significant overhead. Co-designed with GPU-based compression libraries, MPI libraries have been proven to reduce message size significantly, and leverage interconnect bandwidth, thus increasing training efficiency while maintaining acceptable accuracy.\nIn this work, we investigate the efficacy of compression-assisted MPI collectives under the context of distributed LLM training using 3D parallelism and ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen supercomputer. First, we enabled a na\u00efve compression scheme across all collectives and observed a 22.5% increase in TFLOPS per GPU and a 23.6% increase in samples per second for GPT-NeoX-20B training. Nonetheless, such a strategy ignores the sparsity discrepancy among messages communicated in each parallelism degree, thus introducing more errors and causing degradation in training loss. Therefore, we incorporated hybrid compression settings toward each parallel dimension and adjusted the compression intensity accordingly. Given their low-rank structure [1], we apply aggressive compression on gradients when performing DP All-reduce. We adopt milder compression to preserve precision while communicating activations, optimizer states, and model parameters in TP and PP. Using the adjusted hybrid compression scheme, we demonstrate a 17.3% increase in TFLOPS per GPU and a 12.7% increase in samples per second while reaching baseline loss convergence.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, an abundance of Large Language Models (LLM) emerged with impressive abilities in downstream Natural Language Processing (NLP) tasks involving machine translation, dialogue systems, text generation, and so on. Some spotlights include LLaMA [2], GPT-4 [3] and GPT-NeoX-20B [4]. However, to ensure exceptional performance, these models often scale up to billions of parameters, thus requiring increased data size and computation by multiple orders of magnitude. Since the introduction of scaling laws of LLMS [5], model size has been growing from 100 million (BERT [6]) to 500 billion (Megatron-Turing NLG [7]). As a result, one Graphic Processing Unit (GPU) cannot fit a model and its input data anymore, making it necessary to scale out to more workers.\nHigh-Performance Computing (HPC) systems are designed and engineered to support sizeable scientific research and deep learning workloads. These HPC systems typically consist of thousands of nodes equipped with two to four advanced GPUs that maximize floating point operations per second (FLOPS), making them ideal for large-scale data-intensive distributed pre-training of LLMs. Inter- and Intra-node communication play a significant role in accelerating parallel applications. The Message Passing Interface (MPI) supports a variety of highly-optimized communication routines and has been a favored parallel programming model deployed on HPC systems [8]. With the advances in GPUDirect technology [9], GPU-aware MPI libraries [10]\u2013[12] vastly accelerate GPU data transfer and leverage interconnect bandwidth. MPI also serves as a popular communication back-end for distributed machine learning jobs [13]\u2013[15].\nTraining massive language models requires meticulous arrangement of memory resources and parallelism strategies. Two well-known solutions to this problem are 3D parallelism [7] and the Zero Redundancy Optimizer (ZeRO) [16]. Here, 3D parallelism refers to Data Parallelism (DP) [17], Pipeline Parallelism (PP) [18], [19] and Tensor Parallelism (TP) [20], which are often implemented to support extremely parallel execution of pre-training missions of LLMs across hundreds of GPUs. Also, tensor parallelism and pipeline parallelism are generally categorized under model parallelism. Data parallelism generally aims to partition one input data batch into mini-batches and distribute them to each GPU. This method enables parallel input data processing but requires a data-intensive All-reduce to aggregate the gradients at the end of"}, {"title": "A. Motivation", "content": "Applying 3D parallelism with ZeRO for DL model training imposes data-intensive collective communications within and across nodes. Given the limited interconnect bandwidth be-tween GPU nodes, such large data transfer leads to drastic overhead. Profiling conducted by previous works [21] has also addressed that different parallelism strategy requires huge portions of communication and heterogeneous collective op-erations (see Figure 1). Consequently, optimizing point-to-point and collective communication schemes became critical to mitigate such bottlenecks. Previous research has demonstrated that when co-designed with GPU-based compression libraries like ZFP [22] and MPC [23], GPU-aware MPI collectives were able to leverage interconnect bandwidth and stage outstanding throughput benefits [24]\u2013[26].\nConducting GPU-based compression techniques on data buffers drastically decreases the message size being com-municated. However, representing high-precision data using lower precision always results in accuracy degradation, which is often observed within Deep Learning workloads. Prior re-searchers mainly experiment with small deep learning models to pick the most suitable compression rate for lossy ZFP library [26]. Yet, these models contain much fewer parameter numbers than LLMs, emitting smaller message sizes dur-ing transfer. One key motivator of this work is to use compression-assisted MPI collectives to accelerate large language model training."}, {"title": "B. Challenges", "content": "This section addresses the following challenges:\nC1) The first challenge was determining the different model information communicated in parallelism stages. Pipeline parallelism, tensor parallelism, and data paral-lelism usually pose collective communications on var-ious training parameters. Depending on the specific implementation, tensor parallelism [20] usually calls All-reduce and All-gather on activations during forward passes and gradients during backward passes. Pipeline parallelism [18], [19] typically features point-to-point operations to pass activations and gradients from one stage to another. Finally, data parallelism requires All-reduce on the gradients of each data parallel rank and re-distributing the aggregated global gradients back to each model replica to perform subsequent updates [15], [27]. To overcome this challenge, we present a thorough illustration of the communication routines involved in 3D parallelism as well as in ZeRO stage 1 (Figure 2 4). We further explain this demonstration in Section III\nC2) The second challenge is that compressing messages communicated during training depends on the paral-lelism stages and requires prudent design. We observed that na\u00efvely applying compression scheme to all paral-lelism stages introduced outstanding training speedup, but at the same time led to degradation in training quality (Section IV-C). Current studies have shown that model parallelism has inherently different characteristics than data parallelism. In data parallelism, gradients communicated among different ranks are observed to be low-rank or sparse. Yet, in model parallelism where communicating activations is the bottleneck, activations are analyzed as dense [1]. Therefore, applying the same compression intensity to both gradients and activations may lead to a loss of accuracy. Given these differences, compression intensity may vary to achieve a balanced solution that maximizes both throughput benefits and model training performance.\nC3) The third challenge we tackle in this paper originates from the fact that some model information, specifically gradients, is processed in both model-parallel and data-parallel stages. For example, both data parallelism and tensor parallelism feature the communication of gradi-ents in their executions. When designing compression schemes, we should only apply aggressive compression to these gradients once, preventing over-extracting and hurting meaningful information, thus destroying accu-racy. However, we cannot use lossless or high-precision methods towards these gradients since most negligible values will not be adequately extracted. Consequently, we need to avoid over-compression of gradients and maintain compression intensity. This challenge is being solved by applying different compression intensities towards different parallelism communication paths."}, {"title": "C. Solutions", "content": "In this paper, we propose MZHybrid and ZHybrid, two hy-brid compression schemes that utilize GPU-based compression on LLM training data, thus expediting the training process while maintaining acceptable model performance. We adopted MPI collectives co-designed with lossless MPC and lossy ZFP libraries to reduce the amount of data movement and leverage inter-node bandwidth. We analyzed different communication scenarios under 3D parallelism and ZeRO stage 1 optimization and designed an appropriate compression approach that con-siders the sparsity distinction between messages in the training process."}, {"title": "D. Contributions", "content": "This paper contributes in the following manners:\n1) We experimented na\u00efve compression schemes for both MPC and ZFP on modern large-scale HPC systems. We reported up to a 23.6% increase in training samples per second and up to a 22.5% increase in TFLOPS per GPU over non-compressed collective communications. We analyzed such schemes' benefits and shortcomings, leading to further design choices (Section IV-C).\n2) We proposed and designed MZHybrid, a hybrid com-pression scheme that utilizes lossless MPC for model-parallel communications and lossy ZFP rate for data-parallel communications. (Section III-A) We also pro-posed and designed ZHybrid, a hybrid compression scheme that utilizes different ZFP compression rates for communicating model-parallel and data-parallel mes-sages. (Section III-B).\n3) We evaluated MZHybrid performance on modern large-scale HPC systems. We reported up to 4.4% increase in training samples per second and up to 5.3% increase in TFLOPS per GPU compared to non-compressed collective solutions. We demonstrated a significant im-provement in model quality compared to the na\u00efve ZFP scheme (Section IV-E). We also evaluated ZHybrid performance on modern large-scale HPC systems. We reported up to 17.3% increase in training samples per second as well as up to 12.7% increase in TFLOPS per GPU compared to non-compressed collective solutions. We also showed noticeable enhancement in model qual-ity compared to na\u00efve ZFP scheme (Section IV-F).\n4) To the best of our knowledge, this is the first work that utilizes MPI collectives co-designed with GPU-based compression libraries to accelerate both model parallelism, data parallelism, and ZeRO communication (Section V)."}, {"title": "II. BACKGROUND", "content": "This section covers backgrounds on 3D parallelism, which combines data-parallel, pipeline-parallel, and tensor-parallel. We will also discuss relevant works on ZeRO.\n1) Data Parallelism (DP): Data Parallelism [17] is a dis-tributed DL technique that distributes training data across multiple GPUs with model replicas to perform parallel training steps. Data Parallelism can significantly improve throughput compared to single-node training, as evidenced by relevant applications [15], [27], [30]. However, due to memory restric-tions, DP has limitations with large, dense neural networks and high-resolution images.\n2) Model Parallelism (MP): Model parallelism overcomes the limitation of DP by splitting the model into layers and distributing it on different devices. Common approaches to achieve model parallelism include Pipeline Parallelism (PP) [18], [19] and Tensor Parallelism (TP) [20]. PP uses inter-layer model parallelism with micro-batches executed in a pipeline, while TP employs sub-tensor splitting for parallel processing across multiple workers, optimizing tensor operations.\nHigher-degree parallelism strategies emerged to further scale and harness the benefits of individual distributed models. MT-NLG [7] uses 3D parallelism, a combination of DP, PP, and TP, to train a billion-parameter model, facilitating larger model sizes and improved training capabilities.\nWhen analyzing the communication pattern for DP and MP, we observe that DP involves less frequent All-reduce operations on larger data sizes. At the same time, MP requires more frequent point-to-point operations on smaller data.\n3) ZERO: ZERO (Zero Redundancy Optimizer) [16] pro-vides a memory optimization technique and overcomes the memory limitation of DP and the scalability issue of MP. This is achieved by partitioning optimizer states, gradients, and model parameters across GPUs to eliminate redundant storage and GPU memory consumption. Efforts have also been made to offload certain training parameters to CPU and NVMe memory [31] and optimize communication overhead [32]. For"}, {"title": "C. Compression in Data Parallelism and Model Parallelism", "content": "Data parallelism incurs communication overhead due to the All-reduce operation required for gradient aggregation. Various works have proposed gradient compression techniques for DP to address the overhead and improve training speed while maintaining accuracy [33], [34]. These works are based on the sense that most of the gradients communicated in DP are sparse, which means these data structures have most of their elements concentrated in a few dimensions. In contrast, the remaining dimensions contain negligible values. A com-prehensive study [1] has performed a low-rank analysis on the activation data in MP and concluded the opposite findings: activations in MP are dense, leading to its significance in preserving accuracy."}, {"title": "III. DESIGN", "content": "This section first details communication routines and mes-sages in typical 3D parallelism and ZeRO stage 1 scenarios. Next, we will introduce the MZHybrid scheme and the ZHy-brid scheme.\nFigure 2 illustrates a typical 3D parallelism setting split across eight global workers. This setting contains two DP ranks, two PP stages within each DP rank, and two TP degrees within each PP stage. We examine communication calls in each parallelism dimension in the following paragraphs. In Figure 2, the global batch is split into two mini-batches, and each goes into a DP rank. Each DP rank will produce its own local gradients after a forward and a backward pass on the model replicas. Then, the root rank will issue an All-reduce call on these local gradients and re-distribute the global aggregated gradients to each DP rank before updating the model replica parameters. The All-reduce call is graphed between DP ranks in the middle of Figure 2. Typically, this All-reduce is conducted upon sparse gradients. However, as model size increases, such collective communication gradually becomes a bottleneck, given the increase in message size.\nNext, we split the model replica into two PP stages within each DP rank. Each PP stage will include a subset of the network layers. For example, in Figure 2, we assume a model with eight layers, and we split them across two pipeline stages, with pipeline stage 0 having network layers 0-3 and pipeline stage 1 having network layers 4-7. Pipeline stages mainly call point-to-point communication routines like MPI_Send & MPI_Recv to communicate gradients and activations with each other. These are also drawn between pipeline stages in Figure 2.\nNext, we consider Tensor Parallelism. TP focuses on paral-lelizing matrix computations among workers. In this scenario, we parallelize computation workloads across two GPUs. For specific implementation, we refer to Megatron-LM [20]. In their approach, they split GEMM operations in MLP blocks, Self-Attention blocks, and the output embedding layer among GPUs. This method requires two All-reduce primitives for a forward and a backward pass in a single transformer layer. For the output embedding layer, an All-reduce aggregates the different portions of the input embedding, and an All-gather will act to obtain GEMM outputs. Please refer to Figure 3 for details. In this parallelism dimension, the main communication primitives involved are All-reduce and All-gather\u2014these prim-itives aggregate activations in forward passes and gradients in backward passes. We also illustrate these on the sides of pipeline stage blocks in Figure 2."}, {"title": "A. MZHybrid: MPC for MP + ZFP for DP", "content": "In this section, we introduce the first hybrid compression scheme: MZHybrid. MZHybrid uses lossless MPC scheme for MP communication and lossy ZFP scheme for DP communi-cation.\nWe provide illustrations using MZHybrid under the typical 3D parallelism scenario in Figure 5. We enforce lossless MPC for All-reduces in TP, point-to-point sends&recvs in PP, and lossy ZFP for All-reduce between DP ranks. For ZeRO stage 1 under MZHybrid, we enforce lossless MPC for communica-tions. Given that activations communicated in MP are mostly dense (contrary to gradients) [1], we want to preserve precision on these data to maintain model training performance. For communicating large and mostly sparse gradients between DP ranks, we apply an aggressive lossy ZFP scheme. It is worth mentioning that in MP settings, communication on gradients also exists during backward passes. To avoid over-compressing gradients, we apply MPC schemes to those gradients. We use Table II to specify our compression scheme choice for each collective involved for MZHybrid. We also experimented with different rates of ZFP under MZHybrid, please refer to IV-E."}, {"title": "B. ZHybrid: high-rate-ZFP for MP + low-rate-ZFP for DP", "content": "This section presents the second hybrid compression scheme: ZHybrid. ZHybrid adopts a lossy ZFP scheme for all communication, including 3D parallelism stages and ZeRO stage 1. However, for different parallelism ranks, we apply different rates of ZFP. Since high-rate ZFP are better at preserving accuracy [28], we apply them towards MP units and ZeRO stage 1. For DP, We apply low-rate ZFP to large and sparse gradient reduction to eliminate negligible values. For experiments with different rates of ZFP under ZHybrid, please refer to IV-F. We use Table III to specify our compression scheme choice for each collective involved for ZHybrid."}, {"title": "IV. EVALUATION", "content": "In this section, we evaluate different hybrid compression schemes for LLM training in terms of training throughput (TFLOPS) and loss. We conducted experiments on the Lassen supercomputer hosted at Lawrence Livermore National Lab-oratory [35]. The cluster comprises 792 GPU nodes, each with four 16 GB memory NVIDIA Tesla V100 GPUs and two 44-core IBM Power 9 CPUs. Inter-node connection is established through Mellanox Infiniband EDR with a band-width of 100Gb/s, and the 4 GPUs on each node are split into two pairs connected via NVLINK (Table IV). Experiments on more advanced GPU architecture (A100, H100) would be beneficial, yet we believe that the core findings and narrative would largely remain the same."}, {"title": "A. Software Libraries", "content": "We invoked compression-assisted reduce-scatter-allgather All-reduce based on compression-assisted reduce-scatter and all-gather implemented on top of MVAPICH2-GDR 2.3.7 [11] for all training experiments. We chose the GPT-NeoX library given its open-sourced documentation and implemen-tation of LLM parallel training procedures. This library was implemented based on Megatron-LM [20] and DeepSpeed [36] to support 3D parallelism. Furthermore, it has also been augmented with various novel optimizations, including ZERO [16], etc. We compiled PyTorch v1.13.1 and the latest DeepSpeed from source with GPU-aware MPI support."}, {"title": "B. Training Configuration", "content": "The inter-node interconnect is InfiniBand-EDR-100Gb/s, and the 4 GPUs on each node are split into two pairs connected via NVLINK. We agree that experiments on more advanced GPU architecture would be beneficial, but the fundamental storyline would be unchanged.\nWe selected the largest language model checkpointed in the GPT-NeoX library GPT-NeoX-20B. Due to a lack of resources to train the original foundation model, it was necessary to conduct fine-tuning using a more constrained dataset. Specifically, the model was fine-tuned on 'Books3', a subset of the 'Pile' dataset developed by EleutherAI [37]. We set up the model using the same hyperparameters in the original paper [4]. We trained the model for 4000 steps. We changed the parallelism settings to match the Lassen GPU node configuration. We set the pipeline parallelism degree to be 6 across nodes and model parallelism degree to be 4 within nodes. This makes up a base training environment among 24 GPUs for one model replica. We use a gradient accumulation"}, {"title": "C. Na\u00efve ZFP Compression Scheme", "content": "We first forced na\u00efve lossy ZFP compression schemes to all parallelism. We conclude our results of testing na\u00efve ZFP(rate:8) and ZFP (rate:16) by reporting training throughput in two aspects: samples per second and TFLOPS per V100 GPU. We also documented test loss on Books3 for model per-formance validation. In Figure 7a, when compared to default MVAPICH2-GDR implementations, conducting ZFP compres-sion techniques showcased a 23.6% increase in samples per second for rate:8 and a 15.4% respectively for rate:16 on 192 V100 GPUs. In Figure 7b, we observed a 22.5% increase in TFLOPS per V100 GPU for rate:8 and a 11.14% increase in that for rate:16 on 192 V100 GPUs. The throughput benefit difference between ZFP rate:8 and rate:16 is expected since, with a lower rate, we are extracting out more information, thus leading to more bandwidth being freed and better throughput. We continued to evaluate test loss on the trained model; we discern some degradation in the loss curves in Figure 7c. Compared to baseline showing a steep decrease in test loss, na\u00efvely compressing all messages using ZFP produces flatter loss curves and, eventually, larger loss values. It is also worth noting that a larger ZFP compression rate yields loss curves and values closer to baseline since less model information is being compressed during message passing."}, {"title": "D. Na\u00efve MPC Compression Scheme", "content": "Secondly, we switched to using pure lossless MPC compres-sion. We similarly applied MPC to all parallelism in Figure 8a and Figure 8b, and we report that using MPC didn't show significant throughput benefits. Nevertheless, in Figure 8c, we observed that when comparing to baseline loss curves, MPC elicited a better ability to match desired model performance and hardly produced any degradation. This observation is anticipated since MPC compression is a lossless scheme and proficient in preserving model data precision."}, {"title": "E. MZHybrid", "content": "In this section, we evaluate the MZHybrid compression scheme-applying lossless MPC in MP communication and lossy ZFP in DP communication. We recorded results for ZFP rate:8 and ZFP rate:16. In Figure 9a, we saw that using ZFP rate:8 together with MPC, training samples per sec showed a 4.4% increase on 192 GPUs when compared to MVAPICH2-GDR baseline. For TFLOPS per GPU, we demonstrated in Figure 9b a 5.3% raise when training across 192 GPUs. When it comes to model performance, we plotted loss curves with MZHybrid against na\u00efve ZFP approach. Figure 9c staged that MZHybrid significantly reduces loss values when compared to na\u00efve schemes for both ZFP rate:8 and ZFP rate:16 (Figure 7c). We anticipate such observations, although MPC struggles at providing throughput benefits for large message size, the speed-up provided by ZFP offsets such shortcoming. At the same time, the loss curve showed that incorporating MPC significantly improves model performance."}, {"title": "F. ZHybrid", "content": "We continued experimenting with ZHybrid: using different ZFP compression rates for MP and DP stages. We conducted experiments on two cases: one is ZFP(rate:24) for MP and ZFP(rate:8) for DP, and the second one is ZFP(rate:16) for MP and ZFP(rate:8) for DP. While applying ZFP(rate:16) for MP, we observed a 20.4% increase in training samples per second and a 20.6% increase in TFLOPS per GPU. When considering ZFP(rate:24) for MP, we also see a 17.3% increase in training samples per second and a 12.7% increase in TFLOPS per GPU (Figure 10a, 10b). Then we compared ZHybrid against na\u00efve ZFP (Figure 11) in terms of test loss and observed lower final loss values, which translates to better model quality. Increasing the ZFP rate for MP communication improves model performance as expected.\nBoth evaluated ZHybrid cases (rate:24 MP & rate:16 MP) staged lower loss values over na\u00efve ZFP solution. Also, the moving average of the loss is higher for ZHybrid(rate:16 MP), which conforms with our expectation that lower ZFP rates lead to higher overall loss landscape."}, {"title": "G. Discussions", "content": "We compare our hybrid compression approach with NCCL [38], a collective communication library highly optimized for NVIDIA GPUs and networking. Our approach ZHy-brid(rate:16-MP, rate:8-DP) exhibits up to 7.6% increase in samples per second and 12.9% increase in TFLOPS per GPU on 192 V100 GPUs. While higher ZFP rate (rate:24-MP, rate:8-DP) results in less performance gain, we still achieved up to 4.9% increase in samples per second and 5.5% increase in TFLOPS per GPU on the same scale. (Figure 12, 13) The reason for this is that as we scale up, inter-node bandwidth begins to saturate. Compression-assisted MPI collectives are capable to reduce message size during transfer and mitigate communication stress in this scenario, resulting in better GPU compute utilization. Compared to ZHybrid, MZHybrid pro-vided more benefits on loss convergence rather than training throughput due to the overhead of lossless compression. The two proposed hybrid schemes benefit us in either training efficiency or quality; specific choices depend on the end-user's preference and metrics.\nThe key takeaway we addressed is that higher ZFP compres-sion rates(i.e., less aggressive compression)lead to loss closer to baseline than low ZFP rates, which matches intuition. There is no correct answer for a \"proper\" rate since this depends on the use case, and this paper seeks to quantify the ZFP rate tradeoff so that this \"proper\" rate can be selected. The broad guidelines we find are to choose a model-parallel(TP+PP) compression rate slightly smaller than an FP-32 precision but large enough to provide a loss increase that our application can tolerate. At this stage, there is no detailed analysis for every rate, and all that is left to future work."}, {"title": "V. RELATED WORK", "content": "Several studies have been done on compressing gradients to reduce training time [33], [34]. Furthermore, C. C. Chen et al. [39] provided a hybrid communication compression method, which chooses the best compression method for every gradient to maintain model accuracy while reducing training time. Several distributed Deep Learning frameworka have also been integrated with mixed-precision training [40]."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose two hybrid compression schemes, namely MZHybrid and ZHybrid, to leverage LLM training efficiency with adequate model performance. These two de-signs consider the fundamental differences among parallelism strategies (DP, PP, and TP) and the sparsity in the message communicated in these schemes. MZHybrid applied lossless MPC towards model-parallel communication and lossy ZFP towards data-parallel communication. ZHybrid forces different ZFP rates for different parallelism stages. For model-parallel, we chose high-rate ZFP to preserve the precision of dataflow occurring within a model. For data parallel, we adopt low-rate ZFP to reduce gradient size communicated across models.\nThe proposed design MZHybrid demonstrates up to 4.4% increase in training samples per second and 5.3% increase in TFLOPS per GPU compared to non-compression approaches with significant improvement in training quality over na\u00efve compression. The proposed design ZHybrid demonstrates up to 20.4% increase in training samples per second and 20.6% increase in TFLOPS per GPU compared to non-compression approaches and still poses noticeable improvement in model performance over na\u00efve compression\u2014the two hybrid schemes emphasized on training throughput benefits or model quality.\nWe believe our approach can generalize to other use-cases which exhibit heavy collective communication and are saturating interconnect bandwidth.\nIn future work, we plan to co-design more MPI collective operations with GPU-based compression libraries to accelerate more scientific applications and Deep Learning workloads. Choices include All-reduce and other communication routines. Also, we expect that more advanced compression techniques and libraries can be incorporated such as cuSZ [41] and block-based quantization [42]."}]}