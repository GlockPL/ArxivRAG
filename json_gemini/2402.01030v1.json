{"title": "Executable Code Actions Elicit Better LLM Agents", "authors": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"], "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with envi- ronments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have emerged as a pivotal breakthrough in natural language pro- cessing (NLP). When augmented with action modules that allow access to APIs, their action space expands beyond conventional text processing, allowing LLMs to acquire capabilities such as tool invocation and memory management [30, 41] and venture into real-world tasks such as controlling robots [1, 19, 29] and performing scientific experiments [3].\nWe inquire: how to effectively expand LLM agents' action space for solving complex real-world problems? Much existing research has examined using text [62, 34, inter alia] or JSON [40, 5, inter alia] to produce actions (e.g., tool uses in Fig. 1 top left). However, both methods typically suffer from constrained scope of action spaces (actions are usually tailored for specific tasks) and restricted flexibility (e.g., inability to compose multiple tools in a single action). As an alternative approach, several work [26, 44, 48] demonstrate the potential of using LLMs to generate code to control robots or game characters. However, they typically rely on pre-specified control primitives and hand-engineered prompts and, more importantly, struggle to dynamically adjust or emit actions based on new environmental observation and feedback."}, {"title": "CodeAct Makes LLMs Better Agents", "content": "In this section, we first describe CodeAct framework (\u00a72.1) and provide empirical evidence that supports the choice of CodeAct. We focus on Python as the programming language for CodeAct due to its popularity (ranked top-1 at TIOBE index [46]) and numerous open-source packages. We aim to answer several research questions (RQs) using 17 off-the-shelf LLMs. In \u00a72.2, we examine RQ1: Does LLMs' familiarity with code due to a large amount of code pre-training data bring CodeAct advantages over text and JSON? We discuss RQ2 in \u00a72.3: Does CodeAct benefit from Python's innate control and data flow feature in complex problems? Finally, as an additional benefit, we discuss how using CodeAct further enhances LLM agents by enabling multi-turn interactions and allowing them to access existing software in \u00a72.4 and Fig. 3."}, {"title": "What is CodeAct?", "content": "In Fig. 2, we first introduce a general multi-turn interaction framework for LLM agents' real- world usage that considers three roles: agent, user, and environment. We define interaction as the information exchange between the agent and an external entity (user or environment). For each turn of interaction, the agent receives an observation (input) either from the user (e.g., natural"}, {"title": "CodeAct Shows the Promise as a Strong Tool Use Framework", "content": "In this section, we perform a controlled experiment to understand which format (text, JSON, CodeAct) is more likely to lead an LLM to generate correct atomic tool calls. The performance in this experiment reflects LLM's familiarity with the corresponding format. We hypothesize that using CodeAct to call tools is a more natural way to use tools for the models, which typically have extensive exposure to code data during their training.\nSetup. We re-purpose API-Bank [24] and test LLMs' API-calling performance, comparing CodeAct, JSON, and text actions. For each evaluation instance, we instruct LLM to generate one atomic tool call in the format of a Python function call, JSON object, or text expression in a pre-defined format. A concrete example is shown in Tab. A.6. We use API-Bank's level-1 instructions and the provided toolset. To evaluate API-calling, we follow their correctness metric, matching the ground-truth API outputs with the actual model-generated API's execution outputs.\nResults. We present results in Tab. 2. For most LLMs, CodeAct achieves comparable or better performance even in atomic actions (the simplistic tool use scenario) where its control and data flow strengths are ablated. Compared to closed-source LLMs, CodeAct's improvements are more prominent in open-source models. Furthermore, code data is usually more accessible for fine- tuning open-source LLMs than the specialized JSON or text tool-calling format. Although JSON is consistently weaker than other approaches for open-source models, it achieves decent performance with closed-source LLMs, indicating that these closed-source models may have gone through targeted fine-tuning toward their JSON capabilities. These results suggest optimizing for CodeAct is a better route for open-source LLMs than alternatives to improve their tool-use capabilities, as they already show good initial CodeAct capability due to extensive exposure to code data during pre-training."}, {"title": "CodeAct Gets More Done with Fewer Interactions", "content": "In this section, we investigate whether LLM agents can benefit from the control and data flow of code on problems that require complex patterns of tool use.\nM\u00b3 ToolEval. As shown in Tab. A.7, to the best of our knowledge, no existing tool-use benchmarks contain complex tasks requiring the composition of multiple tools while supporting evaluating different action formats. Hence, we curate a benchmark M\u00b3ToolEval to fill this gap, which evaluates LLMs' capabilities in solving complex tasks that typically require multiple calls to multiple tools"}, {"title": "CodeAct Benefits from Multi-turn Interactions and Existing Software Packages", "content": "In Fig. 3, we show how an LLM agent can integrate with Python (i.e., CodeActAgent we trained in \u00a73.2) and use existing software to perform complex tasks in multi-turn interactions. Thanks to its extensive knowledge of Python learned during pre-training, the LLM agent can automatically import the correct Python libraries to solve tasks without requiring user-provided tools or demonstrations. As illustrated in Fig. 3, CodeActAgent can use Pandas to download and process tabular data, use Scikit- Learn for machine learning train-test data split and regression model training, and use Matplotlib"}, {"title": "Empowering Open-source LLM Agent to be Better at CodeAct", "content": "The promising results achieved by CodeAct motivate us to build an open-source LLM agent that can both interact with environments through CodeAct and communicate with humans using language. To improve open-source LLMs' CodeAct capability, in \u00a73.1, we introduce CodeActInstruct, an instruction finetuning dataset that contains agent-environment interaction trajectories. We discuss data selection procedures in \u00a73.1 to promote improvement from interaction behavior. Additionally, we show that CodeAct can be used together with existing agent-user conversation data (\u00a73.1) to balance the dialog capability of the resulting LLM. Our model CodeActAgent, finetuned from LLaMA-2 [47] and Mistral-7B [20] on a mixture of CodeActInstruct and general conversations, improves CodeAct performances without hurting LLM's general performance on a diverse suite of tasks (\u00a73.2)."}, {"title": "CodeActInstruct: Agent-Environment Interactions", "content": "We consider four main use cases in agent-environment interaction and repurpose five existing datasets across different domains to generate trajectories:\n\u2022 Information Seeking: We use a training subset of HotpotQA [59] to generate information-seeking trajectories, where LLMs use the wikipedia_search API (provided as a Python function) to search for information to answer questions.\n\u2022 Software Package (Tool) Usage: We use the training set of code generation problems in APPS [16] and math problems in MATH [18]. The code generation tasks already involve importing packages and/or creating new tools by defining a new Python function. For MATH, we provide an in-context demonstration of importing Python packages (e.g., sympy for symbolic math) for problem-solving.\n\u2022 External Memory: We repurpose the training subset of WikiTableQuestion [35] and tweak it into two variants of tabular reasoning tasks that require accessing external memory: (1) SQL-based, requiring the LLM to interact with an SQL database through sqlite3 package to answer the question via SQL execution; (2) Pandas-based, requiring the model to interact with pandas tables to perform data operations (e.g., select, filter). Examples of instructions can be found in \u00a7G.3.1.\n\u2022 Robot Planning: We use ALFWorld [43], a text-only embodied environment simulator, to generate trajectories that use robot-control APIs (repurposed as Python function) to complete household tasks. Following MINT [53], we provide an in-context demonstration to encourage the use of for-loop and if-statement code blocks to automate repetitive operations (e.g., searching for items by visiting different locations).\nData Down-sampling. We down-sample each dataset by keeping only the most challenging instances, aiming to make trajectory generation more efficient and cost-effective. Furthermore, it also helps remove simple instances that existing LLMs can already solve. The statistics of the filtered dataset can be found in Tab. A.9. Please refer to \u00a7G.1 for details about the down-sample process.\nRepurpose Data for Multi-turn Interaction. Some datasets (APPS, MATH, WikiTableQuestions) are initially single-turn problems that expect one solution per instruction, whereas, in a realistic agent use case, we often require multi-turn interaction to complete each task (Fig. 1 top). Following MINT [53], we repurpose single-turn problems into multi-turn ones by allowing LLM to interact with the environment for multiple turns before it decides to submit one solution for evaluation. Specifically for code generation problems, we provide an in-context example to guide LLMs to test their solution on provided test cases before they submit the solution. Metrics from the original data will evaluate the submitted solution to determine its correctness. We include prompt examples in \u00a7G.3."}, {"title": "CodeActAgent", "content": "We fine-tune Llama-2 7B [47] and Mistral 7B [20] on a mixture of CodeActInstruct and general conversations (Tab. 4) to obtain CodeActAgent.\nTraining Setup. We perform full-parameter supervised fine-tuning with a sequence length of 4,096 tokens for Llama-2 and 16,384 for Mistral. Please refer to \u00a7D for more details.\nEvaluation Setup. We use MINT [53] to evaluate LLMs with CodeAct on a diverse range of agent tasks. CodeActAgent has some training domains overlapping with MINT's evaluation (i.e., MINT includes ALFWorld and MATH), hence we report separate numbers for MINT's in- and out-of-domain performance. Unless otherwise specified, we measure MINT tasks' success rates with interaction turn k = 5. We also evaluate out-of-domain agent tasks using text actions from MiniWob++ (computer tasks, [21]) and ScienceWorld (text-based simulator for elementary science curriculum, [50]) to test whether CodeActAgent can generalize to different action formats. Finally, we include a suite of general LLM evaluation tasks to assess general capability: MMLU [17] for knowledge-based QA, HumanEval [7] for single-turn code-generation, GSM8K [12] for single-turn tool-free math reasoning, and MTBench [67] for instruction-following.\nCodeActAgent Excels in CodeAct Task. As shown in Tab. 5, CodeActAgent (both variants) perform better than all evaluated open-source LLMs on both the in-domain and out-of-domain subsets of MINT. On M\u00b3ToolEval, we find CodeActAgent (Mistral) outperforms open-source LLMs of similar size (7B and 13B) and even reaches similar performance to those 70B models (Tab. 3). Surprisingly, no improvement is observed for the Llama-2 variant. We discuss the potential reasons in \u00a7H.\nCodeActAgent Generalizes to Text Action. When evaluated on out-of-domain text actions, Code- ActAgent (LLaMA2, 7B), which has never been optimized for text action, achieves comparable performance to AgentLM-7B [65] which has explicit tuning for text actions.\nCodeActAgent Maintains or Improves the Performance on General LLM Tasks. In Tab. 5, we find that CodeActAgent (both variants) performs better on generic LLM tasks we tested, except for a slight degradation on MMLU for CodeActAgent (Mistral, 7B).\nAblation Study. Tab. A.8 presents ablation experiments to determine the importance of CodeActIn- struct and general conversations. Both CodeActInstruct and general conversations contribute to agent tasks, while general conversations are essential to maintain performance on general tasks."}, {"title": "Related Work", "content": "As detailed in [49], LLM-based autonomous agents are typically structured around four components: customized profiles [34, 37], long-term memory capabilities [68, 14], reasoning and planning al- gorithms [56, 10], and, most crucially, action modules. The action modules are key to facilitating LLM agents to effectively interact with external entities, including humans [23] and tools [39] in the environment [53]. In this study, we address the critical problem of standardizing the action space for LLM agents. We further discuss the difference between CodeAct and the line of work that uses code generation for problem-solving in \u00a7A. We notice a concurrent study TaskWeaver [38] similarly endorses the use of code. We discuss the principal distinctions in \u00a7B."}, {"title": "Improving LLM Agents", "content": "Two primary methods for enhancing LLM agents are prompt engineering and instruction tuning, as surveyed by [49]. For prompt engineering [27], numerous strategies have been introduced to improve the chain-of-thought reasoning [56], including self-consistency-based reasoning [54, 10] and tree-based approaches [61]. Moreover, LLMs can be strategically prompted to reflect on previous plans [63, 55, 66], enabling them to refine initial actions through trial and error. Contrast to prompt engineering, instruction tuning intrinsically enhances LLMs [11], particularly in their agent capabilities [65, 6]. For effective training, human annotators can curate expert demonstrations for specific agent tasks, such as web browsing [60, 31]. To minimize human annotation efforts, prior work creates synthetic datasets using stronger LLMs to distill agent capabilities into local models,"}, {"title": "Conclusions", "content": "This work introduces CodeAct that employs executable Python code for the LLM agent's action, which is advantageous over using text or JSON action, especially in complex scenarios. We collect CodeAct-focused multi-turn interaction trajectories CodeActInstruct for instruction tuning, and train CodeActAgent that is specially designed for seamless integration with Python and can execute sophisticated tasks (e.g., model training) leveraging existing Python packages and autonomously rectifying errors through self-debugging."}, {"title": "Broader Impacts, Limitations, and Future Work", "content": "This paper presents work whose goal is to advance LLM-based autonomous agents that can com- municate with humans through natural language and assist human users by performing tasks in environments on behalf of humans. In this section, we discuss potential societal consequences, limitations, and future work related to our work and its goal.\nCodeActAgent is an initial prototype of an autonomous agent and still has several practical limitations. For example, it may suffer from hallucination commonly seen in LLMs (e.g., imagine the content of a variable without actually printing it out), suggesting the need for subsequent alignment [33] for further improvements.\nDespite being a prototype, CodeActAgent has already demonstrated limited self-improving capability (e.g., self-debug error messages to improve its action) and the ability to interact with environments. Future work may build upon CodeActAgent to develop better agents by having them perform extensive interactions within a given environment and iteratively bootstrap their self-improving capability to learn to improve from past mistakes. More powerful agents, as results of such algorithms, are potentially beneficial for solving a wide range of real-world problems (e.g., theorem proving, drug discovery). As extensively discussed in [13], a fully autonomous agent may transform the current landscape of the labor market and impact the jobs of existing workers.\nFurthermore, since CodeAct directly grants access for the agent to freely execute code in a sandbox environment, in the worst scenario (e.g., in Sci-Fi movies), such an agent may potentially break free of the sandbox restriction and cause harm to the world through cyber-attack, highlighting the need for future work to design better safety mechanism to safeguard autonomous agents."}, {"title": "CodeActAgent Anomaly on M\u00b3ToolEval", "content": "In \u00a73.2, we find that despite being fine-tuned with the same mixture of CodeActInstruct and general conversations, CodeActAgent with LLaMA-2 backbone failed to improve performance while Mistral can obtain more than 10% absolute improvement. After carefully examining model outputs, we find examples of weird model outputs (bolded in blue below) that hint at the potential existence of training"}, {"title": "Example", "content": "Find the sum of the reciprocals of the roots of $x^2-13x+4=0$."}, {"title": "15 Related Work", "content": "Code4Struct [51] for structured prediction, PaL [15] for math reasoning, code-as-policy [26] for robot control, ViperGPT [45] for visual question answering, Voyager [48] for playing games"}, {"title": "10.711 and the $R^2$ score is 0.790", "content": "2023"}]}