{"title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine.", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "abstract": "This paper provides a detailed examination of the advancements and applications of large language models in the healthcare sector, with a particular emphasis on clinical applications. The study traces the evolution of LLMs from their foundational technologies to the latest developments in domain-specific models and multimodal integration. It explores the technical progression from encoder-based models requiring fine-tuning to sophisticated approaches that integrate textual, visual, and auditory data, thereby facilitating comprehensive AI solutions in healthcare. The paper discusses both the opportunities these technologies present for enhancing clinical efficiency and the challenges they pose in terms of ethics, data privacy, and implementation. Additionally, it critically evaluates the deployment strategies of LLMs, emphasizing the necessity of open-source models to ensure data privacy and adaptability within healthcare environments. Future research directions are proposed, focusing on empirical studies to evaluate the real-world efficacy of LLMs in healthcare and the development of open datasets for further research. This review aims to provide a comprehensive resource for both newcomers and multidisciplinary researchers interested in the intersection of AI and healthcare.", "sections": [{"title": "Introduction", "content": "The development of artificial intelligence (AI) in recent years has opened countless opportunities for various sectors, including healthcare. The potential influence of AI is a subject of debate concerning its impact on humanity. Leading AI experts have called for caution, evidenced by an open letter urging a pause in the expansion of advanced AI models, which reflects growing concerns among policymakers and the public about the ethical, social, and economic ramifications of AI. While some argue that AI can bring substantial advances in efficiency and effectiveness across many sectors, others fear it could exacerbate inequalities, displace jobs, and challenge societal norms [1].\nWhile AI in healthcare has an extensive history of research [2], the emergence of advanced language and multimodal models such as GPT family [3], Gemini [4], and a series of open models like the Llama [5], offers unprecedented perspectives for the transformation of the healthcare sector.\nThis review paper synthesizes and critically examines the landscape of large language models (LLMs) within the medical domain, focusing on their clinical"}, {"title": "1 Background", "content": "A critical milestone in NLP was the introduction of the attention mechanism in neural machine translation [9]. This mechanism, linking the encoder and decoder in sequence-to-sequence models, paved the way for subsequent advancements by enabling the model to focus on different parts of the input sequence for each step of the output, substantially improving the handling of longer input sequences and complex dependencies. Notably, it led to the creation of the Transformer model [10], which exclusively relies on attention mechanisms. This innovation revolutionized not only the field of NLP but also the broader realms of AI and machine learning (ML).\nThe emergence of Transformer-based models has spurred the development of a wide array of large language models, both in commercial and open-source formats. The most prominent series of these models, GPT [11], is based on the decoder part of the original Transformer architecture. Decoder-based models are particularly suitable for autoregressive text generation. In contrast, models like BERT [12] and its variants, which are encoder-based, are typically trained using a masked language modeling approach. While they are arguably less suitable for text generation, their applications are vast as they can be customized and fine-tuned for a plethora of tasks, including question answering, sentiment analysis, information extraction, and many more. There are also models that integrate both encoder and decoder components, such as T5 [13] and BART [14]. These models can effectively transform input sequences into output sequences, thus naturally fitting tasks such as summarization and paraphrasing.\nThe overall task of language modeling can be expressed as estimating the joint probability of a sequence of words w\u2081, w\u2082,..., wn in a sentence, drawn from large text corpora:\n$$P(w_1, w_2,..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, . . . , w_{i-1})$$"}, {"title": "1.1 Language Models", "content": "Despite the simplicity of statistical approaches to language modeling and their initial lack of focus on the underlying rules of language, as critiqued by Chomsky [15], contemporary language models like GPT-4 have exhibited remarkable proficiency in various language understanding tasks [3].\nThese models demonstrate emergent abilities and adeptness in learning natural language patterns. Modern language models have undergone a notable paradigm shift, moving from a pre-training and fine-tuning approach to embracing in-context learning (ICL) [16] and zero-shot learning. This advancement enables them to solve tasks without the need for updating the models' weights. Nevertheless, challenges remain, as underscored by Mahowald et al. [17]. They highlight the gap between formal and functional linguistic competencies in LLMs, noting these models often lack functional competencies. They propose integrating modularity into the system architecture or attaining it through a revised training process.\nNotably, current research is shifting towards multimodal models that can integrate multiple modalities: visual, text, and potentially audio, into a single model [18], thus paving the way for more comprehensive AI solutions."}, {"title": "1.2 Open-source LLMs", "content": "Open-source LLMs are publicly accessible and, unlike their commercial counterparts, are freely available for modification and distribution. This openness allows for customization and adaptation to specific needs, including deployment on private servers, which enhances data privacy and ownership, paramount in sensitive fields like medicine.\nIn the medical domain, the ability to deploy LLMs on-premise is a substantial advantage. It ensures that sensitive medical data does not have to be shared with third-party providers, thereby complying with stringent data protection regulations. Healthcare organizations can also tailor these models to better understand and generate medical language, optimize them for specific types of medical inquiries, and integrate them with confidential datasets, all while maintaining patient confidentiality.\nHowever, open-source LLMs have limitations compared to commercial models. They often lack the extensive support, continuous updates, and robust infrastructure provided by commercial vendors. These deficiencies can lead to challenges in model maintenance, scalability, and performance. The primary barrier to the widespread adoption of advanced open-source LLMs is their significant number of parameters, each typically represented by a floating-point number. Generally, a higher number of parameters enhances reasoning, pattern recognition, and linguistic capabilities, but this comes at the cost of increased storage requirements and the need for more powerful GPUs, both for inference and fine-tuning. Recent research has provided notable solutions for mitigating the computational resource needs of large models by introducing parameter-efficient fine-tuning techniques and reducing memory and storage requirements. For instance, Low-Rank Adaptation (LoRA) [50] optimizes updates through low-rank parameter adjustments instead of updating all model parameters. Additionally, applying quantization techniques [51] allows open-source LLMs to reduce the memory footprint and enhance inference speed, thus making them more efficient and suitable for deployment across a broader range of hardware. Quantized Low-Rank Adaptation (QLORA) [52] combines these techniques to efficiently fine-tune large language models while significantly reducing memory and computational demands. For example, a model with 1 billion parameters, when using 16-bit floating-point precision, requires at least 2GB of GPU memory for inference, depending on the input length. In contrast, a quantized version of the same model could require approximately half of this amount. Furthermore, recent work by Guo et al. proposes a novel memory allocation framework that utilizes low-level GPU virtual memory management. This framework incorporates a Virtual Memory Stitching (VMS) mechanism, reducing GPU memory usage and fragmentation, thereby further enhancing the efficiency of deploying LLMs [53].\nAnother notable advancement, flash attention [54], allows for more efficient hardware usage thus opening additional opportunities for on-premise hardware to utilize larger models."}, {"title": "1.3 Domain-specific Language Models in Healthcare", "content": "Domain-specific LLMs are models tailored for a narrow field or topic, enhancing their ability to produce more accurate and relevant responses within that specific area. Generally, there are two common methods to create a domain-specific model: one approach is to pre-train a model using a set of domain-specific documents, such as medical papers; another is to take a generically trained model and fine-tune or adapt it to the target domain.\nThe medical field, rich in unstructured textual data from sources such as Electronic Health Records (EHR) and other medical documentation, presents an ideal research domain for language models to address a wide array of challenges. These applications range from tasks previously tackled by NLP techniques, such as clinical acronym disambiguation, to those unattainable a decade ago. For example, medical chatbots that can assist both patients and healthcare professionals are now emerging. In response to these advancements, the research community has been actively developing language models specifically designed for medical applications. The evolution of these models, from pre-training and fine-tuning strategies to creating open-source models capable of zero-shot learning, highlights the growing synergy between artificial intelligence and healthcare [88, 94, 95, 113].\nEarly medical language models were predominantly based on BERT and followed the common trend of the pre-training and fine-tuning paradigm. ClinicalBERT [55], pre-trained on the MIMIC-III [56] dataset and fine-tuned to predict hospital readmissions, was one of the pioneering models in this domain. Another notable model, BioBERT, was pre-trained on the PubMed library and fine-tuned for tasks such as named entity recognition (NER), relation extraction (RE), and medical question answering (QA) [57]. The proven efficacy of pre-training language models on biomedical corpora and fine-tuning them for specific downstream tasks, like clinical concept extraction or measuring semantic text similarity (STS), led to the development of many other BERT-based models, including BiomedBERT [70], PubMedBERT [72], BEHRT [78], and GatorTron [80].\nThe debut of BioGPT has marked another milestone in biomedical NLP. This generative pre-trained Transformer, based on GPT-2 and trained on millions of PubMed abstracts, has demonstrated superior performance in specialized tasks, including text classification, biomedical text generation, and data mining [85].\nAlpaCare, a medical instruction-tuned LLM trained on more than fifty thousand instruction-response pairs generated artificially using GPT-4, represents a notable example from the current generation of medical LLMs that can be utilized in the medical domain without the need for fine-tuning on downstream tasks [88]. Clinical Camel [95], another medical LLM fine-tuned from the Llama-2 model, excels in various medical tasks, ranging from clinical note creation to medical triaging. Despite its limitations, such as the potential for generating misleading content and the need for continuous updates, it marked a remarkable advancement in medical LLMs. The issue of needing continuous updates was addressed by the ChatDoctor [94] model. Based on the Llama family and fine-tuned with real-world patient-doctor conversations, this model can access external information sources, a crucial capability in the medical field, especially for emerging diseases.\nRecent advancements in the field, particularly the development of multimodal models that work with both text and images, have substantially impacted the medical domain as well. The LLaVA-Med model [101] can effectively answer open-ended questions about biomedical images, while the CheXagent model [105], designed specifically for chest X-ray interpretation, exemplifies the successful transition from general medical models to specialist models capable of effectively tackling problems in narrow medical fields."}, {"title": "2 Medical Applications of LLMs", "content": "Medical applications of LLMs can be defined as the intersection of two sets: tasks that LLMs can accomplish and potential healthcare needs where predominantly textual LLMs can add value. Although both sets are finite, the cardinality of the first set is arguably smaller given the vast scope of the medical domain. Thus, our approach to categorizing medical applications is based on LLMs' tasks of various granularity, which, in turn, have a large area of intersection with NLP tasks in general. We focus on practically significant LLM tasks from an application perspective, as well as"}, {"title": "2.1 Text Generation", "content": "The task of text generation in the medical domain involves creating contextually accurate and relevant medical texts based on a sequence of prior tokens and specific context. This task may include generating clinical notes, patient reports, or drafts of research papers. The primary challenge is ensuring that the generated text is precise, medically accurate, and adheres to relevant privacy and ethical standards.\nTo achieve this, the probabilistic framework underpinning text generation models is used. The probability of generating the next token xt, given a sequence of previous tokens x1,x2,...,xt\u22121 and additional context, can be defined as:\n$$P(x_t | x_1, x_2,..., x_{t-1}; context)$$\nIn this formulation, 'context' may represent various factors depending on the nature of the task. For instance, when generating patient reports, the context would include the available information about the patient, such as vital statistics, previous diagnoses and treatments.\nDecoder-based LLMs inherently generate text by predicting the probability of the next token based on preceding tokens and contextual information. The methods for providing context vary. The simplest approach is using prompts given directly to the LLM. However, because the knowledge within LLMs is static, integrating external knowledge sources can be advantageous. This integration is often accomplished using variations of Retrieval-Augmented Generation (RAG) techniques [172, 173].\nAdvancements in transformer-based models have led to innovative approaches in generating clinical documentation from patient-provider interactions. A study by Brake and Schaaf compares two model designs for generating clinical notes from doctor-patient conversations using the PEGASUS-X model. The first design, GENMOD, generates the entire note in one step, while the second design, SPECMOD, generates each section independently. The study aims to evaluate the consistency of the generated notes in terms of age, gender, body part, and coherence. Evaluations were performed using ROUGE and Factuality metrics, human reviewers, and the Llama2 LLM. Results indicate that GENMOD improves consistency in age, gender, and body part references, while SPECMOD may have advantages in coherence depending on the interpretation. The study uses a proprietary dataset with 10,859 doctor-patient conversations for training and testing [174]. Nair et al. present MEDSUM-ENT, a multi-stage approach to generating medically accurate summaries from patient-provider dialogues. Using GPT-3 as the backbone, the approach first extracts medical entities and their affirmations from conversations and then constructs summaries based on these extractions through prompt chaining. The model leverages few-shot prompting and dynamic example selection to improve entity extraction and summarization. The dataset used for evaluation consists of 100 de-identified clinical encounters from a telehealth platform. MEDSUM-ENT demonstrates improved clinical accuracy and coherence in summaries compared to a zero-shot, single-prompt baseline, as evidenced by both qualitative physician assessments and quantitative metrics designed to capture medical correctness [118].\nThe integration of LLMs with visual models facilitates the automatic generation of medical imaging reports. Chen et al. developed Dia-LLaMA [115], a framework that utilizes the LLaMA2-7B model combined with a pre-trained ViT3D [175] to manage high-dimensional CT data. It features a disease prototype memory bank and a disease-aware attention module to counteract the imbalance in disease occurrence. The framework, tested on the CTRG-Chest-548K dataset [120], showed superior performance in clinical efficacy and natural language generation metrics, surpassing other methods [115]. Another approach, R2GenGPT, enables radiology report generation by utilizing a visual alignment module that aligns visual features from chest X-ray images with the word embedding space of LLMs, thereby enhancing the capability of static LLMs to process visual data. It explores three alignment strategies shallow, deep, and delta each varying in trainable parameters. Evaluated on the IU-Xray and MIMIC-CXR datasets, R2GenGPT achieved impressive results in model efficiency and clinical metrics, leveraging the Swin Transformer and Llama2-7B model for enhanced integration [114].\nEvaluating commercial models for clinical tasks inevitably draws the interest of the scientific community. Ali et al. evaluated the use of ChatGPT for generating patient clinic letters, focusing on its readability, factual correctness, and human-like quality by testing the model with shorthand instructions simulating clinical input for creating letters addressing skin cancer scenarios. The study involved 38 hypothetical clinical scenarios, including basal cell carcinoma, squamous cell carcinoma, and malignant melanoma. Readability was assessed using the online tool Readable, targeting a sixth-grade reading level. Two independent clinicians evaluated the letters' correctness and human-like quality using a Likert scale. The study found that ChatGPT-generated letters scored highly in correctness and human-like quality, comparable to letters written by humans, highlighting the potential of AI to enhance clinical communication while emphasizing the need for careful regulation and human oversight to mitigate risks [176].\nOverall, while we observe the significant potential of text-only LLMs in text generation tasks, the emergence of multimodal models will inevitably bear fruit in this class of tasks in the medical domain. For example, fully multimodal models will be able to accurately generate clinical documentation based on patient-provider verbal dialogues and summarize them based on the end user's needs, while generating medical imaging reports will become less complicated and more accurate when done by a single multimodal model."}, {"title": "2.2 Token Classification", "content": "Token classification tasks in the medical domain involve labeling individual words or phrases within a text with specific medical annotations, such as identifying and disambiguating medical conditions, medications, dosages, and symptoms from clinical text.\nGiven a sequence of tokens X = (x1,x2,...,xn), the token classification task involves assigning a label y from a set of categories C to each token x\u2081 in the sequence. This can be expressed as:\n$$y_i = f(x_i, context)$$\nwhere yi is the label for the token xi, and f is a function mapping each token and its contextual information to a label in C. In this formulation, \"context\" usually includes the surrounding tokens and may also encompass information outside of the sequence X, such as external vocabulary.\nTypical implementations for the token classification task involve both masked and generative language models. With masked models, such as BERT, the model is fine-tuned on a labeled dataset of clinical notes. Here, tokens are masked not for prediction but rather to infer their labels from the context, using a classification layer. In contrast, generative models typically use prompts to facilitate the generation of text already annotated with labels, thereby directly producing labeled sequences.\nThe widespread use of medical abbreviations and acronyms often leads to misunderstandings, necessitating accurate disambiguation of these terms to safeguard against misinterpretations that could jeopardize patient care [177]. The process of mapping the short forms of medical terms to their full expressions is referred to as clinical acronym disambiguation. Wang and Khanna evaluated the performance of various clinical BERT-based language models on the Clinical Acronym Sense Inventory (CASI) dataset and found that ClinicalBert addresses the task effectively, achieving an F1 score of 91.49% [130]. Additionally, Sivarajkumar et al. assessed the capabilities of generative large language models, including GPT3.5, BARD, and Llama2, in acronym disambiguation using the same dataset. Their study revealed that these models perform well in acronym disambiguation even without fine-tuning, with GPT3.5 achieving the highest accuracy of 0.96 [178].\nWhile the problem of clinical acronym disambiguation might seem largely addressed, several challenges persist. For instance, Kugic et al. conducted a study on clinical acronym disambiguation in German using ChatGPT and BING, achieving an F1 score of 0.679, which highlights the need for improvement [125]. Another concern involves the datasets used in experiments. There is a possibility that LLMs may memorize specific terms, which could misrepresent their true disambiguation capabilities [179] [180]. This issue necessitates further investigation to ensure the reliability of LLMs in clinical abbreviation disambiguation tasks with realistic datasets."}, {"title": "2.3 Sequence Classification", "content": "Sequence classification tasks in the medical domain involve assigning a categorical label to an entire sequence of text, rather than to individual tokens. This could involve classifying entire clinical documents or patient notes into categories such as diagnosis, treatment recommendation, or urgency level.\nThe task can be formulated as follows. Given a document d that consists of a sequence of tokens d = (x1,x2,...,xn), the sequence classification function assigns a label y from a set of categories C, expressed by the equation:\n$$y = f(d)$$\nwhere y is the category associated with the document d, and f is a function that maps the entire document, taking into account its contextual coherence and thematic structure, to a label in C. LLMs can serve as implementations of the function f.\nMasked LLMs are particularly well-suited for sequence classification tasks due to their inherent structure. Originally, these models predict masked tokens within a sequence. For the purpose of classification, they can be adapted by employing either the output of a [CLS] token (in case of BERT-like models) or by utilizing various pooling functions on the embeddings. In the case of the [CLS] token, a fully connected layer is added on top of the token's output, which is designed to capture the context of the entire sequence. Alternatively, pooling functions such as average pooling or max pooling can be applied to the embeddings of the sequence to aggregate information across the entire input.\nGenerative LLMs can be either simply prompted or fine-tuned in a way that they learn to predict class labels as part of the sequence. This can be done by training the model on inputs where a special token or delimiter indicates the end of the input and the start of the output label. Although less common, integrating a linear layer after the final token output in generative models could refine the logits corresponding to class predictions, sharpening the classification boundaries set by the model's generative nature.\nThe array of tasks that fall under sequence classification is vast. The following subsections detail a few of the most prominent applications."}, {"title": "2.3.1 Suicidal Behavior Prediction", "content": "Suicidal behavior prediction tasks predominantly focus on analyzing individuals' social media activities. Dus and Nefedov developed an automated tool for identifying potential self-harm indications in social media posts, treating the task as a binary classification problem [135]. Given an input x, the objective is to predict a binary label y, where:\n$$y = \\begin{cases}\n1 & \\text{if suicidal behavior is detected,}\\\\\n0 & \\text{otherwise.}\n\\end{cases}$$\nHere, x represents features extracted from social media posts. The model estimates P(y = 1|x), the probability that the input suggests suicidal behavior, using a fine-tuned ELECTRA model. This model was trained on data from Kaggle's \"Suicide Watch\" dataset [137] and additional social media sources. Their method attained a noteworthy accuracy rate of 93% and an F1 score of 0.93 [135].\nBeyond mere social media post analysis, Levkovich et al. assessed ChatGPT-3.5 and ChatGPT-4's ability to evaluate suicide risk based on perceived burdensomeness and thwarted belongingness. By comparing ChatGPT's assessments to those made by mental health professionals using vignettes, they discovered that ChatGPT-4's evaluations were in close alignment with professional judgments. In contrast, ChatGPT-3.5 tended to underestimate suicide risk, underscoring the limitations of these models in this specific area [181].\nIn summary, while treating suicidal behavior identification as a straightforward classification task on social media posts can lead to impressive scores using standard classification metrics, the practical and ethical implications of such approaches, including potential breaches of autonomy and principles of non-maleficence, are debatable. Well-structured vignette studies on the effectiveness of LLMs and other models can further advance research in this area. Additionally, exploring the potential of human-AI collaboration represents another promising research direction in this field."}, {"title": "2.3.2 Modeling Patient Timeline", "content": "The task of modeling patient timelines is multifaceted, involving forecasting future medical events, understanding patient trajectories, and predicting medical outcomes. This endeavor employs deep learning, transformers, and generative models to analyze data from various medical records, both structured and unstructured.\nKraljevic et al. introduced Foresight, a GPT-2-based pipeline developed for modeling biomedical concepts extracted from clinical narratives. This pipeline employs NER and linking tools to transform unstructured text into structured, coded concepts. Utilizing datasets from three hospitals, covering over 800,000 patients, Foresight showed promise in forecasting future medical events. Its effectiveness was validated by clinicians on synthetic patient timelines, highlighting its potential in real-world risk forecasting and clinical research [133].\nAside from LLMs, generative adversarial networks (GAN) have gained popularity, extending beyond their initial image generation domain. Shankar et al. proposed Clinical-GAN, which merges Transformer and GAN methodologies to model patient timelines, focusing on predicting future medical events based on past diagnosis, procedure, and medication codes. Tested on the MIMIC-IV dataset [139], Clinical-GAN outperformed baseline methods in trajectory forecasting and sequential disease prediction [182]. Another study employed GAN for predicting the length of stay in emergency departments. The learning process was done in multiple stages. Initially, an unsupervised training phase used a generator and discriminator to approximate the probability distribution and perform feature discovery and reconstruction. Discriminator was then fine-tuned to optimize its parameters for global optimum. A predictor layer, initially randomly initialized, was added and optimized during fine-tuning, enabling the model to map observations to their lengths of stay. The model was trained on data from the Pediatric Emergency Department in CHRU-Lille and proved the potential of GANs in this field [183].\nMedical outcome prediction can be seen as a subtask of modeling patient timeline and is often scoped to either predict mortality, outcomes of a specific disease, or risk of progression from one disease to another. A recent study by Shoham and Rappoport [134] examined data related to chronic kidney disease, acute and unspecified renal failure, and adult respiratory failure from the MIMIC-IV and eICU-CRD datasets. Using this data, the team generated labeled datasets for disease diagnosis prediction based on patient histories. They introduced a method named Clinical Prediction with Large Language Models (CPLLM) by fine-tuning LLMs (Llama2 and BioMedLM) using medical-specific prompts to help the models understand complex medical concept relationships. Xie et al. used EHR analysis to predict epilepsy seizures, leveraging Bio_ClinicalBERT, RoBERTa, and T5, achieving an F1 score of 0.88 in outcome classification [146].\nA notable approach to predict outcomes of COVID-19 patients was proposed by Henriksson et al. [184]. The authors created a model that combines structured data and unstructured clinical notes in a multimodal fashion, leveraging a clinical KB-BERT model for multimodal fine-tuning. Trained on data from six hospitals in Stockholm, Sweden, their model effectively predicted 30-day mortality, safe discharge, and readmission of COVID-19 patients in the emergency department."}, {"title": "2.3.3 Phenotyping and Medical Coding", "content": "The phenotyping task primarily involves identifying phenotypic abnormalities from a patient's various medical records, which aids in the identification of rare diseases. There exists the Human Phenotype Ontology (HPO) project\u00b9 that systematically categorizes human phenotypes with detailed annotations. Thus, the phenotyping task can be framed as a multi-label classification task where we need to find mapping f: X \u2192 2Y, with X representing the set of patient medical records, where each medical record xi \u2208 X corresponds to the medical data (unstructured text in most cases) from the i-th record, Y denotes the set of all possible phenotype labels from HPO, where each label yj \u2208 Y represents a specific phenotype. The objective of the phenotyping task is to map each medical record xi to a subset of phenotype labels YY, such that Yi represents the set of phenotypes exhibited by the patient associated with xi:\n$$Y_i = f(x_i)$$\nTraditionally, the task of phenotyping has relied on named entity recognition, where models similar to BERT have demonstrated proficiency. However, recent studies have started exploring in-context learning and zero-shot learning with contemporary LLMs, yielding promising results [131].\nMedical coding is another multi-label classification task that involves identifying a set of International Classification of Diseases (ICD) codes2 associated with a medical record. This task can be formulated similarly to phenotyping, with the key difference being that Y represents a set of ICD codes rather than phenotype labels. Besides observing trends similar to those in the phenotyping subdomain, it is also noteworthy that there is a shift towards explainable medical coding, as highlighted in [132]."}, {"title": "2.4 Question Answering and Information Extraction", "content": "Question Answering (QA) task can be formulated as finding the answer A from a possible set of answers A, given a question Q and a context C (often a document or set of documents containing information relevant to the question). This can be expressed as:\n$$A = \\underset{a \\in A}{\\text{argmax}} P(a | Q, C)$$\nwhere P(a | Q, C) is the probability of a being the correct answer given the question Q and the context C.\nInformation Extraction (IE) task involves identifying specific pieces of information (entities, relationships, events) within documents. This can be described as a function f that maps a set of documents D to a set of structured attributes S, which includes entities E, relationships R, and other attributes of interest:\n$$S = f(D) = \\{E, R, . . .\\}$$\nHere, D is the input document, and S represents the structured output containing extracted elements.\nMasked models are well-suited for question answering tasks. They can be fine-tuned on specific QA datasets, where the input is a concatenation of the question and context (a paragraph or document containing the answer). The model is then trained to identify the span of text that answers the question, typically by adding a start and end token classifier to the output embeddings of the MLM. These classifiers predict the beginning and end positions of the answer in the text. On the other hand, generative models leverage their extensive pre-training on diverse data. By inputting a question (along with the document of interest when necessary) and following it with a prompt that encourages the model to generate an answer, these models can produce responses without needing explicit pointers to answer spans.\nIn the medical domain, IE and QA systems are instrumental for extracting data from electronic health records, such as medication lists and diagnostic details, essential for patient management and treatment planning. A notable example is quEHRy, a QA system designed to query EHRs using natural language. The primary goal of quEHRy is to provide precise and interpretable answers to clinicians' questions from structured EHR data [150]. Beyond the successful applications of BERT-based models like BioBERT, BiomedBERT, and PubMedBERT for QA and IE, generative models also show proficiency. Agrawal et al. demonstrated the effectiveness of generative LLMs such as InstructGPT and GPT-3 in zero-shot and few-shot information extraction from clinical texts. When tested on the re-annotated CASI dataset, these models showed considerable potential in tasks requiring structured outputs [185]. Furthermore, Ge et al. compared the effectiveness of LLMs versus manual chart reviews for extracting data elements from EHRs, focusing specifically on hepatocellular carcinoma imaging reports. Using the GPT-3.5-turbo model, implemented as \"Versa Chat\" within a secure UCSF 3 environment to protect patient health information, the study analyzed 182 CT or MRI abdominal imaging reports from the Functional Assessment in Liver Transplantation study. It extracted six distinct data elements, including the maximum LI-RADS\u2074 score, number of hepatocellular carcinoma lesions, and presence of macrovascular invasion. The performance was evaluated by calculating accuracy, precision, recall, and F1 scores, showing high overall accuracy (0.889) with variations depending on the complexity of the data elements [186]."}, {"title": "2.5 Summarization and Paraphrasing", "content": "Paraphrasing involves rewriting a text T into a new form P, ensuring that P maintains the same meaning as T but utilizes different vocabulary and potentially altered sentence structures. Summarization, on the other hand, entails generating a brief version of a text T that preserves its core information. Abstractive summarization can be considered a specific case of paraphrasing.\nMasked language models are proficient at extractive summarization. They evaluate sentences within a text to determine their relevance and informativeness. By scoring each sentence, these models identify and concatenate the most important sentences to form a coherent summary. In contrast, abstractive summarization and paraphrasing typically employ generative (decoder-based) or sequence-to-sequence (encoder-decoder) models. These models are trained to understand the entire narrative or document and then recreate its essence in a different form.\nSummarization and paraphrasing tools are used in the medical domain for managing extensive documentation and enhancing communication. Summarization helps healthcare professionals quickly grasp essential details from lengthy clinical notes, generate concise abstracts of medical research papers, and craft clear patient discharge summaries, thereby improving patient comprehension and adherence to medical advice. Paraphrasing makes complex information more accessible by translating medical jargon into simpler language for patient education. It also enhances the clarity and consistency of electronic health records, aiding healthcare providers in better understanding and utilizing the data effectively.\nSummarization and paraphrasing in the medical domain are largely driven by advancements in these tasks in general. Devaraj et al. introduce a new dataset derived from the Cochrane Database of Systematic Reviews, featuring pairs of technical abstracts and plain language summaries. They propose a novel metric based on masked language models to better distinguish between technical and simplified texts. The study utilizes baseline encoder-decoder Transformer models for text simplification and introduces an innovative approach to penalize the generation of jargon terms. The code and data are publicly available for further research [153].\nThe paper titled \"Biomedical Text Readability After Hypernym Substitution with Fine-Tuned Large Language Models\" investigates simplifying biomedical text using LLMs to enhance patient understanding. The authors fine-tuned three LLM variants to replace complex biomedical terms with their hypernyms. The models used include GPT-J-6b, SciFive T5, and an approach combining sequence-to-sequence and sciBERT models. The study processed 1,000 biomedical definitions from the Unified Medical Language System and evaluated readability improvements using metrics such as the Flesch-Kincaid Reading Ease and Grade Level, Automated Readability Index, and Gunning Fog Index. Results showed substantial readability improvements, with the GPT-J-6b model performing best in reducing sentence complexity [156].\nAnother interesting application of paraphrasing is the anonymization of medical documents, which is crucial for balancing ethical principles and research needs. Wiest et al. present an approach to de-identify medical free text using LLMs. The authors benchmarked eight locally deployable LLMs, including Llama-3 8B, Llama-3 70B, Llama-2 7B, Llama-2 70B, and Mistral 7B, on a dataset of 100 clinical letters from a German hospital. They developed the LLM-Anonymizer pipeline, which achieved a success rate of 98.05% in removing personal identifying information using Llama-3 70B. The tool is open-source, operates on local hardware, and does not require programming skills, making it accessible and practical for use in medical institutions. The study demonstrates the potential of LLMs to effectively de-identify medical texts, outperforming traditional NLP methods and providing a robust solution for privacy-preserving data sharing in healthcare [157].\nDespite advancements in summarization and paraphrasing, some challenges persist, particularly in preserving factual accuracy and precision. Jeblick et al. explored the effectiveness of using ChatGPT (version December 15th, 2022) to simplify radiology reports into language understandable by non-experts. A radiologist created three hypothetical radiology reports, which were then simplified by prompting ChatGPT. Fifteen radiologists evaluated the quality of these simplified reports based on criteria such as factual correctness, completeness, and potential harm to patients. The study used Likert scale analysis and inductive free-text categorization to assess the simplified reports. Overall, the radiologists found the simplified reports to be factually correct and complete, with minimal potential for harm. However, some issues were noted, including incorrect information, omissions of relevant medical data, and occasionally misleading or vague statements. These issues highlight the need for careful supervision by medical professionals when using language models to simplify complex medical texts [155]. A recent study by Landman et al. discusses a challenge organized by Pfizer to explore the use of LLMs for automating the summarization of safety tables in clinical study reports. Various teams employed GPT models with prompt engineering techniques to generate summary texts. The datasets included safety outputs from 72 reports from recent clinical studies, split into 70% for training and 30% for testing. The study concluded that while LLMs show promise in automating the summarization of clinical study report tables, human involvement and further research are necessary to optimize their application [151]."}, {"title": "2.6 Conversation", "content": "The task of conversation, or dialogue generation, can"}]}