{"title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine.", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "abstract": "This paper provides a detailed examination of the advancements and applications of large language models in the healthcare sector, with a particular emphasis on clinical applications. The study traces the evolution of LLMs from their foundational technologies to the latest developments in domain-specific models and multimodal integration. It explores the technical progression from encoder-based models requiring fine-tuning to sophisticated approaches that integrate textual, visual, and auditory data, thereby facilitating comprehensive AI solutions in healthcare. The paper discusses both the opportunities these technologies present for enhancing clinical efficiency and the challenges they pose in terms of ethics, data privacy, and implementation. Additionally, it critically evaluates the deployment strategies of LLMs, emphasizing the necessity of open-source models to ensure data privacy and adaptability within healthcare environments. Future research directions are proposed, focusing on empirical studies to evaluate the real-world efficacy of LLMs in healthcare and the development of open datasets for further research. This review aims to provide a comprehensive resource for both newcomers and multidisciplinary researchers interested in the intersection of AI and healthcare.", "sections": [{"title": "Introduction", "content": "The development of artificial intelligence (AI) in recent years has opened countless opportunities for various sectors, including healthcare. The potential influence of AI is a subject of debate concerning its impact on humanity. Leading AI experts have called for caution, evidenced by an open letter urging a pause in the expansion of advanced AI models, which reflects growing concerns among policymakers and the public about the ethical, social, and economic ramifications of AI. While some argue that AI can bring substantial advances in efficiency and effectiveness across many sectors, others fear it could exacerbate inequalities, displace jobs, and challenge societal norms [1]. While AI in healthcare has an extensive history of research [2], the emergence of advanced language and multimodal models such as GPT family [3], Gemini [4], and a series of open models like the Llama [5], offers unprecedented perspectives for the transformation of the healthcare sector.\nThis review paper synthesizes and critically examines the landscape of large language models (LLMs) within the medical domain, focusing on their clinical"}, {"title": "1 Background", "content": ""}, {"title": "1.1 Language Models", "content": "A critical milestone in NLP was the introduction of the attention mechanism in neural machine translation [9]. This mechanism, linking the encoder and decoder in sequence-to-sequence models, paved the way for subsequent advancements by enabling the model to focus on different parts of the input sequence for each step of the output, substantially improving the handling of longer input sequences and complex dependencies. Notably, it led to the creation of the Transformer model [10], which exclusively relies on attention mechanisms. This innovation revolutionized not only the field of NLP but also the broader realms of AI and machine learning (ML).\nThe emergence of Transformer-based models has spurred the development of a wide array of large language models, both in commercial and open-source formats. The most prominent series of these models, GPT [11], is based on the decoder part of the original Transformer architecture. Decoder-based models are particularly suitable for autoregressive text generation. In contrast, models like BERT [12] and its variants, which are encoder-based, are typically trained using a masked language modeling approach. While they are arguably less suitable for text generation, their applications are vast as they can be customized and fine-tuned for a plethora of tasks, including question answering, sentiment analysis, information extraction, and many more. There are also models that integrate both encoder and decoder components, such as T5 [13] and BART [14]. These models can effectively transform input sequences into output sequences, thus naturally fitting tasks such as summarization and paraphrasing.\nThe overall task of language modeling can be expressed as estimating the joint probability of a sequence of words w\u2081, W2,..., Wn in a sentence, drawn from large text corpora:\n$$P(w_1, w_2,..., w_n) = \\prod_{i=1}^{n} P(w_i | w_1, w_2, . . ., w_{i-1})$$"}, {"title": "1.2 Open-source LLMs", "content": "Open-source LLMs are publicly accessible and, unlike their commercial counterparts, are freely available for modification and distribution. This openness allows for customization and adaptation to specific needs, including deployment on private servers, which enhances data privacy and ownership, paramount in sensitive fields like medicine.\nIn the medical domain, the ability to deploy LLMs on-premise is a substantial advantage. It ensures that sensitive medical data does not have to be shared with third-party providers, thereby complying with stringent data protection regulations. Healthcare organizations can also tailor these models to better understand and generate medical language, optimize them for specific types of medical inquiries, and integrate them with confidential datasets, all while maintaining patient confidentiality. However, open-source LLMs have limitations compared to commercial models. They often lack the extensive support, continuous updates, and robust infrastructure provided by commercial vendors. These deficiencies can lead to challenges in model maintenance, scalability, and performance. The primary barrier to the widespread adoption of advanced open-source LLMs is their significant number of parameters, each typically represented by a floating-point number. Generally, a higher number of parameters enhances reasoning, pattern recognition, and linguistic capabilities, but this comes at the cost of increased storage requirements and the need for more powerful GPUs, both for inference and fine-tuning. Recent research has provided notable solutions for mitigating the computational resource needs of large models by introducing parameter-efficient fine-tuning techniques and reducing memory and storage requirements. For instance, Low-Rank Adaptation (LoRA) [50] optimizes updates through low-rank parameter adjustments instead of updating all model parameters. Additionally, applying quantization techniques [51] allows open-source LLMs to reduce the memory footprint and enhance inference speed, thus making them more efficient and suitable for deployment across a broader range of hardware. Quantized Low-Rank Adaptation (QLORA) [52] combines these techniques to efficiently fine-tune large language models while significantly reducing memory and computational demands. For example, a model with 1 billion parameters, when using 16-bit floating-point precision, requires at least 2GB of GPU memory for inference, depending on the input length. In contrast, a quantized version of the same model could require approximately half of this amount. Furthermore, recent work by Guo et al. proposes a novel memory allocation framework that utilizes low-level GPU virtual memory management. This framework incorporates a Virtual Memory Stitching (VMS) mechanism, reducing GPU memory usage and fragmentation, thereby further enhancing the efficiency of deploying LLMs [53]. Another notable advancement, flash attention [54], allows for more efficient hardware usage thus opening additional opportunities for on-premise hardware to utilize larger models."}, {"title": "1.3 Domain-specific Language Models in Healthcare", "content": "Domain-specific LLMs are models tailored for a narrow field or topic, enhancing their ability to produce more accurate and relevant responses within that specific area. Generally, there are two common methods to create a domain-specific model: one approach is to pre-train a model using a set of domain-specific documents, such as medical papers; another is to take a generically trained model and fine-tune or adapt it to the target domain.\nThe medical field, rich in unstructured textual data from sources such as Electronic Health Records (EHR) and other medical documentation, presents an ideal research domain for language models to address a wide array of challenges. These applications range from tasks previously tackled by NLP techniques, such as clinical acronym disambiguation, to those unattainable a decade ago. For example, medical chatbots that can assist both patients and healthcare professionals are now emerging. In"}, {"title": "2 Medical Applications of LLMs", "content": "Medical applications of LLMs can be defined as the intersection of two sets: tasks that LLMs can accomplish and potential healthcare needs where predominantly textual LLMs can add value. Although both sets are finite, the cardinality of the first set is arguably smaller given the vast scope of the medical domain. Thus, our approach to categorizing medical applications is based on LLMs' tasks of various granularity, which, in turn, have a large area of intersection with NLP tasks in general. We focus on practically significant LLM tasks from an application perspective, as well as"}, {"title": "2.1 Text Generation", "content": "The task of text generation in the medical domain involves creating contextually accurate and relevant medical texts based on a sequence of prior tokens and specific context. This task may include generating clinical notes, patient reports, or drafts of research papers. The primary challenge is ensuring that the generated text is precise, medically accurate, and adheres to relevant privacy and ethical standards.\nTo achieve this, the probabilistic framework underpinning text generation models is used. The probability of generating the next token xt, given a sequence of previous tokens x1,x2,...,xt\u22121 and additional context, can be defined as:\n$$P(x_t | x_1, x_2,..., x_{t-1}; context)$$\nIn this formulation, 'context' may represent various factors depending on the nature of the task. For instance, when generating patient reports, the context would include the available information about the patient, such as vital statistics, previous diagnoses and treatments.\nDecoder-based LLMs inherently generate text by predicting the probability of the next token based on preceding tokens and contextual information. The methods for providing context vary. The simplest approach is using prompts given directly to the"}, {"title": "2.2 Token Classification", "content": "Token classification tasks in the medical domain involve labeling individual words or phrases within a text with specific medical annotations, such as identifying and disambiguating medical conditions, medications, dosages, and symptoms from clinical text.\nGiven a sequence of tokens X = (x1,x2,...,xn), the token classification task involves assigning a label y from a set of categories C to each token x\u1d62 in the sequence. This can be expressed as:\nyi = f(xi, context)\nwhere yi is the label for the token xi, and f is a function mapping each token and its contextual information to a label in C. In this formulation, \"context\" usually includes the surrounding tokens and may also encompass information outside of the sequence X, such as external vocabulary.\nTypical implementations for the token classification task involve both masked and generative language models. With masked models, such as BERT, the model is fine-tuned on a labeled dataset of clinical notes. Here, tokens are masked not for prediction but rather to infer their labels from the context, using a classification layer. In contrast, generative models typically use prompts to facilitate the generation of text already annotated with labels, thereby directly producing labeled sequences.\nThe widespread use of medical abbreviations and acronyms often leads to misunderstandings, necessitating accurate disambiguation of these terms to safeguard against misinterpretations that could jeopardize patient care [177]. The process of mapping the short forms of medical terms to their full expressions is referred to as clinical acronym disambiguation. Wang and Khanna evaluated the performance of various clinical BERT-based language models on the Clinical Acronym Sense Inventory (CASI) dataset and found that ClinicalBert addresses the task effectively, achieving an F1 score of 91.49% [130]. Additionally, Sivarajkumar et al. assessed the capabilities of generative large language models, including GPT3.5, BARD, and Llama2, in acronym disambiguation using the same dataset. Their study revealed that these models perform well in acronym disambiguation even without fine-tuning, with GPT3.5 achieving the highest accuracy of 0.96 [178].\nWhile the problem of clinical acronym disambiguation might seem largely addressed, several challenges persist. For instance, Kugic et al. conducted a study on clinical acronym disambiguation in German using ChatGPT and BING, achieving an F1 score of 0.679, which highlights the need for improvement [125]. Another concern involves the datasets used in experiments. There is a possibility that LLMs may memorize specific terms, which could misrepresent their true disambiguation capabilities [179] [180]. This issue necessitates further investigation to ensure the reliability of LLMs in clinical abbreviation disambiguation tasks with realistic datasets."}, {"title": "2.3 Sequence Classification", "content": "Sequence classification tasks in the medical domain involve assigning a categorical label to an entire sequence of text, rather than to individual tokens. This could involve classifying entire clinical documents or patient notes into categories such as diagnosis, treatment recommendation, or urgency level.\nThe task can be formulated as follows. Given a document d that consists of a sequence of tokens d = (x1,x2,...,xn), the sequence classification function assigns a label y from a set of categories C, expressed by the equation:\ny = f(d)\nwhere y is the category associated with the document d, and f is a function that maps the entire document, taking into account its contextual coherence and thematic structure, to a label in C. LLMs can serve as implementations of the function f.\nMasked LLMs are particularly well-suited for sequence classification tasks due to their inherent structure. Originally, these models predict masked tokens within a sequence. For the purpose of classification, they can be adapted by employing either the output of a [CLS] token (in case of BERT-like models) or by utilizing various pooling functions on the embeddings. In the case of the [CLS] token, a fully connected layer is added on top of the token's output, which is designed to capture the context of the entire sequence. Alternatively, pooling functions such as average pooling or max pooling can be applied to the embeddings of the sequence to aggregate information across the entire input.\nGenerative LLMs can be either simply prompted or fine-tuned in a way that they learn to predict class labels as part of the sequence. This can be done by training the model on inputs where a special token or delimiter indicates the end of the input and the start of the output label. Although less common, integrating a linear layer after the final token output in generative models could refine the logits corresponding to class predictions, sharpening the classification boundaries set by the model's generative nature.\nThe array of tasks that fall under sequence classification is vast. The following subsections detail a few of the most prominent applications."}, {"title": "2.3.1 Suicidal Behavior Prediction", "content": "Suicidal behavior prediction tasks predominantly focus on analyzing individuals' social media activities. Dus and Nefedov developed an automated tool for identifying potential self-harm indications in social media posts, treating the task as a binary classification problem [135]. Given an input x, the objective is to predict a binary label y, where:\ny = \\begin{cases}\n1 & \\text{if suicidal behavior is detected,}\\\\\n0 & \\text{otherwise.}\n\\end{cases}\nHere, x represents features extracted from social media posts. The model estimates P(y = 1|x), the probability that the input suggests suicidal behavior, using a fine-tuned ELECTRA model. This model was trained on data from Kaggle's \"Suicide Watch\" dataset [137] and additional social media sources. Their method attained a noteworthy accuracy rate of 93% and an F1 score of 0.93 [135].\nBeyond mere social media post analysis, Levkovich et al. assessed ChatGPT-3.5 and ChatGPT-4's ability to evaluate suicide risk based on perceived burdensomeness and thwarted belongingness. By comparing ChatGPT's assessments to those made by mental health professionals using vignettes, they discovered that ChatGPT-4's"}, {"title": "2.3.2 Modeling Patient Timeline", "content": "The task of modeling patient timelines is multifaceted, involving forecasting future medical events, understanding patient trajectories, and predicting medical outcomes. This endeavor employs deep learning, transformers, and generative models to analyze data from various medical records, both structured and unstructured.\nKraljevic et al. introduced Foresight, a GPT-2-based pipeline developed for modeling biomedical concepts extracted from clinical narratives. This pipeline employs NER and linking tools to transform unstructured text into structured, coded concepts. Utilizing datasets from three hospitals, covering over 800,000 patients, Foresight showed promise in forecasting future medical events. Its effectiveness was validated by clinicians on synthetic patient timelines, highlighting its potential in real-world risk forecasting and clinical research [133].\nAside from LLMs, generative adversarial networks (GAN) have gained popularity, extending beyond their initial image generation domain. Shankar et al. proposed Clinical-GAN, which merges Transformer and GAN methodologies to model patient timelines, focusing on predicting future medical events based on past diagnosis, procedure, and medication codes. Tested on the MIMIC-IV dataset [139], Clinical-GAN outperformed baseline methods in trajectory forecasting and sequential disease prediction [182]. Another study employed GAN for predicting the length of stay in emergency departments. The learning process was done in multiple stages. Initially, an unsupervised training phase used a generator and discriminator to approximate the probability distribution and perform feature discovery and reconstruction. Discriminator was then fine-tuned to optimize its parameters for global optimum. A predictor layer, initially randomly initialized, was added and optimized during fine-tuning, enabling the model to map observations to their lengths of stay. The model was trained on data from the Pediatric Emergency Department in CHRU-Lille and proved the potential of GANs in this field [183].\nMedical outcome prediction can be seen as a subtask of modeling patient timeline and is often scoped to either predict mortality, outcomes of a specific disease, or risk of progression from one disease to another. A recent study by Shoham and Rappoport [134] examined data related to chronic kidney disease, acute and unspecified renal failure, and adult respiratory failure from the MIMIC-IV and eICU-CRD datasets. Using this data, the team generated labeled datasets for disease diagnosis prediction based on patient histories. They introduced a method named Clinical Prediction with Large Language Models (CPLLM) by fine-tuning LLMs (Llama2 and BioMedLM) using medical-specific prompts to help the models understand complex medical concept relationships. Xie et al. used EHR analysis to predict epilepsy seizures, leveraging Bio_ClinicalBERT, RoBERTa, and T5, achieving an F1 score of 0.88 in outcome classification [146].\nA notable approach to predict outcomes of COVID-19 patients was proposed by Henriksson et al. [184]. The authors created a model that combines structured data"}, {"title": "2.3.3 Phenotyping and Medical Coding", "content": "The phenotyping task primarily involves identifying phenotypic abnormalities from a patient's various medical records, which aids in the identification of rare diseases. There exists the Human Phenotype Ontology (HPO) project\u00b9 that systematically categorizes human phenotypes with detailed annotations. Thus, the phenotyping task can be framed as a multi-label classification task where we need to find mapping f: X \u2192 2Y, with X representing the set of patient medical records, where each medical record xi \u2208 X corresponds to the medical data (unstructured text in most cases) from the i-th record, Y denotes the set of all possible phenotype labels from HPO, where each label yj \u2208 Y represents a specific phenotype. The objective of the phenotyping task is to map each medical record xi to a subset of phenotype labels Yi \u2286 Y, such that Yi represents the set of phenotypes exhibited by the patient associated with xi:\n$$Y_i = f(x_i)$$\nTraditionally, the task of phenotyping has relied on named entity recognition, where models similar to BERT have demonstrated proficiency. However, recent studies have started exploring in-context learning and zero-shot learning with contemporary LLMs, yielding promising results [131].\nMedical coding is another multi-label classification task that involves identifying a set of International Classification of Diseases (ICD) codes2 associated with a medical record. This task can be formulated similarly to phenotyping, with the key difference being that Y represents a set of ICD codes rather than phenotype labels. Besides observing trends similar to those in the phenotyping subdomain, it is also noteworthy that there is a shift towards explainable medical coding, as highlighted in [132]."}, {"title": "2.4 Question Answering and Information Extraction", "content": "Question Answering (QA) task can be formulated as finding the answer A from a possible set of answers A, given a question Q and a context C (often a document or set of documents containing information relevant to the question). This can be expressed as:\nA = argmaxa\u2208A P(a | Q, C)\nwhere P(a | Q, C) is the probability of a being the correct answer given the question Q and the context C.\nInformation Extraction (IE) task involves identifying specific pieces of information (entities, relationships, events) within documents. This can be described as a function f that maps a set of documents D to a set of structured attributes S, which includes entities E, relationships R, and other attributes of interest:\nS = f(D) = {E, R, . . .}\nHere, D is the input document, and S represents the structured output containing extracted elements."}, {"title": "2.5 Summarization and Paraphrasing", "content": "Paraphrasing involves rewriting a text T into a new form P, ensuring that P maintains the same meaning as T but utilizes different vocabulary and potentially altered sentence structures. Summarization, on the other hand, entails generating a brief version of a text T that preserves its core information. Abstractive summarization can be considered a specific case of paraphrasing.\nMasked language models are proficient at extractive summarization. They evaluate sentences within a text to determine their relevance and informativeness. By scoring each sentence, these models identify and concatenate the most important sentences to form a coherent summary. In contrast, abstractive summarization and paraphrasing typically employ generative (decoder-based) or sequence-to-sequence (encoder-decoder) models. These models are trained to understand the entire narrative or document and then recreate its essence in a different form.\nSummarization and paraphrasing tools are used in the medical domain for managing extensive documentation and enhancing communication. Summarization helps healthcare professionals quickly grasp essential details from lengthy clinical notes, generate concise abstracts of medical research papers, and craft clear patient"}, {"title": "2.6 Conversation", "content": "The task of conversation, or dialogue generation, can be formulated as follows. Given a dialogue history H = (h1,h2, ..., hn), where each hi represents an utterance in the conversation, the goal is to generate a response R.\nTypically, pre-trained decoder-based large language models are fine-tuned using specialized datasets to develop their conversational capabilities. In the medical domain, conversational applications facilitate interactive communication with patients. For example, conversational AI can be deployed as virtual health assistants that provide initial consultations based on symptoms described by patients. These systems can ask relevant follow-up questions, assess symptoms, and offer preliminary advice or direct patients to seek professional care when necessary. Additionally, these conversational tools can be utilized for patient education, explaining complex medical conditions and treatments in simple language to enhance understanding and compliance. Another notable application is in mental health support, where conversational AI can offer coping strategies and basic support, thereby augmenting traditional therapy sessions."}, {"title": "2.6.1 Chatbots and Health Assistants", "content": "The proficiency of LLMs in generating coherent text and finding patterns in natural language makes them excellent candidates for Conversational Health Agents (CHAs) or chatbots. The impressive capabilities of systems like ChatGPT have sparked researchers' interest in evaluating them as out-of-the-box medical chatbots. These chatbots are capable of holding conversations on medical topics and providing valid, science-based responses, akin to human doctors.\nCung et al. assessed the performance of three commercial systems - ChatGPT, Bing, and Bard in the context of skeletal biology and disorders. The study involved posing 30 questions across three categories, with the responses graded for accuracy by four reviewers. While ChatGPT 4.0 had the highest overall median score, the study revealed that the quality and relevance of responses from all three chatbots varied widely, presenting issues such as inconsistency and failure to account for patient demographics [163]. Another study explored using ChatGPT for patient-provider communication. A survey of 430 participants found that ChatGPT responses were often indistinguishable from those of healthcare providers, indicating a level of trust in chatbots for answering lower-risk health questions [164].\nDespite the success of chatbots in general and low-risk medical interactions, some studies suggest that chatbots are not yet suitable for high-risk subdomains. For instance, a study focusing on resuscitation advice provided by Bing and Bard chatbots revealed that the responses frequently lacked guideline-consistent instructions and occasionally contained potentially harmful advice. Only a small fraction of responses from Bing (9.5%) and Bard (11.4%) completely met the checklist criteria (P < .05), underscoring the current limitations of LLM-based chatbots in critical healthcare scenarios [187].\nAnother research direction in the field of medical chatbots is the implementation of conversational agents specifically designed for the medical domain. Abbasian et al. proposed a complex LLM-based multimodal framework for CHAs, concentrating on critical thinking, knowledge acquisition, and multi-step problem-solving. This"}, {"title": "2.6.2 Mental Health Bots", "content": "The idea of using a machine as a personal psychologist dates back to at least the 1960s when Weizenbaum proposed a simple rule-based system called ELIZA [190].\nContemporary advancements in mental health chatbots are largely driven by LLMs. Yang et al. investigated the capabilities of current LLMs in automated mental health analysis. Their study involved evaluating LLMs across diverse datasets for tasks such as emotional reasoning and detecting mental health conditions, employing various similarity metrics including BLEU, ROUGE family, BERTScore derivatives, BART-score [191], and human assessments. They discovered that while ChatGPT displays robust in-context learning abilities, it still encounters challenges in emotion-related tasks and requires careful prompt engineering to improve performance to enhance its performance [161].\nSaha et al. introduced a Virtual Assistant for supporting individuals with Major Depressive Disorder, using a dataset called MotiVAte [160]. Their system, based on modified GPT-2 model and reinforced learning, shows promising results in generating empathetic and motivational responses, as evidenced by both automated evaluations based on text similarity and human evaluations based on fluency, adaptability, and degree of motivation. Sharma et al. introduced a dataset for training a GPT-3-based model for generating reframes with controlled linguistic attributes [192]. Deployed on the Mental Health America website, this allowed for a randomized field study to gather findings on human preferences. Another team explored the fine-tuning of open-source LLMs on psychotherapy assistant instructions, using a dataset from Alexander Street Press therapy and counseling sessions. Their results indicated that LLMs fine-tuned on domain-specific instructions surpassed their non-fine-tuned counterparts in psychotherapy tasks, underscoring the significance of professional and context-specific training for these models [162].\nPromising outcomes have been observed through collaborations between humans and AI. A recent study conducted a randomized controlled trial involving human peer supporters, demonstrating that an AI-in-the-loop agent led to a 19.60% increase in conversational empathy in interactions between individuals seeking mental health support and support specialists. This was achieved by providing suggestions for response improvements to peer supporters [193]. This research reveals that human-AI collaboration is a crucial area for potential exploration, particularly in the medical domain."}, {"title": "3 Challenges and Opportunities", "content": "This section explores the challenges and opportunities arising from the integration of large language models in healthcare."}, {"title": "3.0.1 Ethical Issues", "content": "Biases present in training datasets can lead to skewed results, disproportionately affecting certain patient groups [195]. Future efforts must be directed towards developing ethical frameworks and strategies that mitigate biases, guaranteeing equitable applications of LLMs in healthcare.\nIt is essential to ensure patient autonomy, data confidentiality, and protection against breaches. The development of domain-specific, open-source models for on-premises use is a notable advancement in preserving patient privacy and reducing dependency on external entities. Further specialization of models and adoption of computationally efficient solutions can ensure controlled data access and compliance with local policies on data handling."}, {"title": "3.0.2 Datasets", "content": "With new applications of textual AI emerging in areas like medication plan generation, triaging, extracting structured data from medical records, and providing medical consultations, the development of novel, open, and de-identified datasets becomes increasingly necessary. Many existing datasets were created before the advent of LLMs, which may inflate study results and lead to an overestimation of the current models' efficacy. Moreover, access to many existing datasets requires special approvals,"}, {"title": "3.0.3 Factuality Challenges", "content": "The issue of factuality is critical in healthcare. Medical solutions need to have access to up-to-date and proven sources and must also undergo rigorous checks for factual accuracy. There is an urgent need to develop automated metrics for assessing the factual accuracy of medical text generation [196]. Moreover, research exploring the integration of medical LLMs with ontologies, graph attention networks, and other more deterministic models represents another promising direction."}, {"title": "3.0.4 Human-AI Collaboration", "content": "Further research is required to enhance our understanding and optimization of human-AI collaboration in healthcare. This includes exploring how medical professionals can best interact with and leverage AI tools for improved decision-making and patient care, as well as reducing routine work to help prevent burnout. An example of this could be the further exploration of AI-in-the-loop agents, similar to those described in [193]."}, {"title": "3.0.5 Necessity for Empirical Studies", "content": "Empirical research on real-world use-cases of AI in healthcare is essential. Theoretical studies have broadened our understanding, but practical challenges in real healthcare environments, such as hospitals and clinics, are less understood. Research should focus on how AI applications integrate with healthcare systems, their impact on workflows and healthcare professionals, and the long-term effects on patient outcomes, staff efficiency, and costs. Additionally, addressing AI implementation challenges, including data privacy, ethical concerns, and the need for ongoing system training and updates, is vital. This will guide best practices for AI integration, reduce risks, and ensure these technologies effectively enhance patient care and healthcare delivery."}, {"title": "4 Summary and Conclusions", "content": "This study offers a detailed exploration of recent progress in large language models in the medical field, with a special focus on clinical applications. It begins by tracing the evolution of LLMs, covering both general-purpose and domain-specific models, and examines their architectures and medical applications. The paper highlights various tasks these models perform, such as text generation, token classification, and question answering, illustrating their utility with real healthcare scenarios. This demonstrates how LLMs enhance both efficiency and effectiveness in medical practice, particularly through the use of open-source models that support custom solutions and protect privacy.\nRecent advancements in LLM research have introduced more comprehensive and efficient approaches, particularly through multimodal models that amalgamate visual, textual, and potentially audio data. These advancements facilitate holistic AI solutions and are complemented by techniques like parameter-efficient fine-tuning and flash attention, which minimize computational demands. The shift towards generative LLMs with in-context learning capabilities marks a critical evolution, specifically within niche medical areas such as radiology report generation."}]}