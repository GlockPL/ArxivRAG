{"title": "MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation", "authors": ["Seonok Kim"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across natural language processing tasks. However, their application to specialized domains such as medicine and biology requires further optimization to ensure factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a domain-adapted biomedical question-answering model designed to enhance both short-form and long-form queries. By integrating fine-tuning and retrieval-augmented generation (RAG), MedBioLM dynamically incorporates domain-specific knowledge, improving reasoning abilities and factual accuracy. To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA datasets, covering structured multiple-choice assessments and complex clinical reasoning tasks. Fine-tuning significantly improves accuracy on benchmark datasets, while RAG enhances factual consistency. These results highlight the potential of domain-optimized LLMs in advancing biomedical research, medical education, and clinical decision support.", "sections": [{"title": "1. Introduction", "content": "The rapid advancements in large language models (LLMs) have significantly transformed their application in specialized domains such as medicine and biology (Oh et al., 2023; Saab et al., 2024). These models have demonstrated remarkable capabilities in various question-answering (QA) tasks, ranging from structured multiple-choice reasoning to open-ended long-form explanations (Singhal et al., 2023; Luo et al., 2022; Saab et al., 2024; Jeong et al., 2024; Nori et al., 2023; Singhal et al., 2022; Chen et al., 2023). However, achieving high accuracy and reliability in biomedical QA remains a substantial challenge due to the complexity, domain specificity, and factual accuracy requirements of medical knowledge. Unlike general-purpose QA tasks, medical QA demands a higher degree of precision, interpretability, and contextual depth, making it crucial to explore optimization strategies tailored to this field.\nThis study focuses on optimizing LLMs for medical and biological QA tasks by integrating fine-tuning, RAG, and prompt engineering techniques (Figure 2). Fine-tuning (Ouyang et al., 2022) adapts pre-trained LLMs to medical datasets, improving their ability to generate contextually appropriate and factually accurate responses. RAG (Lewis et al., 2020) further enhances performance by allowing models to retrieve external domain-specific knowledge, mitigat-"}, {"title": "2. Related Work", "content": "Optimizing LLMs for Medical and Biological Applications. The application of LLMs in medical and biological domains has demonstrated significant improvements in reasoning-based question answering (McDuff et al., 2023; Singhal et al., 2023). One of the most notable advancements is Med-Gemini Saab et al. (2024), a family of models fine-tuned specifically for medical reasoning. Med-Gemini has achieved state-of-the-art performance on the MedQA benchmark (Saab et al., 2024), surpassing previous models through an uncertainty-guided search strategy. This strategy enables the model to refine its responses based on external information retrieved through web search, ensuring greater factual accuracy and reliability.\nLong-form question answering in medical and biological domains presents unique challenges, requiring models to generate coherent, factually accurate, and contextually rich responses. To address this, both Med-Gemini (Saab et al., 2024) and OLAPH (Jeong et al., 2024) have adopted pairwise evaluation methodologies, allowing human experts to assess and compare generated answers against ground truth references. This approach ensures that models produce responses that align with expert consensus while minimizing factual errors and hallucinations.\nOLAPH, on the other hand, employs a preference-based optimization framework to iteratively refine long-text generation. By constructing synthetic preference sets and training on preferred responses, OLAPH enhances factual consistency and linguistic fluency. In parallel, models such as BioGPT (Zhang et al., 2024) and Flan-PaLM (Singhal et al.,"}, {"title": "3. Methodology", "content": "To optimize LLMs for biomedical question-answering tasks, this study integrates fine-tuning, RAG, and prompt engineering (Figure 2). Each component plays a crucial role in enhancing different aspects of model performance: fine-tuning aligns the model with domain-specific knowledge, RAG dynamically retrieves relevant biomedical information to improve factual accuracy, and prompt engineering refines response generation based on task-specific requirements. Experiments were conducted in the Azure cloud environment, leveraging its scalable computing infrastructure to efficiently fine-tune models, and optimize inference performance (Microsoft Azure, 2024b;a)."}, {"title": "3.1. Datasets for QA Tasks in Medicine and Biology", "content": "To evaluate model performance in medical and biological question-answering tasks, multiple datasets covering closed-ended, long-form, and short-form QA formats were utilized. Detailed descriptions of the datasets utilized for fine-tuning can be found in Table 5.\nClose-ended question-answering datasets focus on multiple-"}, {"title": "3.2. Fine-tuning LLMs", "content": "To enhance LLMs for specialized medical question-answering, fine-tuning was performed using domain-specific datasets. The process involved supervised learning with labeled biomedical question-answer pairs, optimizing batch size, epochs, and learning rate for computational efficiency and model improvement. Adaptive optimization was applied, dynamically adjusting hyperparameters based on dataset complexity, while ensuring reproducibility through automatic seed assignment. Fine-tuned models demonstrated superior accuracy in structured reasoning tasks and improved response relevance in free-text generation, highlighting the effectiveness of task-specific adaptation for clinical and research applications. The details of the datasets and training configurations are summarized in Table 5.\nThe default values were used for task parameters, allowing for adaptive optimization during training. The batch size was set to 0.2% of the total training examples. The learning rate was determined based on the original pre-training rate, multiplied by a dynamic scaling factor, typically ranging between 0.5 and 2, ensuring an optimal balance between convergence speed and generalization. The number of training epochs was dynamically adjusted based on dataset size and complexity, allowing for effective learning without manual tuning. Additionally, the seed for randomization was automatically assigned to maintain reproducibility across training runs, ensuring consistency in fine-tuning results.\nBy leveraging fine-tuning, the model demonstrated improved performance in domain-specific evaluations, achieving higher accuracy in structured reasoning tasks and more relevant responses in free-text generation."}, {"title": "3.3. Retrieval-Augmented Generation", "content": "RAG was integrated to enhance medical question-answering by structuring and optimizing search efficiency. The overall RAG pipeline, including query encoding, document retrieval, and answer generation, is illustrated in Figure 3. This diagram highlights how queries (T1, T2,..., Tn) are processed, relevant knowledge chunks (K1, K2,..., Kn) are retrieved, and the final response is generated by integrating retrieved data.\nTo achieve this, a robust indexing framework was designed to systematically store and retrieve medical queries and their corresponding answers. The system was configured"}, {"title": "3.4. Prompting Strategies and Tuning Parameters", "content": "Prompt engineering strategies have been expolored in medical and biomedical domains (Nori et al., 2023). This study evaluates three QA setups-closed-ended, short-form, and long-form by modifying system prompts and decoding parameters. Closed-ended QA enforces strict output constraints with predefined options, using low temperature (0.1), top-p (0.7), and a frequency penalty (0.5) to ensure deterministic, high-confidence selections while preventing extraneous text generation. Long-form QA prioritizes detailed, structured responses for complex medical queries, increasing max tokens to 300 while maintaining temperature (0.2) and top-p (0.8) for scientifically accurate and coherent explanations. Unlike other setups, no penalties are applied, allowing for comprehensive literature-based reasoning and clinical insights. Short-form QA balances precision and informativeness, allowing up to 50 tokens with moderate temperature (0.2) and top-p (0.85) for concise yet accurate responses. No explicit stop sequence is enforced, and penalties are minimized to prevent unnecessary repetition or hallucination. These decoding strategies demonstrate that optimizing generation parameters based on answer format enhances medical QA effectiveness. Future work may explore dynamic prompt tuning techniques to further adapt the model's responses based on real-world medical contexts."}, {"title": "3.5. Evaluation Metrics", "content": "For closed-domain question answering with reasoning, accuracy was the primary evaluation metric. When it comes to long-form and short-form question answering, where responses are more open-ended, a set of text generation evaluation metrics was employed. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) was used to measure the lexical overlap between generated and reference responses. Specifically, ROUGE-1 captures unigram overlap, ROUGE-2 captures bigram overlap, and ROUGE-L measures the longest common subsequence (LCS), reflecting fluency and structural similarity. The ROUGE score is calculated as follows:\n$\\frac{\\Sigma_{s \\in Reference} \\Sigma_{w \\in s} \\mathbb{I}(w \\in Generated)}{\\Sigma_{s \\in Reference} \\Sigma_{w \\in s} \\mathbb{I}(w)}$\nwhere w represents an n-gram in the reference or generated text, and $\\mathbb{I}$ is an indicator function.\nIn addition to ROUGE, BLEU (Bilingual Evaluation Under-study) was used to assess precision by measuring the overlap of n-grams between generated and reference responses. It is calculated as:\n$BP \\cdot exp(\\sum_{n=1}^{N} w_n \\log p_n)$\nwhere pn represents the precision of n-grams, wn are weighting factors, and BP is a brevity penalty to account for length mismatches.\nBased on the evaluation methodology presented in OLAPH (Jeong et al., 2024), we employed BERTScore and BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) to evaluate semantic similarity beyond lexical overlap. BERTScore uses contextualized embeddings from a pre-trained transformer model to compute cosine similarity between reference and generated responses. BLEURT, a learned metric incorporating deep learning models, compares responses against human-written references to provide a more nuanced evaluation of response quality. These combined metrics offer a robust framework for assessing generated medical responses in terms of both lexical similarity and semantic relevance."}, {"title": "4. Evaluation", "content": "4.1. Closed Question Answering with Reasoning\nClosed-domain question answering (QA) involves selecting an answer from a predefined set of options, such as multiple-choice formats (A, B, C, D) or Boolean-style questions (Yes, No, Maybe). We evaluate the performance of different models on three biomedical datasets-MedQA (multiple-choice medical board exam questions), PubMedQA (yes/no/maybe biomedical research questions), and"}, {"title": "4.2. Long-form Question Answering", "content": "Long-form question evaluation results (Table 3) demonstrate that fine-tuning generally enhances model performance, though the degree of improvement varies across datasets. In the MedicationQA dataset, the fine-tuned GPT-40 model significantly outperforms the base model across all metrics. ROUGE-1 increases from 19.85 to 24.69, ROUGE-2 from 4.20 to 8.80, and BLEU more than doubles from 0.98 to 2.49. Additionally, BERTScore improves from -7.63 to 8.98, indicating a substantial enhancement in semantic alignment with human-written responses. These findings suggest that fine-tuning is particularly effective for medication-related long-form question-answering tasks.\nIn contrast, the results for the LiveQA dataset present a more complex trend. The fine-tuned GPT-40 model performs slightly worse than the base model in ROUGE-1 (24.12 vs. 26.96) and ROUGE-L (13.31 vs. 13.42), suggesting that fine-tuning may have introduced some degree of overfitting, reducing the model's ability to generalize to unseen test data. However, improvements in ROUGE-2 (6.18 vs. 5.80) and BLEU (1.63 vs. 1.41) indicate enhanced phrase-level fluency. Further analysis is necessary to determine whether this discrepancy is due to the characteristics of the training data or specific nuances in the LiveQA dataset.\nThe fine-tuned MedBioLM model, trained exclusively for long-form question answering across multiple medical datasets (LiveQA, MedicationQA, PubMedQA, and BioASQ), achieves the highest performance across most metrics. It attains the best ROUGE-1 (26.67), BLEU (3.12), and ROUGE-L (18.71) scores, suggesting that training on a diverse range of datasets enhances the model's ability to generalize across different biomedical question-answering tasks. Additionally, BERTScore (12.08) is the highest among all models, indicating stronger alignment with reference answers. BLEURT, while still negative (-30.26), is relatively better than in single-dataset fine-tuned models.\nA major challenge across all models is the consistently negative BLEURT scores, suggesting that none of the models"}, {"title": "4.3. Short-form Question Answering", "content": "The short-form question evaluation results (Table 4) indicate that the fine-tuned GPT-40 model, both with and without RAG, substantially outperforms the base model across all key evaluation metrics. The short-form question evaluation results indicate that the fine-tuned GPT-40 model, with or without RAG, significantly outperforms the base model across all key evaluation metrics. The ROUGE-1 score improves substantially from 4.35 in the base model to 43.17 in the fine-tuned model, demonstrating a notable enhancement in answer relevance and quality. Similarly, BLEU scores rise from 0.28 in the base model to 11.55 in the fine-tuned model, indicating substantial improvement in fluency and syntactic accuracy. Furthermore, BERTScore, which measures semantic alignment with reference answers, shows a dramatic improvement from -20.45 in the base model to 33.43 in the fine-tuned model, highlighting the effectiveness of fine-tuning in enhancing content coherence.\nWhile RAG provides marginal improvements for the base model, it does not bridge the substantial performance gap with the fine-tuned model. For example, the ROUGE-1 score for the Base Model + RAG rises to 6.86, a slight increase from 4.35 in the base model alone, but still far below the 43.17 achieved by the fine-tuned model. These findings suggest that retrieval augmentation alone is insufficient to significantly enhance performance unless combined with fine-tuning.\nBLEURT scores exhibit the most significant performance gap. The base model records a BLEURT score of - 141.61, indicating poor semantic similarity with reference answers. In contrast, the fine-tuned GPT-40 model achieves a BLEURT score of 8.63, demonstrating that fine-tuning significantly improves the model's ability to generate responses that closely align with human-written answers.\nNotably, there is no substantial difference between the fine-tuned GPT-40 model and its RAG-augmented counterpart. ROUGE and BLEU scores remain nearly identical across these configurations, suggesting that RAG does not contribute substantial performance gains once fine-tuning has been applied. This underscores the dominant role of fine-tuning in improving model performance, while RAG provides only minor effects to an already well-trained model.\nMedQA short-form QA results show that increasing top-k (the number of retrieved documents) does not necessarily improve performance (Figure 4). The best results were observed at k=1, where ROUGE-1 reached 11.33 and BLEU was 3.26. As top-k increased, all evaluation metrics declined, with ROUGE-1 dropping to 2.58 at k=5. These findings indicate that retrieving more documents introduces noise and conflicting information, reducing overall answer quality. The model struggles to synthesize relevant content effectively, leading to lower lexical similarity with reference answers. This suggests that for closed-domain QA tasks, retrieval strategies should prioritize precision over recall, and an optimal top-k value should be carefully selected to balance informativeness and accuracy."}, {"title": "5. Conclusion", "content": "In this work, we explore the optimization of LLMs for biomedical QA by integrating fine-tuning, RAG, and prompt engineering. Through experiments using a state-of-the-art LLM as the base model, we developed a domain-specific approach tailored to biomedical QA. Our findings demonstrate that fine-tuning significantly enhances structured reasoning in closed-ended QA tasks, while prompt engineering plays a crucial role in optimizing response clarity and coherence for both short-form and long-form biomedical answers. These results highlight the importance of domain adaptation in improving LLM performance for specialized fields.\nHowever, challenges remain. RAG's impact on factual accuracy was inconsistent, and fine-tuned models risk overfitting to specific datasets. Additionally, evaluation by a single medical professional may introduce bias. Future research should involve multiple domain experts, explore hybrid retrieval techniques, and leverage human-in-the-loop evaluation to enhance model reliability. This study advances biomedical AI, bridging the gap between general-purpose LLMs and real-world medical applications."}, {"title": "A. Experimental Details", "content": ""}]}