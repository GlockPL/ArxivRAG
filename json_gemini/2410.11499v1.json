{"title": "BSM: SMALL BUT POWERFUL BIOLOGICAL SEQUENCE MODEL FOR GENES AND PROTEINS", "authors": ["Weixi Xiang", "Xueting Han", "Xiujuan Chai", "Jing Bai"], "abstract": "Modeling biological sequences such as DNA, RNA, and proteins is crucial for understanding complex processes like gene regulation and protein synthesis. However, most current models either focus on a single type or treat multiple types of data separately, limiting their ability to capture cross-modal relationships. We propose that by learning the relationships between these modalities, the model can enhance its understanding of each type. To address this, we introduce BSM, a small but powerful mixed-modal biological sequence foundation model, trained on three types of data: RefSeq, Gene Related Sequences, and interleaved biological sequences from the web. These datasets capture the genetic flow, gene-protein relationships, and the natural co-occurrence of diverse biological data, respectively. By training on mixed-modal data, BSM significantly enhances learning efficiency and cross-modal representation, outperforming models trained solely on unimodal data. With only 110M parameters, BSM achieves performance comparable to much larger models across both single-modal and mixed-modal tasks, and uniquely demonstrates in-context learning capability for mixed-modal tasks, which is absent in existing models. Further scaling to 270M parameters demonstrates even greater performance gains, highlighting the potential of BSM as a significant advancement in multimodal biological sequence modeling.", "sections": [{"title": "1 INTRODUCTION", "content": "Biological sequences\u2014such as DNA, RNA, and proteins\u2014are fundamental to an organism's functions, as they encode genetic information that determines structure, function, and regulatory mechanisms (Watson & Crick, 1953; Nirenberg & Matthaei, 1961). Understanding these sequences is vital for unraveling the mysteries of biological evolution, deciphering disease mechanisms, and elucidating molecular interactions.\nBy applying machine learning algorithms to large-scale biological sequence data, one can capture evolutionary effects and extract complex patterns in gene transcription and protein translation. This not only enhances our understanding of gene regulation and protein function but also enables the prediction and generation of complex biological functions, significantly advancing our comprehension of biology and life processes (Nguyen et al., 2024a).\nDespite the rapid advancements in modeling biological sequences with machine learning, current efforts have primarily focused on creating unimodal models specialized for DNA, such as DNABert2 (Zhou et al., 2023), HyenaDNA (Nguyen et al., 2024b), Caduceus (Schiff et al., 2024), NT (Dalla-Torre et al., 2023); RNA, including RNA-FM (Chen et al., 2022); or proteins, like ESM2 (Lin et al., 2023), ProTrans (Ahmed et al., 2020), ProGen2 (Nijkamp et al., 2023). However, complex biological processes such as gene regulation, CRISPR immunity, and genetic transposition involve interactions across multiple modalities.\nRecently, several methods have focused on developing models capable of handling both gene and protein data. For example, Evo (Nguyen et al., 2024a) is a 7B genomic foundation model pretrained on DNA sequences, which inherently contain the potential to express other modalities. It learns"}, {"title": "2 METHODS", "content": ""}, {"title": "2.1 ARCHITECTURE", "content": "BSM employs a single-nucleotide tokenizer with a vocabulary that includes nucleotides, amino acids, and special tokens. It uses an autoregressive architecture to model biological sequences such as genes and proteins. By learning next-token prediction, the model reasons over sequences causally and captures statistical patterns and dependencies in the training data, enabling effective representation and generation of biological sequences. Furthermore, the autoregressive architecture's sequential nature effectively handles long-range dependencies, which is crucial in biological sequences like DNA, RNA, and proteins, where long-context information can reveal critical functional relationships or structural interactions.\nThe BSM family includes models of two sizes, specifically BSM-110M and BSM-270M. BSM-110M is a decoder-only Transformer with 12 layers, each having 12 attention heads and a hidden dimension of 768, and BSM-270M features 20 layers, 16 attention heads, and a hidden dimension of 896. Both models utilize rotary position embedding (RoPE) (Su et al., 2024) with a base frequency hyperparameter of 100,000 and support a context length of 1024 tokens. To accelerate training, we employ flash-attention mechanisms (Dao, 2023)."}, {"title": "2.2 PRETRAINING DATA", "content": "High-quality biological data play a key role in developing effective models for biological sequences. In addition to using unimodal protein and gene data, we incorporate three types of mixed-modal data"}, {"title": "2.3 PRETRAINING PROCEDURE", "content": "Three-round Training We pretrain BSM models from scratch in an end-to-end manner, which includes a three-round training process, as shown in Figure 2. It begins by establishing a foundational understanding of individual types of biological sequences (DNA, RNA, or proteins) using 100B single-modal tokens. The model then advances to incorporate multi-modal data, enhancing its ability"}, {"title": "3 EXPERIMENTS", "content": "We evaluate BSM's capabilities in understanding and generating biological sequences across a variety of tasks, including both mixed-modal and single-modal tasks. BSM demonstrates outstanding performance on multi-modal tasks, even surpassing many billion-scale models. We also assess BSM's in-context learning (ICL) ability in a few-shot setting for mixed-modal tasks, demonstrating that BSM possesses this capability, which has not yet been observed in other biological sequence models. Additionally, we investigate the model's supervised fine-tuning (SFT) and zero-shot performance on protein and gene-related tasks. Scaling experiments confirm that further increasing the model size continues to enhance its performance. We conduct an ablation study on the performance of models from different rounds to verify the value of multi-modal data. Finally, we also evaluate the model's generative abilities using the perplexity metric. Details of evaluation datasets are listed in the Appendix C.\nImplementation Details For tasks requiring SFT, we fine-tune the model using a learning rate of 1e-6 and a batch size of 16. For tasks that require two sequences as input, other baseline models lack the ability to simultaneously process both sequences, especially when it comes to handling gene and protein pairs. Instead, they use a dual-tower structure, employing two independent encoders to encode each sequence separately. In contrast, BSM directly connects the two sequences as input using a <sep> token, allowing the model to evaluate their relationship directly in a unified context. Implementation details are listed in the Appendix A."}, {"title": "3.1 MIXED-MODAL MODELING & FEW-SHOT EVALUATION", "content": "As shown in Figure 3, in mixed-modal tasks, such as RNA-protein interactions, BSM outperforms larger models like LucaOne. In the Central Dogma task, which focuses on DNA-protein associations, BSM achieves performance comparable to LucaOne. Additionally, in the few-shot learning setting without fine-tuning, BSM achieves performance close to SFT. Notably, BSM is the only existing biological sequence model capable of few-shot learning on mixed-modal data. These results highlight BSM's ability to efficiently process and analyze mixed-modal biological sequence data, positioning it as a leading model in the field despite its smaller size.\nncRNA-Protein Interactions (ncRPI) The ncRPI (Han & Zhang, 2023) task is a binary classification task aimed at predicting interactions between various non-coding RNAs (ncRNAs) and proteins, which is crucial for understanding cellular functions. Both BSM-110M and BSM-270M surpass the"}, {"title": "3.2 PROTEIN MODELING EVALUATION", "content": "We evaluate BSM's capabilities on four protein tasks, with results shown in Figure 4. Notably, we surpass all baseline models in both the PPI and ProtLoc tasks, achieving the best results. In the ProtStab task, we obtain results comparable to LucaOne. Additionally, in the zero-shot protein fitness prediction task, we achieve performance similar to Evo-7B and Progen2-large. These results highlight BSM's capability in modeling protein sequences through a deep understanding of protein functions and activities, despite its smaller size.\nProtein-Protein Interaction (PPI) The PPI task is pivotal for mapping out how proteins interact within biological systems. We use the DeepPPI (Sun et al., 2017) database that contains human protein interactions for binary classification. The models are fine-tuned on this dataset, and their performances are assessed based on prediction accuracy. Both BSM-110M and BSM-270M surpass protein-specific models like DeepPPI and ESM2-3B, as well as multimodal biological sequence models like LucaOne."}, {"title": "3.3 GENE MODELING EVALUATION", "content": "We evaluate BSM's capabilities on several critical genomic challenges, with results shown in Figure 5. BSM outperformed Evo 7B in the zero-shot ncRNA fitness prediction task, leveraging its under-standing of genomic sequences to predict the effects of mutations on ncRNA functionality without task-specific fine-tuning. It also performed well in the ncRNAFam multi-class classification task. These collective achievements underscore the model's comprehensive strength in genomic analysis and its potential to contribute significantly to molecular biology research."}, {"title": "3.4 SCALING UP BSM", "content": "To unlock the full potential of BSM, we investigate its scaling properties by increasing its parameters from 110M to 270M. As shown in Figure 6, BSM-270M exhibits lower validation loss across all three rounds of training, demonstrating a significant improvement compared to BSM-110M. Experiments across diverse biological tasks also indicate that a larger model enhances performance. This scaling shows that increasing model size can further enhance BSM's capabilities, underscoring"}, {"title": "3.5 ABLATION STUDY ON MIXED-MODAL DATA", "content": "As shown in Table 1a, we validate the value of three types of mixed-modal data by comparing the performance of models across different rounds. We conduct experiments on the ncRPI and PPI tasks, and the results demonstrate that Round 2, by incorporating RefSeq and Gene Related Sequence data, enhances performance on both tasks. In Round 3, the introduction of interleaved mixed-modal data from the web, even with limited data (33M tokens), further improves the model's performance."}, {"title": "3.6 PERPLEXITY EVALUATION", "content": "Perplexity (PPL) is one of the most common metrics for evaluating the generation capabilities of language models. It is defined as the exponentiated average negative log-likelihood of a sequence, reflecting how well a model can predict the next word based on the preceding context. A lower perplexity score indicates a better ability of the model to accurately predict the next word. We evaluate BSM's generation capability using perplexity on our validation protein data. As illustrated in Table 1b, BSM outperforms the larger model ProGPT2 700M, although it doesn't surpass Progen2 2.7B. This demonstrates that using mixed-modal data for pretraining allows smaller models to effectively model and generate protein sequences."}, {"title": "4 RELATED WORK", "content": ""}, {"title": "4.1 BIOLOGICAL SEQUENCE MODEL", "content": "Biological sequences are the cornerstone of molecular biology, encapsulating the genetic information that governs life processes. Traditional approaches to modeling these sequences often focus on modality-specific models, each tailored to a particular type of sequence\u2014DNA, RNA, or protein. For instance, DNABert2 (Zhou et al., 2023), HyenaDNA (Nguyen et al., 2024b), and Caduceus (Schiff et al., 2024) are designed for DNA sequences, while RNA-FM specializes in RNA. Models like ESM2 (Lin et al., 2023), ProGPT2 (Ferruz et al., 2022), and ProGen2 (Nijkamp et al., 2023) are specifically developed for protein sequences. While these models have made significant progress in their respective areas, they are limited in their ability to capture the complex relationships between different biological data types.\nRecently, new methods have emerged that develop models capable of handling both gene and protein data, such as LucaOne (He et al., 2024) and Evo (Nguyen et al., 2024a). These advancements demonstrate the potential of large-scale models to manage complex multi-modal biological data. For example, Evo suggests that genes can express other modalities, learning from large genomic regions to capture system-wide interactions. LucaOne integrates single-modality gene and protein data with extensive annotations through multi-task learning. However, these models often require significant computational resources. In contrast, our approach operates without task-specific supervision, explicitly leveraging mixed-modal data to improve efficiency and endow smaller models with cross-modal capabilities."}, {"title": "4.2 SMALL LANGUAGE MODEL", "content": "The emergence of Small Language Models (SLMs) has gained significant attention as promising alternatives to resource-intensive Large Language Models (LLMs) (Minaee et al., 2024). Capable small LMs are faster to run and easier to deploy, making them attractive for various applications. Their success can be attributed to strategic training choices, such as the emphasis on high-quality data in models like Phi (Gunasekar et al., 2023), the introduction of weight reuse in Phi-2 (Javaheripi et al., 2023) to scale capabilities, and the use of knowledge distillation techniques in Gemma 2 (Team et al., 2024) to enhance performance. Furthermore, SLMs exhibit significant potential for improvement, as ongoing enhancements in model performance\u2014such as those observed in the Phi series suggest that these models remain under-trained and present ample opportunities for further development. Inspired by these works, we investigate the use of high-quality mixed-modal biological data for pretraining our BSM model, which optimizes performance within a smaller model to effectively process DNA, RNA, and protein data."}, {"title": "5 CONCLUSION", "content": "In this study, we have demonstrated that high-quality mixed-modal biological data is essential for enhancing both cross-modal and single-modal learning capabilities in our BSM models. The results indicate that protein-gene interleaving data has considerable potential to improve model performance, highlighting the importance of data quality in training effective biological models.\nHowever, our research has certain limitations. We utilized only a partial dataset from RefSeq and Gene Related Sequence data, which lacks exploration of other valuable types of cross-modal data, such as gene-protein interactions data. Additionally, we mined only a relatively small dataset of interleaved biological sequences from the web, it still yielded continuous improvements in model performance. This suggests that there is substantial room for further investigation, and we believe that leveraging larger and more diverse datasets could enhance our model's capabilities even further. Our future work will focus on exploring additional types of cross-modal data to fully realize the potential of mixed-modal approaches in biological sequence modeling and contribute to advancements in the field."}]}