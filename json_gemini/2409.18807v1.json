{"title": "LLM With Tools: A Survey", "authors": ["Zhuocheng Shen"], "abstract": "The integration of tools in augmenting large language models (LLMs) presents a novel approach toward enhancing the efficiency and accuracy of these models in handling specific, complex tasks. This paper delves into the methodology, challenges, and developments in the realm of teaching LLMs to use external tools, thereby pushing the boundaries of their capabilities beyond pre-existing knowledge bases. We introduce a standardized paradigm for tool integration guided by a series of functions that map user instructions to actionable plans and their execution, emphasizing the significance of understanding user intent, tool selection, and dynamic plan adjustment. Our exploration reveals the various challenges encountered, such as tool invocation timing, selection accuracy, and the need for robust reasoning processes. In addressing these challenges, we investigate techniques within the context of fine-tuning and in-context learning paradigms, highlighting innovative approaches to ensure diversity, augment datasets, and improve generalization. Furthermore, we investigate a perspective on enabling LLMs to not only utilize but also autonomously create tools, which may redefine their role from mere tool users to tool creators. Finally, we reproduced Chameleon's results on ScienceQA and analyzed the code structure.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of artificial intelligence technology, large language models have become an indispensable part of our lives. The large models represented by ChatGPT [1] and GPT-4 [2] of OpenAI, with their excellent natural language understanding and reasoning abilities, can perform diverse tasks, greatly enriching the boundaries of human-computer interaction. However, these large language models still face many challenges in their application in specific fields.\nAlthough large models have shown strong capabilities in a wide range of tasks, their performance is often unsatisfactory in certain professional fields, especially in scenarios that require precision [3] and real-time performance [4]. This is mainly because large models are essentially probabilistic models, and their training data often cannot cover all real-time, specific information. Therefore, when dealing with problems involving complex mathematical operations or the latest knowledge, large models may experience hallucinations, where the output content or answers deviate from the actual situation. In order to overcome this challenge, researchers have begun to explore new solutions. From the history of human development, we can draw inspiration that tools are the crystallization of human wisdom and an important extension of human abilities. By using tools, we can more effectively complete tasks, improve productivity and efficiency. This idea also applies to large language models. By endowing large models with the ability to use tools, we can make them more accurate and efficient in handling specific tasks.\nSpecifically, the use of tools for teaching large models needs to follow a holistic framework. [5] This framework starts with user instructions and requires the model to develop and execute executable plans related to the tool. To achieve this goal, the model first needs to have the ability to understand user intentions. This requires the model to accurately capture the real needs behind user instructions, thereby ensuring the pertinence and effectiveness of subsequent plans.\nSecondly, the model needs to understand the functionality and usage methods of the tool. This includes understanding the basic operations, applicable scenarios, and limitations of the tool. By deeply understanding the tools, the model can more accurately determine when, where, and how to use them, thereby ensuring the smooth completion of tasks.\nIn addition, the model also needs to have the ability to decompose complex tasks into subtasks. By breaking down tasks into smaller and easier to handle units, the model can adjust plans more flexibly to adapt to different situations and needs. At the same time, the model also needs to have reasoning ability to dynamically adjust the plan during task execution, ensuring the smooth completion of the task.\nBy integrating the use of large models and auxiliary tools, we can not only improve the accuracy of task execution, but also to some extent expand the functional boundaries of large models, making them better serve humans. Research in this field will not only contribute to the development of artificial intelligence technology, but also have profound impacts on the progress of human society."}, {"title": "II. CHALLENGES", "content": "A. Time to invoke the tool\nIt is crucial to correctly identify when to call tools during the use of large models. External tools should be called when the model itself cannot directly provide accurate answers or solutions, such as in scenarios that require access to real-time data, performing complex calculations within specific domains, or processing special format documents. At the same time, if information queries or processing can be derived from the existing knowledge and logical reasoning of the model, calling external tools may not be necessary, as this may lead to reduced efficiency, increased costs, and even erroneous results. [6], [7] In short, tools should not be called for the sake of calling them.\nB. Tool selection and accuracy\nChoosing the appropriate tools is crucial for ensuring the successful completion of tasks. However, as the number of available tools increases, ensuring the accuracy of each call becomes more difficult. Each tool has its specific advantages and limitations, and there may be performance differences between different tools. In complex reasoning processes, especially when involving non-linear reasoning or multi tool links, maintaining call accuracy becomes more challenging, as a small error can lead to the failure of the entire reasoning process. [8]\nC. Method of tool call\nProper use of a tool is not only about knowing its existence, but also about understanding how to effectively call it. This involves understanding the tool interface, including the number, type, and value of incoming parameters. Incorrect parameters may cause the call to fail or return incorrect results. Therefore, understanding the API of each tool and how they respond to different types of requests has become very important. [9]\u2013[11] So, how to efficiently operate these tools is crucial for avoiding call errors and optimizing usage processes.\nD. Robustness of reasoning process\nThroughout the entire reasoning process, the accumulation of errors may lead to the amplification of problems, thereby affecting the quality of the results. [12], [13] Therefore, establishing a robust mechanism to detect and correct errors has become crucial. However, this process is full of challenges, especially in complex inference chains or multi-step operations. How to effectively backtrack and correct errors without causing the entire process to crash or be inefficient is a major challenge in the use of large models.\nE. Time efficiency\nTime efficiency is a key consideration in the use of large models. As the reasoning and tool invocation process becomes more complex, the time cost of the entire pipeline also increases. In addition, some tools may have delays, especially when it comes to network requests or processing large amounts of data. Therefore, how to optimize time efficiency and improve parallelism while ensuring the quality of task completion has become an important challenge when using large models.\nF. Generalization ability\nThe implementation of General Artificial Intelligence (AGI) is one of the ultimate goals in the field of artificial intelligence. However, whether different tools can be called according to different scenarios to solve complex reasoning problems is still an open question. At present, technologies such as large language models are rapidly developing in this direction, but achieving complete AGI still faces many challenges. One of the key factors determining whether AGI can be achieved is whether the model has sufficient generalization ability to create or innovate based on existing tools for unfamiliar tools."}, {"title": "III. RESEARCH STATUS", "content": "The current research trend has shifted towards teaching large models how to properly utilize tools, while recent studies have begun to explore allowing large models to develop their own tools to more effectively solve problems. [14] In order to thoroughly understand the various stages of using or developing tools for large models, we introduce a standardized method for using external tools for large models. This paradigm provides a macro perspective that allows us to gain insight into the actual use of external tools in large-scale models.\nA. Paradigm of tool use\nGiven a set of user instructions $U$, a toolset $T$, and an environment state space $E$, we can define:\n\u2022 A function $f_{intent}: U \\rightarrow I$ that maps a user instruction to a user intent.\n\u2022 A function $f_{plan} : I \\times T \\rightarrow P$ that generates a tool usage plan based on the user intent and the available toolset.\n\u2022 A function $f_{exec}: P \\times E \\rightarrow E$ that executes a plan and updates the environment state.\n\u2022 A function $f_{feedback}: E \\rightarrow R$ that creates feedback results from the environment state.\n\u2022 A function $f_{perceive} : R \\rightarrow S$ that processes feedback and generates a summary.\n\u2022 A function $f_{adjust}: S\\times P \\rightarrow P$ that adjusts the plan based on the summary.\nThe tool usage process can be modeled as follows:\n1) User issues an instruction $u \\in U$.\n2) System identifies user intent $i = f_{intent}(u)$.\n3) System generates a plan $p = f_{plan}(i, T)$.\n4) System executes the plan in environment $e \\in E$, resulting in a new state $e' = f_{exec}(p, e)$.\n5) System generates feedback $r = f_{feedback}(e')$.\n6) System processes feedback and generates a summary $s = f_{perceive}(r)$.\n7) System adjusts the plan based on the summary $p' = f_{adjust}(s, p)$.\nThis process iterates until the task is completed.\nIn the exploration of utilizing external tools within large-scale models, a fundamental step involves the selection of an appropriate tool for a given task. This process is predicated on three prevalent approaches: Firstly, injecting knowledge into the large model through the method of fine-tuning. This approach enables the model to assimilate specific expertise related to the tool and its application. Secondly, leveraging the in-context learning capabilities of the model, which allows it to infer how to utilize the tool based on provided examples or hints within its current context. Lastly, an innovative strategy involves enabling the model itself to generate tools tailored"}, {"title": "B. Fine tuning", "content": "To enhance our understanding of fine-tuning practices, it is imperative to devise a special form of dataset, denoted as $C^*$. For the purposes of demystifying the fine-tuning procedure and providing a foundational concept of the dataset structure, our approach includes a simplification of both the dataset and the fine-tuning process. [15] The aforementioned dataset is comprised of a sequence of samples ranging from $x_1$ to $x_j$. Each sample adheres to the following configuration:\n$X^* = W_{1:i-1}; E(c_i; R_i); W_{i:n}$ (1)\n$E(c; r) = <TOOL> a_c(i_c) ! r </TOOL>$ (2)\nHerein, $E(c; r)$ represents a unique syntactical construct employed to delineate the invocation of tools. Within this context, <TOOL>and </TOOL>are utilized as boundary markers signalling the initiation and conclusion of a tool invocation, respectively. Meanwhile, \"!\" functions as a delineator, distinguishing between the invocation command and the resulting outcome of the tool's operation. Concretely, $a_c$ denotes the designated tool name for invocation, $i_c$ refers to the required input for said tool, and $r$ encapsulates the end result post-tool execution.\nIt is noteworthy that $W_{1:i-1}$ and $W_{i:n}$ are not inherently part of the tool invocation; rather, they are a sequence of words generated by a large-scale model drawing upon its extensive internal knowledge base. These words, in conjunction with the details of the tool invocation, collectively constitute an exhaustive sample.\nUtilizing dataset $C^*$, one can engage in training a comprehensive, unaltered large-scale model, $M$. Throughout the fine-tuning phase, the language is employed in a standardized format to structure the objective function. In the course of assimilating knowledge from $C^*$, the model is incrementally equipped to make informed decisions regarding the timing, placement, and methodology of specific tool utilization, influenced by the context and internal feedback mechanisms.\nIn particular instances, as the model navigates an input sequence and identifies a marker akin to <TOOL>, it recognizes the necessity to summon an external tool. Subsequently, the model deciphers the name, input, and anticipated outcome associated with the tool, formulating a request directed towards the external tools accordingly. Following the receipt of the execution results from the tool, the model tactfully integrates this data into the continuing output sequence, thereby fulfilling the task of text generation to completion."}, {"title": "C. Fine tuned dataset construction", "content": "To enhance the precision and professional integrity of the content concerning the exploration of fine-tuning technology, we must first elucidate its foundational principle: the adaptive calibration of models employing annotated datasets to elevate their efficacy in designated tasks or instrumental applications. A primary impediment in this endeavor is securing high-caliber datasets for fine-tuning purposes. This manuscript endeavors to examine various methodologies for dataset acquisition and to expound upon their merits and drawbacks comprehensively, thereby providing efficacious strategies for the refinement of extensive models.\nAn initial strategy is to derive datasets from empirical human tool interaction, as trailblazing frameworks such as WebGPT [16] and WebCPM [17] have exemplified. The"}, {"title": "D. Other issues during the fine-tune phase", "content": "When examining the deployment of Large Language Models (LLMs) within the domain of tool learning, a number of critical challenges emerge. Foremost, the discussion on LLMs' capability to assimilate new tools through simulated trial and error encounters a principal issue: ensuring the model's continuous evolution and adaptation to new tools whilst preserving the skills previously acquired. LLMs in the Imaginarium: tool learning through simulated trial and error [20] highlights that traditional fine-tuning techniques frequently result in catastrophic forgetting. To mitigate this, experience replay has been identified as an efficacious approach when incorporating new tools.\nNonetheless, the expandability of LLMs as the repertoire of tools broadens warrants attention. Specifically, initial methodologies, such as representing each tool with a distinct token and learning its embedding as suggested by Toolkengpt [24] and similar models, offer convenience. Yet, as the tool inventory expands, the effectiveness of such approaches may diminish due to the increased model complexity and potential for performance decline. The challenge, therefore, lies in devising a strategy that not only handles a vast array of tools efficiently but also sustains model performance.\nMoreover, several studies focus merely on the model's ability to invoke the correct tool API by name, overlooking critical evaluation metrics like the accuracy, number, and variety of input parameters. [11] In practical scenarios, these parameters are crucial for the correct functioning of tool applications. Consequently, future research should aim for a more comprehensive and in-depth assessment of model performance upon the integration of new tools.\nLastly, the potential of LLMs to refine their capabilities based on user feedback is an area worth exploring. In real-world settings, user feedback is a pivotal factor for ongoing model refinement. Models such as WebGPT [16] utilize reinforcement learning techniques to adjust and optimize outputs based on user inputs, enhancing model adaptability and user satisfaction. Integrating user feedback into model training processes is thus imperative for elevating LLM performance in tool learning applications."}, {"title": "E. In context learning without retrieval", "content": "The advancement of in-context learning in large language model (LLM) utilization represents significant strides in intelligent systems' capabilities to comprehend and deploy external tools. Initially, we examine the in-context learning phase involving select tools. Here, the model leverages tool instruction documents or sample tasks as prompts, engaging in few-shot learning. Notable examples of such models include Hugginggpt [25] and Chameleon [26]. These approaches are esteemed for their plug-and-play functionality, diminishing the learning costs and time required for users to interact with the model.\nAs model complexities and task intricacies escalate, Hugginggpt [25] underscores a heightened demand for extensive and high-caliber task demonstrations, deeming them crucial for in-context learning effectiveness. This raises the inquiry: Is there a definitive writing methodology or a template to guide the composition of effective task demonstrations? This inquiry pertains not solely to the form and substance of the prompts but also to the integration of divergent tools and methodologies.\nTo address this issue, the MultiTool-CoT [27] method introduces a novel tactic. This technique utilizes chain-of-thought (CoT) prompting [28], enabling LLMs to invoke sequential"}, {"title": "F. In context learning with retrieval", "content": "There are indeed several challenges associated with the integration and utilization of a multitude of tools. The foremost concern is the limitation concerning the length of context. As the assortment of tools expands, it becomes unavoidable for the context to grow verbose, impeding the process of effectively procuring essential information. In addition, there is the question of whether language models possess the capability to accurately strategize and select the pertinent tools amidst a plethora. In reaction to these predicaments, researchers have proposed in context learning with retrieval.\"\u201d\nWhat, then, does retrieval entail? Here, retrieval is defined as the search conducted based on specific tasks or tools. For instance, Chatcot [12] utilizes the tasks retrieved as exemplars within context learning to aid in the decomposition of tasks by language models (LLMs), such as TaskMatrix.AI [29], which emphasizes tool retrieval itself, permitting LLMs to initially devise a plan, thereafter retrieve specific tools through search, and subsequently integrate prompts to ensure the model aptly invokes the suitable tools.\nCurrent scholarly discourse introduces two distinct methodologies for retrieval: The first methodology advocates for modularity, exemplified by TaskMatrix.AI [29] and Restgpt [13], which categorize tools, initially searching for appropriate categories, followed by the LLM selecting specific tools for invocation. Alternatively, the dense retrieval model operates through the adjustment of embedding vectors, as seen in TaskMatrix.AI [29], Chatcot [12], and Gorilla [30], among others. These adopt text embedding to find the response that semantically resonates with the query.\nMoreover, Tool Documentation Enabling Zero Shot Tool Usage with Large Language Models [31] indicates that in scenarios involving the usage of tools on a large scale, marrying retrieval mechanisms with tool documentation can surpass performance solely dependent on demonstrations. Concurrently, Gorilla's inquiry underscores the pivotal role of the retriever's selection on the outcomes.\nWhat benefits does this approach offer? Primarily, it accommodates the employment of an extensive array of APIs, with research from ToolLLM [8] evidencing commendable transferability of this method. Secondly, through the retrieval of API documentation, LLMs' context learning faculties can be wholly harnessed to accurately invoke APIs. This approach not only circumvents errors, for instance, inaccuracies in parameter numbers and other hallucinations but also enables the content of API documentation to be updated as necessary, swiftly adapting to increments.\nRegarding the API documentation itself, a review of extant research suggests that an efficacious document should encompass the following components:\n\u2022 API Name: The API name provides an abstract of the API. The name should be clear and precise in natural language and avoid ambiguity with other API names.\n\u2022 Parameter List: The parameter list for an API includes the input parameters and return value, and each parameter has a parameter name, parameter description, data type, and default value.\n\u2022 API Description : The API description contains more information about what the API does, how it works, what are its inputs and outputs, and any potential errors or exceptions that may be raised.\n\u2022 Usage Example: Providing usage examples for the Api."}, {"title": "G. Other issues during the in context learning phase", "content": "In the discussion of in context learning, we confront a significant challenge that merits attention. When a large language model (LLM) is preliminarily configured with predefined action paths, such configurations often display inherent static qualities, making it challenging for these models to flexibly adapt to evolving feedback from their environments. This rigidity can result in delayed responses to unforeseen events and, in some cases, may lead to failure due to a single oversight. Thus, it's crucial to explore strategies to mitigate this limitation.\nA notable example of overcoming this hurdle is seen in the implementation of an online planning mechanism by Chatcot [12]. In this model, every decision is considered within the broader context of the entire dialogue, rather than as an isolated event. This approach allows the model to adjust its future actions in real-time, basing these adjustments on the outcomes of preceding decisions and external feedback. Through online planning, the model achieves a level of flexibility that allows it to adapt to dynamic dialogue environments effectively. Importantly, despite the adoption of online planning, the efficiency of Chatcot remains intact, as evidenced by the minimal increase in token consumption.\nAnother strategy, paralleling the design seen in Assistgpt [32], involves the introduction of a learner module to address execution errors. When the model encounters errors during task execution, the Learner module steps in to identify and attempt to rectify these mistakes. This self-corrective mechanism enhances the model's resilience and adaptability, facilitating more robust performance.\nLastly, the use of a DFSDT algorithm for tree search, as employed by ToolLLM [8], represents a third strategic approach. The DFSDT algorithm allows models to explore decision trees more thoroughly, considering multiple possibilities at each layer. This method empowers the model to systematically evaluate various prospective solutions and select the most suitable one when faced with complex challenges. By incorporating DFSDT, the model benefits from improved decision-making quality and accuracy in complex situations."}, {"title": "H. LLM creates its own tools", "content": "In the previous discussion, we mainly focused on how to utilize existing tools to assist large language models (LLMs) in task processing. However, recent research has proposed an innovative paradigm of allowing large models to autonomously create and use tools, which to some extent goes beyond the traditional role of tool users. Specifically, research represented by Creator [14] and Large language models as tool makers [33] has demonstrated the potential of LLM for tool generation and correct solutions on unknown problems.\nThe core of the Creator [14] framework consists of four closely connected modules. Firstly, The creation stage guides LLM to generate the required tools through fixed demonstrations. This stage focuses on stimulating the creativity of LLM and generating auxiliary tools that match the problem. Next, The decision stage utilizes the demonstration method again to guide LLM in generating code or methods on how to use these newly created tools. This stage fully demonstrates LLM's concrete reasoning ability, which is the ability to make logical reasoning based on specific situations. Subsequently, The execution stage is responsible for executing these created tools, which requires LLM to have the ability to solve specific problems and transform abstract tools into practical operations. Finally, The recognition stage makes necessary modifications and optimizations to tools or decisions to ensure that they can more effectively serve the target task.\nThe Creator [14] framework also proves that tools created by large language models have certain transferability in similar scenarios. However, a detailed answer has not yet been provided on how to effectively reuse these tools. On this issue, Large language models as tool makers [33] proposes a solution to achieve tool reuse by introducing a dispatcher module. The dispatcher module searches and filters out suitable tools from all tool caches generated in the past. However, the module uses a naive context learning method without retrieval function when selecting tools. This means that as the number of tools continues to increase, The context length may become a significant challenge as it directly relates to the length of text that the model can handle. Therefore, when the number of tool caches surges, the accuracy of reusing tools solely based on context learning and whether context length limitations can be guaranteed have become a question worthy of in-depth exploration. At present, there is a lack of systematic research in this field. However, we can draw on the retrieve concept mentioned earlier to seek a solution. We can build a retrieval system that contains rich tool information. When reusing tools is needed, the retrieval system can quickly locate the appropriate tool and provide it to the dispatcher module for selection. This solution not only helps to reduce the limitation of context length, but may also improve the accuracy and efficiency of tool reuse."}, {"title": "I. Other methods", "content": "In the process of exploring automated tool calls to answer specific questions, an effective solution is to select the most appropriate tool to call by comprehensively evaluating the similarity between the question and the potential answer, as well as the description of each tool. Taking Gear [34] as an example, this scheme shows an efficient strategy to integrate"}, {"title": "B. Time optimization of model pipelines", "content": "Although sequential, multi-step methods are a common practice for handling complex queries, they may not be optimal or most suitable for practical use. In some cases, using nested, parallel, or iterative function calls may be more effective. How to design algorithms that can shorten the entire pipeline time without reducing accuracy is also a worthwhile research direction."}, {"title": "C. Model optimization and continuous learning", "content": "In order to adapt to a large number of constantly increasing and evolving tools, large models require continuous learning and optimization. How to achieve plug and play without compromising model performance is an important research direction."}, {"title": "D. Compensate for API errors and prevent error cascading", "content": "API calls to other models or tools may result in incorrect results, and it is also possible that the initial plan was not properly planned. So, LLMs should learn to evaluate the reliability of APIs and summarize and recover from errors, avoiding step-by-step errors is also a worthwhile research direction."}, {"title": "E. Learn to use tools", "content": "How to best teach a large model how to use tools is still an unresolved issue. The accuracy of existing research in finding the most suitable tool is far from satisfactory. Therefore, finding the most relevant tools is also a research direction."}, {"title": "F. Pretrain tool_augmented LLMs", "content": "Existing research is mostly based on fine-tuning pre trained models or tool calls for in context learning. Due to the diversity of tool call cases, we can try to pretrain tools to enhance LLMs."}, {"title": "V. EXPERIMENTS ON CHAMELEON", "content": "A. Experiment settings\nTo access the various GPT models securely, we utilized the Azure OpenAI Service, a trusted and compliant cloud platform. In this experiment, we use GPT-3.5 (gpt-35-turbo-16k-0613). We test on 4241 test examples on ScienceQA by using method CoT and Chameleon.\nB. Experiement results\nThe QA accuracy can be seen from TableV-C and TableV-C, and the reproduced results are basically similar to the results given in the article, with only a decimal point difference. For Chameleon's state transition diagram and the reproduced state diagram, which can been seen in Fig.5 and Fig.6, are slightly different. The call scale diagram, which can be seen in Fig.7, is completely the same."}, {"title": "C. Code structure and overall process of Chameleon", "content": "Chameleon [26] first defines a class solver that contains various modules, such as image_capterer, bing_search, solution_generator, etc. For the overall process, we propose a pseudocode, which is shown in Algorithm 2.\nBelow is a specific explanation of the role of each module.\n\u2022 load_data : loading data\n\u2022 get_question_text : obtaining problem text\n\u2022 predict_modules : predicting the modules to be executed\n\u2022 build_prompt for sg_chameleon : constructing prompt texts in chameleon mode\n\u2022 build_prompt for sg_cot : constructing prompt texts in cot mode\n\u2022 image_capterer : processing images\n\u2022 text_detector: processing text in images\n\u2022 bing_search: calling search engines\n\u2022 solution_generator : generating answers based on pre-dicted results\n\u2022 answer_generator : generating final answers"}]}