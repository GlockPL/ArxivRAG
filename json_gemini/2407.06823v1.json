{"title": "CUE POINT ESTIMATION USING OBJECT DETECTION", "authors": ["Giulia Arg\u00fcello", "Luca A. Lanzend\u00f6rfer", "Roger Wattenhofer"], "abstract": "Cue points indicate possible temporal boundaries in a tran- sition between two pieces of music in DJ mixing and con- stitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a com- puter vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our pro- vided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Un- like previous methods, our approach does not require low- level musical information analysis, while demonstrating increased precision in retrieving cue point positions. More- over, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.", "sections": [{"title": "1. INTRODUCTION", "content": "The skills required by a \"Disc Jockey\" (DJ) are diverse. To record and play live DJ mixes, DJs need to prepare and know their tracks well. An integral part of the track preparation phase is the placement of cue points. Coined by scratch DJs who placed stickers on vinyl records to in- dicate important sections, the functionality of cue points remains unchanged in the digital setting. A cue point may serve as an annotation for musical highlights, suitable mix- ing boundaries, or the general track structure which con- sists of musical phrases. Furthermore, digital cue points allow DJs to quickly loop a track segment or skip back- and forward during a live performance, altering the track structure on the spot. Unfortunately, placing cue points and track preparation is often a cumbersome and time- con- suming process. Similarly to other music information re- trieval (MIR) tasks, such as onset detection or beat track- ing, cue point placement is not straightforward, despite the prominent structural regularity in electronic dance mu- sic (EDM) [1]. For instance, the presence of a prelude shifts the track structure, creating irregularity, and simi- larly, tracks with arbitrary number of additional bars or tempo variations create a significant challenge which needs to be addressed. We therefore ask the question whether cue point estimation can be automated with a learned approach, imitating human cue point placements by training a model on a manually annotated dataset.\nThis work addresses the placement of cue points, one of the first tasks during the preparation phase of a DJ mix. With this goal in mind we present CUE-DETR, a fine-tuned DETR image object detection model trained for cue point estimation on EDM tracks. We show CUE- DETR outperforms previous approaches without requiring detailed and meticulously curated rule sets, which leverage underlying low-level audio information.\nOur contributions can be summarized as follows:\n\u2022 We propose CUE-DETR, an object detection model capable of predicting cue points in EDM tracks. Compared to previous methods, our model achieves higher precision and shows significantly closer alignment with manually placed cue points.\n\u2022 We make our EDM-CUE dataset publicly available, which is 35x larger than the previously available cue point dataset [2]. EDM-CUE contains the metadata for 4,710 EDM tracks, which includes tempo, beat, downbeat, and 21k manually placed cue point anno- tations provided by human experts.\n\u2022 To increase evaluation objectivity, we introduce ad- ditional phrase aligned points to evaluate prediction accuracy. Moreover, we open-source the code and model checkpoints to further the research of DJ- related MIR tasks."}, {"title": "2. RELATED WORK", "content": "Recent years have seen emerging interests in building au- tomated DJ systems where most approaches try to recreate a fully automated DJ pipeline [3\u20139]. Such systems aim to create seamless transitions between two tracks, each fo- cusing on a different subset of challenges in the DJ's task pipeline. Cue points are predominantly addressed in the context of finding suitable mix positions in automatic mix- ing systems [3, 6, 7, 10]. Music structure analysis forms the basis for most cue detection algorithms, as DJ mixes tend to adhere to the underlying high-level track structures [11].\nHigh-novelty regions found through self-similarity [12], for instance, allow the determination of suitable mix sec- tions based on the high-level music structure [6, 13, 14]. Furthering the structural knowledge of a track, crowd- sourced scrubbing data from streaming services uncovers additional structural context, as listeners tend to skip for- ward to the most prominent section of a track [10]. Apply- ing learning-based concepts for the direct search of musi- cal highlights [7] reveals useful information about the mu- sical structure in a similar manner.\nGenerally, the accuracy of algorithmically chosen cue points varies depending on the granularity and complete- ness of the rule set implemented in conjunction with the structural analysis [13]. Adding further rules into the set, for instance, introduces a trade-off between the number of correctly estimated cue point positions and the correct- ness of each estimated cue point [14]. The main focal point of the open-source DJ system Automix [14] is a rule- based cue point estimation algorithm, including a valida- tion dataset containing 145 tracks [2]. Automix imple- ments four empirically chosen rules describing possible lo- cations of \"switch points,\" a subset of cue points, on top of structural analysis. Furthermore, the implementation of Automix depends on underlying MIR tasks, such as beat tracking.\nDJ mix reverse-engineering [15, 16] is a related task to cue point estimation, as it addresses the lack of available and ready-to-use datasets [17]. Such \u201cunmixing\u201d meth- ods extract latent mixing information from recorded DJ mixes, whose retrieval typically relies on manual annota- tions, such as mix-in and mix-out points or volume gain curves. The use of pure DJ mix reverse-engineering for cue point estimation is limited as no novel cue points can be retrieved from existing DJ mixes.\nIn the context of lower-level MIR tasks, convolutional neural networks (CNNs) have been studied, for example, in onset detection [18] or beat tracking [19]. Furthermore, CNNs have proven helpful in musical structural analysis and boundary estimation [20]. Using an attention mech- anism in conjunction with a CNN can help alleviating the challenges posed by the sequential nature of music. Never- theless, adding an attention mechanism does not solve the main concern posed by the large amounts of data required for training. Another possible solution is to instead use a large pre-trained model and to then fine-tune the model on task-specific datasets. The Audio Spectrogram Trans- former [21], for instance, demonstrates the possibility to transfer a pre-trained ViT model [22] from the image do- main to the audio domain. Transformer architectures are often designed to apply the attention mechanism together with a pre-trained CNN backbone, leveraging the feature space previously learned by the CNN [23, 24]."}, {"title": "3. METHODOLOGY", "content": "3.1 Dataset\nWe created EDM-CUE, a dataset containing music meta- data from four private collections of professional DJs.\nEach of the four DJs uses the library management tool rekordbox from which we collect the track name, artist name, tempo, beat grid, and cue points for each contained track. Cue points are given by their absolute position in seconds. The beat grid represents a visual metronome, which can be calculated from its stored values: the tempo and grid offset return the beat positions. Applying the time signature in combination with the initial beat num- ber reveals the downbeat. Since we aggregate tracks from four individual collections, all duplicate tracks need to be merged. We summarize the tempo and grid offset to their respective mean values for all duplicate track entries. In or- der to merge duplicate cue points, we group all cue points based on their distance to neighboring points. Cue points within a distance of a quarter beat of one another form a group. The merged cue point value corresponds to the group center position. All dataset tracks are based on a 4/4 time signature and show constant tempo over time, out-lier tracks were excluded during collection. We then pair the information of each track with the track ID found on Deezer to provide an additional reference.\nOur dataset contains 4,710 EDM tracks consisting of around 380 hours of music. The tempo-range lies between 95 and 190 bpm, and track duration ranges from 1 minute 37 seconds to 10 minutes with an average of 4 minutes and 50 seconds. In total, the dataset contains 21,461 cue point annotations with an average count of 4.6 cue points per track. All tracks used to train the model are compressed to 128 kbps MP3 at 44.1 kHz.\n3.2 Phrasing\nAlthough cue points frequently align with high-level struc- tural boundaries and tend to strongly coincide with phrase boundaries [11], the placement of cue points is a subjective task with no clear definition; therefore, annotations col- lected from DJs may not contain all plausible cue points. We first examine the distribution of our training data for cue point positions quantized to bars. Our training cue points exhibit a periodicity with high occurrences of cue points on multiples of 8 and 16 bars. When also taking the inter-cue spacing between neighboring cue points into account, we observe that a ma- jority of our training tracks adhere to phrase lengths of 16 bars, followed by 8 bars. Due to the strong regularity, we will refer to sections with phrase lengths other than 8 or 16 bars as \"irregular.\" Furthermore, analyzing the cue points in EDM-CUE we find DJs often place cue points at the start of such irregular sections.\nSince regular and clearly defined phrasing is common in EDM [1], we generalize our collected ground-truth data by estimating phrase boundaries $B$. Phrase boundaries serve as an approximation of the track structure which we use to further validate model accuracy. Using track duration $t$, phrase length $l$, and an ordered, ground-truth cue point set $C$, we find $B$. The non-empty set $C$ must include cue points $c_i$ which mark the start point of irregular phrase boundaries. Traversing the section preceding the first cue point $c_0 = b_0$ in increments of $l$ yields the first entries of $B$. When the iteration reaches a negative value, the remaining track section from $c_0$ is traversed in the opposite direction until $b_i \\geq t$. A new boundary $b_i$ is added to $B$ if the itera- tion step did not skip or reach any $c_i$. Otherwise, the next cue $c_i$ is added to $B$ as $b_i$. The two simplified examples show resulting boundaries.\n3.3 Model\nOur proposed cue estimation system is based on DETR [23], a pre-trained object detection transformer. For each track in the dataset, we generate Mel spectrograms using 128 Mel bands at a sampling rate of 22,050 Hz. Our window length measures 2,048 samples, and the hop length is 512 samples.\nThe input of the model consists of 128 \u00d7 355 pixel spec- trogram segments to fit the expected input image format for DETR while also maximizing the duration of the de- picted audio to approximately 11 seconds per image. In the following, we refer to a complete track spectrogram as $S$. The training spectrogram segments $S_T$ and inference spectrogram segments $S_I$ denote the input images of the model. The model returns positional encodings for the pre- dicted bounding boxes alongside the accompanying confi- dence scores and class labels represented by logits.\n3.4 Preprocessing\nWe differentiate between preprocessing for training and inference, as the model is required to process complete spectrograms during inference, whereas for training, the model only requires image segments depicting cue points. A training image segment $S_T$ is cut from $S$ around a cue point $p$ found in $S$. Using a random integer offset $o \\in [0, 355)$, image $S_T$ is defined as the segment with left side $p - o$ and right side $p - o + 355$. If image $S_T$ partly lies outside of spectrogram $S$, the additional space in $S_T$ is zero-padded. The inclusion of image offset $o$ acts as a simple data augmentation strategy. For the training anno- tations, each cue point in an image $S_T$ is encapsulated by a bounding box. The aforementioned box occupies the entire height of $S_T$ and is centered around the cue point. In the event that the box extends beyond the image, it is cropped to align with the image borders. Due to this cropping strat- egy, all training tracks are split into training and validation sets and are indexed by their respective cue annotations.\nTo make predictions over the span of a full track, during inference, the complete spectrogram needs to be shown to the model. We employ a sliding window cropping strategy on spectrogram $S$ with an overlap of 0.75 in order to gen- erate inference image segments $S_I$. Similarly to training, the left side of spectrogram $S$ is zero-padded with an arbi- trary offset 0 \u2208 [89, 266] prior to cropping. Applying the zero-padding approaches the uniform distribution of cue point positions seen in the training data, thus increasing the chance to detect cue points at the very start of a spec- trogram. As the final step, the resulting image sequence is normalized.\n3.5 Postprocessing\nWe implement additional postprocessing for inference only since additional processing of the basic DETR out- put is not necessary during training. The model outputs contain the logits and positional encodings mapping to the predicted bounding box coordinates over images $S_I$. Ap- plying a softmax function to the logits yields the class la- bels and confidence scores for each prediction, cue points are retrieved from the respective positional encodings. The positional box representation is converted to pixel coordi- nates in corner format to find the center point on the x- axis. The resulting point is mapped back to the absolute coordinates of track spectrogram $S$ using the left edge of image segment $S_I$. Once all conversion results for spec- trogram $S$ have been accumulated, the confidence scores are sorted by their associated position, resulting in peaks where the confidence is highest. We implement a peak se- lection strategy using radius $r$; final cue point candidates are selected in descending order based on their predicted confidence score. Candidates within radius $r$ of a previ- ously selected candidate are ignored. We use a confidence"}, {"title": "4. EVALUATION", "content": "The final evaluation is conducted on 101 tracks which were excluded from the training and validation split. This test set contains 607 ground-truth cue point annotations.\n4.1 Experiment Setup\nWe initialize CUE-DETR with pre-trained weights from DETR. The backbone is initialized with the ResNet-50 weights, and we set the backbone learning rate to $10^{-6}$. For the transformer, we choose a learning rate of $10^{-5}$, and set the weight decay to $10^{-4}$. The bounding box width $w$ is set to 21 pixels and the postprocessing radius $r$ is fixed at 16 and 8 bars, referenced as $r_{16}$ and $r_8$, respectively. We train the model using AdamW [25] and schedule a learning rate reduction by factor 10 when the validation loss does not improve for 10 epochs. The final model is trained for 50 epochs on one NVIDIA TITAN Xp GPU with a batch size of 192.\nWhile we experimented with training CUE-DETR us- ing randomly initialized transformer weights, we found using pre-trained weights provided significantly better re- sults. Even though the pre-trained transformer weights were trained on COCO 2017 [23, 26], a distinctly different data distribution compared to Mel spectrograms, we cor- roborate previous findings of visual feature space transfer learning [21, 27].\nWe compare our model with two other methods, namely \u201cMixed In Key 10\u201d (MIK), a commercial DJ software, and Automix [14], an open-source research project. We analyze all tracks directly without manual interference in MIK, as the program simultaneously estimates the beat grid to which it snaps generated cue points. From Au- tomix, we used the cue point generation method directly.\n4.2 Evaluation Metrics\nWe investigate the predicted cue points with respect to the manually annotated cue points and phrase alignment sepa- rately. In the following, we address the manually annotated cue point ground-truth set by cues-only and use the phrase length, measured in bars, to reference phrase alignment. Similarly to Automix, we assess the predictions using a tolerance window around the ground-truth cue points to es- timate the hit rate of the predictions. We evaluate the mod- els on two different tolerance windows $T_1$ and $T_{1/2}$ which measure one beat and one half-beat, respectively. On aver- age, one half-beat in our test data measures approximately 172 milliseconds, which is comparable to the standard 150 milliseconds tolerance in beat tracking [28]. The values for precision, recall, the $F_1$-score and Average Precision (AP) scores are retrieved from the hit rate. Lastly, we measure the cosine similarity between the sets of the predicted and actual cue point positions.\n4.3 Ablations\nAs cue points have no clearly defined object boundaries, we further investigate the influence of the spectrogram context around a cue point included in a bounding box. We report the impact of the bounding box width $w$ for the quality of predictions in Table 1 using AP. We report AP for cues-only as $AP_C$ and report AP for phrase alignment"}, {"title": "4.4 Results", "content": "The evaluation of the mean precision, recall, and the $F_1$- score is summarized in Table 2. For all methods, the preci- sion increases from the cues-only to the 16-bars and 8-bars ground-truth sets. Our $r_{16}$-model achieves the highest pre- cision in all cases. The precision increases most notably for tolerance $T_1$ from the cues-only to phrase alignment ground-truth sets. More precisely, our $r_8$-model shows an increase in precision by 0.31 from cues-only to 8-bar phrasing. The change from 16 to 8-bars is not as preva- lent. Automix shows an improvement in precision from 0.14 to 0.24 and 0.3 over the three ground-truth sets. MIK shows little improvement over the different scenarios and produces more stable precision values. Using the tighter tolerance $T_{1/2}$, all precision values fall in proportion to each other. For recall, the difference of values between the two tolerances is similar to what is observed for precision. With the added phrasing boundaries, all methods show a reduction in recall, opposite to precision. The most sig- nificant drop in recall is observed from 16 to 8-bars. Our $r_8$-model reports the highest recall on all accounts. The changes in the $F_1$-score are less pronounced for all meth- ods as the values remain nearly stable for cues-only and 16-bar phrase alignment. The best reported $F_1$-score is as- sociated with our $r_8$-model over 16-bar phrasing at 0.46. For further insight, we look at the distribution of the predicted results. Automix favors cue posi-"}, {"title": "4.5 Discussion", "content": "CUE-DETR shows strong adherence to ground-truth com- pared to other methods. Our method suggests good phrase alignment based on the distribution of our predicted cue point positions, as well as the increase in precision from cues-only to 16 bar phrases. A slight increase in precision is expected for all methods, however, a significant increase is only associated with strong phrase alignment due to the decrease in false positive predictions. The higher number of possible ground-truth positions decreases recall in re- turn. If our method successfully detects irregular sections, the phrasing algorithm from Section 3.2 can be applied in postprocessing, which could further increase the precision while keeping the recall score high.\nDespite using a metronome-agnostic approach, for which we fixed the distances $r$ to the length of a phrase in terms of the dataset median tempo, the chosen values for $r$ yield results with higher precision compared to the other methods. We assume the relatively homogeneous nature of our dataset minimized the impact of different tempos in the test data. For more diverse styles of music, includ- ing the tempo and beat grid information, similar to MIK, might be beneficial. On the other hand, it might be possible to train a model on beat and cue detection simultaneously.\nThe beat detection could then be used during postprocess- ing to identify the tempo, making the need for additional ground-truth beat grid or tempo information redundant.\nOne key limitation remains in the availability of training data, despite building our own dataset. Since we only had access to data with high similarity in style, we would like to investigate the performance of our method over a broader domain of electronic music in the future. Furthermore, our dataset annotations were provided by DJs who specialize in club DJing. Therefore, annotations from other types of DJs, such as scratch DJs or mobile DJs, would likely result in a largely different cue point distribution. We believe one main difference would lie in more cue points distributed around vocals or pickups instead of the first downbeat of phrases.\n5. CONCLUSION\nIn this work we introduced CUE-DETR, an object detec- tion model fine-tuned on Mel spectrograms capable of es- timating cue points in EDM tracks. Candidate cue points produced by CUE-DETR demonstrate high adherence to the underlying music structure and exhibit a higher resem- blance to manually placed cue points compared to previous approaches. Furthermore, we created EDM-CUE, a dataset containing 21k manually annotated cue points from four professional DJs. EDM-CUE also contains tempo, beat, and downbeat annotations for almost 5k EDM tracks. Our implementation includes a postprocessing step to filter the model predictions for the best positions, including a con- version of the results to timestamps. For the evaluation, we presented a complementary phrasing-based evaluation method, which is useful to assess cue point predictions in a more objective manner.\nFurthermore, we demonstrated that CUE-DETR is ca- pable of detecting large structural boundaries in music, de- spite only seeing small excerpts of the entire track. Our findings further acknowledge the potential of transformer- based architectures for the detection of time-based events in music."}]}