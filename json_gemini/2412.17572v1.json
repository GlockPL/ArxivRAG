{"title": "Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor", "authors": ["Yeonju Kim", "Se Jin Park", "Yong Man Rot"], "abstract": "Chatbot research is advancing with the growing importance of chatbots in fields that require human interactions, such as customer support and mental health care. Despite these advancements, chatbots still face significant challenges in understanding subtle nuances and managing long conversation histories. To address these issues, our study introduces a dual approach: firstly, we employ Emotional Preference Optimization (EPO) to train chatbots not only with correct responses but also with counter-emotional responses-those that are contextually similar but emotionally divergent. This training enables the model to discern fine nuance distinctions between correct and counter-emotional responses, thereby enhancing the quality of its responses. Secondly, we introduce MambaCompressor to effectively compress and manage extensive conversation histories, significantly reducing time and memory complexities while improving the chatbot's contextual understanding. Our comprehensive experiments across multiple datasets demonstrate that our model significantly outperforms existing models in generating empathetic responses and efficiently managing lengthy dialogues.", "sections": [{"title": "1. Introduction", "content": "A chatbot is a computer program that mimics conversation, creating text responses based on users' questions or inputs. Chatbots make it easy for users to get the services they need and have the potential to extend to traditionally human tasks such as counseling. These days chatbots are particularly useful in practical domains where human interaction is important, such as call center systems and mental health diagnostics. With the importance of chatbots, studies have enhanced human interaction by developing training methods that reflect people's preferences using reinforcement learning [28] and integrating audio-visual signals into the chatbot [2, 8, 12] to understand the emotions and intentions of users' word.\nDespite significant advances in chatbot technology, two major challenges hinder the implementation of fully empathetic and context-aware systems. First, current models often struggle to capture subtle nuances of human communication, such as emotional undertones and complex intentions. These limitations prevent the model from imitating real conversations and perform poorly in situations where interactions beyond words must be understood. Second, managing long conversation history remains a challenging task. In the transformer-based language model, the computational complexity increases quadratically according to the input size [37]. Consequently, other models often resort to truncation or summarization for long input sequences, which can lead to loss of information. In addition, long input sequences can disrupt the focus of the attention mechanism, hindering the generation of accurate answers. Likewise, transformer-based chatbots also have a problem of having to put a long conversation history as an input.\nAs a solution to the first issue of failing to capture subtle nuances, we hypothesize that explicitly distinguishing between emotionally appropriate and inappropriate responses will help the model learn these fine nuance distinctions. To this end, we categorize responses as emotionally suitable or unsuitable and use these distinctions to train the model. While traditional chatbots are typically trained only on correct responses, our approach also includes emotionally inappropriate responses to promote a more nuance-sensitive and emotionally attuned output through Emotional Preference Optimization (EPO). EPO encourages the generation of emotionally aligned responses by training with both suitable and unsuitable examples. To implement this, we create our own set of counter-emotional responses, which are emotionally inappropriate and potential responses within opposite emotional contexts. We label emotions to construct emotionally contrasting situations and generate responses that reflect these opposite emotions, calling these \"counter-emotional responses\". By using counter-emotional responses for EPO, the model learns a fine contrast between"}, {"title": "2. Related Works", "content": "2.1. Multi-modal Large Language Model\nWith advancements in Large Language Models (LLMs), Multi-modal Large Language Models (MLLMs) have progressed significantly, allowing LLMs to process multiple modalities. Early research focused on image understanding, leading to architectures like BLIP-2 [19], LLaVA [24], and Qwen-VL [3]. This evolved into models like VideoLLaMA [45], LLaVA-NeXT [23] and VideoChat [20] for video comprehension, and SpeechGPT [44] and Qwen-Audio [7, 9] for audio understanding. Some models, such as AnyGPT [43] and NextGPT [38], have further advanced to integrate three or more modalities, handling diverse inputs\nand generating multi-modal outputs. Although research on MLLMs has been steadily emerging, chatbot research using multi-modality [2, 8, 12] remains relatively underexplored. Our work focuses specifically on human interaction within video chatbot applications.\n2.2. Mamba\nTransformers [37] are able to capture long-range dependencies through self-attention. However, they come with the problem of the quadratic scaling of computation with sequence length, which restricts the maximum input size. To address this limitation, state space model(SSMs) like S4 [15], which maintains linear computational complexity while achieving competitive performance on long-range tasks have been proposed. Following S4, Mamba [14] and Mamba2 [10] implement selective state space mechanisms to provide adaptive, dynamic state transitions to address the limitation of fixed dynamics in the previous SSMs.\nBeyond natural language processing, Mamba has demonstrated remarkable potential in various tasks. Vision Mamba [47] introduces a vision backbone with bidirectional Mamba to enhance image understanding, while VideoMamba [21, 30] utilizes Mamba's capabilities to model long-term dependencies in video. Additionally, Meteor [17] leverages Mamba to effectively compress lengthy input sequences into compressed features. Inspired by Meteor, we use MambaCompressor to condense conversation histories into compact embeddings, preserving essential information while ensuring computational efficiency.\n2.3. Preference Optimization\nReinforcement Learning with Human Feedback (RLHF) [28] has been a prominent approach for aligning language models with human preferences. Typical RLHF training includes a supervised fine-tuning stage, followed by reward model training on preferred data and reinforcement learning optimization. This process comes with notable drawbacks, including the need to train multiple models and the computational cost of repeated sampling from the language model.\nTo address these limitations, Direct Preference Optimization (DPO) [35] was introduced. DPO enables preference-based optimization directly on the language model without a separate reward model, simplifying the training process and reducing computational demands. This method has inspired the development of further preference optimization techniques [39, 40, 42]. One of them, SimPO [27] eliminates the need for a reference model and applying length normalization to reduce the tendency to generate longer but lower-quality sequences. Motivated by SimPO, we guide the language model to generate emotionally appropriate responses while avoiding emotionally inappropriate ones, thereby enhancing response quality in a"}, {"title": "3. Method", "content": "3.1. Audio-Visual Emotion Extractor\nIt is challenging to discern a person's emotions from text alone, so we incorporate audio-visual information to capture emotional cues from the speaker. CLIP [33] vision encoder is used to extract visual features from 8 randomly sampled frames of video, while Whisper [34] is adopted to capture audio features. We extract hidden-state feature from the last layer of CLIP and Whisper. These features are then transformed by the Q-former in each encoder into emotion-specific embeddings. We train the Q-former and learnable queries on the emotion recognition task as illustrated in Fig. 2. With this task, the model aims to accurately recognize emotions by utilizing visual and auditory cues from the audio and video, such as facial expressions, gestures, and voice tone. During training, we employed three different input settings for the emotion recognition task: using only audio, only video, or both. This approach prevents reliance on any single modality, thereby enhancing the effectiveness of emotion recognition.\nUsing the emotional video and audio embeddings, the LLM is guided by a system prompt to predict emotions across 7 categories: angry, disgust, fearful, happy, neutral, sad, and surprise. We utilize both audio and video as we have observed that relying on a single modality often proves insufficient for accurate emotion prediction as shown is Fig. 6. When combined, the two modalities mutually compensate for each other in situations where one alone is insufficient.\n3.2. MambaCompressor\nTraditionally, models either processed truncated segments of conversation history, failing to capture the full context. Alternatively, they handled entire conversation histories, which significantly increased both time and memory complexities due to the lengthy input sequences. To address these issues, we have significantly reduced the size of the conversation history input by employing MambaCompressor. This reduction enhances efficiency from both time and memory perspectives, enabling the model to handle large-scale data more effectively without compromising performance.\nAs depicted in Fig. 3, we insert <MEM> tokens between utterances in the conversation, and each <MEM> token is designed to encapsulate the content from the preceding utterance, essentially transforming a utterance into a compact output feature represented by the <MEM> token. To achieve this, we train the MambaCompressor using a frozen LLM on the task of reconstructing the original utterances solely from the compacted <MEM> token repre-"}, {"title": "sentations.", "content": "First, we train MambaCompressor to reconstruct a single utterance. Once the model has converged, we then finetune it to reconstruct multiple utterances. Through this progressive training process, MambaCompressor ultimately gains the ability to summarize lengthy conversations into few features. Therefore, the model can condense each utterance into a single feature, effectively mitigating the quadratic growth in computational complexity typically associated with processing lengthy conversation histories.\n3.3. Counter-Emotional Response Generation\nTo implement Emotional Preference Optimization (EPO), which will be detailed later in Sec. 3.4, it is essential to prepare counter-emotional responses. These are responses designed for situations with an opposite emotional context. We use the same LLM for generating counter-emotional responses as we do for training, because using a different model would result in responses that are too divergent, making them less effective for learning.\nAs illustrated in Fig. 4, we first label the emotional scenario of the given sentence, which assumes the emotion of the speaker uttering a given sentence. We then create seven distinct sentences, each labeled with one of the following emotions: angry, disgust, fearful, happy, neutral, sad, and surprise. These labeled sentences are then inputted into the LLM, which generates responses that are aware of the assigned emotional scenarios. We prompt the model to generate a response to the given sentence while considering the specific emotion. For example, as illustrated in Fig. 4, in a situation where the statement is made in a happy tone, the response is more positive, such as \"Aww, now you have an exciting new angle!\" In contrast, in a Neutral situation, the response is more monotone, as in \u201cYes, you should've.\"\nAmong the generated responses, we select the most emotionally inappropriate sentence. To achieve this, we calculate the emotional similarity between the ground truth and the generated responses by computing the cosine similarity of embeddings from ROBERTa [25] trained on the GoEmotions dataset [11]. ROBERTa is the model known for its effectiveness in sentence classification, and when trained on the GoEmotions emotion classification dataset, it becomes proficient at identifying the emotional content of sentences. As a result, the features extracted by ROBERTa effectively capture the emotional information of the sentences. The response with the lowest similarity is selected as the counter-emotional response and is used in EPO, as detailed in Sec. 3.4.\n3.4. Emotional Preference Optimization(EPO)\nPreviously, models are trained with a single correct sentence, often failing to capture subtle nuances. To address this, we have trained our model using Emotional Preference"}, {"title": "Optimization (EPO).", "content": "This involves learning from a correct sentence along with a counter-emotional sentence-similar in content but slightly different emotionally-allowing the LLM to discern these subtle nuances more effectively. Emotional Preference Optimization (EPO) treats the LLM itself as a reward model and operates without the need for a reference model, similar to SimPO [27]. The learning process of EPO involves increasing rewards for correct responses and decreasing rewards for counter-emotional responses. In this approach, EPO employs the average log likelihood as the reward metric, as shown in Eq. (1). In Eq. (1), x is input sequence, y is target sequence, $\\pi_{\\theta}$ is LLM model, and $ \\beta $ controls the reward scaling between correct and counter-emotional responses.\n$T_{EPO}(x, y) = \\frac{\\beta}{T} \\sum_{i=1}^{T} log \\pi_{\\theta}(y_i | x, y_{<i}).$ (1)\nBy integrating this reward formulation into the Bradley-Terry (BT) ranking objective [4], we can derive the loss function as shown in Eq. (2). Using this objective, we train the LLM to maximize the reward for correct (appropriate) responses $y_a$, making it higher than that for counter-emotional (inappropriate) responses $y_i$. We incorporate a target reward margin term, $ \\gamma $, which boosts the rewards for appropriate responses by a margin of $ \\gamma $. D indicates that the dataset consists of correct and counter-emotional responses and $ \\sigma$ is sigmoid function.\n$L_{EPO}(\\pi_{\\theta})$\n$= -E_{(x,y_a,y_i) \\sim D} [log \\sigma(r(x, y_a) \u2013 r(x, y_i) - \\gamma)].$ (2)\nWe finetune the pretrained LLM using EPO loss in Eq. (2) and the standard autoregressive loss. This additional EPO training enables the LLM to discern subtle nuances between correct and counter-emotional responses, allowing it to respond more accurately and empathetically."}, {"title": "4. Experiments", "content": "4.1. Datasets\nWe evaluate our method using benchmarks similar to real-world conversations with emotion labels.\nMultiDialog [31] is a large-scale audio-visual dataset containing approximately 370 hours of dialogue, with around 9,000 conversations exchanged between six speaker pairs. The scripts are derived from Topical-Chat [13] and each utterance includes emotion annotations. We use the train split for training and the test-freq split for test.\nMultimodal Emotion Lines Dataset (MELD) [32] is a audio-visual conversation dataset based on conversations from the TV series Friends. This dataset contains approximately 1,433 conversations and 13,000 utterances, each annotated with emotion labels.\nIEMOCAP [5] is a audio-visual conversation dataset. The dataset consists of approximately 12 hours of audio-visual data involving 10 actors, who read emotional scripts or act out impromptu situations. Each utterance is annotated with emotion labels. This dataset does not come with predefined train and test splits, so we performed a random split, allocating 90% of the data for training."}, {"title": "4.2. Implementation Details", "content": "We adopt a pre-trained Whisper tiny model [34] for the audio encoder and pre-trained CLIP-ViT-B/32 [33] for video encoder. Q-formers are transformer encoders with 6 layers and 8 attention heads, utilizing 32 learnable queries to extract emotional features. When we train the model on an emotion recognition task to generate emotion-related features, we add RAVDESS [26] and CREMA-D [6] to the training data, because the data contains good facial expression and speech about the emotion. For MambaCompressor, we leverage the pretrained Mamba-370M [14] model and finetune it on our training dataset. We introduce a  token to the vocabulary and insert a  token at the end of every utterance, using its output feature to summarize the preceding content.\nFor the large language model (LLM), we use Qwen2.5-7B-Instruct [41]. During the fine-tuning of the LLM on the conversation dataset, we apply LoRA [16] with a rank of 128 and an alpha of 256, while keeping the pretrained audio-visual emotion extractor and MambaCompressor frozen. For training with EPO, we set $ \\beta$ to 2 and $ \\gamma $ to 0.5, and the learning rate to 1e-6. Before training the model with EPO, we first train our LLM on dialogue task, alongside the memory reconstruction task. This joint training prevents the model from losing the memory understanding capability acquired in Sec. 3.2."}, {"title": "4.3. Baseline Methods", "content": "We compare our model with current state-of-the-art models, including Qwen-2.5-Instruct [41], Claude 3.5 Haiku, Gemini 1.5 Flash [36], GPT-40 [1], Qwen-Audio-Chat [7], and LLaVA-NeXT [23]. We assess both the pretrained Qwen-2.5-Instruct and the fine-tuned version using training data. Claude 3.5 Haiku, Gemini 1.5 Flash, and GPT-40 are proprietary models known for their powerful performance due to large model sizes and extensive training. However, their weights are not publicly available, which prevents any further training. For a fair comparison, by providing the conversation history in a prompt, we enable these proprietary models that have not been directly trained on training data to perform in-context learning. Qwen-Audio-Chat process audio and text input and outputs text. LLaVA-NeXT takes audio and text as input and produces text as output. We finetune Qwen-Audio-Chat and LLaVA-NeXT on the training dataset using LoRA."}, {"title": "5. Results and Analysis", "content": "5.1. Automatic Evaluation of Dialogue Generation\nThe results in Tab. 1 show the effects of our model on generating responses in terms of semantic appropriateness, emotional appropriateness, and diversity. Our model demonstrates robust performance across different datasets, indicating its effectiveness in various conversational contexts. The BertScore results, which are the highest across all benchmarks except for MELD, suggest that our model is good at understanding the flow of conversation and generating semantically appropriate responses. This indicates that MambaCompressor condenses input utterances into a few features while still retaining the context. Furthermore, by enabling the model to fully grasp and respond to the historical context more effectively, it performs better than the"}, {"title": "comparison model.", "content": "A notable improvement in the IEMOCAP dataset, which consists of spontaneous dialogue from video interactions and closely mimics actual dialogue situations, suggests our model is well-suited for real-world conversational scenarios.\nThe EmotionScore results indicate that our model excels at generating emotionally appropriate responses across all benchmarks. This performance suggests that our approach enables the model to discern subtle emotional nuances from audio-visual signals and text, and respond accordingly.\nThe high Dist-1 scores across benchmarks demonstrate our model's ability to generate diverse responses, which is essential for natural conversation. It shows that, unlike previous models which tended to provide uniform responses, our model can respond more diversely by adapting to different emotional situations, leveraging subtle emotional nuances from audio-visual signals and text.\n5.2. Human Evaluation\nThe human evaluation was conducted on the Prolific platform, with participants residing in English-speaking countries. In the experiment, participants were provided with the entire video containing the conversation history and were asked to rate the responses generated by the models using a three-point scale: good, medium, and bad. Scores were assigned as follows: good = 3, medium = 2, and bad = 1, and the average was calculated based on these values. Partici-"}, {"title": "5.3. Ablation study", "content": "The results in Tab. 2 display the ablation study conducted on our method using the IEMOCAP benchmark. We tested five model versions, all based on Qwen2.5-7B-Instruct model. The first model is not finetuned, the second is finetuned, the third includes MambaCompressor, the fourth incorporates an audio-visual emotion extractor, and the last version is a model trained with EPO applied. When the MambaCompressor is not used, we provide conversation history as a prompt to aid contextual understanding.\nThe results demonstrate the effectiveness of our proposed ideas. Comparing the second and third rows, we see an increase in BertScore, suggesting that even with input size compressed by the MambaCompressor, the model can maintain context. However, the EmotionScore decreases slightly in the same comparison, indicating that while context is preserved, some subtle emotional nuances may be missed. When comparing the third and fourth rows, the EmotionScore improves, suggesting that using audio-visual signals enables the model to generate more emotionally resonant responses. Finally, in the last row, we observe that the use of EPO results in increased BertScore, EmotionScore, and Dist-1, highlighting the enhanced overall performance."}, {"title": "5.4. Emotion Recogntion Accuracy", "content": "Fig. 6 shows the result of the experiment designed to verify if adding audio and video features helps in identifying the emotion of a sentence. We utilized the audio encoder and video encoder trained as described in Sec. 3.1 to extract audio and video features for this purpose. The results indicate that using both audio and video features improves accuracy\nby 7.86% compared to using text alone. It is also observed that using both audio and video together is more effective for emotion recognition than using only single video or single audio. This demonstrates that subtle emotional nuances, which may not be conveyed through text, can be captured effectively through audio and video inputs.\n5.5. Efficiency Advantage of MambaCompressor\nTab. 3 shows the results of an experiment conducted using only samples with long conversation histories of 20 or more turns from the MultiDialog dataset. We reported the average input size for prompts, as well as the memory and time required to process the prompts. To ensure a fair comparison, we measured the resources used solely for prompt processing by configuring the setup to avoid generating responses during this stage. To evaluate the quality of generated responses, we obtained responses using a separate setup configured for response generation, and evaluated them using BertScore for quality assessment.\nThe results, shown in Tab. 3, demonstrate that using MambaCompressor enables efficient resource utilization, particularly for handling long contexts. Specifically, input tokens were reduced by 81.6%, memory usage decreased by 3.50%, and time consumption dropped by 28.5%. We also observe that while BertScore is even slightly improved, resource usage was significantly reduced. These findings demonstrate that MambaCompressor can substantially reduce resource usage while maintaining response quality, especially when the conversation history is lengthy."}, {"title": "5.6. Case studies", "content": "We show our generation results in Fig. 7. In Fig. 7a, the conversation begins with a discussion about the Iowa football team and then smoothly transitions into a broader conversation about college football teams. Therefore, the user's final question, \"Do you know what the highest score in football is?\" can be interpreted as asking about the highest score across all football leagues, not just the NFL. LLaVA-NeXT responds with \"I don't know,\" while Claude, Gemini, and GPT-40 focus on the highest score in the NFL. However, our model remembers the context about college football teams and mentions the 222-0 game between Georgia Tech and Cumberland in the college football league as the highest score. In Fig. 7b, there is a discussion about a strange rule preventing cheerleaders from sitting and eating with the players. The user expresses frustration, indicating discomfort through the video. In this context, LLaVA-NeXT downplays the issue by saying it's annoying but not surprising, showing a lack of alignment with the conversation flow and the user's emotions. Gemini simply reconfirms the factual information without showing empathy. However, our model responds emotionally, describing the rule as unfair, showing empathy and aligning with the user's emotions. In Fig. 7c, Chandler and Monica exchange dialogue where Chandler states that their relationship is over. Monica responds with a surprised \"What?\u201d, indicating shock at her unexpected words. In the final Chandler's word, Chandler expresses sadness in the video about the relationship falling apart. Here, Qwen and LLaVA-NeXT respond rather neutrally, while Gemini reacts with overly sarcastic comments toward Chandler's sadness. GPT-40 captures Monica's surprise but responds in an emotionally flat manner. In contrast, our model replys \"What are you talking about? It's not over,\u201d continues with Monica's shocked reaction and delivering a more emotionally appropriate response. In Fig. 7d, Phoebe shows concern about the people and appears nervous in the video. Qwen-Audio-Chat and Gemini seem to respond in a literal manner, understanding her words without recognizing the underlying anxiety. LLaVA-NeXT understands that Phoebe is worried but does not offer any supportive response. In contrast, our model reassures her by saying she's doing well, which demonstrates an understanding of her nervousness and having empathy. These examples demonstrate that our model understands the overall content of conversations well and responds with empathy."}, {"title": "6. Conclusion", "content": "This paper addresses the issues of traditional chatbots by integrating Emotional Preference Optimization (EPO) and MambaCompressor. Traditional chatbots have struggled with understanding subtle emotional nuances and managing long conversation histories. By training chatbot model with both correct and counter-emotional sentences through EPO, our approach enables the chatbot to respond more sensitively to nuances and emotions. Furthermore, MambaCompressor significantly reduces input size, lowering time and memory complexities while also enabling the chatbot to understand conversation history more accurately by compressing lengthy histories. As a result, our chatbot demonstrates a profound capability to comprehend overall conversation content and generate more empathetic responses."}]}