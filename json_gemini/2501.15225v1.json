{"title": "SEAL: SCALING TO EMPHASIZE ATTENTION FOR LONG-CONTEXT RETRIEVAL", "authors": ["Changhun Lee", "Jungyu Jin", "Younghyun Cho", "Eunhyeok Park"], "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has a unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores. Built on this insight, we propose a learning-based mechanism using zero-shot generated data to emphasize these heads, improving the model's performance in long-context retrieval tasks. By applying SEAL, we can achieve significant improvements in in-domain retrieval performance, including document QA tasks from LongBench, and considerable improvements in out-of-domain cases. Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) (Brown et al. (2020), Radford et al. (2019), Touvron et al. (2023)) are capable of rapidly generating high-quality answers to a wide range of questions by leveraging the diverse knowledge embedded in their vast number of parameters. However, in-depth analyses have revealed a common issue known as hallucination (Shuster et al. (2021), Lin et al. (2021), Ji et al. (2023)), where the models confidently produce inaccurate answers. To address this, research has focused on using external information as context to guide the outputs, such as Retrieval-Augmented"}, {"title": "2 RELATED WORK", "content": "Circuit Analysis There have been continuous efforts to identify and interpret the internal mech-anisms of LLMs and Transformers. Elhage et al. (2021) analyzed the mechanism of a two-layer attention-only model, revealing the presence of attention heads that contribute to in-context learning. Ferrando et al. (2024) identified various roles of attention heads, such as copy heads and positional heads. Wu et al. (2024) further demonstrated that certain heads play a role in copying the correct answer during retrieval. These studies have primarily focused on analyzing the roles of individual heads, and in addition, analysis methods such as circuit analysis and logit attribution (Ferrando et al. (2024), Lieberum et al. (2023)) have been proposed."}, {"title": "3 MOTIVATION", "content": "Research on Transformer-based architectures (Elhage et al. (2021), Ferrando et al. (2024)) has shown that attention heads, a key component, perform distinct roles such as copying, retrieval, and rele-vance, working together to shape the network's overall functionality. Notably, some heads specialize in handling long sequences, while others focus on retrieval. This leads to an optimistic prediction: if we can identify and strengthen the heads specialized in long-context retrieval, we might significantly enhance performance in that area."}, {"title": "3.1 PRIMARY OBSERVATION: PER-HEAD PRUNING", "content": "To validate this prediction, we first re-examined whether each attention head contributes differently to the retrieval process and determined if we could identify an attention head specialized for retrieval. Our experimental design is straightforward. As shown in Figure 2(a), we pruned one head at a time on the LongChat-7B-32K (Li et al. (2023)) model and compared the resulting accuracy changes with the accuracy of the baseline network. To simplify the experiment, we used the LongEval (Li et al. (2023)) line retrieval benchmark, where the goal is to retrieve a digit of up to five characters randomly located in a given text. This benchmark was particularly convenient because the target retrieval tokens are limited to the digits 0 through 9.\nAs shown in Figure 2(b), the impact of each head varied significantly, with accuracy changes of approximately \u00b120% or more, indicating that certain attention heads play a crucial role in retrieval."}, {"title": "3.2 GENERALIZED APPROACH: ATTENTION HEAD-WISE SCALING", "content": "Next, we developed a more general approach to extend the head-wise pruning experiment. Since pruning multiple heads simultaneously can lead to performance degradation, a more scalable method was needed. To address this, we adjusted the scale of the identified heads to see if this could holis-tically improve accuracy. Built on this insight, we divided the quadrants in Figure 2(b) based on baseline performance (0.0) on the x and y axes. Instead of pruning individual heads, we tried scal-ing multiple heads together. By scaling the influence of all heads in the first quadrant (Q1)\u2014whose pruning benefits the retrieval task\u2014by 0.9, we observed an accuracy increase from 32% to 56% at the input length of 31K (blue dotted line in Figure 2(d)). In contrast, scaling the heads in Q3-whose pruning degrades retrieval-by 0.9 resulted in a significant drop in performance (yellow line). In-terestingly, when we scaled Q1 by 0.9 and Q3 by 1.1 simultaneously, we observed an even greater improvement in retrieval scores (red line). This suggests that jointly scaling and controlling the influence of these heads can significantly enhance retrieval performance."}, {"title": "3.3 EXTENDED APPROACH: ATTENTION CHANNEL-WISE SCALING", "content": "While previous observations show that head-wise scaling offers new possibilities for improving long-context retrieval performance, there is still room for refining the granularity of scaling. As noted in Quantizable Transformers (Bondarenko et al. (2023)), earlier research suggests that specific channels handle syntactic elements like delimiter tokens, and even encode task-specific knowledge (Rudman et al.). In our LongChat-7B (Li et al. (2023)) pruning experiment, we further applied channel-wise pruning to the head with the greatest performance improvement (L1H18) and the head with the largest performance drop (L13H16), as shown in Figure 2(c). Interestingly, within L1H18's 128 channels, only certain channels accounted for most of the performance changes. Similarly, when we controlled L13H16 at a finer channel level, we discovered that some channels actually improved performance during pruning, though the overall head caused a significant drop. This underscores the need for channel-wise manipulation at a finer granularity than the head-level adjustments."}, {"title": "4 PROPOSED METHOD: SEAL", "content": "Built on these invaluable observations, we introduce a novel method called Scaling to Emphasize Attention for Long-Context Retrieval (SEAL), a framework designed to validate our findings and en-hance the long-context retrieval performance of existing LLMs. In SEAL, we update existing LLMs without altering their learned behavior, instead efficiently adjusting the strength of each attention"}, {"title": "4.1 GENERATING TRAINING DATA FOCUSED ON THE CONTEXT FORMAT", "content": "During the dataset generation stage, we observed that SEAL's focus is not on the inherent value of real-world data, but rather on the format of data representation for long-context tasks. To demonstrate this, we generated synthetic training data using an LLM and the task domain's format, instead of using real data with meaningful values, and used it to train the attention strength.\nInitially, we generated 50 sample input and answer sets for the given downstream long-context task. To avoid contamination, we ensured consistency only in format while generating random content. The method for obtaining format samples may vary depending on the type of downstream task. The left side of Figure 3 visualizes the pipeline for generating training samples for the Needle-in-a-Haystack task, as an example. Below are examples created for line retrieval (a) and Needle-in-a-Haystack (b) tasks.\n(a) Prompt: ... line righteous-ethernet: REGISTER_CONTENT is <40779>\nAnswer_string: The  in line righteous-ethernet is 40779.\n(b) Prompt: ... Based on the content of the book, Question: What is immediately noticeable upon enter-ing the room?\nAnswer_string: Immediately noticeable upon entering the room is the large oak table positioned beneath the chandelier."}, {"title": "4.2 LEARNABLE SPACE DESIGN: SEAL-H AND SEAL-C", "content": "Using the generated data, we trained a learnable scaling for attention components. Based on the intu-ition from pruning experiments of Section 3, we propose two granularities for attention control. The first is SEAL-H (head), which places a learnable scalar head-wise to learn the strength of each head (Figure 3 Right). This process allows us to probe the influence of each head on retrieval while jointly learning scaling appropriate for long contexts. The second option is SEAL-C (channel), which ad-ditionally uses a learnable vector for the hidden dimension of each attention output (channel-wise). As observed in Section 3.3, we found that within the attention heads, there are channels that have both positive and negative impacts. SEAL-C assigns and updates parameters on a per-channel basis. While this increases the number of parameters to be learned, it is expected to allow for more fine-grained manipulation of the attention head outputs, potentially leading to improved performance."}, {"title": "4.3 PEFT BASELINE: SEAL-L (LORA)", "content": "The proposed SEAL method can be categorized under Parameter-Efficient Fine-Tuning (PEFT), as it selects vital, minimal learnable parameters that can impact retrieval and performs supervised fine-tuning on these scales. From this perspective, a representative PEFT, LORA (Hu et al.), can intuitively serve as our baseline and validate the effectiveness of our fine-tuning pipeline. Further-more, comparisons with SEAL-C and SEAL-H suggest that if these methods achieve performance comparable to SEAL-L with fewer parameters, it validates that we accurately identify the key factors contributing to improved retrieval performance. Considering the most basic form of LoRA with rank 1 (r = 1), the learnable vectors of LoRA adjust the retrieval-related influence in a manner similar to SEAL-C by controlling the effect across different channels. For this reason, we propose SEAL-L (LoRA), which can be viewed as a superset of SEAL-C. In SEAL-L, while the LoRA module is used, the data and training scheme are derived from the SEAL framework. In the main experiments, we additionally report the results of the SEAL-D (DoRA). SEAL-D replaces the LoRA module with the DoRA (Liu et al. (2024b)) module, a recent variant of LoRA. Through experiments, we demonstrate that SEAL-H and SEAL-C represent the core components responsible for quality improvement."}, {"title": "5 QUALITATIVE ANALYSIS BASED ON DIRECT EFFECT", "content": "Before measuring SEAL's performance in downstream tasks, we first conducted a qualitative anal-ysis in this section to provide a deeper understanding of how the proposed SEAL contributes to improving retrieval scores. While various circuit analysis techniques have been proposed to ana-lyze the functioning of Transformer architecture, we utilized the direct effect method, which is one of the most intuitive and successful approaches for presenting analysis results. Let f (p) represent the hidden state output of each component (e.g., attention heads, MLPs) for a prompt p whose effect we aim to observe, and we denote the head weight as $W_{head}$. Then the direct effect can be expressed by the following equation:\n$\\Delta = W_{head}f (p)$    (1)\nSpecifically, we utilized a form similar to the direct effect proposed in Lieberum et al. (2023), ex-cluding the normalization term."}, {"title": "5.1 DIRECT EFFECT ANALYSIS BEFORE AND AFTER SEAL", "content": "For the line retrieval task from the LongEval, we selected an example where the baseline LongChat-7B-32K model produced an incorrect answer, while the tuned model with SEAL provided the correct retrieval answer. The selected example is shown below.\nPrompt: ...odd-shrimp: REGISTER_CONTENT is <32616> \\nline verdant-efficiency: REGIS-TER_CONTENT is <24819> \\nline permissible-prostanoid:...\nQuestion: Tell me what is the  in line verdant-efficiency? I need the number.\nCorrect Answer: The  in line verdant-efficiency is 24819.\nWrong Answer: The  in line verdant-efficiency is \"24856\"."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "To validate the effectiveness of the proposed SEAL, we evaluated its retrieval performance on long-context inputs for two widely-used tasks: line retrieval from LongEval and the Needle-in-a-Haystack.\nModels: We validated SEAL on five models: LongChat-7B-v1.5-32K and Mistral-7B-Instruct-v0.2 (Jiang et al. (2023)), which support a 32K context window length, and Vicuna-7B-v1.5-16K (Chiang et al. (2023)), Vicuna-13B-v1.5-16K, LongChat-13B-16K, which support a 16K context window.\nSettings: We utilized the Axolotl\u00b9 framework to tune SEAL-H, SEAL-C, SEAL-L, and SEAL-D. The tuning was performed using the AdamW optimizer without learning rate (lr) decay, and all models were tuned for 1 epoch. For tuning in the line retrieval task, SEAL-C used a lr of 2e-2, while SEAL-H used le-2 and 2e-2 for the 7B and 13B models, respectively. For the Needle-in-a-Haystack task, learning rates of 4e-2 and 5e-2 were used. For SEAL-L and SEAL-D, LORA and DORA modules with r = 4 were applied, respectively, to every linear layer in the attention module (QKVO), with a lr of 2e-4. A single A100 80GB GPU was used for both tuning and evaluation.\nDataset generation: We used 50 generated samples for each task. Models supporting 32K context window length were tuned with samples containing 31K input tokens, while models supporting 16K context window length used 16K input tokens. For the 7B models, tuning with the 31K dataset took about 40 minutes, and tuning with the 16K dataset took about 10 minutes."}, {"title": "6.1 RESULTS ON LINE RETRIEVAL TASK", "content": "In Table 1, the baseline models of LongChat and Vicuna show significant score degradation as the input length approaches their context window limits. However, the proposed SEAL methods demonstrate dramatic improvements over the baseline across all input lengths, with particularly"}, {"title": "6.2 RESULTS ON NEEDLE-IN-A-HAYSTACK TASK", "content": "Figure 5 presents the results of applying SEAL to the Needle-in-a-Haystack task. While Mistral doesn't collapse at longer input than 32K, it still experiences performance degradation with sig-nificantly longer inputs. Despite using only 50 samples and training with synthesized needles that are different from the actual target needle, as depicted in Figure 3, SEAL demonstrates remarkable performance improvement. Below are examples of correct and incorrect responses of the LongChat-7B-v1.5-32K model at a length of 20533 tokens, 22% depth of needle insertion.\nPrompt: ...It's a worrying prospect. The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day. It would be a bummer to have another grim monoculture like...\nQuestion: What is the best thing to do in San Francisco?\nSEAL-C (score: 100%): The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\nBaseline (score: 8.3%): Go to the top of the hill at Lands End and look out at the city.\nAlthough SEAL-H shows slightly lower performance than SEAL-C or SEAL-L, it once again con-firms that retrieval performance can be greatly recovered by simply adjusting the head-wise influence through scalar values, amounting to only 1024 parameters for the entire 7B model."}, {"title": "7 SEAL WITH TRAINING-FREE CONTEXT LENGTH EXTENSION", "content": "In this work, we address one of the two major problems that can arise with lengthy inputs: the gradual decline in performance within the context window. However, our approach can be used orthogonally to methods that extend the context window length itself. In fact, the application of SEAL to models like LongChat is an example where the Llama (Touvron et al. (2023)) model has already been extended with context windows through RoPE scaling and fine-tuning. However, such tuning-based extensions come with significant costs in terms of time, data, and training infrastructure.\nRecently, training-free context length extension methods (e.g., NTK (bloc97 (2023a)), Self-Extend (Jin et al. (2024))) have emerged and garnered considerable attention. However, it is important to note that these methods generally exhibit lower performance compared to fine-tuning-based ap-proaches (e.g., PI (Chen et al. (2023)), YaRN (Peng et al. (2023))). If SEAL could be applied orthogonally to these training-free context length extension methods, it would offer the attractive possibility of simultaneously leveraging the low-cost advantages of the SEAL and tuning-free ap-proach while restoring performance degradation through SEAL.\nThe results in Table 2 show that when extending the effective context length of Llama-2-7b-Chat to over 16K using only NTK or Self-Extend, the retrieval performance at lengths greater than 8K drops significantly. However, by utilizing SEAL in combination to adjust the attention influence, we can dramatically improve performance beyond the original base model's context window limitation"}, {"title": "8 GENERALIZATION ABILITY OF SEAL", "content": "The proposed SEAL method adopts a task-specific approach using formatted data for particular downstream tasks, but it is fundamentally based on the theoretical premise of scaling attention com-ponents to enhance retrieval capabilities. To evaluate whether SEAL can deliver general improve-ments in retrieval performance for out-of-domain tasks, we measured the scores for the QA task type in LongBench using the scaling values learned from the line retrieval task in Section 6.1. We used the learned scaling values of the LongChat-7B model, which showed the largest performance improvement in line retrieval. We also provided results when LongBench was evaluated as an in-domain manner. Additionally, to ensure that SEAL's retrieval-focused scaling does not degrade the inherent knowledge or reasoning abilities of the LLMs, we measured the MMLU (Hendrycks et al. (2020)) scores.\nWhen using scale values tuned for the line retrieval task, the out-of-domain MMLU results are 42.53 / 42.34 / 42.17 for baseline, SEAL-H, and SEAL-C, respectively. The MMLU scores remain nearly unchanged, indicating that our method effectively identifies and scales only the attention heads relevant to long-context retrieval. Additionally, despite SEAL being applied task-specifically to line retrieval, which focuses on retrieving numbers, Table 3 shows that the scores in the out-of-domain LongBench metrics are maintained or even slightly improved. This demonstrates that the retrieval performance gains achieved by SEAL contribute to tasks like document QA, confirming the generalization capability of our approach."}, {"title": "9 CONCLUSION", "content": "The ability to retrieve and extract information from long-length input is an important component of the LLMs. Through our analysis, we found that there are attention heads that have a good or bad impact on the retrieval scores. Based on this, we introduce SEAL, a cost-efficient attention strength"}, {"title": "C ANALYSIS ON NUMBER OF SAMPLES AND LEARNING RATE", "content": "One of the advantages of SEAL is that it can achieve significant performance improvements with a very small number of formatted data samples. To analyze the impact of the number of samples on scale tuning, as well as the influence of the key hyperparameter, learning rate. We tuned the scale of SEAL-H by sweeping the learning rate and the number of samples. For this experiment, we generated a new set of 100 random samples for line retrieval using the same method proposed in Appendix B. The results of applying SEAL-H to LongChat-7B-v1.5-32k with different hyper-parameter configurations are shown in Table 4. Generally, performance improves as the number of samples increases, and for LongChat, a learning rate of 3e-2 was identified as the best configuration. However, for general configurations, we adopted a learning rate of le-2 in the main experiments."}]}