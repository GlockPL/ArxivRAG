{"title": "SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation", "authors": ["Yi Tian", "Juan Andrade-Cetto"], "abstract": "Event cameras generate asynchronous and sparse event streams capturing changes in light intensity. They offer significant advantages over conventional frame-based cameras, such as a higher dynamic range and an extremely faster data rate, making them particularly useful in scenarios involving fast motion or challenging lighting conditions. Spiking neural networks (SNNs) share similar asynchronous and sparse characteristics and are well-suited for processing data from event cameras. Inspired by the potential of transformers and spike-driven transformers (spikeformers) in other computer vision tasks, we propose two solutions for fast and robust optical flow estimation for event cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial neural network (ANN) architecture with spatiotemporal shifted window self-attention (swin) transformer encoders, while SDformerFlow presents its fully spiking counterpart, incorporating swin spikeformer encoders. Furthermore, we present two variants of the spiking version with different neuron models. Our work is the first to make use of spikeformers for dense optical flow estimation. We conduct end-to-end training for all models using supervised learning. Our results yield state-of-the-art performance among SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and show significant reduction in power consumption compared to the equivalent ANNs. Our code is open-sourced at https://github.com/yitian97/SDformerFlow.", "sections": [{"title": "I. INTRODUCTION", "content": "PTICAL flow measures pixel motion with photometric consistency in the image plane and is crucial for numer- ous computer vision and robotics tasks. Whilst traditional frame- based optical flow estimation struggles in low-illumination and fast-motion scenarios, event-based optical flow can better cope with such challenging scenarios thanks to the higher temporal resolution and dynamic range of event cameras. The sparse and asynchronous event streams generated by event cameras directly encode apparent motion patterns, but due to the fundamentally different data throughput of the two camera types, estimating event-based optical flow suggests approaches distinct from those of conventional computer vision. As with many other computer vision problems, methods using ANNs have demonstrated higher accuracy in event-based optical flow estimation [1, 2, 3] compared to classical model-based methods [4, 5]. ANN architectures, however, do not fully exploit the sparse and asynchronous nature of event data, and SNNs have emerged as a promising alternative. In SNNS, neurons integrate input spike trains and generate a binary spike when the membrane potential reaches a threshold, resetting its value afterward. Neurons are active only when spikes arrive, just as individual event camera pixels are active only when intensity changes. Sharing this event-driven characteristic makes SNNS an energy-efficient option for processing event data. However, directly training deep SNNs is challenging due to the non-differentiability of the spike activity. The backpropagation through time with surrogate gradient method [6] has bridged neuromorphic computing with the deep learning community, enabling the training of deeper SNNs. Despite this advancement, the performance of SNNs still lags behind that of ANNs for most computer vision tasks.\nIs it possible to benefit both from the recent advances in ANN architectures and the spike-driven properties of SNNs to achieve an energy-efficient solution for event-based optical flow estimation with competitive performance? For ANNs, the visual transformer (ViT) and its variant architectures have garnered increasing interest as potential replacements for convolution networks in various computer vision tasks. Due to their inherent locality, convolution-only models struggle to capture temporal correlation and to efficiently represent global spatial dependencies [7, 8, 9, 10]. At the same time, the self-attention mechanism in ViT architectures can focus on different parts of the input to capture global context effectively. The integration of ViT, particularly with spatiotemporal attention, has also shown promising results in event-based vision tasks, such as monocular depth estimation [11] or action recognition [12, 13]. Combining SNNs with the ViT architecture for event cameras appears to be a natural choice, as the combination leverages the strengths of both approaches: the temporal dynamics and energy efficiency of SNNs and the representational power of transformers. Moreover, the self-attention mechanism in transformers also shares a biological background with SNNs [14, 15, 16, 17]. The spikeformer architecture, the SNN version of the ViT, has been validated mostly on higher level tasks, such as classification [14, 16], the regression of human pose [18], depth estimation [19], video action recognition [20], and object detection [21].\nThe best-performing event-based optical flow solutions to date use ANNs with correlation volumes [2] or iterative deblur- ring [22]. Correlation volumes require substantial computational and memory resources. Adopting transformers for optical flow in ANNs has also shown superior performance compared to non- transformer-based models, particularly excelling in scenarios involving large displacements due to their ability to capture global dependencies [10, 23, 24, 25, 26]. No one has proposed a pure SNN architecture, specifically utilizing spikeformers for event-based optical flow estimation.\nIn this work, we study the combination of ViT with SNNs for event-based optical flow estimation. We introduce SDformer- Flow, an SNN employing spatiotemporal swin spikeformers. Additionally, for better comparison, we propose STTFlowNet, the ANN counterpart to our SNN model. We conduct end- to-end training using supervised learning. Our work marks the first instance of utilizing spikeformers for optical flow estimation, demonstrating comparable performance to state-of- the-art SNN optical flow estimation methods, while significantly reducing energy consumption. We report on two variants of our SNN model: our first variant SDformerFlow-v1 [27] and the improved variant SDformerFlow-v2 in this paper with better performance and reduced computational complexity.\nOur contributions are threefold: Firstly, we introduce STTFlowNet, a swin transformer-based model for event-based optical flow estimation, equipped with spatiotemporal self- attention to capture dependencies in both the time and space domains. Secondly, we present two spiking versions of our architecture, SDformerFlow, with different neuron models, marking the first known utilization of spikeformers for event- based optical flow estimation. Lastly, we conduct extensive experiments on datasets. Compared with baseline models, our method uncovers the potential of combining transformers with SNNs for regression tasks."}, {"title": "II. RELATED WORK", "content": "A. Learning-based methods for event-based optical flow esti- mation\nDrawing inspiration from frame-based optical flow tech- niques, the estimation of event-based optical flow using deep learning has achieved state-of-the-art performance compared to model-based methods [1, 5, 4]. Early works predominantly employed a U-Net architecture [28, 29, 30, 31] to predict sparse flow and evaluated it using masks due to limited accuracy where no events are present. Inspired by RAFT flow [32], Gehrig et al. [1] proposed E-RAFT and contributed the DSEC dataset and optical flow benchmark [33]. Since then, methods based on recurrent neural networks with correlation features and iterative refinement strategies have become the state-of-the- art [1, 31, 25].\nRecent studies have shifted their focus towards enhancing the temporal continuity of optical flow estimation, aiming to fully leverage the low latency characteristics of event cameras [22, 2, 34], or integrating richer simulated training datasets [25, 35, 36] to improve accuracy. However, these recurrent refinement methods implicate calculating computationally expensive cost columns and an iterative update scheme that brings latency to the inference phase. Some works explore latency reduction at the expense of slight loss in performance [22].\nAnother line of work based on SNNs emerges as a com- putationally efficient solution for event camera optical flow estimation. Most works trained SNNs using self-supervised learning on the MVSEC dataset, yielding sparse flow estima- tion [30, 37]. More recent efforts involve training SNNs using supervised learning on the DSEC dataset, resulting in dense flow estimation [38]. To incorporate longer temporal correla- tions into the SNN model, some works utilize adaptive neural dynamics in comparison with event inputs containing richer temporal information [37], while others introduce external recurrence [34]. In [38], the authors employed 3D convolutions with stateless spiking neurons, neglecting the intrinsic temporal dynamics of the neurons. However, the performance of SNNs still falls behind that of ANNs. While some ANN methods incorporate transformer architectures in some of their stages [10, 23, 24, 25, 26] and show performance improvements, no one has ever combined SNNs with transformer architectures for optical flow estimation.\nB. Spikeformer\nRecently, the combination of SNNs and transformer archi- tectures has garnered increasing interest in the neuromorphic community [14, 15, 39, 40]. Zhou et al. [14] initially proposed spiking self-attention, which eliminates the softmax function as the spike-formed query and key naturally maintains non- negativity. Building upon this, Yao et al. [16] introduced a fully spike-driven transformer with spike-driven self-attention, leveraging only mask and addition operations to facilitate hardware implementation. Later, they expanded it into a meta-architecture [16] for classification, detection, and segmentation tasks. Shi et al. [41] pointed out that the previous spike- formers rely on a shallow convolutional network for feature extraction and lack proper scaling methods [14, 16]. They proposed a multi-stage architecture with a dual spike self- attention (DSSA) and a proper scaling method to address the problem. More recently, Zhou et al.[40] proposed QKFormer with a Q-K attention that adopts spike-based components for the query and key with linear complexity. While most spikeformers only apply spatial-wise attention in a single time step [14, 16, 41, 40], some works also incorporate spatiotemporal attention [18, 42, 20]. However, none of the previous works have utilized the swin variant of the spikeformer for optical flow estimation."}, {"title": "III. METHOD", "content": "A. Preliminaries\n1) Spiking neurons: Spiking neurons are the fundamental units of SNNs. Unlike conventional ANNs that use continuous activation functions, spiking neurons communicate through discrete spikes known as action potentials. The leaky integrate- and-fire (LIF) model is the most commonly used neural model in the literature, although numerous recent studies have investigated adaptive neurons to achieve better performance [37, 43].\na) Leaky Integrate-and-Fire: The LIF model is widely adopted in the literature due to its simplicity of implementation and low computational cost. In SDformerFlow-v1, we use the Spikejelly [44] implementation of the LIF neuron model for all layers, and set $V_{th}$ = 0.1 and $\\tau_m$ = 2.\nThe dynamics of LIF neurons at time step t with hard reset can be modeled as\n$H[t] = V [t - 1] + \\frac{1}{\\tau} (X[t] - (V[t - 1] - V_{reset}))$\t(1)\n$S[t] = \\Theta(H[t] \u2013 V_{th})$\t(2)\n$V[t] = H[t](1 \u2013 S[t]) + V_{reset}S[t]$\t(3)\nwhere X[t] represents the inputs at time step t. H[t] is the membrane potential of the LIF neuron before the neuron fires, while V[t] is the membrane potential after it fires. H[t] changes according to the presynaptic spikes received, and \u03c4 is the time constant. When H[t] reaches the threshold $V_{th}$, the neuron fires, generating a spike S[t], and V[t] resets to $V_{reset}$. \u0398(V) is the Heaviside step function, which outputs a zero value for negative arguments and one for positive arguments.\nb) Parallel Spiking Neuron (PSN): Fang et al. [45] proposed PSN, which enables parallelizable neuronal dynamics after removing the resetting mechanism. It maximizes the utilization of temporal information and yields extremely high simulation speed. In SDformerFlow-v2, we change to the use of PSN neurons with learnable parameters.\nIn PSN, the neuron dynamics for LIF without reset mecha- nism become\n$H[t] = \\sum_{i=0}^{T-1}W_{t,i} \\cdot X[I]_{t-i}$ ,\t(4)\n$S = \\Theta(H \u2013 B)$,\t(5)\nwhere $W_{t,i}$ is the weight between input X[i] and membrane potential H[t]. The non-iterative formulation enables to par- allelize the neuron state across time steps with a learnable weight matrix W and a learnable threshold vector B:\n$H = WX$,\t$W \\in \\mathbb{R}^{T \\times T}, X \\in \\mathbb{R}^{T \\times N}$\n$S = \\Theta(H \u2013 B)$,\t$B \\in \\mathbb{R}^T, S \\in \\{0,1\\}^{T \\times N}$\nwhere T are time steps and N is batch size. In this way, the neuron state integrates the information from all time steps and avoids the iterative process.\n2) Surrogate gradient (SG): One of the key challenges in training SNNs has been the non-differentiable nature of spike generation, which precludes the use of standard gradient-based optimization commonly employed in traditional ANNs. The SG method [6] has bridged the gap by approximating the non- differentiable spike function with a continuous, differentiable surrogate during the backpropagation phase. The common surrogate function choices include inverse tangent and sigmoid.\n3) Spike Self-Attention (SSA): The self-attention in ANN is composed of three floating-point components: query (Q), key (K), and value (V). Zhou et al. [14] first proposed SSA which is based on spike-forms for Q, K, and V,\n$Q_s, K_s, V_s = SN(BN(Linear(I)))$\t(6)\n$SSA'(Q_s, K_s, V_s) = SN (QK^T V * s)$\t(7)\n$SSA(Q, K, V) = SN (BN (Linear(SSA'(Q_s, K_s, V_s))))$.\t(8)\nwhere $Q_s, K_s, V_s$ are the spikes form of query, key, and value. I denotes the input. Linear, BN, and SN denote the linear layer, batch normalization layer, and spiking neuron, respectively, and s is a scaling factor used to control large output values to avoid gradient vanishing.\nB. Event Input Representation\nWe divide the event stream into non-overlapping chunks according to the optical flow ground truth rate. Each chunk, comprising N events within a fixed time window, is represented as $E = \\{(x_i, y_i, t_i, p_i)\\}i\u2208[N]$, where $t_i$ is the timestamp and $p_i$ denotes polarity. We preprocess each event chunk into an event discretized volume representation V using a set of B bins, following the methodology introduced in [28],\n$V(x, y,t) = \\sum_i \\rho_i \\kappa(x \u2013 x_i)\\kappa(y \u2013 y_i)\\kappa(t-t_i)$.\t(9)\nTimestamps are normalized and scaled to the range [0, B \u2013 1], $t_i = (B - 1)(t_i - t_0)/(t_n - t_1)$; and \u03ba(\u03b1) = max(0, 1 \u2212 |\u03b1|) is a bilinear sampling kernel. We encode spatiotemporal information into channels to enable the neural network to learn large temporal correlations. For the ANN model, we take the previous and current chunks of event voxels, dividing the total temporal channels into n blocks. Each event input block comprises 2B/n \u00d7 H \u00d7 W bins. In our case, n = 2.\nFor the SNN model, to mitigate the computational burden associated with large time steps, we use only one event voxel chunk. Similarly, we partition the temporal channel, containing B bins, into n blocks along with their corresponding polarities p. This yields an event representation of size T \u00d7 2n \u00d7 H \u00d7 W, with T = B/n time steps. In most of our implementation, we set B = 10 and n = 2. This representation aligns with the spike representation outlined in [37, 29]. Each event chunk comprises C = 4 channels and T = B/2 time steps, as illustrated in Fig. 1.\nC. Network Architecture\nThe network architecture pipelines of our proposed SNN methods SDformerFlow (Fig. 2) and its ANN equivalent STTFlowNet are similar. We adopt an encoder-decoder ar- chitecture, widely utilized in event-based optical flow litera- ture [30, 28, 46, 38, 36]. For STTFlowNet, the architecture of the swin transformer blocks resembles that of [47]. As shown in Fig. 3, each swin block contains a 3D window multi- head self-attention (3DW-MSA) module, followed by a module consisting of two multi-layer-perceptron (MLP) blocks. Layer normalization (LN) is applied after each module, incorporating residual connections. In the 3DW-MSA module, unlike in the original video swin transformer implementations [47], we utilize scaled cosine attention and logarithmic continuous relative position bias (CPB) from swin transformer v2 [48] to enhance the model's scaling capability. In the following sections, we focus on detailing the architecture of our SNN model: SDformerFlow.\nFor SDformerFlow, the primary architecture comprises three parts: a) spiking feature generator (SFG) with shortcut patch embedding (SPE), b) spatiotemporal swin spikeformer (STSF) encoders, and c) spike decoders and flow prediction. The event stream initially enters the SFG module, which outputs spatiotemporal embeddings for the STSF encoders, which in turn generate spatiotemporal features hierarchically. Subsequently, the output from each encoder is concatenated to the decoder at the same scale to predict the flow map. Two additional residual blocks exist between the encoder and decoder modules.\nIn previous spikeformer implementations [14], residual shortcuts utilize either vanilla or spike-element-wise shortcuts (SEW) [49]. Conversely, in SDformerFlow, we opt for using membrane-potential shortcuts (MS) [50]. The vanilla shortcut adds spikes into the memory potential values, which cannot achieve identity mapping and show degradation problems. In SEW shortcuts, the residuals are applied after the spikes, which results in undesirable integration, whereas with MS shortcuts, residuals are applied before the spikes to preserve the spike- driven property. Fig. 4 illustrates the main differences between vanilla shortcuts, SEW shortcuts, and MS shortcuts.\nSpecific implementation details of each main block in the SDformerFlow architecture are:\n1) Spiking Feature Generator with Shortcut Patch Embed- ding: It comprises two stages: an SFG block to generate spatiotemporal features, followed by an SPE block to project them into token embeddings for the STSF encoder module.\na) Spiking Feature Generator: In the first stage, we process the event input through a spiking convolutional module Conv(\u00b7) followed by two residual blocks with MS shortcut MSRes(\u00b7) to downsample the resolution by half. This projection results in a feature map of shape T \u00d7 C \u00d7 H/2 \u00d7 W/2. Given the events input I, the feature generator module can be formulated as\n$z = BN(Conv(SN(Conv_{head}(I))))$\t(10)\n$z = MSRes(z)$\t(11)\n$MSRes(z) = z + BN_2(Conv_2(SN_2(BN_1 (Conv_1 (SN_1(z))))))$,\t(12)\nwhere BN and SN again account for batch normalization and spiking activation, respectively. For STTFlowNet, both the former and latter chunks are fed into a shared Resblock module while retaining the spatial dimension.\nb) Shortcut Patch Embedding: In the second stage, we split the feature map into spatial patches of size P \u00d7 P, maintaining the time steps as the temporal dimension. This operation creates spatiotemporal tokens of size 1 \u00d7 P \u00d7 P, projecting the spatial-temporal features into spike embeddings of shape T \u00d7 C\u00d7H/(2P)\u00d7W/(2P). Inspired by [40], we add a deformed shortcut for the patch embedding module, which boosts the performance. A convolutional layer $Conv_{deformed}$ with kernel size of 1 \u00d7 1 and stride size of 2 is applied to the residual to meet the output shape of the embeddings. The SPE block can be formulated as\n$z_{res} = Conv_{deformed}(I)$\t(13)\n$z = BN(Conv(SN(I))) + z_{res}$\t(14)\n2) Spatiotemporal Swin Spikeformer Encoder: The STSF module draws inspiration from the video swin transformer [47] and the recent spikeformers [14, 16, 40]. Its detailed architec- ture is illustrated in Fig. 2.\nWe adopt four stages of swin transformers, with each stage comprising 2-2-6-2 numbers of STSF blocks successively, followed by a spiking patch merging layer to reduce the dimension by half.\nWithin the same swin layer, the window-based multi-head attention in the first block is denoted as 3DW-SDSA, as the regular window partitioning is performed. In the latter blocks, the window is shifted along the three axes following the same practices as in [51, 47], which is denoted as 3DSW-SDSA. The STSF block can be formulated as\n$z^m = 3DW \u2013 SDSA(z^{m-1}) + z^{m-1}$\t(15)\n$z^m = SMLP(z^m)) + z^m$\t(16)\n$z^{l+m} = 3DSW \u2013 SDSA(z^{m}) + z^{m}$\t(17)\n$z^{l+m} = SMLP(z^{l+m}) + z^{l+m}$\t(18)\n$SMLP(z) = BN_2(Linear_2(SN_2(BN_1(Linear_1(SN_1(z))))))$,\t(19)\nwhere $\\approx$ are the output features of the spiking 3DW-SDSA module or 3DSW-SDSA, and $z^l$ is the output for the spiking MLP module for block l.\nEach STSF block comprises a spiking multi-head spiking driven self-attention (SDSA) block with a 3D shifted window (3DW), followed by a spiking MLP block (see Fig. 5). Each spatiotemporal token of shape T \u00d7 H \u00d7 W is partitioned into non-overlapping 3D windows of size $T_w$ \u00d7 H \u00d7 $W_w$. We employ a window size of 2 \u00d7 9 \u00d7 9 for cropped resolution and 2 \u00d7 15 \u00d7 15 when fine-tuning the model on a full resolution of 480 \u00d7 640. The SDSA is performed within the window. We utilize different numbers of attention heads 3, 6, 12, 24 for the STSF blocks in different stages. We have implemented two types of spiking self-attention that we called spiking dot product attention (Fig.5) and spiking QK linear attention (Fig.6). The details of the SDSA modules are explained as follows:\na) SDSA - Spiking dot product attention: In our SDSA - spiking dot product attention block, the query, key, and value tensors, denoted as $Q_s, K_s, V_s$, are spiking tensors. We use dot product attention, and since the attention maps are naturally non-negative, softmax is unnecessary [14]. We apply a scale factor s for normalization to prevent gradient vanishing in the case of using LIF. Note that for adaptive neurons like GLIF [43] and PSN [45] no scaling is needed since the threshold values can be learned. The single-head SDSA can be formalized as\n$SDSA(Q_s, K_s, V_s) = BN(Linear((Q_sK_s^T + PE)V_s))$.\t(20)\nb) SDSA - Spiking QK linear attention: The spiking dot product attention has a computational complexity of O(Nw * $N_t^2$ * D). In addition, adding the 3D positioning bias into the spike attention map introduces floating-point computations into the attention. Inspired by recent work to relax the computational complexities in spiking self-attention [16, 40], we adapted QK token attention [40] into our 3D window attention. Given the spiking input I, we can obtain the spike form $Q_s, K_s \u2208 IR^{T \u00d7 N_w \u00d7 W_h\u00d7 W_w\u00d7D}$, where T, Nw, Wh, Ww, D denote time steps, number of windows, window height, window width, and hidden dimensions, respectively. We add the positioning encoding (PE) parameters into the states of k before the spiking activation. In the case of single-head attention, $Q_s, K_s$ can be reshaped into $N_w \u00d7 N_t \u00d7 D$, where $N_t = T * W_h * W_w$ is the number of tokens that includes the spatial and temporal dimensions of one window. For the spatiotemporal QK attention part, first, we generate the spiking token attention vector $A_t$ by summing the dimensions of $Q_s$ matrix followed by a spiking neuron, which models the importance of different tokens. Next, the attention output $z'$ is obtained by applying Hadamard product between the token attention vector $A_t$ and $K_s$. This is equivalent to applying a token (column) mask operation to $K_s$. In such a way, we achieve linear complexity attention with O(Nw * D) while preserving the spike-driven properties. Finally, a spiking neuron is applied after the attention output, followed by a linear projection layer. The detail of the attention process can be formulated as\n$Q_s = SN(BN(Linear(Is)))$,\t$Q_s\u2208\\mathbb{R}^{N_w\u00d7N_t\u00d7D}$\t(21)\n$K_s = SN(BN(Linear(Is)) + PE)$,\t$K_s \u2208 \\mathbb{R}^{N_w\u00d7N_t\u00d7D}$\t(22)\n$A_t = SN(\\sum_{d=0}^{D}Q_s)$,\t$A_t \u2208 \\mathbb{R}^{N_w \u00d7 N_t \u00d71}$\t(23)\n$z' = A_t \\bigodot K_s$,\t$Z \u2208 \\mathbb{R}^{N_wXN_tXD}$\t(24)\n$z = BN(Linear(SN(z')))$ .\t(25)\nc) Spiking Patch Merge (SPM): An SPM layer is added after each STSF encoder except for the last one. It comprises a linear layer followed by a batch normalization layer to down- sample the feature map in the spatial domain while maintaining the temporal dimension. The SPM layer is implemented with\n$SPM(z) = BN(Linear(SN(z)))$\t.(26)\n3) Spike Decoder Block: The decoder consists of three transposed convolutional layers $ConvTrans(\u00b7)$, each increasing the spatial resolution by a factor of two. A skip connection from each STSF encoder is concatenated to the prediction output from the corresponding decoder of the same scale. Flow prediction is generated at each scale and concatenated to the decoders. Loss is applied to the flow prediction upsampled to the full resolution. Given the output from the STSF of each scale l as $z_{en}^l$, the output of each decoder $z_{de}^l$ and the flow prediction can be formulated as\n$z_{de}^l = BN(ConvTrans(SN(z_{en}^l \u2295 pred(z_{de}^{l-1}))))$\t(27)\n$pred(z_{de}^l) = Conv(z_{de}^l)$\t(28)\nD. Loss Function\nWe train our model with supervised learning using the mean absolute error between the predicted optical flow $\\hat{u} = (\\hat{u}^{pred}, \\hat{v}^{pred})$ and the ground-truth flow $u^t = (u^t, v^t)$. Our loss function can be formulated as\n$L = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{u}^{pred}_i - u^t|^2$\t(29)\nwhere n is the number of valid ground truth pixels. For SNN, we employ surrogate gradient [6] with backpropagation through time to train the network. We use the inverse tangent as the surrogate function with a width of 2."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset and training details\nFirst", "33": "for both training and evaluation. The DSEC dataset is a comprehensive outdoor stereo event camera dataset featuring a resolution of 640 \u00d7 480. Ground-truth optical flow annotations are provided at a rate of 10Hz for some of the sequences. To address the lack of ground truth in the test set", "38": "dividing the training sequences into training and validation sets. Notably", "52": ".", "31": "or other small indoor flying dataset [30", "37": "resulting in overfitting problems [4", "53": "."}, {"53": ".", "dataset": "We use average endpoint error (AEE)", "51": ".", "28": "struggles to estimate the correct direction. In contrast"}, {"28": "and other self-supervised trained models [5", "25": ".", "38": "uses stateless neurons and is trained at full resolution", "37": "with limited representation in the benchmark. Notably"}, {"28": "and yields state-of-the-art results among all the SNN methods.\n2) Evaluation on the MVSEC dataset: The quantitative evaluation on the MVSEC dataset is given in Table II", "26": ".", "38": "reported their results for the indoor sequences separately trained on the subsets of the same dataset"}, {"28": "on the DSEC training set as our base model for 60 epochs while randomly cropping to size 288 \u00d7 384. Our ANN model shares the same U-Net architecture as EVFlowNet", "of": "a) the input representation: event voxel or count; b) the number of temporal partitioning blocks: b2 or b4; c) the spatial patch size: p = 2 or p = 4, the swin spatial window size w; and d) the training resolution.\nResults are summarized in Table III. Using the event voxel representation retained more temporal information and notably improved results. The use of swin transformer layers instead of convolutions also led to significant performance gains. For the variants of STTFlowNet, the window size influenced the range of the area to pay attention to, with smaller window sizes making it difficult for the network to learn larger displacements. Adjusting the patch size between 2 and 4 according to the window size and resolution was found to be effective. Partitioning the temporal domain into 2 blocks yielded better results than 4, potentially due to the total number of channels. Further improvements may be achieved by incorporating a local-global chunking approach as described in [11"}]}