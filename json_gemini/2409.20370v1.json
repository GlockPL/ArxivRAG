{"title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "authors": ["Tengyu Xu", "Eryk Helenowski", "Karthik Abinav Sankararaman", "Di Jin", "Kaiyan Peng", "Eric Han", "Shaoliang Nie", "Chen Zhu", "Hejia Zhang", "Wenxuan Zhou", "Zhouhao Zeng", "Yun He", "Karishma Mandyam", "Arya Talabzadeh", "Madian Khabsa", "Gabriel Cohen", "Yuandong Tian", "Hao Ma", "Sinong Wang", "Han Fang"], "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.\nOur results show that CGPO consistently outperforms other commonly used SoTA RLHF algorithms (such as PPO and DPO) on a wide range of tasks - general chat, STEM questions, instruction following, math, coding and knowledge. In particular, CGPO improves over PPO by 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), 2% in IFEval (Instrcution Following), 2% in both MATH and GSM8K (Math & reasoning), 5% in HumanEval (Coding), and 2% in the ARC challenge (Knowledge). We also observe that PPO is susceptible to severe reward hacking behaviors (it exhibits severe regression in popular coding benchmarks) which can be addressed by CGPO. CGPO represents a breakthrough in RLHF, simultaneously addressing reward-hacking and extreme multi-objective optimization, and thereby advancing the state-of-the-art in aligning general-purpose LLMs.", "sections": [{"title": "1 Introduction", "content": "The emergence of general-purpose Large Language Models (LLMs) has significantly transformed the landscape of natural language processing, demonstrating exceptional capabilities across various expert-level domains (Achiam et al., 2023; Brown et al., 2020; Touvron et al., 2023; Anthropic, 2023; Team et al., 2023; Meta, 2024; Tunstall et al., 2023; Zhu et al., 2023). These models are characterized by their extensive parameterization, enabling them to handle a wide array of tasks using a unified parameter set (Zhao et al., 2018; Liu et al., 2019b,a). Central to this versatility is multi-task learning (MTL) (Caruana, 1997; Crawshaw, 2020), a strategy that involves training a single model on multiple tasks simultaneously. This approach fosters the development of shared representations, which enhances the model's ability to generalize better than those trained on isolated tasks. Although prior studies on MTL have concentrated on the integration and processing of multi-task data during both pre-training and fine-tuning stages (Raffel et al., 2020; Liu et al., 2023; Aghajanyan et al., 2021; Aribandi et al., 2021), the application of the primary LLM alignment method, Reinforcement Learning with Human Preference (RLHF) (Ouyang et al., 2022; Ziegler et al., 2019; Zheng et al., 2023b), has not been thoroughly explored within the MTL context. In previous studies, the implementation of RLHF for multi-task post-training has typically involved a linear combination of multiple reward models within the standard"}, {"title": "2 Preliminaries", "content": "In the RLHF finetuing phase, we typically formulate a Markov Decision Process (MDP) as follows: each prompt is considered as the state $s$, and the entire response is the action $a = [a_0, a_1,\\dots,a_{T-1}]$, where $a_i \\in A$ represents the token at position i and A is the vocabulary set. An LLM policy is defined as $\\pi_w(a_t|a_{t-1}, a_{t-2},\\dots, a_0, s)$, which represents a distribution over A at time step t, conditioned on all previous response history before t and prompt: ${a_{t-1}, a_{t-2},\\cdots, a_0, s}$."}, {"title": "2.1 Supervised Finetuing", "content": "RLHF starts by finetuing a pre-trained LLM using supervised learning on high-quality dataset relevant to the downstream target task(s) (such as dialogue, summarization, reasoning, etc.) to obtain SFT."}, {"title": "2.2 Reward Model Training", "content": "After the supervised fine-tuning stage, we need to develop a reward model to assess the quality of an LLM's output. This will enable us to utilize exploration-based online RL alignment method. We typically use the pairwise preference"}, {"title": "2.3 RL Finetuning", "content": "Given a LLM policy $\\pi_w$ with parameter w, a reward model $r_\\phi(a, s)$ and a prompt set $D_\\rho = {s_i}_i=1^M$, we aim to optimize the policy by maximizing the following RL objective (Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023):\n$\\max_{W} E_{s\\sim D_\\rho, a \\sim \\pi_w}[r(s, a)].$\nWhen solving the problem in eq. (3) we typically initialize $\\pi_w$ with SFT policy $\\pi_{SFT}$ instead of starting from scratch. In previous works a number of online RL method such as proximal policy optimization (PPO) (Schulman et al., 2017),\nreward ranking (RAFT) (Dong et al., 2023) and REINFORCE (Williams, 1992) has been utilized to solve Eq. (3).\nAnother direction of RL finetuing involves reward-free methods, which directly optimize $\\pi_w$ using pre-collected preference data, without the need for a reward model. The rationale behind this approach is to fine-tune the model within a neighborhood of SFT, ensuring that the probability of generating both preferred and less preferred samples aligns with the pre-collected preference dataset. Direct Preference Optimization (DPO) (Rafailov et al., 2024b) is the most widely adopted method in this direction."}, {"title": "3 Limitations in Traditional RLHF", "content": "In this section, we discuss several limitations in the current RLHF pipeline, which are major bottlenecks in the multi-task LLM post-training."}, {"title": "3.1 Limitation of Reward Modelling", "content": "Insufficient capability for fine-grained criteria alignment. Despite being based on a sophisticated LLM, the reward model may struggle to provide accurate alignment guidance (Pan et al., 2022), particularly in tasks requiring fine-grained criteria such as identifying correct answers in math questions and assessing code snippet correctness for coding problems. This limitation, inherent to preference-based learning, necessitates additional support to enhance the reward model's effectiveness in handling these specific requirements.\nProxy nature in coarse-grained preference setting. Reward hacking can occur even in coarse-grained settings where the goal is to optimize human preferences, as the reward model, serving as a proxy for true preferences, may contain misspecifications (Gao et al., 2023; Moskovitz et al., 2023). This can lead to the model favoring less-preferred outputs, misdirecting the alignment process. A common mitigation strategy is to include a KL penalty in the RL objective to limit deviation from the initial policy, SFT. However, this approach does not directly address the reward model's imperfections, indicating the need for a more systematic approach to tackle reward hacking."}, {"title": "3.2 Limitation of RLHF Optimizer", "content": "Contradictory optimization objectives. The initial success of LLM hinges on the assumption that human preferences are homogeneous (Bakker et al., 2022), but they actually vary widely (helpfulness, harmlessness, honesty, etc) (Casper et al., 2023; Rame et al., 2024). The current RLHF pipeline trains separate reward models for each task and combines them using linear weights (Ramamurthy et al., 2022; Glaese et al., 2022; Yuan et al., 2023). However, this approach applies the same weight of rewards to all tasks, which can be suboptimal (e.g., 90% helpfulness + 10% harmlessness may work well for safe scenarios but lead to risky responses in dangerous situations).\nRigid optimization strategy for mutli-tasks alignment. In the standard RLHF pipeline, a uniform RL optimizer setup is typically applied across all tasks (Ouyang et al., 2022). However, this approach may not be optimal since the most effective hyperparameters, including number of generations per-prompt, batch-size, and KL-regularization, often differ between tasks due to unique nature of each task. For example, tasks requiring more exploration typically need a larger number of generations per prompt, whereas other tasks can work well with fewer."}, {"title": "3.3 Motivation", "content": "In multi-task LLM alignment settings, where the goal is to enhance LLM performance across various tasks, the limitations of reward modeling and RLHF optimizers discussed in Section 3 are significant bottlenecks that hinder the RLHF process from effectively improving LLM performance across all tasks. In the following section, we will introduce a novel RLHF framework, Constraint Generative Policy Optimization (CGPO), which addresses all the aforementioned limitations in the most principled manner."}, {"title": "4 Constraint Generative Policy Optimization", "content": "In this section, we first explore how to implement the CGPO framework within the scope of a single task with MoJs, as detailed in Section 4.1. Subsequently, we discuss the implementation of CGPO to manage scenarios involving multiple objectives in Section 4.2 for multi-task learning."}, {"title": "4.1 CGPO in Single Task with Single Objective", "content": "The primary design of CGPO is to integrate multiple constraints to mitigate the issue of reward hacking, which arises from the limited capabilities of reward models. Specifically, in addition to optimizing the accumulated reward model value as shown in eq. (3), we also ensure that the model generation meets several constraints. For example, in mathematical reasoning tasks, we strictly require model generations to provide correct answers. This is essential since the model often fails to solve the problem correctly, yet the reward model might still allocate high values to these incorrect solutions. Another example is in general chat tasks with prompts that are free of harmful intent. We require model generations to consistently respond to user queries. This is crucial because there are instances where the model may refuse to answer, and the reward model might erroneously assign high values to such non-responsive generations. In these cases, purely maximizing the reward model could impair the model's reasoning capability and lead to an overly conservative tendency. By introducing these constraints based on our prior knowledge about the weaknesses of each reward model, we can avoid critical reward hacking patterns effectively.\nWe denote the set of constraints that the LLM generations need to satisfy as ${C_1, C_2, . . ., C_M}$ and the state-action set that satisfies constraint $C_k$ as $E_k$, i.e., $\\Sigma_k = {(s, a) \\in S \\times A \\text{ and } (s, a) \\text{ satisfies requirement of } C_k}$. We define the feasible region as the state-action set that satisfies all constraints as $\\Sigma = \\Sigma_1 \\cap \\Sigma_2 \\cap ... \\cap \\Sigma_M$. In the single task setting, CGPO solves the following constrained problem (Ying et al., 2022; Zhang et al., 2024; Luo et al., 2024; Xu et al., 2021)\n$\\max_{W} E_{s\\sim D_\\rho, a \\sim \\pi_\\omega} [r(s, a)]$\n$s.t. \\quad Prob_{s\\sim D_\\rho, a \\sim \\pi_\\omega} ((s, a) \\in \\Sigma) \\geq 1,$\n$\\quad KL_{s\\sim D_\\rho} (\\pi_w|| \\pi_{ref}) \\leq KL_{max},$\nwhere $\\pi_{ref}$ is the initialization model and $KL_{max}$ is the threshold of KL-divergence, which could vary for different tasks.\nThe high-level framework of CGPO in the multiple-constraints and single-objective setting is illustrated in Algorithm 1. At each iteration, we sample a minibatch from the prompt set D, and then apply the current LLM policy to generate K"}, {"title": "4.1.1 Calibrated Regularized Policy Gradient (CRPG)", "content": "In this section, we discuss our new constraint RLHF optimizer, the Calibrated Regularized Policy Gradient (CRPG), which is a policy gradient-based approach.\nCalibrated Reward. In the traditional RLHF algorithm, the reward model is typically directly incorporated into RL optimizers to progressively refine the policy. However, this method can pose difficulties when the reward model value is not properly calibrated. For preference reward models trained with eq. (2), the reward's accuracy may be proficient in distinguishing between good and bad generations from the same prompt. However, the reward model values between generations from different prompts may not be directly comparable due to potential significant variations in the reward model value range for different prompts. Due to such reasons, standard RLHF algorithms, such as PPO and REINFORCE, could lead to suboptimal performance due to the poor calibration of the reward model (Rita et al., 2024). In CRPG, we introduce a novel and low-cost reward calibration strategy to address this issue.\nWe consider the scenario where each prompt s used in RLHF fine-tuning has a corresponding baseline response \u0101. This condition can be easily satisfied in practice."}, {"title": "4.1.2 Constrained Online Direct Preference Optimization (CODPO)", "content": "Based on Direct Preference Optimization (DPO), a widely used offline RLHF alignment algorithm in the unconstrained setting, we propose a new variant called Constrained Online Direct Preference Optimization (CODPO) to solve the constrained RLHF fine-tuning problem.\nRecall that in DPO (Rafailov et al., 2024b), the optimal policy $\\pi^*$, which aligns with human preferences in the \u03b2-regularized MDP setting, satisfies the following preference model:\n$Pr^*(a_p > a_n) = \\frac{1}{1 + exp \\bigg( - \\beta \\big( r^{\\phi^*}(s, a_p) - r^{\\phi^*}(s, a_n)\\big)\\bigg)} .$\nGiven a pairwise preference sample pair (s, ap) and (s, an), we update our policy by solving the following problem:\n$\\min_{W} \\mathcal{L}_{DPO}(w) = - E_{(s, a_p, a_n)}[l_{DPO}(\\pi_w, s, a_p, a_n)].$\nwhere $l_{DPO} (\\pi_w, s, a_p, a_n) = log \\sigma \\bigg( \\beta  \\big(  log  \\frac{\\pi_w (s, a_p)}{\\pi_{ref}(S, a_p)} -  log  \\frac{\\pi_w (s, a_n)}{\\pi_{ref}(S, a_n)} \\big)\\bigg)$"}, {"title": "4.1.3 Calibrated Regularized Reward Ranking Finetuning (CRRAFT)", "content": "In this section, we introduce another constrained RLHF policy optimizers that we proposed: Calibrated Regularized Reward Ranking Finetuning (CRRAFT), which is built upon the RAFT.\nIn the original RAFT algorithm (Dong et al., 2023), each round involves generating multiple responses from a prompt using the current policy model, denoted as ${a_1, a_2,..., a_k} \\sim \\pi_{\\omega_t}(\u00b7 | S_{t,i})$. A reward model r is then utilized to select the response with the highest reward model score, i.e., $a^* = \\underset{k \\in [K]}{\\text{argmax}} r_{pair}(S_{t,i}, a_k)$ (note that whether a calibrated reward is used or not does not affect the reward ranking result). Subsequently, an one-step SFT update is performed to maximize the likelihood of this generated sample $(S_{t,i}, a^*)$. The policy model is iteratively updated to improve its alignment with the reward model $r_{pair}$ as follow\n$\\omega_{t+1} = \\omega_t + \\frac{1}{n} \\sum_{j=1}^n \\nabla_{\\omega_t}  log(\\pi_{\\omega_t} (S_{t,i}, a^*)).$\nIn the multi-constraint setting, we make the following two changes on top of RAFT to develop our CRRAFT optimizer:"}, {"title": "4.1.4 Judges in CGPO", "content": "The key step in implementing multi-constraint CGPO optimizers, as outlined in Section 4.1.1 and Section 4.1.3, is to determine whether a generation (s, a) satisfies a constraint or not. This determination allows us to split generated samples into positive (X+) and negative (X-) groups given the label y predicted by each constraint judge Jh, i.e.,\n$J_h(s, a) = y \\in {0, 1}, \\text{ where } 1 \\le h \\le M,$\nand then apply our customized constraint RLHF optimizers based on that classification. In CGPO, we have developed and integrated the following two types of constraint judge modules to assess whether a generation satisfies a constraint:"}, {"title": "4.2 CGPO in Multi-Taks with Multi-Objectives", "content": "In the multi-tasks environment, CGPO utilizes customized combinations of \"reward models + MoJs + optimizers\" to provide alignment guidance tailored to each task. This approach is designed to better accommodate the specific nature of each problem, thereby enable CGPO to have better chance to achieve optimal alignment outcomes. Figure 2 provides an end-to-end illustration of how the CGPO pipeline functions in the multi-tasks setting. The entire CGPO pipeline has the following two core components: multi-objective reward modeling and multi-experts alignment."}, {"title": "5 Experiments", "content": "In this section, we outline the specifics of our experimental setup designed for multi-task alignment under conditions of extreme multi-constraints and multiple objectives. Specifically, we focus on fine-tuning a LLM to achieve alignment across the following five tasks:"}, {"title": "6.1 CGPO Training Setup", "content": "In this section, we will show how we implment the CGPO in the RLHF finetuning stage.\nRLHF warm-up. Unlike previous studies Ouyang et al. (2022); Achiam et al. (2023), which directly employ the SFT model as the initial point for RLHF, our approach introduces a \"warm-up\" phase. This phase begins with a model that has undergone preliminary fine-tuning through a few steps of DPO, starting from the SFT model. The rationale behind this strategy is that even if DPO and the reward model utilize the same preference training dataset, initiating online RLHF directly from the SFT model and performing policy optimization with the reward model may not be able to explicitly exploit the high-quality preference data, potentially leading to suboptimal performance enhancements. By initiating RLHF with a model already refined by DPO to a certain degree, we can fully harness the advantages of the preference dataset, thereby providing a better starting point for RLHF with improved performance gains and more stable optimization performance.\nIn our experiments, we employ all reward model training datasets in Section 5.2 to conduct DPO training using the SFT model as described in Section 5.1. The warm-up model is developed through 3,000 update steps. As we will show in Section 6.2, CGPO initiated from the warm-up model significantly outperforms that started from the SFT model.\nTraining recipe: We begin the RLHF finetuning process using the warm-up model. We incorporated the reward models and MoJs, developed in Sections 5.2 and 5.3 respectively, into the CGPO framework to facilitate the RLHF finetuning process. Table 1 shows the treatment we applied for each task."}, {"title": "6.2 Experimental Results", "content": "In this section, we present the main results of our experiments. In Section 6.2.1, we highlight the superior performance of CGPO compared to baseline RLHF methods across various benchmarks. We present ablation studies in Section"}, {"title": "6.2.2 Effectiveness of Mixture of Judges", "content": "In this section, we explore the significance of incorporating MoJs within the CGPO framework. We conduct an ablation study by eliminating all MoJs from CGPO, utilizing the CRPG optimizer, while keeping all other variables constant, and then proceed to rerun the RLHF finetuning for 600 steps. Figure 4 presents a comparative analysis of CGPO"}, {"title": "6.2.3 Impact of RLHF Warm-up", "content": "In this section, we discuss the importance of introducing the RLHF warm-up stage. We consider CGPO with CRPG optimizer, and rerun the experiment in Section 6.2.1 but switch the starting point with SFT model. Addtionally, we add one more ablation by starting from the DPO baseline that has been extensively optimized, which has significantly better performance across all benchmarks than the DPO warm-up model (Table 2).\nMonitoring GPT-based helpfulness evaluations like AlpacaEval-2 and Arena-Hard during training is costly. To efficiently assess the effectiveness of the RLHF warm-up stage from the helpfulness perspective, we implement a cost-effective benchmark. We collect prompts from user-LLM interactions (e.g., LMSys-1M) and generate multiple responses using the LIama3.0 70B model. These responses are ranked by a powerful LLM, and the highest and lowest-ranked responses are used to create preference pairs for training a reward model (RM). This RM evaluates helpfulness based on its average score on its training prompts. Although this RM may overfit this prompt set, it remains a valid measure of helpfulness since our finetuning process does not depend on this specific prompt set."}, {"title": "7 Related Works", "content": "RLHF in MTL. Reinforcement Learning with Human Feedback (RLHF) is designed to align language models with human preferences and has become a crucial component of the fine-tuning pipeline for Large Language Models (LLMs) (Stiennon et al., 2020; Ouyang et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bi et al., 2024; Bai et al., 2022). The majority work of RLHF focus optimizing a single reward models (Ouyang et al., 2022; Gao et al., 2023; Dong et al., 2023; Ethayarajh et al., 2023). The exploration of RLHF in the MTL setting remains relatively underexplored. The most commonly adopted approach involves optimizing a weighted sum of several reward models, where each model captures the interests of different tasks (Ramamurthy et al., 2022; Glaese et al., 2022; Yuan et al., 2023; Bakker et al., 2022; Wu et al., 2024). However, a major limitation of this approach is that key information from each individual reward model can be lost through linear combination, particularly when conflicting task goals exist. This can lead to suboptimal performance for each individual task. Additionally, each individual reward model typically requires different treatments (regularization, early stopping, etc) due to their unique properties, thus applying a uniform treatment for a composite reward model can further impair optimization performance across tasks (Moskovitz et al., 2023). Another research direction involves fine-tuning a separate LLM model for each task, followed by linear interpolation of the LLM weights across all learned models to produce a single model that excels in multiple tasks (Rame et al., 2024). However, this method remains computationally expensive and unstable due to the high cost and variability inherent in a single RLHF process (Hu et al., 2023; Rafailov et al., 2024b). (Yang et al., 2024) Proposed to use in-context reward model to manage multiple reward, but introduce additonal cost during inference time. Unlike the approaches mentioned above, CGPO introduces a customized reward model recipe and an RLHF optimizer tailored for each specific task. This method is not only as efficient as the conventional RLHF pipeline, but it also preserves all information within each reward model, thereby optimizing alignment for each task to the fullest extent.\nReward Hacking Mitigation. Compaired with traditional RL, where the reward is typically well-defined and the goal is to maximize it (Sutton and Barto, 2018), RLHF introduces a unique challenge known as \"reward hacking.\" This issue arises because the reward model serves as a proxy for actual human preferences. Over-optimization of the reward model can adversely impact the performance of the language model (Gao et al., 2023; Moskovitz et al., 2023; Stiennon et al.,"}, {"title": "8 Conclusion", "content": "In this paper, we introduced the CGPO framework to address key challenges in multi-task learning for LLM post-training with RLHF. The CGPO framework effectively mitigates issues such as inhomogeneous reward hacking and conflicting task goals through a novel primal-type multi-constraint RL method and a tailored multi-objective optimization strategy. We demonstrate the effectiveness of CGPO in a scenario where we need to handle five tasks with three reward models and seven constraints, marking the first application of RLHF in multi-task learning for general-purpose LLMs. Our experiments show that CGPO achieves significantly better metric gains for all tasks compared to the baseline RLHF methods. Moving forward, it is promising to explore more automated ways to adapt the gradient weights from different tasks to further reduce the hyperparameter burden and advance the Pareto frontier (Sener and Koltun, 2018)."}, {"title": "Appendix", "content": null}, {"title": "A CGPO Training Set", "content": "The detail of of our training dataset is provide in Table 3. Note that in our experiment we adopt the instruction finetuing format, in which the prompt is wrapped as \"[INST] {prompt} [\\INST]\":\nSynthetic IF dataset. Inspired by Zhou et al. (2023), we consider synthetic prompts that require LLM generation to satisfy one or more closed-form instructions, which can be verified exactly. We identify 23 types of closed-form instructions for generation and use Llama 3.0 70B instruct model to create synthetic prompts that address a specific topic and also require these closed-form instructions. We create a template to enable LIama 3.0 70B instruct model to generate all prompts. The prompt template that we input into LIama 3.0 70B instruct model to generate synthetic instruction-following prompts is provided as follows:\nPrompt Template =\n\"You are a helpful AI assistant. You are given a TOPIC and a FORMAT REQUIREMENT,\nand you are expected to generate a PROMPT that is on the given TOPIC and specify the\ngiven FORMAT REQUIREMENT that the corresponding answer should follow. Here are many\nexamples that you can learn from:\nTOPIC: Travel\nFORMAT REQUIREMENT: In your entire response, refrain from the use of any commas\nPROMPT: I am planning a trip to Japan, and I would like thee to write an itinerary\nfor my journey in a Shakespearean style. You are not allowed to use any commas in\nyour response.\nTOPIC: Aerospace engineering\nFORMAT REQUIREMENT: In your entire response, refrain from the use of any commas and\nGive two different responses. Responses and only responses should be separated by 6\nasterisk symbols: ******\nPROMPT: Write two jokes about rockets. Do not contain commas in your response.\nSeparate the two jokes with 6 asterisk symbols: ******\nTOPIC: History\nFORMAT REQUIREMENT: Entire output should be wrapped in JSON format\nPROMPT: What is the history of NYC prospect park? Please wrap your entire answer in\nJSON format.\nTOPIC: Video game\nFORMAT REQUIREMENT: Highlight at least 2 sections in your answer with markdown, i.e.\n*highlighted section* and Answer with at least 40 sentences\nPROMPT: Can you write a poem about the pros and cons of playing a lot of video games?\nPlease make sure it's at least 40 sentences long (don't forget to add punctuations).\nYou must highlight at least sections in your response, like *highlighted phrase*.\nTOPIC: Movie"}, {"title": "BCGPO Constraint Judge", "content": "In this section, we will discuss in detail about how we build MoJs in CGPO."}, {"title": "B.1 Rule-based Constraint Judge", "content": "Math constraint judge. As illustrated in Table 4, for the math prompt sets MATH, GSM8K, and Aqua Math, we explicitly require the model to provide the final answer in a specified format, which can be easily extracted. When implementing the math constraint judge, we extract the LLM's answer by examining the final sentence and comparing it with the ground truth answer in the metadata. There are instances where the model correctly answers the question but fails to provide the answer in the correct format. In such cases, the math constraint judge will indicate that this generation violates the constraint. Although this is a false negative, using CGPO to encourage the model to avoid such patterns can implicitly help improve the model's ability to follow instructions.\nCoding constraint judge. Our coding constraint judge examines the coding block in LLM's response to extract the code snippet. It then runs the snippet through all the unit tests provided in the metadata to determine if it passes each test. Similar to the math constraint, false negatives can occur if LLM's solution is not formatted correctly. Implementing CGPO to discourage such patterns could enhance the model's ability to follow instructions accurately.\nInstruction following constraint judge. The instruction-following constraint judge begins by reading the metadata to understand the specific rules that LLM's output must adhere to. Then, we employ string-matching based logic to determine whether LLM's generation complies with all the specified rules."}, {"title": "B.2 LLM-based Constraint Judge", "content": "The LLM classifier constraint judge utilizes an additional LLM to assess whether the output from our training LLM adheres to a specific predefined criterion. We design the input for this judge using a prompt template that arranges the LLM's response alongside other essential contexts. Within this template, we specify both a negative token and a positive token. The negative token indicates that the LLM's response breaches the constraint, while the positive token signifies compliance. We explicitly direct the judge to issue either the positive or negative token based on their assessment. To minimize the randomness in the judgment process, we do not rely solely on the LLM to generate a token and then check its correspondence to the negative or positive token. Instead, we directly examine the softmax probabilities of the negative and positive tokens. If the probability of the negative token is higher, we conclude that the LLM's response violates the constraint, and vice versa. Table 5 presents the template along with the negative and positive tokens for the LLM classifiers in our experiment.\nFalse refusal constraint judge. We utilize the Llama 3.0 8b pretrained model as a foundation and fine-tune an LLM classifier specifically aimed at identifying refusal patterns in LLM responses. The training data is formatted as follows: \"[INST] {LLM response} [\\INST] judgment\", where \"judgment\" is True if the LLM response indicates refusal, and False otherwise. During the inference phase of deploying this constraint judge, we also encapsulate the generated responses from the training LLM within \"[INST] ... [\\INST]\" and use that as the input for the judge.\nFactuality constraint judge. We employ the Llama 3.0 70b instruct model directly as the factuality constraint judge. Recall that for prompts associated with deterministic factuality, we include the ground truth answer in the metadata. When deploying this constraint judge, we use the template as illustrated in Table 5, incorporating the prompt, ground truth answer, and the LLM response into the template to serve as inputs for the judge.\nSafety constraint judge. We utilize LIamaGuard2 Team (2024), which is fine-tuned from the Llama 3.0 8b pretrained model. We reuse the template as introduced in the LIamaGuard2 paper Team (2024), where we incorporate pre-defined safety guidelines and full completions into the prompt template to serve as inputs for the judge."}, {"title": "C Evaluation Benchmarks", "content": "One example prompt of the MBPP evaluation set:\nYou are an expert Python programmer, and here is your task:\nWrite a function to sort a given matrix in ascending order according to the sum of its\nrows.\nYour code should pass the following tests:"}, {"title": "D Reward Hacking Examples", "content": "We provide examples of reward hacking across various tasks in Table 6. Here, the reward model we trained in Section 5.2 assigns higher values to low quality outputs than to high quality ones. Note that we report the raw reward value in Table 6 instead of the calibrated one."}]}