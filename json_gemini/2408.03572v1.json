{"title": "2D-OOB: Attributing Data Contribution through Joint Valuation Framework", "authors": ["Yifan Sun", "Jingyan Shen", "Yongchan Kwon"], "abstract": "Data valuation has emerged as a powerful framework to quantify the contribution of each datum to the training of a particular machine learning model. However, it is crucial to recognize that the quality of various cells within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar valuation assigned by existing methods blurs the distinction between noisy and clean cells of a data point, thereby compromising the interpretability of the valuation. In this paper, we propose 2D-00B, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples, as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-00B achieves state-of-the-art performance across multiple use cases, while being exponentially faster. 2D-00B excels in detecting and rectifying fine-grained outliers at the cell level, as well as localizing backdoor triggers in data poisoning attacks.", "sections": [{"title": "Introduction", "content": "From customer behavior prediction and medical image analysis to autonomous driving and policy making, machine learning (ML) systems process ever increasing amounts of data. In such data-rich regimes, a fraction of the samples is often noisy, incorrect annotations are likely to occur, and uniform data quality standards become difficult to enforce. To address these challenges, data valuation emerges as a research field receiving increasing attention, focusing on properly assessing the contribution of each datum to ML training [12]. These methods have proven useful in identifying low-quality samples that can be detrimental to model performance, as well as selecting subsets of data that are representative of enhanced model performance [23, 48, 27]. Furthermore, they are widely applicable in data marketplace for fair revenue allocation and incentive design [51, 45, 40].\nNevertheless, existing data valuation methods assign a scalar score to each datum, thereby failing to account for the varied roles of individual cells. This leaves the valuation rationale unclear and can be unsatisfactory and sub-optimal in various practical scenarios. Firstly, whenever a score is assigned to a data point by a particular data valuation method, it is crucial to understand the underlying justifications to ensure transparency and reliability, especially in high-stakes decision making [39]. Secondly, it is important to recognize the fact that even if a data point is of low quality, it is rarely the case that all the cells within this data point are noisy [37, 26, 43]. The absence of detailed insights into how individual cells contribute to ML training inevitably leads to discarding entire data points. This"}, {"title": "Preliminaries", "content": "Notations Throughout this paper, we focus on supervised learning settings. For $d \\in \\mathbb{N}$, we denote an input space and an output space by $\\mathcal{X} \\subseteq \\mathbb{R}^d$ and $\\mathcal{Y} \\in \\mathbb{R}$, respectively. We denote a training dataset with $n$ data points by $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ where $(x_i, y_i)$ is the $i$-th pair of the input covariates $x_i \\in \\mathcal{X}$ and its output label $y_i \\in \\mathcal{Y}$. For an event $A$, an indicator function $\\mathbb{1}(A)$ is 1 if $A$ is true, otherwise 0. For $j \\in \\mathbb{N}$, we set $[j] := \\{1, ..., j\\}$. For a set $S$, we denote its power set by $2^S$ and its cardinality by $|S|$.\nDataShapley The primary goal of data valuation is to quantify the contribution of individual data points to a model's performance. Leveraging the Shapley value in cooperative game theory [38], DataShapley [12] measures the average change in a utility function $U : 2^{\\mathcal{D}} \\rightarrow \\mathbb{R}$ when a data point is removed. For $i \\in [n]$, DataShapley of $i$-th datum is defined as follows.\n$$\n\\Phi^{\\text{Shap}}_i := {1 \\over n} \\sum_{k=1}^n {1 \\over {n-1 \\choose k-1}} \\sum_{S \\subset \\mathcal{D}^{(i)}, |S|=k-1} [U(S \\cup \\{(x_i, y_i)\\}) - U(S)]\n$$\nwhere $\\mathcal{D}^{(i)} := \\{S \\subset \\mathcal{D}\\{(x_i, y_i)\\} \\subset S, |S| = k - 1\\}$. DataShapley $\\Phi^{\\text{Shap}}_i$ in (1) considers every set $S \\subset \\mathcal{D}^{(i)}$ and computes the average difference in utility $U(S \\cup \\{(x_i, y_i)\\}) - U(S)$. It characterizes the impact of a data point, but its computation requires evaluating $U$ for all possible subsets of $\\mathcal{D}$, rendering precise calculations infeasible. Many efficient computation algorithms have been studied"}, {"title": "Attributing Data Contribution through Joint Valuation Framework", "content": "Data valuation quantifies desiderata of data points, however, it does not describe what features contribute and how much to those specific data values. For instance, in anomaly detection tasks, data valuation methods can be deployed to detect anomalous data points, but they do not explain why they are abnormal, which is not generally desirable in practice. To address this challenge, we consider a joint valuation framework and assess a cell score for each feature of a data point. Here, a cell score is designed to quantify how a feature affects the value of an individual data point, attributing a data value to features.\nTo the best of the author's knowledge, Liu et al. [29] first consider a concept of the joint valuation in literature and introduce 2D-Shapley to quantitatively interpret DataShapley. To this end, we denote a 2D utility function by $u : [n] \\times [d] \\rightarrow \\mathbb{R}$, which takes as input a subset of data points $S \\subseteq [n]$ and a subset of features $F \\subseteq [d]$, and measure the utility of a fragment of the given dataset consisting of cells $\\{(i, j)\\}_{i\\in S, j \\in F}$, where a tuple $(i, j)$ denotes a cell at the $i$-th datum and the $j$-th column. Then, 2D-Shapley is defined as\n$$\n\\Phi^{\\text{2D-Shap}}_{ij} := {1 \\over nd} \\sum_{k=1}^n \\sum_{l=1}^d {1 \\over {{n-1} \\choose k-1} {{d-1} \\choose l-1}} \\sum_{(S, F) \\in \\mathcal{D}_{k,l}^{(i,j)}} M^{ij}_{kl}(S, F)\n$$\nwhere $\\mathcal{D}_{k,l}^{(i,j)} := \\{(S, F)|S \\subseteq [n]\\{i\\}, F \\subseteq [d]\\{j\\}, |S| = k - 1, |F| = l - 1\\}$ and\n$M^{ij}_{kl}(S, F) = u(S \\cup \\{i\\}, F \\cup \\{j\\}) + u(S, F) - u(S \\cup \\{i\\}, F) - u(S, F \\cup \\{j\\})$.\nThe function $M^{ij}_{kl}$ allows us to quantify how much removing a specific cell at $(i, j)$ from a given set $(S \\cup \\{i\\}, F \\cup \\{j\\})$ affects the overall utility, and 2D-Shapley $\\Phi^{\\text{2D-Shap}}_{ij}$ evaluates the average $M^{ij}_{kl}$ across all possible data fragments $(S, F) \\in \\mathcal{D}_{k,l}^{(i,j)}$."}, {"title": "2D-OOB: an efficient joint valuation framework", "content": "Our idea builds upon the subset bagging model [14], which is well recognized as an earlier version of Breiman's random forest model [4]. A key distinction from a standard bagging model is that a weak learner in a subset bagging model is trained on a randomly selected subset of features. For $b \\in [B]$, we denote the $b$-th random feature subset by $S_b \\subseteq [d]$. Then, the $b$-th weak learner of a subset bagging model is given as follows.\n$$\nf_b := \\text{argmin}_f \\sum_{i=1}^n w_{bi} l(y_i, f(x_{i, S_b})),\n$$\nwhere $x_{i, S_b}$ is a subvector of $x_i$ that only takes elements in a subset $S_b$. This difference enables us to assess the impact of which features are more influential: if $S_b$ includes a helpful (or detrimental) feature, we can expect the out-of-bag prediction $f(x_{i, S_b})$ to be good (or poor). We formalize this intuition and propose 2D-00B. For $i \\in [n]$, $j \\in [d]$ and $\\{(w_b, S_b, f_b)\\}_{b=1}^B$, the 2D-00B for the $j$-th cell of the $i$-th data point is defined as follows,\n$$\n\\Phi_{ij}^{\\text{2D-OOB}} := {\\sum_{b=1}^B \\mathbb{1}(w_{bi} = 0, j \\in S_b) T(y_i, f_b(x_{i, S_b})) \\over \\sum_{b=1}^B \\mathbb{1}(w_{bi} = 0, j \\in S_b)},\n$$\nwhere $T : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ is a utility function that scores the performance of the weak learner $f_b(x_{i, S_b})$ on the $i$-th datum $(x_i, y_i)$. Specifically, for binary or multi-class classification problems, we can adopt $T(y_i, f_b(x_{i, S_b})) = \\mathbb{1}(y_i = f_b(x_{i, S_b}))$. In this case, 2D-00B measures the average accuracy score of out-of-bag predictions (specifically, when the $i$-th data point is out-of-bag) if a cell $j$ is used in training $f_b$. For regression problems, we can use the negative squared error loss function, defined as $T(y_i, f_b(x_{i, S_b})) = -(y_i - f_b(x_{i, S_b}))^2$. In practice, $\\mathcal{X}$ could also be incorporated into $T$ to suit the specific use case.\nWhile Data-00B in (2) aims to assess the impact of the $i$-th datum, 2D-00B in (4) provides interpretable insights by evaluating the data point with various combinations of features, revealing which cells are influential to model performance. Leveraging subset bagging scheme, 2D-00B requires a single training of the bagging model, and thus it is computational efficiency."}, {"title": "Connection to Data-OOB", "content": "We now present interpretable expressions of how 2D-00B connects to Data-00B in the following proposition. To begin with, we denote a set of subsets of $[d]$ by $\\mathcal{S} := \\{S \\subseteq [d]\\}$. With $\\{(w_b, f_b)\\}_{b=1}^B$, we define the $i$-th Data-00B when a particular subset $S$ is used as follows and denote it by $\\Phi^{\\text{OOB}}(S)$.\n$$\n\\Phi^{\\text{OOB}}_i(S) := {\\sum_{b=1}^B \\mathbb{1}(w_{bi} = 0) T(y_i, f_b(x_{i, S})) \\over \\sum_{b=1}^B \\mathbb{1}(w_{bi} = 0)}\n$$\nProposition 3.1. For all $i \\in [n]$ and $j \\in [d]$, $\\Phi_{ij}^{\\text{2D-OOB}}$ can be expressed as follows.\n$$\n\\Phi_{ij}^{\\text{2D-OOB}} = \\mathbb{E}_{F_S} [\\Phi_i^{\\text{OOB}}(S) | j \\in S],\n$$\nwhere $F_S$ is an empirical distribution with respect to $S$ induced by the sampling process."}, {"title": "Experiments", "content": "In this section, we empirically show the effectiveness of 2D-00B across multiple use cases of the joint valuation: cell-level outlier detection, cell fixation, and backdoor trigger detection. As a summary, 2D-00B can precisely identify anomalous cells that should be prioritized for examination and subsequent fixation to improve model performance. In the context of backdoor trigger detection, 2D-00B demonstrates its efficacy by accurately identifying different types of triggers within poisoned data, showcasing its proficiency in detecting non-random, targeted anomalies. Our method also exhibits high computational efficiency through run-time comparison.\nThroughout all of our experiments, 2D-00B uses a subset bagging model with $B = 1000$ decision trees. We randomly select a fixed ratio of features to build each decision tree. Unless otherwise specified, we utilize half of the features for each weak learner and set $T(y_i, f(x_{i, S_b})) = \\mathbb{1}(y_i = f(x_{i, S_b}))$. The run time is measured on a single Intel Xeon Gold 6226 2.9 Ghz CPU processor."}, {"title": "Cell-level outlier detection", "content": "Experimental setting In practical situations, even when dealing with abnormal data points, it is not always the case that all cells are noisy [37, 29, 21]. To simulate more realistic settings, we introduce noise to certain cells in the following two-step process: First, we randomly select 20% rows for each dataset. We then select 20% columns uniformly at random, allowing each selected row to have a different set of perturbed cells. We inject noises sampled from the low-probability region into these cells, following Du et al. [8] and Liu et al. [29]. Details on the outlier injection process can be found in Appendix A.3.\nWe use 12 publicly accessible binary classification datasets from OpenML, encompassing a range of both low and high-dimensional datasets, which have been widely used in the literature [12, 22, 23]. Details on these datasets are presented in Appendix A.1. For each dataset, 1000 and 3000 data points are randomly sampled for training and test datasets, respectively. For the baseline method, we consider 2D-KNN, a fast and performant variant of 2D-Shapley [29]. We incorporate a distance regularization term in the utility function $T$ for enhanced performance."}, {"title": "Cell fixation experiment", "content": "Experimental setting A naive strategy to handle cell-level outliers is to eliminate data points that contain outliers. This method, however, risks substantial data loss, particularly when outliers are scattered and data points are costly to collect. We instead consider a cell fixation experiment, where we assume that the ground-truth annotations of outlier cells can be restored with external expert knowledge. At each step, we \"fix\" a certain number of cells by substituting them with their ground-truth annotations, prioritizing cells that have the lowest valuations. Then we fit a logistic model and evaluate the model's performance with a test set of 3000 samples. It is important to note that correcting normal cells has no effect, whereas fixing outlier cells is expected to enhance the model's performance. We adopt the same datasets and implementations as in Section 4.1.\nResults Figure 3 illustrates the anticipated trend in the performance of 2D-00B, validating our method's capability to accurately identify and prioritize the most impactful outliers for correction. As cells with the lowest valuations are progressively fixed, 2D-00B demonstrates a consistent improvement in model accuracy. In contrast, when applying the same procedure with 2D-KNN, such notable performance enhancements are not observed.\nAdditionally, we investigate a scenario where ground-truth annotations remain unavailable. We adopt the setup from Liu et al. [29], where we replace the outlier cells with the average of other cells in the same feature column. 2D-00B uniformly demonstrates significant superiority over its counterparts. Results are provided in Appendix B.2."}, {"title": "Backdoor trigger detection", "content": "A common strategy of data poisoning attacks involves inserting a predefined trigger (e.g., a specific pixel pattern in an image) into a few training data [13, 5, 28]. These malicious manipulations can be challenging to detect as they only infect targeted samples. Even when poisoned data are present, it could be difficult to discern the cause of attacks since manually reviewing the images is expensive and time-consuming. In this experiment, we introduce a novel joint valuation task: detecting backdoor triggers in data poisoning attacks. Distinct from random outliers investigated previously, such cell contamination is targeted and deliberate.\nWe consider two popular backdoor attack algorithms: BadNets [13] and Trojan Attack [28]. The poisoned samples, relabeled as the adversarial target class, are mixed up with the clean data in the training process. As a result, the model is trained to incorrectly treat the trigger as a main feature of the poisoned samples. At the test time, those inputs containing the trigger will be misclassified to the target class. In this context, our goal is to effectively pinpoint the triggers by recognizing them as influential features through our joint valuation framework.\nExperimental setting We select 5 pairs of CIFAR-10 classes. For each pair, we designate one as the target attack class and the other as the source class. The training dataset comprises 1000 images. For each attack, we contaminate 15% of the training samples from the source class and relabel them to the target class. Two types of attack triggers are implemented: Trojan square and BadNets square"}, {"title": "Ablation study", "content": "We conduct ablation studies on the cell-level outlier detection task, as outlined in Section 4.1, to examine the impact of the selection and number of weak learners on 2D-00B estimations.\nSelection of weak learners Although our study primarily employs decision trees as weak learners, it is important to note that 2D-00B is model-agnostic, enabling the use of any class of machine learning models as weak learners. We compare efficacy of decision trees, logistic regression, a single-layer MLP with 64 dimensions, and a two-layer MLP with 64 and 32 dimensions.\nTable 2 presents a comparison of detection AUC across 12 datasets, indicating that 2D-00B is not model-free. The selection of weak learners slightly affects the valuation results, with more complex models generally yielding better performance. Nonetheless, all variations of 2D-00B outperform 2D-KNN, highlighting the significant advantages of the 2D-00B approach.\nThe number of weak learners Increasing the number of weak learners allows for a greater number of data-feature subset pairs to be explored, potentially leading to more accurate estimates. However, we empirically observe that beyond a certain threshold, adding extra weak learners does not substantially enhance performance, indicating convergence of the estimation in Appendix B.4. As a summary, we vary the number of weak learners $B \\in \\{500, 1000, 3000\\}$ and compare the cell-level outlier detection performance. Typically, when the number of weak learners is 1000, i.e., $B = 1000$, it is sufficient to achieve converged estimates across different datasets.\nLastly, we present additional ablation study results for other key hyperparameters in Appendix B.4. Apart from the experiments discussed above, we showcase that marginalization of 2D-00B can either match or surpass state-of-the-art data valuation methods on standard benchmarks in Appendix D."}, {"title": "Related work", "content": "Data contribution estimation In addition to the marginal contribution-based methods discussed in Section 2, many other approaches are emerging in the area of data valuation. Just et al. [18] develop a non-conventional class-wise Wasserstein distance between the training and validation sets and use the gradient information to evaluate each data point. Wu et al. [47] extend data valuation to deep neural networks, introducing a training-free data valuation framework based on neural tangent kernel theory. Yoon et al. [48] leverage reinforcement learning techniques to automatically learn data valuation scores by training a regression model. However, all these data valuation methods do not assign importance scores to cells, whereas our method provides additional insights into how individual cells contribute to the data valuations.\nFeature attribution Feature attribution is a pivotal research domain in explainable machine learning that primarily aims to provide insights into how individual features influence model predictions. Various effective methods have been proposed, including SHAP-based explanation [30, 31, 24, 7, 6], counterfactual explanation [44, 17, 36, 32, 33], and backpropagation-based explanation [1, 2, 42, 41, 49]. Among these methods, the SHAP-based explanation stands out as the most widely adopted approach, utilizing cooperative game theory principles to compute the Shapley value [38]. While feature attribution offers a potential method to attribute data valuation scores across individual cells, our empirical experiments in Appendix B.1 reveal that this two-stage scheme falls short in efficacy compared to our proposed joint valuation paradigm, which integrates data valuation and feature attribution in a simultaneous process."}, {"title": "Conclusion", "content": "We propose 2D-00B, an efficient joint valuation framework that assigns a score to each cell in a dataset, thereby facilitating a finer attribution of data contribution and enabling a deeper understanding of datasets. Through comprehensive experiments, we show that 2D-00B is computationally efficient and competitive over state-of-the-art methods in both joint valuation tasks.\nLimitation and future work While our study primarily explores random forest models applied to tabular datasets and simple image datasets, the potential application of neural network models within the 2D-00B framework for more complex vision and language tasks presents a promising avenue for future investigation. For instance, in text datasets, tokens or words can be treated as cells. 2D-00B can be easily integrated into any bagging training scheme that uses language models.\nOverall, we believe that our work will inspire further exploration in the field of joint valuation, with the broader goal of improving the transparency and interpretability of machine learning, as well as developing an equitable incentive mechanism for data sharing."}, {"title": "Supplementary Materials", "content": "In the supplementary materials, we provide implementation details, additional experimental results, rigorous formalized proofs and data valuation experiment results. Code repository can be found at https://github.com/YifanSun99/2d-oob."}, {"title": "Implementation details", "content": "Tabular datasets We use 12 binary classification datasets obtained from OpenML [11]. A summary of all the datasets is provided in Table 3. These datasets are used in Section 4.1, 4.2, and Appendix D.\nFor each dataset, we first employ a standard normalization procedure, where each feature is normalized to have zero mean and unit standard deviation. After preprocessing, we randomly partition a subset of the data into two non-overlapping sets: a training dataset and a test dataset, which consists of 1000 and 3000 samples respectively. The training dataset is used to obtain the joint (or marginal) valuation for each cell (or data point). The test dataset is exclusively used for cell fixation (or point removal) experiments when evaluating the test accuracy. Note that for methods that need a validation dataset such as KNNShapley and DataShapley, we additionally sample a separate validation dataset (disjoint from training dataset and test dataset) to evaluate the utility function. The size of the validation dataset is set to 10% of the training sample size.\nImage datasets We create datasets by pairing CIFAR-10 classes, each pair consisting of a target attack class and a source class. The training and test dataset comprises 1000 and 2000 samples respectively. The size of the validation dataset is set to 10% of the training sample size. To manage the computational challenges posed by the baseline method, we employ the super-pixel technique to transform the (32,32,3) image into a 256-dimensional vector. Specifically, we first average the pixel values across three channels for each pixel. Then, we partition these transformed images into equally sized 2 \u00d7 2 grids. In each grid, we use average pooling to reduce the pixel values to a single cell value. These cell values are then arranged into a flattened input vector. We annotate a cell as poisoned if at least 25% of its corresponding grid area contains the trigger."}, {"title": "Implementation details for different methods", "content": "2D-00B 2D-00B involves fitting a subset random forest model with $B = 1000$ decision trees based on the package \u201cscikit-learn\". When constructing each decision tree, we fix the feature subset size ratio as 0.5. Ablations on the hyperparameters can be found in Appendix B.4. For Section 4.3 and Appendix D, we simply adopt $T(y_i, f(x_{i, S_b})) = \\mathbb{1}(y_i = f(x_{i, S_b}))$. For Section 4.1 and 4.2, we further calculate the normalized negative $L2$ distance between covariates and the class-specific mean in the bootstrap dataset, denoted as $d_{norm}$. Then we use $T(y_i, f(x_{i, S_b})) = \\mathbb{1}(y_i = f(x_{i, S_b})) + d_{norm}$.\n2D-KNN 2D-KNN employs KNN as a surrogate model to approximate 2D-Shapley. We set the number of nearest neighbors as 10 and the number of permutations as 1000. The hyperparameters are selected based on convergence behavior and we determine the run time until the values converge.\""}, {"title": "Implementation details for cell-level outlier generation", "content": "Following Du et al. [8] and Liu et al. [29], we replace a given cell with the outlier value. Here, the outlier value is randomly generated from the two-sided \"tails\" of the Gaussian distribution with the column mean and standard deviation, where the probability of the two-sided tail area is set to be 1%. 4% (20% \u00d7 20%) of the cells in total are replaced with the corresponding outlier value."}, {"title": "Implementation details for backdoor trigger generation", "content": "Following the prior work [13, 28], we generate the BadNets square and the Trojan square trigger. For BadNets, we adopt the implementation in Nicolae et al. [34]. For Trojan Attack, we use a pretrained ResNet18 model on CIFAR-10 dataset and employ the implementation in Pang et al. [35]. For"}, {"title": "Additional experimental results", "content": "In Section 4.1, we demonstrate that 2D-00B shows promising performance in identifying cell-level outliers. This section further shows that our result is not sensitive to the selection of hyperparameters. Furthermore, ours generally performs better than 2D-KNN in different settings. We also provide additional results for Section 4.2 and 4.3."}, {"title": "Additional results for cell-level outlier detection", "content": "Additional results on multi-class classification datasets We have conducted cell-level outlier detection experiments (as in Section 4.1) on three multi-class classification datasets from the UCI Machine Learning repository [19]. As shown in the table, 2D-00B displays superior detection performance and efficiency.\nAdditional baseline: two-stage attribution Once we obtain the data valuation scores, an alternative solution approach to determining cell-level attributions involves leveraging feature attribution methods such as SHAP [30]. We explore an additional baseline method building upon this idea: initially, Data-00B (or any other data valuation method) is computed for the i-th data point, denoted as $d_i$. Subsequently, TreeSHAP [31] is fitted, using $d_i$ as the target and the concatenation of $x_i$ and $y_i$ (denoted as $x_i \\oplus y_i$) as the predictor. The derived local feature attributions are then interpreted as joint valuation results. We refer to this method as 'two-stage attribution'.\nTable 5 indicates that 2D-00B substantially outperforms its two-stage counterpart. We hypothesize that the superiority of our method stems from integrating data valuation and feature attribution into a cohesive framework. Conversely, the two-stage method treats data valuation and feature"}, {"title": "A noisy setting with more outlier cells", "content": "We consider a more challenging scenario with increased outlier levels, where both the row outlier ratio and column outlier ratio increase from 20% (as in Section 4.1) to 50%. Consequently, this leads to 25% (50% \u00d7 50%) of the cells being replaced with outlier values. We follow the same outlier generation procedure outlined in Appendix A.3. The findings, presented in Table 6, demonstrate that our method maintains a significantly superior performance over 2D-KNN, even under such a noisy setting."}, {"title": "Additional results for cell fixation experiment", "content": "Figure 6 presents the results for the cell fixation experiment on 6 additional datasets. 2D-00B excels in precisely detecting and correcting relevant cell outliers.\nThe scenario without ground-truth knowledge Following [29], we examine a situation where external information on the ground-truth annotations of outlier cells is not accessible. In this scenario, we address these outliers by substituting them with the average of other cells in the same feature column. This procedure starts by addressing cells with the lowest valuations, based on the hypothesis"}, {"title": "Additional results for backdoor trigger experiment", "content": "We provide additional qualitative examples of backdoor trigger detection experiments in Figure 8."}, {"title": "Additional results for ablation study", "content": "We present the results of ablation study on the number of base learners $B$ and feature subset ratio $K/d$. Specifically, we examine the AUC of the detection curve in the cell-level outlier detection experiment (refer to Table 1).\nThe number of base learners $B$ When we increase the number of base learners from 500 to 3000, the detection AUC for each dataset remains unchanged, as shown in Table 7. This indicates that 1000 base learners are sufficient to get an equitable joint valuation.\nFeature subset ratio $K/d$ In addition to 0.50, We test two additional feature subset ratios 0.25 and 0.75. The results in Table 8 suggest that in general, the joint valuation capacity of our method is robust to the choice of feature subset ratio."}, {"title": "Proof of Proposition 3.1", "content": "Proof. For simplicity, we denote $\\Phi^{\\text{OOB}}_i(S)$ as $\\Phi_i(S)$ and $\\Phi_{ij}^{\\text{2D-OOB}}$ as $v_{ij}$ in the proof. With the set of subsets $\\mathcal{S} := \\{S \\subseteq [d]\\}$ and the definition Data-00B $\\Phi_i(S)$ for all $i \\in [n]$, where $S$ is"}, {"title": "Data valuation experiment", "content": "In this section, we show that 2D-00B-data data, the marginalization of 2D-00B, offers an effective approach to data valuation. This serves as the basis of our enhanced performance in joint valuation.\nMarginalization 2D-00B aims to attribute data contribution through cells. Consequently, by summing up 2D-00B over all columns, we can derive data contribution values. For $i \\in [n]$, we define the 2D-00B-data data as follows.\n$$\n\\hat{v}^{\\text{data}}_i := {1 \\over d} \\sum_{j=1}^d \\Phi_{ij}^{\\text{2D-OOB}},\n$$\nProof. Based on definition of 2D-00B-Data, for $i \\in [n]$,\n$$\n\\hat{v}^{\\text{data}}_i = {1 \\over d} \\sum_{j=1}^d \\Phi_{ij}^{\\text{2D-OOB}} = {1 \\over d} \\sum_{j=1}^d {\\sum_{l=1}^L \\alpha_{i, j, l} \\Phi_i^{\\text{OOB}}(S_l)} = {\\sum_{l=1}^L (\\sum_{j=1}^d \\alpha_{i, j, l}) \\Phi_i^{\\text{OOB}}(S_l)}\n$$\nwhere $\\alpha_{i,j,l}$ is defined in Appendix C. We have $\\sum_{l=1} (\\sum_{j=1} \\alpha_{i,j,l}) = \\sum_{j=1} \\sum_{l=1} \\alpha_{i,j,l} = 1$. Denote $P_i(S_l | \\{w_{bi}\\}_{b=1}^B) = \\sum_{j=1} \\alpha_{i,j,l}$, which induces the empirical expectation of Data-00B with respect to $S_l$.\nBased on discussions in Section 3.2, the marginalizations also connect with Data-00B:\nProposition D.1. For all $i \\in [n]$, the marginalizations $\\hat{v}^{\\text{data}}_i$ can be expressed as follows.\n$$\n\\hat{v}^{\\text{data}}_i = \\mathbb{E}_{F_S} [\\Phi_i^{\\text{OOB}}(S)],\n$$\nwhere the notations follow the same definitions as Proposition 3.1."}, {"title": "Experimental setting", "content": "Following the standard protocol in Kwon and Zou [22, 23", "16": "we randomly select 10% of the data points and change its label to the other class. For joint valuation methods, we calculate the valuation of each cell and perform the marginalization over features to obtain the data valuation scores. For the baseline methods, we further incorporate several state-of-the-art data valuation methods including DataShapley [12", "15": "DataBanzhaf [46", "18": "and Data-00B [23"}]}