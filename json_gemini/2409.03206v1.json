{"title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations", "authors": ["Mingze Gao", "Jingyu Liu", "Mingda Li", "Jiangtao Xie", "Qingbin Liu", "Bo Zhao", "Xi Chen", "Hui Xiong"], "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.", "sections": [{"title": "1. Introduction", "content": "By leveraging vast open-source and AI-generated datasets [23, 32, 6], along with the impressive development of large language models such as GPT [1], LLaMA [36], and GLM [9], Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in image comprehension tasks [25, 24, 17, 52]. Given the powerful capabilities of image-pretrained MLLMs, a recently emerging research focus is on transferring these models from single-image tasks to video understanding.\nRecently, various approaches [30, 45, 49] have tended to treat a video as a series of concatenated frames in the spatial dimension, thereby transferring video-related tasks back to image-related tasks. However, these methods face two issues as they treat text and visual tokens as the same modality and fed them into the LLMs as a unified input. Firstly, utilizing LLMs' vanilla attention mechanism to uniformly process all tokens overlooks the distinct interactions between visual tokens within individual video frames and those across different frames. Secondly, it neglects the temporal information inherent in the video input, which is crucial for video understanding tasks. Consequently, the constructed video MLLM fails to effectively summarize the dynamic events occurring within videos, reducing the analysis to single frames as if they were still images. For instance, it fails to adequately capture and detail the complex motion changes of the primary subject in the video, particularly in activities such as dancing or gymnastics. This deficiency ultimately results in inaccurate or 'hallucinatory' responses by the model, as depicted in Figure 1.\nIn this paper, we propose Temporal-Considered (TC) LLaVA, a novel video-language framework designed to address the aforementioned issues. The primary innovation is to enhance the temporal awareness of MLLMs and distinguish the attention interactions between text and video modalities through two core strategies. First, we introduce Temporal-Aware Dual RoPE, which assigns each token an independent position id with the original RoPE to preserve global relative positional relationships, while incorporating temporal-aware RoPE to assign the same position id to visual tokens within the same frame and to encode inter-frame relationships to capture the temporal dynamics of videos, as shown in Figure 1. Additionally, we design three different attention masks to optimize token interaction strategies in attention computation, accounting for the distinct characteristics of visual and text tokens. Finally, we select the Frame-wise Block Causal Attention Mask to replace the original causal attention mask, enhancing interaction between visual tokens within and across frames while preserving the causal reasoning paradigm, making it more suitable for causal language model inference.\nTo verify the effectiveness of our TC-LLaVA, we evaluate the model on extensive video benchmarks, including MSVD [43], MSRVTT [43], ActivityNet [5], TGIF [21], VCGbench [30] and MVbench [20]. Comparing with the latest video MLLMS, TC-LLaVA achieves new state-of-the-art performance on these benchmarks at the same model scales, demonstrating the benefits of enhancing visual token interactions within and across frames, as well as the importance of incorporating temporal information in video analysis."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Attention in Vision and Language Models", "content": "The introduction and evolution of the attention mechanism have significantly enhanced model performance in natural language processing (NLP) and computer vision (CV). The earliest attention mechanism by [3] allowed machine translation models to assign different weights to input sentence parts, improving translation accuracy. [37] introduced the Transformer model, which uses a self-attention mechanism to enable parallel processing and superior long-range dependency modeling, achieving significant results in multiple NLP tasks. To further optimize the attention computation, [33] propose Relative Position Encoding (RPE) to improve the token interaction by introducing extra position information. Recently, Rotary Position Embedding (ROPE) [35] is designed for the interaction limitation of RPE by leveraging complex number rotations. In CV, attention mechanisms have proven effective with models like Non-local Neural Networks by [38] and Vision Transformer (ViT) [8], which first applies the Transformer architecture to image classification tasks. There have also been numerous advancements [28, 41, 27] in attention mechanisms that continually improve the performance of Transformer-based models, enhancing their ability to capture essential features and increasing computational efficiency. Our work continues to delve deeply into improving attention computation in the multimodal domain of video and text, and we propose the TC-Attention method to achieve this goal."}, {"title": "2.2. Video Multimodal Large Language Models", "content": "Video Multimodal Large Language Models (Video MLLMs) operate by aligning modalities and performing instruction fine-tuning on video data, enabling them to generate responses based on user instructions and input video streams. Recently, Video MLLMs have experienced rapid development. One significant milestone in this field is BLIP2 [17], which integrates a frozen vision encoder with a Q-Former to enhance video processing efficiency, demonstrating remarkable zero-shot capabilities in Video Question Answering (VQA) and outperforming existing techniques. Video-ChatGPT [30] introduced video instruction tuning and created a high-quality instructional dataset, setting a new standard for video-based text generation benchmarks. VideoChat [18] employed cross-attention mechanisms to condense visual tokens and align user queries with the dialogue context, enhancing interpretative capabilities. Building on this, VideoChat2 [20] refined the approach with a multi-stage bootstrapping technique focused on modality alignment and instruction tuning, utilizing a robust collection of high-quality video data. Chat-UniVi [13] processes longer videos by introducing a method for compressing tokens in both the spatial and temporal dimensions. LLaMA-VID [22] introduced an innovative dual-token approach that effectively condenses video representations by segregating context and content tokens, allowing for more efficient compression. VideoLLaMA and VideoLLaMA2 [47, 7] enhances video understanding by incorporating audio modality information and utilizing a Spatial-Temporal Convolution (STC) connector. ST-LLM [26] intruduce a dynamic masking strategy into MLLM. PLLaVA [44] explore the Image-pretrained LLaVA into video tasks with simple spatial pooling. In this paper, we introduce TC-LLaVA, which considers the differences in visual token interactions within and across frames, and directly incorporates temporal position into the causal attention computation to enhance the understanding of model."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Preliminary: Introducing Position Embeddings", "content": "While Relative Position Encoding (RPE) [33] incorporates relative positional information into the attention mechanism through a position bias element-addition computation with inter-layer attention map, this approach may limit interaction with attention weights and, consequently, hinder the effective utilization of relative positions. To address this limitation, RoFormer [35] introduces RoPE, a novel method that more effectively incorporates relative positional information by leveraging complex number rotations.\nSpecifically, when computing the attention map, the RoPE (Rotary Positional Encoding) technique introduces the multiplication of Euler's formula $e^{i\\theta}$ to the query and key vectors as a relative position embedding. For instance, when considering the n-th and m-th query and key vectors $q_n$ and $k_m$ in R1\u00d7dhead, RoPE is applied as follows:\n$q_n = q_n e^{in\\theta}, k_m = k_m e^{im\\theta}$.\nThen, the (n,m)-th component of the attention matrix is calculated as:\n$A_{(n,m)} = Re[q_n k_m^*] = Re[q_n k_m e^{i(n-m)\\theta}]$,\nwhere $Re[]$ denotes the real part of a complex number and * denotes the complex conjugate. By multiplying complex rotations $e^{i\\theta n}, e^{i\\theta m}$ depending on token position (n,m), ROPE injects relative positions (n \u2013 m) into the attention matrix in a rotational form. In practical implementation, ROPE [35] converts the vectors $q_n$ and $k_m$ from R1\u00d7dhead to complex vectors $\\tilde{q_n}$ and $\\tilde{k_m}$ in C1\u00d7(dhead/2). This is achieved by treating the (2t)-th dimension as the real part and the (2t + 1)-th dimension as the imaginary part, where t\u2208 0,1,..., dhead/2. This method results in the same attention values as $q_n k_m^* = Re[q_n k_m]$ while reducing computational overhead. Additionally, RoPE employs multiple frequencies Ot through the channel dimensions of the query and key vectors as follows:\n$\\theta_t = \\frac{1}{10000^{t/(dhead/2)}}$,\nThis approach allows for more effective integration of relative positional information within the attention mechanism, enhancing the model's capability to process and understand sequential data."}, {"title": "3.2. Temporal-Aware Dual ROPE", "content": "In the RoPE used by most current video-language large language models, the relative distance between the m-th text token $T_m$ and the z-th visual token in the n-th frame $F_n^{V_z}$ is defined as Eqn 4. Each text and visual token is treated as an independent position and assigned a unique position id for embedding. However, this position embedding method fails to distinguish visual tokens within and across different video frames, thereby neglecting the crucial temporal information necessary for effective video understanding tasks. Furthermore, as visual tokens constitute a significant proportion of the total tokens in video understanding tasks, the relative distance $P_{(T_m)} \u2013 P_{(F_n^{V_z})}$ between the generated text tokens and the visual tokens may become substantial. This increased distance can impair the model's ability to fully comprehend the visual information, leading to \"hallucinated\" responses [29].\n$A_{(q_{T_m},k_{F_n^{V_z}})} = Re[q_{T_m} k_{F_n^{V_z}}^* e^{i(P(T_m)-P(F_n^{V_z}))\\theta}]$,\nTo address this limitation, we propose a Temporal-Aware Dual Rotary Positional Embedding (TAD-POPE). It includes one ROPE that retains the global relative position relationships of the visual and textual tokens, and an additional time-aware RoPE to incorporate temporal information pertinent to the video frames. Specifically, in contrast to the original position ids, the additional RoPE ensures that visual tokens within the same video frame share the same position id. Meanwhile, the temporal order is maintained across different frames, with the position ids incrementing accordingly. The proposed temporal position id is defined as follows:\n$I_t(n) = \\begin{cases}\nn, & \\text{if } n < v_s, \\\\\nv_s + [\\frac{n - v_s}{m}], & \\text{if } v_s \\leq n \\leq v_e, \\\\\nv_e + [\\frac{n - (v_e - v_s + 1 - m)}{m}], & \\text{if } n > v_e.\\end{cases}$\nwhere $v_s$ and $v_e$ are the starting and ending position ids of the visual tokens within the global RoPE position id n. m is the number of visual tokens per frame, and [.] denotes the floor function, which rounds down to the nearest integer. By scaling the position ids, temporal information is introduced through the adjusted position \u00f1, defined as:\nn = n + \u03b3\u00b7 It(n),\nwhere y is a scaling factor of constant magnitude. This adjustment ensures that temporal information is effectively incorporated into the original position embedding. For both text and visual tokens, the query and key vectors are updated using the adjusted positions \u00f1 and m:\n$\\tilde{q_n} = q_n e^{i\\tilde{n}\\theta} = q_n e^{i(n+\\gamma\\cdot I_t(n))\\theta}$,\n$\\tilde{k_m} = k_m e^{i\\tilde{m}\\theta} = k_m e^{i(m+\\gamma\\cdot I_t(m))\\theta}$,\nFinally, the attention matrix is calculated as follows:\n$A_{(n,m)} = Re[\\tilde{q_n} \\tilde{k_m}^*] = Re[q_n e^{i(n+\\gamma\\cdot I_t(n))\\theta} k_m e^{i(m+\\gamma\\cdot I_t(m))\\theta}] = Re[q_n k_m e^{i[(n-m)+(I_t(n)-I_t(m))]\\theta}]$\nThis formula combines the updated query and key vectors to compute the attention map, incorporating both global positional and temporal information from video frames. By leveraging these aspects, we enhances the MLLM's ability to process and understand the input video comprehensively, resulting in more accurate and contextually appropriate responses."}, {"title": "3.3. Frame-wise Block Causal Attention Mask", "content": "Another often overlooked key point is the design of attention masks within the transformer layers in large language models. In causal language models like the GPT [1] and Llama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text.\nMathematically, the causal attention mask M \u2208 RT\u00d7T for a sequence of length T is defined as:\n$M_{ij} = \\begin{cases}\n0 & \\text{if } i \\geq j, \\\\\n-\\infty & \\text{if } i < j.\\end{cases}$\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as:\nAttention(Q, K, V) = softmax $(\\frac{Q K^T}{\\sqrt{d_k}} + M )V$,\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, $d_k$ is the dimension of the key vectors, and M is the causal attention mask.\nHowever, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3.\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows:\n$M_{ij}^{Full Visual} = \\begin{cases}\n0 & \\text{if } i \\geq j \\text{ or } i, j \\text{ are visual tokens}, \\\\\n-\\infty & \\text{otherwise}.\n\\end{cases}$\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows:\n$M_{ij}^{Fw Block} = \\begin{cases}\n0 & \\text{if } i \\geq j \\text{ and } i, j \\text{ within the same frame}, \\\\\n-\\infty & \\text{otherwise}.\n\\end{cases}$\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:\n$M_{ij}^{FWBC} = \\begin{cases}\n0 & \\text{if } i \\geq j \\text{or } i, j \\text{ within the same frame}, \\\\\n-\\infty & \\text{otherwise}.\n\\end{cases}$\nBy adjusting these masks, we aim to achieve a better balance between visual and textual information integration, enabling MLLMs to distinguish and process both video and text modalities more effectively while enhancing the spatiotemporal global attention to the most critical visual modality information for video understanding tasks. Finally, we utilized ablation experiments to select the Frame-wise Block causal Attention Mask for constructing TC-LLaVA."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Instruction Tuning Datasets. In alignment with the instruction tuning setting outlined in VideoChat2 [18], which integrates data for a variety of video understanding tasks, we utilized an extensive and diverse collection of datasets. Specifically, these include 27k conversation videos from VideoChat [19] and Video-ChatGPT [30], 80k classification task samples from Kinetics [14] and SthSthV2 [10], 450k captioned data from Webvid [4], YouCook2 [51], TextVR [39], and VideoChat, 117 reasoning data samples from NextQA [40], CLEVRER [46], and 109,000 annotated question answering samples from Webvid, TGIF [21], and Ego4D [11]. In total, we employed 783k video instruction data samples for conducting supervised finetuning (SFT) our TC-LLaVA.\nEvaluation Benchmarks. The performance of our trained TC-LLaVA model is assessed using a series of video understanding benchmarks, specifically targeting open-ended Video Question Answer (VideoQA) tasks. These benchmarks include MSVD-QA [42], MSRVTT-QA [43], Activity-QA [5], and TGIF-QA [21], where responses generally consist of single-word answers. The accuracy (with true/false answers) and quality (scored from 0 to 5) of the models' responses are evaluated using GPT-3.5 [31]. Moreover, we employ the Video-based Generative Performance benchmark (VCG Score), as introduced by VideoChat-GPT [30]. This benchmark requires longer answers and evaluates five key aspects of video understanding: Correctness of Information (CI), Detail Orientation (DO), Context Understanding (CU), Temporal Understanding (TU), and Consistency (CO). The generative performance is also assessed using the GPT-3.5 model. In addition, we evaluate TC-LLaVA on the multi-choice Question Answering benchmark, MVBench [20], which consists of 20 tasks that demand nuanced temporal comprehension of videos.\nImplementation Details Initialized from the image-pretrained MLLM LLaVA-Next [49], which is based on the Vicuna-7B-v1.5 [50], our TC-LLaVA 7B conduct further video instruction supervised finetuning (SFT) and evaluation on the datasets mentioned above. Following the experimental settings in [44], we uniformly sample 16 frames from the raw video as input and use global average pooling to downsample the visual features from a shape of 24*24*d to 12*12*d where d represents the input feature dimension of the LLM part. During the SFT stage, we employ a batch size of 128 and a learning rate of 2e-5, utilizing a cosine scheduler and a warmup ratio of 0.03. All reported results are evaluated on models trained for 7k steps on 8 NVIDIA A100 GPU. For evaluation, we use the GPT-3.5-turbo-0125 model across benchmarks that require additional scoring or assessment."}, {"title": "4.2. Comparison with SOTA", "content": "In this section, we compare our TC-LLaVA with recent advanced works, including Video-LLaMA [47], LLaMA-Adapter [48], Video-ChatGPT [30], Chat-UniVi [13], MovieChat [34], VideoChat [18], VideoChat2 [20], Vista-LLaMA [29], LLaMA-VID [22], IG-VLM LLaVA [15], ST-LLM [26], PLLaVA [44], and GPT-4V [1], across various video understanding benchmarks. The best performance is indicated in bold, and the second-best results are indicated with underlining. As shown in Table 1, our TC-LLaVA achieves a new state-of-the-art performance across MSVD-QA, TGIF-QA, and Video-ChatGPT, surpassing GPT-4V by 2.5%, 7.9%, and 0.02%, respectively. Additionally, our TC-LLaVA achieves the best performance across video question-answering benchmarks on the Score metric. Compared to the latest work PLLaVA, which is also initialized from LLaVA-Next and continues using original causal attention mask and RoPE, our TC-LLaVA outperforms it across all five evaluation benchmarks, demonstrating the effectiveness of our proposed methods.\nFurthermore, we evaluate TC-LLaVA on MVbench, a multiple-choice video question answering benchmark, focusing on questions that require comprehensive understanding of the entire video. As shown in Table 2, TC-LLaVA achieves state-of-the-art performance in the average MVbench score. Specifically, for time-related tasks such as Action Sequence (AS), Object Existence (OE), Moving Count (MC), Moving Attribute (MA), State Change (SC), Character Order (CO), and Counterfactual Inference (CI), TC-LLaVA demonstrates a significant performance margin of at least 0.5% over other open-source models. Even when compared to GPT-4V, we maintain an edge in average performance across all 20 tasks by 13.1%."}, {"title": "4.3. Ablation Studies", "content": "In this subsection, we conduct ablation studies to assess the impact of key components. Specifically, we examine the manual ratio settings y of Time-Aware Dual RoPE and other designs of the attention mask, beyond the proposed Frame-wise Block Causal Mask as shown in Figure 3. For these studies, we use the basic settings as a combination of both the original RoPE and Causal Attention Mask, while keeping the previously mentioned training settings. The evaluation is performed on MVbench. Finally, we present a visualized heatmap comparing the attention weights of our TC-Attention mechanism to the vanilla attention."}, {"title": "4.3.1 Time-Aware ROPE Ablation", "content": "Firstly, Maintaining global Rotary Position Embedding (ROPE) is crucial for preserving the global positional relationships between tokens. LLaVA treated each token in an image as having an independent position. When transitioning from image to video understanding tasks, we aim to retain the characteristics of these pre-trained weights while introducing time-aware RoPE to incorporate temporal information. If we entirely abandon the use of ROPE, it could result in a partial loss of the capabilities encoded in the pre-trained LLMs, ultimately affecting the final performance.\nSecondly, ROPE employs a rotational invariant mechanism, which contrasts with the linear and fixed positional embedding schemes of absolute and learnable embeddings. These inherent differences can hinder ROPE's effective scalability when integrating it with other positional encoding techniques, potentially resulting in suboptimal performance or conflicting representations.\nFinally, we explore the impact of the hyperparameter \u03b3 in Time-Aware Dual ROPE. As shown in Figure 4, we evaluate TC-LLaVA on MVbench by setting the manual ratio \u03b3 across [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]. Compared to the baseline setting, which uses a single global RoPE (indicated by the red dashed line), introducing our Time-Aware ROPE increases performance, particularly when y is close to 1.0, achieving the best performance at 56.0%. However, further increasing y slightly reduces the final performance. We think this occurs because increasing y too much might distort the original global position relationships encoded by the original RoPE, leading to suboptimal integration of spatial and temporal information. In the end, we choose y as 1.0 for TC-LLaVA's experimental setting across the entire paper."}, {"title": "4.3.2 Attention Mask and Combination Ablation", "content": "We further explore other attention mask variances mentioned above. As shown in Table 3, using Full Visual and Frame-wise (Fw.) Block Masks enhances visual token interactions within frames but weakens or sacrifices causal relationships. This is crucial for video understanding, as future frames should be able to reference previous frames, but previous frames should avoid seeing future frames, similar to the way text sequences are handled in autoregressive generation. Our Fw. Block Causal Mask achieves better performance by considering both enhancing visual interactions and preserving the causal relationships between tokens. When combined with Time-Aware Dual ROPE, our TC-LLaVA demonstrates superior performance, scoring 56.6% on MVbench and 3.19 on VCGbench. This combination improves the Attention Module, the core component of the LLM, resulting in a more comprehensive and effective video understanding model."}, {"title": "5. Other Time-Aware Position Embedding", "content": "To further verify the effectiveness of our Time-Aware Dual ROPE, we conducted the ablation study presented in Table 4, which demonstrates the comparative performance of different positional encoding (PE) settings on two benchmarks, MVbench [20] and VCGbench [30]. The experimental settings used are consistent with those outlined in the paper. The baseline setting RoPE achieves a score of 54.3 on MVbench and 3.09 on VCGbench. However, when introducing Time Absolute Position Encoding (APE) and Time Relative Position Encoding (RPE) individually, the models fail to converge. This issue arises because the pre-trained language model (LLM) is based on RoPE, and the inherent differences between APE, RPE, and ROPE result in significant alterations to the inter-layer features learned during pre-training. Consequently, these discrepancies cause instability in the training loss, leading to fluctuations that hinder the model's ability to converge effectively. Therefore, the effective approach to incorporating temporal information into a pre-trained LLM is through Time RoPE, as it minimizes conflicts with the pre-existing model configurations. By aligning more closely with the RoPE framework used during pre-training, Time RoPE ensures a smoother integration of temporal features, thereby reducing instability during supervised finetuning (SFT) stage. By combining with the original RoPE, our proposed RoPE + Time ROPE (Time-Aware Dual RoPE) achieves further improvement and outperforms all other configurations, with enhanced scores of 56.0 on MVbench and 3.15 on VCGbench, demonstrating the effectiveness of our approach in PE setting methods and leveraging the spatial-temporal positional information."}, {"title": "6. TC-Attention on Different Base Model", "content": "To further assess the generalizability and robustness of our TC-Attention mechanism, we extended its application beyond Vicuna-7B-v1.5 to include other pre-trained LLM base models, specifically Llama3-8B-Instruct [2] and Mistral-7B-Instruct-v0.2 [12]. Each model was initialized from the pre-trained LLaVa-Next models [16]\u00b9, with all subsequent fine-tuning experiments conducted under consistent SFT settings. The performance of the fine-tuned models was evaluated on two benchmarks, MVbench [20] and VCGbench [30], with the results summarized in Table 5. Across different base models, the introduction of TC-Attention led to measurable improvements in performance on both benchmarks. These results underscore the efficacy of TC-Attention, demonstrating its ability to enhance the performance of diverse base models. The consistent gains observed across different architectures not only validate the adaptability of TC-Attention but also highlight its potential as a valuable component in optimizing the performance of large language models on complex tasks."}, {"title": "6.0.1 Attention Visualization", "content": "Finally, we illustrate the attention weights of both our TC-Attention and Vanilla Attention. For this experiment, we compare the video-finetuned LLaVA and TC-LLaVA by inputting the same video test samples and visualizing the average attention weights of different heads in the final decoding layer of the LLM. In the visualization of Figure 5, brighter colors represent higher weights while the darker color represent lower weights. The attention weights assigned to visual tokens are markedly more comprehensive and greater in our TC-Attention. This indicates that, unlike Vanilla Attention, which only focuses on the last few visual tokens of each frame, our TC-Attention attends to every visual token within and across frames. Additionally, the proposed TC-Attention assigns greater attention weight to subsequent text (user input), resulting in a considerably more substantial impact of visual tokens on language tokens. This demonstrates the effectiveness of TC-Attention in integrating visual and textual information, enhancing the model's overall understanding and performance."}, {"title": "7. Conclusion", "content": "In this work, we present TC-LLaVA, rethinking the attention design in large language models (LLM) for video tasks. We introduce two core components to achieve this: Temporal-Aware Dual RoPE, incorporating temporal information into the attention module while maintaining the global position information between visual and text tokens, and Frame-wise Block Causal Attention Mask, enhancing the interaction of visual tokens within frames while preserving causal relationships across video frames. By conducting simple supervised finetuning (SFT) on video-related instruction datasets, our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks, showcasing the effectiveness of these methods. As LLMs continue to scale up, their powerful performance has led to the negligence of some design details. We hope our work encourages researchers to rethink these design aspects."}]}