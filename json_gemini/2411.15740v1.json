{"title": "LTCF-Net: A Transformer-Enhanced Dual-Channel Fourier Framework for Low-Light Image Restoration", "authors": ["Gaojing Zhang", "Jinglun Feng"], "abstract": "We introduce LTCF-Net, a novel network architecture designed for enhancing low-light images. Unlike Retinex-based methods, our approach utilizes two color spaces-LAB and YUV\u2014to efficiently separate and process color information, by leveraging the separation of luminance from chromatic components in color images. In addition, our model incorporates the Transformer architecture to comprehensively understand image content while maintaining computational efficiency. To dynamically balance the brightness in output images, we also introduce a Fourier transform module that adjusts the luminance channel in the frequency domain. This mechanism could uniformly balance brightness across different regions while eliminating background noises, and thereby enhancing visual quality. By combining these innovative components, LTCF-Net effectively improves low-light image quality while keeping the model lightweight. Experimental results demonstrate that our method outperforms current state-of-the-art approaches across multiple evaluation metrics and datasets, achieving more natural color restoration and a balanced brightness distribution.", "sections": [{"title": "1. Introduction", "content": "Low-Light Image Enhancement (LLIE) is a critical and complex task within the domain of computer vision. In environments with inadequate lighting, camera images often exhibit severe noise, diminished contrast, and obscured details. These degraded images not only compromise human visual perception but also challenge the downstream visual tasks such as object detection at night. The primary objective of LLIE is to enhance the visibility and contrast of these images, reveal details obscured in shadows, and mitigate distortions that typically occur during the enhancement process, including noise, unwanted artifacts, and inaccurate color reproduction.\nTraditional methods like histogram equalization[1, 10, 11, 33] and gamma correction [21] are among the most straightforward techniques to enhance visibility in low-light images. However, while these methods effectively increase contrast, they often lead to oversaturation in bright areas but also result in a loss of detail [15], without addressing the underlying issues of inadequate illumination. In contrast, Retinex theory, which simulates human visual perception of color, has become a foundational principle in LLIE strategies [6, 37, 41, 43, 45, 47]. Although Retinex-based methods leverage illumination estimation and reflectance for enhancement, their underlying assumption of clean, distortion-free inputs rarely holds in real low-light conditions [43, 45]. This fundamental limitation often results in amplified noise and color artifacts, compromising their practical effectiveness.\nRecent learning-based approaches have attempted direct mapping between low-light and normal-light conditions [16, 17, 51, 52]. While these methods show promise, they often sacrifice perceptual color accuracy and theoretical grounding for performance [28], complicated by their multi-stage training requirements. The introduction of Transformers [35], known for capturing global dependencies, initially seemed promising. However, the computational demands of standard Vision Transformers [12, 44] proved prohibitive [13], leading to hybrid CNN-Transformer architectures like SNR-Net [27] and LYT-Net [4]. While these models incorporate global Transformer layers at reduced resolutions, they have yet to fully capitalize on Transformers' potential for low-light enhancement, indicating room for further advancement.\nTo address these challenges, we present LTCF-Net, a novel approach to low-light image enhancement that leverages a dual-channel color space architecture. Our model uniquely processes illumination and color information in parallel streams, employing self-attention mechanisms [35] to dynamically adjust enhancement based on varying exposure levels across image regions. Unlike traditional Retinex-based methods [6, 25] that rely on complex degradation modeling, LTCF-Net enables direct end-to-end training. By prioritizing illumination processing\u2014aligned with human visual perception, our approach achieves superior detail preservation and natural enhancement while maintaining color fidelity, effectively avoiding common artifacts like overexposure and color distortion.\nExperimental results demonstrate that our method outperforms existing state-of-the-art techniques on multiple datasets, achieving significant performance improvements on datasets such as LOL-v1 [42], LOL-v2 [49], SID [8], and SDSD [39], validating the effectiveness and superiority of our approach. Our main contributions are:\n\u2022 We propose a novel dual-channel color space transformation method that effectively separates and independently processes illumination and color information, simplifying the complex decoupling task. The model utilizes LAB [34] and YUV [19] color spaces for targeted enhancement, implementing a multi-head self-attention mechanism [35] on the luminance and chrominance layers to dynamically enhance low-light recovery capabilities.\n\u2022 The Fourier adjustment module [30, 50] is introduced to convolve the real and imaginary parts of the data in the frequency domain to enhance the features in the frequency domain. It also includes an adaptive enhancement mechanism that balances the light distribution by dealing with regions of different scales or dynamic ranges\n\u2022 Through quantitative and qualitative experiments, our network demonstrates superior performance on LOL [42, 49] and other datasets compared to state-of-the-art methods."}, {"title": "2. Proposed Method", "content": "Fig. 1 and Fig. 2 illustrate the structure of the proposed LTCF-Net. As depicted in Fig. 1, the architecture is divided into two branches dedicated to illumination enhancement, each operating within distinct color spaces: LAB and YUV. Within each branch, as illustrated in Fig. 2(a) and Fig. 2(d), a luminance enhancement channel incorporates both a Multi-Head Self-Attention (MHSA) module and a Fourier Brightness Processing (FBP) module, which are critical in meticulously restoring luminance details. These modules are detailed in Sections 2.2 and 2.5, respectively.\nAdditionally, Fig. 2(c) illustrates that each color channel integrates a Channel Denoising (CD) Block, outlined in Section 3.3. This block enhances the noise reduction capabilities of the system. The denoised outputs are then combined with the luminance channel through a Multi-stage Squeeze and Excited Fusion (MSEF) module, elaborated in Section 3.4. Fig. 2(b) provides an in-depth view of the MSEF module, which is engineered to refine the modeling of both illumination and color features, thereby ensuring the fidelity and uniformity of the final enhanced images.\n2.1. Color Spaces\nThe LAB and YUV color spaces offer significant advantages over the traditional RGB color space, particularly in terms of separating light information. This separation facilitates independent manipulation of the light and color channels, enhancing the process's flexibility and effectiveness. The LAB color space, grounded in human visual perception, allows for precise adjustments as demonstrated by the standard LAB conversion formula presented in Eq.1.\n$L^{*} = 116\\times f(\\frac{Y}{Y_{n}})-16$\n$a^{*} = 500\\left[f(\\frac{X}{X_{n}})-f(\\frac{Y}{Y_{n}})\\right]$\n$b^{*} = 200\\left[f(\\frac{Y}{Y_{n}})-f(\\frac{Z}{Z_{n}})\\right]$\nMeanwhile, the YUV color space is particularly beneficial for low-light image enhancement due to its ability to independently adjust luminance (Y) \u2014 critical for enhancing visibility \u2014 without affecting the chrominance channels (U and V). The formulation of the YUV color space is specified in Eq. 2.\n$Y = 0.299R + 0.587G + 0.114B$\n$U = -0.14713R - 0.28886G + 0.436B$\n$V = 0.615R - 0.51499G - 0.10001B$\n2.2. Multi-header Self-attention Block\nThe MHSA Block, inspired by transformer architecture, begins by transforming the input feature $F_{in} \\in \\mathbb{R}^{H \\times W \\times C}$ into a reshaped format $X \\in \\mathbb{R}^{HW \\times C}$. This reshaped feature is then partitioned into multiple 'heads' as $X = [X_1, X_2, \\dots, X_k]$.\nEach segment, $X_i \\in \\mathbb{R}^{HW \\times d_k}$, where $d_k = \\frac{C}{k}$ and $i = 1, 2, \\dots, k$, undergoes processing by three bias-free fully connected layers (fc), which project $X_i$ into the query $Q_i$, key $K_i$, and value $V_i$ components, which can be defined by $Q_i = X_iW_q, K_i = X_iW_k, V_i = X_iW_v$.\nThe learnable parameters of the fully connected layers are represented by $W_{qi}$, $W_{ki}$, and $W_{vi} \\in \\mathbb{R}^{d_k \\times d_k}$. This structure allows the model to adaptively respond to varying lighting conditions across different image regions, enhancing areas that are typically more challenging due to darkness. Each attention head functions independently, employing the self-attention mechanism as defined below:\n$Attention(Q_i, K_i, V_i) = Softmax(\\frac{Q_iK_i^T}{\\sqrt{d_k}}) \\times V_i$\nThe outputs from all attention heads are concatenated and integrated using a positional encoder, then reshaped back to the original input dimensions, resulting in the final output feature $F_{out} \\in \\mathbb{R}^{H \\times W \\times C}$.\n2.3. Channel Denoising Block\nThe CD Block leverages a four-scale U-shaped network [32], incorporating the MHSA mechanism introduced above at the network bottleneck. This integration of convolutional and attention-based methodologies allows for robust feature extraction and effective denoising. The module consists of multiple convolutional layers characterized by two types of strides and includes skip connections to enhance detail retrieval and noise reduction.\nThis block processes color information from two distinct color spaces, represented by $R_i$, where $i = \\{A, B, U, V\\}$ corresponds to the color space components A, B, U, and V. The process initiates with these color components being processed through a 3 \u00d7 3 convolutional layer with a"}, {"title": "2.4. Multi-stage Squeeze and Excited Fusion Block", "content": "stride of one to extract preliminary features, denoted by $F^{(0)} = Conv_{1\\times1}(R_i)$. The signal then sequentially traverses three 3 \u00d7 3 convolutional layers each with a stride of two, progressively capturing multi-scale features. For each convolution layer $k = 1, 2, 3$, the operation results in $F^{(k)} = Conv_{3\\times3}(F^{(k-1)})$, reducing the spatial dimensions of the feature map by half with each convolution, transitioning from $F^{(1)} \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times C}$ to $F^{(3)} \\in \\mathbb{R}^{\\frac{H}{8} \\times \\frac{W}{8} \\times C}$.\nAt the network bottleneck, minimum-scale feature mapping captures global dependencies effectively, utilizing the MHSA Block. Subsequent stages involve up-sampling, matched in scale to the prior down-sampling phases. The initial up-sampling step, employing a deconvolution with a stride of two, produces $G^{(1)} = Deconv_{3\\times3}(F^{(3)})$. This up-sampling process is repeated twice to restore the feature map to its original dimensions, $H \\times W$. The final output is then refined through two additional convolutional layers followed by a Tanh activation function to maintain color fidelity. The ultimate output, $F_{out}$, restores the feature dimensions to $F_{out} \\in \\mathbb{R}^{H \\times W \\times C}$.\nThe MSEF Block is engineered to integrate multiple feature enhancement mechanisms [20], specifically designed to enhance the model's capability to accurately reconstruct and refine details in images by capturing both global and local features associated with illumination and color information. Initially, the input feature is subjected to Layer Normalization to stabilize its mean and variance. Subsequently, the normalized features are processed through the Squeeze-and-Excitation Block (SEBlock), which employs a two-stage recalibration of feature importance: squeezing to reduce redundancy and excitation to emphasize informative features.\nWithin the SEBlock, features $F_{in}$ initially pass through Global Average Pooling (GAP) to form a global descriptor for each channel, capturing essential contextual information that is broadly representative of the entire image. This descriptor $D_{re}$ is then refined through a fully connected layer equipped with a ReLU activation function, as outlined in Eq. 4, enhancing significant features while filtering out less relevant data. The process continues with another fully connected layer featuring a Tanh activation function, which expands the features back to their original dimensions to achieve optimal recalibration:\n$D_{re} = ReLU (W_1 \\cdot GP (LN (F_{in})))$ \n$D_{ex} = Tanh (W_2 \\cdot D_{re}) LN (F_{in})$\nThis process not only emphasizes and restores key features but also enhances the model's ability to detect variations in color and illumination, producing images that appear more natural. This is particularly vital for low-light image enhancement, where maintaining accurate color and"}, {"title": "2.5. Fourier Branch Processing Block", "content": "detail is crucial. Finally, residual connections also help preserve the original input features, ensuring stability during training and preventing the loss of important details.\nThe Fourier Branch Processing Block is crucial for enhancing and restoring image brightness by manipulating the luminance channel in the frequency domain through the application of Fourier Transform techniques [3]. This approach allows for precise differentiation and manipulation of high-frequency and low-frequency components, thereby significantly improving the details in brightness across the image.\nInitially, the luminance channel undergoes transformation into its frequency-domain representation, with the real and imaginary parts processed separately to refine frequency-domain features. A convolutional layer reduces the number of channels to 16, streamlining features while preserving essential details. This is followed by a Leaky ReLU activation function that captures finer details and mitigates the issue of dying ReLUs in areas of low brightness. Subsequently, features are further refined through an additional convolutional layer that maintains the channel count, ensuring that critical nuances are preserved. A final convolutional layer re-expands the features to match the original input dimensions. This restoration is crucial for aligning the enhanced features with the original image structure, facilitating seamless integration back into the overall image processing workflow. Detailed experimental validations of this process are presented in Section 3.4."}, {"title": "2.6. Combined Loss Function", "content": "In this section, we introduce our loss function framework, dividing six loss variables into two main categories: pixel-level and perceptual-level losses, each targeting different aspects of image quality enhancement.\nPixel-Level Losses. These losses measure discrepancies directly at the pixel level, thus preserving low-level image attributes such as brightness and color fidelity.\nSpecifically, $L_{S1}$ is a smooth L1 loss function, which is defined for two parameters: the predicted image ($y_{pred}$) and the ground truth image ($y_{true}$). This loss function has been proven effective in Fast R-CNN because it is robust to outliers and can provide stable gradients during the training process. Its formula is shown in Eq. 5\n$L_{S1} (y_{true}, y_{pred}) = \\sum_{i\\in\\{x,y,w,h\\}} smooth_{L1}(y_{true} - y_{pred})$\nWhere the function $smooth_{L1}(x)$ is defined as:\n$smooth_{L1}(x) = \\begin{cases}\n0.5x^{2}, & \\text{if } |x| < 1 \\\\\n|x|-0.5, & \\text{otherwise}\n\\end{cases}$\n$L_{PSNR}$ measures and optimizes image quality by assessing the mean squared error between ($y_{pred}$) and ($Y_{true}$). It is designed to enhance pixel accuracy, as detailed in Eq. 7.\n$L_{PSNR} = 40.0 - 20 \\cdot log_{10}(\\sqrt{\\frac{1}{MSE}})$\nThe $L_{Color}$ ensures color fidelity by minimizing color discrepancies between the predicted and reference images, calculated as shown in Eq. 8.\n$L_{Color} = \\sum_{i=1}^{n} |mean(y_{true}) - mean(y_{pred})|$\nThe histogram loss function aligns the global statistical features such as brightness and contrast by comparing histograms, enhancing overall image consistency as illustrated in Eq. 9.\n$L_{Hist} = \\sum_{i=1}^{n} |\\frac{1}{N} \\sum_{n=1}^{N} H_{i,n}^{true} - \\frac{1}{N} \\sum_{n=1}^{N} H_{i,n}^{pred}|$\nWhere $H_{i,n}^{true}$ and $H_{i,n}^{pred}$ are the histogram values for the ground truth and predicted images, respectively, at bin n. This loss encourages the model to generate predictions whose color distributions match those of the ground truth more closely.\nPerceptual-Level Losses. These losses, on the other hand, leverage high-level features to enhance the visual quality of generated images.\n$L_{per}$ utilizes features extracted from a VGG19 network, this perceptual loss measures differences that affect the perceptual quality between the predicted and ground truth images. Its formula is shown in Eq. 10.\n$L_{per} = \\frac{1}{C_jH_jW_j} ||\\phi_j(y_{true}) - \\phi_j(y_{pred}) ||_2$\nAdditionally, $L_{SSIM}$ is based on multi-scale structural similarity, this loss ensures structural consistency across different scales, thereby enhancing the perceptual similarity of the generated image. Its formula is shown in Eq. 11. Where y is the original image and \u0177 is the target image.\n$L_{SSIM}(y, \\hat{y}) = 1 - \\frac{(2\\mu_y\\mu_{\\hat{y}} + C_1)(2\\sigma_{y\\hat{y}} + C_2)}{(\\mu_y^2 + \\mu_{\\hat{y}}^2 + C_1)(\\sigma_y^2 + \\sigma_{\\hat{y}}^2 + C_2)}$\nAs shown in Eq 12, the final loss function is a combined loss between pixel level losses and perceptual level losses, where $a_1$ to $a_5$ are hyperparameters for each loss component. This final loss function includes provides a comprehensive evaluation metric that optimizes various aspects of image quality, ultimately leading to a more realistic and high-quality output.\n$L_{Pixel} = L_{S1} + a_1L_{PSNR} + a_2L_{Color} + a_3L_{Hist}$ \n$L_{Perc} = a_4L_{Per} + a_5L_{SSIM}$\n$L_{Total} = L_{Pixel} + L_{Perc}$"}, {"title": "3. Experiment", "content": "3.1. Datasets and Implementation Details\nWe eveluate our method on LOL(v1 [42] and v2-real [49]), SID [8], SMID [7], SDSD [39], and FiveK [5] datasets.\nLOL. We use both LOL-v1 and LOL-v2-real dataset. The training and testing sets are split with ratios of 485:15, 689:100 for LOL-v1, LOL-v2-real respectively.\nSID. A subset of the SID dataset captured using the Sony a7SII camera is used for evaluation. This subset consists of 2697 pairs of short- and long-exposure RAW images. Low light and normal light RGB images are generated by applying the same in-camera signal processing as used in SID to convert RAW images to RGB. Of these pairs, 2299 are allocated for training and 398 for testing.\nSMID. The SMID benchmark dataset contains 20809 pairs of short- and long-exposure RAW images. These RAW images are also converted to low-light and normal-light RGB image pairs. The dataset provides 18789 pairs for training and 2021 pairs for testing.\nSDSD. We use the static version of the SDSD dataset captured with a Canon EOS 6D Mark II camera and an ND filter. The SDSD dataset includes both indoor and outdoor subsets, with the SDSD-outdoor subset containing 3150 images and the SDSD-indoor subset containing 1963 images.\nFiveK. FiveK dataset is adjusted by experts as references to create low-light photos followed by the method proposed in [5]. It contains 5000 underexposed image pairs with bounding box annotations for 60 object categories. Note that this dataset is particularly used in Object Detection task for enhanced low-light images as discussed in Section 3.3.\nIn addition to the above eight benchmarks, we also tested our method on five datasets: LIME [18], NPE [40], MEF [31], DICM [26], and VV [36], which do not have ground truth annotations.\nImplementation Details. Our model is implemented using the PyTorch framework and trained with the ADAM optimizer [24], using parameters \u03b2\u2081 = 0.9 and B2 = 0.999, for 1000 epochs. Training and testing process were conducted"}, {"title": "3.2. Comparison Study of Low-light Image Enhancement", "content": "on a server equipped with an NVIDIA A40 GPU. The network parameters were randomly initialized, and the model was trained from scratch. An initial learning rate of 2\u00d710-4 was set and gradually reduced to 1 \u00d7 10-6 using a cosine annealing scheduler [29] to facilitate convergence and avoid local minima. For the hyperparameters in the loss function, we set a\u2081 = 0.12, \u03b12 = 0.05, \u03b13 = 0.55, 04 = 0.015, and \u03b15 = 0.25.\nQuantitative Results. The proposed method is quantitatively evaluated against various SOTA algorithms, as detailed in Table 1. While our model registers a slightly lower PSNR on the LOL-v1 dataset, it shows a notable improvement of 1.26 dB in PSNR on the LOL-v2-real dataset compared to the leading DiffLL model. Additionally, it surpasses other top-performing methods on the SID, SMID, SDSD-indoor, and SDSD-outdoor datasets with respective PSNR enhancements of 0.4 dB, 0.43 dB, 0.21 dB, and 0.27 dB. This performance is achieved with only 10.37G FLOPS and 0.155M Params, demonstrating the efficiency and effectiveness of our approach.\nQualitative Results. As shown in Fig. 3, we first compare the visual results of our model against others across the LOL-v1 and LOL-v2-real datasets. For instance, Retinexformer [6] and URetinex [45] demonstrate subpar performance in restoring light and shadow details. Similarly, DiffLL [22] and HVI-CIDNet [14] struggle with accurate color restoration. In contrast, our model excels in accurately enhancing both light and intricate details.\nAdditionally, as depicted in Fig. 4, models like URetinex [45], KinD++ [51], and DiffLL [22] show noticeable color inaccuracies. Moreover, Retinexformer [6] is unable to recover text on a sticker, underscoring its limitations. These observations confirm that our method outperforms others in restoring contrast and detail, particularly in the SID and SMID datasets. Including Fig. 5, in SDSD-indoor and outdoor datasets, our method can restore many details.\nUser Study Score. We conducted a user study involving 95 participants to subjectively evaluate the visual quality of images from seven different datasets as shown in Fig. 6. Each participant evaluated the enhanced images based on three criteria: overall reconstruction quality, presence of overexposure, and accuracy of detailed recovery. Each participant rated the enhanced images on a scale from 0 to 5. According to the aggregated scores presented in Table 2(a), our method consistently outperformed competing methods, effectively avoiding issues of overexposure and underexposure while ensuring uniformly distributed brightness across the images."}, {"title": "3.3. Low-light Object Detection Comparisons", "content": "Additional, we evaluate the impact of various enhancement algorithms on object detection using the FiveK dataset [5]. We used YOLO-v4-tiny [2] as the detector, and then compare the object detection performance on raw images and the images enhanced by different low-light enhancement methods.\nTable 2 shows the average precision (AP) scores. Our model achieves the highest mean AP of 37.7, surpassing the leading self-monitoring method, URetinex, by 1.5 AP. Notably, our approach outperforms in seven object categories: Bicycle, Boat, Bus, Dog, Motor, People, and Table.\nFig. 7 illustrates the qualitative detection results in a low-light scene (left) and after LTCF-Net enhancement (right). The enhanced image enables the detector to identify more objects with higher accuracy, demonstrating the crucial role of our low-light enhancement method in improving object detection performance."}, {"title": "3.4. Ablation Study", "content": "To validate the effectiveness of each component within the LTCF-Net and its contribution to training optimization, we conducted detailed ablation experiments on the SID dataset, specifically examines the dual color branches, the MSEF module, and the FBP module.\nEffectiveness of Dual Color Space Branch. We assessed the impact of individual color spaces on model performance through a series of decomposed experiments, as detailed in Table 3. By separately eliminating the LAB and YUV color spaces, and comparing these to the dual color space setup, we observed the effect on performance metrics. The results indicate that the combined use of both color spaces significantly enhances performance over any single color space configuration.\nEffectiveness of MSEF Block. The incorporation of the MSEF module, despite increasing the number of parameters and FLOPS, results in significant improvements in image quality metrics, boosting the metric of PSNR by 1.38 and SSIM by 0.08. These gains confirm that the inclusion of the MSEF module is crucial for enhancing the model's performance.\nEffectiveness of FBP Block. The primary function of this"}, {"title": "4. Conclusion", "content": "In this work, we introduced LTCF-Net, an innovative low-light image enhancement model that combines dual-channel color spaces with Transformer and Fourier Transform techniques to address the limitations of current enhancement methods. By effectively separating and processing illumination and color information, our model simplifies the enhancement process, allowing for end-to-end, single-stage training that improves operational efficiency and reduces the likelihood of artifacts such as noise and color distortion. Experimental results show that LTCF-Net achieves superior performance compared to existing state-of-the-art methods, as demonstrated on several benchmarks where it showed notable improvements in both quantitative metrics and qualitative assessments. Through extensive evaluations, LTCF-Net not only sets a new standard in low-light image enhancement but also suggests a promising direction for future research in integrating color space knowledge with deep learning architectures to further advance the field of image processing."}]}