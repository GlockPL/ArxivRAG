{"title": "Morphology and Behavior Co-Optimization of Modular Satellites for Attitude Control", "authors": ["Yuxing Wang", "Jie Li", "Cong Yu", "Xinyang Li", "Simeng Huang", "Yongzhe Chang", "Xueqian Wang", "Bin Liang"], "abstract": "The emergence of modular satellites marks a significant transformation in spacecraft engineering, introducing a new paradigm of flexibility, resilience, and scalability in space exploration endeavors. In addressing complex challenges such as attitude control, both the satellite's morphological architecture and the controller are crucial for optimizing performance. Despite substantial research on optimal control, there remains a significant gap in developing optimized and practical assembly strategies for modular satellites tailored to specific mission constraints. This research gap primarily arises from the inherently complex nature of co-optimizing design and control, a process known for its notorious bi-level optimization loop. Conventionally tackled through artificial evolution, this issue involves optimizing the morphology based on the fitness of individual controllers, which is sample-inefficient and computationally expensive. In this paper, we introduce a novel gradient-based approach to simultaneously optimize both morphology and control for modular satellites, enhancing their performance and efficiency in attitude control missions. Specifically, to address the 3D modular design space, we introduce a scalable 3D Neural Cellular Automata (NCA) as the design policy. This NCA encodes local update rules in a Multilayer Perceptron (MLP) neural network, generating different developmental outcomes using a smaller set of trainable parameters. This enables gradient-based optimization and facilitates design pattern regeneration. For control optimization, we utilize a reaction-wheel-based attitude control system modeled with a neural network that shares parameters with the design policy. This shared parameterization enhances the network's generalization ability across different configurations. We integrate design and control processes into a unified Markov Decision Process (MDP), which enables the exploration of a broad design space. To achieve concurrent training of both policies, we employ Twin Delayed Deep Deterministic (TD3) policy gradient reinforcement learning with safety constraints. Our Monte Carlo simulations demonstrate that this co-optimization approach results in modular satellites with better mission performance compared to those designed by evolution-based approaches. Furthermore, this study discusses potential avenues for future research.", "sections": [{"title": "1. Introduction", "content": "As aerospace engineering advances, satellites are increasingly being deployed in complex and dynamic environments [1]. Currently, most satellites are monolithic systems that lack options for maintenance, upgrades, or reconfiguration [2]. Consequently, its operational life is limited by the lifespan of components or the duration of missions. When a satellite reaches the end of its operational life, it often becomes space debris, which poses risks to other satellites and the International Space Station (ISS). Addressing existing debris is expensive, making the development of satellites that are easier to maintain and reconfigure an urgent priority.\nAs a promising solution, modular satellite systems can act as a significant paradigm shift [3-5]. Driven by their adaptability, these systems enable fast customization and alignment with specific mission requirements, revolution- izing satellite deployment and utilization. Recently, a sig- nificant trend in modular satellite technology has been the ongoing miniaturization of interchangeable compo- nents, driven by advances in electronics and materials sci- ence [6]. This development enables the creation of smaller and lighter satellites that are more cost-effective to launch. As computing power becomes increasingly compact and radiation-resistant, sophisticated software can now be de- ployed in relatively small satellite systems, providing satel- lites with highly autonomous on-board capabilities [7]. In addition, standardization efforts have also gained momen- tum, simplifying the development of compatible modules and facilitating the sharing of technology between various research and commercial entities [8-10].\nIn recent years, several modular satellite projects have been launched, such as Cellsat [11], \u201cIntelligent Building- Blocks for On-Orbit Satellite Servicing\" (iBOSS) [12], Phoenix Project [13], Heterogeneous Cellular Satellite [14] and OASIS [15]. Unlike traditional monolithic satellites, as depicted in Fig. 1, modular satellites are composed of in- dividual cubes that each contain different sub-components and perform specific functions. These cubes can be com- bined to create larger and more complex structures because they are interconnected through multi-function interfaces that facilitate mechanical coupling, energy supply, and communication. Moreover, this inherent flexibility offers a valuable opportunity to optimize the satellite's physical structure (morphology) and its control policy (behavior) simultaneously for critical functions, such as attitude con- trol.\nAttitude control [16], the ability of a satellite to accu- rately orient itself in space, is vital for a variety of func- tions, including communication, navigation, and observa- tion. This discipline involves the design and implemen- tation of systems capable of sensing the orientation in a three-dimensional space and making the necessary adjust- ments to maintain or change this orientation as required by the mission objectives. For several decades, autonomous control of the attitude of the satellite has been a routine practice [17, 18]. Beyond the conventional feedback con- trol approach, reinforcement learning (RL) has seen sig- nificant advancements in recent years [19-21]. However, when it comes to modular satellites, attitude control is fur- ther complicated by the satellite's evolving configuration. Changes resulting from the addition or reconfiguration of modules, driven by mission-specific requirements, render traditional control strategies less effective.\nIn summary, addressing the aforementioned issues re-"}, {"title": "2. Methods", "content": ""}, {"title": "2.1 Reinforcement Learning Algorithm", "content": "Reinforcement learning is a crucial branch of machine learning that focuses on how agents learn the optimal interaction strategy in their environments, which means learning the rules to select the best actions in given states to maximize cumulative rewards. An RL problem can typically be modeled as an MDP, which is a mathematical framework for formulating a discrete-time decision- making process. Generally, an MDP is defined by a five- tuple < S, A, P,r,y >. S represents the state space, de- noting the set of all possible states of the environment. A represents the action space, indicating the set of all possi- ble actions the agent can take in each state. P is the density of the transition probability, representing the probability of transitioning to state st+1 after taking action at in state St. This probability is typically defined on the basis of the dynamics of the agent and the environment in which it is operating. rt is the reward function, where rt (St, at) denotes the immediate reward received by the agent for taking action at in state st, indicating the desirability of taking a particular action in a given state. y is the discount factor, with a range of [0, 1), which specifies the degree to which rewards are discounted over time. Consequently, the agent's goal is to find a policy \u03c0\u03c1(\u03b1\u03c4|st) parameterized by $ that maximizes the expected cumulative average reward within the time horizon T.\nIn this study, we use the Twin Delayed Deep De- terministic Policy Gradient (TD3) algorithm [30,31], a leading-edge technique in reinforcement learning, particu- larly suited for continuous action spaces, as the optimiza- tion strategy for designing and controlling modular satel- lites. The workflow of the TD3 algorithm is illustrated in Fig.2. First, all networks and the replay buffer are initial- ized. Next, in each training iteration, the agent interacts with the environment, where the online network executes a noise-perturbed action chosen by the actor, represented as a = \u03c0\u03c6(s) + \u03b5, \u03b5 ~ \u039d(0, \u03c3). The agent then observes the resulting reward and state, storing the experience in the replay buffer in the form (s, a, r, s'). After collecting sufficient experiences, a mini-batch of samples is drawn from the buffer to train the network.\nThe training process begins with the target network of the actor that generates an action \u00e3 = \u03c0\u03c6' (s') + e, e clip(N(0, \u03c3), \u2212c, c), where d' represents the parameters of the target network of the actor, and c is the clipping boundary to prevent excessive noise in the target action. The target Q-value is then calculated using the critic target networks, given by the following formula:\ny = r + \u03b3minQi\u03b8i'(s', \u00e3) (1)\ni=1,2\nwhere (i = 1,2) are the parameters of the two target critic networks. Finally, the parameters of two online critic networks are updated by minimizing the TD error. The optimization objective is performed as:\n\u03b8i = argmin1N\u2211(y - Q\u03b8i (s, a))2 (2)\n\u03b8iNN\nwhere N represents the size of the mini batch. To ensure training stability, the online actor network and all parame- ters in the target network are updated with a delay, typically every d time step. The objective of the online actor net- work is to maximize the Q-value estimated by the online critic network. Thus, the actor's parameters are updated using gradient ascent, with the gradient given by:\n\u2207\u03c6J(\u03c6) = 1N\u2211\u2207aQ\u03b81 (s, a)|a=\u03c0\u03c6(s)\u2207\u03c6\u03c0\u03c6(s) (3)\nNN\nThe parameters of the target networks (both actor and critic) are updated with the following formulas:\n\u03b8i' = \u03c4\u03b8i + (1 \u2212 \u03c4)\u03b8i, \u03c6' = \u03c4\u03c6 + (1 \u2212 \u03c4)\u03c6 (4)\nIn summary, the TD3 algorithm addresses the issue of Q-value overestimation by using two Q-networks. In addi- tion, it adds noise to the target actions to reduce the over- fitting of the policy. By employing delayed updates for the"}, {"title": "2.2 Simulation of the Modular Satellites", "content": "In this work, we build a simulation environment in a real-world problem setting, where a modular satellite sys- tem is modeled similar to the iBoss system [4]. To accu- rately describe the attitude motion of a satellite in orbital space, it is necessary first to establish the geocentric equa- torial inertial coordinate system O \u2013 XYZ, the orbital co- ordinate system o - x,yoz, and the body-fixed coordinate system oXbybzb. The relative relationships between these three coordinate systems are shown in Fig.3. The satellite's attitude is assessed through the relative transfor- mation between its body-fixed coordinate system and the orbital coordinate system. Generally, its attitude can be represented using either Euler angles or quaternions. Euler angles can face singularity problems, whereas quaternions provide more straightforward calculations and avoid these singularities. As a result, we opt to utilize quaternions to describe the attitude. The definition of quaternions is given by the following equation:\nq = [q0qv]T = [cos \u03c62e sin \u03c62]T (5)\nIn this equation, the real number qo is the scalar part of the quaternion, and qv = [qx qy qz]T is referred to as the vector part. e = [ex ey ez]T represents the unit vector that describes the axis of rotation, and \u03c6 denotes the rotation angle. By rotating the body-fixed coordinate system counterclockwise around the axis e by the angle \u03c6, it can be aligned with the orbital coordinate system. The kinematic equation of the satellite attitude expressed by quaternions is:\nqo = \u221212qTv\u03c9 (6)\nqv = 12(qv + q0E3\u00d73)\u03c9 (7)\nwhere w = [ w1 w2 w3 ]T represents the projection of the satellite's angular velocity in the body-fixed coordi- nate system, and E3\u00d73 denotes the 3 \u00d7 3 identity matrix. q\u00d7v represents the cross-product matrix of the vector part, which is given by the following expression:\nq\u00d7v = [0 \u2212qz qyqz 0 \u2212qx\u2212qy qx 0] (8)\nFor the modular satellite considered here, once its mor-"}, {"title": "body satellite. The dynamic equation is expressed as:", "content": "I\u03ce + \u03c9I\u03c9 = Mc + Md (9)\nwhere w represents the cross-product matrix of the angular velocity, calculated in the same way as q\u00d7v. Mc = [Mcx Mcy Mcz]T represents the total control torque applied by the actuators and Ma = [Max Mdy Mdz ]T represents the total disturbance torque acting on the modular satellite. I is its symmetric positive definite inertia matrix, which is expressed as:\nI = [Ix Ixy IxzIxy Iy IyzIxz Iyz Iz] (10)\nwhere the diagonal elements Ix, Iy, and Iz represent the inertia moments of the satellite about the three axes of the body-fixed coordinate system. The off-diagonal elements Ixy, Ixz, and Iyz are referred to as the iner- tia products. When the body-fixed coordinate system aligns with the principal axes of inertia of the satellite, Ixy = Ixz = Iyz = 0. At this point, the dynamic equations become significantly simplified. For modular satellites, different module configurations correspond to different in- ertia matrices. However, as long as the body-fixed coor- dinate system is aligned with the satellite's principal axes of inertia, the inertia products will be zero, allowing the attitude dynamic equations to still be simplified.\nWe consider the combination of modules shown in Fig.4 and assume that there is no gap between two modules. We establish the reference coordinate system oxyozo, where the origin o coincides with the center of mass of the module numbered (1, 1, 1), the xo-axis goes in the same direction as the number of rows increases, the yo-axis and the number of columns increase in the same direction, the zo-axis and the number of layers decrease in the same direction.\nWe set a single module's mass msat = 1.0kg, its length lsat = 0.1m, width wsat = 0.1m, and height hsat = 0.1m, respectively. The combination of modules is described us- ing a three-dimensional matrix M3d with elements only 0 and 1, where 0 indicates that there are no modules at the cor- responding position, 1 indicates that there exist modules. For the module numbered (i, j, k), the position coordinate of the center of mass is poi,j,k and its coordinate component is xj,k, yjk, zjk, then we have:\npoi,j,k = M3dijk [(i \u2212 1)hsat(j \u2212 1)lsat\u2212(k \u2212 1)wsat] (11)\nLet nsat be the total number of modules, and the position of the module's center of mass with respect to the reference coordinate system is:\npm = 1nsat\u2211i=13\u2211j=13\u2211k=13 poi,j,k (12)\nAccording to the parallel-axis theorem, the moment of inertia Ioi,j,k of the module numbered (i, j, k) relative to each axis of the reference coordinate system of the modular satellite can be calculated by Eq.17. Then, the moment of inertia Im of the module relative to each axis of the modular satellite reference coordinate system is calculated by the following formula:\nIo = \u2211i=13\u2211j=13\u2211k=13 Ioi,j,k (13)\nThe reference coordinate system is then translated to the centroid of the modular satellite, represented asXbtybtZbt, and the coordinate pbi,j,k of each member module is\npbi,j,k = poi,j,k \u2212 pm (14)\nSimilarly, we can calculate the moment of inertia Ibi,j,k of each module using Eq.18 and finally the moment of inertia Ibi,j,k of each module relative to each axis of the centroid system using Eq.19. The moment of inertia Ib of the modular satellite relative to each axis of the body-fixed coordinate system is:\nIb = \u2211i=13\u2211j=13\u2211k=13 Tbi,j,k (15)"}, {"title": "", "content": "By substituting Eq.15 into Eq.9, the attitude dynamics equation of the modular satellite can be obtained as:\nI\u03b3\u03ce + \u03c9I\u03c9 = Mc + Md (16)"}, {"title": "2.3 Simulation of the Brain-Body Co-Design Tasks", "content": "This work combines design and control processes into a single MDP and uses an optimization approach based on RL. At the beginning of each episode, a design policy executes a series of actions to formulate the structure of a modular satellite, without assigning any reward signal to the design policy at this stage. Following this, the pro- duced satellite is consumed by a control policy to gather interaction experiences and environmental rewards, which also feedback as learning signals for the design policy. Through the policy gradient method, both policies can be simultaneously optimized to enhance the performance of the specified attitude control task."}, {"title": "2.3.1 State Action Spaces Formalization", "content": "To parameterize the design space, we use a Neural Cel- lular Automata (NCA), which is a MLP that takes in arbi- trary modular satellite morphologies and outputs a series of actions to modify them. We focus on modular satellites that include various types of modules. Each module is repre- sented by a discrete value that corresponds to its type (e.g., empty module=0, rigid module=1, actuator module=2). We define the design state sd = {sd1, sd2, ..., sdn} where sdi for module i is a vector that includes its type and the types of its Neumann neighborhood, together with its po- sition (i, j, k) and distance from the center of mass, as shown in Fig.6. Following the convention in the litera- ture [19-21], the attitude control task is defined as a large- angle slew maneuver that transitions to stabilization in the desired orientation, and we characterize a large-angle slew as targeting an orientation where the angle lies within the range [30, 150] degrees around any rotation axis \u00e9 from the initial orientation.\nWe utilize a reaction-wheel-based attitude control sys- tem, where each actuator module corresponds to a reac- tion wheel. The satellite attitude is represented using Eu- ler parameters (unit quaternions). Thus, the action state vector of the satellite is given as ss = {s1, s2, ..., son } where sdi = {qe, qe, \u1f66, position, distance}, \u1f66 is the an- gular rotation vector about the body-fixed principal axes. For empty modules, we set si as zero vectors. In prac- tice, sai and si are padded with zeros to the same length for convenience in the training process. As illustrated in Fig.7, our co-design policy is consistently represented as \u03c0\u03c6(at st), depicted by a neural network with a shared pa- rameter configuration. This indicates that both the NCA and the control policy share the same input and hidden lay- ers, although they differ in their output layers. The vectors for both the design action ad and the control action ac have a dimension of 3. To confine their values within the inter- val [-1.0, 1.0], we utilize the Tanh activation function on the network output layer. In the design phase, we select the index with the highest value as the candidate module, while in the control phase, each element of the output action vec- tor is limited to a specific peak magnitude by multiplying a scaling factor."}, {"title": "2.3.2 Agent Training Procedure", "content": "The co-design task is packaged according to the standard OpenAI Gym API and is easy to simulate. We limit the maximum episode timestep to 500 with a \u201cframe-skipping\u201d interval of 20. At the beginning of each simulation, a state vector is initialized with random seeds and, at each sub- sequent time step, the co-design policy neural network determines which action to perform according to the cur- rent state vector. The environment simulates this action and returns a new state vector and the corresponding re- ward. During the design phase, no reward is assigned to the policy, while in the control phase, the produced satel- lite is initialized at rest, and the following environment reward with safety constraints is given according to the literature [19-21].\nWe train the design policy and the control policy using the TD3 algorithm, which is based on the Actor-Critic architecture. Here, we estimate the value for the entire morphology by averaging the value per module. With the policy gradient method, we use action masks to inform the"}, {"title": "3. Results", "content": "In this section, we demonstrate the effectiveness of our co-design method in attitude control tasks within different modular satellite design spaces (3\u00d73\u00d73,5\u00d75\u00d75). We seek to answer the following questions: (1) Does our method provide an effective mechanism for learning to design and control modular satellites? (2) How does our method com- pare with the previous EA-based approach in terms of the performance and the satellite morphologies produced?\nWe compare our approach with a popular evolution- based co-design method presented in [33]. Here, we mod- ify the control optimization using TD3, while the mor-"}, {"title": "4. Discussion and Conclusion", "content": "Both the design and control of a satellite are critical to its performance. Despite the well-documented understanding of optimal control by the spacecraft engineering commu- nity, less focus is given to identifying the most effective satellite design. Drawing inspiration from biological or- ganisms, where both body structure and brain are vital for task execution, intelligent agents usually need simultane- ous optimization of their structure and control mechanisms. By applying this principle to modular satellites, we inves- tigate how designing the satellite's modular structure in conjunction with its control systems can enhance optimal attitude control across different configurations, and demon- strates the effectiveness of our RL-based co-design method on the developed simulation environment, enabling nonex- perts to create modular satellites suitable for their require- ments. There are many fascinating directions to further ad- vance our proposed technique: we tested our method using a simulator with relatively fundamental modules as a proof- of-concept to show its effectiveness, and there exist many sim-to-real issues when considering real-world settings. For exploring larger design spaces such as 10 \u00d7 10 \u00d7 10, more advanced techniques, for example, transformer, can be an efficient method to dynamically capture the depen- dencies and relationships between modules, compared to the MLP structure used in this work."}, {"title": "Appendix A. Calculation of the moment of inertia", "content": "Eq.17, Eq.18 and Eq.19 show the calculation process of the moment of inertia used in this work."}, {"title": "rigid body models", "content": "IEEE Transactions on Automatic Control, 29(4):321\u2013331, 1984."}]}