{"title": "Abstaining Machine Learning \u2013 Philosophical Considerations", "authors": ["Daniela Schuster"], "abstract": "This paper establishes a connection between the fields of machine learning (ML) and philosophy concerning the phenomenon of behaving neutrally. It investigates a specific class of ML systems capable of delivering a neutral response to a given task, referred to as abstaining machine learning systems, that has not yet been studied from a philosophical perspective. The paper introduces and explains various abstaining machine learning systems, and categorizes them into distinct types. An examination is conducted on how abstention in the different machine learning system types aligns with the epistemological counterpart of suspended judgment, addressing both the nature of suspension and its normative profile. Additionally, a philosophical analysis is suggested on the autonomy and explainability of the abstaining response. It is argued, specifically, that one of the distinguished types of abstaining systems is preferable as it aligns more closely with our criteria for suspended judgment. Moreover, it is better equipped to autonomously generate abstaining outputs and offer explanations for abstaining outputs when compared to the other type.", "sections": [{"title": "Introduction", "content": "This paper investigates neutral behavior in machine learning (ML). In particular, we investigate so-called Abstaining Machine Learning (AML) systems (Campagner et al., 2019), sometimes also referred to as ML with a reject option (Hendrickx et al., 2021), and draw parallels to the philosophical use of suspension of judgment. While in philosophy, we mostly employ the term \"suspension,\" in the context of machine learning, we will refer to the neutral behavior with the term \"abstention\" following the standard terminology within this field.\nTo fruitfully bridge the phenomena in these fields, it is beneficial to view both as neutral behaviors towards certain questions that is currently \"under discussion.\" We consider questions like: \"Which dog breed is displayed in this image\", \"Is this tumor malignant or benign?\" or \"Is this person creditworthy?\", which have a finite set of well-defined, full answers A. This set consists of all the defined possible answers to the question. For Q\u2081 = \"Is this tumor malignant or benign?\", A\u2081 = {malignant, benign}. For the question Q\u2082 = \"Which dog breed is displayed in the image?\", possibly A\u2082 = {Husky, Labrador, Dachshund, Retriver}. And for propositional questions like \u201cIs this person creditworthy?\" the set can simply be {yes, no}.\nIn the context of Machine Learning, the answers are typically identified with outputs. To indicate the use of a term as an output, we will employ a typewriter font, i.e., malignant and Labrador, and so on.\nIn this work, we focus on those situations in which none of the answers from the answer set is selected. Instead, the question is addressed with a response that expresses neutrality, uncertainty, or indecision about the correct answer.\nIn philosophy, this neutrality is commonly described with the term \"suspension of judgment,\" which is usually characterized as a doxastic, mental stance whose counterparts are belief and disbelief. While belief and disbelief express those doxastic positions that are accompanied by some certainty or decisiveness about a question Q and its correct answer, suspension expresses neutrality and indecision about Q.\nIn machine learning, neutral outputs are described with the term \"abstention.\" Traditionally, for an ML algorithm tasked with answering a question of the above type, the set of possible outputs is equal to the set of the defined answers A."}, {"title": "Abstaining Machine Learning", "content": "In this paper, we consider predicting ML systems. In general, the task of those kinds of ML systems is to select a defined answer from an answer set A for a question Q. The examples considered here refer to cases where the answer set A is a finite, discrete set. A familiar example is that of an image classifier. If an image classifier is to identify the breed of dog depicted in an image, the system is asked the question Q\u2082 = \"Which breed of dog is displayed in the image?\", and a possible set of defined answers is A\u2082 = {Husky, Labrador, Dachshund, Retriver}.\nThis type of ML is often referred to as predicting ML and is distinct, for example, from ML in robotics, where physically acting systems are in focus, and from generative AI, where the task of the AI is to generate text, images, or other data. Moreover, the predicting systems considered here differ from other predicting systems that have a continuous, i.e., infinite, set of possible answers available. What is considered here is often referred to as a classifier.\nMoreover, we only consider so-called supervised ML algorithms. This characteristic concerns the way the system is trained. In ML, one generally distinguishes between an application phase, in which the system solves the task that it is supposed to solve, e.g., answering a question, and an earlier training or learning phase, in which the system learns how to solve the task. In the training phase, the system is equipped with some kind of training data. Supervised systems learn to establish a relationship between the input and the desired output through labeled training data. For the question Q\u2081, whether a certain tumor is malignant or benign, an input data point will not consist of a whole image but of a list of measured features of the tumor, e.g., its size, the number of concave points, its perimeter, and so on. The output will be the answer, i.e., either benign or malignant. In Subsection 2.1, we will illustrate how training data for question Q\u2081 could be visualized and provide an explanation of the mathematical properties of the training data points.\nWhen the system has learned in the training phase to connect certain questions (or lists of features) with certain correct answers (or certain labels or classes), it can later apply this knowledge in the application phase by answering new, previously unanswered questions, i.e., new, unseen tumors.\nWhat distinguishes abstaining classifiers from conventional classifiers is the option to choose none of the defined answers of the answer set A as an output. AML can issue an abstaining output as a response to the question Q allowing an alternative to the defined answers. Therefore, AML systems are often referred to as possibly rejecting the task or refusing to give an answer. This rejection may be issued in the form of an output saying I do not know, I abstain, I reject a prediction, etc.\nThis seems to be appropriate in many application domains. Most prominently, researchers have argued that in high-stakes scenarios like medical decision-making, ML systems with an abstaining option are clearly preferable as diagnostic tools (for example for cancer, COVID-19, or liver disease detection) (Kompa et al., 2021, Brinati et al., 2020, Hamid et al., 2017, Kempt and Nagel, 2022). But also, in other application areas like weather and climate diagnostics (Barnes and Barnes, 2021) or simple spam filters (Artelt et al., 2022), the abstaining option is often considered desirable. If ML systems are to serve as expert or advice systems, it is recommended that these systems liberally admit their own uncertainty in critical situations instead of making a decision at any cost. This also corresponds to our expected behavior of human experts, as Ferri and Hern\u00e1ndez-Orallo (2004, p. 1) point out: \"When we use human assistance for supporting decision making, there are some cases where the expert says 'I don't know' and asks for further assistance (to other experts) or just prefers to postpone the decision. Frequently, we say a person is an expert or a wise person when she prefers to be silent (and ask other experts) rather than to make a mistake.\" Moreover, as Campagner et al. (2019, p. 292) point out, when abstaining ML systems alert us to uncertainties, this often gives us the opportunity to improve the basis for decision-making: \" [...] because it could be used in a human in the loop setting, to point out to the human decision-maker which instances might require the acquisition of further or more precise information.\"\nIn the following, we will illustrate the domain of AML classifiers using two dimensions. Along the first dimension, we distinguish the different reasons for abstention. Thus, we give an overview of situations in which abstaining ML is in play. For this purpose, we distinguish between outlier abstention and ambiguity abstention. The second dimension describes the composition of the algorithms. Here, we basically distinguish two ways in which the abstention option can be technically and conceptually integrated into an ML algorithm. We call these two types of AML systems attached and merged abstention. The two dimensions are fundamentally independent. One dimension concerns the reasons for abstention, and the other dimension concerns the implementation of abstention. In principle, therefore, any combination of outlier or ambiguity abstention with attached or merged abstention is possible.\nIn presenting the AML systems and their distinctions along the two mentioned dimensions, we will revisit the question Q\u2081 concerning cancer detection and furnish an example with real-world parameters and training data points."}, {"title": "An ML Example for Cancer Detection", "content": "A data set for benign and malignant points that is often used can be found in (Wolberg et al., 1992). This data set comprises multiple features, i.e., input variables, from which we have selected two (the smallest nucleus perimeter and the proportion of concave points) to visualize a two-dimensional input space. In Figure 1, an extract of these training data points is sketched.\nFigure 1 illustrates possible training data points for training an algorithm to answer Q\u2081. The training data points are illustrated by the circles and the triangles in the two-dimensional coordinate system in the figure. Mathematically, each training data point can be described by a tuple $(x^{(i)}, y^{(i)})$, $i = 1, ..., n$.\nIn this tuple, $x^{(i)}$ is a two-dimensional vector, which represents two input parameters: the smallest nucleus perimeter and the proportion of the concave points. For example, it could be $x^{(i)} = (0.17,152)$ with 0.17 being the proportion of the concave points (ranging from 0 to 1) and 152 the value for the smallest nucleus perimeter (in micrometers). As each of the two entries of $x^{(i)}$ is real-valued, $x^{(i)}$ is an element of the two-dimensional real space, i.e., $x^{(i)} \\in \\mathbb{R} \\times \\mathbb{R} = \\mathbb{R}^2$. In Figure 1, the value $x^{(i)}$ is represented by the position of the circle (or triangle) in the coordinate system, i.e., by where the circle (or triangle) lies with respect to the horizontal and vertical axis. The space that contains all the training data points is called input space, which is in general denoted by X. For our example, it is $X = \\mathbb{R}^2$.\nSince we consider supervised ML, a training data point, $(x^{(i)},y^{(i)})$, however, consists not only of the input values but also of the respective (ground-truth) label. In the breast cancer example, we not only know for a specific training data point its smallest nucleus perimeter and its proportion of concave points, but we also know whether that training data point is in fact a malignant or a benign one. This information is stored in $y^{(i)}$. In our example case, $y^{(i)}$ can have one of the values: malignant or benign. In Figure 1, the value of $y^{(i)}$ is represented by the shape drawn in the graph. If $y^{(i)}$ = malignant, the point is represented by a triangle, if $y^{(i)}$ = benign, the point is represented by a circle. The set of the potential labels is also called the output set, as the task of the ML system becomes to predict these labels. It is in general denoted by Y. For our example, it is Y = {malignant, benign}, which is identical to the set A\u2081, the set of possible answers to Q\u2081.\nIn total, one example of a training data point $(x^{(i)},y^{(i)})$ with $x^{(i)} \\in X$ and $y^{(i)} \\in Y$ is always an element of the Cartesian product of the input and the output set, i.e., $(x^{(i)},y^{(i)}) \\in X \\times Y$. For our breast cancer example, one concrete training data point could be $(x^{(i)},y^{(i)}) =\u3008(0.17,152), malignant\u3009 \u2208 \\mathbb{R}^2 \\times {malignant,benign}. The complete training data set is denoted by T, i.e., $T = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\} \u2286 X \\times Y$.\nAs for every supervised ML classifier, the goal is to build a classifier that tells you for any arbitrary input (any vector x \u2208 X), representing a new, unseen tumor, whether that input is benign or malignant. For this, a training phase is necessary where a connection between certain input values and the different output classes can be established, based on the given training data.\nFor example, it might be determined that a proportion of concave points above 0.15 occurs only in malignant cases. This means that the algorithm tries to find a decision boundary between the different training data points that separates the data points that belong to the malignant class from the data points that belong to the benign class. An example of such a boundary can be visualized by a line in the input space, separating malignant and benign training data points.\nMathematically, the separation of the data points (in the input space) can be represented by a function f which maps any input vector x \u2208 X to an output y \u2208 Y. According to the above definitions, X is called the input space (or set) and Y is the output set of the function f. How can we find such a function? We can start by considering those functions f : X \u2192 Y that use the simplest decision boundary, i.e., a line, as we will see in Figures 8 and 9. This means, we consider a linear model. Overall, the possible candidate functions of a particular model choice can be collected in a set F. The goal is then to choose one function, to be denoted f, in F that has the property of performing the mapping of the input parameters of the training data in the best possible way. This means that the task in our binary classification problem is to find a f for which $f(x^{(i)}) = y^{(i)}$ for as many i = 1, ..., n as possible.\nBut how can we determine f and derive a boundary that separates the training data labeled malignant from the training data labeled benign best? One option would be to try different functions in F and choose the one that makes the fewest mistakes (trial and error).\nThe different functions in F then have to be evaluated in order to find the \"best one,\" i.e., the one that maps the most $x^{(i)}$ (i = 1,...,n) to their associated $y^{(i)}$. We do this be determining for each f in F how \"bad\" it is, i.e., how much loss it produces for the different training data points. For this, we introduce a loss function l which determines how much loss a particular function f generates for each training data point. This loss occurs when a data point is assigned a different label, according to the decision boundary set by f, compared to its ground-truth label from the training data. For example, the training data point is labeled benign, and the label assigned by the algorithm (according to that boundary) is malignant (or vice versa).\nIn general, the loss function is the heart of a learning algorithm. It determines the loss a candidate function f\u2208 F generates. The total loss (also often referred to as \u201ccost\") is usually determined by summing up the single losses that occur when evaluating a training data point by the candidate function f.\nA simple loss function could in general look like this: $l: Y \\times Y \u2192 \\{0,1\\}$,\n$l(y^{(i)}, f(x^{(i)})) = \\begin{cases}\n1 & \\text{if } y^{(i)} \\neq f(x^{(i)}),\n\\\\\n0 & \\text{if } y^{(i)} = f(x^{(i)}).\n\\end{cases}$       (1)\nGiven a particular candidate function f, the loss function l for one training data point is 0 if the ground-truth label is equal to the label determined by f and is 1 if the ground.truth label is unequal to the label determined by f.\nThe optimal function f is then f\u2208 F for which the sum of the values of the loss function over all training data points $(x^{(i)},y^{(i)})$ (i = 1,...,n) is as small as possible. Mathematically, we find this f by solving the following optimization problem:\n$f = \\underset{f\\in F}{\\operatorname{argmin}} \\sum_{i=1}^{n}l(y^{(i)}, f(x^{(i)}))$.\nIn the following, we will call f sometimes also the regular predictor, to allow for a distinction from other predictors that are obtained in an abstaining setting.\nOnce we have found f in this way, we thereby found a model and a separation boundary, and we can apply the ML model. The application phase can be represented in the following way: We take a new input vector x from X, which the system has not seen before, and put it through the ML system, i.e., the regular predictor f. The output f(x) then indicates the assigned label for the input x. This application phase is visualized in Figure 3."}, {"title": "Reasons for Abstention: Ambiguity versus Outlier Abstention", "content": "The first distinction in abstaining machine learning revolves around the reasons prompting a system to abstain. This distinction describes the handling of a new data point during the application phase of an AML algorithm. Therefore, the following elaborations have to be considered at a stage where the system is already trained and is applied to new data points.\nIn general, if it is too uncertain whether the system will produce the correct output for the new data point, an AML system will abstain. This uncertainty can arise in many ways. While some uncertainties concern the general structure of the model (e.g., an inappropriate model choice for the kind of training data), other uncertainties are due to some characteristic of a specific input.\nThe different uncertainties can be categorized by means of a common distinction in abstaining machine learning: the distinction between ambiguity and outlier abstention. Roughly speaking, when an input is too far away from or too dissimilar to the training data, we are dealing with an outlier; when the input is such that more than one output is likely for the input, we are dealing with ambiguity. This distinction can be found in early works (Dubuisson and Masson, 1993, Denoeux, 1995) and is sometimes referred to with different names, such as novelty rejection versus ambiguity rejection (Hendrickx et al., 2021), distance rejection versus ambiguity rejection (Dubuisson and Masson, 1993) or distance rejection versus confusion rejection (Mouch\u00e8re and Anquetil, 2006b).\nIn outlier abstention (Lotte et al., 2008, Mouch\u00e8re and Anquetil, 2006a,b), the system abstains on data points that are very dissimilar to the training data. This is useful for (at least) two scenarios. First, if an input is very far away from all training data points, it is likely that the input might belong to none of the classes that are in the scope of the classifier. If a classifier is trained to classify different breeds of dogs and the new input is an image of a cat, the cat image will likely be very dissimilar to all of the different dog images that were used for training the classifier. The classifier here really should abstain, as it is only capable of classifying dogs and will not be able to solve the task of classifying a cat. The correct answer for this input of a cat image (and for the question about what is displayed in the image) is not included in the set of defined answers A\u2082 = {Husky, Labrador, Dachshund, Retriver} that the system operates on. Hence, it is reasonable that the algorithm chooses none and abstains.\nSecondly, even in cases where the correct label of an input might be one of the considered labels of the classifier, i.e., the correct answer to the question is one of the defined ones, outliers appear. If an input dog image is very dissimilar to the training images, this suggests that any prediction the system could make will be prone to error. The data point can be dissimilar to the training data for various reasons: There could be measurement inaccuracies, there could be adversarial examples (that are meant to trick the system), or the training data have been just not diverse enough (Hendrickx et al., 2021). In this sense, outlier detection is often used to actually improve the prediction system. If a certain dog image is characterized as an outlier (although the system should recognize the type of dog in the image), this might suggest that the system was trained on too uniform and not sufficiently diverse data, which could be improved based on the detected outliers. Maybe the system was trained on images of dogs that were taken during summertime and the detected outlier is a dog image in the snow. Detecting this outlier can suggest retraining the system with more diverse data; in this case: images taken in different seasons.\nIn contrast to outlier abstention, the problem in ambiguity abstention is not that none of the answers seem likely, but rather that too many of the answers seem likely for the input (Barnes and Barnes, 2021, Campagner et al., 2019, Sarker et al., 2020, Thulasidasan et al., 2019b). Ambiguity is at play when an input appears to belong to more than one class. This can be the case when the input is on a boundary, but also can be due to the structure of the training data itself. Often training data is not perfectly separable. When this is the case, the training data is called noisy. This means that there are certain regions in the training data that overlap (see Figure 4). If an input sample lies in such an overlapping (or noisy) region, ambiguity is present and a prediction for one class or the other would be error-prone. This type of uncertainty can also arise for a variety of reasons. Maybe the input data point simply has certain characteristics of one class as well as characteristics of another class. For example, the size of the dog in an input image might be indicative of a retriever, while the coat color is clearly indicative of a Labrador."}, {"title": "Implementation of Abstention: Attached versus Merged Abstention", "content": "In this section", "the predictor": "which we refer to as f) and \"the rejector", "r (i.e., the part of the system that is relevant for abstaining). There are two ways in which the rejector can be attached to the predictor. The rejector can be attached prior or posterior to the predictor.\n(a) Pre-algorithmic attachment\nIn pre-algorithm abstention models, the abstaining part is executed prior to the predicting classifier (Wu et al., 2007, Mouch\u00e8re and Anquetil, 2006a, Homenda et al., 2014, Coenen et al., 2020). This means that for a given input, the rejector decides whether or not to abstain for the input even before the prediction algorithm starts. If the input is not rejected, the predictor starts running; if the input is rejected, the predictor will not even be started in the first place.\nPre-algorithmic abstention is especially relevant for outlier abstention (Coenen et al., 2020, Lotte et al., 2008). For a given input, the decision of whether the prediction will be too uncertain is made before the prediction is computed. Therefore, it must be a property that is inherent to the input data that determines whether the input will be rejected. This does not work well for ambiguity rejection because ambiguity arises not only due to the input but due to the relationship of the input and the trained model.\n(b) Post-algorithmic attachment\nFor post-algorithmic abstention, the rejector is downstream of the predictor (Campagner et al., 2019, Brinati et al., 2020, Artelt et al., 2022). For every input data point, an ordinary prediction is calculated. This is done independently of any abstention activity. The prediction is computed in the exact same way the prediction would be computed in a non-abstaining system. This means that the question that is under discussion, Q, is answered by choosing one of the defined answers from A. In the second step, the certainty of the prediction, i.e., the likelihood of the selected defined answer being the correct answer is measured. This certainty can be provided by the predictor itself (e.g., as some kind of probability value in a neural network, distance in a support vector machine, or some \u201csoft probabilistic classifier\" (Campagner et al., 2019, Brinati et al., 2020)) or it can be calculated additionally by some uncertainty or reliability measure (Linusson et al., 2018, Mouch\u00e8re and Anquetil, 2006a, Lotte et al., 2008). This certainty value is then used in the posterior attached rejector. In the simplest version, the rejector only consists of a certainty threshold and two if-clauses. If the certainty of the calculated answer being correct is above the threshold, the prediction is passed through and revealed; if the certainty is below the threshold, the predicted answer is rejected, and the system abstains.\nThe crucial difference between merged and attached AML systems is that for the merged systems the abstaining and predicting activity are to some extent inseparable. The abstaining activity is neither upstream nor downstream of the prediction but is included in the predicting activity. Therefore, it is not practical anymore to refer to \"the predictor\" and \"the rejector.\" Instead, the predictor is modified to have the capability to reject as well. For merged AML systems, we can aptly name the modified predictor an \"abstention predictor.\"\nIn a classifier, an extra, abstaining output is introduced. In addition to the outputs represented by the defined answers, there is also the abstaining output. For a given input (e.g., a dog image), the system can either output one of the defined answers (e.g., Husky, Labrador, etc.) or output the abstention output.\nThe property of being \"merged\" can be observed both in the application phase and in the learning or training phase of the algorithm. In the application phase, the fact that the AML system is \"merged\" is illustrated by the fact that decisions about whether to abstain on an input are made neither before nor after the decision about which output to assign (if any). The decision about abstention is made simultaneously with, and as part of the decision about the appropriate output. In the application phase, abstention is simply one additional output among others and in this sense one additional answer. For this, we do not use the regular predictor f, but a special abstention predictor $f^{*}$, which also allows for abstention.\nIn order to obtain such an abstention predictor $f^{*}$, the training phase of a merged AML system has to be adapted. Those adaptions in the training phase, i.e., the way in which the abstaining option is learned, illustrate the second dimension in which merged AML systems differ from attached AML systems. For merged AML, the tasks of rejecting and predicting are blended into one task that is learned simultaneously in the training phase. While it is possible for an attached AML to have the same learning phase as a non-abstaining classifier, the learning phase of a merged AML is necessarily different from a non-abstaining classifier.\nWith Labeled Abstention (a) and Unlabeled Abstention (b), we will distinguish again between two ways of how the learning phase of a merged AML system can allow for abstention-learning. This distinction concerns only the training phase and the way the abstaining class is learned.\nWe will explain this by means of the cancer detection example from Subsection 2.1. There, we introduced how a regular, non-abstaining classifier f can be trained on the training data visualized in Figure 1. This training or learning phase can now, in principle, be adapted in two ways in order to allow for abstention.\n(a) Labeled Abstention\nA simple solution for training a system when to abstain is to extend the general method of supervised learning from the normal outputs to the abstaining output. In the training phase, a classifier is usually given examples of inputs (e.g., images of dogs) along with the correct (ground-truth) label or output we want for that particular image. For the dog classifier, in the training phase, the system would be presented with multiple images of huskies all labeled Husky, multiple images of retrievers all labeled Retriever, etc. The system is shown what a conventional input of a dog image looks like, for which we want to have Retriever as the output. Analogously, we can now proceed for the abstention class. One can label inputs for which one would consider abstention appropriate with the label abstention and put them into the training phase just like the examples of all other classes (Lotte et al., 2008, Mouch\u00e8re and Anquetil, 2006a, Singh and Markou, 2004). For example, one could label images of Shepherds, Bulldogs, or images of cats by hand with abstention since these images should be considered outliers. Moreover, blurry images or images where the dog is only partially visible can also be labeled abstention by hand. Thereby the set of defined answers is in a sense extended from {Husky, Labrador, Dachshund, Retriver} to {Husky, Labrador, Dachshund, Retriever, abstention}.\nConsidering the example in Figure 1, in the original, non-abstaining case, a training data point was a tuple $(x^{(i)}, y^{(i)})$ with $x^{(i)} \\in X = \\mathbb{R}^2$ and $y^{(i)} \\in Y = {malignant, benign}$. In the case of labeled abstention, some of the training data points have the label abstention, i.e., $y^{(i)}$ = abstention. Hence, for a training data point $(x^{(i)},y^{(i)})$, it is $y^{(i)} \\in Y^{*}$ with $Y^{*}$ = {malignant, benign, abstention}. The training data for this would be the set $T^{*} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\\} \u2286 X \\times Y^{*}$. In this approach, there is no categorical change required for the loss function. The loss function only needs to be extended to accommodate the extra class. The loss function for the non-abstaining, binary classification from Equation (1) is a function from $Y \\times Y$ to the loss {0,1}. A loss function for the labeled abstaining case can be the same as l, only mapping from the extended sets, i.e., from $Y^{*} \\times Y^{*}$.\nThis approach has two major drawbacks, though. First, labeling training data points with abstention by hand can be very time-consuming. Second, often it is not useful to label training data as abstention. While in some application domains, we know exactly what a prototypical abstention case might look like (e.g., a blurred image for an image classifier), often we do not, or at least not in advance. In particular, when the uncertainties are due to factors that cannot be readily detected by humans looking at the training data, we cannot tell which samples will be error-prone. Often, the samples that are difficult for the algorithm to process are easy for a human expert and vice versa. This suggests that the human expert will not be able to identify the difficulties for the machine, so that it is unclear how the abstention labels are determined in the training data.\nBesides the straightforward way of inserting abstention as an extra output in the learning process as described in case (a), there is a more indirect, but also more sophisticated way. Here, the training data is not explicitly labeled abstention. In systems like those of Thulasidasan et al. (2019b), Geifman and El-Yaniv (2019), Mozannar and Sontag (2020), Wegkamp and Yuan (2011), Barnes and Barnes (2021), Yuan et al. (2020), the training data looks exactly the same as in a training situation of a non-abstaining classifier. There are images of the different dog breeds, and each image is labeled with one of the normal (defined) labels, i.e., Husky, Retriever, Dachshund, or Labrador. No training image has the label abstention. Hence, for our main working example from Subsection 2.1, the set of training data for the unlabeled abstention case would be $T = \\{(x^{(1)},y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)},y^{(n)})\\} \u2286 X \\times Y$ with $x^{(i)} \\in X = \\mathbb{R}^2$ and $y^{(i)} \\in Y = {malignant, benign}$.\nTherefore, the usual supervised way in which an ML system learns to associate an input with a desired output is not applicable to the abstention cases. In order for the system to learn a connection between certain images and the abstention output, the underlying learning process, i.e., the loss function itself must be adjusted.\nThis can be implemented when for a given training data point, it is possible not only to produce a full loss (if the point is misclassified) or no loss (if the point is classified correctly), but also a small loss if the point is not classified at all. For the breast cancer classifier, the normal (non-abstaining) loss function of Equation (1) was introduced as a function that takes the value 1 for each misclassified data point and the value 0 for each correctly classified point. The abstaining loss function could then include an additional loss of, say, 0.2 if the system does not classify benign or malignant but instead chooses the abstention output for a given input (regardless of what the point's actual ground-truth label is).\nIn the case of unlabeled abstention, we look for f in the set of the candidate functions F*, which consists of functions of a particular model choice that maps from $X = \\mathbb{R}^2$ to $Y^{*} = {malignant, benign, abstention}$.\nWhile the set of the candidate functions was also F* for the case of labeled abstention, in unlabeled abstention training, the loss function $l^{*}$ needs to be adjusted, too. For each single training data point $(x^{(i)}, y^{(i)})$, $l^{*}(y^{(i)}, f(x^{(i)}))$ can add either a loss of 1 for misclassification, a loss of 0 for correct classification, or a loss of some \u03b1 if the system abstains on this point. Hence, $l^{*}": "Y \\times Y^{*} \u2192 \\{0,1, \u03b1\\}$,\n$l^{*}(y^{(i)}, f(x^{(i)})) = \\begin{cases}\n1 & \\text{if } y^{(i)} \\neq f(x^{(i)}) \\text{ and } f(x^{(i)}) \\neq abstention,\n\\\\\n\u03b1 & \\text{if } f(x^{(i)}) = abstention,\n\\\\\n0 & \\text{if } y^{(i)} = f(x^{(i)}).\n\\end{cases}$ (2)\nNote that \u03b1 \u2208 (0,1) since for \u03b1 \u2264 0 the system would always abstain and for \u03b1 \u2265 1 never abstain. If the same \u03b1 is chosen for all classes, it has been noted in Ramaswamy et al. (2018) that \u03b1 < $m^{-1}$ form being the cardinality of Y, the number of possible ground-truth labels. In our example, m = 2. This means that choosing to abstain has to be always less costly than"}]}