{"title": "FRESH: FREQUENCY SHIFTING FOR ACCELERATED NEURAL REPRESENTATION LEARNING", "authors": ["Adam Kania", "Marko Mihajlovic", "Sergey Prokudin", "Jacek Tabor", "Przemys\u0142aw Spurek"], "abstract": "Implicit Neural Representations (INRs) have recently gained attention as a power-ful approach for continuously representing signals such as images, videos, and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to ex-hibit a low-frequency bias, limiting their ability to capture high-frequency details accurately. This limitation is typically addressed by incorporating high-frequency input embeddings or specialized activation layers. In this work, we demonstrate that these embeddings and activations are often configured with hyperparameters that perform well on average but are suboptimal for specific input signals under consideration, necessitating a costly grid search to identify optimal settings. Our key observation is that the initial frequency spectrum of an untrained model's output correlates strongly with the model's eventual performance on a given target signal. Leveraging this insight, we propose frequency shifting (or FreSh), a method that selects embedding hyperparameters to align the frequency spectrum of the model's initial output with that of the target signal. We show that this simple initialization technique improves performance across various neural representation methods and tasks, achieving results comparable to extensive hyperparameter sweeps but with only marginal computational overhead compared to training a single model with default hyperparameters.", "sections": [{"title": "INTRODUCTION", "content": "Implicit Neural Representations (INRs) are advancing computer graphics research by integrating classical algorithms with continuous signal representations. They have been successfully applied in signal representation and inverse problems, with notable applications in neural rendering, compression, and 2D and 3D signal reconstruction (Xie et al., 2022).\nINRs primarily rely on multilayer perceptrons (MLPs), making them susceptible to spectral bias, which refers to the slower convergence of MLPs when approximating high-frequency components of the target signal (Rahaman et al., 2019). Although spectral bias can benefit generalization (Ronen et al., 2019), it can also hinder performance (Gorji et al., 2023; Rahaman et al., 2019), especially in scenarios that require high precision of signal reconstruction, such as the training of INRs. This led to the development of numerous architectures aimed at overcoming spectral bias and its resulting capacity constraints by increasing the frequencies present in the input signal at the first layer of the model (embedding layer), e.g., through frequency-changing activation functions (Sitzmann et al., 2020b; Liu et al., 2024; Tancik et al., 2020) or auxiliary data structures (Chan et al., 2022; M\u00fcller et al., 2022).\nThe frequency of such embedding layers is typically controlled by hyperparameters whose con-figuration can significantly affect performance (see Figure 1). In section 3 we show that default hyperparameter values can hinder performance and lead to blurry reconstructions. Improving per-formance is possible by using parameters sweeps,\nhowever, optimizing embedding parameters by training multiple models introduces significant over-head and is not feasible in practice.\nThe high computational cost of performing a parameter sweep comes from training each model. Instead of training, we approximate model performance using the Fourier transform and the Wasser-stein distance (Wasserstein, 1969), significantly reducing the computational costs (see Table 4). Our method, dubbed FreSh (Frequency Shifting for Accelerated Neural Representation Learning), selects the embedding configuration where the frequency distribution of the model's output is close to that of the target signal. This shift in the model's frequency distribution results in better signal modeling (see Figure 1). We validate our approach experimentally, demonstrating improved quality in representation tasks such as image and video overfitting and in an inverse problem, specifically 3D shape modeling with NeRF. We show that these improvements result from a more accurate ap-proximation of all frequencies (see Figure 6).\nIn existing INR studies (Sitzmann et al., 2020b; Tancik et al., 2020; Saragadam et al., 2023; Liu et al., 2024; M\u00fcller et al., 2022), new architectures are often introduced with minimal attention to simplifying the costly process of hyperparameter selection, even though it is crucial for achieving the best performance. Our framework addresses this gap by leveraging frequency information to guide the selection process for embeddings of various INR models. Our key contributions are:\n\u2022 We develop a technique for comparing frequency contents of images based on the Discrete Fourier Transform and the Wasserstein distance.\n\u2022 We introduce FreSh (Frequency Shifting for Accelerated Neural Representation Learning) a model agnostic method for configuring coordinate embeddings for better performance, that can be easily applied using a provided script 1. FreSh works by adjusting the capacity of an INR to model all target signal's frequencies.\n\u2022 We achieve state-of-the-art results in image and video approximation, as well as 3D shape reconstruction (NeRF), using a fraction of the compute of a conventional grid search."}, {"title": "RELATED WORK", "content": "INRS are neural models used for signal representation that received considerable attention in re-search (Tewari et al., 2022) and have been applied in various domains, including representation of images (Klocek et al., 2019), videos (Chen et al., 2022b), and 3D shapes (Park et al., 2019). Notable applications include 3D shape reconstruction (Mildenhall et al., 2021), robotics (Wang et al., 2021b; Lin et al., 2021), and compression (Lu et al., 2021; Takikawa et al., 2021). INR architectures are often simple, consisting of a single MLP, with improvements focusing on embedding layers (Chen et al., 2022a; M\u00fcller et al., 2022), activations (Sitzmann et al., 2020b), rendering techniques (Barron et al., 2021; 2023), and regularization methods (Yang et al., 2023).\nSpectral bias is a phenomenon observed in MLPs describing their preference for learning low-frequency functions and ignoring high-frequency noise (Rahaman et al., 2019; Ronen et al., 2019; Xu, 2018), which helps explain the remarkable generalization properties of deep models. Nevertheless, this low-frequency bias hurts model performance when high-frequency components of the signal are informative (Gorji et al., 2023). A common way of overcoming this bias in INRs is by introducing high-frequency embeddings (Sitzmann et al., 2020b; Mildenhall et al., 2021; M\u00fcller et al., 2022; Liu et al., 2024) that change the space over which the MLP operates, thus modifying the frequencies of the target function. However, the effectiveness of such approaches is limited as optimal hyperparameter configurations have to be found by trial and error for each target signal.\nPositional encodings are a broad class of functions that map coordinates into a high dimensional space through a function of adjustable frequency. Their particularly prominent use is in Transform-ers (Vaswani et al., 2017), but they are also crucial for INRs. One of their first applications was NeRF (Mildenhall et al., 2021), a model for 3D scene reconstruction from a set of posed 2D images. NeRF embeddings are not stable in sparse settings due to its usage of very high frequencies (Yang et al., 2023), and its axis-alignment makes reconstruction quality rotation-dependent (Tancik et al., 2020). Despite its drawbacks, it is frequently used (Barron et al., 2021; Pumarola et al., 2021; Barron et al., 2023), which could be due to its low sensitivity to hyperparameters compared to alternatives. These alternatives include various activation functions (Sitzmann et al., 2020b; Saragadam et al., 2023) and a direction-invariant version of NeRF using Fourier features (Tancik et al., 2020).\nActivation functions are a popular method of improving the capacity of INRs, where the first layer acts as a positional embedding. A well-known example is Siren (Sitzmann et al., 2020b), which uses a sine activation to increase signal frequencies in the model's first layer. Architectures that generalize this approach utilize the Gabor wavelet (Saragadam et al., 2023), non-periodic functions (Ramasinghe & Lucey, 2022), and variable-periodic functions (Liu et al., 2024). In this work, we particularly focus on Siren due to its popularity, but we also address other activations.\nAuxiliary data structures are used in neural scene representation to associate fragments of the scene with a feature vector of trainable parameters, trading a larger memory footprint for smaller computational costs. However, due to the extremely small MLPs used, these approaches can lose some of global reasoning and implicit regularization (Neyshabur et al., 2014; Goodfellow et al., 2016) capabilities of neural models. As directly storing a fine grid of features would be prohibitively expensive, practical approaches use low-rank approximations (Chen et al., 2022a), 2D feature maps (Chan et al., 2022) and hash tables (M\u00fcller et al., 2022) to reduce the memory cost. In such settings spectral bias is not avoided, and the resolution of the voxel grid needs to be tuned for each scene. Although our main goal is to improve pure neural network-based solutions, we also verify on the recent architecture from M\u00fcller et al. (2022) that FreSh is applicable to grid-based approaches.\nResFields is a novel framework for INRs that improves their capacity for representing complex signals by incorporating temporal residual layers into MLPs (Mihajlovic et al., 2023). By modifying network weights with a time-dependent component represented as a factorized matrix, it increases the performance with only a small impact on parameter count and inference speed. We use ResFields to improve current state-of-the-art results on video representation.\nInitialization schemes for INRs have been extensively studied to accelerate training convergence. IGR (Gropp et al., 2020) introduced an implicit geometric initialization to speed up learning 3D shapes, while others (Rajeswaran et al., 2019; Sitzmann et al., 2020a; Wang et al., 2021a; Tancik et al., 2021) leveraged data-driven meta-learning approaches (Finn et al., 2017; Nichol, 2018) for learning implicit fields. In contrast to these methods, which rely on computationally expensive pre-training or hand-crafted priors, our approach avoids such requirements, offering a more efficient alternative.\nRegularization strategies can be applied to stabilize the training of NeRF-based models, especially in sparse settings (Yang et al., 2023). As our interest is in hyperparameter selection, we do not test the effects of regularization on final results. Moreover, some regularization methods (Yang et al., 2023) were developed only for NeRF-like embeddings and would have to be generalized for a fair comparison. It is also worth noting that the Wasserstein distance has been already applied to improve INRS (Ramasinghe et al., 2024) through regularization, which is different from our application."}, {"title": "MOTIVATION", "content": "In this section, we show that the training of an INR highly depends on configuring the embedding in a way that aligns its frequencies with the target signal. This leaves practitioners with two options, ei-ther using a suboptimal, default embedding configuration or finding a well performing configuration through a costly parameter sweep.\nWe illustrate the impact of proper hyperparameter selection in an image representation task in Fig-ure 1 using the Siren model, which uses an input-scaling parameter $w_o$ to increase embedding fre-quencies. We compare a default, unaligned with the target signal configuration of Siren ($w_o$ = 30) to an aligned configuration ($w_o$ = 90) selected through a parameter sweep over \u03c9 \u2208 {30, . . ., 140}.\nThe aligned configuration speeds up training by employing frequencies three times greater than the baseline, achieving sharp details while the baseline is blurry. Note how the sizes of uniformly col-ored areas in the output at step 0 indicate the size of features the network can easily learn - this observation is pivotal for FreSh. We perform a similar investigation on 5 images, each from a dif-ferent dataset (see section 5), reporting the results in Table 1. We find that optimizing $w_o$ always improves the baseline results, with specific best values of $w_o$ depending on the target signal. The fail-ure of SGD to optimize the embedding layer (see Appendix A) necessitates hyperparameter sweeps, as a one-size-fits-all solution will inevitably be suboptimal for some target signals. Our goal is to perform this search and enhance INR performance while avoiding the high computational cost of repeated model re-training."}, {"title": "FRESH", "content": "In this section, we discuss how to compare the frequency contents of images and introduce FreSh, a computationally efficient method for initializing frequency embeddings that biases the model to-wards the frequencies present in the target signal."}, {"title": "PRELIMINARIES", "content": "This section discusses important theoretical concepts relevant to our study. INR architectures are discussed in the Appendix, with the exception of the Siren model (Sitzmann et al., 2020b), which we use as a high-level example to illustrate how similar approaches work. We provide a list of embedding hyperparameters from each model used in our study in Table 2.\nSiren (Sitzmann et al., 2020b) addresses spectral bias by mapping its inputs, $x \\in \\mathbb{R}^d$, through a high-frequency embedding, given as:\n$y_S(x) = sin(w_oWx + b),$\nwhere $W ~ U[-1,1]$ are the weights of the layer and b is bias. The scaling parameter $w_o$ controls the frequency magnitudes of this embedding and the authors (Sitzmann et al., 2020b) recommend setting $w_o$ = 30, but adjustments are needed to reach optimal performance (Saragadam et al., 2023).\nSiren was specifically designed so that the first layer is the sole contributor to the frequency increase inside the model, which results in the frequency distribution (spectrum) of this model being very concise."}, {"title": "FRESH", "content": "FreSh performs a parameter sweep in which the Discrete Fourier Transform and the Wasserstein distance are used to approximate model performance, instead of the costly model training required for grid search.\nMethod description. Our goal is to select an embedding configuration $\\theta_i$ from a set of M configurations {$\\theta_i$}$_{i \\in {1,...,M}}$ that would maximize performance when fitting a target image $Y \\in \\mathbb{R}^{C\\times N\\times N}$. We propose to use the configuration $\\theta_j$ where the associated output of the model at initialization, $Y_{init}$, has a similar frequency distribution to the target image, in other words $S_n(Y) \\approx S_n(Y_{init})$.\nWe note that the spectrum $S_n(A)$ is absolutely homoge-neous ($S_n(aA) = |a|S_n(A)$), which implies that scaling does not affect the relative presence of different frequen-cies. Additionally, it is equivalent to scaling the original signal, a common data pre-processing step. As such, the spectrum can be interpreted as a probability distribution, making the Wasserstein distance a natural choice for a sim-ilarity measure. This requires only that we use the normal-ized spectrum, defined as $\\hat{S}_n(A) = \\frac{S_n(A)}{||S_n(A)||_1}$. With this, we define the FreSh configuration $\\theta_j$ as the one that minimizes the Wasserstein distance between the target signal and the model, meaning\n$j = arg \\underset{i}{min} W(\\hat{S}_n(Y), \\hat{S}_n(Y_{init}))$\n= $arg \\underset{i}{min} ||CDF(\\hat{S}_n(Y)) \u2013 CDF(\\hat{S}_n(Y_{init}))||_1$.\nIn settings where multiple target images are available (video approximation and NeRF), we select one at random to calculate the Wasserstein distance. We visualize the entire selection process for images in Figure 2, and provide a full algorithm of FreSh in the Appendix (see algorithm 1).\nMeasurement noise. Due to the randomness of the image used as the target signal, Y, on video approximation and NeRF tasks and the randomness of the model output $Y_{init}$ arising from random network weights, the measurement of the Wasserstein distance is noisy. To prevent this from affect-ing the selection process, we measure the Wasserstein distance 10 times and use its mean to select"}, {"title": "LIMITATIONS", "content": "While this work represents an initial step toward automating embedding configuration for INRs, FreSh has several limitations inherited from existing embedding methods. Notably, FreSh does not account for direction-dependent frequency magnitudes, potentially impacting performance in sce-narios where directionality is important, such as video approximation. Although the spectrum size hyperparameter is easier to configure than traditional embedding parameters due to its independence from the target signal, it still can require manual tuning. Moreover, FreSh performance is inherently limited by the constraints of the embeddings it configures and cannot achieve quality better than a conventional grid search. Specific to FreSh, it is not applicable to models that utilize excessively high embedding frequencies, such as NeRF or Wire."}, {"title": "CONCLUSION", "content": "Hyperparameter selection is crucial for INR performance, yet there is limited research in this area. This makes evaluating new architectures costly due to the need for extensive grid searches, or un-fair when suboptimal hyperparameter values are used. We address these challenges by introducing FreSh, a model-agnostic method for configuring coordinate embeddings that significantly reduces the cost of finding effective configurations compared to parameter sweeps. FreSh leverages fre-quency information to select the configuration that best aligns with the target signal, effectively biasing the model to fit all relevant frequencies. While FreSh is not compatible with certain models, such as Wire, it proves highly effective when applicable, facilitating the use of improved embed-dings like Fourier features. This is particularly relevant in the context of Neural Radiance Fields (NeRF), where the adoption of this embeddings has been limited by high hyperparameter sensitiv-ity. Although FreSh introduces a new hyperparameter, it is not sensitive to the target signal and it requires little to none adjustment. By utilizing ResFields, we have observed that frequency mag-nitudes in certain tasks are significantly direction-dependent. This suggests that new embeddings may be needed to account for this dependence, and expanding FreSh to also consider directional dependencies is a promising research direction that could further enhance its effectiveness."}, {"title": "\u0391 \u039f\u03a1\u03a4\u0399MIZATION OF EMBEDDING WEIGHTS", "content": "The embedding layer of an INR requires careful tun-ing of its hyperparameters (as discussed in section 3), even though its parameters should be optimized by SGD. To explain this phenomenon, we investigate the embedding of Siren and observe a pattern similar to that noted by Finn et al. (2017) about the Fourier model: SGD fails to effectively optimize the embed-ding layer, likely due to the periodic nature of the ac-tivations used.\nWe demonstrate that SGD fails to optimize the em-bedding layer of Siren by investigating the frequency magnitudes of this embedding. Denoting the weights of the embedding layer as W and their i-th row as $w_i$, the magnitudes of embedding frequencies are given as $w_o||w_i||_2$. This follows from the form of Siren em-bedding:\nsin($w_oWx + b$).\nWe investigate how frequency magnitudes change during training on 5 images (we use the same im-ages as in section 3) by comparing the distributions of magnitudes between models at initialization and after training (see Figure 7). We additionally provide mag-nitudes of embeddings configured using grid search (see Table 1) as a reference of optimal magnitudes. During training the magnitudes change only slightly, but this increase is negligible when compared to mag-nitudes induced by optimal embeddings, whose size reflects the multiple-fold increase of $w_o$ observed in Table 1. Since SGD fails to notably increase em-bedding frequencies, the magnitudes must be adjusted as a hyperparameter, significantly increasing the cost of finding optimal models."}, {"title": "B ARCHITECTURES FROM RELATED WORKS", "content": "This section covers architectures that were left out of the main text.\nNeRF (Mildenhall et al., 2021) maps 5D coordinates - spatial location (x, y, z) and viewing direction (\u03b8, \u03c6) - to volume density and view-dependent emitted radiance, which are then used to render novel views of a scene. It employs positional embedding (Vaswani et al., 2017), which is a multiresolution sequence of L frequencies:\n$Y_p(x) = [sin(2^1x), cos(2^1x), ..., sin(2^{L-2}x), cos(2^{L-1}x)].$\n$Y_p$ helps overcome the spectral bias of MLPs but is biased toward axis-aligned directions, which can result in performance loss depending on the rotation of the target object (Tancik et al., 2020). This embedding works well with frequencies which are higher than the main components of the target signal (e.g., L = 16). When training NeRF models, we embed the spatial coordinates using $Y_p$ and the viewing direction with the spherical harmonics basis (Fridovich-Keil et al., 2022; Verbin et al., 2022), following the approach of M\u00fcller et al. (2022).\nFourier features (Tancik et al., 2020) is a direction-invariant alternative to eq. (9) which densely samples Fourier basis functions. Although such sampling is not feasible in realistic settings, it can be well approximated through random sampling (Rahimi & Recht, 2007), resulting in the mapping:\n$Y_F(X) = [sin(2\\pi Wx), cos(2\\pi Wx)],$\nwhere weights are sampled from an isotropic frequency distribution, such as Gaussian $W ~ N(0, \\sigma)$. Scale of this distribution, \u03c3, controls frequency magnitudes and it was found to be an"}, {"title": "C PSEUDOCODE FOR FRESH", "content": "We present the pseudocode for FreSh in Algorithm 1. The algorithm is slightly different between image and video/NeRF tasks, due to multiple images being available for the latter. This is reflected in the definition of $Y_{sample}$ which consist of multiple images - one for each measurement of the Wasserstein distance. It is constructed by sampling 10 images for NeRF and video approximation tasks, while for the image approximation task, $Y_{sample}$ consists of the same image repeated 10 times. Even though the target signal is not random for the image approximation task, it is also measured multiple times due to the randomness of the model output, model($\\theta$, X)."}]}