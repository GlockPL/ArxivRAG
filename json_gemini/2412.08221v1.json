{"title": "Generate Any Scene: Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming", "authors": ["Ziqi Gao", "Weikai Huang", "Jieyu Zhang", "Aniruddha Kembhavi", "Ranjay Krishna"], "abstract": "Generative models like DALL-E and Sora have gained attention by producing implausible images, such as \u201castronauts riding a horse in space.\u201d Despite the proliferation of text-to-vision models that have inundated the internet with synthetic visuals, from images to 3D assets, current benchmarks predominantly evaluate these models on real-world scenes paired with captions. We introduce GENERATE ANY SCENE, a framework that systematically enumerates scene graphs representing a vast array of visual scenes, spanning realistic to imaginative compositions. GENERATE ANY SCENE leverages 'scene graph programming,' a method for dynamically constructing scene graphs of varying complexity from a structured taxonomy of visual elements. This taxonomy includes numerous objects, attributes, and relations, enabling the synthesis of an almost infinite variety of scene graphs. Using these structured representations, GENERATE ANY SCENE translates each scene graph into a caption, enabling scalable evaluation of text-to-vision models through standard metrics. We conduct extensive evaluations across multiple text-to-image, text-to-video, and text-to-3D models, presenting key findings on model performance. We find that DiT-backbone text-to-image models align more closely with input captions than UNet-backbone models. Text-to-video models struggle with balancing dynamics and consistency, while both text-to-video and text-to-3D models show notable gaps in human preference alignment. Additionally, we demonstrate the effectiveness of GENERATE ANY SCENE by conducting three practical applications leveraging captions generated by GENERATE ANY SCENE: (1) a self-improving framework where models iteratively enhance their performance using generated data, (2) a distillation process to transfer specific strengths from proprietary models to open-source counterparts, and (3) improvements in content moderation by identifying and generating challenging synthetic data.", "sections": [{"title": "1. Introduction", "content": "Artist Marc Chagall said \u201cGreat art picks up where nature ends.\" The charm of visual content generation lies in the realm of imagination. Since their launch, Dall-E [5, 55] and Sora [7] have promoted their products with implausible generated images of \u201castronauts riding a horse in space\" and \"cats playing chess\". With the proliferation of text-to-vision generation models, the internet is now flooded with generated visual content\u2014images, videos, and 3D assets-most generated from user-provided captions [5, 7, 55]. While there are numerous benchmarks designed for evaluating these text-to-vision models, they are typically collections of real-world visual content paired with captions [9, 34, 70]. To quote Marc Chagall again, \"If I create from heart, nearly everything works; if from the head, almost nothing.\" There is a need for evaluation benchmarks that go beyond real-world scenes and evaluate how well generative models can represent the entire space of imaginary scenes.\nSuch a comprehensive evaluation requires that we first define the space of the visual content. A long list of prior work [26-28, 32, 47] has argued that scene graphs [32] are a cognitively grounded [6] representation of the visual space. A scene graph represents objects in a scene as individual nodes in a graph. Each object is modified by attributes, which describe its properties. For example, attributes can describe the material, color, size, and location of the object in the scene. Finally, relationships are edges that connect the nodes. They define the spatial, functional, social, and interactions between objects [42]. For example, in a living room scene, a \"table\" node might have attributes like \"wooden\" or \"rectangular\" and be connected to a \"lamp\" node through a relation: \"on top of.\" This systematic scene graph structure provides simple yet effective ways to define and model the scene. Make it an ideal structure for GENERATE ANY SCENE to systematically define the diverse space of the visual scenes.\nWe introduce GENERATE ANY SCENE, a system capable of efficiently enumerating the space of scene graphs representing a wide range of visual scenes, from realistic to highly imaginative. GENERATE Any Scene is powered by what we call scene graph programming, a programmatic approach for composing scene graphs of any complexity using a rich taxonomy of visual elements, and for translating each scene graph into a caption. With a space of synthetically diverse captions, we use GENERATE ANY SCENE to prompt Text-to-Vision generation models and evaluate their generations. Like any other representation, scene graphs are also limited: they don't represent tertiary relationships (e.g. \"three people playing frisbee\"). Nonetheless, they account for a large space of possibilities. To systematically define and scalably explore the space of user captions, we adopt the scene graph representation [32] to comprehensively evaluate and improve text-to-vision models.\nWe construct a rich taxonomy of visual concepts consisting of 28,787 objects, 1,494 attributes, 10, 492 relations, 2, 193 image/video/3D scene attributes from various sources. Based on these assets, GENERATE ANY SCENE can programmatically synthesize an almost infinite number of scene graphs of varying complexity [81]. Besides, GENERATE ANY SCENE allows configurable scene graph generation. For example, evaluators can specify the complexity level of the scene graph to be generated or provide a seed scene graph to be expanded. Given an initial scene graph, Generate ANY SCENE programmatically translates it into a caption, which, when combined with existing text-to-vision metrics, e.g., Clip Score [54] and VQA Score [39], can be used to evaluate any text-to-vision model [62]. By automating these steps, our system ensures both scalability and adaptability, providing researchers and developers with diverse, richly detailed scene graphs and corresponding captions tailored to their specific needs.\nWith GENERATE Any ScenE's programmatic generation capability, we release a dataset featuring 10 million diverse and compositional captions, each paired with a corresponding scene graph. This extensive dataset spans a wide range of visual scenarios, from realistic to highly imaginative compositions, providing an invaluable resource for researchers and practitioners in the Text-to-Vision generation field. We also conduct extensive evaluations of 12 text-to-image, 9 text-to-video and 5 text-to-3D models across a broad spectrum of visual scenes. We have several crucial findings: (1) DiT-backbone models show superior faithfulness and comprehensiveness to input captions than UNet-backbone models, with human-alignment data training helping to bridge some of these gaps. (2) Text-to-Video generation face challenges in balancing dynamics and consistency. (3) All Text-to-Video and Text-to-3D models we evaluate show negative ImageReward Score scores, highlighting a substantial gap in human preference alignment.\nFurther, we demonstrate the effectiveness of GENERATE ANY SCENE by conducting three practical applications leveraging captions generated by GENERATE ANY SCENE (Figure 1):"}, {"title": "Application 1: Self-improving", "content": "We show that our diverse captions can facilitate a framework to iteratively improve Text-to-Vision generation models using their own generations. Given a model, we generate multiple images, identify the highest-scoring one, and use it as new fine-tuning data to improve the model itself. We fine-tune Stable Diffusion v1-5 and achieve an average of 5% performance boost compared with original models, and this method is even better than fine-tuning with the same amount of real images and captions from the Conceptual Captions CC3M [9] over different benchmarks."}, {"title": "Application 2: Distilling limitations", "content": "Using our evaluations, we identify limitations in open-sourced models that their proprietary counterparts excel at. Next, we distill these specific capabilities from proprietary models. For example, DaLL-E 3 excels particularly in generating composite images with multiple parts. We distill this capability into Stable Diffusion v1-5, effectively bridging the gap between DaLL-E 3 and Stable Diffusion v1-5."}, {"title": "Application 3: Generated content detector", "content": "Content moderation is a vital application, especially as Text-to-Vision generation models improve. We identify which kinds of data content moderation models are bad at detecting, generate more of such content, and retrain the detectors. We train a ViT-T with our generated data and boost its detection capabilities across benchmarks."}, {"title": "2. Related work", "content": "Recent Text-to-Image generation advances are driven by diffusion models that enhance visual fidelity and semantic alignment. Some open-source models [1, 35, 51, 52, 56] use UNet backbones to refine images iteratively. In parallel, Diffusion Transformers (DiTs) architectures [11, 12, 16, 33] have emerged as a better alternative in capturing long-range dependencies and improving coherence. Proprietary models like DALL-E 3 [5] and Imagen 3 [2] still set the state-of-the-art. Based on Text-to-Image generation method, Text-to-Video generation models typically utilize time-aware architectures to ensure temporal coherence across frames [10, 19, 30, 65, 67, 74, 80, 84]. In Text-to-3D generation, recent proposed models [38, 44, 53, 66, 69] integrate the diffusion models with Neural Radiance Fields (NeRF) rendering to generate diverse 3D object. In this work, we systematically evaluate and deeply analyze these Text-to-Vision generation models.\nPrompts for Text-to-Vision generation models vary greatly in diversity, complexity, and compositionality. This variation makes it challenging and costly to collect large-scale and diverse prompts written by humans. Consequently, synthetic prompts have been widely used for both training [36, 37, 43, 48, 62, 63, 71, 76, 82, 83] and evaluation\nFor example, training methods like LLM-Grounded Diffusion [37] leverage LLM-generated synthetic text prompts to enhance the model's understanding and alignment with human instruction. For evaluation, benchmarks such as T2I-CompBench [23] and T2V-CompBench [63] utilize benchmarks generated by LLMs. However, while LLMs can generate large-scale natural prompts, controlling their content is difficult, and they might exhibit systematic bias. In this work, we propose a programmatic scene graph-based prompt generation system that can generate infinitely diverse scene graph-based prompts for evaluating and improving Text-to-Vision generation models.\nTo accommodate the diverse applications and personalization needs in text-to-vision models, numerous fine-tuning techniques have been developed. For example, LoRA [21] reduces the computational resources required for fine-tuning by approximating weight updates with low-rank matrices, enabling efficient adaptation to new tasks. Textual Inversion [17, 46] introduces new word embeddings to represent novel concepts, allowing models to generate images of user-specified content without extensively altering the original parameters. DreamBooth [57] fine-tunes models on a small set of personalized images to capture specific subjects or styles, facilitating customized content generation. DreamSync [62], provides a novel way to enable models to learn from the high-quality output generated by itself. In this work, we use several fine-tuning techniques with our programmatic scene graph-based prompts to improve Text-to-Vision generation models."}, {"title": "3. Generate Any Scene", "content": "We present our implementation of GENERATE ANY SCENE system. (Figure 2) It programmatically synthesizes diverse scene graphs in terms of both structure and content and translates them into corresponding captions.\nScene graph. A scene graph is a structured representation of a visual scene, where objects are represented as nodes, their attributes (such as color and shape) are properties of those nodes, and their relationships (such as spatial or semantic connections) are represented as edges. In recent years, scene graphs have played a crucial role in visual understanding tasks, such as those found in Visual Genome [32] and GQA [25] for visual question answering (VQA). Their utility has expanded to various Text-to-Vision generation tasks. For example, the DSG score [13] leverages MLMs to evaluate how well captions align with generated scenes by analyzing scene graphs.\nTo construct a scene graph, we use three main metadata types: objects, attributes, and relations. We also have scene attributes that capture the board aspect of the caption, such as art style, to create a complete visual caption. The numbers and the source of our metadata are illustrated in Table 1. Additionally, we build a taxonomy that categorizes metadata into distinct levels and types, enabling fine-grained analysis. This structure allows for detailed assessments, such as evaluating model performance on \"flower\" as a general concept and on specific sub-categories like \"daisy.\u201d More details in Appendix C."}, {"title": "3.1. Scene graph programming", "content": "Our system first generates and stores a variety of scene graph structures based on a specified level of complexity, defined by the total number of objects, relationships, and attributes in each graph. The process begins by determining the number of object nodes, and then by systematically enumerating different combinations of relationships among these objects and their associated attributes. Once all graph structures meeting the complexity constraint are enumerated, they are stored in a database for later use. This enumeration process is executed only once for each level of complexity, allowing us to efficiently query the database for suitable templates when needed.\nGiven a scene graph structure, the next step involves populating the graph with metadata. For each object node, attribute node, and relation edge, we sample the corresponding content from our metadata. This process is highly customizable: users can define the topics and types of metadata to be included (e.g., selecting only common metadata or specifying particular relationships between particular objects, among other options). By determining the scope of metadata sampling, we can precisely control the final content of the captions and easily extend the diversity and richness in the scene graphs by incorporating new datasets.\nIn addition to scene graphs that capture the visual content of the image, we also include scene attributes that describe aspects such as the art style, viewpoint, time span (for video), and 3D attributes (for 3D content). These scene attributes are sampled directly from our metadata, creating a list that provides contextual details to enrich the description of the visual content.\nWe introduce an algorithm that converts scene graphs and a list of scene attributes into captions. The algorithm processes the scene graph in topological order, transforming each object, its attributes, and relational edges into descriptive text. To maintain coherence, it tracks each concept's occurrence, distinguishing objects with identical names using terms like \"the first\" or \"the second.\" Objects that have been previously"}, {"title": "4. Evaluating Text-to-Vision generation models", "content": "Details of experiment settings are in Appendix D."}, {"title": "4.1. Experiment Settings", "content": "We conduct experiments on 12 Text-to-image models [1, 5, 11, 12, 16, 33, 35, 51, 52, 56], 9 Text-to-Video models [10, 19, 30, 61, 65, 67, 74, 80, 84], and 5 Text-to-3D models [38, 44, 53, 66, 69]. Text-to-image models are evaluated at a resolution of 1024 \u00d7 1024 pixels. We standardize the frame length to 16 across all Text-to-Video models for fair comparisons. For Text-to-3D, we generate videos by rendering from 120 viewpoints.\nAcross all Text-to-Vision generation tasks, we use Clip Score [8] (semantic similarity), VQA Score [39] (faithfulness), TIFA Score [13, 22] (faithfulness), Pick Score [31] (human preference), and ImageReward Score [77] (human preference) as general metrics, and for Text-to-Video generation, VBench [24] for fine-grained video analysis like consistency and dynamics.\nWe evaluate our Text-to-Image generation and Text-to-Video generation models on 10K randomly generated captions, with scene graph complexity ranging from 3 to 12 and scene attributes from 0 to 5, using unrestricted metadata. For Text-to-3D generation models, due to their limitations in handling complex captions and time-intensive generation, we restrict scene graph complexity to 1-3, scene attributes to 0-2, and evaluate on 1K captions."}, {"title": "4.2. Overall results", "content": "We evaluate Text-to-Image generation, Text-to-Video generation, and Text-to-3D generation models on GENERATE ANY SCENE. Here, we only list key findings; more details and raw results can be found in Appendix D."}, {"title": "Text-to-Image generation results. (Figure 3)", "content": "DiT-backbone models outperform UNet-backbone models on VQA Score and TIFA Score, indicating greater faithfulness and comprehensiveness to input captions.\nDespite using a UNet architecture, Playground v2.5 achieves higher Pick Score and ImageReward Score scores than other open-source models. We attribute this to Playground v2.5 's alignment with human preferences achieved during training.\nThe closed-source model DaLL-E 3 maintains a significant lead in VQA Score, TIFA Score, and ImageReward Score, demonstrating strong faithfulness and alignment with prompts across generated content."}, {"title": "Text-to-Video generation results. (Table 2,3)", "content": "Text-to-video models face challenges in balancing dynamics and consistency (Table 3). This is especially evident in Open-Sora 1.2, which achieves high consistency but minimal dynamics, and Text2Video-Zero, which excels in dynamics but suffers from frame inconsistency.\nAll models exhibit negative ImageReward Score (Table 2), suggesting a lack of human-preferred visual appeal in the generated content, even in cases where certain models demonstrate strong semantic alignment.\nVideoCrafter2 strikes a balance across key metrics, leading in human-preference alignment, faithfulness, consistency, and dynamic."}, {"title": "Text-to-3D generation results. (Table 4)", "content": "ProlificDreamer outperforms other models, particularly in ImageReward Score, VQA Score and TIFA Score.\nAll models receive negative ImageReward Score scores, highlighting a significant gap between human preference and current Text-to-3D generation generation capabilities."}, {"title": "5. Application 1: Self-Improving Models", "content": "In this section, we explore how GENERATE ANY SCENE facilitates a self-improvement framework for model generation capabilities. By programmatically generating scalable compositional captions from scene graphs, GENERATE ANY SCENE expands the textual and visual space, allowing for a diversity of synthetic images that extend beyond real-world scenes. Our goal is to utilize these richly varied synthetic images to further boost model performance.\nInspired by Dream-Sync [62], we designed an iterative self-improving framework using GENERATE ANY SCENE with Stable Diffusion v1-5 as the baseline model. With VOA Score, which shows strong correlation with human evaluations on compositional images [39], we guide the model's improvement throughout the process.\nSpecifically, GENERATE ANY SCENE generates 3 \u00d7 10K captions across three epochs. For each caption, Stable Diffusion v1-5 generates 8 images, and the image with the highest VQA Score is selected. From each set of 10K optimal images, we then select the top 25% (2.5k image-caption pairs) as the training data for each epoch. In subsequent epochs, we use the fine-tuned model from the prior iteration to generate new images. We employ LoRA [21] for parameter-efficient fine-tuning. Additional details are available in Appendix E.\nTo evaluate the effectiveness of self-improvement using synthetic data generated by Generate Any Scene, we conduct comparative experiments with the CC3M dataset, which comprises high-quality and diverse real-world image-caption pairs [60]. We randomly sample 3 \u00d7 10K captions from CC3M, applying the same top-score selection strategy for iterative fine-tuning of Stable Diffusion v1-5. Additionally, we include a baseline using random-sample fine-tuning strategy to validate the advantage of our highest-scoring selection-based strategy.\nWe evaluate our self-improving pipeline on Text-to-Vision generation benchmarks, including GenAI Bench [34]. For the Text-to-Video generation task, we use Text2Video-Zero as the baseline model, substituting its backbone with the original Stable Diffusion v1-5 and our fine-tuned Stable Diffusion v1-5 models.\nOur results show that fine-tuning with GENERATE ANY SCENE-generated synthetic data consistently outperforms CC3M-based fine-tuning across Text-to-Vision generation tasks (Figure 4), achieving the highest gains with our highest-scoring selection strategy. This highlights GENERATE ANY SCENE's scalability and compositional diversity, enabling models to effectively capture complex scene structures. Additional results are in Appendix F."}, {"title": "Takeaway for application 1", "content": "Iterative self-improving Text-to-Vision generation models with compositional and diverse synthetic captions can surpass fine-tuning with real-world image-caption data.\nThe compositional, synthetic captions generated by Generate AnNY SCENE exhibit greater diversity than real-world data."}, {"title": "6. Application 2: Distilling limitations", "content": "Although self-improving with Generate Any Scene-generated data shows clear advantages over high-quality real-world datasets, its efficiency remains inherently constrained by the limitations of the model's own generation ability. To address this, we leverage the taxonomy and programmatic generation capabilities within GENERATE ANY SCENE to identify specific strengths of proprietary models (DaLL-E 3), and to distill these capabilities into open-source models. More details are in Appendix F."}, {"title": "Takeaway for application 2", "content": "Targeted fine-tuning can distill proprietary model strengths, effectively bridging gaps in compositionality and concept handling for open-source models.\nGENERATE ANY SCENE facilitates fine-grained analysis to identify specific performance gaps, enabling targeted data selection to distill limitations."}, {"title": "7. Application 3: Generated content detector", "content": "Advances in Text-to-Vision generation underscore the need for effective content moderation [50]. Major challenges include the lack of high-quality and diverse datasets and the difficulty of generalizing detection across models Text-to-Vision generation [29, 68]. Generate Any SCENE addresses these issues by enabling scalable, programmatic generation of compositional captions, increasing the diversity and volume of synthetic data. This approach enhances existing datasets by compensating for their limited scope-from realistic to imaginative-and variability.\nTo demonstrate GENERATE ANY SCENE's effectiveness in training generated content detectors, we used the D3 dataset [3] as a baseline. We sampled 5k captioned real and SDv1.4 generated image pairs from D\u00b3 and generated 5k additional images with GENERATE ANY SCENE captions. We trained a ViT-T [73] model with a single-layer linear classifier, varying dataset sizes with N real and N synthetic images. For synthetic data, we compared N samples solely from D\u00b3 with a mixed set of N/2 from GENERATE ANY SCENE and N/2 from D\u00b3, keeping the same training size.\nWe evaluate the detector's generalization on the GenImage [85] validation set and images generated using GENERATE ANY SCENE captions. Figure 7 demonstrates that combining GENERATE ANY SCENE-generated images with real-world captioned images consistently enhances detection performance, particularly across cross-model scenarios and diverse visual scenes. More details are in Appendix G."}, {"title": "Takeaway for application 3", "content": "Compositional synthetic captions robustify generated content detectors.\nGENERATE ANY SCENE can generate more diverse captions to complement real-world image-caption training data by enriching compositional variety and imaginative scope."}, {"title": "8. Limitation", "content": "Programmatically generated prompts can be unrealistic and biased. Programmatically generated prompts can be unrealistic and biased. Although our system is capable of producing a wide range of rare compositional scenes and"}, {"title": "A. More Analysis with GENERATE ANY SCENE", "content": "With GENERAte Any ScenE, we can generate infinitely diverse and highly controllable prompts. Using GENER-ATE ANY SCENE, we conduct several analyses to provide insights into the performance of today's Text-to-Vision generation models."}, {"title": "A.1. Performance analysis across caption properties", "content": "In this section, we delve into how model performance varies with respect to distinct properties of GENERATE ANY SCENE captions. While GENERATE ANY SCENE is capable of generating an extensive diversity of captions, these outputs inherently differ in key characteristics that influence model evaluation. Specifically, we examine three properties of the caption: Commonsense, Perplexity, and Scene Graph Complexity (captured as the number of elements in the captions). These properties are critical in understanding how different models perform across a spectrum of linguistic and semantic challenges presented by captions with varying levels of coherence, plausibility, and compositional richness."}, {"title": "Perplexity", "content": "Perplexity is a metric used to measure a language model's unpredictability or uncertainty in generating a text sequence. A higher perplexity value indicates that the sentences are less coherent or less likely to be generated by the model.\nAs shown in Figure 8, From left to right, when perplexity increases, indicating that the sentences become less reasonable and less typical of those generated by a language model, we observe no clear or consistent trends across all models and metrics. This suggests that the relationship between perplexity and model performance varies depending on the specific model and evaluation metric."}, {"title": "Commonsense", "content": "Commonsense is an inherent property of text. We utilize the Vera Score [40], a metric generated by a fine-tuned LLM to evaluate the text's commonsense level.\nAs shown in Figure 9, from left to right, as the Vera Score increases indicating that the captions exhibit greater commonsense reasoning\u2014we observe a general improvement in performance across all metrics and models, except for Clip Score. This trend underscores the correlation"}, {"title": "Element Numbers (Complexity of Scene Graph)", "content": "Finally, we evaluate model performance across total element numbers in the captions, which represent the complexity of scene graphs (objects + attributes + relations).\nFrom left to right, the complexity of scene graphs becomes higher, reflecting more compositional and intricate captions. Across most metrics and models, we observe a noticeable performance decline as the scene graphs become more complex. However, an interesting exception is observed in the performance of DaLL-E 3. Unlike other models, DaLL-E 3 performs exceptionally well on VQA Score and TIFA Score, particularly on VQA Score, where it even shows a slight improvement as caption complexity increases. This suggests that DaLL-E 3 may have a unique capacity to handle complex and compositional captions effectively."}, {"title": "A.2. Analysis on different metrics", "content": "Compared with most LLM and VLM benchmarks that use multiple-choice questions and accuracy as metrics. There is no universal metric in evaluating Text-to-Vision generation models. Researchers commonly used model-based metrics like Clip Score, VQA Score, etc. Each of these metrics is created and fine-tuned for different purposes with bias. Therefore, we also analysis on different metrics."}, {"title": "Clip Score isn't a universal metric", "content": "Clip Score is one of the most widely used metrics in Text-to-Vision generation for evaluating the alignment between visual content and text. However, our analysis reveals that Clip Score is not a perfect metric and displays some unusual trends. For instance, as shown in Figures 8, 9, and 10, we compute the perplexity across 10k prompts used in our study, where higher perplexity indicates more unpredictable or disorganized text. Interestingly, unlike other metrics, Clip Score decreases as perplexity lowers, suggesting that Clip Score tends to favor more disorganized text. This behavior is counterintuitive and highlights the potential limitations of using Clip Score as a robust alignment metric."}, {"title": "Limitations of human preference-based metrics", "content": "We use two metrics fine-tuned using human preference data:"}, {"title": "A.3. Fairness analysis", "content": "We evaluate fairness by examining the model's performance across different genders and races. Specifically, we calculate the average performance for each node and its associated child nodes within the taxonomy tree constructed for objects. For example, the node \"females\" includes child nodes such as \u201cwaitresses,\" and their combined performance is considered in the analysis."}, {"title": "Gender", "content": "In gender, we observe a notable performance gap between females and males, as could be seen from Figure 11, Models are better at generating male concepts."}, {"title": "Race", "content": "There are also performance gaps in different races. From Figure 12, we found that \"white (person)\" and \"black (person)\" perform better than \"asian (person)\", \"Indian (amerindian)\", and \"Latin American\"."}, {"title": "B. Correlation of GENERATE ANY SCENE with other Text-to-Vision generation benchmarks", "content": "The GENERATE Any Scene benchmark uniquely relies entirely on synthetic captions to evaluate models. To assess the transferability of these synthetic captions, we analyzed the consistency in model rankings across different benchmarks [34, 75, 76]. Specifically, we identified the overlap of models evaluated by two benchmarks and computed the Spearman correlation coefficient between their rankings.\nAs shown in the figure 13, Generate Any Scene demonstrates a strong correlation with other benchmarks, such as Conceptmix [76] and GenAI Bench [34], indicating the robustness and reliability of GENERATE ANY SCENE'S synthetic caption-based evaluations. This suggests that the synthetic captions generated by GENERATE ANY SCENE can effectively reflect model performance trends, aligning closely with those observed in benchmarks using real-world captions or alternative evaluation methods."}, {"title": "C. Details of Taxonomy of Visual Concepts", "content": "To construct a scene graph, we utilize three primary types of metadata: objects, attributes, and relations, which represent the structure of a visual scene. Additionally, scene attributes\u2014which include factors like image style, perspective, and video time span\u2014capture broader aspects of the visual content. Together, the scene graph and scene attributes form a comprehensive representation of the scene.\nOur metadata is further organized using a well-defined taxonomy, enhancing the ability to generate controllable prompts. This hierarchical taxonomy not only facilitates the creation of diverse scene graphs, but also enables fine-grained and systematic model evaluation."}, {"title": "Objects", "content": "To enhance the comprehensiveness and taxonomy of object data, we leverage noun synsets and the structure of WordNet [45]. In WordNet, a physical object is defined as \"a tangible and visible entity; an entity that can cast a shadow.\" Following this definition, we designate the physical object as the root node, constructing a hierarchical tree with all 28,787 hyponyms under this category as the set of objects in our model."}, {"title": "D. Details of Overall Performance (Section 3)", "content": "Details of experiment settings"}, {"title": "D.1. Detailed experiment settings", "content": "For Text-to-Image generation, we select a range of open-source models, including those utilizing UNet backbones, such as DeepFloyd IF [1], Stable Diffusion v2-1 [56], SDXL [52], Playground v2.5 [35], and Wuerstchen v2 [51], as well as models with DiT backbones, including Stable Diffusion 3 Medium [16], PixArt-a [11], PixArt-\u03a3 [12], FLUX.1-schnell [33], FLUX.1-dev [33], and FLUX 1. Closed-source models, such as DaLL-E 3 [5] and FLUX1.1 PRO [33], are also assessed to ensure a comprehensive comparison. All models are evaluated at a resolution of 1024 x 1024 pixels.\nFor Text-to-Video generation, we select nine open-source models: ModelScope [67], ZeroScope [61], Text2Video-Zero [30], CogVideoX-2B [80], VideoCraft2 [10], AnimateLCM [65], AnimateDiff [19], FreeInit [74], and Open-Sora 1.2 [84]. We standardize the frame length to 16 across all video models for fair comparisons.\nFor Text-to-3D generation, we evaluate five recently proposed models: SJC [66], DreamFusion [53], Magic3D [38], Latent-NeRF [44], and Prolific-Dreamer [69]. We employ the implementation and configurations provided by ThreeStudio [18] and generate videos by rendering from 120 viewpoints. To accelerate inference, we omit the refinement stage. For Magic3D and DreamFusion, we respectively use DeepFloyd IF and Stable Diffusion v2-1 as their 2D backbones."}, {"title": "Metrics", "content": "Across all Text-to-Image generation, Text-to-Video generation, and Text-to-3D generation, we employ five widely used Text-to-Vision generation metrics to comprehensively assess model performance:\nClip Score: Assesses semantic similarity between images and text.\nVQA Score and TIFA Score: Evaluate faithfulness by generating question-answer pairs and measuring answer accuracy from images.\nPick Score and ImageReward Score: Capture human preference tendencies.\nWe also use metrics in VBench [24] to evaluate Text-to-Video generation models on fine-grained dimensions, such as consistency and dynamics, providing detailed insights into video performance."}, {"title": "D.2. Detailed overall results", "content": "We evaluate Text-to-Image generation, Text-to-Video generation, and Text-to-3D generation models on GENERATE ANY SCENE. The detailed results of each model and each metric are shown in Tabs. 5 to 8"}, {"title": "D.3. Case study: Pairwise fine-grained model comparison", "content": "Evaluating models using a single numerical average score can be limiting, as different training data often lead models to excel in generating different types of concepts. By leveraging the taxonomy we developed for GENERATE ANY SCENE, we can systematically organize these concepts and evaluate each model's performance on specific concepts over the taxonomy. This approach enables a more detailed comparison of how well models perform on individual concepts rather than relying solely on an overall average score. Our analysis revealed that, while the models may achieve similar average performance, their strengths and weaknesses vary significantly across different concepts. Here we present a pairwise comparison of models across different metrics."}, {"title": "E. Details of Application 1: Self-Improving Models (Section 4)", "content": "Experiment details"}, {"title": "E.1. Experiment details", "content": "To evaluate the effectiveness of our iterative self-improving Text-to-Vision generation model, we generated three distinct sets of 10k captions using GENERATE ANY SCENE, covering a sample complexity range from 3 to 12. These captions were programmatically created to reflect a spectrum of structured scene graph compositions, designed to challenge and enrich the model's learning capabilities.\nFor comparative analysis, we leveraged the Conceptual Captions (CC3M) [9"}]}