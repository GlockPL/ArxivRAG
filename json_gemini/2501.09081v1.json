{"title": "Inferring Transition Dynamics from Value Functions", "authors": ["Jacob Adamczyk"], "abstract": "In reinforcement learning, the value function is typically\ntrained to solve the Bellman equation, which connects the\ncurrent value to future values. This temporal dependency\nhints that the value function may contain implicit informa-\ntion about the environment's transition dynamics. By rear-\nranging the Bellman equation, we show that a converged\nvalue function encodes a model of the underlying dynam-\nics of the environment. We build on this insight to propose\na simple method for inferring dynamics models directly from\nthe value function, potentially mitigating the need for explicit\nmodel learning. Furthermore, we explore the challenges of\nnext-state identifiability, discussing conditions under which\nthe inferred dynamics model is well-defined. Our work pro-\nvides a theoretical foundation for leveraging value functions\nin dynamics modeling and opens a new avenue for bridging\nmodel-free and model-based reinforcement learning.", "sections": [{"title": "Introduction", "content": "The use of reinforcement learning (RL) for solving sequen-\ntial decision-making tasks has grown substantially in recent\nyears, demonstrating its potential to discover novel solu-\ntions in complex, unknown environments. Among RL ap-\nproaches, model-based methods\u2014which involve learning or\nleveraging a model of the environment's dynamics-have\noften shown superior sample efficiency compared to model-\nfree methods, particularly in tasks with limited interaction\nbudgets. This advantage stems from the ability of model-\nbased approaches to plan over imagined trajectories, using\ntools like Monte Carlo tree search to evaluate and improve\npolicies. However, these methods often come at the cost of\nhigher computational requirements and rely on the accuracy\nof the learned dynamics model.\nDespite the promise of model-based techniques, learning\nan accurate model of the environment remains a significant\nchallenge, particularly in domains where simulators are en-\ntirely unavailable or expensive to develop. Even with a well-\ncrafted simulator, discrepancies between simulated and real-\nworld dynamics (i.e., the \"sim-to-real\" gap) can degrade the\nperformance of policies at test time. These challenges high-\nlight the need for efficient and reliable ways to learn and use\ndynamics models, especially in situations where the envi-\nronment cannot be directly explored.\nIn this work, we propose a method for inferring a dynam-\nics model from pre-trained value functions. Our approach\nbuilds on a new perspective of the Bellman equation, which\nlies at the core of RL algorithms. Traditionally, the Bellman\nequation is used to relate the value of a state to the expected\nvalue of its successor states. This temporal relationship sup-\nplies information about the environment's transition dynam-\nics to the value function. By a simple rearrangement of the\nBellman equation, we make this dynamical information ac-\ncessible, allowing one to recover a model of the environment\ndirectly from a previously computed value function.\nThis insight opens new possibilities for model-based RL.\nBy re-purposing pre-trained value functions to infer dynam-\nics models, we can potentially enhance task performance in\nsettings like multi-task RL, where the inferred model can be\nused to solve new tasks with changing reward functions. Our\nmethod not only leverages existing value functions more ef-\nfectively, but also provides a step toward bridging the model-\nfree and model-based communities in RL."}, {"title": "Motivation", "content": "In many reinforcement learning (RL) workflows, the value\nfunction is trained using interactions with the environment\nfor a fixed reward function. Once trained, the value func-\ntion (or the derived policy) is often saved and used solely\nfor evaluation purposes, with limited opportunities for reuse\nwhen new tasks arise. However, in most practical scenarios,\ntasks share a common environment structure but otherwise\ndiffer only in their reward specifications. For example, in\nrobotics, the physical dynamics of the system remain con-\nstant, while the task objectives (e.g., picking up specific ob-\njects; moving to specific target locations) will vary.\nThis observation motivates the need for better leveraging\npre-trained value functions, which will be particularly use-\nful in settings where the reward functions are handcrafted,\nknown, or even learned. If a dynamics model is accessi-\nble, the agent can utilize this model and the known reward\nfunction to solve new tasks, either deriving exact solutions\nor providing strong initializations for further training. Such\nan approach can significantly reduce the burden of learning\nfrom scratch for each new task.\nIn offline RL, a pre-collected dataset is used to train an"}, {"title": "Background", "content": "In this section we will introduce the relevant background\nmaterial for reinforcement learning and required definitions."}, {"title": "Reinforcement Learning", "content": "We will consider discrete or continuous state spaces and\ndiscrete action spaces\u00b9. The RL problem is then mod-\neled by a Markov Decision Process (MDP), which we\nrepresent by the tuple (S, A, p, r, \u03b3) with state space S;\naction space A; potentially stochastic transition function\n(dynamics) p: S \u00d7 A \u2192 S; bounded, real reward function\nr : S \u00d7 A \u2192 R; and the discount factor \u03b3 \u2208 [0, 1).\nThe principle objective in RL is to maximize the total dis-\ncounted reward expected under a policy \u03c0. That is, to find\n\u03c0* that maximizes the following sum of expected rewards:\n\u03c0* = arg max E\u03c0[\u2211\u03b3t r(st, at)]."}, {"title": "Preliminaries", "content": "For the theoretical discussion in later sections, we will need\nseveral definitions and lemmas, which we present in this\n\u00b9If the policy expectation over continuous action spaces can be\ncalculated exactly, the derived results are equally valid."}, {"title": "Results", "content": "We begin by noting the Bellman equation does not only hold\nfor the optimal policy as shown in Equation (2), but also\nholds for arbitrary policies:\nQ\u03c0(s, a) = r(s, a) + \u03b3Es'\u223cp(\u00b7|s,a)V\u03c0(s'),\nwhere V\u03c0 is the (state) value function defined by the expec-\ntation over the policy: V\u03c0(s) = Ea'\u223c\u03c0(\u00b7|s')Q\u03c0(s', a'). We\nwrite Q\u03c0 and V\u03c0 throughout to emphasize that a model can\nbe obtained from the value of any policy (not just the optimal\npolicy) and under regularization (e.g. MaxEnt RL (Haarnoja\net al. 2018b)).\nA simple rearrangement of this equation \u201csolving\u201d for s'\ngives, under deterministic dynamics\nIndeed, under suitable assumptions, Eq. (5) can be in-\nverted to find the transition function:\nProposition 1. Under Assumptions 1 and 2, if an inverse\nstate-value function exists, the successor state can be iden-\ntified:\ns' = f(s, a) = V\u03c0\u22121(Q\u03c0(s, a) \u2212 r(s, a)/\u03b3).\nThis initial result shows that with the right assumptions,\none can use an exact value function to calculate any suc-\ncessor state. However, obtaining an exact Q-function is im-\npractical, so we instead consider a value function suffering\na globally bounded error: that is, we suppose an \u03b5-accurate\nvalue function Q\u201d is given. In the following two sections, we\nconsider the case of continuous state spaces (where we use\nthe reverse Lipschitz assumption) and the case of discrete\nstate spaces (where we introduce a new definition necessary\nfor identifiability)."}, {"title": "Theory for Continuous Spaces", "content": "In the setting of an \u03b5-accurate value, the function V\u03c0(s')\ncannot be inverted exactly, but the next-state can still be\nidentified within an interval, leading to an extension of\nProposition 1:\nTheorem 1. Given an \u03b5-accurate value function\nQ\u03c0(s, a) with reverse Lipschitz constant L, the er-\nror in estimating the next-state s' from any (s, a) is\nupper bounded:\ns'\u2212s'L\u03b5.\nThe proof and an intuitive visualization of Theorem 1 is\nprovided in the Appendix. The error accumulates in both the\nqueried value function V\u03c0(s') and also the \u201cscanned\u201d value,\nwhich we denote V (which depends on (s, a) through the\nright-hand side of Eq. (5)). With a lower bound on the value\nfunction's derivative, this region translates to a confidence\ninterval over state space S.\nIn practice, where obtaining exact value functions is in-\nfeasible, Theorem 1 demonstrates that even inexact value\nfunctions can still be effective for deriving reliable models."}, {"title": "Theory for Discrete Spaces", "content": "In the case of discrete states, we need an alternative way to\nensure the \u201cfunction\u201d (now a table) V\u03c0(s') remains invert-\nible. A necessary condition for all states to be identifiable is\nthat the corresponding values be distinct:\nDefinition 3 (\u03b4-Separable Value Function). A state value\nfunction V is said to be \u03b4-separable if there exists a \u03b4 > 0\nsuch that\n|V(s) \u2212 V(x)| > \u03b4,\nfor all s, x \u2208 S such that s \u2260 x.\nFinding sufficient structural assumptions for such separa-\nbility seems to be a challenging problem in itself, which may\nbe of independent interest. Nevertheless, if \u03b4-separability\ncan be assumed (or determined a posteriori), then identi-\nfiability holds. The following result ensures the next-state\nprediction problem remains identifiable, even in the case of\nerrors:\nTheorem 2 (Successor-State Identifiability). Sup-\npose the true value function (V\u03c0) is \u03b4-separable and\nan \u03b5-accurate estimate of the value function (V\u03c0)\nis given. If \u03b5 < \u03b4(2\u03b3\u22121 + 2)\u22121, then the dynamics\nmodel is identifiable.\nTheorem 2 is the analogue of Theorem 1 in the discrete\ncase. Again, this result highlights that even inaccurate value\nfunctions can provide robust dynamics models which in this\ncase return the exact successor state. The proof of this result\ncan be found in the Appendix. Interestingly, in both con-\ntinuous and discrete spaces, our analysis formally suggests\nthat larger discount factors improve the model accuracy by\n(a) reducing the uncertainty in Theorem 1 and (b) increasing\nthe minimum tolerance for identifiability in Theorem 2."}, {"title": "Experiments", "content": "As a proof of concept, we first consider a simple experi-\nment in the tabular setting to verify our theoretical results.\nHere, the value function can be solved exactly with suffi-\nciently many iterations of the Bellman optimality operator.\nIn the following, we thus use the policy \u03c0 = \u03c0\u2217, though\nthe framework is agnostic to such a choice. Since the state\nspace is discrete, we treat the Q function with a max over\naction dimension as a lookup table, comparing its entries to\nthe value of V, calculated from the right-hand side of Eq. (6).\nWe choose the index whose corresponding value is closest\nto V, and the state index is considered the successor state\nprediction, s'. Indeed, when the value function is calculated\nto high precision, the accuracy of our method remains con-\nsistent: for all possible state-action pairs, the corresponding\nsuccessor state is predicted successfully (corresponding to\n100% on the vertical axis of Fig. 1).\nTo further test the accuracy of our model, in connec-\ntion to the theoretical results derived, we compute the ac-\ncuracy (again over all state-action pairs) for increasingly\nlower precision (that is, larger values of \u03b5, in the defini-\ntion of \u03b5-accurate value function). To validate the idea of\n\u03b4-separability, we also prepare 20 reward-varying MDPs\n(with the same dynamics) each having a similar value gap,\n\u03b4 (within 1% of the stated value). Due to space constraints,\nwe give a full description of the experiment in the Appendix.\nWe find these experiments support the result of Theorem 2."}]}