{"title": "IIU: Independent Inference Units for Knowledge-based Visual Question Answering", "authors": ["Yili Li", "Jing Yu", "Keke Gai", "Gang Xiong"], "abstract": "Knowledge-based visual question answering requires external knowledge beyond visible content to answer the question correctly. One limitation of existing methods is that they focus more on modeling the inter-modal and intra-modal correlations, which entangles complex multimodal clues by implicit embeddings and lacks interpretability and generalization ability. The key challenge to solve the above problem is to separate the information and process it separately at the functional level. By reusing each processing unit, the generalization ability of the model to deal with different data can be increased. In this paper, we propose Independent Inference Units (IIU) for fine-grained multi-modal reasoning to decompose intra-modal information by the functionally independent units. Specifically, IIU processes each semantic-specific intra-modal clue by an independent inference unit, which also collects complementary information by communication from different units. To further reduce the impact of redundant information, we propose a memory update module to maintain semantic-relevant memory along with the reasoning process gradually. In comparison with existing non-pretrained multi-modal reasoning models on standard datasets, our model achieves a new state-of-the-art, enhancing performance by 3%, and surpassing basic pretrained multi-modal models. The experimental results show that our IIU model is effective in disentangling intra-modal clues as well as reasoning units to provide explainable reasoning evidence. Our code is available at https://github.com/Lilidamowang/IIU.", "sections": [{"title": "1 Introduction", "content": "The knowledge-based visual question answering task requires agent to answer questions according to visual content and external knowledge. Compared to AI agent, human can easily combine visual observation with external knowledge to answer questions. Recent development on multi-modal inference mainly focuses on incorporating multi-modal information in the reasoning process. [13] uses GCN to reason on fact graphs"}, {"title": "2 Related Work", "content": "Visual Question Answering. Visual Question Answering (VQA) task requires agent to answer questions according to visual content, which needs the ability to understand and process multi-modal information. In order to reduce the impact of redundant information, [19] [2] apply the attention mechanism to VQA tasks from different perspectives. A trend to solve VQA task is to represent information through graph structure and complete reasoning on the graph. [17] and [9] describe the objects and relationships in the image by constructing a scene graph. This method is also effective for other visual tasks. In knowledge-based visual question answering tasks, instead of building a scene graph, [13] build a fact graph and each node is represented by the fixed form of image-question-entity embedding. However, the visual information is wholly provided which may introduce redundant information for prediction. To reduce noise and combine information from different modalities, [22] depict an image by multi-layer graphs and perform cross-modal heterogeneous graph reasoning on them to capture complementary evidence from different layers that are most relevant to the question. However, reasoning on graph still focuses on modeling the inter-modal and intra-modal relationships in the Visual Question Answering task. Lack of more fine-grained reasoning methods to extract complex clues within the modal and increase the interpretability.\nSeparate Recurrent Models. In multi-step reasoning for VQA tasks, recurrent models like GRU are commonly used. However, our objective is to implement an independent reasoning mechanism. IndRNN [10], a distinct separate recurrent model where each unit operates independently, lays the foundation for this approach. Building on this, RIM [6] introduces sparse inter-unit communication via an attention mechanism. Drawing inspiration from RIM, our model's inference units not only reason independently but also gather complementary information."}, {"title": "3 Methodology", "content": "The task aims to predict an answer while reasoning uses external knowledge to construct memory graphs. The structure of our model is shown in Figure 2. We depict an image by three layers of graphs, including visual graph, semantic graph, and fact graph respectively, imitating the understanding of various properties of an object and relationships. Then we perform independent reasoning based on memory graph by Independent Inference Module which consists of two parts. By stacking the two processes multiple times, our model realizes the reasoning of disentangling information from the functional level and reduces the influence of redundant information."}, {"title": "3.1 Multi-Modal Graph Construction", "content": "As shown in Figure 2, we utilize Multi-modal Heterogeneous Graph which is proposed in [22] to represent the question-image pairs. Specifically, the nodes in the fully-connected visual graph are objects extracted by Faster R-CNN, and edges are the relative spatial relationships between two objects. In addition to visual information, we use dense captions to extract a set of local-level semantics in an image. Then utilize semantic graph parsing model [1] to construct Semantic Graph, where nodes are the name or attribute and edges are relationships. To find the optimal supporting-fact, we retrieve relevant candidate facts from the knowledge base following a score-based approach [13]. Each node in Fact Graph denotes an entity in the supporting-fact set and is represented by GloVe embedding of the entity, and each edge denotes the relationship between two entities."}, {"title": "3.2 Independent Units Inference", "content": "Independent Units Inference is our main module. The key is to reason independently by dividing memory from functional level and obtaining complementary information from each independent part. The Independent Units Inference consists of three parts: Units Activation, Internal Dynamics and Communication.\nUnits Activation. This module learns to dynamically select the unit most relevant to the input. The default hidden state of each unit is set as question embedding after the last state of LSTM. Then each reasoning step, we active the top-ka units through the attention score of hidden state and input. First we concatenate a row of zeros for current input $m^{(t+1)}$: $M = \u00d8 \u2295 m^{(t+1)}$. Then, we use linear transformation to construct keys, values and queries($K = MW^Q$, $V = MW^V$, $Q = h_t W^k$). The soft-attention value of each unit is expressed as $A_{in}$.\nInternal Dynamics of Units. Each activated unit needs to process the input memory independently and output the new inference state. We utilize Gate Recurrent Unit(GRU) [4] to process the input memory and generate the intermediate state $h_t$ of units. We refer to the activated set as $S_t$. The operation of each activated unit is as follows,\n$h_k^{(t)} = GRU(h_k^{(t-1)}, A_{in}^t)$, $\u2200k \u2208 S_t$    (1)\nCommunication. Although each unit works independently, we still need to obtain complementary information between independent parts. We update the inference state by following attention-like methods,\n$h_k^{(t+1)} = softmax(\\frac{Q^{(t)}(K^{(t)})^T}{\\sqrt{d}})V^{(t)} + h_k^{(t)}$, $k \u2208 S_t$   (2)\nwhere $Q^{(t)} = W^Qh_t$, $k \u2208 S_t$, $K^{(t)} = W^Kh_t$, $k$ and $V^{(t)} = W^V h_t$, $\u2200k$. In this way, while realizing disentangling reasoning, it also retains the complementary information of each part."}, {"title": "3.3 Question-guided Memory Update", "content": "Before entering the next reasoning step, the model needs to maintain memory information based on the current inference state $h_t$. This module aims to prevent distortion of memory information and update the memory as indicated by the question status.\nWe first reduce memory to zero in each step of reasoning to reduce information that has been inferred. Each node of the memory graph concatenate a row full of zeros, which is represented as $\\hat{u}^{(t)}$. Then, we use the attention mechanism between zeros and the original representation of memory to obtain results after reduction by the guide of the current inference state $h_t$. we get the attention weights as follows,\n$a_i^R = softmax(W^R_2Tanh(W^R_1\\hat{u}^{(t)}) + W^R_3h^{(t)})$ (3)\nwhere all $W$ are learned parameters. $h^{(t)}$ is the current inference state produced by last inference step. The representation of each node after reduction is computed,\n$v_i^{(t)} = a_i^R \\cdot v_i^{(t)}$ (4)"}, {"title": "3.4 Answer Prediction and Training", "content": "To predict the answer, we update the representation $v_i^F$ with the last inference state $h_t$ via the gate mechanism,\n$gate_i = \u03c3(W_{gate}[h^{(t)}, v_i^F])$ (7)\n$\\hat{v}_i = W_{rep}[gate_i \u03c3[h^{(t)}, v_i^F]]$  (8)\nwhere \u03c3 is sigmoid function, and \u2018o' represents element-wise product, and $v_i^F$ represents the node in the fact graph.\nAll the concepts of are fed into a binary classification to predict the answers. Then we use weighted binary cross-entropy loss to deal with the imbalanced training data,\n$I_n = \u2212 \u2211[a \\cdot y_i ln\u0177_i + b \\cdot (1 \u2212 y_i)ln(1 \u2212 \u0177_i)]$  (9)\nwhere $y_i$ is the ground truth label for $v_i^F$ and a,b represent the weights for negative and positive samples."}, {"title": "4 Experiment", "content": "Implementation Details. We set a=0.7 and b=0.3 in the binary cross-entropy loss. Our model is trained by Adam optimizer with 30 epochs, where the batch size is 32. Warm up strategy is applied for 2 epochs with initial learning rate 1 \u00d7 10-3 and warm-up factor 0.2. In Independent Units Inference, we set 8 units and active 4 units in each step. In this case, the model's parameter size is only 74M, with an average training time of 0.4 hours per epoch with a single V100-32GB GPU, ensuring lightweight training and deployment processes.\nDataset. We evaluate IIU on Outside Knowledge VQA(OK-VQA) dataset [12], which contains 14,031 images and a total of 14,055 questions, covering 10 different categories, such as Vehicles and Transportation, Sports and Recreation, Cooking and Food, etc. In this work, we retrieve the supporting knowledge from ConceptNet. Besides, we alse evaluate on the FVQA [18] dataset, which consists of 2,190 images, 5,286 questions and a knowledge base of 193,449 facts."}, {"title": "4.2 Ablation Study", "content": "Our IIU model is mainly composed of two parts in Independent Inference Module, Question Guide Memory Update, and Independent Units Inference. The reasoning information is decomposed through the functionally independent units. The processing of redundant information requires the cooperation of two modules. In Table 3, we first study the influence of each module on the optimal accuracy without limiting the reasoning steps. In Table 3 model '2', we turn Independent Units to one single GRU which removes the information disentangling, and the performance is 11% lower than the full model. We can also see in model '3' that it is necessary to obtain complementary information from other units. Once we cut off the communication between the independent units, the results of the model dropped slightly by about 3%. We can see from model '1' that the Memory Update module will not have a great impact on the optimal accuracy, but still 2% lower than the full model. Because this module can help the model maintain memory information, on one hand, it removes used or redundant memories, and on the other hand, it helps the model update memory representations based on the state of inference. Besides the structure of our model, we also study the influence of modal information in Table 3 model '5', '6' and '7'."}, {"title": "4.3 Interpretability and Visualization", "content": "In this section we will show how units decompose information and focus on a specific functional level. We set up 8 units, and the inference step is 20. In Figure 3, we show the average units activated by three different modalities information in each inference step. The information of these three modalities comes from Visual Graph, Semantic Graph, and Fact Graph respectively. We can see that the information of different modalities has different activation distributions of units. In modal of Visual Graph, the units of numbers"}, {"title": "4.4 Model Stability", "content": "In order to have better generalization, the model needs to correctly identify and process redundant information. To test whether IIU has this stability, we artificially add zero input as redundant information in the process of reasoning. We select the reasoning step as 30 and add redundant information in the last 10 reasoning. As shown in Figure 4, the units group activated when processing key information is 0, 1, 4, 5. After we add redundant information, 3, 5, 6, and 7 are activated. So we can see that IIU can well identify redundant information and process it separately."}, {"title": "5 Conclusion", "content": "In this paper, we propose Independent Inference Units (IIU) for visual question answering requiring external knowledge. IIU obtains clues hidden in complex multi-modal"}]}