{"title": "KKLIP: Knowledge Distillation Exploiting K-means Clustering for Language-Image Pre-Training", "authors": ["Kuei-Chun Kao"], "abstract": "Recently, CLIP has emerged as a valuable model for aligning image and text information in multi-modal scenarios. However, researchers have observed limitations in the ability of CLIP's text and image encoders to extract detailed knowledge from caption-image pairs. In response, this paper introduces KKLIP, a novel approach designed to enhance the quality of CLIP by incorporating a new knowledge distillation (KD) method derived from Llama 2. Our method comprises three objectives: Text Embedding Distillation, Concept Learning, and Contrastive Learning. Firstly, Text Embedding Distillation involves training the KKLIP text encoder to emulate the teacher model, Llama 2. Secondly, Concept Learning assigns a soft concept label to each caption-image pair through offline k-means clustering of text information from Llama 2, allowing KKLIP to learn from these soft concept labels. Finally, Contrastive Learning harmonizes text and image embeddings. Our experimental results demonstrate that KKLIP enhances the quality of both text and image encoders.", "sections": [{"title": "1 Introduction", "content": "The pre-training of the multimodal encoders associated with vision language, a good example being CLIP (Radford et al., 2021), has been found to be very helpful in learning transferrable features derived from paired data of image and text. CLIP's learning framework is contrastive, typically hinging on data augmentation in a bid to eliminate overfitting and unwanted shortcuts.\nHowever, the impressive results demonstrated by Vision and Language models (VLMs) (Gokhale et al., 2023; Thrush et al., 2022; Yuksekgonul et al., 2023) on myriad recognized benchmarks do not necessarily indicate a comprehensive understanding of the compositional elements of text or images. These models, as exemplified by CLIP, raise questions about their ability to differentiate between sentence structures like \"an orangutan eating and an officer flying\" and \"an orangutan and an officer eating an orangutan\" Scenes in nature pose a significant challenge due to their complexity, resulting from the numerous objects and attributions they carry and their mutual interactions.\nThe challenge inherent in CLIP, stemming from its difficulty in addressing image segmentation and object detection tasks due to the need for per-pixel label knowledge, has been addressed by Li et al. (2022) through the introduction of GLIP. The proposed method unified object detection and phrase grounding through pre-training, effectively leveraging external knowledge, i.e., grounding boxes. The integration of such external information facilitates the alignment of image-language data, enhancing the model's capability in handling complex visual tasks.\nInspired by their work, our objective is to integrate external knowledge from established large language models into CLIP, with the aim of further elevating its overall quality. Therefore, we introduce a pioneering methodology named KKLIP, comprising triple sets of objectives. Firstly, our focus is on knowledge distillation (KD) from large language models, exemplified by Llama 2 (Touvron et al., 2023), with the aim of enhancing the quality of CLIP's text encoder. Secondly, we posit that the embeddings generated by Llama 2 encompass more valuable attributes and conceptual information, such as color and action, than CLIP's text encoder. Thus, employing k-means clustering (Hartigan and Wong, 1979) on Llama 2's embeddings, we derive soft concept labels for caption-image pairs. Subsequently, we leverage these soft concept labels to refine the quality of both CLIP's text and image encoders. Finally, we reused the contrastive objective from CLIP to continually align the text and image embeddings."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Knowledge Distillation", "content": "Knowledge distillation (KD) is a training technique wherein a neural network, referred to as a student, is trained to replicate parts of another neural network, known as a teacher (Ba and Caruana, 2014). The most common approach involves matching the output of the teacher network. However, an alternative option is to match the hidden layers, providing a more nuanced transfer of knowledge (Romero et al., 2015; Aguilar et al., 2020). In terms of the loss functions utilized during this process, KL divergence is a common choice for matching probability outputs, while L2 norm is frequently employed to align the hidden vectors (Kim et al., 2021). This technique allows for the compact representation of knowledge learned by the teacher network to be transferred to the student network, enhancing the student's performance and generalization capabilities."}, {"title": "2.2 CLIP's Text Encoder", "content": "In the context of multimodal models, the text-to-image query such as \"A UCLA CS student in a futuristic lab, donned in virtual reality gear and programming a robotic assistant next to a professor in a lab coat.\" exemplifies the expectations for modern multimodal models. This type of query demands spatial precision (e.g., specifying the position of entities), compositional understanding (highlighting certain attributes like a UCLA CS student but not a UCLA CS assistant), and a touch of imagination, describing scenarios that may not exist in reality. However, recent works (Gokhale et al., 2023; Thrush et al., 2022; Yuksekgonul et al., 2023) shed light on a notable challenge. Despite achieving robust benchmark performance, various multimodal models often struggle with even basic reasoning tasks, particularly those involving spatial relations or attribute attachments. These findings underscore the existing limitations in the reasoning capabilities of multimodal models, especially when confronted with intricate and imaginative textual input."}, {"title": "3 Method", "content": null}, {"title": "3.1 Problem Definition and Annotations", "content": "For the sake of completeness, we first define the setting and notations considered in this paper. During training, we have N caption-image pairs denoted as $X = {X_1, X_2, ..., X_N}$. For the ith caption-image pair, we have a caption and an image, i.e., $X_i = (x_i^t,x_i^v)$. As shown in Figure 1, our proposed KKLIP has five models: CLIP text encoder (ET), CLIP image encoder (E1), Classifier (C), and Llama 2 (L)."}, {"title": "3.2 Text Embedding Distillation", "content": "In a previous KD work (Jiao et al., 2019), they effectively distill the knowledge from BERTbase to TinyBERT with mean squared error loss func-"}, {"title": "3.3 Concept Learning", "content": "Given a caption, there exist multiple attributes or concepts, e.g., color, position, action, etc. However, CLIP text encoder failed to extract these information observed from previous papers. We hypothesize that captions having the same attributes and concepts would have similar embeddings from Llama 2. Therefore, we utilize k-means clustering method to categorize the output embeddings from Llama 2 with inputting captions. Then, we regard the result from k-means clustering as the soft concept labels, denoted as $S = {s_1, s_2, ..., s_N }$. Noted that $s_i \u2208 [K]$ is a K-class categorical variable.\nAfter obtaining soft concept labels, we use the Llama 2's embeddings and their corresponding soft labels to train the Classifier (C). Later, we freeze the Classifier (C) and use it to train CLIP's image encoder with images and their corresponding soft labels. The objective is as follows:"}, {"title": "3.4 Contrastive Learning", "content": "In continuation of our exploration inspired by CLIP, our approach involves leveraging contrastive loss to effectively align text and image embeddings. Through this, we aim to optimize a symmetric cross-entropy loss, named $L_{cont}$, based on similarity scores. Figure 2 provides a visual representation of the pseudocode, outlining the core elements of an implementation of CLIP."}, {"title": "3.5 KKLIP Learning", "content": "We combine Text Embedding Distillation, Concept Distillation, and Contrastive Learning to further improve the quality of CLIP encoders, i.e., text encoder, image encoder. The KKLIP learning objective is as follows:"}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental settings", "content": "The pre-training dataset\u00b9 comprises a subset of CLIP's pretraining data. With a total of 15.7 million records, we selectively extracted 500,000 records for model training and 100,000 records for model evaluation. We run 100 epoch and learning rate is 5 \u00d7 10-5. \u03b1 and \u03b3 are equal to 1, and \u03b2 is equal to 0.01."}, {"title": "4.3 KKLIP's Image Encoder", "content": "In this study, our primary objective is to assess the quality of KKLIP's image encoder. While traditional zero-shot learning tasks such as classification typically overlook attribute values, assuming them to be understood, our approach involves a more detailed examination of these semantic features and their associated attributes (e.g., color, shape, leg, head, etc) which necessitate more sophisticated image encoder's features to align with the specific text.\nUnder this background, our evaluation experiment will first be assigned to a specific class for each image. Then, we predict the attribute descriptions based on their given class. This experiment explores the extent to which the image features generated by the image encoder can comprehend the conceptual meaning of kmeans soft labels, and how effectively our proposed models can bridge the gap between the learning of semantic entities and attribute-based recognition.\nTo address this question, we leverage two common attribute-based datasets, the AWA2 (Xian et al., 2020) and CUB (Wah et al., 2011), and explore the effect of attribute-based learning on our proposed model.\nCUB. The Caltech-UCSD Birds-200-2011 (Wah et al., 2011) dataset features 11,788 images of 200 distinct bird species. Each species is annotated with 312 binary attributes. CUB provides attributes for each image, consisting of an attribute description and an expression. For instance, an attribute might have the description <attr>: \"has head pattern\" and the expression <expr>: \"crested\". Each attribute description has various possible expressions. CUB also provides class attributes, giving a probability that the attribute can be found in an image of that class for each class and attribute. For each attribute description, we select a maximum of one attribute with the highest probability. Therefore, the evaluation prompt would be \"a photo of a  that  .\"\nAWA2. The Animals with Attributes 2 (Xian et al., 2020) dataset includes 37,322 images of 50 animal classes. Each class is annotated with 85 binary attributes. Unlike CUB, AWA2's attributes do not have separate attribute descriptions and expressions. As such, we append the attributes as a comma-separated list at the end of the prompt. To maintain comparability with CUB, our evaluation prompt would be \"a photo of a , with attribute , , ...\".\nDuring evaluation, for each image, we create A prompts for each class C, where C is the number of classes and A is the number of attributes in dataset D. We then calculate the cosine similarities between the image I and the A prompts and apply softmax to the similarity values. The attribute that corresponds to the prompt that is most similar to image I is the predicted class."}, {"title": "5 Discussion", "content": "In this section, we delve into the distinctions between Llama 2 and CLIP's text encoder, shedding light on the unique attributes that set them apart. Additionally, we explore the potential significance of soft labels derived from k-means clustering applied to Llama 2's embeddings.\nTo demonstrate the potential improvement in CLIP's text embeddings through the utilization of Llama 2, we propose an analysis of the distributional characteristics of their respective embeddings under identical dataset settings. Our hypothesis posits that Llama 2's text embeddings exhibit a more uniform distribution compared to those of CLIP since Llama 2 extracts more detailed text information than CLIP.\nEmbedding Extraction: Text embeddings for the training dataset were extracted using both Llama 2 and CLIP text encoder. This initial step ensured that the embeddings were generated under identical conditions, facilitating a direct comparison.\nClustering Analysis: To analyze the distribution of embeddings, we employed k-means clustering with 1000 clusters for each model. This unsupervised clustering method enabled us to discern patterns and groupings within the high-dimensional embedding space.\nFrom figure 4, it is evident that the distribution of Llama 2's embeddings is notably more uniform compared to CLIP's. The visualization provides insights into the dispersion of data points in the embedding space, supporting our claim of enhanced uniformity in Llama 2's embeddings. Additionally, an examination of the most common labels within each clustering reveals a notable distinction. Llama 2 tends to have fewer occurrences of the most common labels, indicating a more diverse representation of information compared to CLIP.\nThis observed uniformity in Llama 2's embeddings suggests its potential to capture a broader range of semantic nuances, enabling finer distinctions between sentences. The reduced concentration on common labels further implies that Llama 2 may offer a richer representation that aids in discerning subtle differences between sentences.\nBased on these findings, we propose leveraging Llama 2's embeddings for fine-tuning CLIP. The potential of Llama 2 to extract more nuanced information from textual data could enhance CLIP's ability to differentiate between sentences, contributing to improved overall performance in downstream tasks. Subsequent sections delve into the fine-tuning process and assess the impact on zero-shot classification accuracy."}, {"title": "5.2 Visualization of K-means clustering with Llama 2", "content": "To show what is learned from our proposed soft concpetual labels, we randomly select 50 samples from a specific class with different attributes from CUB dataset. (e.g. the following experiment uses \"Black footed Albatross\") Then, we cluster the llama2 embedding and visualize the according cluster.\nIn Figure 5, it is clear that the llama2 embedding from each attribute is clustered into same groups, indicating that our method with K-means soft labels can better utilize conceptual learning."}, {"title": "6 Conclusion", "content": "In this paper, we introduce KKLIP, a novel methodology designed to enhance the overall quality of CLIP, a multimodal vision-language model. Our approach leverages a large language model, Llama 2, to guide both the image and text encoders. The experimental results demonstrate the effectiveness of KKLIP in improving the quality of CLIP's text encoder and CLIP's image encoder. Through a comprehensive evaluation on the CC3M dataset, we observe that KKLIP achieves a higher Exact Match rate compared to CLIP. Moreover, the evaluation of KKLIP on image encoder quality, using attribute-based datasets AWA2 and CUB, indicates a slight performance improvement over CLIP. In conclusion, KKLIP presents a promising approach for enhancing the capabilities of multimodal vision-language models like CLIP by incorporating external knowledge, refining embeddings, and addressing specific limitations. However, further investigation and experimentation may be necessary to optimize the model for specific tasks and domains. Future work could involve exploring additional datasets, refining hyperparameters, and investigating the model's applicability to different downstream tasks in vision-language understanding."}]}