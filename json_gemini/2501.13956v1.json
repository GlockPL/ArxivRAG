{"title": "ZEP: A TEMPORAL KNOWLEDGE GRAPH ARCHITECTURE FOR AGENT MEMORY", "authors": ["Preston Rasmussen", "Pavlo Paliychuk", "Travis Beauvais", "Jack Ryan", "Daniel Chalef"], "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti-a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep's effectiveness for deployment in real-world applications.", "sections": [{"title": "Introduction", "content": "The impact of transformer-based large language models (LLMs) on industry and research communities has garnered significant attention in recent years [1]. A major application of LLMs has been the development of chat-based agents. However, these agents' capabilities are limited by the LLMs' context windows, effective context utilization, and knowledge gained during pre-training. Consequently, additional context is required to provide out-of-domain (OOD) knowledge and reduce hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a key area of interest in LLM-based applications. RAG leverages Information Retrieval (IR) techniques pioneered over the last fifty years[2] to supply necessary domain knowledge to LLMs.\nCurrent approaches using RAG have focused on broad domain knowledge and largely static corpora-that is, document contents added to a corpus seldom change. For agents to become pervasive in our daily lives, autonomously solving problems from trivial to highly complex, they will need access to a large corpus of continuously evolving data from users' interactions with the agent, along with related business and world data. We view empowering agents with this broad and dynamic \"memory\" as a crucial building block to actualize this vision, and we argue that current RAG approaches are unsuitable for this future. Since entire conversation histories, business datasets, and other domain-specific content cannot fit effectively inside LLM context windows, new approaches need to be developed for agent"}, {"title": "Knowledge Graph Construction", "content": "In Zep, memory is powered by a temporally-aware dynamic knowledge graph $G = (N,E, \\phi)$, where $N$ represents nodes, $E$ represents edges, and $\\phi : E \\rightarrow N \\times N$ represents a formal incidence function. This graph comprises three hierarchical tiers of subgraphs: an episode subgraph, a semantic entity subgraph, and a community subgraph.\n\u2022 Episode Subgraph $G_e$: Episodic nodes (episodes), $n_i \\in N_e$, contain raw input data in the form of messages, text, or JSON. Episodes serve as a non-lossy data store from which semantic entities and relations are extracted. Episodic edges, $e_i \\in E_e \\subseteq \\phi^*(N_e \\times N_s)$, connect episodes to their referenced semantic entities.\n\u2022 Semantic Entity Subgraph $G_s$: The semantic entity subgraph builds upon the episode subgraph. Entity nodes (entities), $n_i \\in N_s$, represent entities extracted from episodes and resolved with existing graph entities. Entity edges (semantic edges), $e_i \\in E_s \\subseteq \\phi^*(N_s \\times N_s)$, represent relationships between entities extracted from episodes.\n\u2022 Community Subgraph $G_c$: The community subgraph forms the highest level of Zep's knowledge graph. Community nodes (communities), $n_i \\in N_c$, represent clusters of strongly connected entities. Communities contain high-level summarizations of these clusters and represent a more comprehensive, interconnected view of $G$'s structure. Community edges, $e_i \\in E_c \\subseteq \\phi^*(N_c \\times N_s)$, connect communities to their entity members.\nThe dual storage of both raw episodic data and derived semantic entity information mirrors psychological models of human memory. These models distinguish between episodic memory, which represents distinct events, and semantic memory, which captures associations between concepts and their meanings [8]. This approach enables LLM agents using Zep to develop more sophisticated and nuanced memory structures that better align with our understanding of human memory systems. Knowledge graphs provide an effective medium for representing these memory structures, and our implementation of distinct episodic and semantic subgraphs draws from similar approaches in AriGraph [9].\nOur use of community nodes to represent high-level structures and domain concepts builds upon work from GraphRAG [4], enabling a more comprehensive global understanding of the domain. The resulting hierarchical organiza-tion-from episodes to facts to entities to communities\u2014extends existing hierarchical RAG strategies [10][11]."}, {"title": "Episodes", "content": "Zep's graph construction begins with the ingestion of raw data units called Episodes. Episodes can be one of three core types: message, text, or JSON. While each type requires specific handling during graph construction, this paper focuses on the message type, as our experiments center on conversation memory. In our context, a message consists of relatively short text (several messages can fit within an LLM context window) along with the associated actor who produced the utterance.\nEach message includes a reference timestamp $t_{ref}$ indicating when the message was sent. This temporal information enables Zep to accurately identify and extract relative or partial dates mentioned in the message content (e.g., \"next Thursday,\" \"in two weeks,\" or \"last summer\"). Zep implements a bi-temporal model, where timeline $T$ represents the chronological ordering of events, and timeline $T'$ represents the transactional order of Zep's data ingestion. While the $T'$ timeline serves the traditional purpose of database auditing, the $T$ timeline provides an additional dimension for modeling the dynamic nature of conversational data and memory. This bi-temporal approach represents a novel advancement in LLM-based knowledge graph construction and underlies much of Zep's unique capabilities compared to previous graph-based RAG proposals."}, {"title": "Semantic Entities and Facts", "content": "ntity extraction represents the initial phase of episode processing. During ingestion, the system processes both the current message content and the last n messages to provide context for named entity recognition. For this paper and in Zep's general implementation, $n = 4$, providing two complete conversation turns for context evaluation. Given our focus on message processing, the speaker is automatically extracted as an entity. Following initial entity extraction, we employ a reflection technique inspired by reflexion[12] to minimize hallucinations and enhance extraction coverage. The system also extracts an entity summary from the episode to facilitate subsequent entity resolution and retrieval operations.\nAfter extraction, the system embeds each entity name into a 1024-dimensional vector space. This embedding enables the retrieval of similar nodes through cosine similarity search across existing graph entity nodes. The system also performs a separate full-text search on existing entity names and summaries to identify additional candidate nodes. These candidate nodes, together with the episode context, are then processed through an LLM using our entity resolution prompt. When the system identifies a duplicate entity, it generates an updated name and summary.\nFollowing entity extraction and resolution, the system incorporates the data into the knowledge graph using predefined Cypher queries. We chose this approach over LLM-generated database queries to ensure consistent schema formats and reduce the potential for hallucinations."}, {"title": "Facts", "content": "or each fact containing its key predicate. Importantly, the same fact can be extracted multiple times between different entities, enabling Graphiti to model complex multi-entity facts through an implementation of hyper-edges.\nFollowing extraction, the system generates embeddings for facts in preparation for graph integration. The system performs edge deduplication through a process similar to entity resolution. The hybrid search for relevant edges is constrained to edges existing between the same entity pairs as the proposed new edge. This constraint not only prevents erroneous combinations of similar edges between different entities but also significantly reduces the computational complexity of the deduplication process by limiting the search space to a subset of edges relevant to the specific entity pair."}, {"title": "Temporal Extraction and Edge Invalidation", "content": "A key differentiating feature of Graphiti compared to other knowledge graph engines is its capacity to manage dynamic information updates through temporal extraction and edge invalidation processes.\nThe system extracts temporal information about facts from the episode context using $t_{ref}$. This enables accurate extraction and datetime representation of both absolute timestamps (e.g., \"Alan Turing was born on June 23, 1912\") and relative timestamps (e.g., \"I started my new job two weeks ago\"). Consistent with our bi-temporal modeling approach, the system tracks four timestamps: $t'_{created}$ and $t'_{expired} \\in T'$ monitor when facts are created or invalidated in the system, while $t_{valid}$ and $t_{invalid} \\in T$ track the temporal range during which facts held true. These temporal data points are stored on edges alongside other fact information.\nThe introduction of new edges can invalidate existing edges in the database. The system employs an LLM to compare new edges against semantically related existing edges to identify potential contradictions. When the system identifies temporally overlapping contradictions, it invalidates the affected edges by setting their $t_{invalid}$ to the $t_{valid}$ of the invalidating edge. Following the transactional timeline $T'$, Graphiti consistently prioritizes new information when determining edge invalidation.\nThis comprehensive approach enables the dynamic addition of data to Graphiti as conversations evolve, while maintaining both current relationship states and historical records of relationship evolution over time."}, {"title": "Communities", "content": "After establishing the episodic and semantic subgraphs, the system constructs the community subgraph through community detection. While our community detection approach builds upon the technique described in GraphRAG[4], we employ a label propagation algorithm [13] rather than the Leiden algorithm [14]. This choice was influenced by label propagation's straightforward dynamic extension, which enables the system to maintain accurate community representations for longer periods as new data enters the graph, delaying the need for complete community refreshes.\nThe dynamic extension implements the logic of a single recursive step in label propagation. When the system adds a new entity node $n_i \\in N_s$ to the graph, it surveys the communities of neighboring nodes. The system assigns the new node to the community held by the plurality of its neighbors, then updates the community summary and graph accordingly. While this dynamic updating enables efficient community extension as data flows into the system, the resulting communities gradually diverge from those that would be generated by a complete label propagation run. Therefore, periodic community refreshes remain necessary. However, this dynamic updating strategy provides a practical heuristic that significantly reduces latency and LLM inference costs.\nFollowing [4], our community nodes contain summaries derived through an iterative map-reduce-style summarization of member nodes. However, our retrieval methods differ substantially from GraphRAG's map-reduce approach [4]. To support our retrieval methodology, we generate community names containing key terms and relevant subjects from the community summaries. These names are embedded and stored to enable cosine similarity searches."}, {"title": "Memory Retrieval", "content": "The memory retrieval system in Zep provides powerful, complex, and highly configurable functionality. At a high level, the Zep graph search API implements a function $f: S \\rightarrow S$ that accepts a text-string query $a \\in S$ as input and returns a text-string context $\\beta \\in S$ as output. The output $\\beta$ contains formatted data from nodes and edges required for an LLM agent to generate an accurate response to query $a$. The process $f(a) \\rightarrow \\beta$ comprises three distinct steps:\n\u2022 Search ($\\phi$): The process begins by identifying candidate nodes and edges potentially containing relevant information. While Zep employs multiple distinct search methods, the overall search function can be represented as $\\phi: S \\rightarrow E \\times N \\times N_r$. Thus, $\\phi$ transforms a query into a 3-tuple containing lists of semantic edges, entity nodes, and community nodes\u2014the three graph types containing relevant textual information.\n\u2022 Reranker ($\\rho$): The second step reorders search results. A reranker function or model accepts a list of search results and produces a reordered version of those results: $\\rho : \\phi(\\alpha), \\rightarrow E x N \\times N_r$.\n\u2022 Constructor ($\\chi$): The final step, the constructor, transforms the relevant nodes and edges into text context: $\\chi: E x N x N_c n \\rightarrow S$. For each $e_i \\in E_s$, $\\chi$ returns the fact and $t_{valid}$, $t_{invalid}$ fields; for each $n_i \\in N_s$, the name and summary fields; and for each $n_i \\in N_c$, the summary field.\nWith these definitions established, we can express $f$ as a composition of these three components: $f(a) = \\chi(\\rho(\\phi(\\alpha))) = \\beta$.\nSample context string template:\nFACTS and ENTITIES represent relevant context to the current conversation.\nThese are the most relevant facts and their valid date ranges. If the fact is about an event, the event takes place during this time.\nformat: FACT (Date range: from - to)\n<FACTS>\n{facts}\n</FACTS>\nThese are the most relevant entities\nENTITY_NAME: entity summary\n<ENTITIES>\n{entities}\n</ENTITIES>"}, {"title": "Search", "content": "Zep implements three search functions: cosine semantic similarity search ($cos$), Okapi BM25 full-text search ($bm25$), and breadth-first search ($bfs$). The first two functions utilize Neo4j's implementation of Lucene [15][16]. Each"}, {"title": "Reranker", "content": "While the initial search methods aim to achieve high recall, rerankers serve to increase precision by prioritizing the most relevant results. Zep supports existing reranking approaches such as Reciprocal Rank Fusion (RRF) [20] and Maximal Marginal Relevance (MMR) [21]. Additionally, Zep implements a graph-based episode-mentions reranker that prioritizes results based on the frequency of entity or fact mentions within a conversation, enabling a system where frequently referenced information becomes more readily accessible. The system also includes a node distance reranker that reorders results based on their graph distance from a designated centroid node, providing context localized to specific areas of the knowledge graph. The system's most sophisticated reranking capability employs cross-encoders\u2014LLMs that generate relevance scores by evaluating nodes and edges against queries using cross-attention, though this approach incurs the highest computational cost."}, {"title": "Experiments", "content": "This section analyzes two experiments conducted using LLM-memory based benchmarks. The first evaluation employs the Deep Memory Retrieval (DMR) task developed in [3], which uses a 500-conversation subset of the Multi-Session Chat dataset introduced in \"Beyond Goldfish Memory: Long-Term Open-Domain Conversation\" [22]. The second evaluation utilizes the LongMemEval benchmark from \"LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory\" [7]. Specifically, we use the LongMemEval, dataset, which provides an extensive conversation context of on average 115,000 tokens.\nFor both experiments, we integrate the conversation history into a Zep knowledge graph through Zep's APIs. We then retrieve the 20 most relevant edges (facts) and entity nodes (entity summaries) using the techniques described in Section 3. The system reformats this data into a context string, matching the functionality provided by Zep's memory APIs.\nWhile these experiments demonstrate key retrieval capabilities of Graphiti, they represent a subset of the system's full search functionality. This focused scope enables clear comparison with existing benchmarks while reserving the exploration of additional knowledge graph capabilities for future work."}, {"title": "Choice of models", "content": "Our experimental implementation employs the BGE-m3 models from BAAI for both reranking and embedding tasks [23] [24]. For graph construction and response generation, we utilize gpt-40-mini-2024-07-18 for graph construction, and both gpt-40-mini-2024-07-18 and gpt-40-2024-11-20 for the chat agent generating responses to provided context.\nTo ensure direct comparability with MemGPT's DMR results, we also conducted the DMR evaluation using gpt-4-turbo-2024-04-09.\nThe experimental notebooks will be made publicly available through our GitHub repository, and relevant experimental prompts are included in the Appendix."}, {"title": "Deep Memory Retrieval (DMR)", "content": "The Deep Memory Retrieval evaluation, introduced by [3], comprises 500 multi-session conversations, each containing 5 chat sessions with up to 12 messages per session. Each conversation includes a question/answer pair for memory evaluation. The MemGPT framework [3] currently leads performance metrics with 93.4% accuracy using gpt-4-turbo, a significant improvement over the 35.3% baseline achieved through recursive summarization.\nTo establish comparative baselines, we implemented two common LLM memory approaches: full-conversation context and session summaries. Using gpt-4-turbo, the full-conversation baseline achieved 94.4% accuracy, slightly surpassing MemGPT's reported results, while the session summary baseline achieved 78.6%. When using gpt-40-mini, both approaches showed improved performance: 98.0% for full-conversation and 88.0% for session summaries. We were unable to reproduce MemGPT's results using gpt-40-mini due to insufficient methodological details in their published work.\nWe then evaluated Zep's performance by ingesting the conversations and using its search functions to retrieve the top 10 most relevant nodes and edges. An LLM judge compared the agent's responses to the provided golden answers. Zep achieved 94.8% accuracy with gpt-4-turbo and 98.2% with gpt-40-mini, showing marginal improvements over both MemGPT and the respective full-conversation baselines. However, these results must be contextualized: each conversation contains only 60 messages, easily fitting within current LLM context windows.\nThe limitations of the DMR evaluation extend beyond its small scale. Our analysis revealed significant weaknesses in the benchmark's design. The evaluation relies exclusively on single-turn, fact-retrieval questions that fail to assess complex memory understanding. Many questions contain ambiguous phrasing, referencing concepts like \"favorite drink to relax with\" or \"weird hobby\" that were not explicitly characterized as such in the conversations. Most critically, the dataset poorly represents real-world enterprise use cases for LLM agents. The high performance achieved by simple full-context approaches using modern LLMs further highlights the benchmark's inadequacy for evaluating memory systems.\nThis inadequacy is further emphasized by findings in [7], which demonstrate rapidly declining LLM performance on the LongMemEval benchmark as conversation length increases. The LongMemEval dataset [7] addresses many of these shortcomings by presenting longer, more coherent conversations that better reflect enterprise scenarios, along with more diverse evaluation questions."}, {"title": "LongMemEval (LME)", "content": "We evaluated Zep using the LongMemEvals dataset, which provides conversations and questions representative of real-world business applications of LLM agents. The LongMemEvals dataset presents significant challenges to existing LLMs and commercial memory solutions [7], with conversations averaging approximately 115,000 tokens in length. This length, while substantial, remains within the context windows of current frontier models, enabling us to establish meaningful baselines for evaluating Zep's performance.\nThe dataset incorporates six distinct question types: single-session-user, single-session-assistant, single-session-preference, multi-session, knowledge-update, and temporal-reasoning. These categories are not uniformly distributed throughout the dataset; for detailed distribution information, we refer readers to [7].\nWe conducted all experiments between December 2024 and January 2025. We performed testing using a consumer laptop from a residential location in Boston, MA, connecting to Zep's service hosted in AWS us-west-2. This dis-"}, {"title": "LongMemEval and MemGPT", "content": "To establish a comparative benchmark between Zep and the current state-of-the-art MemGPT system [3], we attempted to evaluate MemGPT using the LongMemEval dataset. Given that the current MemGPT framework does not support direct ingestion of existing message histories, we implemented a workaround by adding conversation messages to the archival history. However, we were unable to achieve successful question responses using this approach. We look forward to seeing evaluations of this benchmark by other research teams, as comparative performance data would benefit the broader development of LLM memory systems."}, {"title": "LongMemEval results", "content": "Zep demonstrates substantial improvements in both accuracy and latency compared to the baseline across both model variants. Using gpt-40-mini, Zep achieved a 15.2% accuracy improvement over the baseline, while gpt-4o showed an 18.5% improvement. The reduced prompt size also led to significant latency cost reductions compared to the baseline implementations."}, {"title": "Communities", "content": "After establishing the episodic and semantic subgraphs, the system constructs the community subgraph through community detection. While our community detection approach builds upon the technique described in GraphRAG[4], we employ a label propagation algorithm [13] rather than the Leiden algorithm [14]. This choice was influenced by label propagation's straightforward dynamic extension, which enables the system to maintain accurate community representations for longer periods as new data enters the graph, delaying the need for complete community refreshes.\nThe dynamic extension implements the logic of a single recursive step in label propagation. When the system adds a new entity node $n_i \\in N_s$ to the graph, it surveys the communities of neighboring nodes. The system assigns the new node to the community held by the plurality of its neighbors, then updates the community summary and graph accordingly. While this dynamic updating enables efficient community extension as data flows into the system, the resulting communities gradually diverge from those that would be generated by a complete label propagation run. Therefore, periodic community refreshes remain necessary. However, this dynamic updating strategy provides a practical heuristic that significantly reduces latency and LLM inference costs.\nFollowing [4], our community nodes contain summaries derived through an iterative map-reduce-style summarization of member nodes. However, our retrieval methods differ substantially from GraphRAG's map-reduce approach [4]. To support our retrieval methodology, we generate community names containing key terms and relevant subjects from the community summaries. These names are embedded and stored to enable cosine similarity searches."}, {"title": "Conclusion", "content": "We have introduced Zep, a graph-based approach to LLM memory that incorporates semantic and episodic memory alongside entity and community summaries. Our evaluations demonstrate that Zep achieves state-of-the-art performance on existing memory benchmarks while reducing token costs and operating at significantly lower latencies.\nThe results achieved with Graphiti and Zep, while impressive, likely represent only initial advances in graph-based memory systems. Multiple research avenues could build upon these frameworks, including integration of other GraphRAG approaches into the Zep paradigm and novel extensions of our work.\nResearch has already demonstrated the value of fine-tuned models for LLM-based entity and edge extraction within the GraphRAG paradigm, improving accuracy while reducing costs and latency [19][25]. Similar models fine-tuned for Graphiti prompts may enhance knowledge extraction, particularly for complex conversations. Additionally, while current research on LLM-generated knowledge graphs has primarily operated without formal ontologies [9][4][17][19][26], domain-specific ontologies present significant potential. Graph ontologies, foundational in pre-LLM knowledge graph work, warrant further exploration within the Graphiti framework.\nOur search for suitable memory benchmarks revealed limited options, with existing benchmarks often lacking robustness and complexity, frequently defaulting to simple needle-in-a-haystack fact-retrieval questions [3]. The field requires additional memory benchmarks, particularly those reflecting business applications like customer experience tasks, to effectively evaluate and differentiate memory approaches. Notably, no existing benchmarks adequately assess Zep's capability to process and synthesize conversation history with structured business data. While Zep focuses on LLM memory, its traditional RAG capabilities should be evaluated against established benchmarks such as those in [17], [27], and [28].\nCurrent literature on LLM memory and RAG systems insufficiently addresses production system scalability in terms of cost and latency. We have included latency benchmarks for our retrieval mechanisms to begin addressing this gap, following the example set by LightRAG's authors in prioritizing these metrics."}, {"title": "Appendix", "content": "6.1 Graph Construction Prompts"}, {"title": "Entity Extraction", "content": "<PREVIOUS MESSAGES>\n{previous_messages}\n</PREVIOUS MESSAGES>\n<CURRENT MESSAGE>\n{current_message}\n</CURRENT MESSAGE>\nGiven the above conversation, extract entity nodes from the CURRENT MESSAGE that are explicitly or implicitly mentioned:\nGuidelines:\n1. ALWAYS extract the speaker/actor as the first node. The speaker is the part before the colon in each line of dialogue.\n2. Extract other significant entities, concepts, or actors mentioned in the CURRENT MESSAGE.\n3. DO NOT create nodes for relationships or actions.\n4. DO NOT create nodes for temporal information like dates, times or years (these will be added to edges later).\n5. Be as explicit as possible in your node names, using full names.\n6. DO NOT extract entities mentioned only"}, {"title": "Entity Resolution", "content": "<PREVIOUS MESSAGES>\n{previous_messages}\n</PREVIOUS MESSAGES>\n<CURRENT MESSAGE>\n{current_message}\n</CURRENT MESSAGE>\n<EXISTING NODES>\n{existing_nodes}\n</EXISTING NODES>\nGiven the above EXISTING NODES, MESSAGE, and PREVIOUS MESSAGES. Determine if the NEW NODE extracted from the conversation is a duplicate entity of one of the EXISTING NODES.\n<NEW NODE>\n{new_node}\n</NEW NODE>\nTask:\n1. If the New Node represents the same entity as any node in Existing Nodes, return 'is_duplicate: true' in the response. Otherwise, return 'is_duplicate: false'\n2. If is_duplicate is true, also return the uuid of the existing node in the response\n3. If is_duplicate is true, return a name for the node that is the most complete full name.\nGuidelines:\n1. Use both the name and summary of nodes to determine if the entities are duplicates, duplicate nodes may have different names"}, {"title": "Fact Extraction", "content": "<PREVIOUS MESSAGES>\n{previous_messages}\n</PREVIOUS MESSAGES>\n<CURRENT MESSAGE>\n{current_message}\n</CURRENT MESSAGE>\n<ENTITIES>\n{entities}\n</ENTITIES>\nGiven the above MESSAGES and ENTITIES, extract all facts pertaining to the listed ENTITIES from the CURRENT MESSAGE.\nGuidelines:\n1. Extract facts only between the provided entities.\n2. Each fact should represent a clear relationship between two DISTINCT nodes.\n3. The relation_type should be a concise, all-caps description of the fact (e.g., LOVES, IS_FRIENDS_WITH, WORKS_FOR).\n4. Provide a more detailed fact containing all relevant information.\n5. Consider temporal aspects of relationships when relevant."}, {"title": "Fact Resolution", "content": "Given the following context, determine whether the New Edge represents any of the edges in the list of Existing Edges.\n<EXISTING EDGES>\n{existing_edges}\n</EXISTING EDGES>\n<NEW EDGE>\n{new_edge}\n</NEW EDGE>\nTask:\n1. If the New Edges represents the same factual information as any edge in Existing Edges, return 'is_duplicate: true' in the response. Otherwise, return 'is_duplicate: false'\n2. If is_duplicate is true, also return the uuid of the existing edge in the response\nGuidelines:\n1. The facts do not need to be completely identical to be duplicates, they just need to express the same information."}, {"title": "Temporal Extraction", "content": "<PREVIOUS MESSAGES>\n{previous_messages}\n</PREVIOUS MESSAGES>\n<CURRENT MESSAGE>\n{current_message}\n</CURRENT MESSAGE>\n<REFERENCE TIMESTAMP>\n{reference_timestamp}\n</REFERENCE TIMESTAMP>\n<FACT>\n{fact}\n</FACT>\nIMPORTANT: Only extract time information if it is part of the provided fact. Otherwise ignore the time mentioned. Make sure to do your best to determine the dates if only the relative time is mentioned. (eg 10 years ago, 2 mins ago) based on the provided reference timestamp\nIf the relationship is not of spanning nature, but you are still able to determine the dates, set the valid_at only.\nDefinitions:\nvalid_at: The date and time when the relationship described by the edge fact became true or was established.\ninvalid_at: The date and time when the relationship described by the edge fact stopped being true or ended.\nTask:\nAnalyze the conversation and determine if there are dates that are part of the edge fact. Only set dates if they explicitly relate to the formation or alteration of the relationship itself.\nGuidelines:\n1. Use ISO 8601 format (YYYY-MM-DDTHH:MM:SS.SSSSSSZ) for datetimes.\n2. Use the reference timestamp as the current time when determining the valid_at and invalid_at dates.\n3. If the fact is written in the present tense, use the Reference Timestamp for the valid_at date\n4. If no temporal information is found that establishes or changes the relationship, leave the fields as null.\n5. Do not infer dates from related events. Only use dates that are directly stated to establish or change the relationship.\n6. For relative time mentions directly related to the relationship, calculate the actual datetime based on the reference timestamp.\n7. If only a date is mentioned without a specific time, use 00:00:00 (midnight) for that date.\n8. If only year is mentioned, use January 1st of that year at 00:00:00.\n9. Always include the time zone offset (use Z for UTC if no specific time zone is mentioned)."}]}