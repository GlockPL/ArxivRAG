{"title": "EventVL: Understand Event Streams via Multimodal Large Language Model", "authors": ["Pengteng Li", "Yunfan Lu", "Pinghao Song", "Wuyang Li", "Huizai Yao", "Hui Xiong"], "abstract": "The event-based Vision-Language Model (VLM) recently has made good progress for practical vision tasks. However, most of these works just utilize CLIP for focusing on traditional perception tasks, which obstruct model understanding explicitly the sufficient semantics and context from event streams. To address the deficiency, we propose EventVL, the first generative event-based MLLM (Multi-modal Large Language Model) framework for explicit semantic understanding. Specifically, to bridge the data gap for connecting different modalities semantics, we first annotate a large event-image/video-text dataset, containing almost 1.4 million high-quality pairs of data, which enables effective learning across various scenes, e.g., drive scene or human motion. After that, we design Event Spatiotemporal Representation to fully explore the comprehensive information by diversely aggregating and segmenting the event stream. To further promote a compact semantic space, Dynamic Semantic Alignment is introduced to improve and complete sparse semantic spaces of events. Extensive experiments show that our EventVL can significantly surpass existing MLLM baselines in event captioning and scene description generation tasks. We hope our research could contribute to the development of the event vision community.", "sections": [{"title": "1. Introduction", "content": "Event cameras are sensors that asynchronously measure changes in intensity at each pixel with microsecond-level precision. Compared to traditional cameras, event cameras offer several significant advantages [14], including a high dynamic range (> 120 dB), resistance to motion blur, high temporal resolution, and extremely low power consumption. These advantages have led to significant successes of event-based methods in different domains, such as robotics [18] and imaging applications [36, 44]. Even though event cameras offer richer semantic information\u2014including continuous temporal motion and higher dynamic range lighting, exploration into high-level and open-world understanding tasks remains limited.\nAs VLMs [26, 35] achieve great successes in image understanding, the event-based community began to develop event-based VLMs for a comprehensive understanding of event data, which can be beneficial to downstream tasks [48, 52] like open-vocabulary segments [25] for large-scale scene understanding. Most of these works focus on image-event-text pair alignment based on CLIP [34] by bridging the modality gap between these data according to task scenarios. Though successful, these CLIP-based methods limit themselves in traditional perception task [43, 51] due to a coarse understanding of event data, which makes it hard to generalize to a fine-grained dynamic semantic task such as the detailed description for object motion pattern and the various relation between objects. It leads to the bottleneck in spatial understanding of many scenarios, for instance, autonomous driving or navigation. Recently, current multi-modal large language models (MLLMs) [1, 29] have utilized images or videos as inputs to obtain an accurate and fine-grained text description, lifting visual perception to natural-language-based comprehension of the world. From our perspective, MLLMs have the potential to overcome the scene understanding limitations of event data and offer a versatile language interface for user interaction.\nHowever, to train an event-based MLLM, we are faced with two main challenges. First, a lack of high-quality text annotation in current image-event-text pair datasets hinders the model from capturing fine-grained semantic information [23, 40]. In those datasets, the coarse text descriptions like \"This is a {category}\" limit the model to the learning of category shapes and hinder the model from excavating more diverse event-based category knowledge such as colour or materials, which prohibits the fine-grained semantic understanding of event data. Second, different from images, event data, as a format of spatiotemporal point clouds, is incompatible with the current RGB encoder architecture. Moreover, preserving the highly spatiotemporal correlations within event data during feature extraction is challenging. An inadequate representation of events can impair event-image-text fine-grained alignment, resulting in sub-optimal scene understanding.\nTo solve these challenges mentioned above, we propose EventVL, the first event-based MLLM framework, as shown in Figure 3. As for data engineering, we first annotate almost 1.4 million high-quality image-text-event paired data by utilizing current powerful open-source MLLM models [7], which has surpassed the commercial MLLMS like GPT-40 [1] in many understanding benchmarks. As for EventVL, we propose Event Spatiotemporal Representation to further excavate the spatiotemporal correlation within event data. We spatially and temporally segment event data into different frames and leverage attention blocks to capture the semantic interaction, which can enhance feature expression. To fine-grain align the latent feature space for images and events, we propose the Dynamic Semantic Alignment for rigorous high-resolution semantics extraction, enabling fine-grained alignment between events and images. Comprehensive experiments verify that our proposed EventVL surpasses other SOTAs in zero/few-shot event captioning, event stream description generation tasks, and event-based dialogue. Furthermore, with a tiny number of parameters (almost 2.3B) compared to other MLLMs [30, 38], our model enables a low-cost for deployment to the real world. As shown in Figure 1 (a), EventVL can fully understand semantics and generate precise descriptions for event streams across diverse domains. Additionally, it supports multi-turn dialogues based on event data, enabling deeper semantic understanding, as shown in Figure 1 (b). In summary, our contribution has three folds:\n(1) We present the First Event-based MLLM Framework, named EventVL, which aligns the large-scale event encoder with the image encoder and LLM. The model demonstrates strong performance on event-based generative tasks such as description or dialogue.\n(2) We propose a simple but efficient description generation engine to get high-quality multi-modalities data by utilizing open-source VLMs. The processed datasets contain almost 1.4 million paired data across various domains. As far as we know, it is the biggest multi-modalities pair dataset in the event-based community.\n(3) Event Spatiotemporal Representation is proposed for feature adaptive and efficient aggregation. We also propose a Dynamic Semantic Alignment module for fine-grained feature alignment, resulting in a precise and comprehensive event-based understanding."}, {"title": "2. Related Work", "content": "Event-based VLMs: Unlike the well-established field of image understanding, the event community is in the early stages of developing universal models for event comprehension. Initial efforts [24, 47] focused on using self-supervision to create a pre-trained backbone for event understanding. Inspired by CLIP's success in computer vision, researchers began to explore its application for detailed and fine-grained event analysis. [43] suggested transforming event streams into an RGB-style representation to achieve event-text alignment using CLIP's architecture. Recently, CLIP-based approaches have been applied to more complex downstream tasks such as motion classification [40] and scene segmentation [25]. However, these studies have mainly concentrated on discriminative tasks, falling short of achieving fine-grained understanding, which restricts their application in generative tasks like interactive event-conditioned text generation. In response to the increasing need for a nuanced understanding of event streams, we are committed to introducing a unified event-based MLLM framework. This framework is designed to fully comprehend the semantics of event streams through explicit language and is compatible with various types of event data.\nOpen-Source MLLMs: The emergence of open-source MLLMs [17, 29, 30, 38] has profoundly impacted the field of Artificial General Intelligence (AGI) by bolstering the ability to process both visual and textual data. In the past year, several open-source MLLMs have gained prominence such as MiniGPT-4 [53] and VisionLLM [39]. However, these models typically rely on traditional images or videos as input, which can result in the generation of low-quality images under extreme imaging conditions, thereby impairing comprehensive world understanding. Event cameras, with their distinct properties, can effectively address these challenges. Therefore, we are exploring the integration of event-based data into existing MLLMs to unlock the full semantic potential of event data."}, {"title": "3. Data Engineering", "content": "A high-quality dataset is crucial for training a MLLM. Some datasets pairing images or videos with events [23, 40] offer only basic categorical text annotations, akin to \"This is a category\", which lack detail and variety. While certain studies suggest using coarse image-text data to address the limited diversity of image-event datasets like N-ImageNet, these annotations often have a weak connection to the images and include much extraneous information. Our experiments indicate that training with such datasets leads to a decline in performance. Therefore, it is imperative to re-annotate this data to create a more precise and granular dataset, thereby enhancing the MLLM's comprehension.\nAs shown in Figure 2, we design a simple but effective generation engine for different domain datasets that contain static images, dynamic scenes and human motions. We chose the InternVL2-76B [7] as the foundation model for description generation, which has shown its superiority and surpasses several SOTAs such as GPT-40 or Claude 3.5 on various understanding benchmarks. Utilizing it to label our data can reduce multiple costs compared to commercial engines and bring higher-quality descriptions.\nCoarse Generation. We manually design the prompts and problem lists corresponding to various datasets, which can be referred to in our appendix. For image-event datasets (N-ImageNet, N-Caltech101), we first randomly sample questions from the problem list, then directly send images with questions into the InternVL2-76B for generating coarse captions. For video-event datasets (HARDVS, DSEC), it is inefficiency that utilizes all frames for generating description, which causes a large amount of computational consumption [8, 25]. To effectively extract long-term information from origin videos, we sampled uniformly n frames. If the video contains less than n frames, we adopt all frames. For balancing the cost and performance, we chose n = 14. Then, the video and the correspondiong instruct is sent to InternVL2-76B for getting detailed description.\nManual Check. The cost of manually checking every image/video-text description is enormous. Hence, we develop the sampling strategies based on the category classification of each dataset for checking. For N-ImageNet, N-Caltech101, HARDVS and DSEC, they contain 1000 classes, 101 classes, 300 classes and 20 scenes respectively. We sample five coarse descriptions from each class or scene for checking. If the output description is not so good, all images/videos from this corresponding class or scenes will be sent to MLLM to regenerate the captions again with modified prompts and designated problems. Finally, all annotated multimodal pair datasets except N-Caltech101 will be mixed for model training. The dataset performance is validated as shown in Table 4. Note that our prepared dataset has almost 1.4 million image/video-event-text pairs covering diverse domains, which guarantees the event understanding performance."}, {"title": "4. Method", "content": "The overall workflow of EventVL is as shown in Figure 3, which is composed of a frozen image encoder, a trainable event encoder, and a frozen text decoder. Given batch-wise image-event-text pair data {(xim, Xev, {x}}{_1)}, where event streams Xev contains Ne event points, Xim and {x}1 denotes the image and text token list. We introduce Event Spatiotemporal Representation containing fine-grained spatiotemporal event semantic information, which is obtained by two split techniques \u2013 Hierarchical Temporal Split and Adaptive Spatial Split. After that, Dynamic Semantic Alignment is proposed for further projecting event data on image feature space in a compact latent space. We use the red-blue color map as the event representation consistent with recent approaches [25, 46]. This choice minimizes the gap between the event representation and the natural images for pre-trained models, thereby simplifying cross-modal alignment.\n4.1. Event Spatiotemporal Representation\nEvent streams carry rich information about the captured scene [37]. To enable effective scene understanding, it is crucial to develop a comprehensive representation of event streams that aligns with neural networks for encoding spatiotemporal semantics. Hence, we propose Event Spatiotemporal Representation which divides and captures fine-grained temporal and spatial information. Hierarchical Temporal Split and Adaptive Spatial Split are proposed to obtain different groups of event frames containing temporal and spatial information, respectively. Finally, all event frames are concatenated, gathering the fine-grained spatiotemporal information.\nHierarchical Temporal Split. Due to the temporal dynamics in event data, there is a critical need for a novel approach to extract rich temporal semantics. Our method diverges from prior studies that have opted for introducing a multitude of parameters [52]. Instead, we propose Hierarchical Temporal Split to aggregate event temporal information in various levels, without the need for any additional parameters. As shown in Figure 3 (i), given an event stream xev containing Ne event points, we firstly define the event points per frame \u03bb\u03b7\u03b5, \u03bb\u2208 {1,2} for splitting the overall event stream into several groups, and transform these groups into RGB-style representations. After splitting hierarchically, we expand the events as multi-level events xer = {{21} 1, {ev2}21, Pev3} with three levels contains N1, N2, and 1 RGB frames, respectively, where N\u2081 = N, N2 = Ne, and ev* \u2208 RHW denotes the transformed RGB-style representations with a height of H and a width of W. The level-3 event representation rev3 aggregates all event points. In implementation, we set the Ne as the fixed number and use padding for event streams containing less than Ne event points. Finally, these temporal splits are sent to Adaptive Spatial Split for further spatial semantics extraction.\nAdaptive Spatial Split. Previous research has focused on encoding complete event representations and aligning them with corresponding images within the feature space. While this method has led to improved model performance [46, 48], it requires resizing event frames to a low resolution, such as 224 \u00d7 224. This resizing results in a significant loss of spatial information, particularly for high-resolution events like those in N-ImageNet [23], which can measure 480 x 640. This issue worsens under poor imaging conditions, such as overexposure or rolling shutter effects, leading to even greater information loss. Hence, Adaptive Spatial Split is introduced for exploring full spatial event-based category information, as shown in Figure 3 (ii), which adaptively splits event representation or images into multiple patches uniformly in a high-resolution manner.\nTo preserve natural aspect ratios during processing, we adaptively select the most suitable aspect ratio from a predefined set K, which encompasses all possible aspect ratio combinations derived from nmin to Nmax tiles. We set Nmin to 1 and set Nmax to 6. For each input image or event representation, we determine its aspect ratio and compare it with the predefined ratios by calculating the absolute difference. In cases where multiple predefined aspect ratios are close matches (e.g., 1:1 and 2:2), we prioritize the one that does not exceed twice the area of the event representation to avoid significant enlargement. Subsequently, we divide the event representation into patches according to this chosen ratio and resize these patches to the desired dimensions. In detailed, given the multi-level events \u0109ev = {{1}1, 111, {ev2} 21, Xev3}, we only apply Adaptive Spatial Split to the level-3 event frame \u0109evz and obtain multiple event patches spatially {{123}r=1} (Np = 6). We concatenate multiple event patches with the origin multi-level events, obtaining Event Spatiotemporal Representation xev = {{ev1}, {2} 21, Xev3, {23}-1}.\n4.2. Dynamic Semantic Alignment\nIn this section, we aim to align event and image within the same latent space. We begin by inputting the event representation zev into the event encoder, which is composed of multiple attention blocks. This process yields event embeddings {{1},{\u03a6\u03b5\u03c52}=1, \u03a6\u03b5\u03c53, {\u03a6\u03b53}+1} that are enriched with exchanged spatiotemporal knowledge and enhanced semantics. For feature alignment, we select dev = {\u03c6\u03b5\u03c5\u03c2, \u03a6\u03b5\u03c53}-1}. To obtain the corresponding image embeddings for alignment, we can perform the Adaptive Spatial Split for image data Xim and get the segmented image patches concatenated with Xim to obtain {Xxim, {xim}r=1}, which is inputted into the image encoder to obtain image embeddings $im = {\u0444\u0456\u0442, {m}-1}. Following the feature extraction by the attention blocks, each frame in \u03c6\u03b5\u03c5 is sufficiently endowed with global temporal semantics and enhanced spatial knowledge. Finally, we conduct a cosine alignment between dev and \u0444\u0456\u0442\u00b7\nCompared to ordinary images, event representation lacks detailed surface descriptions, such as material and texture. It motivates us to project events into the same compact feature space as images, thereby implicitly learning missing attribute descriptions. Hence, we leverage simple cosine similarity loss to maintain their mutual information,\n$Lc = 1 - cos(\\phi_{ev}, \\phi_{im}) = 1 - \\frac{\\phi_{ev} \\cdot \\phi_{im}}{||\\phi_{ev}|| ||\\phi_{im}||} $   (1)\nBy applying this loss, spatiotemporal fine-grained alignment can be achieved, and event data are projected into the same latent space as image data.\n4.3. Framework Optimization\nGenerally, maximizing the likelihood function below to align the representation spaces of image/video and text is a widely-used approach for pre-training [29, 45]. We also leverage this pattern to our event-text alignment. For a given event representation embeddings \u00feev and a conversation list of L text tokens xt = {x+, x1,...,x+}, the likelihood of this list can be written as follows:\n$p(x_t | \\phi_{ev}, x_{instruct}) = \\prod_{l=1}^{L} \\phi_l(x_l | \\phi_{ev}, x_{instruct}, x_{1:l-1}),$   (2)\nwhere xinstruct is the instruction token, and $t(\u00b7) is the text decoder. Then we perform event-text alignment by minimizing the negative log-likelihood, as follows:\n$L_{ev,t} = - log p(x_t | \\phi_{ev}, x_{instruct}).$   (3)\nWe also have observed that text inputs often include detailed descriptions such as texture and color, which are not present in the corresponding event data due to the nature of event imaging. Relying solely on event-text alignment can lead to suboptimal outcomes, as it may focus on missing semantic elements. Therefore, to refine the embedding alignment, we incorporate the image embedding as a prior in Equation 3, effectively limiting the search space and enhancing the overall alignment process:\n$L_{ev,im,t} = - log p(x_t | \\frac{1}{2} (\\phi_{ev} + \\phi_{im}), x_{instruct}).$   (4)\nThe overall training objectives can be written as follows:\n$L = \\frac{1}{2} (L_{ev,t} + L_{ev,im,t}) + \\lambda_2 L_c. $   (5)\nNote that the loss is computed in a batch and we omit the batch notation for clarity."}, {"title": "4.4. Inference", "content": "EventVL offers two kinds of inference pipelines. The first one is to only utilize event modality as input. The second type is to incorporate the image and event modality as input, in which we directly add event embeddings with image embeddings as the input embedding."}, {"title": "5. Experiments", "content": "5.1. Experiments Setup\nImplementation details can be referred to appendix.\nDatasets. N-ImageNet [23] is the event camera version of ImageNet and the largest event camera dataset, which contains almost 1.2 million event streams and 1,000 classes. HARDVS [40] is a recently released dataset for event-based action recognition, currently having the largest action classes, namely 107,646 recordings for 300 action categories. DSEC [16] (Driving Stereo Event Camera) dataset is an autonomous driving dataset with a particular focus on event-based vision. We also utilize N-Caltech101 [31] for evaluating model zero-shot performance and further few-shot testing. Similar to N-ImageNet construction, N-Caltech101 contains 8,246 samples from 101 classes. More dataset details can be found in the appendix.\nDataset Preprocessing. Overall processed dataset information can be referred to Table 1. After we annotate these dataset by Section 3, we split the image/video-event-text pairs from N-ImageNet [23], HARDVS [40], N-Caltech101 [31] into the corresponding training sets and validation sets according to their categories. We use the whole DSEC training set by segmenting its scene video into multiple frame groups. The categories in each split do not overlap with each other in the same dataset for zero-shot evaluation. Few-shot datasets setting can be found in the appendix.\n5.2. Caption Generation\nZero-shot Evaluation. To evaluate the event-based understanding performance of our proposed framework, we conduct comparisons with the selected SOTA MLLM baselines, including image-based [29, 30, 38] and video-based MLLMs [9, 28]. As shown in Table 2, we can conclude that the video-based MLLMs can generate more high-quality descriptions for event data, which is determined by the stream properties of event data. Finally, experiment results demonstrate the effectiveness of EventVL to describe the event data. Our proposed EventVL significantly achieves 2.603 and 0.573 CIDEr in HARDVS and N-ImageNet respectively, outperforming existing works by a large margin,"}, {"title": "4.4. Inference", "content": "EventVL offers two kinds of inference pipelines. The first one is to only utilize event modality as input. The second type is to incorporate the image and event modality as input, in which we directly add event embeddings with image embeddings as the input embedding."}, {"title": "6. Conclusion", "content": "In this work, we introduce EventVL, the first event-based MLLM designed to achieve comprehensive understanding of event stream data. Through the integration of an advanced event encoder that captures rich semantic information, the novel Event Spatiotemporal Representation for efficient feature aggregation, and the Dynamic Semantic Alignment mechanism for fine-grained feature alignment, EventVL significantly advances the understanding and reasoning capabilities for event-based data. Additionally, we contribute a large-scale, high-quality annotated dataset with nearly 1.4 million event-image-text pairs, providing a robust foundation for future event-based research. We believe that EventVL sets a new direction for chat-centric event stream comprehension and lays the groundwork for future breakthroughs in event-driven multimodal systems."}, {"title": "7. Related Work", "content": "Event-based Vision. The microsecond-level temporal resolution, high dynamic range (typically 120 dB compared to the 60 dB of standard cameras), and power efficiency of event cameras represent a paradigm shift in imaging, moving beyond traditional frame-based methods [14]. Numerous event-based low-level imaging tasks, including recognition, perception, localization, and reconstruction, have been developed, covering areas such as object recognition [11, 33], object detection [15, 44], optical flow [12, 27], semantic segmentation [25], depth estimation [32], and object reconstruction [13, 21, 22, 42, 49], often utilizing techniques like NERF or Gaussian Splatting. While event-based models have demonstrated success in these traditional perception tasks, the exploration of event-based MLLMs (Multimodal Large Language Models) remains largely un-charted. In this paper, we focus on a novel applica-tion-event captioning and description for comprehensive event stream understanding. This endeavor aims to address the challenges posed by sparse, asynchronous, and high-temporal-resolution event data, with the goal of generating explicit language descriptions. Such capabilities are particularly crucial for safety-critical applications, such as drone or in-vehicle perception systems."}, {"title": "8. Preliminaries", "content": "The original stream output of an event camera consists of a set of asynchronously generated events, where each event is represented by (x, y, t, p). Here, (x, y) are the spatial co-ordinates, t is the timestamp, and p indicates the polarity of the intensity change: +1 for increased brightness and -1 for decreased brightness. These events feature a high dy-namic range greater than 120 dB and a high temporal reso-lution equivalent to thousands of frames per second. However, directly processing event streams is challenging [14]. Previous works often represent events in frame-like rep-resentation, such as gray-style [43] or red-blue color map [25, 52], to simplify cross-modal alignment."}, {"title": "9. Data Engineering Setting", "content": "In this section, we mainly introduce the origin dataset information, the problem list and process details."}, {"title": "10. Methods", "content": "As shown in Figure 7, we give a case to visualize the event stream processed by our proposed Event Spatiotemporal Representation. We decouple the event stream from time and space to obtain a comprehensive representation of the event stream. Then, We give more explanation of the proposed Adaptive Spatial Split. For pre-defined set K, we utilize Algorithm 1 to complete the K. As we set nmin to 1 and set nmax to 6, we have K \u2208{1:1,1:2,...,6:1}. Then, we utilize the K for following event spatial segmentation in Section 4.1."}, {"title": "11. Experiments", "content": "11.1. Implementation\nFundamental Module. We selected the InternViT-300M-448px\n[7] as the foundational architecture for both the image and event encoders, given its proven effectiveness in various image and video understanding benchmarks. This model contains 24 attention blocks, as shown in Figure 8. The term \"Drop\" refers to the drop path, commonly applied in the main path of residual blocks. In our setup, B denotes the batch size, C represents the channel number, and H and W are the height and width of the image or event. P is the patch number, and C is the final output embed-"}, {"title": "12. Discussion", "content": "Training Methods. In this section, we discuss the various training approaches employed in our methods, including full fine-tuning, LORA [19], and LLM Adaptors [20]. LoRA is commonly used in contexts with limited computational resources and small datasets. We initially experimented with LoRA for training our framework on a small batch of data, but found that its multiple manually-tuned parameters increased the risk of training instability within our framework. We also opted not to use adaptor-based training, as adaptors consist of learnable parameters tied to the original modal-ity, which can limit model performance. Event data, containing more comprehensive information about the captured scenes than traditional camera data [37], requires a large parameter space to adequately capture this information. Given our access to abundant, high-quality paired data, we selected full fine-tuning as the primary training method for our framework. This approach effec-tively captures event-based categorical knowledge, supporting a deeper understanding of event-based world semantics.Scale. Our processed datasets span a variety of domains, including static images [23, 31], body motion [40], and driving scenes [16]. However, the data remains insufficient in certain areas, such as sports movements and complex scenes, which limits the model's depth of understanding of event data. This is particularly evident in the driving domain, where high-resolution event data is still lack-ing, hindering the clarity of perception and collaborative develop-ment between event and regular cameras. To address this, we aim to construct a larger and more comprehensive dataset. Additionally, due to computational resource limitations, we are constrained to using models with fewer than 3 billion parameters for training or fine-tuning. We believe that our proposed framework can be readily scaled to models with a larger parameter count, enhancing its capacity for event-based understanding.Event Representation. In line with the current VLM setting [25, 46, 48], we use a red-blue color map event representation for processing and training. While this approach is effective, it may not be optimal for an MLLM-based framework. In future work, we plan to further investigate the impact of different event repre-sentations [41, 50] to explore alternatives that may better support our framework's performance."}]}