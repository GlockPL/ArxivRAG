{"title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection", "authors": ["Arefeh Kazemi", "Sri Balaaji Natarajan Kalaivendan", "Joachim Wagner", "Hamza Qadeer", "Brian Davis"], "abstract": "This study investigates the role of LLM- generated synthetic data in cyberbullying detec- tion. We conduct a series of experiments where we replace some or all of the authentic data with synthetic data, or augment the authentic data with synthetic data. We find that synthetic cyberbullying data can be the basis for train- ing a classifier for harm detection that reaches performance close to that of a classifier trained with authentic data. Combining authentic with synthetic data shows improvements over the baseline of training on authentic data alone for the test data for all three LLMs tried. These re- sults highlight the viability of synthetic data as a scalable, ethically viable alternative in cyber- bullying detection while emphasizing the crit- ical impact of LLM selection on performance outcomes.", "sections": [{"title": "1 Introduction", "content": "The rapid proliferation of social media platforms has raised concerns over the prevalence of cyber- bullying (CB), particularly among vulnerable pop- ulations like pre-adolescents. Detecting and mit- igating CB is crucial for maintaining a safe digi- tal environment and minimizing its psychological impact. However, creating high-quality labeled datasets for training CB detection models remains a significant challenge. Traditional data collection methods, relying on human annotators, are costly, time-consuming, and pose ethical concerns. Anno- tators may experience emotional distress or harm when exposed to harmful content, which raises questions about their well-being (AlEmadi and Za- ghouani, 2024). This highlights the need for alter- native approaches to generate labeled datasets with- out requiring human annotators to engage directly with harmful content. One promising solution is using large language models (LLMs) for synthetic data generation.\nLLMs have shown remarkable capabilities in"}, {"title": "2 Background", "content": "In recent years, the use of LLMs for synthetic data generation has gained significant traction due to its efficiency, scalability, and cost-effectiveness com- pared to human-annotated datasets. Given that LLMs demonstrate human-like behavior in nat- ural language understanding, many studies have explored their potential in generating synthetic datasets for various NLP tasks. Some works fo- cus on creating entire datasets from scratch, while others utilize LLMs as annotators to label existing authentic data. (He et al., 2021, 2022) generate syn- thetic data for knowledge distillation, self-training, and few-shot learning tasks, which was then anno- tated using state-of-the-art classifiers. (Bonifacio"}, {"title": "3 Methodology", "content": "3.1 Notations\nThe notations used throughout this paper are sum- marized in Table 1.\n3.2 Overview of Scenarios\nIn this study, we investigate the role of LLMs in CB detection, focusing on their utility under varying data availability conditions. To establish a base- line for comparison, we first evaluate a scenario in which a classifier is trained exclusively on gold- standard, manually labeled authentic data without any LLM involvement. We then define three addi- tional scenarios, each illustrating how LLMs can aid cyberbullying detection depending on the avail- ability and quantity of authentic data.\nThe scenarios are as follows.\n3.2.1 Scenario 1: Baseline\nThis scenario represents the ideal situation where sufficient manually labeled (gold-standard) data is"}, {"title": "3.2.2 Scenario 2: LLM as Classifier", "content": "This scenario applies when labeled authentic data is unavailable, and there is no intention to train a separate classifier for CB detection. Instead, an instruction-tuned LLM is used directly as a classi- fier, leveraging its pre-trained knowledge and its ability to folow instructions to identify cyberbully- ing instances. This approach is particularly useful in contexts requiring rapid deployment or when computational or time resources for training a new model are limited. The primary advantage of this method is its elimination of the need for labeled data and training time. However, there are trade- offs. While an LLM can handle nuanced language patterns, it may be less efficient and incur higher computational costs compared to simpler BERT- based classifiers with a classification head and fine-tuned on a labeled dataset. This scenario offers a quick solution but presents potential limitations in accuracy and efficiency."}, {"title": "3.2.3 Scenario 3: Fully Synthetic Data", "content": "In this scenario, only a small set of manually la- beled gold data is available for testing, with no access to authentic data for training or validation. To address this, we use an LLM to generate a fully synthetic dataset, consisting of both synthetic mes- sages and corresponding labels, for training and validation.\nThis approach is particularly valuable in low- resource domains or emerging tasks where authen- tic data is scarce or difficult to collect. It is espe- cially useful in situations where creating authen- tic datasets is costly, time-consuming, or ethically challenging, such as annotating harmful or sensi- tive content. This scenario underscores the poten- tial of LLMs as a powerful tool for bootstrapping datasets under extreme resource constraints."}, {"title": "3.2.4 Scenario 4: Data Augmentation with Synthetic Data", "content": "This scenario assumes the availability of a moder- ate amount of gold-labeled data for training and validation, which may be insufficient to achieve optimal performance. To augment the dataset, we use an LLM to generate additional synthetic data, which is then combined with the gold-labeled data during training and validation. The experiment sys- tematically varies the ratio of synthetic-to-gold data to evaluate its impact on model performance. This scenario explores how LLMs can supplement au- thentic data, striking a balance between scalability and accuracy."}, {"title": "3.2.5 Summary of Scenarios", "content": "Table 2 presents an overview of the data used in the baseline system and each scenario, specifying the datasets utilized for training, validation, and testing. For Scenario 2, where no classifier is trained and the LLM is used directly as a classifier, only the test set is included.\n3.3 Intrinsic Evaluation Metrics\nIntrinsic evaluation examines the inherent qualities of datasets, enabling the assessment of linguistic diversity, emotional tone, and conversational struc- ture independently from task-specific performance. For our CB detection task, we utilize four cate- gories of intrinsic metrics to compare the authentic WhatsApp dataset with LLM-generated synthetic data. These categories are: 1) lexical and linguis- tic characteristics, including metrics such as Mean Words per Message, Mean Word Length, and Type- Token Ratio; 2) content and CB indicators, such as rate of Harmful Messages, Bully Messages, Victim Messages, and Toxicity; 3) sentiment and emo- tional tone, which classifies messages into nega- tive, positive, or neutral; and 4) dialogue act distri- bution, categorizing messages into types such as Question, Statement, Greeting, Accept/Reject, and Other. These categories are critical for understand- ing the fundamental differences between authentic and synthetic data in the context of CB detection,"}, {"title": "3.4 Extrinsic Evaluation Metrics", "content": "We evaluate the synthetic data in two scenarios for their usefulness in training a binary classifier for harm detection. We choose accuracy of label prediction for development decisions and reporting since the labels are reasonably balanced in the test data with 30.3% items labelled with the minority label. In the appendix, we further report macro average F1 scores that are also widely used in the area of harmful content detection."}, {"title": "4 Experimental Setup", "content": "4.1 Dataset Description\nSocial science research highlights the importance of detecting, intervening, and preventing cyber- bullying by closely examining social interactions among pre-adolescents at a detailed level. The WhatsApp dataset (Verma et al., 2023; Sprugnoli et al., 2018) is the only publicly available dataset specifically focused on pre-adolescents and we use this for our experiments.The WhatsApp dataset was constructed through role-play activities in What- sApp groups, each containing approximately 10"}, {"title": "4.2 Large Language Models", "content": "In this study, we employ three LLMs for our exper- iments: GPT, LlamA, and Grok. Specifically, we use GPT-4.0, LlaMA 3, and Grok-2-latest models."}, {"title": "4.3 Generating Synthetic Data and Labels", "content": "Prompt Engineering is essential for harnessing the full potential of LLMs. It involves crafting and re- fining the instructions provided to the LLM to elicit the desired outputs. The prompt design process for all scenarios began with a simple initial prompt, which was iteratively refined over multiple rounds of trial and error. This iterative approach allowed us for gradual improvements, with adjustments made"}, {"title": "4.3.1 Designing Prompts for Synthetic Label Generation", "content": "In this section, we describe the prompts designed to instruct an LLM to label unlabeled input data for the task of CB detection. In this context, the input data consists of messages, and the possible labels are \"harmful\" and \"harmless\". Harmful messages show instances of CB, while harmless messages are normal, safe communications. We explore two ap- proaches to prompt design: (1) guideline-free (GF) and (2) guideline-enriched (GE). In the guideline-free approach, the LLM is simply instructed to label messages as \"harmful\" or \"harmless\" for the task of CB detection, without providing additional guideline. In the guideline-enriched approach, the LLM is supplied with detailed instructions for la- beling messages. These instructions are adapted from the annotation guidelines originally used by human annotators for labeling authentic data.\nFor this study, since the test set is derived from the WhatsApp dataset, we utilize the same guide- line employed by human annotators to label this dataset. This guidelines is presented in Table 11 in AppendixA. Table 12 shows the prompts used to generate synthetic labels for both the GE and GF approaches."}, {"title": "4.3.2 Designing Prompts for Synthetic Data Generation", "content": "In this section, we present the prompts designed to guide a LLM in generating synthetic data for CB detection. Specifically, the synthetic data we aim to produce consists of conversations between partici- pants, where the dialogue demonstrates instances of CB, including harmful messages. To train the CB classifier, we require labeled messages. There- fore, we first use the LLM to generate synthetic coversation and subsequently use the LLM to label the messages in the conversation.\nWe used the cases defined in the WhatsApp dataset to generate synthetic conversations.By pro- viding the system with the predefined \"cases\" and \"problem types\" from the Whatsapp dataset, we prompted it to create conversations based on these inputs. Table 12 presents the prompts employed for generating synthetic data."}, {"title": "4.4 Amount of Synthetic Data", "content": "For a more meaningful comparison to results with the authentic data, we sample subsets of the gen- erated data to match the size of the authentic data for each CB case. We also sample subsets of cer- tain percentages of these sizes from 10% to 200%.3 Since the test set is never synthetic, we measure sizes relative to the concatenation of training and validation data, i.e. 1314 + 439 = 1753 messages correspond to 100%."}, {"title": "5 Results and Discussion", "content": "Table 4 shows the results of the intrinsic evaluation across four categories for each dataset. In terms of lexical and linguistic characteristics, the synthetic datasets show significant differences from What- sApp. Both Grok and Llama generate longer mes- sages compared to WhatsApp. Meanwhile, GPT is closer but still produces longer messages. The Type-Token Ratio (TTR) is lower for all synthetic datasets compared to WhatsApp, indicating less lexical diversity in the synthetic data. This sug- gests that the synthetic data may lack the richness in vocabulary typically found in real-world What- sApp conversations.\nIn the content and cyberbullying indicators cat- egory, the synthetic datasets show varying levels of harmful content, with Grok standing out by pro- ducing the highest percentage of harmful messages (65.27%) and toxicity (2.37%). This high level of harmful content in Grok can be valuable for detect- ing extreme cases of CB. While Llama (17.19%) and GPT (6.57%) have much lower harmful con- tent, these datasets may still present valuable data for detecting subtler instances of CB, with Llama's content being more balanced and closely resem- bling real-world interactions.\nIn terms of sentiment scores, Llama and GPT are more similar to WhatsApp overall. Both Llama and GPT have a slightly higher percentage of positive"}, {"title": "5.1 Scenario 1: Baseline", "content": "Table 5 presents the development and test set accu- racies for BERT-based classifier trained on differ- ent portions of the training split of authentic data in scenario 1. The results indicate that increasing the training data generally improves both development and test accuracy. When training on only 20% of the data, test accuracy is the lowest at 75.3%. With 50% of the data, test accuracy increases to 80.4%, indicating a significant performance boost from us- ing more data. At 90%, test accuracy reaches its highest value of 81.0%, suggesting that this amount of data provides the best generalization. Training"}, {"title": "5.2 Scenario 2: LLMs as a Classifier", "content": "Table 6 presents the accuracy of GPT-4o, Grok, and Llama models using GE and GF prompts on both development and test sets in Scenario 2. We evaluate both GE and GF prompts on the develop- ment set and use the winning method (in all cases, GE) on the test set. GPT-4o achieves the highest accuracy with GE prompts, demonstrating its supe- rior generalization, while Grok performs the lowest. The consistent underperformance of GF prompts, particularly for Grok, highlights the importance of prompt design for model accuracy. Overall, GPT- 40 demonstrates the best performance across both sets, while the selection of GE prompts as the win- ning method proves effective for all models in test evaluation."}, {"title": "5.3 Scenario 3: Fully Synthetic Data", "content": "Table 7 presents the development and test set accu- racy for scenario 3: BERT-based classifiers trained on synthetic datasets generated by Llama, GPT, and Grok.\nThe results indicate that Llama consistently out- performs the other two models across different"}, {"title": "5.4 Scenario 4", "content": "Table 8 shows the Development and test set ac- curacies in scenario 4. We investigate two key aspects: (1) The comparison of different LLMs (GPT, Llama, and Grok) for augmentation, (2) The minimum amount of authentic data required to achieve comparable results to using the full au-"}, {"title": "6 Conclusions", "content": "In this paper, we explored the potential of LLM- generated synthetic data for cyberbullying detec- tion, evaluating its effectiveness in various scenar- ios, including direct classification and data aug- mentation for training. Our results highlight that synthetic data can significantly reduce reliance on human annotators while maintaining competitive model performance, especially in low-resource set- tings. However, we also observed that the qual- ity and utility of synthetic data depend heavily on prompt design and data selection strategies.\nFor future work, we want to expand the set of data availability scenarios to the following two sce-"}, {"title": "Scenario 3b: Synthetic Labels for Unlabeled Data", "content": "This scenario addresses the common situation where manual labeling re- sources are limited. Here, gold-standard la- beled data is available only for the test set, while a significant amount of unlabeled au- thentic data is available for training and valida- tion. To utilize this unlabeled data, we employ an LLM to generate synthetic labels. Two strategies are explored: (1) guiding the LLM with detailed labeling instructions, and (2) al- lowing the LLM to generate labels without such guidelines. This scenario demonstrates the utility of LLMs in resource-constrained settings, enabling cost-effective dataset cre- ation from unannotated corpora."}, {"title": "Scenario 4b: Data Augmentation with Synthetic Labels for Unlabeled Data", "content": "In this sce- nario, we assume access to a mix of gold- standard labeled data and unlabeled authentic data for training the CB classifier. To maxi- mize the utility of the unlabeled data, we use an LLM to generate synthetic labels for it. The resulting labeled authentic data and synthetic- labeled authentic data are combined to form the training and validation sets.\nThis setup is particularly relevant when large amounts of unlabeled authentic data are avail- able, but manual annotation is prohibitively expensive. By converting unlabeled data into labeled training examples, this approach en- hances dataset size and diversity, potentially improving the model's generalization capabil- ity."}, {"title": "Acknowledgments", "content": "This research is supported by the Disruptive Tech- nologies Innovation Fund (DTIF) under the project \"Cilter: Protecting Children Online\" Grant No. DT 2021 0362 from the Department of Enterprise, Trade and Employment in Ireland and adminis- tered by Enterprise Ireland (EI). This research was conducted with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Cen- tre at Dublin City University. ADAPT, the SFI Re- search Centre for AI-Driven Digital Content Tech- nology, is funded by Science Foundation Ireland through the SFI Research Centres Programme. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this sub- mission."}]}