{"title": "Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A Novel Weighted Retrieval-Augmented Generation Paradigm", "authors": ["Rajat Khanda"], "abstract": "Technical troubleshooting in enterprise environments often involves navigating diverse, heterogeneous data sources to resolve complex issues effectively. This paper presents a novel agentic AI solution built on a Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise technical troubleshooting. By dynamically weighting retrieval sources such as product manuals, internal knowledge bases, FAQs and troubleshooting guides based on query context, the framework prioritizes the most relevant data. For instance, it gives precedence to product manuals for SKU-specific queries while incorporating general FAQs for broader issues. The system employs FAISS for efficient dense vector search, coupled with a dynamic aggregation mechanism to seamlessly integrate results from multiple sources. A LLaMA-based self-evaluator ensures the contextual accuracy and confidence of the generated responses before delivering them. This iterative cycle of retrieval and validation enhances precision, diversity, and reliability in response generation. Preliminary evaluations on large enterprise datasets demonstrate the framework's efficacy in improving troubleshooting accuracy, reducing resolution times, and adapting to varied technical challenges. Future research aims to enhance the framework by integrating advanced conversational AI capabilities, enabling more interactive and intuitive troubleshooting experiences. Efforts will also focus on refining the dynamic weighting mechanism through reinforcement learning to further optimize the relevance and precision of retrieved information. By incorporating these advancements, the proposed framework is poised to evolve into a comprehensive, autonomous AI solution, redefining technical service workflows across enterprise settings.", "sections": [{"title": "Introduction", "content": "Enterprise technical troubleshooting is a complex, knowledge-intensive process that often requires access to a diverse set of resources, including product manuals, FAQs, troubleshooting guides, and internal knowledge bases. These resources are typically scattered across various data silos, making it difficult to extract the most relevant information quickly, particularly in high-pressure, time-sensitive situations.\nTraditional approaches to troubleshooting rely heavily on keyword-based search engines or static knowledge management systems that fail to capture the nuanced, context-dependent nature of technical issues. For instance, while a keyword-based system might retrieve a generic"}, {"title": "Dynamic Weighting Mechanism", "content": "We introduce a dynamic weighting mechanism that assigns context-dependent weights to each data source during retrieval. This ensures that the system prioritizes the most relevant sources for a given query while maintaining the flexibility to incorporate other useful resources."}, {"title": "Enhanced Retrieval and Aggregation", "content": "The framework utilizes FAISS for dense vector search, incorporating a top-K selection mechanism to retrieve the most relevant results. To ensure precision, a threshold is applied to each individual FAISS index, filtering out retrievals that fall below the set threshold and eliminating irrelevant or weakly related matches. This filtering step minimizes the potential for hallucinations by the LLM, thereby improving response accuracy. Following this, dynamic aggregation techniques are employed across the filtered indices to effectively integrate information from multiple sources, ensuring comprehensive and contextually relevant outputs."}, {"title": "Self-Evaluation for Accuracy Assurance", "content": "A LLAMA-based self-evaluation module is integrated to assess the accuracy and contextual relevance of generated responses before presenting them to the user. By employing this self-evaluator model, the system ensures that outputs align closely with the query's intent, delivering consistently high-quality, contextually appropriate answers tailored to user needs."}, {"title": "Scalability and Versatility", "content": "Our framework is engineered to seamlessly adapt to a wide spectrum of enterprise datasets and technical scenarios, emphasizing both scalability and versatility. By leveraging a Facade Pattern for data sources, it simplifies integration: adding a new data source requires only the creation of a dedicated data source class and attaching it to the existing facade. This design minimizes complexity while maximizing extensibility. Furthermore, the data sources can be searched in parallel, ensuring that retrieval processes are both efficient and scalable. This parallel search capability enhances the system's ability to handle large volumes of data, significantly improving performance in complex troubleshooting tasks.\nThis approach accelerates troubleshooting by enhancing both speed and precision, creating a robust foundation for scalable, AI-driven, and autonomous technical service systems tailored to enterprise environments. The modularity of our framework ensures rapid adaptation to evolving business needs and technological advancements."}, {"title": "Related Work", "content": "Enterprise troubleshooting has traditionally relied on keyword-based search systems, such as Elasticsearch, which offer efficient solutions for querying structured and semi-structured data.\nThese systems typically leverage ranking algorithms like BM25[Robertson et al., 1994] or TF-IDF"}, {"title": "Methodology", "content": null}, {"title": "Embedding Model and Retrieval Infrastructure", "content": "For generating document and query embeddings, we employ the all-MiniLM-L6-v2 model. This model is selected due to its efficiency and semantic accuracy, which allows for the generation of high-quality, compact embeddings that effectively capture the meaning of the textual data. The all-MiniLM-L6-v2 model excels in encoding both short and long texts into fixed-size vectors, preserving their semantic relationships for effective downstream retrieval.\nFor the retrieval process, we use FAISS (Facebook AI Similarity Search) as the infrastructure for dense vector search. FAISS enables fast and scalable retrieval by indexing the embeddings generated by the all-MiniLM-L6-v2 model and performing nearest neighbor searches within a high-dimensional space. This system allows for efficient querying of large datasets, ensuring that relevant documents are retrieved quickly and accurately based on their semantic similarity to the input query."}, {"title": "Dynamic Weighting Strategy", "content": "To prioritize the most relevant data sources during retrieval, we implement a dynamic weighting mechanism. For each FAISS index corresponding to a specific data source (e.g., product manuals, FAQs, troubleshooting guides, internal knowledge bases), a dynamic weight wk is assigned based on query type and context. This weight adjusts the effective distance Dk,i for document i in index k, computed as:\n\u010ek,i = wk. Dk,i\nwhere Dk,i is the original FAISS distance score.\nWeights wk are determined using domain knowledge and patterns observed in query behavior. For example, product manuals are assigned higher weights for queries containing SKU identifiers, whereas FAQs may be emphasized for general troubleshooting queries."}, {"title": "Top-K Selection and Aggregation", "content": "The retrieval process begins by computing the adjusted distances for each data source k. Prior to aggregation, a threshold-based filtering mechanism is applied to each data source to remove results with weak matches. This filtering step ensures that only results that meet a minimum relevance threshold are retained, effectively reducing the risk of hallucinations in the large language model (LLM). The filtered results for each data source are defined as:\nTk = {i | \u010ek,i satisfies the threshold condition for index k}\nAfter filtering, the top K results are selected for each data source k:\nTk = {i | \u010ek,i is among the top K for index k}\nThese top results from all sources are then aggregated into a global pool G as follows:\nG = \u22c3 Tk\nk=1\nFinally, the top K results from the global pool G are selected based on the smallest adjusted distances:\nTfinal = Top-Kmin(G)"}, {"title": "Response Generation and Self-Evaluation with LLaMA", "content": "The final set of retrieved document chunks is provided to a LLaMA-based generative model, along with the user's query, to produce a contextually relevant response. The model synthesizes the answer by leveraging the retrieved information, ensuring comprehensive coverage of the query. The generated response is then passed to a LLaMA-based self-evaluator model, which assesses its accuracy and relevance. This evaluation process ensures that the response aligns with both the query and factual expectations. Responses that meet a predefined confidence threshold for accuracy and relevance are considered valid and presented to the user. By incorporating this self-evaluation step, the system minimizes the risk of hallucinations and enhances the overall quality and reliability of the generated output, particularly in technical troubleshooting scenarios."}, {"title": "System Architecture", "content": "The proposed framework consists of three main components:\n1. Preprocessing and Indexing: The data is first preprocessed and chunked based on the data's granularity. The resulting chunks are then embedded using the all-MiniLM-L6-v2 model, which generates high-quality vector representations. These embeddings are indexed using FAISS, a high-performance library for fast and scalable nearest neighbor search, ensuring efficient retrieval based on query similarity.\n2. Query Handling: Upon receiving a query, it is embedded using same all-MiniLM-L6-v2 model employed for document embeddings. The resulting query embedding is then matched against the indexed embeddings through a weighted retrieval mechanism. This mechanism adjusts the retrieval process by dynamically prioritizing relevant data sources, taking into account the query's context, type, and specific requirements. By incorporating this dynamic weighting, the system ensures the retrieval of the most relevant and contextually appropriate information, optimizing the accuracy of the response generation.\n3. Response Generation and Validation: The top retrieved chunks are passed to a LLaMA-based generative model, which synthesizes a response based on the query and retrieved information. To ensure the response's accuracy and relevance, it is evaluated by a self-evaluator model, which scores the response against predefined confidence thresholds. Only responses that meet these thresholds are presented to the user, ensuring reliable and contextually accurate outputs."}, {"title": "Experimental Setup", "content": null}, {"title": "Dataset Preparation", "content": "To evaluate the proposed framework, we constructed a comprehensive dataset comprising diverse enterprise troubleshooting resources:\n\u2022 Product Manuals: A collection of 1200 detailed technical manuals for various systems and motherboards, indexed based on generations.\n\u2022 FAQs: 40,000 frequently asked questions covering common troubleshooting scenarios."}, {"title": "Experimental Environment", "content": "The experiments were conducted on a system equipped with:\n\u2022 Hardware: A server with four NVIDIA A100 GPUs(80GB VRAM each), and two 40-core Intel Xeon processors.\n\u2022 Software: Python 3.10, PyTorch 2.0, Hugging Face Transformers, and FAISS 1.7.3.\n\u2022 LLaMA Model: LLaMA-3.1(70B FP16) inference and LLaMA-3.1(70B) self evaluator\nThe framework was deployed as a service using FastAPI, enabling real-time query handling and response generation."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of the system, we used the following metrics:\n\u2022 Accuracy: The percentage of responses that contain correct and contextually relevant information, ensuring that the generated answers align with the user's query.\n\u2022 Relevance Score: Measures how well the retrieved information aligns with the context of the query, evaluating the pertinence of the returned results."}, {"title": "Baseline Systems", "content": "To benchmark the effectiveness of our proposed framework, we compared two baseline systems:\n\u2022 Keyword-Based Search: A traditional retrieval system utilizing BM25 to match query keywords against indexed documents. This baseline represents conventional search methodologies commonly employed in troubleshooting systems.\n\u2022 Standard RAG: A Retrieval-Augmented Generation (RAG) framework that assigns equal weights to all data sources during the retrieval process. This approach ensures a static and unbiased retrieval mechanism, serving as a comparison point to highlight the benefits of dynamic weighting.\nThe comparison with these baseline systems highlights the improvements offered by our framework, particularly the enhanced adaptability of the dynamic weighting mechanism, the increased reliability introduced through the LLaMA-based self-evaluation component, and the effectiveness of threshold-based filtering for refining retrieved results."}, {"title": "Results and Discussion", "content": null}, {"title": "Performance Analysis", "content": "Table 1 provides a comparative summary of system performance across key evaluation metrics. The proposed framework consistently outperformed the baseline systems, achieving higher accuracy and relevance scores while maintaining competitive efficiency."}, {"title": "Impact of Dynamic Weighting and Threshold-Based Filtering", "content": "The dynamic weighting mechanism and threshold-based filtering significantly enhanced the system's performance. Dynamic weighting emphasized relevant data sources for specific queries, resulting in higher accuracy and relevance compared to the standard RAG system. For instance, queries involving SKU identifiers benefited from increased weighting of product manuals, yielding more precise results.\nThreshold-based filtering further improved the system's reliability by eliminating low-confidence matches during retrieval. This approach ensured that only highly relevant embeddings were considered, reducing noise and enhancing the quality of the final response. Together, these mechanisms contributed to the framework's robust performance across diverse query scenarios."}, {"title": "Self-Evaluation Effectiveness", "content": "The LLaMA-based self-evaluator played a pivotal role in enhancing accuracy and relevance by suppressing less accurate responses based on a predefined confidence threshold. This mechanism, combined with dynamic weighting and individual index-level threshold filtering, resulted in a 5.6% improvement in accuracy over the baseline RAG framework. These results highlight the effectiveness of integrating generative models with robust validation and filtering strategies to ensure reliability and precision in response generation."}, {"title": "Conclusion and Future Work", "content": "This study introduces a dynamically weighted Retrieval-Augmented Generation (RAG) framework tailored for enterprise technical troubleshooting. By integrating a context-sensitive retrieval mechanism with generative self-evaluation, our approach achieves superior accuracy, interpretability, and robustness compared to traditional static pipelines. The dynamic weighting strategy ensures retrieval is fine-tuned for query-specific contexts, enhancing adaptability and relevance in diverse troubleshooting scenarios involving heterogeneous data sources.\nOur results demonstrate that the proposed system effectively retrieves and synthesizes knowledge to deliver high-quality, actionable solutions for enterprise users. The addition of a self-evaluation component further enhances response reliability, enabling iterative refinement and ensuring consistent performance.\nFuture work will explore:\n\u2022 Real-Time Learning and User Adaptation: Developing mechanisms for real-time feedback to continuously refine retrieval and generation processes, dynamically adjusting weights and model parameters based on user interactions.\n\u2022 Conversational Troubleshooting: Extending the framework to support multi-turn conversational workflows, enabling iterative problem-solving while managing context and query history.\nBy focusing on these enhancements, the framework can evolve into a comprehensive solution for complex enterprise technical troubleshooting, adapting to diverse user needs and advancing intelligent enterprise support systems."}]}