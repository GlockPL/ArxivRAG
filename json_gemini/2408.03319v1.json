{"title": "Training LLMs to Recognize Hedges in Spontaneous Narratives", "authors": ["Amie J. Paige", "Adil Soubki", "John Murzaku", "Owen Rambow", "Susan E. Brennan"], "abstract": "Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-40 and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-40. After an error analysis on the top performing approaches, we used an LLM-in-the-Loop approach to improve the gold standard coding, as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research. This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation.", "sections": [{"title": "1 Introduction", "content": "The virtuosity of LLMs such as ChatGPT has led some to the impression that AI already converses (or will soon be able to converse) as people do. But as language users, LLMs and humans are quite different. The underlying foundations for learning by these distinct kinds of language users share little in common: Humans learn as infants to interact with others well before they learn their first words, and once word learning begins, they can pick up a new word in one or just a few exposures, whereas LLMs are pre-trained on humanly unfathomable quantities of text without ever learning to interact. Transformer-based chat programs can generate paragraphs-worth of text remarkably well without modeling the coordination between agents-but is this conversation?\nWhether a sequence of prompts and responses exchanged in a dialogue between an LLM agent and a human counts as truly (rather than superficially) \"conversational\u201d depends on how conversation is conceptualized. Conversation is often presumed to be the passing back and forth of messages (a \"message model\"); but that does not explain phenomena common to spontaneous conversation such as incremental turns, clarifications, and repair. Here we conceptualize conversation as a collaborative process of grounding meanings (seeking and providing evidence) during which two or more partners signal, coordinate, and align their beliefs or cognitive states (Brennan, 2005; Clark and Wilkes-Gibbs, 1986). This leads to a broader research agenda that we hope will push generative AI to model phenomena such as a partner's knowledge or theory of mind, mutual beliefs or common ground, as well as when to take initiative in a dialogue.\nThe main contributions of this work include:\n(i) After grounding the project in psycholinguistic theory (Section 2) and related work (Section 3), we present the Roadrunner-Hedge Corpus (Section 4), a corpus of spontaneous face-to-face narratives annotated for hedging.\n(ii) We describe a set of experiments on this corpus using zero-shot, few-shot, and fine-tuning methods on modern LLMs (Section 5).\n(iii) We perform a detailed error analysis pinpointing where LLMs fail in detecting hedges (Section 6). With this analysis, we take an LLM-in-the-Loop approach to correcting gold annotations, reducing errors in our top performing systems."}, {"title": "2 Theoretical Foundations from Psycholinguistics", "content": "In conversation, people communicate not only about the purpose or topic at hand, but they also communicate meta-information about what they're saying within the context of interaction, or collateral signals (Clark, 1996). Along with providing evidence for grounding in conversation, about whether a prior turn has been understood as intended (Clark and Brennan, 1991), collateral signals can also provide information about the speaker's relationship with the content of their message-how confident they are in what they are saying, whether it is difficult to recall or express, and whether they would welcome input from their partner. In this project, we focus on a particular kind of collateral signal used for coordination, hedges.\n2.1 Why Speakers Hedge\nThere have been several proposals for why speakers hedge. Hedges have been claimed to characterize powerless \u201cfeminine\" language (Lakoff, 1973) or to serve a politeness function by minimizing threat to a partner's \"face\u201d (Brown and Levinson, 1987); see also (Fraser, 2010). Hedges have also been thought to convey a certain \u201cfuzziness\u201d of category membership when a speaker means to describe a non-prototypical member of a category (e.g., a penguin belonging to the bird category; Lakoff, 1975). Prince et al. (1982) suggested that hedges play two functions: First, to make propositional content less exact (approximators, e.g. \u201csort of\u201d) and second, to change the relationship a speaker has to the content of their message (shield hedges). Shield hedges are further divided into plausibility shields that signal a lack of commitment to the content of a message (\"I think his feet were blue,\u201d Prince et al., 1982, p. 5), and attribution shields that assign responsibility for a message to a source other than the speaker or writer themself (\"According to her estimates...\" Prince et al., 1982, p. 13).\nSeveral experimental studies have demonstrated how hedges can convey speakers' commitment to what they are saying. For example, in a question-answering task, people trying to recall the answers to trivia questions produced more disfluencies, longer latencies, more rising intonation, and more expressions of doubt when they reported having a low feeling of knowing about an answer. This metacognitive information was confirmed to be accurate when compared to the ground truth in the form of their answer to the same (multiple-choice) question later (Smith and Clark, 1993). Not only are hedges informative as collateral signals about what a speaker knows, but they are accurately interpreted as such by listeners (Brennan and Williams, 1995).\nThat hedges function as interactional signals in extended dialogue is evident from studies of referential communication. Typically in such studies, two partners who can't see each other converse in order to arrange and rearrange duplicate sets of objects in matching orders, with the objects needing to be distinguished from similar objects or consisting of Tangrams (abstract geometric shapes unassociated with any conventional or lexicalized labels). Hedges are common in initial referring expressions, where they tend to appear in wordy, disfluent, and often tentative descriptions, and then they drop out in repeated referring expressions once partners have reached a shared conceptualization for that object (marked by entrainment, or re-using the same shortened referring expression) (Brennan and Clark, 1996; Galati and Brennan, 2021), as in this sequence of repeated references to the same object over multiple rounds (adapted from Brennan and Clark, 1996, p. 1488):\nRound 1: \"a car, sort of silvery purple colored\"\nRound 2: \"purplish car going to the left\"\nRound 5: \"the purple car\"\nIn another study that required triads of strangers to reach consensus while recalling the events from a movie clip that they had watched earlier, the speakers often hedged their contributions to the conversation, presumably to mark a lack of certainty about an utterance and an openness to being corrected by their partners (Brennan and Ohaeri, 1999). For example, from a triad that communicated by speaking face-to-face:\nYeah, they were sitting around the fireplace in the night... sort of like a bedtime story kind of thing\nPeople who did the same task by texting rather than speaking used fewer words, but still hedged:\nWe all agree it was a wreathy thingy on his neck???"}, {"title": "2.2 How Listeners React to Hedges", "content": "Hedges convey meaningful information that can affect listeners' subsequent behavior; a handful of psychological studies have measured the impacts of hedges on listeners. For example, children exposed to new words from a speaker who hedged learned fewer novel words compared to children exposed to a speaker who did not hedge (Sabbagh and Baldwin, 2001). Listeners rated utterances as more uncertain when they included shield hedges (e.g., \"I think it was a mug\"), and these ratings were related to speakers' ratings of their own uncertainty in identifying an image (Pogue and Tanenhaus, 2018). Moreover, addressees in a referential communication task expended more effort while grounding (they produced more low-confidence responses such as clarification questions) to demonstrate understanding when the speaker's description had contained a hedge (Dahan, 2023).\nHedges also influence which details are retold to another person; in one study, hedged details were less likely to be repeated to another addressee as compared to unhedged details (Liu and Fox Tree, 2012), although in the same study, hedged information presented in a story was more likely to be remembered by listeners; this was thought to stem from deeper engagement with hedged information when it was first presented (Liu and Fox Tree, 2012). And in tutoring dialogues, where face management can be particularly important, students were more successful at solving problems when their peer tutors used hedges (Madaio et al., 2017)."}, {"title": "3 Related Computational Work", "content": "3.1 Hedging\nSeveral research programs have examined hedges and the criteria for coding them, with computational goals that include automatic hedge detection. Hedging is domain-specific, meaning that their forms and frequencies vary across corpora; they are also context-specific, as they cannot be identified accurately simply by searching for strings (Prokofieva and Hirschberg, 2014). Hedges are distributed differently within different corpora (ibid). Hedges are often ambiguous and difficult to code in the absence of dialogue context. In \u201cI think it's a little odd,\" I think is often a hedge, but might not be when proffered in response to a question (\"So what do you think?\"). Hedges in spoken utterances may be disambiguated by stress and other intonational cues, as in \u201cI think he'll win!\" (not a hedge) vs. \u201cI think he'll win?\u201d (a hedge). Previous work found many cases of tokens that can serve as hedges as well as non-hedges, with systematic tests for coders to use in annotating them for gold standards (Prokofieva and Hirschberg, 2014; Ulinski and Hirschberg, 2019; Ulinski et al., 2018).\nThe coding of hedges is complicated by the fact that in spoken dialogue, they often co-occur with speech disfluencies. In some contexts, it may be difficult to distinguish these two kinds of signals (Prokofieva and Hirschberg, 2014), particularly since listeners can use disfluencies in much the same way they can use hedges to draw conclusions about the speaker's mental state (Arnold et al., 2003, 2007)\nA strong motivation for computational work on hedging comes from work on computer-assisted learning by Cassell and colleagues, specifically tutoring dialogues (Abulimiti et al., 2023a,b; Raphalen et al., 2022). Most similar to our work is Raphalen et al. (2022), where the authors propose a model that combines rule-based classifiers and machine learning models with interpretable features such as unigram and bigram counts, part-of-speech tags, and LIWC categories to identify and classify hedge clauses. Our work differs in two major ways: first, our work operates on the token level rather than on the clause level. Token level classification makes possible a truly end-to-end approach (classifying all hedge and non-hedge tokens in utterances). Second, we include experiments with modern LLMs and offer a detailed error analysis into their mistakes; stemming from this error analysis, we use an LLM-in-the-Loop approach (Dai et al., 2023) to correcting gold standard hedge codings.\n3.2 Belief\nHedging and the notion of belief (how committed the speaker is to the truth of an event) are closely related; hedges are often used by speakers to indicate a lack of belief or commitment towards what they say. Ulinski et al. (2018) improved belief classification using a hedge detector, yielding an improvement for the non-committed and reported belief labels.\nCorpora Several corpora have been created that annotate the author's degree of belief (Diab et al., 2009; Prabhakaran et al., 2010; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018; Pouran Ben Veyseh et al., 2019; Jiang and de Marneffe, 2021)."}, {"title": "4 The Roadrunner-Hedge Corpus", "content": "For training and testing, we obtained a corpus (Galati and Brennan, 2010) of spontaneous narratives produced from memory by 20 speakers who had watched a Roadrunner cartoon. Each speaker narrated the story face-to-face to an audience, a total of three times: first to a na\u00efve addressee, a second time to the same addressee, and a third time to a new na\u00efve addressee (with the latter two episodes counterbalanced for order). The original experiment was designed to detect differences in collateral signals (intelligibility vs. attenuation of speech and gestures) stemming from the speaker's vs. the addressee's knowledge states-that is, whether the story was new for the speaker (told for the first time) vs. old (retold), compared to the addressee's knowledge state (new vs. heard for the second time). Findings included that the attenuation of both referring expressions (Galati and Brennan, 2010) and gestures (Galati and Brennan, 2014) were driven by both speakers' and addressees' knowledge states\u2013that is, shortened upon retelling the story to the same addressee, but lengthened upon retelling to a new addressee.\nGold Standard Coding. The original corpus transcribed the spontaneous narratives in detail, including speaking turns and disfluencies (for details, see Galati and Brennan, 2010), segmented into lines by installments that corresponded to narrative elements in the cartoons. We annotated hedges on the original Roadrunner corpus to create the gold standard for hedge training and detection (the Roadrunner-Hedge corpus; see https://github.com/cogstates/hedging for the annotation codebook).\nThe Roadrunner-Hedge corpus is distributed as a csv file. It is structured as a total of 5,508 lines, over a quarter of which (N=1424) include one or more hedges. The first author annotated hedges in the corpus as in Table 1. Although disfluencies such as fillers (uh, um) and re-starts can function as hedges, we made a principled decision to not code them as such; hedges in our corpus are presumed to be shaped by the speaker's intention, whereas disfluencies are not necessarily under a speaker's control as a communicative signal, but may reflect difficulties in speaking (Grice, 1957; Clark, 1994). Overall word counts for hedges and non-hedges are 1,728 and 38,018 words respectively. Most hedges are one word, but a few cases contain many words. For each line in the csv file (corresponding to a narrative element), hedges are listed (separated by commas) in an adjacent cell. Each line has an average of 0.33 hedges.\nInter-Rater Reliability. To compute inter-rater reliability, a trained research assistant coded 7 randomly-selected transcripts with no overlapping speakers (10% of the corpus). We calculated Cohen's Kappa from each word marked as a hedge within each transcript. There was high agreement"}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nIn this section, we present our hedge classification experiments on the Roadrunner-Hedge corpus, conducted by fine-tuning BERT and performing zero-shot and few-shot experiments with state-of-the-art LLMs. For all experiments, we performed five-fold cross validation using a fixed seed (42), splitting the corpus into a 80/20 train/test split. For our fine-tuning experiments, we did not perform any hyperparameter tuning, and therefore do not have a validation set.\nWe performed all zero-shot, few-shot, and fine-tuning experiments on the fold's respective test sets and report the average and standard deviation over all five folds test sets for F1, precision, and recall.\n5.2 Zero Shot and Few Shot\nFor the zero-shot and few-shot experiments, we used GPT-40 (OpenAI, 2024) and LLaMA-3-8B-Instruct (AI@Meta, 2024), as these two LLMs have achieved state-of-the-art results in many zero-shot or few-shot benchmark tasks.\nWe conducted two classes of zero-shot and few-shot experiments: count/list generation and BIO tag generation. Both prompts began with an instruction detailing the specific task, and a random example. In our few-shot experiments, we provided three fixed hand-crafted examples. For our count/list generation, we prompted the models to list the integer number of hedges present in the utterance and then generated a list of the exact hedge words. For our BIO tag generation, we generated the tokens and their respective tags, where label B represents the beginning of a hedge token or span, I represents the inside of a hedge span, and O represents another token, all separated by \"/\". For example, given the utterance It is like warm, we prompted the model to generate It/O is/O like/B warm/O.\nWe provide our exact prompts with their corresponding instructions in Appendix A. For our GPT-40 experiments, we used the default OpenAI API hyperparameters and a temperature of 1.0.\n5.3 Fine-tuning\nWe performed all fine-tuning experiments using BERT (Devlin et al., 2019), specifically bert-base-uncased. We also performed experiments with the large variants of the model (bert-large), newer encoder-only models like RoBERTa (Liu et al., 2019) and DeBERTa-v3 (He et al., 2021), and encoder-decoder models like Flan-T5 (Chung et al., 2022), but got either worse or closely similar results.\nTask Description All experiments followed a standard BIO token labelling approach to classify hedge tokens (B), tokens inside of hedge spans (I), and all other tokens (O). In other words, given an input utterance of n tokens, the respective BIO labels were output for each of the n tokens. Following the same example as described in our zero-shot and few-shot experiments in Section 5.2, we fine-tuned BERT to classify the tokens as It/O is/O like/B warm/O.\nHyperparameters We followed a standard fine-tuning approach, fine-tuning for a fixed 5 epochs. We set the batch size to 16 and learning rate to 2e-5. We performed five-fold cross validation and test on each folds respective test set. We did not perform any hyperparameter tuning.\n5.4 Results\nThe performance of the models is shown in Table 2, which reports average precision (P), recall (R), and F1 over the five-folds. For our zero-shot, few-shot, and fine-tuning experiments, these metrics are calculated on each fold's test set and then averaged.\nDespite its much smaller parameter count, BERT fine-tuned for BIO tagging outperforms even the best scoring prompting approaches by nearly 20 points in F-measure. This is consistent with a general trend in the literature of more parameter efficient fine-tuning approaches outperforming larger zero-shot and few-shot methods (Liu et al., 2022), though the gap here is larger than one might expect."}, {"title": "6 Error Analysis", "content": "While the fine-tuned BERT model performed fairly well, a certain number of cases did not align with the gold labels in the data. We performed error analysis to understand whether there were any systematic deviations from the corpus annotation.\nWe conducted an error analysis on the top two performing models, the fine-tuned BERT model and the GPT-40 Few-shot List (FSL) model (F1 = 0.91 and 0.71, respectively). Starting with the first fold, we selected the first hundred errors to categorize. These errors are broadly divided into instances where the models failed to detect a hedge (false negatives) and instances where models returned cases that were not annotated hedges (false positives). The remaining errors fell into two other categories: a gold error category, wherein errors in the (human) annotation were discovered, and an \"other\" category.\nOf the hundred errors sampled from the BERT model, approximately the same number of errors were false negatives (26) as false positives (29). Of the hundred errors sampled from the GPT-40 FSL model, 66 were false positives and 25 were false negatives (reflecting the low precision and higher recall for this approach; see Table 3 and 4 for full error descriptions for BERT and GPT-40 FSL models).\nAlthough the corpus annotation does not include the type of hedge (only the presence or absence of hedge tokens), our error analysis looked at hedge types in order to tease apart model behaviors. We observed systematic differences between models in their types of mismatches with the gold standard.\nFalse Positives. First, the GPT-40 FSL model inaccurately classified disfluencies (e.g., \u201cuh\") as hedges in 37 of the 66 false positives reviewed, whereas BERT did not. Second, BERT showed quite a different pattern of mismatches than GPT-40 when classifying \u201clike\u201d, returning false positives that always turned out to be comparatives (e.g., \u201cit's like an open elevator\u201d). These we considered to be true errors in their text form, although some may be ambiguities that could be resolved prosodically.\nFalse Negatives. Tokens denoting approximator hedges (e.g. \u201cthat's basically it\u201d) were frequently misclassified as false negatives by BERT (9 of 26 false negatives reviewed), but never by the GPT-40 FSL model.\nIn addition, Other emerged as a category type for situations that could not clearly be described as false positives, false negatives, or gold errors. In the BERT model, these cases were typically segmentation errors (i.e., an inner token mislabeled as a beginning token).\nNotably, the largest class of errors for the BERT"}, {"title": "7 Discussion", "content": "The results show that even enormous, recently released LLMs cannot reliably recognize hedges. There is no \u201cemergent\u201d ability in LLMs to understand full human linguistic behavior. On the other hand, when we explicitly train a small, rather old LLM (BERT) to perform our task by fine-tuning it, it performs quite well. What this shows is that detecting hedges is a capability that can be learned, but it cannot be learned in the manner that LLMs are taught, namely by simply ingesting large amounts of varied data. We interpret this to mean that if we want to make LLMs able to converse with humans as humans do, we need to understand what capabilities LLMs need and how to provide them with the ability to do so.\nThe prevalence of gold errors discovered by the BERT model raises two interesting points for discussion. First, some of these discrepancies identified by the BERT model were clearly errors made by the human coders; this was true in particular for proxies, which BERT coded for hedges more consistently than did human coders. This error analysis allowed us to iteratively improve the human coding before the final analysis, essentially deploying an LLM-in-the-Loop approach. Second, the discrepancies between BERT and gold coding on the tokens just and like highlight that these types of hedges have high potential for ambiguity-perhaps the very sort of ambiguity that could be resolved by prosody."}, {"title": "8 Limitations and Future Work", "content": "This work represents the first step in our research program that aims to train LLMs to use collateral signals in support of human-LLM dialogue. Once hedges can be recognized by an LLM, it remains to be shown that they can be meaningfully interpreted and generated. Relevant work by Cassell and colleagues has shown that it is possible to generate hedges in tutoring dialogues, but not always positioned where they are most probable or useful (Abulimiti et al., 2023a). In future work, we plan experiments using top-performing models such as BERT and GPT-40 in high- and low-probability situations that systematically vary the certainty associated with prompted-for information (where hedges can be most useful). It is already clear from our pilot trials using ChatGPT 3.5 that LLMs hedge somewhat superficially (hedging where humans wouldn't and failing to hedge where humans would).\nDomains of Dialogue. Here we have used human-generated dialogue from a single domain, retelling stories from Roadrunner cartoons; the training data are text transcripts of speech. Because the initiative was unbalanced in this collaborative task, most of the speaking in each triad was done by the the partner who viewed and retold the cartoon stories in series to the two co-present addressees. A more balanced domain in which partners continuously monitor each other's understanding to do a physical task-such as matching pictures of difficult-to-describe objects-could yield more hedges, distributed differently. We plan to conduct similar tests to replicate the current results on such referential communication corpora collected previously in our lab.\nIt is interesting that despite the fact that there is not a single instance of dialogue in Roadrunner cartoons (apart from Roadrunner's smug, trademark \"meep meep\" upon escaping from Coyote), speakers who retell the story in a dramatic and humorous way do a great deal of what looks like quoting Coyote's and Roadrunner's reactions:\nso then he's saying he's like gone all sad and stuff you know?\nand he's like whatever she's gonna be dead right?\nSuch uses of like in this corpus match the quotation-as-demonstrations forms described by Clark and Gerrig (1990); they count as hedges in that the speaker marks what follows as not verbatim.\nTraining with audio input. Our results for detecting hedges in this transcribed spoken corpus are surprisingly strong, especially given that the LLMs we used were pre-trained primarily on originally written text. But it is well-known that features such as pausing and intonation are related to speakers' levels of commitment to and confidence in their utterances. We plan to incorporate audio into future hedging studies and will explore multi-modal neural architectures fusing both speech and lexical features as we did in (Murzaku et al., 2024) for belief recognition.\nReliability. It is critical to keep in mind that human and LLMs are very different sorts of agents. Psychometric tests show that individual humans are likely to respond consistently when tested repeatedly, whereas an LLM is not (Shu et al., 2024). LLMs have no sense of \"self\" and are likely to respond differently when re-prompted with the same prompt. To the extent that a hedge signals that a speaker does not wish to be held entirely accountable for what they're saying, hedging on the part of an LLM may actually be desirable as a way to encourage users to not assume they can hold it accountable. On the other hand, it may be desirable for an LLM to be able to signal its confidence \u2013 the reliability or quality (or lack thereof) of information it's presenting \u2013 through the presence or absence of hedges. Finally, it remains to be seen whether LLMs can learn about interaction through exposure to collateral signals in meaningful contexts."}, {"title": "9 Conclusion", "content": "Our project is grounded in psycholinguistic theory and aims to capture theory-of-mind aspects of hedging among discourse participants. We present the Roadrunner-Hedge corpus, with hedges annotated from naturally occurring dialogues by speakers describing Roadrunner cartoons. We use the corpus to"}, {"title": "B Glossary", "content": "Due to the interdisciplinary nature of this work, we provide below brief definitions for terms which may be unfamiliar. The numbers refer to the pages in this paper in which the term first appears.\nBERT BERT (Devlin et al., 2018) stands for Bidirectional Encoder Representations from Transformers. BERT is a transformer-based model which produces contextual representations of text by conditioning on both the left and right surrounding words. 4\nBIO BIO, short for Beginning, Inside, Outside, is a format for labeling chunks of tokens. Tokens are assigned B if they begin a sequence which should be labeled (e.g., a named entity), I if they belong to a previously begun sequence, and O otherwise. 5\nCohen's Kappa Measure of agreement between two raters that an item falls within a subjective category; higher values denote higher agreement. 4\nF1 The harmonic mean of precision and recall.\nF1 = 2. \\frac{precision \\cdot recall}{precision + recall}\nIt is also called F-measure or F-score. Loosely speaking, the metric is a balance of how often the model is correct when it predicts a particular class (precision), and how often the model predicts that class when it would be correct to do so (recall). 5\nLLM Large Language Models are large (typically by parameter count) models which take in text and produce a distribution over their vocabulary which can be used to predict the next token. 1\nLSTM Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997) are a type of recurrent neural network designed to capture long-range dependencies. 4\nnarrative element Observable events in the Roadrunner cartoon that and were likely to be mentioned in narrations (see Galati and Brennan, 2010). Segmentation by narrative elements allowed for comparisons across speakers for elements realized in each narration. 4\nprecision The number of correct predictions (true positives) for a class divided by the number of times the model predicted that class (true positives + false positives). 5, 12\nrecall The number of correct predictions (true positives) for a class divided by the number of samples which belong to that class (true positives + false negatives). 5, 12\ntemperature A hyperparameter that modifies the next token distribution of language models. Larger temperature values increase the likelihood of lower probability tokens. 5\ntoken The smallest unit of text, often words or sub-words, which are used as the input for various NLP models. 3\nepoch A single pass through the training data. 5"}]}