{"title": "BEYOND LOCAL SHARPNESS: COMMUNICATION-EFFICIENT GLOBAL SHARPNESS-AWARE MINIMIZATION FOR FEDERATED LEARNING", "authors": ["Debora Caldarola", "Pietro Cagnasso", "Barbara Caputo", "Marco Ciccone"], "abstract": "Federated learning (FL) enables collaborative model training with privacy preservation. Data heterogeneity across edge devices (clients) can cause models to converge to sharp minima, negatively impacting generalization and robustness. Recent approaches use client-side sharpness-aware minimization (SAM) to encourage flatter minima, but the discrepancy between local and global loss landscapes often undermines their effectiveness, as optimizing for local sharpness does not ensure global flatness. This work introduces FEDGLOSS (Federated Global Server-side Sharpness), a novel FL approach that prioritizes the optimization of global sharpness on the server, using SAM. To reduce communication overhead, FEDGLOSS cleverly approximates sharpness using the previous global gradient, eliminating the need for additional client communication. Our extensive evaluations demonstrate that FEDGLOSS consistently reaches flatter minima and better performance compared to state-of-the-art FL methods across various federated vision benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated Learning (FL) (McMahan et al., 2017) provides a powerful framework to collaboratively train machine learning models on private data distributed across multiple endpoints. Unlike traditional methods, FL enables edge devices (clients), like smartphones or IoT (Internet of Things) hardware, to train a shared model without compromising their sensitive information. This is achieved through communication rounds, where clients independently train on their local data and then exchange updated model parameters with a central server, preserving data privacy. The optimization on the server side relies on pseudo-gradients (Reddi et al., 2021), i.e., the average of the difference between the global model and the client's update, which serve as an estimate of the true global gradient on the overall dataset. This approach holds immense potential for privacy-sensitive applications, proving its value in areas like healthcare (Liu et al., 2021; Antunes et al., 2022; Rauniyar et al., 2023; Nevrataki et al., 2023), finance (Nevrataki et al., 2023), autonomous driving (Fantauzzo et al., 2022; Shenaj et al., 2023; Miao et al., 2023), IoT (Zhang et al., 2022), and more (Li et al., 2020a; Wen et al., 2023). However, the real-world deployment of FL presents unique challenges stemming from data heterogeneity and communication costs (Li et al., 2020b). Clients gather their data influenced by various factors such as personal habits or geographical locations, leading to inherent differences across devices (Kairouz et al., 2021; Hsu et al., 2020; Shenaj et al., 2023). This results in the global model suffering from degraded performance and slower convergence (Li et al., 2020d; Karimireddy et al., 2020a;b; Caldarola et al., 2022), with instability emerging as client-specific optimization paths diverge from the global one. This phenomenon, known as client drift (Karimireddy et al., 2020b), limits the model's ability to generalize to the overall underlying distribution.\nWhile many FL approaches focus on mitigating client drift through client-side regularization (Li et al., 2020c; Acar et al., 2021; Varno et al., 2022), a recent trend leverages the geometry of the"}, {"title": "2 RELATED WORKS", "content": "Federated framework. In the last few years, Federated Learning (FL) (McMahan et al., 2017) garnered significant attention from both the machine learning and computer vision communities. While the former has primarily focused on optimizing FL algorithms and guaranteeing their convergence (Li et al., 2020d; Acar et al., 2021; Reddi et al., 2021), the latter has explored its applications in real-world settings, spanning diverse domains like autonomous driving (Fantauzzo et al., 2022; Shenaj et al., 2023; Miao et al., 2023) and healthcare (Liu et al., 2021). The key appeal of FL lies in its ability to efficiently learn from privacy-protected, distributed data while complying with regulations and leveraging edge resources. Real-world deployments of FL range across both cross-silo and cross-device settings (Kairouz et al., 2021). This work focuses on the latter, with up to millions of individual devices at the network edge, with typically limited data and computational power, and potential unavailability due to battery life or network connectivity issues. User-specific factors like geographical location, capturing devices and daily habits introduce inherent bias and statistical heterogeneity into the local datasets. In this setting, FEDGLOSS aims to learn a global model that generalizes to the overall data distribution under statistical heterogeneity without increasing communication complexity, unlike other algorithms for local-global consistency in heterogeneous FL.\nFlatness search in FL. Recent research has explored the connection between loss landscape geometry and generalization in heterogeneous FL. Studies suggest that convergence to sharp minima might hinder generalization performance (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Petzka et al., 2021). SAM (Sharpness-Aware Minimization) (Foret et al., 2021) tackles this issue by guiding the optimization toward flatter regions, seeking minima that exhibit both low loss and low sharpness. FEDSAM (Caldarola et al., 2022; Qu et al., 2022) deploys SAM in local training, marking the first step toward leveraging loss surface geometry in FL to reduce discrepancies between local and global objectives, ultimately improving the global model's generalization ability. Following its success, FEDSPEED (Sun et al., 2023b) uses perturbed gradients as SAM to reduce local overfitting, FEDGAMMA (Dai et al., 2023) combines the stochastic variance reduction of SCAFFOLD with SAM and Shi et al. (2023) show FEDSAM's effectiveness in mitigating the negative effects of differential privacy. However, these approaches rely on local sharpness information, assuming its minimization directly translates to a globally flat minimum. This may not always be true, as we hypothesize discrepancies may exist between the geometries of local and global losses. Optimizing local sharpness alone does not guarantee a server model residing in a flat region of the global loss landscape (Fig. 1). Addressing these limitations, FEDSMOO (Sun et al., 2023a) applies ADMM (Boyd et al., 2011) to the sharpness measure to enforce global and local consistency. This adds communication overhead, doubling the message size in each round and hindering its real-world practicality. In contrast, our work focuses on minimizing global sharpness while maintaining communication efficiency. Lastly, building on Stochastic Weight Averaging (Izmailov et al., 2018), other works (Caldarola et al., 2022; 2023) use a window-based average of global models across rounds to reach wider minima. Being agnostic to the underlying optimization algorithm, they remain orthogonal to our approach.\nHeterogeneity in FL. The de-facto standard algorithm for FL is FEDAVG (McMahan et al., 2017), which updates the global model with a weighted average of the clients' parameters. However, FEDAVG struggles when faced with heterogeneous data distributions, leading to performance degradation and slow convergence due to the local optimization paths diverging from the global one (Karimireddy et al., 2020b). Reddi et al. (2021) shows FEDAVG is equivalent to applying Stochastic Gradient Descent (SGD) (Ruder, 2016) with a unitary learning rate on the server side, using the difference between the initial global model parameters and the clients' updates as pseudo-gradient, opening the door to alternative optimizers beyond SGD to improve performance and convergence speed. Building on this intuition, this work proposes SAM (Foret et al., 2021) as a server-side optimizer to enhance generalization by converging toward global flat minima. Since SAM requires two optimization steps per iteration, a direct adaptation to the FL setting would double communi-"}, {"title": "3 BACKGROUND", "content": "This section introduces the FL problem setting and preliminary notations on SAM (Foret et al., 2021) and FEDSAM (Caldarola et al., 2022; Qu et al., 2022).\n3.1 PROBLEM SETTING\nIn FL, a central server communicates with a set of clients C for T rounds. The goal is to learn a global model f (w) : X \u2192 Y parametrized by w \u2208 Rd, where X and Y are the input and the output spaces respectively. In image classification, X contains the images and Y their corresponding labels. Each client k \u2208 C has access to a local dataset Dk of Nk pairs {(xi, Yi), xi \u2208 X, Yi \u2208 Y}1. In realistic heterogeneous settings, clients usually hold different data distributions and quantity, i.e., Di \u2260 Dj and Ni\u00bf \u2260 Nj Vi \u2260 j \u2208 C. The global FL objective is:\n$$min_w {f(w) = \\frac{1}{|C|} \\sum_{k \\in C} f_k(w), \\quad f_k(w) \\equiv E_{\\xi_k}(w, \\xi_k),}$$\nwhere C |C| is the total number of clients, fk is the empirical loss on the k-th client (e.g., cross-entropy loss) and \u03bek is the data sample randomly drawn from the local data distribution Dk. The training process is a two-phase optimization approach within each round t \u2208 [T]. First, due to potential client unavailability, a subset of selected clients Ct C C trains the received global model using their local optimizer CLIENTOPT (e.g., SGD, SAM). Then, the server aggregates their updates with a server optimizer, SERVEROPT. FEDOPT (Reddi et al., 2021) solves Eq. (1) as\n$$\u0394_\u03a3^t \\leftarrow \\sum_{k \\in C_t} \\frac{N_k}{N}(w_k^t - w^t) \\text{ and}$$\n$$w^{t+1} \\leftarrow w^t - SERVEROPT(w^t, \u0394_\u03c9^t, \u03b7_s),$$\nwhere \u0394 is the global pseudo-gradient at round t, N = \u2211k\u2208ct Nk the total number of images seen during the current round, \u03b7s the server learning rate, wt the global model and w the local update resulting from training on client k's data with CLIENTOPT for E epochs. FEDAVG (McMahan et al., 2017) computes wt+1 as \u2211kect Nk/Nw, corresponding to one SGD step on the pseudo-gradient \u0394 with \u03b7s = 1 (Reddi et al., 2021)."}, {"title": "3.2 SHARPNESS-AWARE MINIMIZATION", "content": "SAM (Foret et al., 2021) jointly minimizes the loss value and the sharpness of the loss landscape by solving the min-max problem\n$$min_w {F(w) = \\max_{||e||_p} f (w + e)},$$\nwhere e is the perturbation to estimate the sharpness, f the loss function, p the neighborhood size and || || the 12 norm. Using the first-order Taylor expansion of f, SAM efficiently solves the inner maximization as\n$$arg \\max_{||e||_p} f (w) + e^T \\nabla w f (w) = p \\frac{\\nabla w f (w)}{||\\nabla w f (w) ||}.$$"}]}