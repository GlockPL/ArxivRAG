{"title": "Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation", "authors": ["Gianni Franchi", "Dat Nguyen Trong", "Nacim Belkhir", "Guoxuan Xia", "Andrea Pilzer"], "abstract": "Uncertainty quantification in text-to-image (T2I) generative models is crucial for understanding model behavior and improving output reliability. In this paper, we are the first to quantify and evaluate the uncertainty of T2I models with respect to the prompt. Alongside adapting existing approaches designed to measure uncertainty in the image space, we also introduce Prompt-based UNCertainty Estimation for T2I models (PUNC), a novel method leveraging Large Vision-Language Models (LVLMs) to better address uncertainties arising from the semantics of the prompt and generated images. PUNC utilizes a LVLM to caption a generated image, and then compares the caption with the original prompt in the more semantically meaningful text space. PUNC also enables the disentanglement of both aleatoric and epistemic uncertainties via precision and recall, which image-space approaches are unable to do. Extensive experiments demonstrate that PUNC outperforms state-of-the-art uncertainty estimation techniques across various settings. Uncertainty quantification in text-to-image generation models can be used on various applications including bias detection, copyright protection, and OOD detection. We also introduce a comprehensive dataset of text prompts and generation pairs to foster further research in uncertainty quantification for generative models. Our findings illustrate that PUNC not only achieves competitive performance but also enables novel applications in evaluating and improving the trustworthiness of text-to-image models.", "sections": [{"title": "1. Introduction", "content": "Generative models have revolutionized artificial intelligence, making significant strides in fields such as robotics [2], healthcare [28], and physics [34]. These models have elevated the performance of Al systems by enabling the generation of complex data representations. However, while generative models offer immense potential, they do not address all challenges, especially those related to robustness and uncertainty in Deep Neural Networks(DNNs)-issues that have become even more pronounced in generative settings.\nIn this paper, we focus on the uncertainty quantification in deep generative models, in particular, we are the first to investigate modern text-to-image (T2I) models where the input is a natural language prompt [8, 18, 48\u201350]. These models have surged in popularity in recent years due to their capacity to generate high-quality images conveniently in a user-guided fashion, finding usage in a wide array of real-world applications. We are the first to investigate and propose a dedicated approach for quantifying uncertainty in text-to-image generation, aiming to fill a critical gap in generative AI research. The task of quantifying uncertainty in these models is not only complex but also crucial. Understanding the uncertainty in generative models can be invaluable for applications such as Out-of-Distribution (OOD) detection, trustworthy AI, and decision-making in high-stakes domains. To the best of our knowledge, no existing work has comprehensively addressed the challenge of uncertainty quantification in text-to-image generative models.\nExisting research explores uncertainty estimation and OOD detection in the image/output space [4, 5, 23, 39]. For example, given a test image, can we determine whether or not it is OOD? On the other hand, the uncertainty of a generation with respect to conditioning (such as text) is underexplored. We argue that this is an important area to be addressed, as it represents a widespread real-world application of deep generative models. In this case, it remains an open question as to how should we define uncertainty, and given a definition, how should we quantify it? As illustrated in Fig. 1, focusing on uncertainty quantification in text-to-image generation could enable novel applications, allowing us to extract and interpret the knowledge embedded within models using language.\nTo answer these questions, we first argue that uncertainties should be rooted in the semantics of the image and the prompt. We then adapt and evaluate a number of existing image-space approaches, as well as proposing a novel method for uncertainty quantification in T2I generative models, which we refer to as \"Prompt-based UNCertainty Estimation for T2I Generation\" (PUNC). Our approach leverages the growing capabilities of Large Vision-Language Models (LVLMs) to extract the semantics from generated images. LVLMs, which have been trained on vast amounts of text and visual data, can serve as powerful tools for interpreting the underlying meaning of text/image inputs and assessing how variations in phrasing or semantics impact the uncertainty of the generated outputs. Our hypothesis is that as LVLMs become increasingly adept at understanding complex relationships between text and images, they will allow us to better analyze and quantify uncertainty. Through this paper, we will demonstrate Prompt-based Uncertainty Estimation for T2I (PUNC) effectiveness in quantifying uncertainties in generative models. Showcasing its ability to decompose and interpret different types of uncertainties arising from the multimodal nature of the input, offering a new perspective on uncertainty quantification in generative AI.\nTo summarize, our contributions are as follows: (1) We introduce a new task aimed at quantifying uncertainty (with respect to the prompt) in text-to-image generation models, and we share a dataset of prompts to support future research on uncertainty in these models. (2) We both adapt existing image-space approaches to our tasks, as well as propose a novel straightforward method, PUNC, designed to better quantify semantic uncertainty in text-to-image generation. (3) Extensive experiments demonstrate that PUNC, despite its simplicity and computational efficiency, achieves competitive performance. (4) We highlight the practical utility of uncertainty quantification in text-to-image generation by showcasing additional applications where this approach could be beneficial."}, {"title": "2. Related Work", "content": "Deep learning models typically encounter two primary types of uncertainty: aleatoric and epistemic uncertainty [26]. Over the years, a variety of methods have been developed to quantify these uncertainties. Ensemble methods [16, 33, 35, 57, 60], for instance, have often achieved state-of-the-art performance by leveraging multiple models to assess prediction confidence. However, these methods tend to be computationally expensive due to the need to train and maintain several models. Another approach involves Bayesian methods [21], especially Bayesian Neural Networks (BNNs) [19, 27, 42, 56], which offer a theoretically grounded way to quantify uncertainty. Additionally, there are techniques aimed at directly predicting uncertainty within DNNs [10, 20, 30]. Despite their effectiveness, most of these methods are challenging to apply to text-to-image generation, as they rely on ensembling predictions or accessing model uncertainty in ways that are difficult to implement for generative models without explicit access to intermediate predictions.\nGenerative models, as a subset of deep learning, also"}, {"title": "3. Uncertainty in Image Generation Models", "content": "Consider a deep generative model $p_\\theta(x)$ with parameters $\\theta$ that models the data distribution $P_{data}(x)$ of $x \\in \\mathbb{R}^D$. We may be interested in measuring the uncertainty of our model $p_\\theta$ with respect to some test datum \u00e6*. For example, consider the case where we want to determine whether or not $x^*$ was drawn from $P_{data}(x)$, i.e. out-of-distribution (OOD) detection. This can be achieved by quantifying the uncertainty of $p_\\theta$ that arises from a lack of knowledge of $x^*$, i.e. epistemic uncertainty [26]. A range of existing work that leverages diffusion-based generative models for OOD image detection [23, 39] fall into the above paradigm. From hereon we will refer to this as generation/image-space uncertainty. This is illustrated in Fig. 2 (top).\nIn this work, however, we are primarily interested in a different scenario: text-to-image generation. Consider instead a conditional model $p_\\theta(x|c)$ that models the conditional data distribution $P_{data}(x|c)$. This describes real-world text-to-image generation, where image x is drawn from $p_\\theta (x|c^*)$ conditioned on some test text prompt $c^*$. In this case, we wish to quantify the uncertainty of $p_\\theta$ not with respect to some test $x^*$ (e.g. image) but instead with respect to a test condition $c^*$. We will hereon refer to this as condition/prompt-space uncertainty. This is illustrated in Fig. 2 (bottom).\nWe argue that in this case, uncertainties should be concerned with semantics, as intuitively, these are what inherently link the user's intentions, represented by the text prompt, and the generated output image. This is similar to recent discourse on uncertainty estimation for large language models [32]. If we consider the semantic concepts in the prompt and generated image, then we can define useful concepts of uncertainty (illustrated in Fig. 3):\n\u2022 Aleatoric uncertainty is irreducible uncertainty in the data distribution $P_{data}(x|c)$. From the perspective of semantics, high aleatoric uncertainty would be where a variation of generated concepts may arise from a single prompt. For example, a spelling mistake where fish is mistyped as fis in the prompt may result in either a \"fish\u201d or a \"fist\" appearing in the image.\n\u2022 Epistemic uncertainty should correspond to a model's lack of knowledge of semantic concepts found in a prompt. For example, a model trained on ImageNet will not know what the prime minister of Japan Kishida Fumio looks like, and thus will have high epistemic uncertainty for this semantic concept.\nWe note that broadly speaking uncertainty estimation for deep conditional models is well explored [20\u201322, 24, 26, 33, 35, 41, 58\u201360, 62, 65]. However, research in this area is generally concerned with problem settings where 1) the models explicitly evaluate $p_\\theta(x|c)$ and 2) downstream applications of uncertainty relate directly to the numerical values of x and $p_\\theta(x|c)$. For example, a cross-entropy-trained image classifier explicitly predicts the probability mass $P_\\theta(x|c^*)$ of label \u00e6 given test image $c^*$. Po(x|c*) directly relates to the likelihood of classification error \u2013 which is an"}, {"title": "4. Techniques to Quantify Uncertainty", "content": "In order to quantify the uncertainty of text-to-image generation models we will both adapt a number of existing approaches as well as propose a novel approach which we refer to as Prompt-based UNCertainty estimation (PUNC). All methods are illustrated in Fig. 4.\n4.1. Adapting Existing Approaches\nThere are a number of existing methods for quantifying the image-space uncertainty of diffusion models. As diffusion-based models represent a majority of current text-to-image foundation models [8, 18, 48, 50], we choose to adapt these in order to quantify prompt-space uncertainty (Fig. 4). We first present a brief primer on diffusion models to the reader.\n4.1.1. Diffusion Models\nDiffusion models [25] are a powerful class of generative model that generate data by starting from pure noise and then performing iterative denoising. We define a forward process over time t \u2208 [0,1] for the random variable $x_t \\in \\mathbb{R}^D$, conditioned on a text prompt c,\n$p(x_t|x_0, c) = N(x_t; a_tx_0, \\sigma_tI)$ (1)\nwhere $x_0$ is sampled from an unknown data distribution $p(x_0)$. The functions $a_t$ and $\\sigma_t$ define the noise schedule, which controls the level of noise added over time. A stochastic differential equation (SDE) that shares the same conditional distributions as Eq. (1) is given by [31]:\n$dx_t = f_t x_t + g_t dw_t, x_0 ~ p(x_0)$, (2)\nwhere $w_t \\in \\mathbb{R}^D$ represents a Wiener process. The drift and diffusion coefficients are defined as\n$f_t = \\frac{d a_t}{d t} \\frac{1}{a_t} , g_t = \\sqrt{2\\xi_t} \\sigma_t.$ (3)\nGiven a neural network $s_\\theta(x_t, t, c)$ that approximates the score $\\nabla_{x_t} log p(x_t|c)$, we can approximately generate samples from $p(x_0|c)$, starting from pure Gaussian noise, by either solving the reverse-time SDE [53],\n$dx_t = [f_t x_t - g_t \\nabla_{x_t} log p(x_t|c)] dt + g_t dw_t,$ (4)\nwhere $w_t$ is a reverse-time Wiener process, or by solving the probability flow ordinary differential equation (ODE):\n$\\frac{dx_t}{dt} = f_t x_t - \\frac{1}{2} g_t^2 \\nabla_{x_t} log p(x_t|c), X_1 ~ p(X_1).$ (5)\nVarious SDE and ODE solvers/samplers can be leveraged to generate high-quality samples via iterative denoising with few integration steps [29, 40, 52, 53, 61, 64, 67].\nLatent diffusion models (LDMs) [39] perform this process within a compressed latent space rather than directly on pixel space. By first encoding images into a more compact representation, LDMs reduce computational demands, enabling high-quality high-resolution generations. We will not consider the latent representation to keep the notation simple, even though experiments will use LDMs.\n4.1.2. Time-step based approaches\nDDPM-OOD [23]. This approach, developed for image-space OOD detection, involves noising a test sample x to different points in the forward process to get {$x_1, x_2, ...$} and denoising them, generating multiple reconstructions {$20_1, 20_2, ...$}. Similarity is then measured between each reconstruction and the original image. The idea is that test samples x for which the model has higher epistemic uncertainty will lead to reconstructions that differ from the test sample. However, in our case, we do not have a test image x. In order to adapt DDPM-OOD we propose measuring similarity between reconstructions at different t and a baseline generation, $S(20_i, 20)$, where $20$ represents an image directly generated using the original test prompt $c^*$ alone. Both aleatoric uncertainty (e.g. ambiguous prompt) and epistemic uncertainty (e.g. unfamiliar concepts in prompt) could lead to more divergent reconstructions.\nLMD [39]. Another similar approach for image-space OOD detection, known as Learned Manifold Denoising (LMD), instead explores alternative transforms on a to noising. The idea is to lift \u00e6 from its original data manifold and then map it back using the trained diffusion model. If the sample is in-distribution, the mapping process should return it close to its original position on the data manifold. Conversely, if it is OOD, the mapped position will likely diverge significantly from the original. The corruption process typically involves masking portions of the image, with various mask patterns applied. Similar to DDPM-OOD, similarity is measured between the original image and the reconstructed version after applying the diffusion process. Similar to before, since we lack a testimage, we measure the"}, {"title": "4.2. Prompt-based Uncertainty Estimation", "content": "In this section, we propose a new method, Prompt-based UNCertainty estimation for text-to-image generation (PUNC), which leverages Large Vision-Language Models (LVLM) in conjunction with T2I models. We aim to assess the uncertainty in generated images by analyzing the alignment between initial text prompts and reconstructed textual descriptions of the generated images.\nBackground on Large Vision-Language Model (LVLM)\nA LVLM integrates a large language model (LLM) with an image encoder, allowing for robust, multimodal understanding. Given a text prompt c and an image x, a LVLM model, with parameters w, includes:\n\u2022 Image Encoder: $f_{img}(\u00b7)$ processes the input image x and produces an embedding: $z_{img} = f_{img}(x)$\n\u2022 Text Processor with LLM: $f_{txt}(\u00b7,\u00b7)$ takes the prompt c and the image embedding $z_{img}$ as inputs, generating a descriptive or answer-based response: $\\hat{c} = f_{txt}(c, z_{img})$\nThus, a LVLM system generates an interpretation \u0109 that reflects the content of the image given an initial prompt.\nPrompt-Based UNCertainty Estimation for T2I Generation (PUNC) The previously introduced approaches to uncertainty estimation rely on measuring similarity between images. For a given text prompt c, a T2I model generates an image $x ~ p_\\theta(x|c)$. We reason that, even under low uncertainty, the model can produce images that vary in aspects such as color, texture, or object positioning, while still faithfully representing the semantics of prompt c. As previously discussed in Sec. 3, this may result in image-based similarity metrics failing to capture uncertainty at a semantic level, since large differences in the image space may not necessarily correspond to meaningful semantic differences.\nWe propose that advanced LVLMs, such as Molmo [11], LLAMA 3 [15], and GPT-4 [1], are a strong candidate for extracting the semantics from an image, through describing an image via natural language. Building on this insight, our proposed method, PUNC, introduces a novel approach that bypasses direct image comparison and instead evalu-ates the alignment between generated images and the origi-"}, {"title": "5. Experiments & Discussions", "content": "We test PUNC on Various task and various dataset and compared PUNC to DDPM-OOD [23], LMD [39] and 2XDM. We first introduce on Sec. 5 how we have constitute the dataset of prompts. Then on Sec. 5.1 we discuss of different results on these dataset. Then Appendix E explains how we can use the uncertainty of text-to-image model to detect if a model has been trained with some concept related to some fictional character of politicians. Appendix E.3 shows how uncertainty can help to detect bias of T2I models. To conduct our experiments and evaluate the various techniques for quantifying uncertainty, we utilized four T2I models: Stable Diffusion 1.5 (SDv1.5) [50], SDXS [66], SDXL [48], and PixArt-\u03a3 [7]. These diverse T2I models allow us to test a range of architectures, each with unique characteristics and potential weaknesses. SDv1.5, being one of the earliest models, likely has a smaller and less diverse training dataset compared to the others. In contrast, SDXS generates in a single-step, different to the multi-step processes in other models.\nDataset of prompts To conduct our experiments, we require both an in-distribution dataset of prompts and datasets with uncertainty. For the in-distribution prompts, we use the dataset proposed by [13], where high-quality descriptions of ImageNet [12] images are generated using GPT-4[13], referred to as Normal. To simulate epistemic uncertainty, we create out-of-distribution (OOD) datasets using images from underrepresented domains like remote sensing, texture, and microscopic datasets, captioned with LLAVA Next[36], labeled as Remote Sensing, Texture, and Microscopic. For aleatoric uncertainty, we generate two datasets: Vague, containing 2,000 minimally descriptive prompts (e.g., \"An image of ***\"), and Corrupted, derived from Normal by adding grammatical errors (Level 1) and removing 50% of words (Level 2). A detailed summary of these datasets is presented in Tab. 1 and Appendix D. In Appendix B, we provide the details of the hyperparameters"}, {"title": "5.1. Results for Uncertainty Estimation", "content": "Tab. 2 presents the results of PUNC when using Molmo as the LVLM to assess epistemic uncertainty in the T2I models. Our approach, PUNC, outperforms most state-of-the-art techniques on average. However, we observe more mixed results on the Texture dataset, likely due to uncertainty about the presence of texture images in the training data of the T2I models.\nIn Tab. 3, we show the results of PUNC under aleatoric uncertainty, again using Molmo as the LVLM. Our approach outperforms most state-of-the-art methods in this setting as well. Notably, while the ROUGE score tends to perform better for epistemic uncertainty, the optimal metric is less clear for aleatoric uncertainty. Here, the choice between ROUGE and BERTScore may depend on the specific application: because corruption introduces spelling and grammatical errors, BERTScore\u2014which captures semantic similarity\u2014may be more suitable. For additional results using different LVLMs, please refer to Appendix C. In Appendix F, we present qualitative results."}, {"title": "5.2. Applications of uncertainty", "content": "Our study explores multiple applications of uncertainty quantification in T2I models, with detailed results provided in Appendix E. First, we investigate deepfake detection by examining the model's ability to generate recognizable images of prominent politicians (see Appendix E.1). Next, we address copyright concerns by evaluating whether T2I models can accurately replicate iconic, copyrighted characters such as Mickey Mouse and Darth Vader (see Appendix E.2). Finally, we analyze bias in T2I models by testing their capacity to generate individuals across different genders and races for specific job roles (see Appendix E.3).\nUsing a LVLM, we evaluate the presence of specific individuals, copyrighted characters, or demographic concepts in the generated images, demonstrating how uncertainty estimation can aid in detecting potential misuses of generative models. These experiments illustrate the value of text-based uncertainty quantification in analyzing T2I model performance across ethically and legally sensitive applications, enabling assessments that are challenging to achieve with image-based generation alone."}, {"title": "6. Conclusion", "content": "In this work, we set out to be first to investigate uncertainty estimation T2I generation, where the uncertainty is with respect to the text prompt. We both adapt existing image-space uncertainty approaches, as well as present Prompt-based UNCertainty Estimation for T2I models (PUNC), the first approach designed specifically to quantify uncertainty in T2I generation. By leveraging the interpretive power of Large Vision-Language Models to extract semantics from a generated image, PUNC addresses the challenges associated with measuring multimodal uncertainty, enabling a deeper understanding of model behavior with respect to prompt-based conditioning. Our results validate that PUNC provides reliable, fine-grained assessments of aleatoric and epistemic uncertainties, outperforming existing methods that work for image generation.\nPUNC opens avenues for new applications, such as improving model robustness in biased or OOD scenarios and safeguarding against unauthorized content generation, including deepfakes and copyrighted characters. Our experiments show that diffusion models, while powerful, still exhibit significant variances in their ability to adhere to prompt details, particularly with ambiguous, corrupt prompts or OOD prompts. By identifying such uncertainties, PUNC not only supports the development of more trustworthy generative AI but also aids in aligning these models with ethical and regulatory standards.\nOur contributions also include a dataset specifically tailored to uncertainty quantification in text-to-image models, which we hope will serve as a benchmark for future research. Ultimately, this work highlights the potential of prompt-based uncertainty estimation as a critical tool for enhancing the reliability and safety of generative models in real-world applications.\nLimitations. While PUNC demonstrates promising results in quantifying uncertainty for text-to-image generation, several limitations remain. First, our approach relies heavily on the interpretive abilities of LVLMs, which, despite their effectiveness, may introduce biases or inaccuracies inherited from their training data. Second, our analysis focuses primarily on English-language prompts, limiting our method's applicability to multilingual or culturally diverse datasets."}]}